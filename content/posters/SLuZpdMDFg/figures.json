[{"figure_path": "SLuZpdMDFg/figures/figures_0_1.jpg", "caption": "Figure 1: ID-to-3D leverages identity conditioning and score distillation sampling on large diffusion models, achieving high-quality 3D human asset generation from \u201cin-the-wild", "description": "This figure shows the results of the ID-to-3D model.  The left side displays renderings of 3D human heads generated by the model, showcasing high-quality textures and realistic appearance. The middle section shows the input images, which are casually captured images of the individuals in the wild, not studio quality photos.  The rightmost section provides the normal maps for the generated heads.  The figure demonstrates the model's ability to create high-quality, realistic 3D human heads from low-quality input images.", "section": "1 Introduction"}, {"figure_path": "SLuZpdMDFg/figures/figures_1_1.jpg", "caption": "Figure 2: (Left) Overall pipeline. ID-to-3D generates expressive 3D head avatars via ArcFace Yid and textual Ytext conditioning. It uses as prior geometry-oriented g and albedo oriented a pretrained models. Training) The training phase uses SDS to optimize 3D geometry 4g, texture Va, and a set of expressions latent codes kexp. It also leverages random lighting 1 and random expression conditioning Yexp. Inference) At deployment time, ID-to-3D extracts high-quality identity-aware expressive 3D meshes. (Right) ID-consistent expressive 3D heads generated by our method. ID-to-3D creates 3D assets that support relighting, ID-consistent editing, and physical simulation.", "description": "This figure illustrates the overall pipeline of ID-to-3D, a method for generating expressive 3D human head avatars. The left panel shows the training and inference stages. During training, a two-stage Score Distillation Sampling (SDS) pipeline optimizes 3D geometry and texture using ArcFace embeddings for identity and text for expression.  Random lighting and expressions are also used. Inference involves extracting high-quality identity-aware and expressive 3D meshes. The right panel displays examples of ID-consistent 3D heads generated by the method, highlighting its ability to support relighting, editing, and physical simulation.", "section": "3 ID-to-3D"}, {"figure_path": "SLuZpdMDFg/figures/figures_5_1.jpg", "caption": "Figure 3: Qualitative results for text-to-3D (*) and image-to-3D methods. Methods are evaluated under the same text prompt and rendering conditions. DreamCraft3D is reported as DC3D. Geometry is displayed via normal maps in camera coordinates. Using only a small set of 5 images as conditioning, ID-to-3D achieves high geometric quality and realistic textures.", "description": "This figure shows a qualitative comparison of different 3D face generation methods, including the proposed ID-to-3D model.  It compares the results of several state-of-the-art text-to-3D and image-to-3D methods against the proposed ID-to-3D approach. The figure highlights the high geometric quality and realistic textures achieved by ID-to-3D using only 5 input images. The normal maps of the generated 3D faces are shown to visualize the geometry quality.", "section": "4.1 Identity Generation"}, {"figure_path": "SLuZpdMDFg/figures/figures_6_1.jpg", "caption": "Figure 4: (Left) Identity Similarity Distribution between \"in-the-wild\" images and renderings of 3D heads. (Right) Comparative Preference Survey on texture quality (outside) and geometry quality (in). We report % of preferences.", "description": "This figure shows a comparison of different methods for generating 3D heads, specifically focusing on identity preservation and quality of generated textures and geometry. The left panel displays the distribution of identity similarity scores for different methods, illustrating how well each method preserves the identity of the input image. The right panel presents the results of a user preference survey comparing the quality of textures and geometry generated by each method.", "section": "4 Experiments"}, {"figure_path": "SLuZpdMDFg/figures/figures_7_1.jpg", "caption": "Figure 5: ID-to-3D expression diversity. Renderings and normal maps in camera coordinates are taken for 3 identities: Will Smith, Anya Taylor Joy, and Kanye West. Our method achieves fine-grained geometry carving and high-quality texture generation, realistically reproducing various skin tones.", "description": "This figure demonstrates the ability of the ID-to-3D model to generate a wide variety of realistic facial expressions while maintaining identity consistency.  It shows renderings and normal maps for three different individuals (Will Smith, Anya Taylor-Joy, and Kanye West), each displaying eight distinct expressions. The high-quality textures and fine-grained details highlight the model's ability to capture subtle nuances in facial features.", "section": "4.2 Expressive ID-conditioned Generation"}, {"figure_path": "SLuZpdMDFg/figures/figures_8_1.jpg", "caption": "Figure 6: Expressions analysis. ID-to-3D creates a variety of expressions with robust ID consistency. (Left) Visualizations of different expressions for 2 identities (i.e. Bill Gates, Alexander Skarsgard). (Middle) Expression diversity. t-SNE plot visualizing the ID embeddings computed considering different camera poses, expressions, and subjects. Different identities and expressions are clustered separately. (Right) Identity similarity distributions between neutral-pose and remaining expressions. Rendered with [2].", "description": "This figure demonstrates the expressiveness and identity consistency of the ID-to-3D model. The left panel shows example renders of various expressions for two individuals. The middle panel uses t-SNE to visualize the clustering of ID embeddings based on various expressions and camera poses, showing clear separation of identities and expressions. The right panel shows the distribution of identity similarity scores for various expressions, highlighting the model's ability to maintain consistent identity.", "section": "4.2 Expressive ID-conditioned Generation"}, {"figure_path": "SLuZpdMDFg/figures/figures_8_2.jpg", "caption": "Figure 7: Identity-Consistent Editing. (Left) De-aged 3D heads generated using different identity conditioning and the textual prompt: \"...as a cute baby\". Normal maps are displayed in world coordinates next to photorealistic renderings. (Right) Geometry and texture editing with text prompts. ID-to-3D edits appearance and geometric features in an ID-consistent manner.", "description": "This figure shows the results of identity-consistent editing using ID-to-3D. The left side displays de-aged 3D heads generated using different identity conditioning and the text prompt \"...as a cute baby\".  Normal maps are shown alongside photorealistic renderings. The right side demonstrates geometry and texture editing using text prompts, highlighting ID-to-3D's ability to modify appearance and geometric features while maintaining identity consistency.", "section": "4.3 ID-conditional Text-based Customization"}, {"figure_path": "SLuZpdMDFg/figures/figures_9_1.jpg", "caption": "Figure 8: Identity-Consistent Editing with Rich Textual Prompts. ID-to-3D generates ID-consistent assets that reflect both subtle and significant changes in geometry and appearance described in a detailed textual input. Normal maps are displayed in world coordinates next to photorealistic renderings and input prompts.", "description": "This figure demonstrates the ability of the ID-to-3D model to perform identity-consistent editing based on rich textual prompts.  It shows three example identities, each with three variations generated using detailed descriptions that specify aspects like hair color, style, and even fantastical elements (e.g., Dragonborn features).  The results highlight the model's capability to make detailed, consistent modifications to the generated 3D head models, showing a combination of photorealistic renderings and normal maps for each variation.", "section": "Identity-Consistent Editing with Rich Textual Prompts"}]