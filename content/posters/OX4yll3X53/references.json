{"references": [{"fullname_first_author": "Pierre-Antoine Absil", "paper_title": "Convergence of the iterates of descent methods for analytic cost functions", "publication_date": "2005-01-01", "reason": "Provides the foundational convergence analysis of gradient descent methods, which is crucial for understanding the learning dynamics of transformer parameters."}, {"fullname_first_author": "Kwangjun Ahn", "paper_title": "Transformers learn to implement preconditioned gradient descent for in-context learning", "publication_date": "2023-01-01", "reason": "Offers a novel perspective on in-context learning by relating it to preconditioned gradient descent, which is directly relevant to the paper's study of transformer learning dynamics."}, {"fullname_first_author": "Francis Bach", "paper_title": "Effortless optimization through gradient flows", "publication_date": "2020-01-01", "reason": "Provides the theoretical framework for analyzing gradient flows, which is essential to the paper's study of the learning dynamics of transformer parameters."}, {"fullname_first_author": "Ashok Vardhan Makkuva", "paper_title": "Attention with Markov: A framework for principled analysis of transformers via Markov chains", "publication_date": "2024-01-01", "reason": "Provides the foundation for the paper's analysis of single-layer transformers with Markovian data, establishing the loss landscape and highlighting the importance of initialization."}, {"fullname_first_author": "Eshaan Nichani", "paper_title": "How transformers learn causal structure with gradient descent", "publication_date": "2024-01-01", "reason": "Offers a complementary analysis of transformer learning dynamics, focusing on how they learn causal structure, which is closely related to the paper's focus on Markov chains."}]}