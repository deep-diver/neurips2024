[{"type": "text", "text": "Local to Global: Learning Dynamics and Effect of Initialization for Transformers ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ashok Vardhan Makkuva \\* Marco Bondaschi \\* Chanakya Ekbote Adway Girish EPFL EPFL EPFL EPFL ", "page_idx": 0}, {"type": "text", "text": "Alliot Nagle Hyeji Kim Michael Gastpar UT Austin UT Austin EPFL ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent years, transformer-based models have revolutionized deep learning, particularly in sequence modeling. To better understand this phenomenon, there is a growing interest in using Markov input processes to study transformers. However, our current understanding in this regard remains limited with many fundamental questions about how transformers learn Markov chains still unanswered. In this paper, we address this by focusing on first-order Markov chains and single-layer transformers, providing a comprehensive characterization of the learning dynamics in this context. Specifically, we prove that transformer parameters trained on nexttoken prediction loss can either converge to global or local minima, contingent on the initialization and the Markovian data properties, and we characterize the precise conditions under which this occurs. To the best of our knowledge, this is the first result of its kind highlighting the role of initialization. We further demonstrate that our theoretical findings are corroborated by empirical evidence. Based on these insights, we provide guidelines for the initialization of single-layer transformers and demonstrate their effectiveness. Finally, we outline several open problems in this arena. Code is available at: https: //github. com/Bond1995/Markov. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Transformers have been at the forefront of recent successes across various fields including natural language processing [34]. To obtain insights into their impressive sequential modeling capabilities, a notable emerging theme among several recent works is to model the input data as a Markov process. ", "page_idx": 0}, {"type": "text", "text": "Using this Markovian perspective, works such as [24, 14, 8], among others, study the in-context learning capabilities of transformer. [23] analyzes the loss-landscape for the next-token prediction task, while [18] shows an equivalence between the attention mechanism and Markov models. Although these works reveal interesting insights about transformers and their capabilities, many fundamental questions about their learning dynamics remain unanswered. In particular, a comprehensive characterization of their training dynamics vis-a-vis the data distributional properties and the role of initialization is still missing. ", "page_idx": 0}, {"type": "text", "text": "To address this gap, in this paper, we focus on the canonical setting of first-order Markov chains and single-layer transformers and analyze the learning dynamics in this context. Specifically, we prove (Thms. 2, 3, and 8) that the input data properties and the parameter initialization play a significant role in the convergence of the transformer parameters to either local or global minima on the loss surface. Further, we precisely characterize (Figs. 1 and Fig. 2) the specific data characteristics and the region of initialization under which this convergence occurs. Based on these insights, we provide guidelines for the initialization of transformer parameters and empirically corroborate our theoretical findings. On the theoretical front, our analysis provides a novel gradient flow analysis of the transformer parameters, capitalizing on their low-rank structure during training. Our main contributions can be summarized asfollows: ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "\u00b7 Theoretical analysis: We precisely characterize the loss landscape and gradient flow dynamics for single-layer transformers with first-order Markov chains (Secs. 3 and 4). We demonstrate that transformer parameters trained on next-token prediction loss can converge to global or local minima, depending on the initialization and the Markovian data properties, and determine the exact conditions under which this occurs (Thms. 2, 3, and 8). To the best of our knowledge, this is the first result of its kind. ", "page_idx": 1}, {"type": "text", "text": "\u00b7 Insights into initialization: Our theoretical analysis underscores the crucial role of initialization in transformer parameter training. Specifically, we demonstrate how the standard Gaussian initialization scheme can lead the convergence to local or global minima depending on the Markovian data properties (Thms. 2 and 8, Figs. 1 and 2). ", "page_idx": 1}, {"type": "text", "text": "\u00b7 Guidelines: Based on these insights, we provide practical guidelines for parameter initialization, corroborated by empirical evidence demonstrating their effectiveness (Sec. 5.2). ", "page_idx": 1}, {"type": "image", "img_path": "OX4yll3X53/tmp/11bfd38f192fc6587a610ef0ec9ca17d340ea56034cfa02d21a55528277745bf.jpg", "img_caption": [], "img_footnote": ["(a) Gradient fow $(p+q<1$ \uff09 (b) Convergence basin $(p\\!+\\!q\\!<\\!1)$ \uff09(c) Gradient fow $(p+q>1)$ \uff09 (d) Convergence basin ( $\\scriptstyle(p+q>1)$ "], "page_idx": 1}, {"type": "text", "text": "Figure 1: Gradient flow dynamics and initialization effect for single-layer transformers. $(p,q)$ are Markov switching probabilities, and $(e,w)$ are the embedding and weight parameters (Sec. 2). (a), (c): The flow is aligned along energy contour lines, converging to local or global optima. (b),(d): $\\mathcal{T}_{\\star}$ the basin of convergence for global minima, $\\mathcal{T}_{\\mathrm{min}}$ for the local minima, and yellow asymptotes for the saddle point. Notice the contrasting behavior for Gaussian initialization around origin for $p+q\\leqslant1$ ", "page_idx": 1}, {"type": "text", "text": "Notation. We denote scalars by italic lower case letters like $x,y$ and Euclidean vectors and matrices in bold: $\\textstyle\\mathbf{x},\\pmb{y},M$ ,etc. $\\parallel\\cdot\\parallel$ denotes the $\\ell_{2}$ -norm for Euclidean vectors and Frobenius norm for matrices. $[k]\\triangleq\\{1,\\ldots,k\\}$ , and for a sequence $(x_{n})_{n\\geq1}$ , define $x_{k}^{m}\\triangleq(x_{k},\\ldots,x_{m})$ if $k\\geq1$ and $(x_{1},\\ldots,x_{m})$ otherwise. For $z\\in\\mathbb{R}$ , the sigmoid $\\sigma(z)\\triangleq1/(1\\!+\\!e^{-z}),\\mathrm{ReLU}(z)\\triangleq\\operatorname*{max}(0,z)$ and the convex logistic loss $\\ell_{\\log}(z)\\triangleq\\log\\left(1+\\exp(-z)\\right)\\in(0,\\infty)$ . For events $A$ and $B$ \uff0c $\\mathbb{P}\\left(A\\right)$ denotes the probability of $A$ whereas $\\mathbb{P}\\left(A\\mid B\\right)$ the conditional probability. Let $(x,y)$ be a pair of discrete random variables on $[k]\\times[k]$ with the probability mass function (pmf) of $x$ being $p_{x}=(p_{1},\\dots,p_{k})\\in[0,1]^{k}$ Then its Shannon entropy is defined as $\\begin{array}{r}{H(x)\\,=\\,H(\\pmb{p}_{x})\\,\\triangleq\\,-\\sum_{i\\in[k]}p_{i}\\log p_{i}}\\end{array}$ . The conditional entropy is defined to be $H(y|x)\\triangleq H(x,y)-H(x)$ . The entropy rate of a stochastic process $(x_{n})_{n\\geq1}$ is defined as $\\textstyle\\operatorname*{lim}_{n\\to\\infty}H(x_{1}^{n})/n$ . We simply write $x=y$ to mean $\\mathbb{P}\\left(x=y\\right)=1$ We also use the shorthand $\\mathbb{P}\\left(y=j\\mid x\\right)$ for $\\mathbb{P}\\left(y=j\\mid x=x\\right)$ as a function of the random variable $x$ . For $p\\in(0,1)$ \uff0c the binary entropy function $h(\\cdot)$ is defined as $h(p)\\triangleq-p\\log p-(1-p)\\log(1-p)$ ", "page_idx": 1}, {"type": "text", "text": "2 Problem Setting ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We formally define the problem setting for analysis of single-layer transformers with Markovian data, following [23]. ", "page_idx": 1}, {"type": "text", "text": "Input data. We assume that the input word sequence $\\{x_{n}\\}_{n=1}^{N}\\,\\in\\,\\{0,1\\}^{N}$ is a frst-order timehomogenous Markov chain with a fixed kernel $P\\,=\\,\\left(\\mathbf{\\dot{P}}_{i j}\\right)$ That is, the transition probability ", "page_idx": 1}, {"type": "text", "text": "$P_{i j}\\,\\triangleq\\,\\mathbb{P}\\,(x_{n+1}=j\\mid x_{n}=i)\\,=\\,\\mathbb{P}\\,\\big(x_{n+1}=j\\mid x_{n}=i,\\,x_{1}^{n-1}\\big)$ , for any $x_{1}^{n-1},i,j\\ \\in\\ \\{0,1\\}$ . In particular, we consider $P=[1-p,p;\\,q,1-q]$ where $p=P_{01}=\\mathbb{P}\\left(x_{n+1}=1\\mid x_{n}=0\\right)$ and $q=$ $P_{10}=\\mathbb{P}\\left(x_{n+1}=0\\mid x_{n}=\\Bar{1}\\right)$ denote the switching probabilities from the states O and 1 respectively. We call $p+q$ , the switching factor. We assume that the process is already mixed, i.e. $x_{n}\\sim\\pi$ for all $n$ , where $\\pi\\triangleq(\\pi_{0},\\pi_{1})=(q,p)/(p+q)$ is the stationary distribution satisfying $\\pi=\\pi P$ . Succinctly, $(x_{n})_{n\\geq1}\\sim(\\pi,P)$ . For this process, the entropy rate, $\\begin{array}{r}{H(x_{n+1}|x_{n})=\\frac{\\check{\\b{\\Gamma}}}{p+q}\\,\\check{(\\b{q}\\,h(p)+p\\,h(q))}}\\end{array}$ the entropy of the marginal, $H(x_{n})=H(\\pi)$ , are both constant in $n$ ", "page_idx": 2}, {"type": "text", "text": "Transformer architecture. We consider a single-layer transformer with a single-head attention and ReLU non-linearity. Given an input sequence $\\{x_{n}\\}_{n=1}^{N}$ . it performs the following mathematical operations at each $n\\in[N]$ to predict the next-token probability $f_{\\theta}(x_{1}^{n})$ ", "page_idx": 2}, {"type": "equation", "text": "$$\nx_{n}\\in\\{0,1\\}\\xrightarrow[]{\\mathrm{Embedding}}x_{n}\\xrightarrow[]{\\mathrm{Attention}}y_{n}\\xrightarrow[]{\\mathrm{Feed-forward}}z_{n}\\xrightarrow[]{\\mathrm{Linear}}\\log\\mathrm{it}_{n}\\xrightarrow[]{\\mathrm{Prediction}}f_{\\theta}(x_{1}^{n}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x_{n}=x_{n}\\,e+p_{n}\\in\\mathbb{R}^{d},}\\\\ &{y_{n}=x_{n}+\\displaystyle\\sum_{i\\in[n]}\\underbrace{\\mathrm{att}_{n,i}}_{\\in(0,1)}\\cdot W_{V}\\,x_{i}\\in\\mathbb{R}^{d},}\\\\ &{\\qquad z_{n}=y_{n}+W_{2}\\,\\mathrm{ReLU}(W_{1}\\,y_{n})\\in\\mathbb{R}^{d},}\\\\ &{\\log\\mathrm{it}_{n}=\\langle a,z_{n}\\rangle+b}\\\\ &{f_{\\theta}(x_{1}^{n})\\triangleq\\mathbb{P}_{\\theta}\\,(x_{n+1}=1\\mid x_{1}^{n})=\\sigma(\\log\\mathrm{it}_{n})\\in[0,1].}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "H $\\pmb{\\theta}\\triangleq(\\pmb{e},\\{\\pmb{p}_{n}\\}_{n=1}^{N},...,\\pmb{W}_{1},\\pmb{W}_{2},b,\\pmb{a})\\in\\mathbb{R}^{D}$ $\\S$ $\\left\\{x_{n}\\right\\}_{n=1}^{N}$ Markovian, i.e. $\\mathbb{P}\\left(x_{n+1}=1\\mid x_{1}^{n}\\right)=\\mathbb{P}\\left(x_{n+1}=1\\mid x_{n}\\right)$ , the transformer is agnostic to this fact and it can potentially utilize the full past $x_{1}^{n}$ in the Attention layer, via the attention weights $\\mathrm{att}_{n,i}$ , to predict the next-symbol probability $f_{\\pmb\\theta}(x_{1}^{n})\\stackrel{=}{=}\\mathbb{P}_{\\pmb\\theta}\\left(x_{n+1}=1\\mid x_{1}^{n}\\right)$ . Note that it suffices to estimate the symbol 1 probability as the vocabulary is binary. We also refer to the above architecture as \"full model'. ", "page_idx": 2}, {"type": "text", "text": "Loss and training. The transformer parameters $\\pmb{\\theta}$ are usually initialized according to standard Gaussian distribution $\\mathcal{N}(0,\\sigma^{2}I)$ with a small variance $\\sigma^{2}$ [26] and are trained using gradient-based methods to minimize the cross-entropy loss on the next-token prediction, i.e. ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}L(\\theta),\\quad L(\\theta)\\triangleq-\\frac{1}{N}\\sum_{n\\in[N]}\\mathbb{E}_{x_{1}^{n+1}}[x_{n+1}\\cdot\\log f_{\\theta}(x_{1}^{n})\\,+(1-x_{n+1})\\cdot\\log(1-f_{\\theta}(x_{1}^{n}))].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "When the input sequence $\\{x_{n}\\}_{n=1}^{N}\\sim(\\pi,P)$ the minmal lo equals its entro-rat . $L_{\\star}$   \nmine $L(\\pmb\\theta)\\doteq H(x_{n+1}|x_{n})$ [11]. ", "page_idx": 2}, {"type": "text", "text": "Loss landscape. A key surprising observation in [23] is that the loss function $L(\\cdot)$ admits both the global and local minima depending on the switching factor $p+q$ of the Markovian data, and the weight-tying of the embedding and linear weights ( $\\left[e=a\\right]$ ) of the transformer. In particular, they showthat ", "page_idx": 2}, {"type": "text", "text": "(i for all $(p,q)~\\in~(0,1)^{2}$ , there exists a global minimum $\\theta_{\\star}$ for the loss $L$ such that its prediction matches the Markov kernel, i.e. $\\mathbb{P}_{\\pmb{\\theta}_{\\star}}$ $(x_{n+1}=1\\mid x_{1}^{n})=\\mathbb{P}\\left(x_{n+1}=1\\mid x_{n}\\right)$   \n(i) if $p+q>1$ and the weights are tied $\\left(e=a\\right)$ 0, there exists a bad local minimum $\\theta_{\\mathrm{min}}$ for $L$ whose prediction equals the marginal, i.e. $\\mathbb{P}_{\\theta_{\\pi}}$ $.\\,(x_{n+1}=1\\mid x_{1}^{n})=\\mathbb{P}\\,(x_{n+1}=1)$ ", "page_idx": 2}, {"type": "text", "text": "In view of these results, we focus on the weight-tying scenario and hence let $e=a$ to be a single parameterin $\\mathbb{R}^{d}$ Thus, $\\pmb{\\theta}=(e=\\pmb{a},\\{p_{n}\\}_{n=1}^{N},\\dots;\\bar{\\pmb{W}}_{1},\\pmb{W}_{2},b)$ . We interchangeably refer to $\\pmb{\\theta}$ as both the transformer and the set of parameters. ", "page_idx": 2}, {"type": "text", "text": "Our objective. While the aforementioned results detail the static landscape of the loss, they do not characterize the learning dynamics on the loss surface and the effect of initialization, which plays a central role in training machine learning models [5]. In view of these shortcomings, the main objective of this paper is to address the following question: ", "page_idx": 2}, {"type": "text", "text": "$(Q.I)$ : Can we explain how the initialization and learning dynamics affect the convergenceof thetransformerparameters $\\pmb{\\theta}$ tothelocal orglobal optima? ", "page_idx": 2}, {"type": "text", "text": "3 Canonical Low-rank Parameterization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Motivation. Given the complexity of the transformer architecture and the non-convex loss function, it is challenging to analyze the learning dynamics directly [24, 14]. To tackle this, we capitalize on the following empirical observation [23] which is the motivating idea behind our approach: when trained by gradient-based methods, the weight matrices $(W_{V},\\dots,W_{1},W_{2})$ at the optima $\\theta_{\\star}$ and $\\theta_{\\mathrm{{min}}}$ exhibit rank-one structure, whose eigenvector is the same direction in which the both the token embedding $^e$ and the positional embeddings $\\pmb{p}_{n}$ are all aligned in. Interestingly, such low-rank solutions can also be shown to be theoretically optimal ( $\\S\\mathrm{~B~}$ details these structures). While these observations illustrate the implicit bias towards low-rank solutions at the final convergence, a natural question arises: if we initialize with low-rank parameters, will they remain low-rank during training? In Sec. 5.1, we affirmatively address this based on a thorough empirical evaluation for single-layer transformers and inspired by these empirical phenomena, without loss of generality, we restrict our attention to these low-rank manifolds to characterize the learning dynamics. This is similar in spirit to [24], where they assume special attention matrix structure for learning induction heads. ", "page_idx": 3}, {"type": "text", "text": "Parameterization. More specifically, we consider a special low-rank parameterization that is empirically observed and capitalize on it to address $(Q.l)$ . Interestingly, along this low-rank manifold, it suffices to consider a reduced set of parameters $\\pmb{\\theta}\\in\\mathbb{R}^{2}$ Or $\\pmb{\\theta}\\in\\mathbb{R}^{3}$ givenby: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pmb{\\theta}=(e,w)\\in\\mathbb{R}^{2},\\mathrm{~or~}\\pmb{\\theta}=(e,w,a)\\in\\mathbb{R}^{3}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here $e$ denotes the embedding scalar, $w$ the weight, and $a$ the attention parameter respectively. Now we describe the parameterization of the transformer vis- $\\acute{\\cdot}$ -vis these scalars and refer to $\\S\\,\\mathrm{C}$ for a more detailed descripton. Let the input $\\{x_{n}\\}_{n=1}^{N}$ be a first-order Markov chain as in Sec. 2 and let $n\\in[N]$ be fixed. Then we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathrm{Embedding:}\\;e=e\\cdot\\alpha,\\,p_{n}=\\left(-\\displaystyle\\frac{e}{2}\\right)\\cdot\\alpha\\to x_{n}=e\\left(x_{n}-\\displaystyle\\frac{1}{2}\\right)\\alpha,\\quad e\\in\\mathbb{R},\\alpha\\in\\{\\pm1\\}^{d}/\\sqrt{d},}\\\\ &{}&{\\mathrm{tion:}\\;W_{V}=\\alpha\\,v^{\\top}\\to y_{n}=e\\left(x_{n}-\\displaystyle\\frac{1}{2}\\right)\\alpha+\\underbrace{\\langle v,\\alpha\\rangle}_{\\propto a\\approx0}\\left(\\sum_{i\\in[n]}{\\mathrm{att}_{n,i}\\cdot e\\left(x_{i}-\\displaystyle\\frac{1}{2}\\right)}\\right)\\alpha,v\\in\\mathbb{R}^{d}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The scalar $a$ is the product of $\\langle\\boldsymbol{v},\\alpha\\rangle$ and the scaling in the attention weights $\\mathrm{att}_{n,i}$ (Eq. (39)), which is empirically close to zero for first-order Markov chains. Hence for the ease of exposition, we first let $a=0$ and analyze the general case when $a\\in\\mathbb{R}$ in Sec. 4.1. We continue: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{rd}:W_{1}=\\frac{|w|}{\\sqrt{d}}\\,\\mathbf{1}\\,\\alpha^{\\top},W_{2}=\\frac{w}{\\sqrt{d}}\\,\\alpha\\,\\mathbf{1}^{\\top}\\rightarrow z_{n}=e\\left(x_{n}-\\frac{1}{2}\\right)\\left(1+4w|w|x_{n}\\right)\\alpha,w\\in\\mathbb{R}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "1 is the all-one vector in $\\mathbb{R}^{r}$ with $r=4d$ typically in practice. Substituting this ${\\boldsymbol{z}}_{n}$ in the linear layer with $e=a$ and bias $b\\in\\mathbb{R}$ , the logits and the probabilities simplify to: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathrm{Linear:logit}_{n}(e,w,b)=e^{2}(1+2w|w|)\\,x_{n}+b-\\displaystyle\\frac{e^{2}}{2}\\in\\mathbb{R},}\\\\ {\\mathrm{~Prediction:~}f_{(\\theta,b)}(x_{1}^{n})=\\sigma\\left(\\mathrm{logit}_{n}\\right)\\in(0,1),\\quad\\theta\\triangleq(e,w).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Finally, using the equivalence between the cross-entropy loss and the logistic loss $\\ell_{\\mathrm{log}}(\\cdot)$ ,theloss function in Eq. (1) can be compactly written as (Lemma 6): ", "page_idx": 3}, {"type": "equation", "text": "$$\nL(\\pmb\\theta,b)=\\frac{1}{N}\\sum_{n\\in[N]}\\mathbb{E}[\\ell_{\\log}\\left((2x_{n+1}-1)\\cdot\\mathrm{logit}_{n}(\\pmb\\theta)\\right)],\\quad\\pmb\\theta\\in\\mathbb{R}^{2},b\\in\\mathbb{R}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Due to convexity of $\\ell_{\\mathrm{log}}(\\cdot)$ , it follows that $L(\\pmb\\theta,b)$ is convex in the bias $b$ for any fixed $\\pmb{\\theta}$ ,whose minimizer, $\\begin{array}{r}{b_{\\star}(\\pmb\\theta)=\\mathrm{argmin}_{b\\in\\mathbb{R}}\\,L(\\pmb\\theta,b)}\\end{array}$ has a closed form expression (Lemma 5). Hence, without loss of generality, we consider the loss with this optimal bias $b_{\\star}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\nL(\\pmb\\theta)\\triangleq L(\\pmb\\theta,b_{\\star})=\\frac{1}{N}\\sum_{n\\in[N]}\\mathbb{E}\\left[\\ell_{\\log}\\left(\\left(2x_{n+1}-1\\right)\\left(e^{2}(1+2w|w|)\\,x_{n}+b_{\\star}-\\frac{e^{2}}{2}\\right)\\right)\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Empirically, this roughly translates to running the gradient-based algorithm for the bias for more steps at each $\\pmb{\\theta}$ . In practice, one additional step is usually sufficient (see Sec. 5). Eq. (5) resembles the standard logistic regression loss [31] whose binary labels are $2x_{n+1}-1\\in\\{\\pm1\\}$ and the logits given by $e^{2}(1+2w|w|)\\,x_{n}+b_{\\star}-e^{2}/2$ , for each $n\\in[N]$ . The key difference here is that the logits are a non-linear function of the parameters $(e,w)$ unlike in the standard setting. ", "page_idx": 4}, {"type": "text", "text": "We briefy summarize our assumptions below. ", "page_idx": 4}, {"type": "text", "text": "Assumption 1 (Canonical parameterization). For our theoretical analysis, we assume that the effective transformer parameters are canonically parameterized as $\\pmb{\\theta}=(e,\\dot{w},a)\\in\\mathbb{R}^{3}$ . First we study the scenario when $a=0$ With $\\pmb\\theta=(e,w)$ and build upon these observations to study the general setting of $\\pmb{\\theta}=(e,w,a)$ in Sec. 4.1. ", "page_idx": 4}, {"type": "text", "text": "3.1 Loss Landscape with Canonical Parameterization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "With the new set of parameters $\\pmb{\\theta}=(e,w)\\in\\mathbb{R}^{2}$ , we are now ready to analyze the loss $L(\\cdot)$ in Eq. (5). First we recall the definition of a critical point [20]. A point $\\pmb{\\theta}_{\\star}^{\\star}\\in\\mathbb{R}^{2}$ is a critical or a stationary point for $L$ if $\\nabla L(\\pmb\\theta_{\\star})=0$ A critical point $\\theta_{\\star}$ is a local minimum if there exists a neighborhood $U$ around $\\theta_{\\star}$ such that $L(\\pmb\\theta_{\\star})\\,\\leq\\,L(\\pmb\\theta)$ for all $\\theta\\:\\in\\:U$ , and a local maximum if $L(\\pmb{\\theta}_{\\star})\\,\\geq\\,L(\\pmb{\\theta})$ If the neighborhood $U$ is whole of $\\textstyle\\operatorname{\\dot{R}}^{2}$ , it is a global minimum/maximum. On the other hand, a critical point is a saddle point if for all neighborhoods $U$ around $\\theta_{\\star}$ , there are $\\theta_{1},\\theta_{2}\\in U$ such that $L(\\pmb\\theta_{1})\\leq L(\\pmb\\theta_{\\star})\\leq L(\\pmb\\theta_{2})$ ", "page_idx": 4}, {"type": "text", "text": "Thm. 1 below provides a complete characterization of the loss landscape in terms of the aforementioned critical points. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1 (All critical points).Let the input sequence $\\{x_{n}\\}_{n=1}^{N}\\sim(\\pi,P)$ the transformer parameters $\\pmb{\\theta}=(e,w)\\in\\mathbf{\\bar{R}}^{2}$ , and the next-token prediction loss $L(\\cdot)$ be as in Eq. (5). Then for any $(p,q)\\in(0,1)^{2}$ with $p+q\\ne1$ and $N\\in\\mathbb{N},$ ", "page_idx": 4}, {"type": "text", "text": "(i) the set of all global minima is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Theta_{\\star}(p,q)\\triangleq\\left\\{(e,w)\\in\\mathbb{R}^{2}:e^{2}(1+2w|w|)=\\log\\frac{(1-p)(1-q)}{p q}\\right\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "(ii) the set of all local minima is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Theta_{\\operatorname*{min}}(p,q)\\triangleq\\left\\{(e,w)\\in\\mathbb{R}^{2}:e=0,\\,(p+q-1)(1+2w|w|)>0\\right\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "(ii) the set of all local maxima is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Theta_{\\operatorname*{max}}(p,q)\\triangleq\\left\\{(e,w)\\in\\mathbb{R}^{2}:e=0,\\,(p+q-1)(1+2w|w|)<0\\right\\},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "(iv)and the set of all saddle points is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Theta_{\\mathrm{sad}}(p,q)\\triangleq\\left\\{(0,-1/{\\sqrt{2}})\\right\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Thus the set of all critical points is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left\\{\\pmb{\\theta}\\in\\mathbb{R}^{2}:\\nabla L(\\pmb{\\theta})=0\\right\\}=\\pmb{\\Theta}_{\\star}\\cup\\pmb{\\Theta}_{\\mathrm{min}}\\cup\\pmb{\\Theta}_{\\mathrm{max}}\\cup\\pmb{\\Theta}_{\\mathrm{sad}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In addition, for any $\\pmb{\\theta}_{\\star}\\in\\Theta_{\\star},\\pmb{\\theta}_{\\operatorname*{min}}\\in\\Theta_{\\operatorname*{min}},\\pmb{\\theta}_{\\operatorname*{max}}\\in\\Theta_{\\operatorname*{max}},$ and $\\pmb{\\theta}_{\\mathrm{sad}}\\in\\Theta_{\\mathrm{sad}}$ the loss values satisfy ", "page_idx": 4}, {"type": "equation", "text": "$$\nH(x_{n+1}\\mid x_{n})=L(\\pmb\\theta_{\\star})<L(\\pmb\\theta_{\\operatorname*{min}})=L(\\pmb\\theta_{\\operatorname*{max}})=L(\\pmb\\theta_{\\mathrm{sad}})=H(x_{n+1}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Proof. We refer to $\\S\\,\\mathrm{E}$ ", "page_idx": 4}, {"type": "text", "text": "Fig. 1 illustrates the loci of these critical points for $p+q<1$ and $p+q>1$ . Motivated by empirical observations, while [23] characterizes local minima for $p+q>1$ , it is interesting to note that our Thm. 1 shows that local minima also exist for $p+q<1$ (Eq. (7) and Fig. 3a). So why did they find the minima only when $p+q>1?$ The answer to this, and more broadly to question $(Q.l)$ lies in the learning dynamics and initialization for $\\pmb{\\theta}$ , which we study in the next section. ", "page_idx": 4}, {"type": "text", "text": "4  Learning Dynamics ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Capitalizing on the loss landscape in terms of the critical points in Thm. 1, we now focus on the convergence of gradient-based algorithms to these points. In this regard, we analyze the dynamics of the gradient-flow (GF), which can be viewed as a continuous-time analogue of gradient-descent [6]. The gradient-flow of the parameters, $(\\pmb\\theta_{t})_{t\\geq0}$ ,On $L$ isgoverned by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\pmb{\\theta}_{t}}{\\mathrm{d}t}=-\\nabla L(\\pmb{\\theta}_{t}),\\quad\\pmb{\\theta}_{t}=(e_{t},w_{t})\\in\\mathbb{R}^{2},\\,t\\geq0,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\pmb{\\theta}_{t}\\triangleq\\pmb{\\theta}(t)$ is a $C^{1}$ (continuously differentiable) curve in $\\mathbb{R}^{2}$ starting with a randomly initalized $\\pmb{\\theta}_{0}$ . To characterize these trajectories, we define an energy function $\\mathcal{E}(\\cdot,\\cdot)$ , which plays a crucial in the GF dynamics. It is defined as ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\mathcal{E}}(e,w)\\triangleq e^{2}-(w^{2}+\\operatorname{sign}(w)\\cdot\\log|w|),\\quad\\forall(e,w)\\in\\mathbb{R}^{2}\\setminus\\mathrm{e.axis},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where e-axis $\\triangleq\\{(e,w=0)\\}$ . Note that $\\mathcal{E}$ is well-defined and finite for all the points in its domain. On the other hand, $\\begin{array}{r}{\\operatorname*{lim}_{w\\to0^{-}}\\mathcal{E}(e,w)=-\\infty}\\end{array}$ whereas $\\mathrm{lim}_{w\\to0^{+}}\\,\\mathcal{E}(e,w)=\\infty$ for any fixed $e\\in\\mathbb{R}$ Thus the e-axis corresponding to $w=0$ serves as an energy barrier for the flow. Figs. 3a and 3b illustrate this by visualizing the energy contour lines. The utility of the energy function is captured in the following lemma. ", "page_idx": 5}, {"type": "text", "text": "Lemma 1 (Constant energy along the fow). For any $(p,q)~\\in~(0,1)^{2}$ and initialization $\\pmb\\theta_{0}\\ =$ $(e_{0},w_{0})\\in\\mathring{\\mathbb{R}}^{2}$ ,let $(\\pmb\\theta_{t})_{t\\geq0}$ be the corresponding $G F$ trajectory starting from $\\pmb\\theta_{0}$ If $\\pmb{\\theta}_{0}\\in\\mathbb{R}^{2}\\setminus\\mathrm{e-a2}$ xis, the energy stays constant along the trajectory, i.e. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{E}(\\pmb{\\theta}_{t})=e_{t}^{2}-\\left(w_{t}^{2}+\\mathrm{sign}(w_{t})\\cdot\\log|w_{t}|\\right)=\\mathcal{E}(\\pmb{\\theta}_{0}),\\quad\\forall t\\geq0.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "On the other hand, if $\\theta_{0}\\in\\mathrm{e}$ -axis, we have that $\\theta_{t}\\in\\mathrm{e}.$ -axisfor all $t\\geq0$ with $w_{t}=w_{0}=0$ i.e.if we initialize on the e-axis, the trajectory always stays there. ", "page_idx": 5}, {"type": "text", "text": "We are now ready to present the main results of our paper. Specifically, Thm. 2 and Thm. 8 highlight the role of the switching factor of the Markovian data, $p+q$ , and the parameter initialization, $\\theta_{0}$ ,in deciding whether the GF converges to local optima or global optima. First we define the energy value $\\mathcal{E}_{\\mathrm{sad}}\\triangleq\\bar{\\mathcal{E}}(e=0,w=-1/\\sqrt{2})\\stackrel{.}{=}-(1+\\log2)/2.$ ", "page_idx": 5}, {"type": "text", "text": "Theorem 2 (GF dynamics for $p+q>1)$ 0. Let $(p,q)\\in(0,1)^{2}$ with $p+q>1$ the input sequence be $\\{x_{n}\\}_{n=1}^{N}\\sim\\(\\pi,\\dot{P}),$ and $(\\pmb\\theta_{t})_{t\\geq0}$ be the corresponding $G F$ trajectory startingfrom $\\theta_{0}$ Then for all initializations $\\pmb{\\theta}_{0}\\in\\mathbb{R}^{2}$ the gradient fow converges toa critical point of the loss $L$ .That is, there exists a $\\theta_{\\mathrm{{lim}}}\\in\\mathbb{R}^{2}$ such that $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}\\pmb{\\theta}_{t}=\\pmb{\\theta}_{\\operatorname*{lim}}}\\end{array}$ and $\\nabla L(\\theta_{\\mathrm{lim}})=0$ In particular, $\\theta_{\\mathrm{lim}}$ is $a$ ", "page_idx": 5}, {"type": "text", "text": "(i) a local minimum if ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\theta_{0}\\in{\\mathcal{Z}}_{\\operatorname*{min}}\\triangleq\\left\\{(e,w):w\\in(-1/\\sqrt{2},0),\\,e\\in(-g(w),g(w)),\\,g(w)=\\sqrt{w^{2}-\\log(-w)+{\\mathcal{E}}_{\\mathrm{sad}}}\\right\\}}\\\\ {\\cup\\left\\{(e,w):w\\ge0\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "(i a saddle pointij $\\theta_{0}\\in\\mathcal{T}_{\\mathrm{sad}}\\triangleq\\Big\\{(e,w):w\\in[-1/\\sqrt{2},0),\\,e=\\pm\\sqrt{w^{2}-\\log(-w)+\\mathcal{E}_{\\mathrm{sad}}}\\Big\\},$   \n(i) a local maximum if $\\theta_{0}\\in\\mathcal{Z}_{\\operatorname*{max}}\\triangleq\\big\\{(e,w):e=0,\\,w<-1/\\sqrt{2}\\big\\}$   \n(iv) and a global minimum if $\\begin{array}{r}{\\pmb{\\theta}_{0}\\in\\mathcal{Z}_{\\star}\\triangleq\\mathbb{R}^{2}\\setminus(\\mathcal{Z}_{\\operatorname*{min}}\\cup\\mathcal{Z}_{\\mathrm{sad}}\\cup\\mathcal{Z}_{\\operatorname*{max}}).}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "Consequently, when $p+q>1$ , if we use the standard initialization $\\theta_{0}\\sim\\mathcal{N}(0,\\sigma^{2}I_{2})$ with $\\sigma^{2}\\ll$ $1/{\\sqrt{2}},$ $\\theta_{\\mathrm{lim}}$ will be a local minimum with high probability. If $p+q<1$ under the same initialization scheme, $\\theta_{\\mathrm{lim}}$ will be a global minimum with high probability. ", "page_idx": 5}, {"type": "text", "text": "Proof sketch. The main idea behind the proof is to show that if we do not initialize on the e-axis, the flows stays on the constant energy contour (Lemma 1) and hence converges to a critical point of theloss $L$ , which is at the intersection of the contour line and the set of critical points (Lemmas. 10 and 11). By determining where these intersections occur, the corresponding basins of convergence $T_{\\mathrm{min}},\\ldots,T_{\\star}$ are obtained by showing that an initialization in a specific set leads to the said critical point (Thm. 1). The proof for $\\theta_{0}\\in\\mathrm{e}$ -axis is similar. \u53e3 ", "page_idx": 5}, {"type": "text", "text": "Figs. 1b and 1d illustrate these initialization sets corresponding to the convergence basins for $p=q=0.9$ and $p=q=0.1$ respectively. An analogous result about GF dynamics for $p+q<1$ is presented in Thm. 8 $(\\S\\,\\mathrm{F}.2)$ . Here a key difference is that small Gaussian initialization around origin leads to a global minimum $\\pmb\\theta_{\\mathrm{lim}}$ with high probability (Fig. 1d). ", "page_idx": 6}, {"type": "text", "text": "Key insights. Together, Thm. 2 and Thm. 8 address our motivating question $(Q.l)$ by fully characterizing the GF dynamics in terms of initialization and input data properties. Specifically, our results explain the phenomenon in [23] wherein they observe local minima for $p+q>1$ more often than for $p+q<1$ , owing to standard Gaussian initialization around origin (Figs. 1b and 1d). However, in practice, we often do not know the input switching factor, raising a natural questions: is there a data-agnostic initialization that always converges to global minima? Indeed, as can be seen from Figs. 1b and 1d, there is a common region of initialization in the negative half-plane above the saddleasymptotes (in yellow) that leads to the global minima convergence irrespective of the switching $p\\!+\\!q$ Mathematically, this region is given by $\\mathcal{Z}_{\\mathrm{common}}\\triangleq\\{(e,w):w<0,|e|>\\sqrt{w^{2}-\\log(-w)+\\mathcal{E}_{\\mathrm{sad}}}\\}$ We empirically corroborate this fact in Sec. 5.2. ", "page_idx": 6}, {"type": "text", "text": "4.1 Gradient Flow with Attention ", "text_level": 1, "page_idx": 6}, {"type": "image", "img_path": "OX4yll3X53/tmp/9e923f5a18e2ef0903fa4487d20b9028228742cfb16bd40debb29e97dadf1eed.jpg", "img_caption": ["Figure 2: Gradient flow dynamics for the canonical parameters $\\pmb{\\theta}=(e,w,a)\\in\\mathbb{R}^{3}$ with the attention scalar $a$ . Notice the contrasting behavior for Gaussian initialization around origin for $p+q$ smaller and greater than one. For an enhanced view of the fow near the origin, please refer to Fig. 5. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "In this section, the consider the attention scalar $a\\in\\mathbb R$ (Sec. 3) and study the gradient flow dynamics with the parameters $\\pmb{\\theta}=(e,w,a)\\,\\in\\,\\mathbb{R}^{3}$ . The parameter $a$ captures the overall scaling from the value, key, and query components in the attention layer. Recall that the soft-max attention weights are given by $\\mathrm{att}_{n,i}\\,\\propto\\,\\mathrm{exp}(\\langle\\pmb{q}_{n},\\pmb{k}_{i}\\rangle/\\sqrt{d})$ , where ${\\pmb q}_{n}\\,=\\,{\\pmb W}_{Q}{\\pmb x}_{n}$ and ${\\pmb k}_{i}\\,=\\,{\\pmb W}_{K}{\\pmb x}_{i}$ are the query and key embeddings for any position $i\\,\\in\\,[n]$ . Using the low-rank structure of the query and key matrices, satisfying $\\pmb{W}_{Q}^{\\top}\\pmb{W}_{K}=(q^{2}d)\\pmb{\\alpha}\\pmb{\\alpha}^{\\top}$ and the value matrix $W_{V}=\\alpha\\pmb{v}^{\\top}$ for some $q\\in\\mathbb{R}$ and $\\pmb{v}\\in\\mathbb{R}^{d}$ (\\$ G), and assuming linear attention $\\mathrm{att}_{n,i}\\propto\\langle\\pmb{q}_{n},\\pmb{k}_{i}\\rangle/\\sqrt{d}$ , we define a single scalar $a\\triangleq\\langle\\pmb{v},\\pmb{\\alpha}\\rangle q^{2}d^{5/2}/4$ that captures the essence of the attention layer (Eq. (39)). We note that linear attention weights are a standard assumption in the transformer analysis literature [3, 36]. Using this parameterization, similar to the steps in Sec. 3, we obtain the final loss function to be ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\tau(\\theta)=\\mathbb{E}\\left[\\ell_{\\log}\\left((2Y-1)\\left(e^{2}\\left[\\left(X-\\frac{1}{2}\\right)\\left(1+a e^{2}\\right)\\left(1+2w|w|\\right)+w|w(1+a e^{2})|\\right]+b_{\\star}\\right)\\right)\\right],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\pmb{\\theta}=(e,w,a)$ and $b_{\\star}$ is the corresponding optimal bias. $L$ recovers the loss in Eq. (5) when $a=0$ . In Thm. 10, we determine the set of all critical points of $L$ in terms of global minima and local optima in closed-form expressions, analogous to Thm. 1. Capitalizing on this characterization, we now shift our focus to the analysis of the gradient flow in $\\mathbb{R}^{3}$ . To this end, let $(\\pmb\\theta_{t})_{t\\geq0}$ be a $C^{1}$ curve in $\\mathbb{R}^{3}$ governed by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\pmb{\\theta}_{t}}{\\mathrm{d}t}=-\\nabla L(\\pmb{\\theta}_{t}),\\quad\\pmb{\\theta}_{t}=(e_{t},w_{t},a_{t})\\in\\mathbb{R}^{3},\\,t\\geq0,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "starting with a randomly initalized $\\pmb{\\theta}_{0}$ . We define the energy function $\\mathcal{E}(\\cdot,\\cdot,\\cdot)$ as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{E}(e,w,a)\\triangleq e^{2}-(w^{2}+\\mathrm{sign}(w)\\cdot\\log|w|)-2a^{2},\\quad\\forall(e,w,a)\\in\\mathbb{R}^{3}\\setminus\\mathrm{ea-plane},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where ea-plane $\\triangleq\\{(e,w=0,a)\\}$ . It is similar to its counterpart in Eq. (11), except for the $2a^{2}$ term. Fig. 2 visualizes this energy surface and the set of critical points, which reveal close resemblance to that of Fig. 1 in $\\mathbb{R}^{2}$ . Capitalizing on the energy function, we now present our main result with the attention. ", "page_idx": 7}, {"type": "text", "text": "Theorem 3 (GF dynamics with attention). For any $(p,q)\\,\\in\\,(0,1)^{2}$ and initialization $\\pmb{\\theta}_{0}\\,\\in\\,\\mathbb{R}^{3}$ \uff0c let $(\\pmb\\theta_{t})_{t\\geq0}$ be the corresponding $G F$ -attn trajectory starting from it. Then for all $\\pmb{\\theta}_{0}\\,\\in\\,\\mathbb{R}^{3}$ ,the gradient flow converges to a critical point of the loss $L$ .That is, there exists a $\\pmb{\\theta}_{\\mathrm{lim}}\\in\\mathbb{R}^{3}$ such that $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}\\pmb{\\theta}_{t}=\\pmb{\\theta}_{\\operatorname*{lim}}}\\end{array}$ and $\\nabla L(\\theta_{\\mathrm{lim}})=0$ Further, ", "page_idx": 7}, {"type": "text", "text": "(i) if $\\pmb{\\theta}_{0}\\in\\mathbb{R}^{3}$ ea-plane, we have $\\mathcal{E}(\\pmb{\\theta}_{\\mathrm{lim}})=\\mathcal{E}(\\pmb{\\theta}_{t})=\\mathcal{E}(\\pmb{\\theta}_{0})$ for all $t\\geq0$ Hence $\\pmb\\theta_{\\mathrm{lim}}$ is at the intersection of the energy contour line $\\mathcal{E}=\\mathcal{E}_{0}$ with that of the set of critical points. (ii) f $\\pmb\\theta_{0}\\in$ ea-plane, we have $\\pmb\\theta_{t}\\in$ ea-plane for all $t\\geq0$ andhence $\\theta_{\\mathrm{lim}}\\in$ ea-plane. ", "page_idx": 7}, {"type": "text", "text": "Proof. We refer to $\\S\\,\\mathrm{G}$ and $\\S\\ N.4$ ", "page_idx": 7}, {"type": "text", "text": "Thm. 3 shows that the learning dynamics with attention closely resemble those without it (Thms. 2 and 8). While the set of all critical points of $L$ , and thus the limit points of the fow, has a closed-form expression (Thm. 10), deriving the same for the initialization sets $\\mathcal{T}_{\\mathrm{min}}$ and $\\mathcal{Z}_{\\star}$ to determine the basin of convergence is technically challenging (see discussion in $\\S\\ \\mathrm{G}_{,}$ 0. Nonetheless, empirical observations with the standard Gaussian initialization around origin reveal a similar picture as in the two-dimensional setting for both the $p+q<1$ and $p+q>1$ cases (Fig. 2). We believe it's an interesting direction of future research to theoretically characterize this, analogous to Thms. 2 and 8. We refer to $\\S\\,\\mathrm{G}$ for additional details and proofs. ", "page_idx": 7}, {"type": "text", "text": "5 Empirical Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We empirically validate our canonical parameterization $\\pmb{\\theta}\\in\\mathbb{R}^{3}$ (Sec. 3) by demonstrating full model convergence to low-rank parameters through both qualitative and quantitative evidence. Qualitatively, we visualize weight matrices across iterations; quantitatively, we plot the percentage of energy captured by the top-rank components across iterations. We then demonstrate the generalization of our theoretical findings on local optima and initialization with canonical parameters to the full model $\\pmb\\theta\\in\\mathbb{R}^{D}$ . We conclude with a discussion on higher-order and multi-state Markov chains. ", "page_idx": 7}, {"type": "text", "text": "5.1Low-rank Parameters ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Full model converges to low-rank. We lt the input Markoy eqguene to be $\\{x_{n}\\}_{n=1}^{N}\\ \\sim$ $(\\pi(p,q),P(p,q))$ for $p~=~0.2,q~=~0.3,N~=~1024$ and consider the full model as defined $d\\ =\\ 8$ $\\theta~=~(e~=$ $\\mathbf{a},\\{p_{n}\\}_{n=1}^{N},\\dots,W_{1},W_{2},b\\}$   \n$B=16$ $t=800$   \nwe track the value matrix $\\overline{W_{V}}\\,\\in\\,\\mathbb{R}^{d\\times d}$ and the weight matrix $W_{1}\\,\\in\\,\\mathbb{R}^{4d\\times d}$ across iterations. We observe that at convergence both $W_{V}$ and $W_{1}$ are approximately rank-one with one of their components being same as the embedding vector (the row in $W_{V}$ and column in $W_{1}$ ). Further, the embedding vector has all entries in $\\{\\pm1\\}$ up to a scaling. We observe the same conclusion for other weight matrices $W_{K,Q},W_{2}$ and for all values of $\\bar{(p,q)}\\in(0,1)^{2}$ . Fig. 3 also quantitatively demonstrates this. ", "page_idx": 7}, {"type": "text", "text": "Full model initialized at low-rank remains low-rank during training. Inpsired by the low-rank structure obtained above, we randomly initialize the weight parameters as rank-one matrices and the embeddings on the hypercube $\\{\\pm1\\}^{d}$ . After the initialization, we train them without any low-rank restrictions, and track them during the course of training. Interestingly, here we observe that the parameters still stay low-rank as illustrated in Fig. 7 and Fig. 3. A similar conclusion holds for the remaining weight matrices. Together these results provide the empirical basis for our canonical parameterization analysis in Sec. 3. ", "page_idx": 7}, {"type": "text", "text": "5.2  Effect of Initialization: Broader Implications ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Now we investigate the findings of Sec. 3 and Sec. 4, derived for the canonical low-rank model, more broadly in the context of full model in Sec. 2. In particular, as shown in Thm. 2 and Fig. 1d for $p+q>1$ , any small initialization around zero would lead a local minima convergence. To test this hypothesis, we compare the standard initialization where all the transformer parameters $\\pmb{\\theta}\\,=\\,(e\\,=\\,\\stackrel{\\cdot}{a},\\{p_{n}\\}_{n=1}^{N},\\dots,\\mathbf{\\dot{W}}_{1},\\pmb{W}_{2},b)$ are randomly chosen around zero with small variance $\\sigma=0.02$ , with a new initialization based on our results, where we initialize the embedding vector $^e$ such that all cordinates are equal to $e=0.5$ $W_{1}$ to be constant with the scalar $w_{1}=1$ and $W_{2}$ constant with $w_{2}=-1$ (corresponding to $\\mathcal{T}_{\\star}$ in Fig. 1d). We indeed observe that the final test loss matches the unigram loss for the standard initialization, while it converges to the optimal bigram loss for our initialization (see Fig. 4). Together these results indicate that though our analysis used canonical parameterization, the corresponding insights are more general and apply more broadly to the general full model. In a similar spirit, analysis of initialization effects for deeper architectures is an interesting avenue of future research. ", "page_idx": 7}, {"type": "image", "img_path": "OX4yll3X53/tmp/fbf5a351c80044fbf87615238cf784df179ebb308f0a18d6cbdf9fa8d83eff76.jpg", "img_caption": ["Figure 3: Convergence to rank one parameters: percentage of energy contained in the first rank component of the weight matrices $W_{1}$ and $W_{V}$ across iterations. The percentage is computed as Eto, where the G's are the ingular values o the marices in descending order. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "OX4yll3X53/tmp/b309f51c62e6fae40939a867062262d6ce47d6161f73240933cd3d5fe83ad85b.jpg", "img_caption": ["Figure 4: Comparison between the average loss curve for the standard gaussian initialization around O and our initialization, for $p\\,=\\,0.5$ and $q\\:=\\:0.8$ .Starting from the standard initialization, the model converges to a local minimum corresponding to the unigram model. With our initialization, it converges to the global minimum corresponding to the bigram model. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5.3  Higher-Order and Multi-State Markov Chains ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "While the primary focus of this paper has been on binary first-order Markov chains, we believe it's possible to extend our analysis to both multi-state and higher-order settings. On the multi-state front, akin to the binary case, [23] already demonstrates the effect of switching probability and weight-tying on the final model convergence. Here, first characterizing the loss landscape and then the associated learning dynamics in line with our approach is an interesting direction. On the other hand, a recent work [28] establishes a surprising result that any $k^{\\mathrm{th}}$ -order Markov chain can be represented by a three layer transformer with just one head per layer, relying on induction head mechanism. Analyzing gradient flow dynamics using appropriate canonical parameterization (cf. [24]) in this scenario is also a fruitful direction of research. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Related Works ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The recent success of transformer models in deep learning has sparked significant interest and active research in understanding them [38, 25, 16, 27, 15, 37, 40, 32]. In relation to our paper, they can be broadly classified into two topics: (i) In-context learning (ICL): ICL refers to the ability of transformers learn and reason from information present in their context [10, 13, 4, 35, 39, 7, 21, 17]. Along this thread, the works most relevant to ours are [8, 14, 24], which use Markovian input data to understand the ICL mechanism. [8, 14] heuristically show how gradient-based updates can learn an induction-head mechanism using a simplified transformer architecture with frozen encodings, query matrices and linear activations. On the other hand, we consider the canonical parameterization, capitalizing on inherent low-rank parameters, to provide a full characterization of the learning dynamics. [24] demonstrates how two-layer transformers with GD learn induction head mechanism when the input has a causal tree dependency, such as in Markov chains. In this work, we focus on the GF dynamics for single-layer transformers and show how they can also converge to local optima, further highlighting the role of initialization. (i) Training dynamics: On the other hand, numerous works have investigated the training dynamics of transformers. For instance, [9] examines the gradient fow in a simplified single-layer transformer, while [33] studies the process by which self-attention integrates input tokens, assuming the decoder learns faster than the attention layer. Unlike these settings, our focus is on understanding the training dynamics of the full transformer model end-to-end. Other related works include [30], which analyzes gradient dynamics in LSTM Seq2seq models, [19], which shows how Vision Transformers learn spatial structures, and [22], which demonstrates that a single-layer transformer can learn a constrained topic model. A closely related work is [18], which shows that self-attention has a Markovian structure, but our focus is on self-attention's capability in modeling Markov chains and the associated training dynamics. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we present a novel characterization of gradient fow dynamics for (weight-tied) singlelayer transformers with first-order Markov chains. Specifically, we highlight the significant role of the parameter initialization and inherent properties of the Markovian data in determining the parameter convergence to either global minima or local optima. Drawing upon these insights, we offer practical guidelines for parameter initialization, corroborated by empirical results demonstrating their effectiveness. While our current analysis is limited to single-layer models, uncovering similar results with gradient flow analysis for deeper architectures and higher order Markov chains is open and an interesting avenue for future research. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Ashok would like to thank Aditya Vardhan Varre for many helpful discussions about the project. This work was supported in part by the Swiss National Science Foundation under Grant 200364. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1]  Pierre-Antoine Absil, Robert Mahony, and Ben Andrews. Convergence of the iterates of descent methods for analytic cost functions. SIAM Journal on Optimization, 16(2):531-547, 2005.   \n[2]  Arzu Ahmadova. Convergence results for gradient flow and gradient descent systems in the artificial neural network training. arXiv preprint arXiv:2306.13086, 2023.   \n[3] Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https : / /openreview. net /forum? id=LziniAxEI9.   \n[4] Ekin Akyurek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? Investigations with linear models. In The Eleventh International Conference on Learning Representations, 2023.   \n[5]  Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient descent for deep linear neural networks, 2019.   \n[6]  Francis Bach. Effortless optimization through gradient fows. https:/francisbach.com/gradientfows/, 2020.   \n[7]  Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. In Workshop on Effcient Systems for Foundation Models @ ICML2023, 2023.   \n[8]  Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, and Leon Bottou. Birth of a transformer: A memory viewpoint. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[9] Pritam Chandra, Tanmay Kumar Sinha, Kabir Ahuja, Ankit Garg, and Navin Goyal. Towards analyzing self-attention via linear neural network, 2024. URL https : //openreview . net/ forum?id=4fVuBf5HE9.   \n[10] Siyu Chen, Heejune Sheen, Tianhao Wang, and Zhuoran Yang. Training dynamics of multi-head softmax attention for in-context learning: Emergence, convergence, and optimality, 2024.   \n[11]  Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2nd edition, 2006.   \n[12]  John M Danskin. The theory of max-min, with applications.  SIAM Journal on Applied Mathematics, 14(4):641-664, 1966.   \n[13] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng,Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. A survey on in-context learning, 2023. URL https : / /arxiv. org/abs/2301.00234.   \n[14] Benjamin L. Edelman, Ezra Edelman, Surbhi Goel, Eran Malach, and Nikolaos Tsilivis. The evolution of statistical induction heads: In-context learning markov chains, 2024.   \n[15] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfeld-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A mathematical framework for transformer circuits. Transformer Circuits Thread, 2021. URL https: //transformer-circuits.pub/2021/framework/index.html.   \n[16] Hengyu Fu, Tianyu Guo, Yu Bai, and Song Mei. What can a single attention layer learn? A study through the random features lens. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[17] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? A case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583-30598, 2022.   \n[18] M Emrullah Idiz, Yixiao Huang, Yingcong Li, Ankit Singh Rawat, and Samet Oymak. From self-attention to markov models: Unveiling the dynamics of generative transformers. arXiv preprint arXiv:2402.13512, 2024.   \n[19]  Samy Jelassi, Michael Eli Sander, and Yuanzhi Li. Vision transformers provably learn spatial structure. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.   \n[20] Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only converges to minimizers. In Conference on learning theory, pages 1246-1257. PMLR, 2016.   \n[21]  Yingcong Li, M. Emrullah Idiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: generalization and stability in in-context learning. In Proceedings of the 40th International Conference on Machine Learning, 2023.   \n[22]  Yuchen Li, Yuanzhi Li, and Andrej Risteski. How do transformers learn topic structure: Towards a mechanistic understanding, 2023.   \n[23]  Ashok Vardhan Makkuva, Marco Bondaschi, Adway Girish, Alliot Nagle, Martin Jaggi, Hyeji Kim, and Michael Gastpar. Attention with Markov: A framework for principled analysis of transformers via Markov chains. arXiv preprint arXiv:2402.04161, 2024.   \n[24] Eshaan Nichani, Alex Damian, and Jason D Lee. How transformers learn causal structure with gradient descent. arXiv preprint arXiv:2402.14735, 2024.   \n[25]  Samet Oymak, Ankit Singh Rawat, Mahdi Soltanolkotabi, and Christos Thrampoulidis. On the role of attention inprompt-tuning. In Proceedings of the 40th International Conference on Machine Learning, 2023.   \n[26]  Matteo Pagliardini. GPT-2 modular codebase implementation. https://github.com/epfml/llmbaselines, 2023.   \n[27]  Jorge P\u00e9rez, Pablo Barcel6, and Javier Marinkovic. Attention is Turing-complete. Journal of Machine Learning Research, 22(75):1-35, 2021.   \n[28] Nived Rajaraman, Marco Bondaschi, Kannan Ramchandran, Michael Gastpar, and Ashok Vardhan Makkuva. Transformers on markov data: Constant depth sufices, 2024. URL https : //arxiv.org/abs/2407.17686.   \n[29]  Alexander Shapiro. Second-order derivatives of extremal-value functions and optimality conditions for semi-infinite programs. Mathematics of Operations Research, 10(2):207-219, 1985.   \n[30] Charlie Snell, Ruiqi Zhong, Dan Klein, and Jacob Steinhardt. Approximating how single head attention learns. CoRR, abs/2103.07601, 2021. URL https : //arxiv . org/abs/2103. 07601.   \n[31] Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of gradient descent on separable data. Journal of Machine Learning Research, 19 (70):1-57, 2018.   \n[32] Davoud Ataee Tarzanagh, Yingcong Li, Xuechen Zhang, and Samet Oymak. Max-margin token selection in attention mechanism. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[33] Yuandong Tian, Yiping Wang, Beidi Chen, and Simon Shaolei Du. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. In Conference on Parsimony and Learning (Recent Spotlight Track), 2023.   \n[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998-6008, 2017.   \n[35] Johannes Von Oswald, Eyvind Niklasson, Etore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In international Conference on Machine Learning, pages 35151-35174, 2023.   \n[36] Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent, 2023.   \n[37]  Colin Wei, Yining Chen, and Tengyu Ma. Statistically meaningful approximation: a case study on approximating Turing machines with transformers. In Advances in Neural Information Processing Systems, volume 35, pages 12071-12083, 2022.   \n[38] Gail Weiss, Yoav Goldberg, and Eran Yahav. Thinking like transformers. In International Conference on Machine Learning, pages 11080-11090, 2021.   \n[39] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of incontext learning as implicit Bayesian inference. arXiv preprint arXiv:2111.02080, 2021. URL https://arxiv.org/abs/2111.02080.   \n[40] Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Are transformers universal approximators of sequence-to-sequence functions? In International Conference onLearning Representations, 2020. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "1Introduction ", "page_idx": 13}, {"type": "text", "text": "Problem Setting 2   \n3 Canonical Low-rank Parameterization 4   \n3.1 Loss Landscape with Canonical Parameterization 5   \nLearning Dynamics 6   \n4.1 Gradient Flow with Attention 7   \nEmpirical Results 8   \n5.1 Low-rank Parameters 8   \n5.2 Effect of Initialization: Broader Implications 8   \n5.3 Higher-Order and Multi-State Markov Chains 9   \n6 Related Works 10   \n7 Conclusion 10   \nA   Single-layer transformer: architecture and results 16   \nA.1 Loss landscape results . 16   \nB Low-rank structure of the optima 17   \nC  Canonical reparameterization 18   \nD Analysis of the loss with the bias, $L(\\theta,b)$ , in Eq. (4) and Eq. (23) 20   \nD.1  Technical lemmas 20   \nAnalysis of the loss without bias, $L(\\theta)$ , and proof of Thm. 1 22   \nE.1 Proof of Thm. 1 . 23   \nF  Gradient fow analysis without atention 25   \nF.1 Proof of Thm. 2 25   \nF.2 Gradient flow dynamics for $p+q<1$ 27   \nG  Gradient flow analysis with attention 29   \nG.1  Canonical parameterization with attention 29   \nG.2Analysis of the loss function $L(\\theta)$ from Eq. (42) 31   \nG.3 Gradient flow analysis 32   \nG.4  Role of standard initialization . 33   \nH  Additional empirical results 34   \nH.1 Gaussian initialization converges to low-rank 34   \nH.2  Low-rank initialization stays low-rank 34 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "1   Model architecture and hyper-parameters 36 ", "page_idx": 14}, {"type": "text", "text": "Proofs of theorems in App. D 37   \nJ.1 Proof of Thm. 7 37   \nJ.2 Proof of Thm. 6 39 ", "page_idx": 14}, {"type": "text", "text": "K  Proofs of technical lemmas in App. D 40 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "K.1 Proof of Lemma 2 40   \nK.2 Proof of Lemma 3 41   \nK.3 Proof of Lemma 4 42   \nL Proofs of lemmas in App. E 45   \nL.1 Proof of Lemma 5 45   \nL.2 Proof of Lemma 6 45   \nL.3 Proof of Lemma 7 45   \nL.4 Proof of Lemma 8 45 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "M Proofs of lemmas in App. F 46 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "M.1 Proof of Lemma 9 46   \nM.2 Proof of Lemma 10 46   \nM.3 Proof of Lemma 11 48   \nM.4 Proof of Lemma 12 48   \nProofs of lemmas in App. G 49   \nN.1 Proof of Lemma 13 49   \nN.2 Proofs of Thm. 9 and Thm. 10 51   \nN.3 Proof of Lemma 14 57   \nN.4 Proofs of Lemma 16, Lemma 15, and Thm. 3 59   \nN.5  Informal proof of Thm. 11 59 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A  Single-layer transformer: architecture and results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We first describe the transformer architecture from Sec. 2: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\pmb{x}_{n}=\\pmb{x}_{n}\\in\\pmb{p}_{n}\\in\\mathbb{R}^{d},}&\\\\ &{\\pmb{y}_{n}=\\pmb{x}_{n}+\\displaystyle\\sum_{i\\in[n]}\\mathrm{att}_{n,i}\\cdot\\pmb{W}_{V}\\,\\pmb{x}_{i}\\in\\mathbb{R}^{d},}&\\\\ &{\\pmb{z}_{n}=\\pmb{y}_{n}+\\pmb{W}_{2}\\,\\mathrm{ReLU}(\\pmb{W}_{1}\\,\\pmb{y}_{n})\\in\\mathbb{R}^{d},}&\\\\ &{\\mathrm{logit}_{n}=\\langle\\pmb{a},\\pmb{z}_{n}\\rangle+b}&\\\\ &{f_{\\theta}(x_{1}^{n})\\triangleq\\mathbb{P}_{\\theta}\\,(x_{n+1}=1\\mid x_{1}^{n})=\\underbrace{\\sigma(\\log\\mathrm{t}_{n})}_{\\in[0,1]}.}&\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Here $\\pmb{\\theta}\\triangleq(\\pmb{e},\\{\\pmb{p}_{n}\\}_{n=1}^{N},...,\\pmb{W}_{1},\\pmb{W}_{2},b,\\pmb{a})\\in\\mathbb{R}^{D}$ denotes the fllist of the ransformer arameters from the embedding layer till the linear layer. In the attention layer, the weight assigned to each value, $\\mathrm{att}_{n,i}$ , is computed by a compatibility function of the query vector ${\\pmb q}_{n}\\,\\triangleq\\,{\\pmb W}_{Q}\\,{\\pmb x}_{n}$ and the corresponding key vectors $\\pmb{k}_{i}~\\triangleq~\\pmb{W}_{K}\\pmb{x}_{i}$ for all $\\textit{i}\\in\\ [n]$ . More precisely, $\\mathrm{att}_{n,i}~\\triangleq$ softmax $\\mathfrak{c}((\\langle\\pmb{q}_{n},\\pmb{k}_{1}\\rangle,\\dots,\\langle\\pmb{q}_{n},\\pmb{k}_{n}\\rangle)/\\sqrt{d})_{i}$ $\\pmb{W}_{K,Q,V}\\in\\mathbb{R}^{d\\times d}$ are the respective key, query, and value matrices. For multi-headed attention, the same operation is performed on multiple parallel heads, whose outputs are additively combined. ", "page_idx": 15}, {"type": "text", "text": "Finally, the transformer parameters $\\pmb{\\theta}\\triangleq(\\pmb{e},\\{p_{n}\\}_{n=1}^{N},\\dots,b,\\pmb{a})$ are trained via the cross-entropy loss on the next-token prediction: ", "page_idx": 15}, {"type": "equation", "text": "$$\nL(\\pmb\\theta)\\triangleq-\\frac{1}{N}\\sum_{n\\in[N]}\\mathbb{E}_{x_{1}^{n+1}}[x_{n+1}\\cdot\\log f_{\\pmb\\theta}(x_{1}^{n})+(1-x_{n+1})\\cdot\\log(1-f_{\\pmb\\theta}(x_{1}^{n}))].\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In this paper, we focus on the weight-tied scenario where $e=a$ . Hence we let them be a single parameter with $\\pmb{\\theta}=(\\pmb{e}=\\pmb{a},\\{\\pmb{p}_{n}\\}_{n=1}^{\\tilde{N}},\\dots,\\pmb{b})\\in\\mathbb{R}^{D}$ where $D$ is the total parameter dimensionality. ", "page_idx": 15}, {"type": "text", "text": "A.1  Loss landscape results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Now we recall the theoretical results from [23] about the loss landscape of $L$ in the form of global and local minima. ", "page_idx": 15}, {"type": "text", "text": "Theorem 4 (Global minimum). Let the input sequence be $\\{x_{n\\atop r}\\}_{n=1}^{N}\\sim(\\pi(p,q),P(p,q))$ for some fixed $(p,q)\\in(0,1)^{2}$ . Then for all $(p,q)$ , there exists a $\\pmb{\\theta}_{\\star}\\in\\mathbb{R}^{D}$ with an explicit construction such that it is a global minimum for the population loss $L(\\cdot)$ in Eq. (14), i.e. ", "page_idx": 15}, {"type": "equation", "text": "$$\nL(\\pmb\\theta)\\geq L(\\pmb\\theta_{\\star})\\,f o r\\,a l l\\,\\pmb\\theta\\in\\mathbb{R}^{D}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Further, $\\theta_{\\star}$ satisfies: ", "page_idx": 15}, {"type": "text", "text": "(ii) $\\mathbb{P}_{\\pmb{\\theta}_{\\star}}$ $\\mathbf{\\alpha}_{\\star}\\;(x_{n+1}=1\\mid x_{1}^{n})=\\mathbb{P}\\,(x_{n+1}=1\\mid x_{n}),$ the Markov kernel.   \n(ii) $L(\\pmb\\theta_{\\star})=H(x_{n+1}|x_{n})$ the entropy rate of the Markov chain.   \n(iv) $\\nabla L(\\pmb\\theta_{\\star})=0$ i.e. $\\theta_{\\star}$ is a stationary point. ", "page_idx": 15}, {"type": "text", "text": "Let $L_{\\star}\\triangleq L(\\pmb\\theta_{\\star})$ be the global minimal loss from Thm. 4. Now we recall the result on the bad local minimum. ", "page_idx": 15}, {"type": "text", "text": "Theorem5 Bad local minimum).Let th input sequence b $\\{x_{n}\\}_{n=1}^{N}\\sim_{-}(\\pi(p,q),P(p,q))$ for some fixed $(p,q)\\in(0,1)^{2}$ f $p+q>1$ , there exists an explicit $\\pmb{\\theta}_{\\operatorname*{min}}\\in\\mathbb{R}^{D}$ such that it is a bad localminimumfor the loss $L(\\cdot)$ i.e. ", "page_idx": 15}, {"type": "text", "text": "(i) there exists a neighborhood $B(\\theta_{\\mathrm{min}},r)$ with $r\\,>\\,0$ such that $L(\\pmb\\theta)\\,\\geq\\,L(\\pmb\\theta_{\\mathrm{min}})$ for all $\\pmb\\theta\\in$ $B(\\theta_{\\mathrm{min}},r)$ with $L(\\theta_{\\mathrm{min}})>L_{\\star}$ ", "page_idx": 15}, {"type": "text", "text": "Further, $\\theta_{\\mathrm{{min}}}$ satisfies: ", "page_idx": 15}, {"type": "text", "text": "(ii) $\\mathbb{P}_{\\theta_{\\pi}}$ $(x_{n+1}=1\\mid x_{1}^{n})=\\mathbb{P}\\left(x_{n+1}=1\\right)=\\pi_{1}$ the marginal distribution.   \n(ii) $L(\\pmb\\theta_{\\mathrm{min}})=H(x_{n+1})=H(\\pmb\\pi)$ the entropy of the marginal.   \n(iv) $\\nabla L(\\pmb{\\theta}_{\\operatorname*{min}})=0$ i.e. $\\theta_{\\mathrm{min}}$ is a stationary point. ", "page_idx": 15}, {"type": "text", "text": "B Low-rank structure of the optima ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Here we recall the low-rank structure for the global minima found by SGD consistently across multiple runs when $p+q<1$ [23, Appendix C.2]. In particular, it is observed that the token and positional encodings point in the same direction $_{\\alpha}$ , which is a low-rank factor for the weight matrices in the attention and the feedforward layers, which in turn are all rank-one. Mathematically, ", "page_idx": 16}, {"type": "text", "text": "Embedding. The embedding vector $^e$ obeys ", "page_idx": 16}, {"type": "equation", "text": "$$\ne=e\\cdot\\alpha\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for some $e>0$ and $\\alpha\\in\\{\\pm1\\}^{d}$ . Further, the positional embeddings $\\pmb{p}_{n}$ are constant across positions $n$ pointing in the same direction albeit with a negative scalar, i.e. ", "page_idx": 16}, {"type": "equation", "text": "$$\np_{n}=-p\\cdot\\alpha\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for $p>0$ and $p\\approx{\\frac{e}{2}}$ such that $e>p$ . Thus from Embedding layer, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\pmb{x}_{n}=(e x_{n}-p)\\cdot\\pmb{\\alpha},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which ensures that the respective embeddings for the bit $x_{n}=0$ and $x_{n}=1$ are $x_{n}=-p\\cdot\\alpha$ and $\\pmb{x}_{n}=(e-p)\\cdot\\pmb{\\alpha}$ , which are roughly anti-podal. ", "page_idx": 16}, {"type": "text", "text": "Attention. Recall from the Attention layer that the output ${\\pmb y}_{n}$ is given by $\\begin{array}{l l l}{{{\\pmb y}_{n}}}&{{=}}&{{{\\pmb x}_{n}\\ +}}\\end{array}$ $\\begin{array}{r}{{\\pmb{W}}_{O}\\sum_{i\\in[n]}\\mathrm{att}_{n,i}\\cdot{\\pmb{W}}_{V}\\,{\\pmb{x}}_{i}}\\end{array}$ where the attention weights $\\mathrm{att}_{n,i}$ are computed according to $\\mathrm{att}_{n,i}=$ $\\begin{array}{r}{\\exp\\left(\\langle q_{n},\\dot{k}_{i}\\rangle/\\sqrt{d}\\right)/\\left(\\sum_{j\\in[n]}\\exp\\left(\\langle q_{n},k_{j}\\rangle/\\sqrt{d}\\right)\\right)}\\end{array}$ with ${\\pmb q}_{n}={\\pmb W}_{Q}\\,{\\pmb x}_{n}$ and ${\\pmb k}_{i}={\\pmb W}_{K}\\,{\\pmb x}_{i}$ . Here it is observed that the matrix products are all rank-one with $_{\\alpha}$ being a factor, i.e. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{O}W_{V}=\\alpha\\cdot v^{\\top}\\in\\mathbb{R}^{d\\times d},\\quad\\mathrm{for~some~}v\\in\\mathbb{R}^{d},}\\\\ &{W_{Q}^{\\top}W_{K}=(q^{2}d)\\,\\alpha\\cdot\\alpha^{\\top}\\in\\mathbb{R}^{d\\times d},\\quad\\mathrm{for~some~}q\\in\\mathbb{R}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Hence, ", "page_idx": 16}, {"type": "equation", "text": "$$\nW_{V}\\,{\\pmb x}_{i}=\\langle{\\pmb v},{\\pmb\\alpha}\\rangle(e x_{i}-p)\\,{\\pmb\\alpha},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\frac{\\langle q_{n},k_{i}\\rangle}{\\sqrt{d}}=\\frac{1}{\\sqrt{d}}\\cdot x_{n}^{\\top}W_{Q}^{\\top}W_{K}x_{n}=\\frac{q^{2}d}{\\sqrt{d}}\\cdot(x_{n}^{\\top}\\alpha)(x_{i}^{\\top}\\alpha)\\overset{(\\|\\alpha\\|_{2}^{2}=d)}{=}\\frac{q^{2}d^{3}}{\\sqrt{d}}\\cdot(e x_{n}-p)(e x_{i}-p)}\\\\ {\\displaystyle=q^{2}d^{5/2}\\cdot(e x_{n}-p)(e x_{i}-p).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{y_{n}=x_{n}+\\displaystyle\\sum_{i\\in[n]}\\mathrm{att}_{n,i}\\cdot W_{O}W_{V}\\,x_{i}}\\\\ &{\\quad=\\displaystyle\\left(e x_{i}-p\\right)\\alpha+\\displaystyle\\sum_{i\\in[n]}\\mathrm{att}_{n,i}\\cdot\\langle v,\\alpha\\rangle(e x_{i}-p)\\,\\alpha}\\\\ &{\\quad=\\displaystyle\\left((e x_{n}-p)+\\langle v,\\alpha\\rangle\\displaystyle\\sum_{i\\in[n]}\\frac{\\exp\\left(q^{2}d^{5/2}\\left(e x_{n}-p\\right)(e x_{i}-p)\\right)}{\\sum_{j\\in[n]}\\exp\\left(q^{2}d^{5/2}\\left(e x_{n}-p\\right)(e x_{j}-p)\\right)}\\cdot\\left(e x_{i}-p\\right)\\right)\\alpha.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "It is further noticed that $\\langle\\boldsymbol{v},\\boldsymbol{\\alpha}\\rangle\\approx0$ and hence ${\\pmb y}_{n}=\\left(e x_{n}-p\\right){\\pmb\\alpha}={\\pmb x}_{n}$ ", "page_idx": 16}, {"type": "text", "text": "Feed-forward. For the Feed-forward layer, both the matrices $\\boldsymbol{W}_{1}\\in\\mathbb{R}^{r\\times d}$ and $\\boldsymbol{W}_{2}\\in\\mathbb{R}^{d\\times r}$ exhibit rank-one structure with $_{\\alpha}$ being one of the factors, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{1}=w\\cdot w\\cdot\\alpha^{\\top},\\quad\\mathrm{for\\;some\\;}w\\in\\{\\pm1\\}^{r},w>0,}\\\\ &{W_{2}=W_{1}^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus $W_{1}\\pmb{y}_{n}=d w(e x_{n}-p)\\,\\pmb{w}$ Since $-p<0$ and $e-p>0$ ,corresponding to $x_{n}=0$ and $x_{n}=1$ respectively, we obtain $\\mathrm{ReLU}(W_{1}y_{n})=d w\\left((1-x_{n})p\\cdot\\mathrm{ReLU}(-w)+x_{n}(e-p)\\cdot\\mathrm{ReLU}(w)\\right)$ Denoting the number of ones in $\\pmb{w}$ as $\\beta$ , i.e. $\\begin{array}{r}{\\beta=\\sum_{i=1}^{r}\\mathbb{1}(w_{i}=1)}\\end{array}$ , we further simplify: ", "page_idx": 16}, {"type": "equation", "text": "$$\nW_{\\mathrm{2ReLU}}({\\pmb W}_{1}{\\pmb y}_{n})={\\pmb W}_{1}^{\\top}\\mathrm{ReLU}({\\pmb W}_{1}{\\pmb y}_{n})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=w^{2}d\\left((1-x_{n})p\\cdot\\langle w,\\mathrm{ReLU}(-w)\\rangle+x_{n}(e-p)\\cdot\\langle w,\\mathrm{ReLU}(w)\\rangle\\right)\\alpha}\\\\ &{=w^{2}d\\left((1-x_{n})p\\cdot(\\beta-r)+x_{n}(e-p)\\cdot\\beta\\right)\\alpha}\\\\ &{=w^{2}d(e x_{n}-p)\\left((2\\beta-r)x_{n}+r-\\beta\\right)\\alpha.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence ", "page_idx": 17}, {"type": "equation", "text": "$$\nz_{n}=y_{n}+W_{2}\\mathrm{ReLU}(W_{1}y_{n})=\\left(e x_{n}-p\\right)\\left(1+w^{2}d\\left((2\\beta-r)x_{n}+r-\\beta\\right)\\right)\\alpha.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Linear. Using the fact that $e=\\pmb{a}=\\pmb{e}\\cdot\\pmb{\\alpha}$ due to weight-tying, we obtain from Linear layer that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{logit}_{n}=\\langle e,z_{n}\\rangle+b=e d(e x_{n}-p)\\left(1+w^{2}d\\left((2\\beta-r)x_{n}+r-\\beta\\right)\\right)+b.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Prediction. We finally obtain that the prediction probability ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\mathsf{\\partial}}_{\\theta}^{\\mathsf{\\Gamma}}(x_{1}^{n})=\\sigma(\\log\\!\\mathrm{i}\\mathrm{t}_{n})=x_{n}\\cdot\\sigma\\left(e d(e-p)\\left(1+\\beta w^{2}d\\right)+b\\right)+(1-x_{n})\\cdot\\sigma\\left(-e d p\\left(1+(r-\\beta)w^{2}d\\right)+b\\right),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus we see that the prediction probability and hence the loss function $L(\\cdot)$ in Eq. (14) is influenced only by the scalars $e,p,w,b$ and $\\beta$ ", "page_idx": 17}, {"type": "text", "text": "C   Canonical reparameterization ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Building on the low-rank strucutre of the transformer parameters described above, we consider a special parameterization for them. A key property of this parameterization is that it covers both the global and local minima from Thm. 4 and Thm. 5 for all $(p,q)\\in(0,1)^{2}$ . Recall that Thm. 5 characterizes local minima only for $p\\!+\\!q>1$ whereas our special parameterization allows to discover local minima even for $p+q<1$ . Our construction follows the same outline as in Eqs. (15)-(20). First we start with the embedding layer. ", "page_idx": 17}, {"type": "text", "text": "Embedding. We let $e=e\\cdot\\alpha$ and $p_{n}=-p\\cdot\\alpha$ for all $n$ where $\\begin{array}{r}{e>0,p=\\frac{e}{2}}\\end{array}$ and $\\alpha\\in\\{\\pm1\\}^{d}/\\sqrt{d}$ Thus the embedding ${\\pmb x}_{n}$ from Eq. (15) simplifies to ", "page_idx": 17}, {"type": "equation", "text": "$$\nx_{n}=e\\left(x_{n}-{\\frac{1}{2}}\\right)\\alpha\\in\\{\\pm{\\frac{e}{2}}\\}\\,\\alpha.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Attention. Substituting this ${\\pmb x}_{n}$ in Eq. (16), we have ", "page_idx": 17}, {"type": "equation", "text": "$$\ny_{n}=e\\left(x_{n}-{\\frac{1}{2}}\\right)\\alpha+\\langle v,\\alpha\\rangle\\left(\\sum_{i\\in[n]}\\operatorname{att}_{n,i}\\cdot e\\left(x_{i}-{\\frac{1}{2}}\\right)\\right)\\alpha,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the attention weights attn, = $\\begin{array}{r}{\\mathrm{att}_{n,i}\\,=\\,\\frac{\\exp\\left(e^{2}q^{2}d^{5/2}\\,\\left(x_{n}-\\frac{1}{2}\\right)\\left(x_{i}-\\frac{1}{2}\\right)\\right)}{\\sum_{j\\in[n]}\\exp\\left(e^{2}q^{2}d^{5/2}\\,\\left(x_{n}-\\frac{1}{2}\\right)\\left(x_{j}-\\frac{1}{2}\\right)\\right)}\\,\\in\\,(0,1)}\\end{array}$ E (0, 1) for some q E R. Since $\\langle\\pmb{v},\\pmb{\\alpha}\\rangle\\approx0$ , we let $\\pmb{v}=0$ and obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\pmb y}_{n}={\\pmb x}_{n}=e\\left(x_{n}-\\frac{1}{2}\\right){\\pmb\\alpha}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Feed-forward. For the feed-forward layer, we observe from Eq. (17) and Eq. (19) that for any $w\\in\\{\\pm1\\}^{r}$ , only the number of 1's in $w,\\beta$ , matters for the final vector ${\\boldsymbol{z}}_{n}$ which further interacts with the weight scalar $w$ . Hence without loss of generality, we set $\\mathbf{\\nabla}w$ to be the all-ones vector: $\\pmb{w}\\,=\\,\\mathbf{1}\\,\\in\\,\\mathbb{R}^{r}$ and hence $\\beta\\,=\\,r\\,=\\,4d$ .While we observe from Eq. (17) that $W_{2}\\,=\\,W_{1}^{\\top}$ for $p+q<1$ , we observe from the proof of the Thm. 4 for $p+q>1$ in [23, Appendix B.2] that we need $W_{2}=-W_{1}^{\\top}$ in this scenario. Hence we consider the following parameterization that covers both these scenarios: ", "page_idx": 17}, {"type": "equation", "text": "$$\nW_{1}=\\frac{|w|}{\\sqrt{d}}\\,\\mathbf{1}\\cdot\\pmb{\\alpha}^{\\top}\\in\\mathbb{R}^{4d\\times d},\\quad\\pmb{W}_{2}=\\frac{w}{\\sqrt{d}}\\,\\pmb{\\alpha}\\cdot\\mathbf{1}^{\\top}\\in\\mathbb{R}^{d\\times4d}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Here $w\\,>\\,0$ ensures $W_{2}\\,=\\,W_{1}^{\\top}$ whereas $w\\,<\\,0$ $W_{2}\\,=\\,-W_{1}^{\\top}$ . Using this parameterization, substituting $\\beta=r=4d$ and $w\\mapsto\\frac{w}{d}$ in Eq. (19), we get ", "page_idx": 17}, {"type": "equation", "text": "$$\nz_{n}=e\\left(x_{n}-{\\frac{1}{2}}\\right)(1+4w|w|x_{n})\\,\\alpha.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Linear. Since $e=\\pmb{a}=e\\cdot\\pmb{\\alpha}$ due to weight-tying, Eq. (20) simplifies to ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\begin{array}{l}{\\displaystyle\\log\\mathrm{it}_{n}=\\left\\langle e,z_{n}\\right\\rangle+b=e^{2}\\left(x_{n}-{\\frac{1}{2}}\\right)\\left(1+4w|w|x_{n}\\right)+b}\\\\ {\\displaystyle\\qquad\\stackrel{\\mathrm{(}x_{n}=x_{n}^{2})}{=}e^{2}\\left(x_{n}+4w|w|x_{n}-{\\frac{1}{2}}-2w|w|x_{n}\\right)+b}\\\\ {\\displaystyle=e^{2}(1+2w|w|)\\,x_{n}+b-{\\frac{e^{2}}{2}}.}\\end{array}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Prediction. The next-token prediction probability is ", "page_idx": 18}, {"type": "equation", "text": "$$\nf_{(\\theta,b)}(x_{1}^{n})=\\sigma\\left(e^{2}(1+2w|w|)\\,x_{n}+b-\\frac{e^{2}}{2}\\right),\\quad\\theta\\triangleq(e,w)\\in\\mathbb{R}^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Loss. While we assumed $e>0$ in the beginning, in view of Eq. (22) and the fact that $\\alpha\\in\\{\\pm1\\}^{d}/\\sqrt{d}$ we see that $e\\in\\mathbb{R}$ gives us the same expression for probability. Thus the final probability depends on just the three scalars $(e,w,b)\\in\\mathbb{R}^{3}$ . Defining $\\pmb{\\theta}=\\bar{(\\ e,w)}\\in\\dot{\\mathbb{R}}^{2}$ , we recall the cross-entropy loss $L(\\cdot)$ from Eq. (4) in Sec. 2 for this canonical model: ", "page_idx": 18}, {"type": "equation", "text": "$$\nL(\\theta,b)=-\\frac{1}{N}\\sum_{n\\in[N]}\\mathbb{E}_{x_{1}^{n+1}}[x_{n+1}\\cdot\\log f_{(\\theta,b)}(x_{1}^{n})\\,+(1-x_{n+1})\\cdot\\log(1-f_{(\\theta,b)}(x_{1}^{n}))].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "It turns out that we can further remove the bias $b$ by minimizing the loss over it which we discuss in App. E. For now in the next section, we analyze when it's present as in Eq. (23). ", "page_idx": 18}, {"type": "text", "text": "D  Analysis of the loss with the bias, $L(\\pmb\\theta,b)$ , in Eq. (4) and Eq. (23) ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we analyze the loss function with the bias, $L(\\pmb\\theta,b)$ , from Eq. (4) and Eq. (23), which will later be useful for studying $L(\\pmb\\theta)$ . First we characterize the set of its critical points in $\\mathbb{R}^{3}$ . To this end, we define the following sets of points ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Gamma_{\\star}(p,q)\\triangleq\\left\\{(e,w,b)\\in\\mathbb{R}^{3}:e^{2}(1+2w|w|)=\\log\\frac{(1-p)(1-q)}{p q},\\,b-\\frac{e^{2}}{2}=\\log\\frac{p}{1-p}\\right\\},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{T}_{\\operatorname*{min}}(p,q)\\triangleq\\left\\{(e,w,b)\\in\\mathbb{R}^{3}:e=0,(p+q-1)(1+2w|w|)>0,b=\\log\\frac{p}{q}\\right\\},}\\\\ &{\\mathbf{T}_{\\mathrm{sad}}(p,q)\\triangleq\\left\\{(e,w,b)\\in\\mathbb{R}^{3}:e=0,(p+q-1)(1+2w|w|)\\leq0,b=\\log\\frac{p}{q}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The following result establishes that these sets exhaust all the critical points. ", "page_idx": 19}, {"type": "text", "text": "Threm Ail  q $\\{x_{n}\\}_{n=1}^{N}\\sim(\\pi,P).$ thetransfomer parameters $(\\pmb\\theta,b)=(e,\\bar{w_{\\ast}}b)\\in\\mathbb{R}^{3}$ , and the next-token prediction loss $L(\\cdot)$ be as in Eq. (23). Then all the stationary points of $L$ are either in $\\Gamma_{\\star}$ $\\Gamma_{\\mathrm{min}}$ or $\\mathbf{T}_{\\mathrm{sad}}$ i.e. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\big\\{(\\pmb{\\theta},b)\\in\\mathbb{R}^{3}:\\nabla L(\\pmb{\\theta},b)=0\\big\\}=\\mathbf{r}_{\\star}\\cup\\mathbf{r}_{\\mathrm{min}}\\cup\\mathbf{r}_{\\mathrm{sad}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof. We refer to App. J.1. ", "page_idx": 19}, {"type": "text", "text": "Recall the definitions of local minima & maxima, global minima, and that of all the saddle points from Sec. 3.1. We are now ready to present the main result about the loss landscape of $L(\\cdot)$ ", "page_idx": 19}, {"type": "text", "text": "Theorem 7 (Loss landscape with bias). Let the input sequence be $\\{x_{n}\\}_{n=1}^{N}\\sim(\\pi,P),$ the transformer parameters $(e,w,b)\\in\\mathbb{R}^{3}$ ,and thenext-token prediction loss $L(\\cdot)$ be as in Eq. (23). Then for any $\\bar{(p,q)}\\in(0,1)^{2}$ with $p+q\\ne1$ and $N\\in\\mathbb{N}$ ", "page_idx": 19}, {"type": "text", "text": "(i) the set of all global minima of $L$ is given by $\\Gamma_{\\star}(p,q)$ (i) the set of all bad local minima of $L$ is given by ${\\Gamma}_{\\mathrm{min}}(p,q)$ (ji) and the set of all saddle points of $L$ is $\\mathbf{T}_{\\mathrm{sad}}(p,q)$ ", "page_idx": 19}, {"type": "text", "text": "Furthermore, for any $\\gamma_{\\star}\\in\\Gamma_{\\star},\\gamma_{\\operatorname*{min}}\\in\\Gamma_{\\operatorname*{min}}$ and $\\gamma_{\\mathrm{sad}}\\in\\Gamma_{\\mathrm{sad}},$ the losses are ordered as ", "page_idx": 19}, {"type": "equation", "text": "$$\nH(x_{n+1}\\mid x_{n})=L(\\gamma_{\\star})<L(\\gamma_{\\mathrm{min}})=L(\\gamma_{\\mathrm{sad}})=H(x_{n+1}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Remark 1. Note that a bad local minimum is a local minimum whose loss value is strictly less than that of the global minimum, as is the case here. Interestingly, Thm. 7 highlights that all local minima for the loss $L$ are indeed bad local minima. ", "page_idx": 19}, {"type": "text", "text": "Proof. We refer to App. J.2. ", "page_idx": 19}, {"type": "text", "text": "D.1  Technical lemmas ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The proofs of both Thm. 6 and Thm. 7 rely on few key lemmas that we present below. First we start with the result that rewrites the loss $L(\\pmb\\theta,b)$ from Eq. (23) in a compact manner using the logistic function $\\ell_{\\mathrm{log}}(\\cdot)$ ", "page_idx": 19}, {"type": "text", "text": "Lemma 2 (Loss as a logistic function). The next-token prediction loss $L(\\cdot)$ in Eq. (23) can be written as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal L}(\\pmb\\theta,b)=\\frac{1}{N}\\sum_{n\\in[N]}\\mathbb{E}[\\ell_{\\log}\\left((2x_{n+1}-1)\\cdot\\mathrm{logit}_{n}\\right)]}}\\\\ {{\\displaystyle=\\mathbb{E}_{X,Y}\\left[\\ell_{\\log}\\left((2Y-1)\\left(e^{2}(1+2w|w|)X+b-\\frac{e^{2}}{2}\\right)\\right)\\right],}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $(X,Y)\\in\\{0,1\\}^{2}$ are distributed according to $(X,Y)\\sim(\\pi,P)$ ,i.e. $X$ is a Bernoulli random variablewith $X\\sim\\pi\\equiv\\operatorname{Bern}(p/(p+q))$ and $Y|X\\sim P(p,q)$ ,the Markov kernel. ", "page_idx": 19}, {"type": "text", "text": "The following lemma establishes the gradients of the loss function with respect to the parameters $e,w$ ,and $b$ ", "page_idx": 20}, {"type": "text", "text": "Lemma 3 (Gradient computation). For any $(e,w,b)\\in\\mathbb{R}^{3}$ and the next-token prediction loss $L(\\cdot)$ in Eq. (23), the gradients are given by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial L}{\\partial e}=\\mathbb{E}_{X}\\left[(f_{1}X+f_{2})(2X(1+2w|w|)-1)\\right]\\cdot e,}\\\\ &{\\displaystyle\\frac{\\partial L}{\\partial w}=\\mathbb{E}_{X}\\left[(f_{1}X+f_{2})X\\right]\\cdot4e^{2}|w|,}\\\\ &{\\displaystyle\\frac{\\partial L}{\\partial b}=\\mathbb{E}_{X}\\left[f_{1}X+f_{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $X\\ \\in\\ \\{0,1\\}$ is a Bernoulli random variable with $X~\\sim~\\mathrm{Bern}(p/(p\\,+\\,q))$ $\\begin{array}{r l}{f_{1}}&{{}=}\\end{array}$ $\\begin{array}{r}{\\sigma\\left(2e^{2}w|w|+b+\\frac{e^{2}}{2}\\right)+q-1-\\sigma\\left(b-\\frac{e^{2}}{2}\\right)+p,}\\end{array}$ and $f_{2}=\\sigma\\left(b-\\textstyle{\\frac{e^{2}}{2}}\\right)-p$ ", "page_idx": 20}, {"type": "text", "text": "Remark 2. It is interesting to note that the gradients for both $e$ and $w$ are product of an expectation term and an $e$ factor. Also, except for scaling factors in terms of $(e,w,b)$ , all the gradients are governed by the two expectation terms $\\mathbb{E}[(f_{1}X+f_{2})X]$ and $\\mathbb{E}[f_{1}X+f_{2}]$ . This observation plays a key role in obtaining an ordinary differential equation which yields the energy function $\\mathcal{E}$ , defined in Eq. (11). ", "page_idx": 20}, {"type": "text", "text": "Now we characterize the Hessian at both local-minima and saddle points. ", "page_idx": 20}, {"type": "text", "text": "Lemma 4 (Hessian at local-minima and saddle points). For the canonical parameterization $\\gamma=$ $(b,e,w)\\in\\mathbb{R}^{3}$ and thenext-token prediction loss $L(\\cdot)$ in Eq. (23), the Hessian at any $\\gamma_{\\mathrm{min}}\\in\\Gamma_{\\mathrm{min}}$ $r\\,\\gamma_{\\mathrm{sad}}\\in\\Gamma_{\\mathrm{sad}}$ isgivenby ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\nabla^{2}L(\\gamma)\\biggl|_{\\gamma=\\gamma_{\\mathrm{min}},\\gamma_{\\mathrm{sad}}}=\\pi_{0}\\pi_{1}\\left[\\!\\!\\begin{array}{c c c}{{1}}&{{0}}&{{0}}\\\\ {{0}}&{{2(p+q-1)(1+2w|w|)}}&{{0}}\\\\ {{0}}&{{0}}&{{0}}\\end{array}\\!\\!\\right],\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where To = $\\begin{array}{r}{\\pi_{0}=\\frac{q}{p+q}}\\end{array}$ and $\\begin{array}{r}{\\pi_{1}=\\frac{p}{p+q}}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "Remark 3. We note that the Hessian is computed with the parameter ordering $(b,e,w)$ ", "page_idx": 20}, {"type": "text", "text": "The proofs of the lemmas are presented in App. K. ", "page_idx": 20}, {"type": "text", "text": "E  Analysis of the loss without bias, $L(\\pmb\\theta)$ , and proof of Thm. 1 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The proof of Thm. 1, concerning the loss $L(\\pmb\\theta)$ in Eq. (5), is similar to that of Thm. 7 which studies the loss $L(\\pmb\\theta,b)$ with the bias present. The main idea is to establish the analogous set of lemmas, as in App. D, when the bias is substituted with its optimal choice. First we recall the loss function ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\dot{z}(\\theta)\\triangleq L(\\theta,b_{\\star})=-\\displaystyle\\frac{1}{N}\\displaystyle\\sum_{n\\in[N]}\\mathbb{E}_{x_{1}^{n+1}}\\bigl[x_{n+1}\\cdot\\log f_{(\\theta,b_{\\star})}(x_{1}^{n})+(1-x_{n+1})\\cdot\\log(1-f_{(\\theta,b_{\\star})}(x_{1}^{n}))\\bigr],}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\nb_{\\star}=\\operatorname*{argmin}_{b\\in\\mathbb{R}}L(\\pmb{\\theta},b).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We start with the result that establishes a closed form expression for $b_{\\star}$ ", "page_idx": 21}, {"type": "text", "text": "Lemma 5 (Optimal bias). For $\\pmb{\\theta}=(e,w)\\in\\mathbb{R}^{2}$ and $b\\in\\mathbb{R}$ let $L(\\pmb\\theta,b)$ be the next-token prediction loss defined in Eq. (23). Then, for any $\\pmb{\\theta}\\,\\in\\,\\mathbb{R}^{2},~L(\\pmb{\\theta},b)$ is convex in $b$ and the minimizer $b_{\\star}$ = $\\begin{array}{r}{\\mathrm{argmin}_{b\\in\\mathbb{R}}\\,L(\\pmb{\\theta},b)}\\end{array}$ is given by ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\exp\\left(b_{\\star}-\\frac{e^{2}}{2}\\right)=\\frac{1}{2A}\\left[\\frac{p}{q}-1+\\sqrt{\\left(\\frac{p}{q}-1\\right)^{2}+4\\cdot\\frac{p}{q}\\cdot A}\\right],\\quad A\\triangleq\\exp(e^{2}(1+2w|w|)).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now we rewrite the loss in terms of the logistic function. ", "page_idx": 21}, {"type": "text", "text": "Lemma 6 (Loss as a logistic function). For any $\\pmb{\\theta}\\in\\mathbb{R}^{2}$ ,the next-token prediction loss $L(\\pmb\\theta)$ in Eq. (29) can be written as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal L}(\\pmb\\theta)=\\frac{1}{N}\\sum_{n\\in[N]}\\mathbb{E}[\\ell_{\\log}\\left((2x_{n+1}-1)\\cdot\\log\\mathrm{it}_{n}\\right)]}}\\\\ {~~~~=\\mathbb{E}_{X,Y}\\left[\\ell_{\\log}\\left((2Y-1)\\left(e^{2}(1+2w|w|)X+b_{\\star}-\\frac{e^{2}}{2}\\right)\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $b_{\\star}$ follows from Eq. (30), $(X,Y)\\in\\{0,1\\}^{2}$ are distributed according to $(X,Y)\\sim(\\pi,P),$ i.e. $X$ is a Bernoulli random variable with $X\\sim\\pi\\equiv\\operatorname{Bern}(p/(p+q))$ and $Y|X\\sim P(p,q)$ theMarkov kernel. ", "page_idx": 21}, {"type": "text", "text": "The following lemma establishes the gradients of the loss. ", "page_idx": 21}, {"type": "text", "text": "Lemma 7 (Gradient computation). For any $\\pmb{\\theta}=(e,w)\\in\\mathbb{R}^{2}$ and the next-token prediction loss $L(\\pmb\\theta)$ in Eq. (29), the gradients are given by ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial{\\cal L}}{\\partial e}=\\mathbb{E}_{X}\\left[(f_{1}X+f_{2})X\\right]\\cdot2(1+2w|w|)e,}\\\\ {\\displaystyle\\frac{\\partial{\\cal L}}{\\partial w}=\\mathbb{E}_{X}\\left[(f_{1}X+f_{2})X\\right]\\cdot4e^{2}|w|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $X\\ \\in\\ \\{0,1\\}$ is a Bernoulli random variable with $X~\\sim~\\mathrm{Bern}(p/(p\\,+\\,q))$ $\\begin{array}{r l}{f_{1}}&{{}=}\\end{array}$ $\\begin{array}{r}{\\sigma\\left(2e^{2}w|w|+b_{\\star}+\\overset{\\cdot}{\\frac{e^{2}}{2}}\\right)\\!+\\!q\\!-\\!1\\!-\\!\\sigma\\left(b_{\\star}-\\frac{e^{2}}{2}\\right)\\!+\\!p,}\\end{array}$ and $f_{2}=\\sigma\\left(b_{\\star}-\\frac{e^{2}}{2}\\right)\\!-\\!p$ Further, $\\pi_{1}f_{1}\\!+\\!f_{2}=0$ ", "page_idx": 21}, {"type": "text", "text": "Remark 4. We observe above that the gradients for both $e$ and $w$ are proportional to each other, except for the scaling factors in terms of $e$ and $w$ . This forms the basis for the derivation of the energy function discussed in App. F. ", "page_idx": 21}, {"type": "text", "text": "The following lemma characterizes the Hessian. ", "page_idx": 21}, {"type": "text", "text": "Lemma 8 (Hessian computation). Let $\\gamma=(b,\\pmb\\theta)\\in\\mathbb{R}^{3}$ with $\\pmb{\\theta}=(e,w)\\in\\mathbb{R}^{2}$ and $L(\\gamma)$ bethe next-token prediction loss in Eq. (23) and $L(\\pmb\\theta)$ be the one in Eq. (29). Let the Hessian of $L$ at $\\gamma\\,b e$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{H(\\gamma)\\triangleq\\nabla_{\\gamma\\gamma}^{2}L=\\left[H_{b\\theta}^{h_{b}}\\quad H_{b\\theta}\\right]=\\left[(\\nabla_{b\\theta}^{2}L\\mathrm{\\boldmath~\\sigma~}\\nabla_{b\\theta}^{2}L\\right]\\in\\mathbb{R}^{3\\times3}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then the Hessian of $L$ at $\\pmb{\\theta}\\in\\mathbb{R}^{2}$ is given by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{H(\\pmb\\theta)\\triangleq\\nabla_{\\pmb\\theta\\pmb\\theta}^{2}L=H_{\\pmb\\theta\\pmb\\theta}-H_{b\\pmb\\theta}^{\\top}\\cdot H_{b b}^{-1}\\cdot H_{b\\pmb\\theta}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Consequently, for any $\\gamma=(b,e,w)\\in\\Gamma_{\\mathrm{min}}\\cup\\Gamma_{\\mathrm{sad}},$ the Hessian $H(\\pmb\\theta)$ at $\\pmb\\theta=(e,w)$ is given by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{H(\\pmb\\theta)=\\pi_{0}\\pi_{1}\\left[2(p+q-1)(1+2w|w|)\\right.\\ \\ 0\\right],}\\\\ {\\qquad\\qquad\\qquad\\left.0\\qquad\\qquad\\qquad\\quad0\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where To = $\\begin{array}{r}{\\pi_{0}=\\frac{q}{p+q}}\\end{array}$ m\u2193a and \u03c01 = $\\begin{array}{r}{\\pi_{1}=\\frac{p}{p+q}}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "The proofs of the above lemmas are deferred to App. L. We are now ready to present the proof of Thm. 1. ", "page_idx": 22}, {"type": "text", "text": "E.1 Proof of Thm. 1 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Proof. Let $\\pmb{\\theta}\\in\\mathbb{R}^{2}$ and $\\pmb{\\gamma}(\\pmb{\\theta})\\,=\\,(\\pmb{\\theta},b_{\\star}(\\pmb{\\theta})\\,\\in\\,\\mathbb{R}^{3}$ be its embedding in $\\mathbb{R}^{3}$ with the optimal bias $b_{\\star}(\\pmb\\theta)=\\mathrm{argmin}_{b\\in\\mathbb R}\\,L(\\pmb\\theta,b)$ from Lemma 5. Define the following four sets of points: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Theta_{\\star}(p,q)\\triangleq\\left\\{(e,w)\\in\\mathbb{R}^{2}:e^{2}(1+2w|w|)=\\log\\frac{(1-p)(1-q)}{p q}\\right\\},}\\\\ &{\\Theta_{\\operatorname*{min}}(p,q)\\triangleq\\left\\{(e,w)\\in\\mathbb{R}^{2}:e=0,\\,(p+q-1)(1+2w|w|)>0\\right\\},}\\\\ &{\\Theta_{\\operatorname*{max}}(p,q)\\triangleq\\left\\{(e,w)\\in\\mathbb{R}^{2}:e=0,\\,(p+q-1)(1+2w|w|)<0\\right\\},}\\\\ &{\\Theta_{\\operatorname*{sad}}(p,q)\\triangleq\\left\\{(e,w):e=0,\\,w=-1/\\sqrt{2}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "First we show that any critical point of $L:\\mathbb{R}^{2}\\,\\rightarrow\\,\\mathbb{R}$ has to lie in one of these sets. Then we characterize that they correspond to the set of all global minima, local minima $\\&$ maxima, and saddle points respectively. ", "page_idx": 22}, {"type": "text", "text": "(i) Set of all critical points: Recall from Thm. 7 that for any critical point $\\pmb{\\gamma}=(\\pmb{\\theta},b)=(e,w,b)\\in$ $\\overline{{\\mathbb{R}^{3}}}$ of $L$ $,\\gamma\\in\\Gamma_{\\star}\\cup\\Gamma_{\\mathrm{min}}\\cup\\Gamma_{\\mathrm{sad}}$ . Here the main observation is that all these critical points are of the form $(\\pmb\\theta,b_{\\star}(\\pmb\\theta))$ where $\\pmb\\theta\\in\\Theta_{\\star}\\cup\\Theta_{\\mathrm{min}}\\cup\\Theta_{\\mathrm{max}}\\cup\\Theta_{\\mathrm{sad}}$ . To see this, let $\\gamma\\in\\Gamma_{\\star}$ . Here we have $\\begin{array}{r}{e^{2}(1+2w|w|)=\\log\\frac{(1-p)(1-q)}{p q}}\\end{array}$ from Eq. (24) and hence $\\theta\\in\\Theta_{\\star}$ Further, by Lemma 5, we have that the optimal bias for this $\\pmb{\\theta}$ satisfies $\\begin{array}{r}{b_{\\star}-\\frac{e^{2}}{2}=\\log\\frac{p}{1-p}}\\end{array}$ , which is precisely the characterization of the bias $b$ for $\\gamma\\,=\\,(e,w,b)$ in Eq. (24). Likewise, if $\\gamma\\,\\in\\,\\Gamma_{\\mathrm{min}}\\cup\\Gamma_{\\mathrm{sad}}$ , we have $e\\,=\\,0$ and hence $\\pmb{\\theta}\\in\\Theta_{\\mathrm{min}}\\cup\\Theta_{\\mathrm{max}}\\cup\\Theta_{\\mathrm{sad}}$ Heneeby Lemma , $\\begin{array}{r}{b_{\\star}\\,=\\,\\log\\frac{p}{q}}\\end{array}$ matchng tat of Eqg (25) and Eq. (26). Thus the set of all critical points of in $\\mathbb{R}^{3}$ are of the form $(\\pmb\\theta,b_{\\star}(\\pmb\\theta))$ with where $\\pmb\\theta\\in\\Theta_{\\star}\\cup\\Theta_{\\mathrm{min}}\\cup\\Theta_{\\mathrm{max}}\\cup\\Theta_{\\mathrm{sad}}$ Since $\\mathbf{T}_{\\star}\\cup\\mathbf{T}_{\\mathrm{min}}\\cup\\mathbf{T}_{\\mathrm{sad}}$ covers the entirety of stationary points of $L$ in $\\mathbb{R}^{3}$ , it follows that the set of all stationary points in $\\mathbb{R}^{2}$ is precisely $\\Theta_{\\star}\\cup\\Theta_{\\mathrm{min}}\\cup\\Theta_{\\mathrm{max}}\\cup\\Theta_{\\mathrm{sad}}$ Also, the ordering of losses directly follows from the aformentioned observation. ", "page_idx": 22}, {"type": "text", "text": "Now we characterize these critical points in terms of the extrema. ", "page_idx": 22}, {"type": "text", "text": "(i) Set of global and local minima: From Eq. (24), for any global minimum $\\boldsymbol{\\gamma}_{\\star}=(\\pmb{\\theta}_{\\star},b_{\\star}(\\pmb{\\theta}_{\\star}))$ of $L$ in $\\mathbb{R}^{3}$ , we have $\\pmb{\\theta}_{\\star}\\in\\Theta_{\\star}\\subseteq\\mathbb{R}^{2}$ . Hence by definition, $\\Theta_{\\star}$ is the set of all global minima in $\\mathbb{R}^{2}$ .A similar argument holds for $\\Theta_{\\mathrm{min}}$ , which establishes that it is a set of all local minima. ", "page_idx": 22}, {"type": "text", "text": "(ii Set of local maxima and saddle points: From Eq. (26), for any saddle point $\\gamma\\quad=$ $\\overline{{(e,w,b_{\\star}(e,w))}}$ of $L$ in $\\mathbb{R}^{3}$ , we have that $\\textit{e}=\\mathrm{~0~}$ and $(p+q\\,-\\,1)(1+\\,2w|w|)\\ \\leq\\ 0$ Hence $\\pmb\\theta=(e,w)\\in\\Theta_{\\mathrm{max}}\\cup\\Theta_{\\mathrm{sad}}$ . Suppose $\\theta\\in\\Theta_{\\mathrm{max}}$ which implies $e=0,(p+q-1)(1+2w|w|)<0$ By Lemma 8, the Hessian at $\\pmb{\\theta}$ (upto a positive scale) is a diagonal matrix with the entries $(p+q-1)(1+2w|w|)<0$ and O, corresponding to the directions of $e$ and $w$ respectively. Though one of the eigenvalue here is zero, using a continuity argument as in the proof of Thm. 7 for local minima, we can establish that $\\pmb{\\theta}$ is indeed a local maximum. Thus $\\Theta_{\\mathrm{max}}$ is a set of local minima. ", "page_idx": 22}, {"type": "text", "text": "Now suppose(e, w) sad. Thus e = 0 and w =- . Since it lies at the intersection of $\\Theta_{\\mathrm{min}}$ and $\\Theta_{\\mathrm{max}}$ , using a neighborhood argument, it's straightforward to see that $\\Theta_{\\mathrm{sad}}$ is indeed a set of saddle points. ", "page_idx": 22}, {"type": "text", "text": "Finally it follows that $\\Theta_{\\mathrm{min}}$ $\\Theta_{\\mathrm{max}}$ $\\Theta_{\\mathrm{sad}}$ are the only set of local minima, maxima, and saddle points from the above fact about the characterization of the set of all critical points in terms of these sets and $\\Theta_{\\star}$ , the ordering of the losses, and using the same argument as in the final steps of the proof of Thm. 7 with the bias. This concludes the proof. ", "page_idx": 23}, {"type": "text", "text": "F  Gradient flow analysis without attention ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we analyze the learning dynamics of the transformer parameters $\\pmb{\\theta}=(e,w)\\in\\mathbb{R}^{2}$ without the attention scalar. First, we present few important lemmas regarding the same, useful for the proofs of Thm. 2 and Thm. 8 later. Recall from Sec. 4 that the trajectory $(\\pmb\\theta_{t})_{t\\geq0}$ isgovernedby ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\pmb{\\theta}_{t}}{\\mathrm{d}t}=-\\nabla L(\\pmb{\\theta}_{t}),\\quad\\pmb{\\theta}_{t}=(e_{t},w_{t})\\in\\mathbb{R}^{2},\\,t\\geq0,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "starting with a randomly initalized $\\pmb{\\theta}_{0}$ . The energy function $\\mathcal{E}(\\cdot,\\cdot)$ is defined as ", "page_idx": 24}, {"type": "equation", "text": "$$\n{\\mathcal{E}}(e,w)\\triangleq e^{2}-(w^{2}+\\operatorname{sign}(w)\\cdot\\log|w|),\\quad\\forall(e,w)\\in\\mathbb{R}^{2}\\setminus\\mathrm{e.axis},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where-axis $\\triangleq\\{(e,w=0)\\}$ and w-axis $\\triangleq\\{(e=0,w)\\}$ Note that $\\begin{array}{r}{\\mathcal{E}_{\\mathrm{sad}}=\\mathcal{E}(0,-\\frac{1}{\\sqrt{2}})=-\\frac{1+\\log2}{2}}\\end{array}$ We re-present the Lemma 1 from Sec. 4 below for the sake of completeness. ", "page_idx": 24}, {"type": "text", "text": "Lemma 9 (Constant energy along the fow). For any $(p,q)~\\in~(0,1)^{2}$ and initialization $\\pmb\\theta_{0}\\ =$ $(e_{0},w_{0})\\in\\mathring{\\mathbb{R}}^{2}$ , let $(\\pmb\\theta_{t})_{t\\geq0}$ be the corresponding $G F$ trajectory starting from $\\theta_{0}$ f $w_{0}\\neq0$ then the energy stays constant along the trajectory, i.e. ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathcal{E}(\\pmb{\\theta}_{t})=e_{t}^{2}-\\left(w_{t}^{2}+\\mathrm{sign}(w_{t})\\cdot\\log|w_{t}|\\right)=\\mathcal{E}(\\pmb{\\theta}_{0}),\\quad\\forall t\\geq0.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "On the other hand, if $w_{0}=0,$ $w_{t}=0$ forall $t\\geq0$ .Hence, if we initialize on e-axis the trajectory always stays on the e-axis. ", "page_idx": 24}, {"type": "text", "text": "Now we establish that the GF trajectories always converge. ", "page_idx": 24}, {"type": "text", "text": "Lemma 10 (GF convergence). Let $(\\pmb\\theta_{t})_{t\\geq0}$ be a continuously diferentiable $G F$ trajectory starting from $\\pmb{\\theta}_{0}$ .Then for all initializations $\\pmb{\\theta}_{0}\\in\\overline{{\\mathbb{R}}}^{2}$ \uff0c ", "page_idx": 24}, {"type": "text", "text": "(i) $(\\pmb\\theta_{t})_{t\\geq0}$ is bounded, (i) there exists a $\\theta_{\\mathrm{{lim}}}\\in\\mathbb{R}^{2}$ such that $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}\\pmb{\\theta}_{t}=\\pmb{\\theta}_{\\operatorname*{lim}}}\\end{array}$ and (iii) $\\begin{array}{r}{\\operatorname*{lim}_{t\\rightarrow\\infty}\\Vert\\nabla L(\\pmb{\\theta}_{t})\\Vert=\\Vert\\nabla L(\\pmb{\\theta}_{\\mathrm{lim}})\\Vert=0.}\\end{array}$ ", "page_idx": 24}, {"type": "text", "text": "Hence $\\theta_{\\mathrm{lim}}$ is a critical point of $L$ ", "page_idx": 24}, {"type": "text", "text": "The following result characterizes the energy of the limit point. ", "page_idx": 24}, {"type": "text", "text": "Lemma 11 (Energy at the limit point). Consider the same setting as in Lemma 10. I1f $\\pmb{\\theta}_{0}\\in\\mathbb{R}^{2}\\backslash\\mathrm{e}.$ -axis, then $\\mathcal{E}(\\pmb{\\theta}_{\\mathrm{lim}})=\\mathcal{E}(\\pmb{\\theta}_{0})$ . Hence $\\pmb\\theta_{\\mathrm{lim}}$ lies at the intersection of the contour line $\\mathcal{E}(e,w)=\\mathcal{E}_{0}$ with the setofcriticalpointsof $L$ in $\\mathbb{R}^{2}$ ", "page_idx": 24}, {"type": "text", "text": "On the other hand, if $\\theta_{0}\\in\\mathrm{e}$ -axis, then $\\theta_{\\mathrm{lim}}\\in\\mathrm{e}$ -axis. ", "page_idx": 24}, {"type": "text", "text": "We now study the energy function on the w-axis which plays a key role in the GF analysis. ", "page_idx": 24}, {"type": "text", "text": "Lemma 12 (Analysis of the energy function). Let $\\mathcal{E}(\\cdot,\\cdot)$ be the energy function defined in Eq. (48)   \nand $f(w)\\,\\triangleq\\,\\mathcal{E}(e\\,=\\,0,w)\\,=\\,-\\big(w^{2}+\\mathrm{sign}(w)\\cdot\\log|w|\\big)$ be the energy evaluated on w-axis for   \n$w\\in\\mathbb{R}\\setminus\\{0\\}$ .Then   \n(i) $f:(-\\infty,-1/\\sqrt{2}]\\to(-\\infty,\\,\\mathcal{E}_{\\mathrm{sad}}]$ is monotonically increasing with $\\mathrm{lim}_{w\\to-\\infty}\\,f(w)\\,=\\,-\\infty$ and the maximum being $f(-1/{\\sqrt{2}})=\\mathcal{E}_{\\mathrm{sad}}.$   \n(i) $f:[-1/\\sqrt{2},0)\\to[{\\mathcal{E}}_{\\mathrm{sad}},-\\infty)$ is monotonically decreasing with $\\begin{array}{r}{\\operatorname*{lim}_{w\\to0^{-}}\\ f(w)=-\\infty,}\\end{array}$   \n(ii) $f^{\\prime}(-{\\frac{1}{\\sqrt{2}}})=0,$ and   \n(iv) $f\\;:\\;\\;(0,\\infty)\\;\\;\\rightarrow\\;\\;(-\\infty,\\infty)$ is monotonically decreasing with $\\begin{array}{r}{\\operatorname*{lim}_{w\\to0^{+}}f(w)~=~\\infty}\\end{array}$ and $\\operatorname*{lim}_{w\\to\\infty}f(w)=-\\infty$ ", "page_idx": 24}, {"type": "text", "text": "We are now ready to prove Thm. 2 corresponding to $p+q>1$ ", "page_idx": 24}, {"type": "text", "text": "F.1 Proof of Thm. 2 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Proof. Let $\\pmb{\\theta}_{0}=(e_{0},w_{0})\\in\\mathbb{R}^{2}$ be the initialization for the GF trajectory $(\\pmb\\theta_{t})_{t\\geq0}$ . Recall that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb Z_{\\operatorname*{min}}\\triangleq\\left\\lbrace(e,w):w\\in(-1/\\sqrt{2},0),\\,e\\in(-g(w),g(w)),\\,g(w)=\\sqrt{w^{2}-\\log(-w)+\\mathcal{E}_{\\mathrm{sad}}}\\right\\rbrace}\\\\ &{\\qquad\\qquad\\cup\\left\\lbrace(e,w):w>0\\right\\rbrace\\cup\\left\\lbrace(e,w):w=0\\right\\rbrace,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{Z}_{\\mathrm{sad}}\\triangleq\\left\\{(e,w):w\\in[-1/\\sqrt{2},0),\\,e=\\pm\\sqrt{w^{2}-\\log(-w)+\\mathcal{E}_{\\mathrm{sad}}}\\right\\},}\\\\ &{\\mathbb{Z}_{\\mathrm{max}}\\triangleq\\left\\{(e,w):e=0,\\,w<-1/\\sqrt{2}\\right\\},}\\\\ &{\\quad\\mathbb{Z}_{\\star}\\triangleq\\mathbb{R}^{2}\\setminus\\left(\\mathbb{Z}_{\\mathrm{min}}\\cup\\mathbb{Z}_{\\mathrm{sad}}\\cup\\mathbb{Z}_{\\mathrm{max}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We consider the cases $\\theta_{0}\\in\\mathrm{e}$ -axis and $\\pmb{\\theta}_{0}\\in\\mathbb{R}^{2}\\setminus\\mathbf{e}.$ -axis separately. First recall from Thm. 1 and Eq. (6) that for $p+q>1$ , the loci of the global minima, $\\begin{array}{r}{e^{2}(1+2w|w|)\\,=\\,\\log{\\frac{(1-p)(1-q)}{p q}}\\,<\\,0}\\end{array}$ lies entirely in the negative haf-planecorespondingto $w<-\\textstyle{\\frac{1}{\\sqrt{2}}}$ On the other hand, althe local minima, maxima and the saddle points span the w-axis corresponding to $e=0$ ", "page_idx": 25}, {"type": "text", "text": "i $\\pmb{\\theta}_{0}\\in\\mathbb{R}^{2}\\setminus\\mathrm{e{-axis}:}$ Let $\\mathcal{E}_{0}\\,=\\,\\mathcal{E}(\\pmb{\\theta}_{0})\\,\\in\\,\\mathbb{R}$ . By Lemmas. (9), (10), and (11), we have that the trajectory $\\overline{{(\\pmb\\theta_{t})_{t\\geq0}}}$ always stays on the contour line $\\mathcal{E}(e,w)=\\mathcal{E}_{0}$ and converges to the limit $\\theta_{\\mathrm{lim}}$ which is an intersection of this contour line with the set of critical points of $L$ . Hence the crux of the proof is to establish where these intersections occur based on the initialization $\\theta_{0}$ and the initial energy $\\mathcal{E}_{0}$ . This gives rise to the set of initializations $\\mathcal{T}_{\\mathrm{min}},\\mathcal{T}_{\\mathrm{max}},\\mathcal{T}_{\\mathrm{sad}}$ , and $\\mathcal{T}_{\\star}$ that correspond to the limit being a local minimum/maximum, a saddle point, or a global minimum. ", "page_idx": 25}, {"type": "text", "text": "We characterize them individually below starting with $\\mathcal{T}_{\\mathrm{min}}$ ", "page_idx": 25}, {"type": "text", "text": "Initializations for local minima, $\\mathcal{T}_{\\mathrm{min}}$ . For $\\pmb{\\theta}_{0}\\,=\\,(e_{0},w_{0})\\,\\in\\,\\mathbb{R}^{2}\\;\\backslash$ e-axis, assume that $w_{0}\\,>\\,0$ Since $\\mathcal{E}_{0}\\in\\mathbb{R}$ , there exists an unique $w_{\\star}>0$ such that $f(w_{\\star})=\\mathcal{E}(0,\\dot{w}_{\\star})=\\mathcal{E}_{0}$ by Lemma 12, (iv). Further using the fact that the energy contour lines do not cross each other (by definition of a contour line) and the fact they do not intersect the e-axis (it's an energy barrier as discussed in Sec. 4), it follows that the contour line $\\mathcal{E}(e,w)=\\mathcal{E}_{0}$ stays entirely in the positive half-plane corresponding to $w>0$ and $w_{\\star}>0$ is the unique (and only) intersection of this line with the w-axis, and hence the set of critical points. Since the w-axis corresponding to $w>0$ is a set of a local minima (Eq. (7)), it follows that any initialization $(e_{0},w_{0})$ with $w_{0}>0$ converges to a local minimum. ", "page_idx": 25}, {"type": "text", "text": "Now suppose $\\begin{array}{r}{-\\frac{1}{\\sqrt{2}}<w_{0}<0}\\end{array}$ and $e_{0}\\in(-g(w_{0}),g(w_{0}))$ , where $g(w_{0})=\\sqrt{w^{2}-\\log(-w)+\\mathcal{E}_{\\mathrm{sad}}}$ Thus $|e_{0}|<g(w_{0})$ and hence $e_{0}^{2}\\!-\\!\\big(w_{0}^{2}\\!-\\!\\log(-w_{0})\\big)=\\mathcal{E}\\big(e_{0},w_{0}\\big)=\\mathcal{E}_{0}<\\mathcal{E}_{\\mathrm{sad}}.$ Hence by Lemma 12, (ii), there is a unique intersection of the contour line $\\mathcal{E}(e,w)=\\mathcal{E}_{0}$ with the w-axis, which lies in the region $\\left(-{\\frac{1}{\\sqrt{2}}},0\\right)$ . Further note tha this contour line cannot intersect with the lobal minima loci as it lies in the haf-plane $w<-\\textstyle{\\frac{1}{\\sqrt{2}}}$ andhenitwitfcral ntss segment of w-axis, which is precisely the set of local minima the GF initialized on this line would converge to. ", "page_idx": 25}, {"type": "text", "text": "Thus we have shown that any initialization in $\\mathcal{Z}_{\\mathrm{min}}\\backslash\\cup\\{(e,w):w=0\\}$ converges to a local minimum, the set of which exhausts all the set of local minima $\\Theta_{\\mathrm{min}}$ except for the origin. Below we will estbalish that any initialization on $\\mathrm{e-axis}=\\{(e,w):w=0\\}$ converges to the origin, implying $\\mathcal{T}_{\\mathrm{min}}$ is the full set of initializations for which the limit is a local minimum. ", "page_idx": 25}, {"type": "text", "text": "Initializations for saddle points, $\\mathcal{Z}_{\\mathrm{sad}}$ . It's straightforward to see that for any $\\theta_{0}\\in\\mathcal{T}_{\\mathrm{sad}}$ \uff0c $e_{0}^{2}-(w_{0}^{2}-$ $\\begin{array}{r}{\\log(-w_{0}))=\\mathcal{E}(0,-\\frac{1}{\\sqrt{2}})\\stackrel{\\cdot}{=}\\mathcal{E}_{\\mathrm{sad}}}\\end{array}$ . Since \u2264wo < 0, the point (w,e) = (- V, 0) is the only intersection of the contour line with the set of critical points, any initialization in $\\mathcal{Z}_{\\mathrm{sad}}$ converges to the saddle point. On the other hand, there also exists a contour line $e_{0}^{2}-(w_{0}^{2}-\\log(-w_{0}))=\\bar{\\mathcal{E}_{\\mathrm{sad}}}$ for $w_{0}<-\\frac{1}{\\sqrt{2}}$ that passes through $(-{\\frac{1}{\\sqrt{2}}},0)\\in\\mathbb{R}^{2}$ and further intersecting with the global minima loci $\\Theta_{\\star}$ . However, if we initialize on this line the flow escapes away from the saddle point and converges instead to a global minimum. To show this, it suffices to prove that $\\begin{array}{r}{\\frac{\\mathrm{d}e_{t}}{\\mathrm{d}t}>0}\\end{array}$ and $\\begin{array}{r}{\\frac{\\mathrm{d}w_{t}}{\\mathrm{d}t}<0}\\end{array}$ \u8fd1 $e_{0}>0$ and $w_{0}<-\\textstyle{\\frac{1}{\\sqrt{2}}}$ , such that $(w_{0},e_{0})$ is close to the addle point $(-{\\frac{1}{\\sqrt{2}}},0)$ (the case for $e_{0}\\,<\\,0$ is similar as the flow is symmetric in $e\\in\\mathbb{R}$ ). From Lemma 7 and the definition of the GF, we have that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\mathrm{d}e_{t}}{\\mathrm{d}t}=-\\frac{\\partial L}{\\partial e}(e_{0},w_{0})=2\\mathbb{E}_{X}\\left[(f_{1}X+f_{2})X\\right]\\cdot(1-2w_{0}^{2}))e_{0}}\\\\ {\\displaystyle\\frac{\\mathrm{d}w_{t}}{\\mathrm{d}t}=-\\frac{\\partial L}{\\partial w}(e_{0},w_{0})=4\\mathbb{E}_{X}\\left[(f_{1}X+f_{2})X\\right]\\cdot(-e_{0}^{2}w_{0}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "So it suffices to show that $\\mathbb{E}_{X}\\left[(f_{1}X+f_{2})X\\right]>0$ . To establish this, we have from Lemma 5 that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X}\\left[(f_{1}X+f_{2})X\\right]=\\mathbb{E}[X](f_{1}+f_{2})=\\pi_{1}\\left(-\\frac{f_{2}}{\\pi_{1}}+f_{2}\\right)=-\\pi_{0}\\cdot f_{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "From the defintion of $f_{2}$ and the optimal bias $b_{\\star}$ in Lemma 7 and Lemma 5 respectively, we obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{f_{2}=\\sigma\\left(b_{\\star}-\\frac{e_{0}^{2}}{2}\\right)-p=\\left(1+\\exp\\left(-b_{\\star}+\\frac{e_{0}^{2}}{2}\\right)\\right)^{-1}-p}}\\\\ {{\\mathrm{\\boldmath~\\alpha~}=\\left(1+\\frac{2A}{\\frac{p}{q}-1+\\sqrt{\\left(\\frac{p}{q}-1\\right)^{2}+4\\cdot\\frac{p}{q}\\cdot A}}\\right)^{-1}-p,\\quad A\\triangleq\\exp(e_{0}^{2}(1-2w_{0}^{2})).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "When $e_{0}=0$ , we have $A=1$ and hence ", "page_idx": 26}, {"type": "equation", "text": "$$\nf_{2}=\\left(1+\\frac{q}{p}\\right)^{-1}-p=\\frac{p}{p+q}-p=-\\frac{p}{p+q}(p+q-1)<0,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where we used the fact that $p+q>1$ . Hence by continuity of $f_{2}$ in $e_{0}$ , for $e_{0}$ sufficiently close to 0, $f_{2}<0$ which proves our claim about the direction of the flow close to the saddle point. By using the continuity of the fow, it follows that GF cannot converge to saddle point when initialized on this contour line for wo<-2\u00b7 . Thus $\\mathcal{Z}_{\\mathrm{sad}}$ is the only set of initializations for convergence to $\\Theta_{\\mathrm{sad}}$ ", "page_idx": 26}, {"type": "text", "text": "Initializations for local maxima, $\\mathcal{Z}_{\\mathrm{max}}$ .If $p+q\\;>\\;1$ , we have from Thm. 1 that $\\Theta_{\\mathrm{max}}\\ =$ $\\{(e,w)\\in\\mathbb{R}^{2}:e=0$ $(1+2w|w|)<0\\big\\}=\\Big\\{(e,w)\\in\\mathbb{R}^{2}:e=0.$ $\\begin{array}{r}{w<-\\frac{1}{\\sqrt{2}}\\Big\\}}\\end{array}$ Thus forany $\\pmb\\theta_{0}\\in$ Omax, ddt $\\begin{array}{r}{\\frac{\\mathrm{d}\\theta_{t}}{\\mathrm{d}d t}=0}\\end{array}$ for all $t\\geq0$ and hence $\\theta_{\\mathrm{{lim}}}=\\theta_{0}$ Further if we slightlyperturb away from this set, from Eq. (36) it follows that the flow diverges and hence it's an unstable set of critical points (they are local maxima indeed). Thus the only set of initializations leading to local maxima are $Z_{\\operatorname*{max}}=\\Theta_{\\operatorname*{max}}$ ", "page_idx": 26}, {"type": "text", "text": "Initializations for the global minima, $\\mathcal{Z}_{\\star}$ . Since the set of all critical points of $L$ is $\\Theta_{\\star}\\cup\\Theta_{\\mathrm{min}}\\cup$ $\\Theta_{\\mathrm{max}}\\cup\\Theta_{\\mathrm{sad}}$ , and the initializations in $\\mathcal{T}_{\\mathrm{min}}$ \uff0c $\\mathcal{Z}_{\\mathrm{sad}}$ , and $\\mathcal{Z}_{\\mathrm{max}}$ converge to $\\Theta_{\\mathrm{min}}$ \uff0c $\\Theta_{\\mathrm{sad}}$ , and $\\Theta_{\\mathrm{max}}$ respectively, it follows that the set of initializations for which the GF converges to global minima is $\\mathcal{T}_{\\star}\\,\\dot{=}\\,\\mathbb{R}^{2}\\setminus\\dot{(\\mathcal{Z}_{\\mathrm{min}}\\cup\\mathcal{Z}_{\\mathrm{sad}}\\cup\\mathcal{Z}_{\\mathrm{max}})}$ ", "page_idx": 26}, {"type": "text", "text": "In fact, sincethe loi of the global minimalies in the halfplane corespondinto $w\\ <\\ -\\frac{1}{\\sqrt{2}}$ when $p+q>1$ , we can precisely determine the location of the global minimum for which the intersection occurs for any $\\theta_{0}\\in\\mathcal{Z}_{\\star}$ . Specifically, we can solve the pair of equations $\\mathcal{E}(e,w)\\,=$ $e^{2}-w^{2}+\\log(-w)=\\mathcal{E}_{0}$ and $\\begin{array}{r}{e^{2}(1-2w^{2})=\\log\\frac{(1-p)(1-q)}{p q}}\\end{array}$ which has aunique solutionfor $w<0$ (upto a sign flip in $e$ ", "page_idx": 26}, {"type": "text", "text": "(i) $\\pmb{\\theta}_{0}\\in\\mathrm{e-axis}\\Rightarrow\\pmb{\\theta}_{0}\\in\\mathcal{Z}_{\\mathrm{min}}\\pmb{:}$ If $\\pmb{\\theta}_{0}=(e_{0},w_{0})\\in\\mathfrak{\\epsilon}$ e-axis, we have that $w_{0}=0$ and hence $w_{t}=0$ for all $t\\,\\geq\\,0$ (Lemma 9). Lemma 10-(i) also establishes that the iterates $(\\pmb\\theta_{t}\\,=\\,(e_{t},0))_{t\\geq0}$ stay bounded on the e-axis and monotonically decrease. Since the origin is the only critical point of $L$ on the $\\mathrm{^e}$ -axis, and $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}\\pmb{\\theta}_{t}=\\pmb{\\theta}_{\\operatorname*{lim}}}\\end{array}$ exists, it follows that $\\pmb{\\theta}_{\\mathrm{lim}}\\bar{=}\\,(0,0)$ , a local minima. Thus $\\theta_{0}\\in\\mathcal{T}_{\\operatorname*{min}}$ ", "page_idx": 26}, {"type": "text", "text": "This concludes the proof for all the initializations $\\theta_{0}\\in\\mathbb{R}^{2}$ ", "page_idx": 26}, {"type": "text", "text": "Gaussian initialization $\\mathcal{N}(0,\\sigma^{2}I_{2})$ : When $\\theta_{0}$ is initialized according to the standard Gaussian distribution $\\mathcal{N}(0,\\sigma^{2}I_{2})$ with $\\begin{array}{r}{\\overline{{\\sigma^{2}\\ll\\frac{1}{\\sqrt{2}}}}}\\end{array}$ , we note that $\\theta_{0}$ lands in the set $\\mathcal{T}_{\\mathrm{min}}$ With high probability. In fact, this probability can be made arbitrarily close to 1 depending on $\\sigma^{2}$ . Thus this initialization will lead to a local minimum convergence on the w-axis. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "F.2  Gradient flow dynamics for $p+q<1$ ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Theorem 8 (GF dynamics for $p+q<1)$ ). Under the same setting as in Thm. 2 with $p+q<1$ and any initialization $\\mathbf{\\dot{\\theta}}_{0}\\in\\mathbb{R}^{2}$ the $G F$ trajectory always converges to a $\\pmb{\\theta}_{\\mathrm{lim}}\\in\\mathbb{R}^{2}$ which is a critical point of the loss $L$ More specifically, $\\pmb\\theta_{\\mathrm{lim}}$ is ", "page_idx": 26}, {"type": "text", "text": "(i) alocal minimum if ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\theta_{0}\\in\\mathcal{T}_{\\operatorname*{min}}\\triangleq\\left\\{(e,w):w<-1/\\sqrt{2},\\,e\\in(-g(w),g(w)),\\,g(w)=\\sqrt{w^{2}-\\log(-w)+\\mathcal{E}_{\\mathrm{sad}}}\\right\\},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$\\begin{array}{r}{\\pmb{\\theta}_{0}\\in\\mathcal{T}_{\\mathrm{sad}}\\triangleq\\Big\\{(e,w):w\\leq-1/\\sqrt{2},\\,e=\\pm\\sqrt{w^{2}-\\log(-w)+\\mathcal{E}_{\\mathrm{sad}}}\\Big\\},}\\end{array}$ ", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "(i) a local maximum if $\\bullet\\theta_{0}\\in\\mathcal{Z}_{\\operatorname*{max}}\\triangleq\\big\\{(e,w):e=0,\\,w>-1/\\sqrt{2}\\big\\}.$ (iv) and a global minimum if $\\pmb{\\theta}_{0}\\in\\mathbb{R}^{2}\\setminus(\\mathcal{Z}_{\\mathrm{min}}\\cup\\mathcal{Z}_{\\mathrm{sad}}\\cup\\mathcal{Z}_{\\mathrm{max}})$ ", "page_idx": 27}, {"type": "text", "text": "Consequentely, if we use the standard initialization $\\theta_{0}\\sim\\mathcal{N}(0,\\sigma^{2}I_{2})$ with $\\sigma^{2}\\ll1/{\\sqrt{2}}$ $\\theta_{\\mathrm{lim}}$ willbe a global minimum. ", "page_idx": 27}, {"type": "text", "text": "Proof. The proof for the case of $p+q<1$ essentially follows the same steps as that of $p+q>1$ If the initialization is not on the e-axis we use the energy equation to establish the convergence to the critical point at the intersection of the energy contour line with the critical set and if it starts on the e-axis, the only change is that it now converges to the global minimum instead of the origin as in the earlier case. This is due to the fact that origin turns out to be a local maximum when $p+q<1$ and hence it's an unstable critical point (which can be established as in the proof of Thm. 2 for $\\mathcal{Z}_{\\mathrm{max}}$ ).\u53e3 ", "page_idx": 27}, {"type": "text", "text": "G  Gradient fow analysis with attention ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section, we analyze the learning dynamics of the transformer parameters $\\pmb{\\theta}\\in\\mathbb{R}^{3}$ with the attention scalar $a\\in\\mathbb R$ ,i.e. $\\pmb{\\theta}=(e,w,\\bar{a})\\bar{\\in}\\mathbb{R}^{3}$ . Similar to the analysis for $\\pmb{\\theta}=(e,w)\\in\\mathbb{R}^{2}$ ,we first introduce the canonical parameterization including $a\\in\\mathbb R$ , then analyze the corresponding loss function $L(\\cdot)$ in terms of its gradients and critical points, and capitalize on it to study the gradient flow dynamics using the energy. We first start with the parameterization. ", "page_idx": 28}, {"type": "text", "text": "G.1  Canonical parameterization with attention ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Embedding. Recall from App. C that we let $e=e\\cdot\\alpha$ and $p_{n}=-p\\cdot{\\pmb{\\alpha}}$ for all $n$ where $\\begin{array}{r}{e>0,p=\\frac{e}{2}}\\end{array}$ and $\\alpha\\in\\{\\pm1\\}^{d}/\\sqrt{d}$ This results in the embedding ", "page_idx": 28}, {"type": "equation", "text": "$$\n{\\pmb x}_{n}=e\\left(x_{n}-\\frac{1}{2}\\right){\\pmb\\alpha}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Attention. Similarly, we recall from Eq. (21) that the attention output $\\pmb{y}_{n}$ is given by ", "page_idx": 28}, {"type": "equation", "text": "$$\ny_{n}=e\\left(x_{n}-{\\frac{1}{2}}\\right)\\alpha+\\langle v,\\alpha\\rangle\\left(\\sum_{i\\in[n]}\\operatorname{att}_{n,i}\\cdot e\\left(x_{i}-{\\frac{1}{2}}\\right)\\right)\\alpha,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\mathrm{att}_{n,i}\\triangleq\\exp\\left(\\langle q_{n},k_{i}\\rangle/\\sqrt{d}\\right)/\\left(\\sum_{j\\in[n]}\\exp\\left(\\langle q_{n},k_{j}\\rangle/\\sqrt{d}\\right)\\right),\\,q_{n}=W_{Q}\\,x_{n},\\,k_{i}=W_{K}\\,x_{i},}\\\\ &{}&{W_{Q}^{\\top}W_{K}=(q^{2}d)\\,\\alpha\\cdot\\alpha^{\\top}\\in\\mathbb{R}^{d\\times d},\\quad\\mathrm{for~some~}q\\in\\mathbb{R}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Instead of the softmax, now we assume that the attention weights are linear in the scaled dot product i.e. ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{att}_{n,i}=\\displaystyle\\frac{\\langle q_{n},k_{i}\\rangle}{n\\sqrt{d}}=\\displaystyle\\frac{1}{\\sqrt{d}}\\cdot x_{n}^{\\top}W_{Q}^{\\top}W_{K}x_{n}=\\displaystyle\\frac{q^{2}d}{n\\sqrt{d}}\\cdot(x_{n}^{\\top}\\alpha)(x_{i}^{\\top}\\alpha)}\\\\ &{\\phantom{a a a a a a}{\\frac{q^{2}d}{n\\sqrt{d}}}\\displaystyle\\frac{q^{2}d^{3}}{n\\sqrt{d}}\\cdot(e x_{n}-p)(e x_{i}-p)}\\\\ &{\\phantom{a a a a a a a a}=\\displaystyle\\frac{q^{2}d^{5/2}}{n}\\cdot(e x_{n}-p)(e x_{i}-p)}\\\\ &{\\phantom{a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a}{n}}\\\\ &{\\phantom{a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a}}=\\displaystyle\\frac{q^{2}d^{5/2}e^{2}}{n}\\cdot\\left(x_{n}-\\displaystyle\\frac{1}{2}\\right)\\left(x_{i}-\\displaystyle\\frac{1}{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Note that the $1/n$ factor is to ensure normalization for the attention weights in Eq. (37). Now substituting Eq. (38) in Eq. (37), we obtain ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{y_{n}=e\\left(x_{n}-\\frac{1}{2}\\right)\\alpha+\\langle v,\\alpha\\rangle\\left(\\underset{i\\in[n]}{\\sum}\\mathfrak{a t t t}_{n,i}\\cdot e\\left(x_{i}-\\frac{1}{2}\\right)\\right)\\alpha}\\\\ &{\\ =\\left[e\\left(x_{n}-\\frac{1}{2}\\right)+\\langle v,\\alpha\\rangle\\left(\\underset{i\\in[n]}{\\sum}\\frac{1}{n}q^{2}d^{5/2}e^{2}(x_{n}-\\frac{1}{2})(x_{i}-\\frac{1}{2})\\right)\\cdot e\\left(x_{i}-\\frac{1}{2}\\right)\\right]\\alpha}\\\\ &{\\ =\\left[e\\left(x_{n}-\\frac{1}{2}\\right)\\left(1+\\langle v,\\alpha\\rangle q^{2}d^{5/2}e^{2}\\left(x_{i}-\\frac{1}{2}\\right)^{2}\\right)\\right]\\alpha}\\\\ &{\\ =\\left[e\\left(x_{n}-\\frac{1}{2}\\right)\\left(1+\\langle v,\\alpha\\rangle q^{2}d^{5/2}\\frac{1}{4}e^{2}.\\right)\\right]\\alpha}\\\\ &{\\ =e\\left(x_{n}-\\frac{1}{2}\\right)\\left(1+\\langle v,\\alpha\\rangle q^{2}d^{5/2}\\frac{1}{4}e^{-\\frac{1}{2}}\\right)\\right]\\alpha}\\\\ &{\\ =e\\left(x_{n}-\\frac{1}{2}\\right)\\left(1+\\alpha e^{2}\\right)\\alpha,}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where we used the fact that $\\begin{array}{r}{(x_{i}-\\frac{1}{2})^{2}=\\frac{1}{4}}\\end{array}$ since $x_{i}\\in\\{0,1\\}$ , and ", "page_idx": 29}, {"type": "equation", "text": "$$\na\\triangleq\\frac{\\langle\\boldsymbol{v},\\boldsymbol{\\alpha}\\rangle q^{2}d^{5/2}}{4}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "is the attention scalar. Note that this includes the scaling $\\langle v,\\alpha\\rangle$ from the value matrix $W_{V}$ and $q^{2}$ from the query-key dot product. Thus we succinctly have ", "page_idx": 29}, {"type": "equation", "text": "$$\ny_{n}=e\\left(x_{n}-{\\frac{1}{2}}\\right)\\left(1+a e^{2}\\right)\\alpha.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Feed-forward. For the feed-forward layer, we have that $\\begin{array}{r}{W_{1}\\,=\\,\\frac{|w|}{\\sqrt{d}}\\,{\\mathbf1}\\cdot\\alpha^{\\top}\\,\\in\\,\\mathbb{R}^{4d\\times d},\\quad W_{2}\\,=\\,}\\end{array}$ $\\textstyle{\\frac{w}{\\sqrt{d}}}\\,\\alpha\\cdot{\\bf1}^{\\top}\\in\\mathbb{R}^{d\\times4d}$ .HenceEq.(40) implies ", "page_idx": 29}, {"type": "equation", "text": "$$\nW_{1}y_{n}={\\frac{|w|}{\\sqrt{d}}}\\,\\mathbf{1}\\cdot\\alpha^{\\top}\\left[e\\left(x_{n}-{\\frac{1}{2}}\\right)\\left(1+a e^{2}\\right)\\right]\\alpha={\\frac{|w|}{\\sqrt{d}}}\\left[e\\left(x_{n}-{\\frac{1}{2}}\\right)\\left(1+a e^{2}\\right)\\right]\\mathbf{1}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Thus, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{3eLU}(W_{1}y_{n})=\\frac{|w|}{\\sqrt{d}}\\mathbf{1}\\cdot\\mathrm{ReLU}\\left(\\left[e\\left(x_{n}-\\frac{1}{2}\\right)\\left(1+a e^{2}\\right)\\right]\\right)}\\\\ &{\\qquad\\qquad=\\frac{|w|}{\\sqrt{d}}\\mathbf{1}\\cdot e\\,\\mathrm{ReLU}\\left(\\left[\\left(x_{n}-\\frac{1}{2}\\right)\\left(1+a e^{2}\\right)\\right]\\right)}\\\\ &{\\qquad\\qquad=\\frac{|w|}{\\sqrt{d}}\\mathbf{1}\\cdot e\\,\\left(\\frac{x_{n}}{2}\\,\\mathrm{ReLU}\\left(1+a e^{2}\\right)+\\frac{1-x_{n}}{2}\\,\\mathrm{ReLU}\\left(-1-a e^{2}\\right)\\right)}\\\\ &{\\qquad\\qquad=\\frac{|w|}{2\\sqrt{d}}\\mathbf{1}\\cdot e\\,\\left(x_{n}\\left[\\mathrm{ReLU}\\left(1+a e^{2}\\right)-\\mathrm{ReLU}\\left(-1-a e^{2}\\right)\\right]+\\mathrm{ReLU}\\left(-1-a e^{2}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Using $\\mathrm{ReLU}(x)-\\mathrm{ReLU}(-x)=x$ above, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\operatorname{ReLU}(W_{1}y_{n})=\\frac{|w|}{2\\sqrt{d}}\\,\\mathbf{1}\\cdot e\\,\\left(x_{n}\\left(1+a e^{2}\\right)+\\operatorname{ReLU}\\left(-1-a e^{2}\\right)\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Hence, ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{2}\\mathrm{ReLU}(W_{1}y_{n})=\\displaystyle\\frac{w}{\\sqrt{d}}\\alpha\\cdot\\mathbf{1}^{\\top}\\frac{|w|}{2\\sqrt{d}}\\mathbf{1}\\cdot e\\,\\left(x_{n}\\left(1+a e^{2}\\right)+\\mathrm{ReLU}\\left(-1-a e^{2}\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=2w|w|e\\,\\left(x_{n}\\left(1+a e^{2}\\right)+\\mathrm{ReLU}\\left(-1-a e^{2}\\right)\\right)\\alpha}\\\\ &{\\qquad\\qquad\\qquad=2w|w|e\\,\\left(x_{n}\\left(1+a e^{2}\\right)+\\frac{\\left(-1-a e^{2}\\right)}{2}+\\frac{\\left|1+a e^{2}\\right|}{2}\\right)\\alpha}\\\\ &{\\qquad\\qquad\\qquad=2w|w|e\\,\\left(\\left(x_{n}-\\frac{1}{2}\\right)\\left(1+a e^{2}\\right)+\\frac{\\left|1+a e^{2}\\right|}{2}\\right)\\alpha.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Thus the embedding ${\\boldsymbol{z}}_{n}$ is given by ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{z_{n}=y_{n}+W_{2}\\mathrm{ReLU}(W_{1}y_{n})}}\\\\ {{\\mathrm{}}}\\\\ {{\\displaystyle{}=\\left[e\\left(x_{n}-\\frac{1}{2}\\right)\\left(1+a e^{2}\\right)\\right]\\alpha+2w|w|e\\left(\\left(x_{n}-\\frac{1}{2}\\right)\\left(1+a e^{2}\\right)+\\frac{|1+a e^{2}|}{2}\\right)\\alpha+2\\alpha|w|\\alpha}}\\\\ {{\\mathrm{}}}\\\\ {{\\displaystyle{}=e\\left[\\left(x_{n}-\\frac{1}{2}\\right)\\left(1+a e^{2}\\right)\\left(1+2w|w|\\right)+w|w(1+a e^{2})|\\right]\\alpha.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Linear. Since $e=\\pmb{a}=e\\cdot\\pmb{\\alpha}$ due to weight-tying, the logits are given by ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathrm{ogit}_{n}(e,w,a,b)=\\langle a,z_{n}\\rangle+b=e^{2}\\left[\\left(x_{n}-\\frac{1}{2}\\right)\\left(1+a e^{2}\\right)\\left(1+2w|w|\\right)+w|w(1+a e^{2})|\\right]+b.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Loss. Denote $\\pmb{\\theta}\\triangleq\\,(e,w,a)\\,\\in\\,\\mathbb{R}^{3}$ .Similar to the case without $a$ (Eq. (14) and Lemma 2), the cross-entropy loss in our setting can be compactly written as ", "page_idx": 29}, {"type": "equation", "text": "$$\nL(\\theta,b)=\\frac{1}{N}\\sum_{n\\in[N]}\\mathbb{E}[\\ell_{\\log}\\left((2x_{n+1}-1)\\cdot\\log\\mathrm{it}_{n}(\\theta,b)\\right)]=\\mathbb{E}_{X,Y}\\left[\\ell_{\\log}\\left((2Y-1)\\cdot\\log\\mathrm{it}_{X}(\\theta,b)\\right)\\right],\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\begin{array}{r}{\\log\\mathrm{it}_{X}(\\theta,b)\\triangleq e^{2}\\left[\\left(X-\\frac{1}{2}\\right)\\left(1+a e^{2}\\right)(1+2w|w|)+w|w(1+a e^{2})|\\right]+b,(X,Y)\\in\\{0,1\\}^{2}}\\end{array}$ are distributed according to $(X,Y)\\sim(\\pi,P)$ ,i.e. $X$ is a Bernoulli random variable with $X\\sim\\pi\\equiv$ $\\mathrm{Bern}(p/(p+q))$ and $Y|X\\sim P(p,q)$ , the Markov kernel. Further, using the convexity of $b$ in $L(\\cdot,b)$ \uff0c we can consider the optimal bias $\\begin{array}{r}{b_{\\star}(\\pmb\\theta)=\\mathrm{argmin}_{b\\in\\mathbb{R}}\\,L(\\pmb\\theta,b)}\\end{array}$ in Eq. (41) to obtain the loss $L(\\theta)$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Sigma(\\theta)\\triangleq L(\\theta,b_{\\star})=\\mathbb{E}_{X,Y}\\left[\\ell_{\\log}\\left((2Y-1)\\cdot\\log\\mathrm{it}_{X}(\\theta,b_{\\star})\\right)\\right]}\\\\ &{\\qquad=\\mathbb{E}_{X,Y}\\left[\\ell_{\\log}\\left((2Y-1)\\cdot\\left(e^{2}\\left[\\left(X-\\frac{1}{2}\\right)\\left(1+a e^{2}\\right)\\left(1+2w|w|\\right)+w|w(1+a e^{2})\\right|\\right]+b_{\\star}\\right)\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We derive the expression for the optimal bias $b_{\\star}$ in the proof of Lemma 13 below in App. N. ", "page_idx": 30}, {"type": "text", "text": "G.2  Analysis of the loss function $L(\\theta)$ from Eq. (42) ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Now we establish the gradients of the loss function. ", "page_idx": 30}, {"type": "text", "text": "Lemma 13 (Gradient computation and optimal bias). For any $\\pmb{\\theta}=(e,w,a)\\in\\mathbb{R}^{3}$ and the next-token predictionloss $L(\\pmb\\theta)$ in Eq. (42), the gradients are given by ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial{\\cal L}}{\\partial e}=-\\mathbb{E}\\left[(f_{1}X+f_{2})\\left(X-\\frac{1}{2}\\right)\\right]\\cdot2e\\left(1+a e^{2}\\right)\\left(1+2w|w|\\right)}\\\\ {\\displaystyle\\qquad-\\mathbb{E}\\left[(f_{1}X+f_{2})\\left(X-\\frac{1}{2}\\right)\\right]\\cdot2e^{3}a\\left(1+2w|w|\\right),}\\\\ {\\displaystyle\\frac{\\partial{\\cal L}}{\\partial w}=-\\mathbb{E}\\left[(f_{1}X+f_{2})\\left(X-\\frac{1}{2}\\right)\\right]\\cdot2e^{2}\\left(1+a e^{2}\\right)\\left(|w|+\\mathrm{sign}\\left(w\\right)w\\right),}\\\\ {\\displaystyle\\frac{\\partial{\\cal L}}{\\partial a}=-\\mathbb{E}\\left[(f_{1}X+f_{2})\\left(X-\\frac{1}{2}\\right)\\right]\\cdot e^{4}\\left(1+2w|w|\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $X\\in\\{0,1\\}$ is a Bernoulli random variable with $X\\sim\\operatorname{Bern}(p/(p+q))$ ,and ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{1}\\triangleq1-p-q-\\phi_{1}+\\phi_{0},\\quad f_{2}\\triangleq p-\\phi_{0},}\\\\ &{\\phi_{1}\\triangleq\\sigma\\left(e^{2}\\left(\\cfrac{1}{2}\\left(1+a e^{2}\\right)\\left(1+2w|w|\\right)+w|w(1+a e^{2})|\\right)+b_{\\star}\\right),}\\\\ &{\\phi_{0}\\triangleq\\sigma\\left(e^{2}\\left(\\cfrac{-1}{2}\\left(1+a e^{2}\\right)\\left(1+2w|w|\\right)+w|w(1+a e^{2})|\\right)+b_{\\star}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the optimal bias $b_{\\star}$ is obtained by solving $\\pi_{1}f_{1}+f_{2}=0$ ", "page_idx": 30}, {"type": "text", "text": "Proof. We defer to App. N. ", "page_idx": 30}, {"type": "text", "text": "Theorem 9 (All critical points for linear attention in $\\mathbb{R}^{4}$ ).Let the input sequence be $\\{x_{n}\\}_{n=1}^{N}\\sim$ $(\\pi,P)$ ,the transformer parameters $\\pmb{\\theta}=(e,w,b,a)\\in\\mathbb{R}^{4}$ , and the next-token prediction loss $L(\\cdot)$ be as in Eq. (41). Then for any $(p,q)\\in(0,1)^{2}$ with $p+q\\ne1$ and $N\\in\\mathbb{N}$ ", "page_idx": 30}, {"type": "text", "text": "(i) the set of all global minima is given by ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbf{T}_{\\star}(p,q)\\triangleq\\{(e,w,b,a)\\in\\mathbb{R}^{4}:e^{2}w|w(1+a e^{2})|+b=\\frac{1}{2}\\log\\frac{p(1-q)}{q(1-p)},}}\\\\ {\\displaystyle e^{2}\\left(1+a e^{2}\\right)(1+2w|w|)=\\log\\frac{(1-q)(1-p)}{p q}\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "(i) a set of local minima is given by ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\Gamma_{\\operatorname*{min}}(p,q)\\triangleq\\left\\{\\gamma_{\\operatorname*{min}}=(e,w,b,a)\\in\\mathbb{R}^{4}:e=0,(p+q-1)(1+2w|w|)>0,b=\\log\\frac{p}{q}\\right\\},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "(i) a set of saddle points is ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Gamma_{\\mathrm{sad}}(p,q)\\triangleq\\left\\{\\gamma_{\\mathrm{sad}}=(e,w,b,a)\\in\\mathbb{R}^{4}:e=0,(p+q-1)(1+2w|w|)\\leq0,b=\\log\\frac{p}{q}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "(iv)a set of stationary points is ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\Gamma_{\\mathrm{station}}(p,q)\\triangleq\\left\\{\\mathcal{T}_{\\mathrm{station}}=(e,w,b,a)\\in\\mathbb{R}^{4}:e\\neq0,1+a e^{2}=0,1+2w|w|=0,b=\\log\\frac{p}{q}\\right\\},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Thus the set of all critical points is ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\left\\{\\pmb\\theta\\in\\mathbb{R}^{2}:\\nabla L(\\pmb\\theta)=0\\right\\}=\\mathbf{r}_{\\star}\\cup\\mathbf{r}_{\\mathrm{min}}\\cup\\mathbf{r}_{\\mathrm{sad}}\\cup\\mathbf{r}_{\\mathrm{station}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "In addition, for any $\\theta_{\\star}\\in\\Gamma_{\\star},\\theta_{\\operatorname*{min}}\\in\\Gamma_{\\operatorname*{min}}$ and $\\theta_{\\mathrm{sad}}\\in\\Gamma_{\\mathrm{sad}}$ the loss values satisfy ", "page_idx": 31}, {"type": "equation", "text": "$$\nH(x_{n+1}\\mid x_{n})=L(\\pmb\\theta_{\\star})<L(\\pmb\\theta_{\\operatorname*{min}})=L(\\pmb\\theta_{\\operatorname*{max}})=L(\\pmb\\theta_{\\mathrm{sad}})=H(x_{n+1}).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Theorem 10 (Allcritical points in $\\mathbb{R}^{3}$ ). Let the input sequence be $\\{x_{n}\\}_{n=1}^{N}\\sim(\\pi,P),$ the transformer parameters $\\pmb{\\theta}=(e,w,a\\bar{)}\\in\\mathbb{R}^{3}$ and the next-token prediction loss $L(\\cdot)$ be as in Eq. (42). Then for any $(p,q)\\in(0,1)^{2}$ with $p+q\\ne1$ and $N\\in\\mathbb{N}$ ", "page_idx": 31}, {"type": "text", "text": "(i) the set of all global minima is given by ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\Theta_{\\star}(p,q)\\triangleq\\{(e,w,a)\\in\\mathbb{R}^{3}:e^{2}\\left(1+a e^{2}\\right)(1+2w|w|)=\\log\\frac{(1-q)(1-p)}{p q}\\}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "(i) a set of local minima is given by ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Theta_{\\operatorname*{min}}(p,q)\\triangleq\\left\\{(e,w,a)\\in\\mathbb{R}^{3}:e=0,(p+q-1)(1+2w|w|)>0\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "(i) a set of local maxima is given by ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\Theta_{\\operatorname*{min}}(p,q)\\triangleq\\left\\{(e,w,a)\\in\\mathbb{R}^{3}:e=0,(p+q-1)(1+2w|w|)<0\\right\\},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "(iv) a set of saddle points is ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\Theta_{\\mathrm{sad}}(p,q)\\triangleq\\left\\{(e,w,a)\\in\\mathbb{R}^{3}:\\left(0,-1/\\sqrt{2},a\\right)\\right\\}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Defining a set of stationary po $i n t s\\,\\Theta_{\\mathrm{station}}(p,q)\\triangleq\\big\\{(e,w,a)\\in\\mathbb{R}^{3}:e\\neq0,1+a e^{2}=0,1+2w|w|=0\\big\\},$ the set of all critical points is ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\{\\pmb{\\theta}\\in\\mathbb{R}^{2}:\\nabla L(\\pmb{\\theta})=0\\}=\\pmb{\\Theta}_{\\star}\\cup\\pmb{\\Theta}_{\\mathrm{min}}\\cup\\pmb{\\Theta}_{\\mathrm{max}}\\cup\\pmb{\\Theta}_{\\mathrm{sad}}\\cup\\pmb{\\Theta}_{\\mathrm{station}}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "In addition, for any $\\theta_{\\star}\\in\\Theta_{\\star},\\theta_{\\mathrm{min}}\\in\\Theta_{\\mathrm{min}}$ and $\\pmb{\\theta}_{\\mathrm{sad}}\\in\\Theta_{\\mathrm{sad}}$ the loss values satisfy ", "page_idx": 31}, {"type": "equation", "text": "$$\nH(x_{n+1}\\mid x_{n})=L(\\pmb\\theta_{\\star})<L(\\pmb\\theta_{\\operatorname*{min}})=L(\\pmb\\theta_{\\operatorname*{max}})=L(\\pmb\\theta_{\\mathrm{sad}})=H(x_{n+1}).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Remark 5. While the remaining set of stationary points could be classified to global minima, local minima, etc., it's technically unclear what category the set of critical points $\\Theta_{\\mathrm{station}}(p,q)$ belongto, as the Hessian is undefined here. We would need to rely on local perturbation analysis to characterize these class of points. ", "page_idx": 31}, {"type": "text", "text": "We defer the proofs of the theorems to App. N.2. ", "page_idx": 31}, {"type": "text", "text": "G.3 Gradient flow analysis ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Analogous to the gradient flow analysis for $\\pmb{\\theta}=(e,w)\\in\\mathbb{R}^{2}$ in App. F, we now study its countepart toegether with the attention scalar, i.e. $\\pmb{\\theta}=(e,w,a)\\in\\mathbb{R}^{3}$ . To this end, let $(\\pmb\\theta_{t})_{t\\geq0}$ be a $C^{1}$ curve in $\\mathbb{R}^{3}$ governed by ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\pmb{\\theta}_{t}}{\\mathrm{d}t}=-\\nabla L(\\pmb{\\theta}_{t}),\\quad\\pmb{\\theta}_{t}=(e_{t},w_{t},a_{t})\\in\\mathbb{R}^{3},\\,t\\geq0,\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "starting with a randomly initalized $\\pmb{\\theta}_{0}$ . We define the energy function $\\mathcal{E}(\\cdot,\\cdot,\\cdot)$ as ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{E}(e,w,a)\\triangleq e^{2}-(w^{2}+\\mathrm{sign}(w)\\cdot\\log|w|)-2a^{2},\\quad\\forall(e,w,a)\\in\\mathbb{R}^{3}\\setminus\\mathrm{ea-plane},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where ea-plane $\\triangleq\\{(e,w=0,a)\\}$ . The following lemma presents the crucial result that the energy is constant along the flow in GF-Attention. ", "page_idx": 31}, {"type": "text", "text": "Lemma 14 (Constant energy along the flow). For any $(p,q)\\,\\in\\,(0,1)^{2}$ and initialization $\\pmb\\theta_{0}\\,=$ $(e_{0},w_{0},a_{0})\\,\\in\\,\\mathbb{R}^{3}$ ,let $(\\pmb\\theta_{t})_{t\\geq0}$ be the corresponding $G F$ -Attention trajectory starting from $\\theta_{\\mathrm{0}}$ .f $w_{0}\\neq0$ then the energy stays constant along the trajectory, i.e. ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}(\\pmb{\\theta}_{t})=e_{t}^{2}-(w_{t}^{2}+\\mathrm{sign}(w_{t})\\cdot\\log|w_{t}|)-2a_{t}^{2}=\\mathcal{E}(\\pmb{\\theta}_{0}),\\quad\\forall t\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "On the other hand, i $f w_{0}=0,\\,w_{t}=0\\}$ forall $t\\geq0$ Hence, $i f$ we initialize on ea-plane the trajectory always stays on the ea-plane. ", "page_idx": 32}, {"type": "text", "text": "Now we characterize the convergence of the gradient flow. ", "page_idx": 32}, {"type": "text", "text": "Lemma 15 (GF convergence). Let $(\\pmb\\theta_{t})_{t\\geq0}$ be a continuously diferentiable GF-Attention trajectory startingfrom $\\pmb{\\theta}_{0}$ .Then for all initializations $\\pmb{\\theta}_{0}\\in\\mathbb{R}^{3}$ ", "page_idx": 32}, {"type": "text", "text": "(i) $(\\pmb\\theta_{t})_{t\\geq0}$ is bounded, ", "page_idx": 32}, {"type": "text", "text": "(i) there exists a $\\pmb{\\theta}_{\\mathrm{lim}}\\in\\mathbb{R}^{3}$ such that $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}\\pmb{\\theta}_{t}=\\pmb{\\theta}_{\\operatorname*{lim}}}\\end{array}$ and ", "page_idx": 32}, {"type": "text", "text": "(ii) $\\begin{array}{r}{\\operatorname*{lim}_{t\\rightarrow\\infty}\\Vert\\nabla L(\\pmb{\\theta}_{t})\\Vert=\\Vert\\nabla L(\\pmb{\\theta}_{\\mathrm{lim}})\\Vert=0.}\\end{array}$ ", "page_idx": 32}, {"type": "text", "text": "Hence $\\theta_{\\mathrm{lim}}$ is a critical point of $L$ ", "page_idx": 32}, {"type": "text", "text": "The following result characterizes the energy of the limit point. ", "page_idx": 32}, {"type": "text", "text": "Lemma 16 (Energy at the limit point). Consider the same seting as in Lemma 15. If $\\pmb{\\theta}_{0}\\ \\in$ $\\mathbb{R}^{3}\\ \\backslash$ ea-plane, then ${\\mathcal E}(\\pmb{\\theta}_{\\mathrm{lim}})\\;=\\;\\bar{\\mathcal E}(\\pmb{\\theta}_{0})$ .Hence $\\theta_{\\mathrm{lim}}$ lies at the intersection of the contour line $\\mathcal{E}(e,\\boldsymbol{w})=\\mathcal{E}_{0}$ with the set of critical points of $L$ in $\\mathbb{R}^{3}$ ", "page_idx": 32}, {"type": "text", "text": "On the other hand, if $\\pmb\\theta_{0}\\in$ ea-plane, then $\\theta_{\\mathrm{lim}}\\in$ ea-plane. ", "page_idx": 32}, {"type": "text", "text": "We defer the proofs of the lemmas to App. N. ", "page_idx": 32}, {"type": "text", "text": "G.4Role of standard initialization ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "The following picture enhances the behavior of the GF dynamics near the origin. ", "page_idx": 32}, {"type": "image", "img_path": "OX4yll3X53/tmp/e53d93cf119233bfeb3c05fabb788a14b6196512d6a2bf5442cfacfbd63d16c3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "OX4yll3X53/tmp/7a965593c05236018c4e9594dbd7efd9ccffe0646c55caf9888ae6f29b2f61bf.jpg", "img_caption": [], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "Figure 5: Gradient flow dynamics in $\\mathbb{R}^{3}$ , near the origin, for the transformer parameters with attention scalar $a$ (Sec. 4.1). The local minima are repellors for $p+q<1$ , while attracting for $p+q>1$ ", "page_idx": 32}, {"type": "text", "text": "Based on the above theoretical results and empirical evidence, we conjecture the following theorem, whose informal proof we defer to App. N.5. ", "page_idx": 32}, {"type": "text", "text": "Theorem 11 ( [Informal] Role of standard initialization for $p+q>1)$ 0.If we use the standard initialization $\\theta_{0}\\sim\\mathcal{N}(0,\\sigma^{2}I_{3})$ with $\\sigma^{2}\\ll1/{\\sqrt{2}}$ for the $G F$ -Attention, $\\pmb\\theta_{\\mathrm{lim}}$ will be a local minimum with high probability. ", "page_idx": 32}, {"type": "text", "text": "H  Additional empirical results ", "text_level": 1, "page_idx": 33}, {"type": "image", "img_path": "OX4yll3X53/tmp/f7440619372820d9186f2234a4f3ed7eeff4bc076d24ac3837681dabdffd30b4.jpg", "img_caption": ["H.1 Gaussian initialization converges to low-rank ", "Figure 6: Evolution of parameters $W_{1}$ and $W_{V}$ across iterations, starting from a standard Gaussian initialization. At convergence, all the parameter matrices are approximately rank-one. "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "H.2 Low-rank initialization stays low-rank ", "page_idx": 33}, {"type": "image", "img_path": "OX4yll3X53/tmp/c705b336634a0673f52b3daa18dcd6e61149fe1d675a4d4f8b4522350e630360.jpg", "img_caption": ["Figure 7: Evolution of parameters $W_{1}$ and $W_{V}$ across iterations, starting from a rank-one initialization. The parameters maintain a rank-one structure across the entire training. "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "I  Model architecture and hyper-parameters ", "text_level": 1, "page_idx": 35}, {"type": "table", "img_path": "OX4yll3X53/tmp/d76bb18b6ed03bf2d2c7e573cd22da3626caf2bb9cb3dfdec23b1a6445adc9fd.jpg", "table_caption": ["Table 1: Parameters in the transformer architecture with their shape. "], "table_footnote": [], "page_idx": 35}, {"type": "text", "text": "Table 2: Settings and parameters for the transformer model used in the experiments. ", "page_idx": 35}, {"type": "table", "img_path": "OX4yll3X53/tmp/7fdb61c1dc2909442e843520107d06717e09512507f409112e6dc16d8f9a078c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 35}, {"type": "text", "text": "JProofs of theorems in App. D ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "J.1 Proof of Thm. 7 ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Proof. We characterize the set of global minima, local minima, and that of the saddle points individually. ", "page_idx": 36}, {"type": "text", "text": "(i) Set of all global minima. Let $\\gamma_{\\star}\\in\\mathbb{R}^{3}$ be arbitrary. From [23, Lemma 1], we have that $\\gamma_{\\star}$ is a global minimum for the loss $L(\\cdot)$ in Eq. (23) if and only if its prediction probability satisfies $f_{\\gamma_{\\star}}(x_{1}^{n})=\\mathbb{P}\\left(x_{n+1}=1\\mid x_{n}\\right)$ , the Markov kernel. Since the input $\\{x_{n}\\}_{n=1}^{N}\\stackrel{{}_{\\sim}}{\\sim}(\\pi(p,q)\\dot{,}P(p,q))$ \uff0c we have that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(x_{n+1}=1\\mid x_{n}\\right)=(1-x_{n})p+x_{n}(1-q)=(1-p-q)x_{n}+p.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "On the other hand, by definition, from Eq. (3), $\\begin{array}{r}{f_{\\gamma_{\\star}}(x_{1}^{n})=\\sigma\\left(e^{2}(1+2w|w|)\\,x_{n}+b-\\frac{e^{2}}{2}\\right)}\\end{array}$ where $\\boldsymbol{\\gamma}_{\\star}=(e,w,b)$ . Since $x_{n}\\in\\{0,1\\}$ , this can be further simplified to ", "page_idx": 36}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{f_{\\gamma_{\\star}}(x_{1}^{n})=\\sigma\\left(e^{2}(1+2w|w|)\\,x_{n}+b-{\\frac{e^{2}}{2}}\\right)}\\\\ &{\\qquad\\quad=x_{n}\\cdot\\sigma\\left(e^{2}(1+2w|w|)+b-{\\frac{e^{2}}{2}}\\right)+(1-x_{n})\\cdot\\sigma\\left(b-{\\frac{e^{2}}{2}}\\right)}\\\\ &{\\qquad=x_{n}\\left(\\sigma\\left(2e^{2}w|w|\\right)+b+{\\frac{e^{2}}{2}}\\right)-\\sigma\\left(b-{\\frac{e^{2}}{2}}\\right)\\right)+\\sigma\\left(b-{\\frac{e^{2}}{2}}\\right).}\\end{array}}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Since both $f_{\\gamma_{\\star}}(x_{1}^{n})$ and $\\mathbb{P}\\left(x_{n+1}=1\\mid x_{n}\\right)$ are linear functions of $x_{n}$ , equating them for all vallues of $x_{n}\\in\\{0,1\\}$ implies that the respective coeffecients in these functions in Eq. (50) and Eq. (51) are also equal, i.e. ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\sigma\\left(b-\\frac{e^{2}}{2}\\right)=p,}}\\\\ {{\\displaystyle\\sigma\\left(2e^{2}w|w|+b+\\frac{e^{2}}{2}\\right)-\\sigma\\left(b-\\frac{e^{2}}{2}\\right)=1-p-q,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "and hence ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sigma\\left(b-\\frac{e^{2}}{2}\\right)=p,\\quad\\sigma\\left(2e^{2}w|w|\\right)+b+\\frac{e^{2}}{2}\\right)=1-q.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Since $\\sigma(z)=y$ for $y\\in(0,1)$ implies $z=\\log{\\frac{y}{1-y}}$ , Eq. (52) can be rewritten as ", "page_idx": 36}, {"type": "equation", "text": "$$\nb-\\frac{e^{2}}{2}=\\log\\frac{p}{1-p},\\quad2e^{2}w|w|+b+\\frac{e^{2}}{2}=\\log\\frac{1-q}{q}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Using $\\begin{array}{r}{2e^{2}w|w|+b+\\frac{e^{2}}{2}\\,=\\,e^{2}(1+2w|w|)+b-\\frac{e^{2}}{2}\\,=\\,e^{2}(1+2w|w|)+\\log\\frac{p}{1-p}}\\end{array}$ in the second equality above, we obtain ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{b-\\displaystyle\\frac{e^{2}}{2}=\\log\\displaystyle\\frac{p}{1-p},}}\\\\ {{e^{2}(1+2w|w|)=\\log\\displaystyle\\frac{1-q}{q}+\\log\\displaystyle\\frac{1-p}{p}=\\log\\displaystyle\\frac{(1-p)(1-q)}{p q}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Thus $\\gamma_{\\star}\\in\\mathbb{R}^{3}$ is a global minimum for $L(\\cdot)$ if and only if it satisfies Eq. (53) (note that it's already a critical point, as established in Thm. 6). Thus, the set of all global minimum $\\Gamma_{\\star}(p,q)$ is given by ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\Gamma_{\\star}(p,q)\\triangleq\\left\\{\\gamma_{\\star}=(e,w,b)\\in\\mathbb{R}^{3}:e^{2}(1+2w|w|)=\\log\\frac{(1-p)(1-q)}{p q},\\,b-\\frac{e^{2}}{2}=\\log\\frac{p}{1-p}\\right\\}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Since the prediction $f_{\\gamma_{\\star}}(\\cdot)$ equals the Markov kernel for any $\\gamma_{\\star}\\in\\Gamma_{\\star}$ , it follows from Thm. 4 (or [23, Lemma 1]) that $L({\\dot{\\gamma}}_{\\star})={\\dot{H}}(x_{n+1}\\mid x_{n})$ , the entropy rate of the Markov chain. ", "page_idx": 36}, {"type": "text", "text": "(i) Set of local minima and saddle points. ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Define $\\Gamma_{\\mathrm{min}}(p,q)\\subseteq\\mathbb{R}^{3}$ and $\\Gamma_{\\mathrm{sad}}\\subseteq\\mathbb{R}^{3}$ as follows: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{T}_{\\operatorname*{min}}(p,q)\\triangleq\\left\\{\\gamma_{\\operatorname*{min}}=(e,w,b)\\in\\mathbb{R}^{3}:e=0,(p+q-1)(1+2w|w|)>0,b=\\log\\frac{p}{q}\\right\\},}\\\\ &{\\mathbf{T}_{\\operatorname*{sad}}(p,q)\\triangleq\\left\\{\\gamma_{\\mathrm{sad}}=(e,w,b)\\in\\mathbb{R}^{3}:e=0,(p+q-1)(1+2w|w|)\\leq0,b=\\log\\frac{p}{q}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "To show that $\\Gamma_{\\mathrm{min}}$ is the set of all bad local minima for $L(\\cdot)$ : we first show that any $\\gamma_{\\mathrm{min}}\\in\\Gamma_{\\mathrm{min}}$ is a bad local minimum and then show that every bad local minimum should belong to $\\Gamma_{\\mathrm{min}}$ . Similarly for $\\mathbf{T}_{\\mathrm{sad}}$ . We start with the local minima. ", "page_idx": 37}, {"type": "text", "text": "Let $\\gamma_{\\mathrm{min}}=(e,w,b)\\in\\Gamma_{\\mathrm{min}}$ Recall that $\\gamma_{\\mathrm{min}}$ is a stationary point (Thm. 6),i.e. ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\nabla L(\\gamma_{\\mathrm{min}})=0.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Rearragning the order of scalars and writing $\\gamma_{\\mathrm{min}}=(b,e,w)$ , from Lemma 4, the Hessian of the loss at $\\gamma_{\\mathrm{min}}$ is ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\nabla^{2}L(\\gamma_{\\mathrm{min}})=\\pi_{0}\\pi_{1}\\left[\\!\\!\\begin{array}{c c c}{{1}}&{{0}}&{{0}}\\\\ {{0}}&{{2(p+q-1)(1+2w|w|)}}&{{0}}\\\\ {{0}}&{{0}}&{{0}}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "By definition, $\\gamma_{\\mathrm{min}}=(b,e,w)$ satisfies $(p+q-1)(1+2w|w|)>0$ . Thus its Hessian in Eq. (54) has a block diagonal structure of the form $\\left[\\begin{array}{l l}{H_{b,e}}&{0}\\\\ {0}&{0}\\end{array}\\right]$ where $\\boldsymbol{H_{b,e}}$ has both the eigen values positive, and hence positive-definite. In other words, $\\gamma_{\\mathrm{min}}$ is a local minimum for $L(\\cdot)$ in the $(b,e)\\in\\mathbb{R}^{2}$ space for any fixed $w$ in the set. Interestingly, using the continuity argument and the fact that $\\begin{array}{r}{\\bar{L}(b=\\log\\frac{p}{q};e=0,w)}\\end{array}$ is constant in $w\\in\\mathbb{R}$ , we can essentially follow the same steps as in proo of Theorem 2 in [23, Appendix B.3] (Thm. 5 above) to show that $\\gamma_{\\mathrm{min}}=(b,e,w)$ is a also local minimum for $L(\\cdot)$ in the full parameter space $\\mathbb{R}^{3}$ . This establishes that $\\gamma_{\\mathrm{min}}$ is a local minimum for $L(\\cdot)$ ", "page_idx": 37}, {"type": "text", "text": "For the saddle points, let $\\gamma_{\\mathrm{sad}}=(e,w,b)\\in\\Gamma_{\\mathrm{sad}}$ We have that $\\gamma_{\\mathrm{sad}}$ is a stationary point (Thm. 6) and Lemma 4 implies its Hessian (after rearraging the order of scalars as above with $\\gamma_{\\mathrm{sad}}=(b,e,w))$ is: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\nabla^{2}L(\\gamma_{\\mathrm{sad}})=\\pi_{0}\\pi_{1}\\left[\\!\\!\\begin{array}{c c c}{{1}}&{{0}}&{{0}}\\\\ {{0}}&{{2(p+q-1)(1+2w|w|)}}&{{0}}\\\\ {{0}}&{{0}}&{{0}}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "$w\\neq-\\frac{1}{\\sqrt{2}}$ \uff0c $(p+q-1)(1+2w|w|)<0$ for any $\\gamma_{\\mathrm{sad}}\\in\\Gamma_{\\mathrm{sad}}$ , and hence the Hessian $\\nabla^{2}L(\\gamma_{\\mathrm{sad}})$ in Eq. 55)as both sitive,egative, and zero eigenvalu.h $\\gamma_{\\mathrm{sad}}$ is a saddle point for $L(\\cdot)$ Using a neighborhood argument, we can similarly argue for w = to establish that it's also a saddle point. Now we compute the loss value. ", "page_idx": 37}, {"type": "text", "text": "For any $\\gamma_{\\mathrm{min}}=(e,w,b)\\in\\Gamma_{\\mathrm{min}}$ $\\gamma_{\\mathrm{sad}}=(e,w,b)\\in\\Gamma_{\\mathrm{sad}}$ we have that $e=0$ and $\\begin{array}{r}{b=\\log\\frac{p}{q}}\\end{array}$ Thus for $\\gamma=\\gamma_{\\mathrm{min}}$ 0 $\\mathbf{T}_{\\mathrm{sad}}$ , the prediction probability in view of Eq. (3) is ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\gamma}\\!\\left(x_{n+1}=1\\;|\\;x_{1}^{n}\\right)=\\sigma\\left(e^{2}(1+2w|w|)\\,x_{n}+b-\\frac{e^{2}}{2}\\right)=\\sigma(b)=\\frac{p}{p+q}=\\mathbb{P}\\left(x_{n+1}=1\\right),\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "the marginal distribution. Substituting this equality in the definition of cross-entropy loss $L(\\cdot)$ in Eq. (1) and the fact that $\\begin{array}{r}{\\mathbb{P}\\left(x_{n+1}=1\\right)=\\frac{p}{p+q}\\overset{\\cdot}{=}\\pi_{1}^{\\cdot}}\\end{array}$ , following the same steps as in [23, Appendix B.3], we obtain ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal L}(\\gamma)=-\\,\\frac{1}{N}\\sum_{n\\in[N]}\\mathbb{E}_{x_{1}^{n+1}}[x_{n+1}\\cdot\\log f_{\\gamma}(x_{1}^{n})+(1-x_{n+1})\\cdot\\log(1-f_{\\gamma}(x_{1}^{n}))]}}\\\\ {{\\displaystyle=-\\frac{1}{N}\\sum_{n\\in[N]}\\mathbb{E}_{x_{1}^{n+1}}\\left[x_{n+1}\\cdot\\log\\pi_{1}+(1-x_{n+1})\\cdot\\log\\pi_{0}\\right]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{}}&{=\\frac{1}{N}\\sum_{n\\in[N]}\\left[-\\pi_{1}\\log\\pi_{1}-\\pi_{0}\\log\\pi_{0}\\right]}\\\\ &{=H(\\pi)=H(x_{n+1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Thus $L(\\gamma_{\\mathrm{min}})\\:=\\:L(\\gamma_{\\mathrm{sad}})\\:=\\:H(x_{n+1})$ . To see that $H(x_{n+1}\\;\\;|\\;\\;x_{n})\\;=\\;L(\\gamma_{\\star})\\;<\\;L(\\gamma_{\\mathrm{min}})\\;=$ $L(\\gamma_{\\mathrm{sad}})=H(x_{n+1})$ for any global minimum $\\gamma_{\\star}$ , observe that the gap ", "page_idx": 38}, {"type": "equation", "text": "$$\nL(\\gamma_{\\mathrm{min}})-L_{\\star}=H(x_{n+1})-H(x_{n+1}|x_{n})=I(x_{n};x_{n+1})\\geq0,\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $I(x_{n};x_{n+1})$ is the mutual information between $x_{n}$ and $x_{n+1}$ [11]. Hence the optimality gap equals zero if and only if the mutual information equals zero, which happens when $x_{n}$ and $x_{n+1}$ are independent, i.e. $\\mathbb{P}\\left(x_{n+1}=1\\mid x_{n}\\right)$ doesn't depend on $x_{n}$ . Since $\\mathbb{P}\\left(x_{n+1}=1\\mid x_{n}\\right)=$ $(1-p-q)x_{n}+p$ from Eq. (50), this happens only when $p+q=1$ which contradicts the theorem assumption that $p+q\\ne1$ . Hence $H(x_{n+1}\\stackrel{\\cdot\\cdot}{\\vert}x_{n})\\stackrel{\\cdot\\cdot}{=}L(\\gamma_{\\star})<\\bar{L}(\\gamma_{\\mathrm{min}})=L(\\gamma_{\\mathrm{sad}})=H(x_{n+1}).$ ", "page_idx": 38}, {"type": "text", "text": "Now we finally show that $\\mathbf{T}_{\\mathrm{min}}$ and $\\mathbf{T}_{\\mathrm{sad}}$ are the only set of bad local minima and saddle points respectively. Let $\\gamma$ is a bad local minimum for $L(\\cdot)$ . By definition, it's also a critical point. Recall from Thm. 6 that any stationary point $\\gamma=(e,w,b)$ for the loss $L(\\cdot)$ satisfies that either $\\gamma\\in\\Gamma_{\\star}$ $\\gamma\\in\\Gamma_{\\operatorname*{min}}$ or $\\gamma\\in\\Gamma_{\\mathrm{{sad}}}$ . Clearly $\\gamma\\not\\in\\Gamma_{\\star}$ , as $\\Gamma_{\\star}$ is the set of all global minima. Similarly, $\\gamma\\notin\\mathbf{T}_{\\mathrm{sad}}$ as every point in $\\mathbf{T}_{\\mathrm{sad}}$ is a saddle point for the loss $L(\\cdot)$ as established above. Hence $\\gamma\\in\\Gamma_{\\operatorname*{min}}$ . Thus every bad local minimum in $\\mathbb{R}^{3}$ belongs to $\\Gamma_{\\mathrm{min}}$ . This coupled with the fact above that $\\Gamma_{\\mathrm{min}}$ is a set of bad local minima implies $\\Gamma_{\\mathrm{min}}$ is indeed the set of all bad local minima. The proof for $\\mathbf{T}_{\\mathrm{sad}}$ is similar. ", "page_idx": 38}, {"type": "text", "text": "J.2 Proof of Thm. 6 ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Proof. Let $\\gamma=(e,w,b)\\in\\mathbb{R}^{3}$ be such that $\\begin{array}{r}{\\nabla L(\\gamma)=\\left(\\frac{\\partial L}{\\partial e},\\frac{\\partial L}{\\partial w},\\frac{\\partial L}{\\partial b}\\right)^{\\top}=0,}\\end{array}$ By Lemma 3, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial L}{\\partial e}=\\mathbb{E}_{X}\\left[(f_{1}X+f_{2})(2X(1+2w|w|)-1))\\right]\\cdot e=0,}\\\\ &{\\displaystyle\\frac{\\partial L}{\\partial w}=\\mathbb{E}_{X}\\left[(f_{1}X+f_{2})X\\right]\\cdot4e^{2}|w|=0,}\\\\ &{\\displaystyle\\frac{\\partial L}{\\partial b}=\\mathbb{E}_{X}\\left[f_{1}X+f_{2}\\right]=0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where $\\begin{array}{r}{\\zeta\\sim\\mathrm{Bern}(p/(p+q)),\\,f_{1}=\\sigma\\left(2e^{2}w|w|+b+\\frac{e^{2}}{2}\\right)+q-1-\\sigma\\left(b-\\frac{e^{2}}{2}\\right)+p\\,,}\\end{array}$ and $f_{2}=$ $\\sigma\\left(b-{\\frac{e^{2}}{2}}\\right)-p$ Our goalis to now show that Eqs.(56)-(58) hold only ifeither $\\begin{array}{r}{(e=0,b=\\log\\frac{p}{q}).}\\end{array}$ or $(f_{1}=0,f_{2}=0)$ . We consider two cases corresponding to $e=0$ and $e\\neq0$ ", "page_idx": 38}, {"type": "text", "text": "$e=0$ $e=0$ $\\begin{array}{r}{\\frac{\\partial L}{\\partial e}=\\frac{\\partial L}{\\partial e}=0}\\end{array}$ Furher, $f_{1}=p+q-1$ $f_{2}=\\sigma(b)-p$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{\\ell}_{X}\\left[f_{1}X+f_{2}\\right]=(p+q-1)\\mathbb{E}[X]+\\sigma(b)-p=(p+q-1)\\frac{p}{p+q}+\\sigma(b)-p=\\sigma(b)-\\frac{p}{p+q}=0\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "which implies that $b=\\log\\frac{p}{q}$ . Since $w\\in\\mathbb{R}$ is arbitrary, we see in this case that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\gamma\\in\\left\\{(e,w,b)\\in\\mathbb{R}^{3}:e=0,(p+q-1)(1+2w|w|)>0,b=\\log\\frac{p}{q}\\right\\}\\bigcup}\\\\ &{\\qquad\\left\\{(e,w,b)\\in\\mathbb{R}^{3}:e=0,(p+q-1)(1+2w|w|)<0,b=\\log\\frac{p}{q}\\right\\},}\\\\ &{=\\Gamma_{\\operatorname*{min}}\\cup\\Gamma_{\\mathrm{sad}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "(ii): $e\\neq0$ : Suppose $e\\neq0$ . Here we show that $f_{1}=f_{2}=0$ and hence $\\gamma\\in\\Gamma_{\\star}$ . We consider two cases corresponding to $w\\ne0$ and $w=0$ Let $w\\ne0$ . Since both $e\\neq0$ and $w\\ne0$ , and $X=X^{2}$ in Eq. (57) with $\\begin{array}{r}{\\mathbb{E}[X]=\\pi_{1}=\\frac{p}{p+q}>0}\\end{array}$ we obtain that ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X}[f_{1}X^{2}+f_{2}X]=(f_{1}+f_{2})\\mathbb{E}[X]=(f_{1}+f_{2})\\pi_{1}=0,\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "and hence $f_{1}+\\,f_{2}\\,=\\,0$ .Further Eq. (58) implies that $f_{1}\\pi_{1}+f_{2}\\,=\\,0$ . Together, this implies $f_{1}=f_{2}=0$ ", "page_idx": 39}, {"type": "text", "text": "Now suppose $w=0$ . From Eq. (58), we have that $\\mathbb{E}[f_{1}X+f_{2}]=f_{1}\\pi_{1}+f_{2}=0$ . Since $e\\neq0$ Eq. (56) yields ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbb{E}_{X}\\left[(f_{1}X+f_{2})(2X-1))\\right]=2\\mathbb{E}_{X}\\left[(f_{1}X+f_{2})X\\right]=2\\pi_{1}(f_{1}+f_{2})=0.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "So $f_{1}=f_{2}=0$ in this case too. Thus we have showed that whenever $e\\neq0$ , we have $f_{1}=f_{2}=0$ Recalling the expressions for $f_{1}$ and $f_{2}$ \uff0c ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\dot{\\bar{\\mathbf{\\Gamma}}}}_{2}=\\sigma\\left(b-\\frac{e^{2}}{2}\\right)-p=0\\Rightarrow b-\\frac{e^{2}}{2}=\\log\\frac{p}{1-p},}}\\\\ {{\\displaystyle{\\dot{\\mathbf{\\Gamma}}}_{1}=\\sigma\\left(2e^{2}w|w|+b+\\frac{e^{2}}{2}\\right)+q-1-\\sigma\\left(b-\\frac{e^{2}}{2}\\right)+p=\\sigma\\left(2e^{2}w|w|+b+\\frac{e^{2}}{2}\\right)+q-1=0,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and hence, ", "page_idx": 39}, {"type": "equation", "text": "$$\n2e^{2}w|w|+b+\\frac{e^{2}}{2}=\\log\\frac{1-q}{q}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Substuting $\\begin{array}{r}{b-{\\frac{e^{2}}{2}}=\\log{\\frac{p}{1-p}}}\\end{array}$ in the above equation, ", "page_idx": 39}, {"type": "equation", "text": "$$\n2e^{2}w|w|+e^{2}+b-\\frac{e^{2}}{2}=2e^{2}w|w|+e^{2}+\\log\\frac{p}{1-p}=\\log\\frac{1-q}{q},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and thus, ", "page_idx": 39}, {"type": "equation", "text": "$$\ne^{2}(1+2w|w|)=\\log\\frac{(1-p)(1-q)}{p q}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "In view of Eq. (59) and Eq. (60), we have that $\\gamma=(e,w,b)\\in\\mathbf{\\Gamma}\\Gamma_{\\star}$ ", "page_idx": 39}, {"type": "text", "text": "Together, we have shown that whenever $\\nabla L(\\gamma)\\,=\\,0$ ,wehave $\\gamma\\,\\in\\,\\Gamma_{\\star}\\cup\\Gamma_{\\mathrm{min}}\\cup\\Gamma_{\\mathrm{sad}}$ .Since $\\Gamma_{\\star}\\overset{\\cdot}{\\cup}\\Gamma_{\\mathrm{min}}\\cup\\Gamma_{\\mathrm{sad}}\\subseteq\\{\\gamma:\\nabla L(\\gamma)=0\\}$ wearedone. ", "page_idx": 39}, {"type": "text", "text": "K   Proofs of technical lemmas in App. D ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "K.1 Proof of Lemma 2 ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Proof. Recall from Eq. (14) that the cross-entropy loss $L(\\cdot)$ is defined as ", "page_idx": 39}, {"type": "equation", "text": "$$\nL(\\gamma)=-\\frac{1}{N}\\sum_{n\\in[N]}\\mathbb{E}_{x_{1}^{n+1}}[x_{n+1}\\cdot\\log f_{\\gamma}(x_{1}^{n})\\,+(1-x_{n+1})\\cdot\\log(1-f_{\\gamma}(x_{1}^{n}))],\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where $\\begin{array}{r}{f_{\\gamma}(x_{1}^{n})~=~\\sigma(\\mathrm{logit}_{n})~=~\\sigma\\left(e^{2}(1+2w|w|)\\,x_{n}+b-\\frac{e^{2}}{2}\\right)}\\end{array}$ from Eq. (3).For any $Y\\in$ $\\{0,1\\},Z\\in\\mathbb{R}$ , using the fact that $1-\\sigma(Z)=\\sigma(-Z)$ ,we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\left[Y\\log\\sigma(Z)+(1-Y)\\log(1-\\sigma(Z))\\right]=-\\left[Y\\log\\sigma(Z)+(1-Y)\\log\\sigma(-Z)\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\overset{(a)}{=}Y\\log(1+\\exp(-Z))+(1-Y)\\log(1+\\exp(Z))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\log\\left(1+\\exp(-(2Y-1)Z)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\overset{(b)}{=}\\ell_{\\log}\\left((2Y-1)Z\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where $(a)$ and $(b)$ follow from the definitions of sigmoid and the logistic functions: $\\sigma(z)\\;=\\;$ I+exp(-)\uff0clog(2) = log(1 + exp(-)\uff09) for z \u2208 R. Substituting Y= En+1 E {0,1} and $Z=\\mathrm{logit}_{n}\\in\\mathbb{R}$ in Eq. (62), Eq. (61) simplifies to ", "page_idx": 39}, {"type": "equation", "text": "$$\nL(\\gamma)=\\frac{1}{N}\\sum_{n\\in[N]}\\mathbb{E}[\\ell_{\\mathrm{log}}\\left((2x_{n+1}-1)\\cdot\\mathrm{logit}_{n}\\right)].\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Since $\\mathrm{logit}_{n}$ is only a function of $x_{n}$ , the above expectation is over the distribution of the pairs $(x_{n},x_{n+1})$ , which for all $n\\,\\in\\,[N]$ have the same law as a pair of random variables $(X,Y)$ with $\\Tilde{X}\\sim\\pi\\equiv\\mathrm{Bern}(p/(p+q))$ and $\\dot{Y(x}\\sim P(p,q)$ , the Markov kernel. Hence the above equality can be rewritten using the definition of $\\mathrm{logit}_{n}$ as ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\therefore(\\gamma)=\\frac{1}{N}\\sum_{n\\in[N]}\\mathbb{E}\\big[\\ell_{\\log}\\left((2x_{n+1}-1)\\cdot\\log\\mathrm{it}_{n}\\right)\\big]=\\mathbb{E}_{X,Y}\\left[\\ell_{\\log}\\left((2Y-1)\\left(e^{2}(1+2w|w|)X+b-\\frac{e^{2}}{2}\\right)\\right)\\right],\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "K.2 Proof of Lemma 3 ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Proof. With $\\gamma=(e,w,b)$ and $\\theta$ denoting either of the scalars $e,w$ ,or $b$ , we have from [23, Lemma 2] that the gradient of the loss $L(\\cdot)$ is given by ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}L(\\gamma)=-\\frac{1}{N}\\sum_{n\\in[N]}\\mathbb{E}_{x_{1}^{n+1}}\\left[(x_{n+1}-f_{\\theta}(x_{1}^{n}))\\cdot\\nabla_{\\theta}\\log\\mathrm{it}_{n}\\right],\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Where $\\begin{array}{r}{f_{\\gamma}(x_{1}^{n})=\\sigma(\\mathrm{logit}_{n})=\\sigma\\left(e^{2}(1+2w|w|)\\,x_{n}+b-\\frac{e^{2}}{2}\\right)}\\end{array}$ . Using the same argument as in the proof of Lemma 6, we can replace the expecatations in Eq. (63) with that of a pair of random variables $(X,Y)$ With $X\\sim\\pi\\equiv\\mathrm{Bern}(p/(p+q))$ and $Y|X\\sim P(p,q)$ , the Markov kernel. That is, ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\7_{\\theta}L(\\gamma)=-\\mathbb{E}_{X,Y}\\left[\\left(Y-\\sigma\\left(e^{2}(1+2w|w|)X+b-\\frac{e^{2}}{2}\\right)\\right)\\cdot\\nabla_{\\theta}\\left(e^{2}(1+2w|w|)X+b-\\frac{e^{2}}{2}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Now we define the error term $\\begin{array}{r}{\\mathcal{E}(X,Y)\\triangleq-\\left(Y-\\sigma\\left(e^{2}(1+2w|w|)X+b-\\frac{e^{2}}{2}\\right)\\right)}\\end{array}$ . Our goal is to show that $\\mathbb{E}[\\mathcal{E}(X,Y)\\mid X]=f_{1}X\\!+\\!f_{2}$ where $\\begin{array}{r}{f_{1}\\triangleq\\sigma\\left(2e^{2}w|w|+b+\\frac{e^{2}}{2}\\right)\\!+\\!q\\!-\\!1\\!-\\!\\sigma\\left(b-\\frac{e^{2}}{2}\\right)\\!+\\!p,}\\end{array}$ and $\\begin{array}{r}{f_{2}\\,\\triangleq\\,\\sigma\\left(b-\\frac{e^{2}}{2}\\right)\\,-\\,p}\\end{array}$ which ss t prve tmaT this en uin efat t $X\\in\\{0,1\\}$ , we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}(X,Y)=-\\left(Y-\\sigma\\left(e^{2}(1+2w|w|)X+b-\\frac{e^{2}}{2}\\right)\\right)}\\\\ &{\\quad\\quad=-\\left(Y-X\\cdot\\sigma\\left(2e^{2}w|w|+b+\\frac{e^{2}}{2}\\right)-(1-X)\\cdot\\sigma\\left(b-\\frac{e^{2}}{2}\\right)\\right)}\\\\ &{\\quad\\quad=-Y+X\\left(\\sigma\\left(2e^{2}w|w|+b+\\frac{e^{2}}{2}\\right)-\\sigma\\left(b-\\frac{e^{2}}{2}\\right)\\right)+\\sigma\\left(b-\\frac{e^{2}}{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Now taking the conditional expectation with respect to $X$ and using the fact that $\\mathbb{E}[Y|X]\\;=$ $\\mathbb{P}\\left(Y=1\\mid{\\overline{{X}}}\\right)=(1-p-q)X{\\overline{{+}}}\\,p$ (since $Y|X\\sim P(p,q))$ ,we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Im[\\mathcal{E}(X,Y)\\mid X]=-(1-p-q)X-p+X\\left(\\sigma\\left(2e^{2}w|w|+b+\\frac{e^{2}}{2}\\right)-\\sigma\\left(b-\\frac{e^{2}}{2}\\right)\\right)+\\sigma\\left(b-\\frac{e^{2}}{2}\\right)}\\\\ &{\\qquad\\qquad=X\\left(\\sigma\\left(2e^{2}w|w|+b+\\frac{e^{2}}{2}\\right)+q-1-\\sigma\\left(b-\\frac{e^{2}}{2}\\right)+p\\right)+\\sigma\\left(b-\\frac{e^{2}}{2}\\right)-p}\\\\ &{\\qquad\\qquad\\overset{(a)}{=}f_{1}X+f_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where $(a)$ follows from the definition of $f_{1}$ and $f_{2}$ above. Thus Eq. (64) simplifies to ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}L(\\gamma)=\\mathbb{E}_{X}\\left[(f_{1}X+f_{2})\\cdot\\nabla_{\\theta}\\ \\left(e^{2}(1+2w|w|)X+b-\\frac{e^{2}}{2}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Letting $\\theta=e,w$ ,and $b$ in the above equation, we finally obtain the individual gradients: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\frac{\\partial L}{\\partial e}=\\mathbb{E}_{X}\\left[(f_{1}X+f_{2})(2X(1+2w|w|-1))\\right]\\cdot e,\n$$", "text_format": "latex", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial L}{\\partial w}=\\mathbb{E}_{X}\\left[(f_{1}X+f_{2})X\\right]\\cdot4e^{2}|w|,}\\\\ {\\displaystyle\\frac{\\partial L}{\\partial b}=\\mathbb{E}_{X}\\left[f_{1}X+f_{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "K.3Proof of Lemma 4 ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Proof. Slightly changing the variable order, for any $\\gamma=(b,e,w)\\in\\mathbb{R}^{3}$ wedefine ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r}{H(\\gamma)\\triangleq\\nabla^{2}L(\\gamma)=\\left[\\begin{array}{l l l}{\\frac{\\partial^{2}L}{\\partial b^{2}}}&{\\frac{\\partial^{2}L}{\\partial b\\partial e}}&{\\frac{\\partial^{2}L}{\\partial b\\partial w}}\\\\ {\\frac{\\partial^{2}L}{\\partial e\\partial b}}&{\\frac{\\partial^{2}L}{\\partial e^{2}}}&{\\frac{\\partial^{2}L}{\\partial e\\partial w}}\\\\ {\\frac{\\partial^{2}L}{\\partial w\\partial b}}&{\\frac{\\partial^{2}L}{\\partial w\\partial e}}&{\\frac{\\partial^{2}L}{\\partial w^{2}}}\\end{array}\\right]\\in\\mathbb{R}^{3\\times3}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Recall that for any $\\gamma_{\\mathrm{min}}\\in\\Gamma_{\\mathrm{min}}$ and $\\gamma_{\\mathrm{sad}}\\in\\Gamma_{\\mathrm{sad}}$ we have $e=0$ and $\\begin{array}{r}{b=\\log\\frac{p}{q}}\\end{array}$ Now we compute the second deriatives of $L$ with respect to any $\\textstyle\\gamma=(b=\\log{\\frac{p}{q}},e=0,w)$ . We stat with the irst derivatives. By Lemma 7, the gradients are ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial{\\cal L}}{\\partial b}=\\mathbb{E}_{X}\\left[f_{1}X+f_{2}\\right],}\\\\ {\\displaystyle\\frac{\\partial{\\cal L}}{\\partial e}=\\mathbb{E}_{X}\\left[(f_{1}X+f_{2})(2X(1+2w|w|)-1))\\right]\\cdot e,}\\\\ {\\displaystyle\\frac{\\partial{\\cal L}}{\\partial w}=\\mathbb{E}_{X}\\left[(f_{1}X+f_{2})X\\right]\\cdot4e^{2}|w|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{l}{f_{1}\\triangleq\\sigma\\left(2e^{2}w|w|+b+\\displaystyle\\frac{e^{2}}{2}\\right)+q-1-\\sigma\\left(b-\\displaystyle\\frac{e^{2}}{2}\\right)+p,}\\\\ {f_{2}\\triangleq\\sigma\\left(b-\\displaystyle\\frac{e^{2}}{2}\\right)-p.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "From Eq. (66), we see that the second derivaties of $L$ depend on the first-derivatives of $f_{1}$ and $f_{2}$ , which we now compute. Recall that the derivative of the sigmoid function obeys $\\sigma^{\\prime}(z)\\,=$ $\\sigma(z)(1-\\sigma(z))=\\sigma(z)\\bar{\\sigma}(-z)$ for any $z\\in\\mathbb{R}$ . Now the gradients of $f_{1}$ and $f_{2}$ withrespectto $b,e$ and $w$ are ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial f_{1}}{\\partial b}=\\sigma\\left(2\\epsilon^{2}w|w|+b+\\frac{\\epsilon^{2}}{2}\\right)\\sigma\\left(-2\\epsilon^{2}w|w|-b-\\frac{\\epsilon^{2}}{2}\\right)-\\sigma\\left(b-\\frac{\\epsilon^{2}}{2}\\right)\\sigma\\left(-b+\\frac{\\epsilon^{2}}{2}\\right),}\\\\ {\\displaystyle\\frac{\\partial f_{2}}{\\partial b}=\\sigma\\left(b-\\frac{\\epsilon^{2}}{2}\\right)\\sigma\\left(-b+\\frac{\\epsilon^{2}}{2}\\right),}\\\\ {\\displaystyle\\frac{\\partial f_{1}}{\\partial e}=\\left(4\\epsilon w|w|+\\epsilon\\right)\\sigma\\left(2\\epsilon^{2}w|w|+b+\\frac{\\epsilon^{2}}{2}\\right)\\sigma\\left(-2\\epsilon^{2}w|w|-b-\\frac{\\epsilon^{2}}{2}\\right)+\\epsilon\\sigma\\left(b-\\frac{\\epsilon^{2}}{2}\\right)\\sigma\\left(-b+\\frac{\\epsilon^{2}}{2}\\right),}\\\\ {\\displaystyle\\frac{\\partial f_{2}}{\\partial e}=(-\\epsilon)\\sigma\\left(b-\\frac{\\epsilon^{2}}{2}\\right)\\sigma\\left(-b+\\frac{\\epsilon^{2}}{2}\\right),}\\\\ {\\displaystyle\\frac{\\partial f_{1}}{\\partial w}=(4\\epsilon^{2}\\cdot w\\mathrm{sign}(w))\\sigma\\left(2\\epsilon^{2}w|w|+b+\\frac{\\epsilon^{2}}{2}\\right)\\sigma\\left(-2\\epsilon^{2}w|w|-b-\\frac{\\epsilon^{2}}{2}\\right),}\\\\ {\\displaystyle\\frac{\\partial f_{2}}{\\partial w}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Using the fact that $\\begin{array}{r}{\\sigma\\left(\\log\\frac{p}{q}\\right)\\,=\\,\\frac{p}{p+q}\\,=\\,\\pi_{1}}\\end{array}$ and $\\begin{array}{r}{\\sigma\\left(-\\log\\frac{p}{q}\\right)\\,=\\,\\frac{q}{p+q}\\,=\\,\\pi_{0}}\\end{array}$ , the above gradients evaluated for any $\\textstyle\\pmb{\\gamma}=(b=\\log\\frac{p}{q},e=0,w)$ further reduce to ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\frac{\\partial f_{1}}{\\partial b}\\bigg\\rvert_{\\gamma}=0,~~~\\displaystyle\\frac{\\partial f_{2}}{\\partial b}\\bigg\\rvert_{\\gamma}=\\pi_{0}\\pi_{1},}\\\\ {\\displaystyle\\left.\\frac{\\partial f_{1}}{\\partial e}\\right\\rvert_{\\gamma}=0,~~~\\displaystyle\\frac{\\partial f_{2}}{\\partial e}\\bigg\\rvert_{\\gamma}=0,}\\\\ {\\displaystyle\\frac{\\partial f_{1}}{\\partial w}\\bigg\\rvert_{\\gamma}=0,~~~\\displaystyle\\frac{\\partial f_{2}}{\\partial w}\\bigg\\rvert_{\\gamma}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Now substituting Eq. (67) when computing the second-derivatives of $L$ in Eq. (66), we obtain ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{\\partial^{2}\\hat{L}}{\\partial\\beta^{2}}\\Bigg|_{\\infty}=\\mathbb{E}_{\\mathcal{X}}\\Bigg[\\frac{\\partial\\hat{L}}{\\partial\\beta^{2}}\\Bigg|_{\\infty}\\times+\\frac{\\partial\\hat{L}}{\\partial\\beta^{2}}\\Bigg|_{\\infty}\\Bigg]=\\pi_{0}\\nu_{1}\\;,}&{}\\\\ {\\frac{\\partial\\hat{L}}{\\partial\\beta^{2}}\\Bigg|_{\\infty}=\\mathbb{E}_{\\mathcal{X}}\\Bigg[\\frac{\\partial\\hat{L}}{\\partial\\beta^{2}}\\Bigg|_{\\infty}\\times+\\frac{\\partial\\hat{L}}{\\partial\\beta^{2}}\\Bigg|_{\\infty}\\Bigg]=0\\;,}&{}\\\\ {\\frac{\\partial\\hat{L}}{\\partial\\beta^{2}}\\Bigg|_{\\infty}=\\mathbb{E}_{\\mathcal{X}}\\Bigg[\\frac{\\partial\\hat{L}}{\\partial\\beta^{2}}\\Bigg|_{\\infty}\\times+\\frac{\\partial\\hat{L}}{\\partial\\beta^{2}}\\Bigg|_{\\infty}\\Bigg]=0\\;,}&{}\\\\ {\\frac{\\partial\\hat{L}}{\\partial\\beta^{2}}\\Bigg|_{\\infty}=\\mathbb{E}_{\\mathcal{X}}\\Bigg[\\frac{\\partial\\hat{L}}{\\partial\\beta^{2}}\\Bigg|_{\\infty}\\times\\hat{L}\\Bigg(\\Delta\\eta\\Big(\\Delta\\eta\\Big(\\Delta\\eta\\Big(\\Delta\\beta\\Big(\\Delta\\beta\\Big)-1\\Big)\\Big)\\Big|_{\\infty}\\Bigg)}\\\\ {\\frac{\\partial\\hat{L}}{\\partial\\beta^{2}}\\Bigg|_{\\infty}\\Bigg.\\Bigg.=\\mathbb{E}_{\\mathcal{X}}\\Bigg[\\hat{L}(\\hat{L}_{1}+2\\alpha|\\alpha|\\beta|)-\\hat{L}_{1}+2\\beta(1+2\\alpha|\\alpha|)\\Bigg|_{\\infty}-\\hat{L}_{1}\\Bigg]\\;,}&{}\\\\ {-\\mathbb{E}_{\\mathcal{X}}\\Bigg[\\hat{L}(\\hat{L}_{1}+4\\alpha|\\alpha|)+\\hat{L}_{2}(\\hat{L}_{2}+4\\alpha|\\alpha|)\\Bigg|_{\\infty}\\Bigg.\\Bigg.}\\\\ {\\left.\\left.-\\left.\\mathbb{E}_{\\mathcal{X}}\\Bigg[\\hat{L}(1+\\alpha|\\alpha|)+\\hat{L}_{1}(2+\\alpha|\\alpha|)\\right]\\right|_{\\infty}-\\hat{L}\\right]\\Bigg|_{\\infty}\\Bigg.,}&{}\\\\ {\\left.\\left.\\left(\\hat{L}_{1}+4\\alpha|\\alpha|\\right)+\\hat{L}_{2}(2 \n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Wherco $(a)$ folowsfrom the fathat $\\begin{array}{r}{f_{1}|_{\\gamma}=p\\!+\\!q\\!-\\!1,f_{2}|_{\\gamma}=\\sigma(b)\\!-\\!p=\\frac{p}{p+q}-p=\\frac{-p}{p+q}(p\\!+\\!q\\!-\\!1)=}\\end{array}$ $-\\pi_{1}(p+q-1)$ and $(b)$ from $1-\\pi_{1}=\\pi_{0}$ .Returning to theremaining second derivatives, ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{\\partial^{2}\\left(\\mathbf{k}\\left(\\left(f_{1},\\mathbf{x}+f_{2}\\right)\\mathbf{x}\\right)+\\mathbf{k}^{2}\\mathbf{k}^{2}\\mathbf{k}^{3}\\right)}{\\partial\\mathbf{k}\\partial\\mathbf{x}}\\Bigg|_{0}^{1},}&{}\\\\ &{\\qquad-\\frac{\\partial}{\\partial\\mathbf{k}}\\left(\\left(\\mathbf{k}(f_{1}+f_{2})\\mathbf{x}\\right)\\mathbf{x}+\\mathbf{k}^{2}\\mathbf{k}^{3}\\right)\\Bigg|_{0}^{1},}\\\\ &{\\qquad-\\frac{\\partial}{\\partial\\mathbf{k}}\\left(\\left(f_{1}+f_{2}\\right)\\mathbf{x}+\\mathbf{k}^{2}\\mathbf{k}^{3}\\right)\\Bigg|_{0}^{1},}\\\\ &{=\\frac{\\partial}{\\partial\\mathbf{k}}\\left(\\left(\\rho\\left(\\left(\\rho\\mathbf{x}^{2}\\mathbf{x}^{\\intercal}\\mathbf{u}\\right)+\\mathbf{k}+\\frac{\\mathbf{x}^{2}}{2}\\right)+\\mathbf{q}\\left(-\\mathbf{k}\\right)^{2}\\mathbf{x}^{2}\\mathbf{u}\\right)\\right),}\\\\ &{\\qquad-\\left(\\frac{\\partial}{\\partial\\mathbf{k}}\\left(\\rho^{2}\\mathbf{x}^{\\intercal}\\mathbf{u}\\right)+\\mathbf{k}+\\frac{\\mathbf{x}^{2}}{2}\\right)\\Bigg)\\mathbf{a}\\mathbf{r}_{0}^{2}\\mathbf{k}\\Bigg|_{0}^{1},}\\\\ &{\\qquad+\\left(\\rho\\left(2\\sigma^{2}\\mathbf{x}^{\\intercal}\\mathbf{u}\\right)+\\mathbf{k}+\\frac{\\mathbf{x}^{2}}{2}\\right)+\\eta\\mathbf{x}-\\left(\\mathbf{k}\\right)\\frac{\\partial}{\\partial\\mathbf{k}}\\left(\\alpha_{\\mathbf{k}}\\mathbf{r}_{0}^{2}\\mathbf{x}\\right)\\Bigg|_{0}^{1},}\\\\ &{\\qquad-\\frac{\\partial}{\\partial\\mathbf{k}}\\left(\\mathbf{g}\\left(f_{1},\\mathbf{x}+f_{2}\\right)\\mathbf{x}\\right)\\mathbf{x}+\\frac{\\partial^{2}\\mathbf{x}}{\\partial\\mathbf{x}}\\left(\\alpha_{\\mathbf{k}}\\right)}\\\\ &{\\qquad-\\left(\\frac{\\partial}{\\partial\\mathbf{k}}\\mathbf{g}\\left(f_{1},\\mathbf{x}+f_{2}\\right)\\mathbf{x}\\right)\\cdot4\\mathbf{r}_{0}^{2}\\mathbf{k}\\Bigg)}\\\\ &{\\qquad-\\left(\\frac{\\partial}{\\partial\\mathbf{k}}\\mathbf{g \n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Congregating all the second derivatives from Eq. (68) and Eq. (69) into the Hessian $H(\\gamma)$ in Eq. (65), wefinallyobtain ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{H}(\\gamma)=\\pi_{0}\\pi_{1}\\left[\\!\\!\\begin{array}{c c c}{1}&{0}&{0}\\\\ {0}&{2(p+q-1)(1+2w|w|)}&{0}\\\\ {0}&{0}&{0}\\end{array}\\!\\!\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "L Proofs of lemmas in App. E ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "L.1 Proof of Lemma 5 ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Proof. Recall from Lemma 2 that for any $\\pmb{\\theta}=(e,w)\\in\\mathbb{R}^{2}$ and $b\\in\\mathbb{R}$ ,wehave ", "page_idx": 44}, {"type": "equation", "text": "$$\nL(\\pmb{\\theta},b)=\\mathbb{E}_{X,Y}\\left[\\ell_{\\log}\\left((2Y-1)\\left(e^{2}(1+2w|w|)X+b-\\frac{e^{2}}{2}\\right)\\right)\\right].\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Since $\\ell_{\\mathrm{log}}(\\cdot)$ is a convex function, $Y\\,\\in\\,\\{0,1\\}$ and thus $2Y-1\\in\\{\\pm1\\}$ , the convexity of $L$ in $b$ follows from the following fact: ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\frac{\\partial^{2}L}{\\partial b^{2}}=\\mathbb{E}_{X,Y}\\left[\\ell_{\\log}^{\\prime\\prime}\\left((2Y-1)\\left(e^{2}(1+2w|w|)X+b-\\frac{e^{2}}{2}\\right)\\right)\\right]\\ge0.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "To find the optimal $b_{\\star}$ , we set the gradient $\\begin{array}{r}{\\frac{{\\partial}L}{{\\partial}b}=0}\\end{array}$ Thus from Lemma3, we obtain ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\frac{\\partial L}{\\partial b}=\\mathbb{E}_{X}\\left[f_{1}X+f_{2}\\right]=0,}\\\\ {f_{1}=\\sigma\\left(2e^{2}w|w|+b+\\frac{e^{2}}{2}\\right)+q-1-\\sigma\\left(b-\\frac{e^{2}}{2}\\right)+p,f_{2}=\\sigma\\left(b-\\frac{e^{2}}{2}\\right)-p.}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Substituting $\\begin{array}{r}{E\\triangleq e^{2}(1+2w|w|),B\\triangleq b-\\frac{e^{2}}{2}}\\end{array}$ and $\\mathbb{E}[X]=\\pi_{1}=p/(p+q)$ in the above equations, ", "page_idx": 44}, {"type": "text", "text": "m(o(E+B)+\u03b1-1-o(B)+p)+\u03b1(B)-p=m\u00b7o(E+B)+\u03c0o o(B)+pD+-1) =0. Further simplifying, ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\pi_{0}\\cdot\\sigma(B)=\\pi_{1}\\cdot(1-\\sigma(E+B))=\\pi_{1}\\cdot\\sigma(-E-B).\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "In other words, ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\frac{(1+\\exp(-B))^{-1}}{(1+\\exp(E+B))^{-1}}=\\frac{\\pi_{1}}{\\pi_{0}}=\\frac{p}{q}\\Rightarrow\\frac{1+\\exp(E)\\exp(B)}{1+\\exp(-B)}=\\frac{p}{q}.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Defining $x\\triangleq\\exp(B)$ and $A\\triangleq\\exp(E)$ , we thus obtain the following quadratic equation in $x$ and its corresponding roots: ", "page_idx": 44}, {"type": "equation", "text": "$$\nA x^{2}-x\\left({\\frac{p}{q}}-1\\right)-{\\frac{p}{q}}=0\\Rightarrow x={\\frac{1}{2A}}\\left[{\\frac{p}{q}}-1\\pm{\\sqrt{\\left({\\frac{p}{q}}-1\\right)^{2}+4\\cdot{\\frac{p}{q}}\\cdot A}}\\right].\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Since $x~>~0$ , we take the root corresponding to the addition choice above and resubstituting $\\begin{array}{r}{x=\\exp(b-\\frac{e^{2}}{2})}\\end{array}$ and $A=\\exp(e^{2}(1+2w|w|))$ , we obtain the final expression for $b_{\\star}$ . In particular, $\\textit{e}=\\mathrm{~0~}$ i is asy to see that $A\\ =\\ 1$ and hence $x\\;=\\;\\exp(b_{\\star})\\;\\stackrel{\\cdot}{=}\\;\\frac{p}{q}$ ,implying $b_{\\star}^{\\ \\bar{}}=\\ \\log_{q}^{\\ p}$ Similaly ift $\\begin{array}{r}{A\\,=\\,{\\frac{(1-p)(1-q)}{p q}}}\\end{array}$ , it' straighforward to se that $\\begin{array}{r}{\\exp(b_{\\star}\\,-\\,e^{2}/2)\\,=\\,\\frac{p}{1-p}}\\end{array}$ and hence b+ - e\u00b2 /2 = log 1-p \u53e3 ", "page_idx": 44}, {"type": "text", "text": "L.2 Proof of Lemma 6 ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Proof. The proof directly follows from Lemma 2 by substituting $b=b_{\\star}$ ", "page_idx": 44}, {"type": "text", "text": "L.3Proof of Lemma 7 ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Proof. By Danskin's theorem [12] it follows that for $b_{\\star}=\\mathrm{argmin}_{b\\in\\mathbb{R}}\\,L(\\pmb\\theta,b)$ and $L(\\pmb\\theta)=L(\\pmb\\theta,b_{\\star})$ we have $\\nabla_{\\theta}L(\\pmb\\theta)=\\nabla_{\\theta}L(\\pmb\\theta,b_{\\star})$ Using the gradient expressions of $L(\\pmb\\theta,b)$ w.r.t $\\pmb{\\theta}$ from Lemma 3, and using the fact that $\\begin{array}{r}{\\frac{\\partial L}{\\partial b}=\\mathbb{E}[f_{1}X+f_{2}]}\\end{array}$ $b=b_{\\star}$ , the claim fllows. \u53e3 ", "page_idx": 44}, {"type": "text", "text": "L.4 Proof of Lemma 8 ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Proof. Since $L(\\pmb\\theta)\\,=\\,L(\\pmb\\theta,b_{\\star})$ where $\\begin{array}{r}{b_{\\star}=\\operatorname*{argmin}_{b\\in\\mathbb{R}}L(\\pmb\\theta,b)}\\end{array}$ , the identity in Eq. (32) about the Hessian of the loss $L$ with respect to $\\pmb{\\theta}$ follows from the classical result of [29, Lemma 2.2] about second-derivatives of extremal-value functions. Finally Eq. (33) follows from substituting the full Hessian in $\\mathbb{R}^{3\\times3}$ from Lemma 4 in this identity. \u53e3 ", "page_idx": 44}, {"type": "text", "text": "M   Proofs of lemmas in App. F ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "M.1 Proof of Lemma 9 ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Proof.Denote $(e_{t},w_{t})\\,=\\,(e,w)$ with the dependence on time implicitly assumed. Then by the definition of GF and the gradient expressions in Lemma 7, we have that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\mathrm{d}e}{\\mathrm{d}t}=-\\frac{\\partial L}{\\partial e}(\\pmb{\\theta}_{t})=-2\\mathbb{E}[(f_{1}X+f_{2})X]\\cdot(1+2w|w|)e,}\\\\ {\\displaystyle\\frac{\\mathrm{d}w}{\\mathrm{d}t}=-\\frac{\\partial L}{\\partial w}(\\pmb{\\theta}_{t})=-\\mathbb{E}[(f_{1}X+f_{2})X]\\cdot4e^{2}|w|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Dividing Eq. (70) by $(1+2w|w|)e$ and Eq. (71) by $4e^{2}|w|$ , we have ", "page_idx": 45}, {"type": "equation", "text": "$$\n{\\frac{1}{1+2w|w|}}{\\frac{\\mathrm{d}e}{\\mathrm{d}t}}={\\frac{1}{2e|w|}}{\\frac{\\mathrm{d}w}{\\mathrm{d}t}}\\Rightarrow e{\\frac{\\mathrm{d}e}{\\mathrm{d}t}}=\\left(w+{\\frac{1}{2|w|}}\\right){\\frac{\\mathrm{d}w}{\\mathrm{d}t}}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Noting hat $\\begin{array}{r}{{\\frac{\\mathrm{d}}{\\mathrm{d}t}}(\\mathrm{sign}(w)\\cdot\\log|w|)={\\frac{1}{|w|}}{\\frac{\\mathrm{d}w}{\\mathrm{d}t}}}\\end{array}$ theavequatarrit ", "page_idx": 45}, {"type": "equation", "text": "$$\n{\\frac{\\mathrm{d}}{\\mathrm{d}t}}\\left(e^{2}-w^{2}-\\mathrm{sign}(w)\\cdot\\log|w|\\right)=0.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Thus defining ${\\mathcal{E}}(e,w)\\;=\\;e^{2}\\,-\\,w^{2}\\,-\\,\\mathrm{sign}(w)\\,\\cdot\\,\\log|w|$ for $w~\\ne~0$ , the above equation implies $\\mathcal{E}(\\pmb{\\theta}_{t})=\\mathcal{E}(\\pmb{\\theta}_{0})$ for $\\pmb\\theta_{0}=(e_{0},w_{0})$ with $w_{0}\\neq0$ . On the other hand, it's easy to see that if $w_{0}=0$ Eq. (71) implies $\\begin{array}{r}{\\frac{\\mathrm{d}w}{\\mathrm{d}t}=0}\\end{array}$ $t=0$ and hence $w_{t}=0$ for all $t\\geq0$ \u53e3 ", "page_idx": 45}, {"type": "text", "text": "M.2 Proof of Lemma 10 ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Proof. To prove the convergence of the trajectory $(\\pmb\\theta_{t})_{t\\geq0}$ , we use the classical result due to Lojasiewicz [1, Theorem 2.2] which gurantees the convergence of gradient flow for real analytic functions, as long as the trajectory is bounded. Hence we first show the boundedness of the trajectory. ", "page_idx": 45}, {"type": "text", "text": "(i) $(\\pmb\\theta_{t})_{t\\geq0}$ is bounded. ", "page_idx": 45}, {"type": "text", "text": "We consider the cases $\\theta_{0}\\in\\mathrm{e}.$ -axis and $\\pmb{\\theta}_{0}\\in\\mathbb{R}^{2}\\setminus\\mathrm{e}.$ -axis separately. ", "page_idx": 45}, {"type": "text", "text": "Let's suppose $\\pmb\\theta_{0}=(e_{0},w_{0})\\in\\mathrm{e}$ -axis, i.e. $w_{0}=0$ . Thus it follows from Lemma 9 that $w_{t}=0$ for all $t\\geq0$ . That is, the trajectory always stays on the e-axis and it suffices to track $(e_{t})_{t\\geq0}$ andshow that they are bounded. To tis end, we show that if $e_{0}>0$ we have $\\begin{array}{r}{\\frac{\\mathrm{d}e}{\\mathrm{d}t}<0}\\end{array}$ and if $e_{0}<0$ we have $\\begin{array}{r}{\\frac{\\mathrm{d}e}{\\mathrm{d}t}>0}\\end{array}$ for all $t\\geq0$ , which establishes our claim. We have from the GF and Lemma 7 that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}e_{t}}{\\mathrm{d}t}=-\\frac{\\partial L}{\\partial e}(e_{t},w_{t}=0)=-2\\mathbb{E}_{X}\\left[(f_{1}X+f_{2})X\\right]e_{t}=-2\\pi_{1}(f_{1}+f_{2})e_{t},\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "and ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{f_{1}+f_{2}=-\\displaystyle\\frac{f_{2}}{\\pi_{1}}+f_{2}=-\\displaystyle\\frac{\\pi_{0}}{\\pi_{1}}f_{2}=-\\displaystyle\\frac{\\pi_{0}}{\\pi_{1}}\\left[\\sigma\\left((b_{\\star})_{t}-\\displaystyle\\frac{e_{t}^{2}}{2}\\right)-p\\right]}}\\\\ {{\\phantom{\\displaystyle}=-\\displaystyle\\frac{\\pi_{0}}{\\pi_{1}}\\left[\\left(1+\\exp\\left(-(b_{\\star})_{t}+\\displaystyle\\frac{e_{t}^{2}}{2}\\right)\\right)^{-1}-p\\right]}}\\\\ {{\\phantom{\\displaystyle}=-\\displaystyle\\frac{\\pi_{0}}{\\pi_{1}}\\left[\\left(1+\\displaystyle\\frac{2x_{t}}{\\displaystyle\\frac{p}{q}-1+\\sqrt{\\left(\\displaystyle\\frac{p}{q}-1\\right)^{2}+4\\cdot\\frac{p}{q}\\cdot x_{t}}}\\right)^{-1}-p\\right],\\quad x_{t}\\triangleq\\exp(e_{t}^{2}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Defining ", "page_idx": 45}, {"type": "equation", "text": "$$\ng(x)\\triangleq{\\frac{2x}{{\\frac{p}{q}}-1+{\\sqrt{\\left({\\frac{p}{q}}-1\\right)^{2}+4\\cdot{\\frac{p}{q}}\\cdot x}}}},\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "and substituting Eq. (74) and Eq. (73) in Eq. (72), we obtain ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}e_{t}}{\\mathrm{d}t}=2\\pi_{0}\\left(\\frac{1}{1+g(x_{t})}-p\\right)\\cdot e_{t}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Since $x_{t}=\\exp(e_{t}^{2})=\\exp(-e_{t}^{2})$ , in view of Eq. (75), with out loss of generality, we can assume that $e_{0}>0$ and show that $\\frac{\\mathrm{d}e_{t}}{\\mathrm{d}t}\\,<\\,0$ for all $t\\geq0$ . That is,the RHS Eq. (75) is negative. Note that $x_{t}\\geq1$ since $x_{t}=\\exp(e_{t}^{2})$ and $g(x_{t})>0$ since the denominator $\\begin{array}{r}{\\frac{p}{q}-1\\!+\\!\\sqrt{\\Big(\\frac{p}{q}-1\\Big)^{2}+4\\cdot\\frac{p}{q}\\cdot x_{t}}>}\\end{array}$ $\\textstyle{\\frac{p}{q}}-1+{\\frac{p}{q}}+1=2\\cdot{\\frac{p}{q}}>0$ Further $\\textstyle g(1)={\\frac{q}{p}}$ and $\\begin{array}{r}{\\operatorname*{lim}_{x\\rightarrow\\infty}g(x)=\\infty}\\end{array}$ If we show that $g(x)$ .s increasingin  for z \u22651,i implies1+g() p<1+(1)- P = $\\begin{array}{r}{\\frac{1}{1+g(x)}-p\\overset{.}{<}\\frac{1}{1+g(1)}-p=\\frac{1}{1+\\frac{q}{p}}-p=-\\frac{p}{p+q}(p+q-1)<0.}\\end{array}$ Thus the gradient in Eq. (75) remains negative starting at $t=0$ and hence the sequence $(e_{t})_{t\\geq0}$ will be bounded. Now it remains to show $g(\\cdot)$ is increasing, i.e. $g^{\\prime}(\\cdot)>0$ Defining $\\begin{array}{r}{C=\\left(\\frac{p}{q}-1\\right)\\bar{/}\\left(2\\sqrt{\\frac{p}{q}}\\right)}\\end{array}$ and $D=C^{2}$ , we have that $g(x)$ upto a postive scaling is ", "page_idx": 46}, {"type": "equation", "text": "$$\ng(x)=\\frac{x}{C+\\sqrt{x+D}}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Hence ", "page_idx": 46}, {"type": "equation", "text": "$$\ng^{\\prime}(x)=\\frac{C+\\sqrt{x+D}-\\frac{x}{2\\sqrt{x+D}}}{(C+\\sqrt{x+D})^{2}}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Thus it suffices to show that $\\begin{array}{r}{h_{1}(x)\\,\\triangleq\\,C+\\sqrt{x+D}\\,>\\,h_{2}(x)\\,\\triangleq\\,\\frac{x}{2\\sqrt{x+D}}}\\end{array}$ for $x\\,\\geq\\,1$ . Note that $h_{1}(1)-h_{2}(1)$ is given by ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{1}(1)-h_{2}(1)=\\frac{\\frac{p}{q}-1}{2\\sqrt{\\frac{p}{q}}}+\\sqrt{1+\\left(\\frac{\\frac{p}{q}-1}{2\\sqrt{\\frac{p}{q}}}\\right)^{2}}-\\frac{1}{2\\sqrt{1+\\left(\\frac{\\frac{p}{q}-1}{2\\sqrt{\\frac{p}{q}}}\\right)^{2}}}}\\\\ &{\\hphantom{h s p a c e{2p c}}=\\sqrt{\\frac{p}{q}}-\\frac{\\sqrt{\\frac{p}{q}}}{1+\\frac{p}{q}}}\\\\ &{\\hphantom{h s p a c e{2p c}}>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Now we show that $h^{\\prime}(x)>h_{2}^{\\prime}(x)$ for all $x\\geq1$ which implies that $h_{1}(x)>h_{2}(x)$ for all $x\\geq1$ ,thus establishing our claim. To this end, we have that ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{h_{1}^{\\prime}(x)-h_{2}^{\\prime}(x)=\\displaystyle\\frac{1}{2\\sqrt{x+D}}-\\frac{\\sqrt{x+D}-\\frac{x}{2\\sqrt{x+D}}}{2(x+D)}}}\\\\ {{=\\displaystyle\\frac{x}{2\\sqrt{x+D}(x+D)}>0.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "This proves our claim that $g(\\cdot)$ is increasing and hence $(e_{t})_{t\\geq0}$ , and consequently $(\\pmb\\theta_{t})_{t\\geq0}$ , is bounded when $\\theta_{0}\\in\\mathrm{e}$ -axis. ", "page_idx": 46}, {"type": "text", "text": "Now let's assume that $\\pmb{\\theta}_{0}=(e_{0},w_{0})\\in\\mathbb{R}^{2}\\setminus\\mathrm{e-axis}$ 3. Since $(\\pmb{\\theta}_{t})_{t\\geq0}\\subseteq\\mathbb{R}^{2}\\setminus\\$ e-axis, it follows that   \nthe loss $L(\\cdot)$ is analytic on the trajectory (since the logistic function is analytic), and hence by [1,   \nTheorem 2.2], it follows that $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}\\|\\pmb{\\theta}_{t}\\|}\\end{array}$ exists. Now we show that $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}\\|\\pmb{\\theta}_{t}\\|\\neq\\infty}\\end{array}$ which   \nimplies the desired result about boundedness. To show $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}\\left\\|\\pmb{\\theta}_{t}\\right\\|\\neq\\infty}\\end{array}$ , we show that there exists $B>0$ $\\pmb{\\theta}_{t}=(e,w)\\in\\mathbb{R}^{2}$ $\\|(e,w)\\|\\geq B$ $\\textstyle{\\frac{\\mathrm{d}\\theta}{\\mathrm{d}t}}$ poits $B$   \nbounded. To establish this, let's denote $(e_{t},w_{t})=(e,w)$ with the dependence on time implicitly   \nassumed. Then by the definition of GF and the gradient expressions in Lemma 7, we have that ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\mathrm{d}e}{\\mathrm{d}t}=-\\frac{\\partial L}{\\partial e}(\\pmb{\\theta}_{t})=-2\\mathbb{E}[(f_{1}X+f_{2})X]\\cdot(1+2w|w|)e}\\\\ {\\displaystyle\\frac{\\mathrm{d}w}{\\mathrm{d}t}=-\\frac{\\partial L}{\\partial w}(\\pmb{\\theta}_{t})=-\\mathbb{E}[(f_{1}X+f_{2})X]\\cdot4e^{2}|w|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where $\\begin{array}{r}{f_{1}=\\sigma\\left(2e^{2}w|w|+b_{\\star}+\\frac{e^{2}}{2}\\right)+q-1-\\sigma\\left(b_{\\star}-\\frac{e^{2}}{2}\\right)+p}\\end{array}$ and $\\begin{array}{r}{f_{2}=\\sigma\\left(b_{\\star}-\\frac{e^{2}}{2}\\right)-p}\\end{array}$ with $\\pi_{1}f_{1}+f_{2}=0$ . Given that only $\\textstyle{\\frac{\\mathrm{d}e}{\\mathrm{d}t}}$ flips in sign under the transformation $(e,w)\\mapsto(-e,w)$ with out loss of generality we can assume $e>0$ . Now let's also assume $w>0$ . Thus, in view of Eq. (76) and GF, to show that the derivative points inwards, it sufices to show that $\\mathbb{E}[(f_{1}X+f_{2})X]>0$ for reasonably large $B$ with $\\|(e,w)\\|\\stackrel{*}{=}B$ . Similar to Eq. (73) and Eq. (74) above, using the relation $\\pi_{1}f_{1}+f_{2}=0$ ,we obtain ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\mathbb{E}[(f_{1}X+f_{2})X]=\\pi_{1}(f_{1}+f_{2})=-\\pi_{0}\\left(\\frac{1}{1+g(x)}-p\\right),\\quad x\\triangleq\\exp(e^{2}(1+2w|w|)).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Using the fact that $g(x)$ is increasing for $x\\geq1$ with $\\begin{array}{r}{\\operatorname*{lim}_{x\\rightarrow\\infty}g(x)=\\infty}\\end{array}$ , and $|w|=w>0$ we can chose a $B>0$ such that for any $\\|(e,w)\\|\\geq B$ , in Eq. (78) we have $1/(1+g(x))<p$ and hence $\\mathbb{E}[(f_{1}X+f_{2})X]>0$ . This finishes the proof of our claim. The proof for $w<0$ is similar, where we make use of the fact that $\\textstyle\\operatorname*{lim}_{x\\to0}g(x)=0$ to show $\\mathbb{E}[(f_{1}X+\\bar{f}_{2})X]<0$ for $e,w$ reasonably large. ", "page_idx": 47}, {"type": "text", "text": "(ii) $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}\\pmb{\\theta}_{t}=\\pmb{\\theta}_{\\operatorname*{lim}}}\\end{array}$ _ Since the logistic function $\\ell_{\\mathrm{log}}(\\cdot)$ is analytic, it follows from Lemma 6 that the loss $L(\\pmb\\theta)$ is analytic too whenever $w\\ne0$ . On the other hand, when $w=0$ , it's easy to see that $L$ is an analytic function of $e\\in\\mathbb{R}$ . By Lemma 9, we know that if $w_{0}\\neq0$ \uff0c $w_{t}\\neq0$ and if $w_{0}=0$ $w_{t}=0$ for all $t\\geq0$ . Thus the loss is analytic on the trajectory for all $t\\geq0$ . Since the trajectory is bounded, it follows from Lojasiewicz's theorem [1, Theorem 2.2] that there exists a $\\pmb{\\theta}_{\\mathrm{{lim}}}\\in\\mathbb{R}^{2}$ such that $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}\\pmb{\\theta}_{t}=\\pmb{\\theta}_{\\operatorname*{lim}}}\\end{array}$ ", "page_idx": 47}, {"type": "text", "text": "(ii) $\\begin{array}{r}{\\operatorname*{lim}_{t\\rightarrow\\infty}\\|\\nabla L(\\pmb{\\theta}_{t})\\|=\\|\\nabla L(\\pmb{\\theta}_{\\mathrm{lim}})\\|=0}\\end{array}$ Since the trajectory is bounded, it follows from [2, Theorem 2] that the gradient converges to zero, i.e. $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}\\|\\nabla L(\\pmb{\\theta}_{t})\\|\\,=\\,0}\\end{array}$ Since $\\nabla L(\\cdot)$ is a continuous function and $\\begin{array}{r}{\\operatorname*{lim}_{t\\to\\infty}\\pmb{\\theta}_{t}=\\pmb{\\theta}_{\\operatorname*{lim}}}\\end{array}$ , we have $\\begin{array}{r}{\\operatorname*{lim}_{t\\rightarrow\\infty}\\|\\nabla L(\\pmb{\\theta}_{t})\\|=\\|\\nabla L(\\pmb{\\theta}_{\\mathrm{lim}})\\|=0}\\end{array}$ ", "page_idx": 47}, {"type": "text", "text": "M.3 Proof of Lemma 11 ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Proof. Since the energy function $\\mathcal{E}(\\cdot,\\cdot)$ in Eq. (48) is a continuous function in $\\mathbb{R}^{2}\\setminus\\mathrm{e}$ -axis, and any trajectory $(\\pmb\\theta_{t})_{t\\geq0}$ with intialization $\\theta\\in\\mathbb{R}^{2}\\setminus\\mathrm{e}$ -axis stays in $\\mathbb{R}^{2}\\setminus\\mathrm{e}$ -axis for all $t\\geq0$ (Lemma 9), it follows that $\\begin{array}{r}{\\ddot{\\operatorname*{lim}_{t\\rightarrow\\infty}\\mathcal{E}}(\\pmb{\\theta}_{t})=\\mathcal{E}(\\pmb{\\theta}_{\\operatorname*{lim}})=\\mathcal{E}(\\pmb{\\dot{\\theta}}_{0})}\\end{array}$ As $\\nabla L(\\pmb{\\theta}_{\\mathrm{lim}})=0$ from Lemma 10, it follows that $\\pmb\\theta_{\\mathrm{lim}}$ lies at the intersection of the contour line $\\mathcal{E}(e,w)=\\mathcal{E}_{0}$ with the set of critical points of $L$ in $\\mathbb{R}^{2}$ ", "page_idx": 47}, {"type": "text", "text": "On the other hand, if $\\theta_{0}\\ \\in$ e-axis, we have $\\pmb\\theta_{t}\\ \\in$ e-axis from Lemma 9 for all $t\\,\\geq\\,0$ . Hence $\\theta_{\\mathrm{lim}}\\in\\mathrm{e}.$ -axis. \u53e3 ", "page_idx": 47}, {"type": "text", "text": "M.4 Proof of Lemma 12 ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Proof. Recall that $f:\\mathbb{R}\\setminus\\{0\\}\\rightarrow\\mathbb{R}$ defined as $f(w)\\triangleq{\\mathcal{E}}(e=0,w)=-(w^{2}+\\operatorname{sign}(w)\\cdot\\log|w|).$ If $w<0$ wehave ", "page_idx": 47}, {"type": "equation", "text": "$$\nf(w)=-(w^{2}-\\log(-w)),\\quad f^{\\prime}(w)=-2w+\\frac{1}{w}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Hence $f^{\\prime}(w)\\geq0$ for $w\\in(-\\infty,-1/\\sqrt{2}]$ and $f^{\\prime}(w)\\leq0$ for $w\\in[-1/\\sqrt{2},0)$ with $f^{\\prime}(-{\\frac{1}{\\sqrt{2}}})=0$ It's also straightforward to see that $\\mathrm{lim}_{w\\to-\\infty}\\,f(w)\\ =\\ -\\infty$ $\\mathrm{lim}_{w\\to0^{-}}\\;f(w)\\;\\;=\\;\\;-\\stackrel{.}{\\infty}$ and $f(-1/{\\sqrt{2}})=\\mathcal{E}_{\\mathrm{sad}}$ (by the definition of $f$ ). This establishes (i), (ii), and (ii). ", "page_idx": 47}, {"type": "text", "text": "On the other hand, for $w>0$ ,we have $f(w)=-(w^{2}\\!+\\!\\log w)$ and $f^{\\prime}(w)=-(2w\\!+\\!1/w)$ Hence $f$ is monotonically decreasing for $w>0$ with $\\mathrm{lim}_{w\\to0^{+}}\\;f(w)=\\infty$ and $\\begin{array}{r}{\\operatorname*{lim}_{w\\to\\infty}f(w)=-\\infty.}\\end{array}$ Note that $w=0$ acts as an energy barrier since $\\mathrm{lim}_{w\\to0^{-}}\\;f(w)=-\\infty$ whereas $\\mathrm{lim}_{w\\to0^{+}}\\;f(w)=\\infty$ \uff1a\u53e3 ", "page_idx": 47}, {"type": "text", "text": "N  Proofs of lemmas in App. G ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "N.1 Proof of Lemma 13 ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Proof. First we recall the loss with the bias $L(\\pmb\\theta,b)$ from Eq. (41): ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L(\\pmb{\\theta},b)=\\mathbb{E}_{X,Y}\\left[\\ell_{\\log}\\left((2Y-1)\\cdot\\mathrm{logit}_{X}(\\pmb{\\theta},b)\\right)\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where $\\begin{array}{r l r}{\\log\\mathrm{it}_{X}(\\pmb{\\theta},b)}&{=}&{e^{2}\\left[\\left(X-\\frac{1}{2}\\right)\\left(1+a e^{2}\\right)\\left(1+2w|w|\\right)+w|w(1+a e^{2})|\\right]\\;+\\;b=}\\end{array}$ with $\\pmb\\theta\\ =$ $(e,w,a)$ . Using the fact that $\\ell_{\\log}^{\\prime}(z)=\\sigma(z)\\!-\\!1$ and $2Y\\!-\\!1\\in\\{\\pm1\\}$ , we have for any $\\theta\\in\\{e,w,a,b\\}$ that ", "page_idx": 48}, {"type": "text", "text": "$\\nabla_{\\theta}L=\\mathbb{E}\\left[\\left(\\sigma((2Y-1)\\cdot\\log\\operatorname{it}_{X})-1\\right)(2Y-1)\\cdot\\nabla_{\\theta}\\log\\operatorname{it}_{X}\\right]=\\mathbb{E}\\left[\\left(\\sigma(\\log\\operatorname{it}_{X})-Y\\right)\\cdot\\nabla_{\\theta}\\log\\operatorname{it}_{X}\\right].$ (79) ", "page_idx": 48}, {"type": "text", "text": "Now we simplify $\\sigma(\\mathrm{logit}_{X})$ using the fact that $X\\in\\{0,1\\}$ ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma(\\log\\mathrm{i}\\chi_{X})=\\sigma\\left(e^{2}\\left[\\left(X-\\frac{1}{2}\\right)\\left(1+a e^{2}\\right)\\left(1+2w|w|\\right)+w|w(1+a e^{2})\\right|\\right]+b\\right)}\\\\ &{\\qquad\\qquad=X\\underbrace{\\sigma\\left(e^{2}\\left[\\frac{1}{2}\\left(1+a e^{2}\\right)\\left(1+2w|w|\\right)+w|w(1+a e^{2})\\right]\\right]+b}_{\\triangleq\\phi_{1}}}\\\\ &{\\qquad\\qquad+\\left(1-X\\right)\\underbrace{\\sigma\\left(e^{2}\\left[\\frac{-1}{2}\\left(1+a e^{2}\\right)\\left(1+2w|w|\\right)+w|w(1+a e^{2})\\right]\\right]+b}_{\\triangleq\\phi_{0}}}\\\\ &{\\qquad\\qquad=X\\phi_{1}+(1-X)\\phi_{0}}\\\\ &{\\qquad\\qquad=X(\\phi_{1}-\\phi_{0})+\\phi_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Thus the gradients in Eq. (79) are given by ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\theta}L=-\\mathbb{E}\\left[\\left(Y-X(\\phi_{1}-\\phi_{0})-\\phi_{0}\\right)\\nabla_{\\theta}\\mathrm{logit}_{X}\\right]}\\\\ &{\\quad\\quad=-\\mathbb{E}_{X}\\left[\\mathbb{E}_{x_{1}^{n+1}}\\left[\\left(\\mathbb{E}[Y\\mid X]-X(\\phi_{1}-\\phi_{0})-\\phi_{0}\\right)\\nabla_{\\theta}\\mathrm{logit}_{X}\\right]\\right]}\\\\ &{\\quad\\quad=-\\mathbb{E}_{X}\\left[\\left(\\left(1-p-q\\right)X+p-X(\\phi_{1}-\\phi_{0})-\\phi_{0}\\right)\\nabla_{\\theta}\\mathrm{logit}_{X}\\right]}\\\\ &{\\quad\\quad=-\\mathbb{E}_{X}\\left[\\left(\\left(\\underbrace{1-p-q-\\phi_{1}+\\phi_{0}}_{f_{1}}\\right)X+\\underbrace{p-\\phi_{0}}_{f_{2}}\\right)\\nabla_{\\theta}\\mathrm{logit}_{X}\\right]}\\\\ &{\\quad\\quad=-\\mathbb{E}_{X}\\left[\\left(f_{1}X+f_{2}\\right)\\nabla_{\\theta}\\mathrm{logit}_{X}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Now we compute the individual gradients with respect to $e,w,a$ and $b$ . Recall that ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\mathrm{logit}_{X}=e^{2}\\left[\\left(X-\\frac{1}{2}\\right)\\left(1+a e^{2}\\right)\\left(1+2w|w|\\right)+w|w(1+a e^{2})|\\right]+b.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Thus, ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\epsilon}\\log\\mathrm{i}_{X}=2e\\left[\\left(X-\\frac{1}{2}\\right)\\left(1+a e^{2}\\right)\\left(1+2w\\vert w\\vert\\right)+w\\vert w(1+a e^{2})\\right]\\right]}\\\\ &{\\qquad\\qquad+e^{2}\\left[2a e\\left(X-\\frac{1}{2}\\right)\\left(1+2w\\vert w\\vert\\right)+w\\mathrm{sign}\\left(w\\left(1+a e^{2}\\right)\\right)\\left(2a e\\right)\\right]}\\\\ &{\\qquad\\qquad=2e\\left(X-\\frac{1}{2}\\right)\\left(1+a e^{2}\\right)\\left(1+2w\\vert w\\vert\\right)+2e w\\vert w(1+a e^{2})\\right]}\\\\ &{\\qquad\\qquad+2e^{3}a\\left(X-\\frac{1}{2}\\right)\\left(1+2w\\vert w\\vert\\right)+2e^{3}a w\\sin\\left(w\\left(1+a e^{2}\\right)\\right)}\\\\ &{\\qquad\\quad=2e\\left(X-\\frac{1}{2}\\right)\\left(1+a e^{2}\\right)\\left(1+2w\\vert w\\vert\\right)+2e^{3}a\\left(X-\\frac{1}{2}\\right)\\left(1+2w\\vert w\\vert\\right)}\\\\ &{\\qquad\\qquad+2e w\\vert w(1+a e^{2})\\right)+2e^{3}a w\\sin\\left(w\\left(1+a e^{2}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Substituting the above equation in Eq. (80), we obtain ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{e}L=-\\left(\\mathbb{E}\\left[\\left(f_{1}X+f_{2}\\right)\\left(X-\\displaystyle\\frac{1}{2}\\right)\\right]\\right)\\cdot2e\\left(1+a e^{2}\\right)\\left(1+2w|w|\\right)}\\\\ &{\\qquad-\\left(\\mathbb{E}\\left[\\left(f_{1}X+f_{2}\\right)\\left(X-\\displaystyle\\frac{1}{2}\\right)\\right]\\right)\\cdot2e^{3}a\\left(1+2w|w|\\right)}\\\\ &{\\qquad-\\left(\\mathbb{E}\\left[\\left(f_{1}X+f_{2}\\right)\\right]\\right)\\cdot2e w|w(1+a e^{2})|}\\\\ &{\\qquad-\\left(\\mathbb{E}\\left[\\left(f_{1}X+f_{2}\\right)\\right]\\right)\\cdot2e^{3}a w\\,\\mathrm{sign}\\left(w\\left(1+a e^{2}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Now we compute the derivative with respect to $w$ ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\log\\mathrm{it}_{X}=e^{2}\\left[\\left(X-\\frac12\\right)\\left(1+a e^{2}\\right)\\left(1+2w|w|\\right)+w|w(1+a e^{2})|\\right]+b}}\\\\ {{\\displaystyle\\Rightarrow\\nabla_{w}\\mathrm{logit}_{X}=2e^{2}\\left(X-\\frac12\\right)\\left(1+a e^{2}\\right)\\left(|w|+\\mathrm{sign}\\left(w\\right)w\\right)}}\\\\ {{\\displaystyle\\qquad\\qquad+e^{2}\\left[|w(1+a e^{2})|+w\\left(1+a e^{2}\\right)\\mathrm{sign}\\left(w\\left(1+a e^{2}\\right)\\right)\\right]}}\\\\ {{\\displaystyle\\Rightarrow\\nabla_{w}L=-\\left(\\mathbb{E}\\left[\\left(f_{1}X+f_{2}\\right)\\left(X-\\frac12\\right)\\right]\\right)2e^{2}\\left(1+a e^{2}\\right)\\left(|w|+\\mathrm{sign}\\left(w\\right)w\\right)}}\\\\ {{\\displaystyle\\qquad\\qquad-\\left(\\mathbb{E}\\left[\\left(f_{1}X+f_{2}\\right)\\right]\\right)e^{2}\\left[|w(1+a e^{2})|+w\\left(1+a e^{2}\\right)\\mathrm{sign}\\left(w\\left(1+a e^{2}\\right)\\right)\\right].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Similarly, for $a$ ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\mathrm{logit}_{X}=e^{2}\\displaystyle\\left[\\left(X-\\frac12\\right)\\left(1+2w|w|\\right)+w|w(1+a e^{2})|\\right]+b}}\\\\ {{\\Rightarrow\\nabla_{a}\\mathrm{logit}_{X}=e^{4}\\displaystyle\\left(X-\\frac12\\right)\\left(1+a e^{2}\\right)\\left(1+2w|w|\\right)}}\\\\ {{\\qquad\\qquad+e^{4}w^{2}\\mathrm{sign}\\left(w(1+a e^{2})\\right)}}\\\\ {{\\Rightarrow\\nabla_{a}L=-\\left(\\mathbb{E}\\left[\\left(f_{1}X+f_{2}\\right)\\left(X-\\frac12\\right)\\right]\\right)e^{4}\\left(1+2w|w|\\right)}}\\\\ {{\\qquad\\qquad-\\left(\\mathbb{E}\\left[\\left(f_{1}X+f_{2}\\right)\\right]\\right)e^{4}w^{2}\\mathrm{sign}\\left(w(1+a e^{2})\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Finally, since $\\nabla_{b}\\mathrm{logit}_{X}=1$ , it follows from Eq. (80) that ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\nabla_{b}L=-\\mathbb{E}\\left[f_{1}X+f_{2}\\right].\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "For the optimal $b_{\\star}$ , we have $\\nabla_{b}L=0$ and hence $\\mathbb{E}\\left[f_{1}X+f_{2}\\right]=0$ , simplifying the expressions for the gradients of $e,w$ , and $a$ above. In fact, there exists a closed form expression for $b_{\\star}$ in terms of $e,w$ , and $a$ . Recall from Eq. (80) that ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-f_{1}=\\sigma(z_{1})-\\sigma(z_{2})+p+q-1,}\\\\ &{-f_{2}=\\sigma(z_{2})-p,}\\\\ &{\\quad z_{1}\\triangleq e^{2}\\left[\\frac{1}{2}\\left(1+a e^{2}\\right)(1+2w|w|)+w|w(1+a e^{2})|\\right]+b,}\\\\ &{\\quad z_{2}\\triangleq e^{2}\\left[\\frac{-1}{2}\\left(1+a e^{2}\\right)(1+2w|w|)+w|w(1+a e^{2})|\\right]+b.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Substituting Eq. (82) in Eq. (81) and setting the gradient to zero there, we have ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\mathbb{E}_{X}\\left[f_{1}X+f_{2}\\right]=(\\sigma(z_{1})-\\sigma(z_{2})+p+q-1)\\,\\mathbb{E}[X]+\\sigma(z_{2})-p}\\\\ &{\\qquad\\qquad\\qquad=(\\sigma(z_{1})-\\sigma(z_{2})+p+q-1)\\pi_{1}+\\sigma(z_{2})-p=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Simplifying, ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~~(\\sigma(z_{1})-\\sigma(z_{2})+p+q-1)\\pi_{1}=p-\\sigma(z_{2})}\\\\ &{\\Rightarrow(\\sigma(z_{1})-1)\\,\\pi_{1}-\\sigma(z_{2})\\pi_{1}+p=p-\\sigma(z_{2})}\\\\ &{~~~~~~~~~~~~~~~~~\\Rightarrow(\\sigma(z_{1})-1)\\,\\pi_{1}=\\sigma(z_{2})(\\pi_{1}-1)}\\\\ &{~~~~~~~~~~~~~~~~~~\\Rightarrow\\displaystyle\\frac{\\sigma(z_{2})}{1-\\sigma(z_{1})}=\\frac{\\pi_{1}}{1-\\pi_{1}}=\\frac{p}{q},}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Using the definition of the sigmoid function and rearranging, ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\frac{1+\\exp(z_{1})}{1+\\exp(-z_{2})}=\\frac{p}{q}}}\\\\ {{\\Rightarrow\\exp(z_{1})+1=\\frac{p}{q}\\left(1+\\exp(-z_{2})\\right)}}\\\\ {{\\Rightarrow\\exp(2z_{1})+\\exp(z_{1})=\\frac{p}{q}\\exp(z_{1})+\\frac{p}{q}\\exp(z_{1}-z_{2})}}\\\\ {{\\Rightarrow(\\exp(z_{1}))^{2}+\\exp(z_{1})(1-\\frac{p}{q})-\\frac{p}{q}\\cdot\\exp(z_{1}-z_{2})=0.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "By definitions of $z_{1}$ and $z_{2}$ in Eq. (82), we have $z_{1}\\textrm{--}z_{2}\\,=\\,e^{2}(1\\,+\\,a e^{2})(1\\,+\\,2w|w|)$ and thus $A\\;\\triangleq\\;\\exp(z_{1}\\,-\\,z_{2})\\,=\\,\\exp(e^{2}(1+a e^{2})(1+2w|w|))$ .Thus the quadratic equation in Eq. (83) simplifies to ", "page_idx": 50}, {"type": "equation", "text": "$$\n(\\exp(z_{1}))^{2}+\\exp(z_{1})(1-\\frac{p}{q})-\\frac{p}{q}\\cdot A=0.\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "On solving the quadratic equation for $\\exp(z_{1})$ ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\exp(z_{1})=\\displaystyle\\frac{1}{2}\\left[\\frac{p}{q}-1+\\sqrt{\\left(\\frac{p}{q}-1\\right)^{2}+4\\cdot\\frac{p}{q}\\cdot A}\\right]}\\\\ &{\\phantom{a a a a a a a a}\\Rightarrow z_{1}=\\log\\left(\\frac{1}{2}\\left[\\frac{p}{q}-1+\\sqrt{\\left(\\frac{p}{q}-1\\right)^{2}+4\\cdot\\frac{p}{q}\\cdot A}\\right]\\right)}\\\\ &{\\phantom{a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a}\\Rightarrow b_{\\varepsilon}=\\log\\left(\\frac{1}{2}\\left[\\frac{p}{q}-1+\\sqrt{\\left(\\frac{p}{q}-1\\right)^{2}+4\\cdot\\frac{p}{q}\\cdot A}\\right]\\right)}\\\\ &{\\phantom{a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a}\\)-e^{2}\\left[\\frac{1}{2}\\left(1+a e^{2}\\right)\\left(1+2w|w|\\right)+w|w(1+a e^{2})\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Note that when $a\\,=\\,0$ above, we recover the expression for the optimal bias in Lemma 5. This concludes the proof. ", "page_idx": 50}, {"type": "text", "text": "N.2 Proofs of Thm. 9 and Thm. 10 ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "We prove Thm. 9 and Thm. 10 below. Note that Thm. 10 directly follows from the former by removing the bias $b$ , since for any critical point $\\gamma=(e,w,b,a)\\in\\mathbb{R}^{4}$ with $\\nabla L(\\gamma)=0$ , the bias $b$ is already the optimal one corresponding to $\\pmb{\\theta}=(e,w,a)\\in\\mathbb{R}^{3}$ , i.e. $\\begin{array}{r}{b=b_{\\star}(\\pmb\\theta)=\\mathrm{argmin}_{b\\in\\mathbb R}\\,L(\\pmb\\theta,b)}\\end{array}$ This is similar to the proof of Thm. 1, which follows from Thm. 7. ", "page_idx": 50}, {"type": "text", "text": "Now we prove Thm. 9. ", "page_idx": 50}, {"type": "text", "text": "Proof. We characterize the set of global minima first. ", "page_idx": 51}, {"type": "text", "text": "Set of all global minima. Let $\\gamma_{\\star}\\,\\in\\,\\mathbb{R}^{4}$ be arbitrary. From [23, Lemma 1], we have that $\\gamma_{\\star}$ is a global minimum for the loss $L(\\cdot)$ in Eq. (23) if and only if its prediction probability satisfies $f_{\\gamma_{\\star}}(x_{1}^{n})=\\mathbb{P}\\left(x_{n+1}=1\\mid x_{n}\\right)$ , the Markov kernel. Since the input $\\overbar{\\{x_{n}\\}}_{n=1}^{N}\\\\stackrel{\\cdot}{\\sim}(\\pi(p,q)\\overbar{,}P(p,q))$ \uff0c we have that ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(x_{n+1}=1\\mid x_{n}\\right)=(1-x_{n})p+x_{n}(1-q)=(1-p-q)x_{n}+p.\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "On  theotherhand,bydefinition\uff0cfrom \u3000Eq.(3)\uff0c $f_{\\gamma_{\\star}}(x_{1}^{n})$ $\\begin{array}{r}{\\sigma\\left(e^{2}\\left[\\left(x_{n}-\\frac{1}{2}\\right)\\left(1+a e^{2}\\right)\\left(1+2w|w|\\right)+w|w(1+a e^{2})|\\right]+b\\right)}\\end{array}$ ,where $\\gamma_{\\star}\\,=\\,(e,w,b,a)$ .Since $x_{n}\\in\\{0,1\\}$ , this can be further simplified to ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{f_{\\gamma_{+}}(x_{1}^{n})=\\sigma\\left(e^{2}\\left[\\left(x_{n}-\\frac{1}{2}\\right)\\left(1+\\alpha e^{2}\\right)\\left(1+2w|w|\\right)+w|w(1+\\alpha e^{2})\\right]\\right]+b\\right)}\\\\ &{=x_{n}\\sigma\\left(e^{2}\\left[\\frac{1}{2}\\left(1+\\alpha e^{2}\\right)\\left(1+2w|w|\\right)+w|w(1+\\alpha e^{2})\\right]\\right]+b\\right)}\\\\ &{\\phantom{=}+(1-x_{n})\\sigma\\left(e^{2}\\left[\\frac{-1}{2}\\left(1+\\alpha e^{2}\\right)\\left(1+2w|w|\\right)+w|w(1+\\alpha e^{2})\\right]\\right]+b\\right)}\\\\ &{=x_{n}\\sigma\\left(e^{2}\\left[\\frac{1}{2}\\left(1+\\alpha e^{2}\\right)\\left(1+2w|w|\\right)+w|w(1+\\alpha e^{2})\\right]\\right]+b\\right)}\\\\ &{\\phantom{=}-x_{n}\\sigma\\left(e^{2}\\left[\\frac{-1}{2}\\left(1+\\alpha e^{2}\\right)\\left(1+2w|w|\\right)+w|w(1+\\alpha e^{2})\\right]\\right]+b\\right)}\\\\ &{\\phantom{=}+\\sigma\\left(e^{2}\\left[\\frac{-1}{2}\\left(1+\\alpha e^{2}\\right)\\left(1+2w|w|\\right)+w|w(1+\\alpha e^{2})\\right]\\right]+b\\right)}\\\\ &{\\phantom{=}+\\sigma\\left(e^{2}\\left[\\frac{-1}{2}\\left(1+\\alpha e^{2}\\right)\\left(1+2w|w|\\right)+w|w(1+\\alpha e^{2})\\right]\\right]+b\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Since both $f_{\\gamma_{\\star}}(x_{1}^{n})$ and $\\mathbb{P}\\left(x_{n+1}=1\\mid x_{n}\\right)$ are linear functions of $x_{n}$ , equating them for all vallues of $x_{n}\\in\\{0,1\\}$ implies that the respective coeffecients in these functions in Eq. (84) and Eq. (85) are also equal, i.e. ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{1-p-q=\\sigma\\left(e^{\\displaystyle2\\left[\\frac{1}{2}\\left(1+a e^{2}\\right)\\left(1+2w|w|\\right)+w|w(1+a e^{2})|\\right]+b\\right)}\\displaystyle2\\left[(k+a e^{2})\\left(1+2w|w|\\right)+w|w(1+a e^{2})|\\right]+b\\right)}}\\\\ {{\\displaystyle-\\sigma\\left(e^{2}\\left[\\frac{-1}{2}\\left(1+a e^{2}\\right)\\left(1+2w|w|\\right)+w|w(1+a e^{2})|\\right]+b\\right),}}\\\\ {{\\displaystyle p=\\sigma\\left(e^{2}\\left[\\frac{-1}{2}\\left(1+a e^{2}\\right)\\left(1+2w|w|\\right)+w|w(1+a e^{2})|\\right]+b\\right),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "and hence ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\sigma\\left(e^{2}\\left[\\displaystyle\\frac{1}{2}\\left(1+a e^{2}\\right)\\left(1+2w|w|\\right)+w|w(1+a e^{2})|\\right]+b\\right)=1-q,}}\\\\ {{\\sigma\\left(e^{2}\\left[\\displaystyle\\frac{-1}{2}\\left(1+a e^{2}\\right)\\left(1+2w|w|\\right)+w|w(1+a e^{2})|\\right]+b\\right)=p.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Since $\\sigma(z)=y$ for $y\\in(0,1)$ implies $z=\\log{\\frac{y}{1-y}}$ , Eq. (87) can be rewritten as ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{e^{2}\\left[{\\displaystyle\\frac{1}{2}\\left(1+a e^{2}\\right)\\left(1+2w|w|\\right)+w|w(1+a e^{2})|\\right]+b=\\log\\displaystyle\\frac{1-q}{q}},}}\\\\ {{e^{2}\\left[{\\displaystyle\\frac{-1}{2}\\left(1+a e^{2}\\right)\\left(1+2w|w|\\right)+w|w(1+a e^{2})|\\right]+b=\\log\\displaystyle\\frac{p}{1-p}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Adding and subtracting the above two equations, we obtain ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad e^{2}w|w(1+a e^{2})|+b=\\frac{1}{2}\\log\\frac{p(1-q)}{q(1-p)},}\\\\ &{\\quad e^{2}\\left(1+a e^{2}\\right)(1+2w|w|)=\\log\\frac{(1-q)(1-p)}{p q}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Thus $\\gamma_{\\star}\\in\\mathbb{R}^{4}$ is a global minimum for $L(\\cdot)$ if and only if it satisfies Eq. (88). It's easy to see that $\\gamma_{\\star}$ is already a critical point for $L$ as Eq. (86) is equivalent to $f_{1}=f_{2}=0$ in Lemma 13. Thus, the set of all global minimum $\\mathbf{\\Gamma}_{\\star}(p,q)$ is given by ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{T}_{\\star}(p,q)\\triangleq\\{\\gamma_{\\star}=(e,w,b,a)\\in\\mathbb{R}^{4}:e^{2}w|w(1+a e^{2})|+b=\\frac{1}{2}\\log\\frac{p(1-q)}{q(1-p)},\\qquad}\\\\ {e^{2}\\left(1+a e^{2}\\right)(1+2w|w|)=\\log\\frac{(1-q)(1-p)}{p q}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Since the prediction $f_{\\gamma_{\\star}}(\\cdot)$ equals the Markov kernel for any $\\gamma_{\\star}\\in\\Gamma_{\\star}$ , it follows from Thm. 4 (or [23, Lemma 1j) that $L(\\gamma_{\\star}){\\overset{}{=}}\\,H(x_{n+1}\\mid x_{n})$ the entropy rate of the Markov chain. Now we characterize the remaining set of stationary points. ", "page_idx": 52}, {"type": "text", "text": "Non-global-min critical points. For any critical point $\\pmb{\\gamma}=(e,w,a,b)\\,\\in\\,\\mathbb{R}^{4}$ , we have from the gradient expressions in Lemma 13 that (denoting $-f_{1}$ and $-f_{2}$ from the lemma as $f_{1}$ and $f_{2}$ respectively) ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\frac{\\partial{\\cal L}}{\\partial b}=\\mathbb E_{X}\\left[f_{1}X+f_{2}\\right]=0,}}\\\\ {{\\displaystyle\\frac{\\partial{\\cal L}}{\\partial e}=\\mathbb E_{X}\\left[\\left(f_{1}X+f_{2}\\right)\\left(X-\\frac12\\right)\\right]{2e}\\left(1+a e^{2}\\right)\\left(1+2w|w|\\right)}}\\\\ {{\\displaystyle\\qquad+\\mathbb E_{X}\\left[\\left(f_{1}X+f_{2}\\right)\\left(X-\\frac12\\right)\\right]{2e}^{3}a\\left(1+2w|w|\\right)=0,}}\\\\ {{\\displaystyle\\frac{\\partial{\\cal L}}{\\partial{w}}=\\mathbb E_{X}\\left[\\left(f_{1}X+f_{2}\\right)\\left(X-\\frac12\\right)\\right]{2e}^{2}\\left(1+a e^{2}\\right)\\left(|w|+\\mathrm{sign}\\left(w\\right)w\\right)=0,}}\\\\ {{\\displaystyle\\frac{\\partial{\\cal L}}{\\partial a}=\\mathbb E_{X}\\left[\\left(f_{1}X+f_{2}\\right)\\left(X-\\frac12\\right)\\right]{e}^{4}\\left(1+2w|w|\\right)=0.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "From Eq. (89), we have that $\\mathbb{E}_{X}\\left[f_{1}X+f_{2}\\right]\\;=\\;0$ .If $\\begin{array}{r}{\\mathbb{E}_{X}\\left[\\left(f_{1}X+f_{2}\\right)\\left(X-\\frac{1}{2}\\right)\\right]\\;=\\;\\mathbb{E}[\\left(f_{1}X\\,+\\,\\right.}\\end{array}$ $f_{2})X]=0$ , we have that $f_{1}=f_{2}=0$ , implying $\\gamma$ is a global minimum. Hence without loss of generality, assume that $\\mathbb{E}_{X}\\left[(f_{1}X+f_{2})\\left(X-{\\frac{1}{2}}\\right)\\right]\\neq0$ . Then we can partition the above set of equations into the following regions of stationarity: ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{X}\\left[f_{1}X+f_{2}\\right]=0,e=0,}\\\\ &{\\mathbb{E}_{X}\\left[f_{1}X+f_{2}\\right]=0,e\\neq0,1+a e^{2}=0,1+2w|w|=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Slightly changing the variable order, for any $\\gamma=(b,e,w,a)\\in\\mathbb{R}^{4}$ ,we define ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r}{H(\\gamma)\\triangleq\\nabla^{2}L(\\gamma)=\\left[\\begin{array}{l l l l}{\\frac{\\partial^{2}L}{\\partial b^{2}}}&{\\frac{\\partial^{2}L}{\\partial b\\partial e}}&{\\frac{\\partial^{2}L}{\\partial b\\partial w}}&{\\frac{\\partial^{2}L}{\\partial b\\partial a}}\\\\ {\\frac{\\partial^{2}L}{\\partial e\\partial b}}&{\\frac{\\partial^{2}L}{\\partial e^{2}}}&{\\frac{\\partial^{2}L}{\\partial e\\partial w}}&{\\frac{\\partial^{2}L}{\\partial e\\partial a}}\\\\ {\\frac{\\partial^{2}L}{\\partial w\\partial b}}&{\\frac{\\partial^{2}L}{\\partial w\\partial e}}&{\\frac{\\partial^{2}L}{\\partial w^{2}}}&{\\frac{\\partial^{2}L}{\\partial w\\partial a}}\\\\ {\\frac{\\partial^{2}L}{\\partial a\\partial b}}&{\\frac{\\partial^{2}L}{\\partial a\\partial e}}&{\\frac{\\partial^{2}L}{\\partial a\\partial w}}&{\\frac{\\partial^{2}L}{\\partial a^{2}}}\\end{array}\\right]\\in\\mathbb{R}^{4\\times4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Recall that ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{f_{1}=\\sigma(z_{1})-\\sigma(z_{2})+p+q-1,}}\\\\ {{f_{2}=\\sigma(z_{2})-p,}}\\\\ {{z_{1}\\triangleq e^{2}\\left[\\displaystyle\\frac{1}{2}\\left(1+a e^{2}\\right)(1+2w|w|)+w|w(1+a e^{2})|\\right]+b,}}\\\\ {{z_{2}\\triangleq e^{2}\\left[\\displaystyle\\frac{-1}{2}\\left(1+a e^{2}\\right)(1+2w|w|)+w|w(1+a e^{2})|\\right]+b.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "From Eq. (89), we see that the second derivaties of $L$ depend on the first-derivatives of $f_{1}$ and $f_{2}$ , which we now compute. Recall that the derivative of the sigmoid function obeys $\\sigma^{\\prime}(z)\\,=$ $\\bar{\\sigma}(z)(1-\\sigma(z))=\\sigma(z)\\bar{\\sigma}(-z)$ for any $z\\in\\mathbb{R}$ . Now the gradients of $f_{1}$ and $f_{2}$ with respect to $b,e,w$ and $a$ are ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\tilde{U}_{j,\\ell}^{(k)}=\\alpha_{0}\\omega_{0}\\omega_{1}\\omega^{\\prime}-\\alpha_{1}\\omega_{1}}&{=0}\\\\ {\\tilde{U}_{j,\\ell}^{(k)}=\\alpha_{1}(k)(r-1)\\sin\\left(\\frac{\\theta}{r}\\right)\\left(1+\\alpha^{2}\\left(1+2\\alpha^{2}\\right)(1+2\\alpha^{2}\\omega_{1}+8\\beta\\omega_{1}+4\\alpha^{2}\\nu_{1})\\right)}\\\\ &{\\quad+\\alpha_{1}(\\omega_{1})\\omega_{1}-\\beta_{2}\\omega_{1}\\sin\\left(\\frac{\\theta}{r}\\right)\\left(2\\alpha^{2}\\sin\\left(\\theta+2\\nu\\right)\\left(1+2\\alpha^{2}\\omega_{1}+8\\beta\\omega_{1}\\right)\\right)}\\\\ &{\\quad-\\alpha_{1}(\\omega_{1})\\omega_{1}-2\\beta_{2}\\omega_{1}\\sin^{2}\\left(\\frac{\\theta}{r}\\right)\\left(1+2\\alpha^{2}\\omega_{1}\\right)\\left(1+2\\beta\\omega_{1}+8\\beta\\omega_{1}+16\\omega_{1}^{2}\\right)}\\\\ &{\\quad-\\alpha_{1}(\\omega_{1})\\omega_{1}-\\beta_{2}\\omega_{1}\\sin\\left(2\\frac{\\theta}{r}\\right)\\left(1-\\frac{\\theta}{r}\\right)\\left(1+2\\alpha^{2}\\omega_{1}\\omega_{1}+8\\beta\\omega_{1}+24\\alpha^{2}\\nu_{1}\\right)\\right)}\\\\ {\\tilde{U}_{j,\\ell}^{(k)}=\\alpha_{0}(\\omega_{1})\\omega_{1}-2\\beta_{2}\\omega_{1}\\sin^{2}\\left(1+\\frac{\\theta}{r}\\right)\\left(1+2\\alpha^{2}\\omega_{1}\\left(9+\\beta\\omega_{1}+4\\alpha^{2}\\nu_{1}\\right)\\right)}\\\\ &{\\quad+\\alpha_{1}(\\omega_{1})\\omega_{1}-\\beta_{2}\\omega_{1}\\left(\\frac{\\theta}{r}\\right)\\left(1-\\frac{2}{3}\\left(1+\\alpha^{2}\\omega_{1}\\left(1+2\\alpha^{2}\\right)\\sin\\left(1+4\\alpha^{2}\\nu_{1}\\right)\\right)+}\\\\ &{\\quad+\\alpha_{1}(\\omega_{1})\\omega_{1}-2\\beta_{2}\\omega_{1}\\left(\\frac{\\theta}{r}\\right)\\left(1- \n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Now we characterize the first set of critical points. ", "page_idx": 53}, {"type": "text", "text": "(i) Stationary points with $\\mathbb{E}_{X}\\left[f_{1}X+f_{2}\\right]=0,e=0$ When $e\\,=\\,0$ , we have that $z_{1}\\;=\\;z_{2}\\;=\\;b$ Hence, ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{l}{f_{1}=\\sigma(b)+p+q-1-\\sigma(b)=p+q-1,}\\\\ {f_{2}=\\sigma(b)-p.}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Thus, $\\mathbb{E}_{X}\\left[f_{1}X+f_{2}\\right]=(p+q-1)\\,\\mathbb{E}_{X}\\left[X\\right]+\\sigma(b)-p=\\left(p+q-1\\right)\\pi_{1}+\\sigma(b)-p=0.$ Rearranging and simplifying,. $\\begin{array}{r}{\\sigma(b)=\\frac{p}{p+q}}\\end{array}$ and hence $b=\\log\\frac{p}{q}$ Using th fathat $\\begin{array}{r}{\\sigma\\left(\\log\\frac{p}{q}\\right)=\\frac{p}{p+q}=\\pi_{1}}\\end{array}$ and $\\begin{array}{r}{\\sigma\\left(-\\log\\frac{p}{q}\\right)=\\frac{q}{p+q}=\\pi_{0}}\\end{array}$ habegrasoray $\\textstyle\\gamma=(b=\\log{\\frac{p}{q}},e=0,w,a)$ further reduce to ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r l}{\\left.\\frac{\\partial f_{1}}{\\partial b}\\right|_{\\gamma}=0,}&{\\left.\\frac{\\partial f_{2}}{\\partial b}\\right|_{\\gamma}=\\pi_{0}\\pi_{1},}\\\\ {\\left.\\frac{\\partial f_{1}}{\\partial e}\\right|_{\\gamma}=0,}&{\\left.\\frac{\\partial f_{2}}{\\partial e}\\right|_{\\gamma}=0,}\\\\ {\\left.\\frac{\\partial f_{1}}{\\partial w}\\right|_{\\gamma}=0,}&{\\left.\\frac{\\partial f_{2}}{\\partial w}\\right|_{\\gamma}=0,}\\\\ {\\left.\\frac{\\partial f_{1}}{\\partial a}\\right|_{\\gamma}=0,}&{\\left.\\frac{\\partial f_{2}}{\\partial a}\\right|_{\\gamma}=0.}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Now substituting Eq. (92) when computing the second-derivatives of $L$ in Eq. (89), we obtain ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{\\partial\\rho L}{\\partial t}{\\partial t}{\\partial t}}&{=\\operatorname{\\mathcal{E}}_{i}\\left[\\frac{\\partial\\rho L}{\\partial t}\\left|X_{i}+\\frac{\\partial\\rho L}{\\partial t}\\right|_{1}\\right]=\\operatorname{\\mathcal{E}}_{i}\\eta_{1},}\\\\ {\\frac{\\partial\\rho L}{\\partial t}{\\partial t}}&{=\\operatorname{\\mathcal{E}}_{i}\\left[\\frac{\\partial L}{\\partial t}\\left|X_{i}+\\frac{\\partial\\rho L}{\\partial t}\\right|_{1}\\right]=0,}\\\\ {\\frac{\\partial\\rho L}{\\partial t}{\\partial t}{\\partial t}{\\partial t}}&{=\\operatorname{\\mathcal{E}}_{i}\\left[\\frac{\\partial L}{\\partial t}\\left|X_{i}+\\frac{\\partial L}{\\partial t}\\right|_{1}\\right]=0,}\\\\ {\\frac{\\partial\\rho L}{\\partial t}{\\partial t}{\\partial t}{\\partial t}}&{=\\operatorname{\\mathcal{E}}_{i}\\left[\\frac{\\partial L}{\\partial t}\\left|X_{i}+\\frac{\\partial L}{\\partial t}\\right|_{1}\\right]=0,}\\\\ {\\frac{\\partial\\rho L}{\\partial t}{\\partial t}{\\partial t}{\\partial t}}&{=\\operatorname{\\mathcal{E}}_{i}\\left[\\frac{\\partial L}{\\partial t}\\left|X_{i}+\\frac{\\partial L}{\\partial t}\\right|_{1}\\right]=0,}\\\\ {\\frac{\\partial\\rho L}{\\partial t}{\\partial t}{\\partial t}{\\partial t}}&{=\\operatorname{\\mathcal{E}}_{i}\\left[(\\mu_{1}X_{i}+\\mu_{0})\\left(X_{i}-\\frac{1}{\\mu_{0}}\\right)\\right]=1(1+\\alpha^{2})\\left(1+2\\alpha^{3}\\mu_{0}\\right)\\Big[,}\\\\ &{+\\underbrace{\\operatorname{\\mathcal{E}}_{i}\\left[(\\mu_{1}X_{i}+\\mu_{0})\\left(X_{i}-\\frac{1}{\\mu_{0}}\\right)\\right]}_{=0}\\right]}\\\\ &{\\quad+\\underbrace{\\operatorname{\\mathcal{E}}_{i}\\left[(\\mu_{1}X_{i}+\\mu_{0})\\left(X_{i}-\\frac{1}{\\mu_{0}}\\right)\\right]}_{=0}\\right]}\\\\ &{\\quad+\\underbrace{\\operatorname{\\mathcal{E}}_{i}\\left[\\left(\\frac{\\partial L}{\\partial t}\\right)_{i}X_{i}+\\frac{\\partial L}{\\partial t}\\right]}_{=0}\\Big(\\sum_{s=1}^{T}\\bigg)\\left[2(1+\\alpha^{2})^{2}\\left(1+ \n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "where $(a)$ $\\begin{array}{r}{f_{1}|_{\\gamma}=p\\!+\\!q\\!-\\!1,f_{2}|_{\\gamma}=\\sigma(b)\\!-\\!p=\\frac{p}{p+q}-p=\\frac{-p}{p+q}(p\\!+\\!q\\!-\\!1)=}\\end{array}$ $-\\pi_{1}(p+q-1)$ and $(b)$ from $1-\\pi_{1}=\\pi_{0}$ . Returning to the remaining second derivatives, ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\frac{\\partial^{2}\\hat{L}}{\\partial\\hat{\\Delta}t\\partial\\nu}\\Bigg[\\eta_{+}^{2}-\\frac{\\partial}{\\hbar}\\frac{\\partial}{\\partial\\tau}\\Bigg[(f_{+}X+\\hat{f}_{i})\\left(X-\\frac{1}{2}\\right)\\Bigg]\\mathrm{e}^{\\tau^{2}(1+\\alpha^{2})\\tau/\\hbar}\\Bigg],}&{}\\\\ {-\\frac{\\partial}{\\hbar}\\sum_{\\tau}\\left[(f_{+}X+\\hat{f}_{i})\\left(X-\\frac{1}{2}\\right)\\right]\\left(\\kappa\\left(1+\\alpha^{2}\\right)\\kappa^{(1)}+\\kappa^{(2)}\\omega^{(1)}\\right)\\Bigg[\\eta_{+}^{2}}\\\\ {+\\frac{\\partial}{\\hbar}\\sum_{\\tau}\\left(\\frac{\\partial^{2}\\hat{L}}{\\partial\\tau}\\right)\\Bigg]\\Bigg[\\frac{\\partial^{2}\\hat{L}}{\\partial\\tau^{2}}+\\frac{\\partial^{2}\\hat{L}}{\\partial\\tau^{2}}\\Bigg]\\Bigg[\\kappa^{(2)}(1+\\omega^{(2)})\\left(1+\\omega^{(2)}\\right)\\Bigg],}&{}\\\\ {\\frac{\\partial^{2}\\hat{L}}{\\partial\\tau\\partial\\tau}}&{=0,}\\\\ {\\frac{\\partial^{2}\\hat{L}}{\\partial\\tau\\partial\\tau}\\Bigg[(f_{+}X+\\hat{f}_{i})\\left(X-\\frac{1}{2}\\right)\\Bigg]\\mathrm{e}^{\\tau^{2}(1+2\\alpha^{2})\\tau/\\hbar}\\Bigg],}&{}\\\\ {-\\frac{\\partial^{2}\\hat{L}}{\\partial\\tau}\\Bigg[(f_{+}X+\\hat{f}_{i})\\left(X-\\frac{1}{2}\\right)\\Bigg]\\mathrm{e}^{\\tau^{2}(1+2\\alpha^{2})\\tau/\\hbar}\\Bigg[\\eta_{+}^{2}}\\\\ {+\\frac{\\partial}{\\hbar}\\sum_{\\tau}\\left(\\frac{\\partial^{2}\\hat{L}}{\\partial\\tau}\\right)\\Bigg[\\frac{\\partial^{2}\\hat{L}}{\\partial\\tau^{2}}+\\frac{\\partial^{2}\\hat{L}}{\\partial\\tau^{2}}\\Bigg]\\Bigg[\\kappa^{(1)}(1+\\alpha^{2})\\left(1+\\omega^{(1)}\\right)}\\\\ {+\\frac{\\partial^{2}\\hat{L}}{\\partial\\tau^{2}}\\Bigg]\\Bigg[\\frac{\\partial^{2}\\hat{L}}{\\partial\\tau^{2}}\\Bigg]\\Bigg[\\eta_ \n$$", "text_format": "latex", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\}}\\\\ &{=\\mathrm{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\}[\\mathrm{\\\\\\\\\\\\\\\\\\}}\\\\ &{=\\mathrm{\\\\\\\\\\\\\\}}\\\\ &{=\\mathrm{\\\\\\\\\\\\}}\\\\ &{=\\mathrm{\\\\\\\\}}\\\\ &{=\\mathrm{\\\\\\\\}}\\\\ &{=\\mathrm{\\\\\\\\}}\\\\ &{=\\mathrm{\\\\\\\\}}\\\\ &{=\\mathrm{\\\\\\}}\\end{array}\\left(\\mathrm{\\\\\\\\\\}}\\Lambda+\\mathrm{\\\\it{\\mathcal{N}}}_{k}^{\\top}\\left(\\mathrm{\\Lambda}+\\mathrm{\\it{\\mathcal{N}}}_{k}\\right)\\int_{k}^{\\top}\\mathrm{\\{\\\\\\\\+}}^{-1}\\mathrm{\\\\\\\\+}^{-1}\\mathrm{\\\\\\\\+}^{-1}\\mathrm{\\\\\\+}^{-1}\\mathrm{\\\\\\\\-}}\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\mathrm{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\}\\\\ &{=\\mathrm{\\\\\\\\\\\\\\\\\\\\\\\\}}\\\\ &{\\mathrm\\ \\ \\ \\ \\ \\ \\ \\ \\ +\\mathrm{\\it{\\mathcal{N}}}_{k}^{\\top}\\left(\\mathrm{\\Lambda}+\\mathrm{\\it{\\mathcal{N}}}_{k}^{\\top}\\left(\\mathrm{\\Lambda}-\\mathrm{\\it{\\mathcal{N}}}_{k}\\right)\\right)\\left(\\mathrm{\\Lambda}-\\mathrm{\\\\\\it{\\mathcal{N}}}_{k}^{\\top}\\mathrm{\\\\\\\\+}^{-1}\\mathrm{\\\\\\+}^{-1}\\mathrm{\\\\\\+}^{-1}\\mathrm{\\\\+}^{-1}\\mathrm{\\\\+}^{-1}\\mathrm{\\\\+}^{-1}\\mathrm{\\\\\\\\-}}\\right)}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathrm{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\}+\\mathrm{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\}}\\\\ &{\\mathrm{\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\}+\\mathrm{\\\\\\\\\\\\\\\\}[\\mathrm{\\\\\\\\\\\\\\\\\\}}\\\\ &{=\\mathrm{\\\\\\\\\\\\\\}}\\\\ &{=\\mathrm{\\\\\\\\\\\\}}\\end{\\mathrm{\\\\\\\\\\\\\\\\\\\\\\\\\\}}[\\mathrm{\\ \n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Congregating all the second derivatives from Eq. (93) and Eq. (94) into the Hessian $H(\\gamma)$ in Eq. (90), we finally obtain ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{H}(\\gamma)=\\pi_{0}\\pi_{1}\\left[\\begin{array}{c c c c}{1}&{0}&{0}&{0}\\\\ {0}&{2(p+q-1)(1+2w|w|)}&{0}&{0}\\\\ {0}&{0}&{0}&{0}\\\\ {0}&{0}&{0}&{0}\\end{array}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "which is identical to the Hessian obtained in the proof of Thm. 7 (App. J.2) for $e=0$ .Thus it follows that $\\Gamma_{\\mathrm{min}}(p,q)\\subseteq\\,\\mathbb{R}^{4}$ and $\\mathbf{T}_{\\mathrm{sad}}\\subseteq\\mathbb{R}^{4}$ defined below are a set of local minima and saddle points respectively: ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{T}_{\\operatorname*{min}}(p,q)\\triangleq\\left\\{\\gamma_{\\operatorname*{min}}=(e,w,b,a)\\in\\mathbb{R}^{4}:e=0,(p+q-1)(1+2w|w|)>0,b=\\log\\frac{p}{q}\\right\\},}\\\\ &{\\mathbf{T}_{\\operatorname*{sad}}(p,q)\\triangleq\\left\\{\\gamma_{\\mathrm{sad}}=(e,w,b,a)\\in\\mathbb{R}^{4}:e=0,(p+q-1)(1+2w|w|)\\leq0,b=\\log\\frac{p}{q}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Now we focus on the remaining set of critical points (ii) Stationary points with $\\mathbb{E}_{X}\\left[f_{1}X+f_{2}\\right]=0,e\\neq0,1+a e^{2}=0,1+2w|w|=0.$ For this set of pintsthHianne ,, ,  do not exist (Eq. (91)). This nonexistence arises since sig $\\mathrm{n}\\left(1+a e^{2}\\right)$ is undefine $1+a e^{2}=0$ . However, even in this scenario,when $e\\neq0,1+a e^{2}=0,1+2\\dot{w}|w|=0$ we have $z_{1}=z_{2}=b$ . Hence, ", "page_idx": 56}, {"type": "text", "text": "", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{l}{f_{1}=\\sigma(b)+p+q-1-\\sigma(b)=p+q-1,}\\\\ {f_{2}=\\sigma(b)-p.}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Thus the expectation term $\\mathbb{E}_{X}\\left[f_{1}X+f_{2}\\right]=\\left(p+q-1\\right)\\mathbb{E}_{X}\\left[X\\right]+\\sigma(b)-p=\\left(p+q-1\\right)\\pi_{1}+$ $\\sigma(b)-p=\\overline{{0}}$ Simplitying, $\\begin{array}{r}{\\sigma(b)=\\frac{p}{p+q}}\\end{array}$ .whihimplies $b=\\log\\frac{p}{q}$ ", "page_idx": 56}, {"type": "text", "text": "We could attempt to understand the characterization of the points on this manifold through local perturbation analysis. However, in this work, we classify them as stationary points and leave the comprehensive characterization for future research. This set of points $\\Gamma_{\\mathrm{station}}(\\bar{p},q)\\subseteq\\mathbb{R}^{4}$ isdefined as ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{I}_{\\mathrm{station}}(p,q)\\triangleq\\left\\{\\gamma_{\\mathrm{min}}=(e,w,b,a)\\in\\mathbb{R}^{4}:e\\neq0,1+a e^{2}=0,1+2w|w|=0,b=\\log\\frac{p}{q}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 56}, {"type": "text", "text": "N.3 Proof of Lemma 14 ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Proof. Recall from Lemma 13 that for $\\pmb{\\theta}=(e,w,a)\\in\\mathbb{R}^{3}$ ,we have ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial{\\cal L}}{\\partial e}=-\\mathbb{E}\\left[(f_{1}X+f_{2})\\left(X-\\frac{1}{2}\\right)\\right]\\cdot2e\\left(1+a e^{2}\\right)\\left(1+2w|w|\\right)}\\\\ {\\displaystyle\\qquad-\\mathbb{E}\\left[(f_{1}X+f_{2})\\left(X-\\frac{1}{2}\\right)\\right]\\cdot2e^{3}a\\left(1+2w|w|\\right),}\\\\ {\\displaystyle\\frac{\\partial{\\cal L}}{\\partial w}=-\\mathbb{E}\\left[(f_{1}X+f_{2})\\left(X-\\frac{1}{2}\\right)\\right]\\cdot2e^{2}\\left(1+a e^{2}\\right)\\left(|w|+\\mathrm{sign}\\left(w\\right)w\\right),}\\\\ {\\displaystyle\\frac{\\partial{\\cal L}}{\\partial a}=-\\mathbb{E}\\left[(f_{1}X+f_{2})\\left(X-\\frac{1}{2}\\right)\\right]\\cdot e^{4}\\left(1+2w|w|\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "where $X\\in\\{0,1\\}$ is a Bernoulli random variable with $X\\sim\\operatorname{Bern}(p/(p+q))$ , and ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{1}\\triangleq1-p-q-\\phi_{1}+\\phi_{0},\\quad f_{2}\\triangleq p-\\phi_{0},}\\\\ &{\\phi_{1}\\triangleq\\sigma\\left(e^{2}\\left(\\cfrac{1}{2}\\left(1+a e^{2}\\right)\\left(1+2w|w|\\right)+w|w(1+a e^{2})|\\right)+b_{\\star}\\right),}\\\\ &{\\phi_{0}\\triangleq\\sigma\\left(e^{2}\\left(\\cfrac{-1}{2}\\left(1+a e^{2}\\right)\\left(1+2w|w|\\right)+w|w(1+a e^{2})|\\right)+b_{\\star}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "where the optimal bias $b_{\\star}$ is obtained by solving $\\pi_{1}f_{1}+f_{2}=0$ . Using the definition of the gradient flow that $\\dot{\\pmb{\\theta}}=-\\nabla L(\\pmb{\\theta})$ for $\\pmb\\theta=\\pmb\\theta_{t}$ ,wehave ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\dot{w}=-\\frac{L(\\theta)}{\\partial w}=\\mathbb{E}\\left[\\left(f_{1}X+f_{2}\\right)\\left(X-\\displaystyle\\frac12\\right)\\right]\\cdot2e^{2}\\left(1+a e^{2}\\right)\\left(|w|+\\mathrm{sign}\\left(w\\right)w\\right)}\\\\ {\\displaystyle\\Rightarrow\\frac{\\dot{w}}{e^{2}\\left(|w|+\\mathrm{sign}\\left(w\\right)w\\right)}=\\mathbb{E}\\left[\\left(f_{1}X+f_{2}\\right)\\left(X-\\displaystyle\\frac12\\right)\\right]2\\left(1+a e^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Similarly for $a$ ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\dot{a}=-\\displaystyle\\frac{L(\\pmb{\\theta})}{\\partial a}=\\mathbb{E}\\left[\\left(f_{1}X+f_{2}\\right)\\left(X-\\displaystyle\\frac{1}{2}\\right)\\right]e^{4}\\left(1+2w|w|\\right)}}\\\\ {{\\Rightarrow\\displaystyle\\frac{\\dot{a}}{e^{4}}=\\mathbb{E}\\left[\\left(f_{1}X+f_{2}\\right)\\left(X-\\displaystyle\\frac{1}{2}\\right)\\right]\\left(1+2w|w|\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Likewise, for $e$ ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\dot{e}=-\\displaystyle\\frac{L(\\pmb{\\theta})}{\\partial e}=\\mathbb{E}\\left[\\left(f_{1}{\\cal X}+f_{2}\\right)\\left({\\cal X}-\\displaystyle\\frac{1}{2}\\right)\\right]2\\left(1+a e^{2}\\right)e\\left(1+2w|w|\\right)}\\\\ {\\displaystyle\\qquad\\qquad+\\,\\mathbb{E}\\left[\\left(f_{1}{\\cal X}+f_{2}\\right)\\left({\\cal X}-\\displaystyle\\frac{1}{2}\\right)\\right]\\left(1+2w|w|\\right)2e^{3}a.}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "By substituting the expressions of Eq. (95) and Eq. (96) into Eq. (97): ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\dot{e}=\\underbrace{\\mathbb{E}\\left[\\left(f_{1}X+f_{2}\\right)\\left(X-\\frac{1}{2}\\right)\\right]2\\left(1+a e^{2}\\right)}_{E q.\\;(95)}e\\left(1+2w|w|\\right)}\\\\ &{\\quad+\\underbrace{\\mathbb{E}\\left[\\left(f_{1}X+f_{2}\\right)\\left(X-\\frac{1}{2}\\right)\\right]\\left(1+2w|w|\\right)2e^{3}a.}_{E q.\\;(96)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Thus, we obtain ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\dot{e}=\\frac{\\dot{w}}{2e^{2}\\left(|w|+\\mathrm{sign}\\left(w\\right)w\\right)}e\\left(1+2w|w|\\right)+\\frac{\\dot{a}}{e^{4}}2e^{3}a.\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "On rearranging and simplifying: ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{e\\dot{e}=\\displaystyle\\frac{\\dot{w}}{\\left(|w|+\\mathrm{sign}\\left(w\\right)w\\right)}\\left(1+2w|w|\\right)+\\dot{a}2a}}\\\\ {{\\Rightarrow e\\dot{e}=\\displaystyle\\frac{\\dot{w}}{2\\left(|w|\\right)}\\left(1+2w|w|\\right)+2a\\dot{a}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Integrating the above equation on both sides: ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\int e\\dot{e}=\\int\\frac{\\dot{w}}{4\\,(|w|)}\\left(1+2w|w|\\right)+\\int2a\\dot{a}}\\\\ {\\Rightarrow\\displaystyle\\frac{e^{2}(t)}{2}=\\frac{\\mathrm{sign}(w(t))\\cdot\\log|w(t)|+w(t)^{2}}{2}+a(t)^{2}+\\frac{c}{2}}\\\\ {\\Rightarrow e^{2}(t)=\\mathrm{sign}(w(t))\\cdot\\log|w(t)|+w(t)^{2}+2a(t)^{2}+c.}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "Note that here $c\\in\\mathbb R$ , is a constant that depends on the initial conditions. Thus the energy $\\mathcal{E}(\\pmb{\\theta}_{t})=$ $\\mathcal{E}(\\pmb{\\theta}_{0})$ for $w_{0}\\neq0$ ", "page_idx": 57}, {"type": "text", "text": "N.4 Proofs of Lemma 16, Lemma 15, and Thm.3 ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Proof. We note that the proofs of Lemma 16, Lemma 15 directly follow from that of their counterparts Lemma 11 and Lemma 10 using Lojasiewicz's theorem to characterize the convergence of the gradient flow. Thm. 3 is a direct consequence of Lemma 16, Lemma 15. \u53e3 ", "page_idx": 58}, {"type": "text", "text": "N.5  Informal proof of Thm. 11 ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Proof.[Informal] For $\\pmb{\\theta}=(e,w,a)$ , recall from Lemma 13 that the derivative of the loss $L$ with respectto $e$ is ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\frac{\\partial L}{\\partial e}=\\mathbb{E}_{X}\\left[\\left(f_{1}X+f_{2}\\right)\\left(X-\\displaystyle\\frac{1}{2}\\right)\\right]2e\\left(1+a e^{2}\\right)\\left(1+2w|w|\\right)}\\\\ {\\displaystyle+\\mathbb{E}_{X}\\left[\\left(f_{1}X+f_{2}\\right)\\left(X-\\displaystyle\\frac{1}{2}\\right)\\right]2e^{3}a\\left(1+2w|w|\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "where ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{f_{1}=\\sigma(z_{1})-\\sigma(z_{2})+p+q-1,}}\\\\ {{f_{2}=\\sigma(z_{2})-p,}}\\\\ {{z_{1}\\triangleq e^{2}\\left[\\displaystyle\\frac{1}{2}\\left(1+a e^{2}\\right)(1+2w|w|)+w|w(1+a e^{2})|\\right]+b,}}\\\\ {{z_{2}\\triangleq e^{2}\\left[\\displaystyle\\frac{-1}{2}\\left(1+a e^{2}\\right)(1+2w|w|)+w|w(1+a e^{2})|\\right]+b.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Assuming the initialization is very small, making any product of quantities in $\\pmb{\\theta}=(e,w,a,b)$ much smaller than the individual quantities. Therefore, we can consider these products to be approximately zero. That is, $\\forall x,y\\in\\pmb{\\theta},x\\ \\stackrel{+}{\\geq}x y\\ \\&\\ y\\geq x y\\ \\&\\ x y\\approx0.$ Hence, ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{l}{z_{1}=b,}\\\\ {z_{2}=b,}\\\\ {f_{1}=p+q-1,}\\\\ {f_{2}=\\sigma(b)-p.}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Hence the gradient with respect to $e$ is ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\frac{\\partial L}{\\partial e}=2\\mathbb{E}_{X}\\left[\\left(f_{1}X+f_{2}\\right)\\left(X\\right)\\right]e.\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Simplifying the expectation term, $\\operatorname{\\mathbb{E}}_{X}\\left[\\left(f_{1}X+f_{2}\\right)X\\right]=(f_{1}+f_{2})\\pi_{1}=f_{1}\\pi_{1}-f_{1}\\pi^{2}=(p+q-$ $1)(\\bar{\\pi_{1}}-\\bar{\\pi_{1}^{2})}$ , where we used the fact that $b$ is optimal in the above equations, specifically where $f_{1}\\pi_{1}+f_{2}=0$ . Thus the gradient fow for the parameter $e$ is governed by ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\dot{e}=-\\frac{\\partial L}{\\partial e}=-(p+q-1)(\\pi_{1}-\\pi_{1}^{2})e\\Rightarrow e=e_{0}\\exp(-(p+q-1)(\\pi_{1}-\\pi_{1}^{2})t).\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Since $(p+q-1)(\\pi_{1}-\\pi_{1}^{2})>0.$ $e\\to0$ which denotes it converges to the local minima. ", "page_idx": 58}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 59}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 59}, {"type": "text", "text": "\u00b7 You should answer [Yes] , [No] , or [NA] .   \n\u00b7 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u00b7 Please provide a short (1-2 sentence) justification right after your answer (even for NA). ", "page_idx": 59}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 59}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 59}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 59}, {"type": "text", "text": "\u00b7 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u00b7 Keep the checklist subsection headings, questions/answers and guidelines below. \u00b7 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 59}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 59}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Justification: All claims are in the form of theorems that are proved in Sections 3 and 4 and experimental results in Sec. 5.2, for which our code is made available for reproducibility. ", "page_idx": 59}, {"type": "text", "text": "Guidelines: ", "page_idx": 59}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 59}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 59}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 59}, {"type": "text", "text": "Justification: Though we do not have an explicit section for limitations, we outline the shortcomings in our approach in the conclusion and list them as future direction. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 60}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 60}, {"type": "text", "text": "Justification: All theorems are stated with required assumptions and proofs are provided either in the appendix or the main paper. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 60}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 60}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 60}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 60}, {"type": "text", "text": "Justification: We provide all our code as open access and describe the experimental setup in App. I. ", "page_idx": 60}, {"type": "text", "text": "Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPs does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 61}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 61}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Justification: We provide open access to the code, available at https : / /anonymous . 4open.   \nScience/r/Local-to-Global-C70B/. ", "page_idx": 61}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ", "page_idx": 61}, {"type": "text", "text": "\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 62}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 62}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 62}, {"type": "text", "text": "Justification: All experiment details are in App. 1. ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 62}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 62}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 62}, {"type": "text", "text": "Justification: All experiments are repeated 5 times. ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 62}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 62}, {"type": "text", "text": "Answer: [No] ", "page_idx": 62}, {"type": "text", "text": "Justification: These are unrelated to the results of the paper. ", "page_idx": 62}, {"type": "text", "text": "Guidelines: \u00b7 The answer NA means that the paper does not include experiments. ", "page_idx": 62}, {"type": "text", "text": "\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 63}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 63}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 63}, {"type": "text", "text": "Justification: All ethical practices were followed. ", "page_idx": 63}, {"type": "text", "text": "Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 63}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 63}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 63}, {"type": "text", "text": "Justification: No direct societal impacts. ", "page_idx": 63}, {"type": "text", "text": "Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 63}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 63}, {"type": "text", "text": "Answer: [NA] Justification: No risk. ", "page_idx": 63}, {"type": "text", "text": "Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 64}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 64}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 64}, {"type": "text", "text": "Justification: We use code from [26] which is cited. ", "page_idx": 64}, {"type": "text", "text": "Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 64}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 64}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 64}, {"type": "text", "text": "Justification: No assets are releasd, but code is made publicly available ", "page_idx": 64}, {"type": "text", "text": "Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 64}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 64}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 65}, {"type": "text", "text": "Justification: No human subjects. ", "page_idx": 65}, {"type": "text", "text": "Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 65}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 65}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 65}, {"type": "text", "text": "Justification: No human subjects. ", "page_idx": 65}, {"type": "text", "text": "Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 65}]