[{"figure_path": "OX4yll3X53/tables/tables_35_1.jpg", "caption": "Table 1: Parameters in the transformer architecture with their shape.", "description": "This table lists the parameters used in the single-layer transformer architecture described in the paper, along with their corresponding matrix shapes.  The shapes are expressed in terms of the embedding dimension (d) and the sequence length (N).  This table provides a concise summary of the parameter dimensions for those familiar with transformer architectures.", "section": "Model architecture and hyper-parameters"}, {"figure_path": "OX4yll3X53/tables/tables_35_2.jpg", "caption": "Table 2: Settings and parameters for the transformer model used in the experiments.", "description": "This table details the hyperparameters and settings used for training the transformer model in the paper's experiments.  It includes information on the dataset used (k-th order binary Markov source), architecture (based on GPT-2), batch size, optimizer, learning rate, scheduler, number of training iterations, weight decay, dropout rate, sequence length, embedding dimension, number of transformer layers, number of attention heads, and the number of repetitions of the experiment.", "section": "Empirical Results"}]