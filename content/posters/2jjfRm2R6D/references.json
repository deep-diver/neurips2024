{"references": [{"fullname_first_author": "M. Artetxe", "paper_title": "Unsupervised neural machine translation", "publication_date": "2018", "reason": "This paper introduces a groundbreaking method for unsupervised neural machine translation, a crucial technique for addressing the scarcity of parallel data in multilingual settings, which is directly relevant to the autoformalization task tackled in the main paper."}, {"fullname_first_author": "Z. Azerbayev", "paper_title": "Proofnet: Autoformalizing and formally proving undergraduate-level mathematics", "publication_date": "2023-02-12", "reason": "This paper presents ProofNet, a benchmark dataset and approach for autoformalization, providing a direct comparison point and context for evaluating the methods proposed in the main paper."}, {"fullname_first_author": "G. Bancerek", "paper_title": "Automatic translation in formalized mathematics", "publication_date": "2006", "reason": "This paper offers insights into the challenges and methods of automated translation in the domain of formalized mathematics, providing historical context and highlighting the difficulties in creating large-scale datasets for this task."}, {"fullname_first_author": "M. Chen", "paper_title": "Evaluating large language models trained on code", "publication_date": "2021-07-03", "reason": "This paper offers insights into the capabilities and limitations of large language models (LLMs) trained on code, providing crucial background knowledge on the strengths and weaknesses of LLMs as the foundation of the method proposed in the main paper."}, {"fullname_first_author": "Y. Wu", "paper_title": "Autoformalization with large language models", "publication_date": "2022", "reason": "This paper explores the use of large language models for autoformalization, providing a direct comparison and contrast to the approach and findings presented in the main paper."}]}