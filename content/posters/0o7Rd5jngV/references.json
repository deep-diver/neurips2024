{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, which is the foundation of the research in this paper."}, {"fullname_first_author": "Colin Raffel", "paper_title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "publication_date": "2020-07-01", "reason": "This paper introduced the T5 model and its relative positional encoding scheme, which are crucial components of the theoretical analysis."}, {"fullname_first_author": "Weinan E", "paper_title": "A priori estimates of the population risk for two-layer neural networks", "publication_date": "2019-01-01", "reason": "This paper's theoretical framework on approximation rates for neural networks underpins the theoretical analysis of Transformers in this paper."}, {"fullname_first_author": "Zhong Li", "paper_title": "On the curse of memory in recurrent neural networks", "publication_date": "2021-01-01", "reason": "This paper highlights the limitations of recurrent neural networks in handling long-range dependencies, motivating the study of Transformers as an alternative."}, {"fullname_first_author": "Haotian Jiang", "paper_title": "Approximation theory of transformer networks for sequence modeling", "publication_date": "2023-01-01", "reason": "This paper, also focusing on approximation properties of Transformers, provides a comparative analysis and further insights that were used in this research."}]}