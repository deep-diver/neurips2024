[{"figure_path": "fHq4x2YXVv/tables/tables_4_1.jpg", "caption": "Table 1: Evaluating shape metrics versus scale metrics on allocating layerwise sparsities on LLMs. Shape metrics are obtained from the shapes of the ESDs. Scale metrics are norm-based metrics measuring the scale of weights matrices (which can also be obtained from the ESD). The results are conducted on LLaMA-7B at 70% sparsity. We show WikiText validation perplexity and average accuracy on seven different zero-shot tasks as evaluation metrics. We observe that shape metrics outperform scale metrics and PL_Alpha_Hill performs the best.", "description": "This table compares the performance of different metrics (shape vs. scale) for allocating layer-wise sparsity in LLMs.  It shows the WikiText perplexity and average accuracy across seven zero-shot tasks when using various metrics in conjunction with three different intra-layer pruning methods (Magnitude, Wanda, and SparseGPT).  The results demonstrate the superiority of shape metrics, particularly PL_Alpha_Hill, for guiding sparsity allocation.", "section": "3.4 Shape vs. scale metrics for sparsity allocation"}, {"figure_path": "fHq4x2YXVv/tables/tables_6_1.jpg", "caption": "Table 2: WikiText validation perplexity for pruned LLaMA and LLaMA-2 models at 70% sparsity. Our method (AlphaPruning) is compared to uniform layerwise sparsity and OWL, each paired with magnitude-based pruning, Wanda, and SparseGPT. Lower perplexity indicates improved model performance.", "description": "This table compares the WikiText validation perplexity of LLaMA and LLaMA-2 models pruned to 70% sparsity using different methods.  The methods compared include AlphaPruning (the proposed method), uniform layerwise sparsity, and OWL (a state-of-the-art non-uniform sparsity allocation method).  Each of these methods is combined with three different intra-layer pruning techniques: Magnitude, Wanda, and SparseGPT.  Lower perplexity scores indicate better model performance.", "section": "4.2 Main results"}, {"figure_path": "fHq4x2YXVv/tables/tables_6_2.jpg", "caption": "Table 2: WikiText validation perplexity for pruned LLaMA and LLaMA-2 models at 70% sparsity. Our method (AlphaPruning) is compared to uniform layerwise sparsity and OWL, each paired with magnitude-based pruning, Wanda, and SparseGPT. Lower perplexity indicates improved model performance.", "description": "This table shows the WikiText validation perplexity for different LLMs (LLaMA and LLaMA-2) pruned to 70% sparsity using different methods.  The methods compared are AlphaPruning (the proposed method), uniform layerwise sparsity, and OWL (a state-of-the-art non-uniform sparsity allocation method). Each of these sparsity allocation methods is combined with three different intra-layer pruning techniques: Magnitude, Wanda, and SparseGPT. Lower perplexity values indicate better performance.", "section": "4.2 Main results"}, {"figure_path": "fHq4x2YXVv/tables/tables_7_1.jpg", "caption": "Table 4: End-to-end decode latency and speedup of AlphaPruning measured on the DeepSparse inference engine.", "description": "This table presents the results of measuring the end-to-end decode latency and speedup of the AlphaPruning method on the DeepSparse inference engine. The results show the decode latency and speedup at various sparsity levels (from 10% to 90%). The speedup is calculated relative to the dense model (1.00x).  The table demonstrates that AlphaPruning achieves significant speedups at higher sparsity levels, reaching a 3.06x speedup at 80% sparsity.", "section": "4.2 Main results"}, {"figure_path": "fHq4x2YXVv/tables/tables_7_2.jpg", "caption": "Table 5: WikiText validation perplexity (\u2193) of more LLMs pruned by uniform sparsity and our method combined with Wanda.", "description": "This table compares the performance of AlphaPruning against uniform sparsity and OWL on various LLMs.  The perplexity (a measure of how well a model predicts text) is shown for different levels of sparsity (60%, 70%, 80%), for each of the models listed (LLaMA-V3-7B, Vicuna-7B, and Mistral-7B). Lower perplexity indicates better performance after pruning. The results demonstrate that AlphaPruning consistently outperforms the other methods, especially at higher sparsity levels.", "section": "4.3 Corroborating results"}, {"figure_path": "fHq4x2YXVv/tables/tables_7_3.jpg", "caption": "Table 5: WikiText validation perplexity (\u2193) of more LLMs pruned by uniform sparsity and our method combined with Wanda.", "description": "This table compares the WikiText validation perplexity and accuracy of several LLMs pruned using uniform sparsity and AlphaPruning, both combined with the Wanda pruning method.  It demonstrates the effectiveness of AlphaPruning across various model sizes and architectures, showing that it consistently improves perplexity (lower is better) and often increases accuracy (higher is better) at high sparsity levels (60%, 70%, 80%).  The results highlight the generalizability of AlphaPruning.", "section": "4.3 Corroborating results"}, {"figure_path": "fHq4x2YXVv/tables/tables_21_1.jpg", "caption": "Table 7: Comparing perplexity of sparse LLaMA-7B (sparsity=70%) pruned by four types of sparsity allocation method.", "description": "This table compares the perplexity results of LLaMA-7B pruned to 70% sparsity using four different sparsity allocation methods: Uniform, Per-matrix, Per-block, and Mixed.  The perplexity is measured using the WikiText validation set.  Each sparsity allocation method is combined with three different pruning methods: Magnitude, Wanda, and SparseGPT. The table highlights the superior performance of the proposed 'Mixed' method.", "section": "4.2 Main results"}, {"figure_path": "fHq4x2YXVv/tables/tables_22_1.jpg", "caption": "Table 8: Left: Hyperparameters setting for results in Section 4.2. We report the optimal \u03c4 after a small hyperparameter sweep within the range of \u03c4 \u2208 [0.2, 0.3, 0.4, 0.5]. Right: Hyperparameters setting for results in Section 4.3 for Vision Transformers at 40%, 50%, 60% sparsity. We report the optimal \u03c4 after a small hyperparameter sweep within the range of \u03c4 \u2208 [0.1, 0.2, 0.3, 0.4, 0.5].", "description": "This table shows the hyperparameter settings used in the experiments. The left part shows the optimal \u03c4 values used for the experiments in Section 4.2, which were obtained through a small hyperparameter sweep.  The right part shows the optimal \u03c4 values used for the experiments in Section 4.3 involving Vision Transformers, which were also obtained via a hyperparameter sweep. The experiments in section 4.2 involve language models, while the experiments in section 4.3 involve vision models, and the different hyperparameter ranges reflect differences in model architectures and tasks.", "section": "4.1 Experimental setup"}, {"figure_path": "fHq4x2YXVv/tables/tables_22_2.jpg", "caption": "Table 9: WikiText validation perplexity (\u2193) of LLaMA-7B pruned by different allocation methods at various global sparsities using Wanda. AlphaPruning outperforms other layerwise sparsity at high sparsity range.", "description": "This table presents the WikiText validation perplexity results for the LLaMA-7B model pruned using different layer-wise sparsity allocation methods at various global sparsity levels (10%, 20%, 30%, 40%, 50%, 60%, 70%, 80%).  The methods compared include Uniform, Global, LAMP, ER, ER-Plus, OWL, and the proposed AlphaPruning method. Lower perplexity values indicate better performance. The table highlights that AlphaPruning consistently achieves lower perplexity than other methods, particularly at higher sparsity levels.", "section": "4.2 Main results"}, {"figure_path": "fHq4x2YXVv/tables/tables_23_1.jpg", "caption": "Table 2: WikiText validation perplexity for pruned LLaMA and LLaMA-2 models at 70% sparsity. Our method (AlphaPruning) is compared to uniform layerwise sparsity and OWL, each paired with magnitude-based pruning, Wanda, and SparseGPT. Lower perplexity indicates improved model performance.", "description": "This table compares the WikiText validation perplexity of pruned LLaMA and LLaMA-2 models at 70% sparsity using different methods.  The methods compared include AlphaPruning (the proposed method), uniform layerwise sparsity, and OWL.  Each of these is combined with three different intra-layer pruning techniques (Magnitude, Wanda, and SparseGPT). Lower perplexity values indicate better model performance after pruning.", "section": "4.2 Main results"}, {"figure_path": "fHq4x2YXVv/tables/tables_23_2.jpg", "caption": "Table 1: Evaluating shape metrics versus scale metrics on allocating layerwise sparsities on LLMs. Shape metrics are obtained from the shapes of the ESDs. Scale metrics are norm-based metrics measuring the scale of weights matrices (which can also be obtained from the ESD). The results are conducted on LLaMA-7B at 70% sparsity. We show WikiText validation perplexity and average accuracy on seven different zero-shot tasks as evaluation metrics. We observe that shape metrics outperform scale metrics and PL_Alpha_Hill performs the best.", "description": "This table compares the performance of various heavy-tailed self-regularization (HT-SR) metrics for allocating layer-wise sparsity in LLMs.  It uses WikiText perplexity and average accuracy across seven zero-shot tasks to evaluate the effectiveness of different metrics when pruning the LLaMA-7B model to 70% sparsity.  The results highlight that shape metrics generally outperform scale metrics, with PL_Alpha_Hill showing the best performance.", "section": "3.4 Shape vs. scale metrics for sparsity allocation"}, {"figure_path": "fHq4x2YXVv/tables/tables_23_3.jpg", "caption": "Table 1: Evaluating shape metrics versus scale metrics on allocating layerwise sparsities on LLMs. Shape metrics are obtained from the shapes of the ESDs. Scale metrics are norm-based metrics measuring the scale of weights matrices (which can also be obtained from the ESD). The results are conducted on LLaMA-7B at 70% sparsity. We show WikiText validation perplexity and average accuracy on seven different zero-shot tasks as evaluation metrics. We observe that shape metrics outperform scale metrics and PL_Alpha_Hill performs the best.", "description": "This table compares the performance of various heavy-tailed self-regularization (HT-SR) metrics for guiding layer-wise sparsity allocation in Large Language Models (LLMs).  It uses WikiText perplexity and average accuracy across seven zero-shot tasks to evaluate the effectiveness of different metrics (shape vs. scale).  The results demonstrate that shape metrics significantly outperform scale metrics in assigning layer-wise sparsity, with PL_Alpha_Hill showing the best performance.", "section": "3.4 Shape vs. scale metrics for sparsity allocation"}, {"figure_path": "fHq4x2YXVv/tables/tables_24_1.jpg", "caption": "Table 1: Evaluating shape metrics versus scale metrics on allocating layerwise sparsities on LLMs. Shape metrics are obtained from the shapes of the ESDs. Scale metrics are norm-based metrics measuring the scale of weights matrices (which can also be obtained from the ESD). The results are conducted on LLaMA-7B at 70% sparsity. We show WikiText validation perplexity and average accuracy on seven different zero-shot tasks as evaluation metrics. We observe that shape metrics outperform scale metrics and PL_Alpha_Hill performs the best.", "description": "This table compares the performance of various heavy-tailed self-regularization (HT-SR) metrics in assigning layer-wise sparsity for Large Language Models (LLMs). It uses WikiText perplexity and accuracy across seven zero-shot tasks to evaluate the performance of LLaMA-7B model pruned to 70% sparsity using different metrics. The results demonstrate that shape metrics generally outperform scale metrics, with PL_Alpha_Hill being the most effective metric.", "section": "Shape vs. scale metrics for sparsity allocation"}, {"figure_path": "fHq4x2YXVv/tables/tables_24_2.jpg", "caption": "Table 14: Increasing the minimum sparsity of our method, while maintaining a global sparsity of 70%, still yields performance improvements compared to a uniform sparsity ratio. We present the WikiText validation perplexity for LLaMA-7B pruned by both the Uniform method and our method, in conjunction with Wanda.", "description": "This table shows the results of adjusting the minimum sparsity while maintaining the global sparsity at 70%.  It compares the WikiText validation perplexity achieved by the AlphaPruning method with different minimum sparsity levels (50%, 55%, 57%, 60%, 65%, 67%) to the Uniform method's perplexity. The results highlight that AlphaPruning consistently achieves lower perplexity even when the minimum sparsity is increased, demonstrating its robustness and effectiveness in controlling the minimum layer sparsity for memory-limited hardware.", "section": "4.2 Main results"}, {"figure_path": "fHq4x2YXVv/tables/tables_25_1.jpg", "caption": "Table 1: Evaluating shape metrics versus scale metrics on allocating layerwise sparsities on LLMs. Shape metrics are obtained from the shapes of the ESDs. Scale metrics are norm-based metrics measuring the scale of weights matrices (which can also be obtained from the ESD). The results are conducted on LLaMA-7B at 70% sparsity. We show WikiText validation perplexity and average accuracy on seven different zero-shot tasks as evaluation metrics. We observe that shape metrics outperform scale metrics and PL_Alpha_Hill performs the best.", "description": "This table compares different metrics for allocating layerwise sparsity in LLMs. It shows that shape metrics, which describe the shape of the ESDs of the weight matrices, outperform scale metrics, which measure the size of the ESDs.  The PL_Alpha_Hill metric performs the best.  The experiment is conducted on the LLaMA-7B model with 70% sparsity, measuring both WikiText validation perplexity and average accuracy across seven zero-shot tasks. ", "section": "3.4 Shape vs. scale metrics for sparsity allocation"}, {"figure_path": "fHq4x2YXVv/tables/tables_25_2.jpg", "caption": "Table 16: The perplexity of OPT models pruned by uniform sparsity and our method combined with magnitude pruning. The perplexity is evaluated on WikiText validation set.", "description": "This table compares the perplexity results of OPT models (OPT-125M, OPT-350M, OPT-2.7B, and OPT-6.7B) pruned using two methods: uniform sparsity and the proposed AlphaPruning method combined with magnitude-based pruning.  The perplexity scores are provided for 40% and 50% sparsity levels. Lower perplexity values indicate better model performance after pruning.", "section": "4.2 Main results"}, {"figure_path": "fHq4x2YXVv/tables/tables_25_3.jpg", "caption": "Table 17: The perplexity of OPT models pruned by uniform sparsity and our method combined with Wanda and SparseGPT. The perplexity is evaluated on WikiText validation set.", "description": "This table compares the perplexity of OPT models (OPT-125M, OPT-350M, OPT-2.7B, OPT-6.7B) pruned using two different methods: uniform sparsity and the proposed AlphaPruning method combined with Wanda and SparseGPT.  For each model, perplexity is shown at 70% sparsity. The results demonstrate that AlphaPruning, when combined with Wanda and SparseGPT, generally yields lower perplexity scores compared to using a uniform sparsity allocation strategy, indicating that the proposed method is more effective at preserving model performance during pruning.", "section": "4.2 Main results"}, {"figure_path": "fHq4x2YXVv/tables/tables_25_4.jpg", "caption": "Table 18: WikiText validation perplexity of pruned LLaMA-7B in Mixed N:8 sparsity configuration. The results are shown with Wanda and our non-uniform layerwise sparsity. Ours can lead to performance improvement at various sparsity levels.", "description": "This table compares the performance of different sparsity allocation methods for pruning LLaMA-7B using the Wanda pruning technique. The methods compared are uniform sparsity, and the proposed AlphaPruning method which uses a mixed N:8 sparsity strategy. The results are reported for different sparsity levels (4:8, 3:8, 2:8) and indicate the WikiText validation perplexity for each scenario. The table demonstrates that AlphaPruning outperforms uniform sparsity allocation across various sparsity levels, leading to significant performance improvements.", "section": "More results on semi-structured and structured pruning"}, {"figure_path": "fHq4x2YXVv/tables/tables_26_1.jpg", "caption": "Table 19: Applying AlphaPruning to structured pruning method LLM-Pruner. The results are shown in WikiText validation perplexity of pruned LLaMA-7B at various sparsity levels.", "description": "This table compares the performance of applying AlphaPruning with LLM Pruner on the WikiText and Penn Treebank datasets at different sparsity levels (20%, 40%, 60%, 80%). It shows that AlphaPruning consistently improves the model's performance in terms of lower perplexity across various sparsity levels when compared against uniform pruning strategies.", "section": "4.3 Corroborating results"}, {"figure_path": "fHq4x2YXVv/tables/tables_26_2.jpg", "caption": "Table 2: WikiText validation perplexity for pruned LLaMA and LLaMA-2 models at 70% sparsity. Our method (AlphaPruning) is compared to uniform layerwise sparsity and OWL, each paired with magnitude-based pruning, Wanda, and SparseGPT. Lower perplexity indicates improved model performance.", "description": "This table compares the WikiText validation perplexity of pruned LLaMA and LLaMA-2 models at 70% sparsity using different layerwise sparsity allocation methods.  The methods compared are AlphaPruning (the proposed method), uniform layerwise sparsity, and OWL (a state-of-the-art non-uniform method).  Each of these sparsity allocation methods was combined with three different intra-layer pruning techniques: Magnitude, Wanda, and SparseGPT. Lower perplexity values indicate better model performance after pruning.", "section": "4.2 Main results"}, {"figure_path": "fHq4x2YXVv/tables/tables_27_1.jpg", "caption": "Table 2: WikiText validation perplexity for pruned LLaMA and LLaMA-2 models at 70% sparsity. Our method (AlphaPruning) is compared to uniform layerwise sparsity and OWL, each paired with magnitude-based pruning, Wanda, and SparseGPT. Lower perplexity indicates improved model performance.", "description": "This table compares the WikiText validation perplexity of pruned LLaMA and LLaMA-2 models at 70% sparsity using different methods.  The methods compared are AlphaPruning (the proposed method), uniform layerwise sparsity, and OWL (a state-of-the-art non-uniform sparsity allocation method).  Each of these methods is combined with three different intra-layer pruning techniques (Magnitude, Wanda, and SparseGPT) to thoroughly evaluate their effectiveness. Lower perplexity values indicate better model performance.", "section": "4.2 Main results"}, {"figure_path": "fHq4x2YXVv/tables/tables_27_2.jpg", "caption": "Table 1: Evaluating shape metrics versus scale metrics on allocating layerwise sparsities on LLMs. Shape metrics are obtained from the shapes of the ESDs. Scale metrics are norm-based metrics measuring the scale of weights matrices (which can also be obtained from the ESD). The results are conducted on LLaMA-7B at 70% sparsity. We show WikiText validation perplexity and average accuracy on seven different zero-shot tasks as evaluation metrics. We observe that shape metrics outperform scale metrics and PL_Alpha_Hill performs the best.", "description": "This table compares the performance of different heavy-tailed self-regularization (HT-SR) metrics for allocating layer-wise sparsity in large language models (LLMs).  It shows that shape metrics, which capture the shape of the empirical spectral density (ESD) of weight matrices, generally outperform scale metrics (e.g., matrix norms) in determining layer importance for pruning.  The best performing shape metric is PL_Alpha_Hill. The experiment uses LLaMA-7B model with 70% sparsity, and it is evaluated using WikiText perplexity and accuracy across seven zero-shot tasks.", "section": "3.4 Shape vs. scale metrics for sparsity allocation"}, {"figure_path": "fHq4x2YXVv/tables/tables_28_1.jpg", "caption": "Table 1: Evaluating shape metrics versus scale metrics on allocating layerwise sparsities on LLMs. Shape metrics are obtained from the shapes of the ESDs. Scale metrics are norm-based metrics measuring the scale of weights matrices (which can also be obtained from the ESD). The results are conducted on LLaMA-7B at 70% sparsity. We show WikiText validation perplexity and average accuracy on seven different zero-shot tasks as evaluation metrics. We observe that shape metrics outperform scale metrics and PL_Alpha_Hill performs the best.", "description": "This table compares the performance of different heavy-tailed self-regularization (HT-SR) metrics for allocating layer-wise sparsity in large language models (LLMs).  It shows WikiText perplexity and average accuracy across seven zero-shot tasks when pruning a LLaMA-7B model to 70% sparsity using various metrics in conjunction with different intra-layer pruning methods. The results demonstrate that shape metrics generally outperform scale metrics, with PL_Alpha_Hill being the most effective.", "section": "3.4 Shape vs. scale metrics for sparsity allocation"}, {"figure_path": "fHq4x2YXVv/tables/tables_29_1.jpg", "caption": "Table 2: WikiText validation perplexity for pruned LLaMA and LLaMA-2 models at 70% sparsity. Our method (AlphaPruning) is compared to uniform layerwise sparsity and OWL, each paired with magnitude-based pruning, Wanda, and SparseGPT. Lower perplexity indicates improved model performance.", "description": "This table presents the WikiText validation perplexity results for LLaMA and LLaMA-2 models after pruning to 70% sparsity using different methods.  The table compares the performance of AlphaPruning against uniform sparsity and OWL (a state-of-the-art non-uniform sparsity allocation method) combined with three different intra-layer pruning techniques: Magnitude, Wanda, and SparseGPT. Lower perplexity values indicate better model performance after pruning.", "section": "4.2 Main results"}]