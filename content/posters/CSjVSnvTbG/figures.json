[{"figure_path": "CSjVSnvTbG/figures/figures_8_1.jpg", "caption": "Figure 1: Estimated transport cost as a function k, for various choices of m and \u03b7 = 1.", "description": "This figure shows the estimated transport cost as a function of the number of iterations (k) for various choices of the parameter 'm' and a fixed value of \u03b7=1.  The plots (a) and (b) represent the results from two different algorithms, SVI (Sinkhorn Value Iteration) and SPI (Sinkhorn Policy Iteration), respectively.  The different lines in each plot represent different values of 'm', illustrating the effect of this parameter on the convergence rate and final accuracy of the algorithms.", "section": "Experiments"}, {"figure_path": "CSjVSnvTbG/figures/figures_8_2.jpg", "caption": "Figure 2: Visual representation of the distances computed between the chains Mx and My.", "description": "This figure visually represents the distances computed between two Markov chains, Mx and My.  Chain Mx is a 3x3 grid world with a starting state (s0) in the upper left corner and a reward of +1 in the lower left corner. Chain My is a 4-room environment with the same reward placement but a different arrangement. The color intensity in the third image represents the optimal transport distance between the initial state of Mx and each state in My.  Darker colors indicate larger distances, showing how the distances reflect the structural similarities and symmetries between the two Markov chains.", "section": "Experiments"}, {"figure_path": "CSjVSnvTbG/figures/figures_33_1.jpg", "caption": "Figure 3: Error of estimated transport cost as a function k, for various choices of \u03b7.", "description": "This figure shows the impact of the regularization parameter \u03b7 on the performance of both SVI and SPI algorithms.  The plots show the estimation error of the transport cost as a function of the number of iterations for various values of \u03b7 (10/\u221aK, 0.1, 1, and 10). The results indicate that larger \u03b7 values lead to faster initial convergence, but eventually prevent the algorithm from reaching the optimal solution. Smaller \u03b7 values lead to better solutions at the cost of slower initial convergence.  A time-dependent learning rate schedule (\u03b7k ~ 1/\u221ak) is shown to achieve the best performance.", "section": "Additional experimental results"}, {"figure_path": "CSjVSnvTbG/figures/figures_34_1.jpg", "caption": "Figure 4: Comparison of the computational time of the different methods proposed to obtain a near-optimal solution for different values of \u03b3. For each Markov chain size, the results obtained in 5 randomly generated instances are compared, showing the standard deviation in the plot. Data is displayed on a log-log scale.", "description": "This figure compares the computation time of five different methods for calculating optimal transport distances between Markov chains, varying the discount factor (\u03b3) and the size of the Markov chains.  The methods compared include the authors' Sinkhorn Value Iteration (SVI) and Sinkhorn Policy Iteration (SPI),  alongside two existing methods (EntropicOTC and dWL).  The plot shows that SVI and SPI are consistently faster than the other methods, especially when \u03b3 is large, highlighting their efficiency for computing optimal transport distances between Markov chains.", "section": "Experiments"}, {"figure_path": "CSjVSnvTbG/figures/figures_35_1.jpg", "caption": "Figure 5: Result of applying MDS to the pairwise distances between the set of 4-room instances studied. On the plot, the first two coordinates of the MDS embedding are used as the spatial coordinates, and the third coordinate is encoded via the color bar provided on the left-hand side of the axes. It can be observed how the elements in the same cluster present common features that differentiate them from those in another cluster. In the examples shown in the figure we can see how the instances in which the closest reward involves crossing a door are concentrated in one cluster, while the instances in which the reward and the initial state are located in the same room belong to a different cluster. The remaining clusters correspond to having to cross two doors for a reward (set of green points on the top), or having no reward that is accessible from the initial state (set of blue points in the middle, with large negative z-coordinates).", "description": "This figure shows the result of applying multidimensional scaling (MDS) to the pairwise distances between a set of 35 4-room instances (Markov chains). Each instance has a different initial state and obstacle positions but a fixed reward function. The resulting MDS plot reveals clusters of instances with similar behaviors, illustrating how the optimal transport distances capture similarities and symmetries.", "section": "F.3 Optimal transport distances as similarity metrics"}]