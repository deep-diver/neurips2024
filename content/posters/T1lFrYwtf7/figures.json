[{"figure_path": "T1lFrYwtf7/figures/figures_0_1.jpg", "caption": "Figure 1: Effect of paraphrasing data in knowledge injection.", "description": "This figure compares the F1 scores of different methods on three question answering datasets (StreamingQA, SQUAD, ArchivalQA).  The methods compared include standard fine-tuning (FT), fine-tuning with paraphrases (FT + Paraphrases), the proposed LaPael method (Ours), and the combination of LaPael and paraphrases (Ours + Para.). The results show that adding paraphrases enhances knowledge injection, and that LaPael further improves the performance, with the combination of LaPael and paraphrases yielding the best results. This highlights the effectiveness of data augmentation and the latent paraphrasing approach.", "section": "1 Introduction"}, {"figure_path": "T1lFrYwtf7/figures/figures_1_1.jpg", "caption": "Figure 2: A conceptual illustration of the proposed approach. On the left, we show the existing method of knowledge injection by paraphrasing each document for data-level augmentation. On the right, we present the conceptual illustration of LaPael with trained latent paraphrasers. Unlike the method on the left, LaPael can eliminate the need for users to repeatedly paraphrase using LLMs once latent paraphrasers are trained.", "description": "This figure illustrates the difference between the traditional data augmentation approach for knowledge injection and the proposed LaPael method.  The left side shows the existing method where each document is paraphrased repeatedly using LLMs for data augmentation, leading to high computational costs. The right side presents LaPael, which uses trained latent paraphrasers to augment the LLM directly at the latent level. This eliminates the need for repetitive external LLM usage for paraphrasing, significantly reducing computational costs.", "section": "4 Proposed Method"}, {"figure_path": "T1lFrYwtf7/figures/figures_3_1.jpg", "caption": "Figure 3: (a) Illustration of the latent paraphraser. The linear layer embeds each token's latent feature h into \u03bc. We then sample stochastic noise \u03b1 from N(\u03bc, I) and apply a mask mt to control the scale. (b) Training pipeline of LaPael. To train the latent paraphraser, we estimate the parameters of Gaussian distributions. We then minimize the KL divergence between these distributions to optimize the latent paraphrasers.", "description": "This figure illustrates the architecture and training process of the Latent Paraphraser (LaPael).  (a) shows how the latent paraphraser, a neural network module, takes a token's latent feature (h) as input, generates a noise vector (z) sampled from a Gaussian distribution N(\u03bc, I), and applies a learned mask (m) to control the scale of the noise added to the original latent feature, producing a modified latent feature. (b) depicts the training procedure. Two Gaussian distributions are estimated: one for the latent features processed by the latent paraphraser, and one for the original latent features. The parameters of the latent paraphraser are trained to minimize the Kullback-Leibler (KL) divergence between these distributions, ensuring that the paraphrased and original sentences have similar latent representations.", "section": "4.2 Introducing Latent Paraphraser"}, {"figure_path": "T1lFrYwtf7/figures/figures_8_1.jpg", "caption": "Figure 4: Effect of the Number of Paraphrases. Each plot shows the relationship between the number of paraphrases (x-axis) and F1 scores (y-axis) in knowledge injection. The F1 scores of both standard fine-tuning and our method improve as the number of paraphrases increases.", "description": "This figure displays three graphs, one for each of the SQUAD-syn, StreamingQA-syn, and ArchivalQA-syn datasets.  Each graph shows the F1 score (a measure of accuracy) on the question-answering task plotted against the number of paraphrases used for data augmentation. Both the standard fine-tuning approach and the proposed LaPael method show improved performance as the number of paraphrases increases, indicating that data augmentation with paraphrases is beneficial for knowledge injection in language models.  LaPael consistently outperforms standard fine-tuning across all three datasets.", "section": "5.3 Ablation Studies"}, {"figure_path": "T1lFrYwtf7/figures/figures_8_2.jpg", "caption": "Figure 5: (a) We conduct experiments varying the size of Dtrain on SQUAD-syn, where 100% indicates 1,000 documents. We report mean and std. over three runs. (b) We conduct experiments on StreamingQA-syn varying the start position of latent paraphrasers where '# layers' denotes the number of latent paraphrasers.", "description": "This figure shows two subfigures. Subfigure (a) shows the impact of varying the size of the training dataset (Dtrain) on the F1 score of the SQUAD-syn benchmark. The x-axis represents the percentage of the training dataset used, and the y-axis represents the F1 score. The results show that LaPael's performance improves steadily with increasing training data size. Subfigure (b) shows the effect of varying the starting layer position for applying the latent paraphrasers within the language model. The x-axis represents the starting layer, and the y-axis represents the F1 score. Three lines represent the results for using 3, 5, and 7 layers with latent paraphrasers. The results indicate that applying the latent paraphrasers to earlier layers produces the best results.", "section": "5.2 Experimental Results"}, {"figure_path": "T1lFrYwtf7/figures/figures_18_1.jpg", "caption": "Figure 1: Effect of paraphrasing data in knowledge injection.", "description": "The figure shows a bar chart comparing the F1 scores of question answering tasks for three different methods: Fine-tuning, Fine-tuning with paraphrases, and the proposed method.  The proposed method shows a significant improvement in F1 score compared to the other two methods across multiple QA datasets (StreamingQA, SQUAD, and ArchivalQA). This demonstrates the effectiveness of paraphrasing data in improving knowledge injection in language models.", "section": "1 Introduction"}, {"figure_path": "T1lFrYwtf7/figures/figures_21_1.jpg", "caption": "Figure 7: Visualization of Latent Features. We visualize the latent features from the last layers of LLMs using 5 randomly sampled data from ArchivalQA dataset. Each color denotes the different data, circles denote the original sentences, triangles denote the paraphrases, diamonds denote the questions, and crosses ('x') denote the original sentence with latent paraphrasing.", "description": "This figure visualizes the latent features extracted from the final layers of LLMs using t-SNE dimensionality reduction.  It compares the latent feature distributions of original sentences, their paraphrases, and questions from the ArchivalQA dataset.  The use of latent paraphrasing is shown to generate more diverse samples compared to the original data.", "section": "5.2 Experimental Results"}]