{"references": [{"fullname_first_author": "A. Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-00-00", "reason": "This paper introduced the Transformer architecture, which is the foundation of the models studied in this research."}, {"fullname_first_author": "T. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-00-00", "reason": "This is a foundational paper demonstrating the capabilities of large language models, providing the context for investigating their compositional abilities."}, {"fullname_first_author": "Z. Zhang", "paper_title": "Anchor function: a type of benchmark functions for studying language models", "publication_date": "2024-00-00", "reason": "This paper introduces the anchor function framework used in this work, providing the experimental setup for studying compositional capabilities."}, {"fullname_first_author": "G. F. Marcus", "paper_title": "The algebraic mind: Integrating connectionism and cognitive science", "publication_date": "2003-00-00", "reason": "This book discusses human cognitive abilities related to compositionality, providing a theoretical basis for the investigation."}, {"fullname_first_author": "S. Bubeck", "paper_title": "Sparks of artificial general intelligence: Early experiments with gpt-4", "publication_date": "2023-00-00", "reason": "This paper discusses recent advancements in large language models, which contextualizes the focus on compositional capabilities."}]}