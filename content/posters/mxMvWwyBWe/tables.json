[{"figure_path": "mxMvWwyBWe/tables/tables_5_1.jpg", "caption": "Table 1: Mean test correlation when comparing QA-Emb computed via many LLM calls to QA-Emb computed via a single distilled model. Distillation does not significantly degrade performance. All standard errors of the mean are below 10-3.", "description": "This table compares the mean test correlation achieved by the original QA-Emb method (using multiple LLM calls) against two distilled versions: one producing binary embeddings and another producing probabilistic embeddings.  It also includes the performance of the Eng1000 baseline for comparison. The results show that distilling the QA-Emb model does not significantly reduce its predictive performance on the fMRI dataset.", "section": "4.2 fMRI predictive performance"}, {"figure_path": "mxMvWwyBWe/tables/tables_7_1.jpg", "caption": "Table 2: Information retrieval results for different interpretable embedding models. QA-Emb in combination with BM-25 achieves a slight improvement over the interpretable baselines. QA-Emb additionally yields reasonably strong performance compared to its embedding size. Note that QA-Emb embeddings are binary, so the raw number of dimensions overrepresents the embedding's size relative to other methods. Error bars show standard error of the mean.", "description": "This table presents the performance comparison of several interpretable embedding models in an information retrieval task. The models include bag-of-words (unigrams, bigrams, and trigrams), QA-Emb, and BM-25. The performance metrics are mean reciprocal rank, recall@1, and recall@5. The table also shows the size (dimensionality) of each embedding.  QA-Emb, when combined with BM-25, shows a slight improvement over the baseline models, while demonstrating relatively high performance considering its relatively low dimensionality.", "section": "6 Secondary results: evaluating QA-Emb in simple NLP tasks"}, {"figure_path": "mxMvWwyBWe/tables/tables_8_1.jpg", "caption": "Table 3: Clustering scores before and after zero-shot adaptation (higher is better). Errors give standard error of the mean.", "description": "This table presents the clustering scores for four different text classification datasets (Rotten tomatoes, AG News, Emotion, Financial phrasebank) before and after a zero-shot adaptation using QA-Emb.  The \"Original\" row shows the clustering performance using embeddings generated with general questions, while the \"Adapted\" row shows the performance after adapting the questions to be dataset-specific via LLM prompting.  Higher scores indicate better clustering performance (i.e., data points within the same class are closer together in the embedding space, and data points from different classes are further apart). The average embedding size is also shown for both the original and adapted embeddings.  The zero-shot adaptation significantly improves the clustering scores across all datasets, demonstrating the flexibility and adaptability of QA-Emb.", "section": "Zero-shot adaptation in text clustering"}, {"figure_path": "mxMvWwyBWe/tables/tables_14_1.jpg", "caption": "Table A1: Questions list for model with 29 questions. Importance denotes the average absolute coefficient for each question (normalized by the importance of the top question).", "description": "This table lists 29 yes/no questions used in the QA-Emb model, along with their importance scores.  The importance score for each question represents its average absolute coefficient in the final model, normalized by the highest coefficient. This indicates the relative contribution of each question in predicting fMRI responses.  A higher score suggests greater importance in the model's predictions.", "section": "A.1 fMRI question details"}, {"figure_path": "mxMvWwyBWe/tables/tables_15_1.jpg", "caption": "Table A2: Mean test correlation for QA-Emb with different settings: varying the underlying prompts to source questions and the LLM used to answer the questions (fixing the number of time-lagged delays to 8). Ensemble generally provides a small boost over other models and Mistral slightly underperforms LLaMA-3 (8B).", "description": "This table presents the mean test correlation achieved by the QA-Emb model under different experimental conditions. Specifically, it shows how the performance varies depending on the set of prompts used to generate questions (Prompts 1-3, 1-5, 1-6) and the large language model (LLM) employed to answer these questions (Ensemble, LLaMA-3 (8B), LLaMA-3 (8B)-fewshot, Mistral (7B)).  The number of time-lagged delays was kept constant at 8 for all experiments. The results indicate that using an ensemble of LLMs generally leads to slightly better performance, while the Mistral model shows slightly lower accuracy than LLaMA-3 (8B).", "section": "A.2 fMRI prediction results extended"}, {"figure_path": "mxMvWwyBWe/tables/tables_16_1.jpg", "caption": "Table A3: Mean test correlation for different baseline models as a function of hyperparameters (number of time-lagged delays and layer for extracting embeddings)", "description": "This table presents the mean test correlation results for various baseline models (BERT, Eng1000, and different LLaMA models) in predicting fMRI responses.  It explores the impact of two hyperparameters on prediction accuracy: the number of time-lagged delays included as features and the layer from which embeddings are extracted from the LLMs. The results are shown separately for each subject (S01, S02, S03) and on average across subjects.", "section": "4.2 fMRI predictive performance"}, {"figure_path": "mxMvWwyBWe/tables/tables_18_1.jpg", "caption": "Table A4: 54 binary classification datasets along with their underlying yes/no question and corpus statistics from a recent collection [89, 90].", "description": "This table lists 54 binary classification datasets used to evaluate the question-answering accuracy of LLMs. For each dataset, it provides the dataset's name and topic, the underlying yes/no question used for classification, and corpus statistics including the number of examples and unique unigrams.", "section": "5.2 Evaluating question-answering faithfulness"}]