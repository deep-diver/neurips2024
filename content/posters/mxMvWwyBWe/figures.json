[{"figure_path": "mxMvWwyBWe/figures/figures_1_1.jpg", "caption": "Figure 1: QA-Emb produces an embedding for an input text by prompting an LLM with a series of yes/no questions. This embedding can then be used in downstream tasks such as fMRI response prediction or information retrieval.", "description": "The figure illustrates the QA-Emb method.  An input text is fed into a Large Language Model (LLM).  The LLM is prompted with a series of yes/no questions about the input text.  The answers (1 for yes, 0 for no) to these questions form the embedding vector.  This embedding vector is then used for downstream tasks such as predicting fMRI responses to the text or for information retrieval.", "section": "2 Methods"}, {"figure_path": "mxMvWwyBWe/figures/figures_5_1.jpg", "caption": "Figure 2: Predictive performance for QA-Emb compared to baselines. (A) Test correlation for QA-Emb outperforms the interpretable Eng1000 baseline, is on par with the black-box BERT baseline, and is worse than the best-performing LLaMA model. (B) Test correlation for method quickly grows as a function of the number of included questions. (C) Test correlation per voxel for QA-Emb. (D) Difference in the test correlation per voxel for subject between QA-Emb and BERT. Error bars for (A) and (B) (standard error of the mean) are within the points (all are below 0.001). (B), (C), and (D) show results for subject S03.", "description": "This figure presents the results of comparing QA-Emb's predictive performance against three baselines: Eng1000, BERT, and LLaMA.  Panel A shows a bar chart comparing the test correlation for each method across three subjects and on average.  Panel B shows a line graph illustrating how QA-Emb's performance improves as more questions are added to the model (i.e., more features). Panel C displays a heatmap visualizing test correlation per voxel for QA-Emb in subject S03, showcasing its ability to predict fMRI responses at the voxel level. Finally, panel D presents another heatmap comparing the differences in test correlation between QA-Emb and BERT in subject S03, highlighting the brain regions where QA-Emb shows improved prediction compared to BERT.", "section": "4.2 fMRI predictive performance"}, {"figure_path": "mxMvWwyBWe/figures/figures_6_1.jpg", "caption": "Figure 3: Learned feature weights for 3 example questions capture known selectivity and are consistent across subjects. All feature weights are jointly rescaled to the range (-1, 1) for visualization. Abbreviations: Pr = precuneus, pTemp = posterior temporal cortex, PFC = prefrontal cortex, IPS = intraparietal sulcus, RSC = retrosplenial complex, OPA = occipital place area, PPA = parahippocampal place area, Broca = Broca's area, sPMv = superior premotor ventral speech area, AC = auditory cortex.", "description": "This figure visualizes the learned regression weights for three example questions from the QA-Emb model across two subjects (S02 and S03).  The weights are displayed as flatmaps, showing the distribution of weights across the brain's cortical surface.  Each question's weights highlight specific brain regions associated with the semantic content of the question, demonstrating consistent activation patterns across subjects. For instance, the question about physical environment activates known place-selective areas like the retrosplenial cortex and occipital place area.", "section": "4.3 Interpreting the fitted representation from QA-Emb"}, {"figure_path": "mxMvWwyBWe/figures/figures_7_1.jpg", "caption": "Figure 4: Performance of question-answering for underlying LLMs on the D3 collection of binary classification datasets. Each point shows an individual dataset and error bars show the 95% confidence interval.", "description": "This figure demonstrates the performance of four different Large Language Models (LLMs) in answering yes/no questions from the D3 collection of binary classification datasets.  Each data point represents the accuracy of a single LLM on a specific dataset, and the error bars represent the 95% confidence intervals. The figure visually compares the overall accuracy of each LLM across the various datasets.  It helps to assess the reliability and capability of these LLMs in answering a diverse range of yes/no questions.", "section": "5.2 Evaluating question-answering faithfulness"}, {"figure_path": "mxMvWwyBWe/figures/figures_15_1.jpg", "caption": "Figure 2: Predictive performance for QA-Emb compared to baselines. (A) Test correlation for QA-Emb outperforms the interpretable Eng1000 baseline, is on par with the black-box BERT baseline, and is worse than the best-performing LLaMA model. (B) Test correlation for method quickly grows as a function of the number of included questions. (C) Test correlation per voxel for QA-Emb. (D) Difference in the test correlation per voxel for subject between QA-Emb and BERT. Error bars for (A) and (B) (standard error of the mean) are within the points (all are below 0.001). (B), (C), and (D) show results for subject S03.", "description": "This figure compares the performance of QA-Emb against several baselines for fMRI response prediction. Panel A shows that QA-Emb outperforms the interpretable Eng1000 model and performs similarly to BERT, while slightly underperforming the best-performing LLaMA model. Panel B illustrates the improvement in test correlation with an increasing number of questions used for QA-Emb. Panels C and D visualize the performance and difference in performance against BERT for a particular subject across different brain voxels.", "section": "4.2 fMRI predictive performance"}]