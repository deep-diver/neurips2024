[{"figure_path": "GOgKhunkfw/figures/figures_2_1.jpg", "caption": "Figure 1: Comparison of the learned trajectories. The final train loss (MSE) and training NFE are shown above each plot. (a) We consider deterministic regression task of four data pairs, each of which is represented by two circles (filled and empty circles) connected by dotted lines. (b) NODEs can correctly associate the pairs but through complex paths (solid lines) that require large NFEs. (c) Flow matching with linear velocity can greatly reduce the training NFEs by simulation-free training, but fails to associate the correct pairs due to the crossing trajectories induced by predefined dynamics. (d) The proposed method can alleviate the problems by learning the embeddings for data jointly with the flow matching.", "description": "This figure compares the learned trajectories of four different methods on a simple deterministic regression task with four data pairs.  The ground truth shows simple linear mappings between input and output pairs.  A standard Neural ODE successfully learns the mapping but uses complex, computationally expensive trajectories.  A flow matching approach, while efficient, fails to accurately pair inputs and outputs due to trajectory crossings.  The proposed method, which uses embedding and flow matching, avoids crossings and correctly pairs inputs and outputs with minimal computational cost.", "section": "Challenges in Flow Matching for Paired Data"}, {"figure_path": "GOgKhunkfw/figures/figures_3_1.jpg", "caption": "Figure 2: An overview of our framework. We avoid the crossing trajectory problem in data space by introducing learnable encoders that project data and label to embedding space. In the learned embedding space, the presumed dynamics induce valid target velocity field.", "description": "This figure illustrates the proposed framework for simulation-free training of Neural ODEs.  It shows how data and labels are first encoded into an embedding space using separate encoders. In this embedding space, a predefined flow (dynamics) is applied to generate a trajectory that avoids the problematic crossing trajectories seen in the original data space. The embedding space is learned alongside the dynamic function, ensuring the validity of the flow. Finally, the label is decoded from the embedding space. This process greatly reduces the computational cost compared to standard training methods.", "section": "4 End-to-End Latent Flow Matching"}, {"figure_path": "GOgKhunkfw/figures/figures_6_1.jpg", "caption": "Figure 3: RMSE over NFEs on UCI regression tasks. To control the NFE, we use Euler solver for the evaluation. By assuming linear dynamics, our model shows better performance in low NFE regime.", "description": "This figure compares the root mean squared error (RMSE) achieved by different models on various UCI regression datasets as a function of the number of function evaluations (NFEs).  The x-axis represents the NFEs, while the y-axis shows the RMSE. Each subplot corresponds to a different dataset.  The lines represent the mean RMSE across multiple runs, and the shaded areas show the standard deviation. The figure demonstrates that the proposed method (Ours) achieves lower RMSE than the baseline methods (NODE, STEER, RNODE, CARD) especially when the number of NFEs is low, highlighting the efficiency of the proposed model, particularly when the computational resources are limited.", "section": "6.2 Main Results"}, {"figure_path": "GOgKhunkfw/figures/figures_8_1.jpg", "caption": "Figure 5: Ablation study of optimization techniques on CIFAR10. Explicitly sampling t = 0 in training prevents suboptimal solutions while adding noise to label autoencoding improves generalization.", "description": "This ablation study investigates the impact of two optimization techniques on the performance of the proposed model for CIFAR10 image classification.  The first technique is explicitly sampling t=0 during training. The second technique involves adding noise to the label autoencoding process. The results show that explicitly sampling t=0 prevents the model from converging to suboptimal solutions, leading to improved training and test accuracy. Adding noise to label autoencoding further enhances generalization performance. The figure visually represents these effects by plotting the training flow loss, training accuracy, and test accuracy across different training steps for the baseline model and the variants with each technique removed.", "section": "6.3 Analysis and Discussion"}, {"figure_path": "GOgKhunkfw/figures/figures_8_2.jpg", "caption": "Figure 4: Analysis on predefined dynamics. (Left) Change of coefficients in interpolant with respective to time. (Right) Prediction RMSE over NFE on UCI Boston dataset.", "description": "This figure analyzes the effect of different predefined dynamics on the model's performance. The left panel shows the change in coefficients (\u03b1t and \u03b2t) of the interpolant zt = \u03b1t*z0 + \u03b2t*z1 over time t for three types of dynamics: linear, convex, and concave.  The right panel presents the prediction Root Mean Squared Error (RMSE) versus the number of function evaluations (NFE) for the Boston housing dataset, using each of the three dynamics.  It demonstrates how the choice of dynamics affects the model's ability to achieve good performance with varying computational costs.", "section": "6.3 Analysis and Discussion"}, {"figure_path": "GOgKhunkfw/figures/figures_14_1.jpg", "caption": "Figure 6: Failure cases of NODES.", "description": "This figure shows examples where training of Neural Ordinary Differential Equations (NODEs) fails to converge. The x-axis represents the training step, the y-axis on the left shows the training loss, and the y-axis on the right shows the number of function evaluations (NFEs).  The different colored lines represent different runs. The figure indicates that training NODEs can be unstable, failing to converge in multiple runs, despite adaptive step-size solvers.", "section": "Experiment Details for Classification Tasks"}, {"figure_path": "GOgKhunkfw/figures/figures_15_1.jpg", "caption": "Figure 7: Reconstruction from the autoencoder.", "description": "This figure shows the reconstruction of images from an autoencoder.  The top row displays the input images, and the bottom row shows their reconstructions generated by the autoencoder. The purpose is to illustrate the quality of reconstruction achieved by the autoencoder used in the proposed method, specifically showing that the autoencoder trained without the flow loss fails to preserve the original coupling between data and label, resulting in a poor reconstruction. The images show that the reconstruction is quite good, suggesting that the autoencoder is effectively learning the relevant features of the input images.", "section": "D.1 Additional Result of Learning Encoders with Flow Loss"}, {"figure_path": "GOgKhunkfw/figures/figures_16_1.jpg", "caption": "Figure 1: Comparison of the learned trajectories. The final train loss (MSE) and training NFE are shown above each plot. (a) We consider deterministic regression task of four data pairs, each of which is represented by two circles (filled and empty circles) connected by dotted lines. (b) NODEs can correctly associate the pairs but through complex paths (solid lines) that require large NFEs. (c) Flow matching with linear velocity can greatly reduce the training NFEs by simulation-free training, but fails to associate the correct pairs due to the crossing trajectories induced by predefined dynamics. (d) The proposed method can alleviate the problems by learning the embeddings for data jointly with the flow matching.", "description": "This figure compares the learned trajectories of four different methods for a deterministic regression task.  The ground truth shows four pairs of points directly connected. The Neural ODE method correctly connects the pairs but with complex and inefficient trajectories. Flow Matching, while efficient, causes trajectory crossings which is undesirable.  The proposed 'Embed.+FM' method learns embeddings, avoiding crossings while maintaining efficiency.", "section": "Challenges in Flow Matching for Paired Data"}, {"figure_path": "GOgKhunkfw/figures/figures_16_2.jpg", "caption": "Figure 5: Ablation study of optimization techniques on CIFAR10. Explicitly sampling t = 0 in training prevents suboptimal solutions while adding noise to label autoencoding improves generalization.", "description": "This figure shows the results of an ablation study conducted on the CIFAR10 dataset to investigate the effects of two optimization techniques: explicitly sampling t=0 during training and adding noise to the label autoencoding loss.  The leftmost graph displays the training flow loss, showing that explicitly sampling t=0 prevents the model from converging to suboptimal solutions. The middle graph shows training accuracy, demonstrating that adding noise to the label autoencoding improves the model's generalization ability. The rightmost graph displays test accuracy, confirming the benefit of adding noise to the label autoencoding for improved generalization performance.  Overall, this ablation study highlights the importance of these optimization techniques in achieving optimal results.", "section": "Ablation on Optimization Techniques"}]