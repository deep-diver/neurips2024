[{"figure_path": "GOgKhunkfw/tables/tables_6_1.jpg", "caption": "Table 1: Experiment results on image classification. Training cost and few-/full-step performances are reported in three datasets. For classification accuracy, numbers indicate the number of function evaluations with Euler solver, where \u221e denotes the result of dopri5 adaptive-step solver. For CARD, we report the 1000-step decoding results instead of using the adaptive solver, as the model was trained on discrete timesteps. CARD\u2020 is trained with 4 times longer steps.", "description": "This table presents a comparison of different methods for image classification on three datasets: MNIST, SVHN, and CIFAR10.  It shows the training cost (NFEs and throughput), and the classification accuracy achieved using different numbers of function evaluations (NFEs) with both the Euler solver (1, 2, 10, 20 NFEs) and the adaptive-step Dopri5 solver (\u221e NFEs).  The table also includes results for CARD, a diffusion-based model, trained with 1000 steps (and a variant trained with 4 times longer steps).", "section": "6.1 Experimental Setup"}, {"figure_path": "GOgKhunkfw/tables/tables_7_1.jpg", "caption": "Table 2: The effectiveness of learning encoders with flow loss. Training accuracy and the proportion of disagreement in prediction between a one-step Euler solver and an adaptive-step solver are shown. Simply augmenting dimensions (ANODE+FM) does not effectively prevent trajectory crossing. Furthermore, learning encoders without flow loss (Autoencoder+FM) also fails to preserve the original coupling due to crossing trajectories.", "description": "This table presents a comparison of three different methods for learning encoders in a continuous-depth model, focusing on the impact of using flow loss.  The methods compared are ANODE+FM (augmenting dimensions and using flow matching), Autoencoder+FM (learning an embedding space with autoencoding and then applying flow matching), and the authors' proposed method. The table shows the training accuracy and the percentage of disagreements between predictions made using a one-step Euler solver and an adaptive-step solver. The results demonstrate the effectiveness of the authors' method in preventing trajectory crossing and achieving high accuracy.", "section": "6.3 Analysis and Discussion"}, {"figure_path": "GOgKhunkfw/tables/tables_17_1.jpg", "caption": "Table 3: Experiment results on UCI regression tasks with Euler 1-step solver.", "description": "This table presents the results of experiments conducted on UCI regression datasets using the Euler method with a single step.  It compares the performance of the proposed method ('Ours') against several baseline methods (NODE, STEER, RNODE, and CARD) in terms of Root Mean Square Error (RMSE).  The RMSE values reflect the accuracy of each method in predicting the target variable, with lower values indicating better performance.  The table showcases the relative performance of different methods using this specific experimental setup.", "section": "6.1 Experimental Setup"}, {"figure_path": "GOgKhunkfw/tables/tables_17_2.jpg", "caption": "Table 4: Experiment results on UCI regression tasks with Euler 2-step solver.", "description": "This table presents the results of UCI regression tasks using the Euler method with 2 steps.  It compares the performance of the proposed method ('Ours') against several baseline methods (NODE, STEER, RNODE, and CARD) across various datasets. The metrics used are likely RMSE (root mean square error) values, with the plus/minus values representing some measure of uncertainty (e.g., standard deviation or confidence interval).  The table highlights the relative performance of each method in terms of accuracy and computational efficiency.  The '> 10<sup>4</sup>' entries likely indicate that the baseline methods required far more than 10,000 function evaluations.", "section": "6.2 Main Results"}, {"figure_path": "GOgKhunkfw/tables/tables_17_3.jpg", "caption": "Table 1: Experiment results on image classification. Training cost and few-/full-step performances are reported in three datasets. For classification accuracy, numbers indicate the number of function evaluations with Euler solver, where \u221e denotes the result of dopri5 adaptive-step solver. For CARD, we report the 1000-step decoding results instead of using the adaptive solver, as the model was trained on discrete timesteps. CARD\u2020 is trained with 4 times longer steps.", "description": "This table presents a comparison of different models' performance on image classification tasks using three datasets: MNIST, SVHN, and CIFAR10.  It shows training costs (NFEs and throughput), and classification accuracy at different numbers of function evaluations (NFEs) using both Euler and adaptive-step solvers.  It highlights the trade-off between training efficiency and accuracy. The table also includes results for CARD, a model based on diffusion processes, comparing 1000-step decoding with other methods' performance at various NFE.", "section": "6.1 Experimental Setup"}, {"figure_path": "GOgKhunkfw/tables/tables_17_4.jpg", "caption": "Table 1: Experiment results on image classification. Training cost and few-/full-step performances are reported in three datasets. For classification accuracy, numbers indicate the number of function evaluations with Euler solver, where \u221e denotes the result of dopri5 adaptive-step solver. For CARD, we report the 1000-step decoding results instead of using the adaptive solver, as the model was trained on discrete timesteps. CARD\u2020 is trained with 4 times longer steps.", "description": "This table presents a comparison of different methods for image classification on three datasets (MNIST, SVHN, CIFAR10).  It shows the training cost (NFEs and throughput), and the classification accuracy achieved with varying numbers of function evaluations (NFEs) using both the Euler solver (few-step inference) and the dopri5 adaptive-step solver (full-step inference).  The table also highlights the performance of CARD, a baseline method, trained with both standard and longer timesteps.", "section": "6.1 Experimental Setup"}, {"figure_path": "GOgKhunkfw/tables/tables_18_1.jpg", "caption": "Table 1: Experiment results on image classification. Training cost and few-/full-step performances are reported in three datasets. For classification accuracy, numbers indicate the number of function evaluations with Euler solver, where \u221e denotes the result of dopri5 adaptive-step solver. For CARD, we report the 1000-step decoding results instead of using the adaptive solver, as the model was trained on discrete timesteps. CARD\u2020 is trained with 4 times longer steps.", "description": "This table presents a comparison of different models on image classification tasks using three datasets: MNIST, SVHN, and CIFAR10.  It shows the training cost (number of function evaluations (NFEs) and training throughput), and the classification accuracy achieved by each model at varying numbers of function evaluations (NFEs) during both few-step and full-step inference.  The models compared include NODE, STEER, RNODE, CARD, and the proposed method.  The table highlights the computational efficiency and accuracy of the proposed method, particularly in low-NFE settings.", "section": "6.1 Experimental Setup"}, {"figure_path": "GOgKhunkfw/tables/tables_18_2.jpg", "caption": "Table 1: Experiment results on image classification. Training cost and few-/full-step performances are reported in three datasets. For classification accuracy, numbers indicate the number of function evaluations with Euler solver, where \u221e denotes the result of dopri5 adaptive-step solver. For CARD, we report the 1000-step decoding results instead of using the adaptive solver, as the model was trained on discrete timesteps. CARD\u2020 is trained with 4 times longer steps.", "description": "This table presents a comparison of different models' performance on image classification tasks using three datasets (MNIST, SVHN, CIFAR10).  It shows training costs (NFEs and throughput), and classification accuracy at various numbers of function evaluations (NFEs) using both Euler and adaptive solvers.  The table highlights the efficiency gains of the proposed simulation-free method compared to standard Neural ODEs and other baselines like STEER, RNODE, and CARD.", "section": "6.1 Experimental Setup"}, {"figure_path": "GOgKhunkfw/tables/tables_18_3.jpg", "caption": "Table 1: Experiment results on image classification. Training cost and few-/full-step performances are reported in three datasets. For classification accuracy, numbers indicate the number of function evaluations with Euler solver, where \u221e denotes the result of dopri5 adaptive-step solver. For CARD, we report the 1000-step decoding results instead of using the adaptive solver, as the model was trained on discrete timesteps. CARD\u2020 is trained with 4 times longer steps.", "description": "This table presents a comparison of different methods for image classification on three datasets: MNIST, SVHN, and CIFAR10.  It shows the training cost (NFEs and throughput), and the accuracy achieved using different numbers of function evaluations (NFEs) for each method.  The table highlights the efficiency gains from simulation-free training, particularly with the proposed method.", "section": "6.1 Experimental Setup"}]