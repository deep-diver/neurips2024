[{"figure_path": "D6nlm2AYHi/figures/figures_2_1.jpg", "caption": "Figure 1: Architecture of CTR model.", "description": "This figure illustrates the architecture of the Contrastive Trajectory Representation (CTR) model. The model consists of three main components: an encoder, an autoregressive model, and a contrastive learning module. The encoder maps the observation and action sequences of an agent to a latent representation. The autoregressive model then summarizes these latent representations to generate a trajectory representation for each agent. Finally, the contrastive learning module encourages the learning of distinguishable trajectory representations by maximizing the mutual information between the trajectory representations and identity representations of different agents.  The trajectory representations are visualized as points on a hypersphere, with points representing different agents being spaced apart.", "section": "3 Contrastive Trajectory Representation"}, {"figure_path": "D6nlm2AYHi/figures/figures_5_1.jpg", "caption": "Figure 2: The performance comparison between our proposed CTR and baselines in Pac-Men.", "description": "This figure shows the results of the Pac-Men experiment, comparing the performance of CTR with other baselines.  Subfigure (a) illustrates the Pac-Men environment.  Subfigure (b) presents learning curves, showing the average rewards over time for each algorithm. Subfigures (c) and (d) display visitation heatmaps for QMIX and CTR respectively.  These heatmaps visualize where agents tend to explore the environment, highlighting the diversity promoted by CTR.", "section": "4 Experiments"}, {"figure_path": "D6nlm2AYHi/figures/figures_5_2.jpg", "caption": "Figure 3: Performance comparison between our proposed CTR and baselines in SMAC scenarios. Without loss of generality, all results are presented with the mean and standard deviation of performance tested with five random seeds.", "description": "This figure compares the performance of the proposed CTR method with several baselines across six different StarCraft Multi-Agent Challenge (SMAC) scenarios.  The scenarios range in difficulty from easy (3s5z) to super hard (6h_vs_8z, corridor, 3s5z_vs_3s6z). The results show that CTR consistently outperforms the baselines, particularly in the more challenging scenarios, suggesting that CTR is more effective at promoting multi-agent diversity and efficient exploration. The mean and standard deviation are shown for each scenario, based on five independent runs with different random seeds. The x-axis represents the number of timesteps during training, while the y-axis shows the test win rate.", "section": "4 Experiments"}, {"figure_path": "D6nlm2AYHi/figures/figures_6_1.jpg", "caption": "Figure 4: Performance comparison between our proposed CTR and baselines in SMACv2 scenarios.", "description": "This figure compares the performance of CTR+QMIX against several other multi-agent reinforcement learning algorithms across three SMACv2 scenarios: terran_5_vs_5, protoss_5_vs_5, and zerg_5_vs_5.  Each graph plots the test win rate (%) over time (timesteps) for each algorithm.  The shaded area represents the standard deviation across multiple trials. This figure shows that CTR+QMIX generally outperforms the baselines in all three scenarios, highlighting its effectiveness in cooperative multi-agent tasks.", "section": "4 Experiments"}, {"figure_path": "D6nlm2AYHi/figures/figures_6_2.jpg", "caption": "Figure 5: Visitation heatmaps of different algorithms in the terran_5_vs_5 scenario.", "description": "This figure visualizes the visitation heatmaps of different algorithms (MAVEN, EOI, SCDS, LIPO, FoX, and CTR+QMIX) in the terran_5_vs_5 scenario of SMACv2.  The heatmaps illustrate where agents tend to concentrate their actions during gameplay.  The purpose is to demonstrate the impact of each algorithm on exploration and diversity of agent behavior.  Specifically, it shows whether agents explore the map effectively or get stuck in repetitive actions.  The CTR+QMIX heatmap is expected to show a more even distribution of visits across the map, indicating better exploration compared to the other methods.", "section": "4.2 SMAC and SMACV2"}, {"figure_path": "D6nlm2AYHi/figures/figures_7_1.jpg", "caption": "Figure 6: Performance comparison between CTR and ablation variants in SMAC scenarios.", "description": "This figure shows the ablation study results on the SMAC benchmark.  It compares the performance of the main CTR model against several variants where key components (autoregressive model, identity representation, and contrastive learning loss) have been removed or altered.  The results demonstrate the importance of each component to the overall performance of the CTR method, highlighting the contribution of each element towards achieving better results compared to the baseline QMIX model.", "section": "4.3 Ablation Study"}, {"figure_path": "D6nlm2AYHi/figures/figures_7_2.jpg", "caption": "Figure 7: T-SNE plots of trajectory representations of different agents learned by different variants of CTR ((a) QMIX (b)CTR-With-One-Hot (c) CTR-With-TC-Loss (d) CTR-With-InfoNCE (e) CTR), emerging in the corridor scenario of SMAC.", "description": "This figure uses t-distributed stochastic neighbor embedding (t-SNE) to visualize the trajectory representations learned by different variants of the Contrastive Trajectory Representation (CTR) method and the baseline QMIX in the corridor scenario of the StarCraft Multi-Agent Challenge (SMAC). Each color represents a different agent. The visualization shows how the different methods result in different levels of distinguishability among trajectory representations.  The CTR method, specifically, shows a clear separation of agents, indicative of its success in encouraging multi-agent diversity.", "section": "4.3 Ablation Study"}, {"figure_path": "D6nlm2AYHi/figures/figures_16_1.jpg", "caption": "Figure 8: T-SNE plots of trajectory representations of different agents learned by CTR and baselines, respectively, that emerge in the corridor scenario, initial (left) to final (right). Each color represents the trajectory representations of an agent.", "description": "This figure shows the visualization of trajectory representations learned by different algorithms in the corridor scenario of SMAC.  It uses t-SNE to reduce the dimensionality of the trajectory representations to two dimensions for visualization.  Each point represents a trajectory, and each color represents a different agent.  The figure is organized as a grid, with each row representing a different algorithm (QMIX, EOI, SCDS, and CTR), and each column representing a different phase of the game (from initial to final). The visualization helps to understand how the trajectory representations evolve over time and how they differ for different agents and algorithms.  It provides evidence to support the claim that CTR leads to more distinguishable trajectory representations for different agents, especially compared to baselines like QMIX, EOI, and SCDS, which show more mixing of trajectories from different agents.", "section": "4.3 Ablation Study"}, {"figure_path": "D6nlm2AYHi/figures/figures_16_2.jpg", "caption": "Figure 8: T-SNE plots of trajectory representations of different agents learned by CTR and baselines, respectively, that emerge in the corridor scenario, initial (left) to final (right). Each color represents the trajectory representations of an agent.", "description": "This figure shows the trajectory representations of different agents in the corridor scenario of SMAC, visualized using t-SNE.  It compares the trajectory representations learned by CTR with those from QMIX, EOI, and SCDS.  The four columns represent different phases of the game (initial to final), and each color represents a different agent. The visualization aims to demonstrate how CTR creates more distinguishable trajectory representations among agents compared to the baselines.  The distinct clustering of points in the CTR plots shows that CTR generates more diverse and separable trajectory representations leading to more efficient exploration compared to the baseline methods.", "section": "4.2 SMAC and SMACv2"}, {"figure_path": "D6nlm2AYHi/figures/figures_17_1.jpg", "caption": "Figure 8: T-SNE plots of trajectory representations of different agents learned by CTR and baselines, respectively, that emerge in the corridor scenario, initial (left) to final (right). Each color represents the trajectory representations of an agent.", "description": "This figure shows the visualization of trajectory representations using t-SNE for different agents trained by four different methods (QMIX, EOI, SCDS, and CTR). The visualization is shown for four different phases (phase 1 to phase 4) of the training process in the corridor scenario.  Each color represents an individual agent. The plots illustrate how the trajectory representations of different agents evolve over time and how well separated they are by each method.  The figure aims to demonstrate the effectiveness of CTR in creating distinguishable trajectory representations compared to baselines.", "section": "4.2 SMAC and SMACV2"}, {"figure_path": "D6nlm2AYHi/figures/figures_18_1.jpg", "caption": "Figure 11: Visualization examples of diverse policies emerging in 6h_vs_8z (top), corridor (medium), and 3s5z_vs_3s6z (bottom) from initial (left) to final (right). Green and red shadows represent agents and enemies, respectively. Green and red arrows represent the moving directions of agents and enemies, respectively.", "description": "This figure visualizes examples of diverse policies learned by the CTR model in three different SMAC scenarios. It shows the initial and final states of each scenario, highlighting the different strategies employed by the agents to defeat the enemies. Green represents the agents and red represents enemies. Arrows indicate their moving directions. The diverse policies demonstrate the effectiveness of CTR in encouraging multi-agent diversity and efficient exploration.", "section": "4.2 SMAC and SMACv2"}]