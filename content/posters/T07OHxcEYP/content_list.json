[{"type": "text", "text": "Differentially Private Reinforcement Learning with Self-Play ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "DanQiao Department of Computer Science & Engineering University of California, San Diego San Diego, CA 92093 d2qiao@ucsd.edu ", "page_idx": 0}, {"type": "text", "text": "Yu-Xiang Wang   \nHalicioglu Data Science Institute   \nUniversity of California, San Diego San Diego, CA 92093 yuxiangw@ucsd.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We study the problem of multi-agent reinforcement learning (multi-agent RL) with differential privacy (DP) constraints. This is well-motivated by various real-world applications involving sensitive data, where it is critical to protect users\u2019 private information. We first extend the definitions of Joint DP (JDP) and Local DP (LDP) to two-player zero-sum episodic Markov Games, where both definitions ensure trajectory-wise privacy protection. Then we design a provably efficient algorithm based on optimistic Nash value iteration and privatization of Bernsteintype bonuses. The algorithm is able to satisfy JDP and LDP requirements when instantiated with appropriate privacy mechanisms. Furthermore, for both notions of DP, our regret bound generalizes the best known result under the single-agent RL case, while our regret could also reduce to the best known result for multi-agent RL without privacy constraints. To the best of our knowledge, these are the first results towards understanding trajectory-wise privacy protection in multi-agent RL. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper considers the problem of multi-agent reinforcement learning (multi-agent RL), wherein several agents simultaneously make decisions in an unfamiliar environment with the goal of maximizing their individual cumulative rewards. Multi-agent RL has been deployed not only in large-scale strategy games like Go [Silver et al., 2017], Poker [Brown and Sandholm, 2019] and MOBA games [Ye et al., 2020], but also in various real-world applications such as autonomous driving [ShalevShwartz et al., 2016], negotiation [Bachrach et al., 2020], and trading in financial markets [Shavandi and Khedmati, 2022]. In these applications, the learning agent analyzes users\u2032 private feedback in order to refine its performance, where the data from users usually contain sensitive information. Take autonomous driving as an instance, here a trajectory describes the interaction between the cars in a neighborhood during a fixed time window. At each timestamp, given the current situation of each car, the system (central agent) will send a command for each car to take (e.g. speed up, pull over), and finally the system gathers the feedback from each car (e.g. whether the driving is safe, whether the customer feels comfortable) and enhances its policy. Here, (situation, command, feedback) corresponds to (state, action, reward) in a Markov Game where the state and reward of each user are considered as sensitive information. Therefore, leakage of such information is not acceptable. Regrettably, it has been demonstrated that without the implementation of privacy safeguards, learning agents tend to inadvertently memorize details from individual training data points [Carlini et al., 2019], regardless of their relevance to the learning process [Brown et al., 2021]. This susceptibility exposes multi-agent RL agents to potential privacy threats. ", "page_idx": 0}, {"type": "text", "text": "To handle the above privacy issue, Differential privacy (DP) [Dwork et al., 2006] has been widely considered. The output of a differentially private reinforcement learning algorithm cannot be discerned ", "page_idx": 0}, {"type": "table", "img_path": "T07OHxcEYP/tmp/31e990f9a8c7ad8b1a77bd1d1ade3353c1ceacb3abb746b462fe92eda7f7f73f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "Table 1: Comparison of our results (in blue) to existing work regarding regret without privacy (i.e. the privacy budget is infinity), regret under $\\epsilon_{}$ -Joint DP and regret under $\\epsilon$ -Local DP. In the above, $S$ is the number of states, $A,B$ are the number of actions for both players, $H$ is the planning horizon and $K$ is the number of episodes ( $T=H K$ is the number of steps). Markov decision processes (MDPs) is a special case of Markov Games where $B=1$ \uff1a $\\ast:$ This result is the best known regret bound when there is no privacy concern. $\\star.$ More discussions about this bound can be found in Chowdhury and Zhou [2022]. $\\dagger$ : The original regret bound in Garcelon et al. [2021] is derived under the setting of stationary MDP, and can be directly transferred to the bound here by adding $\\sqrt{H}$ to the first term. $^{\\ddag}$ This algorithm achieved the best known results under single-agent MDPs, and our Algorithm 1 can obtain the same regret bounds under this setting. ", "page_idx": 1}, {"type": "text", "text": "from its output in an alternative reality where any specific user is substituted, which effectively mitigates the privacy risks mentioned earlier. However, it is shown [Shariff and Sheffet, 2018] that standard DP will lead to linear regret even under contextual bandits. Therefore, Vietri et al. [2020] considered a relaxed surrogate of DP: Joint Differential Privacy (JDP) [Kearns et al., 2014] for RL. Briefly speaking, JDP protects the information about any specific user even given the output of all other users. Meanwhile, another variant of DP: Local Differential Privacy (LDP) [Duchi et al., 2013] has also been extended to RL by Garcelon et al. [2021] due to its stronger privacy protection. LDP requires that the raw data of each user is privatized before being sent to the agent. Although following works [Chowdhury and Zhou, 2022, Qiao and Wang, 2023] established near optimal results under these two notions of DP, all of the previous works focused on the single-agent RL setting while the solution to multi-agent RL with differential privacy is still unknown. Therefore we question: ", "page_idx": 1}, {"type": "text", "text": "Question 1.1. Is it possible to design a provably effcient self-play algorithm to solve Markov games while satisfying the constraints of differential privacy? ", "page_idx": 1}, {"type": "text", "text": "Our contributions. In this paper, we answer the above question affirmatively by proposing a general algorithm for DP multi-agent RL: DP-Nash-VI (Algorithm 1). Our contributions are threefold. ", "page_idx": 1}, {"type": "text", "text": "\u00b7 We first extend the definitions of Joint DP (Definition 2.2) and Local DP (Definition 2.3) to the multi-agent RL setting. Both notions of DP focus on protecting the sensitive information of each trajectory, which is consistent with the counterparts under single-agent RL.   \n\u00b7 We design a new algorithm DP-Nash-VI (Algorithm 1) based on optimistic Nash value iteration and privatization of Bernstein-type bonuses. The algorithm can be combined with any Privatizer (for JDP or LDP) that possesses a corresponding regret bound (Theorem 4.1). Moreover, when there is no privacy constraint (i.e. the privacy budget is infinity), our regret reduces to the best known regret for non-private multi-agent RL.   \n\u00b7 Under the constraint of $\\epsilon$ -JDP, DP-Nash-VI achieves a regret of $\\widetilde{\\cal O}(\\sqrt{H^{2}S A B T}\\ +$ $H^{3}S^{2}A B/\\epsilon)$ (Theorem 5.2). Compared to the regret lower bound (Theorem 5.3), the main term is nearly optimal while the additional cost due to JDP has optimal dependence on $\\epsilon$ .Under the $\\epsilon$ -LDP constraint, DP-Nash-VI achieves a regret of $\\widetilde{\\cal O}(\\sqrt{H^{2}S A B T}\\,+$ $S^{2}A B\\sqrt{H^{5}T}/\\epsilon)$ (Theorem 5.5), where the dependence on $K,\\epsilon$ is optimal according to the lower bound (Theorem 5.6). The pair of results strictly generalizes the best known results for single-agent RL with DP [Qiao and Wang, 2023]. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We compare our results with existing works on differentially private reinforcement learning [Vietri et al., 2020, Garcelon et al., 2021, Chowdhury and Zhou, 2022, Qia0 and Wang, 2023] and regret minimization under Markov Games [Liu et al., 2021] in Table 1, while more discussions about differentially private learning algorithms are deferred to Appendix A. Notably, all existing DP RL algorithms focus on the single-agent case. In comparison, our algorithm works for the more general two-player setting and our results directly match the best known regret bounds [Qiao and Wang, 2023] when applied to the single-agent setting. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Recently, several works provide non-asymptotic theoretical guarantees for learning Markov Games. Bai and Jin [2020] developed the first provably-efficient algorithms in MGs based on optimistic value iteration, and the result is improved by Liu et al. [2021] using model-based approach. Meanwhile, model-free approaches are shown to break the curse of multiagency and improve the dependence on action space [Bai et al., 2020, Jin et al., 2021, Ma0 et al., 2022, Wang et al., 2023, Cui et al., 2023]. However, all these algorithms base on the original data from users, and thus are vulnerable to various privacy attacks. While several works [Hossain and Lee, 2023, Hossain et al., 2023, Zhao et al., 2023b, Gohari et al., 2023] study the privatization of communications between multiple agents, none of them provide regret guarantees. In comparison, we design algorithms that provably protect the sensitive information in each trajectory, while achieving near-optimal regret bounds simultaneously. ", "page_idx": 2}, {"type": "text", "text": "Technically speaking, we follow the idea of optimistic Nash value iteration and privatization of Bernstein-type bonuses. Optimistic Nash value iteration aims to construct both upper bounds and lower bounds for value functions, which could guide the exploration. Such idea has been applied by previous model-based approaches [Bai and Jin, 2020, Liu et al., 2021] to derive tight regret bounds. To satisfy the privacy guarantees, we are required to construct the UCB and LCB privately. In this work, we privatize the transition kernel estimate and construct a private bonus function for our purpose. Among different bonuses, we generalize the approach in Qiao and Wang [2023] and directly operate on the Bernstein-type bonus, which could enable tight regret analysis while the privatization is more technically demanding due to the variance term. To handle this, we first privatize the visitation counts such that they satisfy several nice properties, then we use these counts to construct private transition estimates and private bonuses. Lastly, we manage to prove UCB and LCB, and bound the private terms by their non-private counterparts to complete the regret analysis. ", "page_idx": 2}, {"type": "text", "text": "2 Problem Setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider reinforcement learning under Markov Games (MGs) [Shapley, 1953] with Differential Privacy (DP) [Dwork et al., 2006]. Below we introduce MGs and define DP under multi-agent RL. ", "page_idx": 2}, {"type": "text", "text": "2.1  Markov Games and Regret ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Markov Games (MGs) are the generalization of Markov Decision Processes (MDPs) to the multiplayer setting, where each player aims to maximize her own reward. We consider two-player zero-sum episodic MGs, denoted byatuple $\\mathcal{M G}=(S,\\mathcal{A},\\mathcal{B},H,\\{P_{h}\\}_{h=1}^{H},\\{r_{h}\\}_{h=1}^{H},s_{1})$ where $\\boldsymbol{S}$ is the state space with $\\cal S\\,=\\,|\\cal S|$ \uff0c $\\boldsymbol{\\mathcal{A}}$ and $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ are the action space for the max-player (who aims to maximize the total reward) and the min-player (who aims to minimize the total reward) respectively with $A=|A|,B=|B|$ . Besides, $H$ is the horizon while the non-stationary transition kernel $P_{h}(\\cdot|s,a,b)$ gives the distribution of the next state if action $(a,b)$ is taken at state $s$ and time step $h$ . In addition, we assume that the reward function $r_{h}(s,a,b)\\in[0,1]$ is deterministic and known'. For simplicity, we assume each episode starts from a fixed initial state $s_{1}$ . Then at each time step $h\\in[H]$ , two players observe $s_{h}$ and choose their actions $a_{h}\\in A$ and $b_{h}\\in\\boldsymbol{B}$ simultaneously, after which both players observe the action of their opponent and receive reward $r_{h}(s_{h},a_{h},b_{h})$ , the environment will transit to $s_{h+1}\\sim P_{h}(\\cdot|s_{h},a_{h},b_{h})$ ", "page_idx": 2}, {"type": "text", "text": "Markov policy, value function. A Markov policy $\\mu$ of the max-player can be seen as a series of mappings $\\dot{\\mu}~=~\\{\\mu_{h}\\}_{h=1}^{H}$ , where each $\\mu_{h}$ maps each state $s~\\in~s$ to a probability distribution over actions $\\boldsymbol{\\mathcal{A}}$ , i.e. $\\bar{\\mu_{h}}\\,:\\,S\\,\\rightarrow\\,\\Delta(A)$ . A Markov policy $\\nu$ for the min-player is defined similarly. Given a pair of policies $(\\mu,\\nu)$ and time step $\\textit{h}\\in[H]$ , the value function $V_{h}^{\\mu,\\nu}(\\cdot)$ is defined as $V_{h}^{\\mu,\\nu}(s)=\\mathbb{E}_{\\mu,\\nu}[\\sum_{t=h}^{H}r_{t}|s_{h}=s]$ while the Q-value function $Q_{h}^{\\mu,\\nu}(\\cdot,\\cdot,\\cdot)$ is defined as $\\begin{array}{r}{Q_{h}^{\\mu,\\nu}(s,a,b)=\\mathbb{E}_{\\mu,\\nu}[\\sum_{t=h}^{H}r_{t}|s_{h},a_{h},b_{h}=s,a,b]}\\end{array}$ for ll $s,a,b$ According to the defntions,the following Bellman equation holds: ", "page_idx": 2}, {"type": "equation", "text": "$$\nQ_{h}^{\\mu,\\nu}(s,a,b)=[r_{h}+P_{h}V_{h+1}^{\\mu,\\nu}](s,a,b),\\;\\;V_{h}^{\\mu,\\nu}(s)=[\\mathbb{E}_{\\mu,\\nu}Q_{h}^{\\mu,\\nu}](s),\\;\\;\\forall\\;(h,s,a,b).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Best responses, Nash equilibrium. For any policy $\\mu$ of the max-player, there exists a best response policy $\\nu^{\\dagger}(\\mu)$ of the min-player such that $V_{h}^{\\mu,\\nu^{\\dagger}(\\mu)}(s)=\\operatorname*{inf}_{\\nu}V_{h}^{\\mu,\\nu}(s)$ for ll $(s,h)$ . For simplicity,. we denote $V_{h}^{\\mu,\\dagger}:=V_{h}^{\\mu,\\nu^{\\dagger}(\\mu)}$ .Also, $\\mu^{\\dagger}(\\nu)$ and $V_{h}^{\\dagger,\\nu}$ can be defined by symmetry.Iti shown [Filar and Vrieze, 2012] that there exists a pair of policies $(\\mu^{\\star},\\nu^{\\star})$ that are best responses against each other, i.e. $V_{h}^{\\mu^{\\star},\\dagger}(s)=V_{h}^{\\mu^{\\star},\\nu^{\\star}}(s)=V_{h}^{\\dagger,\\nu^{\\star}}(s)$ $\\forall\\ (s,h)\\in S\\times[H]$ The pair of policies $(\\mu^{\\star},\\nu^{\\star})$ is called the Nash equilibrium of the Markov game, which further satisfies the following minimax property: fo all $(s,h)\\in S\\times[H]$ $\\operatorname*{sup}_{\\mu}\\operatorname*{inf}_{\\nu}\\bar{V_{h}^{\\mu,\\nu}}(s)=V_{h}^{\\mu^{\\star},\\nu^{\\star}}(s)=\\operatorname*{inf}_{\\nu}\\operatorname*{sup}_{\\mu}V_{h}^{\\mu,\\nu}(s)$ The alue functions of $(\\mu^{\\star},\\nu^{\\star})$ are called Nash value functions and we denote $V_{h}^{\\star}=V_{h}^{\\mu^{\\star},\\nu^{\\star}},Q_{h}^{\\star}=Q_{h}^{\\mu^{\\star},\\nu^{\\star}}$ for simplicity. Nash equilibrium means that no player could gain more from updating her own policy. ", "page_idx": 3}, {"type": "text", "text": "Learning objective: regret. Following previous works [Bai and Jin, 2020, Liu et al., 2021], we aim to minimize the regret, which is defined as below: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{Regret}(K)=\\sum_{k=1}^{K}\\left[V_{1}^{\\dagger,\\nu^{k}}(s_{1})-V_{1}^{\\mu^{k},\\dagger}(s_{1})\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $K$ is the number of episodes the agent interacts with the environment and $(\\mu^{k},\\nu^{k})$ are the policies executed by the agent in the $k$ -th episode. Note that any sub-linear regret bound can be transferred to a PAC guarantee according to the standard online-to-batch conversion [Jin et al., 2018]. ", "page_idx": 3}, {"type": "text", "text": "2.2  Differential Privacy in Multi-agent RL ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "For RL with self-play, each trajectory corresponds to the interaction between a pair of users and the environment. The interaction generally follows the protocol below. At time step $h$ of the $k$ -th episode, the users send their state $s_{h}^{k}$ to a central agent $\\mathcal{M}$ , then $\\mathcal{M}$ sends back a pair of actions $(a_{h}^{k},b_{h}^{k})$ for the users to take, and finally the users send their reward $r_{h}^{k}$ to $\\mathcal{M}$ . Following previous works [Vietri et al., 2020, Chowdhury and Zhou, 2022, Qia0 and Wang, 2023], here we let $\\mathcal{U}=(u_{1},\\cdot\\cdot\\cdot,u_{K})$ denote the sequence of $K$ unique 2 pairs of users who participate in the above RL protocol. Besides, each pair of users $u_{k}$ is characterized by the $\\{s_{h}^{k},r_{h}^{k}\\}_{h=1}^{H}$ information they would respond to all $(A B)^{H3}$ possible Ler $\\bar{\\mathcal{M}}(\\mathcal{U})=\\{(a_{h}^{k},b_{h}^{k})\\}_{h,k=1,1}^{H,K}$   \nactions suggested by the agent $\\mathcal{M}$ . Then a direct adaptation of differential privacy [Dwork et al., 2006] is defined below, which says that $\\mathcal{M}(\\mathcal{U})$ and all other pairs excluding $u_{k}$ together will not disclose much information about user $u_{k}$ ", "page_idx": 3}, {"type": "text", "text": "Definition 2.1 (Differential Privacy (DP). For any $\\epsilon>0$ and $\\delta\\in[0,1]$ a mechanism $\\mathcal{M}:\\mathcal{U}\\to$ $(\\mathcal{A}\\times\\mathcal{B})^{K H}$ .s $(\\epsilon,\\delta)$ -differentially private if for any possible user sequences $\\boldsymbol{\\mathcal{U}}$ and $\\mathcal{U}^{\\prime}$ that is different on one pair of users and any subset $E$ of $(\\dot{\\boldsymbol{A}}\\times\\dot{\\boldsymbol{B}})^{\\dot{K}H}$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{P}[\\mathcal{M}(\\mathcal{U})\\in E]\\le e^{\\epsilon}\\cdot\\mathbb{P}[\\mathcal{M}(\\mathcal{U}^{\\prime})\\in E]+\\delta.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$I f\\delta=0$ , we say that $\\mathcal{M}$ is $\\epsilon$ -differentially private $(\\epsilon{-}D P)$ ", "page_idx": 3}, {"type": "text", "text": "Unfortunately, privately recommending actions to the pair of users $u_{k}$ while protecting their own state and reward information is shown to be impractical even for the single-player setting. Therefore, we consider a relaxed version of DP, known as Joint Differential Privacy (JDP) [Kearns et al., 2014]. JDP says that for all pairs of users $u_{k}$ , the recommendation to all other pairs excluding $u_{k}$ will not disclose the sensitive information about $u_{k}$ . Although being weaker than DP, JDP could still provide meaningful privacy protection by ensuring that even if an adversary can observe the interactions between all other users and the environment, it is statistically hard to reconstruct the interaction between $u_{k}$ and the environment. JDP is first studied by Vietri et al. [2020] under single-agent reinforcement learning, and we extend the definition to the two-player setting. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.2 (Joint Differential Privacy (JDP). For any $\\epsilon>0$ a mechanism $\\mathcal{M}:\\mathcal{U}\\rightarrow(\\mathcal{A}\\times\\mathcal{B})^{K H}$ is e-jointdifferentiallyprivateiffor any $k\\in[K]$ anyusersequences $\\boldsymbol{\\mathcal{U}}$ and $\\mathcal{U}^{\\prime}$ that is different on the $k$ -thpairof usersand anysubset $E$ of $(\\bar{A}\\times\\bar{B})^{(K-1)H}$ \uff0c ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{P}[\\mathcal{M}_{-k}(\\mathcal{U})\\in E]\\le e^{\\epsilon}\\cdot\\mathbb{P}[\\mathcal{M}_{-k}(\\mathcal{U}^{\\prime})\\in E],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{M}_{-k}(\\mathcal{U})\\in E$ means the sequence of actions sent to all pairs of users excluding $u_{k}$ belongs toset $E$ ", "page_idx": 4}, {"type": "text", "text": "In the example of autonomous driving, JDP ensures that even if an adversary observes the interactions between cars within all time windows except one, it is hard to know what happens during the specific time window. While providing strong privacy protection, JDP requires the central agent $\\mathcal{M}$ to have access to the real trajectories from users. However, in various scenarios the users are not even willing to directly share their data with the agent. To address such circumstances, Duchi et al. [2013] developed a stronger notion of privacy named Local Differential Privacy (LDP). Now that when considering LDP, the agent can not observe the state of users, we consider the following protocol specific for LDP: at the beginning of the $k$ -th episode, the agent $\\mathcal{M}$ first sends a policy pair $\\bar{\\pi}_{k}=\\bar{(\\mu_{k},\\nu_{k})}$ to the pair of users $u_{k}$ , after running $\\pi_{k}$ and getting a trajectory $X_{k}$ $u_{k}$ privatizes their trajectory to $X_{k}^{\\prime}$ and sends it back to $\\mathcal{M}$ . We present the definition of Local DP below, which generalizes the LDP under single-agent reinforcement learning by Garcelon et al. [2021]. Briefly speaking, Local DP ensures that it is impractical for an adversary to reconstruct the whole trajectory of $u_{k}$ even if observing their whole response. ", "page_idx": 4}, {"type": "text", "text": "Definition 2.3 (Local Differential Privacy (LDP)). For any. $\\epsilon\\mathrm{~\\,~>~\\,~0~}$ a mechanism M is Elocal differentially private if for any possible trajectories $X,X^{\\prime}$ and any possible set $E\\subseteq$ $\\{{\\widetilde{\\mathcal{M}}}(X)|X$ is any possible trajectory}, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{P}[\\widetilde{\\mathcal{M}}(X)\\in E]\\le e^{\\epsilon}\\cdot\\mathbb{P}[\\widetilde{\\mathcal{M}}(X^{\\prime})\\in E].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In the example of autonomous driving, LDP ensures that the system can only observe a private version of the interactions between cars instead of the raw data. ", "page_idx": 4}, {"type": "text", "text": "Remark 2.4. Note that here our definitions of JDP and LDP both provide trajectory-wise privacy protection, which is consistent with previous works [Chowdhury and Zhou, 2022, Qiao and Wang, 2023]. Moreover, under the special case where the min-player plays a fixed and known deterministic policy(orequivalently, $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ only contains asingleaction and $B\\,=\\,1$ ),theMarkovGamesetting reduces to a single-agent Markov decision process while our JDP and LDP directly matches previous definitions for the MDP setting. Therefore, our setting strictly generalizes previous works and requires novel techniquestohandlethemin-player. ", "page_idx": 4}, {"type": "text", "text": "Remark 2.5. In the following sections we will show that LDP is consistent with sub-linear regret bounds, while it is known that we can not derive sub-linear regret bounds under the constraint of DP. We remark that there is no contradictory since here the RL protocols for DP and LDP are different. As a result, here a guarantee of LDP does not directly imply a guarantee of DP and the two notions areindeednotdirectlycomparable. ", "page_idx": 4}, {"type": "text", "text": "3 Algorithm ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this part, we introduce DP-Nash-VI (Algorithm 1). Note that the algorithm takes Privatizer as an input. We analyze the regret of Algorithm 1 for all Privatizers satisfying the Assumption 3.1 below, which includes the cases where the Privatizer is chosen as Central (for JDP) or Local (for LDP). ", "page_idx": 4}, {"type": "text", "text": "We frst inroducethedefnition f vistation counts where $\\begin{array}{r}{N_{h}^{k}(s,a,b)\\,=\\,\\sum_{i=1}^{k-1}\\mathbb{1}(s_{h}^{i},a_{h}^{i},b_{h}^{i}\\,=\\,}\\end{array}$ $s,a,b)$ denotes the visitation count of $(s,a,b)$ at time step until the beginning of the $k$ -th episode. $\\begin{array}{r}{N_{h}^{k}(s,a,b,s^{\\prime})=\\sum_{i=1}^{k-1}\\mathbb{1}(s_{h}^{i},a_{h}^{i},b_{h}^{i},s_{h+1}^{i}=s,a,b,s^{\\prime})}\\end{array}$ $(h,s,a,b,s^{\\prime})$ before the $k$ -th episode. In multi-agent RL without privacy constraints, such visitation counts aresfficientforestimating the ransition ermel $\\{P_{h}\\}_{h=1}^{H}$ and updating the exploration poliey, as in previous model-based approaches [Liu et al., 2021]. However, these counts base on the original trajectories from the users, which could reveal sensitive information. Therefore, with the concern of privacy, we can only incorporate these counts after a privacy-preserving step. In other words, we use aPrivatizerto transfer the original counts tothe private version $\\widetilde{N}_{h}^{k}(s,\\bar{a},b),\\widetilde{N}_{h}^{k}(s,a,b,s^{\\prime})$ We make the following Assumption 3.1 for Privatizer, which says that the private counts are close to real ones. Privatizers for JDP and LDP that satisfy Assumption 3.1 will be proposed in Section 5. ", "page_idx": 4}, {"type": "text", "text": "Assumption 3.1 (Private counts). For any privacy budget $\\epsilon>0$ andfailureprobability $\\beta\\in[0,1]$ \uff0c there exists some $E_{\\epsilon,\\beta}>0$ suchthatwithprobabilityatleast $1-\\beta/3,$ for all $(h,s,a,b,s^{\\prime},\\dot{k})\\dot{\\in}$ $[H]\\times{\\cal{S}}\\times{\\cal{A}}\\times{\\cal{B}}\\times{\\cal{S}}\\times[K],$ .the $\\widetilde{N}_{h}^{k}(s,a,b,s^{\\prime})$ and $\\widetilde{N}_{h}^{k}(s,a,b)$ from Privatizer satisfes: ", "page_idx": 4}, {"type": "text", "text": "1: Input: Number of episodes $K$ , privacy budget $\\epsilon$ , failure probability $\\beta$ and a Privatizer (can be   \neither Central or Local).   \n2: Initialize: Private counts $\\widetilde{N}_{h}^{1}(s,a,b)\\,=\\,\\widetilde{N}_{h}^{1}(s,a,b,s^{\\prime})\\,=\\,0$ for all $(h,s,a,b,s^{\\prime})$ .Set up the   \nconfidence bound $E_{\\epsilon,\\beta}$ w.r.t the Privatizer, the minimal gap $\\Delta\\,=\\,H$ and universal constants   \n$C_{1},C_{2}>0$ $\\iota=\\log(30H S A B K/\\beta)$ \uff1a   \n3: for $k=1,2,\\cdots\\,,K$ do   \n4V+=Vh+1(=0 do   \n6: for $(s,a,b)\\in\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{B}$ do   \n7: Compute private transition kernel $\\widetilde{P}_{h}^{k}(\\cdot|s,a,b)$ as in (1).   \n8: Comp $\\begin{array}{r l}&{\\mathfrak{u t e}\\,\\gamma_{h}^{k}(s,a,b)=\\frac{C_{1}}{H}\\cdot\\widetilde{P}_{h}^{k}(\\overline{{V}}_{h+1}^{k}-\\underline{{V}}_{h+1}^{k})(s,a,b).}\\\\ &{\\mathfrak{u t e}\\,\\Gamma_{h}^{k}(s,a,b)=C_{2}\\sqrt{\\frac{\\mathrm{Var}_{\\widetilde h}(\\cdot\\vert s,a,b)}{\\widetilde P_{h}^{k}(\\cdot\\vert s,a,b)}\\!\\left[\\left(\\frac{\\overline{{V}}_{h+1}^{k}+\\underline{{V}}_{h+1}^{k}}{2}\\right)(\\cdot)\\right]}\\cdot a}\\\\ &{\\overline{{Q}}_{h}^{k}(s,a,b)=\\operatorname*{min}\\{\\sum_{s^{\\prime}}\\widetilde{P}_{h}^{k}(s^{\\prime}\\vert s,a,b)\\cdot\\overline{{V}}_{h+1}^{k}(s^{\\prime})+[r_{h}+\\gamma_{h}^{k}+\\Gamma_{h}^{k}](s,a,b),H\\}.}\\\\ &{\\underline{{Q}}_{h}^{k}(s,a,b)=\\operatorname*{max}\\{\\sum_{s^{\\prime}}\\widetilde{P}_{h}^{k}(s^{\\prime}\\vert s,a,b)\\cdot\\underline{{V}}_{h+1}^{k}(s^{\\prime})+[r_{h}-\\gamma_{h}^{k}-\\Gamma_{h}^{k}](s,a,b),0\\}.}\\end{array}$   \n9:   \n10: UCE   \n11: LCB   \n12: end for   \n13: for $s\\in S$ do   \n14: Compute the policy $\\pi_{h}^{k}(\\cdot,\\cdot|s)=\\mathrm{CCE}(\\overline{{Q}}_{h}^{k}(s,\\cdot,\\cdot),\\underline{{Q}}_{h}^{k}(s,\\cdot,\\cdot))$   \n15: Compute the ale funetions $\\overline{{V}}_{h}^{k}(s)=\\mathbb{E}_{\\pi_{h}^{k}}\\overline{{Q}}_{h}^{k}(s)$ \uff0c $\\begin{array}{r}{\\underline{{V}}_{h}^{k}(s)=\\mathbb{E}_{\\pi_{h}^{k}}\\underline{{Q}}_{h}^{k}(s).}\\end{array}$   \n16: end for   \n17: end for   \n18: Deploy policy $\\pi^{k}=(\\pi_{1}^{k},\\cdot\\cdot\\cdot,\\pi_{H}^{k})$ and get trajectory $(s_{1}^{k},a_{1}^{k},b_{1}^{k},r_{1}^{k},\\cdot\\cdot\\cdot\\,,s_{H+1}^{k})$   \n19: Update the private counts to $\\widetilde N^{k+1}$ via Privatizer.   \n20: $(\\overline{{V}}_{1}^{k}-\\underline{{V}}_{1}^{k})(s_{1})<\\Delta$ then   \n21: $\\Delta=(\\overline{{V}}_{1}^{k}-\\underline{{V}}_{1}^{k})(s_{1})$ and $\\pi^{\\mathrm{out}}=\\pi^{k}=(\\pi_{1}^{k},\\cdot\\cdot\\cdot\\,,\\pi_{H}^{k})$   \n22: end if   \n23: end for   \n24: Return: The marginal policies of $\\pi^{\\mathrm{out}}$ $(\\mu^{\\mathrm{out}},\\nu^{\\mathrm{out}})$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{I)\\,|\\tilde{N}_{h}^{k}(s,a,b,s^{\\prime})-N_{h}^{k}(s,a,b,s^{\\prime})|\\le E_{\\epsilon,\\beta}\\,,\\,|\\tilde{N}_{h}^{k}(s,a,b)-N_{h}^{k}(s,a,b)|\\le E_{\\epsilon,\\beta}\\,.\\,\\tilde{N}_{h}^{k}(s,a,b,s^{\\prime})>0.}\\\\ &{2)\\,\\,\\tilde{N}_{h}^{k}(s,a,b)=\\sum_{s^{\\prime}\\in S}\\tilde{N}_{h}^{k}(s,a,b,s^{\\prime})\\ge N_{h}^{k}(s,a,b).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Given the private counts satisfying Assumption 3.1, the private estimate of transition kernel is defined asbelow. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widetilde{P}_{h}^{k}(s^{\\prime}|s,a,b)=\\frac{\\widetilde{N}_{h}^{k}(s,a,b,s^{\\prime})}{\\widetilde{N}_{h}^{k}(s,a,b)},\\;\\;\\forall\\;(h,s,a,b,s^{\\prime},k).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Remark 3.2. Assumption 3.1 is a generalization of Assumption 3.1 of Qiao and Wang [2023] to the two-player setting. The assumption (2) guarantees that the private transition kernel $\\widetilde{P}_{h}^{k}(\\cdot|s,a,b)$ is $a$ valid probability distribution, which enables our usage of Bernstein-type bonus. Besides, $\\widetilde{P}$ isclose to the empirical transition kernel based on original visitation counts according to Assumption $(I)$ ", "page_idx": 5}, {"type": "text", "text": "Algorithmic design. Following previous non-private approaches [Liu et al., 2021], DP-Nash-VI (Algorithm 1) maintains a pair of value functions $\\overline{{Q}}$ and $Q$ which are the upper bound and lower bound of the Q value of the current policy when facing best responses (with high probability). More specifically, we use private visitation counts $\\widetilde{N}_{h}^{k}$ to construct a private estimate of transition kernel $\\widetilde{P}_{h}^{k}$ (line 7) and a pair of private bonus $\\gamma_{h}^{k}$ (line 8) and $\\Gamma_{h}^{k}$ (line 9). Intuitively, the frst term of $\\Gamma_{h}^{k}$ .s derived from Bernstein's inequality while the second term is the additional bonus due to differential privacy. Next we do value iteration with bonuses to construct the UCB function $\\overline{{Q}}_{h}^{k}$ (line 10) and the LCB function $\\boldsymbol{\\underline{{Q}}}_{h}^{k}$ (line 11). The policy $\\pi^{k}$ for the $k$ -th episode is calculated using the CCE function (discussed below) and we run $\\pi^{k}$ to collect a trajectory (line 14,18). Finally, the Privatizer transfers the non-private counts to private ones for the next episode (line 19). The output policy $\\pi^{\\mathrm{out}}$ ischosen as the policy $\\pi^{k}$ withminimalgap $(\\overline{{V}}_{1}^{k}-\\underline{{V}}_{1}^{k})(s_{1})$ (line 21). Decomposing the output policy, the output policy $(\\mu^{\\mathrm{out}},\\nu^{\\mathrm{out}})$ for both players are the marginal policies of $\\pi^{\\mathrm{out}}$ , i.e. $\\begin{array}{r}{\\bar{\\mu}_{h}^{\\mathrm{out}}(\\cdot|s\\bar{)}=\\bar{\\sum_{b\\in\\mathcal{B}}\\pi_{h}^{\\mathrm{out}}(\\cdot,\\bar{b}|s)}}\\end{array}$ and $\\begin{array}{r}{\\nu_{h}^{\\mathrm{out}}(\\cdot|s)=\\sum_{a\\in\\mathcal{A}}\\pi_{h}^{\\mathrm{out}}(a,\\cdot|s)}\\end{array}$ for all $(h,s)\\in[H]\\times S$ ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Coarse Correlated Equilibrium (CCE). Intuitively speaking, CCE of a Markov Game is a potentially correlated policy where no player could benefit from unilateral unconditional deviation. As a computationally friendly relaxation of Nash Equilibrium, CCE has been applied by previous works [Xie et al., 2020, Liu et al., 2021] to design efficient algorithms. Formally, for any two functions $\\overline{{Q}}(\\cdot,\\cdot),\\underline{{Q}}(\\cdot,\\cdot):\\mathcal{A}\\times\\mathcal{B}\\to[0,H]$ $\\mathrm{CCE}(\\overline{{Q}},\\underline{{Q}})$ returns a policy $\\pi\\in\\Delta(A\\times B)$ suchthat ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{(a,b)\\sim\\pi}\\overline{{Q}}(a,b)\\geq\\operatorname*{max}_{a^{\\prime}}\\mathbb{E}_{(a,b)\\sim\\pi}\\overline{{Q}}(a^{\\prime},b),\\ \\ \\mathbb{E}_{(a,b)\\sim\\pi}\\underline{{Q}}(a,b)\\leq\\operatorname*{min}_{b^{\\prime}}\\mathbb{E}_{(a,b)\\sim\\pi}\\underline{{Q}}(a,b^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Since Nash Equilibrium (NE) is a special case of CCE and a NE always exists, a CCE always exists. Moreover, a CCE can be derived in polynomial time via linear programming. Note that the policies given by CCE can be correlated for the two players, therefore deploying such policy requires the cooperation of both players (line 18). ", "page_idx": 6}, {"type": "text", "text": "4 Main results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We first state the regret analysis of DP-Nash-VI (Algorithm 1) based on Assumption 3.1, which can be combined with any Privatizers. The proof of Theorem 4.1 is sketched in Appendix B with details in the Appendix. Note that $(\\mu^{k},\\nu^{k})$ denote the marginal policies of $\\pi^{k}$ for both players. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.1. For any privacy budget $\\epsilon\\,>\\,0$ failure probability $\\beta\\,\\in\\,[0,1]$ and any Privatizer satisfying Assumption 3.1, with probability at least $1-\\beta$ theregret of $D P$ -Nash-VI (Algorithm $^{\\,I}$ )is ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname{Regret}(K)=\\sum_{k=1}^{K}\\left[V_{1}^{\\dagger,\\nu^{k}}(s_{1})-V_{1}^{\\mu^{k},\\dagger}(s_{1})\\right]\\leq\\widetilde{O}\\left(\\sqrt{H^{2}S A B T}+H^{2}S^{2}A B E_{\\epsilon,\\beta}\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $K$ is the number of episodes and $T=H K$ ", "page_idx": 6}, {"type": "text", "text": "Under the special case where the privacy budget $\\epsilon\\rightarrow\\infty$ (i.e. there is no privacy concern), plugging $E_{\\epsilon,\\beta}\\;=\\;0$ in Theorem 4.1 will imply a regret bound of $\\widetilde{\\cal O}(\\sqrt{H^{2}S A B T})$ . Such result directly matches the best known result for regret minimization without privacy constraints [Liu et al., 2021] and nearly matches the lower bound of $\\Omega(\\sqrt{H^{2}S(A+B)T})$ [Bai and Jin, 2020]. Furthermore, under the special case of single-agent MDP (where $B=1$ ), our result reduces to ${\\mathrm{Regret}}(K)\\leq$ $\\widetilde{O}(\\sqrt{H^{2}S A T}+H^{2}S^{2}A E_{\\epsilon,\\beta})$ . Such result matches the best known result under the same set of conditions (Theorem 4.1 of Qiao and Wang [2023]). Therefore, Theorem 4.1 is a generalization of the best known results under MARL [Liu et al., 2021] and Differentially Private (single-agent) RL [Qiao and Wang, 2023] simultaneously. ", "page_idx": 6}, {"type": "text", "text": "PAC guarantee. Recall that we output a policy $\\pi^{\\mathrm{out}}$ whose marginal policies are $(\\mu^{\\mathrm{out}},\\nu^{\\mathrm{out}})$ We highlight that the output policy for each player is a single Markov policy that is convenient to store and deploy. Moreover, as a corollary of the regret bound, we give a PAC bound for the output policy. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.2. For any privacy budget $\\begin{array}{r l r}{\\epsilon}&{{}>}&{0,}\\end{array}$ failureprobability $\\beta\\ \\ \\in\\ \\ [0,1]$ and any Privatizer that satisfies Assumption 3.1, if the number of episodes satisfies that $K\\_{\\_}$ $\\begin{array}{r}{\\widetilde\\Omega\\left(\\frac{H^{3}S A B}{\\alpha^{2}}+\\operatorname*{min}\\left\\{K^{\\prime}|\\frac{H^{2}S^{2}A B\\bar{E}_{\\epsilon,\\beta}}{K^{\\prime}}\\leq\\alpha\\right\\}\\right)}\\end{array}$ wih pobabiry $1-\\beta,$ $(\\mu^{\\mathrm{out}},\\nu^{\\mathrm{out}})$ $\\alpha$ aproximate Nash, i.e., $V_{1}^{\\dagger,{\\nu}^{\\mathrm{out}}}(s_{1})-V_{1}^{{\\mu}^{\\mathrm{out}},\\dagger}(s_{1})\\leq\\alpha$ ", "page_idx": 6}, {"type": "text", "text": "The proof is deferred to Appendix C.4. Here the second term of the sample complexity bound4 ensures that the additional cost due to DP is bounded by $O(\\alpha)$ . The detailed PAC guarantees under the special cases where the Privatizer is either Central or Local will be provided in Section 5. ", "page_idx": 6}, {"type": "text", "text": "5 Privatizers for JDP and LDP ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we propose Privatizers that provide DP guarantees (JDP or LDP) while satisfying Assumption 3.1. The proofs for this section can be found in Appendix D. ", "page_idx": 6}, {"type": "text", "text": "5.1  Central Privatizer for Joint DP ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Given the number of episodes $K$ , the Central Privatizer applies $K$ -bounded Binary Mechanism [Chan et al., 2011] to privatize all the visitation counter streams $\\mathbf{\\dot{\\boldsymbol{N}}}_{h}^{k}(s,a,b)$ \uff0c $N_{h}^{k}(s,a,b,s^{\\prime})$ , thus protecting the information of all single users. Briefly speaking, Binary mechanism takes a stream of partial sums as input and outputs a surrogate stream satisfying differential privacy, while the error for each item scales only logarithmically on the length of the stream?. Here in multi-agent RL, for each $(h,s,a,b)$ , the stream $\\begin{array}{r}{\\{N_{h}^{k}(s,a,b)=\\sum_{i=1}^{k-1}\\mathbb{1}(s_{h}^{i},a_{h}^{i},b_{h}^{i}=s,a,b)\\}_{k\\in[K]}}\\end{array}$ can be considered as the partial sums of $\\{\\mathbb{1}(s_{h}^{i},a_{h}^{i},b_{h}^{i}=s,a,b)\\}$ . Therefore, after observing $\\bar{\\mathbb{1}}(\\dot{s}_{h}^{k},a_{h}^{k},b_{h}^{k}=s,a,b)$ at the endof episode $k$ thinaryMechanisilutuarivateerionf $\\begin{array}{r}{\\sum_{i=1}^{k}\\mathbb{1}(s_{h}^{i},a_{h}^{i},b_{h}^{i}=s,a,b)}\\end{array}$ However, Binary Mechanism alone does not satisfy (2) of Assumption 3.1, and a post-processing step is required. To sum up, we let the Central Privatizer follow the workflow below: ", "page_idx": 7}, {"type": "text", "text": "Given the privacy budget for JDP $\\epsilon>0$ ", "page_idx": 7}, {"type": "text", "text": "(1) For all $(h,s,a,b,s^{\\prime})$ , we apply Binary Mechanism (Algorithm 2 in Chan et al. [2011]) with input prameter $\\begin{array}{r}{\\epsilon^{\\prime}=\\frac{\\epsilon}{2H\\log K}}\\end{array}$ to privtaizealthe vstaion coueter seamns $\\{N_{h}^{k}(s,a,b)\\}_{k\\in[K]}$ and $\\{N_{h}^{k}(s,a,b,s^{\\prime})\\}_{k\\in[K]}$ We denote the output of Binary Mechanism by $\\widehat{N}_{h}^{k}$ ", "page_idx": 7}, {"type": "text", "text": "(2) The private counts $\\widetilde{N}_{h}^{k}$ are derived through Section 5.3 with $\\begin{array}{r}{E_{\\epsilon,\\beta}=O(\\frac{H}{\\epsilon}\\log(H S A B K/\\beta)^{2})}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "Our Central Privatizer satisfies the privacy guarantee below. ", "page_idx": 7}, {"type": "text", "text": "Lemma5.1.For anypossible $\\epsilon,\\beta$ theCentralPrivatizer satisfies $\\epsilon$ JDPandAssumption3.1with $\\begin{array}{r}{E_{\\epsilon,\\beta}=\\widetilde{O}(\\frac{H}{\\epsilon})}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "Combining Lemma 5.1 with Theorem 4.1 and Theorem 4.2, we have the following regret $\\&$ PAC guaranteeunder $\\epsilon_{}$ JDP. ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.2 (Results under JDP). For any possible $\\epsilon,\\beta$ withprobability $1-\\beta$ theregret from running $D P$ -Nash-VI(Algorithm $^{\\,I}$ ) instantiated with Central Privatizer satisfies: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{Regret}(K)\\le\\widetilde{O}(\\sqrt{H^{2}S A B T}+H^{3}S^{2}A B/\\epsilon).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "$K$ $\\begin{array}{r}{\\widetilde\\Omega(\\frac{H^{3}S A B}{\\alpha^{2}}+\\frac{H^{3}S^{2}A B}{\\epsilon\\alpha})}\\end{array}$ $1-\\beta$ $(\\mu^{\\mathrm{out}},\\nu^{\\mathrm{out}})$ $\\alpha$ ", "page_idx": 7}, {"type": "text", "text": "Similar to the single-agent (MDP) setting $\\langle B=1\\rangle$ 0, the additional cost due to JDP is a lower order term under the most prevalent regime where the privacy budget $\\epsilon$ is a constant. When applied to the single-agent case, our regret matches the best known regret $\\widetilde{O}(\\sqrt{H^{2}S A T}+H^{3}S^{2}A/\\epsilon)$ [Qiao and Wang, 2023]. Moreover, when compared to the regret lower bound below, our main term is nearly optimal while the lower order term has optimal dependence on $\\epsilon$ ", "page_idx": 7}, {"type": "text", "text": "Theorem 5.3. For any algorithm Alg satisfying $\\epsilon$ -JDP, there exists a Markov Game such that the expected regret from running $\\mathrm{Alg}$ for $K$ episodes $T=H K$ steps) satisfies: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[{\\mathrm{Regret}}(K)\\right]\\geq\\widetilde{\\Omega}(\\sqrt{H^{2}S(A+B)T}+\\frac{H S(A+B)}{\\epsilon}).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The regret lower bound results from the lower bound for the non-private learning [Bai and Jin, 2020] and an adaptation of the lower bound under JDP guarantees [Vietri et al., 2020] to the multi-player setting. Details are deferred to the appendix. ", "page_idx": 7}, {"type": "text", "text": "5.2Local Privatizer for Local DP ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "At the end of episode $k$ , the Local Privatizer perturbs the statistics calculated from the new trajectory before sending it to the agent. Since the set of original visitation counts $\\{\\sigma_{h}^{k}(s,a,b)\\,=$ $\\mathbb{1}(s_{h}^{k},a_{h}^{k},b_{h}^{k}\\,=\\,s,a,b)\\}_{(h,s,a,b)}$ has $\\ell_{1}$ sensitivity $H$ : we can achieve $\\frac{\\epsilon}{2}$ -LDP by directly adding Laplace noise, i.e., $\\begin{array}{r}{\\widetilde{\\sigma}_{h}^{k}(s,a,b)=\\sigma_{h}^{k}(s,a,b)+\\mathrm{Lap}(\\frac{2H}{\\epsilon})}\\end{array}$ . Similarly, repeating the above perturbation $\\{\\mathbb{1}(s_{h}^{k},a_{h}^{k},b_{h}^{k},s_{h+1}^{k}=\\mathit{s},a,b,s^{\\prime})\\}_{(h,\\mathit{s},a,b,s^{\\prime})}$ Wwillead toidentical results. herefore,the Llocal Privatizer with budget $\\epsilon$ is as below: ", "page_idx": 7}, {"type": "text", "text": "(1) We perturb $\\sigma_{h}^{k}(s,a,b)\\:=\\:\\mathbb{1}(s_{h}^{k},a_{h}^{k},b_{h}^{k}\\:=\\:s,a,b)$ and $\\sigma_{h}^{k}(s,a,b,s^{\\prime})\\,=\\,\\mathbb{1}(s_{h}^{k},a_{h}^{k},b_{h}^{k},s_{h+1}^{k}\\,=\\,$ $s,a,b,s^{\\prime})$ by adding independent Laplace noises: for all $(h,s,a,b,s^{\\prime},k)$ \uff0c ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\widetilde{\\sigma}_{h}^{k}(s,a,b)=\\sigma_{h}^{k}(s,a,b)+\\operatorname{Lap}\\left(\\frac{2H}{\\epsilon}\\right),\\,\\,\\,\\widetilde{\\sigma}_{h}^{k}(s,a,b,s^{\\prime})=\\sigma_{h}^{k}(s,a,b,s^{\\prime})+\\operatorname{Lap}\\left(\\frac{2H}{\\epsilon}\\right).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "(2) Then the noisy counts are derived according to ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\widehat{N}_{h}^{k}(s,a,b)=\\sum_{i=1}^{k-1}\\widetilde{\\sigma}_{h}^{i}(s,a,b),\\;\\;\\widehat{N}_{h}^{k}(s,a,b,s^{\\prime})=\\sum_{i=1}^{k-1}\\widetilde{\\sigma}_{h}^{i}(s,a,b,s^{\\prime}),\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "and the private counts $\\widetilde{N}_{h}^{k}$ are solved through Section 5.3 with $\\begin{array}{r}{E_{\\epsilon,\\beta}=O(\\frac{H}{\\epsilon}\\sqrt{K\\log(H S A B K/\\beta)})}\\end{array}$ ", "page_idx": 8}, {"type": "text", "text": "Our Local Privatizer satisfies the privacy guarantee below. ", "page_idx": 8}, {"type": "text", "text": "Lemma 5.4. For any possible $\\epsilon,\\beta$ , the Local Privatizer satisfes $\\epsilon$ -LDP and Assumption 3.1 with $\\begin{array}{r}{E_{\\epsilon,\\beta}=\\widetilde{O}(\\frac{H}{\\epsilon}\\sqrt{K})}\\end{array}$ ", "page_idx": 8}, {"type": "text", "text": "Combining Lemma 5.4 with Theorem 4.1 and Theorem 4.2, we have the following regret $\\&$ PAC guaranteeunder $\\epsilon$ -LDP. ", "page_idx": 8}, {"type": "text", "text": "Theorem 5.5 (Results under LDP). For any possible $\\epsilon,\\beta_{\\cdot}$ with probability $1-\\beta$ the regret from running $D P$ -Nash-VI(Algorithm $^{\\,I}$ ) instantiated with Local Privatizer satisfies: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathrm{Regret}(K)\\le\\widetilde{O}\\left(\\sqrt{H^{2}S A B T}+S^{2}A B\\sqrt{H^{5}T}/\\epsilon\\right).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Moreover, ifthe number of episodes K is larger than  (SAB + H\\$AB2) with probability $1-\\beta$ the output policy $(\\mu^{\\mathrm{out}},\\nu^{\\mathrm{out}})$ is $\\alpha$ -approximateNash. ", "page_idx": 8}, {"type": "text", "text": "Similar to the single-agent case, the additional cost due to LDP is a multiplicative factor to the regret bound. When applied to the single-agent case, our regret matches the best known regret $\\widetilde{O}\\left(\\sqrt{H^{2}S A T}+S^{2}A\\sqrt{H^{5}T}/\\epsilon\\right)$ [Qiao and Wang, 2023]. Moreover, we state the lower bound. ", "page_idx": 8}, {"type": "text", "text": "Theorem 5.6. For any algorithm Alg satisfying $\\epsilon$ -LDP, there exists a Markov Game such that the expected regret fromrunning $\\mathrm{Alg}$ for $K$ episodes $T=H K$ steps) satisfies: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\mathrm{Regret}(K)\\right]\\geq\\widetilde{\\Omega}\\left(\\sqrt{H^{2}S(A+B)T}+\\frac{\\sqrt{H S(A+B)T}}{\\epsilon}\\right).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The lower bound is adapted from Garcelon et al. [2021]. While our regret has optimal dependence on $\\epsilon$ and $K$ , the optimal dependence on $H,S,A,B$ remains open. ", "page_idx": 8}, {"type": "text", "text": "5.3  The post-processing step ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Now we introduce the post-processing step. At the end of episode $k$ given the noisy counts $\\widehat{N}_{h}^{k}(s,a,b)$ and $\\widehat{N}_{h}^{k}(s,a,b,s^{\\prime})$ for all $(h,s,a,b,s^{\\prime})$ , the private visitation counts are constructed as following: for all $(h,s,a,b)$ ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Big\\{\\widetilde{N}_{h}^{k}(s,a,b,s^{\\prime})\\Big\\}_{s^{\\prime}\\in\\mathcal{S}}=\\underset{\\{x_{s^{\\prime}}\\}_{s^{\\prime}\\in\\mathcal{S}}}{\\mathrm{argmin}}\\ \\underset{s^{\\prime}\\in\\mathcal{S}}{\\mathrm{max}}\\,\\left|x_{s^{\\prime}}-\\widehat{N}_{h}^{k}(s,a,b,s^{\\prime})\\right|}\\\\ &{\\mathrm{uch\\,that}\\ \\left|\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}}x_{s^{\\prime}}-\\widehat{N}_{h}^{k}(s,a,b)\\right|\\le\\frac{E_{\\epsilon,\\beta}}{4}\\ \\mathrm{and}\\ x_{s^{\\prime}}\\ge0,\\,\\forall s^{\\prime}.\\Big.\\ \\ \\ \\ \\widetilde{N}_{h}^{k}(s,a,b)=\\displaystyle\\sum_{s^{\\prime}\\in\\mathcal{S}}\\widetilde{N}_{h}^{k}(s,a,b,s^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Lastly, we add a constant term to each count to ensure no underestimation (with high probability). ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\widetilde{N}_{h}^{k}(s,a,b,s^{\\prime})=\\widetilde{N}_{h}^{k}(s,a,b,s^{\\prime})+\\frac{E_{\\epsilon,\\beta}}{2S},\\quad\\widetilde{N}_{h}^{k}(s,a,b)=\\widetilde{N}_{h}^{k}(s,a,b)+\\frac{E_{\\epsilon,\\beta}}{2}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Remark 5.7. Solving problem (7) is equivalent to solving: ", "page_idx": 9}, {"type": "equation", "text": "$$\ns.t.\\ \\left|x_{s^{\\prime}}-\\widehat{N}_{h}^{k}(s,a,b,s^{\\prime})\\right|\\leq t,\\ x_{s^{\\prime}}\\geq0,\\ \\forall\\,s^{\\prime}\\in\\mathcal{S},\\ \\left|\\sum_{s^{\\prime}\\in\\mathcal{S}}x_{s^{\\prime}}-\\widehat{N}_{h}^{k}(s,a,b)\\right|\\leq\\frac{E_{\\epsilon,\\beta}}{4},\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "whichisaLinearProgrammingproblemwith $O(S)$ variables and $O(S)$ linear constraints. This can be solved in polynomial time [Nemhauser and Wolsey, 1988]. Note that the computation of $C C E$ (line 14 in Algorithm 1) is also a $L P$ problem,thereforethecomputationalcomplexityof $D P$ -Nash-VI is dominatedby $O(H S A B K)$ LinearProgramming problems,which is computationally friendly. ", "page_idx": 9}, {"type": "text", "text": "We summarize the properties of private counts $\\widetilde{N}_{h}^{k}$ below, which says that the post-processing step ensures that our private transition kernel estimate is a valid probability distribution while only enlarging the error by a constant factor. ", "page_idx": 9}, {"type": "text", "text": "Lemma 5.8. Suppose $\\widehat{N}_{h}^{k}$ satisfies that with probability $\\textstyle1-{\\frac{\\beta}{3}}$ , uniformly over all $(h,s,a,b,s^{\\prime},k),$ ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\Big|\\widehat{N}_{h}^{k}(s,a,b,s^{\\prime})-N_{h}^{k}(s,a,b,s^{\\prime})\\Big|\\le\\frac{E_{\\epsilon,\\beta}}{4},\\quad\\Big|\\widehat{N}_{h}^{k}(s,a,b)-N_{h}^{k}(s,a,b)\\Big|\\le\\frac{E_{\\epsilon,\\beta}}{4},\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "thenthe $\\widetilde{N}_{h}^{k}$ derived above satisfies Assumption 3.1. ", "page_idx": 9}, {"type": "text", "text": "5.4  Some discussions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this part, we generalize the Privatizers in Qiao and Wang [2023] (for single-agent case) to the two-player setting, which enables our usage of Bernstein-type bonuses. Such techniques lead to a tight regret analysis and a near-optimal \u201cnon-private part\u2019 of the regret bound eventually. ", "page_idx": 9}, {"type": "text", "text": "Meanwhile, the additional cost due to DP has sub-optimal dependence on parameters regarding the Markov Game. The issue appears even in the single-agent case and is considered to be inherent to model-based algorithms due to the explicit estimation of private transitions [Garcelon et al., 2021]. The improvement requires new algorithmic designs (e.g., private Q-learning) and we leave those as futureworks. ", "page_idx": 9}, {"type": "text", "text": "Lastly, the Laplace Mechanism can be replaced with other mechanisms, such as Gaussian Mechanism [Dwork et al., 2014] with approximate DP guarantee (or zCDP). The regret and PAC guarantees are readily derived by plugging in the corresponding $E_{\\epsilon,\\beta}$ to Theorem 4.1 and Theorem 4.2. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We take the initial steps to study trajectory-wise privacy protection in multi-agent RL. We extend the definitions of Joint DP and Local DP to multi-player RL. In addition, we design a provablyefficient algorithm: DP-Nash-VI (Algorithm 1) that could satisfy either of the two DP constraints with corresponding regret guarantee. Moreover, our regret bounds strictly generalize the best known results under DP single-agent RL. There are various interesting future directions, such as improving the additional cost due to DP via model-free approaches and considering Markov Games with function approximations. We believe the techniques in this paper could serve as basic building blocks. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The research is partially supported by NSF Awards #2007117 and $\\#2048091$ . The work was done while DQ and YW were with the Department of Computer Science at UCSB. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based reinforcement learning with value-targeted regression. In International Conference on Machine Learning, pages 463-474.PMLR,2020. ", "page_idx": 9}, {"type": "text", "text": "Mohammad Gheshlaghi Azar, Ian Osband, and R\u00e9mi Munos. Minimax regret bounds for reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 263-272. JMLR. 0rg, 2017. ", "page_idx": 9}, {"type": "text", "text": "Yoram Bachrach, Richard Everet, Edward Hughes, Angeliki Lazaridou, Jol Z Leibo, Marc Lantt, Michael Jhanson, WojciechMCzarnecki, and Thore Graeel Negotiating team formationusng deep reinforcement learning. Artificial Intelligence, 288:103356, 2020.   \nYu Bai and Chi Jin. Provable self-play algorithms for competitive reinforcement learning. In International Conference on Machine Learning, pages 551-560. PMLR, 2020.   \nYu Bai, Chi Jin, and Tiancheng Yu. Near-optimal reinforcement learning with self-play. Advances in neural information processing systems, 33:2159-2170, 2020.   \nBorja Balle, Maziar Gomrokchi, and Doina Precup. Differentially private policy evaluation. In International Conference on Machine Learning, pages 2130-2138. PMLR, 2016.   \nGavin Brown, Mark Bun, Vitaly Feldman, Adam Smith, and Kunal Talwar. When is memorization of irrelevant training data necessary for high-accuracy learning? In ACM SIGACT Symposium on Theory of Computing, pages 123-132, 2021.   \nNoam Brown and Tuomas Sandholm. Superhuman ai for multiplayer poker. Science, 365(6456): 885-890, 2019.   \nMark Bun and Thomas Steinke. Concentrated diferential privacy: Simplifications, extensions, and lower bounds. In Theory of Cryptography Conference, pages 635-658. Springer, 2016.   \nNicholaCalin Chang LUfaEngsson,J Ks andDawngh ecret har: al ating and testing unintended memorization in neural networks. In USENIX Security Symposium (USENIX Security 19), pages 267-284, 2019.   \nT-H Hubert Chan, Elaine Shi, and Dawn Song. Private and continual release of statistics. ACM Transactions on Information and System Security (TISSEC), 14(3):1-24, 2011.   \nSayak Ray Chowdhury and Xingyu Zhou. Differentially private regret minimization in episodic markov decision processes. In Proceedings of the AAAl Conference on Artificial Intelligence, 2022.   \nSayak Ray Chowdhury, Xingyu Zhou, and Ness Shroff. Adaptive control of differentially private linear quadratic systems. In 2021 IEEE International Symposium on Information Theory (ISIT), pages 485-490. IEEE, 2021.   \nSayak Ray Chowdhury, Xingyu Zhou, and Nagarajan Natarajan. Differentially private reward estimation with preference feedback. arXiv preprint arXiv:2310.19733, 2023.   \nQiwen Cui, Kaiqing Zhang, and Simon Du. Breaking the curse of multiagents in a large state space: Rl in markov games with independent linear function approximation. In The Thirty Sixth Annual Conference on Learning Theory, pages 2651-2652. PMLR, 2023.   \nChris Cundy and Stefano Ermon. Privacy-constrained policies via mutual information regularized policy gradients. arXiv preprint arXiv:2012.15019, 2020.   \nChristoph Dann, Tor Lattimore, and Emma Brunskill. Unifying pac and regret: Uniform pac bounds for episodic reinforcement learning. In Advances in Neural Information Processing Systems, pages 5713-5723, 2017.   \nJohn C Duchi, Michael I Jordan, and Martin J Wainwright. Local privacy and statistical minimax rates. In2013 IEEE 54th Anual Symposium on Foundations of Computer Science, pages 429-438. IEEE, 2013.   \nCynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Theory of cryptography conference, pages 265-284. Springer, 2006.   \nCynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Found. Trends Theor. Comput. Sci., 9(3-4):211-407, 2014.   \nJerzy Filar and Koos Vrieze. Competitive Markov decision processes. Springer Science & Business Media, 2012.   \nEvrard Garcelon, Vianney Perchet, Ciara Pike-Burke, and Matteo Pirotta. Local differential privacy for regret minimization in reinforcement learning. Advances in Neural Information Processing Systems, 34, 2021.   \nParham Gohari, Matthew Hale, and Ufuk Topcu. Privacy-engineered value decomposition networks for cooperative multi-agent reinforcement learning. In 2023 62nd IEEE Conference on Decision and Control (CDC), pages 8038-8044. IEEE, 2023.   \nMd Tamjid Hossain and John WT Lee. Hiding in plain sight: Differential privacy noise exploitation for evasion-resilient localized poisoning attacks in multiagent reinforcement learning. In 2023 International Conference on Machine Learning and Cybernetics (ICMLC), pages 209-216. IEEE, 2023.   \nMd Tamjid Hossain, Hung Manh La, Shahriar Badsha, and Anton Netchaev. Brnes: Enabling security and privacy-aware experience sharing in multiagent robotic and autonomous systems. In 2023 IEEE/RSJ International Conference on Inteligent Robots and Systems (IROS), pages 9269-9276. IEEE,2023.   \nJustin Hsu, Zhiyi Huang, Aaron Roth, Tim Roughgarden, and Zhiwei Steven Wu. Private matchings and allocations. In Proceedings of the forty-sixth annual ACM symposium on Theory of computing, pages 21-30, 2014.   \nThomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement learning. Journal of Machine Learning Research, 11(4), 2010.   \nChi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is q-learning provably efficient? In Advances in Neural Information Processing Systems, pages 4863-4873, 2018.   \nChi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In Conference on Learning Theory, pages 2137-2143. PMLR, 2020.   \nChi Jin, Qinghua Liu, Yuanhao Wang, and Tiancheng Yu. V-laning-a simple, effcient, decentralized algorithm for multiagent rl. arXiv preprint arXiv:2110.14555, 2021.   \nPeter Kairouz, Brendan McMahan, Shuang Song, Om Thakkar, Abhradeep Thakurta, and Zheng Xu. Practical and private (deep) learning without sampling or shufling. In International Conference on Machine Learning, pages 5213-5225. PMLR, 2021.   \nMichael Kearns, Mallesh Pai, Aaron Roth, and Jonathan Ulman. Mechanism design in large games: Incentives and privacy. In Proceedings of the 5th conference on Innovations in theoretical computer science, pages 403-410, 2014.   \nJonathanLbensold, William Hamilton,orjBalle, andDoina Precup. Actor critic withdifferentially private critic. arXiv preprint arXiv: 1910.05876, 2019.   \nChonghuaLiaJifaanduaquaGLallydiffetallyprivatiornt for linear mixture markov decision processes. In Asian Conference on Machine Learning, pages 627-642. PMLR, 2023.   \nQinghua Liu, Tiancheng Yu, Yu Bai, and Chi Jin. A sharp analysis of model-based reinforcement learning with self-play. In International Conference on Machine Learning, pages 7001-7010. PMLR, 2021.   \nPaul Luyo, Evrard Garcelon, Alessandro Lazaric, and Matteo Pirotta. Differentially private exploration in reinforcement learning with linear representation. arXiv preprint arXiv:2112.01585, 2021.   \nWeichao Mao, Lin Yang, Kaiqing Zhang, and Tamer Basar. On improving model-free algorithms for decentralized multi-agent reinforcement learning. In International Conference on Machine Learning, pages 15007-15049. PMLR, 2022.   \nGeorge Nemhauser and Laurence Wolsey. Polynomial-time algorithms for linear programming. Integer and Combinatorial Optimization, pages 146-181, 1988.   \nDung Daniel T Ngo, Giuseppe Vietri, and Steven Wu. Improved regret for differentially private exploration in linear mdp. In International Conference on Machine Learning, pages 16529-16552. PMLR, 2022.   \nHajime Ono and Tsubasa Takahashi. Locally private distributed reinforcement learning. arXiv preprint arXiv:2001.11718, 2020.   \nDan Qiao and Yu-Xiang Wang. Near-optimal deployment efficiency in reward-free reinforcement learning with linear function approximation. arXiv preprint arXiv:2210.00701, 2022a.   \nDan Qiao and Yu-Xiang Wang. Offine reinforcement learning with differential privacy. arXiv preprint arXiv:2206.00810, 2022b.   \nDan Qiao and Yu-Xiang Wang. Near-optimal differentially private reinforcement learning. In International Conference on Artificial Intelligence and Statistics, pages 9914-9940. PMLR, 2023.   \nDan Qiao and Yu-Xiang Wang. Near-optimal reinforcement learning with self-play under adaptivity constraints. arXiv preprint arXiv:2402.01111, 2024.   \nDan Qiao, Ming Yin, Ming Min, and Yu-Xiang Wang. Sample-efficient reinforcement learning with loglog(T) switching cost. In International Conference on Machine Learning, pages 18031-18061. PMLR, 2022.   \nDan Qiao, Ming Yin, and Yu-Xiang Wang. Logarithmic switching cost in reinforcement learning beyond linear mdps. arXiv preprint arXiv:2302.12456, 2023.   \nShai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement learning for autonomous driving. arXiv preprint arXiv: 1610.03295, 2016.   \nLloyd S Shapley. Stochastic games. Proceedings of the national academy of sciences, 39(10): 1095-1100, 1953.   \nRoshan Shariff and Or Sheffet. Differentially private contextual linear bandits. Advances in Neural Information Processing Systems, 31, 2018.   \nAli Shavandi and Majid Khedmati. A multi-agent deep reinforcement learning framework for algorithmic trading in financial markets. Expert Systems with Aplications, 208:118124, 2022.   \nDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. nature, 550(7676):354-359, 2017.   \nImdad Ullah, Najm Hasan, Sukhpal Singh Gill, Basem Suleiman, Tariq Ahamed Ahanger, Zawar Shah, Junaid Qadir, and SallS Kanhere.Privacy preserving large languag model: Chat case study based vision and framework. arXiv preprint arXiv:2310.12523, 2023.   \nGiuseppe Vietri, Borja Balle, Akshay Krishnamurthy, and Steven Wu. Private reinforcement learning with pac and regret guarantees. In International Conference on Machine Learning, pages 9754- 9764. PMLR, 2020.   \nBaoxiang Wang and Nidhi Hegde. Privacy-preserving q-learning with functional noise in continuous spaces. Advances in Neural Information Processing Systems, 32, 2019.   \nYuanhao Wang, Qinghua Liu, Yu Bai, and Chi Jin. Breaking the curse of multiagency: Provably effcient decentralized multi-agent rl with function approximation. arXiv preprint arXiv:2302.06606, 2023.   \nFan Wu, Huseyin A Inan, Arturs Backurs, Varun Chandrasekaran, Janardhan Kulkarni, and Robert Sim. Privately aligning language models with reinforcement learning. arXiv preprint arXiv:2310.16960, 2023a.   \nYulian Wu, Xingyu Zhou, Sayak Ray Chowdhury, and Di Wang. Differentially private episodic reinforcement learning with heavy-tailed rewards. arXiv preprint arXiv:2306.01121, 2023b.   \nQiaomin Xie, Yudong Chen, Zhaoran Wang, and Zhuoran Yang. Learning zero-sum simultaneousmove markov games using function approximation and correlated equilibrium. In Conference on learning theory, pages 3674-3682. PMLR, 2020.   \nTengyang Xie, Philip S Thomas, and Gerome Miklau. Privacy preserving off-policy evaluation. arXiv preprint arXiv:1902.00174, 2019.   \nDeheng Ye, Guibin Chen, Wen Zhang, Sheng Chen, Bo Yuan, Bo Liu, Jia Chen, Zhao Liu, Fuhao Qiu, Hongsheng Yu, et al. Towards playing full moba games with deep reinforcement learning. Advances in Neural Information Processing Systems, 33:621-632, 2020.   \nCanzhe Zhao, Yanjie Ze, Jing Dong, Baoxiang Wang, and Shuai Li. Differentially private temporal difference learning with stochastic nonconvex-strongly-concave optimization. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining, pages 985-993, 2023a.   \nCanzhe Zhao, Yanjie Ze, Jing Dong, Baoxiang Wang, and Shuai Li. Dpmac: differentially private communication for cooperative multi-agent reinforcement learning. arXiv preprint arXiv:2308.09902, 2023b.   \nFuheng Zhao, Dan Qiao, Rachel Redberg, Divyakant Agrawal, Amr El Abbadi, and Yu-Xiang Wang. Differentially private linear sketches: Efficient implementations and applications. arXiv preprint arXiv:2205.09873, 2022.   \nXingyu Zhou. Differentially private reinforcement learning with linear function approximation. Proceedings of the ACM on Measurement and Analysis of Computing Systems, 6(1):1-27, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Extended related works", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Differentially private reinforcement learning. The stream of research on DP RL started from the offline setting. Balle et al. [2016] first studied privately evaluating the value of a fixed policy from running it for several episodes (the on policy setting). Later, Xie et al. [2019] considered a more general setting of DP off policy evaluation. Recently, Qiao and Wang [2022b] provided the first results for offline reinforcement learning with DP guarantees. ", "page_idx": 14}, {"type": "text", "text": "More efforts focused on solving regret minimization. Under the setting of tabular MDP, Vietri et al. [2020] designed PUCB by privatizing UBEV [Dann et al., 2017] to satisfy Joint DP. Besides, under the constraints of Local DP, Garcelon et al. [2021] designed LDP-OBI based on UCRL2 [Jaksch et al., 2010]. Chowdhury and Zhou [2022] designed a general framework for both JDP and LDP based on UCBVI [Azar et al., 2017], and improved upon previous results. Finally, the best known results are obtained by Qiao and Wang [2023] via incorporating Bernstein-type bonuses. Meanwhile, Wu et al. [2023b] studied the case with heavy-tailed rewards. Under linear MDP, the only algorithm with JDP guarantee: Private LSVI-UCB [Ngo et al., 2022] is a private and low switching 6 version of LSVI-UCB [Jin et al., 2020], while LDP under linear MDP still remains open. Under linear mixture MDP, LinOpt-VI-Reg [Zhou, 2022] generalized UCRL-VTR [Ayoub et al., 2020] to guarantee JDP, while Liao et al. [2023] also privatized UCRL-VTR for LDP guarantee. In addition, Luyo et al. [2021] provided a unified framework for analyzing joint and local DP exploration. ", "page_idx": 14}, {"type": "text", "text": "There are several other works regarding DP RL. Wang and Hegde [2019] proposed privacy-preserving Q-learning to protect the reward information. Ono and Takahashi [2020] studied the problem of distributed reinforcement learning under LDP. Lebensold et al. [2019] presented an actor critic algorithm with differentially private critic. Cundy and Ermon [2020] tackled DP-RL under the policy gradient framework. Chowdhury et al. [2021] considered the adaptive control of differentially private linear quadratic (LQ) systems. Zhao et al. [2023a] studied differentially private temporal difference (TD) learning. Chowdhury et al. [2023] analyzed reward estimation with preference feedback under the constraints of DP. Hossain and Lee [2023], Hossain et al. [2023], Zha0 et al. [2023b], Gohari et al. [2023] focused on the privatization of communications between multiple agents in multi-agent RL. For applications, DP RL was applied to protect sensitive information in natural language processing and large language models (LLM) [Ullah et al., 2023, Wu et al., 2023a]. Meanwhile, Zhao et al. [2022] considered linear sketches with DP. ", "page_idx": 14}, {"type": "text", "text": "B Proof overview ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we provide a proof sketch of Theorem 4.1, which can further imply the PAC guarantee (Theorem 4.2) and the regret bounds under JDP (Theorem 5.2) or LDP (Theorem 5.5). The proof consists of the following steps: ", "page_idx": 14}, {"type": "text", "text": "(1) Bound the difference between the private statistics and their non-private counterparts. ", "page_idx": 14}, {"type": "text", "text": "(2) Prove that UCB and LCB hold with high probability.   \n(3) Bound the regret via telescoping over time steps and replace the private terms by non-private ones. ", "page_idx": 14}, {"type": "text", "text": "Below we explain the key steps in detail. Recall that $N_{h}^{k}$ denotes the real visitation counts, while $\\widetilde{N}_{h}^{k},\\widetilde{P}_{h}^{k}$ are the rivate itaton count and privae rasion keme respetively ", "page_idx": 14}, {"type": "text", "text": "Step (1).  According to Assumption 3.1 and standard concentration inequalities, we provide high probability upper bounds for $\\|\\widetilde{P}_{h}^{k}(\\cdot|s,a,b)\\:-\\:P_{h}(\\cdot|s,a,b)\\|_{1}$ and $|\\widetilde{P}_{h}^{k}(s^{\\prime}|s,a,b)\\;-$ $P_{h}(s^{\\prime}|s,a,b)|$ . Besides, we upper bound the following key term $|(\\widetilde{P}_{h}^{k}-P_{h})\\cdot V_{h+1}^{\\star}(s,a,b)|$ by $\\widetilde{O}\\left(\\sqrt{\\mathrm{Var}_{\\widehat{P}_{h}^{k}(\\cdot|s,a,b)}V_{h+1}^{\\star}(\\cdot)/\\widetilde{N}_{h}^{k}(s,a,b)}+H S E_{\\epsilon,\\beta}/\\widetilde{N}_{h}^{k}(s,a,b)\\right)$ Detal re defere to pendi e C.1. ", "page_idx": 14}, {"type": "text", "text": "Step (2). Then we prove that UCB and LCB hold with high probability via backward induction over timesteps (Appendix C.2). More specifically, the variance term of $\\Gamma_{h}^{\\bar{k}}$ is the private Bernstein-type bonus, while the difference between the private variance and its non-private counterpart can be bounded by $\\gamma_{h}^{k}$ and thelowerdertermsin $\\Gamma_{h}^{k}$ ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Step (3). Lastly, the regret can be bounded by telescoping: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname{Regret}(K)\\leq O\\left(\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\Gamma_{h}^{k}\\big(s_{h}^{k},a_{h}^{k},b_{h}^{k}\\big)\\right)_{\\textstyle1}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\widetilde{O}\\left(\\underbrace{\\displaystyle\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\sqrt{\\frac{\\mathrm{Var}_{P_{h}(\\cdot\\vert s_{h}^{k},a_{h}^{k},b_{h}^{k})}V_{h+1}^{\\pi^{k}}}{N_{h}^{k}(s_{h}^{k},a_{h}^{k},b_{h}^{k})}}}_{\\mathrm{bound~by~Cauchy-Schwarz~inequality~and~L.T.V.}}+\\displaystyle\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\frac{H S E_{\\epsilon,\\beta}}{N_{h}^{k}(s_{h}^{k},a_{h}^{k},b_{h}^{k})}\\right)}\\\\ &{\\leq\\widetilde{O}(\\sqrt{H^{2}S A B T}+H^{2}S^{2}A B E_{\\epsilon,\\beta}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The details about each inequality above and the lower order terms we ignore are deferred to Appendix C.3. ", "page_idx": 15}, {"type": "text", "text": "C Proof of main theorems ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we prove Theorem 4.1 and Theorem 4.2. ", "page_idx": 15}, {"type": "text", "text": "C.1  Properties of private estimations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We begin with some concentration results about our private transition kernel estimate $\\widetilde{P}$ that will be useful for the proof. Throughout the paper, let the non-private empirical transition kernel be: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\widehat{P}_{h}^{k}(s^{\\prime}|s,a,b)=\\frac{N_{h}^{k}(s,a,b,s^{\\prime})}{N_{h}^{k}(s,a,b)},\\;\\;\\forall\\;(h,s,a,b,s^{\\prime},k).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In addition, recall that our private transition kernel estimate is defined as below. ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\widetilde{P}_{h}^{k}(s^{\\prime}|s,a,b)=\\frac{\\widetilde{N}_{h}^{k}(s,a,b,s^{\\prime})}{\\widetilde{N}_{h}^{k}(s,a,b)},\\;\\;\\forall\\;(h,s,a,b,s^{\\prime},k).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now we are ready to list the properties below. Note that $\\iota=\\log(30H S A B K/\\beta)$ throughout the paper. ", "page_idx": 15}, {"type": "text", "text": "Lemma C.1. With probability $\\textstyle1-{\\frac{\\beta}{15}}$ for all $(h,s,a,b,k)\\in[H]\\times\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{B}\\times[K],$ it holds that: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\left\\|\\widetilde{P}_{h}^{k}(\\cdot|s,a,b)-P_{h}(\\cdot|s,a,b)\\right\\|_{1}\\leq2\\sqrt{\\frac{S\\iota}{\\widetilde{N}_{h}^{k}(s,a,b)}}+\\frac{2S E_{\\epsilon,\\beta}}{\\widetilde{N}_{h}^{k}(s,a,b)},}}\\\\ &{\\qquad\\quad\\left\\|\\widetilde{P}_{h}^{k}(\\cdot|s,a,b)-\\widehat{P}_{h}^{k}(\\cdot|s,a,b)\\right\\|_{1}\\leq\\frac{2S E_{\\epsilon,\\beta}}{\\widetilde{N}_{h}^{k}(s,a,b)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma C.1. The proof is a direct generalization of Lemma B.2 and Remark B.3 in Qiao and Wang [2023] to the two-player setting. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Lemma C.2. With probability $\\textstyle1-{\\frac{2\\beta}{15}}$ for all $(h,s,a,b,s^{\\prime},k)\\in[H]\\times\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{B}\\times\\mathcal{S}\\times[K],$ \u00fc holds that: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\widetilde{P}_{h}^{k}(s^{\\prime}|s,a,b)-P_{h}(s^{\\prime}|s,a,b)\\right|\\leq2\\sqrt{\\frac{\\operatorname*{min}\\{P_{h}(s^{\\prime}|s,a,b),\\widetilde{P}_{h}^{k}(s^{\\prime}|s,a,b)\\}\\iota}{\\widetilde{N}_{h}^{k}(s,a,b)}}+\\frac{2E_{\\epsilon,\\beta}\\iota}{\\widetilde{N}_{h}^{k}(s,a,b)},}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\left|\\widetilde{P}_{h}^{k}(s^{\\prime}|s,a,b)-\\widehat{P}_{h}^{k}(s^{\\prime}|s,a,b)\\right|\\leq\\frac{2E_{\\epsilon,\\beta}}{\\widetilde{N}_{h}^{k}(s,a,b)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof of Lemma C.2. The proof is a direct generalization of Lemma B.4 and Remark B.5 in Qiao and Wang [2023] to the two-player setting. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Lemma C.3. With probability $\\textstyle1-{\\frac{2\\beta}{15}}$ for all $(h,s,a,b,k)\\in[H]\\times S\\times A\\times B\\times[K],$ it holds that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\widetilde{P}_{h}^{k}-P_{h}\\right)\\cdot V_{h+1}^{\\star}(s,a,b)\\Big|\\leq\\operatorname*{min}\\left\\{\\sqrt{\\frac{2\\mathrm{Var}_{P_{h}(\\cdot\\vert s,a,b)}V_{h+1}^{\\star}(\\cdot)\\cdot\\iota}{\\widetilde{N}_{h}^{k}(s,a,b)}},\\sqrt{\\frac{2\\mathrm{Var}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a,b)}V_{h+1}^{\\star}(\\cdot)\\cdot\\iota}{\\widetilde{N}_{h}^{k}(s,a,b)}}\\right\\}+\\frac{2\\mathrm{Var}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a,b)}V_{h+1}^{\\star}(\\cdot)\\cdot\\iota}{\\widetilde{N}_{h}^{k}(s,a,b)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Big|\\Big(\\widetilde{P}_{h}^{k}-\\widehat{P}_{h}^{k}\\Big)\\cdot V_{h+1}^{\\star}(s,a,b)\\Big|\\le\\frac{2H S E_{\\epsilon,\\beta}}{\\widetilde{N}_{h}^{k}(s,a,b)}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma C.3. The proof is a direct generalization of Lemma B.6 and Remark B.7 in Qiao and Wang [2023] to the two-player setting. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "According to a union bound, the following lemma holds. ", "page_idx": 16}, {"type": "text", "text": "Lemma C.4. Under the high probability event that Assumption 3.1 holds, with probability at least $\\textstyle1-{\\frac{\\beta}{3}}$ the conclusions in Lemma C.1 Lemma C.2, Lemma C.3 holdsimulhaneously. ", "page_idx": 16}, {"type": "text", "text": "Throughout the proof, we will assume that Assumption 3.1 and Lemma C.4 hold, which will happen with high probability. Before we prove the main theorems, we present the following lemma which bounds the two variances. ", "page_idx": 16}, {"type": "text", "text": "Lemma C.5 (Lemma C.5 of Qiao and Wang [2022b]). For any function $V\\in\\mathbb{R}^{S}$ suchthat $\\|V\\|_{\\infty}\\leq$ $H$ it holdsthat ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\sqrt{\\mathrm{Var}_{\\tilde{P}_{h}^{k}(\\cdot\\vert s,a,b)}(V)}-\\sqrt{\\mathrm{Var}_{\\hat{P}_{h}^{k}(\\cdot\\vert s,a,b)}(V)}\\right|\\leq\\sqrt{3}H\\cdot\\sqrt{\\left\\Vert\\widetilde{P}_{h}^{k}(\\cdot\\vert s,a,b)-\\widehat{P}_{h}^{k}(\\cdot\\vert s,a,b)\\right\\Vert_{1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In addition, according to Lemma C.1, the left hand side can be further bounded by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left|\\sqrt{\\mathrm{Var}_{\\tilde{P}_{h}^{k}(\\cdot\\vert s,a,b)}(V)}-\\sqrt{\\mathrm{Var}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a,b)}(V)}\\right|\\leq3H\\sqrt{\\frac{S E_{\\epsilon,\\beta}}{\\widetilde{N}_{h}^{k}(s,a,b)}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "C.2  Proof of UCB and LCB ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For notational simplicity, for $V\\in\\mathbb{R}^{S}$ such that $\\|V\\|_{\\infty}\\leq H$ , we define ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widetilde{V}_{h}^{k}V(s,a,b)=\\operatorname{Var}_{\\widetilde{P}_{h}^{k}(\\cdot\\vert s,a,b)}V(\\cdot),\\quad V_{h}V(s,a,b)=\\operatorname{Var}_{P_{h}(\\cdot\\vert s,a,b)}V(\\cdot).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then the bonus term $\\Gamma$ can be represented as below ( $C_{2}$ is the universal constant in Algorithm 1). ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Gamma_{h}^{k}(s,a,b)=C_{2}\\sqrt{\\frac{\\widetilde{V}_{h}^{k}\\left(\\frac{\\overline{{V}}_{h+1}^{k}+\\underbar{V}_{h+1}^{k}}{2}\\right)(s,a,b)\\cdot\\iota}{\\widetilde{N}_{h}^{k}(s,a,b)}+\\frac{C_{2}H S E_{\\epsilon,\\beta}\\cdot\\iota}{\\widetilde{N}_{h}^{k}(s,a,b)}+\\frac{C_{2}H^{2}S\\iota}{\\widetilde{N}_{h}^{k}(s,a,b)}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We state the following lemma that can bound the lower order term, which is helpful for proving UCB and LCB. ", "page_idx": 16}, {"type": "text", "text": "Lemma C.6. Suppose Assumption 3.1 and Lemma C.4 hold, then there exists a universal constant $c_{1}>0$ suchthat:iffunction $g(s)$ satisies $|g|(s)\\leq({\\overline{{V}}}_{h+1}^{k}-{\\underline{{V}}}_{h+1}^{k})(s),$ then itholdsthat: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|(\\widetilde{P}_{h}^{k}-P_{h})g(s,a,b)\\right|\\leq\\!\\!\\frac{c_{1}}{H}\\operatorname*{min}\\Big\\{P_{h}(\\overline{{V}}_{h+1}^{k}-\\underline{{V}}_{h+1}^{k})(s,a,b),\\widetilde{P}_{h}^{k}(\\overline{{V}}_{h+1}^{k}-\\underline{{V}}_{h+1}^{k})(s,a,b)\\Big\\}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\frac{c_{1}H^{2}S_{l}}{\\widetilde{N}_{h}^{k}(s,a,b)}+\\frac{c_{1}H S E_{\\epsilon,\\beta}t}{\\widetilde{N}_{h}^{k}(s,a,b)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma C.6. If $|g|(s)\\leq({\\overline{{V}}}_{h+1}^{k}-{\\underline{{V}}}_{h+1}^{k})(s)$ it holds that: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|(\\widetilde{P}_{h}^{k}-P_{h})g(s,a,b)\\right|\\leq\\sum_{s^{\\prime}}\\left|\\left(\\widetilde{P}_{h}^{k}-P_{h}\\right)(s^{\\prime}|s,a,b)\\right|\\cdot\\left|g\\right|(s^{\\prime})}\\\\ &{\\leq\\sum_{s^{\\prime}}\\left|\\left(\\widetilde{P}_{h}^{k}-P_{h}\\right)(s^{\\prime}|s,a,b)\\right|\\cdot\\left(\\overline{{V}}_{h+1}^{k}-\\underbar{V}_{h+1}^{k}\\right)(s^{\\prime})}\\\\ &{\\leq\\sum_{s^{\\prime}}\\left(2\\sqrt{\\frac{P_{h}(s^{\\prime}|s,a,b)t}{\\widetilde{N}_{h}^{k}(s,a,b)}}+\\frac{2E_{e,\\beta}t}{\\widetilde{N}_{h}^{k}(s,a,b)}\\right)\\cdot\\left(\\overline{{V}}_{h+1}^{k}-\\underbar{V}_{h+1}^{k}\\right)(s^{\\prime})}\\\\ &{\\leq\\sum_{s^{\\prime}}\\left(\\frac{P_{h}(s^{\\prime}|s,a,b)}{\\bar{H}}+\\frac{H_{\\tau}}{\\widetilde{N}_{h}^{k}(s,a,b)}+\\frac{2E_{e,\\beta}t}{\\widetilde{N}_{h}^{k}(s,a,b)}\\right)\\cdot\\left(\\overline{{V}}_{h+1}^{k}-\\underbar{V}_{h+1}^{k}\\right)(s^{\\prime})}\\\\ &{\\leq\\frac{C_{1}}{\\bar{H}}P_{h}(\\overline{{V}}_{h+1}^{k}-\\underbar{V}_{h+1}^{k})(s,a,b)+\\frac{C_{1}}{\\widetilde{N}_{h}^{k}(s,a,b)}+\\frac{C_{1}}{\\widetilde{N}_{h}^{k}(s,a,b)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the third inequality is because of Lemma C.2. The forth inequality results from AM-GM inequality. The last inequality holds for some universal constant $c_{1}$ ", "page_idx": 17}, {"type": "text", "text": "The empirical part with the R.H.S to be $\\widetilde{P}_{h}^{k}$ can be proven using identical proof according to (13). ", "page_idx": 17}, {"type": "text", "text": "Then we prove that the UCB and LCB functions are actually upper and lower bounds of the best responses. Recall that $\\pi^{k}$ is the (correlated) policy executed in the $k$ -th episode and $(\\mu^{k},\\nu^{k})$ for both players are the marginal policies of $\\pi^{k}$ . In other words, $\\begin{array}{r}{\\mu_{h}^{k}(\\cdot|s)\\;=\\;\\sum_{b\\in\\mathcal{B}}\\pi_{h}^{k}(\\cdot,b|s)}\\end{array}$ and $\\begin{array}{r}{\\nu_{h}^{k}(\\cdot|s)=\\sum_{a\\in\\mathcal{A}}\\pi_{h}^{k}(a,\\cdot|s)}\\end{array}$ for all $(h,s)\\in[H]\\times S$ ", "page_idx": 17}, {"type": "text", "text": "Lemma C.7. Suppose Assumption 3.1 and Lemma C.4 hold, then there exist universal constants $C_{1},C_{2}>0$ (inAlgorithm $^{\\,I}$ )suchthatforall $(h,s,a,b,k)\\in[H]\\times\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{B}\\times[K],$ it holds that: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\{\\overline{{Q}}_{h}^{k}(s,a,b)\\geq Q_{h}^{\\dagger,\\nu^{k}}(s,a,b)\\geq Q_{h}^{\\mu^{k},\\dagger}(s,a,b)\\geq\\underline{{Q}}_{h}^{k}(s,a,b),\\right.}\\\\ &{\\left.\\left.\\left[\\overline{{V}}_{h}^{k}(s)\\geq V_{h}^{\\dagger,\\nu^{k}}(s)\\geq V_{h}^{\\mu^{k},\\dagger}(s)\\geq\\underline{{V}}_{h}^{k}(s).\\right.\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof of Lemma C.7. We prove by backward induction. For each $k\\in[K]$ , the conclusion is obvious for $h=H+1$ . Suppose UCB and LCB hold for $\\mathrm{^Q}$ value functions in the $(h+1)$ -th time step, we first prove the bounds for $\\mathrm{v}$ functions in the $\\left(h+1\\right)$ -th step and then prove the bounds for Q functions in the $h$ -th step. For all $s\\in S$ , it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{V}}_{h+1}^{k}(s)=\\!\\mathbb{E}_{\\pi_{h+1}^{k}}\\overline{{Q}}_{h+1}^{k}(s)}\\\\ &{\\qquad\\qquad\\geq\\underset{\\mu}{\\operatorname*{sup}}\\,\\mathbb{E}_{\\mu,\\nu_{h+1}^{k}}\\overline{{Q}}_{h+1}^{k}(s)}\\\\ &{\\qquad\\quad\\geq\\underset{\\mu}{\\operatorname*{sup}}\\,\\mathbb{E}_{\\mu,\\nu_{h+1}^{k}}Q_{h+1}^{\\dagger,\\nu^{k}}(s)}\\\\ &{\\qquad\\qquad\\geq\\!V_{h+1}^{\\dagger,\\nu^{k}}(s).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The conclusion $\\underline{{V}}_{h+1}^{k}(s)\\leq V_{h+1}^{\\mu^{k},\\dagger}(s)$ can be proven by symmetry. Therefore, it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{V}}_{h+1}^{k}(s)\\geq V_{h+1}^{\\dagger,\\nu^{k}}(s)\\geq V_{h+1}^{\\star}(s)\\geq V_{h+1}^{\\mu^{k},\\dagger}(s)\\geq\\underline{{V}}_{h+1}^{k}(s).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Next we prove the bounds for $\\mathrm{\\DeltaQ}$ value functions at the $h$ -th step. For all $(s,a,b)$ , it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left(\\overline{{Q}}_{h}^{k}-Q_{h}^{\\dagger,\\nu^{k}}\\right)(s,a,b)\\geq\\operatorname*{min}\\left\\{\\left(\\widetilde{P}_{h}^{k}\\overline{{V}}_{h+1}^{k}-P_{h}V_{h+1}^{\\dagger,\\nu^{k}}+\\gamma_{h}^{k}+\\Gamma_{h}^{k}\\right)(s,a,b),0\\right\\}}\\\\ &{\\geq\\operatorname*{min}\\left\\{\\left(\\widetilde{P}_{h}^{k}V_{h+1}^{\\dagger,\\nu^{k}}-P_{h}V_{h+1}^{\\dagger,\\nu^{k}}+\\gamma_{h}^{k}+\\Gamma_{h}^{k}\\right)(s,a,b),0\\right\\}}\\\\ &{=\\operatorname*{min}\\left\\{\\left(\\underbrace{\\widetilde{P}_{h}^{k}-P_{h}}_{(\\mathrm{i})}\\left(V_{h+1}^{\\dagger,\\nu^{k}}-V_{h+1}^{\\star}\\right)(s,a,b)}_{(\\mathrm{i})}+\\underbrace{\\left(\\widetilde{P}_{h}^{k}-P_{h}\\right)V_{h+1}^{\\star}(s,a,b)}_{(\\mathrm{i})}+\\gamma_{h}^{k}(s,a,b)+\\Gamma_{h}^{k}(s,a,b)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The absolute value of term (i) can be bounded as below. ", "page_idx": 18}, {"type": "equation", "text": "$$\n|(\\mathrm{i})|\\le\\frac{c_{1}}{H}\\widetilde{P}_{h}^{k}(\\overline{{V}}_{h+1}^{k}-\\underline{{V}}_{h+1}^{k})(s,a,b)+\\frac{c_{1}H^{2}S\\iota}{\\widetilde{N}_{h}^{k}(s,a,b)}+\\frac{c_{1}H S E_{\\epsilon,\\beta}\\iota}{\\widetilde{N}_{h}^{k}(s,a,b)},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for some universal constant $c_{1}$ according to Lemma C.6. ", "page_idx": 18}, {"type": "text", "text": "The absolute value of term (ii) can be bounded as below. ", "page_idx": 18}, {"type": "equation", "text": "$$\n(\\mathrm{ii})|\\le\\sqrt{\\frac{2\\mathrm{Var}_{\\widehat{P}_{h}^{k}(\\cdot\\vert s,a,b)}V_{h+1}^{\\star}(\\cdot)\\cdot\\iota}{\\widetilde{N}_{h}^{k}(s,a,b)}+\\frac{2H S E_{\\epsilon,\\beta}\\iota}{\\widetilde{N}_{h}^{k}(s,a,b)}}\\le\\sqrt{\\frac{2\\mathrm{Var}_{\\widetilde{P}_{h}^{k}(\\cdot\\vert s,a,b)}V_{h+1}^{\\star}(\\cdot)\\cdot\\iota}{\\widetilde{N}_{h}^{k}(s,a,b)}+\\frac{8H S E_{\\epsilon,\\beta}\\iota}{\\widetilde{N}_{h}^{k}(s,a,b)}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the first inequality is because of Lemma C.3 while the second inequality holds due to Lemma C.5. ", "page_idx": 18}, {"type": "text", "text": "We further bound the term $\\operatorname{Var}_{\\widetilde P_{h}^{k}(\\cdot|s,a,b)}V_{h+1}^{\\star}(\\cdot)$ as below. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\widetilde{V}_{h}^{k}\\left(\\frac{\\overline{{V}}_{h+1}^{k}+\\underbar{V}_{h+1}^{k}}{2}\\right)-\\widetilde{V}_{h}^{k}{V}_{h+1}^{\\star}(\\cdot)\\right|(s,a,b)}\\\\ &{\\leq\\left|\\widetilde{P}_{h}^{k}\\cdot\\left(\\frac{\\overline{{V}}_{h+1}^{k}+\\underbar{V}_{h+1}^{k}}{2}\\right)^{2}-\\widetilde{P}_{h}^{k}\\cdot\\left({V}_{h+1}^{k}\\right)^{2}\\right|(s,a,b)+\\left|\\left[\\widetilde{P}_{h}^{k}\\cdot\\left(\\frac{\\overline{{V}}_{h+1}^{k}+\\underbar{V}_{h+1}^{k}}{2}\\right)(s,a,b)\\right]^{2}-\\left[\\widetilde{P}_{h}^{k}\\right]^{2}\\right|(s,a,b)}\\\\ &{\\leq4H\\widetilde{P}_{h}^{k}\\cdot\\left(\\overline{{V}}_{h+1}^{k}-\\underbar{V}_{h+1}^{k}\\right)(s,a,b).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, the term (i) can be further bounded as below. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle|(\\mathrm{ii})|\\leq\\sqrt{\\frac{2\\mathrm{Var}_{\\widetilde{P}_{h}^{k}\\left(\\cdot\\vert s,a,b\\right)}V_{h+1}^{\\star}(\\cdot)\\cdot\\iota}{\\widetilde{N}_{h}^{k}\\left(s,a,b\\right)}+\\frac{8H S E_{\\epsilon,\\beta}\\iota}{\\widetilde{N}_{h}^{k}\\left(s,a,b\\right)}}\\ }\\\\ {\\displaystyle\\leq\\sqrt{\\frac{2\\cdot\\widetilde{V}_{h}^{k}\\left(\\frac{\\widetilde{V}_{h+1}^{k}+\\widetilde{V}_{h+1}^{k}}{2}\\right)\\left(s,a,b\\right)+2\\iota\\cdot4H\\widetilde{P}_{h}^{k}\\cdot\\left(\\overline{{V}}_{h+1}^{k}-\\underline{{V}}_{h+1}^{k}\\right)\\left(s,a,b\\right)}{\\widetilde{N}_{h}^{k}\\left(s,a,b\\right)}+\\frac{8H S E_{\\epsilon,\\beta}\\iota}{\\widetilde{N}_{h}^{k}\\left(s,a,b\\right)}}}\\\\ {\\displaystyle\\leq\\sqrt{\\frac{2\\widetilde{V}_{h}^{k}\\left(\\frac{\\overline{{V}}_{h+1}^{k}+\\underline{{V}}_{h+1}^{k}}{2}\\right)\\left(s,a,b\\right)}{\\widetilde{N}_{h}^{k}\\left(s,a,b\\right)}}+\\frac{\\widetilde{P}_{h}^{k}\\cdot\\left(\\overline{{V}}_{h+1}^{k}-\\underline{{V}}_{h+1}^{k}\\right)\\left(s,a,b\\right)}{H}+\\frac{2H^{2}\\iota}{\\widetilde{N}_{h}^{k}\\left(s,a,b\\right)}+\\frac{8H S E_{\\epsilon,\\beta}\\iota}{\\widetilde{N}_{h}^{k}\\left(s,a,b\\right)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the second inequality results from (29) and the third inequality is due to AM-GM inequality. Combining the upper bounds of $|(\\mathrm{i})|$ and I(i)], there exist universal constants $C_{1},C_{2}>0$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathrm{(i)}+\\mathrm{(ii)}+\\gamma_{h}^{k}(s,a,b)+\\Gamma_{h}^{k}(s,a,b)\\geq0.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The  inequality  implies  that $\\begin{array}{r l r}{\\left(\\overline{{Q}}_{h}^{k}-Q_{h}^{\\dagger,\\nu^{k}}\\right)(s,a,b)}&{{}\\ge}&{0}\\end{array}$ By symmetry,  we have $\\left(\\underline{{{Q}}}_{h}^{k}-Q_{h}^{\\mu^{k},\\dagger}\\right)(s,a,b)\\leq0$ As a result, it holds that $\\overline{{Q}}_{h}^{k}(s,a,b)\\geq Q_{h}^{\\dagger,\\nu^{k}}(s,a,b)\\geq Q_{h}^{\\star}(s,a,b)\\geq$ $Q_{h}^{\\mu^{k},\\dagger}(s,a,b)\\geq\\underline{{Q}}_{h}^{k}(s,a,b).$ ", "page_idx": 18}, {"type": "text", "text": "According to backward induction, the conclusion holds for all $(h,s,a,b,k)$ ", "page_idx": 18}, {"type": "text", "text": "C.3Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Given the UCB and LCB property, we are now ready to prove our main results. We first state the following lemma that controls the error of the empirical variance estimator. ", "page_idx": 18}, {"type": "text", "text": "Lemma C.8. Suppose Assumption 3.1 and Lemma C.4 hold, then there exists a universal constant $c_{2}>0$ suchthatforall $(h,s,a,b,k)\\in[H]\\times S\\times A\\times B\\times[K]$ itholdsthat ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Biggl|\\widetilde{V}_{h}^{k}\\left(\\frac{\\overline{{V}}_{h+1}^{k}+\\underbar{V}_{h+1}^{k}}{2}\\right)-V_{h}V_{h+1}^{\\pi^{k}}\\Biggr|(s,a,b)}\\\\ &{\\leq\\!4H P_{h}\\left(\\overline{{V}}_{h+1}^{k}-\\underbar{V}_{h+1}^{k}\\right)(s,a,b)+\\frac{c_{2}H^{2}S E_{\\epsilon,\\beta}}{\\widetilde{N}_{h}^{k}(s,a,b)}+c_{2}H^{2}\\sqrt{\\frac{S t}{\\widetilde{N}_{h}^{k}(s,a,b)}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Proof of Lemma C.8. According to Lemma C.7, $\\overline{{V}}_{h}^{k}(s)\\geq V_{h}^{\\pi^{k}}(s)\\geq\\underline{{V}}_{h}^{k}(s)$ always holds. Then it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\widetilde{V}_{h}^{k}\\left(\\frac{\\widetilde{V}_{h+1}^{k}+\\widetilde{V}_{h+1}^{k}}{2}\\right)-V_{h}V_{h+1}^{n,k}\\right|(s,a,b)}\\\\ &{\\leq\\left|\\widetilde{V}_{h}^{k}\\left(\\frac{\\widetilde{V}_{h+1}^{k}+\\widetilde{V}_{h+1}^{k}}{2}\\right)^{2}-P_{h}\\left(V_{h+1}^{n,k}\\right)^{2}-\\left[\\widetilde{P}_{h}^{k}\\left(\\frac{\\widetilde{V}_{h+1}^{k}+\\widetilde{V}_{h+1}^{k}}{2}\\right)\\right]^{2}+\\left(P_{h}V_{h+1}^{n,k}\\right)^{2}\\right|(s,a,b)}\\\\ &{\\leq\\left|\\widetilde{V}_{h}^{k}\\left(\\widetilde{V}_{h+1}^{k}\\right)^{2}-P_{h}\\left(\\Sigma_{h+1}^{k}\\right)^{2}-\\left(\\widetilde{P}_{h}^{k}E_{h+1}^{k}\\right)^{2}+\\left(P_{h}\\widetilde{V}_{h+1}^{k}\\right)^{2}\\right|(s,a,b)}\\\\ &{\\leq\\left|\\overline{{\\left(\\widetilde{P}_{h}^{k}-P_{h}\\right)\\left(\\widetilde{V}_{h+1}^{k}\\right)^{2}}}\\right|(s,a,b)+\\left|\\mathcal{N}_{h}\\left[\\overline{{\\left(V_{h+1}^{k}\\right)^{2}-\\left(\\frac{V_{h+1}^{k}}{4}\\right)^{2}}}\\right]\\right|(s,a,b)}\\\\ &{\\ \\ \\ +\\left|\\left(\\widetilde{P}_{h}^{k}E_{h+1}^{k}\\right)^{2}-\\left(P_{h}V_{h+1}^{k}\\right)^{2}\\right|(s,a,b)+\\left|\\left(P_{h}V_{h+1}^{k}\\right)^{2}-\\left(P_{h}V_{h+1}^{k}\\right)^{2}\\right|(s,a,b).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The term (i) can be bounded as below due to Lemma C.1. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{(i)}\\le2{H^{2}}\\sqrt{\\frac{S\\iota}{\\widetilde{N}_{h}^{k}(s,a,b)}}+\\frac{2{H^{2}}S E_{\\epsilon,\\beta}}{\\widetilde{N}_{h}^{k}(s,a,b)}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The term (ii) can be directly bounded as below. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{(ii)}\\leq2H P_{h}\\left(\\overline{{V}}_{h+1}^{k}-\\underline{{V}}_{h+1}^{k}\\right)(s,a,b).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The term (i) can be bounded as below due to Lemma C.1. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathrm{(iii)}\\leq2H\\left|\\left(\\widetilde{P}_{h}^{k}-P_{h}\\right)\\underbar{V}_{h+1}^{k}\\right|(s,a,b)\\leq4H^{2}\\sqrt{\\frac{S\\iota}{\\widetilde{N}_{h}^{k}(s,a,b)}}+\\frac{4H^{2}S E_{\\epsilon,\\beta}}{\\widetilde{N}_{h}^{k}(s,a,b)}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The term (iv) can be directly bounded as below. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{(iv)}\\leq2H P_{h}\\left(\\overline{{V}}_{h+1}^{k}-\\underline{{V}}_{h+1}^{k}\\right)(s,a,b).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The conclusion holds according the upper bounds of term (i), (ii), (i) and (iv). ", "page_idx": 19}, {"type": "text", "text": "Finally we prove the regret bound of Algorithm 1. ", "page_idx": 19}, {"type": "text", "text": "Proof of Theorem 4.1. Our proof base on Assumption 3.1 and Lemma C.4. We define the following notations. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\Delta_{h}^{k}=\\left(\\overline{{V}}_{h}^{k}-\\underbar{V}_{h}^{k}\\right)(s_{h}^{k}),}\\\\ {\\zeta_{h}^{k}=\\Delta_{h}^{k}-\\left(\\overline{{Q}}_{h}^{k}-\\underbar{Q}_{h}^{k}\\right)(s_{h}^{k},a_{h}^{k},b_{h}^{k}),}\\\\ {\\xi_{h}^{k}=P_{h}\\left(\\overline{{V}}_{h+1}^{k}-\\underbar{V}_{h+1}^{k}\\right)(s_{h}^{k},a_{h}^{k},b_{h}^{k})-\\Delta_{h+1}^{k}.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then it holds that $\\zeta_{h}^{k}$ and $\\xi_{h}^{k}$ are martingale differences bounded by $H$ . In addition, we use the following abbreviations for notational simplicity. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\gamma_{h}^{k}=\\gamma_{h}^{k}(s_{h}^{k},a_{h}^{k},b_{h}^{k}),}\\\\ {\\Gamma_{h}^{k}=\\Gamma_{h}^{k}(s_{h}^{k},a_{h}^{k},b_{h}^{k}),}\\\\ {N_{h}^{k}=N_{h}^{k}(s_{h}^{k},a_{h}^{k},b_{h}^{k}),}\\\\ {\\tilde{N}_{h}^{k}=\\tilde{N}_{h}^{k}(s_{h}^{k},a_{h}^{k},b_{h}^{k}).}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then we have thfllowing analysisabut $\\Delta_{h}^{k}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\begin{array}{r l}&{\\Delta_{h}^{k}=\\zeta_{h}^{k}+\\left(\\overline{{Q}}_{h}^{k}-\\underline{{Q}}_{h}^{k}\\right)(s_{h}^{k},a_{h}^{k},b_{h}^{k})}\\\\ &{\\zeta_{h}^{k}+2\\gamma_{h}^{k}+2\\Gamma_{h}^{k}+\\widetilde{P}_{h}^{k}\\left(\\overline{{V}}_{h+1}^{k}-\\underline{{V}}_{h+1}^{k}\\right)(s_{h}^{k},a_{h}^{k},b_{h}^{k})}\\\\ &{\\zeta_{h}^{k}+2\\Gamma_{h}^{k}+\\left(1+\\frac{2C_{1}}{H}\\right)\\cdot\\left[\\left(1+\\frac{C_{1}}{H}\\right)\\cdot P_{h}\\left(\\overline{{V}}_{h+1}^{k}-\\underline{{V}}_{h+1}^{k}\\right)(s_{h}^{k},a_{h}^{k},b_{h}^{k})+\\frac{c_{1}H^{2}S_{h}}{\\widetilde{N}_{h}^{k}}+\\frac{c_{1}H S E_{\\epsilon,\\beta}}{\\widetilde{N}_{h}^{k}}\\right.}\\end{array}}\\\\ &{\\Biggr.}&{\\left.5\\zeta_{h}^{k}+\\left(1+\\frac{C_{3}}{H}\\right)\\cdot P_{h}\\left(\\overline{{V}}_{h+1}^{k}-\\underline{{V}}_{h+1}^{k}\\right)(s_{h}^{k},a_{h}^{k},b_{h}^{k})+\\frac{c_{3}H^{2}S_{H}}{\\widetilde{N}_{h}^{k}}+\\frac{c_{3}H S E_{\\epsilon,\\beta}}{\\widetilde{N}_{h}^{k}}\\right.}\\\\ &{\\left.\\rule{0ex}{5ex}+\\sum_{h}\\sqrt{\\frac{\\widetilde{V}_{h}^{k}\\left(\\frac{\\widetilde{V}_{h+1}^{k}+\\sum_{h+1}^{k}{\\alpha_{h}^{k}}}{\\widetilde{N}_{h}^{k}}\\right)(s_{h}^{k},a_{h}^{k},b_{h}^{k})}{\\left(\\frac{\\widetilde{V}_{h}^{k}}{\\omega_{h}^{k}}\\right)}},}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the first inequality holds because of the definition of $\\overline{{Q}}$ and $Q$ . The second inequality holds due to the definition of $\\gamma_{h}^{k}$ and Lemma C.6. The last inequality holds for some universal constant $c_{3}>0$ The term (i) can be further bounded as below according to Lemma C.8 and AM-GM inequality. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{i})\\leq\\sqrt{\\frac{V_{h}V_{h+1}^{\\pi^{k}}(s_{h}^{k},a_{h}^{k},b_{h}^{k})\\bar{\\iota}}{\\tilde{N}_{h}^{k}}}+\\sqrt{\\frac{4H P_{h}\\left(\\overline{{V}}_{h+1}^{k}-\\underline{{V}}_{h+1}^{k}\\right)(s_{h}^{k},a_{h}^{k},b_{h}^{k})\\bar{\\iota}}{\\tilde{N}_{h}^{k}}+\\frac{H\\sqrt{c_{2}S E_{\\epsilon,\\beta}{\\iota}}}{\\tilde{N}_{h}^{k}}}+c_{2}\\sqrt{\\frac{\\bar{\\iota}}{\\tilde{N}_{h}^{k}}}}\\\\ &{\\quad\\leq\\sqrt{\\frac{V_{h}V_{h+1}^{\\pi^{k}}(s_{h}^{k},a_{h}^{k},b_{h}^{k})\\bar{\\iota}}{\\tilde{N}_{h}^{k}}}+\\frac{c_{4}P_{h}\\left(\\overline{{V}}_{h+1}^{k}-\\underline{{V}}_{h+1}^{k}\\right)(s_{h}^{k},a_{h}^{k},b_{h}^{k})}{H}+\\frac{c_{4}H^{2}\\sqrt{S}\\bar{\\iota}}{\\tilde{N}_{h}^{k}}+\\frac{c_{4}H\\sqrt{S E_{\\epsilon,\\beta}{\\iota}}}{\\tilde{N}_{h}^{k}}+}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the first inequality results from Lemma C.8 and AM-GM inequality on the last term of (32).   \nThe second inequality holds for some universal constant $c_{4}>0$ according to AM-GM inequality. ", "page_idx": 20}, {"type": "text", "text": "Plugging in the upper bound of term (i), for some universal constant $c_{5}>0$ , it holds that: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Delta_{h}^{k}\\leq\\zeta_{h}^{k}+\\left(1+\\frac{c_{5}}{H}\\right)\\xi_{h}^{k}+\\left(1+\\frac{c_{5}}{H}\\right)\\Delta_{h+1}^{k}+c_{5}\\sqrt{\\frac{V_{h}V_{h+1}^{\\pi k}(s_{h}^{k},a_{h}^{k},b_{h}^{k})\\iota}{\\tilde{N}_{h}^{k}}}+c_{5}\\sqrt{\\frac{\\iota}{\\tilde{N}_{h}^{k}}}+\\frac{c_{5}H^{2}S t}{\\tilde{N}_{h}^{k}}+\\frac{c_{6}\\tilde{N}_{h}^{k}}{\\tilde{N}_{h}^{k}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Summing ${\\Delta}_{1}^{k}$ over $k\\in[K]$ , we have for some universal constant $c_{6}>0$ , it holds that: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{k=1}^{K}\\Delta_{1}^{k}\\leq\\underset{\\underbrace{\\mathrm{K}\\displaystyle=\\ln\\left[\\atop\\mathrm{K}\\right]}_{\\mathrm{(it)}}}{\\underbrace{K_{1}^{K}\\sum_{h=1}^{H}\\left(1+\\frac{c_{5}}{H}\\right)^{h-1}\\zeta_{h}^{k}}}+\\underset{\\underbrace{\\mathrm{K}\\displaystyle=\\ln\\left[\\atop\\mathrm{K}\\right]}_{\\mathrm{(it)}}}{\\underbrace{\\sum_{\\mathrm{(it)}}^{K}\\sum_{h=1}^{H}\\left(1+\\frac{c_{5}}{H}\\right)^{h}\\xi_{h}^{k}}}+\\underset{\\underbrace{\\mathrm{K}\\displaystyle=\\ln\\left[\\atop\\mathrm{K}\\right]}_{\\mathrm{(it)}}}{\\underbrace{\\sum_{\\mathrm{(is)}}^{K}\\sum_{h=1}^{H}\\left(\\frac{k_{h}^{k},a_{h}^{k},b_{h}^{k}}{\\bar{N}_{h}^{k}}\\right)}}}\\\\ &{\\qquad\\qquad+\\underset{\\underbrace{\\mathrm{K}\\displaystyle=\\ln\\left[\\atop\\mathrm{K}\\right]}_{\\mathrm{(v)}}}{\\underbrace{K_{1}^{K}}}+c_{6}\\underset{\\underbrace{\\mathrm{K}\\displaystyle=\\ln\\left[\\atop\\mathrm{K}\\right]}_{\\mathrm{(v)}}}{\\underbrace{K_{1}^{K}}}\\frac{H^{2}S t+H S E_{\\epsilon,\\beta}t}{\\widetilde{N}_{h}^{k}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The term (i and term (i) can be bounded by Azuma-Hoeffding inequality. With probability $\\begin{array}{r}{1-\\frac{2\\beta}{9}}\\end{array}$ it holdsthat ", "page_idx": 21}, {"type": "equation", "text": "$$\n|(\\mathrm{ii})|\\leq O\\left(\\sqrt{H^{3}K\\iota}\\right),\\quad|(\\mathrm{iii})|\\leq O\\left(\\sqrt{H^{3}K\\iota}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The main term (iv) is bounded as below. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathrm{(iv)}\\leq\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\sqrt{\\frac{V_{h}V_{h+1}^{\\pi^{k}}(s_{h}^{k},a_{h}^{k},b_{h}^{k})_{l}}{N_{h}^{k}}}}\\\\ &{\\leq\\sqrt{\\displaystyle\\sum_{k=1}^{K}\\sum_{h=1}^{H}V_{h}V_{h+1}^{\\pi^{k}}(s_{h}^{k},a_{h}^{k},b_{h}^{k})_{l}\\cdot\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\frac{1}{N_{h}^{k}}}}\\\\ &{\\leq\\sqrt{O\\left(H^{2}K+H^{3}\\iota\\right)\\iota\\cdot O(H S A B\\iota)}}\\\\ &{=\\widetilde{O}\\left(\\sqrt{H^{3}S A B K}+H^{2}\\sqrt{S A B}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The first inequality is because $\\widetilde{N}_{h}^{k}\\ge N_{h}^{k}$ (Assumption 3.1). The second inequality holds due to Cauchy-Schwarz inequality. The third inequality holds with probability $\\textstyle1-{\\frac{\\beta}{9}}$ becauseof Lawof total variance and standard concentration inequalities (for details please refer to Lemma 8 of Azar et al. [2017]). ", "page_idx": 21}, {"type": "text", "text": "The term (v) is bounded as below due to pigeon-hole principle. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{(v)}\\le\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\sqrt{\\frac{\\iota}{N_{h}^{k}}}\\le O(\\sqrt{H^{2}S A B K\\iota}),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the first inequality is because $\\widetilde{N}_{h}^{k}\\ge N_{h}^{k}$ (Assumption 3.1). The last one results from pigeonhole principle. ", "page_idx": 21}, {"type": "text", "text": "The term (vi) can be bounded as below. ", "page_idx": 21}, {"type": "equation", "text": "$$\n(\\mathrm{vi})\\le\\sum_{k=1}^{K}\\sum_{h=1}^{H}\\frac{H^{2}S t+H S E_{\\epsilon,\\beta}\\iota}{N_{h}^{k}}\\le O(H^{3}S^{2}A B\\iota^{2})+O(H^{2}S^{2}A B E_{\\epsilon,\\beta}\\iota^{2}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Combining the upper bounds for term (i), I(i)], (iv), (v) and (vi). The regret of Algorithm 1 can be bounded as below. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathrm{Regret}(K)=\\sum_{k=1}^{K}\\left[V_{1}^{\\dagger,\\nu^{k}}(s_{1})-V_{1}^{\\mu^{k},\\dagger}(s_{1})\\right]\\leq\\sum_{k=1}^{K}\\left[\\overline{{V}}_{1}^{k}(s_{1})-\\underline{{V}}_{1}^{k}(s_{1})\\right]}}\\\\ &{}&{=\\displaystyle\\sum_{k=1}^{K}\\Delta_{1}^{k}\\leq\\widetilde{O}\\left(\\sqrt{H^{2}S A B T}+H^{3}S^{2}A B+H^{2}S^{2}A B E_{\\epsilon,\\beta}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $T=H K$ is the number of steps ", "page_idx": 21}, {"type": "text", "text": "The failure probability is bounded by $\\beta$ $\\frac{\\beta}{3}$ for Assumption 3.1, $\\frac{\\beta}{3}$ for Lemma C.4, $\\frac{\\beta}{3}$ for terms (i), (ii) and (iv). The proof of Theorem 4.1 is complete. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "C.4Proof of Theorem 4.2 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this part, we provide a proof of the PAC guarantee: Theorem 4.2. The proof directly follows from the proof of the regret bound (Theorem 4.1). ", "page_idx": 21}, {"type": "text", "text": "Proof of Theorem 4.2. Recallthat we choose $\\pi^{\\mathrm{out}}=\\pi^{\\overline{{k}}}$ such that ${\\overline{{k}}}=\\operatorname{argmin}_{k}\\left({\\overline{{V}}}_{1}^{k}-{\\underline{{V}}}_{1}^{k}\\right)(s_{1})$ Therefore, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\nV_{1}^{\\dagger,\\nu^{\\mathrm{out}}}(s_{1})-V_{1}^{\\mu^{\\mathrm{out}},\\dagger}(s_{1})\\leq\\overline{{V}}_{1}^{\\overline{{k}}}(s_{1})-\\underline{{V}}_{1}^{\\overline{{k}}}(s_{1})\\leq\\frac{1}{K}\\tilde{O}\\left(\\sqrt{H^{3}S A B K}+H^{2}S^{2}A B E_{\\epsilon,\\beta}\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "if ignoring the lower order term of the regret bound. ", "page_idx": 21}, {"type": "text", "text": "Therefore, choosing $\\begin{array}{r}{K\\ge\\widetilde\\Omega\\left(\\frac{H^{3}S A B}{\\alpha^{2}}+\\operatorname*{min}\\left\\{K^{\\prime}|\\frac{H^{2}S^{2}A B E_{\\epsilon,\\beta}}{K^{\\prime}}\\le\\alpha\\right\\}\\right)}\\end{array}$ bounds the R.H.S y $\\alpha$ ", "page_idx": 21}, {"type": "text", "text": "D Missing proof in Section 5 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we provide the missing proof for results in Section 5. Recall that $N_{h}^{k}$ is the real visitation count, $\\widehat{N}_{h}^{k}$ is the intermediate noisy count calculated by both Privatizers and $\\widetilde{N}_{h}^{k}$ is the final private count after the post-processing step. Note that most of the proof here are generalizations of Appendix $\\mathrm{D}$ in Qiao and Wang [2023] to the multi-player setting, and here we state the proof for completeness. ", "page_idx": 22}, {"type": "text", "text": "ProofofLemma 5.1. Due to Theorem 3.5 of Chan et al.[2011] and Lemma 34 of Hsu et al. [2014], the release of $\\{\\widehat{N}_{h}^{k}(s,a,b)\\}_{(h,s,a,b,k)}$ satisfies ${\\frac{\\epsilon}{2}}{\\bf-D P}.$ Similarly, the release of $\\{\\widehat{N}_{h}^{k}(s,a,b,s^{\\prime})\\}_{(h,s,a,b,s^{\\prime},k)}$ also satisfies $\\frac\\epsilon2$ -DP. Therefore, the release of the following private counters $\\{\\widehat{N}_{h}^{k}(s,a,b)\\}_{(h,s,a,b,k)},\\,\\{\\widehat{N}_{h}^{k}(s,a,b,s^{\\prime})\\}_{(h,s,a,b,s^{\\prime},k)}$ satisfy $\\epsilon$ DP. Due to post-processing Lemma2.3ofBu andStenke 2016), threlasef bth privatcots $\\{\\widetilde{N}_{h}^{k}(s,a,b)\\}_{(h,s,a,b,k)}$ and $\\{\\widetilde{N}_{h}^{k}(s,a,b,s^{\\prime})\\}_{(h,s,a,b,s^{\\prime},k)}$ also satisfies $\\epsilon$ DP. Then it holds that the release of all $\\pi^{k}$ .s $\\epsilon_{}$ DP according to post-processing. Finally, the guarantee of $\\epsilon$ -JDP results from Billboard Lemma (Lemma 9 of Hsu et al. [2014]). ", "page_idx": 22}, {"type": "text", "text": "Forutiltalyisfha $\\begin{array}{r}{\\epsilon^{\\prime}=\\frac{\\epsilon}{2H\\log K}}\\end{array}$ in Binary Mechanism and a union bound, with probability $\\textstyle1-{\\frac{\\beta}{3}}$ , for all $(h,s,a,b,s^{\\prime},k)$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\left|\\widehat{N}_{h}^{k}(s,a,b,s^{\\prime})-N_{h}^{k}(s,a,b,s^{\\prime})\\right|\\le O\\left(\\frac{H}{\\epsilon}\\log(H S A B K/\\beta)^{2}\\right),}\\\\ &{}&{\\left|\\widehat{N}_{h}^{k}(s,a,b)-N_{h}^{k}(s,a,b)\\right|\\le O\\left(\\frac{H}{\\epsilon}\\log(H S A B K/\\beta)^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Together with Lemma 5.8, the Central Privatizer satisfies Assumption 3.1 with $\\begin{array}{r}{E_{\\epsilon,\\beta}=\\widetilde{O}\\left(\\frac{H}{\\epsilon}\\right)}\\end{array}$ \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Proof of Theorem 5.2. The proof directly results from plugging $\\begin{array}{r}{E_{\\epsilon,\\beta}=\\widetilde{O}\\left(\\frac{H}{\\epsilon}\\right)}\\end{array}$ into Theorem 4.1 and Theorem 4.2. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "ProofofTheorem 5.3. The first term results from the non-private regret lower bound $\\Omega(\\sqrt{H^{2}S(A+B)T})$ [Bai and Jin, 2020]. The second term is a direct adaptation of the $\\Omega(H S A/\\epsilon)$ lower bound for any algorithms with $\\epsilon$ JDP guarantee under single-agent MDP [Vietri et al., 2020]. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Proof of Lemma 5.4. The privacy guarantee directly results from properties of Laplace Mechanism and composition of DP [Dwork et al., 2014]. ", "page_idx": 22}, {"type": "text", "text": "For utility analysis, because of Corollary 12.4 of Dwork et al. [2014] and a union bound, with probaity $\\textstyle1-{\\frac{\\beta}{3}}$ forall posible $(h,s,a,b,s^{\\prime},k)$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\widehat{N}_{h}^{k}(s,a,b,s^{\\prime})-N_{h}^{k}(s,a,b,s^{\\prime})\\right|\\le O\\left(\\frac{H}{\\epsilon}\\sqrt{K\\log(H S A B K/\\beta)}\\right),}\\\\ {\\left|\\widehat{N}_{h}^{k}(s,a,b)-N_{h}^{k}(s,a,b)\\right|\\le O\\left(\\frac{H}{\\epsilon}\\sqrt{K\\log(H S A B K/\\beta)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Togcther wit Lemma 8,th LlocalPrivaize atisfes ssumpto . with $\\begin{array}{r}{E_{\\epsilon,\\beta}=\\widetilde{O}\\left(\\frac{H}{\\epsilon}\\sqrt{K}\\right)}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "Proof of Theorem 5.5. The proof directly results from plugging $\\begin{array}{r}{E_{\\epsilon,\\beta}=\\widetilde{O}\\left(\\frac{H}{\\epsilon}\\sqrt{K}\\right)}\\end{array}$ into Theorem 4.1 and Theorem 4.2. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "ProofofTheorem 5.6. The first term results from the non-private regret lower bound $\\Omega(\\sqrt{H^{2}S(A+B)T})$ [Bai and Jin, 2020]. The second term is a direct adaptation of the $\\Omega(\\sqrt{H S A T}/\\epsilon)$ lower bound for any algorithms with $\\epsilon$ -LDP guarante under single-agent MDP [Garcelon et al., 2021]. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Proof of Lemma 5.8. For clarity, we denote the solution of (7) by $\\bar{N}_{h}^{k}$ and therefore $\\widetilde{N}_{h}^{k}(s,a,b,s^{\\prime})=$ $\\begin{array}{r}{\\bar{N}_{h}^{k}(s,a,b,s^{\\prime})+\\frac{E_{\\epsilon,\\beta}}{2S},\\widetilde{N}_{h}^{k}(s,\\dot{a},b)=\\bar{N}_{h}^{k}(s,a,b)+\\frac{E_{\\epsilon,\\beta}}{2}.}\\end{array}$ ", "page_idx": 23}, {"type": "text", "text": "When the condition (two inequalities) in Lemma 5.8 holds, the original counts $\\{N_{h}^{k}(s,a,b,s^{\\prime})\\}_{s^{\\prime}\\in S}$ is a feasible solution to the optimization problem, which means that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{s^{\\prime}}\\left|\\bar{N}_{h}^{k}(s,a,b,s^{\\prime})-\\widehat{N}_{h}^{k}(s,a,b,s^{\\prime})\\right|\\leq\\operatorname*{max}_{s^{\\prime}}\\left|N_{h}^{k}(s,a,b,s^{\\prime})-\\widehat{N}_{h}^{k}(s,a,b,s^{\\prime})\\right|\\leq\\frac{E_{\\epsilon,\\beta}}{4}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Combining with the condition in Lemma 5.8 with respect to $\\widehat{N}_{h}^{k}(s,a,b,s^{\\prime})$ , it holds that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\bar{N}_{h}^{k}(s,a,b,s^{\\prime})-N_{h}^{k}(s,a,b,s^{\\prime})\\big|\\leq\\Big|\\bar{N}_{h}^{k}(s,a,b,s^{\\prime})-\\hat{N}_{h}^{k}(s,a,b,s^{\\prime})\\Big|\\!+\\!\\Big|\\widehat{N}_{h}^{k}(s,a,b,s^{\\prime})-N_{h}^{k}(s,a,b,s^{\\prime})\\Big|\\!-\\!N_{h}^{k}(s,a,b,s^{\\prime})\\Big|}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since $\\begin{array}{r}{\\widetilde{N}_{h}^{k}(s,a,b,s^{\\prime})=\\bar{N}_{h}^{k}(s,a,b,s^{\\prime})+\\frac{E_{\\epsilon,\\beta}}{2S}}\\end{array}$ and $\\bar{N}_{h}^{k}(s,a,b,s^{\\prime})\\geq0$ we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\widetilde{N}_{h}^{k}(s,a,b,s^{\\prime})>0,\\quad\\Big|\\widetilde{N}_{h}^{k}(s,a,b,s^{\\prime})-N_{h}^{k}(s,a,b,s^{\\prime})\\Big|\\leq E_{\\epsilon,\\beta}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For $\\bar{N}_{h}^{k}(s,a,b)$ , according to the constraints in the optimization problem (7), it holds that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left|\\bar{N}_{h}^{k}(s,a,b)-\\widehat{N}_{h}^{k}(s,a,b)\\right|\\leq\\frac{E_{\\epsilon,\\beta}}{4}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Combining with the condition in Lemma 5.8 with respect to $\\widehat{N}_{h}^{k}(s,a,b)$ , it holds that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\big|\\bar{N}_{h}^{k}(s,a,b)-N_{h}^{k}(s,a,b)\\big|\\leq\\Big|\\bar{N}_{h}^{k}(s,a,b)-\\widehat{N}_{h}^{k}(s,a,b)\\Big|+\\Big|\\widehat{N}_{h}^{k}(s,a,b)-N_{h}^{k}(s,a,b)\\Big|\\leq\\frac{E_{\\epsilon,\\beta}}{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since $\\begin{array}{r}{\\widetilde{N}_{h}^{k}(s,a,b)=\\bar{N}_{h}^{k}(s,a,b)+\\frac{E_{\\epsilon,\\beta}}{2}}\\end{array}$ we have ", "page_idx": 23}, {"type": "equation", "text": "$$\nN_{h}^{k}(s,a,b)\\leq\\widetilde{N}_{h}^{k}(s,a,b)\\leq N_{h}^{k}(s,a,b)+E_{\\epsilon,\\beta}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "According to the last line of the optimization problem (7), we have $\\begin{array}{r l}{\\bar{N}_{h}^{k}(s,a,b)}&{{}=}\\end{array}$ $\\textstyle\\sum_{s^{\\prime}\\in{\\cal{S}}}\\bar{N}_{h}^{k}(s,a,b,s^{\\prime})$ and therefore, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\widetilde{N}_{h}^{k}(s,a,b)=\\sum_{s^{\\prime}\\in\\mathcal{S}}\\widetilde{N}_{h}^{k}(s,a,b,s^{\\prime}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "The proof is complete by combining (52), (53) and (54). ", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The abstract claims that this paper is about differentially private reinforcement learning with self-play. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We discuss in Section 5 that the additional cost due to DP does not have optimal dependence on $H,S,A,B$ ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should refect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper provides the full set of assumptions and a complete proof. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This is a theory paper and we do not conduct experiments. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This is a theory paper and we do not conduct experiments. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This is a theory paper and we do not conduct experiments. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This is a theory paper and we do not conduct experiments. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative errorrates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This is a theory paper and we do not conduct experiments. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: This is a theory paper regarding privacy protection, which does not have negative societal impact. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: There is no risk of misuse of the algorithm in this paper. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This is a theory paper and we do not use other assets. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 28}, {"type": "text", "text": "\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: We do not introduce any new assets. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This is a theory paper and we do not conduct any experiments. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]