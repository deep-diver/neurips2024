[{"figure_path": "T07OHxcEYP/tables/tables_1_1.jpg", "caption": "Table 1: Comparison of our results (in blue) to existing work regarding regret without privacy (i.e., the privacy budget is infinity), regret under e-Joint DP and regret under e-Local DP. In the above, S is the number of states, A, B are the number of actions for both players, H is the planning horizon and K is the number of episodes (T = HK is the number of steps). Markov decision processes (MDPs) is a special case of Markov Games where B = 1. *: This result is the best known regret bound when there is no privacy concern. *: More discussions about this bound can be found in Chowdhury and Zhou [2022]. \u2020: The original regret bound in Garcelon et al. [2021] is derived under the setting of stationary MDP, and can be directly transferred to the bound here by adding \u221aH to the first term. \u2021: This algorithm achieved the best known results under single-agent MDPs, and our Algorithm 1 can obtain the same regret bounds under this setting.", "description": "This table compares the regret bounds (a measure of performance) of different algorithms for solving Markov Games (a type of game used in reinforcement learning) under various conditions.  It contrasts the regret without any privacy constraints, with the regret under e-Joint Differential Privacy (e-JDP) and e-Local Differential Privacy (e-LDP).  The table shows that the proposed algorithm (DP-Nash-VI) achieves comparable or better regret bounds than existing methods, even when considering privacy constraints. The different algorithms are categorized as algorithms designed for Markov Games and algorithms specifically optimized for Markov Decision Processes (MDPs, a simpler case of Markov Games).  The table clearly shows the superior performance of the new algorithm, especially in the context of privacy.", "section": "Related work"}]