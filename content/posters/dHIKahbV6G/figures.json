[{"figure_path": "dHIKahbV6G/figures/figures_1_1.jpg", "caption": "Figure 1: On DomainNet dataset, we visualize (a) The accuracy of CLIP on the six domains. (b) The image features extracted by CLIP's image encoder across different domains. The visualization show that CLIP exhibits inherent model bias. (c) The number of predictions for different classes on quickdraw and painting domains.", "description": "This figure visualizes CLIP's performance and inherent bias across different domains of the DomainNet dataset.  Panel (a) shows the accuracy of CLIP across six different domains (clipart, infograph, painting, quickdraw, real, sketch), highlighting significant performance variations across domains even within the same class. Panel (b) displays a t-SNE visualization of image features extracted by CLIP's image encoder, revealing that features from the same domain cluster together, indicating domain bias. Finally, panel (c) shows the distribution of predictions for several classes in the 'quickdraw' and 'painting' domains, demonstrating that CLIP's textual encoder shows a domain-specific preference for certain classes.", "section": "3.2 Model Bias in CLIP"}, {"figure_path": "dHIKahbV6G/figures/figures_8_1.jpg", "caption": "Figure 2: On DomainNet dataset, we visualize (a) The image features extracted by UMFC image encoder across different domains. (b) The classification probabilities of CLIP\u2019s text features on different domains.", "description": "This figure shows the results of the proposed UMFC method. (a) is a t-SNE visualization of image features extracted by UMFC image encoder from different domains. It shows that UMFC successfully reduces the domain bias by aligning image feature distributions from different domains. (b) shows the classification probabilities of CLIP\u2019s text features on different domains, comparing the uniform distribution, original CLIP and the calibrated CLIP by UMFC. It shows that UMFC also successfully reduces the text bias by making class probabilities more uniform across domains.", "section": "4 Unsupervised Multi-domain Feature Calibration"}, {"figure_path": "dHIKahbV6G/figures/figures_12_1.jpg", "caption": "Figure 3: Visualization of Image Features based on OpenCLIP series. (a) The OpenCLIP models are pre-trained with three different dataset (LAION-80M, LAION-400M and LAION-2B). (b) The OpenCLIP models are pre-trained on LAION-2B with different number of seen samples (3B, 34B) and architectures (ViT-B-16, ViT-B-32).", "description": "This figure visualizes image features extracted by various OpenCLIP models, demonstrating that the inherent bias of CLIP is not limited to a specific dataset or model architecture.  The top row shows the feature distributions from models pre-trained on different datasets (LAION-80M, 400M, and 2B), while the bottom row shows the feature distributions from models pre-trained on the LAION-2B dataset with varying numbers of images and different model architectures (ViT-B-16 and ViT-B-32). In all cases, features from the same domain cluster together, indicating the domain bias observed in CLIP.", "section": "A The Prevalence of the Observed Model Bias"}, {"figure_path": "dHIKahbV6G/figures/figures_13_1.jpg", "caption": "Figure 4: The domain transition direction between texts is similar to that between images.", "description": "This figure illustrates the core idea behind the Text Feature Calibration Module in UMFC.  It shows that the directional shift between different domains (e.g., from 'real' to 'quickdraw' images) in the image embedding space is similar to the directional shift in the text embedding space.  This observation supports UMFC's approach of using image features to estimate and counteract bias in text encoders, thereby improving domain generalization.", "section": "4 Unsupervised Multi-domain Feature Calibration"}]