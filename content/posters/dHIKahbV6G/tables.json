[{"figure_path": "dHIKahbV6G/tables/tables_6_1.jpg", "caption": "Table 1: Results on DomainNet under multi-domain Unsupervised Calibration. CLIP denotes zero-shot CLIP with a fixed text prompt template \"a photo of a [class]\", CLIP-E uses the ensemble prompt templates designed for Imagenet [39], CLIP-D uses the domain-specific templates, i.e., \u201ca [domain] image of [class]\u201d. CoOp and CLIP-Adapter are trained on multi-domain labeled data, e.g., 6 \u00d7 1 \u00d7 345 denotes the number of labeled data.", "description": "This table presents the results of different methods on the DomainNet dataset under unsupervised multi-domain calibration. It compares the performance of zero-shot CLIP (with fixed and ensemble prompts) and domain-specific prompts with methods using labeled data (CoOp and CLIP-Adapter).  The results show the average accuracy across six different domains (Clipart, Infograph, Painting, Quickdraw, Real, Sketch) and highlight the performance improvement of the proposed UMFC method.", "section": "5.2 Main Results on Unsupervised Calibration"}, {"figure_path": "dHIKahbV6G/tables/tables_7_1.jpg", "caption": "Table 2: Results on DomainNet under single-domain Unsupervised Calibration. 8 \u00d7 345 samples (each class has 8 samples) from a single domain are provided. CoOp (C/Q/I) and UMFC (C/Q/I) denote training samples for CoOp and UMFC from the \"Clipart\"/\"Quickdraw\"/\"Infograph\" domains, respectively.", "description": "This table presents the results of unsupervised calibration experiments on the DomainNet dataset.  It compares the performance of the proposed UMFC method against the baseline method CoOp when only a limited amount of labeled data from a single domain (Clipart, Quickdraw, or Infograph) is available. The table shows the average accuracy across six domains for each method. The numbers in parentheses following CoOp and UMFC indicate the source domain for the limited training data.", "section": "5.2 Main Results on Unsupervised Calibration"}, {"figure_path": "dHIKahbV6G/tables/tables_7_2.jpg", "caption": "Table 1: Results on DomainNet under multi-domain Unsupervised Calibration. CLIP denotes zero-shot CLIP with a fixed text prompt template \"a photo of a [class]\", CLIP-E uses the ensemble prompt templates designed for Imagenet [39], CLIP-D uses the domain-specific templates, i.e., \u201ca [domain] image of [class]\". CoOp and CLIP-Adapter are trained on multi-domain labeled data, e.g., 6 \u00d7 1 \u00d7 345 denotes the number of labeled data.", "description": "This table presents the results of multi-domain unsupervised calibration experiments on the DomainNet dataset. It compares the performance of several methods, including the zero-shot CLIP with fixed and ensemble prompts, a domain-specific prompt version, and two few-shot learning approaches (CoOp and CLIP-Adapter). The results are presented as average accuracy across six domains (Clipart, Infograph, Painting, Quickdraw, Real, Sketch) and show the impact of unsupervised calibration on improving model generalizability.", "section": "5.2 Main Results on Unsupervised Calibration"}, {"figure_path": "dHIKahbV6G/tables/tables_7_3.jpg", "caption": "Table 4: Comparison Results on ImageNet-Variants under Transductive Learning.", "description": "This table presents the performance comparison of different methods on the ImageNet-Variants dataset under the transductive learning setting.  The methods compared are CLIP, CLIP-E (an ensemble of CLIP prompts), UMFC (Unsupervised Multi-domain Feature Calibration), and UMFC combined with CLIP-E. The results are presented as the average accuracy across three ImageNet-Variants subsets: IN-A, IN-R, and IN-S.  It demonstrates the effectiveness of UMFC in improving the performance of CLIP-based models on this challenging dataset.", "section": "5.3 Main Results on Transductive Learning"}, {"figure_path": "dHIKahbV6G/tables/tables_7_4.jpg", "caption": "Table 5: Comparison Results on ImageNet-Variants under Test-Time Adaptation.", "description": "This table presents the comparison of different methods' performance on ImageNet-Variants under Test-Time Adaptation setting.  It shows the average accuracy across three ImageNet variants (IN-A, IN-R, IN-S) for CLIP, TPT, and two variants of UMFC (UMFC-Memory and UMFC-EMA).  The results highlight the performance improvement achieved by UMFC compared to the baseline methods.", "section": "5.4 Main Results on Test-Time Adaptation"}, {"figure_path": "dHIKahbV6G/tables/tables_7_5.jpg", "caption": "Table 6: Comparison Results on DomainNet under Test-Time Adaptation. UMFC-Memory and UMFC-EMA represent different ways to update the statics vectors for calibration.", "description": "This table presents the results of the Test-Time Adaptation experiments on the DomainNet dataset.  It compares the performance of CLIP, TPT, and two variants of the UMFC method (UMFC-Memory and UMFC-EMA) across six domains (Clipart, Infograph, Painting, Quickdraw, Real, Sketch).  The key difference between UMFC-Memory and UMFC-EMA lies in how they update the statistical information used for calibration. The table shows that both UMFC methods achieve higher accuracy than CLIP and TPT, indicating that the proposed calibration method is effective in adapting to the test-time setting even when data arrives in batches.", "section": "5.4 Main Results on Test-Time Adaptation"}, {"figure_path": "dHIKahbV6G/tables/tables_8_1.jpg", "caption": "Table 7: Ablation study on the effects of TFC and IFC under Transductive Learning.", "description": "This table presents the ablation study results on the effects of the Text Feature Calibration (TFC) and Image Feature Calibration (IFC) modules under the transductive learning setting. It shows the performance of CLIP, IFC only, TFC only, and UMFC (combining both IFC and TFC) across different domains (C, I, P, Q, R, S) of the DomainNet dataset.  The average accuracy across all domains is also provided for each method, demonstrating the individual and combined impact of IFC and TFC on the overall performance.", "section": "5.5 Ablation Study"}, {"figure_path": "dHIKahbV6G/tables/tables_9_1.jpg", "caption": "Table 8: The impact of cluster number M on DomainNet under Transductive Learning.", "description": "This table presents the results of the UMFC method on the DomainNet dataset under the transductive learning setting.  It shows how the average accuracy varies across six different domains (Clipart, Infograph, Painting, Quickdraw, Real, Sketch) as the number of clusters (M) used in the UMFC algorithm is changed.  The purpose is to demonstrate the robustness of the UMFC method to the choice of the hyperparameter M.", "section": "5.5 Ablation Study"}, {"figure_path": "dHIKahbV6G/tables/tables_9_2.jpg", "caption": "Table 6: Comparison Results on DomainNet under Test-Time Adaptation. UMFC-Memory and UMFC-EMA represent different ways to update the statics vectors for calibration.", "description": "This table shows the performance comparison of different methods on the DomainNet dataset under the test-time adaptation setting.  It compares the performance of CLIP, TPT, UMFC-Memory, and UMFC-EMA across six different domains (Clipart, Infograph, Painting, Quickdraw, Real, Sketch). UMFC-Memory and UMFC-EMA are two different variations of the UMFC algorithm, distinguished by how they update their statistical vectors during the calibration process. The table highlights the effectiveness of UMFC in adapting to new data without requiring training, demonstrating improvement over CLIP and TPT in most domains.", "section": "5.4 Main Results on Test-Time Adaptation"}, {"figure_path": "dHIKahbV6G/tables/tables_12_1.jpg", "caption": "Table 10: Comparision Results on DominaNet using OpenCLIP.", "description": "This table presents the comparison results on the DomainNet dataset using OpenCLIP models with different architectures (ViT-B-16, ViT-B-32, ViT-H-14).  For each architecture, the table shows the average accuracy across six domains (Clipart, Infograph, Painting, Quickdraw, Real, Sketch) before and after applying the UMFC method. The results demonstrate the improvement in accuracy achieved by UMFC across various architectures, highlighting its generalizability and effectiveness.", "section": "A The Prevalence of the Observed Model Bias"}, {"figure_path": "dHIKahbV6G/tables/tables_14_1.jpg", "caption": "Table 11: The impact of cluster number M on ImageNet-Variants under Transductive Learning.", "description": "This table presents the results of the UMFC method on the ImageNet-Variants dataset under the transductive learning setting. It shows the impact of varying the number of clusters (M) used in the clustering algorithm on the performance of the model.  The performance is evaluated across three subsets of ImageNet-Variants (IN-A, IN-R, and IN-S) and an average across the three. The table shows that the performance of the model is relatively stable across different numbers of clusters, suggesting that the method is robust to the choice of this hyperparameter.", "section": "5.3 Main Results on Transductive Learning"}, {"figure_path": "dHIKahbV6G/tables/tables_15_1.jpg", "caption": "Table 12: Computation Cost under Transductive Learning.", "description": "This table compares the training time, inference time, number of epochs, and memory usage of different methods under the transductive learning setting.  It shows that UMFC has significantly lower computational costs compared to other methods like CLIP, MUST, and CoOp, making it a more efficient approach.", "section": "5.2 Main Results on Unsupervised Calibration"}, {"figure_path": "dHIKahbV6G/tables/tables_15_2.jpg", "caption": "Table 13: Computation Cost under Test-Time Adaptation.", "description": "This table compares the computation cost (inference time and memory usage) of the proposed UMFC method and the baseline TPT method in the Test-Time Adaptation (TTA) setting. UMFC shows significantly lower inference time and memory consumption.", "section": "5.4 Main Results on Test-Time Adaptation"}]