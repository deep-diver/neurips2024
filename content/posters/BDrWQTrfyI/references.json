{"references": [{"fullname_first_author": "Sainbayar Sukhbaatar", "paper_title": "Branch-train-mix: Mixing expert LLMs into a mixture-of-experts LLM", "publication_date": "2024-03-14", "reason": "This paper is a direct antecedent to the current work, proposing a similar method for upcycling pre-trained models into MoEs but with limitations addressed by the current work."}, {"fullname_first_author": "William Fedus", "paper_title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity", "publication_date": "2022-07-15", "reason": "This paper introduces Switch Transformers, a key technique used in Mixture of Experts models, which is foundational to the current research and the methods used to improve efficiency."}, {"fullname_first_author": "Noam Shazeer", "paper_title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "publication_date": "2017-01-17", "reason": "This seminal paper introduced the Mixture of Experts (MoE) layer, providing the foundation for the current research's focus on MoE models and parameter upcycling."}, {"fullname_first_author": "Jared Kaplan", "paper_title": "Scaling Laws for Neural Language Models", "publication_date": "2020-01-20", "reason": "This highly influential paper establishes scaling laws for LLMs, which is crucial context for understanding the motivations behind using MoEs to improve efficiency and the challenges of scaling LLM size."}, {"fullname_first_author": "Xiaofeng Zhang", "paper_title": "Mixture of Attention Heads: Selecting Attention Heads per Token", "publication_date": "2022-10-14", "reason": "This work extends MoEs to include attention experts, a crucial concept that the current research builds upon and improves by incorporating soft-routing and KV upcycling."}]}