[{"figure_path": "BDrWQTrfyI/tables/tables_6_1.jpg", "caption": "Table 1: Perplexity evaluation (\u2193) of BAM versus BTX for small-scale experiments (using a seed model with 590M parameters). Highlighted entries denote models that outperform the BTX baseline. BAM consistently outperforms the baseline under both compute matching and token matching regimes.", "description": "This table presents the perplexity scores achieved by different models in small-scale experiments. The models include a baseline model, specialized dense models, and generalist models like BAM and BTX. The perplexity scores are compared under two scenarios: data-matching (DM) and compute-matching (CM). Lower perplexity indicates better performance. The highlighted entries show where BAM outperforms the baseline BTX model.", "section": "6 Results"}, {"figure_path": "BDrWQTrfyI/tables/tables_6_2.jpg", "caption": "Table 2: Perplexity evaluation (\u2193) of BAM versus BTX for large-scale experiments (using a seed model of 2B parameters). Highlighted entries indicate models outperforming the BTX baseline. BAM consistently surpasses BTX in all domains under both compute matching and token matching regimes.", "description": "This table presents the perplexity scores achieved by different models on a large-scale experiment using a 2-billion parameter seed model.  It compares the baseline BTX model with various configurations of the proposed BAM model (with and without key-value sharing, and under data-matching and compute-matching conditions). Lower perplexity scores indicate better model performance. The highlighted entries show where BAM outperforms BTX.", "section": "6 Results"}, {"figure_path": "BDrWQTrfyI/tables/tables_7_1.jpg", "caption": "Table 3: Benchmark evaluations (\u2191) for BAM versus BTX. Table shows large-scale experiments (using a seed model of 2B parameters). Highlighted entries indicate models outperform the BTX baseline. All BAM variants outperform BTX on average.", "description": "This table presents the benchmark results of different models on various downstream tasks for large-scale experiments. It compares the performance of two variants of BAM (with and without shared key-value parameters in attention experts) against the baseline BTX model and specialized dense models.  The results are presented as average scores across different domains and tasks, showing that BAM consistently outperforms BTX.", "section": "6 Results"}, {"figure_path": "BDrWQTrfyI/tables/tables_8_1.jpg", "caption": "Table 4: Perplexity (\u2193) ablation studies for small-scale experiments assess how BAM's total parameter count compares with BTX. To match BTX's total parameters with those of BAM, we incrementally increase the number of FFN experts in BTX from 4 up to a maximum of 8. The additional experts are upcycled using the FFN parameters from the same dense seed model.", "description": "This table presents an ablation study comparing the performance of BAM and BTX models with varying numbers of total and active parameters.  It shows how perplexity changes as the number of FFN experts in BTX is increased to match BAM's total parameter count.  The goal is to determine whether BAM's superior performance is due to its unique parameter upcycling method or simply a result of having more parameters.", "section": "7 Ablations"}, {"figure_path": "BDrWQTrfyI/tables/tables_8_2.jpg", "caption": "Table 5: Perplexity ablation studies on small-scale experiments compare the performance of BAM to BTX under conditions where both models have equivalent numbers of active parameters and total parameters. In addition to utilizing six FFN experts, we have adjusted BTX\u2019s routing mechanism from top-1 to top-3 routing. This change means that instead of activating just one FFN expert per token representation, three are now activated, effectively increasing the number of active parameters per input.", "description": "This table compares perplexity scores for BAM and a modified version of BTX where the number of active parameters and total parameters are matched to BAM.  The modification to BTX involves increasing the number of experts and using top-3 routing instead of top-1 routing in the MoE layers. The results show that even when the number of parameters is matched, BAM still outperforms BTX.", "section": "7 Ablations"}, {"figure_path": "BDrWQTrfyI/tables/tables_8_3.jpg", "caption": "Table 6: Perplexity ablation (\u2193) of BAM compute matched with different attention experts routing methods.", "description": "This table presents the perplexity scores achieved by different routing methods for attention experts within the BAM model. The results are compared against the baseline BTX model to show the effectiveness of soft routing in achieving lower perplexity.  The various routing methods compared are soft routing (all experts), top-2 routing, and top-1 routing.  The perplexity is broken down by domain (Pretrain, Code, Law, Math) and averaged across all domains.  The results indicate that soft routing provides superior performance to the baseline.", "section": "7 Ablations"}, {"figure_path": "BDrWQTrfyI/tables/tables_9_1.jpg", "caption": "Table 7: Estimates for FLOPs per token during inference. Row 1 shows BAM with KV experts, row 2 shows the standard BTX, and row 3 shows a parameter-matching variant of BTX with 6 FFN experts & top-3 routing (refer to Table 5 in the paper). We use a prompt with context length of 256.", "description": "This table compares the computational cost (FLOPs) per token during inference for different model configurations. It compares BAM with its variant that shares key-value parameters (KV) across experts, the standard BTX model, and a modified BTX model with more experts and a different routing strategy.  The goal is to demonstrate the trade-offs between model performance and computational efficiency in different approaches.", "section": "7.3 Analysis on Inference Efficiency"}, {"figure_path": "BDrWQTrfyI/tables/tables_14_1.jpg", "caption": "Table 8: Model architecture details for large and small scale experiments", "description": "This table details the architectural hyperparameters used for both large and small scale experiments.  It shows the embedding dimension, FFN dimension, number of heads, number of key-value heads, vocabulary size, activation function, number of layers, positional embedding type, whether input and output embeddings are shared, and the number of parameters in the seed model used for each experimental setting. These hyperparameters significantly affect the model's capacity and performance.", "section": "5 Experiment Details"}, {"figure_path": "BDrWQTrfyI/tables/tables_14_2.jpg", "caption": "Table 9: Active and total parameter counts for small scale ablation experiments. All transformer parameters are reported as per transformer block.", "description": "This table details the parameter counts for different model components in small-scale ablation experiments. It shows the number of active and total parameters for each component in the different model architectures: Dense, BAM, BAM (KV sharing), BTX top-1, and BTX top-3. The table helps understand the computational differences between the models.  Active parameters refer to the subset of parameters used during inference, while total parameters represent the overall model size.", "section": "A Model Architecture Details"}, {"figure_path": "BDrWQTrfyI/tables/tables_15_1.jpg", "caption": "Table 10: Detailed breakdown of parameters and FLOPs per token in forward pass (inference) for each MoE layer, comparing the standard BTX and BAM with KV experts. Note that BAM uses soft-gating MoA which utilizes all the attention experts, increasing total attention FLOPs and introduces a second router / gating network for attention experts.", "description": "This table provides a detailed breakdown of the parameters and FLOPs (floating point operations) used per token during the inference phase for both the standard BTX (Branch-Train-Mix) model and the proposed BAM (Branch-Attend-Mix) model with KV (Key-Value) experts.  It shows the computational cost of each operation within the MoE (Mixture of Experts) layer, including the attention router, attention mechanisms (QKV projection, masking, projection), and the FFN (feed-forward network) router and FFN itself.  The table highlights the differences in computational cost between BTX and BAM, particularly due to BAM's use of soft-gating MoA (Mixture of Attention), which involves all attention experts, resulting in increased FLOPs compared to BTX's top-k routing mechanism.", "section": "B Inference Efficiency Analysis"}]