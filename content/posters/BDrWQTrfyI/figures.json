[{"figure_path": "BDrWQTrfyI/figures/figures_1_1.jpg", "caption": "Figure 1: BAM operates in three phases. Different colors correspond to different expert domains, which indicates the pre-trained seed model. White indicates random parameter initialization, gradient color indicates parameter merging 1) Branching: Begin with a pre-trained dense seed model and create N copies of it. 2) Continued Pre-training: Continue to pre-train each copy independently on its own data mixture. This process yields specialized dense expert models. 3) Mixture Model Training: Utilize these specialized dense expert models to initialize both the FFN and attention experts of the mixture model. The router layers are initialized randomly. All other parameters are derived by averaging the corresponding layers in each of the dense experts. Note that BAM employs a parallel attention transformer architecture that concurrently computes attention experts and FFN experts. The figure is loosely based on Figure 1 from Sukhbaatar et al. [1].", "description": "This figure illustrates the three phases of the Branch-Attend-Mix (BAM) model training process.  First, a single dense seed model is branched into multiple copies. Second, each copy is independently pre-trained on a specialized dataset, creating specialized dense expert models. Finally, these specialized models are used to initialize the attention and feed-forward network (FFN) experts of the BAM mixture model, which uses a parallel attention transformer architecture for improved efficiency.", "section": "4 BAM: Branch-Attend-Mix"}]