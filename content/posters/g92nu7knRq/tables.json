[{"figure_path": "g92nu7knRq/tables/tables_7_1.jpg", "caption": "Table 1: Comprehensive assessment of model's fundamental capabilities, in which DHA models demonstrate competitive performance while requiring significantly fewer training resources. Models with \u2020 use MHA.", "description": "This table presents a comprehensive evaluation of the DHA and GQA models' performance across various downstream tasks, including commonsense reasoning, reading comprehension, and language modeling.  It compares models of different sizes (LLaMA2-7B, Sheared-LLaMA-2.7B, Sheared-LLaMA-1.3B) with varying head budget ratios (50% and 25%).  The results highlight DHA's ability to achieve competitive performance while utilizing significantly fewer training resources compared to the original MHA models and the GQA baseline. The table shows scores for multiple tasks, which allows for a thorough comparison across different model architectures and resource budgets.", "section": "5.2 Experimental Results"}, {"figure_path": "g92nu7knRq/tables/tables_7_2.jpg", "caption": "Table 2: Ablation Results of DHA w.o. Linear Heads Fusion and Adaptvie Transformation. Experiments are conducted with LLaMA2-7B with 25% heads budget and 0.5B & 1B training budget on 0-shot Evaluation.", "description": "This table presents the ablation study results for the Decoupled-Head Attention (DHA) model. It shows the impact of removing the linear heads fusion and adaptive transformation components on the model's performance. The experiments were conducted using the LLaMA2-7B model with a 25% head budget, and the results are evaluated using 0-shot evaluation on several downstream tasks.  The table compares the performance of the full DHA model to versions without linear heads fusion and without adaptive transformation, revealing the contribution of each component to the overall model performance.", "section": "5.2 Experimental Results"}, {"figure_path": "g92nu7knRq/tables/tables_8_1.jpg", "caption": "Table 3: Data budget allocation to fusion and continued pre-training(CT) and 0-shot Task Average Accuracy (%) in DHA-1.3B.", "description": "This table presents the results of an experiment investigating the impact of different budget allocations between the fusion and continued pre-training phases on the performance of a DHA-1.3B model.  The model's performance is measured using 0-shot task average accuracy. The table shows that increasing the fusion budget (while maintaining a fixed total budget of 2 billion tokens) leads to improved performance at the initialization point, indicating the importance of the fusion phase for knowledge retention.  The continued pre-training phase, however, sees only minor performance changes despite the varying budget allocated.", "section": "5.3 Analysis"}, {"figure_path": "g92nu7knRq/tables/tables_17_1.jpg", "caption": "Table 1: Comprehensive assessment of model's fundamental capabilities, in which DHA models demonstrate competitive performance while requiring significantly fewer training resources. Models with \u2020 use MHA.", "description": "This table presents a comprehensive evaluation of the fundamental capabilities of various LLMs, including DHA models and those using MHA.  It compares their performance across multiple common sense and reading comprehension tasks, as well as language modeling (LM) tasks. The key takeaway is that DHA models achieve competitive performance with significantly lower training resource requirements compared to models using the standard MHA method.", "section": "5 Empirical Evaluation"}, {"figure_path": "g92nu7knRq/tables/tables_18_1.jpg", "caption": "Table 1: Comprehensive assessment of model's fundamental capabilities, in which DHA models demonstrate competitive performance while requiring significantly fewer training resources. Models with \u2020 use MHA.", "description": "This table presents a comprehensive evaluation of the fundamental capabilities of various language models, including those using the Decoupled-Head Attention (DHA) method and the Multi-Head Attention (MHA) method.  It compares the performance across several downstream tasks (commonsense reasoning, reading comprehension, and language modeling) for different model sizes (LLaMA2-7B, Sheared-LLaMA-2.7B, and Sheared-LLaMA-1.3B) and under varying head budget and training budget conditions. The table highlights that the DHA models achieve competitive performance using significantly less training data compared to models using the MHA method.", "section": "5 Empirical Evaluation"}, {"figure_path": "g92nu7knRq/tables/tables_19_1.jpg", "caption": "Table 1: Comprehensive assessment of model's fundamental capabilities, in which DHA models demonstrate competitive performance while requiring significantly fewer training resources. Models with \u2020 use MHA.", "description": "This table presents a comprehensive evaluation of the fundamental capabilities of different model architectures across various sizes and training budgets.  It compares the performance of Decoupled-Head Attention (DHA) models with different head budget ratios (50% and 25%) against Grouped-Query Attention (GQA) models and Multi-Head Attention (MHA) baselines (indicated by \u2020) on nine representative downstream tasks, including commonsense reasoning, reading comprehension, and language modeling. The table highlights the competitive performance of DHA models while significantly reducing the training resource requirements compared to MHA and achieving faster training compared to GQA.  The results demonstrate DHA's efficiency in achieving competitive performance with a fraction of the training cost and computational resources.", "section": "5 Empirical Evaluation"}, {"figure_path": "g92nu7knRq/tables/tables_21_1.jpg", "caption": "Table 1: Comprehensive assessment of model\u2019s fundamental capabilities, in which DHA models demonstrate competitive performance while requiring significantly fewer training resources. Models with \u2020 use MHA.", "description": "This table presents a comprehensive evaluation of the DHA and GQA models' performance across various downstream tasks, including commonsense reasoning, reading comprehension, and language modeling.  It compares models of different sizes (LLaMA2-7B, Sheared-LLaMA-2.7B, and Sheared-LLaMA-1.3B) with varying head budget ratios (50% and 25%). The table highlights DHA's ability to achieve competitive performance with significantly less training data (1B tokens vs. 2T tokens) compared to the baseline MHA models.  It also shows the improvement of DHA over GQA under low-resource settings.", "section": "5 Empirical Evaluation"}, {"figure_path": "g92nu7knRq/tables/tables_22_1.jpg", "caption": "Table 1: Comprehensive assessment of model's fundamental capabilities, in which DHA models demonstrate competitive performance while requiring significantly fewer training resources. Models with \u2020 use MHA.", "description": "This table presents a comprehensive evaluation of the DHA and GQA models' performance across various downstream tasks, including commonsense reasoning, reading comprehension, and language modeling.  It compares the performance of DHA models with different head budget ratios (50% and 25%) against baseline LLaMA models and GQA models.  The results are broken down by task and show that DHA consistently achieves comparable or superior performance while using significantly less training data.", "section": "5 Empirical Evaluation"}, {"figure_path": "g92nu7knRq/tables/tables_23_1.jpg", "caption": "Table 5: Comparison of log(PPL) between DHA and GQA with NACL.", "description": "This table compares the log-perplexity (PPL) scores of GQA and DHA models, both with and without the NACL (Neural Cache Alignment) method, at a 25% compression rate.  Lower PPL scores indicate better model performance.  The comparison demonstrates how DHA maintains or improves performance compared to GQA, even when combined with a KV cache compression technique.", "section": "5.2 Experimental Results"}, {"figure_path": "g92nu7knRq/tables/tables_23_2.jpg", "caption": "Table 6: Comparison of Avg ACC and PPL between different methods at 7B-25% (5B).", "description": "This table compares the average accuracy (Avg ACC) and perplexity (PPL) of three different methods: DHA, GQA, and GQA with CKA-based grouping, all using the LLaMA 7B model with 25% head compression and trained with 5 billion tokens.  It shows that DHA achieves better results than both GQA approaches, highlighting its effectiveness in achieving a balance between efficiency and performance.", "section": "5.2 Experimental Results"}]