[{"figure_path": "RwgNbIpCpk/figures/figures_2_1.jpg", "caption": "Figure 1: Left: The MRConv block is composed of a MRConv layer, GELU activation, pointwise linear layer, to mix the channels, and a gated linear unit. Middle: During training, the MRConv layer processes the input using N branches each with it's own convolution kernel of increasing length and BatchNorm parameters. The output of the layer is given by pointwise multiplying each branch by ai and summing. Right: At inference the branches can be reparameterised into a single convolution.", "description": "This figure illustrates the MRConv block architecture. The left panel shows the block's components: a MRConv layer, GELU activation, pointwise linear layer, GLU, and normalization. The middle panel details the training process, where the input is processed through N parallel branches, each with its own convolution kernel and BatchNorm parameters, then summed after pointwise multiplication with learnable weights. The right panel shows the inference stage, where the parallel branches are reparameterized into a single convolution for efficiency.", "section": "3 Reparameterized Multi-Resolution Convolutions"}, {"figure_path": "RwgNbIpCpk/figures/figures_4_1.jpg", "caption": "Figure 2: Multi-resolution structural reparameterization. During training, we parameterize each branch with a kernel of increasing length but fixed number of parameters. For the Fourier kernels, we use only a handful of low-frequency modes and for the dilated kernels we increase the dilation factor. At inference, we combine the kernels into a single kernel by merging the BN parameters with the kernel parameters and performing a learnt weighted summation.", "description": "This figure illustrates the multi-resolution structural reparameterization used in MRConv. During training, multiple branches with kernels of increasing length but a fixed number of parameters are used.  For Fourier kernels, only low-frequency modes are employed, while for dilated kernels, the dilation factor is increased.  At inference, these branches are reparameterized into a single kernel by merging batch normalization (BN) parameters with the kernel parameters and performing a learned weighted summation. This improves efficiency and allows for the use of longer kernels effectively.", "section": "3.2 Multi-Resolution Convolutions"}, {"figure_path": "RwgNbIpCpk/figures/figures_7_1.jpg", "caption": "Figure 3: Left: ImageNet Top-1 Acc. vs. Throughput. Right: Distribution of ||\u03b1||\u00b2 norms for each depth for MRConv trained on ListOps and CIFAR respectively. Changing composition of kernels highlights how the convolution kernels are non-stationary with respect to depth.", "description": "This figure shows two plots. The left plot shows a comparison of the top-1 accuracy and throughput of different models on the ImageNet dataset.  The right plot displays the distribution of the squared L2 norm of the learned weights (\u03b1) across different depths and resolutions of the MRConv model. The color intensity represents the magnitude of ||\u03b1||\u00b2.  This visualization helps illustrate how the kernels' composition changes across different depths, indicating the model's non-stationary behavior with respect to depth.", "section": "5.2 Pixel-Level 1D Image Classification"}, {"figure_path": "RwgNbIpCpk/figures/figures_15_1.jpg", "caption": "Figure 4: Runtime Comparison. Runtime comparison of MRConv versus PyTorch's Multi-Head attention (MHA) implementation and FlashAttention at inference with increasing sequence length.", "description": "This figure compares the inference time of various sequence models against increasing sequence lengths. The models compared are MRConv, FlashAttention, S4D, and Multi-Head Attention. The graph shows that FlashAttention and MRConv scale better than Multi-Head Attention and S4D for longer sequences, with MRConv demonstrating the best performance in terms of inference speed. This highlights MRConv's efficiency for long sequences.", "section": "D.1 Runtime Comparison"}, {"figure_path": "RwgNbIpCpk/figures/figures_19_1.jpg", "caption": "Figure 1: Left: The MRConv block is composed of a MRConv layer, GELU activation, pointwise linear layer, to mix the channels, and a gated linear unit. Middle: During training, the MRConv layer processes the input using N branches each with it's own convolution kernel of increasing length and BatchNorm parameters. The output of the layer is given by pointwise multiplying each branch by ai and summing. Right: At inference the branches can be reparameterised into a single convolution.", "description": "This figure illustrates the MRConv block architecture.  The left panel shows the block's components: a multi-resolution convolution (MRConv) layer, GELU activation, a pointwise linear layer for channel mixing, and a gated linear unit (GLU). The middle panel details the training process, where the input is processed through N parallel branches, each with its own kernel and batch normalization.  The branches' outputs are weighted and summed. Finally, the right panel shows that during inference, these branches are reparameterized into a single convolution for efficiency.", "section": "3 Reparameterized Multi-Resolution Convolutions"}, {"figure_path": "RwgNbIpCpk/figures/figures_19_2.jpg", "caption": "Figure 1: Left: The MRConv block is composed of a MRConv layer, GELU activation, pointwise linear layer, to mix the channels, and a gated linear unit. Middle: During training, the MRConv layer processes the input using N branches each with it's own convolution kernel of increasing length and BatchNorm parameters. The output of the layer is given by pointwise multiplying each branch by ai and summing. Right: At inference the branches can be reparameterised into a single convolution.", "description": "This figure shows the architecture of the MRConv block. The left panel shows the components of the block, including a multi-resolution convolution layer, GELU activation, a pointwise linear layer, and a gated linear unit. The middle panel illustrates the training process, where the input is processed by N branches, each with its own convolution kernel and batch normalization parameters. The outputs of the branches are weighted and summed. The right panel depicts the inference process, where the branches are reparameterized into a single convolution.", "section": "3 Reparameterized Multi-Resolution Convolutions"}, {"figure_path": "RwgNbIpCpk/figures/figures_25_1.jpg", "caption": "Figure 1: Left: The MRConv block is composed of a MRConv layer, GELU activation, pointwise linear layer, to mix the channels, and a gated linear unit. Middle: During training, the MRConv layer processes the input using N branches each with it's own convolution kernel of increasing length and BatchNorm parameters. The output of the layer is given by pointwise multiplying each branch by ai and summing. Right: At inference the branches can be reparameterised into a single convolution.", "description": "This figure shows the architecture of the MRConv block.  The left panel shows the block's composition: a MRConv layer, a GELU activation function, a pointwise linear layer, and a gated linear unit (GLU). The middle panel illustrates the training process, where the input is processed through N parallel branches, each having its own convolution kernel of increasing length and batch normalization (BatchNorm) parameters.  The branches' outputs are weighted (by learnable parameters \u03b1\u1d62) and summed.  The right panel demonstrates the inference process, where the N branches are efficiently reparameterized into a single convolution operation.", "section": "3 Reparameterized Multi-Resolution Convolutions"}]