[{"figure_path": "eOAPWWOGs9/figures/figures_2_1.jpg", "caption": "Figure 1: An overview of AUTOPSV. It utilizes an outcome-supervised verifier to automatically generate process annotations for each reasoning step by detecting its own confidence variations, without relying on ground truth annotations. AUTOPSV efficiently produces annotations serving as process supervision during the LLM training, which sidesteps costly annotations.", "description": "The figure illustrates the AUTOPSV framework.  It starts with an outcome-supervised verifier trained on the correctness of final answers. This verifier assigns confidence scores to each reasoning step.  AUTOPSV analyzes the relative changes in these confidence scores to automatically generate process annotations, labeling steps as correct or incorrect.  These automatically generated annotations then serve as training data for a process-supervised verifier, improving the overall performance in selecting correct answers from multiple LLM-generated outputs. The process avoids the need for expensive human or model-based annotations.", "section": "3 AUTOPSV"}, {"figure_path": "eOAPWWOGs9/figures/figures_17_1.jpg", "caption": "Figure 1: An overview of AUTOPSV. It utilizes an outcome-supervised verifier to automatically generate process annotations for each reasoning step by detecting its own confidence variations, without relying on ground truth annotations. AUTOPSV efficiently produces annotations serving as process supervision during the LLM training, which sidesteps costly annotations.", "description": "The figure illustrates the AUTOPSV framework.  It starts with an outcome-supervised verifier trained on the correctness of final answers. This verifier assigns confidence scores to each reasoning step.  AUTOPSV then analyzes the relative changes in these confidence scores to automatically generate process annotations, labeling steps as correct or incorrect.  These automatically generated annotations are then used to train a process-supervised verifier, improving the LLM's reasoning capability. The process avoids the need for expensive manual or model-induced annotations.", "section": "3 AUTOPSV"}, {"figure_path": "eOAPWWOGs9/figures/figures_19_1.jpg", "caption": "Figure 1: An overview of AUTOPSV. It utilizes an outcome-supervised verifier to automatically generate process annotations for each reasoning step by detecting its own confidence variations, without relying on ground truth annotations. AUTOPSV efficiently produces annotations serving as process supervision during the LLM training, which sidesteps costly annotations.", "description": "This figure illustrates the AUTOPSV framework.  It starts with an outcome-supervised verifier trained on the correctness of final answers. This verifier assigns confidence scores to each reasoning step.  AUTOPSV then analyzes the relative changes in these confidence scores across steps to automatically generate process annotations.  These annotations highlight potential errors even without ground truth, serving as process supervision data for training an enhanced verifier.  This avoids the need for manual annotations or computationally expensive model-induced methods.", "section": "3 AUTOPSV"}]