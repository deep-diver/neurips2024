[{"figure_path": "Eok6HbcSRI/tables/tables_9_1.jpg", "caption": "Table 1: Performance of Topological Vision Transformers with tree-based masking. For each attention kernel, we present the results of the best variant in bold and Performer baselines in blue.", "description": "This table shows the performance of different Topological Vision Transformer models using tree-based masking. Each row represents a different configuration of the model, specifying the activation function used, whether parameter sharing was used across different attention heads (synced), and the number of extra learnable parameters per layer. The table compares the accuracy of these models on the ImageNet and Places365 datasets. The best-performing model for each configuration is highlighted in bold, and the results for the Performer baselines are shown in blue.", "section": "4.4 Large Scale Transformer Experiments using FTFI"}, {"figure_path": "Eok6HbcSRI/tables/tables_21_1.jpg", "caption": "Table 2: Statistics of the graph classification datasets used in this paper.", "description": "This table presents the characteristics of various graph datasets employed for graph classification in the research paper.  For each dataset, it lists the number of graphs, the number of classes (labels), the average number of nodes per graph, the average number of edges per graph, the number of node labels (if any), and the number of node attributes (if any).  This information is crucial for understanding the scale and complexity of the datasets used in the experiments and for evaluating the generalizability of the results.", "section": "4.2 Approximation Quality of FTFI"}, {"figure_path": "Eok6HbcSRI/tables/tables_21_2.jpg", "caption": "Table 3: Feature processing time of FTFI compared to exact shortest path kernel computation. We observe that FTFI achieves significant speedups up to 90% reduction in processing time. All times are reported in seconds (s).", "description": "This table compares the time taken for feature processing using the Fast Tree-Field Integrator (FTFI) method against the exact shortest path kernel computation (BGFI).  It shows the processing time for various graph datasets (MUTAG, ENZYMES, NCI1, PTC-MR, D&D, PROTEINS) and highlights the significant speedup achieved by FTFI in most cases, with reductions up to 90%.", "section": "4.2 Approximation Quality of FTFI"}, {"figure_path": "Eok6HbcSRI/tables/tables_23_1.jpg", "caption": "Table 4: Comparison of FTFI with a broad range of graph kernel-based classification approaches. We observe that FTFI achieves performance similar to that of Exact SP, its exact counterpart, across almost all datasets. The baseline results have been compiled from Nikolentzos et al. [2021]. OOT and OOM indicate that the corresponding algorithm ran out of time or memory respectively.", "description": "This table compares the performance of the Fast Tree-Field Integrator (FTFI) algorithm against various baseline graph kernel classification methods on several datasets. It demonstrates that FTFI achieves comparable accuracy to the exact shortest path (SP) kernel, while offering significant speed improvements over other methods.", "section": "4.2 Approximation Quality of FTFI"}, {"figure_path": "Eok6HbcSRI/tables/tables_23_2.jpg", "caption": "Table 4: Comparison of FTFI with a broad range of graph kernel-based classification approaches. We observe that FTFI achieves performance similar to that of Exact SP, its exact counterpart, across almost all datasets. The baseline results have been compiled from Nikolentzos et al. [2021]. OOT and OOM indicate that the corresponding algorithm ran out of time or memory respectively.", "description": "This table compares the performance of the proposed Fast Tree-Field Integrator (FTFI) method against various existing graph kernel-based classification approaches across multiple datasets.  It shows that FTFI achieves comparable accuracy to the exact Shortest Path (SP) kernel method, which it is designed to approximate, while significantly outperforming many other techniques. Note that 'OOT' signifies that the algorithm ran out of time, and 'OOM' means it ran out of memory. The baseline results are taken from a previous publication by Nikolentzos et al. (2021).", "section": "4.2 Approximation Quality of FTFI"}, {"figure_path": "Eok6HbcSRI/tables/tables_24_1.jpg", "caption": "Table 5: Hyperparameters for the different ViT models used in this paper", "description": "This table presents the hyperparameters used for the various Vision Transformer (ViT) models in the experiments.  It shows the number of heads, layers, hidden dimension, MLP dimension, total number of parameters, and patch size for ViT-Base and ViT-Large (16). These settings are crucial for understanding the computational cost and performance of the different models.", "section": "4.4 Large Scale Transformer Experiments using FTFI"}, {"figure_path": "Eok6HbcSRI/tables/tables_24_2.jpg", "caption": "Table 1: Performance of Topological Vision Transformers with tree-based masking. For each attention kernel, we present the results of the best variant in bold and Performer baselines in blue.", "description": "This table presents the performance of different Topological Vision Transformer models on ImageNet and Places365 datasets.  The models use tree-based masking, and for each attention kernel, the table shows the accuracy achieved by the best performing model variant. It also includes results for Performer baselines for comparison.", "section": "4.4 Large Scale Transformer Experiments using FTFI"}]