[{"figure_path": "Eok6HbcSRI/figures/figures_2_1.jpg", "caption": "Figure 1: Pictorial representation of the IntegratorTree (see: Sec 3.1) data structure for the nine-vertex input tree T on the left. Numbers in blue next to the input tree denote the weights of its edges. Leaves of the IntegratorTree object represent f-transformed (element-wise) distance matrices: D0, D1, D2, D3 for sub-trees induced by vertex-sets: {1, 2, 4}, {1,3, 0}, {5, 7,8} and {5,6,0} respectively. Different levels correspond to different distances from the pivot point.", "description": "This figure illustrates the IntegratorTree data structure, a key component of the Fast Tree-Field Integrators (FTFIs) algorithm.  The IntegratorTree is a binary tree that recursively decomposes the input tree (T) into smaller subtrees. Each leaf node of the IntegratorTree represents an f-transformed distance matrix for its corresponding subtree, where f is a function applied element-wise to the distance matrix. The non-leaf nodes store information to efficiently navigate and compute the integration across subtrees.  The diagram shows how the tree is decomposed, highlighting pivot points and the resulting distance matrices at the leaf nodes.", "section": "3.1 IntegratorTrees (ITs) - preliminaries"}, {"figure_path": "Eok6HbcSRI/figures/figures_5_1.jpg", "caption": "Figure 2: Pictorial representations of the main concepts behind efficient matrix-vector multiplications Mv with M\u2208 R5\u00d74, for the polynomial f and f(x) = exp(x). In the polynomial case, M is re-written as a sum of low-rank outer-product matrices corresponding to terms of different degrees (e.g., constant, linear, quadratic, etc.). Matrix associativity property is applied for efficient calculations (dotted-border blocks indicating the order of computations). In the second case, M is high-rank, but the so-called low displacement rank operator AD1,D2: X \u2192 D1M \u2013 MD2 for diagonal D1, D2 can be applied to make it a low-rank outer-product matrix. The multiplication with M can be efficiently performed using the theory of LDR matrices [Thomas et al., 2018].", "description": "This figure demonstrates how matrix-vector multiplications can be made efficient using low displacement rank matrices. It shows two examples, one for polynomial functions and another for exponential functions. The polynomial case uses a sum of low-rank outer product matrices, and the exponential case uses a low displacement rank operator to reduce the rank of the matrix.", "section": "3.2.1 The case for structured matrices: multiplications with C, CT and cordiality"}, {"figure_path": "Eok6HbcSRI/figures/figures_6_1.jpg", "caption": "Figure 3: Runtime comparison of FTFI with BTFI as a function of the number of vertices, N. Left: Synthetic graphs. Right: Mesh-graphs from Thingi10K. The speed is not necessarily monotonic in N as it depends on the distribution of lengths of the shortest paths. For each graph, 10 experiments were run (std. shown via dotted lines).", "description": "This figure compares the runtime performance of the proposed Fast Tree-Field Integrators (FTFI) algorithm against a brute-force baseline (BTFI) for tree field integration.  The comparison is shown for two types of graphs: synthetic graphs and mesh graphs from the Thingi10K dataset. The x-axis represents the number of vertices (N) in the graph, and the y-axis represents the runtime in seconds.  The results show that FTFI is significantly faster than BTFI, especially for larger graphs, achieving speedups of up to 13x for mesh graphs and 5.7x for synthetic graphs. Error bars representing standard deviation across 10 runs are included.", "section": "4.1 Runtime Efficiency of FTFI"}, {"figure_path": "Eok6HbcSRI/figures/figures_7_1.jpg", "caption": "Figure 4: Speed (pre-processing time) and accuracy (cosine similarity) comparison of the FTFI and other baselines for vertex normal prediction on meshes. Cosine similarity of BFFI and FTFI almost overlaps. The last two figures are qualitative examples showcasing the tradeoff between cosine similarity and preprocessing time for meshes of sizes 3K and 5K nodes respectively.", "description": "This figure compares the performance of FTFI against other methods for vertex normal prediction on meshes from the Thingi10K dataset.  It shows that FTFI is much faster (pre-processing time) than other methods while maintaining comparable accuracy (cosine similarity). The plots illustrate the trade-off between pre-processing time and accuracy for different mesh sizes (3K and 5K nodes).", "section": "4.2 Approximation Quality of FTFI"}, {"figure_path": "Eok6HbcSRI/figures/figures_7_2.jpg", "caption": "Figure 5: Trade-off plot comparing graph classification accuracy and feature processing time for the classifiers using FTFI and BGFI. FTFI achieves similar accuracy as BGFI while significantly reducing fp time across most datasets. We report the reduction in FTFI's processing time (\u00b1x%) compared to BGFI using a dotted line.", "description": "This figure shows a comparison of the trade-off between graph classification accuracy and feature processing time for FTFI and BGFI across multiple datasets. It demonstrates that FTFI achieves comparable accuracy to BGFI while significantly reducing feature processing time.", "section": "4.2 Approximation Quality of FTFI"}, {"figure_path": "Eok6HbcSRI/figures/figures_8_1.jpg", "caption": "Figure 6: Left: Relative Frobenius norm error as a function of the number of training iterations for different sizes n and learnable quadratic f. Middle: Comparison of the training of different rational functions f with num:d defining the degree of the numerator and den:d, the degree of the denominator for the synthetic graph obtained from a path on N = 800 by adding 600 random edges and assigning random weights taken from (0, 1). Right: constructed similarly, but for a sampled mesh graphs from Thingi10k dataset.", "description": "The figure shows the results of training learnable f-distance matrices using different rational functions with varying degrees of numerator and denominator. The left panel displays the relative Frobenius norm error for different graph sizes, while the middle and right panels compare the training curves for synthetic and mesh graphs, respectively.", "section": "4.3 Improving approximation quality with learnable f-distance matrices"}, {"figure_path": "Eok6HbcSRI/figures/figures_9_1.jpg", "caption": "Figure 7: Left: Experiments with the RPE mechanism for ViT-B and on ImageNet. We observe that FTFI provides 7% accuracy gain compared to the Performer variant.", "description": "This figure shows the training accuracy curves for the ViT Performer model and the ViT Performer model with FTFI on the ImageNet dataset.  The FTFI-augmented model shows a significantly higher accuracy than the baseline Performer model, demonstrating a 7% improvement.", "section": "4.4 Large Scale Transformer Experiments using FTFI"}, {"figure_path": "Eok6HbcSRI/figures/figures_19_1.jpg", "caption": "Figure 8: Relative Frobenius norm error as a function of the number of training iterations for different sizes n and learnable quadratic f. We report the results for 3 mesh graphs from Thingi10k.", "description": "The figure shows the relative Frobenius norm error for different sizes of synthetic graphs (n = 100, 200, 300, 400, 500, 600) and different rational functions (num:d=2, den:d=0, 1, 2).  The training loss is plotted against the number of iterations. Each subplot shows results for a different mesh graph from the Thingi10k dataset.", "section": "4.3 Improving approximation quality with learnable f-distance matrices"}, {"figure_path": "Eok6HbcSRI/figures/figures_20_1.jpg", "caption": "Figure 9: Left: Variation in FTFI performance with different f-distance functions on the CUBES dataset. We use general rational functions (GRF) of varying polynomial degrees. GRF(i) indicates a rational function of the i-th degree. We observe a general trend of accuracy increase with function complexity up to a certain degree. The coefficients of the GRF were learnt using a few graph instances. Right: We show the training loss curves for estimating the coefficients of the rational function, f, for samples in the CUBES dataset. We report the training loss for rational functions with varying polynomial degrees. We observe that the training loss is lower when we use rational functions with high-degree polynomials.", "description": "The left plot shows the accuracy of graph classification on the CUBES dataset for different rational functions (GRF).  The degree of the GRF polynomials is varied from 1 to 4.  The right plot shows the training loss curves for learning the coefficients of those same rational functions; the plot indicates that using higher degree rational functions leads to lower training loss. ", "section": "4.3 Improving approximation quality with learnable f-distance matrices"}, {"figure_path": "Eok6HbcSRI/figures/figures_20_2.jpg", "caption": "Figure 10: Comparison of field integration time between GW and FTFI-GW. We observe that FTFI achieves significant computation time gain over the baseline.", "description": "The figure compares the computation time of Gromov-Wasserstein (GW) distance calculation against the proposed Fast Tree-Field Integrator (FTFI) enhanced GW approach. The x-axis represents the number of vertices in the graph, and the y-axis shows the computation time in seconds.  The results demonstrate that FTFI-GW significantly reduces the computation time compared to the standard GW method, especially as the number of vertices increases.", "section": "4.2 Approximation Quality of FTFI"}]