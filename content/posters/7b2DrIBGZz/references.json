{"references": [{"fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020-00-00", "reason": "This paper is foundational to diffusion models, introducing the core denoising diffusion probabilistic model framework that many subsequent text-to-image models build upon."}, {"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-00-00", "reason": "This paper is highly influential for introducing the latent diffusion model (LDM) architecture, improving efficiency and enabling the generation of higher-resolution images compared to previous diffusion models."}, {"fullname_first_author": "Chitwan Saharia", "paper_title": "Photorealistic text-to-image diffusion models with deep language understanding", "publication_date": "2022-00-00", "reason": "This work significantly advanced the state-of-the-art in text-to-image generation by integrating deep language understanding capabilities into the diffusion model, demonstrating superior image quality and prompt following."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-00-00", "reason": "CLIP, introduced in this paper, is a crucial component in many text-to-image models, providing a strong foundation for aligning image and text representations."}, {"fullname_first_author": "William Peebles", "paper_title": "Scalable diffusion models with transformers", "publication_date": "2023-00-00", "reason": "This paper introduced the Diffusion Transformer (DiT) architecture, which leverages the efficiency and scalability of transformers to improve diffusion model performance, serving as a direct basis for the architecture developed in the current work."}]}