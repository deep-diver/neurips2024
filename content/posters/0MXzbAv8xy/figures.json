[{"figure_path": "0MXzbAv8xy/figures/figures_2_1.jpg", "caption": "Figure 1: Graph tasks (top) and the corresponding computation trees (bottom). A virtual node can be added at the top to connect all task-relevant nodes, unifying different tasks as the tree-level task.", "description": "This figure illustrates how computation trees can represent various graph tasks. The top row shows examples of node-level, link-level, and graph-level tasks.  The bottom row shows the corresponding computation trees derived from the message-passing process of a graph neural network.  A key aspect is the addition of a virtual node to connect task-relevant nodes, making it possible to unify different graph tasks into a single \"tree-level\" task.", "section": "2.2 Computation Tree as Transferable Pattern"}, {"figure_path": "0MXzbAv8xy/figures/figures_3_1.jpg", "caption": "Figure 2: Synthetic graphs composed of two basic blocks. More blocks can scale up the graph sizes.", "description": "This figure shows three synthetic graphs (G1, G2, and G3) constructed from two basic blocks.  The number of blocks is varied to demonstrate how the size of the graphs can be increased.  These graphs are used in the experiments to study the transferability of computation trees, showing how the similarity of tree structures affects the ability of a model to generalize to new tasks.", "section": "2 Rethinking Transferable Patterns on Graphs"}, {"figure_path": "0MXzbAv8xy/figures/figures_4_1.jpg", "caption": "Figure 4: During pre-training, GFT encodes general knowledge from a graph database into a tree vocabulary through tree reconstruction. In fine-tuning, the learned tree vocabulary is applied to unify graph-related tasks as tree classification, adapting the general knowledge to specific tasks.", "description": "This figure illustrates the two-stage training process of the GFT model.  In the pre-training stage (a), a graph database is used to generate computation trees which are encoded into a tree vocabulary via a tree reconstruction task. This involves reconstructing features of the root node, the connectivity among nodes, and the semantics of the trees. An orthogonal regularizer is applied to improve the quality of the tree vocabulary. In the fine-tuning stage (b), the pre-trained tree vocabulary is used to unify graph-related tasks (node, link, and graph-level) into a computation tree classification task. This involves querying the fixed tree vocabulary, classifying computation trees, and using both a prototype classifier and a linear classifier to generate predictions. The process adapts the general knowledge encoded in the tree vocabulary to specific tasks.", "section": "3 GFT: Graph Foundation Model with Transferable Tree Vocabulary"}, {"figure_path": "0MXzbAv8xy/figures/figures_7_1.jpg", "caption": "Figure 5: Negative transfer gap on Cora in node classification.", "description": "The figure shows the negative transfer gap (NT gap) on the Cora dataset for node classification. The NT gap is calculated as R(S,T) - R(0,T), where R(S,T) is the risk on task T with pre-training on task S and R(0,T) is the risk without pre-training.  The plot compares the NT gap for two approaches: one without a tree vocabulary and the other using the authors' proposed method (GFT).  The results illustrate that employing the learned tree vocabulary to align the tree reconstruction task (pre-training) and tree classification task (fine-tuning) significantly reduces negative transfer. The y-axis represents the NT gap and the x-axis represents the number of training epochs.", "section": "4.3 Transferability"}, {"figure_path": "0MXzbAv8xy/figures/figures_9_1.jpg", "caption": "Figure 7: GFT consistently improves model performance with more pre-training datasets.", "description": "This figure shows the impact of using different combinations of pre-training datasets on the performance of GFT across four different graph datasets (Cora, WikiCS, WN18RR, and HIV). Each subplot represents a different dataset and task (node classification, node classification, link prediction, and graph classification, respectively).  The x-axis shows the combination of pre-training datasets used (FB15k237, Arxiv, and Chembl), and the y-axis shows the accuracy achieved by GFT.  The lines represent the trend of accuracy with an increasing number of pre-training datasets used. The shaded area indicates the standard deviation of the accuracy over multiple runs.  This figure demonstrates that GFT consistently achieves better performance as more pre-training datasets are used, illustrating the effectiveness of the model in learning transferable patterns across diverse graphs.", "section": "4.2 Effectiveness on Cross-Domain and Cross-Task Datasets"}, {"figure_path": "0MXzbAv8xy/figures/figures_20_1.jpg", "caption": "Figure 8: The efficiency analysis between computation trees and subgraphs. Our GFT is based on the computation trees and we further replace the computation trees with subgraphs called GFT-Subgraph. We compare their memory usage (a) and time consumption (b) during pretraining. With the increase of batch sizes, Subgraph-based GFT encounters out-of-memory, yet computation tree-based GFT can still fit in the GPU.", "description": "This figure compares the efficiency of using computation trees versus subgraphs for GFT model training.  It shows memory usage and training time per epoch across different batch sizes.  The results demonstrate that the computation tree approach is significantly more memory-efficient and scalable, as subgraph-based methods run out of memory at larger batch sizes.", "section": "C More Analysis"}, {"figure_path": "0MXzbAv8xy/figures/figures_20_2.jpg", "caption": "Figure 8: The efficiency analysis between computation trees and subgraphs. Our GFT is based on the computation trees and we further replace the computation trees with subgraphs called GFT-Subgraph. We compare their memory usage (a) and time consumption (b) during pretraining. With the increase of batch sizes, Subgraph-based GFT encounters out-of-memory, yet computation tree-based GFT can still fit in the GPU.", "description": "This figure compares the efficiency of using computation trees versus subgraphs in the GFT model.  The top graph shows memory usage, and the bottom shows the time taken per epoch, both plotted against increasing batch size (log scale). The results indicate that using computation trees is significantly more memory-efficient and allows for much larger batch sizes before running into out-of-memory errors, compared to using subgraphs.  This highlights the efficiency advantage of GFT's approach.", "section": "C More Analysis"}, {"figure_path": "0MXzbAv8xy/figures/figures_22_1.jpg", "caption": "Figure 4: During pre-training, GFT encodes general knowledge from a graph database into a tree vocabulary through tree reconstruction. In fine-tuning, the learned tree vocabulary is applied to unify graph-related tasks as tree classification, adapting the general knowledge to specific tasks.", "description": "This figure illustrates the two-stage training process of the GFT model.  The pre-training phase uses a tree reconstruction task to learn a general tree vocabulary from a diverse graph database. This vocabulary captures fundamental patterns common to different graph tasks. In the fine-tuning stage, this vocabulary is utilized to unify various graph tasks (node, link, and graph level) as a single tree classification task, thus improving model generalization and reducing negative transfer.", "section": "3 GFT: Graph Foundation Model with Transferable Tree Vocabulary"}, {"figure_path": "0MXzbAv8xy/figures/figures_23_1.jpg", "caption": "Figure 1: Graph tasks (top) and the corresponding computation trees (bottom). A virtual node can be added at the top to connect all task-relevant nodes, unifying different tasks as the tree-level task.", "description": "This figure illustrates how different graph tasks (node, link, and graph classification) can be represented as computation trees.  A computation tree is a specialized subtree pattern derived from unfolding the message-passing process in a graph neural network.  By adding a virtual node at the top of the computation tree, all the task-relevant nodes are connected, unifying the different graph tasks as a single tree-level classification task. This shows the basic idea behind using computation trees as transferable patterns across various graph tasks.", "section": "2.2 Computation Tree as Transferable Pattern"}, {"figure_path": "0MXzbAv8xy/figures/figures_27_1.jpg", "caption": "Figure 3: Transfer performance on synthetic graphs with G1 as the target graph. Higher tree similarity correlates with enhanced transferability.", "description": "This figure shows the relationship between computation tree similarity and transfer learning performance on synthetic graphs. Three distinct graphs (G1, G2, G3) were constructed. G1 and G2 share similar motifs but have different computation tree distributions. G1 and G3 have dissimilar motifs but similar computation tree distributions.  The x-axis represents the number of blocks used to construct the synthetic graphs, representing the scale of the graph. The y-axis on the leftmost subplot shows the transferability, measured using the inverse of the Central Moment Discrepancy (CMD). The y-axis on the middle subplot represents the computation tree similarity (Tree Sim.) measured using the Weisfeiler-Lehman subtree kernel.  The y-axis on the rightmost subplot displays the motif similarity (Motif Sim.)  measured using the graphlet sampling kernel. The results indicate that higher computation tree similarity is strongly correlated with improved transfer learning performance, while motif similarity shows less correlation.", "section": "Supportive Observations \u2014 Synthetic Graphs"}, {"figure_path": "0MXzbAv8xy/figures/figures_27_2.jpg", "caption": "Figure 3: Transfer performance on synthetic graphs with G1 as the target graph. Higher tree similarity correlates with enhanced transferability.", "description": "This figure shows the results of transfer learning experiments on synthetic graphs. Three different graphs (G1, G2, and G3) were used, with G1 being the target graph.  The graphs were designed to have varying levels of computation tree similarity and motif similarity. The x-axis represents the number of blocks in the graph, which is a measure of the graph's size.  The y-axis represents transferability,  which is measured using the inverse of the Central Moment Discrepancy (CMD). The figure demonstrates that higher tree similarity between the source and target graphs correlates strongly with improved transferability, while the impact of motif similarity is less pronounced. This supports the paper's central argument that computation trees are more effective transferable patterns than motifs for graph learning.", "section": "Supportive Observations \u2014 Synthetic Graphs"}]