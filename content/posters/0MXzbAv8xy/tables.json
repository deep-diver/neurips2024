[{"figure_path": "0MXzbAv8xy/tables/tables_3_1.jpg", "caption": "Table 1: Transfer learning performance on homophily (above) and heterophily (below) graphs. For any target graph, source graphs with higher tree similarity lead to improved accuracy, highlighted with Blue. Conversely, the influence of motif similarity is marginal, marked by LightBlue.", "description": "This table presents the results of transfer learning experiments conducted on both homophilic and heterophilic graphs.  It demonstrates a correlation between the computation tree similarity between source and target graphs and the transfer learning performance.  Higher tree similarity between graphs generally leads to better transfer learning accuracy, while the impact of motif similarity is less significant.  The table is divided into two parts, one for homophilic graphs (airport networks) and one for heterophilic graphs (WebKB networks), with each part showing results for various combinations of source and target graphs.", "section": "Supportive Observations \u2014 Real-world Graphs"}, {"figure_path": "0MXzbAv8xy/tables/tables_8_1.jpg", "caption": "Table 2: Model performance in pre-training and fine-tuning setting. Bold and underline highlight the best and sub-best performance, and * and * denote a 2% and 5% improvement over the best baseline. The model performance with standard deviation is in Appendix G.", "description": "This table presents the performance of various models (including the proposed GFT model) on node, link, and graph classification tasks.  The results are split based on the dataset and the task.  The best and second-best performances for each task and dataset are highlighted, indicating the performance gain of the GFT model compared to existing methods.  Detailed results with standard deviations are available in Appendix G.", "section": "4.2 Effectiveness on Cross-Domain and Cross-Task Datasets"}, {"figure_path": "0MXzbAv8xy/tables/tables_8_2.jpg", "caption": "Table 3: Few-shot learning performance. Additional results with more baselines are in Appendix H.", "description": "This table shows the few-shot learning performance of the GFT model compared to other self-supervised methods and graph foundation models.  The results are presented for various datasets and different numbers of shots (training instances per class).  The table highlights GFT's superior performance, especially when training instances are extremely limited.", "section": "4.2 Effectiveness on Cross-Domain and Cross-Task Datasets"}, {"figure_path": "0MXzbAv8xy/tables/tables_9_1.jpg", "caption": "Table 4: Ablation on tree reconstruction (above) and tree classification (bottom).", "description": "This table presents the ablation study results on the tree reconstruction tasks in pre-training and tree classifiers in fine-tuning.  It shows the impact of various reconstruction tasks (reconstructing features of the root node, connectivity among nodes, and overall semantics of computation trees) in pre-training on the model performance across node-level, link-level, and graph-level tasks. It also shows the comparison results with different tree classifiers (prototype classifier and linear classifier).", "section": "4.4 Ablation Study"}, {"figure_path": "0MXzbAv8xy/tables/tables_9_2.jpg", "caption": "Table 2: Model performance in pre-training and fine-tuning setting. Bold and underline highlight the best and sub-best performance, and * and * denote a 2% and 5% improvement over the best baseline. The model performance with standard deviation is in Appendix G.", "description": "This table presents the performance of various models in node, link, and graph classification tasks, comparing the GFT model with baselines such as GCN, GAT, GIN, DGI, BGRL, GraphMAE, and GIANT.  The results are shown for pre-training and fine-tuning settings, indicating the average performance across multiple datasets with standard deviations detailed in Appendix G.  The best and second-best performances are highlighted, with additional notes on performances that exceed baselines by 2% and 5%.", "section": "4.2 Effectiveness on Cross-Domain and Cross-Task Datasets"}, {"figure_path": "0MXzbAv8xy/tables/tables_9_3.jpg", "caption": "Table 6: The impact of tree vocabulary.", "description": "This table presents the results of an ablation study evaluating the effect of using different sizes of tree vocabulary and a setting without a tree vocabulary in the GFT model.  It compares the average performance across node, link, and graph classification tasks for various vocabulary sizes, showing the impact of the tree vocabulary on model performance. The results demonstrate that using the tree vocabulary enhances model generalization and improves performance across various tasks.", "section": "4.3 Transferability"}, {"figure_path": "0MXzbAv8xy/tables/tables_20_1.jpg", "caption": "Table 7: The comparison between computation trees and subgraphs.", "description": "This table compares the performance of using computation trees versus subgraphs as transferable patterns in the GFT model.  It shows accuracy results for node, link, and graph classification tasks across four models: GAT, GraphMAE, GFT with subgraphs (GFT - Subgraph), and GFT with computation trees (GFT - Tree).  The results highlight the superior performance of using computation trees over subgraphs for various graph tasks. ", "section": "C More Analysis"}, {"figure_path": "0MXzbAv8xy/tables/tables_22_1.jpg", "caption": "Table 8: Comparison of number of parameters across different models", "description": "This table compares the number of parameters across various graph foundation models, including Prodigy, OFA, UniGraph, and the proposed GFT model.  It highlights the relative model complexity and efficiency, demonstrating that GFT achieves comparable performance with significantly fewer parameters than some of the other models.", "section": "C.7 Comparison to Subgraph-based GFMs"}, {"figure_path": "0MXzbAv8xy/tables/tables_28_1.jpg", "caption": "Table 1: Transfer learning performance on homophily (above) and heterophily (below) graphs. For any target graph, source graphs with higher tree similarity lead to improved accuracy, highlighted with Blue. Conversely, the influence of motif similarity is marginal, marked by LightBlue.", "description": "This table presents the results of transfer learning experiments conducted on both homophily and heterophily graph datasets.  It shows the relationship between computation tree similarity (a measure of how similar the computation trees of two graphs are), motif similarity (a measure of how similar the graph motifs of two graphs are), and transfer learning performance (accuracy).  The table highlights that higher computation tree similarity correlates with better transfer learning accuracy, while motif similarity has a less significant effect.", "section": "Supportive Observations \u2014 Real-world Graphs"}, {"figure_path": "0MXzbAv8xy/tables/tables_29_1.jpg", "caption": "Table 10: Dataset statistics [45].", "description": "This table presents the statistics of nine graph datasets used in the experiments, including the domain, task type, number of graphs, average number of nodes and edges, and number of classes.  The datasets represent various types of graphs, including citation networks, web link networks, knowledge graphs, and molecular graphs, covering node-level, link-level, and graph-level tasks.", "section": "4.1 Experimental Setting"}, {"figure_path": "0MXzbAv8xy/tables/tables_30_1.jpg", "caption": "Table 11: Hyper-parameters in fine-tuning.", "description": "This table lists the hyperparameters used for fine-tuning the GFT model across different datasets.  It includes learning rate, number of epochs, early stopping criteria, batch size, number of instances per class used for creating prototypes, temperature parameters for both prototype and linear classifiers, and weights for the loss functions of these classifiers.", "section": "3.2 Fine-tuning with Computation Tree Classification"}, {"figure_path": "0MXzbAv8xy/tables/tables_30_2.jpg", "caption": "Table 2: Model performance in pre-training and fine-tuning setting. Bold and underline highlight the best and sub-best performance, and * and * denote a 2% and 5% improvement over the best baseline. The model performance with standard deviation is in Appendix G.", "description": "This table shows the performance comparison of various models on different tasks (node, link, and graph classification) and datasets. The models are compared under two training schemes: pre-training and fine-tuning. The best performing models are highlighted and improvement over the best baseline is also specified.", "section": "4.2 Effectiveness on Cross-Domain and Cross-Task Datasets"}, {"figure_path": "0MXzbAv8xy/tables/tables_31_1.jpg", "caption": "Table 3: Few-shot learning performance. Additional results with more baselines are in Appendix H.", "description": "This table presents the few-shot learning performance of the GFT model compared to several baselines, including self-supervised methods and other graph foundation models.  It shows the performance across different datasets and different numbers of shots (training samples per class) for node and link classification tasks. The results demonstrate the effectiveness of GFT in few-shot learning scenarios.", "section": "4.2 Effectiveness on Cross-Domain and Cross-Task Datasets"}, {"figure_path": "0MXzbAv8xy/tables/tables_31_2.jpg", "caption": "Table 2: Model performance in pre-training and fine-tuning setting. Bold and underline highlight the best and sub-best performance, and * and * denote a 2% and 5% improvement over the best baseline. The model performance with standard deviation is in Appendix G.", "description": "This table presents the average performance of different models across various graph datasets and tasks in both pre-training and fine-tuning settings.  The models include supervised and self-supervised GNNs, as well as graph foundation models.  The table highlights the superior performance of the GFT model, with significant improvements over the best baseline in most cases, indicating its ability to transfer learning effectively across different tasks and domains.", "section": "4.2 Effectiveness on Cross-Domain and Cross-Task Datasets"}, {"figure_path": "0MXzbAv8xy/tables/tables_31_3.jpg", "caption": "Table 3: Few-shot learning performance. Additional results with more baselines are in Appendix H.", "description": "This table presents the few-shot learning performance of the GFT model compared to several baselines.  Few-shot learning is a technique where the model is trained with a limited number of labeled examples per class. The table shows the performance across different tasks and datasets, and for various numbers of training examples per class (shots). The results demonstrate that GFT significantly outperforms the baselines even with limited training data, highlighting its effectiveness in few-shot learning scenarios.", "section": "4.2 Effectiveness on Cross-Domain and Cross-Task Datasets"}, {"figure_path": "0MXzbAv8xy/tables/tables_31_4.jpg", "caption": "Table 3: Few-shot learning performance. Additional results with more baselines are in Appendix H.", "description": "This table presents the few-shot learning performance of the GFT model compared to several baselines (BGRL, GraphMAE, GIANT, Prodigy, OFA) across different datasets (Arxiv, FB15K237, Cora, HIV, PCBA) and various shot settings (1-shot, 3-shot, 5-shot) with different numbers of training instances per class (5, 10, 20, 30).  The results showcase GFT's superior performance in few-shot learning scenarios, emphasizing its ability to rapidly adapt to new tasks with limited labeled data.", "section": "4.2 Effectiveness on Cross-Domain and Cross-Task Datasets"}, {"figure_path": "0MXzbAv8xy/tables/tables_32_1.jpg", "caption": "Table 3: Few-shot learning performance. Additional results with more baselines are in Appendix H.", "description": "This table presents the results of few-shot learning experiments, comparing the performance of GFT against various baselines across different datasets and settings.  The results demonstrate the effectiveness of GFT, particularly in low-shot learning scenarios where training data is limited. The table showcases performance for different numbers of shots (e.g., 1-shot, 3-shot, 5-shot) and across various datasets (e.g., Arxiv, FB15K237, Cora, HIV, PCBA).  Appendix H provides more detailed results with additional baselines.", "section": "4.2 Effectiveness on Cross-Domain and Cross-Task Datasets"}, {"figure_path": "0MXzbAv8xy/tables/tables_32_2.jpg", "caption": "Table 3: Few-shot learning performance. Additional results with more baselines are in Appendix H.", "description": "This table presents the few-shot learning performance of the GFT model compared to other self-supervised methods and graph foundation models.  The results show accuracy across various datasets (Arxiv, FB15K237, Cora, and HIV) with different numbers of shots (1, 3, 5, 10) and training instances per class (indicated by # train).  It demonstrates GFT's effectiveness with limited labeled data during the fine-tuning phase.  Appendix H contains additional results.", "section": "4.2 Effectiveness on Cross-Domain and Cross-Task Datasets"}, {"figure_path": "0MXzbAv8xy/tables/tables_32_3.jpg", "caption": "Table 3: Few-shot learning performance. Additional results with more baselines are in Appendix H.", "description": "This table presents the few-shot learning performance of the proposed model (GFT) and other existing models.  Few-shot learning is a setting where models are trained on a limited number of examples for each class. The table shows the performance of different models across several datasets and varying numbers of training examples per class (shot).  The results highlight the effectiveness of GFT in low-data scenarios.", "section": "4.2 Effectiveness on Cross-Domain and Cross-Task Datasets"}, {"figure_path": "0MXzbAv8xy/tables/tables_32_4.jpg", "caption": "Table 2: Model performance in pre-training and fine-tuning setting. Bold and underline highlight the best and sub-best performance, and * and * denote a 2% and 5% improvement over the best baseline. The model performance with standard deviation is in Appendix G.", "description": "This table presents the performance of various models on node classification, link classification, and graph classification tasks, comparing the performance of GFT with various baseline models. The results are presented as the average accuracy across multiple runs, with standard deviations reported in Appendix G. Bold and underlined values indicate the best and second-best performances, respectively, and asterisks indicate performance improvements (2% or 5%) over the best baseline. The table highlights the effectiveness of GFT's cross-domain and cross-task transferability.", "section": "4.2 Effectiveness on Cross-Domain and Cross-Task Datasets"}, {"figure_path": "0MXzbAv8xy/tables/tables_33_1.jpg", "caption": "Table 2: Model performance in pre-training and fine-tuning setting. Bold and underline highlight the best and sub-best performance, and * and * denote a 2% and 5% improvement over the best baseline. The model performance with standard deviation is in Appendix G.", "description": "This table shows the performance of different models (including the proposed GFT model) on various node, link, and graph classification tasks.  The results are presented for several benchmark datasets, showing the average performance across multiple runs to account for randomness. The table indicates significant performance gains by GFT compared to other baselines (especially on link and graph classification tasks) highlighting the efficacy of the proposed transferable tree vocabulary.", "section": "4.2 Effectiveness on Cross-Domain and Cross-Task Datasets"}, {"figure_path": "0MXzbAv8xy/tables/tables_33_2.jpg", "caption": "Table 2: Model performance in pre-training and fine-tuning setting. Bold and underline highlight the best and sub-best performance, and * and * denote a 2% and 5% improvement over the best baseline. The model performance with standard deviation is in Appendix G.", "description": "This table presents the performance of different models across various graph datasets and tasks, including node classification, link classification, and graph classification.  The results are shown for both pre-training and fine-tuning settings. GFT demonstrates consistently better performance than the other models across a range of datasets, significantly outperforming the best baseline in several cases. The table highlights the effectiveness of the GFT model in leveraging computation trees as transferable patterns. Standard deviations for all results can be found in Appendix G.", "section": "4.2 Effectiveness on Cross-Domain and Cross-Task Datasets"}, {"figure_path": "0MXzbAv8xy/tables/tables_33_3.jpg", "caption": "Table 2: Model performance in pre-training and fine-tuning setting. Bold and underline highlight the best and sub-best performance, and * and * denote a 2% and 5% improvement over the best baseline. The model performance with standard deviation is in Appendix G.", "description": "This table presents the performance of different models (including the proposed GFT model) on various graph-related tasks (Node, Link, and Graph classification). The results are obtained using both pre-training and fine-tuning procedures. The best and second-best performing models are highlighted.  The table also indicates improvements of the GFT model compared to the best baselines.  More detailed results, including standard deviations, can be found in Appendix G.", "section": "4.2 Effectiveness on Cross-Domain and Cross-Task Datasets"}, {"figure_path": "0MXzbAv8xy/tables/tables_33_4.jpg", "caption": "Table 2: Model performance in pre-training and fine-tuning setting. Bold and underline highlight the best and sub-best performance, and * and * denote a 2% and 5% improvement over the best baseline. The model performance with standard deviation is in Appendix G.", "description": "This table presents the average performance of different models across various graph datasets for node, link, and graph classification tasks, comparing supervised and self-supervised models with the proposed GFT model. It highlights the superior performance of GFT compared to other state-of-the-art models. The results include standard deviations (available in Appendix G) providing a more complete understanding of the model's performance.", "section": "4.2 Effectiveness on Cross-Domain and Cross-Task Datasets"}]