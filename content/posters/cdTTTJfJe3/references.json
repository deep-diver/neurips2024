{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational to the field of large language models (LLMs) and their capabilities, directly relevant to the topic of AI-generated text detection."}, {"fullname_first_author": "Jacob Devlin", "paper_title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2018-10-11", "reason": "BERT is a highly influential transformer-based language model that sets the foundation for many approaches to natural language processing tasks, including AI-generated text detection."}, {"fullname_first_author": "Colin Raffel", "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "publication_date": "2020-07-01", "reason": "This paper introduces a unified text-to-text transformer architecture (T5) that's widely used in various NLP tasks, and its relevance to AI-generated text detection is evident in the paper's citations."}, {"fullname_first_author": "Tianyu Gao", "paper_title": "SimCSE: Simple contrastive learning of sentence embeddings", "publication_date": "2021-11-01", "reason": "SimCSE is a significant contribution to sentence embedding techniques using contrastive learning, a method directly applied and extended in this paper for AI-generated text detection."}, {"fullname_first_author": "Yafu Li", "paper_title": "Deepfake text detection in the wild", "publication_date": "2023-05-11", "reason": "This paper directly addresses the task of AI-generated text detection, providing a benchmark dataset (Deepfake) that is used and evaluated in this paper, making it highly relevant."}]}