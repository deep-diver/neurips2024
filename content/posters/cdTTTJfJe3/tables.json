[{"figure_path": "cdTTTJfJe3/tables/tables_7_1.jpg", "caption": "Table 1: Experimental results on M4-monolingual [68], M4-multilingual [68], TuringBench [61] and Deepfake's Cross-domains & Cross-models subset [39]. The best number is highlighted in bold, while the second best one is underlined.", "description": "This table presents a comparison of the performance of DeTeCtive against other state-of-the-art methods for AI-generated text detection.  The results are reported across four datasets: M4-monolingual, M4-multilingual, TuringBench, and Deepfake (Cross-domains & Cross-models subset). The metrics used for evaluation are Average Recall (AvgRec) and F1-score.  The best performing model for each dataset and metric is highlighted in bold, with the second-best underlined. This allows for easy comparison and shows the superior performance of DeTeCtive.", "section": "4.2 Main Results"}, {"figure_path": "cdTTTJfJe3/tables/tables_7_2.jpg", "caption": "Table 2: Experimental results of AvgRec on six scenarios proposed in Deepfake [39] dataset. In Out-of-distribution detection, our method produces two results. The left one is the regular testing result while the right one indicates the result combining with TFIA. The best number is highlighted in bold, while the second best one is underlined. For detailed results, please refer to Table 12.", "description": "This table presents the average recall (AvgRec) scores achieved by DeTeCtive and other baseline methods across six different scenarios in the Deepfake dataset.  The scenarios are categorized as in-distribution and out-of-distribution detection. In-distribution detection scenarios evaluate model performance on data from seen domains and models, while out-of-distribution scenarios assess generalization on unseen domains and models. For out-of-distribution detection, the table shows two AvgRec values for DeTeCtive: one from standard testing and another incorporating the Training-Free Incremental Adaptation (TFIA) technique, which enhances performance by incorporating unseen data without retraining.  The best performing method is highlighted in bold for each scenario.", "section": "4.2 Main Results"}, {"figure_path": "cdTTTJfJe3/tables/tables_8_1.jpg", "caption": "Table 3: Experimental results on attack robustness on OUTFOX [33] dataset, including DIPPER [35] attack and OUTFOX attack methods. The best number is highlighted in bold.", "description": "This table presents the results of the attack robustness experiments conducted on the OUTFOX dataset. The experiments involved three different attack methods: Non-attacked, DIPPER, and OUTFOX. The detectors used were: ROBERTa-base, ROBERTa-large, HC3 Detector, OUTFOX, and DeTeCtive. For each attack method and detector, the average recall (AvgRec) and F1-score are reported.  The best performance for each attack scenario is highlighted in bold.", "section": "4.3 More Applications"}, {"figure_path": "cdTTTJfJe3/tables/tables_8_2.jpg", "caption": "Table 4: Experimental results of authorship attribution detection on TuringBench [61] dataset. The best number is highlighted in bold.", "description": "This table presents the performance of various authorship attribution detection methods on the TuringBench dataset.  The methods are compared based on precision, accuracy, recall, and F1-score.  DeTeCtive achieves the highest scores across all metrics, showcasing its effectiveness in authorship attribution.", "section": "4.3 More Applications"}, {"figure_path": "cdTTTJfJe3/tables/tables_8_3.jpg", "caption": "Table 5: Ablation studies on loss design and classification approach, all conducted on Deepfake's Cross-domains & Cross-models subset [39].", "description": "This table presents the results of ablation studies performed on the DeTeCtive model using the Deepfake dataset's Cross-domains & Cross-models subset.  The goal was to evaluate the contribution of different components of the model, specifically the multi-level contrastive loss and the choice of classification approach.  The table shows the performance metrics (HumanRec, MachineRec, AvgRec*, F1*) achieved under various configurations, including removing parts of the loss function or switching classification methods, highlighting the importance of each component for optimal model performance.", "section": "4.4 Ablation studies and Analysis"}, {"figure_path": "cdTTTJfJe3/tables/tables_16_1.jpg", "caption": "Table 6: Experimental results of applying our method to multiple text encoders on Cross-domains & Cross-models subset of the Deepfake [39] dataset. The best number is highlighted in bold, while the second best one is underlined.", "description": "This table presents the results of fine-tuning multiple pre-trained text encoders using the proposed method on the Cross-domains & Cross-models subset of the Deepfake dataset.  It compares the baseline performance (zero-shot results) of various encoders with their performance after fine-tuning with the proposed method.  The table shows the average recall (AvgRec) and F1-score for each encoder before and after fine-tuning, highlighting the improvement achieved by the proposed method. The number of parameters for each encoder is also provided.", "section": "4.2 Main Results"}, {"figure_path": "cdTTTJfJe3/tables/tables_18_1.jpg", "caption": "Table 7: Models included in Deepfake [39].", "description": "This table lists the different large language models (LLMs) used in the Deepfake dataset, categorized by their model set (OpenAI GPT, Meta LLaMA, Facebook OPT, GLM-130B, Google FLAN-T5, BigScience, and EleutherAI).  Each model set contains several specific models. This information is crucial for understanding the diversity of AI-generated text included in the dataset and how the models were used to evaluate the performance of the proposed AI-generated text detection method.", "section": "4.1 Experimental Setup"}, {"figure_path": "cdTTTJfJe3/tables/tables_18_2.jpg", "caption": "Table 8: The specific origins and splits of Deepfake [39].", "description": "This table details the composition of the Deepfake dataset, breaking down the number of samples used for training, validation, and testing. It also specifies the sources of the data used (CMV, Yelp, XSum, TLDR, ELI5, WP, ROC, HellaSwag, SQUAD, and SciGen) for each split. The data is further categorized into different subsets for various experimental scenarios within the Deepfake dataset.", "section": "4.1 Experimental Setup"}, {"figure_path": "cdTTTJfJe3/tables/tables_18_3.jpg", "caption": "Table 9: Data statistics of M4 Monolingual setting over Train/Dev/Test splits.", "description": "This table shows the data statistics for the M4 monolingual dataset, broken down by source (Wikipedia, Wikihow, Reddit, arXiv, PeerRead), split (Train, Dev, Test), and generator (DaVinci-003, ChatGPT, Cohere, Dolly-v2, BLOOMz, GPT-4, Machine, Human).  It provides the number of samples for each category in the dataset.", "section": "4.1 Experimental Setup"}, {"figure_path": "cdTTTJfJe3/tables/tables_19_1.jpg", "caption": "Table 10: Data statistics of M4 Multilingual setting over Train/Dev/Test splits.", "description": "This table shows the data statistics for the M4 Multilingual dataset, broken down by split (Train, Dev, Test), language, and model/generator.  It details the number of samples for each language and model in each dataset split.  This information is crucial for understanding the dataset's composition and the experimental setup of the paper.", "section": "4.1 Experimental Setup"}, {"figure_path": "cdTTTJfJe3/tables/tables_19_2.jpg", "caption": "Table 11: The number of data samples generated by each generator in TuringBench [61].", "description": "This table shows the number of data samples generated by various text generators in the TuringBench dataset.  The dataset includes human-written text and text generated by a range of large language models (LLMs), and this table provides a breakdown of the sample counts for each generator.", "section": "4.1 Experimental Setup"}, {"figure_path": "cdTTTJfJe3/tables/tables_20_1.jpg", "caption": "Table 12: The detailed results on six scenarios of Deepfake [39] dataset. The best number is highlighted in bold, while the second best one is underlined. In the table, the value of N/A indicates that we are unable to infer specific results based on the data from the Deepfake paper [39]. The notation \"w/C&C database\" represents the results combined with TFIA.", "description": "This table presents the detailed experimental results of the proposed DeTeCtive method and several baselines across six different scenarios of the Deepfake dataset [39].  The scenarios are categorized into in-distribution and out-of-distribution detection, each with three sub-scenarios focusing on the combinations of specific/cross domains and models. The table reports the performance of each method using AvgRec and F1-score metrics.  AvgRec considers human recall and machine recall. Results combining with Training-Free Incremental Adaptation (TFIA) are also included.", "section": "4.2 Main Results"}, {"figure_path": "cdTTTJfJe3/tables/tables_21_1.jpg", "caption": "Table 1: Experimental results on M4-monolingual [68], M4-multilingual [68], TuringBench [61] and Deepfake's Cross-domains & Cross-models subset [39]. The best number is highlighted in bold, while the second best one is underlined.", "description": "This table presents a comparison of the performance of DeTeCtive against other state-of-the-art methods for AI-generated text detection.  The evaluation is performed on four different datasets: M4-monolingual, M4-multilingual, TuringBench, and the Cross-domains & Cross-models subset of Deepfake. The metrics used for comparison are Average Recall (AvgRec) and F1-score. The best performing method for each dataset is highlighted in bold, and the second-best is underlined.", "section": "4.2 Main Results"}]