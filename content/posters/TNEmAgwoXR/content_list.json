[{"type": "text", "text": "Confident Natural Policy Gradient for Local Planning in $q_{\\pi}$ -realizable Constrained MDPs ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Tian Tian University of Alberta, Edmonton ttian@ualberta.ca ", "page_idx": 0}, {"type": "text", "text": "Lin F. Yang \u2217 University of California, Los Angeles linyang@ee.ucla.edu ", "page_idx": 0}, {"type": "text", "text": "Csaba Szepesv\u00e1ri \u2217 University of Alberta, Google DeepMind, Edmonton szepesva@ualberta.ca ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The constrained Markov decision process (CMDP) framework emerges as an important reinforcement learning approach for imposing safety or other critical objectives while maximizing cumulative reward. However, the current understanding of how to learn efficiently in a CMDP environment with a potentially infinite number of states remains under investigation, particularly when function approximation is applied to the value functions. In this paper, we address the learning problem given linear function approximation with $q_{\\pi}$ -realizability, where the value functions of all policies are linearly representable with a known feature map, a setting known to be more general and challenging than other linear settings. Utilizing a localaccess model, we propose a novel primal-dual algorithm that, after $\\tilde{O}(\\mathrm{poly}(d)\\epsilon^{-3})^{1}$ queries, outputs with high probability a policy that strictly satisfies the constraints while nearly optimizing the value with respect to a reward function. Here, $d$ is the feature dimension and $\\epsilon>0$ is a given error. The algorithm relies on a carefully crafted off-policy evaluation procedure to evaluate the policy using historical data, which informs policy updates through policy gradients and conserves samples. To our knowledge, this is the first result achieving polynomial sample complexity for CMDP in the $q_{\\pi}$ -realizable setting. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In the classical reinforcement learning (RL) framework, optimizing a single objective above all else can be challenging for safety-critical applications like autonomous driving, robotics, and Large Language Models (LLMs). For example, it may be difficult for an LLM agent to optimize a single reward that fulfills the objective of generating helpful responses while ensuring that the messages are harmless (Dai et al., 2024). In autonomous driving, designing a single reward often requires reliance on complex parameters and hard-coded knowledge, making the agent less efficient and adaptive (Kamran et al., 2022). Optimizing a single objective in motion planning involves combining heterogeneous quantities like path length and risks, which depend on conversion factors that are not necessarily straightforward to determine (Feyzabadi and Carpin, 2014). ", "page_idx": 0}, {"type": "text", "text": "The constrained Markov decision process (CMDP) framework (Altman, 2021) emerges as an important RL approach for imposing safety or other critical objectives while maximizing cumulative reward (Wachi and Sui, 2020; Dai et al., 2024; Kamran et al., 2022; Wen et al., 2020; Girard and Reza Emami, 2015; Feyzabadi and Carpin, 2014). ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In addition to the single reward function optimized under a standard Markov decision process (MDP), CMDP considers multiple reward functions, with one designated as the primary reward function. The goal of a CMDP is to find a policy that maximizes the primary reward function while satisfying constraints defined by the other reward functions. Although the results of this paper can be applied to multiple constraint functions, for simplicity of presentation, we consider the CMDP problem with only one constraint function. ", "page_idx": 1}, {"type": "text", "text": "Our current understanding of how to learn efficiently in a CMDP environment with a potentially infinite number of states remains limited, particularly when function approximation is applied to the value functions. Most works studying the sample efficiency of a learner have focused on the tabular or simple linear CMDP setting (see related works for more details). However, there has been little work in the more general settings such as the $q_{\\pi}$ -realizability, which assumes the value function of all policies can be approximated by a linear combination of a feature map with unknown parameters. Unlike Linear MDPs (Yang and Wang, 2019; Jin et al., 2020), where the transition model is assumed to be linearly representable by a feature map, $q_{\\pi}$ -realizability only imposes the assumption on the existence of a feature map to represent value functions of policies. ", "page_idx": 1}, {"type": "text", "text": "Nevertheless, the generality of $q_{\\pi}$ -realizability comes with a price, as it becomes considerably more challenging to design effective learning algorithms, even for the unconstrained settings. For the general online setting, we are only aware of one sample-efficient MDP learning algorithm (Weisz et al., 2023), which, however, is computationally inefficient. To tackle this issue, a line of research (Kearns et al., 2002; Yin et al., 2022; Hao et al., 2022; Weisz et al., 2022) applies the local-access model, where the RL algorithm can restart the environment from any visited states - a setting that is also practically motivated, especially when a simulator is provided. The local-access model is more general than the generative model (Kakade, 2003; Sidford et al., 2018; Yang and Wang, 2019; Lattimore et al., 2020; Vaswani et al., 2022), which allows visitation to arbitrary states in an MDP. The local-access model provides the ability to unlock both the sample and computational efficiency of learning with $q_{\\pi}$ -realizability for the unconstrained MDP settings. However, it remains unclear whether we can harness the power of local-access for CMDP learning. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we present a systematic study of CMDP for large state spaces, given $q_{\\pi}$ -realizable function approximation in the local-access model. We summarize our contributions as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We design novel, computationally efficient primal-dual algorithms to learn CMDP nearoptimal policies with the local-access model and $q_{\\pi}$ -realizable function classes. The algorithms can return policies with small constraint violations or even no constraint violations and can handle model misspecification.   \n\u2022 We provide theoretical guarantees for the algorithms, showing that they can compute an $\\epsilon$ -optimal policy with high probability, making no more than $\\tilde{O}(\\mathrm{poly}(d)\\dot{\\epsilon}^{-3})$ queries to the local-access model. The returned policies can strictly satisfy the constraint.   \n\u2022 Under the misspecification setting with a misspecification error $\\omega$ , we show that our algorithms achieve an $\\tilde{O}(\\omega)+\\epsilon$ sub-optimality with high probability, maintaining the same sample efficiency of $\\tilde{O}(\\mathrm{poly}(d)\\epsilon^{-3})$ . ", "page_idx": 1}, {"type": "text", "text": "2 Related works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Most provably efficient algorithms developed for CMDP are in the tabular and linear MDP settings. In the tabular setting, most notably are the works by (Efroni et al., 2020; Liu et al., 2021; Zheng and Ratliff, 2020; Vaswani et al., 2022; Kalagarla et al., 2021; Yu et al., 2021; Gattami et al., 2021; HasanzadeZonuzy et al., 2021; Chen et al., 2021; Kitamura et al., 2024). Work by Vaswani et al. (2022) have showed their algorithm uses no more than $\\begin{array}{r}{\\tilde{O}\\left(\\frac{S A}{(1-\\gamma)^{3}\\epsilon^{2}}\\right)}\\end{array}$ samples to achieve relaxed feasibility and $\\begin{array}{r}{\\tilde{O}\\left(\\frac{S A}{(1-\\gamma)^{5}\\zeta^{2}\\epsilon^{2}}\\right)}\\end{array}$ samples to achieve strict feasibility. Here, the $\\gamma\\,\\in\\,[0,1)$ is the discount factor and $\\zeta\\in(0,\\frac{1}{1-\\gamma}]$ is the Slater\u2019s constant, which characterizes the size of the feasible region and hence the hardness of the CMDP. In their work, they have also provided a lower bound of $\\begin{array}{r}{\\Omega\\left(\\frac{S A}{(1-\\gamma)^{5}\\zeta^{2}\\epsilon^{2}}\\right)}\\end{array}$ on the sample complexity under strict feasibility. However, all the aforementioned results all scale polynomially with the cardinality of the state space. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "For problems with large or possibly infinite state spaces, works by (Jain et al., 2022; Ding et al., 2021; Miryoosefi and Jin, 2022; Ghosh et al., 2024; Liu et al., 2022) have used linear function approximations to address the curse of dimensionality. All these works, except Jain et al. (2022); Liu et al. (2022), make the linear MDP assumption, where the transition function is linearly representable. ", "page_idx": 2}, {"type": "text", "text": "Under the generative model, for the infinite \u221ahoriz\u221aon discounted\u221a case\u221a, the online algorithm proposed in Jain et al. (2022) achieves a regret of ${\\tilde{O}}({\\sqrt{d}}/{\\sqrt{K}})$ with ${\\tilde{O}}({\\sqrt{d}}/{\\sqrt{K}})$ constraint violation, where $K$ is the number of iterations. Work by Liu et al. (2022) is able to achieve a faster $O(\\ln(K)/K)$ convergence rate for both the reward suboptimality and constraint violation. For the online access setting under linear M\u221aDP assumption, Ding et al. (2\u221a021); Ghosh et al. (2024) achieve a regret of $\\tilde{O}\\bar{(}p o l y(d)p o l y(H)\\sqrt{T})$ with $\\begin{array}{r}{\\bar{O}(p o l y(d)\\bar{p}o l y(H)\\sqrt{T});}\\end{array}$ ) violations, where $T$ is the number of episodes and $H$ is the horizon term. ", "page_idx": 2}, {"type": "text", "text": "Miryoosef iand Jin (2022) presented an algorithm that achieves a sample complexity of $\\tilde{O}\\left(\\frac{d^{3}H^{6}}{\\epsilon^{2}}\\right)$ , where $d$ is the dimension of the feature space and $H$ is the horizon term in the finite horizon CMDP setting. In the more general setting under $q_{\\pi}$ -realizability, the best-known upper bounds are in the unconstrained MDP setting. ", "page_idx": 2}, {"type": "text", "text": "In the unconstrained MDP setting with access to a local-access model, early work by Kearns et al. (2002) have developed a tree-search style algorithms under this model, albeit in the tabular setting. Under $v^{*}$ -realizability, Weisz et al. (2021) presented a planner that returns an $\\epsilon$ -optimal policy using $O((d H/\\epsilon)^{|A|})$ queries to the simulator. More works by (Yin et al., 2022; Hao et al., 2022; Weisz et al., 2022) have considered the local-access model with $q_{\\pi}$ -realizability assumption. Recent work by Weisz et al. (2022) have shown their algorithm can return a near-optimal policy that achieves a sample complexity of O\u02dc (1\u2212\u03b3d)4\u03f52 . ", "page_idx": 2}, {"type": "text", "text": "3 Problem formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Constrained MDP ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We consider an infinite-horizon discounted CMDP $(S,\\mathcal{A},P,r,c,\\gamma,b,s_{0})$ consisting a possibly infinite state space $\\boldsymbol{S}$ with a finite set of actions $\\boldsymbol{\\mathcal{A}}$ , a reward function $r:S\\times A\\to[0,1].$ , a constraint function $c:S\\times A\\to[0,1]$ , a discount factor $\\gamma\\in[0,1)$ , a constraint threshold $b\\geq0$ , and a fixed initial state $s_{0}$ . Let $\\mathcal{M}_{1}(X)$ denote the space of probability distributions supported on the set $X$ . Then, the transition probability $P:S\\times\\bar{A}\\to\\bar{M_{1}^{\\prime}}(S)$ . ", "page_idx": 2}, {"type": "text", "text": "Define a set of stationary randomized policies $\\Pi_{\\mathrm{rand}}$ , and a policy $\\pi\\,\\in\\,\\Pi_{\\mathrm{rand}}$ maps states to probability distributions over the actions (i.e., $\\pi\\,:\\,{\\mathcal{S}}\\,\\rightarrow\\,{\\mathcal{M}}_{1}({\\mathcal{A}}))$ . Given a $\\pi~\\in~\\Pi_{\\mathrm{rand}}$ , the policy $\\pi$ interacts with the CMDP starting from any state $s\\ \\in\\ S$ through discrete steps indexed by $t\\in\\ensuremath{\\mathbb{N}}_{0}$ , where $\\mathbb{N}_{0}=\\{0,1,2,\\dots\\}$ . This interaction generates a trajectory of $\\{S_{t},A_{t}\\}_{t\\in\\mathbb{N}_{0}}$ , where $S_{0}\\,=\\,s,A_{t}\\,\\sim\\,\\pi(\\cdot|S_{t})$ , and $S_{t+1}\\'\\sim P(\\cdot|S_{t},A_{t})$ . The reward action-value function is defined as $\\begin{array}{r}{q_{\\pi}^{r}(s,a)\\,=\\,\\mathbb{E}\\,[\\sum_{t=0}^{\\infty}\\gamma^{t}r(S_{t},A_{t})|S_{0}=s,A_{0}=a],}\\end{array}$ . Similarly, the constraint action-value function is defined as $\\begin{array}{r}{q_{\\pi}^{c}(s,a)\\,=\\,\\mathbb{E}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}c(S_{t},A_{t})\\vert S_{0}=s,A_{0}=a\\right]}\\end{array}$ . The reward state-value function $v_{\\pi}^{r}(s)\\,=\\,\\langle\\pi(\\cdot|s),q_{\\pi}^{r}(s,\\cdot)\\rangle$ , where $\\langle\\cdot,\\cdot\\rangle$ denotes the inner product over actions. Likewise, the constraint state-value function $v_{\\pi}^{c}(s)=\\langle\\bar{\\pi}(\\cdot|s),q_{\\pi}^{c}(s,\\cdot)\\rangle$ . ", "page_idx": 2}, {"type": "text", "text": "The objective of the CMDP is to find a policy $\\pi$ that maximizes the state-value function $v_{\\pi}^{r}$ starting from a given state $s_{0}$ , while ensuring that the constraint $v_{\\pi}^{c}(s_{0})\\geq b$ is satisfied: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi\\in\\Pi_{\\mathrm{rand}}}\\upsilon_{\\pi}^{r}(s_{0})\\quad s.t.\\quad\\upsilon_{\\pi}^{c}(s_{0})\\geq b.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We assume the existence of a feasible solution to eq. (1) and let $\\pi^{*}$ denote the solution to eq. (1). A quantity unique to CMDP is the Slater\u2019s constant, which is denoted as $\\zeta=\\operatorname*{max}_{\\pi}v_{\\pi}^{c}\\big(s_{0}\\big)-b$ . Slater\u2019s constant characterizes the size of the feasibility region, and hence the hardness of the problem. ", "page_idx": 2}, {"type": "text", "text": "Because the state space can be large or possibly infinite, we use linear function approximation to approximate the values of stationary randomized policies. Let $\\phi:S\\times A\\rightarrow\\mathbb{R}^{d}$ be a feature map, we make the following assumption: ", "page_idx": 2}, {"type": "text", "text": "Definition 1 $\\stackrel{\\cdot}{q}_{\\pi}$ -realizability) There exists $B\\,>\\,0$ and a misspecification error $\\omega\\ge0$ such that for every $\\pi\\in\\Pi_{r a n d;}$ , there exists a weight vector $w_{\\pi}\\in\\mathbb{R}^{d}$ , $\\|w_{\\pi}\\|_{2}\\leq B_{!}$ , and ensures $|q_{\\pi}(s,a)-$ $\\langle w_{\\pi},\\phi(s,a)\\rangle|\\leq\\omega$ for all $(s,a)\\in S\\times{\\bar{A}}$ . ", "page_idx": 3}, {"type": "text", "text": "We assume to have access to a local access model, where the agent can only query simulator for states that have been encountered in previous simulations. Then, our goal is to design an algorithm that returns a near-optimal mixture policy $\\bar{\\pi}$ , whose performance can be characterized in two ways. For a given target error $\\epsilon>0$ , the ", "page_idx": 3}, {"type": "text", "text": "relaxed feasibility requires the returned policy $\\bar{\\pi}$ whose sub-optimality gap $v_{\\pi^{*}}^{r}(s_{0})-v_{\\bar{\\pi}}^{r}(s_{0})$ is bounded by $\\epsilon$ , while allowing for a small constraint violation. Formally, we require $\\bar{\\pi}$ such that ", "page_idx": 3}, {"type": "equation", "text": "$$\nv_{\\pi^{*}}^{r}(s_{0})-v_{\\bar{\\pi}}^{r}(s_{0})\\leq\\epsilon\\quad s.t\\quad v_{\\bar{\\pi}}^{c}(s_{0})\\geq b-\\epsilon.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "On the other hand, strict-feasibility requires the returned policy $\\bar{\\pi}$ whose sub-optimality gap $v_{\\pi^{*}}^{r}(s_{0})-v_{\\bar{\\pi}}^{r}(s_{0})$ is bounded by $\\epsilon$ while not allowing any constraint violation. Formally, we require $\\bar{\\pi}$ such that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{v_{\\pi^{*}}^{r}(s_{0})-v_{\\bar{\\pi}}^{r}(s_{0})\\le\\epsilon\\quad s.t\\quad v_{\\bar{\\pi}}^{c}(s_{0})\\geq b.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Notations ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "For any real number $a\\in\\mathbb{R}$ , we let $\\lfloor a\\rfloor$ to denote the smallest integer $i$ such that $i\\leq a$ . For vector $x\\,\\in\\,\\mathbb{R}^{d}$ , let $\\textstyle\\|x\\|_{1}\\,=\\,\\sum_{i}|x_{i}|$ , $\\|\\boldsymbol{x}\\|_{2}\\,=\\,\\sqrt{\\sum_{i}x_{i}^{2}}$ , and $\\left\\|{\\boldsymbol{x}}\\right\\|_{\\infty}=\\operatorname*{max}_{i}\\left|x_{i}\\right|$ . For a positive definite matrix $A\\,\\in\\,\\mathbb{R}^{d\\times d}$ , t he $\\|x\\|_{A}^{2}\\;=\\;x^{\\top}A x$ . We let $\\mathrm{proj}_{[a_{1},a_{2}]}(\\lambda)\\;=\\;\\mathrm{arg}\\,\\mathrm{min}_{p\\in[a_{1},a_{2}]}\\,|\\lambda-p|$ , and $\\mathrm{trunc}_{[a_{1},a_{2}]}(y)=\\mathrm{min}\\{\\mathrm{max}\\{y,a_{1}\\},a_{2}\\}$ . For any two positive numbers $a,b$ , we write $a=O(b)$ if there exists an absolute constant $c>0$ such that $a\\leq c b$ . We use the $\\tilde{O}$ to hide any polylogarithmic terms. ", "page_idx": 3}, {"type": "text", "text": "4 Confident-NPG-CMDP, a local-access algorithm for CMDP ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we introduce a primal-dual algorithm, which we call Confident-NPG-CMDP (see algorithm 1). ", "page_idx": 3}, {"type": "text", "text": "4.1 A primal-dual approach ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We approach solving the CMDP problem by framing it as an equivalent saddle-point problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi}\\operatorname*{min}_{\\lambda\\geq0}L(\\pi,\\lambda),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $L:\\Pi_{\\mathrm{rand}}\\times\\mathbb{R}_{+}\\,\\rightarrow\\,\\mathbb{R}$ is the Lagrange function. For a policy $\\pi\\,\\in\\,\\Pi_{\\mathrm{rand}}$ and a Lagrange multiplier $\\lambda\\in\\mathbb{R}_{+}$ , we have ", "page_idx": 3}, {"type": "equation", "text": "$$\nL(\\pi,\\lambda)=v_{\\pi}^{r}(s_{0})+\\lambda(v_{\\pi}^{c}(s_{0})-b).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Let $(\\pi^{*},\\lambda^{*})$ be the solution to this saddle-point problem. By an equivalence to a LP formulation and strong duality (Altman, 2021), $\\pi^{*}$ is the policy that achieves the optimal value in the CMDP as defined in eq. (1). The optimal Lagrange multiplier $\\begin{array}{r}{\\lambda^{*}=\\arg\\operatorname*{min}_{\\lambda\\geq0}L(\\pi^{*},\\lambda)}\\end{array}$ , Therefore, solving eq. (1) is equivalent to finding the saddle-point of the Lagrange function. ", "page_idx": 3}, {"type": "text", "text": "A typical primal dual algorithm that finds the saddle-point will proceed in an iterative fashion alternating between a policy update using policy gradient and a dual variable update using mirror descent. The policy gradient is computed with respect to the primal value $q_{\\pi_{k},\\lambda_{k}}^{p}\\doteq q_{\\pi_{k}}^{r}+\\lambda\\bar{\\,}q_{\\pi_{k}}^{c}$ and the mirror descent is computed with respect to the constraint value $\\begin{array}{r}{v_{\\pi_{k}}^{c}(s_{0})=\\left\\langle\\pi_{k}(\\cdot|s_{0}),q_{\\pi_{k}}^{c}(s_{0},\\cdot)\\right\\rangle}\\end{array}$ . Given that we do not have access to an oracle for exact policy evaluations, we must collect data to estimate the primal and constraint values. ", "page_idx": 3}, {"type": "text", "text": "If we have the least-squares estimates of $q_{\\pi_{k}}^{r}$ and $q_{\\pi_{k}}^{c}$ , denoted by $Q_{k}^{r}$ and $Q_{k}^{c}$ , respectively, then we can compute the least-squares estimate $Q_{k}^{p}=Q_{k}^{r}+\\lambda_{k}Q_{k}^{c}$ to be the estimate of the primal value ", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 Confident-NPG-CMDP ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "1: Input: $s_{0}$ (initial state), $\\epsilon$ (target accuracy), $\\delta\\in(0,1]$ (failure probability); $\\gamma$ (discount factor)   \n2: Initialize:   \n3: Define $K,\\eta_{1},m$ according to Theorem 1 for relaxed-feasibility and Theorem 2 for strict  \nfeasibility,   \n4: Set $L\\gets\\lfloor\\lfloor K\\rfloor/(\\lfloor m\\rfloor+1)\\rfloor$ .   \n5: For each iteration $k\\in\\{0,\\ldots,\\lfloor K\\rfloor\\}:\\pi_{k}\\leftarrow\\operatorname{Unif}({\\cal A}),~\\tilde{Q}_{k}^{p}(\\cdot,\\cdot)\\leftarrow0.$ , $\\tilde{V}_{k}^{c}(\\cdot)\\gets0$ , and $\\lambda_{k}\\gets0$ .   \n6: For each phase $l\\in\\{0,\\dots,L+1\\}:\\mathcal{C}_{l}\\gets(),\\,D_{l}\\gets$ $D_{l}\\leftarrow\\{\\}$ ", "page_idx": 4}, {"type": "text", "text": "7: For $a\\in A$ : if $(s_{0},a)\\not\\in\\mathrm{ActionCov}(\\mathcal{C}_{0})$ , then append $(s_{0},a)$ to $\\ensuremath{\\mathcal{C}}_{0}$ and set $\\bot$ to $D_{0}[(s_{0},a)]$ \u25b7 see ActionCov defined in eq. (4) ", "page_idx": 4}, {"type": "text", "text": "8: while True do ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "$\\triangleright$ main loop ", "page_idx": 4}, {"type": "text", "text": "9: Let $\\ell$ be the smallest integer s.t. $D_{\\ell}[z^{\\prime}]=\\!\\perp$ for some $z^{\\prime}\\in\\mathcal{C}_{\\ell}$   \n10: Let $z$ be the first state-action pair in $\\mathcal{C}_{\\ell}$ s.t. $D_{\\ell}[z]=\\!\\perp$ ", "page_idx": 4}, {"type": "text", "text": "$\\begin{array}{l}{{k_{\\ell}\\gets\\ell\\times(\\lfloor m\\rfloor+1)}}\\\\ {{(r e s u l t,d i s c o v e r e d)\\gets\\mathrm{Gather-data}(\\pi_{k_{\\ell}},\\mathcal{C}_{\\ell},\\alpha,z)}}\\end{array}$ $\\triangleright$ iteration corresponding to phase $\\ell$ ", "page_idx": 4}, {"type": "text", "text": "if discovered is True then ", "page_idx": 4}, {"type": "text", "text": "Append result to $\\mathcal{C}_{0}$ and set $\\perp$ to D0[result] \u25b7result is a state-action pair Goto line 8 ", "page_idx": 4}, {"type": "equation", "text": "$$\nD_{\\ell}[z]\\leftarrow r e s u l t\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "if $\\nexists z^{\\prime}\\in\\mathcal{C}_{\\ell}$ s.t. $D_{\\ell}[z^{\\prime}]=\\!\\bot$ then $k_{\\ell+1}\\gets k_{\\ell}+(\\lfloor m\\rfloor+1)$ if $k_{\\ell}+(\\lfloor m\\rfloor+1)\\leq\\lfloor K\\rfloor$ otherwise $\\lfloor K\\rfloor$ ", "page_idx": 4}, {"type": "text", "text": "for $k=k_{\\ell},\\dots,k_{\\ell+1}-1$ do $\\triangleright$ off-policy iterations reusing $\\mathcal{C}_{\\ell},D_{\\ell}$ $Q_{k}^{r},\\,\\,Q_{k}^{c}\\gets L S E(\\mathcal{C}_{\\ell},D_{\\ell},\\pi_{k},\\pi_{k_{\\ell}})$ ", "page_idx": 4}, {"type": "text", "text": "22: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{Q}_{k}^{p}(s,a)\\gets\\mathrm{trunc}_{\\left[0,\\frac{1}{1-\\gamma}\\right]}\\,Q_{k}^{r}(s,a)+\\lambda_{k}\\,\\mathrm{trunc}_{\\left[0,\\frac{1}{1-\\gamma}\\right]}\\,Q_{k}^{c}(s,a)}\\\\ &{\\tilde{V}_{k}^{c}(s)\\gets\\mathrm{trunc}_{\\left[0,\\frac{1}{1-\\gamma}\\right]}\\,\\langle\\pi_{k}(\\cdot|s),Q_{k}^{c}(s,\\cdot)\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "25:   \n26: ", "page_idx": 4}, {"type": "text", "text": "$\\triangleright$ update policy ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pi_{k+1}(a|s)\\leftarrow\\left\\{\\!\\!\\!\\begin{array}{l l}{\\pi_{k+1}(a|s)}&{\\mathrm{if~}s\\in\\operatorname{Cov}(\\mathcal{C}_{\\ell+1})}\\\\ {\\pi_{k}(a|s)\\frac{\\exp(\\eta_{k}\\tilde{Q}_{k}^{p}(s,a))}{\\sum_{a^{\\prime}\\in\\mathcal{A}}\\pi_{k}(a^{\\prime}|s)\\exp(\\eta_{1}\\tilde{Q}_{k}^{p}(s,a^{\\prime}))}}&{\\mathrm{otherwise}}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "28: ", "page_idx": 4}, {"type": "text", "text": "$\\triangleright$ update dual variable ", "page_idx": 4}, {"type": "text", "text": "29: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\lambda_{k+1}\\gets\\left\\{\\!\\!\\!\\begin{array}{l l}{\\lambda_{k+1}}&{\\mathrm{if}\\;s_{0}\\in\\mathrm{Cov}(\\mathscr{C}_{\\ell+1})}\\\\ {\\mathrm{proj}_{[0,U]}\\left(\\lambda_{k}-\\eta_{2}(\\tilde{V}_{k}^{c}(s_{0})-b)\\right)}&{\\mathrm{otherwise}.}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "30: ", "page_idx": 4}, {"type": "text", "text": "For $z\\in{\\mathcal{C}}_{\\ell}$ s.t. $z\\not\\in\\mathcal{C}_{\\ell+1}$ : append $z$ to $\\mathcal{C}_{\\ell+1}$ and set $\\bot$ to $D_{\\ell+1}[z]$ ", "page_idx": 4}, {"type": "text", "text": "$q_{\\pi_{k},\\lambda_{k}}^{p}$ . Additionally, we can compute $V_{k}^{c}(s_{0})=\\langle\\pi_{k}(\\cdot|s_{0}),Q_{k}^{c}(s_{0},\\cdot)\\rangle$ to be the least-squares estimate of the constraint value $v_{\\pi_{k}}^{c}(s_{0})$ . Then, for any given $(s,a)\\in S\\times A$ , our algorithm makes a policy update of the following form: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pi_{k+1}(a|s)\\propto\\pi_{k}(a|s)\\exp(\\eta_{1}Q_{k}^{p}(s,a)),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "followed by a dual variable update of the following form: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\lambda_{k+1}\\leftarrow\\lambda_{k}-\\eta_{2}\\left(V_{k}^{c}(s_{0})-b\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where the $\\eta_{1}$ and $\\eta_{2}$ are the step-sizes. ", "page_idx": 5}, {"type": "text", "text": "4.2 Core set and least square estimates ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "To construct the least-squares estimates, let us assume for now that we are given a set of state-action pairs, which we call the core set $\\mathcal{C}$ . By organizing the feature vector of each state-action pair in $\\mathcal{C}$ row-wise into a matrix $\\Phi_{\\mathcal{C}}\\in\\mathbb{R}^{|\\mathcal{C}|\\times d}$ , we can write the covariance matrix as $V(\\mathcal{C},\\alpha)=\\Phi_{\\mathcal{C}}^{\\top}\\Phi_{\\mathcal{C}}+\\alpha I$ For each $(s,a)\\,\\in\\,{\\mathcal{C}}$ , suppose we have run Monte Carlo rollouts using the rollout policy $\\pi$ with the local access simulator to obtain an averaged Monte Carlo return denoted by $\\bar{q}(s,a)$ . Then we gather all the state-action pairs into a vector $\\bar{q}\\in\\mathbb{R}^{|c|}$ . For any state-action pair $(s,a)\\in S\\times A$ , the least-square estimate of action-value $q_{\\pi}$ is defined to be ", "page_idx": 5}, {"type": "equation", "text": "$$\nQ(s,a)=\\langle\\phi(s,a),V(\\mathcal{C},\\alpha)^{-1}\\Phi_{\\mathcal{C}}^{\\top}\\bar{q}\\rangle.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Since the algorithm can only rely on estimates for policy improvement and constraint evaluation, it is imperative that these estimates closely approximate their true action values. In the local access setting, an algorithm may not be able to visit all state-action pairs, so we cannot guarantee that the estimates will closely approximate the true action values for all state-action pairs. However, we can ensure the accuracy of the estimates for a subset of states. ", "page_idx": 5}, {"type": "text", "text": "Given $\\mathcal{C}$ , let us define a set of state-action pairs whose features satisfies the condition $\\|\\phi(s,a)\\|_{V(\\mathcal{C},\\alpha)^{-1}}\\leq1$ , then we call this set the action-cover of $\\mathcal{C}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname{ActionCov}({\\mathcal{C}})=\\{(s,a)\\in S\\times A:\\|\\phi(s,a)\\|_{V({\\mathcal{C}},\\alpha)^{-1}}\\leq1\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Following from the action-cover, we have the cover of $\\mathcal{C}$ . For a state $s$ to be in the cover of $\\mathcal{C}$ , all its actions $a\\in A$ , the pair $(s,a)$ is in the action-cover of $\\mathcal{C}$ . In other words, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname{Cov}({\\mathcal{C}})=\\{s\\in S:\\forall a\\in{\\mathcal{A}},(s,a)\\in{\\mathrm{ActionCov}}({\\mathcal{C}})\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "For any $s\\,\\in\\,\\mathrm{Cov}(\\mathcal{C})$ , we can ensure the least square estimate $Q(s,a)$ defined by eq. (3) closely approximates its true action value $q_{\\pi}(s,a)$ for all $a\\in A$ . However, such a core set $\\mathcal{C}$ is not available before the algorithm is run. Therefore, we need an algorithm that will build a core set incrementally in the local-access setting while planning. To achieve this, we build our algorithm on CAPI-QPI-Plan (Weisz et al., 2022), using similar methodology for core set building and data gathering. ", "page_idx": 5}, {"type": "text", "text": "4.3 Core set building and data gathering to control the accuracy of the least-square estimates ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Confident-NPG-CMDP does not collect data in every iteration but collects data in interval of $m=$ $O\\left(\\ln(1+\\rho_{0})\\operatorname{poly}(\\epsilon^{-1}(1-\\gamma)^{-1})\\right)$ , where $\\rho_{0}\\geq0$ is an user defined constant. During each data collection phase, the algorithm performs on-policy evaluation. Between these phases, it conducts $(\\lfloor m\\rfloor+1)$ off-policy evaluations, reusing data from the most recent on-policy iteration. ", "page_idx": 5}, {"type": "text", "text": "By setting $\\rho_{0}$ to a positive value, we impose an upper bound of $1+\\rho_{0}$ on the per-trajectory importance sampling ratio used in off-policy evaluations, and $m$ is adjusted accordingly to maintain this bound. The total number of data collection phases is $L=\\lfloor\\lfloor K\\rfloor/(\\lfloor m\\rfloor+1)\\rfloor$ , where $K$ is the total number of iterations. When $\\rho_{0}$ is set to zero, we have $L=K$ , resulting in a purely on-policy version of the algorithm. ", "page_idx": 5}, {"type": "text", "text": "Confident-NPG-CMDP maintains a set of core sets $\\{{\\mathcal{C}}_{l}\\}_{l=0}^{L+1}$ , one for each data collection phases. Each core set $\\mathcal{C}_{l}$ is a list of state-action pairs. Due to the off-policy evaluations, Confident-NPGCMDP also maintains a set of data sets $\\bar{\\{D_{l}\\}}_{l=0}^{L}$ . Initially, all core sets are empty, all policies are initialized to the uniform policy, and all data sets are empty. ", "page_idx": 5}, {"type": "text", "text": "The algorithm begins by adding the feature vectors corresponding to $(s_{0},a)$ for all actions $a\\in A$ that are not in the action-cover of $\\mathcal{C}_{\\mathrm{0}}$ . These feature vectors are considered informative. For every $(s,a)\\in{\\mathcal{C}}_{0}$ , the algorithm adds an entry to $D_{0}$ and sets its value to the placeholder $\\perp$ , indicating that there is no roll-out data yet. Then, in line 9 of algorithm 1, the algorithm finds the smallest integer $l\\in\\{0,\\ldots,L\\}$ such that the corresponding $D_{l}$ has an entry without roll-out data (i.e., it contains the placeholder $\\bot$ ). When such a phase is found, a running phase begins, denoted by $\\ell$ in algorithm 1. We note that when $\\ell=L+1$ , the algorithm returns and no roll-outs are stored. ", "page_idx": 6}, {"type": "text", "text": "Since only one running phase $\\ell$ can be active at a time, and $\\ell$ can only take value $l\\in\\{0,\\ldots,L\\}$ , the algorithm updates the policies of the corresponding iterations in line 27, updates the dual variables of these iterations in line 29, and extends the core set for the next phase in line 30. ", "page_idx": 6}, {"type": "text", "text": "Suppose during a running phase with $\\ell=l$ , while performing the roll-out in Gather-data subroutine (algorithm 3 in appendix A), if any state-action pair $(s,a)\\in S\\times A$ is not in the action-cover of $\\mathcal{C}_{\\ell}$ , the current running phase stops and the newly discovered state-action pair is added to $\\mathcal{C}_{0}$ in line 15. The same state-action pair is then propagated to $\\mathcal{C}_{1}$ and so on by line 30. ", "page_idx": 6}, {"type": "text", "text": "Once a state-action pair is added to a core set by line 7, line 15, and line 30, it remains in that core set for the duration of the algorithm. This means that any $\\mathcal{C}_{l}$ , $l\\in\\{0,\\ldots,L+1\\}$ can grow in size and be extended multiple times during the execution of the algorithm. When any new state-action pair is added to a core set, the least-square estimate should be recomputed with the newly added information. This implies that the policy needs to be updated and data re-collected. However, we can avoid restarting the entire data collection procedure by updating only the policy for states that are newly added to the extended core set. We elaborate on this approach further in the next paragraph. ", "page_idx": 6}, {"type": "text", "text": "When the algorithm enters the running phase $\\ell=l$ , and the Gather-data subroutine returns, the LSE subroutine (algorithm 4) computes the least-squares estimate $Q_{k}^{r},Q_{k}^{c}$ using the most recently extended core set $\\mathcal{C}_{\\ell}$ for each corresponding iteration $k=k\\ell,\\dots,k_{\\ell+1}-1$ . Subsequently, $\\tilde{Q}_{k}^{p}$ of line 23 of algorithm 1 is updated with the newly updated least-square estimates $Q_{k}^{r},Q_{k}^{c}$ . However, the policy $\\pi_{k+1}$ will only be updated for states that are newly covered by $\\mathcal{C}_{\\ell}$ (i.e., $\\ddot{s}\\in\\dot{\\mathrm{Cov}}(\\mathcal{C}_{\\ell})\\setminus\\mathrm{Cov}(\\bar{\\mathcal{C}}_{\\ell+1})\\dot{)}$ . For any states that are already covered by $\\mathcal{C}_{\\ell}$ (i.e., $s\\in\\operatorname{Cov}(\\mathcal{C}_{\\ell+1}))$ , the policy remains unchanged from its previous update using the ${\\tilde{Q}}^{p}$ at that time. By updating the policy in this manner, the accuracy guarantee of $\\tilde{Q}_{k}^{p}(s,a)$ with respect to $q_{\\pi_{k},\\lambda_{k}}^{p}(s,a)$ is ensured not just for $\\pi_{k}$ , but for an extended set of policies defined as follows: ", "page_idx": 6}, {"type": "text", "text": "Definition 2 For any policy $\\pi$ from the set of randomized policies $\\Pi_{r a n d}$ and any subset $\\mathcal{X}\\subseteq S$ , the extended set of policies is defined as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\Pi_{\\pi,X}=\\{\\pi^{\\prime}\\in\\Pi_{r a n d}\\mid\\pi(\\cdot|s)=\\pi^{\\prime}(\\cdot|s)\\,f o r\\,a l l\\,s\\in\\mathcal{X}\\}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "By maintaining a set of core sets, gathering data via the Gather-data subroutine (algorithm 3 in appendix A), making policy updates by line 27, and dual variable updates by line 29, we have: ", "page_idx": 6}, {"type": "text", "text": "Lemma 1 Whenever LSE subroutine in line 21 of Confident-NPG-CMDP is executed during a running phase $\\ell=l$ for $l\\in\\{0,\\ldots,L\\}$ , the least-square estimate $\\tilde{Q}_{k}^{p}(s,a)$ satisfies the following condition for all iterations $k=k\\ell,\\dots,k\\ell{+}1-1$ associated with this phase and for all $s\\in\\mathrm{Cov}(\\mathcal{C}_{\\ell})$ and $a\\in A$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\tilde{Q}_{k}^{p}(s,a)-q_{\\pi_{k}^{\\prime},\\lambda_{k}}^{p}(s,a)|\\le\\epsilon^{\\prime}\\ \\ \\,f o r\\,a l l\\,\\pi_{k}^{\\prime}\\in\\Pi_{\\pi_{k},\\mathrm{Cov}({\\mathcal C}_{\\ell})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\epsilon^{\\prime}=(1+U)(\\omega+\\sqrt{\\alpha}B+(\\omega+\\epsilon)\\sqrt{\\tilde{d}})$ with $\\tilde{d}=\\tilde{O}(d)$ and $U$ is an upper bound on the optimal Lagrange multiplier. Similarly, for initial state $s_{0}$ , we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n|\\tilde{V}_{k}^{c}(s_{0})-v_{\\pi_{k}^{\\prime}}^{c}(s_{0})|\\leq\\omega+\\sqrt{\\alpha}B+(\\omega+\\epsilon)\\sqrt{\\tilde{d}}\\;\\;\\;f o r\\,a l l\\;\\pi_{k}^{\\prime}\\in\\Pi_{\\pi_{k},\\mathrm{Cov}(\\mathcal{C}_{\\ell})}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The accuracy guarantee of eq. (5) and eq. (6) are maintained throughout the execution of the algorithm. By lemma 4.5 of Weisz et al. (2022) (restated in lemma 6 in appendix A), for any past version $\\mathcal{C}_{l}^{\\mathrm{past}}$ of Cl and the corresponding policy \u03c0pkas associated with $\\mathcal{C}_{l}^{\\mathrm{past}}$ , we have $\\Pi_{\\pi_{k},\\mathrm{Cov}({\\mathcal C}_{l})}\\subseteq\\Pi_{\\pi_{k}^{\\mathrm{past}},\\mathrm{Cov}({\\mathcal C}_{l}^{\\mathrm{past}})}.$ This means that if eq. (5) and eq. (6) hold true for any policy in $\\Pi_{\\pi_{k}^{\\mathrm{past}},\\mathrm{Cov}(C_{l}^{\\mathrm{past}})}$ , they will also hold true for any future updated policy $\\pi_{k}$ . ", "page_idx": 6}, {"type": "text", "text": "4.4 Differences between Confident-NPG-CMDP and CAPI-QPI-Plan ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "CAPI-QPI-Plan is designed for unconstrained MDPs and returns a deterministic policy, which may not be feasible in the constrained setting. In contrast, Confident-NPG-CMDP returns a soft mixture policy $\\bar{\\pi}_{K}$ , ensuring that $\\bar{\\pi}_{K}(a|s)>0$ for all $(s,a)\\in S\\times A$ . ", "page_idx": 7}, {"type": "text", "text": "In constrained MDPs, controlling the dual variable via mirror descent adds an $\\epsilon^{-2}$ factor to the sample complexity. Directly applying CAPI-QPI-Plan would increase the complexity to $\\tilde{O}(\\epsilon^{-4})$ due to the need to manage both the dual variable and estimation error. To address this, Confident-NPG-CMDP employs the natural policy gradient for policy improvement and leverages the softmax policy structure to perform off-policy estimation, thereby reducing the complexity to $\\bar{O}(\\epsilon^{-3})$ . ", "page_idx": 7}, {"type": "text", "text": "By employing a per-trajectory importance sampling ratio, we weigh the Monte Carlo returns generated from data collected in earlier on-policy phases, resulting in unbiased estimates of action values with respect to the target policy. However, this ratio can become large if there is a substantial difference between the on-policy and target policies. To mitigate this, the algorithm collects data at intervals of $m$ , effectively determining when to gather new data as the policy significantly diverges from an earlier recent data-gathering iteration. By setting $\\rho_{0}~>~0$ , we can bound the per-trajectory importance sampling ratio, thus controlling the interval $m$ for resampling on-policy data to produce well-controlled estimators. ", "page_idx": 7}, {"type": "text", "text": "Key algorithmic differences between Confident-NPG-CMDP and CAPI-QPI-Plan: ", "page_idx": 7}, {"type": "text", "text": "1. Policy Improvement Step: Confident-NPG-CMDP utilizes a softmax over the estimated action-values, whereas CAPI-QPI-Plan employs a greedy approach.   \n2. Dual Variable Computation: Confident-NPG-CMDP requires computation of the dual variable inherent in primal-dual algorithms.   \n3. Data Sampling Strategy: Unlike CAPI-QPI-Plan, Confident-NPG-CMDP does not sample data at every iteration but collects data at specific intervals to control the importance sampling ratio. ", "page_idx": 7}, {"type": "text", "text": "In the next two sections, we will demonstrate how these changes ensure a feasible mixture policy for the CMDP and address the additional analytical challenges. ", "page_idx": 7}, {"type": "text", "text": "5 Confident-NPG-CMDP satisfies relaxed-feasibility ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "With the accuracy guarantee of the least-square estimates, we prove that at the termination of Confident-NPG-CMDP, the returned mixture policy $\\bar{\\pi}_{K}$ satisfies relaxed-feasibility. We note that because of the execution of line 30 in algorithm 1, at termination, one can show using induction that $\\mathcal{C}_{0}=\\mathcal{C}_{1}=\\cdots=\\mathcal{C}_{L+1}$ . Therefore, $\\operatorname{Cov}(\\mathcal C_{0})=\\operatorname{Cov}(\\mathcal C_{1})=\\cdots=\\operatorname{Cov}(\\mathcal C_{L})$ . Thus, it is sufficient to only consider $\\ensuremath{\\mathcal{C}}_{0}$ at the termination of the algorithm. By line 7 of algorithm 1, we have ensured $s_{0}\\in\\dot{\\mathrm{Cov}}(\\mathcal{C}_{0})$ . ", "page_idx": 7}, {"type": "text", "text": "By employing the primal-dual approach discussed in section 4, we reduce the CMDP problem to an unconstrained problem with a single reward function of the form $r_{\\lambda}=r+\\lambda c$ . Therefore, we can apply lemma 12 from the Confident-NPG algorithm in the single-reward setting (see appendix A) to our Confident-NPG-CMDP algorithm, replacing $\\pi$ with $\\pi^{*}$ . Consequently, the value difference between $\\pi^{*}$ and $\\bar{\\pi}_{K}$ can be bounded, which leads to: ", "page_idx": 7}, {"type": "text", "text": "Lemma 2 Let $\\delta\\,\\in\\,(0,1]$ be the failure probability, $\\epsilon>0$ be the target accuracy, and $s_{0}$ be the initial state. Assuming for all $s\\,\\in\\,\\mathrm{Cov}(\\mathcal{C}_{0})$ and all $a\\in A;$ , $|\\tilde{Q}_{k}^{p}(s,a)-q_{\\pi_{k}^{\\prime},\\lambda_{k}}^{p}(s,a)|\\,\\leq\\,\\epsilon^{\\prime}$ and $|\\tilde{V}_{k}^{c}(s_{0})-v_{\\pi_{k}^{\\prime}}^{c}(s_{0})|\\le\\omega+\\sqrt{\\alpha}B+(\\omega+\\epsilon)\\sqrt{\\tilde{d}}$ for all $\\pi_{k}^{\\prime}\\in\\Pi_{\\pi_{k},\\mathrm{Cov}({\\mathcal{C}}_{0})}$ , then, with probability $1-\\delta$ , Confident-NPG-CMDP returns a mixture policy $\\bar{\\pi}_{K}$ that satisfies the following, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{v_{\\pi^{*}}^{r}(s_{0})-v_{\\bar{\\pi}_{K}}^{r}(s_{0})\\le\\displaystyle\\frac{5\\epsilon^{\\prime}}{1-\\gamma}+\\frac{(\\sqrt{2\\ln(A)}+1)(1+U)}{(1-\\gamma)^{2}\\sqrt{K}},}&\\\\ &{b-v_{\\bar{\\pi}_{K}}^{c}(s_{0})\\le[b-v_{\\bar{\\pi}_{K}}^{c}(s_{0})]_{+}\\le\\displaystyle\\frac{5\\epsilon^{\\prime}}{(1-\\gamma)(U-\\lambda^{*})}+\\frac{(\\sqrt{2\\ln(A)}+1)(1+U)}{(1-\\gamma)^{2}(U-\\lambda^{*})\\sqrt{K}},}&\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\begin{array}{r}{\\epsilon^{\\prime}=(1+U)(\\omega+(\\sqrt{\\alpha}B+(\\omega+\\epsilon)\\sqrt{\\tilde{d}}))}\\end{array}$ with $\\tilde{d}=\\tilde{O}(d)$ , and $U$ is an upper bound on the optimal Lagrange multiplier. ", "page_idx": 8}, {"type": "text", "text": "By setting the parameters to appropriate values, it follows from lemma 2 that we obtain the following result: ", "page_idx": 8}, {"type": "text", "text": "Theorem 1 With probability $1-\\delta$ , the mixture policy $\\begin{array}{r}{\\bar{\\pi}_{K}=\\frac{1}{K}\\sum_{k=0}^{K-1}\\pi_{k}}\\end{array}$ returned by confidentNPG-CMDP ensures that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v_{\\pi^{*}}^{r}(s_{0})-v_{\\bar{\\pi}_{K}}^{r}(s_{0})=\\tilde{O}(\\sqrt{d}(1-\\gamma)^{-2}\\zeta^{-1}\\omega)+\\epsilon,}\\\\ &{\\qquad\\qquad\\quad v_{\\bar{\\pi}_{K}}^{c}(s_{0})\\geq b-\\left(\\tilde{O}(\\sqrt{d}(1-\\gamma)^{-2}\\zeta^{-1}\\omega)+\\epsilon\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "if we choose $n\\,=\\,\\tilde{O}(\\epsilon^{-2}\\zeta^{-2}(1-\\gamma)^{-4}d)$ , $\\alpha\\,=\\,O\\left(\\epsilon^{2}\\zeta^{2}(1-\\gamma)^{4}\\right)$ , $K\\,=\\,\\tilde{O}\\left(\\epsilon^{-2}\\zeta^{-2}(1-\\gamma)^{-6}\\right)$ , $\\eta_{1}=\\tilde{O}\\left((1-\\gamma)^{2}\\zeta K^{-1/2}\\right)$ , $\\eta_{2}=\\zeta^{-1}K^{-1/2}$ , $H=\\tilde{O}\\left((1-\\gamma)^{-1}\\right)$ , $m=\\tilde{O}\\left(\\epsilon^{-1}\\zeta^{-2}(1-\\gamma)^{-2}\\right)$ , and $L=\\lfloor K/(\\lfloor m\\rfloor+1)\\rfloor=\\tilde{O}\\left(\\epsilon^{-1}(1-\\gamma)^{-4}\\right)$ total number of data collection phases. ", "page_idx": 8}, {"type": "text", "text": "Furthermore, the algorithm utilizes at most $\\tilde{O}(\\epsilon^{-3}\\zeta^{-3}d^{2}(1-\\gamma)^{-11})$ queries in the local-access setting. ", "page_idx": 8}, {"type": "text", "text": "Remark 1: In the presence of misspecification error $\\omega>0$ , the reward suboptimality and constraint violation is $\\tilde{O}(\\omega)+\\epsilon$ with the same sample complexity. ", "page_idx": 8}, {"type": "text", "text": "Remark 2: Suppose the Slader\u2019s constant $\\zeta$ is much smaller than the suboptimality bound of $\\tilde{O}(\\omega)+\\epsilon$ , and it is reasonable to set $\\zeta=\\epsilon$ . Then, the sample complexity is $\\tilde{O}\\bar{(\\epsilon^{-6}(1-\\gamma)^{-11}d^{2})}$ , which is independent of $\\zeta$ . ", "page_idx": 8}, {"type": "text", "text": "Remark 3: Our algorithm requires the Slater\u2019s constant $\\zeta$ , which can be estimated by running CAPI-QPI-Plan only on the constraint function $c$ , treating it as an unconstrained optimization problem. This yields an approximation of max\u03c0 $v_{\\pi}^{c}(s_{0})$ , allowing us to estimate $\\zeta$ . Performing this estimation before executing Confident-NPG-CMDP adds only an additive term to the overall sample complexity. ", "page_idx": 8}, {"type": "text", "text": "6 Confident-NPG-CMDP satisfies strict-feasibility ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To address the strict feasibility problem, where no constraint violations are permitted (i.e., $v_{\\bar{\\pi}_{K}}^{c}\\geq b$ ), the algorithm must solve a more conservative CMDP. We define a surrogate CMDP with the tuple $(S,\\overbar{A},P,r,c,\\gamma,b^{\\prime},s_{0})$ , where $b^{\\prime}=\\,b+\\Delta$ for some $\\Delta\\geq0$ . Note that ${\\big[}b^{\\prime}\\geq b$ , imposing stricter constraints than the original problem. The optimal policy of this surrogate CMDP ensures compliance with the original constraint and is defined as follows: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\pi_{\\triangle}^{*}\\in\\arg\\operatorname*{max}v_{\\pi}^{r}(s_{0})\\quad s.t.\\quad v_{\\pi}^{c}(s_{0})\\geq b^{\\prime}.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Notice that $\\pi_{\\triangle}^{*}$ is a more conservative policy than $\\pi^{*}$ , where $\\pi^{*}$ is the optimal policy of the original CMDP objective eq. (1). By solving this surrogate CMDP using Confident-NPG-CMDP and applying the result of theorem 1, we obtain a $\\bar{\\pi}_{K}$ that would satisfy ", "page_idx": 8}, {"type": "equation", "text": "$$\nv_{\\bar{\\pi}^{*}}^{r}(s_{0})-v_{\\bar{\\pi}_{K}}^{r}(s_{0})\\leq\\bar{\\epsilon}\\quad s.t.\\quad v_{\\bar{\\pi}_{K}}^{c}(s_{0})\\geq\\quad b^{\\prime}-\\bar{\\epsilon},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\bar{\\epsilon}=\\tilde{O}(\\omega)+\\epsilon$ . Expanding out $b^{\\prime}$ , we have $v_{\\bar{\\pi}_{K}}^{c}(s_{0})\\geq b+\\triangle-\\bar{\\epsilon}$ . If we can set $\\triangle$ such that $\\triangle-\\bar{\\epsilon}\\ge0$ , then $v_{\\bar{\\pi}_{K}}^{c}(s_{0})\\geq b$ , which satisfies strict-feasibility. We show this formally in the next theorem, where $\\triangle=O(\\epsilon(1-\\gamma)\\zeta)$ and is incorporated into the algorithmic parameters for ease of presentation. ", "page_idx": 8}, {"type": "text", "text": "Theorem 2 With probability $1\\ -\\ \\delta$ , a target $\\epsilon\\mathrm{~\\ensuremath~{~\\leftmoon~}~}>\\mathrm{~\\ensuremath~{~\\leftmoon~}~}$ , the mixture policy $\\bar{\\pi}_{K}$ returned by confident-NPG-CMDP ensures that $v_{\\pi^{*}}^{r}(s_{0})\\ -\\ v_{\\bar{\\pi}_{K}}^{r}(s_{0})\\ \\ \\leq\\ \\ \\epsilon$ and $v_{\\bar{\\pi}_{K}}^{c}(s_{0})\\quad\\geq\\quad b,\\;\\;\\;i$ f assuming the misspecification error $\\omega\\;\\;\\leq\\;\\;\\epsilon\\zeta^{2}(1\\;-\\;\\gamma)^{3}(1\\;+\\;\\sqrt{\\tilde{d}})^{-1}$ , and if we choose ( $\\begin{array}{r l r l r}{\\iota\\!}&{=}&{\\!\\!\\!O\\left(\\epsilon^{2}\\zeta^{3}(1-\\gamma)^{5}\\right),K\\!}&{=}&{\\!\\!\\!\\tilde{O}\\left(\\epsilon^{-2}\\zeta^{-4}(1-\\gamma)^{-8}\\right),n\\!}&{=}&{\\!\\!\\!\\tilde{O}\\left(\\epsilon^{-2}\\zeta^{-4}(1-\\gamma)^{-8}d\\right),H\\!}&{=}&{\\!\\!\\!\\tilde{O}\\left(\\epsilon^{-2}\\zeta^{-4}(1-\\gamma)^{-8}d\\right),}\\end{array}$ $\\tilde{O}\\left((1-\\gamma)^{-1}\\right)$ , $m=\\tilde{O}\\left(\\epsilon^{-1}\\zeta^{-2}(1-\\gamma)^{-3}\\right)$ , and $L=\\lfloor K/(\\lfloor m\\rfloor+1)\\rfloor=\\tilde{O}((\\epsilon^{-1}\\zeta^{-2}(1-\\gamma)^{-5}))$ total data collection phases. ", "page_idx": 8}, {"type": "text", "text": "Furthermore, the algorithm utilizes at most $\\tilde{O}(\\epsilon^{-3}\\zeta^{-6}(1-\\gamma)^{-14}d^{2})$ queries in the local-access setting. ", "page_idx": 8}, {"type": "text", "text": "Remark 1: We note that by solving this conservative CMDP incurs a higher sample complexity, necessitating a separate treatment for this setting. Additionally, in the presence of a misspecification error $\\omega\\,>\\,0$ , the strict-feasibility setting requires additional assumptions on $\\omega$ , whereas the relaxed-feasibility setting does not. The sample complexity of the relaxed-feasibility setting can be independent of Slater\u2019s constant, whereas for strict feasibility, the returned policy must strictly adhere to constraints, and we cannot simply set Slater\u2019s constant $\\zeta$ to $\\epsilon$ and disregard its impact. ", "page_idx": 9}, {"type": "text", "text": "7 A discussion on memory cost and some implementation details ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The overall memory requirement is $\\tilde{d n}H(L+1)+\\tilde{d}+(L+1)(m+1)\\tilde{d}d$ . The term $\\tilde{d}n H(L+1)$ comes from maintaining $L+1$ copies of the core sets, and each core set contains no more than $\\tilde{d}$ state-action pairs. For each state-action pair in $\\mathcal{C}_{l}$ for $l\\;\\in\\;\\{0,\\ldots,L\\}$ , the algorithm stores $n$ trajectories consisting of $H$ tuples $(s,a,r,c)$ . ", "page_idx": 9}, {"type": "text", "text": "In phase $L+1$ , the algorithm terminates, so no roll-outs are stored. The second term $\\tilde{d}$ accounts for the elements stored in $\\mathcal{C}_{L+1}$ , which has no more than d\u02dc elements. ", "page_idx": 9}, {"type": "text", "text": "Finally, the last term is the memory required to store the least-square weights of the estimator during core set extensions. Each core set $\\mathcal{C}_{l}$ can undergo up to d\u02dc extensions. Recall that one state-action pair is added to $\\scriptstyle{\\mathcal{C}}_{0}$ at a time, and subsequently propagated to $\\mathcal{C}_{1},\\mathcal{C}_{2}$ and so on, ensuring that each core set contains no more than $\\tilde{d}$ elements. During every extension of $\\mathcal{C}_{l}$ , the newly added state-action pairs are marked, and up to $m+1$ least-square weights are stored to account for the corresponding iterations associated with $\\mathcal{C}_{l}$ . Since each weight vector has a dimension $d$ , and there are $L+1$ core sets maintained this manner, the total memory required to store all least-square weights is bounded by $(L+1)(m+1)\\tilde{d}d$ . ", "page_idx": 9}, {"type": "text", "text": "We store the least-squares weights because the algorithm must return a mixture policy, which requires access to all policies $\\pi_{0},\\ldots,\\pi_{K-1}$ . Instead of storing each $\\pi_{k}$ for $k=0,\\ldots,K-1$ across the entire state-action space, the algorithm tracks the state-action pairs newly added to each core set during extensions and saves their corresponding least-squares weights for each extension. With this stored information and the initialization of $\\pi_{0}$ , a subroutine can reconstruct the policies $\\pi_{k}(\\cdot|s)$ for any $s$ and iteration $k$ as needed. Please refer to appendix E for a brief discussion on how to mark the state-action pairs, store the least-square weights, and use this information to reconstruct the policies as required. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We have presented a primal-dual algorithm for planning in CMDP with large state spaces, given $q_{\\pi}$ - realizable function approximation. The algorithm, with high probability, returns a policy that achieves both the relaxed and strict feasibility CMDP objectives, using no more than $\\tilde{O}(\\epsilon^{-3}d^{2}\\,\\mathrm{poly}(\\zeta^{-1}(1-$ $\\gamma)^{-1})$ ) queries to the local-access simulator. ", "page_idx": 9}, {"type": "text", "text": "Our algorithm does not query the simulator and collect data in every iteration. Instead, the algorithm queries the simulator only at fixed intervals. Between these data collection intervals, our algorithm improves the policy using off-policy optimization. This approach makes it possible to achieve the desired sample complexity in both feasibility settings. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Tian Tian would like to thank Roshan Shariff and Kenny Young for their insightful comments and helpful feedback during the preparation of this manuscript. Csaba Szepesv\u00e1ri also gratefully acknowledges funding from the Canada CIFAR AI Chairs Program, Amii, and NSERC. Lin Yang is supported in part by NSF #2221871 and an Amazon Faculty Award. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Altman, E. (2021). Constrained Markov decision processes. Routledge. ", "page_idx": 9}, {"type": "text", "text": "Chen, Y., Dong, J., and Wang, Z. (2021). A primal-dual approach to constrained markov decision processes. arXiv preprint arXiv:2101.10895.   \nDai, J., Pan, X., Sun, R., Ji, J., Xu, X., Liu, M., Wang, Y., and Yang, Y. (2024). Safe RLHF: Safe reinforcement learning from human feedback. In The Twelfth International Conference on Learning Representations.   \nDing, D., Wei, X., Yang, Z., Wang, Z., and Jovanovic, M. (2021). Provably efficient safe exploration via primal-dual policy optimization. In International Conference on Artificial Intelligence and Statistics, pages 3304\u20133312. PMLR.   \nEfroni, Y., Mannor, S., and Pirotta, M. (2020). Exploration-exploitation in constrained mdps. CoRR, abs/2003.02189.   \nFeyzabadi, S. and Carpin, S. (2014). Risk-aware path planning using hirerachical constrained markov decision processes. In 2014 IEEE International Conference on Automation Science and Engineering (CASE), pages 297\u2013303.   \nGattami, A., Bai, Q., and Aggarwal, V. (2021). Reinforcement learning for constrained markov decision processes. In International Conference on Artificial Intelligence and Statistics, pages 2656\u20132664. PMLR.   \nGhosh, A., Zhou, X., and Shroff, N. (2024). Towards achieving sub-linear regret and hard constraint violation in model-free rl. In International Conference on Artificial Intelligence and Statistics, pages 1054\u20131062. PMLR.   \nGirard, J. and Reza Emami, M. (2015). Concurrent markov decision processes for robot team learning. Engineering Applications of Artificial Intelligence, 39:223\u2013234.   \nHao, B., Lazic, N., Yin, D., Abbasi-Yadkori, Y., and Szepesvari, C. (2022). Confident least square value iteration with local access to a simulator. In International Conference on Artificial Intelligence and Statistics, pages 2420\u20132435. PMLR.   \nHasanzadeZonuzy, A., Kalathil, D., and Shakkottai, S. (2021). Model-based reinforcement learning for infinite-horizon discounted constrained markov decision processes. IJCAI 2021.   \nJain, A., Vaswani, S., Babanezhad, R., Szepesvari, C., and Precup, D. (2022). Towards painless policy optimization for constrained mdps. In Uncertainty in Artificial Intelligence, pages 895\u2013905. PMLR.   \nJin, C., Yang, Z., Wang, Z., and Jordan, M. I. (2020). Provably efficient reinforcement learning with linear function approximation. In Conference on learning theory, pages 2137\u20132143. PMLR.   \nKakade, S. M. (2003). On the sample complexity of reinforcement learning. University of London, University College London (United Kingdom).   \nKalagarla, K. C., Jain, R., and Nuzzo, P. (2021). A sample-efficient algorithm for episodic finitehorizon mdp with constraints. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 8030\u20138037.   \nKamran, D., Sim\u00e3o, T. D., Yang, Q., Ponnambalam, C. T., Fischer, J., Spaan, M. T., and Lauer, M. (2022). A modern perspective on safe automated driving for different traffic dynamics using constrained reinforcement learning. In 2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC), pages 4017\u20134023. IEEE.   \nKearns, M., Mansour, Y., and Ng, A. Y. (2002). A sparse sampling algorithm for near-optimal planning in large markov decision processes. Machine learning, 49:193\u2013208.   \nKitamura, T., Kozuno, T., Kato, M., Ichihara, Y., Nishimori, S., Sannai, A., Sonoda, S., Kumagai, W., and Matsuo, Y. (2024). A policy gradient primal-dual algorithm for constrained mdps with uniform pac guarantees. arXiv preprint arXiv:2401.17780.   \nLattimore, T. and Szepesv\u00e1ri, C. (2020). Bandit algorithms. Cambridge University Press.   \nLattimore, T., Szepesvari, C., and Weisz, G. (2020). Learning with good feature representations in bandits and in rl with a generative model. In International conference on machine learning, pages 5662\u20135670. PMLR.   \nLiu, T., Zhou, R., Kalathil, D., Kumar, P. R., and Tian, C. (2021). Learning policies with zero or bounded constraint violation for constrained mdps. CoRR, abs/2106.02684.   \nLiu, T., Zhou, R., Kalathil, D., Kumar, P. R., and Tian, C. (2022). Policy optimization for constrained mdps with provable fast global convergence.   \nMiryoosef,i S. and Jin, C. (2022). A simple reward-free approach to constrained reinforcement learning. In International Conference on Machine Learning, pages 15666\u201315698. PMLR.   \nSidford, A., Wang, M., Wu, X., Yang, L., and Ye, Y. (2018). Near-optimal time and sample complexities for solving markov decision processes with a generative model. Advances in Neural Information Processing Systems, 31.   \nVaswani, S., Yang, L., and Szepesv\u00e1ri, C. (2022). Near-optimal sample complexity bounds for constrained mdps. Advances in Neural Information Processing Systems, 35:3110\u20133122.   \nWachi, A. and Sui, Y. (2020). Safe reinforcement learning in constrained markov decision processes. In International Conference on Machine Learning, pages 9797\u20139806. PMLR.   \nWeisz, G., Amortila, P., Janzer, B., Abbasi-Yadkori, Y., Jiang, N., and Szepesv\u00e1ri, C. (2021). On query-efficient planning in mdps under linear realizability of the optimal state-value function. In Conference on Learning Theory, pages 4355\u20134385. PMLR.   \nWeisz, G., Gy\u00f6rgy, A., and Szepesv\u00e1ri, C. (2022). Confident approximate policy iteration for efficient local planning in q pi realizable mdps. arXiv preprint arXiv, 2210.   \nWeisz, G., Gy\u00f6rgy, A., and Szepesv\u00e1ri, C. (2023). Online rl in linearly $q^{\\pi}$ -realizable mdps is as easy as in linear mdps if you learn what to ignore.   \nWen, L., Duan, J., Li, S. E., Xu, S., and Peng, H. (2020). Safe reinforcement learning for autonomous vehicles through parallel constrained policy optimization. In 2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC), pages 1\u20137.   \nYang, L. and Wang, M. (2019). Sample-optimal parametric q-learning using linearly additive features. In International conference on machine learning, pages 6995\u20137004. PMLR.   \nYin, D., Hao, B., Abbasi-Yadkori, Y., Lazic\u00b4, N., and Szepesv\u00e1ri, C. (2022). Efficient local planning with linear function approximation. In International Conference on Algorithmic Learning Theory, pages 1165\u20131192. PMLR.   \nYu, T., Tian, Y., Zhang, J., and Sra, S. (2021). Provably efficient algorithms for multi-objective competitive rl. In International Conference on Machine Learning, pages 12167\u201312176. PMLR.   \nZheng, L. and Ratliff, L. (2020). Constrained upper confidence reinforcement learning. In Bayen, A. M., Jadbabaie, A., Pappas, G., Parrilo, P. A., Recht, B., Tomlin, C., and Zeilinger, M., editors, Proceedings of the 2nd Conference on Learning for Dynamics and Control, volume 120 of Proceedings of Machine Learning Research, pages 620\u2013629. PMLR. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Confident-NPG in a single reward setting ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The pseudo code of Confident-NPG with a single reward setting is the same as Confident-NPG-CMDP in algorithm 1, except that line 24 and line 29 will not appear in Confident-NPG. Additionally, the LSE subroutine returns just $Q^{r}$ , and the policy update will be with respect to ${\\tilde{Q}}^{r}$ . For the complete pseudo code of Confident-NPG in the single reward setting, please see algorithm 2. In the following analysis, for convenience, we omit the superscript $r$ . ", "page_idx": 12}, {"type": "text", "text": "Algorithm 2 Confident-NPG ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Input: $s_{0}$ (initial state), $\\epsilon$ (target accuracy), $\\delta\\in(0,1]$ (failure probability), $c\\geq0$ , \u03b3, $\\begin{array}{r}{K=\\frac{2\\ln(|A|)}{(1-\\gamma)^{4}\\epsilon^{2}}}\\end{array}$ , $\\begin{array}{r}{\\eta_{1}=(1-\\gamma)\\sqrt{\\frac{2\\ln(|A|)}{K}},\\,m=\\frac{\\ln(1+\\rho_{0})}{2\\epsilon(1-\\gamma)\\ln\\left(\\frac{4}{\\epsilon(1-\\gamma)^{2}}\\right)},L=\\lfloor\\lfloor K\\rfloor/(\\lfloor m\\rfloor+1)\\rfloor.}\\end{array}$ ", "page_idx": 12}, {"type": "text", "text": "Initialize: for each iteration $k\\;\\in\\;\\{0,\\ldots,\\lfloor K\\rfloor\\}\\;:\\;\\pi_{k}\\;\\gets\\;\\mathrm{Uniform}({\\cal A}),\\;\\;\\tilde{Q}_{k}^{r}(s,a)\\;\\gets\\;0$ , for all   \n$s,a\\in{\\mathcal{S}}\\times{\\mathcal{A}}$ , and $\\lambda_{k}\\gets0$ . For each phase $l\\in\\{0,\\dots,L+1\\}:{\\mathcal{C}}_{l}\\gets(),\\;D_{l}\\gets\\{\\}$   \n1: for $a\\in{\\mathcal{A}}$ do   \n2: if $(s_{0},a)\\not\\in\\mathrm{ActionCov}(\\mathcal{C}_{0})$ then   \n3: Append $(s_{0},a)$ to $\\ensuremath{\\mathcal{C}}_{0}$ ; $\\ln[(s_{0},a)]\\gets\\perp$   \n4: while True do \u25b7main loop   \n5: Get the smallest integer $\\ell$ s.t. $D_{\\ell}[z^{\\prime}]=\\!\\perp$ for some $z^{\\prime}\\in\\mathcal{C}_{\\ell}$   \n6: Get the first state-action pair $z$ in $\\mathcal{C}_{\\ell}$ s.t. $D_{\\ell}[z]=\\!\\perp$   \n7: if \u2113= L + 1 then return\u230aK1\u230b \u230akK=0\u230b\u2212   \n8: $\\begin{array}{l}{{k_{\\ell}\\gets\\ell\\times(\\lfloor m\\rfloor+1)}}\\\\ {{(r e s u l t,d i s c o v e r e d)\\gets\\mathrm{Gather-data}(\\pi_{k_{\\ell}},\\mathcal{C}_{\\ell},\\alpha,z)}}\\end{array}$ $\\triangleright$ iteration corresponding to phase $\\ell$   \n9:   \n10: if discovered is True then   \n11: \u25b7result is a state-action pair   \n12: Append result to $\\mathcal{C}_{0}$ ; $\\bar{D_{0}[r e s u l t]}\\gets\\perp$   \n13: break   \n14: \u25b7result is a set of $n\\,H$ -horizon trajectories $\\sim\\pi_{k_{\\ell}}$ starting at $z$   \n15: $D_{\\ell}[z]\\leftarrow r e s u l t$   \n16: if $\\nexists z^{\\prime}\\in\\mathcal{C}_{\\ell}$ s.t. $D_{\\ell}[z^{\\prime}]=\\!\\bot$ then   \n17: $k_{\\ell+1}\\gets k_{\\ell}+(\\lfloor m\\rfloor+1)$ if $k_{\\ell}+(\\lfloor m\\rfloor+1)\\leq\\lfloor K\\rfloor$ otherwise $\\lfloor K\\rfloor$   \n18: $\\triangleright$ update policy for every $k\\in[k_{\\ell},k_{\\ell+1}-1]$ using $\\mathcal{C}_{\\ell},D_{\\ell}$   \n19: for $k=k_{\\ell},\\dots,k_{\\ell+1}-1$ do   \n20: $Q_{k}^{r}\\;,\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\mathrm{\\!\\!\\!\\!\\rho}}_{\\theta})_{\\theta}\\;,$   \n21: $\\triangleright$ update variables and improve policy   \n22: for all $s\\in\\mathrm{Cov}(\\mathcal{C}_{\\ell})\\setminus\\mathrm{Cov}(\\mathcal{C}_{\\ell+1})$ , and for all $a\\in{\\mathcal{A}}$ do   \n23: $\\tilde{Q}_{k}^{r}(s,a)\\gets T r u n c_{\\left[0,\\frac{1}{1-\\gamma}\\right]}Q_{k}^{r}(s,a)$   \n24: for all $s,a\\in{\\mathcal{S}}\\times{\\mathcal{A}}$ do   \n25: $\\pi_{k+1}(a|s)\\leftarrow\\left\\{\\!\\!\\!\\begin{array}{l l}{\\pi_{k+1}(a|s)}&{\\mathrm{if~}s\\in\\operatorname{Cov}(\\mathcal{C}_{\\ell+1})}\\\\ {\\pi_{k}(a|s)\\frac{\\exp(\\eta_{\\mathrm{t}}\\tilde{Q}_{k}^{r}(s,a))}{\\sum_{a^{\\prime}\\in\\mathcal{A}}\\pi_{k}(a^{\\prime}|s)\\exp(\\eta_{\\mathrm{t}}\\tilde{Q}_{k}^{r}(s,a^{\\prime}))}}&{\\mathrm{otherwise}}\\end{array}\\!\\!\\right.$   \n26: for $z\\in{\\mathcal{C}}_{\\ell}$ s.t. $z\\not\\in\\mathcal{C}_{\\ell+1}$ do   \n27: Append $z$ to $\\mathcal{C}_{\\ell+1}$ ; $D_{\\ell+1}[z]\\gets\\perp$ ", "page_idx": 12}, {"type": "text", "text": "A.1 The Gather-data subroutine ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Given a core set $\\mathcal{C}$ , a behaviour policy $\\mu$ , a starting state-action pair $(s,a)\\,\\in\\,S\\times A$ along with some algorithmic parameters, the Gather-data subroutine (algorithm 3) will either 1) return a newly ", "page_idx": 12}, {"type": "text", "text": "Input: policy $\\pi$ , core set $\\mathcal{C}$ , regression parameter $\\begin{array}{r}{\\alpha\\,=\\,\\frac{\\epsilon^{2}(1-\\gamma)^{2}}{25B^{2}(1+U)}}\\end{array}$ 25B2(1+U), H = $\\begin{array}{r}{H\\;=\\;\\frac{\\ln(4/(\\epsilon(1-\\gamma)))}{1-\\gamma}}\\end{array}$ ln(4/(\u03f5(1\u2212\u03b3))), n =   \n$\\frac{(1{+}\\rho_{0})^{2}\\ln\\left(\\frac{8\\tilde{d}L}{\\delta}\\right)}{2\\epsilon^{2}(1{-}\\gamma)^{2}}$ , starting state and action $(s,a)$   \nInitialize: Trajectories \u2190()   \n1: if $s\\not\\in\\operatorname{Cov}(\\mathcal{C})$ then   \n2: for $a\\in A$ do   \n3: if $(s,a)\\not\\in A c t i o n C o v(C)$ then return $((s,a),T r u e)$   \n4: for $i\\in[n]$ do   \n5: $\\tau_{s,a}^{i}\\gets()$   \n6: $S_{0}^{i}\\gets s,\\;A_{0}^{i}\\gets a$   \n7: append $S_{0}^{i},A_{0}^{i}$ to $\\tau_{s,a}^{i}$   \n8: for $\\begin{array}{l}{h=0,\\dots,H-1\\,\\mathbf{do}}\\\\ {S_{h+1}^{i},R_{h+1}^{i},C_{h+1}^{i}\\leftarrow\\mathrm{simulator}(S_{h}^{i},A_{h}^{i})}\\end{array}$   \n9:   \n10: if $S_{h+1}^{i}\\not\\in\\operatorname{Cov}(\\mathcal{C})$ then   \n11: for $a\\in A$ do   \n12: if $(S_{h+1}^{i},a)\\not\\in A c t i o n C o v(C)$ then return $((S_{h+1}^{i},a),T r u e)$   \n13: $\\triangleright$ no new informative features discovered   \n14: $A_{h+1}^{i}\\sim\\pi(\\cdot|S_{h+1}^{i})$   \n15: append $R_{h+1}^{i},C_{h+1}^{i},S_{h+1}^{i},A_{h+1}^{i}$ to $\\tau_{s,a}^{i}$   \n16: append $\\tau_{s,a}^{i}$ to Trajectories   \nreturn (Trajectories, False) ", "page_idx": 13}, {"type": "text", "text": "discovered state-action pair, or 2) return a set of $n$ trajectories. Each trajectory is generated by running the behaviour policy $\\mu$ with the simulator for $H$ consecutive steps. For $i=1,\\hdots,n$ , let $\\tau_{s,a}^{i}$ denote the $i$ th trajectory starting from $s,a$ to be $\\{S_{0}^{i}=s,A_{0}^{i}=a,R_{1}^{i},C_{1}^{i},\\cdot\\cdot\\cdot\\ ,S_{H-1}^{i},A_{H-1}^{i},R_{H}^{i},C_{H}^{i},S_{H}^{i}\\}$ . tThhee en mthpei $i$ -ctahl  dmisecaonu onft etdh ec udimscuolautnivtee dr esuwamr dosf $\\begin{array}{r}{G(\\tau_{s,a}^{i})=\\sum_{h=0}^{H-1}\\gamma^{h}R_{h+1}^{i}}\\end{array}$ . For a target policy $\\pi$ , then ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\bar{q}(s,a)=\\frac{1}{n}\\sum_{i=1}^{n}\\rho(\\tau_{s,a}^{i})G(\\tau_{s,a}^{i}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\begin{array}{r}{\\rho(\\tau_{s,a}^{i})=\\Pi_{h=1}^{H-1}\\frac{\\pi(A_{h}^{i}|S_{h}^{i})}{\\mu(A_{h}^{i}|S_{h}^{i})}}\\end{array}$ is the per-trajectory importance sampling ratio. ", "page_idx": 13}, {"type": "text", "text": "For some given $\\bar{s}$ and $\\bar{a}$ , we establish the following relationship between the target policy $\\pi$ and the behavior policy $\\mu$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\pi(\\bar{a}|\\bar{s})\\propto\\mu(\\bar{a}|\\bar{s})\\exp(f(\\bar{s},\\bar{a}))\\quad\\mathrm{s.t.}\\quad\\operatorname*{sup}_{\\bar{s},\\bar{a}}|f(\\bar{s},\\bar{a})|\\leq\\frac{\\ln(1+\\rho_{0})}{2H},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $f(\\bar{s},\\bar{a}):S\\times\\mathcal{A}\\rightarrow\\mathbb{R}^{+}$ and $\\rho_{0}\\geq0$ is a given constant. By establishing the relationship stated in eq. (9), the importance sampling ratio $\\rho(\\tau_{s,a}^{i^{-}})$ can be bounded by $1+\\rho_{0}$ as this is proven in the following lemma: ", "page_idx": 13}, {"type": "text", "text": "Lemma 3 Suppose the trajectory $\\tau=(S_{0},A_{0},R_{1},S_{1},A_{1},\\cdot\\cdot\\cdot,S_{H-1},A_{H-1},R_{H})$ is sampled from a behaviour policy $\\mu$ , and $\\mu$ is related to the target policy $\\pi$ via eq. (9). The per-trajectory importance sampling ratio ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\rho(\\tau)=\\Pi_{h=1}^{H-1}\\frac{\\pi(A_{h}|S_{h})}{\\mu(A_{h}|S_{h})}\\leq1+\\rho_{0}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi(a|s)=\\mu(a|s)\\frac{\\exp(f(s,a))}{\\sum_{a^{\\prime}}\\mu(a^{\\prime}|s)\\exp(f(s,a^{\\prime}))}\\leq\\mu(a|s)\\frac{\\exp(l_{0})}{\\sum_{a^{\\prime}}\\mu(a^{\\prime}|s)\\exp(-l_{0})}}\\\\ &{\\qquad\\quad\\leq\\mu(a|s)\\exp(2l_{0}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We see tha \u03c0(a|s) t $\\frac{\\pi(a|s)}{\\mu(a|s)}\\,\\leq\\,\\exp(2l_{0})$ , and it follows that $\\begin{array}{r}{\\Pi_{h=1}^{H-1}\\frac{\\pi(A_{h}|S_{h})}{\\mu(A_{h}|S_{h})}\\le\\exp(2l_{0}H)}\\end{array}$ . By assumption, $\\begin{array}{r}{l_{0}\\leq\\frac{\\ln\\left(1+\\rho_{0}\\right)}{2H}}\\end{array}$ , then $\\begin{array}{r}{\\exp(2l_{0}H)\\leq\\exp\\left(2H\\frac{\\ln(1+\\rho_{0})}{2H}\\right)\\leq1+\\rho_{0}}\\end{array}$ . ", "page_idx": 14}, {"type": "text", "text": "Define the probability distribution $P_{\\pi,s,a}$ over trajectory $\\{S_{h},A_{h},R_{h+1}\\}_{h\\ge0}$ as follows: the initial state $S_{0}$ is set deterministically to $s$ , and the initial action $A_{0}$ is set deterministically to $a$ . For each subsequent time step $h\\geq0$ , the next state $S_{h+1}$ is sampled according to the transition probability $P(\\cdot|S_{h}^{\\bar{}},A_{h})$ , and the next action $A_{h+1}$ is sampled from the policy $\\pi(\\cdot|S_{h+1})$ . It follows that $\\mathbb{E}_{\\pi,s,a}$ denotes the expectation with respect to distribution $P_{\\pi,s,a}$ . ", "page_idx": 14}, {"type": "text", "text": "Now, we show that for all $(s,a)\\in{\\mathcal{C}}$ , $|\\bar{q}(s,a)-q_{\\pi}(s,a)|\\leq\\epsilon.$ , where $\\epsilon>0$ is a given target error. Additionally, the accuracy guarantee of $|\\bar{q}(s,a)-q_{\\pi}(s,a)|\\leq\\epsilon$ continues to holds for the extended set of policies defined in definition 2. Formally, we state the main result of this section. ", "page_idx": 14}, {"type": "text", "text": "Lemma 4 For any $s,a\\,\\in\\,S\\,\\times\\,{\\mathcal{A}},{\\mathcal{X}}\\,\\subset\\,S$ , the Gather-data subroutine will either return with $((s^{\\prime},a^{\\prime})$ , True) for some $s^{\\prime}\\notin\\mathcal{X}$ , or it will return with $(D[(s,a)],F a l s e)$ , where $D[(s,a)]$ is a set of $n$ independent trajectories generated by a behavior policy $\\mu$ starting from $(s,a)$ . When Gather-data returns False for $(s,a)$ , we assume $^{\\,I}$ ) the behavior policy $\\mu$ and target policy $\\pi$ for all the states and actions encountered in the trajectories stored in $D[(s,a)]$ satisfy eq. (9) and 2) $\\bar{q}(s,a)$ is an unbiased ecsotnismtratuec toef $\\mathbb{E}_{\\pi^{\\prime},s,a}[\\sum_{h=0}^{H-1}\\gamma^{h}R_{h+1}]$ fgo tro  aellq $\\pi^{\\prime}\\in\\Pi_{\\pi,\\mathcal{X}}$ w .i tThh eprn,o tbhaeb iilmitpy e,-weighted return $\\bar{q}(s,a)$ $D[(s,a)]$ $1-\\delta^{\\prime}$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\left|\\bar{q}(s,a)-q_{\\pi^{\\prime}}(s,a)\\right|\\le\\epsilon\\ \\ \\,f o r\\,a l l\\,\\pi^{\\prime}\\in\\Pi_{\\pi,\\chi}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof: The proof follows similar reasoning to Lemma 4.2 Weisz et al. (2022). ", "page_idx": 14}, {"type": "text", "text": "Recall $D[(s,a)]$ stores $n$ number of trajectories indexed by $i$ , where each trajectory $\\tau_{s,a}^{i}\\;=\\;$ $(S_{0}^{i}\\,=\\,s,A_{0}^{i}\\,=\\,a,R_{0}^{i},\\ldots,S_{H-1}^{i})\\,\\sim\\,\\mu$ . The per-trajectory importance sampling ratio $\\rho(\\tau_{s,a}^{i})=$ $\\Pi_{h=1}^{H-1}\\frac{\\pi(A_{h}^{i}|S_{h}^{i})}{\\mu(A_{h}^{i}|S_{h}^{i})}$ , and the discounted cumulative return is $\\textstyle\\sum_{h=0}^{H-1}\\gamma^{h}R_{h+1}^{i}$ . By the triangle inequality, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\bar{q}(s,a)-q_{\\pi}(s,a)|=|\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\Pi_{h=0}^{H-1}\\frac{\\pi\\big(A_{h}^{i}|S_{h}^{i}\\big)}{\\mu\\big(A_{h}^{i}|S_{h}^{i}\\big)}\\displaystyle\\sum_{h=0}^{H-1}\\gamma^{h}R_{h+1}^{i}-q_{\\pi}(s,a)|}\\\\ &{\\leq|\\frac{1}{n}\\displaystyle\\sum_{i=1}^{n}\\rho(\\tau_{s,a}^{i})\\displaystyle\\sum_{h=0}^{H-1}\\gamma^{h}R_{h+1}^{i}-\\mathbb{E}_{\\pi,s,a}\\displaystyle\\sum_{h=0}^{H-1}\\gamma^{h}R_{h+1}|+|E_{\\pi,s,a}\\displaystyle\\sum_{h=0}^{H-1}\\gamma^{h}R_{h+1}-q_{\\pi}(s,a)|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The goal is to bound each of the two terms in eq. (10) by $\\frac{\\epsilon}{4}$ so that the sum of the two is $\\frac{\\epsilon}{2}$ . ", "page_idx": 14}, {"type": "text", "text": "By assumption, the policies $\\pi$ and $\\mu$ satisfies eq. (9) for all state-action pairs $(S_{h}^{i},A_{h}^{i})$ extracted from the $i$ -trajectory $\\tau_{s,a}^{i}$ . Second, $\\bar{q}(s,a)$ is assumed to be an unbiased estimate of $\\mathbb{E}_{\\pi,s,a}\\left[\\sum_{h=0}^{H-1}\\gamma^{h}R_{h+1}\\right]$ . Note that for all $i\\;=\\;1,\\ldots,n$ , the importance weighted cumulative return $\\begin{array}{r}{\\rho(\\tau_{s,a}^{i})\\sum_{h=0}^{H-1}\\gamma^{h}R_{h+1}^{i}}\\end{array}$ are independent random variables, and the value of each such random variable $\\in\\left[0,\\frac{1+\\rho_{0}}{1-\\gamma}\\right]$ . This is because 1) $\\begin{array}{r}{\\sum_{h=0}^{H-1}\\gamma^{h}R_{h+1}^{i}\\leq\\frac{1}{1-\\gamma}}\\end{array}$ since the rewards take values in the range of $[0,1]$ , and 2) $\\rho(\\tau_{s,a}^{i})\\leq1+\\rho_{0}$ by lemma 3. We apply Hoeffding\u2019s inequality, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\left|\\frac{1}{n}\\sum_{i=1}^{n}\\rho(\\tau_{s,a}^{i})\\sum_{h=0}^{H-1}\\gamma^{h}R_{h+1}^{i}-\\mathbb{E}_{\\pi,s,a}\\sum_{h=0}^{H-1}\\gamma^{h}R_{h+1}\\right|>\\frac{\\epsilon}{4}\\right)\\le2\\exp\\left(-\\frac{2n\\left(\\frac{\\epsilon}{4}\\right)^{2}}{\\left(\\frac{1+\\rho_{0}}{1-\\gamma}\\right)^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then, we have with probability $1-\\delta^{\\prime}/2$ , where $\\begin{array}{r}{\\delta^{\\prime}=2\\exp\\left(-\\frac{2n\\epsilon^{2}}{16\\left(\\frac{1+\\rho_{0}}{1-\\gamma}\\right)^{2}}\\right)}\\end{array}$ , the first term in eq. (10) $\\begin{array}{r}{\\left|\\frac{1}{n}\\sum_{i=1}^{n}\\rho(\\tau_{s,a}^{i})\\sum_{h=0}^{H-1}\\gamma^{h}R_{h+1}^{i}-\\mathbb{E}_{\\pi,s,a}\\sum_{h=0}^{H-1}\\gamma^{h}R_{h+1}\\right|\\leq\\frac{\\epsilon}{4}}\\end{array}$ For the second term in eq. (10), ", "page_idx": 15}, {"type": "equation", "text": "$$\n|E_{\\pi,s,a}\\sum_{h=0}^{H-1}\\gamma^{h}R_{h+1}-q_{\\pi}(s,a)|=|\\mathbb{E}_{\\pi,s,a}\\sum_{h=H}^{\\infty}\\gamma^{h}R_{h+1}|\\leq\\frac{\\gamma^{H}}{1-\\gamma}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "By the choice of H = ln(4/1(\u22121\u03b3\u2212\u03b3)\u03f5), we have 1\u03b3\u2212H\u03b3 \u2264 \u03f54. Putting everything together, we get $|\\bar{q}(s,a)-q_{\\pi}(s,a)|\\le\\frac{\\epsilon}{2}$ . To get the final result, we need to upper bound $|q_{\\pi}(s,a)-q_{\\pi^{\\prime}}(s,a)|$ by $\\frac{\\epsilon}{2}$ , so that $|\\bar{q}(s,a)-\\bar{q}_{\\pi^{\\prime}}(\\bar{s_{,}}a)|\\leq|\\bar{q}(s,a)-q_{\\pi}(s,a)|+|q_{\\pi}(s,a)-q_{\\pi^{\\prime}}(s,a)|\\leq\\epsilon$ . ", "page_idx": 15}, {"type": "text", "text": "Recall that $\\pi$ and $\\pi^{\\prime}$ differs in distributions over states that are not in $\\mathcal{X}$ . For a trajectory $\\mathrm{\\Delta}S_{0}\\,=$ $s,A_{0}=a,S_{1},\\ldots)$ , let $T$ be the smallest positive integer such that $S_{T}\\notin\\boldsymbol{\\mathcal{X}}$ , then the distribution of the trajectory $(S_{0}\\,=\\,s,A_{0}\\,=\\,a,S_{1},...\\,,S_{T})$ are the same under $P_{\\pi,s,a}$ and $P_{\\pi^{\\prime},s,a}$ because $\\pi(\\cdot|s)={\\dot{\\pi}}^{\\prime}(\\cdot|s)$ for all $s\\in\\mathscr{X}$ . Then, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu_{\\Phi}(s,a)-q_{r}(s,a)\\Big|=\\Bigg|\\mathbb{E}_{\\pi,s}\\Bigg[\\frac{\\Gamma-1}{\\sqrt{\\pi}}\\gamma^{i}R_{t}+\\gamma^{\\pi}\\nu_{\\pi}(S_{T})\\Bigg]-\\mathbb{E}_{\\pi^{\\pi},s}\\Bigg[\\sum_{s^{\\prime}=0}^{T-1}\\gamma^{i}R_{t}+\\gamma^{\\pi}\\nu_{\\pi^{\\prime}}(S_{T})\\Bigg]\\Bigg|}\\\\ &{=\\Big|\\mathbb{E}_{\\pi^{\\pi},s}\\Bigg[\\gamma^{\\pi}\\nu_{\\pi}(S_{T})\\Big]-\\mathbb{E}_{\\pi^{\\pi},s}\\Big[\\gamma^{\\pi}\\nu_{\\pi^{\\prime}}(S_{T})\\Big]\\Bigg|}\\\\ &{=\\sum_{s^{\\prime}=0}^{T-1}P_{\\pi,s^{\\prime}}(S_{T})-\\xi^{\\pi}\\big|X_{T}-1\\alpha^{\\pi}\\big|P(S_{T})\\nu_{\\pi}(S_{T})\\tau^{\\pi}\\nu_{\\pi}(S_{T})}\\\\ &{-\\displaystyle\\sum_{s^{\\prime}=0}^{T-1}P_{\\pi^{\\prime},s^{\\prime}\\alpha}(S_{T}-\\pi^{\\prime},A T_{-1}=\\alpha^{\\gamma})P(S_{T})\\nu_{\\pi^{\\prime}}(S_{T})\\tau^{\\pi}\\nu_{\\pi^{\\prime}}(S_{T})}\\\\ &{=\\displaystyle\\sum_{s^{\\prime}=0}^{T-1}P_{\\pi,s^{\\prime}\\alpha}(S_{T-1}=\\xi^{\\pi},A T_{-1}=\\alpha^{\\gamma})P(S_{T})\\nu_{\\pi^{\\prime}}(S_{T})\\tau^{\\pi}\\nu_{\\pi}(S_{T})}\\\\ &{\\le\\displaystyle\\sum_{s^{\\prime}=0}^{T-1}\\sum_{s^{\\prime}=0}^{T}P_{\\pi^{\\prime},s^{\\prime}\\alpha^{\\gamma}}(S_{T-1}=\\sigma^{\\gamma},A T_{-1}=\\alpha^{\\gamma})P(S_{T}|\\nu_{\\pi^{\\prime}}^{\\pi},\\alpha^{\\gamma})\\gamma^{\\pi}\\left(\\pi_{\\pi^{\\prime}}(S_{T})\\right)}\\\\ &{=\\displaystyle\\frac{1}{1-\\gamma}\\sum_{s^{\\prime}=0}^{T-1}\\gamma^{\\pi}\\nu_{\\pi,s^{\\prime}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Recall $\\begin{array}{r}{P_{\\pi,s,a}(1\\leq T<H)=\\sum_{t=1}^{H-1}\\sum_{s^{\\prime}\\in\\mathcal{X},a^{\\prime}\\in A}P_{\\pi,s,a}(S_{t-1}=s^{\\prime},A_{t-1}=a^{\\prime})P(S_{T}|s^{\\prime},a^{\\prime})}\\end{array}$ , and recall $S_{0}=s,A_{0}=a$ , then by the law of total probability, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{\\pi,s,a}(S_{t}=s^{\\prime},A_{t}=a^{\\prime})}\\\\ &{=\\displaystyle\\sum_{\\stackrel{s_{1},\\ldots,s_{t-1}}{a_{1},\\ldots,a_{t-1}},}\\Pi_{i=0}^{t-1}P(S_{i+1}=s_{i+1}|S_{i}=s_{i},A_{i}=a_{i})\\left(\\Pi_{i=1}^{t}\\pi(A_{i}=a_{i}|S_{i}=s_{i})\\right)}\\\\ &{=\\displaystyle\\sum_{\\stackrel{s_{1},\\ldots,s_{t-1}}{a_{1},\\ldots,a_{t-1}}}\\Pi_{i=0}^{t-1}P(S_{i+1}=s_{i+1}|S_{i}=s_{i},A_{i}=a_{i})\\Pi_{i=1}^{t}\\frac{\\pi(A_{i}=a_{i}|S_{i}=s_{i})}{\\mu(A_{i}=a_{i}|S_{i}=s_{i})}\\mu(A_{i}=a_{i}|S_{i}=s_{i})}\\\\ &{\\overset{\\leq}\\displaystyle(1+\\rho_{0})\\sum_{\\stackrel{s_{1},\\ldots,s_{t-1}}{a_{1},\\ldots,a_{t-1}}}\\Pi_{i=0}^{t-1}P(S_{i+1}=s_{i+1}|S_{i}=s_{i},A_{i}=a_{i})\\Pi_{i=1}^{t}\\mu(A_{i}=a_{i}|S_{i}=s_{i})\\qquad\\mathrm{(11}}\\\\ &{=(1+\\rho_{0})P_{\\mu,s,a,a}(S_{t}=s^{\\prime},A_{t}=a^{\\prime}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "To get eq. (11), we use lemma 3 and that $1\\leq t\\leq H-1$ . ", "page_idx": 15}, {"type": "text", "text": "Altogether, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|q_{\\pi}(s,a)-q_{\\pi^{\\prime}}(s,a)|}\\\\ &{\\le\\displaystyle\\frac{1}{1-\\gamma}\\sum_{t=1}^{H-1}\\sum_{s^{\\prime}\\in\\mathcal{X},a^{\\prime}\\in\\mathcal{A}}(1+\\rho_{0})P_{\\mu,s,a}(S_{t-1}=s^{\\prime},A_{t-1}=a^{\\prime})P(S_{T}|s^{\\prime},a^{\\prime})}\\\\ &{+\\displaystyle\\frac{\\gamma^{H}}{1-\\gamma}}\\\\ &{\\le\\displaystyle\\frac{1+\\rho_{0}}{1-\\gamma}P_{\\mu,s,a}(1\\le T<H)+\\frac{\\epsilon}{4}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now, we bound $P_{\\mu,s,a}(1\\,\\leq\\,T\\,<\\,H)$ . For each $(s,a)\\in\\mathcal{X}$ , in the ith rollouts, let $I_{i}(s,a)$ be an indicator function, where it takes the value 1 when the event that $S_{T}\\notin\\boldsymbol{\\mathcal{X}}$ occurs during $1\\leq T<H$ . Then $\\mathbb{E}_{\\mu,s,a}[I_{i}(s,a)]=P_{\\mu,s,a}(1\\leq T<H)$ . By another Hoeffding\u2019s inequality, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|\\mathbb{E}_{\\mu,s,a}[I_{i}(s,a)]-\\frac{1}{n}\\sum_{i=1}^{n}I_{i}(s,a)|>\\frac{\\epsilon(1-\\gamma)}{4(1+\\rho_{0})}\\right)\\le2\\exp\\left(-\\frac{2n(\\epsilon(1-\\gamma)/4(1+\\rho_{0}))^{2}}{{(1)}^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, with probability $1-\\delta^{\\prime}/2$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\vert\\mathbb{E}_{\\mu,s,a}[I_{i}(s,a)]-\\frac{1}{n}\\sum_{i=1}^{n}I_{i}(s,a)\\vert\\leq\\frac{\\epsilon(1-\\gamma)}{4(1+\\rho_{0})},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "When Gather-data subroutine returns, all indicators $I_{i}(s,a)=0$ for all $(s,a)\\in\\mathcal{X}$ and $i\\in[n]$ , then we have ", "page_idx": 16}, {"type": "equation", "text": "$$\nP_{\\mu,s,a}(1\\leq T<H)\\leq\\frac{\\epsilon(1-\\gamma)}{4(1+\\rho_{0})}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Putting everything together, we have the result. ", "page_idx": 16}, {"type": "text", "text": "A.2 The LSE subroutine ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Algorithm 4 LSE ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "1: for s, $a\\in{\\mathcal{C}}$ do   \n2: for every $\\tau_{s,a}^{i}\\in D[(s,a)]$ for every $i\\in[n]$ do   \n3: extract $\\{S_{0}^{i},A_{0}^{i},R_{1}^{i},C_{1}^{i},S_{1}^{i},A_{1}^{i}\\cdot\\cdot\\cdot S_{H}^{i},A_{H}^{i}\\}$ from $\\tau_{s,a}^{i}$   \n45:: ccoommppuuttee $\\rho^{i}(s,a)\\gets\\Pi_{h=1}^{H-1}\\frac{\\pi_{k}(A_{h}^{i}|S_{h}^{i})}{\\pi_{k}^{\\prime}(A_{h}^{i}|S_{h}^{i})}$ $\\begin{array}{r}{G_{r}^{i}(s,a)\\gets\\sum_{h=0}^{H-1}\\gamma^{h}R_{h+1}^{i};G_{c}^{i}(s,a)\\gets\\sum_{h=0}^{H-1}\\gamma^{h}C_{h+1}^{i}}\\end{array}$   \n6: $\\begin{array}{r}{\\bar{q}^{r}(s,a)\\gets\\frac{1}{n}\\sum_{i=1}^{n}\\rho^{i}(s,a)G_{r}^{i}(s,a);~\\bar{q}^{c}(s,a)\\gets\\frac{1}{n}\\sum_{i=1}^{n}\\rho^{i}(s,a)G_{c}^{i}(s,a)}\\end{array}$   \n7: $\\boldsymbol{w}^{r}\\gets\\left(\\Phi_{\\mathcal{C}}^{\\top}\\Phi_{\\mathcal{C}}+\\alpha\\boldsymbol{I}\\right)^{-1}\\Phi_{\\mathcal{C}}^{\\top}\\bar{\\boldsymbol{q}}^{r};\\ \\boldsymbol{w}^{c}\\gets\\left(\\Phi_{\\mathcal{C}}^{\\top}\\Phi_{\\mathcal{C}}+\\alpha\\boldsymbol{I}\\right)^{-1}\\Phi_{\\mathcal{C}}^{\\top}\\bar{\\boldsymbol{q}}^{c}$   \n8: $Q^{r}(s,a)\\gets\\langle w^{r},\\phi(s,a)\\rangle,Q^{c}(s,a)\\gets\\langle w^{c},\\phi(s,a)\\rangle\\;\\mathrm{for}$ all $s,a$   \nreturn $Q^{r},Q^{c}$ ", "page_idx": 16}, {"type": "text", "text": "Given a core set $\\mathcal{C}$ , a set of trajectories, a behaviour policy $\\mu$ , a target policy $\\pi$ , the LSE subroutine (algorithm 4) returns a least-square estimate $Q$ of $q_{\\pi}$ . ", "page_idx": 16}, {"type": "text", "text": "If the core set $\\mathcal{C}$ is empty, we define $Q(\\cdot,\\cdot)$ to be zero. Then, for a target accuracy $\\epsilon>0$ and a uniform misspecification error $\\omega$ defined in definition 1, we have a bound on the accuracy of $\\bar{q}$ with respect to $q_{\\pi}$ as given by the next lemma. ", "page_idx": 16}, {"type": "text", "text": "Lemma 5 [Lemma 4.3 of Weisz et al. (2022)] Let $\\pi$ be a randomized policy. Let $\\mathcal{C}=\\left\\{\\left(s_{i},a_{i}\\right)\\right\\}_{i\\in[N]}$ be a set of state-action pairs of set size $N\\in\\mathbb{N}.$ . Assume for all $i\\in[N],\\,|\\bar{q}(s_{i},a_{i})-q_{\\pi}(s_{i},a_{i})|\\leq\\dot{\\epsilon}$ . Then, for all $s,a\\in S\\times A$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n|Q(s,a)-q_{\\pi}(s,a)|\\leq\\omega+\\|\\phi(s,a)\\|_{V(\\mathcal{C},\\alpha)^{-1}}\\left(\\sqrt{\\alpha}B+(\\omega+\\epsilon)\\sqrt{N}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof: By the assumption that definition 1 holds, then there exists a $w_{\\pi}\\,\\in\\,\\mathbb{R}^{d}$ , $\\|w_{\\pi}\\|_{2}\\leq B$ such that for any $(s,a)\\in S\\times A$ , $|\\phi(s,a)^{\\top}w_{\\pi}-q_{\\pi}(s,a)|\\leq\\omega$ , where $\\omega$ is the misspecification error. Let $\\begin{array}{r}{\\bar{w}_{\\pi}\\,=\\,V(C,\\alpha)^{-1}\\sum_{i\\in[N]}\\phi(s_{i},a_{i})\\phi(s_{i},a_{i})^{\\top}w_{\\pi}}\\end{array}$ . For any $(s,a)\\,\\in\\,S\\,\\times\\,A$ , recall $Q(s,a)\\,=$ $\\phi(s,a)^{\\top}w$ , where $\\begin{array}{r}{w=V(C,\\alpha)^{-1}\\sum_{i\\in[N]}\\phi(s_{i},a_{i})\\bar{q}(s_{i},a_{i})}\\end{array}$ . It follows that ", "page_idx": 17}, {"type": "text", "text": "$|Q(s,a)-q_{\\pi}(s,a)|$ $\\leq|\\phi(s,a)^{\\top}(w-\\bar{w}_{\\pi})|+|\\phi(s,a)^{\\top}(\\bar{w}_{\\pi}-w_{\\pi})|+|\\phi(s,a)^{\\top}w_{\\pi}-q_{\\pi}(s,a)|.$ ", "page_idx": 17}, {"type": "text", "text": "To bound the second term in eq. (14), we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle|\\phi(s,a)^{\\top}(\\bar{w}_{\\pi}-w_{\\pi})|\\leq||\\phi(s,a)||_{V(\\mathcal{C},\\alpha)^{-1}}||\\bar{w}_{\\pi}-w_{\\pi}||_{V(\\mathcal{C},\\alpha)}}\\\\ &{\\le||\\phi(s,a)||_{V(\\mathcal{C},\\alpha)^{-1}}||-\\alpha V(\\mathcal{C},\\alpha)^{-1}w_{\\pi}||_{V(\\mathcal{C},\\alpha)}}\\\\ &{=\\alpha||\\phi(s,a)||_{V(\\mathcal{C},\\alpha)^{-1}}||w_{\\pi}||_{V(\\mathcal{C},\\alpha)^{-1}}}\\\\ &{\\le\\alpha||\\phi(s,a)||_{V(\\mathcal{C},\\alpha)^{-1}}||w_{\\pi}||_{\\frac{1}{\\alpha}I}}\\\\ &{\\le\\alpha||\\phi(s,a)||_{V(\\mathcal{C},\\alpha)^{-1}}\\sqrt{\\frac{1}{\\alpha}}B=\\sqrt{\\alpha}B||\\phi(s,a)||_{V(\\mathcal{C},\\alpha)^{-1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let $\\alpha$ be the smallest eigenvalue of $V({\\mathcal{C}},\\alpha)$ , then by eigendecomposition, $V(\\mathcal{C},\\alpha)=Q\\Lambda Q^{\\top}\\geq$ $Q(\\alpha I)Q^{\\top}=\\alpha Q Q^{\\top}\\geq\\bar{\\alpha}I$ since $Q Q^{\\top}$ is orthonormal. This implies that $\\begin{array}{r}{V(\\mathcal{C},\\alpha)^{-1}\\leq\\frac{1}{\\alpha}I}\\end{array}$ , which leads to eq. (15). ", "page_idx": 17}, {"type": "text", "text": "Finally, we bound the first term in eq. (14). For every $i\\in[N]$ , let $\\xi_{i}=\\phi(s_{i},a_{i})^{\\top}w_{\\pi}-\\bar{q}(s_{i},a_{i})$ . Then, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\xi_{i}|=|\\bar{q}(s_{i},a_{i})-\\phi(s_{i},a_{i})^{\\top}w_{\\pi}|\\leq|\\bar{q}(s_{i},a_{i})-q_{\\pi}(s_{i},a_{i})|+|q_{\\pi}(s_{i},a_{i})-\\phi(s_{i},a_{i})^{\\top}w_{\\pi}|}\\\\ {\\leq\\epsilon+\\omega.\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{limenthet~flued~th},\\quad u\\in S_{\\mathbb{N}},\\quad u\\in\\mathcal{N}_{r}\\setminus\\mathcal{A},}\\\\ &{\\mathrm{limest},\\quad u\\in\\mathcal{N}_{r}\\cup\\left[\\mathrm{th}^{\\top}(\\Gamma_{\\theta,\\theta^{\\star}})\\!\\!:\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Putting everything together complete the proof. ", "page_idx": 17}, {"type": "text", "text": "A.3 The accuracy of least-square estimates ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Given a core set $\\mathcal{C}$ and a target policy $\\pi$ , for any $s\\,\\in\\,\\mathrm{Cov}(\\mathcal{C}),a\\,\\in\\,\\mathcal{A}$ , the feature vector $\\phi(s,a)$ satisfies $\\|\\phi(s,a)\\|_{V(\\mathcal{C},\\alpha)^{-1}}\\leq\\bar{1}$ . Then, by lemma 5, we have $|\\dot{Q}(s,a)-q_{\\pi}(s,a)|=O(\\omega+\\epsilon)$ for any $s\\in\\operatorname{Cov}(\\mathcal{C})$ . In this section, we verify whether this accuracy is maintained throughout the execution of our algorithm. ", "page_idx": 18}, {"type": "text", "text": "We note that policy improvements can only occur during a running phase $\\ell=l$ . When all $(s,a)$ pairs in $\\mathcal{C}_{\\ell}$ have their placeholder value $\\perp$ replaced by trajectories, algorithm 2 executes line 17 to line 27. During each iteration from $k_{\\ell}$ to $k_{\\ell+1}-1$ , the LSE subroutine is executed. The accuracy of $\\bar{q}_{k}$ is used to bound the estimation error in lemma 5. Therefore, we will first verify that the accuracy guarantee of $\\bar{q}_{k}(s,a)$ used in lemma 4 is indeed satisfied by the main algorithm and maintained throughout its execution. ", "page_idx": 18}, {"type": "text", "text": "Once a state-action pair is added to a core set, it remains in that core set for the duration of the algorithm. This means that any core set $\\mathcal{C}_{l}$ for $l\\in\\{0,\\ldots,L+1\\}$ can grow in size over time. When a core set $\\mathcal{C}_{l}$ is extended during a running phase $\\ell=l-1$ , the least-square estimate will need be updated based on the newly extended $\\mathcal{C}_{l}$ in running phase $\\ell=l$ , which contains newly discovered features. However, the policy is update only for states that are newly covered by the extended core set $\\mathcal{C}_{l}$ using the newly improved estimates. Meanwhile, the policy for other states that have already been updated by a prior softmax update remain unchanged. Note that after line 27 of algorithm 2 is run, the next phase\u2019s core set $\\mathcal{C}_{l+1}$ will be set to $\\mathcal{C}_{l}$ , which means that any state that was once newly covered by $\\mathcal{C}_{l}$ is no longer considered newly covered. Consequently, the policy for those states will remain unchanged throughout the rest of the algorithm\u2019s execution. By updating the policies accordingly, we arrive at the following lemma, which will be crucial in proving the accuracy guarantee of the least-squares estimators. ", "page_idx": 18}, {"type": "text", "text": "Lemma 6 For any $l~\\in~\\{0,\\ldots,L\\}$ , let $\\mathcal{C}_{l}^{p a s t}$ be any past version of $\\mathcal{C}_{l}$ and let $\\pi_{k}^{p a s t}$ for $k\\ =$ $k_{l},\\cdot\\cdot\\cdot,k_{l+1}-1$ be the corresponding policies associated with $\\mathcal{C}_{l}^{p a s t}$ . If at any later point during the execution of the algorithm, $\\pi_{k}$ is updated again, then it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\pi_{k}\\in\\Pi_{\\pi_{k},\\operatorname{Cov}({\\mathcal{C}}_{l})}\\subseteq\\Pi_{\\pi_{k}^{p a s t},\\operatorname{Cov}({\\mathcal{C}}_{l}^{p a s t})}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Additionally, for any states that have been covered by $\\mathcal{C}_{l}^{p a s t}$ , it will continue to be covered by $\\mathcal{C}_{l}$ throughout the execution of the algorithm. In other words, $s\\in\\mathrm{Cov}(\\mathcal{C}_{l}^{p a s t})\\subseteq\\mathrm{Cov}(\\mathcal{C}_{l})$ . ", "page_idx": 18}, {"type": "text", "text": "Proof: The proof follows similar logic to Lemma 4.5 of Weisz et al. (2022). Recall for matrices $A,B$ , $A\\geq B$ means that $A-B$ is positive semidefinite. ", "page_idx": 18}, {"type": "text", "text": "Because $\\mathcal{C}_{l}\\supseteq\\mathcal{C}_{l}^{\\mathrm{past}}$ , there may be more rows added to $\\Phi_{\\mathcal{C}_{l}}$ than $\\Phi_{\\mathcal{C}_{l}^{\\mathrm{past}}}$ . Recall $V(\\mathcal{C}_{l},\\alpha)=\\Phi_{\\mathcal{C}_{l}}^{\\top}\\Phi_{\\mathcal{C}_{l}}\\!+\\!\\alpha I,$ and likewise for $V(\\mathcal{C}_{l}^{\\mathrm{past}})$ except $\\Phi_{\\mathcal{C}_{l}}$ is replaced by $\\mathcal{C}_{l}^{\\mathrm{past}}$ . Note that both $V(\\mathcal{C}_{l},\\alpha),V(\\mathcal{C}_{l}^{\\mathrm{past}},\\alpha)$ are dimension $d\\times d$ . Let $\\mathcal{C}_{l}^{\\prime}$ contain a set of state-action pairs that are in $\\mathcal{C}_{l}\\setminus\\mathcal{C}_{l}^{\\mathrm{past}}$ . Then, $V(\\mathcal{C}_{l},\\alpha)-$ $\\begin{array}{r}{V(\\mathcal{C}_{l}^{\\mathrm{past}},\\alpha)\\,=\\,\\sum_{(s,a)\\in\\mathcal{C}_{l}^{\\prime}}\\phi(s,a)\\phi(s,a)^{\\top}}\\end{array}$ . Since any rank-1 matrices is positive semidefinite and their sum is also positive semidefinite, it follows that $V(\\mathcal{C}_{l},\\alpha)\\geq V(\\mathcal{C}_{l}^{\\mathrm{past}},\\alpha)$ . ", "page_idx": 18}, {"type": "text", "text": "Since $V(\\mathcal{C}_{l},\\alpha)$ and $V(\\mathcal{C}^{\\mathrm{past}},\\alpha)$ are symmetric positive definite matrices, then it follows that $V(\\mathcal{C}_{l},\\alpha)^{-1}\\leq V(\\mathcal{C}_{l}^{\\mathrm{past}},\\alpha)^{-1}$ . From this, we see that for any $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ $,\\|x\\|_{V(\\mathcal{C}_{l},\\alpha)^{-1}}\\leq\\|x\\|_{V(\\mathcal{C}_{l}^{\\mathrm{past}},\\alpha)^{-1}}.$ . Then, it follows that $\\operatorname{ActionCov}(\\mathscr{C}_{l}^{\\mathrm{past}})\\ \\subseteq\\ \\operatorname{ActionCov}(\\mathscr{C}_{l})$ , and likewise $\\mathrm{Cov}(\\mathscr{C}_{l}^{\\mathrm{past}})\\,\\subseteq\\,\\mathrm{Cov}(\\mathscr{C}_{l})$ . Therefore, for an $s\\in\\mathrm{Cov}(\\mathcal{C}_{l}^{\\mathrm{past}})$ , the same state $s\\in\\mathrm{Cov}(\\mathcal{C}_{l})$ , and the second result follows. Finally, by the definition of the extended policy set definition 2, $\\begin{array}{r}{\\Pi_{\\pi_{k},\\mathrm{Cov}({\\mathcal{C}}_{l})}\\subseteq\\Pi_{\\pi_{k}^{\\mathrm{past}}}}\\end{array}$ , $\\mathrm{Cov}(\\mathcal{C}_{l}^{\\mathrm{past}})$ . \u25a0 ", "page_idx": 18}, {"type": "text", "text": "Lemma 7 For any $l\\in\\{0,\\ldots,L\\}$ and any $(s,a)\\in{\\mathcal{C}}_{l},$ , the importance-weighted $\\bar{q}_{k}(s,a)$ computed in the LSE subroutine during the running phase $\\ell=l$ is an unbiased estimator of the expected discounted reward: ", "page_idx": 18}, {"type": "equation", "text": "$$\nE_{\\pi_{k}^{\\prime},s,a}\\left[\\sum_{h=0}^{H-1}\\gamma^{h}R_{h+1}\\right]\\quad f o r\\:\\pi_{k}^{\\prime}\\in\\Pi_{\\pi_{k},\\mathrm{Cov}(\\mathcal{C}_{l})},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for iterations $k=k_{l},\\cdot\\cdot\\cdot,k_{l+1}-1$ associated with this phase. ", "page_idx": 18}, {"type": "text", "text": "Proof: For the algorithm to execute the LSE subroutine, every $(s,a)\\in{\\mathcal{C}}_{l}$ must have its placeholder value $\\bot$ in $D_{l}[(s,a)]$ replaced with trajectories. Trajectories are stored in $D_{l}$ only when the Gather-data subroutine returns \u201cdiscovered is False\u201d during the running phase $\\ell\\ =\\ l$ . This ensures that every state within these trajectories has passed the uncertainty test, thereby ensuring that all such states are in the cover of $\\mathcal{C}_{l}$ and will remain in the cover of $\\mathcal{C}_{l}$ for the duration of the algorithm, as established in lemma 6. Additionally, once trajectories for a $(s,a)\\,\\in\\,{\\mathcal{C}}_{l}$ are stored in $D_{l}$ , they remain unchanged throughout the algorithm\u2019s execution. We aim to show that $\\bar{q}_{k}(s,a)$ computed using $D_{l}[(s,a)]$ , is an unbiased estimate of the stated quantity for all iterations associated with the phase. This will be established through the following inductive arguments. ", "page_idx": 19}, {"type": "text", "text": "Base case: for a $(s,a)~\\in~{\\mathcal{C}}_{l}$ , the trajectories are generated and stored in $D_{l}[(s,a)]$ for the first time during the running phase $\\ell=l$ . ", "page_idx": 19}, {"type": "text", "text": "We let $\\tau_{s,a}^{i}$ denote the $i$ -th trajectory $(S_{0}^{i}=s,A_{0}^{i}=a,R_{1}^{i},S_{1}^{i},\\ldots,S_{H-1}^{i},A_{H-1}^{i},R_{H}^{i})$ generated by $\\pi_{k_{l}}$ interacting with the simulator, and there are $n$ such trajectories stored in $D_{l}[(s,a)]$ . Then, for all $k=k_{l},\\cdot\\cdot\\cdot,k_{l+1}-1$ , the return ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\bar{q}_{k}(s,a)=\\frac{1}{n}\\sum_{i=1}^{n}\\Pi_{h=1}^{H-1}\\frac{\\pi_{k}(A_{h}^{i}|S_{h}^{i})}{\\pi_{k_{l}}(A_{h}^{i}|S_{h}^{i})}G(\\tau_{s,a}^{i}),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\begin{array}{r}{G(\\tau_{s,a}^{i})=\\sum_{h=0}^{H-1}\\gamma^{h}R_{h+1}}\\end{array}$ . ", "page_idx": 19}, {"type": "text", "text": "The behavior policy $\\pi_{k_{l}}$ is updated in a previous loop through the algorithm when $\\ell=l-1$ . For iterations, starting with $k=k_{l}+1,\\ldots,k_{l+1}-1$ , the policy $\\pi_{k}$ is updated in iteration $k-1$ . Thus, the most recent policy $\\pi_{k}$ and the behaviour policy $\\pi_{k_{l}}$ are available for the computation of the importance sampling ratio: \u03c1k(\u03c4 si,a) = \u03a0hH=\u221211\u03c0\u03c0kkl((AAhih||SShih)). We show that the importance weighted return $\\rho_{k}(\\tau_{s,a}^{i})G(\\tau_{s,a}^{i})$ is an unbiased estimate of $\\mathbb{E}_{\\pi_{k},s,a}[G(\\tau_{s,a}^{i})]$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\mathbb{E}_{\\pi_{k_{l}},s,a}\\left[\\rho_{k}(\\tau_{s,a}^{i})\\displaystyle\\sum_{h=0}^{H-1}\\gamma^{h}R_{h+1}^{i}\\right]}\\\\ {=\\mathbb{E}_{\\pi_{k_{l}},s,a}\\left[\\frac{\\delta(s,a)P(S_{1}|S_{0}=s,A_{0}=a)\\pi_{k}(A_{1}|S_{1})\\dots\\pi_{k}(A_{H-1}|S_{H-1})}{\\delta(s,a)P(S_{1}|S_{0}=s,A_{0}=a)\\pi_{k_{l}}(A_{1}|S_{1})\\dots\\pi_{k_{l}}(A_{H-1}|S_{H-1})}\\displaystyle\\sum_{h=0}^{H-1}\\gamma^{h}R_{h+1}^{i}\\right]}\\\\ {=\\mathbb{E}_{\\pi_{k},s,a}\\left[\\displaystyle\\sum_{h=0}^{H-1}\\gamma^{h}R_{h+1}^{i}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\delta(s,a)$ is the dirac-delta function. Note, for the on-policy iteration $k=k_{l}$ , the importance sampling ratio $\\rho_{k}(\\tau_{s,a}^{i})=1$ , and the result is trivially satisfied. ", "page_idx": 19}, {"type": "text", "text": "Finally, since all the states in the trajectories are in $\\operatorname{Cov}(\\mathcal{C}_{l})$ , it follows that any policy $\\pi_{k}^{\\prime}\\ \\in$ $\\Pi_{\\pi_{k},\\mathrm{Cov}({\\mathcal{C}}_{l})}$ produces the same $\\rho_{k}\\mathopen{}\\mathclose\\bgroup\\left(\\tau_{s,a}^{i}\\aftergroup\\egroup\\right)$ . The return $\\rho_{k}(\\tau_{s,a}^{i})G(\\tau_{s,a}^{i})$ is an unbiased estimate of $E_{\\pi_{k}^{\\prime},s,a}[G(\\tau_{s,a}^{i}]$ for all $\\pi_{k}^{\\prime}\\in\\Pi_{\\pi_{k},\\operatorname{Cov}(\\mathcal{C}_{l})}.$ This is true for all $i=1,\\hdots,n$ . Consequently, $\\bar{q}_{k}(s,a)$ is an unbiased estimate of $\\begin{array}{r}{\\mathbb{E}_{\\pi_{k}^{\\prime},s,a}[\\sum_{h=0}^{H-1}R_{h+1}]}\\end{array}$ for all for all $\\pi_{k}^{\\prime}\\in\\Pi_{\\pi_{k},\\mathrm{Cov}({\\mathcal{C}}_{l})}$ . ", "page_idx": 19}, {"type": "text", "text": "The requirement that the importance weighted $\\bar{q}_{k}$ be unbiased for all policies $\\pi_{k}^{\\prime}\\in\\Pi_{\\pi_{k},\\mathrm{Cov}({\\mathcal{C}}_{l})}$ is important. This ensures that if $\\pi_{k}$ is to be updated in a future loop through the algorithm again, the estimates remain unbiased and unchanged. ", "page_idx": 19}, {"type": "text", "text": "Previously generated trajectories: for any $(s,a)\\in{\\mathcal{C}}_{l}$ , the trajectories have already been generated and stored in $D_{l}[(s,a)]$ during a previous loop of the algorithm when $\\ell=l$ . ", "page_idx": 19}, {"type": "text", "text": "Let $D_{l}^{\\mathrm{past}}[(s,a)]$ denote a past snapshot of the data stored for a $(s,a)\\,\\in\\,\\mathcal{C}_{l}^{\\mathrm{past}}$ . Let $\\pi_{k}^{\\mathrm{past}}$ for $k=$ $k_{l},\\cdot\\cdot\\cdot k_{l+1}-1$ denote the policies associated with $\\mathcal{C}_{l}^{\\mathrm{past}}$ after line 27 has been run. Finally, let $\\tau_{s,a}^{i,\\mathrm{p}\\dot{}}$ ast denote the $i$ -th trajectory stored in $D_{l}^{\\mathrm{past}}[(s,a)]$ . ", "page_idx": 19}, {"type": "text", "text": "Assume the importance weighted return $\\rho_{k}(\\tau_{s,a}^{i,\\mathrm{past}})G(\\tau_{s,a}^{i,\\mathrm{past}})$ is an unbiased estimate of $\\mathbb{E}_{\\tilde{\\pi}_{k},s,a}[G(\\tau_{s,a}^{i,\\mathrm{past}})]$ for all $\\tilde{\\pi}_{k}\\in\\Pi_{\\pi_{k}^{\\mathrm{past}},\\operatorname{Cov}(\\mathcal{C}_{l}^{\\mathrm{past}})}$ for $k=k_{l},\\ldots,k_{l+1}-1$ . When the algorithm executes a loop with $\\ell=l$ again, by lemmka 6, thel most recent policy $\\pi_{k}\\,\\in\\,\\Pi_{\\pi_{k},\\mathrm{Cov}({\\mathcal C}_{l})}\\,\\subseteq\\,\\Pi_{\\pi_{k}^{\\mathrm{past}},\\mathrm{Cov}({\\mathcal C}_{l}^{\\mathrm{past}})}$ , then $\\rho_{k}(\\tau_{s,a}^{i,\\mathrm{past}})G(\\tau_{s,a}^{i,\\mathrm{past}})$ is also an unbiased estimate of $\\mathbb{E}_{\\pi_{k}^{\\prime},s,a}[G(\\tau_{s,a}^{i,\\mathrm{past}})]$ for all $\\pi_{k}^{\\prime}\\in\\Pi_{\\pi_{k},\\mathrm{Cov}({\\mathcal{C}}_{l})}$ . ", "page_idx": 19}, {"type": "text", "text": "Once $D_{l}^{\\mathrm{past}}[(s,a)]$ is populated with trajectories, $D_{l}^{\\mathrm{past}}[(s,a)]$ remain unchanged throughout the execution of the algorithm. Therefore, $G(\\tau_{s,a}^{i})=G(\\tau_{s,a}^{i,\\mathrm{past}})$ . Since all the states in the trajectories are in $\\mathrm{Cov}(\\mathcal{C}_{l}^{\\mathrm{past}})\\subseteq\\mathrm{Cov}(\\mathcal{C}_{l})$ by lemma 6, any policy $\\tilde{\\pi}_{k}\\in\\Pi_{\\pi_{k}^{\\mathrm{past}},\\operatorname{Cov}(\\mathcal{C}_{l}^{\\mathrm{past}})}$ produces the same $\\rho_{k}(\\tau_{s,a}^{i})$ . Thus, we have $\\rho_{k}(\\tau_{s,a}^{i})G(\\tau_{s,a}^{i})=\\rho_{k}(\\tau_{s,a}^{i,\\mathrm{past}})G(\\tau_{s,a}^{i,\\mathrm{past}})$ . It follows that $\\rho_{k}(\\tau_{s,a}^{i})G(\\tau_{s,a}^{i})$ is an unbiased estimate of $\\mathbb{E}_{\\pi_{k}^{\\prime},s,a}[G(\\tau_{s,a}^{i})]$ for all $\\pi_{k}^{\\prime}\\in\\Pi_{\\pi_{k},\\operatorname{Cov}(\\mathcal{C}_{l})}.$ This is true for all $i=1,\\hdots,n$ . Consequently, $\\bar{q}_{k}(s,a)$ is an unbiased estimate of $\\mathbb{E}_{\\pi_{k}^{\\prime},s,a}[\\sum_{h=0}^{H-1}R_{h+1}]$ for all $\\pi_{k}^{\\prime}\\in\\Pi_{\\pi_{k},\\mathrm{Cov}({\\mathcal{C}}_{l})}$ . \u25a0 ", "page_idx": 20}, {"type": "text", "text": "Lemma 8 Whenever the LSE-subroutine of Confident-NPG is executed during a running phase $\\ell=l$ for $l\\in\\{0,\\ldots,L\\}$ , the behaviour policy $\\pi_{k_{\\ell}}(\\cdot|s)$ and target policy $\\pi_{k}(\\cdot|s)$ for $k=k_{\\ell},\\ldots,k_{\\ell+1}-1$ satisfy eq. (9) for any $s\\in\\mathrm{Cov}(\\mathcal{C}_{\\ell})$ . ", "page_idx": 20}, {"type": "text", "text": "Proof: Recall that the behavior policy $\\pi_{k_{l}}$ is updated during a previous loop of the algorithm when $\\ell=l-1$ . By the time the LSE subroutine is executed, $\\pi_{k_{\\ell}}$ will be the policy that generated the data. Therefore, for the on-policy iteration where $k=k_{\\ell}$ , eq. (9) is trivially satisfied. ", "page_idx": 20}, {"type": "text", "text": "For subsequent iterations, starting with $k\\;=\\;k_{\\ell}\\,+\\,1,\\ldots,k_{\\ell+1}\\,-\\,1$ , for any $s~\\in~\\mathrm{Cov}(\\mathcal{C}_{\\ell})$ , the policy $\\pi_{k}(\\bar{\\cdot}|s)$ will have either performed a softmax update for the first time or remain unchanged from a previous softmax update based on an earlier least-square estimate. Either way, for any $s\\,\\in\\,\\mathrm{Cov}(\\mathcal{C}_{\\ell})$ , the target policy $\\pi_{k}$ and behaviour policy $\\pi_{k_{\\ell}}$ relate to each other in the form of $\\begin{array}{r}{\\pi_{k}(\\cdot|s)\\propto\\pi_{k_{\\ell}}(\\cdot|s)\\exp(\\eta_{1}\\sum_{t=k_{\\ell}}^{k-1}\\tilde{Q}_{t}(s,a))}\\end{array}$ . Since $\\begin{array}{r}{\\tilde{Q}_{t}(s,a)\\in[0,\\frac{1}{1-\\gamma}]}\\end{array}$ for any $t=k_{\\ell},\\ldots,k-1$ , then it follows that ", "page_idx": 20}, {"type": "equation", "text": "$$\n0\\leq\\eta_{1}\\sum_{t=k_{\\ell}}^{k-1}\\tilde{Q}_{t}(s,a)\\leq\\eta_{1}(k-k_{\\ell})\\frac{1}{1-\\gamma}\\leq\\frac{\\eta_{1}((\\lfloor m\\rfloor+1)-1)}{1-\\gamma}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By choosing $\\begin{array}{r}{\\eta_{1}=(1-\\gamma)\\sqrt{\\frac{2\\ln(|A|)}{K}},H=\\frac{\\ln(4/\\epsilon(1-\\gamma))}{1-\\gamma},m=\\frac{\\ln(1+\\rho_{0})}{2H\\epsilon(1-\\gamma)^{2}}.}\\end{array}$ 2H\u03f5(1\u2212\u03b30)2 , and K = $\\begin{array}{r}{K=\\frac{2\\ln(A)}{(1-\\gamma)^{4}\\epsilon^{2}}}\\end{array}$ (1\u2212\u03b3)4\u03f52 , we have $\\begin{array}{r}{\\frac{\\eta_{1}((\\lfloor m\\rfloor+1)-1)}{1-\\gamma}\\leq\\frac{\\eta_{1}m}{1-\\gamma}=\\frac{\\ln(1+\\rho_{0})}{2H}}\\end{array}$ . Then it follows that eq. (9) is satisfied. ", "page_idx": 20}, {"type": "text", "text": "Lemma 9 For any $l\\in\\{0,\\ldots,L\\}$ and any $(s,a)\\in{\\mathcal{C}}_{l},$ , the importance-weighted $\\bar{q}_{k}(s,a)$ computed in the LSE subroutine during the running phase $\\ell=l$ satisfies the following with probability $1-\\delta^{\\prime}$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\bar{q}_{k}(s,a)-q_{\\pi_{k}^{\\prime}}(s,a)|\\le\\epsilon\\quad f o r\\,\\pi_{k}^{\\prime}\\in\\Pi_{\\pi_{k},\\mathrm{Cov}({\\mathcal C}_{l})}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for all iterations $k=k_{l},\\ldots,k_{l+1}-1$ associated with this phase. ", "page_idx": 20}, {"type": "text", "text": "Proof: We apply lemma 4 to each $(s,a)\\in{\\mathcal{C}}_{l}$ . To ensure the applicability of the lemma, we verify its two conditions: 1) the policies satisfy eq. (9) and 2) the estimate are unbiased. ", "page_idx": 20}, {"type": "text", "text": "We note that when the LSE-subroutine is executed during a running phase with $\\ell=l$ , the Gather-data subroutine has already completed, and the algorithm trajectories for each state-action pair $(s,a)\\in{\\mathcal{C}}_{l}$ are stored in $D_{l}[(s,\\bar{a})]$ . For any trajectory to be stored in $D_{l}$ , this means that every state within the trajectories has passed the uncertainty test, ensuring that all such states are in the cover of $\\mathcal{C}_{l}$ . By lemma 6, these states will continue to be covered by $\\mathcal{C}_{l}$ throughout the execution of the algorithm. The implication of this is that all the states in a trajectory of $\\bar{D_{l}}[(s,a)]$ satisfy eq. (9) by lemma 8. ", "page_idx": 20}, {"type": "text", "text": "Second, by lemma 7, the importance weighted return $\\bar{q}_{k}(s,a)$ is unbiased estimate of any $\\begin{array}{r}{\\mathbb{E}_{\\pi_{k}^{\\prime},s,a}\\left[\\sum_{h=0}^{H-1}\\gamma^{h}R_{h+1}\\right]}\\end{array}$ for all $\\pi_{k}^{\\prime}\\in\\Pi_{\\pi_{k},\\operatorname{Cov}(\\mathcal{C}_{l})}$ . Altogether, by lemma 4, we can ensure eq. (16) holds. ", "page_idx": 20}, {"type": "text", "text": "Consider a past loop through the algorithm with $\\ell\\,=\\,l$ , let $\\mathcal{C}_{l}^{\\mathrm{past}}$ be the core set and $\\pi_{k}^{\\mathrm{past}}$ for $k=k_{l},\\ldots,k_{l+1}-1$ be the policies associated with $\\mathcal{C}_{l}^{\\mathrm{past}}$ after line 27 has been run. If eq. (16) holds for all $\\tilde{\\pi}_{k}\\in\\Pi_{\\pi_{k}^{\\mathrm{past}},\\mathrm{Cov}(\\mathcal{C}_{I}^{\\mathrm{past}})}$ , then the accuracy of $\\bar{q}_{k}$ will continue to hold for any future update of $\\pi_{k}$ because $\\pi_{k}\\in\\dot{\\Pi}_{\\pi_{k},\\mathrm{Cov}(\\mathcal{C}_{l})}\\subseteq\\Pi_{\\pi_{k}^{\\mathrm{past}},\\mathrm{Cov}(\\mathcal{C}_{l}^{\\mathrm{past}})}$ by lemma 6. ", "page_idx": 20}, {"type": "text", "text": "Lemma 10 (Weisz et al. (2022)) $A t$ any time during the execution of the main algorithm, for all $l\\in\\{0,\\ldots,L\\}$ , the size of each $\\mathcal{C}_{l}$ is bounded: ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\mathcal{C}_{l}|\\leq4d\\ln\\left(1+\\frac{4}{\\alpha}\\right)=\\tilde{d}=\\tilde{O}(d),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the \u03b1 is the smallest eigenvalue of $V({\\mathcal{C}},\\alpha)$ and $N$ is the radius of the Euclidean ball containing all the feature vectors. ", "page_idx": 21}, {"type": "text", "text": "Lemma 11 Whenever LSE subroutine of Confident-NPG is executed during a running phase $\\ell=l$ for $l\\in\\{0,\\ldots,L\\}$ , the least-square estimate $\\tilde{Q}_{k}(s,a)$ satisfies the following condition for all iterations $k=k_{\\ell},\\cdot\\cdot\\cdot\\,,k_{\\ell+1}-1$ associated with this phase and for all $s\\in\\mathrm{Cov}(C_{\\ell})$ and $a\\in{\\mathcal{A}}$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n|\\tilde{Q}_{k}(s,a)-q_{\\pi_{k}^{\\prime}}(s,a)|\\leq\\epsilon^{\\prime}\\ \\ \\,f o r\\,a l l\\,\\pi_{k}^{\\prime}\\in\\Pi_{\\pi_{k},\\mathrm{Cov}({\\mathcal C}_{\\ell})},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\epsilon^{\\prime}=\\omega+\\sqrt{\\alpha}B+(\\omega+\\epsilon)\\sqrt{\\tilde{d}}.$ . ", "page_idx": 21}, {"type": "text", "text": "Proof: We prove the result by induction similar to Lemma F.1 of Weisz et al. (2022). We let $\\mathcal{C}_{l}^{-},\\pi_{k}^{-},\\tilde{Q}_{k}^{-}$ to denote the value of variable $\\mathcal{C}_{l},\\pi_{k},\\tilde{Q}_{k}$ at the time when line 17 to line 27 were most recently executed with $\\ell=l$ in a previous loop through the algorithm. If such time does not exist, we let their values be the initialization values. Only after the execution of line line 27 will $\\mathcal{C}_{l}^{-}$ change and as well as $\\mathcal{C}_{l+1}$ , and this is the only time that $\\mathcal{C}_{l+1}$ can be changed. Therefore, at the start of a new loop, we see that $\\mathcal{C}_{l+1}=\\mathcal{C}_{l}^{-}$ . This also holds at the initialization of the algorithm, we conclude that at the start of each loop, $\\mathrm{Cov}(\\mathcal{C}_{l+1})=\\mathrm{Cov}(\\mathcal{C}_{l}^{-})$ . ", "page_idx": 21}, {"type": "text", "text": "At initialization, $\\tilde{Q}_{k}=0$ for any $k\\in\\{0,\\ldots,K\\}$ and $C_{l}=()$ for all $l\\in\\{0,\\ldots,L\\}$ . By applying lemma 5 (Lemma 4.3 of Weisz et al. (2022)), for any $(s,a)\\in S\\times A$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n|\\tilde{Q}_{k}(s,a)-q_{\\pi_{k}^{\\prime}}(s,a)|\\leq\\omega+\\sqrt{\\alpha}B\\leq\\epsilon^{\\prime},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which satisfies eq. (17). ", "page_idx": 21}, {"type": "text", "text": "Next, let us consider the start of a loop after $\\ell=l$ is set and assume that the inductive hypothesis holds for the previous time line 17 to line 27 were executed with the same value of $\\ell=l$ . For any $s\\,\\in\\,\\mathrm{Cov}(\\bar{C_{l-1}^{-}})$ , policy $\\pi_{k_{l}}(\\cdot|s)$ would have already been set in a previous loop with value $l-1$ and remains unchanged in the current loop. By lemma 9, the condition of lemma 5 holds, then by lemma 5, we have for any $s\\in\\mathrm{Cov}(\\mathcal{C}_{l-1}^{-})$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n|\\tilde{Q}_{k_{l}}^{-}(s,\\cdot)-q_{\\pi_{k_{l}}^{\\prime}}(s,\\cdot)|\\leq\\omega+\\sqrt{\\alpha}B+(\\omega+\\epsilon)\\sqrt{\\tilde{d}}\\quad\\mathrm{for}\\;\\pi_{k_{l}^{\\prime}}\\in\\Pi_{\\pi_{k_{l}}^{-},\\mathrm{Cov}(\\mathcal{C}_{l-1}^{-})},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\|\\phi(s,\\cdot)\\|_{V(\\mathcal{C}_{l-1}^{-},\\alpha)^{-1}}\\leq1$ because $s\\in\\mathrm{Cov}(\\mathcal{C}_{l-1}^{-})$ and $|C_{l-1}^{-}|\\leq\\tilde{d}$ by lemma 10. Recall by definition, $\\tilde{Q}_{k_{l}}=\\tilde{Q}_{k_{l}}^{-},\\pi_{k_{l}}=\\pi_{k_{l}}^{-}$ , $C_{l}=C_{l-1}^{-}$ , and $\\mathrm{Cov}(\\mathcal{C}_{l})=\\mathrm{Cov}(\\mathcal{C}_{l-1}^{-})$ . It follows that for any $s\\in\\mathrm{Cov}(\\mathcal{C}_{l}),|\\tilde{Q}_{k_{l}}(s,\\cdot)-q_{\\pi_{k_{l}}^{\\prime}}(s,\\cdot)|\\leq\\epsilon^{\\prime}\\;\\mathrm{for}\\;\\pi_{k_{l}^{\\prime}}\\in\\Pi_{\\pi_{k_{l}},\\mathrm{Cov}(\\mathcal{C}_{l})}.$ ", "page_idx": 21}, {"type": "text", "text": "For any $s$ that is already covered by $\\mathcal{C}_{l}$ (i.e., $s\\,\\in\\,\\mathrm{Cov}(\\mathcal{C}_{l}^{-}))$ , and for any off-policy iteration $k=$ $k_{l}+1,\\cdot\\cdot\\cdot\\;,k_{l+1}-1,\\tilde{Q}_{k}(s,\\cdot)=\\tilde{Q}_{k}^{-}(s,\\cdot)$ . Additionally, the policy $\\pi_{k}(\\cdot|s)$ would already have been set in a previous running loop with the same value of $l$ and remains unchanged in the current loop. For $s\\in\\mathrm{Cov}(\\mathcal{C}_{l}^{-})$ , by lemma 9, the condition of lemma 5 holds, and then by lemma 5, ", "page_idx": 21}, {"type": "equation", "text": "$$\n|\\tilde{Q}_{k}^{-}(s,\\cdot)-q_{\\pi_{k}^{\\prime}}(s,\\cdot)|\\leq\\omega+\\sqrt{\\alpha}B+(\\omega+\\epsilon)\\sqrt{\\tilde{d}}\\quad\\mathrm{for}\\;\\pi_{k^{\\prime}}\\in\\Pi_{\\pi_{k}^{-},\\mathrm{Cov}(\\mathcal{C}_{l}^{-})},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\|\\phi(s,\\cdot)\\|_{V(\\mathcal{C}_{l}^{-},\\alpha)^{-1}}\\leq1$ because $s\\in\\mathrm{Cov}(\\mathcal{C}_{l}^{-})$ and $|\\mathcal{C}_{l}^{-}|\\leq\\sqrt{\\tilde{d}}$ by lemma 10. By lemma 6, $\\Pi_{\\pi_{k},\\mathrm{Cov}({\\mathcal{C}}_{l})}\\subseteq\\Pi_{\\pi_{k}^{-},\\mathrm{Cov}({\\mathcal{C}}_{l}^{-})}$ . By definition, $\\tilde{Q}_{k}(s,\\cdot)=\\tilde{Q}_{k}^{-}(s,\\cdot)$ for $s\\,\\in\\,\\mathrm{Cov}(\\mathcal{C}_{l+1})\\,=\\,\\mathrm{Cov}(\\mathcal{C}_{l}^{-})$ , $|\\tilde{Q}_{k}(s,\\cdot)-q_{\\pi_{k}^{\\prime}}(s,\\cdot)|\\leq\\epsilon^{\\prime}$ for any $\\pi_{k}^{\\prime}\\in\\Pi_{\\pi_{k},\\mathrm{Cov}(C_{l+1})}$ . ", "page_idx": 21}, {"type": "text", "text": "Finally, for any $s$ that is newly covered by $\\mathcal{C}_{l}$ (i.e., $s\\not\\in\\mathrm{Cov}(\\mathcal{C}_{l+1}))$ , and for all $k=k_{l},\\ldots,k_{l+1}-1$ , $\\tilde{Q}_{k}(s,\\cdot)=Q_{k}(s,\\cdot)$ . By lemma 9, the condition of lemma 5 holds, and then by lemma 5, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n|Q_{k}(s,\\cdot)-q_{\\pi_{k}^{\\prime}}(s,\\cdot)|\\leq\\omega+\\sqrt{\\alpha}B+(\\omega+\\epsilon)\\sqrt{\\tilde{d}}\\quad\\mathrm{for}\\;\\pi_{k^{\\prime}}\\in\\Pi_{\\pi_{k},\\mathrm{Cov}(\\mathcal{C}_{l})},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\|\\phi(s,\\cdot)\\|_{V(\\mathcal{C}_{l},\\alpha)^{-1}}\\leq1$ and $|\\mathcal{C}_{l}|\\leq\\tilde{d}$ by lemma 10. ", "page_idx": 21}, {"type": "text", "text": "Lemma 12 For any $\\delta^{\\prime}\\in(0,1]$ , a target accuracy $\\epsilon>0$ , misspecification error $\\omega\\ge0,$ , and initial state $s_{0}$ , with probability at least $1-\\delta^{\\prime}$ , the value difference between any $\\pi\\in\\Pi_{r a n d}$ and the mixture policy $\\bar{\\pi}_{K}$ returned by Confident-NPG has the following bound: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\varphi_{\\pi}(s_{0})-v_{\\bar{\\pi}_{K}}(s_{0})\\leq\\frac{4\\epsilon^{\\prime}}{1-\\gamma}+\\frac{1}{K(1-\\gamma)}\\sum_{k=0}^{K-1}\\mathbb{E}_{s^{\\prime}\\sim d_{\\pi}(s_{0}),s^{\\prime}\\in\\mathrm{Cov}(\\mathcal{C}_{0})}\\left[\\langle\\tilde{Q}_{k}(s^{\\prime},\\cdot),\\pi(\\cdot|s^{\\prime})-\\pi_{k}(\\cdot|s^{\\prime})\\rangle\\right].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof: For any $l\\in\\{0,\\ldots,L\\}$ and for all iterations $k=k_{l},\\ldots,k_{l+1}-1$ associated with $l$ , define ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\pi_{k}^{+}(\\cdot|s)={\\binom{\\pi_{k}(\\cdot|s)}{\\pi(\\cdot|s)}}\\quad{\\mathrm{if~}}s\\in\\operatorname{Cov}({\\mathcal{C}}_{l})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then, for any $s\\in\\mathrm{Cov}(\\mathcal{C}_{l})$ , ", "page_idx": 22}, {"type": "text", "text": "$\\begin{array}{l}{{v_{\\pi}(s)-v_{\\pi_{k}}(s)=v_{\\pi}(s)-v_{\\pi_{k}^{+}}(s)+v_{\\pi_{k}^{+}}(s)-v_{\\pi_{k}}(s)}}\\\\ {{\\displaystyle=\\underbrace{\\frac{1}{1-\\gamma}\\mathbb{E}_{s^{\\prime}\\sim d_{\\pi}(s)}\\left[\\langle q_{\\pi_{k}^{+}}(s^{\\prime},\\cdot),\\pi(\\cdot|s^{\\prime})-\\pi_{k}^{+}(\\cdot|s^{\\prime})\\rangle\\right]}_{I}}}\\\\ {{\\displaystyle+\\underbrace{\\langle q_{\\pi_{k}^{+}}(s,\\cdot),\\pi_{k}^{+}(\\cdot|s)\\rangle-\\langle q_{\\pi_{k}}(s,\\cdot),\\pi_{k}(\\cdot|s)\\rangle}_{I I},}}\\end{array}$ by performance difference lemma ", "page_idx": 22}, {"type": "text", "text": "where $d_{\\pi}(s)$ is the discounted state occupancy measure induced by following $\\pi$ starting from $s$ . ", "page_idx": 22}, {"type": "text", "text": "To bound term $I I$ , we note that for any $s~\\in~\\mathrm{Cov}(\\mathcal{C}_{l})$ , we have $\\pi_{k}^{+}(\\cdot|s)\\;=\\;\\pi_{k}(\\cdot|s)$ and both $\\pi_{k},\\pi_{k}^{+}(\\cdot|s)\\;\\in\\;\\Pi_{\\pi_{k},\\mathrm{Cov}({\\mathcal C}_{l})}$ . By lemma 11, we have for any $s\\,\\in\\,\\mathrm{Cov}({\\mathcal{C}}_{l}),a\\,\\in\\,{\\mathcal{A}}$ , $|\\tilde{Q}_{k}(s,a)-$ $q_{\\pi_{k}^{\\prime}}(s,a)|\\leq\\epsilon^{\\prime}$ for any $\\bar{\\pi}_{k}^{\\prime}\\in\\Pi_{\\pi_{k},\\mathrm{Cov}(\\mathcal{C}_{l})}$ . Then, for any $s\\in\\mathrm{Cov}(\\mathcal{C}_{l}),a\\in\\mathcal{A}$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|q_{\\pi_{k}^{+}}(s,a)-q_{\\pi_{k}}(s,a)|\\le|q_{\\pi_{k}^{+}}(s,a)-\\tilde{Q}_{k}(s,a)|+|\\tilde{Q}_{k}(s,a)-q_{\\pi_{k}}(s,a)|\\le2\\epsilon^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "It follows that for any $s\\in\\mathrm{Cov}(\\mathcal{C}_{l})$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\langle q_{\\pi_{k}^{+}}(s,\\cdot),\\pi_{k}^{+}(\\cdot|s)\\rangle-\\langle q_{\\pi_{k}}(s,\\cdot),\\pi_{k}(\\cdot|s)\\rangle=\\langle\\pi_{k}(\\cdot|s),q_{\\pi_{k}^{+}}(s,\\cdot)-q_{\\pi_{k}}(s,\\cdot)\\rangle}&{}\\\\ {\\leq|\\langle\\pi_{k}(\\cdot|s),q_{\\pi_{k}^{+}}(s,\\cdot)-q_{\\pi_{k}}(s,\\cdot)\\rangle|}&{}\\\\ {\\leq\\|q_{\\pi_{k}^{+}}(s,\\cdot)-q_{\\pi_{k}}(s,\\cdot)\\|_{\\infty}\\|\\pi_{k}(\\cdot|s)\\|_{1}}&{}\\\\ {\\leq2\\epsilon^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "To bound term $I$ , we note that for any $s\\not\\in\\mathrm{Cov}(\\mathscr{C}_{l}),\\pi_{k}^{+}(\\cdot|s)=\\pi(\\cdot|s)$ and $\\pi_{k}^{+}\\in\\Pi_{\\pi_{k},\\operatorname{Cov}(\\mathcal{C}_{l})}$ , then ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{1-\\gamma}\\mathbb{E}_{s^{\\prime}\\sim\\alpha_{s}^{\\delta}\\sim\\delta}\\Big[\\big(\\pi_{\\pi_{\\pi_{\\pi}^{\\varepsilon}}^{\\delta}}(s^{\\prime},\\cdot),\\pi\\big(\\cdot|s^{\\prime})-\\pi_{\\pi_{\\pi}^{\\delta}}^{\\delta}(\\cdot|s^{\\prime})\\big)\\Big]}\\\\ &{=\\frac{1}{1-\\gamma}\\mathbb{E}_{s^{\\prime}\\sim\\alpha_{s}^{\\delta}\\sim\\delta}\\mathrm{corce}(c_{0})\\Big[\\big(\\pi_{\\pi_{\\pi_{\\pi}^{\\varepsilon}}^{\\delta}}(s^{\\prime},\\cdot),\\pi\\big(\\cdot|s^{\\prime})-\\pi_{\\pi_{\\pi}^{\\delta}}^{\\delta}(\\cdot|s^{\\prime})\\big)\\Big]}\\\\ &{+\\frac{1}{1-\\gamma}\\mathbb{E}_{s^{\\prime}\\sim\\alpha_{s}^{\\delta}\\sim\\delta}\\mathrm{corce}(c_{0})\\Big[\\big(\\pi_{\\pi_{\\pi_{\\pi}^{\\varepsilon}}^{\\delta}}(s^{\\prime},\\cdot),\\pi\\big(\\cdot|s^{\\prime})-\\pi_{\\pi_{\\pi}^{\\delta}}^{\\delta}(\\cdot|s^{\\prime})\\big)\\Big]}\\\\ &{=\\frac{1}{1-\\gamma}\\mathbb{E}_{s^{\\prime}\\sim\\alpha_{s}^{\\delta}\\sim\\delta}\\mathrm{corce}(c_{0})\\Big[\\big(\\pi_{\\pi_{\\pi_{\\pi}^{\\varepsilon}}^{\\delta}}(s^{\\prime},\\cdot),\\pi\\big(\\cdot|s^{\\prime})-\\pi_{\\pi_{\\pi}^{\\delta}}^{\\delta}(\\cdot|s^{\\prime})\\big)\\Big]}\\\\ &{=\\frac{1}{1-\\gamma}\\mathbb{E}_{s^{\\prime}\\sim\\alpha_{s}^{\\delta}\\sim\\delta}\\mathrm{corce}(c_{0})\\Big[\\big(\\pi_{\\pi_{\\pi_{\\pi}^{\\varepsilon}}^{\\delta}}(s^{\\prime},\\cdot)-\\bar{\\mathcal{Q}}_{\\Lambda}(s^{\\prime},\\cdot),\\pi\\big(\\cdot|s^{\\prime})-\\pi_{\\pi_{\\pi}^{\\delta}}^{\\delta}(\\cdot|s^{\\prime})\\big)\\Big]}\\\\ &{+\\frac{1}{1-\\gamma}\\mathbb{E}_{s^{\\prime}\\sim\\alpha_{s}^{\\delta}\\sim\\delta}\\mathrm{corce}(c_{0 \n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In summary, for any $l$ , for any $k=k_{l},\\ldots,k_{l+1}-1$ associated with $l$ , and for any $s\\in\\mathrm{Cov}(\\mathcal{C}_{l})$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\nv_{\\pi}(s)-v_{\\pi_{k}}(s)\\leq\\frac{4\\epsilon^{\\prime}}{1-\\gamma}+\\frac{1}{1-\\gamma}\\mathbb{E}_{s^{\\prime}\\sim d_{\\pi(s),s^{\\prime}\\in\\mathrm{Cov}(c_{l})}}\\left[\\langle\\tilde{Q}_{k}(s^{\\prime},\\cdot),\\pi(\\cdot|s^{\\prime})-\\pi_{k}(\\cdot|s^{\\prime})\\rangle\\right].\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Because of line 27 of algorithm 2, one can use induction to show that by the time Confident-NPG terminates, all the $\\mathcal{C}_{l}$ for $l~\\in~\\{0,\\ldots,L+1\\}$ will be equal. Therefore, the cover of $\\mathcal{C}_{l}$ for all $l\\in\\{0,\\ldots,L\\!+\\!1\\}$ are also equal. Thus, it is sufficient to only consider $\\mathcal{C}_{\\mathrm{0}}$ at the end of the algorithm. Because of line 3 of algorithm 2, $s_{0}\\in\\mathrm{Cov}(\\mathcal{C}_{0})$ . Putting everything together, the value difference can be bounded as follows, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac1K\\sum_{k=0}^{K-1}\\left(v_{\\pi}(s_{0})-v_{\\pi_{k}}(s_{0})\\right)=\\frac1K\\sum_{l=0}^{L}\\sum_{k=k_{l}}^{k_{l+1}-1}\\left(v_{\\pi}(s_{0})-v_{\\pi_{k}}(s_{0})\\right)}\\\\ {\\displaystyle\\le\\frac1K\\sum_{l=0}^{L}\\sum_{k=k_{l}}^{k_{l+1}-1}\\frac{4\\epsilon^{\\prime}}{1-\\gamma}}\\\\ {\\displaystyle+\\frac1K(1-\\gamma)\\sum_{l=0}^{L}\\sum_{k=l_{l}}^{k_{l+1}-1}\\mathbb{E}_{s^{\\prime}\\sim d\\tau(s_{0}),s^{\\prime}\\in\\mathrm{Cov}(c)}^{*}\\left[\\langle\\tilde{Q}_{k}(s^{\\prime},\\cdot),\\pi(\\cdot|s^{\\prime})-\\pi_{k}(\\cdot|s^{\\prime})\\rangle\\right]}\\\\ {\\displaystyle\\le\\frac{4\\epsilon^{\\prime}}{1-\\gamma}+\\frac1{K(1-\\gamma)}\\sum_{k=0}^{K-1}\\mathbb{E}_{s^{\\prime}\\sim d\\tau(s_{0}),s^{\\prime}\\in\\mathrm{Cov}(c_{0})}\\left[\\langle\\tilde{Q}_{k}(s^{\\prime},\\cdot),\\pi(\\cdot|s^{\\prime})-\\pi_{k}(\\cdot|s^{\\prime})\\rangle\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "B Confident-NPG-CMDP ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We include the proofs of lemmas that appear in prior works and supporting lemmas that are helpful proving the lemmas in the main section. The lemmas that appear in the main section will have the same numbering here. ", "page_idx": 23}, {"type": "text", "text": "B.1 The accuracy of least-square estimates ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Once a state-action pair is added to a core set, it remains in that core set for the duration of the algorithm. This means that any $\\mathcal{C}_{l}$ for $l\\in\\{0,\\ldots,L+1\\}$ can grow in size. When a core set $\\mathcal{C}_{l}$ is extended during a running phase $\\ell=l$ , the least-square estimates will need be updated based on the newly extended $\\mathcal{C}_{l}$ which contains newly discovered features. However, the policy is update only for states that are newly covered by the extended core set $\\mathcal{C}_{l}$ using the newly improved estimates. Note that after line 30 of algorithm 1 is run, the next phase\u2019s core set $\\mathcal{C}_{l+1}$ will be set to $\\mathcal{C}_{l}$ , which means that any state that was once newly covered by $\\mathcal{C}_{l}$ is no longer considered newly covered. Consequently, the policy for those states will remain unchanged throughout the rest of the algorithm\u2019s execution. ", "page_idx": 23}, {"type": "text", "text": "We introduce hypothetical $\\tilde{Q}_{k}^{r}$ and $\\tilde{Q}_{k}^{c}$ to reflect the value of $\\tilde{Q}_{k}^{p}$ , used in the update of $\\pi_{k+1}$ for $k=$ $k_{l},\\ldots,k_{l+1}-1$ associated with running phase $\\ell=l$ . At initialization, $\\tilde{Q}_{k}^{r}(s,a)=0,\\tilde{Q}_{k}^{c}(s,a)=0$ for all $k=0,\\ldots,K$ , $s\\in S$ and $a\\in A$ . The values are specified in the following cases when line 27 is run: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{Q}_{k}^{r}(s,a)\\gets\\left\\{\\begin{array}{l l}{\\tilde{Q}_{k}^{r}(s,a)}&{\\mathrm{if~}s\\in\\mathrm{Cov}(\\mathcal{C}_{l+1})}\\\\ {Q_{k}^{r}(s,a)}&{\\mathrm{if~}s\\in\\mathrm{Cov}(\\mathcal{C}_{l})\\setminus\\mathrm{Cov}(\\mathcal{C}_{l+1})}\\\\ {\\mathrm{initial~value~0}}&{\\mathrm{if~}s\\not\\in\\mathrm{Cov}(\\mathcal{C}_{l}),}\\end{array}\\right.}\\\\ &{\\tilde{Q}_{k}^{c}(s,a)\\gets\\left\\{\\begin{array}{l l}{\\tilde{Q}_{k}^{c}(s,a)}&{\\mathrm{if~}s\\in\\mathrm{Cov}(\\mathcal{C}_{l+1})}\\\\ {Q_{k}^{c}(s,a)}&{\\mathrm{if~}s\\in\\mathrm{Cov}(\\mathcal{C}_{l})\\setminus\\mathrm{Cov}(\\mathcal{C}_{l+1})}\\\\ {\\mathrm{initial~value~0}}&{\\mathrm{if~}s\\not\\in\\mathrm{Cov}(\\mathcal{C}_{l}),}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $Q_{k}^{r}(s,a),Q_{k}^{c}(s,a)$ are the least-square estimates using the most recently extended $\\mathcal{C}_{l}$ at that time. The dual variable $\\lambda_{k}$ is defined in line 29. Therefore, the $\\tilde{Q}_{k}^{p}(s,a)$ used in the update of policy at line 27 can be written as $\\begin{array}{r}{\\tilde{Q}_{k}^{p}(s,a)=\\mathrm{trunc}_{[0,\\frac{1}{1-\\gamma}]}\\,\\tilde{Q}_{k}^{r}(s,a)+\\tilde{\\lambda}_{k}\\,\\mathrm{trunc}_{[0,\\frac{1}{1-\\gamma}]}\\,\\tilde{Q}_{k}^{c}(s,a).}\\end{array}$ . ", "page_idx": 23}, {"type": "text", "text": "Lemma 1 Whenever LSE subroutine in line 21 of Confident-NPG-CMDP is executed during a running phase $\\ell=l$ for $l\\in\\{0,\\ldots,L\\}$ , the least-square estimate $\\tilde{Q}_{k}^{p}(s,a)$ satisfies the following condition for all iterations $k=k\\ell,\\dots,k\\ell{+}1-1$ associated with this phase and for all $s\\in\\mathrm{Cov}(\\mathcal{C}_{\\ell})$ and $a\\in A$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n|\\tilde{Q}_{k}^{p}(s,a)-q_{\\pi_{k}^{\\prime},\\lambda_{k}}^{p}(s,a)|\\le\\epsilon^{\\prime}\\ \\ \\,f o r\\,a l l\\,\\pi_{k}^{\\prime}\\in\\Pi_{\\pi_{k},\\mathrm{Cov}({\\mathcal C}_{\\ell})},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\epsilon^{\\prime}=(1+U)(\\omega+\\sqrt{\\alpha}B+(\\omega+\\epsilon)\\sqrt{\\tilde{d}})$ with $\\tilde{d}=\\tilde{O}(d)$ and $U$ is an upper bound on the optimal Lagrange multiplier. Similarly, for initial state $s_{0}$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n|\\tilde{V}_{k}^{c}(s_{0})-v_{\\pi_{k}^{\\prime}}^{c}(s_{0})|\\leq\\omega+\\sqrt{\\alpha}B+(\\omega+\\epsilon)\\sqrt{\\tilde{d}}\\;\\;\\;f o r\\,a l l\\;\\pi_{k}^{\\prime}\\in\\Pi_{\\pi_{k},\\mathrm{Cov}(\\mathcal{C}_{\\ell})}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof: By using the primal-dual approach, we have reduced the CMDP problem to an unconstrained problem with a single reward of the form $r_{\\lambda}=r+\\lambda c$ . ", "page_idx": 24}, {"type": "text", "text": "Because of line 7 have executed before entering the loop and line 30 have been executed in the previous phase $\\ell=l-1$ , the initial state $s_{0}\\in\\mathrm{Cov}(C_{\\ell})$ . If $s_{0}$ is in $\\mathrm{Cov}(\\mathcal{C}_{\\ell})$ for the first time (i.e. $s_{0}\\,\\in\\,\\mathrm{Cov}({\\mathcal{C}}_{\\ell})\\,\\setminus\\,\\mathrm{Cov}({\\mathcal{C}}_{\\ell+1})$ ), then the dual variable $\\lambda_{k}$ makes a mirror descent update in line 29 using $V_{k}^{c}(s_{0})$ at that time. After line 30 is executed, the core set for the next phase $C_{\\ell+1}=\\mathcal{C}_{\\ell}$ . This means that any states, including $s_{0}$ , that are covered by $\\mathcal{C}_{\\ell}$ are then covered by $\\mathcal{C}_{\\ell+1}$ . By lemma 6, the initial state $s_{0}$ will continue to be covered by $\\mathcal{C}_{\\ell+1}$ for the remainder of the algorithm\u2019s execution. This implies that the dual variable $\\lambda_{k}$ referenced in this lemma remains fixed at the value set when $s_{0}$ is covered by $\\mathcal{C}_{\\ell}$ for the first time and does not change thereafter for the duration of the algorithm\u2019s execution. ", "page_idx": 24}, {"type": "text", "text": "Then the proof of this lemma follows similar logic to lemma 11 in the single reward setting. The result of lemma 11 uses lemma 5. For lemma 5 to hold, lemma 9 is used to verify the conditions sufficient for lemma 5 to hold. For lemma 9 to hold, one of the requirement is that the behaviour policy $\\pi_{k_{\\ell}}$ and the target policy $\\pi_{k}$ must satisfy eq. (9). In the following paragraphs, we show that eq. (9) indeed hold with appropriate changes to the parameters of interest. Then it follows that lemma 9 holds and consequently lemma 5 holds. Once all the sufficient conditions hold, by following similar logic as in lemma 11, we have the proof. ", "page_idx": 24}, {"type": "text", "text": "Since the policies are updated with respect to ${\\tilde{Q}}^{p}$ instead of $\\tilde{Q}$ of the single-reward setting, we need to make adjustment to $\\eta_{1},H,m,K$ to ensure $\\pi_{k_{\\ell}}$ and $\\pi_{k}$ indeed satisfy eq. (9). First, note the value $\\tilde{Q}_{k}^{p}$ for $k=0,\\ldots,K$ are in the range of 0 and $\\scriptstyle{\\frac{1+U}{1-\\gamma}}$ . The upper bound value is the result of the primary reward function taking values in the range of $[0,1]$ and the dual variable taking values in the range of $[0,U]$ . The value $U$ is defined in lemma 13 for relaxed-feasibility and in lemma 15 for strict-feasibility, and it is an upper bound on the optimal dual variable (i.e., $\\lambda^{*}\\leq U$ ). By similar argument to lemma 8, we make the following changes to $\\eta_{1},H,m,K$ . We set the step size $\\begin{array}{r}{\\eta_{1}=\\frac{1-\\gamma}{1+U}\\sqrt{\\frac{2\\ln(|\\mathcal{A}|)}{K}}}\\end{array}$ 2 ln(|A|), the total number of iterations K = $\\begin{array}{r}{K\\,=\\,\\frac{6^{2}(\\sqrt{2\\ln(|A|)}+1)^{2}(1+U)^{2}}{(1-\\gamma)^{4}\\epsilon^{2}}}\\end{array}$ , and $\\begin{array}{r}{H=\\frac{\\ln((30\\sqrt{\\tilde{d}}(1+U))/((1-\\gamma)^{2}\\epsilon))}{1-\\gamma}}\\end{array}$ . Then, it follows that $\\begin{array}{r}{m=\\frac{\\left(1+U\\right)\\ln\\left(1+\\rho_{0}\\right)}{2H\\epsilon\\left(1-\\gamma\\right)^{2}}}\\end{array}$ ", "page_idx": 24}, {"type": "text", "text": "Next, from lemma 7, we have each $\\bar{q}_{k}^{r}(s,a)$ and $\\bar{q}_{k}^{c}(s,a)$ is an unbiased estimate of $\\mathbb{E}_{\\pi_{k}^{\\prime},s,a}[\\sum_{h=0}^{H-1}\\gamma^{h}R_{h+1}]$ and $\\begin{array}{r}{\\mathbb{E}_{\\pi_{k}^{\\prime},s,a}[\\sum_{h=0}^{H-1}\\gamma^{h}C_{h+1}]}\\end{array}$ respectively for all $\\pi_{k}^{\\prime}\\,\\in\\,\\Pi_{\\pi_{k},\\mathrm{Cov}({\\mathcal{C}}_{l})}$ . Let $\\begin{array}{r}{\\delta^{\\prime}=2\\exp\\left(-\\frac{2n\\left(\\frac{\\epsilon}{4}\\right)^{2}}{\\left(\\frac{\\left(1+\\rho_{0}\\right)}{1-\\gamma}\\right)^{2}}\\right)}\\end{array}$ By lemma 9, with probability $1-\\delta^{\\prime}$ , we have for any $(s,a)\\in{\\mathcal{C}}_{l}$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n|\\bar{q}_{k}^{r}(s,a)-q_{\\pi_{k}^{\\prime}}^{r}|\\leq\\epsilon,\\quad|\\bar{q}_{k}^{c}(s,a)-q_{\\pi_{k}^{\\prime}}^{c}|\\leq\\epsilon\\quad\\mathrm{for~all~}\\pi_{k}^{\\prime}\\in\\Pi_{\\pi_{k},\\mathrm{Cov}(\\mathcal{C}_{l})}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then the conditions of lemma 5 hold, and by similar argument to lemma 11 using lemma 5, we have for each $(s,a)\\in\\operatorname{Cov}(\\mathcal{C}_{l})$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\tilde{Q}_{k}^{r}(s,a)-q_{\\pi_{k}^{\\prime}}^{r}(s,a)|\\leq\\omega+\\sqrt{\\alpha}B+(\\omega+\\epsilon)\\sqrt{\\tilde{d}},}\\\\ &{|\\tilde{Q}_{k}^{c}(s,a)-q_{\\pi_{k}^{\\prime}}^{c}(s,a)|\\leq\\omega+\\sqrt{\\alpha}B+(\\omega+\\epsilon)\\sqrt{\\tilde{d}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "for all $\\pi_{k}^{\\prime}\\in\\Pi_{\\pi_{k},\\operatorname{Cov}(\\mathcal{C}_{\\ell})}.$ . Then it follows that for a given $\\lambda_{k}$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n|\\tilde{Q}_{k}^{p}(s,a)-q_{\\pi_{k}^{\\prime},\\lambda_{k}}^{p}(s,a)|=|(\\tilde{Q}_{k}^{r}(s,a)-q_{\\pi_{k}^{\\prime}}^{r}(s,a))+\\lambda_{k}(\\tilde{Q}_{k}^{c}(s,a)-q_{\\pi_{k}^{\\prime}}^{c}(s,a))|\\leq\\epsilon^{\\prime},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for all \u03c0\u2032k \u2208\u03a0\u03c0k,Cov(C\u2113).", "page_idx": 25}, {"type": "text", "text": "Finally, since $s_{0}\\,\\in\\,\\mathrm{Cov}(\\mathcal{C}_{\\ell})$ and $\\tilde{V}_{k}^{c}(s_{0})\\,=\\,\\langle\\pi_{k}^{\\prime}(\\cdot|s_{0}),\\tilde{Q}_{k}^{c}(s_{0},\\cdot)\\rangle$ , therefore $|\\tilde{V}_{k}^{c}(s_{0})-v_{\\pi_{k}}^{c}(s_{0})|=$ $|\\langle\\pi_{k}^{\\prime}(\\cdot|s_{0}),\\tilde{Q}_{k}^{c}(s_{0},\\cdot)-q_{\\pi_{k}^{\\prime}}^{c}(s_{0},\\cdot)\\rangle|\\leq\\omega+\\sqrt{\\alpha}B+(\\omega+\\epsilon)\\sqrt{\\tilde{d}}$ for all $\\pi_{k}^{\\prime}\\in\\Pi_{\\pi_{k},\\operatorname{Cov}(\\mathcal{C}_{\\ell})}$ . ", "page_idx": 25}, {"type": "text", "text": "C Relaxed-feasibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Lemma 13 [Lemma 4.1 of Jain et al. (2022)] Let $\\lambda^{*}$ be the optimal dual variable that satisfies $m i n_{\\lambda\\geq0}\\operatorname*{max}_{\\pi}v_{\\pi}^{r}(\\rho)+\\lambda(v_{\\pi}^{c}(\\rho)-b)$ . If we choose ", "page_idx": 25}, {"type": "equation", "text": "$$\nU=\\frac{2}{\\zeta(1-\\gamma)},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "then $\\lambda^{*}\\leq U$ . ", "page_idx": 25}, {"type": "text", "text": "Proof: Let $\\pi_{c}^{*}(\\rho)=\\arg\\operatorname*{max}{v_{\\pi}^{c}(\\rho)}$ , and recall that $\\zeta=v_{\\pi_{c}^{*}}^{c}(\\rho)-b>0$ , then ", "page_idx": 25}, {"type": "equation", "text": "$$\nv_{\\pi^{*}}^{r}(\\rho)=\\operatorname*{max}_{\\pi}\\operatorname*{min}_{\\lambda\\geq0}v_{\\pi}^{r}(\\rho)+\\lambda(v_{\\pi}^{c}(\\rho)-b).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By Altman (2021), ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v_{\\pi^{*}}^{r}(\\rho)=\\underset{\\lambda\\geq0}{\\operatorname*{min}}\\operatorname*{max}{v}_{\\pi}^{r}(\\rho)+\\lambda(v_{\\pi}^{c}(\\rho)-b)}\\\\ &{\\quad\\quad\\quad=\\underset{\\pi}{\\operatorname*{max}}\\,v_{\\pi}^{r}(\\rho)+\\lambda^{*}(v_{\\pi}^{c}(\\rho)-b)}\\\\ &{\\quad\\quad\\quad\\geq v_{\\pi_{c}^{*}}^{r}(\\rho)+\\lambda^{*}(v_{\\pi_{c}^{*}}^{c}(\\rho)-b)}\\\\ &{\\quad\\quad\\quad\\geq v_{\\pi_{c}^{*}}^{r}(\\rho)+\\lambda^{*}\\zeta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "After rearranging terms, we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\lambda^{*}\\leq\\frac{v_{\\pi^{*}}^{r}(\\rho)-v_{\\pi_{c}^{*}}^{r}(\\rho)}{\\zeta}\\leq\\frac{1}{\\zeta(1-\\gamma)}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By choosing $\\begin{array}{r}{U=\\frac{2}{\\zeta(1-\\gamma)}}\\end{array}$ \u03b6(12\u2212\u03b3), we have \u03bb\u2217\u2264U. ", "page_idx": 25}, {"type": "text", "text": "Definition 3 ", "text_level": 1, "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R^{p}(\\pi^{*},K)=\\displaystyle\\sum_{k=0}^{K-1}\\mathbb{E}_{s^{\\prime}\\sim d_{\\pi^{*}}(s_{0}),s^{\\prime}\\in\\mathrm{Cov}(\\mathcal{C}_{0})}\\left[\\langle\\pi^{*}(\\cdot|s^{\\prime})-\\pi_{k}(\\cdot|s^{\\prime}),\\tilde{Q}_{k}^{r}(s^{\\prime},\\cdot)+\\lambda_{k}\\tilde{Q}_{k}^{c}(s^{\\prime},\\cdot)\\rangle\\right],}\\\\ &{R^{d}(\\lambda,K)=\\displaystyle\\sum_{k=0}^{K-1}(\\lambda_{k}-\\lambda)(\\tilde{V}_{k}^{c}(s_{0})-b).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Lemma 2 Let $\\delta\\,\\in\\,(0,1]$ be the failure probability, $\\epsilon\\,>\\,0$ be the target accuracy, and $s_{0}$ be the initial state. Assuming for all $s\\,\\in\\,\\mathrm{Cov}(\\mathcal{C}_{0})$ and all $a\\in A;$ , $|\\tilde{Q}_{k}^{p}(s,a)-q_{\\pi_{k}^{\\prime},\\lambda_{k}}^{p}(s,a)|\\,\\leq\\,\\epsilon^{\\prime}$ and $|\\tilde{V}_{k}^{c}(s_{0})-v_{\\pi_{k}^{\\prime}}^{c}(s_{0})|\\le\\omega+\\sqrt{\\alpha}B+(\\omega+\\epsilon)\\sqrt{\\tilde{d}}$ for all $\\pi_{k}^{\\prime}\\in\\Pi_{\\pi_{k},\\mathrm{Cov}({\\mathcal{C}}_{0})}$ , then, with probability $1-\\delta$ , Confident-NPG-CMDP returns a mixture policy $\\bar{\\pi}_{K}$ that satisfies the following, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{v_{\\pi^{*}}^{r}(s_{0})-v_{\\pi_{K}}^{r}(s_{0})\\le\\displaystyle\\frac{5\\epsilon^{\\prime}}{1-\\gamma}+\\frac{(\\sqrt{2\\ln(|\\mathcal{A}|)}+1)(1+U)}{(1-\\gamma)^{2}\\sqrt{K}},}&\\\\ &{b-v_{\\bar{\\pi}_{K}}^{c}(s_{0})\\le[b-v_{\\bar{\\pi}_{K}}^{c}(s_{0})]_{+}\\le\\displaystyle\\frac{5\\epsilon^{\\prime}}{(1-\\gamma)(U-\\lambda^{*})}+\\frac{(\\sqrt{2\\ln(|\\mathcal{A}|)}+1)(1+U)}{(1-\\gamma)^{2}(U-\\lambda^{*})\\sqrt{K}},}&\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\begin{array}{r}{\\epsilon^{\\prime}=(1+U)(\\omega+(\\sqrt{\\alpha}B+(\\omega+\\epsilon)\\sqrt{\\tilde{d}}))}\\end{array}$ with $\\tilde{d}=\\tilde{O}(d)$ . ", "page_idx": 25}, {"type": "text", "text": "Proof: For the following result, we consider a $k\\in\\{0,\\ldots,K\\}$ with its corresponding $l\\in\\{0,\\ldots,L\\}$ .   \nAt the time of termination, all $\\mathcal{C}_{l}$ are equal. ", "page_idx": 26}, {"type": "text", "text": "To obtain a bound on the suboptimality and the constraint violation, we apply lemma 12 with $\\pi=\\pi^{*}$ of CMDP, $\\tilde{Q}_{k}^{p}=\\tilde{Q}_{k}^{r}+\\lambda_{k}\\tilde{Q}_{k}^{c}$ instead of $\\tilde{Q}_{k}$ , and lemma 1 instead of lemma 11 of the single reward setting. Then, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{K}\\displaystyle\\sum_{k=0}^{K-1}v_{\\pi^{*},\\lambda_{k}}^{p}(s_{0})-v_{\\pi_{k},\\lambda_{k}}^{p}(s_{0})}\\\\ &{\\le\\displaystyle\\frac{4\\epsilon^{\\prime}}{1-\\gamma}+\\frac{1}{K(1-\\gamma)}\\sum_{k=0}^{K-1}\\mathbb{E}_{s^{\\prime}\\sim d_{\\pi^{*}}(s_{0}),s^{\\prime}\\in\\mathrm{Cov}(\\mathcal{C}_{0})}\\left[\\langle\\tilde{Q}_{k}^{r}(s^{\\prime},\\cdot)+\\lambda_{k}\\tilde{Q}_{k}^{c}(s^{\\prime},\\cdot),\\pi^{*}(\\cdot|s^{\\prime})-\\pi_{k}(\\cdot|s^{\\prime})\\rangle\\right]}\\\\ &{=\\displaystyle\\frac{4\\epsilon^{\\prime}}{1-\\gamma}+\\frac{R^{p}(\\pi^{*},K)}{K(1-\\gamma)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By Proposition 28.6 of Lattimore and Szepesv\u00e1ri (2020), the primal regret $R^{p}(\\pi^{*},K)\\;\\;\\leq$ $\\begin{array}{r}{\\frac{1+U}{1-\\gamma}\\sqrt{2K\\ln(|A|)}}\\end{array}$ with $\\begin{array}{r}{\\eta_{1}=\\frac{1-\\gamma}{1+U}\\sqrt{\\frac{2\\ln(|\\mathcal{A}|)}{K}}}\\end{array}$ 2 ln(K|A|). Expanding eq. (18) in terms of vr, vc, we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac1K\\sum_{k=0}^{K-1}v_{\\pi^{*}}^{r}(s_{0})-v_{\\pi}^{r}(s_{0})+\\frac1K\\sum_{k=0}^{K-1}\\lambda_{k}(v_{\\pi^{*}}^{c}(s_{0})-v_{\\pi_{k}}^{c}(s_{0}))}\\\\ &{\\displaystyle\\le\\frac{4\\epsilon^{\\prime}}{1-\\gamma}+\\frac{1+U}{(1-\\gamma)^{2}}\\sqrt{\\frac{2\\ln(|A|)}{K}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Furthermore, by lemma 1, we have $|\\tilde{Q}_{k}^{c}(s,a)\\,-\\,q_{\\pi_{k}^{\\prime}}^{c}(s,a)|\\,\\le\\,\\omega+\\sqrt{\\alpha}B\\,+\\,(\\omega+\\epsilon)\\sqrt{\\tilde{d}}$ for any $s\\in\\mathrm{Cov}(\\mathcal{C}_{l})$ . Recall $\\tilde{V}_{k}^{c}(s_{0})=\\langle\\pi_{k}(\\cdot|s_{0}),\\tilde{Q}_{k}^{c}(s_{0},\\cdot)\\rangle$ , then it follows that $\\lambda_{k}(v_{\\pi_{k}}^{c}(s_{0})-\\tilde{V}_{k}^{c}(s_{0}))\\le$ $|\\lambda_{k}(v_{\\pi_{k}}^{c}(s_{0})-\\tilde{V}_{k}^{c}(s_{0}))|\\le U(\\omega+\\sqrt{\\alpha}B+(\\omega+\\epsilon)\\sqrt{\\tilde{d}})\\le\\epsilon^{\\prime}.$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac1K\\sum_{k=0}^{K-1}\\lambda_{k}\\big(v_{\\pi_{k}}^{c}(s_{0})-v_{\\pi^{*}}^{c}(s_{0})\\big)\\le\\frac1K\\sum_{k=0}^{K-1}\\lambda_{k}\\big(v_{\\pi_{k}}^{c}(s_{0})-b\\big)}\\\\ &{=\\displaystyle\\frac1K\\sum_{k=0}^{K-1}\\lambda_{k}\\big(v_{\\pi_{k}}^{c}(s_{0})-\\tilde{V}_{k}^{c}(s_{0})\\big)+\\lambda_{k}\\big(\\tilde{V}_{k}^{c}(s_{0})-b\\big)}\\\\ &{\\le\\epsilon^{\\prime}+\\displaystyle\\frac{R^{d}(0,K)}{K}}\\\\ &{\\le\\epsilon^{\\prime}+\\displaystyle\\frac{U}{(1-\\gamma)\\sqrt{K}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The update to the dual variable is a mirror descent\u221a algorithm. By Proposition 28.6 of Lattimore and Szepesv\u00e1ri (2020), the dual regret $\\begin{array}{r}{R^{d}(0,K)\\le\\frac{U\\sqrt{K}}{1-\\gamma}}\\end{array}$ with $\\begin{array}{r}{\\eta_{2}=\\frac{U(1-\\gamma)}{\\sqrt{K}}}\\end{array}$ U(\u221a1\u2212\u03b3). Altogether, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{K}\\sum_{k=0}^{K-1}v_{\\pi^{*}}^{r}(s_{0})-v_{\\pi}^{r}(s_{0})\\le\\frac{4\\epsilon^{\\prime}}{1-\\gamma}+\\frac{1+U}{(1-\\gamma)^{2}}\\sqrt{\\frac{2\\ln(|A|)}{K}}+\\epsilon^{\\prime}+\\frac{U}{(1-\\gamma)\\sqrt{K}}}\\\\ &{\\leq\\displaystyle\\frac{5\\epsilon^{\\prime}}{1-\\gamma}+\\frac{(\\sqrt{2\\ln(|A|)}+1)(1+U)}{(1-\\gamma)^{2}\\sqrt{K}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For bounding the constraint violations, we first incorporate $R^{d}(\\lambda,K)$ into eq. (19) and rearrange terms to obtain: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{K}\\displaystyle\\sum_{k=0}^{K-1}v_{*}^{\\prime}\\left(s_{0}\\right)-v_{*}^{\\prime}\\left(s_{0}\\right)+\\frac{\\lambda}{K}\\displaystyle\\sum_{k=0}^{K-1}(b-v_{*}^{\\prime}\\left(s_{0}\\right))}\\\\ &{\\leq\\displaystyle\\frac{1}{K}\\displaystyle\\sum_{k=0}^{K-1}(\\lambda_{k}-\\lambda)(v_{*}^{\\prime}\\left(s_{0}\\right)-b)+\\frac{4e^{\\prime}}{1-\\gamma}+\\frac{(1+U)\\sqrt{2}\\ln(|\\mathcal{A}|)}{(1-\\gamma)^{2}\\sqrt{K}}}\\\\ &{=\\displaystyle\\frac{1}{K}\\displaystyle\\sum_{k=0}^{K-1}(\\lambda_{k}-\\lambda)(v_{*}^{\\prime}\\left(s_{0}\\right)-\\tilde{V}_{*}^{\\prime}(s_{0}))+\\frac{1}{K}\\displaystyle\\sum_{k=0}^{K-1}(\\lambda_{k}-\\lambda)(\\tilde{V}_{k}^{\\prime}(s_{0})-b)}\\\\ &{+\\frac{4e^{\\prime}}{1-\\gamma}+\\frac{(1+U)\\sqrt{2}\\ln[(|\\mathcal{A}|)]}{(1-\\gamma)^{2}\\sqrt{K}}}\\\\ &{=e^{\\prime}+\\frac{R^{4}\\lambda}{K}+\\frac{4e^{\\prime}}{1-\\gamma}+\\frac{(1+U)\\sqrt{2}\\ln[(|\\mathcal{A}|)]}{(1-\\gamma)^{2}\\sqrt{K}}}\\\\ &{\\leq\\displaystyle\\frac{\\mathcal{E}^{\\prime}}{1-\\gamma}+\\frac{(1+U)(\\sqrt{2}\\ln[(|\\mathcal{A}|)])}{(1-\\gamma)^{2}\\sqrt{K}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "There are two constraint cases. Case one is no violation: $b-v_{\\bar{\\pi}_{K}}^{c}(s_{0})\\,\\leq\\,0$ . Then, it also holds that $b-\\triangle-v_{\\bar{\\pi}_{K}}^{c}(s_{0})\\le0$ for any $\\triangle\\ge0$ , which is what we want to show. Case two is violation: $b-v_{\\bar{\\pi}_{K}}^{c}(s_{0})>\\dot{0}$ , for which case, $\\lambda=U$ . Using notation $[x]_{+}=\\operatorname*{max}\\{x,0\\}$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{K}\\sum_{k=0}^{K-1}v_{\\pi^{*}}^{r}(s_{0})-v_{\\pi_{k}}^{r}(s_{0})+\\frac{U}{K}\\left[\\displaystyle\\sum_{k=0}^{K}b-v_{\\pi}^{c}(s_{0})\\right]_{+}}\\\\ &{\\leq\\displaystyle\\frac{5\\epsilon^{\\prime}}{1-\\gamma}+\\frac{(1+U)(\\sqrt{2\\ln(|A|)}+1)}{(1-\\gamma)^{2}\\sqrt{K}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By Lemma B.2 of Jain et al. (2022), we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n[b-v_{\\bar{\\pi}_{K}}^{c}(s_{0})]_{+}\\leq\\frac{5\\epsilon^{\\prime}}{(1-\\gamma)(U-\\lambda^{*})}+\\frac{(\\sqrt{2\\ln(|A|)}+1)(1+U)}{(1-\\gamma)^{2}(U-\\lambda^{*})\\sqrt{K}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Theorem 1 With probability $1-\\delta_{i}$ , the mixture policy $\\begin{array}{r}{\\bar{\\pi}_{K}\\,=\\,\\frac{1}{k}\\sum_{k=0}^{K-1}\\pi_{k}}\\end{array}$ returned by confidentNPG-CMDP ensures that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{v_{\\pi^{*}}^{r}(s_{0})-v_{\\bar{\\pi}_{K}}^{r}(s_{0})=\\displaystyle\\frac{5(1+U)(1+\\sqrt{\\hat{d}})}{1-\\gamma}\\omega+\\epsilon,\\ \\ \\ }\\\\ &{}&{v_{\\bar{\\pi}_{K}}^{c}(s_{0})\\geq b-\\left(\\displaystyle\\frac{5(1+U)(1+\\sqrt{\\hat{d}})}{(1-\\gamma)}\\omega+\\epsilon\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "if we choo $\\begin{array}{r}{\\varkappa e\\,n=\\frac{30^{2}(1+\\rho_{0})^{2}(1+U)^{2}\\tilde{d}}{2\\epsilon^{2}(1-\\gamma)^{4}}\\ln\\Big(\\frac{8\\tilde{d}(L+1)}{\\delta}\\Big),\\,\\alpha=\\frac{(1-\\gamma)^{2}\\epsilon^{2}}{30^{2}(1+U)^{2}B^{2}},\\,K=\\frac{6^{2}(\\sqrt{2\\ln(|A|)+1})^{2}(1+U)^{2}}{(1-\\gamma)^{4}\\epsilon^{2}},\\,}\\end{array}$ $\\begin{array}{r}{\\eta_{1}\\,=\\,\\frac{1-\\gamma}{1+U}\\sqrt{\\frac{2\\ln(|\\mathcal{A}|)}{K}}}\\end{array}$ , $\\begin{array}{r}{\\eta_{2}\\,=\\,\\frac{U(1-\\gamma)}{\\sqrt{K}}}\\end{array}$ , $\\begin{array}{r}{H\\,=\\,\\frac{\\ln((30\\sqrt{\\tilde{d}}(1+U))/((1-\\gamma)^{2}\\epsilon))}{1-\\gamma}}\\end{array}$ , $\\begin{array}{r}{m\\,=\\,\\frac{(1+U)\\ln(1+\\rho_{0})}{2\\epsilon H(1-\\gamma)^{2}}}\\end{array}$ , and $\\begin{array}{r}{U=\\frac{2}{\\zeta(1-\\gamma)}}\\end{array}$ . ", "page_idx": 27}, {"type": "text", "text": "Furthermore, the algorithm utilizes at most $\\tilde{O}(d^{2}(1~+~U)^{3}\\epsilon^{-3}(1~-~\\gamma)^{-8})$ queries in the local-access setting. ", "page_idx": 27}, {"type": "text", "text": "Proof: From lemma 2, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{v_{\\pi^{*}}^{r}(s_{0})-v_{\\bar{\\pi}_{K}}^{r}(s_{0})\\leq\\displaystyle\\frac{5\\epsilon^{\\prime}}{(1-\\gamma)}+\\frac{(\\sqrt{2\\ln(|A|)}+1)(1+U)}{(1-\\gamma)^{2}\\sqrt{K}},}&\\\\ &{\\qquad b-v_{\\bar{\\pi}_{K}}^{c}(s_{0})\\leq\\displaystyle\\frac{5\\epsilon^{\\prime}}{(1-\\gamma)(U-\\lambda^{*})}+\\frac{(\\sqrt{2\\ln(|A|)}+1)(1+U)}{(1-\\gamma)^{2}(U-\\lambda^{*})\\sqrt{K}},}&\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Let $\\begin{array}{r}{C=\\frac{1}{\\zeta(1-\\gamma)}}\\end{array}$ for a $\\zeta\\in(0,\\frac{1}{1-\\gamma}]$ . By lemma 13, we chose $U=2C$ and $\\lambda^{*}\\leq C$ . It follows that $\\begin{array}{r}{\\frac{1}{U-\\lambda^{*}}\\leq\\frac{1}{C}=\\zeta(1-\\gamma)\\leq1}\\end{array}$ , and thus the right hand side of eq. (23) is upper bounded by the right hand side of eq. (22). Recall $\\epsilon^{\\prime}=(1+U)\\left(\\omega+\\left(\\sqrt{\\alpha}B+(\\omega+\\epsilon)\\sqrt{\\tilde{d}}\\right)\\right)$ . Then, the goal is to set the parameters $H,n,K$ , and $\\alpha$ appropriately so that the $A,B$ and $C$ of the following expression, when added together, is less than $\\epsilon$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{5(1+U)(1+\\sqrt{\\hat{d}})\\omega}{1-\\gamma}+\\underbrace{\\frac{5(1+U)\\sqrt{\\alpha}B}{1-\\gamma}}_{A}+\\underbrace{\\frac{5(1+U)\\epsilon\\sqrt{\\hat{d}}}{1-\\gamma}}_{B}+\\underbrace{\\frac{(\\sqrt{2\\ln(|A|)}+1))(1+U)}{(1-\\gamma)^{2}\\sqrt{K}}}_{C}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "First, we set $n$ appropriately so that the failure probability is well controlled. The failure probability depends on the number of times Gather-data subroutine (algorithm 3) is executed. Gather-data is run for phase $0,\\dots,L$ . Each phase has at most $\\tilde{d}$ elements, and recall $\\tilde{d}$ is defined in lemma 10. Therefore, Gather-data would return success at most d\u02dc times. Altogether, Gather-data can return success at most $\\tilde{d}(L+1)$ times, each with probability of at least $1-\\delta^{\\prime}=1-\\delta/(\\tilde{d}(L+1))$ . By a union bound, Gather-data returns success in all occasions with probability $1-\\delta$ . ", "page_idx": 28}, {"type": "text", "text": "By setting $\\begin{array}{r}{H=\\frac{\\ln((30\\sqrt{\\tilde{d}}(1+U))/((1-\\gamma)^{2}\\epsilon))}{1-\\gamma}}\\end{array}$ and $\\begin{array}{r}{n=\\frac{30^{2}(1+\\rho_{0})^{2}(1+U)^{2}\\tilde{d}}{2\\epsilon^{2}(1-\\gamma)^{4}}\\ln\\left(\\frac{8\\tilde{d}(L+1)}{\\delta}\\right)}\\end{array}$ , we have for any $l\\in\\{0,\\ldots,L\\}$ , $k=k_{l},\\ldots,k_{l+1}-1$ , the $\\begin{array}{r}{|\\bar{q}_{k}^{r}(s,a)-q_{\\pi_{k}^{\\prime}}^{r}(s,a)|\\leq\\frac{4}{6}\\frac{(1-\\gamma)\\epsilon}{5(1+U)\\sqrt{\\tilde{d}}}}\\end{array}$ and $|\\bar{q}_{k}^{c}(s,a)-$ $\\begin{array}{r}{q_{\\pi_{k}^{\\prime}}^{c}(s,a)|\\le\\frac{4}{6}\\frac{(1-\\gamma)\\epsilon}{5(1+U)\\sqrt{{\\tilde{d}}}}}\\end{array}$ hold for all $\\pi_{k}^{\\prime}\\in\\Pi_{\\pi_{k},\\mathrm{Cov}({\\mathcal{C}}_{l})}$ with probability at least $1-\\delta$ . Then, this is used in the accuracy guarantee of the least-square estimate (lemma 1) and finally in the suboptimality bound of lemma 2. ", "page_idx": 28}, {"type": "text", "text": "Then, we\u221a can set $\\alpha$ of eq. (24) to be equal to $\\frac{\\epsilon}{6}$ and solve for $\\begin{array}{r}{\\alpha=\\frac{\\epsilon^{2}(1-\\gamma)^{2}}{30^{2}(1+U)^{2}B^{2}}}\\end{array}$ . Finally, by setting $\\begin{array}{r}{K=\\frac{6^{2}(\\sqrt{2\\ln(|\\mathcal{A}|)}+1)^{2}(1+U)^{2}}{(1-\\gamma)^{4}\\epsilon^{2}}}\\end{array}$ , we have $C$ of eq. (24) be less than $\\frac{\\epsilon}{6}$ . Altogether, we have the reward suboptimality satisfying eq. (20) and constraint satisfying eq. (21). ", "page_idx": 28}, {"type": "text", "text": "For the query complexity, we note that our algorithm does not query the simulator in every iteration, but at fixed intervals, which we call phases. Each phase is $m$ iterations in length. There are total of $L=\\lfloor K/(\\lfloor m\\rfloor+1)\\rfloor\\le K/m=\\tilde{O}\\left((1+U)(1-\\gamma)^{-3}\\epsilon^{-1}\\right)$ phases. In each phases, Gather-data subroutine (algorithm 3) can be run. Each time Gather-data returns success with trajectories, the subroutine would have made at most $n H$ queries. Gather-data is run for each of the elements in $\\mathcal{C}_{l}$ $\\iota,\\,l\\,\\in\\,\\{0,\\ldots,L\\}$ . By the time the algorithm terminates, all $\\mathcal{C}_{l}$ \u2019s are the same. Since there are at most $\\tilde{O}(d)$ elements in each $\\mathcal{C}_{l}$ , the algorithm will make a total of $n H(L+1)|\\mathcal{C}_{0}|$ number of queries to the simulator. Since we have $H=\\tilde{O}((1-\\gamma)^{-1})$ , $n=\\tilde{O}((1+U)^{2}d\\epsilon^{-2}(1-\\gamma)^{-4})$ and $L=\\tilde{O}((1+U)\\epsilon^{-1}(1-\\gamma)^{-3})$ , the sample complexity is $\\tilde{O}(d^{2}(1+U)^{3}(1-\\gamma)^{-8}\\epsilon^{-3})$ . \u25a0 ", "page_idx": 28}, {"type": "text", "text": "D Strict-feasibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Lemma 14 Let $\\pi_{\\triangle}^{*}$ be defined as in eq. (7) and $\\pi^{*}$ be the optimal policy of CMDP. Then, for $a$ $\\triangle>0$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\nv_{\\pi^{*}}^{r}(s_{0})-v_{\\pi_{\\triangle}^{*}}^{r}(s_{0})\\leq\\lambda^{*}\\triangle,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\lambda^{*}$ is the optimal dual variable that satisfies $\\begin{array}{r}{\\operatorname*{min}_{\\lambda\\geq0}\\operatorname*{max}_{\\pi}v_{\\pi}^{r}(s_{0})+\\lambda(v_{\\pi}^{c}(s_{0})-b^{\\prime}).}\\end{array}$ . ", "page_idx": 28}, {"type": "text", "text": "Proof: ", "page_idx": 28}, {"type": "equation", "text": "$$\nv_{\\pi_{\\widehat{\\bigcirc}}^{\\ast}}^{r}(s_{0})=\\operatorname*{max}_{\\pi}\\operatorname*{min}_{\\lambda\\geq0}v_{\\pi}^{r}(s_{0})+\\lambda\\big(v_{\\pi}^{c}(s_{0})-b^{\\prime}\\big).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By Altman (2021), ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v_{\\pi_{\\widehat{\\Delta}}^{\\star}}^{r}(s_{0})=\\underset{\\lambda\\geq0}{\\operatorname*{min}}\\operatorname*{max}v_{\\pi}^{r}(s_{0})+\\lambda(v_{\\pi}^{c}(s_{0})-b^{\\prime})}\\\\ &{\\qquad\\qquad=\\underset{\\pi}{\\operatorname*{max}}v_{\\pi}^{r}(s_{0})+\\lambda^{\\ast}(v_{\\pi}^{c}(s_{0})-b^{\\prime})}\\\\ &{\\qquad\\quad\\geq v_{\\pi^{\\star}}^{r}(s_{0})+\\lambda^{\\ast}(v_{\\pi^{\\star}}^{c}(s_{0})-(b+\\triangle))}\\\\ &{\\qquad\\quad\\geq v_{\\pi^{\\star}}^{r}(s_{0})+\\lambda^{\\ast}(b-b-\\triangle)\\quad\\mathrm{because~}v_{\\pi^{\\star}}^{c}(s_{0})\\geq b}\\\\ &{=v_{\\pi^{\\star}}^{r}(s_{0})-\\lambda^{\\ast}\\triangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "After rearranging the terms, we get the result. ", "page_idx": 29}, {"type": "text", "text": "Lemma 15 Let $\\lambda^{*}$ be the optimal dual variable that satisfies m $;n_{\\lambda\\geq0}\\operatorname*{max}_{\\pi}V_{\\pi}^{r}(s_{0})\\!+\\!\\lambda(V_{\\pi}^{c}(s_{0})\\!-\\!b^{\\prime})$ . If we choose ", "page_idx": 29}, {"type": "equation", "text": "$$\nU=\\frac{4}{\\zeta(1-\\gamma)},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "then $\\lambda^{*}\\leq U$ requiring that $\\triangle\\in(0,\\frac{\\zeta}{2})$ . ", "page_idx": 29}, {"type": "text", "text": "Proof: Let $\\pi_{c}^{*}(s_{0})=\\arg\\operatorname*{max}V_{\\pi}^{c}(s_{0})$ , and recall that $\\zeta=V_{\\pi_{c}^{*}}^{c}(s_{0})-b>0$ , then ", "page_idx": 29}, {"type": "equation", "text": "$$\nv_{\\pi_{\\widehat{\\bigcirc}}^{\\ast}}^{r}(s_{0})=\\operatorname*{max}_{\\pi}\\operatorname*{min}_{\\lambda\\geq0}v_{\\pi}^{r}(s_{0})+\\lambda(v_{\\pi}^{c}(s_{0})-b^{\\prime})\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "By Altman (2021), ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{v_{\\pi_{\\widehat{\\alpha}}^{*}}^{r}\\left(s_{0}\\right)=\\underset{\\lambda\\geq0}{\\operatorname*{min}}\\operatorname*{max}_{\\pi}v_{\\pi}^{r}(s_{0})+\\lambda(v_{\\pi}^{c}(s_{0})-b^{\\prime})}\\\\ &{\\qquad\\qquad=\\underset{\\pi}{\\operatorname*{max}}\\,v_{\\pi}^{r}(s_{0})+\\lambda^{*}(V_{\\pi}^{c}(s_{0})-b^{\\prime})}\\\\ &{\\qquad\\quad\\geq v_{\\pi_{c}^{*}}^{r}(s_{0})+\\lambda^{*}(v_{\\pi_{c}^{*}}^{c}(s_{0})-(b+\\triangle))}\\\\ &{\\qquad\\quad=v_{\\pi_{c}^{*}}^{r}(s_{0})+\\lambda^{*}(\\zeta-\\triangle).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "If we require $\\triangle\\in(0,\\frac{\\zeta}{2})$ , then we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{v_{\\pi_{\\widehat{\\triangle}}^{*}}^{r}(s_{0})\\geq v_{\\pi_{c}^{*}}^{r}(s_{0})+\\lambda^{*}(\\zeta-\\displaystyle\\frac{\\zeta}{2})}}\\\\ {{=v_{\\pi_{c}^{*}}^{r}(s_{0})+\\displaystyle\\frac{\\lambda^{*}\\zeta}{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "After rearranging terms in eq. (25), we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\lambda^{*}\\leq\\frac{2(v_{\\pi_{\\widehat{c}}^{*}}^{r}(s_{0})-v_{\\pi_{c}^{*}}^{r}(s_{0}))}{\\zeta}\\leq\\frac{2}{\\zeta(1-\\gamma)}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "By choosing $\\begin{array}{r}{U=\\frac{4}{\\zeta(1-\\gamma)}}\\end{array}$ \u03b6(14\u2212\u03b3), \u03bb\u2217\u2264U. ", "page_idx": 29}, {"type": "text", "text": "Theorem 2 With probability $1-\\delta$ , a target $\\epsilon>0$ , the mixture policy $\\bar{\\pi}_{K}$ returned by confidentNPG-CMDP ensures that $v_{\\pi^{*}}^{r}(s_{0})\\,-\\,v_{\\bar{\\pi}_{K}}^{r}(s_{0})\\,\\le\\,\\epsilon$ and ${v_{\\bar{\\pi}_{K}}^{c}}(s_{0})\\;\\geq\\;b,$ , if assuming the misspecificiation error $\\begin{array}{r}{\\omega\\ \\leq\\ \\frac{\\triangle(1-\\gamma)}{70(1+U)(1+\\sqrt{\\tilde{d}})}}\\end{array}$ , and if we choose \u25b3 = \u03f5(1\u22128\u03b3)\u03b6, $\\begin{array}{r}{\\widehat{\\triangle}^{\\prime}=\\,\\frac{\\epsilon(1-\\gamma)\\zeta}{8},\\alpha\\;=\\;\\frac{\\triangle^{2}(1-\\gamma)^{2}}{70^{2}(1+U)^{2}B^{2}},K\\;=}\\end{array}$ $\\begin{array}{r l}&{\\frac{4^{2}(\\sqrt{2\\ln(|A|)}+1)^{2}(1+U)^{2}}{(1-\\gamma)^{4}\\Delta^{2}},n\\,=\\,\\frac{(14*5)^{2}(1+\\rho_{0})^{2}\\widetilde{d}(1+U)^{2}}{2\\Delta^{2}(1-\\gamma)^{4}}\\ln\\left(\\frac{8\\widetilde{d}(L+1)}{\\delta}\\right),H\\,=\\,\\frac{\\ln\\left(\\frac{14*5(1+U)\\sqrt{d}}{\\Delta(1-\\gamma)^{2}}\\right)}{1-\\gamma},m\\,=\\,}\\\\ &{\\frac{1+U)\\ln(1+\\rho_{0})}{2\\Delta H(1-\\gamma)^{2}},U=\\frac{4}{\\zeta(1-\\gamma)}.}\\end{array}$ ", "page_idx": 29}, {"type": "text", "text": "Furthermore, the algorithm utilizes at most $\\tilde{O}(d^{2}(1+U)^{3}(1-\\gamma)^{-11}\\epsilon^{-3}\\zeta^{-3})$ queries in the localaccess setting. ", "page_idx": 29}, {"type": "text", "text": "Proof: Let $\\lambda^{*}$ be the optimal dual variable that satisfies the Lagrangian primal-dual of the surrogate CMDP defined by eq. ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(7)\\,(\\mathrm{i}.\\mathrm{e}.,\\,\\lambda^{*}=\\arg\\operatorname*{min}_{\\lambda\\ge0}\\operatorname*{max}_{\\pi}v_{\\pi}^{r}(s_{0})+\\lambda\\big(v_{\\pi}^{c}(s_{0})-b^{\\prime}\\big)).}\\\\ &{v_{\\pi^{*}}^{r}(s_{0})-v_{\\bar{\\pi}_{K}}^{r}(s_{0})}\\\\ &{\\,=\\underbrace{\\Big[v_{\\pi^{*}}^{r}(s_{0})-v_{\\pi_{\\triangle}^{*}}^{r}(s_{0})\\Big]}_{\\mathrm{surrogate~suboptimality}}+\\underbrace{\\Big[v_{\\pi_{\\triangle}}^{r}(s_{0})-v_{\\bar{\\pi}_{K}}^{r}(s_{0})\\Big]}_{\\mathrm{Confident-NPG-CMDP~suboptimality}}}\\\\ &{\\,\\le\\lambda^{*}\\triangle+\\bar{\\epsilon},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where \u03f5\u00af = $\\begin{array}{r}{\\bar{\\epsilon}\\,=\\,\\frac{5(1+U)(1+\\sqrt{\\hat{d}})\\omega}{1-\\gamma}+\\frac{5(1+U)\\sqrt{\\alpha}B}{1-\\gamma}+\\frac{5(1+U)\\epsilon\\sqrt{\\hat{d}}}{1-\\gamma}+\\frac{(\\sqrt{2\\ln(|A|)}+1)(1+U)}{(1-\\gamma)^{2}\\sqrt{K}}}\\end{array}$ . By lemma 14, $v_{\\pi^{*}}^{r}(s_{0})-v_{\\pi_{\\triangle}^{*}}^{r}(s_{0})\\leq\\lambda^{*}\\triangle$ . We can further upper bound $\\lambda^{*}$ by $\\begin{array}{r}{U=\\frac{4}{\\zeta(1-\\gamma)}}\\end{array}$ \u03b6(14\u2212\u03b3) using lemma 15 and requiring $\\triangle\\in\\left(0,{\\frac{\\zeta}{2}}\\right)$ . Together with theorem 1, we have Confident-NPG-CMDP return $\\bar{\\pi}_{K}$ s.t. ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\displaystyle v_{\\pi^{*}}^{r}(s_{0})-v_{\\bar{\\pi}_{K}}^{r}(s_{0})\\leq\\frac{4\\triangle}{\\zeta(1-\\gamma)}+\\bar{\\epsilon}}}&{{\\mathrm{and}}}\\\\ {{\\displaystyle b^{\\prime}-V_{\\bar{\\pi}_{K}}^{c}(s_{0})\\leq\\bar{\\epsilon}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Now, we need to set $\\triangle$ such that 1) $\\triangle\\in\\left(0,\\frac{\\zeta}{2}\\right)$ and 2) $\\triangle-\\bar{\\epsilon}\\ge0$ are satisfied. If we choose $\\begin{array}{r}{\\triangle=\\frac{\\epsilon(1-\\gamma)\\zeta}{8}}\\end{array}$ , then the first condition is satisfied. This is because $\\begin{array}{r}{\\epsilon\\in\\left(0,\\frac{1}{1-\\gamma}\\right]}\\end{array}$ , and thus $\\begin{array}{r}{\\triangle\\le\\frac{\\zeta}{8}<\\frac{\\zeta}{2}}\\end{array}$ . Next, we check if our choice of $\\begin{array}{r}{\\triangle=\\frac{\\epsilon(1-\\gamma)\\zeta}{8}}\\end{array}$ satisfies $\\triangle-\\bar{\\epsilon}\\ge0$ . For the condition $\\triangle-\\bar{\\epsilon}\\ge0$ to be true, we make an assumption on the misspecification error $\\begin{array}{r}{\\omega\\,\\leq\\,\\frac{\\bigtriangleup(1-\\gamma)}{70(1+U)(1+\\sqrt{\\tilde{d}})}}\\end{array}$ 70(1+U)(1+\u221ad\u02dc), and pick $n,\\alpha,K,\\eta_{1},\\eta_{2},H,m$ to be the values outlined in this theorem. Consequently, we have $\\begin{array}{r}{\\bar{\\epsilon}=\\frac{1}{2}\\bigtriangleup}\\end{array}$ . Then, we have ensured the condition $\\triangle-\\bar{\\epsilon}\\ge0$ is satisfied. ", "page_idx": 30}, {"type": "text", "text": "We note that because $\\zeta\\;\\in\\;\\left(0,\\frac{1}{1\\!-\\!\\gamma}\\right)$ , we have $\\bar{\\epsilon}\\;\\leq\\;\\frac{\\epsilon}{16}\\;\\leq\\;\\epsilon$ . following from eq. (26), we have $v_{\\pi^{*}}^{r}(s_{0})-v_{\\bar{\\pi}_{K}}^{r}(s_{0})\\,\\leq\\,\\epsilon$ and $b^{\\prime}-V_{\\bar{\\pi}_{K}}^{c}(s_{0})\\,\\leq\\,\\frac{\\triangle}{2}$ . Then it follows that $b+\\textstyle\\frac{\\triangle}{2}\\,\\leq\\,V_{\\bar{\\pi}_{K}}^{c}(s_{0})$ . Strictfeasilbility is achieved. ", "page_idx": 30}, {"type": "text", "text": "For the query complexity, we note that our algorithm does not query the simulator in every iteration, but at fixed intervals, which we call phases. Each phase is $m$ iterations in length. There are total of $L=\\lfloor K/(\\lfloor m\\rfloor+1)\\rfloor\\le K/m=\\tilde{O}\\left((1+U)(1\\stackrel{\\cdot}{-}\\gamma)^{-3}\\triangle^{-1}\\right)$ phases. In each phase, Gather-data subroutine (algorithm 3) can be run. Each time Gather-data subroutine returns with trajectories, the subroutine would have made at most $n H$ queries. Gather-data is run for each of the element in $\\mathcal{C}_{l}$ , $l\\in\\{0,\\ldots,L\\}$ . By the time the algorithm terminates, all $\\mathcal{C}_{l}$ \u2019s are the same. Since there are at most $\\tilde{O}(d)$ elements in each $\\mathcal{C}_{l}$ , the algorithm will make a total of $n H(L+1)|\\mathcal{C}_{0}|$ number of queries to the simulator. Since we have $\\bar{H}=\\tilde{O}((1-\\gamma)^{-1})$ , $n=\\tilde{O}((1+U)^{2}d(1-\\gamma)^{-4}\\triangle^{-2})$ , $L=\\tilde{O}\\left((1+U)(1-\\gamma)^{-3}\\triangle^{-1}\\right)$ , and $\\begin{array}{r}{\\triangle=\\frac{\\epsilon\\zeta(1-\\gamma)}{8}}\\end{array}$ , the sample complexity is $\\tilde{O}(d^{2}(1+U)^{3}(1-$ $\\gamma)^{-11}\\epsilon^{-3}\\zeta^{-3})$ . \u25a0 ", "page_idx": 30}, {"type": "text", "text": "E A discussion on memory cost and some implementation details ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "By recording the states added to each core set during extensions and their corresponding least-squares weights, we can reconstruct the policy as needed. This section explains how to track this information and how it facilitates policy reconstruction. ", "page_idx": 30}, {"type": "text", "text": "In phase $l$ , the policies $\\pi_{k}$ for iterations $k\\;=\\;k_{l}\\,+\\,1,\\ldots,k_{l+1}\\,-\\,1$ depend on the core set $\\mathcal{C}_{l}$ . Since $\\mathcal{C}_{l}$ can be extended multiple times, these policies may change accordingly. However, we do not want to change the action distribution for states have already passed the uncertainty test in previous extensions (i.e. $s\\,\\in\\,\\mathrm{Cov}(\\mathcal{C}_{l+1}))$ . For such states, action distributions are based on the least-square estimation of the core set at that time they passed the uncertainty test for the first time (i.e., $\\bar{s}\\in\\mathrm{Cov}(\\mathcal{C}_{l})\\setminus\\mathrm{Cov}(\\mathcal{C}_{l+1}))$ . Therefore, it is essential to track newly added states in each extension and store their corresponding least-square weights to recompute their action distributions. ", "page_idx": 30}, {"type": "text", "text": "To achieve this, $\\scriptstyle{\\mathcal{C}}_{0}$ is extended only via line 7 and line 15 of algorithm 1, while other core sets $\\mathcal{C}_{l}$ , where $l\\in\\{1,\\ldots,L+1\\}$ are extended solely via line 30 during the running phase $\\ell=l-1$ . We mark newly added elements in line 7, line 15, and line 30. After executing line 21, we store the least-square weights associated with these newly added state-action pairs. ", "page_idx": 30}, {"type": "text", "text": "By keeping track of the state-action pairs that are newly added in each extension and saving the corresponding least-square weights, we can construct the policy $\\pi_{k+1}$ associated with $\\mathcal{C}_{l}$ . Let $\\bar{\\mathcal{C}}_{l}^{0}=\\bar{\\mathcal{V}}$ , and $\\mathcal{C}_{l}^{i}$ denote all state-action pairs added to $\\mathcal{C}_{l}$ in extension $i$ for $i=1$ up to at most $\\tilde{d}$ . Let $\\boldsymbol{w_{k}^{i}}$ represent the least-square weight computed using $\\mathcal{C}_{l}=\\mathcal{C}_{l}^{0}\\cup\\mathcal{C}_{l}^{1}\\cup\\mathcal{C}_{l}^{2}\\cup\\cdots\\cup\\mathcal{C}_{l}^{i}$ for the $k$ -th iteration. When $\\mathcal{C}_{l}$ is extended for the $(i+1)$ -th time, let $\\mathcal{C}_{l}^{i+1}$ be the set of newly added state-action pairs, making the latest $\\mathcal{C}_{l}=\\mathcal{C}_{l}^{0}\\cup\\mathcal{C}_{l}^{1}\\cup\\mathcal{C}_{l}^{2}\\cup\\cdot\\cdot\\cup\\mathcal{C}_{l}^{i+1}$ . The least-squares weight $w_{k}^{i+1}$ is then computed using $\\mathcal{C}_{l}$ . When line 27 of algorithm 1 is executed, $\\pi_{k+1}$ remains unchanged for the rest of the algorithm\u2019s execution for any states already in $\\mathrm{Cov}(\\mathscr{C}_{l}^{0}\\cup\\dot{\\mathscr{C}}_{l}^{1}\\cup\\cdot\\cdot\\cdot\\cup\\mathscr{C}_{l}^{i})$ , equivalent to $\\mathrm{Cov}(\\mathcal{C}_{l+1})$ in line 27, because line 30 would have been executed in the $i$ -th extension, making $\\mathcal{C}_{l+1}=\\mathcal{C}_{l}^{0}\\cup\\mathcal{C}_{l}^{1}\\cup\\cdot\\cdot\\cdot\\cup\\mathcal{C}_{l}^{i}$ . For states in $\\mathrm{Cov}(\\mathscr{C}_{l}^{0}\\cup\\mathscr{C}_{l}^{1}\\cup\\cdot\\cdot\\cdot\\cup\\mathscr{C}_{l}^{i+1})\\,\\backslash\\,\\mathrm{Cov}(\\mathscr{C}_{l}^{0}\\cup\\mathscr{C}_{l}^{1}\\cup\\cdot\\cdot\\cdot\\cup\\mathscr{C}_{l}^{i})$ (equivalent to $\\mathrm{Cov}(\\mathcal{C}_{l})\\setminus\\mathrm{Cov}(\\mathcal{C}_{l+1})$ in line 27), $\\pi_{k+1}$ makes a softmax update using wik+1. For all other states not in Cov(Cl), the policy remains as \u03c0k. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "A subroutine can start with $\\pi_{0}$ , use the stored data to compute and return $\\pi_{k}(\\cdot|s)$ for any $s$ and $k$ . By tracking newly added elements and the corresponding least-square weights, the algorithm can reconstruct policies $\\pi_{0},\\ldots,\\pi_{K}$ . This approach enables the algorithm to return the value of a mixture policy at termination. ", "page_idx": 31}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Reviewers may find results from section 4 leading to the main results stated in section 5 and section 6. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 32}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: The limitations are discussed ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The assumptions are listed in every lemma and theorems. In the supplementary section, all supporting lemmas will be proven or cited. All the lemmas are presented in sequence leading up to the two main theorems. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 33}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: This paper does not include experiment ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This paper does not include experiment Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 34}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This paper does not include experiment ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 34}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: This paper does not include experiment ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: This paper does not include experiment ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 35}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: I have read the ethics guidelines. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 35}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 35}, {"type": "text", "text": "Answer: [No] ", "page_idx": 35}, {"type": "text", "text": "Justification: By understanding the sample complexity of CMDP, a framework used by many of the safe reinforcement learning research, we can design more efficient algorithms. This potentially has broader positive impact for real world applications. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This paper does not pose such risk as it contains no data or models ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This paper does not use any data, model or code. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 36}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 37}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: This paper does not contain any new assets. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 37}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor resarch with human subjects. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 37}]