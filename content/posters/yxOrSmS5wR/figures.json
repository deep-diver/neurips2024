[{"figure_path": "yxOrSmS5wR/figures/figures_1_1.jpg", "caption": "Figure 1: AV-Cloud is an audio rendering framework synchronic with the visual perspective. Given video collections, it constructs Audio-Visual Anchors for scene representation and transforms monaural reference sound into spatial audio.", "description": "This figure illustrates the AV-Cloud framework, which synchronizes audio and visual rendering. It starts with video collections used to construct Audio-Visual Anchors (AV Anchors), representing the scene with 3D coordinates, RGB, and audio effects. These AV Anchors are input to the Audio-Visual Cloud Splatting module which transforms monaural reference audio into spatial audio aligned with the visual perspective. The Spatial Audio Render Head module generates the final stereo spatial audio output.", "section": "3 Methods"}, {"figure_path": "yxOrSmS5wR/figures/figures_4_1.jpg", "caption": "Figure 2: AVCS consists of two components: Anchor Projection (left) and Visual-to-Audio Splatting Transformer (right). Audio-Visual Anchors are projected into the coordinate system of the listener head, and the transformer decodes features for each audio frequency band, outputting two acoustic masks to convert the monaural reference sound into stereo audio at the target viewpoint.", "description": "This figure illustrates the Audio-Visual Cloud Splatting (AVCS) module, a core component of the AV-Cloud framework.  AVCS takes Audio-Visual Anchors as input, projects them into the listener's coordinate system, and then uses a transformer network to decode audio spatial effect features for each frequency band. The output of the transformer is two acoustic masks: a mixture mask and a difference mask. These masks are used to transform a monaural input sound into a stereo output sound tailored to the specific viewpoint of the listener.", "section": "3.3 Audio-Visual Cloud Splatting (AVCS)"}, {"figure_path": "yxOrSmS5wR/figures/figures_5_1.jpg", "caption": "Figure 3: SARH implements a single-layer residual structure to transform monaural reference sound into stereo audio. It contains two convolution modules: Time Filters and Conv2D layers to adjust energy distribution in both the time and frequency domain and enhance the stereo output.", "description": "The SARH (Spatial Audio Render Head) module takes the acoustic masks (mixture and difference masks) from the AVCS (Audio-Visual Cloud Splatting) module and the monaural reference sound as inputs.  It uses a single-layer residual structure with two convolutional modules: Time Filters and Conv2D layers. The Time Filters module adjusts the energy distribution in the time domain, improving the quality of the sound and taking into account things like reverberation time. The Conv2D Layers smooth and enhance the time-frequency distribution of the sound, using a stacked convolutional network to achieve this. The output of this module is a stereo audio signal, with the left and right channels calculated using the mixture and difference masks.", "section": "3.4 Spatial Audio Render Head (SARH)"}, {"figure_path": "yxOrSmS5wR/figures/figures_9_1.jpg", "caption": "Figure 4: Qualitative Results Comparisons. Left: output spectrogram and stereo waveforms; Bottom Right: LRE error bar chart. The blue-circled region on the spectrogram highlights the reverberation effect, demonstrating ability of AV-Cloud to capture prolonged energy decay.", "description": "This figure compares the qualitative results of AV-Cloud with other state-of-the-art methods (NAF, VIGAS, AV-NeRF) for spatial audio rendering.  The left side shows the input audio spectrogram, the spectrograms generated by each method, and finally the ground truth spectrogram. The bottom-left section displays the corresponding waveforms.  The right side shows a bar chart comparing the Left-Right Energy Ratio (LRE) error for each method. The blue circles in the spectrograms highlight the reverberation effect, visually demonstrating AV-Cloud's superior ability to capture and reproduce the prolonged energy decay characteristic of reverberation.", "section": "4.4 Qualitative Results Comparison"}, {"figure_path": "yxOrSmS5wR/figures/figures_13_1.jpg", "caption": "Figure 1: AV-Cloud is an audio rendering framework synchronic with the visual perspective. Given video collections, it constructs Audio-Visual Anchors for scene representation and transforms monaural reference sound into spatial audio.", "description": "This figure illustrates the AV-Cloud framework.  It starts with video collections which are processed to obtain structure from motion (SfM) points. These points are then clustered to create Audio-Visual Anchors. These anchors contain visual and audio information to represent the 3D scene.  A novel Audio-Visual Cloud Splatting module decodes the anchors into a spatial audio transfer function based on the listener's viewpoint. Finally, a Spatial Audio Render Head module takes monaural input and creates viewpoint-specific stereo audio. The whole system is designed to be synchronized with the visual rendering.", "section": "1 Introduction"}, {"figure_path": "yxOrSmS5wR/figures/figures_13_2.jpg", "caption": "Figure 1: AV-Cloud is an audio rendering framework synchronic with the visual perspective. Given video collections, it constructs Audio-Visual Anchors for scene representation and transforms monaural reference sound into spatial audio.", "description": "This figure illustrates the AV-Cloud framework, showing how it processes video data to generate synchronized spatial audio.  The input is a collection of videos from which structure-from-motion (SfM) points are extracted. These points are used to create Audio-Visual Anchors (AV anchors) which capture both visual and audio information from the scene. These AV anchors form the Audio-Visual Cloud, and are used by the Audio-Visual Cloud Splatting module to decode a spatial audio transfer function. This function, along with a Spatial Audio Render Head, transforms a monaural input sound into viewpoint-specific spatial audio, aligned with the visual perspective. The figure highlights the key components of the system and the flow of data.", "section": "1 Introduction"}, {"figure_path": "yxOrSmS5wR/figures/figures_16_1.jpg", "caption": "Figure 6: Visualization for AV-Cloud Interpretation. The black triangle indicates the emitter, the green cross represents the listener, and the green divergent perspective starting from the listener shows the listener's head orientation. Point clouds are shown in blue, and the Audio-Visual Anchor is visualized with a red point whose radius is proportional to attention weights. Our splatting transformer is able to localize the emitter and assign higher attention weights to the anchor closest to it for spatial audio effect learning.", "description": "This figure visualizes how the AV-Cloud model interprets the scene to generate spatial audio.  The left side shows a 3D point cloud of the scene, with the emitter (black triangle), listener (green cross), and AV Anchors (red circles) highlighted. The size of the red circles representing AV Anchors is proportional to their attention weights, indicating their importance in the spatial audio rendering. The right side shows images from real-world scenes corresponding to the point clouds on the left. This visualization demonstrates the ability of AV-Cloud to focus on relevant anchors based on listener position and emitter location, resulting in improved accuracy and realism.", "section": "A.7 Visualization for Interpretation"}]