[{"figure_path": "6A29LUZhfv/figures/figures_1_1.jpg", "caption": "Figure 1: Benchmark correlations (%) with Chatbot Arena Elo, against the total costs of evaluating a single GPT-3.5-Turbo-0125 model. MixEval and MixEval-Hard show the highest correlations with Arena Elo and Arena Elo (En) among leading benchmarks. We reference the crowdsourcing price for Amazon Mechanical Turk ($0.05 per vote) when estimating the cost of evaluating a single model on Chatbot Arena (approximately $2,936). Chatbot Arena is prohibitively expensive, while MixEval and MixEval-Hard are cheap and cost-effective alternatives. Details on the correlation and evaluation cost values are provided in Section E.", "description": "This figure shows the correlation of various benchmarks with Chatbot Arena Elo, plotted against their evaluation costs.  MixEval and MixEval-Hard demonstrate the highest correlation with Chatbot Arena, indicating strong alignment with real-world human preferences, while being significantly more cost-effective than other benchmarks.", "section": "2 LLM Benchmarks are Biased from Realistic User Queries and Preferences"}, {"figure_path": "6A29LUZhfv/figures/figures_2_1.jpg", "caption": "Figure 2: Query Topic Distribution of the Benchmarks. Ground-truth-based benchmarks are represented by orange dots, wild datasets by yellow dots, and LLM-judged benchmarks (MT-Bench and Arena-Hard) by yellow dots, all plotted against our detected web queries shown as blue dots. Query sentence embeddings were dimensionally reduced to map them onto a unified 2-D space, facilitating direct comparisons of topic distributions across benchmarks. As we move from the bottom to the top of the figure, query topics transition from non-technical to technical. Topic summaries for each region are detailed in Figure 3.", "description": "This figure visualizes the topic distributions of various LLM benchmarks and compares them to the distribution of real-world web queries.  It uses a 2D embedding to show how different benchmarks cluster together, revealing the relative balance between technical and non-technical topics in each. The distribution of web queries serves as a baseline for comparison, highlighting the degree to which different benchmarks align with actual user query topics.", "section": "2 LLM Benchmarks are Biased from Realistic User Queries and Preferences"}, {"figure_path": "6A29LUZhfv/figures/figures_3_1.jpg", "caption": "Figure 2: Query Topic Distribution of the Benchmarks. Ground-truth-based benchmarks are represented by orange dots, wild datasets by yellow dots, and LLM-judged benchmarks (MT-Bench and Arena-Hard) by yellow dots, all plotted against our detected web queries shown as blue dots. Query sentence embeddings were dimensionally reduced to map them onto a unified 2-D space, facilitating direct comparisons of topic distributions across benchmarks. As we move from the bottom to the top of the figure, query topics transition from non-technical to technical. Topic summaries for each region are detailed in Figure 3.", "description": "This figure visualizes the topic distribution of various benchmarks, including ground-truth, wild datasets, and LLM-judged ones, compared to web queries.  It uses a 2D embedding to represent query topics, showing a transition from non-technical to technical topics as you move from the bottom to the top of the figure.  Each point represents a query, and the color indicates the benchmark it originated from.", "section": "2 LLM Benchmarks are Biased from Realistic User Queries and Preferences"}, {"figure_path": "6A29LUZhfv/figures/figures_4_1.jpg", "caption": "Figure 4: MixEval, a two-stage benchmark reconstruction pipeline, comprises (1) web query detection and (2) benchmark mixture. We further introduce MixEval-Hard to enhance model separability, alongside a dynamic updating mechanism to mitigate contamination risk.", "description": "The figure illustrates the MixEval pipeline, which consists of two main stages: web query detection and benchmark mixture.  The pipeline starts with mining user queries from Common Crawl, filtering them and creating a benchmark pool. These user queries are then grounded in existing benchmarks by finding the most similar benchmark questions to create MixEval. To further enhance the model ranking capability, MixEval-Hard, a challenging subset of MixEval, is generated. Furthermore, a dynamic evaluation component ensures ongoing data updates to mitigate overfitting issues.", "section": "3 MixEval"}, {"figure_path": "6A29LUZhfv/figures/figures_7_1.jpg", "caption": "Figure 5: Our approach improves the correlation with Arena Elo and Arena Elo (En) (Figure 12) for all the main splits of MixEval and outperforms benchmark-level and uniform mixture.", "description": "This figure shows the improvement in correlation with Arena Elo and Arena Elo (En) achieved by using the MixEval approach compared to benchmark-level and uniform mixture methods.  The x-axis represents different benchmark datasets (TriviaQA, MMLU, etc.), while the y-axis shows the correlation with Arena Elo.  The bars are grouped into pairs, with the light gray bars showing the original correlation before MixEval and the dark blue bars indicating the correlation after applying MixEval. The figure demonstrates that the MixEval method significantly improves correlations across all benchmarks, highlighting its effectiveness in aligning benchmark query distributions with real-world user queries.", "section": "4.2 Effectiveness of MixEval"}, {"figure_path": "6A29LUZhfv/figures/figures_8_1.jpg", "caption": "Figure 6: Activated parameters and API price per performance of open-source and proprietary models.", "description": "This figure compares the performance of various open-source and proprietary large language models (LLMs) across two key metrics: activated parameters (a proxy for model size and computational cost) and API price per million output tokens (a proxy for cost-effectiveness).  Panel (a) shows a positive correlation between activated parameters and performance on MixEval, suggesting that larger models tend to perform better. However, the relationship isn't perfectly linear, indicating that parameter efficiency varies across different model architectures. Panel (b) shows a similar positive correlation between API cost and MixEval performance, again demonstrating that more expensive models generally perform better, but with variations in cost-effectiveness among different models. The figure highlights the trade-off between model size/computational cost and performance, with some models demonstrating better cost-effectiveness than others.", "section": "Results"}, {"figure_path": "6A29LUZhfv/figures/figures_8_2.jpg", "caption": "Figure 7: The performance of chat and base models of the same model series in Table 3. Chat and base model scores show a high correlation.", "description": "This figure shows a bar chart comparing the performance of chat models and base models from the same series.  The data is taken from Table 3 in the paper.  A high Spearman correlation (0.95) is noted, indicating that the performance of chat models and base models are strongly related. This suggests that the improvements in capabilities observed between the base and chat versions of these models are consistent across different model series. ", "section": "Results"}, {"figure_path": "6A29LUZhfv/figures/figures_15_1.jpg", "caption": "Figure 8: The normalized number of queries in MixEval and the original benchmarks.", "description": "This figure shows a bar chart comparing the normalized number of queries used in MixEval versus the original benchmarks.  The height of each bar represents the proportion of queries from each benchmark in MixEval.  It illustrates how the query distribution in MixEval differs from the original benchmarks, showing that some benchmarks are more heavily represented in MixEval than others. This is related to the benchmark mixture technique used to create MixEval, which strategically mixes queries from multiple benchmarks to achieve a more representative and unbiased query distribution.", "section": "E Implementation details for Benchmark Correlation Matrix, Query Distribution, and Evaluation Cost"}, {"figure_path": "6A29LUZhfv/figures/figures_16_1.jpg", "caption": "Figure 1: Benchmark correlations (%) with Chatbot Arena Elo, against the total costs of evaluating a single GPT-3.5-Turbo-0125 model. MixEval and MixEval-Hard show the highest correlations with Arena Elo and Arena Elo (En) among leading benchmarks. We reference the crowdsourcing price for Amazon Mechanical Turk ($0.05 per vote) when estimating the cost of evaluating a single model on Chatbot Arena (approximately $2,936). Chatbot Arena is prohibitively expensive, while MixEval and MixEval-Hard are cheap and cost-effective alternatives. Details on the correlation and evaluation cost values are provided in Section E.", "description": "This figure shows the correlation between different benchmarks and Chatbot Arena Elo, plotted against their respective evaluation costs. MixEval and MixEval-Hard demonstrate the highest correlation with Chatbot Arena Elo at a significantly lower cost compared to other benchmarks, highlighting their efficiency and cost-effectiveness as LLM evaluation methods.", "section": "2 LLM Benchmarks are Biased from Realistic User Queries and Preferences"}, {"figure_path": "6A29LUZhfv/figures/figures_17_1.jpg", "caption": "Figure 1: Benchmark correlations (%) with Chatbot Arena Elo, against the total costs of evaluating a single GPT-3.5-Turbo-0125 model. MixEval and MixEval-Hard show the highest correlations with Arena Elo and Arena Elo (En) among leading benchmarks. We reference the crowdsourcing price for Amazon Mechanical Turk ($0.05 per vote) when estimating the cost of evaluating a single model on Chatbot Arena (approximately $2,936). Chatbot Arena is prohibitively expensive, while MixEval and MixEval-Hard are cheap and cost-effective alternatives. Details on the correlation and evaluation cost values are provided in Section E.", "description": "The figure shows the correlation between different benchmarks and Chatbot Arena Elo, plotted against their respective evaluation costs. MixEval and MixEval-Hard exhibit the highest correlation with Chatbot Arena Elo while having significantly lower costs compared to other benchmarks. This highlights the cost-effectiveness and accuracy of MixEval and MixEval-Hard as LLM evaluation methods.", "section": "2 LLM Benchmarks are Biased from Realistic User Queries and Preferences"}, {"figure_path": "6A29LUZhfv/figures/figures_18_1.jpg", "caption": "Figure 1: Benchmark correlations (%) with Chatbot Arena Elo, against the total costs of evaluating a single GPT-3.5-Turbo-0125 model. MixEval and MixEval-Hard show the highest correlations with Arena Elo and Arena Elo (En) among leading benchmarks. We reference the crowdsourcing price for Amazon Mechanical Turk ($0.05 per vote) when estimating the cost of evaluating a single model on Chatbot Arena (approximately $2,936). Chatbot Arena is prohibitively expensive, while MixEval and MixEval-Hard are cheap and cost-effective alternatives. Details on the correlation and evaluation cost values are provided in Section E.", "description": "This figure shows the correlation between various LLM benchmarks and Chatbot Arena Elo, plotted against their respective evaluation costs.  MixEval and MixEval-Hard demonstrate the highest correlation with Chatbot Arena Elo, indicating strong alignment with human preferences, while being significantly more cost-effective than other benchmarks.  The high cost of Chatbot Arena is highlighted, emphasizing the advantage of MixEval and MixEval-Hard.", "section": "2 LLM Benchmarks are Biased from Realistic User Queries and Preferences"}, {"figure_path": "6A29LUZhfv/figures/figures_19_1.jpg", "caption": "Figure 5: Our approach improves the correlation with Arena Elo and Arena Elo (En) (Figure 12) for all the main splits of MixEval and outperforms benchmark-level and uniform mixture.", "description": "This figure shows the improvement in correlation with Arena Elo and Arena Elo (En) achieved by MixEval compared to the original benchmarks. It demonstrates that MixEval and MixEval-Hard significantly outperform both benchmark-level and uniform mixtures, highlighting the effectiveness of the proposed benchmark mixture technique in aligning with real-world user preferences. The figure presents bar charts showing correlations for various benchmark splits, with MixEval and MixEval-Hard consistently showing higher correlations than the other methods.", "section": "4.2 Effectiveness of MixEval"}, {"figure_path": "6A29LUZhfv/figures/figures_20_1.jpg", "caption": "Figure 13: Averaged error rates of open-source, proprietary, and all models on MixEval splits.", "description": "This figure shows the average error rates achieved by open-source, proprietary, and all models across different splits of the MixEval benchmark.  The x-axis represents the different benchmark splits (AGIEval, HellaSwag, SIQA, MMLU, TriviaQA, CommonSenseQA, GSM8k, DROP, BoolQ, PIQA), and the y-axis represents the average error rate. Three bars are presented for each split, indicating the performance of open-source models, proprietary models, and the overall average error rate across all models.", "section": "H Error Analysis"}, {"figure_path": "6A29LUZhfv/figures/figures_24_1.jpg", "caption": "Figure 1: Benchmark correlations (%) with Chatbot Arena Elo, against the total costs of evaluating a single GPT-3.5-Turbo-0125 model. MixEval and MixEval-Hard show the highest correlations with Arena Elo and Arena Elo (En) among leading benchmarks. We reference the crowdsourcing price for Amazon Mechanical Turk ($0.05 per vote) when estimating the cost of evaluating a single model on Chatbot Arena (approximately $2,936). Chatbot Arena is prohibitively expensive, while MixEval and MixEval-Hard are cheap and cost-effective alternatives. Details on the correlation and evaluation cost values are provided in Section E.", "description": "This figure compares the correlation of various LLM benchmarks with Chatbot Arena Elo against their evaluation costs.  MixEval and MixEval-Hard demonstrate the highest correlation while having significantly lower costs compared to other benchmarks like MMLU and Chatbot Arena.", "section": "2 LLM Benchmarks are Biased from Realistic User Queries and Preferences"}]