[{"figure_path": "qbvt3ocQxB/figures/figures_0_1.jpg", "caption": "Figure 1: Performance improvement by IODA. The performance of the real-world pre-trained weights (from DF2K dataset [1]) provided in the original paper serves as the baseline on the real-world dataset RealSR_Canon [2].", "description": "This figure shows the performance improvement achieved by using IODA on three different network architectures (SAFMN, SRFormer, and HAT). The baseline performance represents the results obtained using the original pre-trained weights on the RealSR_Canon dataset.  The orange bars indicate the improved performance after adding IODA, showcasing the effectiveness of the method in enhancing super-resolution performance, especially in real-world scenarios.", "section": "1 Introduction"}, {"figure_path": "qbvt3ocQxB/figures/figures_2_1.jpg", "caption": "Figure 2: Overall framework", "description": "The figure illustrates the overall framework of IODA, which consists of two stages. In stage one, a source domain SR network (Ms) is pre-trained using the source domain dataset (DSource). In stage two, domain adaptation is performed on the SR network (Ms). The weights of the pre-trained network (Ms) are frozen and used to initialize the target domain SR network (MT). LR and SR images from both source and target domains are used to compute cross-domain direction vectors in the Alpha-CLIP space. The Ldirection loss is calculated to align these two cross-domain direction vectors, enabling domain adaptation training for the target domain SR network (MT).", "section": "2.2 Overview"}, {"figure_path": "qbvt3ocQxB/figures/figures_3_1.jpg", "caption": "Figure 3: Image-guided domain adaptation method. This figure depicts the second stage of domain adaptation. Ms and MT represent the source domain SR network and the target domain SR network, respectively.", "description": "This figure illustrates the second stage of the IODA framework, focusing on image-guided domain adaptation.  It shows how the source domain SR network (Ms) and the target domain SR network (MT) are used. LR and SR images from both source and target domains are inputted into a CLIP model to compute cross-domain direction vectors (ALR and ASR). These vectors represent the difference in features between the domains and are used to guide the adaptation of the target network (MT) to the target domain. The goal is to make the output of the target network (SRTarget) similar to the source network's output on the target domain (LRTarget).", "section": "2.3 Image-guided domain adaptation"}, {"figure_path": "qbvt3ocQxB/figures/figures_4_1.jpg", "caption": "Figure 4: Feature distribution comparison. Inputting a single target domain LR image into the image encoders of the CLIP model (a, b) and the Alpha-CLIP model (c), visualizing the output features after dimension reduction using T-SNE [27] (repeated 1000 times). It's worth noting that the more dispersed the scatter plot distribution, the more diverse the target domain feature distribution.", "description": "This figure compares feature distribution visualizations using t-SNE after inputting a single target domain LR image to CLIP and Alpha-CLIP encoders.  The results show that using Alpha-CLIP with instance-specific features (Region-range Mask) significantly increases the dispersion of the feature distribution compared to using just CLIP or CLIP with occlusion masks.  A more dispersed distribution indicates a more diverse representation of the target domain, which is beneficial for domain adaptation.", "section": "2.4 Instance-guided target domain distribution expansion strategy"}, {"figure_path": "qbvt3ocQxB/figures/figures_4_2.jpg", "caption": "Figure 5: Instance-guided target domain distribution expansion strategy. The source domain and target domain SR images SRSource, SRTarget are generated from the corresponding LR images LRSource, LRTarget through their respective domain-specific SR networks Ms and MT.", "description": "This figure illustrates the Instance-guided target domain distribution expansion strategy.  It shows how the source and target domain low-resolution (LR) images are processed through their respective super-resolution (SR) networks to generate high-resolution (HR) images. The strategy involves using the Segment Anything Model (SAM) to generate region-range masks, then using Alpha-CLIP with these masks to generate instance-specific features, which expand the diversity of the target domain feature distribution. This is crucial for one-shot domain adaptation where only one LR target image is available for training.", "section": "2.4 Instance-guided target domain distribution expansion strategy"}, {"figure_path": "qbvt3ocQxB/figures/figures_15_1.jpg", "caption": "Figure 6: Text-guided domain adaptation. Existing generative networks typically sample from random noise to generate latent vectors, which are then input into the generator network to produce images. Red indicates adjustable weights, while blue indicates frozen weight.", "description": "This figure illustrates the typical approach of text-guided domain adaptation used in image generation. In stage one, the source domain generation network (GSource) is pre-trained. In stage two, the network is adapted to the target domain.  Random noise is sampled to produce latent vectors, which are input to both the source and target domain generation networks (GSource and GTarget respectively).  The generated images are then compared using CLIP. Text descriptions guide the adaptation by constraining the direction vectors (\u0394Image and \u0394Text) in the CLIP embedding space. The goal is for the images generated to shift based on the text change (e.g., from \"cat\" to \"dog\").", "section": "B Text-guided domain adaptation"}, {"figure_path": "qbvt3ocQxB/figures/figures_16_1.jpg", "caption": "Figure 7: Alpha-CLIP", "description": "This figure shows the architecture of Alpha-CLIP.  It takes an image and a mask as input. The image is processed by an RGB convolutional branch, while the mask is processed by an Alpha convolutional branch. The outputs of both branches are combined and fed into the Alpha-CLIP image encoder. The CLIP image encoder's output is then compared with the output of the CLIP text encoder, which processes a text description of the image (e.g., \"A bird\"), generating a similarity matrix.  Alpha-CLIP leverages region-range masking to improve performance by directing attention to specific regions within the image. ", "section": "C Alpha-CLIP"}, {"figure_path": "qbvt3ocQxB/figures/figures_16_2.jpg", "caption": "Figure 8: Visual comparisons.The source domain dataset is the GTA [30] daytime scene dataset, and the target domain dataset includes various scene branches from the ACDC [32] dataset, such as rain, night, and snow. The network architecture used is the SAFMN [26] network.", "description": "This figure shows a visual comparison of super-resolution results on images from the ACDC dataset (rain, night, and snow scenes) using the proposed IODA method and a baseline method.  The source domain data for training was the GTA dataset (daytime scenes). The figure demonstrates that IODA significantly improves the visual quality of super-resolution images, especially in challenging conditions like rain, night, and snow, where the baseline method struggles to produce clear results.", "section": "D Visualization results"}, {"figure_path": "qbvt3ocQxB/figures/figures_17_1.jpg", "caption": "Figure 9: The large image on the left is the LR image, and the sub-images on the right are DADA [56], DASR [15], ZSSR [34], SRTTA [48] (first row), SAFMN [26] +IODA, SRFormer [25] +IODA, HAT [24] +IODA, and GT images (second row). Please zoom-in on screen.", "description": "This figure shows the visual comparison of different super-resolution methods on a low-resolution (LR) image. The first row contains the results of four existing methods (DADA, DASR, ZSSR, SRTTA), and the second row shows the results of three methods in the proposed work (SAFMN+IODA, SRFormer+IODA, HAT+IODA) along with the ground truth (GT) image. This aims to showcase the visual improvement achieved by the proposed IODA method.", "section": "Visualization results"}]