[{"figure_path": "JBAUg7o8Yv/figures/figures_1_1.jpg", "caption": "Figure 1: Our method achieves state-of-the-art rendering quality while maintaining the fastest runtime. (a) Qualitative results: LGM [29] and GTA [30] are generalizable but in lower quality, TeCH [21] exhibits issues with multi-face rendering and is time-consuming. In contrast, our method achieves higher fidelity in a much shorter time. (b) Performance and runtime comparison: metrics are evaluated on the challenging Twindom dataset.", "description": "This figure shows a comparison of the proposed HumanSplat method with other state-of-the-art single-image human reconstruction methods.  Part (a) presents qualitative results, demonstrating HumanSplat's superior rendering quality and faster speed compared to LGM, GTA, and TeCH. Part (b) provides a quantitative comparison of the methods on the Twindom dataset, showing HumanSplat achieves the best PSNR score with the shortest reconstruction time.", "section": "1 Introduction"}, {"figure_path": "JBAUg7o8Yv/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of HumanSplat. (a) Multi-view latent features are first generated by a fine-tuned multi-view diffusion model (Novel View Synthesizer in Sec. 3.3). (b) Then, the Latent Reconstruction Transformer (Sec. 3.4) interacts global latent features (Sec. 3.4.1) and human geometric prior (Sec. 3.4.2). (c) Finally, the semantic-guided objectives (Sec. 3.5) are proposed to reconstruct the final human 3DGS.", "description": "This figure illustrates the overall architecture of the HumanSplat model.  It shows three main stages: (a) A novel-view synthesizer (a fine-tuned multi-view diffusion model) that generates multi-view latent features from a single input image. (b) A latent reconstruction transformer that integrates these latent features with human geometric and semantic priors to predict the Gaussian splatting properties.  (c) Semantics-guided objectives that refine the model's output for higher-fidelity reconstruction and rendering.", "section": "3 Method"}, {"figure_path": "JBAUg7o8Yv/figures/figures_5_1.jpg", "caption": "Figure 3: Illustration of latent reconstruction Transformer. It first divides Fo and Fi into non-overlapping patches, which are then processed through an intra-attention module (Sec. 3.4.1). Within the iter-attention module (Sec. 3.4.2), we introduce the projection-aware attention with a window W(Kwin \u00d7 Kwin), and the attributes of 3D Gaussians are decoded with a Conv 1 \u00d7 1 layer.", "description": "This figure illustrates the architecture of the Latent Reconstruction Transformer, a key component of the HumanSplat model.  It shows how multi-view latent features (from a novel-view synthesizer) and human geometric priors are integrated. The process begins with dividing the latent features into patches and processing them through an intra-attention module to capture spatial correlations.  Then, a geometry-aware interaction module incorporates human geometric priors using a novel projection-aware attention mechanism. This mechanism improves efficiency and robustness by focusing attention within local windows of the 2D projection of the 3D geometric prior. Finally, the processed features are used to decode the attributes of 3D Gaussians for the final 3D representation.", "section": "3.4 Latent Reconstruction Transformer"}, {"figure_path": "JBAUg7o8Yv/figures/figures_7_1.jpg", "caption": "Figure 4: Qualitative comparison of ours against TeCH [21], GTA [30] and LGM [29] on Thuman2.0 [105], Twindom [106] and in-the-wild images. Our method achieves the highest quality. Note that TeCH achieves clearer results but fails to preserve the face identity.", "description": "This figure compares the qualitative results of the proposed HumanSplat method against three other state-of-the-art methods (TeCH, GTA, and LGM) on three datasets (THuman2.0, Twindom, and in-the-wild images). The comparison is based on visual quality and focuses on the reconstruction of human faces and clothing. HumanSplat shows better quality in most cases, while TeCH occasionally has clearer results but suffers from identity preservation issues.", "section": "4.2 Comparison"}, {"figure_path": "JBAUg7o8Yv/figures/figures_8_1.jpg", "caption": "Figure 5: Qualitative results showcasing reconstructions of humans in challenging poses, diverse identities, and varying camera viewpoints from in-the-wild images.", "description": "This figure presents a qualitative comparison of the proposed HumanSplat model against the LGM* method. It showcases the ability of HumanSplat to generate high-fidelity 3D human reconstructions from single images, even in challenging scenarios.  The images demonstrate HumanSplat's robustness to diverse poses, body types, clothing styles, and viewpoints. The results highlight the superior performance of HumanSplat in terms of detail preservation and overall reconstruction quality compared to LGM*. The in-the-wild images show HumanSplat successfully reconstructs complex poses and clothing details, which the other method struggles with.", "section": "4.2 Comparison"}, {"figure_path": "JBAUg7o8Yv/figures/figures_8_2.jpg", "caption": "Figure 6: Qualitative Comparison of ours against HumanSGD [22] on in-the-wild images.", "description": "This figure presents a qualitative comparison of the proposed HumanSplat method against the HumanSGD method on in-the-wild images.  It showcases the results of both methods on several examples, highlighting HumanSplat's superior ability to reconstruct realistic human models with detailed clothing and body shapes, even when dealing with complex poses or less-than-ideal image quality. The figure demonstrates the advantages of HumanSplat in terms of accuracy and visual fidelity.", "section": "4.2 Comparison"}, {"figure_path": "JBAUg7o8Yv/figures/figures_19_1.jpg", "caption": "Figure 8: Detailed network architecture of latent reconstruction Transformer.", "description": "This figure illustrates the detailed architecture of the latent reconstruction Transformer, a key component of the HumanSplat model.  It shows the flow of information from latent multi-view features to the final 3D Gaussian Splatting (3DGS) attributes.  The process involves patchifying the input features, applying linear embedding, and using intra-attention and inter-attention modules to capture spatial and cross-view relationships.  The attributes of the 3D Gaussians\u2014position, rotation, scale, opacity, and color\u2014are then predicted through a deconvolution block. This transformer efficiently integrates geometric and latent features to reconstruct high-fidelity human representations.", "section": "3.4 Latent Reconstruction Transformer"}, {"figure_path": "JBAUg7o8Yv/figures/figures_20_1.jpg", "caption": "Figure 9: Ablation Study on Reweighting Loss. Comparison of the original HumanSplat, trained with reweighting loss, and SIFU featuring texture.", "description": "This figure presents an ablation study to evaluate the effect of the reweighting loss on the HumanSplat model. It compares the results of three different models: the original HumanSplat model, the HumanSplat model trained with reweighting loss, and the SIFU model. The results show that the HumanSplat model trained with reweighting loss produces better results than the original HumanSplat model and the SIFU model, especially in terms of texture quality.", "section": "4.3 Ablation Study"}, {"figure_path": "JBAUg7o8Yv/figures/figures_20_2.jpg", "caption": "Figure 10: Qualitative 3D Gaussian Splatting results of diversified evaluation dataset. (a) Input Image. (b) Novel view Rendering Results.", "description": "This figure shows the qualitative comparison results of HumanSplat on the diversified evaluation datasets. The left column shows the input image, and the right columns show the novel view rendering results of HumanSplat. It demonstrates the capability of the model to generate high-fidelity novel views from single images.", "section": "4 Experiments"}, {"figure_path": "JBAUg7o8Yv/figures/figures_21_1.jpg", "caption": "Figure 11: Qualitative 4D Gaussian Splatting results on In-the-wild images, including novel view and pose rendering images. (Please zoom in for a detailed view)", "description": "This figure shows qualitative results of 4D Gaussian Splatting on in-the-wild images. It demonstrates the model's capability to generate novel views and poses of humans. The figure is organized into four rows.  Each row shows a sequence of images, starting from an input image and transitioning through various novel views and poses. The results highlight the model's ability to accurately reconstruct human appearance and handle diverse clothing styles and body poses, demonstrating its potential for applications in fields like virtual reality, gaming, and fashion.", "section": "More Results"}]