[{"type": "text", "text": "Bridging Geometric States via Geometric Diffusion Bridge ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shengjie Luo1,\u2217 Yixian $\\mathbf{Xu}^{1,4*}$ , $\\mathbf{Di}\\,\\mathbf{He}^{1\\dagger}$ , Shuxin Zheng2, Tie-Yan Liu2, Liwei Wang1,3\u2020 ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1National Key Laboratory of General Artificial Intelligence, School of Intelligence Science and Technology, Peking University 2Microsoft Research AI4Science 3Center for Data Science, Peking University 4Pazhou Laboratory (Huangpu), Guangzhou, China luosj@stu.pku.edu.cn, xyx050@stu.pku.edu.cn, {shuz, tyliu}@microsoft.com, {dihe, wanglw}@pku.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The accurate prediction of geometric state evolution in complex systems is critical for advancing scientific domains such as quantum chemistry and material modeling. Traditional experimental and computational methods face challenges in terms of environmental constraints and computational demands, while current deep learning approaches still fall short in terms of precision and generality. In this work, we introduce the Geometric Diffusion Bridge (GDB), a novel generative modeling framework that accurately bridges initial and target geometric states. GDB leverages a probabilistic approach to evolve geometric state distributions, employing an equivariant diffusion bridge derived by a modified version of Doob\u2019s $h$ -transform for connecting geometric states. This tailored diffusion process is anchored by initial and target geometric states as fixed endpoints and governed by equivariant transition kernels. Moreover, trajectory data can be seamlessly leveraged in our GDB framework by using a chain of equivariant diffusion bridges, providing a more detailed and accurate characterization of evolution dynamics. Theoretically, we conduct a thorough examination to confirm our framework\u2019s ability to preserve joint distributions of geometric states and capability to completely model the underlying dynamics inducing trajectory distributions with negligible error. Experimental evaluations across various real-world scenarios show that GDB surpasses existing state-of-theart approaches, opening up a new pathway for accurately bridging geometric states and tackling crucial scientific challenges with improved accuracy and applicability. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Predicting the evolution of the geometric state of a system is essential across various scientific domains [46, 88, 55, 17, 20, 101], offering valuable insights into difficult tasks such as drug discovery [25, 29], reaction modeling [9, 24], and catalyst analysis [13, 105]. Despite its critical importance, accurately predicting future geometric states of interest is challenging. Experimental approaches often face obstacles due to strict environmental requirements and physical limits of instruments [102, 3, 69]. Computational approaches seek to solve the problem by simulating the dynamics based on underlying equations [81, 88]. Though providing greater flexibility, such calculations are typically driven by first-principle methods or empirical laws, either requiring extensive computational costs [68] or sacrificing accuracy [40]. ", "page_idx": 0}, {"type": "text", "text": "In recent years, deep learning has emerged as a pivotal tool in scientific discovery for many fields [43, 23, 69, 107], offering new avenues for tackling this problem. One line of approach aims to train models to predict target geometric states (e.g., equilibrium states) from initial states directly and develop neural network architectures that respect inherent symmetries of geometric states, such as the equivariance of rotation and translation [104, 31, 8, 87, 89, 103]. However, this paradigm requires encoding the iterative evolution into a single-step prediction model, which lacks the ability to fully capture the system\u2019s underlying dynamics and potentially leading to reduced accuracy. Another line of research trains machine learning force fields (MLFFs) to simulate the trajectory of geometric states over time [32, 34, 6, 70, 5, 58], showing a better efficiency-accuracy balance [15, 13, 105, 84]. Nevertheless, MLFFs are typically trained to predict intermediate labels, such as the force of the (local) current state. During inference, states are iteratively updated step by step. Since small local errors can accumulate, reliable predictions over long trajectories highly depend on the quality of intermediate labels, which cannot be guaranteed [7, 106, 30]. Therefore, an ideal solution that can precisely bridge initial and target geometric states and effectively leverage trajectory data (if available) as guidance is in great demand. ", "page_idx": 1}, {"type": "text", "text": "In this work, we introduce Geometric Diffusion Bridge (GDB), a general framework for bridging geometric states through generative modeling. From a probabilistic perspective, predicting target geometric states from initial states requires modeling the joint state distribution across different time steps. The diffusion models [37, 99] are standard choices to achieve this goal. However, these methods ideally generate data by denoising samples drawn from a Gaussian prior distribution, which makes it challenging to bridge pre-given geometric states or leverage trajectories in a unified manner. To address the issue, we establish a novel equivariant diffusion bridge by developing a modified version of Doob\u2019s $h$ -transform [82, 81, 16]. The proposed stochastic differential equation (SDE) is anchored by initial and target geometric states to simultaneously model the joint state distribution and is governed by equivariant transition kernels to satisfy symmetry constraints. Intriguingly, we further demonstrate that this framework can seamlessly leverage trajectory data to improve prediction. With available trajectory data, we can construct chains of equivariant diffusion bridges, each modeling one segment in the trajectory. The segments are interconnected by properly setting the boundary conditions, allowing complete modeling of trajectory data. For model training, we derive a scalable and simulation-free matching objective similar to [59, 61, 77], which requires no computational overhead when trajectory data is leveraged. ", "page_idx": 1}, {"type": "text", "text": "Overall, our GDB framework offers a unified solution that precisely bridges geometric states by modeling the joint state distribution and comprehensively leverages available trajectories as finegrained depiction of dynamics for enhanced performance. Mathematically, we prove that the joint distribution of geometric states across different time steps can be completely preserved by our (chains of) equivariant diffusion bridge technique, confirming its expressiveness in bridging geometric states and underscoring the necessity of design choices in our framework. Furthermore, under mild and practical assumptions, we prove that our framework can approximate the underlying dynamics governing the evolution of geometric state trajectories with negligible error in convergence, remarking on the completeness and usefulness of our framework in different scenarios. These advantages show the superiority of our framework over existing approaches. ", "page_idx": 1}, {"type": "text", "text": "Practically, we provide a comprehensive guidance for implementing our GDB framework in realworld applications. To verify its effectiveness and generality, we conduct extensive experiments covering diverse data modalities (simple molecules & adsorbate-catalyst complex), scales (small, medium and large scales) and scenarios (with & without trajectory guidance). Numerical results show that our GDB framework consistently outperforms existing state-of-the-art machine learning approaches by a large margin. In particular, our method even surpasses strong MLFF baselines that are trained on $10\\times$ more data in the challenging structure relaxation task of OC22 [105], and trajectory guidance can further enhance our performance. The significantly superior performance demonstrates the high capacity of our framework to capture the complex evolution dynamics of geometric states and determine valuable and crucial geometric states of interest in critical real-world challenges. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Problem Definition ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Our task of interest is to capture the evolution of geometric states, i.e., predicting future states from initial states. Formally, let $S$ denote a system consisting of a set of objects located in the three-dimensional Euclidean space. We use $\\mathbf{H}\\in\\mathbb{R}^{n\\times d}$ to denote the objects with features, where $n$ is the number of objects, and $d$ is the feature dimension. For object $i$ , let $\\mathbf{\\bar{r}}_{i}\\in\\mathbb{R}^{3}$ denote its Cartesian coordinate. We define the system as $S\\,=\\,({\\bf H},R)$ , where $R\\,=\\,\\{{\\bf r}_{1},...,{\\bf r}_{n}\\}$ . This data structure ubiquitously corresponds to various real-world systems such as molecules and proteins [17, 20, 101]. In practice, the geometric state is governed by physical laws and evolves over time, and we denote the geometric state at a given time $t$ as $R^{t}=\\{\\mathbf{\\bar{r}}_{1}^{t},...,\\mathbf{r}_{n}^{t}\\}$ . Given a system $S^{t_{0}}=({\\bf H},R^{t_{0}})$ at time $t_{0}$ , our goal is to predict $\\mathbf{\\zeta}S^{t_{1}}=(\\mathbf{H},R^{t_{1}})$ at a future time $t_{1}$ . As an example, in a molecular system, $R^{t_{1}}$ can be the equilibrium state of interest evolved from the initial state $R^{t_{0}}$ . ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "In this problem, inherent symmetries in geometric states should be considered. For example, a rotation that is applied to the coordinate system at time $t_{0}$ should also be applied to subsequent time steps. These symmetries are related to the concept of equivariance in group theory [19, 18, 91]. Formally, let $\\phi:\\mathcal{X}\\rightarrow\\mathcal{Y}$ denote a function mapping between two spaces. Given a group $G$ , let $\\rho^{\\mathcal{X}}$ and $\\dot{\\rho}^{y}$ denote its group representations, which describe how the group elements act on these spaces. A function $\\phi:\\mathcal{X}\\rightarrow\\mathcal{Y}$ is said to be equivariant if it satisfies the following condition: $\\dot{\\rho}^{\\mathcal{P}}(g)[\\phi(x)]=\\phi\\left(\\rho^{\\mathcal{X}}(g)[x]\\right)$ , $\\forall g\\in G,x\\in\\mathcal{X}$ . When $\\rho^{y}=\\mathcal{Z}^{y}$ (identity transformation), it is also known as invariance. SE(3) group, which pertains to translations $(\\mathrm{T}(3))$ and rotations $\\mathrm{(SO(3))}$ in 3D Euclidean space, is one of the most widely used groups and is employed in our framework. ", "page_idx": 2}, {"type": "text", "text": "2.2 Diffusion Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Diffusion models [95, 37, 99] have emerged as the state-of-the-art generative modeling approaches across various domains [83, 85, 47, 115, 113, 117]. The main idea of this method is to construct a diffusion process that maps data to noise, and train models to reverse such process by using a tractable objective. ", "page_idx": 2}, {"type": "text", "text": "Formally, to model the data distribution $q_{d a t a}(\\mathbf{X})$ , where $\\mathbf{X}\\in\\mathbb{R}^{d}$ , we construct a diffusion process $(\\mathbf{X}_{t})_{t\\in[0,T]}$ , which is represented as a sequence of random variables indexed by time steps. We set $\\mathbf{X}_{0}^{\\mathrm{~\\,~}}\\sim q_{\\mathrm{data}}(\\mathbf{X})$ and $\\mathbf{X}_{T}\\sim p_{\\mathrm{prior}}(\\mathbf{X})$ , where $p_{\\mathrm{prior}}(\\mathbf{X})$ has a tractable form to generate samples efficiently, e.g. standard Gaussian distribution. Mathematically, we model $(\\mathbf{X}_{t})_{t\\in[0,T]}$ as the solution to the following stochastic differential equation (SDE): ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{X}_{t}=\\mathbf{f}(\\mathbf{X}_{t},t)\\mathrm{d}t+\\sigma(t)\\mathrm{d}\\mathbf{B}_{t},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{f}\\left(\\cdot,\\cdot\\right):\\mathbb{R}^{d}\\times\\left[0,T\\right]\\rightarrow\\mathbb{R}^{d}$ is a vector-valued function called the drift coefficient, $\\sigma(\\cdot):[0,T]\\rightarrow$ $\\mathbb{R}$ is a scalar function known as the diffusion coefficient, and $(\\mathbf{B}_{t})_{t\\in[0,T]}$ is the standard Wiener process (a.k.a., Brownian motion) [26]. We hereafter denote by $p_{t}(\\mathbf{X})$ the marginal distribution of $\\mathbf{X}_{t}$ . Let $p(x^{\\prime},t^{\\prime}|x,t)$ denote the transition density function such that $P(\\mathbf{X}_{t^{\\prime}}\\in A|\\mathbf{X}_{t}=x)=$ $\\begin{array}{r}{\\int_{A}p(x^{\\prime},t^{\\prime}|x,t)\\mathrm{d}x^{\\prime}}\\end{array}$ for any Borel set $A$ . By simulating this diffusion process forward in time, the distribution of $\\mathbf{X}_{t}$ will become $p_{\\mathrm{prior}}(\\mathbf{X})$ at the final time $T$ . In the literature, there exist various design choices of the SDE formulation in Eqn. (1) such that it transports the data distribution into the fixed prior distribution [98, 37, 99, 72, 97, 47]. ", "page_idx": 2}, {"type": "text", "text": "In order to sample $\\mathbf{X}_{0}\\sim p_{0}(\\mathbf{X}):=q_{\\mathrm{data}}(\\mathbf{X})$ , an intriguing fact can be leveraged: the reverse of a diffusion process is also a diffusion process [2]. This reverse process runs backward in time and can be formulated by the following time-reversal SDE: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{X}_{t}=\\left[\\mathbf{f}(\\mathbf{X}_{t},t)-\\sigma^{2}(t)\\nabla_{\\mathbf{X}_{t}}\\log p_{t}(\\mathbf{X}_{t})\\right]\\mathrm{d}t+\\sigma(t)\\mathrm{d}\\mathbf{B}_{t},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\nabla_{\\mathbf{X}}\\log p_{t}(\\mathbf{X})$ denote the score of the marginal distribution at time $t$ . If the score is known for all time, then we can derive the reverse diffusion process from Eqn. (2), sample from $p_{\\mathrm{prior}}(\\mathbf{X})$ , and simulate this process to generate samples from the data distribution $q_{\\mathrm{data}}(\\mathbf{X})$ . In particular, the score $\\nabla_{\\mathbf{X}}\\log p_{t}(\\mathbf{X})$ can be estimated by training a parameterized model $\\mathbf{s}_{\\theta}(\\mathbf{X},t)$ with a denoising score matching objective [98, 97]. In theory, the minimizer of this objective approximates the ground-truth score [99] and this objective is tractable. ", "page_idx": 2}, {"type": "text", "text": "3 Geometric Diffusion Bridge ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "As discussed in the introduction, effectively capturing the evolution of geometric states is crucial, for which three desiderata should be carefully considered: ", "page_idx": 2}, {"type": "text", "text": "\u2022 Coupling Preservation: From a probabilistic perspective, the evolution of geometric states transports their distribution from $q_{\\mathrm{data}}(S^{t_{0}})$ to $q_{\\mathrm{data}}\\bar{(S^{t_{1}})}$ , and we are interested in modeling the distribution of target geometric states given the initial states, i.e., $q_{\\mathrm{data}}(S^{t_{1}}|S^{t_{0}}):=$ $q_{\\mathrm{data}}(R^{t_{1}}|\\mathbf{H},R^{t_{0}})$ , which can be achieved by preserving the coupling of geometric states, i.e., $q_{\\mathrm{data}}(R^{t_{0}},R^{t_{1}}|\\mathbf{H})$ . For brevity, we hereafter omit the condition of $\\mathbf{H}$ because it keeps the same along the evolution and can be easily incorporated into the models. \u2022 Symmetry Constraints: Since the law governing the evolution is unchanged regardless of how the system is rotated or translated, the distribution of the geometric states should satisfy symmetry constraints, i.e., $q_{\\mathrm{data}}(\\rho^{\\mathcal{R}}(g)[R^{t_{1}}]|\\rho^{\\mathcal{R}}(g)[R^{t_{0}}])^{\\flat}\\,=\\,q_{\\mathrm{data}}(R^{t_{1}}|R^{t_{0}})$ and $q_{\\mathrm{data}}(\\bar{\\rho^{\\mathcal{R}}(g)}[R^{t_{0}}],\\rho^{\\mathcal{R}}(g)[R^{t_{1}}])=q_{\\mathrm{data}}(\\overbrace{R^{t_{0}}}^{\\star},R^{t_{1}})$ for all $g\\in\\mathrm{SE}(3),R^{t}\\in\\mathcal{R}$ . \u2022 Trajectory Guidance: Trajectories of geometric states are sometimes accessible and provide fine-grained descriptions of the evolution dynamics. For completeness, it is crucial to develop a unified framework that can characterize and leverage trajectory data as guidance for better bridging geometric states and capturing the evolution. ", "page_idx": 3}, {"type": "text", "text": "However, existing approaches typically have their limitations for this task, which we thoroughly discuss in Sec. 5 and summarize into Table 1. In this section, we introduce Geometric Diffusion Bridge (GDB), a general framework for bridging geometric states through generative modeling. We will elaborate on key techniques for completely preserving couping under symmetry constraints (Sec. 3.1), and demonstrate how our framework can be seamlessly extended to leverage trajectory data (Sec. 3.2). Theoretically, we conduct a thorough analysis on the capability of our unified framework, showing its completeness and superiority. All proofs of theorems are presented in Appendix B. A detailed guidance of practical implementing our framework is further provided (Sec. 3.3). ", "page_idx": 3}, {"type": "table", "img_path": "zcEPOB9rCR/tmp/aa5ad6ec0e933eb0bc2d4a566f705616234bc8967283fa708f98e45f21fa0a51.jpg", "table_caption": ["Table 1: Comparisons of different candidates for bridging geometric states "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.1 Equivariant Diffusion Bridge ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our key design lies in the construction of equivariant diffusion bridge, a tailored diffusion process $({\\bf R}^{t})_{t\\in[0,T]}$ for bridging initial states $\\mathbf{R}^{0}{\\sim}q_{\\mathrm{data}}(R^{t_{0}})$ and target states $\\breve{\\mathbf{R}}^{T}{\\sim}q_{\\mathrm{data}}(R^{t_{1}}|R^{t_{0}})$ , completely preserving coupling of geometric states and satisfying symmetry constraints. Firstly, we investigate necessary conditions for a diffusion process on geometric states to meet the symmetric constraints: ", "page_idx": 3}, {"type": "text", "text": "Proposition 3.1. Let $\\mathcal{R}$ denote the space of geometric states and $\\mathbf{f}_{\\mathcal{R}}(\\cdot,\\cdot):\\mathcal{R}\\times[0,T]\\rightarrow\\mathcal{R}$ denote the drift coefficient on $\\mathcal{R}$ . Let $(\\mathbf{W}^{t})_{t\\in[0,T]}$ denote the Wiener process on $\\mathcal{R}$ . Given an SDE on geometric states $\\mathrm{d}\\mathbf{R}^{t}=\\mathbf{f}_{\\mathcal{R}}(\\mathbf{R}^{t},t)\\mathrm{d}t+\\sigma(t)\\mathrm{\\dot{d}\\mathbf{W}}^{t}$ , ${\\bf R}^{0}\\sim q({\\bf R}^{0}).$ , its transition density $p_{\\mathcal{R}}(z^{\\prime},t^{\\prime}|z,t),z,z^{\\prime}\\in\\mathcal{R}$ is SE(3)-equivariant, i.e., $p_{\\mathcal{R}}(\\mathbf{R}^{t^{\\prime}},t^{\\prime}|\\mathbf{R}^{t},t)=p_{\\mathcal{R}}(\\rho^{\\mathcal{R}}(g)[\\mathbf{R}^{t^{\\prime}}],t^{\\prime}|\\rho^{\\mathcal{R}}(g)[\\mathbf{R}^{t}],t),\\forall g\\in\\mathrm{SE}(3),0\\leq$ $t,t^{\\prime}\\leq T_{\\mathrm{~\\,~}}$ , if these conditions are satisfied: (1) $q(\\mathbf{R}^{0})$ is $\\mathrm{SE}(3)$ -invariant; (2) $\\mathbf{f}_{\\mathcal{R}}(\\cdot,t)$ is SO(3)- equivariant and $\\mathrm{T}(3)$ -invariant; (3) the transition density of $(\\dot{\\mathbf{W}}^{t})_{t\\in[0,T]}$ is $\\mathrm{SE}(3)$ -equivariant. ", "page_idx": 3}, {"type": "text", "text": "Using Proposition 3.1, we can obtain a diffusion process that respect symmetry constraints by properly considering conditions for key components. Next, we modify a useful tool in probability theory called Doob\u2019s $h$ -transform [82, 81, 16], which plays an essential role in the construction of our equivariant diffusion bridge for preserving coupling of geometric states: ", "page_idx": 3}, {"type": "text", "text": "Proposition 3.2. Let $p_{\\mathcal{R}}(z^{\\prime},t^{\\prime}|z,t)$ be the transition density of the SDE in Proposition 3.1. Let $h_{\\mathcal{R}}(\\cdot,\\cdot):\\mathcal{R}\\times[0,T]\\stackrel{}{\\rightarrow}\\mathbb{R}_{>0}$ be a smooth function satisfying: $(I)$ $h_{\\mathcal{R}}(\\cdot,t)$ is SE(3)-invariant; (2) $\\begin{array}{r}{h_{\\mathcal{R}}(z,t)=\\int{\\bar{p_{\\mathcal{R}}}(z^{\\prime},t^{\\prime}|z,t)h_{\\mathcal{R}}(z^{\\prime},t^{\\prime})\\mathrm{d}z^{\\prime}}}\\end{array}$ . Then we can derive the following $h_{\\mathcal{R}}$ -transformed $S D E$ on geometric states: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{R}^{t}=\\left[\\mathbf{f}_{\\mathcal{R}}(\\mathbf{R}^{t},t)+\\sigma^{2}(t)\\nabla_{\\mathbf{R}^{t}}\\log h_{\\mathcal{R}}(\\mathbf{R}^{t},t)\\right]\\mathrm{d}t+\\sigma(t)\\mathrm{d}\\mathbf{W}^{t},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "$p_{\\mathcal{R}}^{h}(z^{\\prime},t^{\\prime}|z,t)$ $\\begin{array}{r}{p_{\\mathcal{R}}(z^{\\prime},t^{\\prime}|z,t)\\frac{h_{\\mathcal{R}}(z^{\\prime},t^{\\prime})}{h_{\\mathcal{R}}(z,t)}}\\end{array}$ ", "page_idx": 3}, {"type": "text", "text": "Proposition 3.2 provides an equivariant version of Doob\u2019s $h$ -transform, which can be used to guide a free SDE on geometric states to hit an event almost surely. For example, if we set $h_{\\mathcal{R}}(\\cdot,\\bar{t})=$ $p_{\\mathcal{R}}(z,T|\\cdot,t),z\\in\\mathcal{R}$ , i.e., the transition density of the original SDE evaluated at ${\\mathbf{R}}^{T}=z$ , then the $h_{\\mathcal{R}}$ -transformed SDE in Eqn. (3) arrives at the specific geometric state $z$ almost surely at the final time (see Proposition B.7 in the appendix for more details). Therefore, if we derive a proper $h_{\\mathcal{R}}(\\cdot,\\cdot)$ function under the symmetry constraints, our target process $(\\mathbf{R}^{t})_{t\\in[0,T]}$ can be constructed: ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.3 (Equivariant Diffusion Bridge). Let $\\mathrm{d}\\mathbf{R}^{t}\\,=\\,\\mathbf{f}_{\\mathcal{R}}(\\mathbf{R}^{t},t)\\mathrm{d}t+\\sigma(t)\\mathrm{d}\\mathbf{W}^{t}$ be an $S D E$ cParno pdoesriitvieo nt h3e. 1f.o llLoetw $\\begin{array}{r}{h_{\\mathcal{R}}(z,t;z_{0})=\\int p_{\\mathcal{R}}(z^{\\prime},T|z,t)\\frac{q_{d a t a}(z^{\\prime}|z_{0})}{p_{\\mathcal{R}}(z^{\\prime},T|z_{0},0)}\\mathrm{d}z^{\\prime}}\\end{array}$ $p_{\\mathcal{R}}(z^{\\prime},t^{\\prime}|z,t),z,z^{\\prime}\\,\\in\\,\\mathcal{R}$ By using Proposition 3.2, we $h_{\\mathcal{R}}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\vert\\mathbf{R}^{t}=\\left[\\mathbf{f}_{\\mathcal{R}}(\\mathbf{R}^{t},t)+\\sigma^{2}(t)\\mathbb{E}_{q\\approx(\\mathbf{R}^{T},T\\vert\\mathbf{R}^{t},t;\\mathbf{R}^{0},0)}[\\nabla_{\\mathbf{R}^{t}}\\log p_{\\mathcal{R}}(\\mathbf{R}^{T},T\\vert\\mathbf{R}^{t},t)\\vert\\mathbf{R}^{0},\\mathbf{R}^{t}]\\right]\\mathrm{d}t+\\sigma(t)\\mathrm{d}\\mathbf{W}^{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which corresponds to a process $(\\mathbf{R}^{t})_{t\\in[0,T]},\\mathbf{R}^{0}\\sim q_{d a t a}(R^{t_{0}})$ satisfying the following properties: ", "page_idx": 4}, {"type": "equation", "text": "$q_{\\mathcal{R}}(\\mathbf{R}^{t^{\\prime}},t^{\\prime}|\\mathbf{R}^{t},t;\\mathbf{R}^{0},0){=}q_{\\mathcal{R}}(\\rho^{\\mathcal{R}}(g)[\\mathbf{R}^{t^{\\prime}}],t^{\\prime}|\\rho^{\\mathcal{R}}(g)[\\mathbf{R}^{t}],t;\\rho^{\\mathcal{R}}(g)[\\mathbf{R}^{0}],0),$ ", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We call the tailored diffusion process $(\\mathbf{R}^{t})_{t\\in[0,T]}$ an equivariant diffusion bridge. ", "page_idx": 4}, {"type": "text", "text": "According to Theorem 3.3, given an initial geometric state $R^{t_{0}}$ , we can predict target geometric states $R^{t_{1}}$ by simulating the equivariant diffusion bridge $(\\mathbf{R}^{t})_{t\\in[0,T]}$ from $\\mathbf{\\dot{R}}^{0}=R^{\\tilde{t_{0}}}$ , which arrives at ${\\bf R}^{T}\\sim q_{\\mathrm{data}}(R^{t_{1}}|R^{t_{0}})$ . However, the score $\\mathbb{E}_{q\\mathcal{R}_{+}(\\mathbf{R}^{T},T|\\mathbf{R}^{t},t;\\mathbf{R}^{0},0)}\\big[\\nabla_{\\mathbf{R}_{+}^{t}}\\log p_{\\mathcal{R}}(\\mathbf{R}^{T},T|\\mathbf{R}^{t},t)|\\mathbf{R}^{0},\\mathbf{R}^{t}\\big]$ in Eqn. (4) is not tractable in general. Inspired by the score matching objective in diffusion models [99], we use a parameterized model ${\\bf v}_{\\theta}({\\bf R}^{t},t;{\\bf\\dot{R}}^{0})$ to estimate the score by using the following training objective: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Sigma(\\theta)=\\mathbb{E}_{(z_{0},z_{1})\\sim q_{\\mathrm{dat}}(R^{t_{0}},R^{t_{1}}),\\mathbf{R}^{t}\\sim q_{\\mathcal{R}}(\\mathbf{R}^{t},t|z_{1},T;z_{0},0)}\\lambda(t)||\\mathbf{v}_{\\theta}(\\mathbf{R}^{t},t;z_{0})\\!-\\!\\nabla_{\\mathbf{R}^{t}}\\log p_{\\mathcal{R}}(z_{1},T|\\mathbf{R}^{t},t)||^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $t\\,\\sim\\,\\mathcal{U}(0,T)$ (the uniform distribution on $[0,T])$ , and $\\lambda(\\cdot)\\,:\\,[0,T]\\,\\rightarrow\\,\\mathbb{R}_{\\geq0}$ is a positive weighting function. Theoretically, we prove that the minimizer of Eqn. (5) approximates the groundtruth score (see Appendix B.5 for more details). Moreover, this objective is tractable because the transition density $p_{\\mathcal{R}}$ and $q_{\\mathcal{R}}$ can be designed to have simple and explicit forms such as Gaussian, which we will elaborate on in Sec. 3.3. ", "page_idx": 4}, {"type": "text", "text": "3.2 Chain of Equivariant Diffusion Bridges for Leveraging Trajectory Guidance ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this subsection, we elaborate on how to leverage trajectories of geometric states as a fine-grained guidance in our framework. Let $(\\tilde{R}^{i})_{i\\in[N]}$ denote a trajectory of $N+1$ geometric states and $q_{\\mathrm{traj}}(\\tilde{R}^{0},...,\\tilde{R}^{N})$ denote the joint probability density function of geometric states in a trajectory. In practice, the markov property of trajectories typically holds [109, 78]. Under this assumption, $q_{\\mathrm{traj}}(\\tilde{R}^{0},...,\\tilde{R}^{N})$ can be equivalently reformulated into $\\begin{array}{r}{\\bar{q}_{\\mathrm{traj}}^{0}(\\tilde{R}^{0})\\prod_{i=1}^{N}q_{\\mathrm{traj}}^{i}(\\tilde{R}^{i}|\\tilde{R}^{i-1})}\\end{array}$ by the chain rule of probability. If $q_{\\mathrm{traj}}^{i}(\\tilde{R}^{i}|\\tilde{R}^{i-1})$ can be well modeled, we can capture the distribution of trajectories of geometric states completely. ", "page_idx": 4}, {"type": "text", "text": "According to Theorem 3.3, given ${\\bf R}^{0}\\sim q_{\\mathrm{traj}}^{0}(\\tilde{R}^{0})$ , an equivariant diffusion bridge $(\\mathbf{R}^{t})_{t\\in[0,T]}$ can be constructed to model the joint distribution $q_{\\mathrm{traj}}(\\tilde{R}^{0},\\tilde{R}^{1})$ and hence $q_{\\mathrm{traj}}^{1}(\\tilde{R}^{1}|\\tilde{R}^{0})$ is preserved. Therefore, if we construct a series of interconnected equivariant diffusion bridges, the distribution of trajectories can be modeled: ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.4 (Chain of Equivariant Diffusion Bridges). Let $\\{(\\mathbf{R}_{i}^{t})_{t\\in[0,T]}\\}_{i\\in[N-1]}$ denote a series of $N$ equivaraint diffusion bridges defined in Theorem 3.3. For the $i$ -th bridge $(\\bar{\\mathbf{R}}_{i}^{t})_{t\\in[0,T]}$ , if we set (1) hiR(z, t; z0) =  pR(z\u2032, T|z, t)pqRtr(ajz \u2032,(Tz \u2032||zz00,)0) ; (2) ${\\bf R}_{0}^{0}\\sim q_{t r a j}^{0}(\\tilde{R}^{0}),{\\bf R}_{i}^{0}={\\bf R}_{i-1}^{T},\\forall0<i<N_{\\!\\!\\perp}$ then the joint distribution $q_{\\mathcal{R}}(\\mathbf{R}_{0}^{0},\\mathbf{R}_{0}^{T},\\mathbf{R}_{1}^{T},\\cdot\\cdot\\cdot\\mathbf{\\Omega},\\mathbf{R}_{N-1}^{T})$ induced by $\\{(\\mathbf{R}_{i}^{t})_{t\\in[0,T]}\\}_{i\\in[N-1]}$ equals to $q_{t r a j}(\\tilde{R}^{0},...,\\tilde{R}^{N})$ . We call this process a chain of equivariant diffusion bridges. ", "page_idx": 4}, {"type": "text", "text": "In this way, a chain of equivariant diffusion bridge can be used to model prior trajectory data, and simulating this chain not only bridges initial and target geometric states but also yields intermediate evolving states. Similarly, we can also use a parameterized model to estimate the scores of bridges in this chain. Instead of having only one objective in all time steps, we now have $N$ bridges in total, which categorize the time span into $N$ groups with different time-dependent objectives. Therefore, by properly specifying time steps and initial conditions, the objective in Eqn. (5) can be seamlessly extended (see Appendix B.7 for more details on its provable guarantee): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}^{\\prime}(\\theta)=\\mathbb{E}_{(z_{0},\\ldots,z_{N})\\sim q_{\\mathrm{trij}}(\\Tilde{R}^{0},\\ldots,\\Tilde{R}^{N}),t,\\mathbf{R}_{i}^{\\prime}}\\lambda(t)\\lVert\\mathbf{v}_{\\theta}(\\mathbf{R}_{i}^{t^{\\prime}},t;z_{i})-\\nabla_{\\mathbf{R}_{i}^{\\prime\\prime}}\\log p_{\\mathcal{R}}^{i}(z_{i+1},T|\\mathbf{R}_{i}^{t^{\\prime}},t^{\\prime})\\rVert^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\textsf{e}t\\sim\\mathcal{U}(0,N\\times T),i=\\lfloor\\frac{t}{T}\\rfloor,t^{\\prime}=t-i\\times T,\\mathbf{R}_{i}^{t^{\\prime}}\\sim q_{\\mathcal{R}}^{i}(\\mathbf{R}_{i}^{t^{\\prime}},t^{\\prime}|z_{i+1},T;z_{i},0).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Lastly, we provide the following theoretical result, which further characterizes our framework\u2019s expressiveness to completely model the underlying dynamics that induce the trajectory distributions: ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.5. Assume $(\\tilde{R}^{i})_{i\\in[N]}$ is sampled by simulating a prior SDE on geometric states $\\mathrm{d}\\tilde{\\mathbf{R}}^{t}=$ $-\\nabla H_{\\mathcal{R}}^{*}(\\tilde{\\mathbf{R}}^{t})\\mathrm{d}t+\\sigma\\mathrm{d}\\tilde{\\mathbf{W}}^{t}$ . Let $\\mu_{i}^{*}$ denote the path measure of this prior SDE when $t\\in[i T,(i+1)T]$ . Building upon $(\\tilde{R}^{i})_{i\\in[N]}$ , let $\\{\\mu_{\\mathcal{R}}^{i}\\}_{i\\in[N-1]}$ denote the path measure of our chain of equivariant diffusion bridges. Under mild assumptions, we have $\\operatorname*{lim}_{N\\to\\infty}\\operatorname*{max}_{i}\\mathrm{KL}(\\mu_{i}^{*}||\\mu_{\\mathcal{R}}^{i})=0.$ . ", "page_idx": 5}, {"type": "text", "text": "It is noteworthy that the assumption of the prior SDE existence holds in various real-world applications. For example, in geometry optimization, we can formulate the iterative updating process of a molecular system as $\\mathrm{d}\\bar{\\mathbf{R^{\\prime}}}=-\\dot{\\alpha}\\nabla_{\\mathbf{R}^{t}}V(\\mathbf{R}^{t})\\mathrm{d}t+\\beta\\mathrm{d}\\mathbf{W}^{t}$ , where $V(\\mathbf{R}^{t})$ denotes the potential energy at $\\mathbf{R}^{t}$ and $\\alpha,\\beta$ are step sizes [88]. From Theorem 3.5, such prior SDE serves as the underlying law governing the evolution dynamic, and our chain of equivariant diffusion bridges constructed from empirical trajectory data can well approximate it, showing the completeness of our framework. ", "page_idx": 5}, {"type": "text", "text": "3.3 Practical Implementation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this subsection, we elaborate on how to practically implement our framework. According to Eqn. (5), it is necessary to carefully design (1) tractable distribution $q_{\\mathcal{R}}(\\mathbf{R}^{t},t|z_{1},T;z_{0},0)$ for sampling $\\mathbf{R}^{t}$ ; (2) closed-form matching objective $\\nabla_{\\mathbf{R}^{t}}\\log p_{\\mathcal{R}}(z_{1},T|\\mathbf{R}^{t},t)$ . ", "page_idx": 5}, {"type": "text", "text": "Matching objective. Inspired by diffusion models that use Gaussian transition kernels for tractable computation, we design the SDE on geometric states in Proposition 3.1 to be: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{R}^{t}=\\sigma\\mathrm{d}\\mathbf{W}^{t},\\quad w i t h\\ t r a n s i t i o n\\ d e n s i t y\\quad p_{\\mathcal{R}}(z^{\\prime},t^{\\prime}|z,t)=\\mathcal{N}(z_{0},\\sigma^{2}(t^{\\prime}-t)\\mathbf{I})\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "$\\begin{array}{r}{\\nabla_{\\mathbf{R}^{t}}\\log p_{\\mathcal{R}}(z_{1},T|\\mathbf{R}^{t},t)=\\frac{z_{1}-\\mathbf{R}^{t}}{\\sigma^{2}(T-t)}}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "Sampling distribution. According to Theorem 3.3, the transition density $q_{\\mathcal{R}}(\\mathbf{R}^{t},t|z_{1},T;z_{0},0)$ can be calculated by using the Doob\u2019s $h$ -transform in Proposition 3.2, i.e., $q_{\\mathcal{R}}(\\mathbf{R}^{t},t|z_{1},T;z_{0},0)=$ pR(Rt, t|z1, T) hhRR((zR1,,Tt ;;zz00)) . Moreover, $h_{\\mathcal{R}}$ is determined by $q_{\\mathrm{data}}$ and $p_{\\mathcal{R}}$ , which is already specified in Eqn. (7). Therefore, we can also calculate $\\begin{array}{r}{q_{\\mathcal{R}}(\\mathbf{R}^{t},t|z_{1},T;z_{0},0)=\\mathcal{N}(\\frac{t}{T}z_{1}+\\frac{T-t}{T}z_{0},\\sigma^{2}\\frac{t(T-t)}{T^{2}}\\mathbf{I}).}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "Symmetry constraints. In proposition 3.1, we have several conditions that should be satisfied to meet the symmetry constraints. Firstly, since a parameterized model ${\\bf v}_{\\theta}({\\bf R}^{t},t;{\\bf R}^{0})$ is used to estimate the score of our equivariant diffusion bridge, it should be SO(3)-equivariant and $\\mathrm{T}(3)$ -invariant. Besides, we follow [50, 115] to consider CoM-free systems: given $R\\,=\\,\\{{\\bf r}_{1},...,{\\bf r}_{n}\\}$ , we define $\\textstyle{\\bar{\\mathbf{r}}}={\\frac{1}{n}}\\sum_{i=1}^{n}\\mathbf{r}_{i}$ and the CoM-free version of $R=\\{{\\bf r}_{1}-\\bar{\\bf r},...,{\\bf r}_{n}-\\bar{\\bf r}\\}$ . To sample from $\\mathcal{N}(z_{0},\\sigma^{2}\\mathbf{I})$ with $z_{0}\\in\\mathcal{R}$ consisting of $n$ objects, we (1) sample $\\epsilon=\\{\\epsilon_{i}\\}_{i=1}^{n}$ by i.i.d. drawing $\\epsilon_{i}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I}_{3})$ ; (2) calculate the CoM-free $\\epsilon^{\\prime}$ of $\\epsilon$ ; (3) obtain $z_{0}+\\sigma\\epsilon^{\\prime}$ . ", "page_idx": 5}, {"type": "text", "text": "Trajectory guidance. According to Eqn. (6), both $p_{\\mathcal{R}}^{i}$ and $q_{\\mathcal{R}}^{i}$ for all $i\\{N{-}1\\}$ should be determined. Similarly, we set $p_{\\mathcal{R}}^{i}(z_{i+1},T|\\mathbf{R}^{t^{\\prime}},t^{\\prime}){=}\\mathcal{N}(\\mathbf{R}^{t^{\\prime}},\\sigma_{i}^{2}(T{-}t^{\\prime})\\mathbf{I})$ , which further induces $\\begin{array}{r}{q_{\\mathcal{R}}^{i}(\\mathbf{R}^{t^{\\prime}},t^{\\prime}|z_{i+1},T;z_{i},0)=\\mathcal{N}(\\frac{t^{\\prime}}{T}z_{i+1}+\\frac{T-t^{\\prime}}{T}z_{i},\\sigma_{i}^{2}\\frac{t^{\\prime}(T-t^{\\prime})}{T^{2}}\\mathbf{I})}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "Combining all the above design choices, we have the following algorithms for training our Geometric Diffusion Bridge (Alg. 3) and leveraging trajectory guidance if available (Alg. 4). After the model is well trained, we leverage ODE numerical solvers [12] to simulate the bridge process by using its equivalent probability flow ODE [99]. In this way, we can effectively and deterministically predict future geometric states of interest from initial states in an efficient iterative process. Lastly, it is also noteworthy that our framework is general to be implemented by using other advanced design strategies [99, 47, 48], which we leave as future work. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "zcEPOB9rCR/tmp/8ff2d466d3ff929e472e2377ef14eaccf804f32de21fe11877a53dca9f1280af.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we empirically study the effectiveness of our Geometric Diffusion Bridge on crucial real-world challenges requiring bridging geometric states. In particular, we carefully design several experiments covering different types of data, scales and scenarios, as shown in Table 2. Due to space limits, we present more details in Appendix D. ", "page_idx": 6}, {"type": "table", "img_path": "zcEPOB9rCR/tmp/268c36a494665de7378f5fb7d5a6ed00918dce5d3d3dc1f6b6bf8f3012813f51.jpg", "table_caption": ["Table 2: Summary of experimental setup. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.1 Equilibrium State Prediction ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Task. Equilibrium states typically represent local minima on the Born-Oppenheimer potential energy surface of a molecular system [54], which correspond to its most stable geometric state and play an essential role in determining its properties in various aspects [4, 21]. In this task, our goal is to accurately predict the equilibrium state from the initial geometric state of a molecular system. ", "page_idx": 6}, {"type": "text", "text": "Dataset. Two popular datasets are used: (1) QM9 [79] is a medium-scale dataset that has been widely used for molecular modeling, consisting of $\\tilde{1}30{,}000$ organic molecules. In convention, 110k, 10k, and 11k molecules are used for train/valid/test sets respectively; (2) Molecule3D [116] is a largescale dataset curated from the PubChemQC project [67, 71], consisting of 3,899,647 molecules in total and its train/valid/test splitting ratio is $6:2:2$ . In particular, both random and scaffold splitting methods are adopted to thoroughly evaluate the in-distribution and out-of-distribution performance. For each molecule, an initial geometric state is generated by using fast and coarse force field [73, 52] and geometry optimization is conducted to obtain DFT-calculated equilibrium geometric structure. ", "page_idx": 6}, {"type": "text", "text": "Setting. In this task, we parameterize ${\\bf v}_{\\theta}({\\bf R}^{t},t;{\\bf R}^{0})$ by extending a Graph-Transformer based equivariant network [92, 63] to encode both time steps and initial geometric states as conditions. For inference, we use 10 time steps with the Euler solver [12]. Following [111], we choose several strong baselines for a comprehensive comparison, and use three metrics for measuring the error between predicted target states and ground-truth states: C-RMSD, D-MAE and D-RMSE. The detailed descriptions of the baselines, evaluation metrics and training settings are presented in Appendix D.1. ", "page_idx": 6}, {"type": "text", "text": "Results. Results on QM9 and Molecule3D are shown in Table 3 and 4 respectively. It can be easily seen that our GDB framework consistently surpasses all baselines by a significantly large margin on ", "page_idx": 6}, {"type": "table", "img_path": "zcEPOB9rCR/tmp/4fd03a41e30960afed75eedf09bbfccb13e02cb65dea842e9d2f29a732f392cc.jpg", "table_caption": ["Table 3: Results on the QM9 dataset (\u00c5). We report the official results of baselines from [111] "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "zcEPOB9rCR/tmp/7d7adc600642859ed5a9b08c0f8b9fd5567339cc8155a61d967ab33ee26e4ba9.jpg", "table_caption": ["Table 4: Results on the Molecule3D dataset (\u00c5). We report the official results of baselines from [111] "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "QM9, e.g., $60.5\\%/59.7\\%$ relative C-RMSD reduction on valid/test sets respectively, establishing a new state-of-the-art performance. Similar trends also can be observed in Molecule3D, i.e., $12.6\\%/13.2\\%$ relative C-RMSD reduction for valid/test sets of the random split and $12.7\\%/13.0\\%$ reduction for the scaffold split, largely outperforming the best baseline. These significant error reduction results show the superiority of our GDB framework for bridging geometric states, and its generality on both medium and large-scale challenges. Moreover, our framework performs consistently across valid and tests of both random and scaffold splits, further verifying its robustness in challenging scenarios. ", "page_idx": 7}, {"type": "text", "text": "4.2 Structure Relaxation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Task. Catalyst discovery is crucial for various applications. Adsorbate candidates are placed on catalyst surfaces and evolve through structure relaxation to adsorption states, in which the adsorption structures can be determined for measuring catalyst activity and selectivity. Our goal is thus to accurately predict adsorption states from initial states of adsorbate-catalyst complexes. ", "page_idx": 7}, {"type": "text", "text": "Dataset. We adopt Open Catalyst 2022 (OC22) dataset [105], which has great significance for the development of Oxygen Evolution Reaction (OER) catalysts. Each data is in the form of the adsorbatecatalyst complex. Both initial and adsorption states with trajectories connecting them are provided. The training set consists of 45,890 catalyst-adsorbate complexes. To better evaluate the model\u2019s performance, the validation and test sets consider the in-distribution (ID) and out-of-distribution (OOD) settings which use unseen catalysts, containing approximately 2,624 and 2,780 complexes respectively. ", "page_idx": 7}, {"type": "text", "text": "Setting. Following [105], we use the Average Distance within Threshold (ADwT) as the evaluation metric, which reflects the percentage of structures with an atom position MAE below thresholds. We parameterize ${\\bf v}_{\\theta}({\\bf R}^{t},t;{\\bf\\^{\\alpha}R}^{0})$ by using GemNet-OC [34], which also serves as a verification that our framework is compatible with different backbone models. For inference, we also use 10 time steps with the Euler solver. Following [105], we choose strong MLFF baselines trained on force field data for a challenging comparison. The detailed descriptions of baselines and settings are presented in Appendix D.2. ", "page_idx": 7}, {"type": "table", "img_path": "zcEPOB9rCR/tmp/210f50212ef9c986873d878fd2757d0d925c1f73d6d94b22e5d7a044269a4b83.jpg", "table_caption": ["Table 5: Results on the OC22 IS2RS Validation set. \" $\\mathrm{^{\\prime}O C20+O C22\"}$ denotes using both OC20 [13] and OC22 data; $\"\\mathrm{OC}20{\\rightarrow}\\mathrm{OC}22\"$ means pre-training on OC20 data then fine-tuning on OC22 data; \"OC22-only\" means only using OC22 data. We report the official results of baselines from [105] "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Results. In Table 5, our GDB significantly outperforms the best baseline, e.g., $3.3\\%/3.6\\%/3.4\\%$ relative improvement on the ADwT metric of ID, OOD and Avg respectively. It is noteworthy that the best baseline is the GemNet-OC force field trained on both OC20 and OC22 data, which is 10 times more than OC22 data only. Nevertheless, our framework still achieves better performance on predicting the adsorption geometric states. Moreover, our framework without using any trajectory data still can achieve better performance compared to the best baseline, e.g., 58.54 v.s. $57.42\\,\\mathrm{Avg}[\\%]$ . All the results on this challenging task further demonstrate the superiority and completeness of our framework. ", "page_idx": 8}, {"type": "text", "text": "Ablation study. Furthermore, we conduct ablation studies to examine key designs of our framework in Table 5. Firstly, we can see that using trajectory guidance indeed improves the performance of our framework, e.g., $1.4\\%$ relative improvement on Avg ADwT. Moreover, we also investigate the impact of $\\mathbf{R}^{0}$ condition in ${\\bf v}_{\\theta}({\\bf R}^{t},t;{\\bf R}^{0})$ , which plays an essential role in preserving the joint distribution of geometric states. Without this condition, we can see a significant drop, e.g., $6.5\\%/10.3\\%$ relative ADwT drop on Avg/OOD respectively. Overall, these ablation studies serve as strong supports on the necessity of developing a unified framework that can precisely bridge geometric states by preserving their joint distributions and effectively leverage trajectory data as guidance for enhanced performance. ", "page_idx": 8}, {"type": "text", "text": "5 Related Works ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Direct Prediction. One line of approach for bridging geometric states is direct prediction, i.e., training a model to directly predict target geometric states given initial states as input. Models that carefully respect symmetry constraints such as the equivariance to 3D rotations and translations are typically used, which are called Geometric Equivariant Networks [11, 36, 120, 27]. Different techniques have been explored to encode such priors, which mainly include vector operations such as scalar and vector product [35, 87, 89, 41, 103, 14], e.g., the scalar-vector product used in EGNN [87], and tensor product based operations [104, 31, 8, 57, 64]. Despite its simplicity and efficiency, direct prediction requires encoding the iterative evolution of geometric states into a single-step prediction model, which lacks the ability to capture the underlying dynamics and cannot leverage trajectories of geometric states. ", "page_idx": 8}, {"type": "text", "text": "Machine Learning Force Field. Another line of approach is called machine learning force field (MLFF) [106, 5, 6, 70, 75, 58], which are trained to predict intermediate labels, such as the potential energy or force of the (local) current geometric state instead. After training, MLFFs can be used to simulate the trajectory of geometric states over time based on underlying equations. Using Geometric Equivariant Networks as the backbone, MLFFs typically satisfy the symmetry constraints. Besides, trajectory data with additional energy or force labels can directly be used for training MLFFs. However, this paradigm highly depends on the existence and quality of intermediate labels since small local errors in energy or force prediction can accumulate along the simulation process [7, 106, 30]. Moreover, there exists no guarantee that MLFFs can completely model joint state distributions, which is another limitation for bridging geometric states. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Geometric Diffusion Models. In recent years, diffusion models [37, 99] have emerged with stateof-the-art generative modeling performance across various domains [85, 108, 51, 56]. In geometric domain, diffusion models are typically used for molecule conformation generation [115, 114, 38] and protein design [108, 117]. By properly design the noising process and model architectures, symmetry constraints on the transition kernel and prior distribution can be satisfied, which guarantees the generated data is sampled from roto-translational invariant distributions [115, 38]. In addition to the score-based formulation, recent advances further extend new techniques such as flow matching [59, 61, 1] to satisfy symmetry constraints for these generation tasks [49, 100]. Nevertheless, there exists no guaratee that these approaches can model the joint distribution of geometric states [61, 96]. And how to leverage trajectory data as guidance for bridging geometric states is also challenging. ", "page_idx": 9}, {"type": "text", "text": "Other techniques. MoreRed [45] trains a diffusion model on equilibrium molecule conformations with a time step predictor, and directly use it for bridging any conformations to their equilibrium states. GTMGC [111] instead develop a Graph Transformer to directly predict equilibrium conformations from their 2D graph forms. Both of them are limited to the equilibrium conformation prediction task, cannot preserve the joint state distribution and leverage trajectory data. EGNO [112] is a concurrent work that develops a neural operator based approach to model dynamics of trajectories. By carefully designing temporal convolution in fourier spaces, EGNO can learn from trajectory data. However, this tailored approach cannot be directly used without trajectory guidance. To preserve joint data distributions, [22, 121] coincide with us to leverage Doob\u2019s $h$ -transform to repurposing standard diffusion processes, but they do not respect symmetry constraints and cannot leverage trajectories. There also exist recent works that study the diffusion bridge framework [76, 93] and apply it to various domains such as images and graphs [110, 62, 42]. Compared to all above approaches, our GDB framework stands out as a unique and ideal solution that can precisely bridge geometric states and effectively leverage trajectory data (if available) in a unified manner. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we introduce Geometric Diffusion Bridge (GDB), a general framework for bridging geometric states through generative modeling. We leverage a modified version of Doob\u2019s $h$ -transform to constructe an equivariant diffusion bridge for bridging initial and target geometric states. Trajectory data can further be seamlessly leveraged as guidance by using a chain of equivariant diffusion bridges, allowing complete modeling of trajectory data. Mathematically, we conduct a comprehensive theoretical analysis showing our framework\u2019s ability to preserve joint distributions of geometric states and capability to completely model the evolution dynamics. Empirical comparisons on different settings show that our GDB significantly surpasses existing state-of-the-art approaches and ablation studies further underscore the necessity of several key designs in our framework. In the future, it is worth exploring better implementation strategies of our framework for enhanced performance, and applying our GDB to other critical challenges involving bringing geometric states. ", "page_idx": 9}, {"type": "text", "text": "Broader Impacts and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work newly proposes a general framework to bridge geometric states, which has great significance in various scientific domains. Our experimental results have also demonstrated considerable positive potential for various applications, such as catalyst discovery and molecule optimization, which can significantly contribute to the advancement of renewable energy processes and chemistry discovery. However, it is essential to acknowledge the potential negative impacts including the development of toxic drugs and materials. Thus, stringent measures should be implemented to mitigate these risks. ", "page_idx": 9}, {"type": "text", "text": "There also exist some limitations to our work. For the sake of generality, we do not experiment with advanced implementation strategies of training objectives and sampling algorithms, which leave room for further improvement. Besides, the employment of Transformer-based architectures may also limit the efficiency of our framework. This has also become a common issue in transformer-based diffusion models, which we have earmarked for future research. ", "page_idx": 10}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank all the anonymous reviewers for the very careful and detailed reviews as well as the valuable suggestions. Their help has further enhanced our work. Liwei Wang is supported by National Science and Technology Major Project (2022ZD0114902) and National Science Foundation of China (NSFC62276005). Di He is supported by National Science Foundation of China (NSFC62376007). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Michael S Albergo, Nicholas M Boff,i and Eric Vanden-Eijnden. Stochastic interpolants: A unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023.   \n[2] Brian DO Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3):313\u2013326, 1982. [3] Muratahan Aykol, Joseph H Montoya, and Jens Hummelsh\u00f8j. Rational solid-state synthesis routes for inorganic materials. Journal of the American Chemical Society, 143(24):9244\u20139259, 2021.   \n[4] Keld L Bak, J\u00fcrgen Gauss, Poul J\u00f8rgensen, Jeppe Olsen, Trygve Helgaker, and John F Stanton. The accurate determination of molecular equilibrium structures. The Journal of Chemical Physics, 114(15):6548\u20136556, 2001. [5] Ilyes Batatia, David P Kovacs, Gregor Simm, Christoph Ortner, and G\u00e1bor Cs\u00e1nyi. Mace: Higher order equivariant message passing neural networks for fast and accurate force fields. Advances in Neural Information Processing Systems, 35:11423\u201311436, 2022. [6] Simon Batzner, Albert Musaelian, Lixin Sun, Mario Geiger, Jonathan P Mailoa, Mordechai Kornbluth, Nicola Molinari, Tess E Smidt, and Boris Kozinsky. E (3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials. Nature communications, 13(1):2453, 2022.   \n[7] Jorg Behler. Perspective: Machine learning potentials for atomistic simulations. The Journal of chemical physics, 145(17), 2016.   \n[8] Johannes Brandstetter, Rob Hesselink, Elise van der Pol, Erik J Bekkers, and Max Welling. Geometric and physical quantities improve e(3) equivariant message passing. In International Conference on Learning Representations, 2022.   \n[9] Linda J Broadbelt, Scott M Stark, and Michael T Klein. Computer generated pyrolysis modeling: on-the-fly generation of species, reactions, and rates. Industrial & Engineering Chemistry Research, 33(4):790\u2013799, 1994.   \n[10] Shaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? arXiv preprint arXiv:2105.14491, 2021.   \n[11] Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Velic\u02c7kovic\u00b4. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. arXiv preprint arXiv:2104.13478, 2021.   \n[12] John Charles Butcher. Numerical methods for ordinary differential equations. John Wiley & Sons, 2016.   \n[13] Lowik Chanussot, Abhishek Das, Siddharth Goyal, Thibaut Lavril, Muhammed Shuaibi, Morgane Riviere, Kevin Tran, Javier Heras-Domingo, Caleb Ho, Weihua Hu, et al. Open catalyst 2020 (oc20) dataset and community challenges. Acs Catalysis, 11(10):6059\u20136072, 2021.   \n[14] Tianlang Chen, Shengjie Luo, Di He, Shuxin Zheng, Tie-Yan Liu, and Liwei Wang. GeoMFormer: A general architecture for geometric molecular representation learning. In Forty-first International Conference on Machine Learning, 2024.   \n[15] Stefan Chmiela, Alexandre Tkatchenko, Huziel E Sauceda, Igor Poltavsky, Kristof T Sch\u00fctt, and Klaus-Robert M\u00fcller. Machine learning of accurate energy-conserving molecular force fields. Science advances, 3(5):e1603015, 2017.   \n[16] Kai Lai Chung and John B Walsh. Markov processes, Brownian motion, and time symmetry, volume 249. Springer Science & Business Media, 2006.   \n[17] Jonathan Clayden, Nick Greeves, and Stuart Warren. Organic chemistry. Oxford University Press, USA, 2012.   \n[18] John F Cornwell. Group theory in physics: An introduction. Academic press, 1997.   \n[19] F Albert Cotton. Chemical applications of group theory. John Wiley & Sons, 1991.   \n[20] F Albert Cotton, Geoffrey Wilkinson, Carlos A Murillo, and Manfred Bochmann. Advanced inorganic chemistry. John Wiley & Sons, 1999.   \n[21] Attila G Cs\u00e1sz\u00e1r, G\u00e1bor Czak\u00f3, Tibor Furtenbacher, Jonathan Tennyson, Viktor Szalay, Sergei V Shirin, Nikolai F Zobov, and Oleg L Polyansky. On equilibrium structures of the water molecule. The Journal of chemical physics, 122(21), 2005.   \n[22] Valentin De Bortoli, Guan-Horng Liu, Tianrong Chen, Evangelos A Theodorou, and Weilie Nie. Augmented bridge matching. arXiv preprint arXiv:2311.06978, 2023.   \n[23] Jonas Degrave, Federico Felici, Jonas Buchli, Michael Neunert, Brendan Tracey, Francesco Carpanese, Timo Ewalds, Roland Hafner, Abbas Abdolmaleki, Diego de Las Casas, et al. Magnetic control of tokamak plasmas through deep reinforcement learning. Nature, 602(7897):414\u2013 419, 2022.   \n[24] Amanda L Dewyer, Alonso J Arg\u00fcelles, and Paul M Zimmerman. Methods for exploring reaction space in molecular systems. Wiley Interdisciplinary Reviews: Computational Molecular Science, 8(2):e1354, 2018.   \n[25] Jacob D Durrant and J Andrew McCammon. Molecular dynamics simulations and drug discovery. BMC biology, 9:1\u20139, 2011.   \n[26] Rick Durrett. Probability: theory and examples, volume 49. Cambridge university press, 2019.   \n[27] Alexandre Duval, Simon V Mathis, Chaitanya K Joshi, Victor Schmidt, Santiago Miret, Fragkiskos D Malliaros, Taco Cohen, Pietro Li\u00f2, Yoshua Bengio, and Michael Bronstein. A hitchhiker\u2019s guide to geometric gnns for 3d atomic systems. arXiv preprint arXiv:2312.07511, 2023.   \n[28] Andreas Eberle. Stochastic analysis.   \n[29] Ferran Feixas, Steffen Lindert, William Sinko, and J Andrew McCammon. Exploring the role of receptor flexibility in structure-based drug discovery. Biophysical chemistry, 186:31\u201345, 2014.   \n[30] Xiang Fu, Zhenghao Wu, Wujie Wang, Tian Xie, Sinan Keten, Rafael Gomez-Bombarelli, and Tommi S. Jaakkola. Forces are not enough: Benchmark and critical evaluation for machine learning force fields with molecular simulations. Transactions on Machine Learning Research, 2023. Survey Certification.   \n[31] Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. Se (3)-transformers: 3d roto-translation equivariant attention networks. Advances in neural information processing systems, 33:1970\u20131981, 2020.   \n[32] Johannes Gasteiger, Florian Becker, and Stephan G\u00fcnnemann. Gemnet: Universal directional graph neural networks for molecules. Advances in Neural Information Processing Systems, 34:6790\u20136802, 2021.   \n[33] Johannes Gasteiger, Janek Gro\u00df, and Stephan G\u00fcnnemann. Directional message passing for molecular graphs. arXiv preprint arXiv:2003.03123, 2020.   \n[34] Johannes Gasteiger, Muhammed Shuaibi, Anuroop Sriram, Stephan G\u00fcnnemann, Zachary Ward Ulissi, C. Lawrence Zitnick, and Abhishek Das. Gemnet-OC: Developing graph neural networks for large and diverse molecular simulation datasets. Transactions on Machine Learning Research, 2022.   \n[35] Mojtaba Haghighatlari, Jie Li, Xingyi Guan, Oufan Zhang, Akshaya Das, Christopher J Stein, Farnaz Heidar-Zadeh, Meili Liu, Martin Head-Gordon, Luke Bertels, et al. Newtonnet: A newtonian message passing network for deep learning of interatomic potentials and forces. Digital Discovery, 1(3):333\u2013343, 2022.   \n[36] Jiaqi Han, Yu Rong, Tingyang Xu, and Wenbing Huang. Geometrically equivariant graph neural networks: A survey. arXiv preprint arXiv:2202.07230, 2022.   \n[37] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[38] Emiel Hoogeboom, V\u0131ctor Garcia Satorras, Cl\u00e9ment Vignac, and Max Welling. Equivariant diffusion for molecule generation in 3d. In International conference on machine learning, pages 8867\u20138887. PMLR, 2022.   \n[39] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. arXiv preprint arXiv:1905.12265, 2019.   \n[40] Frank Jensen. Introduction to computational chemistry. John wiley & sons, 2017.   \n[41] Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael John Lamarre Townshend, and Ron Dror. Learning from protein structure with geometric vector perceptrons. In International Conference on Learning Representations, 2021.   \n[42] Jaehyeong Jo, Dongki Kim, and Sung Ju Hwang. Graph generation with destination-predicting diffusion mixture, 2024.   \n[43] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin \u017d\u00eddek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583\u2013589, 2021.   \n[44] Wolfgang Kabsch. A discussion of the solution for the best rotation to relate two sets of vectors. Acta Crystallographica Section A: Crystal Physics, Diffraction, Theoretical and General Crystallography, 34(5):827\u2013828, 1978.   \n[45] Khaled Kahouli, Stefaan Simon Pierre Hessmann, Klaus-Robert M\u00fcller, Shinichi Nakajima, Stefan Gugler, and Niklas Wolf Andreas Gebauer. Molecular relaxation by reverse diffusion with time step prediction. arXiv preprint arXiv:2404.10935, 2024.   \n[46] Martin Karplus and J Andrew McCammon. Molecular dynamics simulations of biomolecules. Nature structural biology, 9(9):646\u2013652, 2002.   \n[47] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems, 35:26565\u201326577, 2022.   \n[48] Diederik Kingma and Ruiqi Gao. Understanding diffusion objectives as the elbo with simple data augmentation. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 65484\u201365516. Curran Associates, Inc., 2023.   \n[49] Leon Klein, Andreas Kr\u00e4mer, and Frank No\u00e9. Equivariant flow matching. Advances in Neural Information Processing Systems, 36, 2024.   \n[50] Jonas K\u00f6hler, Leon Klein, and Frank No\u00e9. Equivariant flows: exact likelihood generative learning for symmetric densities. In International conference on machine learning, pages 5361\u20135370. PMLR, 2020.   \n[51] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. In International Conference on Learning Representations, 2021.   \n[52] Greg Landrum. Rdkit: Open-source cheminformatics software. Github, 2016.   \n[53] Christian L\u00e9onard. Girsanov theory under a finite entropy condition. In S\u00e9minaire de Probabilit\u00e9s XLIV, pages 429\u2013465. Springer, 2012.   \n[54] Ira N Levine, Daryle H Busch, and Harrison Shull. Quantum chemistry, volume 6. Pearson Prentice Hall Upper Saddle River, NJ, 2009.   \n[55] Raphael D Levine. Molecular reaction dynamics. Cambridge University Press, 2009.   \n[56] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang, and Tatsunori B Hashimoto. Diffusion-lm improves controllable text generation. Advances in Neural Information Processing Systems, 35:4328\u20134343, 2022.   \n[57] Yi-Lun Liao and Tess Smidt. Equiformer: Equivariant graph attention transformer for 3d atomistic graphs. arXiv preprint arXiv:2206.11990, 2022.   \n[58] Yi-Lun Liao, Brandon M Wood, Abhishek Das, and Tess Smidt. Equiformerv2: Improved equivariant transformer for scaling to higher-degree representations. In The Twelfth International Conference on Learning Representations, 2024.   \n[59] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023.   \n[60] Meng Liu, Cong Fu, Xuan Zhang, Limei Wang, Yaochen Xie, Hao Yuan, Youzhi Luo, Zhao Xu, Shenglong Xu, and Shuiwang Ji. Fast quantum property prediction via deeper 2d and 3d graph networks. arXiv preprint arXiv:2106.08551, 2021.   \n[61] Xingchao Liu, Chengyue Gong, and qiang liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2023.   \n[62] Xingchao Liu, Lemeng Wu, Mao Ye, and qiang liu. Learning diffusion bridges on constrained domains. In The Eleventh International Conference on Learning Representations, 2023.   \n[63] Shuqi Lu, Zhifeng Gao, Di He, Linfeng Zhang, and Guolin Ke. Highly accurate quantum chemical property prediction with uni-mol+. arXiv preprint arXiv:2303.16982, 2023.   \n[64] Shengjie Luo, Tianlang Chen, and Aditi S. Krishnapriyan. Enabling efficient equivariant operations in the fourier basis via gaunt tensor products. In The Twelfth International Conference on Learning Representations, 2024.   \n[65] Shengjie Luo, Tianlang Chen, Yixian Xu, Shuxin Zheng, Tie-Yan Liu, Liwei Wang, and Di He. One transformer can understand both 2d & 3d molecular data. In The Eleventh International Conference on Learning Representations, 2023.   \n[66] Shengjie Luo, Shanda Li, Shuxin Zheng, Tie-Yan Liu, Liwei Wang, and Di He. Your transformer may not be as powerful as you expect. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.   \n[67] Nakata Maho. The pubchemqc project: A large chemical database from the first principle calculations. In AIP conference proceedings, volume 1702, page 090058. AIP Publishing LLC, 2015.   \n[68] Richard M Martin. Electronic structure: basic theory and practical methods. Cambridge university press, 2020.   \n[69] Amil Merchant, Simon Batzner, Samuel S Schoenholz, Muratahan Aykol, Gowoon Cheon, and Ekin Dogus Cubuk. Scaling deep learning for materials discovery. Nature, 624(7990):80\u201385, 2023.   \n[70] Albert Musaelian, Simon Batzner, Anders Johansson, Lixin Sun, Cameron J Owen, Mordechai Kornbluth, and Boris Kozinsky. Learning local equivariant representations for large-scale atomistic dynamics. Nature Communications, 14(1):579, 2023.   \n[71] Maho Nakata and Tomomi Shimazaki. Pubchemqc project: a large-scale first-principles electronic structure database for data-driven chemistry. Journal of chemical information and modeling, 57(6):1300\u20131308, 2017.   \n[72] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International conference on machine learning, pages 8162\u20138171. PMLR, 2021.   \n[73] Noel M O\u2019Boyle, Michael Banck, Craig A James, Chris Morley, Tim Vandermeersch, and Geoffrey R Hutchison. Open babel: An open chemical toolbox. Journal of cheminformatics, 3:1\u201314, 2011.   \n[74] Bernt \u00d8ksendal and Bernt \u00d8ksendal. Stochastic differential equations. Springer, 2003.   \n[75] Saro Passaro and C Lawrence Zitnick. Reducing so (3) convolutions to so (2) for efficient equivariant gnns. In International Conference on Machine Learning, pages 27420\u201327438. PMLR, 2023.   \n[76] Stefano Peluchetti. Diffusion bridge mixture transports, schr\u00f6dinger bridge problems and generative modeling. Journal of Machine Learning Research, 24(374):1\u201351, 2023.   \n[77] Stefano Peluchetti. Non-denoising forward-time diffusions. arXiv preprint arXiv:2312.14589, 2023.   \n[78] Jan-Hendrik Prinz, Hao Wu, Marco Sarich, Bettina Keller, Martin Senne, Martin Held, John D Chodera, Christof Sch\u00fctte, and Frank No\u00e9. Markov models of molecular kinetics: Generation and validation. The Journal of chemical physics, 134(17), 2011.   \n[79] Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole Von Lilienfeld. Quantum chemistry structures and properties of 134 kilo molecules. Scientific data, 1(1):1\u20137, 2014.   \n[80] Ladislav Ramp\u00e1\u0161ek, Michael Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, and Dominique Beaini. Recipe for a general, powerful, scalable graph transformer. Advances in Neural Information Processing Systems, 35:14501\u201314515, 2022.   \n[81] Dennis C Rapaport. The art of molecular dynamics simulation. Cambridge university press, 2004.   \n[82] L Chris G Rogers and David Williams. Diffusions, Markov processes and martingales: Volume 2, It\u00f4 calculus, volume 2. Cambridge university press, 2000.   \n[83] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10684\u201310695, June 2022.   \n[84] David Rosenberger, Justin S Smith, and Angel E Garcia. Modeling of peptides with classical and novel machine learning force fields: A comparison. The Journal of Physical Chemistry B, 125(14):3598\u20133612, 2021.   \n[85] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:36479\u201336494, 2022.   \n[86] Simo S\u00e4rkk\u00e4 and Arno Solin. Applied stochastic differential equations, volume 10. Cambridge University Press, 2019.   \n[87] V\u0131ctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E (n) equivariant graph neural networks. In International conference on machine learning, pages 9323\u20139332. PMLR, 2021.   \n[88] H Bernhard Schlegel. Geometry optimization. Wiley Interdisciplinary Reviews: Computational Molecular Science, 1(5):790\u2013809, 2011.   \n[89] Kristof Sch\u00fctt, Oliver Unke, and Michael Gastegger. Equivariant message passing for the prediction of tensorial properties and molecular spectra. In International Conference on Machine Learning, pages 9377\u20139388. PMLR, 2021.   \n[90] Kristof T Sch\u00fctt, Huziel E Sauceda, P-J Kindermans, Alexandre Tkatchenko, and K-R M\u00fcller. Schnet\u2013a deep learning architecture for molecules and materials. The Journal of Chemical Physics, 148(24), 2018.   \n[91] William Raymond Scott. Group theory. Courier Corporation, 2012.   \n[92] Yu Shi, Shuxin Zheng, Guolin Ke, Yifei Shen, Jiacheng You, Jiyan He, Shengjie Luo, Chang Liu, Di He, and Tie-Yan Liu. Benchmarking graphormer on large-scale molecular modeling datasets. arXiv preprint arXiv:2203.04810, 2022.   \n[93] Yuyang Shi, Valentin De Bortoli, Andrew Campbell, and Arnaud Doucet. Diffusion schr\u00f6dinger bridge matching. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[94] Muhammed Shuaibi, Adeesh Kolluru, Abhishek Das, Aditya Grover, Anuroop Sriram, Zachary Ulissi, and C Lawrence Zitnick. Rotation invariant graph neural networks using spin convolutions. arXiv preprint arXiv:2106.09575, 2021.   \n[95] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 2256\u20132265, Lille, France, 07\u201309 Jul 2015. PMLR.   \n[96] Vignesh Ram Somnath, Matteo Pariset, Ya-Ping Hsieh, Maria Rodriguez Martinez, Andreas Krause, and Charlotte Bunne. Aligned diffusion schr\u00f6dinger bridges. In Uncertainty in Artificial Intelligence, pages 1985\u20131995. PMLR, 2023.   \n[97] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021.   \n[98] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019.   \n[99] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021.   \n[100] Yuxuan Song, Jingjing Gong, Minkai Xu, Ziyao Cao, Yanyan Lan, Stefano Ermon, Hao Zhou, and Wei-Ying Ma. Equivariant flow matching with hybrid probability transport for 3d molecule generation. Advances in Neural Information Processing Systems, 36, 2024.   \n[101] Howard Stephen Stoker and G Lynn Carlson. General, organic, and biological chemistry. Houghton Mifflin, 2004.   \n[102] Challapalli Suryanarayana. Experimental techniques in materials and mechanics. Crc Press, 2011.   \n[103] Philipp Th\u00f6lke and Gianni De Fabritiis. Equivariant transformers for neural network based molecular potentials. In International Conference on Learning Representations, 2022.   \n[104] Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley. Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds. arXiv preprint arXiv:1802.08219, 2018.   \n[105] Richard Tran, Janice Lan, Muhammed Shuaibi, Brandon M Wood, Siddharth Goyal, Abhishek Das, Javier Heras-Domingo, Adeesh Kolluru, Ammar Rizvi, Nima Shoghi, et al. The open catalyst 2022 (oc22) dataset and challenges for oxide electrocatalysts. ACS Catalysis, 13(5):3066\u20133084, 2023.   \n[106] Oliver T Unke, Stefan Chmiela, Huziel E Sauceda, Michael Gastegger, Igor Poltavsky, Kristof T Schutt, Alexandre Tkatchenko, and Klaus-Robert Muller. Machine learning force fields. Chemical Reviews, 121(16):10142\u201310186, 2021.   \n[107] Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, et al. Scientific discovery in the age of artificial intelligence. Nature, 620(7972):47\u201360, 2023.   \n[108] Joseph L Watson, David Juergens, Nathaniel R Bennett, Brian L Trippe, Jason Yim, Helen E Eisenach, Woody Ahern, Andrew J Borst, Robert J Ragotte, Lukas F Milles, et al. De novo design of protein structure and function with rfdiffusion. Nature, 620(7976):1089\u20131100, 2023.   \n[109] E Weinan and Eric Vanden-Eijnden. Transition-path theory and path-finding algorithms for the study of rare events. Annual review of physical chemistry, 61(2010):391\u2013420, 2010.   \n[110] Lemeng Wu, Chengyue Gong, Xingchao Liu, Mao Ye, and qiang liu. Diffusion-based molecule generation with informative prior bridges. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.   \n[111] Guikun Xu, Yongquan Jiang, PengChuan Lei, Yan Yang, and Jim Chen. Gtmgc: Using graph transformer to predict molecule\u2019s ground-state conformation. In The Twelfth International Conference on Learning Representations, 2023.   \n[112] Minkai Xu, Jiaqi Han, Aaron Lou, Jean Kossaif,i Arvind Ramanathan, Kamyar Azizzadenesheli, Jure Leskovec, Stefano Ermon, and Anima Anandkumar. Equivariant graph neural operator for modeling 3d dynamics. arXiv preprint arXiv:2401.11037, 2024.   \n[113] Minkai Xu, Alexander S Powers, Ron O. Dror, Stefano Ermon, and Jure Leskovec. Geometric latent diffusion models for 3D molecule generation. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 38592\u201338610. PMLR, 23\u201329 Jul 2023.   \n[114] Minkai Xu, Alexander S Powers, Ron O Dror, Stefano Ermon, and Jure Leskovec. Geometric latent diffusion models for 3d molecule generation. In International Conference on Machine Learning, pages 38592\u201338610. PMLR, 2023.   \n[115] Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. Geodiff: A geometric diffusion model for molecular conformation generation. In International Conference on Learning Representations, 2022.   \n[116] Zhao Xu, Youzhi Luo, Xuan Zhang, Xinyi Xu, Yaochen Xie, Meng Liu, Kaleb Dickerson, Cheng Deng, Maho Nakata, and Shuiwang Ji. Molecule3d: A benchmark for predicting 3d geometries from molecular graphs. arXiv preprint arXiv:2110.01717, 2021.   \n[117] Jason Yim, Brian L. Trippe, Valentin De Bortoli, Emile Mathieu, Arnaud Doucet, Regina Barzilay, and Tommi Jaakkola. SE(3) diffusion model with application to protein backbone generation. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 40001\u201340039. PMLR, 23\u201329 Jul 2023.   \n[118] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform badly for graph representation? In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021.   \n[119] Bohang Zhang, Shengjie Luo, Liwei Wang, and Di He. Rethinking the expressive power of GNNs via graph biconnectivity. In The Eleventh International Conference on Learning Representations, 2023.   \n[120] Xuan Zhang, Limei Wang, Jacob Helwig, Youzhi Luo, Cong Fu, Yaochen Xie, Meng Liu, Yuchao Lin, Zhao Xu, Keqiang Yan, et al. Artificial intelligence for science in quantum, atomistic, and continuum systems. arXiv preprint arXiv:2307.08423, 2023.   \n[121] Linqi Zhou, Aaron Lou, Samar Khanna, and Stefano Ermon. Denoising diffusion bridge models. In The Twelfth International Conference on Learning Representations, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "A Organization of the Appendix ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The supplementary material is organized as follows. In Appendix B, we first recall some definitions and tools from stochastic calculus and then give the proofs of all theorems. In Appendix C, we give the derivation of our practical objective function and our sampling algorithms. In Appendix D, we give some details of our experiments, including a comprehensive introduction to the datasets, baselines, metrics and settings. ", "page_idx": 18}, {"type": "text", "text": "B Proof of Theorems ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "B.1 Review of Stochastic Calculus ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Let $(X_{t})_{t\\in[0,T]}$ be a stochastic process. We use $p(x^{\\prime},t^{\\prime}|x_{1},t_{1};x_{2},t_{2};\\ldots;x_{n},t_{n})$ to denote its conditional density function satisfying ", "page_idx": 18}, {"type": "equation", "text": "$$\nP(\\mathbf{X}_{t^{\\prime}}\\in A|\\mathbf{X}_{t_{1}}=x_{1},\\mathbf{X}_{t_{2}}=x_{2},\\ldots,\\mathbf{X}_{t_{n}}=x_{n})=\\int_{A}p(x^{\\prime},t^{\\prime}|x_{1},t_{1};x_{2},t_{2};\\ldots;x_{n},t_{n})\\mathrm{d}x^{\\prime}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for any Borel set $A$ , where $t_{1}~<~t_{2}~<~\\cdot\\cdot~<~t_{n}$ . If $(\\mathbf{X}_{t})_{t\\in[0,T]}$ is a Markov process, $p(x^{\\prime},t^{\\prime}|x_{1},t_{1};x_{2},t_{2};\\ldots;x_{n},t_{n})=p(x^{\\prime},t^{\\prime}|x_{n},t_{n})$ ), which is also called a transition density function. One of the most important results of stochastic calculus is the Ito\u2019s formula. The precise statements are as follows. ", "page_idx": 18}, {"type": "text", "text": "Theorem B.1 (Ito\u2019s formula for Brownian Motion). Let $\\mathbf{B}_{t}$ be the $d-$ dimensional Brownian Motion. Assume $f$ is a bounded real valued function with continuous second-order partial derivatives, i.e. $f\\in C_{b}^{2}(\\ensuremath{\\mathbb{R}}^{d})$ . Then the Ito\u2019s formula is given by ", "page_idx": 18}, {"type": "equation", "text": "$$\nf(\\mathbf{B}_{t})=f(\\mathbf{B}_{0})+\\int_{0}^{T}\\nabla f(\\mathbf{B}_{t})\\cdot\\mathrm{d}\\mathbf{B}_{t}+\\frac{1}{2}\\int_{0}^{T}\\nabla^{2}f(\\mathbf{B}_{t})\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We follow [86] for the proof of Doob\u2019s $_\\mathrm{h}$ -transform. The infinitesimal generator of the Markov process plays an important role in the proof of the Doob\u2019s $_\\mathrm{h}$ -transform. The precise definitions are as follows. ", "page_idx": 18}, {"type": "text", "text": "Definition B.2. (Generator of a Process) The infinitesimal generator $\\boldsymbol{A}_{t}$ of a stochastic process $\\left(\\mathbf{X}_{t}\\right)$ for a function $\\phi(x)$ is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{A}_{t}\\phi(x)=\\operatorname*{lim}_{s\\to0^{+}}\\frac{\\mathbb{E}[\\phi(\\mathbf{X}_{t+s})|\\mathbf{X}_{t}=x]-\\phi(x)}{s},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\phi$ is a suitably regular function. For an It\u00f4 process defined as the solution to the SDE $\\mathrm{d}\\mathbf{X}_{t}=\\mathbf{f}(\\mathbf{X}_{t},t)\\mathrm{d}t+\\sigma(t)\\mathrm{d}\\mathbf{B}_{t},$ , the generator is ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\cal A}_{t}=\\sum_{i=1}^{d}{\\bf f}^{i}(x,t)\\frac{\\partial}{\\partial x_{i}}+\\frac{1}{2}\\sum_{i=1}^{d}\\sigma^{2}(t)\\frac{\\partial^{2}}{\\partial x_{i}^{2}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The Fokker-Planck\u2019s Equation is an useful tool to track the evolution of the transition density function associated with an SDE. The precise statements are as follows. ", "page_idx": 18}, {"type": "text", "text": "Proposition B.3. (Fokker-Planck\u2019s Equation) Let $p(x^{\\prime},t^{\\prime}|x,t)$ be the transition density function of the SDE $\\mathrm{d}\\mathbf{X}_{t}=\\mathbf{f}(\\mathbf{X}_{t},t)\\mathrm{d}t+\\sigma(t)\\mathrm{d}\\mathbf{B}_{t}^{\\top}$ . Then $p(x^{\\prime},t^{\\prime}|x,t)$ satisfies the Fokker-Planck\u2019s Equation ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\partial p(x,t|x_{0},0)}{\\partial t}=-\\sum_{i=1}^{d}\\frac{\\partial(\\mathbf{f}^{i}(x,t)p(x,t|x_{0},0))}{\\partial x_{i}}+\\frac{1}{2}\\sum_{i=1}^{d}\\sigma^{2}(t)\\frac{\\partial^{2}p(x,t|x_{0},0)}{\\partial x_{i}^{2}}=0,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with the initial condition $p(x,0|x_{0},0)\\,=\\,\\delta(x-x_{0})$ . The Fokker-Planck\u2019s Equation can also be written in a compact form using the generator $\\boldsymbol{A}_{t}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial t}p(x,t|x_{0},0)=A_{t}^{*}p(x,t|x_{0},0),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\mathbf{\\mathcal{A}}_{t}^{*}$ is the adjoint operator of $\\mathcal{A}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\nA_{t}^{*}=-\\sum_{i=1}^{d}\\frac{\\partial(\\mathbf{f}^{i}(x,t)\\cdot)}{\\partial x_{i}}+\\frac{1}{2}\\sum_{i=1}^{d}\\sigma^{2}(t)\\frac{\\partial^{2}(\\cdot)}{\\partial x_{i}^{2}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "When the terminal is fixed, the evolution of the transition density function can also given by a PDE, which is called the Backward Kolmogorov Equation. We give the precise statement as follows. ", "page_idx": 19}, {"type": "text", "text": "Proposition B.4. (Backward Kolmogorov Equation) Let $p(x^{\\prime},t^{\\prime}|x,t)$ be the transition density function of the SDE $\\mathrm{d}\\mathbf{X}_{t}=\\mathbf{f}(\\mathbf{X}_{t},t)\\mathrm{d}t+\\sigma(t)\\mathrm{d}\\mathbf{B}_{t}$ . Then $p(x^{\\prime},t^{\\prime}|x,t)$ satisfies the Backward Kolmogorov Equation ", "page_idx": 19}, {"type": "equation", "text": "$$\n-\\frac{\\partial p(x_{t},t|x,s)}{\\partial s}=\\sum_{i=1}^{d}\\mathbf{f}^{i}(x,s)\\frac{\\partial p(x_{t},t|x,s)}{\\partial x_{i}}+\\frac{1}{2}\\sum_{i=1}^{d}\\sigma^{2}(s)\\frac{\\partial^{2}p(x_{t},t|x,s)}{\\partial x_{i}^{2}}=0,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with the initial condition $p(x_{t},t|x,t)=\\delta(x-x_{t})$ . The Backward Kolmogorov Equation can also be written in a compact form using the generator $\\mathcal{A}_{s}$ : ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left(\\frac{\\partial}{\\partial s}+\\boldsymbol{A}_{s}\\right)p(x_{t},t|x,s)=0.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "B.2 Proof of Proposition 3.1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proposition B.5. Let $\\mathcal{R}$ denote the space of geometric states and $\\mathbf{f}_{\\mathcal{R}}(\\cdot,\\cdot):\\mathcal{R}\\times[0,T]\\rightarrow\\mathcal{R}$ denote the drift coefficient on $\\mathcal{R}$ . Let $(\\mathbf{W}^{t})_{t\\in[0,T]}$ denote the Wiener process on $\\mathcal{R}$ . Given an $S D E$ on geometric states $\\mathrm{d}\\mathbf{R}^{t}=\\mathbf{f}_{\\mathcal{R}}(\\mathbf{R}^{t},t)\\mathrm{d}t+\\sigma(t)\\mathrm{\\dot{d}\\mathbf{W}}^{t}$ , ${\\bf R}^{0}\\sim q({\\bf R}^{0})$ , its transition density $p_{\\mathcal{R}}(z^{\\prime},t^{\\prime}|z,t),z,z^{\\prime}\\in\\mathcal{R}$ is SE(3)-equivariant, i.e., $p_{\\mathcal{R}}(\\mathbf{R}^{t^{\\prime}},t^{\\prime}|\\mathbf{R}^{t},t)=p_{\\mathcal{R}}(\\rho^{\\mathcal{R}}(g)[\\mathbf{R}^{t^{\\prime}}],t^{\\prime}|\\rho^{\\mathcal{R}}(g)[\\mathbf{R}^{t}],t),\\forall g\\in\\mathrm{SE}(3),\\forall0\\leq$ $t\\,<\\,t^{\\prime}\\,\\leq\\,T$ , if the following conditions are satisfied: $(I)$ $q(\\mathbf{R}^{0})$ is $\\mathrm{SE}(3)$ -invariant; (2) $\\mathbf{f}_{\\mathcal{R}}(\\cdot,t)$ is SO(3)-equivariant and $\\mathrm{T}(3)$ -invariant; (3) the transition density of $(\\mathbf{W}^{t})_{t\\in[0,T]}$ is SE(3)- equivariant. ", "page_idx": 19}, {"type": "text", "text": "Proof. In this section, we view $R=\\left\\{\\mathbf{r}_{1},...,\\mathbf{r}_{n}\\right\\}\\in\\mathcal{R}$ as $\\mathbf{r}_{1}\\oplus\\mathbf{r}_{2}\\oplus\\cdots\\mathbf{r}_{n}\\,\\in\\,\\mathbb{R}^{3n}$ , which is the concatenation of $\\mathbf{r}_{i}$ . So from this perspective, the space $\\mathcal{R}$ is isomorphic to the Euclidean space $\\mathbb{R}^{3n}$ . Then $(\\mathbf{W}^{t})_{t\\in[0,T]}$ is the Wiener process with dimension $d=3n$ . ", "page_idx": 19}, {"type": "text", "text": "For any $g\\in\\mathrm{SE}(3)$ , $\\rho^{\\mathcal{R}}(g)$ can be characterized by an orthogonal matrix $\\mathbf{O}(g)\\in\\mathbb{R}^{3\\times3}$ , satisfying $\\operatorname*{det}(\\mathbf{O}(g))=1$ , and a translation vector $\\mathbf{t}\\in\\mathbb{R}^{3}$ . Then the representation of SE(3) on $\\mathbb{R}^{3n}$ is given by ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\rho^{\\mathcal{R}}(g)[R]=\\mathbf{O}_{\\mathcal{R}}(g)R+\\mathbf{t}_{\\mathcal{R}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where ${\\bf O}_{\\mathcal{R}}(g)=\\mathrm{diag}\\{{\\bf O}(g),{\\bf O}(g),\\ldots,{\\bf O}(g)\\}$ , $\\mathbf{t}_{\\mathcal{R}}=\\mathbf{t}\\oplus\\mathbf{t}\\oplus\\dots\\mathbf{t}\\in\\mathbb{R}^{3n}$ . It\u2019s obvious that $\\mathbf{O}_{\\mathcal{R}}(g)$ is also an orthogonal matrix in $\\mathbb{R}^{3n\\times3n}$ , satisfying $\\mathbf{O}_{\\mathcal{R}}^{-1}(g)=\\mathbf{O}_{\\mathcal{R}}^{T}(g)$ . ", "page_idx": 19}, {"type": "text", "text": "According to Proposition B.3, the evolution of the transition density function is given by the Fokker-Planck\u2019s Equation ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\partial p_{\\mathcal{R}}(x,t|x_{0},0)}{\\partial t}=-\\sum_{i=1}^{d}\\frac{\\partial\\left(\\mathbf{f}^{i}(x,t)p_{\\mathcal{R}}(x,t|x_{0},0)\\right)}{\\partial x_{i}}+\\frac{1}{2}\\sum_{i=1}^{d}\\sigma^{2}(t)\\frac{\\partial^{2}\\left(p_{\\mathcal{R}}(x,t|x_{0},0)\\right)}{\\partial x_{i}^{2}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with the initial condition $p_{\\mathcal{R}}(x,0|x_{0},0)=\\delta(x-x_{0})$ . ", "page_idx": 19}, {"type": "text", "text": "Let $y=\\mathbf{O}_{\\mathcal{R}}(g)x+\\mathbf{t}_{\\mathcal{R}}$ , $y_{0}=\\mathbf{O}_{\\mathcal{R}}(g)x_{0}+\\mathbf{t}_{\\mathcal{R}}$ , then we have ", "page_idx": 19}, {"type": "equation", "text": "$$\np_{\\mathcal{R}}(\\rho^{\\mathcal{R}}(g)[x],t|\\rho^{\\mathcal{R}}(g)[x_{0}],0)=p_{\\mathcal{R}}(\\mathbf{O}_{\\mathcal{R}}(g)x+\\mathbf{t}_{\\mathcal{R}},t|\\mathbf{O}_{\\mathcal{R}}(g)x_{0}+\\mathbf{t}_{\\mathcal{R}},0)=p_{\\mathcal{R}}(y,t|y_{0},0).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The evolution of the transition density function $p_{\\mathcal{R}}(y,t|y_{0},0)$ is also given by the Fokker-Planck\u2019s Equation: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\partial p_{\\mathcal{R}}(y,t|y_{0},0)}{\\partial t}=-\\sum_{i=1}^{d}\\frac{\\partial\\left(\\mathbf{f}^{i}(y,t)p_{\\mathcal{R}}(y,t|y_{0},0)\\right)}{\\partial y_{i}}+\\frac{1}{2}\\sum_{i=1}^{d}\\sigma^{2}(t)\\frac{\\partial^{2}\\left(p_{\\mathcal{R}}(y,t|y_{0},0)\\right)}{\\partial y_{i}^{2}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with the boundary condition $p_{\\mathcal{R}}(y,0|y_{0},0)=\\delta(y-y_{0})=\\delta(x-x_{0})$ . Since $y=\\mathbf{O}_{\\mathcal{R}}(g)x+\\mathbf{t}_{\\mathcal{R}}$ , we have $x=\\mathbf{O}_{\\mathcal{R}}^{-1}(g)(y-\\mathbf{t}_{\\mathcal{R}})$ . Then by the chain rule, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial y_{i}}=\\sum_{j=1}^{d}\\frac{\\partial x_{j}}{\\partial y_{i}}\\frac{\\partial}{\\partial x_{j}}=\\sum_{j=1}^{d}(\\mathbf{O}_{\\mathcal{R}}^{-1}(g))_{j i}\\frac{\\partial}{\\partial x_{j}}=\\sum_{j=1}^{d}(\\mathbf{O}_{\\mathcal{R}}(g))_{i j}\\frac{\\partial}{\\partial x_{j}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since $\\mathbf{f}_{\\mathcal{R}}(\\cdot,t)$ is a SO(3)-equivariant and $\\mathrm{T}(3)$ -invariant function, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{f}_{\\mathcal{R}}^{i}(y,t)=\\mathbf{f}_{\\mathcal{R}}^{i}(\\mathbf{O}_{\\mathcal{R}}(g)x+\\mathbf{t}_{\\mathcal{R}},t)=(\\mathbf{O}_{\\mathcal{R}}(g)\\mathbf{f}_{\\mathcal{R}}(x,t))_{i}=\\sum_{k=1}^{d}(\\mathbf{O}_{\\mathcal{R}}(g))_{i k}\\mathbf{f}_{\\mathcal{R}}^{k}(x,t).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then the Fokker-Planck\u2019s equation becomes ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{\\partial p_{R}(y,t|y_{0},0)}{\\partial t}=-\\,\\frac{d}{\\iota-1}\\frac{\\partial\\bigl(\\mathbf{f}^{i}(y,t)p_{R}(y,t|y_{0},0)\\bigr)}{\\partial y_{\\iota}}+\\frac{1}{2}\\sigma^{2}(t)\\sum_{i=1}^{d}\\frac{\\partial^{2}\\bigl(p_{K}(y,t|y_{0},0)\\bigr)}{\\partial y_{\\iota}^{2}}}&{}\\\\ {\\displaystyle}&{=-\\,\\frac{d}{\\iota-1}\\sum_{j=1}^{d}\\sum_{k=1}^{d}(\\mathbf{o}_{R}(g))_{i\\cdot j}\\partial_{\\mathbf{r}}^{(((\\mathbf{o}_{R}(g))_{i}\\cdot\\mathbf{f}_{k}|)p_{R}(y,t|y_{0},0)})}\\\\ &{+\\,\\frac{1}{2}\\sigma^{2}(t)\\underset{i=1}{\\overset{d}{\\longrightarrow}}\\sum_{j=1}^{d}\\sum_{k=1}^{d}(\\mathbf{o}_{R}(g))_{i\\cdot}\\frac{\\partial}{\\partial x_{k}}(\\mathbf{o}_{R}(g))_{i\\cdot j}\\frac{\\partial^{2}\\bigl(p_{R}(y,t|y_{0},0)\\bigr)}{\\partial x_{j}}}\\\\ &{=-\\,\\underset{i=1}{\\overset{d}{\\longrightarrow}}\\sum_{j=1}^{d}(\\mathbf{o}_{R}(g))_{i\\cdot j}(\\mathbf{o}_{R}(g))_{i\\cdot k}\\frac{\\partial\\bigl(\\mathbf{f}_{\\mathbf{r}}^{k}(x,t)p_{R}(y,t|y_{0},0)\\bigr)}{\\partial x_{j}}}\\\\ &{+\\,\\frac{1}{2}\\sigma^{2}(t)\\underset{i=1}{\\overset{d}{\\longrightarrow}}\\sum_{j=1}^{d}(\\mathbf{o}_{R}(g))_{i\\cdot}(\\mathbf{o}_{R}(g))_{i\\cdot j}\\frac{\\partial}{\\partial x_{k}}\\frac{\\partial\\bigl(p_{R}(y,t|y_{0},0)\\bigr)}{\\partial x_{j}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $\\mathbf{O}_{\\mathcal{R}}(g)$ is an orthogonal matrix, the columns of $\\mathbf{O}_{\\mathcal{R}}(g)$ are orthogonal to each other, i.e. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{d}(\\mathbf{O}_{\\mathcal{R}}(g))_{i k}(\\mathbf{O}_{\\mathcal{R}}(g))_{i j}=\\delta_{j k}=\\left\\{\\!\\!\\begin{array}{r l}{0}&{{}j\\neq k,}\\\\ {1}&{{}j=k.}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "So the Fokker-Planck\u2019s equation can be simplified to ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{\\partial p_{\\mathcal{R}}(\\boldsymbol{y},t|\\boldsymbol{y}_{0},0)}{\\partial t}=-\\sum_{j=1}^{d}\\sum_{k=1}^{d}\\frac{\\partial(\\mathbf{f}_{\\mathcal{R}}^{k}(\\boldsymbol{x},t)p_{\\mathcal{R}}(\\boldsymbol{y},t|\\boldsymbol{y}_{0},0))}{\\partial x_{j}}}}\\\\ &{}&{\\qquad+\\frac{1}{2}\\sigma^{2}(t)\\underset{j=1}{\\overset{d}{\\sum}}\\sum_{k=1}^{d}\\delta_{j k}\\frac{\\partial}{\\partial x_{k}}\\frac{\\partial\\left(p_{\\mathcal{R}}(\\boldsymbol{y},t|\\boldsymbol{y}_{0},0)\\right)}{\\partial x_{j}}}\\\\ &{}&{\\qquad=-\\sum_{j=1}^{d}\\frac{\\partial(\\mathbf{f}_{\\mathcal{R}}^{j}(\\boldsymbol{x},t)p_{\\mathcal{R}}(\\boldsymbol{y},t|\\boldsymbol{y}_{0},0))}{\\partial x_{j}}+\\frac{1}{2}\\sigma^{2}(t)\\underset{j=1}{\\overset{d}{\\sum}}\\frac{\\partial^{2}\\left(p_{\\mathcal{R}}(\\boldsymbol{y},t|\\boldsymbol{y}_{0},0)\\right)}{\\partial(x_{j})^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which is same as Eqn.(17). Since the boundary condition $p_{\\mathcal{R}}(y,0|y_{0},t_{0})\\,=\\,\\delta(y\\,-\\,y_{0})\\,=\\,\\delta(x\\,-\\,$ $x_{0})=p_{\\mathcal{R}}(x,0|x_{0},0)$ , then $p_{\\mathcal{R}}(y,t|y_{0},t_{0})=p_{\\mathcal{R}}\\dot{(}x,t|x_{0},t_{0})\\overset{\\cdot}{,}\\forall t\\in[\\dot{0},T]$ . Thus we have proved that $p_{\\mathcal{R}}(\\mathbf{R}^{t^{\\prime}},t^{\\prime}|\\mathbf{R}^{t},t)=p_{\\mathcal{R}}(\\rho^{\\mathcal{R}}(g)[\\mathbf{R}^{t^{\\prime}}],t^{\\prime}|\\rho^{\\mathcal{R}}(g)[\\mathbf{R}^{t}],t),\\forall g\\in\\mathrm{SE}(3),\\forall0\\leq t<t^{\\prime}\\leq T$ . \u53e3 ", "page_idx": 20}, {"type": "text", "text": "B.3 Proof of Proposition 3.2 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proposition B.6 (Doob\u2019s $h$ -transform). Let $p_{\\mathcal{R}}(z^{\\prime},t^{\\prime}|z,t)$ be the transition density of the SDE in Proposition 3.1. Let $h_{\\mathcal{R}}(\\cdot,\\cdot)\\,:\\,\\mathcal{R}\\,\\times\\,[0,T]\\,\\rightarrow\\,\\mathbb{R}_{>0}$ be a smooth function satisfying: (1) $h_{\\mathcal{R}}(\\cdot,t)$ is SE(3)-invariant; (2) $\\begin{array}{r}{h_{\\mathcal{R}}(z,t)=\\int{\\bar{p}_{\\mathcal{R}}(\\bar{z}^{\\prime},t^{\\prime}|z,t)h_{\\mathcal{R}}(z^{\\prime},t^{\\prime})\\mathrm{d}z^{\\prime}}}\\end{array}$ . We can derive the following $h_{\\mathcal{R}}$ - transformed SDE on geometric states: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{R}^{t}=\\left[\\mathbf{f}_{\\mathcal{R}}(\\mathbf{R}^{t},t)+\\sigma^{2}(t)\\nabla_{\\mathbf{R}^{t}}\\log h_{\\mathcal{R}}(\\mathbf{R}^{t},t)\\right]\\mathrm{d}t+\\sigma(t)\\mathrm{d}\\mathbf{W}_{t},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "with transition density $\\begin{array}{r}{p_{\\mathcal{R}}^{h}(z^{\\prime},t^{\\prime}|z,t)=p_{\\mathcal{R}}(z^{\\prime},t^{\\prime}|z,t)\\frac{h_{\\mathcal{R}}(z^{\\prime},t^{\\prime})}{h_{\\mathcal{R}}(z,t)}}\\end{array}$ preserving the symmetry constraints. ", "page_idx": 20}, {"type": "text", "text": "Proof. We use the definition of the infinitesimal generator to prove the proposition. The infinitesimal generator of $p_{\\mathcal{R}}^{h}(x^{\\prime},t^{\\prime}|x,t)$ for a function $\\phi(x)$ is given by ", "page_idx": 20}, {"type": "equation", "text": "$$\nA_{t}^{h}\\phi(x)=\\operatorname*{lim}_{s\\to0^{+}}\\frac{\\mathbb{E}^{h}[\\phi({\\bf R}^{t+s})|{\\bf R}^{t}=x]-\\phi(x)}{s}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $\\begin{array}{r}{p_{\\mathcal{R}}^{h}(z^{\\prime},t^{\\prime}|z,t)=p_{\\mathcal{R}}(z^{\\prime},t^{\\prime}|z,t)\\frac{h_{\\mathcal{R}}(z^{\\prime},t^{\\prime})}{h_{\\mathcal{R}}(z,t)}}\\end{array}$ ) hRR(z,t) , so we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{E}^{h}[\\phi({\\mathbf{R}}^{t+s})\\vert{\\mathbf{R}}^{t}=x]=\\frac{\\mathbb{E}[\\phi({\\mathbf{R}}^{t+s})h({\\mathbf{R}}^{t+s},t+s)\\vert{\\mathbf{R}}^{t}=x]}{h(x,t)}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then ${\\mathcal{A}}_{t}^{h}\\phi(x)$ can be simplified as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{A_{t}^{\\Phi}(\\sigma|^{2})=\\operatorname*{lim}_{s\\rightarrow0}\\frac{\\mathbb{E}^{\\Phi}[\\langle\\mathbf{R}^{+}(\\mathbf{R}^{+})|\\mathbf{R}^{\\mathrm{t}}-\\sigma\\rangle-\\theta^{\\mathrm{L}}]}{\\mathbb{E}^{\\Phi}[\\langle\\mathbf{R}^{+}\\rangle]}}\\quad}&{}\\\\ &{=\\operatorname*{lim}_{s\\rightarrow0}\\frac{\\mathbb{E}[\\langle\\mathbf{R}^{+}(\\mathbf{R}^{+})|\\mathbf{R}^{\\mathrm{t}}-\\mathbf{r}^{\\mathrm{t}},\\phi\\rangle]\\mathbb{E}^{\\Phi}[\\sigma\\cdot\\mathbf{z}]-\\phi[\\mathcal{Z}]h(x,t)}{\\mathbb{E}^{\\Phi}[\\langle\\mathbf{R}^{+}\\rangle]}}\\\\ &{=\\frac{1}{B\\langle\\mathbf{z},t\\rangle}\\frac{\\langle\\mathbf{R}|\\langle\\mathbf{z},t\\rangle}{\\mathbb{H}}\\phi(z)+\\sum_{i=1}^{r}\\left(\\frac{\\partial h(x,t)}{\\partial z_{i}}\\phi(z)+h(x,t)\\frac{\\partial\\phi(x)}{\\partial x_{i}}\\right)\\mathrm{e}^{\\mathrm{i}}(x,t)}\\\\ &{+\\frac{1}{2}\\frac{\\displaystyle\\sum_{i=1}^{d}\\sigma^{2}(\\theta)^{\\frac{\\partial\\phi}{\\partial x_{i}}(z,t)}}{\\partial x_{i}^{2}}\\phi(z)+\\sum_{i=1}^{d}\\sigma^{2}(\\theta)\\frac{\\partial\\phi(x,t)\\partial\\phi(z)}{\\partial x_{i}}}\\\\ &{+\\frac{1}{2}\\frac{\\displaystyle\\sum_{i=1}^{d}\\sigma^{2}(\\theta)^{\\frac{\\partial\\phi}{\\partial x_{i}^{2}}(z)}h(x,t)}{\\partial x_{i}^{2}}}\\\\ &{=\\frac{1}{B\\langle\\mathbf{z},t\\rangle}\\frac{\\langle\\mathbf{\\hat{D}}(\\mathbf{z},t)\\phi(z)\\rangle}{\\partial x_{i}}(x)+(A_{i}\\Lambda(x,t))\\phi(x)\\frac{\\partial\\phi}{\\partial x_{i}}h(x,t)\\frac{\\partial\\phi(x)}{\\partial x_{i}}\\mathrm{e}^{\\mathrm{i}}(x,t)}\\\\ &{+\\sum_{i=1}^{d}\\sigma^{2}(\\theta)\\frac{\\partial\\phi(x,t)\\partial\\phi(z)}{\\partial x_{i}}+\\frac{1}{2}\\frac{\\displaystyle\\sum_{i=1}^{d}\\sigma\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since $\\begin{array}{r}{h(x,t)=\\int p_{\\mathcal{R}}(x^{\\prime},t^{\\prime}|x,t)h(x^{\\prime},t^{\\prime})\\mathrm{d}x}\\end{array}$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left(\\frac{\\partial}{\\partial t}+{\\cal A}_{t}\\right)h(x,t)=\\int\\left(\\frac{\\partial p(x^{\\prime},t^{\\prime}|x,t)}{\\partial t}+{\\cal A}_{t}p(x^{\\prime},t^{\\prime}|x,t)\\right)h(x^{\\prime},t^{\\prime})\\mathrm{d}x.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "According to the Backward Kolmogorov Equation (Proposition B.4), we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\frac{\\partial p(x^{\\prime},t^{\\prime}|x,t)}{\\partial t}+A_{t}p(x^{\\prime},t^{\\prime}|x,t)=0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "So we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left(\\frac{\\partial}{\\partial t}+A_{t}\\right)h(x,t)=0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then ${\\mathcal{A}}_{t}^{h}\\phi(x)$ can be simplified as ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{A^{h}\\phi(x)={\\displaystyle{\\frac{1}{h(x,t)}}}[{\\displaystyle{\\sum_{i=1}^{d}}\\,h(x,t){\\frac{\\partial\\phi(x)}{\\partial x_{i}}}{\\bf f}^{i}(x,t)}+{\\displaystyle{\\sum_{i=1}^{d}}\\sigma^{2}(t){\\frac{\\partial h(x,t)}{\\partial x_{i}}}{\\frac{\\partial\\phi(x)}{\\partial x_{i}}}}}\\\\ &{\\qquad\\qquad+\\,{\\displaystyle{\\frac{1}{2}}\\sum_{i=1}^{d}\\sigma^{2}(t){\\frac{\\partial^{2}\\phi(x)}{\\partial x_{i}^{2}}}h(x,t)}]}\\\\ &{\\qquad=\\sum_{i=1}^{d}{\\displaystyle{\\frac{\\partial\\phi(x)}{\\partial x_{i}}}\\mathbf{f}^{i}(x,t)}+{\\displaystyle{\\sum_{i=1}^{d}}\\sigma^{2}(t){\\frac{1}{h(x,t)}}{\\frac{\\partial h(x,t)}{\\partial x_{i}}}{\\frac{\\partial\\phi(x)}{\\partial x_{i}}}+{\\displaystyle{\\frac{1}{2}}\\sum_{i=1}^{d}\\sigma^{2}(t){\\frac{\\partial^{2}\\phi(x)}{\\partial x_{i}^{2}}}}}\\\\ &{\\qquad=\\sum_{i=1}^{d}\\left(\\mathbf{f}^{i}(x,t)+\\sigma^{2}(t){\\displaystyle{\\frac{\\partial\\log h(x,t)}{\\partial x_{i}}}}\\right){\\frac{\\partial\\phi(x)}{\\partial x_{i}}}+{\\displaystyle{\\frac{1}{2}}\\sum_{i=1}^{d}\\sigma^{2}(t){\\frac{\\partial^{2}\\phi(x)}{\\partial x_{i}^{2}}}}.}\\end{array}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "So we show that ", "page_idx": 21}, {"type": "equation", "text": "$$\nA_{t}^{h}=\\sum_{i=1}^{d}\\left({\\bf f}^{i}(x,t)+\\sigma^{2}(t)\\frac{\\partial\\log h(x,t)}{\\partial x_{i}}\\right)\\frac{\\partial}{\\partial x_{i}}+\\frac{1}{2}\\sum_{i=1}^{d}\\sigma^{2}(t)\\frac{\\partial^{2}}{\\partial x_{i}^{2}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "According to the correspondence between SDE and its generator (Definition B.2), the equation above implies that the $_\\mathrm{h}$ -transformed SDE is given by ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{R}^{t}=\\left[\\mathbf{f}_{\\mathcal{R}}(\\mathbf{R}^{t},t)+\\sigma^{2}(t)\\nabla_{\\mathbf{R}^{t}}\\log h_{\\mathcal{R}}(\\mathbf{R}^{t},t)\\right]\\mathrm{d}t+\\sigma(t)\\mathrm{d}\\mathbf{W}_{t}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Additionally, we need to show that the h-transformed transition density function satisfies the symmetric constraints. First, we show that if $h(\\cdot,t_{0})$ is SE(3)-invariant, then $h(\\cdot,t)$ is also SE(3)-invariant $\\forall t\\in[0,T]$ . For any $g\\in\\mathrm{SE}(3)$ , assume $\\rho^{\\mathcal{R}}(g)[z]=\\mathbf{O}_{\\mathcal{R}}(g)z+\\mathbf{t}_{\\mathcal{R}}$ , where $\\mathbf{O}_{\\mathcal{R}}(g)$ is an orthogonal matrix and $\\operatorname*{det}(\\mathbf{O}_{\\mathcal{R}}(g))=1$ . Since $h_{\\mathcal{R}}(z,t)$ satisfies ", "page_idx": 22}, {"type": "equation", "text": "$$\nh_{\\mathcal{R}}(z,t)=\\int p_{\\mathcal{R}}(z^{\\prime},t_{0}|z,t)h(z^{\\prime},t_{0})\\mathrm{d}z^{\\prime},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "then we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{h_{\\mathcal{R}}(\\rho^{\\mathcal{R}}(g)[z],t)=\\int p_{\\mathcal{R}}(z^{\\prime},t_{0}|\\rho^{\\mathcal{R}}(g)[z],t)h(z^{\\prime},t_{0})\\mathrm{d}z^{\\prime}}&{{\\scriptstyle(\\mathcal{S})}}\\\\ &{\\qquad\\qquad\\qquad=\\int p_{\\mathcal{R}}\\left(\\rho^{\\mathcal{R}}(g)(\\rho^{\\mathcal{R}}(g))^{-1}[z^{\\prime}],t_{0}|\\rho^{\\mathcal{R}}(g)[z],t\\right)h(\\rho^{\\mathcal{R}}(g)(\\rho^{\\mathcal{R}}(g))^{-1}[z^{\\prime}],t_{0})\\mathrm{d}z^{\\prime}.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By Proposition 3.1, $p_{\\mathcal{R}}\\left(\\rho^{\\mathcal{R}}(g)(\\rho^{\\mathcal{R}}(g))^{-1}[z^{\\prime}],t_{0}|\\rho^{\\mathcal{R}}(g)[z],t\\right)\\ =\\ p_{\\mathcal{R}}\\left((\\rho^{\\mathcal{R}}(g))^{-1}[z^{\\prime}],t_{0}|z,t\\right)$ , let $z_{1}=\\rho^{\\mathcal{R}}(g))^{-1}[z^{\\prime}]$ , then ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{\\mathcal{R}}(\\rho^{\\mathcal{R}}(g)[z],t)=\\displaystyle\\int p_{\\mathcal{R}}\\left((\\rho^{\\mathcal{R}}(g))^{-1}[z^{\\prime}],t_{0}|z,t\\right)h(\\rho^{\\mathcal{R}}(g)(\\rho^{\\mathcal{R}}(g))^{-1}[z^{\\prime}],t_{0})\\mathrm{d}z^{\\prime}}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\int p_{\\mathcal{R}}\\left(z_{1},t_{0}|z,t\\right)h(\\rho^{\\mathcal{R}}(g)z_{1},t_{0})\\operatorname*{det}(\\mathbf{O}_{\\mathcal{R}}(g))\\mathrm{d}z_{1}}\\\\ &{\\qquad\\qquad=\\displaystyle\\int p_{\\mathcal{R}}\\left(z_{1},t_{0}|z,t\\right)h(z_{1},t_{0})\\mathrm{d}z_{1}}\\\\ &{\\qquad\\qquad=h_{\\mathcal{R}}(z,t).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "So $h(\\cdot,t)$ is $\\mathrm{SE}(3)$ -invariant $\\forall t\\in[0,T],\\,h(\\cdot,t)$ is well-defined under these symmetric constraints. Then we show $p_{\\mathcal{R}}^{h}(z^{\\prime},t^{\\prime}|z,t)$ preserves the symmetric constraints: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{\\mathcal{R}}^{h}(\\rho^{\\mathcal{R}}(g)[z^{\\prime}],t^{\\prime}|\\rho^{\\mathcal{R}}(g)[z],t)=p_{\\mathcal{R}}(\\rho^{\\mathcal{R}}(g)[z^{\\prime}],t^{\\prime}|\\rho^{\\mathcal{R}}(g)[z],t)\\frac{h_{\\mathcal{R}}(\\rho^{\\mathcal{R}}(g)[z^{\\prime}],t^{\\prime})}{h_{\\mathcal{R}}(\\rho^{\\mathcal{R}}(g)[z],t)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=p_{\\mathcal{R}}(z^{\\prime},t^{\\prime}|z,t)\\frac{h_{\\mathcal{R}}(\\rho^{\\mathcal{R}}(g)[z^{\\prime}],t^{\\prime})}{h_{\\mathcal{R}}(\\rho^{\\mathcal{R}}(g)[z],t)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=p_{\\mathcal{R}}(z^{\\prime},t^{\\prime}|z,t)\\frac{h_{\\mathcal{R}}(z^{\\prime},t^{\\prime})}{h_{\\mathcal{R}}(z,t)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=p_{\\mathcal{R}}^{h}(z^{\\prime},t^{\\prime}|z,t).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus we have proved that ", "page_idx": 22}, {"type": "equation", "text": "$$\np_{\\mathcal{R}}^{h}(\\rho^{\\mathcal{R}}(g)[z^{\\prime}],t^{\\prime}|\\rho^{\\mathcal{R}}(g)[z],t)=p_{\\mathcal{R}}^{h}(z^{\\prime},t^{\\prime}|z,t),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which implies that $p_{\\mathcal{R}}^{h}(z^{\\prime},t^{\\prime}|z,t)$ preserves the symmetric constraints for any $g\\,\\in\\,\\mathrm{SE}(3)$ . So the proof is completed. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Next, we show how to construct a SDE with a fixed terminal point as an simple application of the Doob\u2019s $\\mathbf{h}$ -transform. The result of this example is very useful to construct diffusion bridge. ", "page_idx": 22}, {"type": "text", "text": "Proposition B.7. Assume the original SDE is given by $\\mathrm{d}\\mathbf{X}^{t}=\\mathbf{f}(\\mathbf{X}^{t},t)\\mathrm{d}t\\!+\\!\\sigma(t)\\mathrm{d}\\mathbf{W}_{t}$ . Let $h_{\\mathcal{R}}(x,t)=$ $p_{\\mathcal{R}}(y,T|x,t)$ which is the transition density function of the original SDE evaluated at $\\mathbf{X}_{T}=y$ . Then the $h$ -transformed SDE ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{R}^{t}=\\left[\\mathbf{f}(\\mathbf{R}^{t},t)+\\sigma^{2}(t)\\nabla_{\\mathbf{R}^{t}}\\log p_{\\mathcal{R}}(y,T|\\mathbf{R}^{t},t)\\right]\\mathrm{d}t+\\sigma(t)\\mathrm{d}\\mathbf{W}_{t},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "arrive at y almost surely at the final time. ", "page_idx": 22}, {"type": "text", "text": "Proof. The original SDE is given by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{X}^{t}=\\mathbf{f}(\\mathbf{X}^{t},t)\\mathrm{d}t+\\sigma(t)\\mathrm{d}\\mathbf{W}_{t}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "First, we need to verify that $h_{\\mathcal{R}}(x,t)$ satisfies the condition ", "page_idx": 23}, {"type": "equation", "text": "$$\nh_{\\mathcal{R}}(x,t)=\\int p_{\\mathcal{R}}(x^{\\prime},t_{0}|x,t)h(x^{\\prime},t_{0})\\mathrm{d}x^{\\prime}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since $h_{\\mathcal{R}}(x,t)=p_{\\mathcal{R}}(y,T|x,t)$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\int p_{\\mathcal{R}}(x^{\\prime},t^{\\prime}|x,t)h_{\\mathcal{R}}(x^{\\prime},t^{\\prime})=\\int p_{\\mathcal{R}}(x^{\\prime},t^{\\prime}|x,t)p_{\\mathcal{R}}(y,T|x^{\\prime},t^{\\prime})\\mathrm{d}x^{\\prime}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then by the Chapman\u2013Kolmogorov\u2019s equation ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\int p_{\\mathcal{R}}(x^{\\prime},t^{\\prime}|x,t)p_{\\mathcal{R}}(y,T|x^{\\prime},t^{\\prime})\\mathrm{d}x^{\\prime}=p_{\\mathcal{R}}(y,T|x,t),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\int p_{\\mathcal{R}}(x^{\\prime},t^{\\prime}|x,t)h_{\\mathcal{R}}(x^{\\prime},t^{\\prime})=p_{\\mathcal{R}}(y,T|x,t)=h_{\\mathcal{R}}(x,t).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "So the condition is satisfied. Then we can use the result of the Proposition 3.2. The $h$ -transformed SDE is given by ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{R}^{t}=\\left[\\mathbf{f}(\\mathbf{R}^{t},t)+\\sigma^{2}(t)\\nabla_{\\mathbf{R}^{t}}\\log p_{\\mathcal{R}}(y,T|\\mathbf{R}^{t},t)\\right]\\mathrm{d}t+\\sigma(t)\\mathrm{d}\\mathbf{W}_{t}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "And the $_\\mathrm{h}$ -transformed transition density function satisfies ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\int_{A}p_{\\mathcal{R}}^{h}(x^{\\prime},t^{\\prime}|x,t)\\mathrm{d}x^{\\prime}=\\int_{A}p_{\\mathcal{R}}(x^{\\prime},t^{\\prime}|x,t)\\frac{h_{\\mathcal{R}}(x^{\\prime},t^{\\prime})}{h_{\\mathcal{R}}(x,t)}\\mathrm{d}x^{\\prime}}}\\\\ &{}&{\\qquad=\\int_{A}p_{\\mathcal{R}}(x^{\\prime},t^{\\prime}|x,t)\\frac{p_{\\mathcal{R}}(y,T|x^{\\prime},t^{\\prime})}{p_{\\mathcal{R}}(y,T|x,t)}\\mathrm{d}x^{\\prime}}\\\\ &{}&{\\qquad=P(\\mathbf{X}_{t^{\\prime}}\\in A|\\mathbf{X}_{t}=x,\\mathbf{X}_{T}=y),\\quad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we use the Bayes\u2019 theorem to deduce the last equality and $A$ is an arbitrary Borel set. Since ${\\bf R}_{t}$ is a process conditioning on $\\mathbf{X}_{T}=y$ , then $\\mathbf{R}_{T}=y$ almost surly. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "B.4 Proof of Theorem 3.3 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Theorem B.8 (Equivariant Diffusion Bridge). Given an SDE on geometric states $\\mathrm{d}\\mathbf{R}^{t}\\ =$ $\\mathbf{f}_{\\mathcal{R}}(\\mathbf{R}^{t},t)\\mathrm{d}t+\\sigma(t)\\mathrm{\\bar{d}}\\mathbf{W}^{t}$ $\\begin{array}{r}{h_{\\mathcal{R}}(z,t;z_{0})=\\int p_{\\mathcal{R}}(z^{\\prime},T|z,t)\\frac{q_{d a t a}(z^{\\prime}|z_{0})}{p_{\\mathcal{R}}(z^{\\prime},T|z_{0},0)}\\mathrm{d}z^{\\prime}}\\end{array}$ $p_{\\mathcal{R}}(z^{\\prime},t^{\\prime}|z,t),z,z^{\\prime}\\in\\mathcal{R}$ using Proposition 3.2, we $h_{\\mathcal{R}}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\mathrm{i}}\\mathbf{R}^{t}=\\left[\\mathbf{f}_{\\mathcal{R}}(\\mathbf{R}^{t},t)+\\sigma^{2}(t)\\mathbb{E}_{q\\approx(\\mathbf{R}^{T},T|\\mathbf{R}^{t},t;\\mathbf{R}^{0})}[\\nabla_{\\mathbf{R}^{t}}\\log p_{\\mathcal{R}}(\\mathbf{R}^{T},T|\\mathbf{R}^{t},t)|\\mathbf{R}^{0},\\mathbf{R}^{t}]\\right]{\\mathrm{d}}t+\\sigma(t){\\mathrm{d}}\\mathbf{W}_{-\\infty}^{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which corresponds to a process $(\\mathbf{R}^{t})_{t\\in[0,T]},\\mathbf{R}^{0}\\sim q_{d a t a}(R^{t_{0}})$ satisfying the following properties: ", "page_idx": 23}, {"type": "text", "text": "\u2022 let $q(\\cdot,\\cdot)\\,:\\,\\mathcal{R}\\times\\mathcal{R}\\,\\to\\,\\mathbb{R}_{\\geq0}$ denote the joint distribution induced by $(\\mathbf{R}^{t})_{t\\in[0,T]}$ , then $q(\\mathbf{R}^{0},\\mathbf{R}^{T})$ equals to $q_{d a t a}(R^{t_{0}},R^{t_{1}})$ ;   \n\u2022 its transition densit $q_{\\mathcal{R}}(\\mathbf{R}_{\\cdot}^{t^{\\prime}},t^{\\prime}|\\mathbf{R}^{t},t;\\mathbf{R}^{0}){=}q_{\\mathcal{R}}(\\rho^{\\mathcal{R}}(g)[\\mathbf{R}^{t^{\\prime}}],t^{\\prime}|\\rho^{\\mathcal{R}}(g)[\\mathbf{R}^{t}],t;\\rho^{\\mathcal{R}}(g)[\\mathbf{R}^{0}]),$ $\\forall0{\\le}t{<}t^{\\prime}{\\le}T,g{\\in}\\mathrm{SE}(3),\\stackrel{\\cdot}{,}\\stackrel{\\cdot}{\\sim}{q}_{d a t a}(R^{t_{0}})$ . ", "page_idx": 23}, {"type": "text", "text": "We call the tailored diffusion process $(\\mathbf{R}^{t})_{t\\in[0,T]}$ an equivariant diffusion bridge. ", "page_idx": 23}, {"type": "text", "text": "Proof. Let $\\begin{array}{r}{h_{\\mathcal{R}}(z,T;z_{0})=\\frac{q_{\\mathrm{data}}(z|z_{0})}{p_{\\mathcal{R}}(z,T|z_{0},0)}}\\end{array}$ pRqd(atza,(Tz ||zz00,)0), then we define ", "page_idx": 23}, {"type": "equation", "text": "$$\nh_{\\mathcal{R}}(z,t;z_{0})=\\int p_{\\mathcal{R}}(z^{\\prime},T|z,t)\\frac{q_{\\mathrm{data}}(z^{\\prime}|z_{0})}{p_{\\mathcal{R}}(z^{\\prime},T|z_{0},0)}\\mathrm{d}z^{\\prime},\\forall t\\in[0,T).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "So we can easily show that $h_{\\mathcal{R}}(z,t;z_{0})$ satisfies the condition ", "page_idx": 24}, {"type": "equation", "text": "$$\nh_{\\mathcal{R}}(z,t;z_{0})=\\int p_{\\mathcal{R}}(z^{\\prime},T|z,t)h(z^{\\prime},T;z_{0})\\mathrm{d}z^{\\prime},\\forall t\\in[0,T],\\forall z,z_{0}\\in\\mathcal{R}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then we can use the result of Theorem 3.2 to get the $_\\mathrm{h}$ -transformed SDE. By Theorem 3.2, the h-transformed SDE is ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{R}^{t}=\\left[\\mathbf{f}_{\\mathcal{R}}(\\mathbf{R}^{t},t)+\\sigma^{2}(t)\\nabla_{\\mathbf{R}^{t}}\\log h_{\\mathcal{R}}(\\mathbf{R}^{t},t;\\mathbf{R}^{0})\\right]\\mathrm{d}t+\\sigma(t)\\mathrm{d}\\mathbf{W}_{t}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Next, we need to find the explicit form of $\\nabla_{\\mathbf{R}^{t}}\\log h_{\\mathcal{R}}(\\mathbf{R}^{t},t;\\mathbf{R}^{0})$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{z}\\log h_{\\mathcal{R}}(z,t;z_{0})=\\frac{\\nabla_{z}h_{\\mathcal{R}}(z,t;z_{0})}{h_{z}(z,t;z_{0})}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\frac{1}{h_{\\mathcal{R}}(z,t;z_{0})}\\int\\nabla_{z}p_{\\mathcal{R}}(z^{\\prime},T|z,t)\\frac{q_{\\mathrm{data}}(z^{\\prime}|z_{0})}{p_{\\mathcal{R}}(z^{\\prime},T|z_{0},0)}\\mathrm{d}z^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The $_\\mathrm{h}$ -transformed density function is ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q_{\\mathcal{R}}(z^{\\prime},T|z,t;z_{0},0)=p_{\\mathcal{R}}(z^{\\prime},T|z,t)\\frac{h_{\\mathcal{R}}(z^{\\prime},T;z_{0})}{h_{\\mathcal{R}}(z,t;z_{0})}\\qquad}\\\\ &{\\qquad\\qquad\\qquad\\qquad=p_{\\mathcal{R}}(z^{\\prime},T|z,t)\\frac{q_{\\mathrm{data}}(z^{\\prime}|z_{0})}{p_{\\mathcal{R}}(z^{\\prime},T;z_{0},0)h_{\\mathcal{R}}(z,t;z_{0})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{z}\\log h_{\\mathcal{R}}(z,t;z_{0})=\\frac{1}{h_{\\mathcal{R}}(z,t;z_{0})}\\int\\nabla_{z}p_{\\mathcal{R}}(z^{\\prime},T|z,t)\\frac{q_{\\mathrm{data}}(z^{\\prime}|z_{0})}{p_{\\mathcal{R}}(z^{\\prime},T|z_{0},0)}\\mathrm{d}z^{\\prime}}\\\\ &{\\qquad\\qquad\\qquad=\\int\\nabla_{z}p_{\\mathcal{R}}(z^{\\prime},T|z,t)\\frac{q_{\\mathcal{R}}(z^{\\prime},T|z,t;z_{0},0)}{p_{\\mathcal{R}}(z^{\\prime},T|z,t)}\\mathrm{d}z^{\\prime}}\\\\ &{\\qquad\\qquad\\qquad=\\int\\nabla_{z}\\log p_{\\mathcal{R}}(z^{\\prime},T|z,t)q_{\\mathcal{R}}(z^{\\prime},T|z,t;z_{0},0)\\mathrm{d}z^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "So we get a explicit form of $\\nabla_{\\mathbf{R}^{t}}\\log h_{\\mathcal{R}}(\\mathbf{R}^{t},t;\\mathbf{R}^{0})$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{{\\mathbf{R}}^{t}}\\log h_{\\mathcal{R}}({\\mathbf{R}}^{t},t;{\\mathbf{R}}^{0})=\\mathbb{E}_{q_{\\mathcal{R}}({\\mathbf{R}}^{T},T|{\\mathbf{R}}^{t},t;z_{0})}[\\nabla_{{\\mathbf{R}}^{t}}\\log p_{\\mathcal{R}}({\\mathbf{R}}^{T},T|{\\mathbf{R}}^{t},t)|z_{0},{\\mathbf{R}}^{t}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then the $_\\mathrm{h}$ -transformed SDE becomes ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}\\mathbf{R}^{t}=\\left[\\mathbf{f}_{\\mathcal{R}}(\\mathbf{R}^{t},t)+\\sigma^{2}(t)\\mathbb{E}_{q\\Re{(\\mathbf{R}^{T},T|\\mathbf{R}^{t},t;z_{0})}}[\\nabla_{\\mathbf{R}^{t}}\\log p_{\\mathcal{R}}(\\mathbf{R}^{T},T|\\mathbf{R}^{t},t)|z_{0},\\mathbf{R}^{t}]\\right]\\mathrm{d}t+\\sigma(t)\\mathrm{d}\\mathbf{W}^{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{q_{\\mathcal{R}}(z^{\\prime},T|z_{0},0)=p_{\\mathcal{R}}(z^{\\prime},T|z_{0},0)\\frac{q_{\\mathrm{data}}(z^{\\prime}|z_{0})}{p_{\\mathcal{R}}(z^{\\prime},T;z_{0},0)h_{\\mathcal{R}}(z_{0},0;z_{0})}=q_{\\mathrm{data}}(z^{\\prime}|z_{0}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which means $q_{\\mathcal{R}}(\\mathbf{R}^{T},T|\\mathbf{R}^{0},0)=q_{\\mathrm{data}}(\\mathbf{R}^{T}|\\mathbf{R}^{0})$ . Since the initial distribution ${\\bf R}^{0}\\sim q_{\\mathrm{data}}(R^{t_{0}})$ , so $q_{\\mathcal{R}}({\\mathbf{R}^{0}})=q_{\\mathrm{data}}({\\dot{\\mathbf{R}}^{0}})$ . So we can deduce that ", "page_idx": 24}, {"type": "equation", "text": "$$\nq(\\mathbf{R}^{0},\\mathbf{R}^{T})=q_{\\mathcal{R}}(\\mathbf{R}^{0})q_{\\mathcal{R}}(\\mathbf{R}^{T},T|\\mathbf{R}^{0},0)=q_{\\mathrm{data}}(\\mathbf{R}^{0})q_{\\mathrm{data}}(\\mathbf{R}^{T}|\\mathbf{R}^{0})=q_{\\mathrm{data}}(\\mathbf{R}^{0},\\mathbf{R}^{T}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Finally, we need to show that the transition density function satisfies the corresponding symmetric constrains. Since $\\begin{array}{r}{h_{\\mathcal{R}}(z^{\\prime},T;z_{0})=\\frac{q_{\\mathrm{data}}(z^{\\prime}|z_{0})}{p_{\\mathcal{R}}(z^{\\prime},T|z_{0},0)}}\\end{array}$ = pRqd(atza\u2032(,zT ||zz00,)0) is SE(3)-invariant, i.e. ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h_{\\mathscr{R}}(\\rho^{\\mathscr{R}}(g)[z],T;\\rho^{\\mathscr{R}}(g)[z_{0}])=h_{\\mathscr{R}}(z^{\\prime},T;z_{0}),\\forall g\\in\\mathrm{SE}(3),}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "we can show that $h(\\cdot,t;\\cdot)$ is also SE(3)-invariant $\\forall t\\in[0,T]$ using the following property ", "page_idx": 24}, {"type": "equation", "text": "$$\nh_{\\mathcal{R}}(z,t;z_{0})=\\int p_{\\mathcal{R}}(z^{\\prime},T|z,t)h(z^{\\prime},T;z_{0})\\mathrm{d}z^{\\prime}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For any $g\\,\\in\\,\\mathrm{SE}(3)$ , assume $\\rho^{\\mathcal{R}}(g)[z]\\,=\\,{\\bf O}_{\\mathcal{R}}(g)z+{\\bf t}_{\\mathcal{R}}$ , where $\\mathbf{O}_{\\mathcal{R}}(g)$ is an orthogonal matrix satisfying $\\operatorname*{det}(\\mathbf{O}_{\\mathcal{R}}(g))=1$ , then we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{\\mathcal{R}}(\\rho^{\\mathcal{R}}(g)[z],t;\\rho^{\\mathcal{R}}(g)[z_{0}])=\\displaystyle\\int p_{\\mathcal{R}}(z^{\\prime},T|\\rho^{\\mathcal{R}}(g)[z],t)h(z^{\\prime},T;\\rho^{\\mathcal{R}}(g)[z_{0}])\\mathrm{d}z^{\\prime}}\\\\ &{\\,=\\displaystyle\\int p_{\\mathcal{R}}\\left(\\rho^{\\mathcal{R}}(g)(\\rho^{\\mathcal{R}}(g))^{-1}[z^{\\prime}],T|\\rho^{\\mathcal{R}}(g)[z],t\\right)h(\\rho^{\\mathcal{R}}(g)(\\rho^{\\mathcal{R}}(g))^{-1}[z^{\\prime}],T;\\rho^{\\mathcal{R}}(g)[z_{0}])\\mathrm{d}z^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By Proposition 3.1, $p_{\\mathcal{R}}\\left(\\rho^{\\mathcal{R}}(g)(\\rho^{\\mathcal{R}}(g))^{-1}[z^{\\prime}],t_{0}|\\rho^{\\mathcal{R}}(g)[z],t\\right)\\;=\\;p_{\\mathcal{R}}\\left((\\rho^{\\mathcal{R}}(g))^{-1}[z^{\\prime}],t_{0}|z,t\\right)\\!.$ , let $z_{1}=\\rho^{\\mathcal{R}}(g))^{-1}[z^{\\prime}]$ , then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{h_{\\mathcal{R}}(\\rho^{\\mathcal{R}}(g)[z],t;\\rho^{\\mathcal{R}}(g)[z_{0}])}\\\\ &{=\\displaystyle\\int p_{\\mathcal{R}}\\left((\\rho^{\\mathcal{R}}(g))^{-1}[z^{\\prime}],T|z,t\\right)h(\\rho^{\\mathcal{R}}(g)(\\rho^{\\mathcal{R}}(g))^{-1}[z^{\\prime}],T;\\rho^{\\mathcal{R}}(g)[z_{0}])\\mathrm{d}z^{\\prime}}\\\\ &{=\\displaystyle\\int p_{\\mathcal{R}}\\left(z_{1},T|z,t\\right)h(\\rho^{\\mathcal{R}}(g)[z_{1}],T;\\rho^{\\mathcal{R}}(g)[z_{0}])\\operatorname*{det}(\\mathbf{O}_{\\mathcal{R}}(g))\\mathrm{d}z_{1}}\\\\ &{=\\displaystyle\\int p_{\\mathcal{R}}\\left(z_{1},t_{0}|z,t\\right)h(z_{1},t_{0};z_{0})\\mathrm{d}z_{1}}\\\\ &{=h_{\\mathcal{R}}(z,t;z_{0}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "So $h(\\cdot,t;\\cdot)$ is $\\mathrm{SE}(3)$ -invariant $\\forall t\\in[0,T]$ . Then we show $q_{\\mathcal{R}}(z^{\\prime},t^{\\prime}|z,t;z_{0},0)$ preserves the symmetric constraints: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{q_{\\mathcal{R}}(\\rho^{\\mathcal{R}}(g)[z^{\\prime}],t^{\\prime}|\\rho^{\\mathcal{R}}(g)[z],t;\\rho^{\\mathcal{R}}(g)[z_{0}],0)}\\\\ &{\\quad=p_{\\mathcal{R}}(\\rho^{\\mathcal{R}}(g)[z^{\\prime}],t^{\\prime}|\\rho^{\\mathcal{R}}(g)[z],t)\\frac{h_{\\mathcal{R}}(\\rho^{\\mathcal{R}}(g)[z^{\\prime}],t^{\\prime};\\rho^{\\mathcal{R}}(g)[z_{0}])}{h_{\\mathcal{R}}(\\rho^{\\mathcal{R}}(g)[z],t;\\rho^{\\mathcal{R}}(g)[z_{0}])}}\\\\ &{\\quad=p_{\\mathcal{R}}(z^{\\prime},t^{\\prime}|z,t)\\frac{h_{\\mathcal{R}}(\\rho^{\\mathcal{R}}(g)[z^{\\prime}],t^{\\prime};\\rho^{\\mathcal{R}}(g)[z_{0}])}{h_{\\mathcal{R}}(\\rho^{\\mathcal{R}}(g)[z],t;\\rho^{\\mathcal{R}}(g)[z_{0}])}}\\\\ &{\\quad=p_{\\mathcal{R}}(z^{\\prime},t^{\\prime}|z,t)\\frac{h_{\\mathcal{R}}(z^{\\prime},t^{\\prime};z_{0})}{h_{\\mathcal{R}}(z,t;z_{0})}}\\\\ &{\\quad=q_{\\mathcal{R}}(z^{\\prime},t^{\\prime}|z,t;z_{0},0),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "which completes our proof. ", "page_idx": 25}, {"type": "text", "text": "B.5 Objective Function of the Equivariant Diffusion Bridge ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Lemma B.9. Let ${\\bf X}_{1},\\cdot\\cdot\\cdot\\mathrm{{\\bf~,}}{\\bf X}_{n},{\\bf Y},{\\bf Z}$ be random variables. Then the optimal approximation of $\\mathbf{Y}$ based on $\\{{\\bf X}\\}_{i=1}^{n}$ is $f^{*}(\\mathbf{X}_{1},\\cdot\\cdot\\cdot,\\mathbf{X}_{n})=\\mathop{\\mathrm{arg}\\,\\mathrm{min}}_{f}\\mathbb{E}\\|\\mathbf{Y}-f(\\mathbf{X}_{1},\\cdot\\cdot\\cdot,\\bar{\\mathbf{X}_{n}})\\|^{2}=\\mathbb{E}[\\mathbf{Y}|\\mathbf{X}_{1},\\cdot\\cdot\\cdot\\,,\\bar{\\mathbf{X}}_{n}]$ . ", "page_idx": 25}, {"type": "text", "text": "Proof. Denote $\\mathbf{X}=(\\mathbf{X}_{1},\\cdot\\cdot\\cdot\\mathbf{\\mu},\\mathbf{X}_{n})$ . We show the following decomposition first: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{{\\mathbb{E}}\\|{\\mathbf{Y}}-f({\\mathbf{X}})\\|^{2}={\\mathbb{E}}\\|{\\mathbf{Y}}-{\\mathbb{E}}[{\\mathbf{Y}}|{\\mathbf{X}}]\\|^{2}+{\\mathbb{E}}\\left[\\|{\\mathbb{E}}[{\\mathbf{Y}}|{\\mathbf{X}}]-f({\\mathbf{X}})\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We can compute $\\mathbb{E}\\|\\mathbf{Y}-f(\\mathbf{X})\\|^{2}$ directly by ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|\\mathbf{Y}-f(\\mathbf{X})\\|^{2}=\\mathbb{E}\\|\\mathbf{Y}-\\mathbb{E}[\\mathbf{Y}|\\mathbf{X}]+\\mathbb{E}[\\mathbf{Y}|\\mathbf{X}]-f(\\mathbf{X})\\|^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\mathbb{E}\\|\\mathbf{Y}-\\mathbb{E}[\\mathbf{Y}|\\mathbf{X}]\\|^{2}+\\mathbb{E}\\left[\\|\\mathbb{E}[\\mathbf{Y}|\\mathbf{X}]-f(\\mathbf{X})\\|^{2}\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\mathbb{E}\\langle\\mathbf{Y}-\\mathbb{E}[\\mathbf{Y}|\\mathbf{X}],\\mathbb{E}[\\mathbf{Y}|\\mathbf{X}]-f(\\mathbf{X})\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since ", "page_idx": 25}, {"type": "text", "text": "$\\operatorname{\\mathbb{E}}\\langle\\mathbf{Y}-\\operatorname{\\mathbb{E}}[\\mathbf{Y}|\\mathbf{X}],\\operatorname{\\mathbb{E}}[\\mathbf{Y}|\\mathbf{X}]-f(\\mathbf{X})\\rangle=\\operatorname{\\mathbb{E}}\\left[\\operatorname{\\mathbb{E}}\\langle\\mathbf{Y}-\\operatorname{\\mathbb{E}}[\\mathbf{Y}|\\mathbf{X}],\\operatorname{\\mathbb{E}}[\\mathbf{Y}|\\mathbf{X}]-f(\\mathbf{X})\\rangle|\\mathbf{X}\\right]=0,$ (107 we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\|{\\mathbf{Y}}-f({\\mathbf{X}})\\|^{2}=\\mathbb{E}\\|{\\mathbf{Y}}-\\mathbb{E}[{\\mathbf{Y}}|{\\mathbf{X}}]\\|^{2}+\\mathbb{E}\\left[\\|\\mathbb{E}[{\\mathbf{Y}}|{\\mathbf{X}}]-f({\\mathbf{X}})\\|^{2}\\right]}\\\\ &{\\qquad\\qquad+\\operatorname{\\mathbb{E}}\\langle{\\mathbf{Y}}-\\mathbb{E}[{\\mathbf{Y}}|{\\mathbf{X}}],\\mathbb{E}[{\\mathbf{Y}}|{\\mathbf{X}}]-f({\\mathbf{X}})\\rangle}\\\\ &{\\qquad\\qquad=\\mathbb{E}\\|{\\mathbf{Y}}-\\mathbb{E}[{\\mathbf{Y}}|{\\mathbf{X}}]\\|^{2}+\\mathbb{E}\\left[\\|\\mathbb{E}[{\\mathbf{Y}}|{\\mathbf{X}}]-f({\\mathbf{X}})\\|^{2}\\right]}\\\\ &{\\qquad\\qquad\\geq\\mathbb{E}\\|{\\mathbf{Y}}-\\mathbb{E}[{\\mathbf{Y}}|{\\mathbf{X}}_{1},\\cdot\\cdot\\cdot,{\\mathbf{X}}_{n}]\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The inequality becomes equality if and only if $f(\\mathbf{X}_{1},\\cdot\\cdot\\cdot,\\mathbf{X}_{n})\\,=\\,\\mathbb{E}[\\mathbf{Y}|\\mathbf{X}_{1},\\cdot\\cdot\\cdot\\,,\\mathbf{X}_{n}]$ . So the the optimal approximation of $\\mathbf{Y}$ based on $\\{\\mathbf{X}\\}_{i=1}^{n}$ is $\\mathbb{E}[\\mathbf{Y}|\\mathbf{X}_{1},\\cdots,\\mathbf{X}_{n}]$ , i.e. ", "page_idx": 26}, {"type": "equation", "text": "$$\nf^{*}(\\mathbf{X}_{1},\\cdot\\cdot\\cdot,\\mathbf{X}_{n})=\\arg\\operatorname*{min}_{f}\\mathbb{E}\\|\\mathbf{Y}-f(\\mathbf{X}_{1},\\cdot\\cdot\\cdot,\\mathbf{X}_{n})\\|^{2}=\\mathbb{E}[\\mathbf{Y}|\\mathbf{X}_{1},\\cdot\\cdot\\cdot\\cdot,\\mathbf{X}_{n}].\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proposition B.10. The training objective function of Equivariant Diffusion Bridge is: ", "page_idx": 26}, {"type": "text", "text": "$\\begin{array}{r}{\\Sigma(\\theta)=\\mathbb{E}_{(z_{0},z_{1})\\sim q_{d a t a}(R^{t_{0}},R^{t_{1}}),\\mathbf{R}^{t}\\sim q_{\\mathcal{R}}(\\mathbf{R}^{t},t|z_{1},T;z_{0},0)}\\lambda(t)\\|\\mathbf{v}_{\\theta}(\\mathbf{R}^{t},t;z_{0})\\!-\\!\\nabla_{\\mathbf{R}^{t}}\\log p_{\\mathcal{R}}(z_{1},T|\\mathbf{R}^{t},t)\\|^{2},}\\end{array}$ (113) where $t\\sim\\mathcal{U}(0,T)$ . Then the optimal parameter $\\theta^{*}=\\arg\\operatorname*{min}_{\\theta}\\mathcal{L}(\\theta)$ satisfies ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbf{v}_{\\theta^{*}}(\\mathbf{R}^{t},t;z_{0})=\\mathbb{E}_{q_{\\mathcal{R}}(\\mathbf{R}^{T},T|\\mathbf{R}^{t},t;\\mathbf{R}^{0})}[\\nabla_{\\mathbf{R}^{t}}\\log p_{\\mathcal{R}}(\\mathbf{R}^{T},T|\\mathbf{R}^{t},t)|\\mathbf{R}^{0},\\mathbf{R}^{t}].\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. Let $\\mathcal{L}(\\theta)=\\mathbb{E}_{t\\sim\\mathcal{U}(0,T)}\\lambda(t)\\mathcal{L}^{t}(\\theta)$ , where ", "page_idx": 26}, {"type": "text", "text": "$\\begin{array}{r}{\\mathcal{L}^{t}(\\theta)=\\mathbb{E}_{(z_{0},z_{1})\\sim q_{\\mathrm{dat}}(R^{t_{0}},R^{t_{1}}),\\mathbf{R}^{t}\\sim q_{\\mathcal{R}}(\\mathbf{R}^{t},t|z_{1},T;z_{0},0)}\\|\\mathbf{v}_{\\theta}(\\mathbf{R}^{t},t;z_{0})-\\nabla_{\\mathbf{R}^{t}}\\log p_{\\mathcal{R}}(z_{1},T|\\mathbf{R}^{t},t)\\|^{2}.}\\end{array}$ (115) Then by Lemma B.9, $\\begin{array}{r}{{\\bf v}_{\\theta}({\\bf R}^{t},t;z_{0})=\\mathbb{E}_{q_{\\mathcal{R}}({\\bf R}^{T},T|{\\bf R}^{t},t;{\\bf R}^{\\mathrm{o}})}[\\nabla_{{\\bf R}^{t}}\\log p_{\\mathcal{R}}({\\bf R}^{T},T|{\\bf R}^{t},t)|{\\bf R}^{0},{\\bf R}^{t}]}\\end{array}$ minimize ${\\mathcal{L}}^{t}(\\theta),\\forall t\\in[0,T]$ . Since $\\lambda(t)\\geq0$ , so the optimal parameter $\\theta^{*}=\\arg\\operatorname*{min}_{\\theta}\\mathcal{L}(\\theta)$ satisfies ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{v}_{\\theta^{*}}(\\mathbf{R}^{t},t;z_{0})=\\mathbb{E}_{q_{\\mathcal{R}}(\\mathbf{R}^{T},T|\\mathbf{R}^{t},t;\\mathbf{R}^{0})}[\\nabla_{\\mathbf{R}^{t}}\\log p_{\\mathcal{R}}(\\mathbf{R}^{T},T|\\mathbf{R}^{t},t)|\\mathbf{R}^{0},\\mathbf{R}^{t}],\\forall t\\in[0,T].}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "B.6 Proof of Theorem 3.4 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Theorem B.11 (Chain of Equivariant Diffusion Bridges). Let $\\{(\\mathbf{R}_{i}^{t})_{t\\in[0,T]}\\}_{i\\in[N-1]}$ denote a series of $N$ equivaraint diffusion bridges defined in Theorem 3.3. For the $i$ -th bridge $(\\mathbf{R}_{i}^{t})_{t\\in[0,T]}$ , if we set (1) hiR(z, t; z0) =  pR(z\u2032, T|z, t)pqRtir(a+jz 1\u2032,(Tz \u2032||zz00,)0) ; (2) ${\\bf R}_{0}^{0}\\sim q_{t r a j}^{0}(\\tilde{R}^{0}),{\\bf R}_{i}^{0}={\\bf R}_{i-1}^{T},\\forall0<i<N_{\\!\\!\\perp}$ then the joint distribution $q_{\\mathcal{R}}(\\mathbf{R}_{0}^{0},\\mathbf{R}_{0}^{T},\\mathbf{R}_{1}^{T},\\cdot\\cdot\\cdot\\mathbf{\\Omega},\\mathbf{R}_{N-1}^{T})$ induced by $\\{(\\mathbf{R}^{t})_{t\\in[0,T]}\\}_{i\\in[N-1]}$ equals to $q_{t r a j}(\\tilde{R}^{0},...,\\tilde{R}^{N})$ . We call this process a chain of equivariant diffusion bridges. ", "page_idx": 26}, {"type": "text", "text": "Proof. By Theorem 3.3, the transition density function of $(\\mathbf{R}_{i}^{t})_{t\\in[0,T]}$ satisfies $q_{\\mathcal{R}}^{i}(\\mathbf{R}_{i}^{T}|\\mathbf{R}_{i}^{0})\\ =$ $q_{\\mathrm{traj}}^{i}(\\mathbf{R}_{i}^{T}|\\mathbf{R}_{i}^{0}),\\forall0\\leq i\\leq N-1$ . The ground truth probability density function has the decomposition $\\begin{array}{r}{q_{\\mathrm{traj}}^{0}(\\tilde{R}^{0})\\prod_{i=1}^{N}q_{\\mathrm{traj}}^{i}(\\tilde{R}^{i}|\\tilde{R}^{i-1})}\\end{array}$ . Then we use the boundary condition, ${\\bf R}_{0}^{0}\\sim q_{\\mathrm{traj}}^{0}(\\tilde{R}^{0}),{\\bf R}_{i}^{0}=$ ${\\mathbf{R}}_{i-1}^{T},\\forall0<i<N$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle q({\\bf R}_{0}^{0},{\\bf R}_{0}^{T},{\\bf R}_{1}^{T},\\cdots,{\\bf R}_{N-1}^{T})=q_{R}^{0}({\\bf R}_{0}^{0})\\prod_{i=1}^{N}q_{R}^{i}({\\bf R}_{i}^{T}|{\\bf R}_{i-1}^{T})}}\\\\ {~~}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}=q_{R}^{0}({\\bf R}_{0}^{0})\\prod_{i=1}^{N}q_{R}^{i}({\\bf R}_{i}^{T}|{\\bf R}_{i}^{0})}}\\\\ {~~}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}=q_{\\mathrm{rai}}^{0}({\\bf R}_{0}^{0})\\prod_{i=1}^{N}q_{\\mathrm{rai}}^{i}({\\bf R}_{i}^{T}|{\\bf R}_{i}^{0})}}\\\\ {~~~}\\\\ {{\\displaystyle~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}=q_{\\mathrm{rai}}({\\bf R}_{0}^{0},{\\bf R}_{0}^{T},{\\bf R}_{1}^{T},\\cdots,{\\bf R}_{N-1}^{T}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "So the joint distribution $q_{\\mathcal{R}}(\\mathbf{R}_{0}^{0},\\mathbf{R}_{0}^{T},\\mathbf{R}_{1}^{T},\\cdot\\cdot\\cdot\\mathbf{\\Omega},\\mathbf{R}_{N-1}^{T})$ induced by $\\{(\\mathbf{R}^{t})_{t\\in[0,T]}\\}_{i\\in[N-1]}$ equals to $q_{\\mathrm{traj}}(\\tilde{R}^{0},...,\\tilde{R}^{N})$ . \u53e3 ", "page_idx": 26}, {"type": "text", "text": "B.7 Objective of the Chain of Equivariant Diffusion Bridge ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Proposition B.12. The training objective function of the Chain of Equivariant Diffusion Bridge is: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{L}^{\\prime}(\\theta)=\\mathbb{E}_{(z_{0},\\ldots,z_{N})\\sim q_{\\pi\\circ j}(\\Tilde{R}^{0},\\ldots,\\Tilde{R}^{N}),t,\\mathbf{R}_{i}^{\\prime}}\\lambda(t)\\lVert\\mathbf{v}_{\\theta}(\\mathbf{R}_{i}^{t^{\\prime}},t;z_{i})-\\nabla_{\\mathbf{R}_{i}^{\\prime\\prime}}\\log p_{\\mathcal{R}}^{i}(z_{i+1},T|\\mathbf{R}_{i}^{t^{\\prime}},t^{\\prime})\\rVert^{2},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\begin{array}{r}{t\\sim\\mathcal{U}(0,N\\times T),i=\\lfloor\\frac{t}{T}\\rfloor,t^{\\prime}=t-i\\times T,\\mathbf{R}_{i}^{t^{\\prime}}\\sim q_{\\mathcal{R}}^{i}(\\mathbf{R}^{t^{\\prime}},t^{\\prime}|z_{i+1},T;z_{i},0)}\\end{array}$ . Then the optimal parameter $\\theta^{*}=\\arg\\operatorname*{min}_{\\theta}\\mathcal{L}^{\\prime}(\\theta)$ satisfies ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{v}_{\\theta^{*}}(\\mathbf{R}_{i}^{t^{\\prime}},t;z_{i})=\\mathbb{E}_{q_{\\mathcal{R}}^{i}(\\mathbf{R}_{i}^{T},T|\\mathbf{R}_{i}^{t^{\\prime}},t;\\mathbf{R}_{i}^{0})}[\\nabla_{\\mathbf{R}_{i}^{t}}\\log p_{\\mathcal{R}}^{i}(\\mathbf{R}_{i}^{T},T|\\mathbf{R}_{i}^{t},t)|\\mathbf{R}_{i}^{0},\\mathbf{R}_{i}^{t}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. Let $\\mathcal{L}^{\\prime}(\\theta)=\\mathbb{E}_{t\\sim\\mathcal{U}(0,N T)}\\lambda(t)\\mathcal{L}_{t}^{\\prime}(\\theta)$ , where ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathcal{L}_{t}^{\\prime}(\\theta)=\\mathbb{E}_{(z_{0},\\dots,z_{N})\\sim q_{\\mathrm{tri}}(\\tilde{R}^{0},\\dots,\\tilde{R}^{N}),t,\\mathbf{R}_{i}^{\\prime}}||\\mathbf{v}_{\\theta}(\\mathbf{R}_{i}^{t^{\\prime}},t;z_{i})-\\nabla_{\\mathbf{R}_{i}^{\\prime}}\\log p_{\\mathcal{R}}^{i}(z_{i+1},T|\\mathbf{R}_{i}^{t^{\\prime}},t^{\\prime})||^{2},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "whe $\\begin{array}{r}{\\mathrm{~r~e~}t\\sim\\mathcal{U}(0,N\\times T),i=\\big\\lfloor\\frac{t}{T}\\big\\rfloor,t^{\\prime}=t-i\\times T,\\mathbf{R}_{i}^{t^{\\prime}}\\sim q_{\\mathcal{R}}^{i}(\\mathbf{R}^{t^{\\prime}},t^{\\prime}|z_{i+1},T;z_{i},0).\\mathrm{~Then}\\,!}\\end{array}$ by Lemma B.9, $\\begin{array}{r}{\\mathbf{v}_{\\theta^{*}}(\\mathbf{R}_{i}^{t^{\\prime}},t;z_{i})=\\mathbb{E}_{q_{\\mathcal{R}}^{i}(\\mathbf{R}_{i}^{T},T|\\mathbf{R}_{i}^{t^{\\prime}},t;\\mathbf{R}_{i}^{0})}[\\nabla_{\\mathbf{R}_{i}^{t}}\\log p_{\\mathcal{R}}^{i}(\\mathbf{R}_{i}^{T},T|\\mathbf{R}_{i}^{t},t)|\\mathbf{R}_{i}^{0},\\mathbf{R}_{i}^{t}]}\\end{array}$ minimize $\\mathcal{L}_{t}^{\\prime}(\\theta),\\forall t\\in$ $[0,N T]$ . Since $\\lambda(t)\\geq0$ , so the optimal parameter $\\theta^{*}=\\arg\\operatorname*{min}_{\\theta}\\mathcal{L}(\\theta)$ satisfies ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{v}_{\\theta^{*}}(\\mathbf{R}_{i}^{t^{\\prime}},t;z_{i})=\\mathbb{E}_{q_{\\mathcal{R}}^{i}(\\mathbf{R}_{i}^{T},T|\\mathbf{R}_{i}^{t^{\\prime}},t;\\mathbf{R}_{i}^{0})}[\\nabla_{\\mathbf{R}_{i}^{t}}\\log p_{\\mathcal{R}}^{i}(\\mathbf{R}_{i}^{T},T|\\mathbf{R}_{i}^{t},t)|\\mathbf{R}_{i}^{0},\\mathbf{R}_{i}^{t}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "B.8 Proof of Theorem 3.5 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this paper, we choose the Brownian bridge as our matching target. Let\u2019s first recall the definition and properties of the Brownian bridge. A Brownian bridge $(X_{t})_{t\\in[0,T]}$ with the initial position $X_{0}$ and the terminal position $X_{T}$ is given by the following SDE ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{X}_{t}=\\frac{\\mathbf{X}_{T}-\\mathbf{X}_{t}}{T-t}\\mathrm{d}t+\\sigma\\mathrm{d}\\mathbf{W}_{t},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\mathbf{W}_{t}$ is the standard wiener process. The solution of the Brownian bridge is given by ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbf{X}_{t}\\sim\\mathcal{N}\\left((1-t)\\mathbf{X}_{0}+t\\mathbf{X}_{1},\\sigma^{2}t(1-t)\\right).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Next, we recall the definition of the KL Divergence: ", "page_idx": 27}, {"type": "text", "text": "Definition B.13 (KL Divergence). The relative entropy (or Kullback\u2013Leibler Divergence) $\\operatorname{KL}(f||g)$ between two probability density functions $f(x)$ and $g(x)$ is defined by: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathrm{KL}(f||g)=\\int f(x)\\log\\frac{f(x)}{g(x)}\\mathrm{d}x.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "In general, let $\\mathbb{P}$ and $\\mathbb{Q}$ be two probability measures on space $\\mathcal{X}$ . Assume $\\mathbb{P}$ is absolutely continuous with respect to $\\mathbb{Q}$ then the Kullback\u2013Leibler Divergence between $\\mathbb{P}$ and $\\mathbb{Q}$ is defined as follows ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathrm{KL}(\\mathbb{P}||\\mathbb{Q})=\\int_{\\mathcal{X}}\\log\\frac{\\mathrm{d}\\mathbb{P}}{\\mathrm{d}\\mathbb{Q}}\\mathrm{d}\\mathbb{P},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\frac{\\mathrm{d}\\mathbb{P}}{\\mathrm{d}\\mathbb{Q}}$ is the Radon\u2013Nikodym derivative of $\\mathbb{P}$ with respect to $\\mathbb{Q}$ . ", "page_idx": 27}, {"type": "text", "text": "When we need to compute the KL divergence between the path measures associated with two SDEs, the Girsanov\u2019s theorem [53] is an useful tool to get the Radon\u2013Nikodym derivative between the two measure. The precise statements are as follows. ", "page_idx": 27}, {"type": "text", "text": "Theorem B.14 (Girsanov\u2019s Theorem). Let $\\mathbf{W}_{t}$ be a $d$ -dimensional Wiener process defined on $(\\Omega,\\mathcal{F},(\\mathcal{F}_{t}),\\mathbb{P})$ . Let $\\mathbf{H}_{t}$ be a $d$ -dimensional $\\mathcal{F}_{t}$ \u2212adapted process such that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\int_{0}^{T}\\|\\mathbf{H}_{t}\\|^{2}\\mathrm{d}t<\\infty,\\mathbb{P}-a.s.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Define ", "page_idx": 28}, {"type": "equation", "text": "$$\nZ_{t}=\\exp\\left(\\int_{0}^{t}\\mathbf{H}_{s}\\cdot\\mathrm{d}\\mathbf{W}_{t}-\\frac12\\int_{0}^{t}\\|\\mathbf{H}_{t}\\|^{2}\\mathrm{d}s\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Assume $Z_{t}$ is a martingale. Define the probability measure $\\mathbb{Q}$ on $\\mathcal{F}_{T}$ by ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbb{Q}=Z_{T}\\mathrm{d}\\mathbb{P}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Let $\\begin{array}{r}{\\mathbf{M}_{t}=\\mathbf{W}_{t}-\\int_{0}^{t}\\mathbf{H}_{s}\\mathrm{d}s,}\\end{array}$ , then ${{\\bf{M}}_{t}}$ is a $d-$ dimensional Wiener process with respect to $\\mathbb{Q}$ . ", "page_idx": 28}, {"type": "text", "text": "In practice, the condition that $Z_{t}$ is a martingale is hard to vertify. So the condition is often replaced by the Novikov\u2019s condition ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\exp\\left(\\frac{1}{2}\\int_{0}^{T}\\|\\mathbf{H}_{t}\\|^{2}\\mathrm{d}t\\right)\\right]<\\infty.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "For more discussions and applications of the Girsanov\u2019s theorem, please see [86, 74, 28]. Now we can give the precise assumptions and proof of Theorem 3.5 using the properties of Brownian Bridge and Theorem B.14. ", "page_idx": 28}, {"type": "text", "text": "Theorem B.15. Assume $(\\tilde{R}^{i})_{i\\in[N]}$ is sampled by simulating a prior SDE on geometric states $\\mathrm{d}\\tilde{\\mathbf{R}}^{t}=$ $-\\nabla H_{\\mathcal{R}}^{*}(\\tilde{\\mathbf{R}}^{t})\\mathrm{d}t+\\sigma\\mathrm{d}\\tilde{\\mathbf{W}}^{t}$ . Let $\\mu_{i}^{*}$ denote the path measure of this prior SDE when $t\\in[i T,(i+1)T]$ . Building upon $(\\tilde{R}^{i})_{i\\in[N]}$ , let $\\{\\mu_{\\mathcal{R}}^{i}\\}_{i\\in[N-1]}$ denote the path measure of our chain of equivariant diffusion bridges. Assume $\\{\\mu_{\\mathcal{R}}^{i}\\}_{i\\in[N-1]}$ is composed of chain of the Brownian Bridge. Assume the total time is $N T=1$ . Under the following assumptions: ", "page_idx": 28}, {"type": "text", "text": "\u2022 $H_{\\mathcal{R}}^{*}(\\cdot):\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ is a scalar function with continuous second-order partial derivative; ", "page_idx": 28}, {"type": "text", "text": "\u2022 The drift function is Lipschitz: there exist a constant $L$ such that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla H_{\\mathcal{R}}^{*}(x)-\\nabla H_{\\mathcal{R}}^{*}(y)\\|\\leq L\\|x-y\\|,\\forall x,y\\in\\mathbb{R}^{d};}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "\u2022 $H_{\\mathcal{R}}^{*}(\\cdot)$ satisfies $\\|\\nabla H_{\\mathcal{R}}^{*}(x)\\|\\le K(1+\\|x\\|),\\forall x\\in\\mathbb{R}^{d},$ ; ", "page_idx": 28}, {"type": "text", "text": "\u2022 $\\mathbb{E}\\Vert\\tilde{\\mathbf{R}}^{t}\\Vert^{2}<M,\\forall t\\in[0,N T],$ ; ", "page_idx": 28}, {"type": "text", "text": "\u2022 $h(t)=\\mathbb{E}[H_{\\mathcal{R}}^{*}(\\tilde{\\mathbf{R}}^{t})]$ is a continuous function on $t\\in[0,N T].$ ; ", "page_idx": 28}, {"type": "text", "text": "\u2022 The Novikov\u2019s condition: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[\\exp\\left(\\frac{1}{2}\\int_{0}^{N T}\\|\\nabla H_{\\mathcal{R}}^{*}(\\tilde{\\mathbf{W}}^{t})\\|^{2}\\mathrm{d}t\\right)\\right]<\\infty;\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "\u2022 The function $H_{\\mathcal{R}}^{*}$ satisfies the following regulaity condition: there exist a constant $C$ such that $\\nabla^{2}H_{\\mathcal{R}}^{*}(x)-\\|\\nabla H_{\\mathcal{R}}^{*}(x)\\|^{2}/\\sigma^{2}<C,\\forall x\\in\\mathbb{R}^{d}$ ; ", "page_idx": 28}, {"type": "text", "text": "then we have $\\operatorname*{lim}_{N\\to\\infty}\\operatorname*{max}_{i}\\mathrm{KL}(\\mu_{i}^{*}||\\mu_{\\mathcal{R}}^{i})=0.$ . ", "page_idx": 28}, {"type": "text", "text": "Proof. Let $p^{*}$ be the probability density function associated with the ground truth SDE $\\mathrm{d}\\tilde{\\mathbf{R}}^{t}\\,=$ $\\mathbf{f}_{\\mathcal{R}}^{*}(\\mathbf{\\widetilde{R}}^{t},t)\\mathrm{d}t{+}\\sigma\\mathrm{d}\\mathbf{\\widetilde{W}}^{t},\\mathbf{\\widetilde{R}}^{0}=\\mathbf{R}^{0}$ . Let $\\{(\\mathbf{R}_{i}^{t})_{t\\in[0,T]}\\}_{i\\in[N-1]}$ denote a series of $N$ equivaraint diffusion bridges defined in Theorem 3.4. Then by theorem 3.4, $q_{\\mathcal{R}}(\\mathbf{R}_{0}^{0},\\mathbf{R}_{0}^{T},\\mathbf{R}_{1}^{T},\\cdot\\cdot\\cdot\\mathbf{\\Omega},\\mathbf{R}_{N-1}^{T})$ induced by $\\{(\\mathbf{R}^{t})_{t\\in[0,T]}\\}_{i\\in[N-1]}$ equals to $p_{\\mathcal{R}}^{*}(\\mathbf{R}_{0}^{0},\\mathbf{R}_{0}^{T},\\mathbf{R}_{1}^{T},\\cdot\\cdot\\cdot\\mathbf{\\Omega},\\mathbf{R}_{N-1}^{T})$ . Additionally, the conditional probability density function $q_{\\mathcal{R}}(\\mathbf{R}_{i}^{t}|\\mathbf{R}_{i}^{T},\\mathbf{R}_{i}^{0})$ , for $i T\\,\\,\\leq\\,t\\,<\\,(i+1)T$ , is associated with the Brownian bridge ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{R}_{i}^{t}=\\frac{\\mathbf{R}_{i}^{T}-\\mathbf{R}_{i}^{t}}{T-t^{\\prime}}\\mathrm{d}t^{\\prime}+\\sigma\\mathrm{d}\\mathbf{W}_{t},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $t^{\\prime}=t-i T$ . Then by the chain rule of KL divergence ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}(\\mu_{i}^{*}||\\mu_{\\mathcal{R}}^{i})=\\mathrm{KL}(p_{i}^{*}(\\tilde{\\mathbf{R}}^{(i+1)T},\\tilde{\\mathbf{R}}^{i T})||q_{\\mathcal{R}}^{i}(\\tilde{\\mathbf{R}}^{(i+1)T},\\tilde{\\mathbf{R}}^{i T})+}\\\\ &{\\qquad\\qquad\\qquad\\mathbb{E}_{p_{i}^{*}(\\tilde{\\mathbf{R}}^{(i+1)T},\\tilde{\\mathbf{R}}^{i T})}\\left[\\mathrm{KL}(\\mu_{i}^{*}(\\cdot|\\tilde{\\mathbf{R}}^{(i+1)T},\\tilde{\\mathbf{R}}^{i T})||\\mu_{\\mathcal{R}}^{i}(\\cdot|\\tilde{\\mathbf{R}}^{(i+1)T},\\tilde{\\mathbf{R}}^{i T}))\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Since $p_{i}^{*}(\\tilde{\\mathbf{R}}^{(i+1)T},\\tilde{\\mathbf{R}}^{i T})=q_{\\mathcal{R}}^{i}(\\tilde{\\mathbf{R}}^{(i+1)T},\\tilde{\\mathbf{R}}^{i T})$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathrm{KL}(\\mu_{i}^{*}||\\mu_{\\mathcal{R}}^{i})=\\mathbb{E}_{p_{i}^{*}(\\tilde{\\mathbf{R}}^{i+1})T,\\tilde{\\mathbf{R}}^{i T})}\\left[\\mathrm{KL}(\\mu_{i}^{*}(\\cdot|\\tilde{\\mathbf{R}}^{(i+1)T},\\tilde{\\mathbf{R}}^{i T})||\\mu_{\\mathcal{R}}^{i}(\\cdot|\\tilde{\\mathbf{R}}^{(i+1)T},\\tilde{\\mathbf{R}}^{i T}))\\right].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Since the prior SDE is time homogeneous, we can only consider the case $i\\,=\\,0$ without loss of generality. Let $\\upsilon$ be the path measure of the Brownian motion $\\sigma\\mathbf{\\widetilde{W}}^{t}$ on space $\\mathcal{R}$ . Since the condition of Theorem B.14 is satisfied, then we can use Theorem B.14 and get ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mu_{0}^{*}(\\cdot|\\tilde{\\mathbf{R}}^{0})=\\exp\\left(-\\frac{1}{\\sigma}\\int_{0}^{T}\\nabla H_{\\mathcal{R}}^{*}(\\sigma\\tilde{\\mathbf{W}}^{t})\\cdot\\mathrm{d}\\tilde{\\mathbf{W}}^{t}-\\frac{1}{2\\sigma^{2}}\\int_{0}^{T}\\|\\nabla H_{\\mathcal{R}}^{*}(\\sigma\\tilde{\\mathbf{W}}^{t})\\|^{2}\\mathrm{d}t\\right)\\mathrm{d}v(\\cdot|\\tilde{\\mathbf{R}}^{0}).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then we can use the Ito\u2019s formula (Theorem B.1) to simplify the expression ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mu_{0}^{*}(\\cdot|\\tilde{\\mathbf{R}}^{0})=\\exp\\left(\\frac{1}{\\sigma^{2}}(H_{\\mathcal{R}}^{*}(\\sigma\\tilde{\\mathbf{W}}^{0})-H_{\\mathcal{R}}^{*}(\\sigma\\tilde{\\mathbf{W}}^{T}))+\\frac{1}{2}\\int_{0}^{T}(\\nabla^{2}H_{\\mathcal{R}}^{*}(\\sigma\\tilde{\\mathbf{W}}^{t})-\\frac{1}{\\sigma^{2}}\\|\\nabla H_{\\mathcal{R}}^{*}(\\sigma\\tilde{\\mathbf{W}}^{t})\\|^{2})\\mathrm{d}t\\right)\\mathrm{d}v(\\cdot|\\tilde{\\mathbf{R}}^{0}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "To simplify our notation, we denote ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathcal{Z}_{T}=\\exp\\left(\\frac{1}{\\sigma^{2}}(H_{\\mathcal{R}}^{*}(\\sigma\\tilde{\\mathbf{W}}^{0})-H_{\\mathcal{R}}^{*}(\\sigma\\tilde{\\mathbf{W}}^{T}))+\\frac{1}{2}\\int_{0}^{T}(\\nabla^{2}H_{\\mathcal{R}}^{*}(\\sigma\\tilde{\\mathbf{W}}^{t})-\\frac{1}{\\sigma^{2}}\\|\\nabla H_{\\mathcal{R}}^{*}(\\sigma\\tilde{\\mathbf{W}}^{t})\\|^{2})\\mathrm{d}t\\right).\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Let $F,g$ be measurable functions on $C[0,T],\\mathbb{R}^{d}$ , respectively. Then by the disintegration of Wiener measure into pinned Wiener measures (path measure of the Brownian Bridge), we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\mu_{0}^{*}(\\cdot|\\tilde{\\mathbf{R}}^{0})}[F g(\\sigma\\mathbf{\\tilde{W}}^{T})]=\\mathbb{E}_{v(\\cdot|\\tilde{\\mathbf{R}}^{0})}[F g(\\sigma\\mathbf{\\tilde{W}}^{T})Z_{T}]=\\int\\mathbb{E}_{v(\\cdot|\\tilde{\\mathbf{R}}^{0},\\tilde{\\mathbf{R}}^{T}=x)}[F Z_{T}]g(x)p_{T}(x|\\tilde{\\mathbf{R}}^{0})\\mathrm{d}x,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $p_{T}(x|\\tilde{\\mathbf{R}}^{0})$ is the transition density function of $\\sigma\\mathbf{\\widetilde{W}}^{t}$ . Let $F=1$ , we get ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\int\\mathbb{E}_{v(\\cdot|\\tilde{\\mathbf{R}}^{0},\\tilde{\\mathbf{R}}^{T}=x)}[Z_{T}]g(x)p_{T}(x|\\tilde{\\mathbf{R}}^{0})\\mathrm{d}x=\\int g(x)p_{0}^{*}(x|\\tilde{\\mathbf{R}}^{0})\\mathrm{d}x.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "So we have $\\mathbb{E}_{v(\\cdot|\\tilde{\\mathbf{R}}^{0},\\tilde{\\mathbf{R}}^{T}=x)}[Z_{T}]=p_{0}^{*}(x|\\tilde{\\mathbf{R}}^{0})/p_{T}(x|\\tilde{\\mathbf{R}}^{0})$ . Let $g=1$ , then we get ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\int\\mathbb{E}_{\\mu_{0}^{*}(\\cdot|\\tilde{\\mathbf{R}}^{0},\\tilde{\\mathbf{R}}^{T}=x)}[F]p_{0}^{*}(x|\\tilde{\\mathbf{R}}^{0})\\mathrm{d}x=\\int\\mathbb{E}_{v(\\cdot|\\tilde{\\mathbf{R}}^{0},\\tilde{\\mathbf{R}}^{T}=x)}[F Z_{T}]p_{T}(x|\\tilde{\\mathbf{R}}^{0})\\mathrm{d}x.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "So we can conclude that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\mathrm{i}\\mu_{0}^{*}(\\cdot|\\tilde{\\mathbf{R}}^{0},\\tilde{\\mathbf{R}}^{T})}{\\mathrm{d}v(\\cdot|\\tilde{\\mathbf{R}}^{0},\\tilde{\\mathbf{R}}^{T})}=\\frac{p_{T}(\\tilde{\\mathbf{R}}^{T}|\\tilde{\\mathbf{R}}^{0})}{p_{0}^{*}(\\tilde{\\mathbf{R}}^{T}|\\tilde{\\mathbf{R}}^{0})}\\cdot\\frac{\\exp\\left(\\frac{1}{\\sigma^{2}}(H_{\\mathcal{R}}^{*}(\\tilde{\\mathbf{R}}^{0}))\\right)}{\\exp\\left(\\frac{1}{\\sigma^{2}}(H_{\\mathcal{R}}^{*}(\\tilde{\\mathbf{R}}^{T}))\\right)}\\exp\\left(\\frac{1}{2}\\int_{0}^{T}(\\nabla^{2}H_{\\mathcal{R}}^{*}(\\cdot)-\\frac{1}{\\sigma^{2}}\\|\\nabla H_{\\mathcal{R}}^{*}(\\cdot)\\|^{2})\\mathrm{d}t\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Note that $\\mu_{\\mathcal{R}}^{i}(\\cdot|\\tilde{\\mathbf{R}}^{0},\\tilde{\\mathbf{R}}^{T})=v(\\cdot|\\tilde{\\mathbf{R}}^{0},\\tilde{\\mathbf{R}}^{T})$ . Now we can calculate the KL divergence by ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{KL}(\\mu_{0}^{*}||\\mu_{R}^{0})=\\mathbb{E}_{p_{0}^{*}(\\tilde{\\mathbf{R}}^{T},\\tilde{\\mathbf{R}}^{0})}\\left[\\mathrm{KL}(\\mu_{0}^{*}(|\\tilde{\\mathbf{R}}^{T},\\tilde{\\mathbf{R}}^{0})|\\mu_{R}^{0}(\\cdot|\\tilde{\\mathbf{R}}^{T},\\tilde{\\mathbf{R}}^{0}))\\right]}&{\\quad{\\scriptstyle(144)}}\\\\ {\\leq\\mathbb{E}_{p_{0}^{*}(\\tilde{\\mathbf{R}}^{T},\\tilde{\\mathbf{R}}^{0})}\\left[\\log\\left(\\frac{p_{T}(\\tilde{\\mathbf{R}}^{T}|\\tilde{\\mathbf{R}}^{0})}{p_{0}^{*}(\\tilde{\\mathbf{R}}^{T}|\\tilde{\\mathbf{R}}^{0})}\\cdot\\frac{\\exp\\Big(\\frac{\\sigma_{r}^{2}(\\tilde{\\mathbf{R}}^{*}(\\tilde{\\mathbf{R}}^{0}))}{\\sigma^{2}}\\Big)}{\\exp\\Big(\\frac{1}{\\sigma^{2}}(I\\tilde{\\mathbf{R}}_{R}^{*}(\\tilde{\\mathbf{R}}^{T})\\Big)\\Big)}\\right]+\\frac{C T}{2}}&{\\quad{\\scriptstyle(145)}}\\\\ {=\\mathbb{E}_{p_{0}^{*}(\\tilde{\\mathbf{R}}^{0},\\tilde{\\mathbf{R}}^{0})}\\left[\\log\\left(\\frac{p_{T}(\\tilde{\\mathbf{R}}^{T}|\\tilde{\\mathbf{R}}^{0})}{p_{0}^{*}(\\tilde{\\mathbf{R}}^{T}|\\tilde{\\mathbf{R}}^{0})}\\right)\\right]+\\mathbb{E}[\\frac{1}{\\sigma^{2}}H_{\\mathcal{R}}^{*}(\\tilde{\\mathbf{R}}^{0})]-\\mathbb{E}[\\frac{1}{\\sigma^{2}}H_{\\mathcal{R}}^{*}(\\tilde{\\mathbf{R}}^{T})]+\\frac{C T}{2}}&{\\quad{\\scriptstyle(146)}}\\\\ {=-\\mathbb{E}_{p_{0}^{*}(\\tilde{\\mathbf{R}}^{0})}\\mathrm{KL}(p_{0}^{*}(\\tilde{\\mathbf{R}}^{T}|\\tilde{\\mathbf{R}}^{0})||p_{T}(\\tilde{\\mathbf{R}}^{T}|\\tilde{\\mathbf{R}}^{0})) \n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "When $N\\rightarrow\\infty$ , $\\begin{array}{r}{T=\\frac{1}{N}\\to0}\\end{array}$ . Since $h(t)$ is continuous by our assumption, then $\\mathrm{KL}(\\mu_{0}^{*}||\\mu_{\\mathcal{R}}^{0})\\to$ 0. \u53e3 ", "page_idx": 29}, {"type": "text", "text": "C Derivation of Practical Objective Function ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In this subsection, we show the implementation details of our framework. We set $T=1$ in all the experiments. ", "page_idx": 30}, {"type": "text", "text": "Matching objective. We design the SDE on geometric states in Proposition 3.1 to be: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{R}^{t}=\\sigma\\mathrm{d}\\mathbf{W}^{t},\\quad w i t h\\ t r a n s i t i o n\\ d e n s i t y\\quad p_{\\mathcal{R}}(z^{\\prime},t^{\\prime}|z,t)=\\mathcal{N}(z_{0},\\sigma^{2}(t^{\\prime}-t)\\mathbf{I})\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "The explicit form of the objective is ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{R}^{t}}\\log p_{\\mathcal{R}}(z_{1},1|\\mathbf{R}^{t},t)=\\nabla_{\\mathbf{R}^{t}}\\log N(z_{0},\\sigma^{2}(1-t)\\mathbf{I})=\\frac{z_{1}-\\mathbf{R}^{t}}{\\sigma^{2}(1-t)}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then the $_\\mathrm{h}$ -transformed SDE becomes ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{R}^{t}=\\frac{\\mathbf{R}^{1}-\\mathbf{R}^{t}}{1-t}\\mathrm{d}t+\\sigma\\mathrm{d}\\mathbf{W}^{t},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which is known as the Brownian bridge. The corresponding $_\\mathrm{h}$ -transformed density is ", "page_idx": 30}, {"type": "equation", "text": "$$\nq_{\\mathcal{R}}(\\mathbf{R}^{t},t|z_{1},1;z_{0},0)=\\mathcal{N}(t z_{1}+(1-t)z_{0},\\sigma^{2}t(1-t)\\mathbf{I}).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "In practice, we do not use $q_{\\mathcal{R}}(\\mathbf{R}_{-}^{0},0|z_{1},1;z_{0},0)=\\delta(\\mathbf{R}^{0}-z_{0})$ as our initial distribution. We use $q_{\\mathcal{R}}\\mathbf{\\dot{R}}^{0},0|z_{1},1;z_{0},0)=\\mathcal{N}(z_{0},\\sigma^{2}\\mathbf{I})$ instead. Since the solution of the Brownian bridge is given by ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbf{R}^{t}=(1-t)\\mathbf{R}^{0}+t\\mathbf{R}^{1}+\\sigma\\sqrt{t(1-t)}\\mathbf{Z},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\mathbf{Z}\\sim\\mathcal{N}(0,\\mathbf{I})$ , then the marginal distribution of $\\mathbf{R}^{t}$ becomes $\\mathcal{N}((1-t)z_{0}+t z_{1},(1-t)\\sigma^{2}\\mathbf{I})$ We use this distribution to sample geometric state $\\mathbf{R}^{t}$ in the training stage. ", "page_idx": 30}, {"type": "text", "text": "Trajectory guidance. Similarly, we set $\\begin{array}{r}{T=\\frac{1}{N}}\\end{array}$ , $p_{\\mathcal{R}}^{i}(z_{i+1},T|\\mathbf{R}^{t^{\\prime}},t^{\\prime})=\\mathcal{N}(\\mathbf{R}^{t^{\\prime}},\\sigma_{i}^{2}(T\\!-\\!t^{\\prime})\\mathbf{I})$ when we use the trajectory guidance. So the h-transformed SDE becomes ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathrm{d}\\mathbf{R}_{i}^{t}=\\frac{\\mathbf{R}_{i}^{T}-\\mathbf{R}_{i}^{t}}{T-t}\\mathrm{d}t+\\sigma_{i}\\mathrm{d}\\mathbf{W}^{t},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which is a Brownian bridge with $\\begin{array}{r}{T=\\frac{1}{N}}\\end{array}$ . Then associated density function is ", "page_idx": 30}, {"type": "equation", "text": "$$\nq_{\\mathcal{R}}^{i}(\\mathbf{R}^{t^{\\prime}},t^{\\prime}|z_{i+1},T;z_{i},0)=\\mathcal{N}(\\frac{t^{\\prime}}{T}z_{i+1}+\\frac{T-t^{\\prime}}{T}z_{i},\\sigma_{i}^{2}\\frac{t^{\\prime}(T-t^{\\prime})}{T^{2}}\\mathbf{I}).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Additionally, we set $\\sigma_{i}$ decays linearly with respect to $\\frac{i}{N}$ , i.e. $\\begin{array}{r}{\\sigma_{i}=\\frac{N-i}{N}\\sigma}\\end{array}$ , where $\\sigma$ is a hyperparameter. Again, in training stage, we set $q_{\\mathcal{R}}^{i}(\\mathbf{R}_{i}^{0},0|z_{1},1;z_{0},0)=\\mathcal{N}(z_{0},\\sigma_{i}^{2}\\mathbf{I})$ as initial distribution, and the terminal distribution is $q_{\\mathcal{R}}^{i}(\\mathbf{R}_{i}^{0},0|z_{1},1;z_{0},0)=\\mathcal{N}(z_{1},\\sigma_{i+1}^{2}\\mathbf{I})$ , which is same as the initial distribution of the next bridge. ", "page_idx": 30}, {"type": "text", "text": "Sampling Algorithm We use the ODE-based method to generate samples at inference time. After the training process, the neural network $\\mathbf{v}_{\\theta}$ is trained as described in Algorithm 3 and Algorithm 4. When the network is trained without trajectory guidance, we simulate the following ODE to generate samples: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\,\\mathbf{R}^{t}}{\\mathrm{d}\\,t}=\\mathbf{v}_{\\theta}(\\mathbf{R}^{t},t;\\mathbf{R}^{0}),\\mathbf{R}^{0}\\sim q_{\\mathrm{data}}(R^{t_{0}}),t\\in[0,T]\\,.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "When the network is trained with trajectory guidance, we solve the following ODE to generate samples: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\,\\mathbf{R}^{t}}{\\mathrm{d}\\,t}=\\mathbf{v}_{\\theta}(\\mathbf{R}^{t},t;\\mathbf{R}^{\\lfloor\\frac{t}{T}\\rfloor T}),\\mathbf{R}^{0}\\sim q_{\\mathrm{data}}(R^{t_{0}}),t\\in[0,N\\times T]\\,.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Denote a black box ODE solver by $\\operatorname{Solver}(\\mathbf{v},t)$ . $\\operatorname{Solver}(\\mathbf{v},t)$ takes a vector field $\\mathbf{v}$ and a time point as inputs, then returns the solution of the ODE ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}\\,\\mathbf{X}^{t}}{\\mathrm{d}\\,t}=\\mathbf{v}(\\mathbf{X}^{t},t;\\phi),\\mathbf{X}^{0}=x_{0},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "at the specific time $t$ , i.e. $\\operatorname{Solver}(\\mathbf{v},t)=\\mathbf{X}^{t}$ . Combining all the above design choices, we have the following algorithms for sampling our Geometric Diffusion Bridge (Algorithm 5) and leveraging trajectory guidance if available (Algorithm 6). ", "page_idx": 30}, {"type": "image", "img_path": "zcEPOB9rCR/tmp/185f8294fd88db877a941627e2b13b6a60d2554b97ba0957143eaa161527f1fc.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Algorithm 5 Sampling ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Require: Initial geometric state $z_{0}\\ \\sim\\ q_{\\mathrm{data}}(R^{t_{0}})$ , a trained neural network $\\mathbf{v}_{\\theta}$ , a numerical ODE solver Solver $\\mathbf{\\rho}(\\mathbf{v},t)$   \n1: R0 = z0   \n2: $\\mathbf{R}^{T}=\\mathrm{Solver}(\\mathbf{v}_{\\theta}(\\mathbf{R}^{t},t;\\mathbf{R}^{0}),T)$   \nEnsure: ${\\mathbf{R}}^{T}$ ", "page_idx": 31}, {"type": "text", "text": "Algorithm 4 Training with trajectory guidance ", "page_idx": 31}, {"type": "image", "img_path": "zcEPOB9rCR/tmp/66084adbbf0c437a579fe34952a0a3da0e54d9a2c69e800dd0ed9d4dd2c93fb1.jpg", "img_caption": ["Algorithm 6 Sampling with trajectory guidance Require: Initial geometric state $z_{0}\\ \\sim\\ q_{\\mathrm{data}}(R^{t_{0}})$ , a trained neural network $\\mathbf{v}_{\\theta}$ , a numerical ODE solver Solver(v, t) 1: R0 = z0 2: RNT = Solver(v\u03b8(Rt, t; R\u230aTt \u230bT ), t = NT) Ensure: RNT "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "D Experiments ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "D.1 Equilibrium State Prediction ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Dataset. QM9 [79] is a quantum chemistry benchmark consisting of 134k stable small organic molecules, which has been widely used for molecular modeling. These molecules correspond to the subset of all 133,885 species out of the GDB-17 chemical universe of 166 billion organic molecules. In convention, 110k, 10k, and 11k molecules are used for train/valid/test sets respectively. The geometric conformations that are minimal in energy are provided in the QM9 dataset. The equilibrium conformation and its relative properties are all calculated at the B3LYP/6-31G(2df,p) level of quantum chemistry. ", "page_idx": 31}, {"type": "text", "text": "Molecule3D [116] is a large-scale dataset curated from the PubChemQC project [67, 71], consisting of 3,899,647 molecules in total, 2,339,788 molecules in training set, 779,929 molecules in the validation set, 779,930 molecules in the test set, and its train/valid/test splitting ratio is $6:2:2$ . For each molecule, the 2D atom graph, the 3D equilibrium geometric conformation, and four extra properties are provided. In particular, both random and scaffold splitting methods are adopted to thoroughly evaluate the in-distribution and out-of-distribution performance. For each molecule, an initial geometric state is generated by using fast and coarse force field [73, 52] and geometry optimization is conducted to obtain B3LYP/6-31G\\* level DFT-calculated equilibrium geometric structure. ", "page_idx": 31}, {"type": "text", "text": "Baselines. We comprehensively compare our GDB framework with previous equilibrium conformation prediction methods. Following [111], we use DG and ETKDG algorithms implemented by RDkit as our fundamental baselines. The benchmark [116] used the DeeperGCN-DAGNN framework [60] which proposed a deep graph neural network architecture to predict 3D geometric conformation of the molecule based on its 2D graph structure, and got impressive performance on the Molecule3D dataset. GINE [39] proposed a method for pretraining GNN to improve the performance and capacity of GNN. GATv2 [10] proposed a dynamic graph attention mechanism and improved the performance of the graph attention network on several tasks. GPS [80] proposed a general framework that supported multiple types of encodings with efficiency and scalability guarantees in both small and large graph prediction tasks. GTMGC [111] proposed a novel neural network based on Graph-Transformer (GT) [118, 66, 119, 65] to predict the equilibrium conformation of the molecule in 3D based on its 2D graph structure. ", "page_idx": 31}, {"type": "text", "text": "Metric. Following [116], three metrics are adopted to evaluate predictions of equilibrium states: (1) C-RMSD: given prediction $\\hat{R}=\\{\\hat{\\mathbf{r}}_{i}\\}_{i=1}^{N}$ which is rigidly aligned to the ground-truth $R^{*}=\\{\\mathbf{r}_{i}^{*}\\}_{i=1}^{N}$ by the Kabsch algorithm [44], Root Mean Square Deviation between their atoms is calculated, i.e., C- $\\begin{array}{r}{\\operatorname{RMSD}(\\hat{R},R^{*})\\,=\\,\\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}\\|\\hat{\\mathbf{r}}_{i}-\\mathbf{r}_{i}^{*}\\|_{2}^{2}}}\\end{array}$ ; (2) D-RMSE: based on $\\hat{R}$ and $R^{*}\\,=\\,\\{\\mathbf{r}_{i}^{*}\\}_{i=1}^{N}$ , interatomic distances can be calculated, i.e., $\\{\\hat{d}_{i}\\}_{i=1}^{N^{\\prime}}$ and $\\{\\hat{d}_{i}^{*}\\}_{i=1}^{N^{\\prime}}$ . Root Mean Square Error be", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{tween~these~distances~is~calculated,i.e.,D.RMSE}(\\{\\hat{d}_{i}\\}_{i=1}^{N^{\\prime}},\\{\\hat{d}_{i}^{*}\\}_{i=1}^{N^{\\prime}})=\\sqrt{\\frac{1}{N^{\\prime}}\\sum_{i=1}^{N}(d_{i}-d_{i}^{*})^{2}};}\\\\ &{\\mathrm{D.MAE}(\\{\\hat{d}_{i}\\}_{i=1}^{N^{\\prime}},\\{\\hat{d}_{i}^{*}\\}_{i=1}^{N^{\\prime}})=\\frac{1}{N^{\\prime}}\\sum_{i=1}^{N}|d_{i}-d_{i}^{*}|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Settings. In this task, we parameterize ${\\bf v}_{\\theta}({\\bf R}^{t},t;{\\bf R}^{0})$ by extending a Graph-Transformer based equivariant network [92, 63] to encode both time steps and initial geometric states as conditions. For training, we use AdamW as the optimizer, and set the hyper-parameter $\\epsilon$ to 1e-8 and $(\\beta_{1},\\beta_{2})$ to (0.9,0.999). The gradient clip norm is set to 5.0. The peak learning rate is set to 1e-4. The batch size is set to 512. The weight decay is set to 0.0. The model is trained for $500\\mathrm{k}$ steps with a $30\\mathbf{k}.$ -step warm-up stage. After the warm-up stage, the learning rate decays linearly to zero. The noise scale $\\sigma$ is set to 0.5. For inference, we use 10 time steps with the Euler solver [12]. All models are trained on 16 NVIDIA V100 GPU. ", "page_idx": 32}, {"type": "text", "text": "D.2 Structure Relaxation ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Dataset. Open Catalyst 2022 (OC22) dataset [105] is a widely used dasaset, which has great significance for the development of Oxygen Evolution Reaction (OER) catalysts. Each data in the dataset is in the form of the adsorbate-catalyst complex. Both initial and adsorption states with trajectories connecting them are provided. The dataset consists of 62,331 Density Functional Theory (DFT) relaxations trajectories, and about 9,854,504 single-point DFT calculations across a range of oxide materials, coverages, and adsorbates.The training set consists of 45,890 catalyst-adsorbate complexes. To better evaluate the model\u2019s performance, the validation and test sets consider the in-distribution (ID) and out-of-distribution (OOD) settings which use unseen catalysts, containing approximately 2,624 and 2,780 complexes respectively. ", "page_idx": 32}, {"type": "text", "text": "Baselines. Following [105], we choose strong MLFF baselines trained on force field data for a challenging comparison. Spinconv [94] introduced a novel approach called spin convolution to model angular information between sets of neighboring atoms in a graph neural network and got impressive performance in molecular simulation tasks. Gemnet [32] proposed multiple structural improvements for geometric GNN with theoretical insights, which significantly improved the experimental performance as well. Based on Gemnet\u2019s framework, Gemnet-OC [34] modified the architecture of the network and improved the experimental performance on more diverse tasks. ", "page_idx": 32}, {"type": "text", "text": "In [105], there are still other baseline setting. [105] introduce a large-scale dataset Open Catalyst 2020 (OC20), which consists of 1,281,040 Density Functional Theory (DFT) relaxations and 264,890,000 single point evaluations to help training the baseline model. [105] presented baselines using both OC20 and OC22 data in training stage and baselines using only OC20/OC22 for comparison. ", "page_idx": 32}, {"type": "text", "text": "Metric. Following [105], we use the Average Distance within Threshold (ADwT) as the evaluation metric, which reflects the percentage of structures with an atom position MAE below thresholds. To be more precise, the ADWT metric across thresholds ranging from $\\beta=0.01\\mathring{A}$ to $\\beta=0.5\\mathring{A}$ in increments of $0.001\\mathring{A}$ . The computation of ADwT metric is to count the percentage of structures with an atom position MAE below the threshold. ", "page_idx": 32}, {"type": "text", "text": "Settings. In this task, We parameterize ${\\bf v}_{\\theta}({\\bf R}^{t},t;{\\bf R}^{0})$ by using GemNet-OC [34], which also serves as a verification that our framework is compatible with different backbone models. For training, we use AdamW as the optimizer, and set the hyper-parameter $\\epsilon$ to 1e-8 and $(\\beta_{1},\\beta_{2})$ to (0.9,0.999). The gradient clip norm is set to 10.0. The peak learning rate is set to 5e-4. The batch size is set to 64. The weight decay is set to 0.0. The model is trained for $200\\mathbf{k}$ steps. After the warm-up stage, the learning rate decays linearly to zero. The noise scale $\\sigma$ is set to 0.5. The trajectory length is set to $N=10$ . For inference, we also use 10 time steps with the Euler solver [12]. All models are trained on 8 NVIDIA A100 GPU. ", "page_idx": 32}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 33}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 33}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 33}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 33}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 33}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 33}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 33}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: Section 3, 4. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 33}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We have discussed several future directions in Section 3 and 6 ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 34}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: All assumptions and complete proofs are provided in the appendix ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 34}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Section 4 and Appendix D. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 34}, {"type": "text", "text": "\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 35}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 35}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Justification: The code and model checkpoints will be publicly available after the submission is acceptance. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 35}, {"type": "text", "text": "\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 36}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Section 4 and Appendix D. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 36}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [No] ", "page_idx": 36}, {"type": "text", "text": "Justification: There exists little randomness in all the experiments of this submission, which means that results of using different random seeds are almost the same. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 36}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Section 4 and Appendix D ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 36}, {"type": "text", "text": "\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 37}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The research in this work conforms with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 37}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 37}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 37}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 38}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 38}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 38}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 39}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 39}]