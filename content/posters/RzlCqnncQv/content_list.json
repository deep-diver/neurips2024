[{"type": "text", "text": "Leveraging Environment Interaction for Automated PDDL Translation and Planning with Large Language Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sadegh Mahdavi\u2217 University of British Columbia smahdavi@ece.ubc.ca ", "page_idx": 0}, {"type": "text", "text": "Raquel Aoki Borealis AI raquel.aoki@borealisai.com ", "page_idx": 0}, {"type": "text", "text": "Keyi Tang Borealis AI keyi.tang@borealisai.com ", "page_idx": 0}, {"type": "text", "text": "Yanshuai Cao Borealis AI yanshuai.cao@borealisai.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large Language Models (LLMs) have shown remarkable performance in various natural language tasks, but they often struggle with planning problems that require structured reasoning. To address this limitation, the conversion of planning problems into the Planning Domain Definition Language (PDDL) has been proposed as a potential solution, enabling the use of automated planners. However, generating accurate PDDL files typically demands human inputs or correction, which can be time-consuming and costly. In this paper, we propose a novel approach that leverages LLMs and environment feedback to automatically generate PDDL domain and problem description flies without the need for human intervention. Our method introduces an iterative refinement process that generates multiple problem PDDL candidates and progressively refines the domain PDDL based on feedback obtained from interacting with the environment. To guide the refinement process, we develop an Exploration Walk (EW) metric, which provides rich feedback signals for LLMs to update the PDDL file. We evaluate our approach on 10 PDDL environments. We achieve an average task solve rate of $66\\%$ compared to a $29\\%$ solve rate by GPT-4\u2019s intrinsic planning with chain-of-thought prompting. Our work enables the automated modeling of planning environments using LLMs and environment feedback, eliminating the need for human intervention in the PDDL translation process and paving the way for more reliable LLM agents in challenging problems. Our code is available at https://github.com/BorealisAI/llm-pddl-planning ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) have demonstrated remarkable success across various domains, including mathematics, coding, and even the bar exam [1]. These models excel at understanding and generating natural language, offering flexibility and adaptability to a wide range of tasks. However, when it comes to planning and long-horizon reasoning, LLMs have shown limited performance [8, 28], despite some promising results [3]. ", "page_idx": 0}, {"type": "text", "text": "Planning is a crucial aspect of intelligence that involves reasoning to find a sequence of actions to achieve a desired goal state from an initial state. The Planning Domain Definition Language (PDDL) [18] is a widely used formalism for describing planning problems. PDDL provides a structured way to define the domain, which includes the types of objects, predicates, and actions, as well as the problem instance, which specifies the initial state and goal conditions. PDDL enables the application of search-based algorithms, such as breadth-first search (BFS) or $\\mathbf{A}^{*}$ search, which can guarantee to find a valid solution if one exists. However, the downside of PDDL is that it requires a well-defined and structured domain and problem definition, which can be challenging to create, especially for complex scenarios. Figure 1 showcases snippets of some PDDL problems and domain files along with an action plan produced by a classical planner. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Recent studies explored combining the strengths of LLMs and PDDL-based planning [15, 7, 9]. The idea is to leverage LLM for translation from natural language (NL) problem descriptions into PDDL formal descriptions, and then use a classical planner to solve the translated PDDL problem [9]. This hybrid approach could theoretically take advantage of the flexibility of NL input and the correctness guarantees provided by the classical planner. If the translation from NL to PDDL is accurate, the resulting plan is guaranteed to be valid. ", "page_idx": 1}, {"type": "text", "text": "Unfortunately, existing approaches have not been able to generate both PDDL problem and domain descriptions with reasonable success rates without humans in the loop, as we shall elaborate in Sec. 2. While translating PDDL problems is feasible given the domain PDDL description [15], generating domain PDDL from NL correctly is a more nuanced and challenging problem. To do so requires identifying causally relevant objects to design predicates, as well as their inter-relationships, in a way that accurately reflects the possible states and transitions of the environment. A small error, for example in predicate design, could lead to entirely incorrect domain description and failed planning (see Appendix A.2 for a real example). Guan et al. [9] take a step toward this goal relying on human-in-the-loop to detect and correct mistakes made by LLMs. ", "page_idx": 1}, {"type": "text", "text": "In this work, we develop a fully automated method for generating PDDL domain and problem definitions using LLMs and environment feedback without relying on human intervention. Intuitively, our method lets an LLM build hypothetical \u201cmental models\u201d of the environment, in the form of proposed PDDL domain descriptions. The LLM then verifies and updates the \u201cmental model\u201d by observing discrepancies between the feasibility of actions under its \u201cmental model\u201d and the real environment. This method enables LLMs to use classical planners to solve complex planning problems whose solutions may require hundreds or thousands of steps that all need to be correct. ", "page_idx": 1}, {"type": "text", "text": "We first highlight the challenges of this task and then propose our solution. In particular, our contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We demonstrate that even small modifications to PDDL domains can render plan search infeasible, limiting the feedback information for LLMs to perform in context update. \u2022 To address this, we introduce a new Exploration Walk (EW) metric, which is a smooth similarity measure between two domains by comparing the executability of random action sequences sampled from one domain on the other. Crucially, EW only requires access to the action interface and executability of the environments, not directly the ground-truth PDDL. \u2022 We propose an EW-guided tree search approach that leverages LLMs to generate and refine the PDDL domain and problem files iteratively and automatically. \u2022 We evaluate our method on 10 challenging PDDL domains, where a number of them are from the International Planning Competition, and show that it outperforms a baseline that generates PDDL flies in a single attempt without refinement. Our method solves 7 out of 10 environments, achieving an average task solve rate of $66\\%$ and average EW score of 0.84, compared to $34\\%$ task solve rate and $0.53\\;\\mathrm{EW}$ score for the baseline, and $29\\%$ solve rate by GPT-4 (gpt-4-1106-preview)\u2019s intrinsic planning with chain-of-thought prompting. ", "page_idx": 1}, {"type": "text", "text": "To the best of our knowledge, this is the first work that enables modeling a planning environment via PDDL translation using LLMs and environment interaction, without the need for human intervention. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "LLMs and Classical Planning. There has been recent interest in integrating LLMs with PDDL [15, 28, 9, 7, 30, 23, 10, 20, 26], and more generally neural networks with PDDL [24, 2]. Silver et al. [25] leverage LLMs to take domain PDDLs and problem PDDL specifications, and synthesize a Python function to generate domain-specific plans, as a replacement for search-based planning. Liu et al. [15] show that using LLMs to translate problem specification to PDDL, and using classical solvers results into a higher planning accuracy that using LLM directly as a planner. Dagan et al. [7] consider a similar setting, but assume that the list of objects is partially observable, and the LLM needs to interact with the world to observe the list of objects. All of the mentioned works, however, assume that a domain PDDL files is already provided. Oswald et al. [20] generate domain PDDL from natural language and propose heuristics for comparing PDDL action domains. However, their approach assumes that predicates are provided, whereas our work makes no such assumption. Additionally, Oswald et al. [20] rely on ground-truth problem instances for domain compatibility evaluation, whereas we directly translate problem PDDL without any such assumptions. Guan et al. [9] translate both Domain and Problem from natural language description but rely on human experts to correct mistakes in the domain translation before generating problem PDDLs. In this work, our goal is to lift the human-intervention assumption, and instead, use domain interaction for evaluation and verification. See Table 1 for a summary of related work comparison. ", "page_idx": 1}, {"type": "table", "img_path": "RzlCqnncQv/tmp/848d7fd07f12a8ed4afc2d39bb3df60365ff5f74eb0f6b7ca9e33680936037f9.jpg", "table_caption": ["Table 1: Summary of comparison to most closely related prior studies.\u22c6Require at least one problem instance to be translated by a human into the target domain as an in-context example. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Direct Reasoning with LLMs. Recent research has explored eliciting direct reasoning capabilities within Large Language Models (LLMs). This reasoning can be either entirely direct [31, 29] or partially direct with the assistance of basic external tools [16]. However, the primary limitation of these approaches lies in the inherent tendency of auto-regressive LLMs to produce errors in longhorizon reasoning tasks [28]. Even a minor mistake in a single reasoning step can lead to cascading errors, ultimately resulting in an incorrect final answer [8]. When applied to classical planning, this approach delegates the entire plan generation process to an LLM instead of leveraging a dedicated classical planner. Studies have demonstrated that this strategy is suboptimal compared to generating PDDL code directly [9, 15], highlighting the importance of incorporating classical planning tools for faithful plan generation in classical planning tasks. ", "page_idx": 2}, {"type": "text", "text": "External Reasoning and Code Generation. This last line of work focuses on generating executable code from natural language instructions such as SQL or Python code generation [4, 19, 17, 5, 16, 32]. Here, the LLM often acts as a code translator, and the reasoning logic lies within the generated code. Chen et al. [4] show that LLMs are capable of Python code generation from docstrings to high accuracy. The authors also find that taking multiple code samples from an LLM and picking the best samples results in an accuracy boost. Later works show that iterative refinement of LLM responses improves the accuracy on the downstream task [17, 5], especially given external feedback such as unit tests or human feedback. Our work is related to code generation as we produce structured PDDL flies. However, our setting presents three challenges: (1) there are two types of PDDL flies, in contrast to a single Python script, and the two files need to be consistent with each other; (2) more importantly, getting external feedback and the evaluation of a generated PDDL code is not as easy as python unit tests, and as we show in Section 4.3, (domain generation) errors are abundant and hard to trace; (3) LLMs are trained with a lot more Python code compared to PDDL, as the later is much scarcer. ", "page_idx": 2}, {"type": "text", "text": "3 Notation and Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Notation. We denote $\\mathbb{I}[\\cdot]$ as the indicator function. The notation $1:N$ refers to the sequence of integers ranging from 1 to $N$ . For a set $\\mathcal{A}$ , we define $A^{*}$ as the set comprising all possible sequences of elements drawn from $\\boldsymbol{\\mathcal{A}}$ , and define $2^{A}$ as the power set of $\\boldsymbol{\\mathcal{A}}$ . ", "page_idx": 2}, {"type": "text", "text": "PDDL. Planning Domain Definition Language (PDDL) is a formal language used to describe and specify planning problems for automated planning. Here, we have two types of PDDL files: (1) Domain PDDL, which defines possible predicates (i.e., states), and actions in the environment. Executing each action requires some precondition (i.e., a set of predicates to have a specific value), and the execution leads to some effect (i.e., a change in the values of some predicates). (2) Problem PDDL, which contains a set of initial predicates and a set of goal predicates. ", "page_idx": 2}, {"type": "text", "text": "The problem PDDL instantiates the domain definition PDDL to form a concrete environment. Together, the planning problem is fully defined and formalized. A classical planner takes in both flies and searches for a plan based on the provided specification. A plan is a sequence of actions, starting from the initial state, leading to a state satisfying the goal conditions, with each action respecting the rules of the environment. Formally, let $\\mathcal{D},\\mathcal{P},A$ be the set of all possible domains, problems, and actions, respectively. Then, given a domain $d\\,\\in\\,{\\mathcal{D}}$ and problem $p\\,\\in\\,\\mathcal P$ , a classical planner $C:{\\mathcal{D}}\\times{\\mathcal{P}}\\rightarrow{\\dot{A}}^{*}\\cup\\{\\bot\\}$ takes in domain $d$ and plan $p$ , and produces a plan $q:=C(d,p)$ which is either set of actions from $\\mathcal{A}^{*}$ , or a planning error $\\perp$ . A planning error may be due to an infeasible plan search (i.e., plan not found), syntax errors, or incompatible domain and problem. A plan validator verifies whether a plan $q$ is executable and achieves the desired problem goal given a domain PDDL $d$ and problem PDDL $p$ , i.e., whether $q$ solves the planning problem instance. The validator function, denoted as $V_{d,p}(q):{\\mathcal{A}}^{*}\\to\\{0,1\\}$ , is 1 if the plan is valid, and 0 otherwise. For convenience, we assume $V_{d,p}(\\bot)\\,=\\,0$ . Similarly, we define plan execution checker $E_{d,p}:A^{*}\\,\\rightarrow\\,\\{0,1\\}$ , which only checks whether an action sequence is executable in a domain or not. Note that the difference between $V$ and $E$ is that the former checks for both plan executability and goal satisfaction, while the latter only checks for plan executability. We also define $\\boldsymbol{S}$ as the set of all possible states. Function $A_{d,p}:\\dot{S}\\stackrel{\\cdot}{\\rightarrow}2^{\\mathcal{A}}$ delineates the set of legal actions given the current states (i.e., actions that would not immediately result in $E_{d,p}$ returning 0). The function $S_{d,p}:{\\mathcal{A}}\\times{\\mathcal{S}}\\rightarrow{\\mathcal{S}}$ denotes the state transition function ( i.e., $S_{d,p}(a,s)$ determines the subsequent state given the current state $s$ and action $a$ ). Finally, we denote the initial state induced by $d$ and $p$ to be $s_{d,p,0}\\in\\mathcal{S}$ . See Table 3 in the Appendix for a summary of notations. ", "page_idx": 3}, {"type": "text", "text": "To illustrate the definitions with an example, consider the Grippers [13] environment with several rooms containing robots and boxes. Robots can move balls between rooms using their left and right grippers. Given an initial setting of robots and balls in different rooms, the main goal is to move specific balls to specific rooms using the robots. ", "page_idx": 3}, {"type": "text", "text": "Figure 1 shows an annotated example domain, problem, and plan for this environment. The domain determines predicates and actions. Predicates such as at-robby keep track of object states (e.g., whether a particular robot is in a particular room) and defining suitable predicates is a crucial part of domain design. The move action for moving a robot from one room to another has three parameters: robot r, departure room from, and destination room to. Each action has preconditions and effects, which comprise the main logic of the domain for determining the actionability of an action. In the case of the move action, the precondition is that the robot must be in the from room, and the effect is that it will no longer be in that room ", "page_idx": 3}, {"type": "image", "img_path": "RzlCqnncQv/tmp/2c51e4e1eff896a71e3d225ebec1a35d8a662a3ae752df17893e98e4cd3add50.jpg", "img_caption": ["Figure 1: Snippets of PDDL domain, problem, and plan. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "and will be in the to room. A problem PDDL $p$ specifies the initial state of robots, boxes, rooms, and the final goal. For instance, (at-robby robot2 room3) means that robot2 is initially at room3. The predicate (at ball1 room2) specifies the goal condition that ball1 must eventually be moved to room2. A plan constitutes a sequence of actions to reach the goal. For instance, one action could be (move robot2 room3 room1), moving robot2 from room3 to room1. If robot2 is not already in room3, this action is considered illegal, and the environment will produce an error. For a complete example of domain $d$ , problem $p$ , and plan $q$ , see Listings 1, 2, and 7, respectively in the Appendix. ", "page_idx": 3}, {"type": "text", "text": "Large Language Models (LLMs). We assume access to a powerful language model LLM. $\\mathrm{LLM}_{n}(X)$ denotes sampling $n$ responses from the LLM given prompt $X$ . Following the prior works, we set a temperature of $\\tau=0$ for sampling with $n=1$ (i.e., greedy sampling), and a temperature of $\\tau=0.7$ for $n>1$ [5]. Whenever possible, we use zero-shot or one-shot chain-of-thought prompts [14, 29] for the LLM to reason before generating a response. ", "page_idx": 3}, {"type": "text", "text": "4 Method ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Given an environment $e$ , its domain NL description and a task NL description, the environment\u2019s object list and action interface, our goal is to model the environment by generating a domain PDDL $\\hat{d}\\in\\mathcal{D}$ and a problem PDDL $\\hat{p}\\in\\mathcal{P}$ , such that applying a classical planner $C$ on the PDDL files produces a valid plan for the environment, i.e., $C(\\hat{d},\\hat{p})$ is a valid plan for $e$ , i.e., $V_{d,p}(C(\\hat{d},\\hat{p}))=1$ . ", "page_idx": 4}, {"type": "text", "text": "4.1 Setup", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For evaluation, we assume there exists a ground truth domain PDDL $d\\in\\mathcal D$ , and a corresponding problem instance $p\\in\\mathcal{P}$ . However, the ground truth is not directly compared to generated $\\hat{d},\\hat{p}$ , but to validate the plan $\\hat{q}:=C(\\hat{d},\\hat{p})$ by executing the validator of the ground-truth environment, $V_{d,p}(\\hat{q})$ . ", "page_idx": 4}, {"type": "text", "text": "Formally, for each environment $e$ with domain PDDL $d\\in\\mathcal D$ , and $N$ tasks with their corresponding ground-truth problem PDDLs $p_{1:N}\\,:=\\,(p_{1},p_{2},\\dotsc\\,,p_{N}),p_{1:N}\\,\\in\\,\\mathcal{P}^{N}$ , our goal is to generate a domain PDDL $\\hat{d}.$ , and a sequence of task PDDLs $\\hat{p}_{1:N}:=(\\hat{p}_{1},\\hat{p}_{2},...,\\hat{p}_{N})$ such that the average solve rate $\\overline{V}$ is maximized: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\underset{\\hat{d}\\in\\mathcal{D},\\hat{p}_{1:N}\\in\\mathcal{P}^{N}}{\\mathrm{argmax}}\\,\\overline{{V}}(\\hat{d},\\hat{p}_{1:N};e):=\\frac{1}{N}\\sum_{i=1}^{N}V_{d,p_{i}}\\left(C(\\hat{d},\\hat{p}_{i})\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Generating accurate $\\hat{d}$ and $\\hat{p}_{1:N}$ in one attempt is often impractical [9], and some form of feedback is required to refine the response. Guan et al. [9] leverage human expert feedback on $\\hat{d}$ to correct the generated domain. However, human feedback may not always be reliable and is not scalable. Before introducing our method that relies on environment feedback instead, we first state our assumptions: ", "page_idx": 4}, {"type": "text", "text": "Assumption 1 (Environment access) We assume the list of objects and action interfaces are known. Furthermore, we assume that executability and verifiability of actions can be observed (through the functions $E_{d,p}$ and $V_{d,p.}$ ). ", "page_idx": 4}, {"type": "text", "text": "Assumption 2 (Natural language description) We assume the natural language descriptions of the domain and task are both given. ", "page_idx": 4}, {"type": "text", "text": "The action interfaces are equivalent to APIs available to LLM agents. So it is reasonable to assume that the exact API call signatures are known. On the other hand, one may wonder why the object list, which appears in problem PDDLs as illustrated in Figure 1 needs to be assumed to be given, when the NL problem description should describe the objects involved in the planning tasks. This is because the NL description may not refer to the object instances using exactly the same label as the environment induced by $d$ and $p$ . If $p$ refers to a robot as robot1 but the user specifying the natural language problem description calls it Jarvis, then the environment only recognizes robot1 and not Jarvis, so the LLM would have no way to correct this mistake due to trivial name mismatch. See Appendix A.1 for a detailed example of our assumptions on the Grippers environment. ", "page_idx": 4}, {"type": "text", "text": "Note that our assumptions do not require the underlying environment to be a PDDL environment, but it can be any environment as long as PDDL is expressive enough to capture the working mechanisms of the environment. For digital agents in virtual environments, the list of objects and action interfaces are just different data objects and APIs available. The assumptions could even hold true for physical agents in the real world, provided recognition and control are sufficiently accurate. In this work, we focus on PDDL environments only, although our framework is more general. ", "page_idx": 4}, {"type": "text", "text": "4.2 Difficulty of domain PDDL generation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Generating the correct domain PDDL is challenging, as small mistakes could make the plan search fail. To demonstrate this brittleness, we simulate random omission of $k$ terms, where $0\\leq k\\leq10$ , from the action precondition and effects of the original domain $d$ . For instance, in the case of the Grippers (Figure 1), we may create a new synthetic domain by removing the (at robby $?r?\\tot{0})$ term from the effects of the move action. Namely, we define $\\hat{d}_{k}\\sim\\mathbb{P}_{k}(d)$ , where $\\mathbb{P}_{k}(d)$ represents the uniform random removal of $k$ terms. Then, for each generated $\\hat{d}_{k}$ , coupled with the ground truth task PDDLs, we compute whether the classical planner is able to find a plan without error and compute the Plan-Not-Found rate under $k$ omissions, $\\mathrm{PNF}_{k}$ , of the environment. ", "page_idx": 4}, {"type": "image", "img_path": "RzlCqnncQv/tmp/f04098fa073fcac64fb059239ccb3db32e432a28a943648fbb63ed2936487d11.jpg", "img_caption": ["Figure 2: (a) Effect of the number of removed terms on plan search failure. Each gray line shows the $\\mathrm{PNF}_{k}$ (Plan-Not-Found) metric for one environment. The red line is the average of all 15 environments. (b) Correlation between average exploration walk (EW) score and average domain difference. The $x$ -axis shows how many terms each pair of domains differs in. The $y$ -axis shows the average EW score over various pairs. All the domains show the average monotonicity of the EW score with respect to term difference. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "We empirically measure the value of $\\mathrm{PNF}_{k}$ using Monte-Carlo estimation on 15 environments. As shown in Figure 2a, $\\mathrm{PNF_{1}}$ has an average of 0.14 among different environments. This means that on average $\\bar{1}4\\%$ of the terms in domain PDDLs are so critical that removing them results in a plan-not-found error. This situation is exacerbated for larger $k$ : at $k=3$ , the average $\\mathrm{PNF}_{k}$ reaches around 0.3. In practice, the problem PDDL $\\hat{p}_{i}$ also needs to be generated, and the generated domain $\\hat{d}$ may have extra terms, both of which may further increase the planning-not-found rate. ", "page_idx": 5}, {"type": "text", "text": "4.3 Domain alignment measure via Exploration Walk metrics ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Whenever the plan search fails, absolutely no information is available to the LLM about which part of the problem or domain has issues. This is because the underlying search algorithm (such as BFS and $\\mathbf{A}^{*}$ ) fails and as a result, it does not produce any output. For example, with BFS, it enumerates all paths (possibly several thousand paths or more), and finds none satisfy the goal conditions, leaving the plan search without any useful insights. As an alternative, we introduce the Exploration Walk (EW): a smooth feedback signal that provides incremental feedback for LLM in-context learning. EW both provides a mechanism to gather richer feedback information that feeds into LLM context for PDDL refinement, as well as computing a smooth scoring metric that to compare multiple PDDLs and guide the refinement process forward. ", "page_idx": 5}, {"type": "text", "text": "Intuitively, the idea is to take legal random action sequences and verify their executability under LLM\u2019s \"mental model\" environment induced by an LLM-generated PDDL domain. This is analogous to the retrodiction step in scientific methodology, where existing observations and experimental data need to be explained by the existing model. ", "page_idx": 5}, {"type": "text", "text": "And in the other direction, EW takes executable random action sequences from an LLM-generated PDDL domain and verifies whether they are correct in the real environment. This is analogous to hypothesis testing in scientific methodology, where new predictions are verified experimentally. ", "page_idx": 5}, {"type": "text", "text": "We now describe the EW and EW metrics formally. We define an Exploration Walk of length $T$ to be any action sequence sampled from a strictly positive distribution $\\mathbb{P}_{d,p,T}$ over executable $T_{\\cdot}$ -step action sequences in $A^{*}$ corresponding to domain $d$ and task $p$ . We assume the probability of non-executable action sequences to be zero under $\\mathbb{P}_{d,p,T}$ . In other words, $\\forall q_{1:T},\\mathbb{P}_{d,p,T}\\big(q_{1:T}\\big)>0$ iff $E_{d,p}(q_{1:T})=1$ ", "page_idx": 5}, {"type": "text", "text": "For the rest of this paper, we use the simplest possible EW, with a uniform distribution over valid actions at each step. Note that to sample uniform random EW from the ground truth environment induced by $d$ and $p$ , we do not need direct access to the full $d$ and $p$ . We only need the list of objects in $p$ and the action interface in $d$ , and executability checker $E_{d,p}$ , consistent with our Assumption 1. At each step, running $E_{d,p}$ on all possible actions yields the legal actions at that step for EW. ", "page_idx": 5}, {"type": "text", "text": "Given an EW distribution, we define an EW metric using the fractions of executability of EW walks from one domain under another, averaged over all different lengths. ", "page_idx": 5}, {"type": "text", "text": "Definition 1 (EW Metrics) Let $p_{1:N}$ and $\\hat{p}_{1:N}$ be problems in domain $d$ and $\\hat{d}$ respectively, such that the set of objects in $p_{j}$ and $\\hat{p}_{j}$ are consistent. We define the one-sided measure $m_{d\\to\\hat{d}}$ and the symmetric one $m_{d\\leftrightarrow\\hat{d}}$ for the degree of alignment between two domains $d$ and $\\hat{d}\\,a s$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{m_{d\\to\\hat{d}}(p_{1:N},\\hat{p}_{1:N}):=\\frac{1}{N T_{m a x}}\\sum_{j=1}^{N}\\sum_{T=1}^{T_{m a x}}\\mathbb{E}_{q\\sim\\mathbb{P}_{d,p_{j},T}}\\left[E_{\\hat{d},\\hat{p}_{j}}(q)\\right]}\\\\ &{m_{d\\leftrightarrow\\hat{d}}(p_{1:N},\\hat{p}_{1:N}):=2\\Big/\\big(1/m_{d\\rightarrow\\hat{d}}(p_{1:N},\\hat{p}_{1:N})+1/m_{\\hat{d}\\rightarrow d}(\\hat{p}_{1:N},p_{1:N})\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $T_{m a x}$ is the largest EW walk length. ", "page_idx": 6}, {"type": "text", "text": "md d\u02c6 measures what fraction of EWs sampled from domain d are executable on the domain d\u02c6. Then, $m_{d\\leftrightarrow\\hat{d}}$ takes the harmonic mean of $m_{d\\to\\hat{d}}$ and $m_{\\hat{d}\\to d}$ to produce the final EW measure. This metric has two favourable properties: (1) it en sures that $m_{d\\leftrightarrow\\hat{d}}=m_{\\hat{d}\\leftrightarrow d}$ , thereby providing a consistent measure of similarity regardless of the order of domain comparison. (2) the harmonic mean is resistant to trivial domain similarity inflation. By employing the harmonic mean rather than the arithmetic mean, the symmetric EW metric prevents domains that are overly permissive (e.g., domains where all actions are permissible without any preconditions) from being similar to more restrictive domains. For example, in a scenario where domain $\\hat{d}$ allows all possible actions without restrictions, $m_{d\\rightarrow\\hat{d}}=1$ An arithmetic mean in this context would yield $m_{d\\leftrightarrow\\hat{d}}\\,\\geq\\,0.5$ , overestimating the similar ity. In contrast, the harmonic mean results in $m_{d\\leftrightarrow{\\hat{d}}}=\\epsilon$ , where $\\leftr\\ll1\\right.$ ) for most cases. ", "page_idx": 6}, {"type": "text", "text": "Note that while the PDDL problems $p_{1:N}$ and $\\hat{p}_{1:N}$ appear in the definition of EW metrics, we only use the fact there are aligned object sets in them. We could also use an arbitrarily sampled object list to form an $\\tilde{P}$ and pair $\\tilde{P}$ with $D$ and $\\hat{D}$ for EW metrics. But since for PDDL generation, we already generate $\\hat{p}_{1:N}$ , it is more convenient to use them. ", "page_idx": 6}, {"type": "text", "text": "Importantly, EW metrics can be computed without direct access to the full ground truth domain $d$ and problems $p$ \u2019s. As established before, to sample uniform random EW, we just need access to the object list and action interface, plus the environment executability checker of the source domain. So even for $m_{d\\to\\hat{d}}$ , where the EW action sequences come from $d$ , we do not need more than what is available thro ugh Assumption 1. ", "page_idx": 6}, {"type": "text", "text": "To demonstrate the relationship between $m_{d\\leftrightarrow\\hat{d}}$ and domain disparity, we use the same simulated random omission study setup from Sec. 4.2. For a pair of modified domains, we count the number of terms that differ, and inspect $m_{d\\leftrightarrow\\hat{d}}$ as function of increasing number of differing terms in Figure 2b for six example domains (see Figure 4 in the Appendix for the full set). We observe that, on average, a greater discrepancy in the number of terms between two domains correlates with a reduced EW score $m_{d\\leftrightarrow{\\hat{d}}}$ . This observation provides additional support to the use of the EW score as an effective measure for domain differences. ", "page_idx": 6}, {"type": "text", "text": "4.4 Leveraging LLMs to generate PDDL files ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now show our overall LLM-based method for PDDL generation using the EW score to guide and measure the progress of domain generation. To illustrate the process, we first focus on a domain $d$ with a single task $p$ . Recall that we are given NL description of the environment domain $d_{\\mathrm{NL}}$ and problem $p_{\\mathrm{{NL}}}$ (Assumption 2), as well as the object list in $p$ and action interface from $d$ (Assumption 1). Then, by using $d_{\\mathrm{NL}},p_{\\mathrm{NL}}$ , and access to environment action feedback, we seek to generate $\\hat{d}\\in\\mathcal{D},\\hat{p}\\in\\mathcal{P}$ . ", "page_idx": 6}, {"type": "text", "text": "Our method starts by initializing templated $\\hat{d}^{(0)}$ based on action interfaces and templated $\\hat{p}^{(0)}$ using object list. Example template $\\hat{d}^{(0)}$ and $\\hat{p}^{(0)}$ are shown in Listings 6 and 4 of Appendix A.1. We then use an LLM to improve the initial $\\hat{d}^{(0)}$ and $\\hat{p}^{(0)}$ . ", "page_idx": 6}, {"type": "text", "text": "Given that domain PDDL files are typically more complex than problem PDDL files, our strategy prioritizes the generation of a problem PDDL file $\\hat{p}$ first, followed by the domain $\\hat{d}$ . This approach enables us to assess the quality of the generated domain immediately. Moreover, prior works on code generation [4], tree-of-thought [31], and self-debug [5] have found that taking multiple samples from the LLM response and taking the best response leads to better performance. However, they often require an evaluation metric on the generated response (such as unit test cases, or execution traces). Here, we use the EW metric introduced in Section 4.3 to serve as an evaluator of the generated domain. These considerations lead to our proposed Algorithm 1. We emphasize again that the ", "page_idx": 6}, {"type": "image", "img_path": "RzlCqnncQv/tmp/fab446f6530914f32c488a5c9988af270dd233feca716b529e06b8e7e70503a2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 3: Overview of our method. Right: The process begins with natural language descriptions translated into problem PDDL by the LLM (red arrows). Then a domain is generated and refined through iterative cycles involving exploration walks in the environment, interaction with a classical planner, and feedback from the LLM (blue/black arrows). Left: The iterative refinement process depicted on the right corresponds to single paths in the structures shown on the left. Each node represents a state in the refinement process, with arrows indicating problem translation (red), domain refinement (blue). ", "page_idx": 7}, {"type": "text", "text": "ground-truth domain and problem $d,p$ are only used to take exploration walks and evaluate a plan through the environment in 1. ", "page_idx": 7}, {"type": "text", "text": "Algorithm 1 Generating Domain PDDL and Problem PDDL Using Environment Feedback Require: Natural language descriptions $d_{\\mathrm{NL}}$ , $p_{\\mathrm{{NL}}}$ , environment action interface. 1: $\\hat{p}^{(1)},\\hat{p}^{(2)},\\cdot\\cdot\\cdot,\\hat{p}^{(n_{p}^{-})}\\overset{\\cdot\\cdot}{\\leftarrow}\\mathrm{LLM}_{n_{p}}^{\\;\\;\\cdot}(p_{\\mathrm{NL}})$ {Problem PDDL candidates} 2: for $i=1,2,\\dots,n_{p}$ do 3: $h^{(i)}\\leftarrow[\\hat{p}^{(i)},d_{\\mathrm{NL}}]$ {Keep a history of conversation}. 4: $\\hat{d}_{\\mathrm{best}}^{(i)}\\gets d_{\\mathrm{NL}}$ {Initialize with an empty template}. 5: for $c=1,2,\\dots,c_{\\mathrm{max}}\\;\\mathbf{c}$ do 6: 7: $\\begin{array}{r l}&{d^{(\\cdot,\\cdot,\\cdot)},d^{(\\cdot,\\cdot,\\cdot)},\\dotsc,d^{(\\cdot,\\cdot,\\cdot,\\cdot_{d})}\\gets\\mathsf{L L M}_{n_{d}}(n^{\\cdot\\cdot,\\cdot})}\\\\ &{\\hat{d}^{(c)}\\gets\\arg\\!\\operatorname*{max}_{\\hat{d}\\in\\{\\hat{d}^{(i,1)},\\dotsc,\\hat{d}^{(i,n_{d})}\\}}m_{d\\leftrightarrow\\hat{d}}\\left(p,\\hat{p}^{(i)}\\right)}\\\\ &{f^{(c)}\\gets\\mathrm{Natural\\;language\\;feedback\\;from\\;EW\\;on}\\;d}\\\\ &{h^{(i)}\\gets h^{(i)}+[\\hat{d}^{(c)},f^{(c)}]}\\\\ &{\\hat{d}_{\\mathrm{bsst}}^{(i)}\\gets\\arg\\!\\operatorname*{max}_{\\hat{d}\\in\\{\\hat{d}^{(c)},\\hat{d}_{\\mathrm{bst}}\\}}m_{d\\leftrightarrow\\hat{d}}\\left(p,\\hat{p}^{(i)}\\right)}\\end{array}$ $\\hat{d}^{(i,1)},\\hat{d}^{(i,2)},\\cdot\\cdot\\cdot,\\hat{d}^{(i,n_{d})}\\gets\\mathrm{LLM}_{n_{d}}(h^{(i)})$ {Evaluate LLM responses using EW} 8: $d,p$ . 9: 10: 11: end for 12: end for 13: $\\hat{d},\\hat{p}\\leftarrow\\mathrm{argmax}_{\\{(\\hat{d}_{\\mathrm{best}}^{(i)},\\hat{p}^{(i)})\\,|\\,i=1,2,\\dots,n_{p}\\}}\\,m_{d\\leftrightarrow\\hat{d}_{\\mathrm{best}}^{(i)}}(p,\\hat{p}^{(i)})$ 14: return $\\hat{d},\\hat{p}$ {Return the final refined domain and problem PDDLs} ", "page_idx": 7}, {"type": "text", "text": "Note that each environment contains $N>1$ problems, therefore, we need to translate all problem instances into PDDL. Similar to Liu et al. [15], given one problem $p_{\\mathrm{1_{NL}}}$ and its generated translation $\\hat{p}_{1}$ , we translate the rest of the problems $p_{2:N_{\\mathrm{NL}}}$ in a one-shot manner. That is, we generate $\\hat{p}_{i}:=$ $\\mathrm{LLM}_{1}\\left(p_{1_{\\mathrm{NL}}},\\hat{p}_{1},p_{i_{\\mathrm{NL}}}\\right)$ as the final problem translation for problem $i$ for all $2\\leq i\\leq N$ . ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Dataset. We consider PDDL flies from real environments, taking nine domains from a combination of domain PDDLs from Liu et al. [15] and Seipp et al. [22]. The LLM may have seen the mentioned domains in its pre-training data, which is a common issue for current benchmarks. To mitigate this issue, we also modify the original Grippers domain, and create a modified domain called \u201cGrippersood\u201d domain, to ensure no LLM has seen it previously. We generate natural domain descriptions for all PDDL files by back-translating them using GPT-4 and manually inspecting and modifying the translations for correctness. For each environment, we consider one domain PDDL $d$ and $N=10$ problem PDDLs $p_{1:N}$ . We use one problem for domain translation and EW evaluation, and all problems for evaluating a final domain response. We reserve the Blocksworld environment as an incontext example for prompting the LLM. As such, we do not evaluate the Blocksworld environment itself in our evaluations. See Appendices A.1 and C for more details on dataset curation. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Feedback Format. The natural language feedback given to LLM is in the following form: [Action sequence] [State description]. That is, we first provide LLM with the sequence of actions taken from one exploration walk, up until one action fails. Then, we provide the environment state description from the last step. We show an example of environment feedback and LLM response for the Termes environment in Listings 9 in the Appendix. We deliberately choose a simple feedback format to maintain the general applicability of our framework. ", "page_idx": 8}, {"type": "text", "text": "Baselines and Metrics. We use GPT-4 [1] (gpt-4-1106-preview) as the LLM since models with lower capability may struggle with syntax errors [9]. We consider the following methods: (1, 2) Intrinsic Planning $(\\mathbf{CoT})$ : where the language model generates a complete plan without the help of any external planning library, based on the given descriptions, both with and without chain-of-thought prompting. This baseline does not leverage any classical planner or PDDL translation. (3) P&D Chain: Our proposed method (Algorithm 1) with $n_{d}\\,=\\,n_{p}\\,=\\,1$ . (4) P&D Tree: Our proposed method with multiple response generations $(n_{d}=10,n_{p}=5)$ ). (5) P&D Tree $^+$ DomProp: Our proposed method with multiple response generations and domain proposals for each problem (see Appendix B.2). Following prior works [17, 5], we set a maximum conversation turns of $c_{\\operatorname*{max}}=4$ . ", "page_idx": 8}, {"type": "text", "text": "We run each algorithm for four seeds and compute the Best $@4$ metric, which takes the highest score among the four seeds. We report two metrics: (1) tasks solved2, measuring the fraction of the $N\\,=\\,10$ tasks successfully solved (Eq. (1)), and (2) EW score, comparing the final domain through running exploration walks on all $N$ problems (Eq. (2) with $T_{\\mathrm{max}}=10]$ ). We use the original fast-downward [11] library for planning, the modified fast-downward library from text-world [6] for python-compatible state explorations, and the VAL [12] library to validate plans. ", "page_idx": 8}, {"type": "text", "text": "Results. Table 2 shows the final results on various environments. We consider a domain generation to be solved if a method achieves $>~0.5$ solve rate since we observe the rest of the errors are problem translation errors rather than domain translation errors. Our proposed method solves 7 out of 10 domains, compared to 3 solved by the Intrinsic CoT baseline. We also generally observe the correlation of EW score with task solve rate. Particularly, even when the task solve rate is zero, the EW metric shows signs of progress, e.g., in domains such as Barman and Childsnack where all task solve rates are zero, the EW metric shows a clear distinction between method performances. Moreover, when the EW metric is high, such as 1.0, we observe a generated PDDL domain to be very close to the ground-truth domain, and differing in very few predicates. For instance, in the case of the \u201cHiking\u201d environment, the P&D Chain achieves zero solve rate, but a perfect EW score, which we observe perfect solution in the case of P&D Tree. ", "page_idx": 8}, {"type": "text", "text": "Computational Cost. For the results in Table 2 using the GPT-4 model, we used 12.40 million input tokens and 8.73 million output tokens. Computing the EW is relatively negligible compared to the cost of LLM inference. In our experiments, computing the EW score for a single domain-problem pair takes less than two minutes on a 64-core server CPU. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this work, we present a novel approach for modeling planning environments via PDDL translation using large language models (LLMs) and environment feedback, without relying on human intervention. The key contributions include introducing the Exploration Walk (EW) metric to measure domain similarity and guide domain refinement, and an iterative method that leverages LLMs to generate and refine PDDL domain and problem files. Evaluation on 10 real-world PDDL domains demonstrates the effectiveness of the proposed approach, outperforming a baseline that generates PDDL files in a single attempt without refinement. The method solves 7 out of 10 environments, achieving an average task solve rate of $66\\%$ and an average EW score of 0.84. ", "page_idx": 8}, {"type": "table", "img_path": "RzlCqnncQv/tmp/d6c8b7f01b9ed3ce9de9b1497b24363f007e406c8dc7133076c3c1ee6aa2f3df.jpg", "table_caption": ["Table 2: Best $@4$ (Tasks solved / Exploration Walk) for different domains. For intrinsic planning no domain is generated, therefore the EW score is not defined. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "The current limitations include potentially insufficient and efficient exploration caused by random EW. More sophisticated EW strategies could improve the success rate while lowering the cost in the future. For example, strategies from the reinforcement learning literature (e.g., [27, 21]) could be adapted to improve exploration efficiency and success rates. Another limitation is that we have only applied the framework to PDDL environments, despite it being applicable to digital or even physical environments. We hope this work will inspire further research at the intersection of language models and planning, enabling the development of more advanced and autonomous planning systems. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   \n[2] A. Ahmetoglu, M. Y. Seker, J. Piater, E. Oztop, and E. Ugur. Deepsym: Deep symbol generation and rule learning for planning from unsupervised robot interaction. Journal of Artificial Intelligence Research, 75:709\u2013745, 2022.   \n[3] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.   \n[4] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.   \n[5] X. Chen, M. Lin, N. Sch\u00e4rli, and D. Zhou. Teaching large language models to self-debug. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id $\\cdot$ KuPixIqPiq.   \n[6] M.-A. C\u00f4t\u00e9, A. K\u00e1d\u00e1r, X. Yuan, B. Kybartas, T. Barnes, E. Fine, J. Moore, R. Y. Tao, M. Hausknecht, L. E. Asri, M. Adada, W. Tay, and A. Trischler. Textworld: A learning environment for text-based games. CoRR, abs/1806.11532, 2018.   \n[7] G. Dagan, F. Keller, and A. Lascarides. Dynamic planning with a llm. arXiv preprint arXiv:2308.06391, 2023.   \n[8] N. Dziri, X. Lu, M. Sclar, X. L. Li, L. Jiang, B. Y. Lin, S. Welleck, P. West, C. Bhagavatula, R. Le Bras, et al. Faith and fate: Limits of transformers on compositionality. Advances in Neural Information Processing Systems, 36, 2024.   \n[9] L. Guan, K. Valmeekam, S. Sreedharan, and S. Kambhampati. Leveraging pre-trained large language models to construct and utilize world models for model-based task planning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https: //openreview.net/forum?id=zDbsSscmuj.   \n[10] M. Han, Y. Zhu, S.-C. Zhu, Y. N. Wu, and Y. Zhu. Interpret: Interactive predicate learning from language feedback for generalizable task planning. In Robotics: Science and Systems (RSS), 2024.   \n[11] M. Helmert. The fast downward planning system. Journal of Artificial Intelligence Research, 26:191\u2013246, 2006.   \n[12] R. Howey, D. Long, and M. Fox. Val: Automatic plan validation, continuous effects and mixed initiative planning using pddl. In Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence, ICTAI \u201904, page 294\u2013301, USA, 2004. IEEE Computer Society. ISBN 076952236X. doi: 10.1109/ICTAI.2004.120. URL https://doi.org/10. 1109/ICTAI.2004.120.   \n[13] IPC. International planning competition, 1998. URL https://www.icaps-conference. org/competitions/.   \n[14] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large language models are zeroshot reasoners. In Advances in Neural Information Processing Systems, volume 35, pages 22199\u201322213, 2022.   \n[15] B. Liu, Y. Jiang, X. Zhang, Q. Liu, S. Zhang, J. Biswas, and P. Stone. Llm+p: Empowering large language models with optimal planning proficiency. ArXiv, abs/2304.11477, 2023. URL https://api.semanticscholar.org/CorpusID:258298051.   \n[16] Q. Lyu, S. Havaldar, A. Stein, L. Zhang, D. Rao, E. Wong, M. Apidianaki, and C. CallisonBurch. Faithful chain-of-thought reasoning. In J. C. Park, Y. Arase, B. Hu, W. Lu, D. Wijaya, A. Purwarianti, and A. A. Krisnadhi, editors, Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 305\u2013329, Nusa Dua, Bali, Nov. 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. ijcnlp-main.20. URL https://aclanthology.org/2023.ijcnlp-main.20.   \n[17] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang, S. Gupta, B. P. Majumder, K. Hermann, S. Welleck, A. Yazdanbakhsh, and P. Clark. Self-refine: Iterative refinement with self-feedback. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum? id=S37hOerQLB.   \n[18] D. McDermott. The 1998 ai planning systems competition. AI Magazine, 21(2):35\u201355, 2000.   \n[19] A. Ni, S. Iyer, D. Radev, V. Stoyanov, W.-t. Yih, S. I. Wang, and X. V. Lin. Lever: Learning to verify language-to-code generation with execution. In Proceedings of the 40th International Conference on Machine Learning (ICML\u201923), 2023.   \n[20] J. Oswald, K. Srinivas, H. Kokel, J. Lee, M. Katz, and S. Sohrabi. Large language models as planning domain generators. In 34th International Conference on Automated Planning and Scheduling, 2024. URL https://openreview.net/forum?id=C88wQIv0aJ.   \n[21] S. Sagar, A. Taparia, and R. Senanayake. Failures are fated, but can be faded: Characterizing and mitigating unwanted behaviors in large-scale vision and language models. arXiv preprint arXiv:2406.07145, 2024.   \n[22] J. Seipp, \u00c1. Torralba, and J. Hoffmann. PDDL generators. https://doi.org/10.5281/ zenodo.6382173, 2022.   \n[23] T. Silver, V. Hariprasad, R. S. Shuttleworth, N. Kumar, T. Lozano-P\u00e9rez, and L. P. Kaelbling. PDDL planning with pretrained large language models. In NeurIPS 2022 Foundation Models for Decision Making Workshop, 2022. URL https://openreview.net/forum?id $=$ 1QMMUB4zfl.   \n[24] T. Silver, R. Chitnis, N. Kumar, W. McClinton, T. Lozano-P\u00e9rez, L. Kaelbling, and J. B. Tenenbaum. Predicate invention for bilevel planning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 12120\u201312129, 2023.   \n[25] T. Silver, S. Dan, K. Srinivas, J. Tenenbaum, L. Kaelbling, and M. Katz. Generalized planning in PDDL domains with pretrained large language models. In AAAI Conference on Artificial Intelligence (AAAI), 2024.   \n[26] K. Stein and A. Koller. Autoplanbench:: Automatically generating benchmarks for llm planners from pddl. arXiv preprint arXiv:2311.09830, 2023.   \n[27] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. A Bradford Book, Cambridge, MA, USA, 2018. ISBN 0262039249.   \n[28] K. Valmeekam, M. Marquez, A. Olmo, S. Sreedharan, and S. Kambhampati. Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https://openreview.net/forum?id $\\equiv$ YXogl4uQUO.   \n[29] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-ofthought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824\u201324837, 2022.   \n[30] Y. Xie, C. Yu, T. Zhu, J. Bai, Z. Gong, and H. Soh. Translating natural language to planning goals with large-language models. arXiv preprint arXiv:2302.05128, 2023.   \n[31] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. R. Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum? id=5Xc1ecxO1h.   \n[32] T. Zhang, T. Yu, T. Hashimoto, M. Lewis, W.-t. Yih, D. Fried, and S. Wang. Coder reviewer reranking for code generation. In International Conference on Machine Learning, pages 41832\u201341846. PMLR, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "table", "img_path": "RzlCqnncQv/tmp/e3714cb46bbe7153495c2f1ccedaaadfd35333293665670ea4d3bbf3bf20b06d.jpg", "table_caption": ["Table 3: Summary of Notation and Definitions "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "A Dataset ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Dataset Details. ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Dataset Examples. We provide an example of each file for the Grippers environment: (1) The ground-truth domain $d$ (Listing 1) of ground truth PDDL domain (2) One ground-truth problem $p$ (Listing 2) (3) Domain natural language description along with a PDDL template for action interfaces $d_{\\mathrm{NL}}$ (Listings 5 and 6) (4) Problem natural language description along with a PDDL template with the list of objects (Listings 3 and 4) ", "page_idx": 12}, {"type": "text", "text": "define (domain gripper -strips)   \n(: requirements :strips :typing)   \n(: types room obj robot gripper)   \n(: predicates (at -robby ?r - robot ?x - room) (at ?o - obj ?x - room) (free ?r - robot ?g - gripper) (carry ?r - robot ?o - obj ?g - gripper))   \n(: action move :parameters (?r - robot ?from ?to - room) :precondition (and (at -robby ?r ?from)) :effect (and (at -robby ?r ?to) (not (at -robby ?r ?from))))   \n(: action pick ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "image", "img_path": "RzlCqnncQv/tmp/c77ca239d6e55eb2f901dc18952f52327ffe9c3ade29289ccd45c637498b51e5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Figure 4: Correlation between average exploration walk score and average domain difference ", "page_idx": 13}, {"type": "text", "text": ":parameters (?r - robot ?obj - obj ?room - room ?g - gripper)   \n:precondition (and (at ?obj ?room) (at -robby ?r ?room) free ?r ?g))   \n:effect (and (carry ?r ?obj ?g) (not (at ?obj ?room)) (not (free ?r ?g)))) ", "page_idx": 13}, {"type": "text", "text": "16   \n17   \n18   \n19   \n20   \n21   \n22   \n23   \n24   \n25   \n26   \n27 ", "page_idx": 13}, {"type": "text", "text": "(: action drop ", "page_idx": 13}, {"type": "text", "text": ":parameters (?r - robot ?obj - obj ?room - room ?g gripper) ", "page_idx": 13}, {"type": "text", "text": ":precondition (and (carry ?r ?obj ?g) (at -robby ?r ?room))   \n:effect (and (at ?obj ?room) (free ?r ?g) (not (carry ?r ?obj ?g))))) Listing 1: Grippers domain PDDL [15]. ", "page_idx": 13}, {"type": "text", "text": "(define (problem gripper -2-3-4) ", "page_idx": 13}, {"type": "text", "text": "(: domain gripper -strips)   \n(: objects robot1 robot2 - robot rgripper1 lgripper1 rgripper2 lgripper2 - gripper room1 room2 room3 - room ball1 ball2 ball3 ball4 - obj) ", "page_idx": 13}, {"type": "text", "text": "(: init ", "page_idx": 13}, {"type": "text", "text": "(at -robby robot1 room2) ", "page_idx": 13}, {"type": "text", "text": "9 (free robot1 rgripper1)   \n10 (free robot1 lgripper1)   \n11 (at -robby robot2 room3)   \n12 (free robot2 rgripper2)   \n13 (free robot2 lgripper2)   \n14 (at ball1 room3)   \n15 (at ball2 room1)   \n16 (at ball3 room1)   \n17 (at ball4 room3)   \n18 )   \n19 (: goal   \n20 (and   \n21 (at ball1 room2)   \n22 (at ball2 room2)   \n23 (at ball3 room3)   \n24 (at ball4 room3)   \n25 )   \n26 )   \n27 ) ", "page_idx": 14}, {"type": "text", "text": "Listing 2: Grippers problem PDDL. ", "page_idx": 14}, {"type": "text", "text": "You control two robots , each equipped with a left and right gripper , capable of moving objects (balls) between different rooms.   \n2   \n3 Initially:   \n4 - Robot1 is in room2 and both its grippers (rgripper1 and lgripper1) are free. - Robot2 is in room3 and both its grippers (rgripper2 and lgripper2) are free. Ball1 and Ball4 are in room3. - Ball2 and Ball3 are in room1.   \n8   \n9 Your goal is to achieve the following configuration:   \n10 - Ball1 must be moved to room2.   \n11 - Ball2 must be moved to room2.   \n12 - Ball3 must remain in room3.   \n13 - Ball4 must remain in room3. ", "page_idx": 14}, {"type": "text", "text": "Listing 3: Grippers problem natural language. ", "page_idx": 14}, {"type": "text", "text": "(define (problem gripper -2-3-4) (: domain gripper -strips) (: objects lgripper1 lgripper2 rgripper1 rgripper2 - gripper ball1 ball2 ball3 ball4 - obj robot1 robot2 - robot room1 room2 room3 - room) (: init ) (: goal (and )) ", "page_idx": 14}, {"type": "text", "text": "Listing 4: Grippers problem template PDDL. ", "page_idx": 14}, {"type": "text", "text": "The gripper domain involves a world with multiple rooms , robots , and objects (balls). Each robot has two grippers that can be used to pick up and drop objects. The goal is to move objects from their initial locations to the ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "desired goal locations using the robots and their grippers. ", "page_idx": 15}, {"type": "text", "text": "The domain includes three actions: ", "page_idx": 15}, {"type": "text", "text": "1. move: This action allows a robot to move from one room to another. The precondition is that the robot must be in the starting room. The effect is that the robot is no longer in the starting room and is now in the destination room. ", "page_idx": 15}, {"type": "text", "text": "2. pick: This action allows a robot to pick up an object using one of its grippers. The preconditions are that the object and the robot must be in the same room , and the specified gripper must be free (not holding any object). The effect is that the robot is now carrying the object with the specified gripper , the object is no longer in the room , and the gripper is no longer free. ", "page_idx": 15}, {"type": "text", "text": "9 3. drop: This action allows a robot to drop an object it is carrying in a specific room using one of its grippers. The preconditions are that the robot must be carrying the object with the specified gripper and the robot must be in the specified room. The effect is that the object is now in the room , the gripper is free , and the robot is no longer carrying the object with that gripper. ", "page_idx": 15}, {"type": "text", "text": "Listing 5: Grippers domain natural language. ", "page_idx": 15}, {"type": "text", "text": "(define (domain gripper -strips)   \n2 (: requirements :strips :typing)   \n3 (: types room obj robot gripper)   \n4 (: predicates)   \n5   \n(: action move   \n7 :parameters (?r - robot ?from ?to - room)   \n8 :precondition ()   \n9 :effect ())   \n10   \n11 (: action pick   \n12 :parameters (?r - robot ?o - obj ?room - room ?g - gripper)   \n13 :precondition ()   \n14 :effect ())   \n15   \n16 (: action drop   \n17 :parameters (?r - robot ?o - obj ?room - room ?g - gripper)   \n18 :precondition ()   \n19 :effect ())) ", "page_idx": 15}, {"type": "text", "text": "Listing 6: Grippers domain PDDL template. ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "(move $\\begin{array}{r}{\\Delta p\\thinspace{\\circ}\\thinspace{\\ b}\\thinspace{\\circ}\\thinspace{\\tan}^{\\circ}}\\\\ {\\tau\\thinspace{\\circ}\\thinspace{\\ b}\\thinspace{\\circ}\\thinspace{\\ t}\\thinspace{\\geq}}\\\\ {\\tau\\thinspace{\\circ}\\thinspace{\\ b}\\thinspace{\\circ}\\thinspace{\\ t}\\thinspace{\\geq}}\\\\ {\\tau\\thinspace{\\circ}\\thinspace{\\ b}\\thinspace{\\circ}\\thinspace{\\ t}\\thinspace{\\geq}}\\end{array}$ room3 room1) (pick ball2 room1 lgripper2) 3 (move room1 room2) (drop ball2 room2 lgripper2) ", "page_idx": 15}, {"type": "image", "img_path": "RzlCqnncQv/tmp/8ebfe33a028591c1eddedc6e5025f18b928713f6acacb46b2566cadbf938baac.jpg", "img_caption": ["Figure 5: Historgram of average number of lines of domains in [22]. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "$\\begin{array}{r l}&{\\mathrm{~(~move~}}\\\\ &{\\mathrm{~(~p~i~c~k~}}\\\\ &{\\mathrm{~(~move~}}\\\\ &{\\mathrm{~(~p~i~c~k~}}\\\\ &{\\mathrm{~(~d~r~o~p~}}\\\\ &{\\mathrm{~(~move~}}\\\\ &{\\mathrm{~(~d~r~o~p~}}\\\\ &{\\mathrm{~(~move~}}\\\\ &{\\mathrm{~(~d~r~o~p~}}\\end{array}$ $\\begin{array}{r}{\\mathbf{r}\\circ\\mathbf{b}\\circ\\mathbf{t}\\,1}\\\\ {\\mathbf{r}\\circ\\mathbf{b}\\circ\\mathbf{t}\\,1}\\\\ {\\mathbf{r}\\circ\\mathbf{b}\\circ\\mathbf{t}\\,1}\\\\ {\\mathbf{r}\\circ\\mathbf{b}\\circ\\mathbf{t}\\,1}\\\\ {\\mathbf{r}\\circ\\mathbf{b}\\circ\\mathbf{t}\\,1}\\\\ {\\mathbf{r}\\circ\\mathbf{b}\\circ\\mathbf{t}\\,1}\\\\ {\\mathbf{r}\\circ\\mathbf{b}\\circ\\mathbf{t}\\,1}\\end{array}$ room2 room1) ball3 room1 lgripper1) room1 room3)   \n8 ball1 room3 rgripper1)   \n9 ball3 room3 lgripper1)   \n10 room3 room2)   \n11 ball1 room2 rgripper1) ", "page_idx": 16}, {"type": "text", "text": "Listing 7: Grippers problem plan example. ", "page_idx": 16}, {"type": "text", "text": "A.2 Criticality of predicate design. ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Here, we give an example on the delicacy of predicate design. Consider the Grippers environment, where each robot has two grippers: left gripper and right gripper. In our experiments, one of the main predicates that the LLM incorrectly generates is the free predicate (see Listing 8). This predicate keeps track of whether a gripper is free or not. Therefore, at first sight, (free ?g - gripper) seems a natural choice to show a particular gripper is not occupied and hence is capable of picking a ball. However, when designed this way, in contrast to (free ?r - robot $\\mathfrak{T}\\mathfrak{g}$ - gripper) (missing the robot argument), this small detail causes the final domain to be entirely wrong! The reason is that there would no longer be any association between a robot and its two grippers. Therefore, on the incorrect domain, one robot will be able to pickup an object with the gripper of another robot! In fact, we observe that this incorrect design for the free predicate, is the reason behind the failure of the \u201cP&D Chain\u201d method in Table 2. ", "page_idx": 16}, {"type": "text", "text": "We provide one more example from the Barman environment, illustrating the criticality of predicate design. The Barman environment involves actions related to manipulating containers (e.g., shot glasses, shakers) to prepare and serve drinks using various ingredients. One of the key predicates used in the domain is (used ?c - container ?b - beverage), which keeps track of which beverage has been used in a specific container. This is important for actions like refliling or cleaning, where knowing the specific beverage type is essential to ensure conformation to the environment rules (e.g., a container can be refliled only with the beverage that it already had, otherwise, it needs to be cleaned first). However, we have observed that when the LLM generates the domain, it sometimes mistakenly omits the beverage argument, simplifying the predicate to (used ?c - container). At first glance, this might seem like a harmless simplification, as the container usage is still tracked. However, this change results in significant problems in the overall domain behavior. Since the beverage is no longer specified, the domain can no longer differentiate between containers used for different types of beverages. This leads to situations where a container that has already been used for one beverage can be incorrectly treated as if it can hold another beverage without requiring proper cleaning or resetting actions. Such a mistake can cause the final domain to generate invalid plans, as the planner will fail to ensure that containers are used properly with respect to their contents, leading to cascading errors in tasks like mixing drinks, cleaning containers, or pouring from shakers. ", "page_idx": 16}, {"type": "text", "text": "(define (problem gripper -2-3-4) (: domain gripper -strips) (: objects robot1 robot2 - robot rgripper1 lgripper1 rgripper2 lgripper2 - gripper ", "page_idx": 16}, {"type": "text", "text": "5 room1 room2 room3 - room   \n6 ball1 ball2 ball3 ball4 - obj)   \n7 (: init   \n8 (at -robby robot1 room2)   \n9 (free rgripper1) ; Correct: (free robot1 rgripper1)   \n10 (free lgripper1) ; Correct: (free robot1 lgripper1)   \n11 (at -robby robot2 room3)   \n12 (free rgripper2) ; Correct: (free robot2 rgripper2)   \n13 (free lgripper2) ; Correct: (free robot2 lgripper2)   \n14 (at ball1 room3)   \n15 (at ball2 room1)   \n16 (at ball3 room1)   \n17 (at ball4 room3)   \n18 )   \n19 (: goal   \n20 (and   \n21 (at ball1 room2)   \n22 (at ball2 room2)   \n23 (at ball3 room3)   \n24 (at ball4 room3)   \n25 )   \n26 )   \n27 ", "page_idx": 17}, {"type": "text", "text": "Listing 8: Incorrect generated grippers problem PDDL. The free predicate has only one parameter. ", "page_idx": 17}, {"type": "text", "text": "B Implementation Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we explain our implementation details. ", "page_idx": 17}, {"type": "text", "text": "B.1 One-shot prompting ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To generate PDDL flies (problem PDDL and domain PDDL), we always include a one-shot example prompt from the BlocksWorld environment. This environment is concise easy enough to fit into context, and explanatory enough to demonstrate example to the LLM for better output steerability. This includes problem generation, domain proposal, and problem refinement. For instance, when prompting the LLM to generate problem translation from natural language, e.g., $\\mathrm{LLM}(p_{\\mathrm{NL}})$ , we also prompt the LLM with an example from Blocksworld. ", "page_idx": 17}, {"type": "text", "text": "B.2 P&D Tree with Domain Proposal ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "As discussed in A.2, predicate design is challenging. Therefore, in one variant of our method, which we call \u201cP&D Tree DomProp\u201d, we propose for the LLM to first draft a domain proposal, then generate a problem PDDL based on the predicates found in the draft. This way, the LLM first generates domain-aware predicates, then generates the problem PDDL. Formally, line one in Algorithm 1 will be changed to the following two lines: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{d}_{\\mathrm{pr}}^{(1)},\\hat{d}_{\\mathrm{pr}}^{(2)},\\dots,\\hat{d}_{\\mathrm{pr}}^{(n_{p})}\\leftarrow\\mathrm{LLM}_{n_{p}}(d_{\\mathrm{NL}})}\\\\ &{\\hat{p}^{(i)}\\leftarrow\\mathrm{LLM}_{1}(\\hat{d}_{\\mathrm{pr}}^{(i)},p_{\\mathrm{NL}})\\;\\;\\mathbf{for\\;all}\\;\\;1\\leq i\\leq n_{p}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the problem PDDL is generated by first creating a domain proposal. ", "page_idx": 17}, {"type": "text", "text": "B.3 Domain Refinement Strategy ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Refinement Interface. For the domain refinement stage, in our early experiments we observed that prompting the LLM to regenerate the domain results into redundant output generation and ", "page_idx": 17}, {"type": "image", "img_path": "RzlCqnncQv/tmp/fe4964e95d2db8a58ed644c6eda9124a505cf1c0f3cb29e4128f047c5af89302.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Figure 6: Overview of our method with domain proposal. To generate a problem PDDL, the LLM first drafts a domain proposal to find suitable predicates for the problem PDDL. Then, the draft is discarded, and the domain refinement stage starts. ", "page_idx": 18}, {"type": "text", "text": "more importantly, sometimes modifies incorrect parts of the domain. For instance, the LLM had a high tendency towards changing the action interface signature, despite the instructions explicitly mentioning not to change the signature. As such, we provide a python interface for the LLM to modify a domain. The interface provides the LLM with the following two functions: ", "page_idx": 18}, {"type": "text", "text": "modify_action ( action_name : str , new_preconditions : L i s t [ s t r ] , new_effects : L i s t [ s t r ] ", "page_idx": 18}, {"type": "text", "text": "The first function adds predicates to the list of already created predicates, and the second one modifies the preconditions and effects of a particular action. Guan et al. [9] use a similar approach where they generate the domain PDDL one action at a time, and gradually create predicates. However, our python function interface allows for more flexibility, such as more convenient implementation as well as enabling the LLM to modify an action several times, or introduce predicates in between reasoning steps. ", "page_idx": 18}, {"type": "text", "text": "Domain Rating. Our main domain rating originates from the EW metric. When generating domain refinement strategies, the LLM may make mistakes hence failing before even the EW metric could be computed. For instance, the modification may be invalid, containing syntax error, or failing to flil parts of the template. To facilitate incorporating these into the EW metric strategy, we create the following rating system for each domain refinement modification: ", "page_idx": 18}, {"type": "table", "img_path": "RzlCqnncQv/tmp/e68e4543400ba3b5e3be48684cee37c3f6a6e64225866f42ad7e8b54709137ef.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "when the EW metric is perfect (i.e., equals 1.0), we also run the planning on the environment (i.e., evaluate $V_{d,p}(.)]$ ) and stop early if the plan is valid. ", "page_idx": 19}, {"type": "text", "text": "User: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "2 Incorrect. The environment returned the following error:   \n3   \n$4$ Sampled a set of consecutive random actions from the ground truth environment , but the actions are not executable in the generated environment.   \n5 Executing the following actions sequentially on the environment: (move pos -2-0 pos -3-0 n0)   \n7 (move pos -3-0 pos -2-0 n0) (create -block pos -2-0) (move pos -2-0 pos -3-0 n0)   \n0 (move pos -3-0 pos -2-0 n0) (place -block pos -2-0 pos -1-0 n0 n1) (remove -block pos -2-0 pos -1-0 n1 n0) (place -block pos -2-0 pos -2-1 n0 n1) (remove -block pos -2-0 pos -2-1 n1 n0) (place -block pos -2-0 pos -3-0 n0 n1) (create -block pos -2-0) (place -block pos -2-0 pos -2-1 n0 n1) (create -block pos -2-0) (place -block pos -2-0 pos -1-0 n0 n1) (create -block pos -2-0) (move -up pos -2-0 n0 pos -1-0 n1)   \n2   \n3 Result: Error when executing the action (move -up pos -2-0 n0 pos -1-0 n1).   \n4 Current state: The robot is at position pos -2-0.   \n25 This action is not executable on the environment.   \n6   \n27 Please reason about the issue with your generated code. The current domain pddl is as follows: ", "page_idx": 19}, {"type": "text", "text": "pddl ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "[Current Generated PDDL] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In your response , please generate a new code to fix the issue. ", "page_idx": 19}, {"type": "text", "text": "Assistant: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "6 It seems that there is an issue with the \u2018move -up\u2018 action precondition. The precondition for \u2018move -up\u2018 should check that the destination position \u2018?hto\u2018 is one less than the starting position \u2018?hfrom \u2018, not the other way around. This is because the robot is moving to a higher position , so the destination height should be greater than the starting height. ", "page_idx": 19}, {"type": "text", "text": "8 Similarly , the precondition for \u2018move -down \u2018 should check that the destination position \u2018?hto\u2018 is one less than the starting position \u2018?hfrom \u2018, indicating that the robot is moving to a lower position. ", "page_idx": 19}, {"type": "text", "text": "39   \n40 Let\u2019s correct the preconditions for \u2018move -up \u2018 and \u2018move -down \u2018:   \n41   \n42 python   \n43 [Python Domain Refinement Function Calls]   \n44   \n45   \n46 With these corrected preconditions , the \u2018move -up \u2018 and \u2018move -down \u2018 actions should now accurately reflect the natural language description of the Termes domain , and the error should be resolved. ", "page_idx": 20}, {"type": "text", "text": "Listing 9: Example of domain feedback from the Termes environment, where the LLM output refinement results into a correct domain. ", "page_idx": 20}, {"type": "text", "text": "B.4 Further experiment details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "LLM calls per task. For each task in Algorithm 1, the overall complexity of LLM calls is ${\\cal O}(n_{p}\\times$ $n_{d}\\times c_{m a x}+N)$ . The complexity of domain generation is $O(n_{p}\\times c_{\\operatorname*{max}}\\times n_{d})$ . This is because at first, $n_{p}$ problem candidates are generated and for each problem candidate the algorithm goes through a refinement procedure (lines 1 and 2 of Algorithm 1). The refinement is a tree with depth $c_{\\mathrm{max}}$ (where $c_{\\mathrm{max}}$ is the maximum number of refinement turns) (line 5), and at each level of the tree, one node is expanded with $n_{d}$ children (where $n_{d}$ is the number of domain refinement candidates) (line 6), which leads to $O(n_{p}\\times c_{\\mathrm{max}}\\times n_{d})$ complexity. Once the domain is ready, the complexity of task generation for $N$ tasks is $O(N)$ since for each task we only call the LLM once to get a problem translation. ", "page_idx": 20}, {"type": "text", "text": "Number of successful seeds. In Table 2, we report the results over four seeds. To provide further analysis, we report the number of seeds a domain was successful in successfully generating a correct domain. The number of seeds that succeed in generating correct domain for the Termes, Movie, Miconic, Grippers, Hiking, Grippers-ood, and Floortile, are 4, 3, 3, 3, 2, 1, 1, respectively. ", "page_idx": 20}, {"type": "text", "text": "C Natural Language Description Generation ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "To generate natural language description of domains, problems, and environment states, we use the following strategies: ", "page_idx": 20}, {"type": "text", "text": "\u2022 Domain: We use a few-shot translation strategy. We first pick three diverse environments of \u201cGrippers\u201d, \u201cChildsnack\u201d, and \u201cTermes\u201d to manually (with assistance of GPT-4) curate domain translation. Then, we use these three domains as three-shot in-context examples to translate the rest of domains. The example prompt is provided in Listing 10.   \n\u2022 Problems: We use a similar few-shot translation strategy for problem translation. We first pick two diverse environments of \u201cTermes\u201d and \u201cSatellite\u201d for problem two-shot problem translation. Once one problem from a target domain is translated, we use that problem translation as in-context example to translate the rest of the problems. This step is crucial to ensure all problems from the same domain are translated in a consistent manner. The example prompt is provided in Listing 11.   \n\u2022 Natural Language Predicate Description: To generate natural language description of states, we generate a python files for each domain, with one function to produce natural language description of predicates for state description. The example prompt is provided in Listing 12.   \n1 Your task is to translate PDDL files into natural language.   \n2 Ensure that the resulting text covers natural language description of its actions , their preconditions , and effects. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "3 DO NOT translate the problem PDDL files , only use problem PDDL to understand the domain. ALWAYS wrap your code in the appropriate markdown syntax. Two examples are provided below. 5 Q: Domain PDDL: \u2018\u2018\u2018pddl (define (domain gripper -strips) (: requirements :strips :typing) 10 (: types room obj robot gripper) 11 (: predicates (at -robby ?r - robot ?x - room) 12 (at ?o - obj ?x - room) 13 (free ?r robot ?g - gripper) 14 (carry ?r robot ?o - obj ?g - gripper)) 15 (: action move 16 :parameters (?r - robot ?from ?to - room) 17 :precondition (and (at -robby ?r ?from)) 18 :effect (and (at -robby ?r ?to) 19 (not (at -robby ?r ?from)))) 20 (: action pick 21 :parameters (?r - robot ?obj - obj ?room - room ?g gripper) 22 :precondition (and (at ?obj ?room) (at -robby ?r ?room) (free ?r ?g)) 23 :effect (and (carry ?r ?obj ?g) 24 (not (at ?obj ?room)) 25 (not (free ?r ?g)))) 26 (: action drop 27 :parameters (?r robot ?obj - obj ?room - room ?g gripper) 28 :precondition (and (carry ?r ?obj ?g) (at -robby ?r ?room)) 29 :effect (and (at ?obj ?room) 30 (free ?r ?g) 31 (not (carry ?r ?obj ?g))))) 32 33 Problem PDDL: 34 \u2018\u2018\u2018pddl 35 (define (problem gripper -2-4-6) 36 (: domain gripper -strips) 37 (: objects robot1 robot2 - robot 38 rgripper1 lgripper1 rgripper2 lgripper2 - gripper 39 room1 room2 room3 room4 - room 40 ball1 ball2 ball3 ball4 ball5 ball6 - obj) 41 (: init 42 (at -robby robot1 room2) 43 (free robot1 rgripper1) 44 (free robot1 lgripper1) 45 (at -robby robot2 room3) 46 (free robot2 rgripper2) 47 (free robot2 lgripper2) 48 (at ball1 room3) 49 (at ball2 room1) 50 (at ball3 room3) ", "page_idx": 21}, {"type": "text", "text": "59 (at ball3 room1)   \n60 (at ball4 room2)   \n61 (at ball5 room1)   \n62 (at ball6 room1)   \n63 )   \n64 )   \n65 )   \n66 \u2018\u2018\u2018   \n67 A:   \n68 \u2018\u2018\u2018markdown   \n69 The gripper domain involves a world with multiple rooms , robots , and objects (balls). Each robot has two grippers that can be used to pick up and drop objects. The goal is to move objects from their initial locations to the desired goal locations using the robots and their grippers.   \n70 The domain includes three actions:   \n71 1. move: This action allows a robot to move from one room to another. The precondition is that the robot must be in the starting room. The effect is that the robot is no longer in the starting room and is now in the destination room.   \n72 2. pick: This action allows a robot to pick up an object using one of its grippers. The preconditions are that the object and the robot must be in the same room , and the specified gripper must be free (not holding any object). The effect is that the robot is now carrying the object with the specified gripper , the object is no longer in the room , and the gripper is no longer free.   \n73 3. drop: This action allows a robot to drop an object it is carrying in a specific room using one of its grippers. The preconditions are that the robot must be carrying the object with the specified gripper and the robot must be in the specified room. The effect is that the object is now in the room , the gripper is free , and the robot is no longer carrying the object with that gripper.   \n74   \n75 Q:   \n76 Domain PDDL:   \n77 \u2018\u2018\u2018pddl   \n78 (define (domain child -snack)   \n79 (: requirements :typing :equality)   \n80 (: types child bread -portion content -portion sandwich tray place)   \n81 (: constants kitchen - place)   \n82 (: predicates ( at_kitchen_bread ?b - bread -portion)   \n83 ( at_kitchen_content ?c - content -portion) ", "page_idx": 22}, {"type": "text", "text": "85 ( no_gluten_bread ?b - bread -portion)   \n86 ( no_gluten_content ?c - content -portion)   \n87 (ontray ?s - sandwich ?t - tray)   \n88 ( no_gluten_sandwich ?s sandwich)   \n89 ( allergic_gluten ?c - child)   \n90 ( not_allergic_gluten ?c - child)   \n91 (served ?c - child)   \n92 (waiting ?c - child ?p place)   \n93 (at ?t - tray ?p - place)   \n94 (notexist ?s sandwich)   \n95 )   \n96 (: action make_sandwich_no_gluten   \n97 :parameters (?s - sandwich ?b - bread -portion ?c -   \ncontent -portion)   \n98 :precondition (and ( at_kitchen_bread ?b)   \n99 ( at_kitchen_content ?c)   \n100 ( no_gluten_bread ?b)   \n101 ( no_gluten_content ?c)   \n102 (notexist ?s))   \n103 :effect (and   \n104 (not ( at_kitchen_bread ?b))   \n105 (not ( at_kitchen_content ?c))   \n106 ( at_kitchen_sandwich ?s)   \n107 ( no_gluten_sandwich ?s)   \n108 (not (notexist ?s))   \n109 ))   \n110 (: action make_sandwich   \n111 :parameters (?s sandwich ?b - bread -portion ?c -   \ncontent -portion)   \n112 :precondition (and ( at_kitchen_bread ?b)   \n113 ( at_kitchen_content ?c)   \n114 (notexist ?s)   \n115 )   \n116 :effect (and   \n117 (not ( at_kitchen_bread ?b))   \n118 (not ( at_kitchen_content ?c))   \n119 ( at_kitchen_sandwich ?s)   \n120 (not (notexist ?s))   \n121 ))   \n122 (: action put_on_tray   \n123 :parameters (?s - sandwich ?t - tray)   \n124 :precondition (and ( at_kitchen_sandwich ?s)   \n125 (at ?t kitchen))   \n126 :effect (and   \n127 (not ( at_kitchen_sandwich ?s))   \n128 (ontray ?s ?t)))   \n129 (: action serve_sandwich_no_gluten   \n130 :parameters (?s - sandwich ?c - child ?t - tray ?p - place)   \n131 :precondition (and   \n132 ( allergic_gluten ?c)   \n133 (ontray ?s ?t)   \n134 (waiting ?c ?p)   \n135 ( no_gluten_sandwich ?s)   \n136 (at ?t ?p)   \n137 )   \n138 :effect (and (not (ontray ?s ?t))   \n139 (served ?c)))   \n140 (: action serve_sandwich   \n141 :parameters (?s sandwich ?c - child ?t - tray ?p - place)   \n142 :precondition (and ( not_allergic_gluten ?c)   \n143 (waiting ?c ?p)   \n144 (ontray ?s ?t)   \n145 (at ?t ?p))   \n146 :effect (and (not (ontray ?s ?t))   \n147 (served ?c)))   \n148 (: action move_tray   \n149 :parameters (?t - tray ?p1 ?p2 - place)   \n150 :precondition (and (at ?t ?p1))   \n151 :effect (and (not (at ?t ?p1))   \n152 (at ?t ?p2)))   \n153   \n154 )   \n155   \n156 Problem PDDL:   \n157 \u2018\u2018\u2018pddl   \n158 \u4e1a child -snack task with 6 children and 0.4 gluten factor   \n159 ; constant factor of 1.3   \n160 ; random seed: 234324   \n161 (define (problem prob -snack)   \n162 (: domain child -snack)   \n163 (: objects   \n164 child1 child2 child3 child4 child5 child6 - child   \n165 bread1 bread2 bread3 bread4 bread5 bread6 - bread -portion   \n166 content1 content2 content3 content4 content5 content6   \ncontent -portion   \n167 tray1 tray2 tray   \n168 table1 table2 table3 place   \n169 sandw1 sandw2 sandw3 sandw4 sandw5 sandw6 sandw7 sandw8   \nsandwich   \n170 )   \n171 (: init   \n172 (at tray1 kitchen)   \n173 (at tray2 kitchen)   \n174 ( at_kitchen_bread bread1)   \n175 ( at_kitchen_bread bread2)   \n176 ( at_kitchen_bread bread3)   \n177 ( at_kitchen_bread bread4)   \n178 ( at_kitchen_bread bread5)   \n179 ( at_kitchen_bread bread6)   \n180 ( at_kitchen_content content1)   \n181 ( at_kitchen_content content2)   \n182 ( at_kitchen_content content3)   \n183 ( at_kitchen_content content4)   \n184 ( at_kitchen_content content5)   \n185 ( at_kitchen_content content6)   \n186 ( no_gluten_bread bread2)   \n187 ( no_gluten_bread bread5)   \n188 ( no_gluten_content content3) ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "189 ( no_gluten_content content6)   \n190 ( allergic_gluten child1)   \n191 ( allergic_gluten child5)   \n192 ( not_allergic_gluten child2)   \n193 ( not_allergic_gluten child3)   \n194 ( not_allergic_gluten child4)   \n195 ( not_allergic_gluten child6)   \n196 (waiting child1 table2)   \n197 (waiting child2 table1)   \n198 (waiting child3 table3)   \n199 (waiting child4 table2)   \n200 (waiting child5 table1)   \n201 (waiting child6 table1)   \n202 (notexist sandw1)   \n203 (notexist sandw2)   \n204 (notexist sandw3)   \n205 (notexist sandw4)   \n206 (notexist sandw5)   \n207 (notexist sandw6)   \n208 (notexist sandw7)   \n209 (notexist sandw8)   \n210 )   \n211 (: goal   \n212 (and   \n213 (served child1)   \n214 (served child2)   \n215 (served child3)   \n216 (served child4)   \n217 (served child5)   \n218 (served child6)   \n219 )   \n220 )   \n221 )   \n222 \u2018\u2018\u2018   \n223 A:   \n224 \u2018\u2018\u2018markdown   \n225 The child -snack domain is designed to plan the preparation and serving of sandwiches to children , some of whom have gluten allergies. The domain includes actions for making sandwiches with and without gluten , placing sandwiches on trays , moving trays between places , and serving the sandwiches to the children.   \n226 The actions defined in this domain include:   \n227 1. make_sandwich_no_gluten : This action is for making a gluten -free sandwich. The preconditions for this action are that there must be gluten -free bread and gluten -free content available in the kitchen , and the sandwich must not already exist. The effect of this action is that the gluten -free bread and content are no longer available in the kitchen , and a gluten -free sandwich is now available in the kitchen.   \n228 2. make_sandwich: This action is for making a regular sandwich that may contain gluten. The preconditions are that there +h ", "page_idx": 25}, {"type": "text", "text": "the sandwich must not already exist. The effect of this ", "page_idx": 26}, {"type": "text", "text": "action is that the bread and content are no longer available in the kitchen , and a sandwich is now available in the kitchen. 229 3. put_on_tray: This action is for placing a sandwich on a tray. The preconditions are that the sandwich must be in the kitchen and the tray must be at the kitchen. The effect is that the sandwich is no longer in the kitchen and is now on the tray. 230 4. serve_sandwich_no_gluten : This action is for serving a gluten -free sandwich to a child who is allergic to gluten. The preconditions are that the child is allergic to gluten , waiting at a place , and there is a gluten -free sandwich on a tray at the same place. The effect is that the sandwich is no longer on the tray and the child is served. 231 5. serve_sandwich : This action is for serving a sandwich to a child who is not allergic to gluten. The preconditions are that the child is not allergic to gluten , waiting at a place , and there is a sandwich on a tray at the same place. The effect is that the sandwich is no longer on the tray and the child is served. 232 6. move_tray: This action allows a tray to be moved from one place to another. The precondition is that the tray is at the starting place. The effect is that the tray is no longer at the starting place and is now at the destination place. 233 234 Q: 235 Domain PDDL: 236 \u2018\u2018\u2018pddl 237 (define (domain termes) 238 (: requirements :typing :negative -preconditions) 239 (: types 240 numb - object 241 position - object 242 ) 243 (: predicates 244 (height ?p - position ?h - numb) 245 (at ?p - position) 246 (has -block) 247 ; 248 ; static predicates 249 (SUCC ?n1 numb ?n2 numb) 250 (NEIGHBOR ?p1 - position ?p2 - position) 251 (IS -DEPOT ?p - position) 252 ) 253 (: action move 254 :parameters (? from - position ?to - position ?h - numb) 255 :precondition 256 (and 257 (at ?from) 258 (NEIGHBOR ?from ?to) 259 (height ?from ?h) ", "page_idx": 26}, {"type": "text", "text": "260 (height ?to ?h)   \n261 )   \n262 :effect   \n263 (and   \n264 (not (at ?from))   \n265 (at ?to)   \n266 )   \n267 )   \n268 (: action move -up   \n269 :parameters (? from - position ?hfrom - numb ?to - position   \n?hto - numb)   \n270 :precondition   \n271 (and   \n272 (at ?from)   \n273 (NEIGHBOR ?from ?to)   \n274 (height ?from ?hfrom)   \n275 (height ?to ?hto)   \n276 (SUCC ?hto ?hfrom)   \n277   \n278 :effect   \n279 (and   \n280 (not (at ?from))   \n281 (at ?to)   \n282 )   \n283 )   \n284 (: action move -down   \n285 :parameters (? from - position ?hfrom - numb ?to - position   \n?hto numb)   \n286 :precondition   \n287 (and   \n288 (at ?from)   \n289 (NEIGHBOR ?from ?to)   \n290 (height ?from ?hfrom)   \n291 (height ?to ?hto)   \n292 (SUCC ?hfrom ?hto)   \n293 )   \n294 :effect   \n295 (and   \n296 (not (at ?from))   \n297 (at ?to)   \n298 )   \n299 )   \n300 (: action place -block   \n301 :parameters (? rpos position ?bpos - position ?hbefore   \nnumb ?hafter numb)   \n302 :precondition   \n303 (and   \n304 (at ?rpos)   \n305 (NEIGHBOR ?rpos ?bpos)   \n306 (height ?rpos ?hbefore)   \n307 (height ?bpos ?hbefore)   \n308 (SUCC ?hafter ?hbefore)   \n309 (has -block)   \n310 (not (IS -DEPOT ?bpos))   \n311 )   \n312 :effect   \n313 (and   \n314 (not (height ?bpos ?hbefore))   \n315 (height ?bpos ?hafter)   \n316 (not (has -block))   \n317 )   \n318 )   \n319 (: action remove -block   \n320 :parameters (? rpos - position ?bpos - position ?hbefore   \nnumb ?hafter - numb)   \n321 :precondition   \n322 (and   \n323 (at ?rpos)   \n324 (NEIGHBOR ?rpos ?bpos)   \n325 (height ?rpos ?hafter)   \n326 (height ?bpos ?hbefore)   \n327 (SUCC ?hbefore ?hafter)   \n328 (not (has -block))   \n329 )   \n330 :effect   \n331 (and   \n332 (not (height ?bpos ?hbefore))   \n333 (height ?bpos ?hafter)   \n334 (has -block)   \n335 )   \n336 )   \n337 (: action create -block   \n338 :parameters (?p position)   \n339 :precondition   \n340 (and   \n341 (at ?p)   \n342 (not (has -block))   \n343 (IS -DEPOT ?p)   \n344 )   \n345 :effect   \n346 (and   \n347 (has -block)   \n348 )   \n349 )   \n350 (: action destroy -block   \n351 :parameters (?p position)   \n352 :precondition   \n353 (and   \n354 (at ?p)   \n355 (has -block)   \n356 (IS -DEPOT ?p)   \n357 )   \n358 :effect   \n359 (and   \n360 (not (has -block))   \n361 )   \n362 )   \n363 )   \n364 \u2018\u2018\u2018   \n365 Problem PDDL:   \n366 \u2018\u2018\u2018pddl   \n367 (define (problem prob)   \n368 (: domain termes)   \n369 ; Initial state:   \n370 ; 0 0 R0D   \n371 ; 0 0 0   \n372 ; 0 0 0   \n373 ; Goal state:   \n374 ; 0 0 0   \n375 ; 0 1 0   \n376 ; 0 0 0   \n377 ; Maximal height: 1   \n378 (: objects   \n379 n0 - numb   \n380 n1 - numb   \n381 pos -0-0 $\\begin{array}{r}{\\begin{array}{c}{\\mathrm{po\\thinspacesit\\thinspacei\\thinspaceo\\thinspacen}}\\\\ {\\mathrm{po\\thinspacesit\\thinspacei\\thinspaceo\\thinspacen}}\\\\ {\\mathrm{po\\thinspacesit\\thinspacei\\thinspaceo\\thinspacen}}\\\\ {\\mathrm{po\\thinspacesit\\thinspacei\\thinspaceo\\thinspacen}}\\\\ {\\mathrm{po\\thinspacesit\\thinspacei\\thinspaceo\\thinspacen}}\\\\ {\\mathrm{po\\thinspacesit\\thinspacei\\thinspaceo\\thinspacen}}\\\\ {\\mathrm{po\\thinspacesit\\thinspacei\\thinspaceo\\thinspacen}}\\\\ {\\mathrm{po\\thinspacesit\\thinspacei\\thinspaceo\\thinspacen}}\\\\ {\\mathrm{po\\thinspacesit\\thinspacei\\thinspaceo\\thinspacen}}\\\\ {\\mathrm{po\\thinspacesit\\thinspacei\\thinspaceo\\thinspacen}}\\end{array}}\\end{array}$   \n382 pos -0-1 -   \n383 pos -0-2 -   \n384 pos -1-0 -   \n385 pos -1-1 -   \n386 pos -1-2 -   \n387 pos -2-0 -   \n388 pos -2-1 -   \n389 pos -2-2 -   \n390 )   \n391 (: init   \n392 (height pos -0-0 n0)   \n393 (height pos -0-1 n0)   \n394 (height pos -0-2 n0)   \n395 (height pos -1-0 n0)   \n396 (height pos -1-1 n0)   \n397 (height pos -1-2 n0)   \n398 (height pos -2-0 n0)   \n399 (height pos -2-1 n0)   \n400 (height pos -2-2 n0)   \n401 (at pos -2-0)   \n402 (SUCC n1 n0)   \n403 (NEIGHBOR pos -0-0 pos -1-0)   \n404 (NEIGHBOR pos -0-0 pos -0-1)   \n405 (NEIGHBOR pos -0-1 pos -1-1)   \n406 (NEIGHBOR pos -0-1 pos -0-0)   \n407 (NEIGHBOR pos -0-1 pos -0-2)   \n408 (NEIGHBOR pos -0-2 pos -1-2)   \n409 (NEIGHBOR pos -0-2 pos -0-1)   \n410 (NEIGHBOR pos -1-0 pos -0-0)   \n411 (NEIGHBOR pos -1-0 pos -2-0)   \n412 (NEIGHBOR pos -1-0 pos -1-1)   \n413 (NEIGHBOR pos -1-1 pos -0-1)   \n414 (NEIGHBOR pos -1-1 pos -2-1)   \n415 (NEIGHBOR pos -1-1 pos -1-0)   \n416 (NEIGHBOR pos -1-1 pos -1-2)   \n417 (NEIGHBOR pos -1-2 pos -0-2)   \n418 (NEIGHBOR pos -1-2 pos -2-2)   \n419 (NEIGHBOR pos -1-2 pos -1-1)   \n420 (NEIGHBOR pos -2-0 pos -1-0)   \n421 (NEIGHBOR pos -2-0 pos -2-1)   \n422 (NEIGHBOR pos -2-1 pos -1-1)   \n423 (NEIGHBOR pos -2-1 pos -2-0)   \n424 (NEIGHBOR pos -2-1 pos -2-2)   \n425 (NEIGHBOR pos -2-2 pos -1-2)   \n426 (NEIGHBOR pos -2-2 pos -2-1)   \n427 (IS -DEPOT pos -2-0)   \n428   \n429 (: goal ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "430 (and ", "page_idx": 30}, {"type": "text", "text": "431 (height pos -0-0 n0)   \n432 (height pos -0-1 n0)   \n433 (height pos -0-2 n0)   \n434 (height pos -1-0 n0)   \n435 (height pos -1-1 n1)   \n436 (height pos -1-2 n0)   \n437 (height pos -2-0 n0)   \n438 (height pos -2-1 n0)   \n439 (height pos -2-2 n0)   \n440 (not (has -block)) ", "page_idx": 30}, {"type": "text", "text": "446 \u2018\u2018\u2018markdown   \n447 The Termes domain is a planning domain that simulates the behavior of robotic agents (inspired by termites) that can move around , pick up blocks , stack them to build structures , and remove blocks from structures. The domain includes actions for moving the robot , placing and removing blocks , and creating and destroying blocks at a depot.   \n448 The actions defined in this domain include:   \n449 1. move: This action allows the robot to move from one position to another at the same height. The preconditions are that the robot is at the starting position , the starting position is a neighbor to the destination position , and both positions have the same height. The effect is that the robot is no longer at the starting position and is now at the destination position.   \n450 2. move -up: This action allows the robot to move from a lower position to a neighboring higher position. The preconditions are that the robot is at the starting position , the starting position is a neighbor to the destination position , the starting position has a certain height , and the destination position \u2019s height is one less than the starting position \u2019s height. The effect is that the robot is no longer at the starting position and is now at the destination position.   \n451 move -down: This action allows the robot to move from a higher position to a neighboring lower position. The preconditions are that the robot is at the starting position , the starting position is a neighbor to the destination position , the starting position has a certain height , and the destination position \u2019s height is one less than the starting position \u2019s height. The effect is that the robot is no longer at the starting position and is now at the destination position. 4. place -block: This action allows the robot to place a block at a neighboring position , increasing the height of that position by one. The preconditions are that the robot is at a position next to the block position , both positions have the same height , the robot has a block , and the block position is not a depot. The effect is that the height of the block position is increased by one , and the robot no longer has a block.   \n453 5. remove -block: This action allows the robot to remove a block from a neighboring position , decreasing the height of that position by one. The preconditions are that the robot is at a position next to the block position , the robot \u2019s position is one height unit higher than the block position , and the robot does not have a block. The effect is that the height of the block position is decreased by one , and the robot now has a block.   \n454 6. create -block: This action allows the robot to create a block at a depot. The preconditions are that the robot is at the depot and does not have a block. The effect is that the robot now has a block.   \n455 7. destroy -block: This action allows the robot to destroy a block at a depot. The preconditions are that the robot is at the depot and has a block. The effect is that the robot no longer has a block.   \n456 \u2018\u2018\u2018   \n457 Q:   \n458 Domain PDDL:   \n459 \u2018\u2018\u2018pddl   \n460 [Target Domain PDDL Code]   \n461 \u2018\u2018\u2018   \n462 Problem PDDL:   \n463 \u2018\u2018\u2018pddl   \n464 [Target Problem PDDL Code]   \n465 \u2018\u2018\u2018 Listing 10: Domain back-translation prompt template, with domain PDDL and problem PDDL placeholders for each target domain. Some PDDL credit comments are omitted for clarity. Your task is to translate problem PDDL files into natural language. Ensure that the resulting description covers all initial state and goal conditions. 2 DO NOT be lazy in your response , be extremely precise in your descriptions such that all conditions are covered in your description and there is no ambiguity in your description. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "If you do not find any common rule about some conditions , list all of them. ", "page_idx": 31}, {"type": "text", "text": "For the initial conditions , start with \"Initially:\", and for the goal conditions , start with \"Your goal is to\". ", "page_idx": 32}, {"type": "text", "text": "ALWAYS wrap your code in the appropriate markdown syntax.   \n6 Two examples are provided below. ", "page_idx": 32}, {"type": "text", "text": "Q: ", "page_idx": 32}, {"type": "text", "text": "8 Domain Description: ", "page_idx": 32}, {"type": "text", "text": "markdown ", "page_idx": 32}, {"type": "text", "text": "10 The Termes domain is a planning domain that simulates the behavior of robotic agents (inspired by termites) that can move around , pick up blocks , stack them to build structures , and remove blocks from structures. The domain includes actions for moving the robot , placing and removing blocks , and creating and destroying blocks at a depot. ", "page_idx": 32}, {"type": "text", "text": "11 The actions defined in this domain include: ", "page_idx": 32}, {"type": "text", "text": "1. move: This action allows the robot to move from one position to another at the same height. The preconditions are that the robot is at the starting position , the starting position is a neighbor to the destination position , and both positions have the same height. The effect is that the robot is no longer at the starting position and is now at the destination position. ", "page_idx": 32}, {"type": "text", "text": "2. move -up: This action allows the robot to move from a lower position to a neighboring higher position. The preconditions are that the robot is at the starting position , the starting position is a neighbor to the destination position , the starting position has a certain height , and the destination position \u2019s height is one less than the starting position \u2019s height. The effect is that the robot is no longer at the starting position and is now at the destination position. ", "page_idx": 32}, {"type": "text", "text": "3. move -down: This action allows the robot to move from a higher position to a neighboring lower position. The preconditions are that the robot is at the starting position , the starting position is a neighbor to the destination position , the starting position has a certain height , and the destination position \u2019s height is one less than the starting position \u2019s height. The effect is that the robot is no longer at the starting position and is now at the destination position. ", "page_idx": 32}, {"type": "text", "text": "5 4. place -block: This action allows the robot to place a block at a neighboring position , increasing the height of that position by one. The preconditions are that the robot is at a position next to the block position , both positions have the same height , the robot has a block , and the block position is not a depot. The effect is that the height of the block position is increased by one , and the robot no longer has a block. ", "page_idx": 32}, {"type": "text", "text": "16 5. remove -block: This action allows the robot to remove a block from a neighboring position , decreasing the height of that position by one. The preconditions are that the robot is at a position next to the block position , the robot \u2019s position is one height unit higher than the block position , and the robot does not have a block. The effect ", "page_idx": 32}, {"type": "text", "text": "is that the height of the block position is decreased by   \none , and the robot now has a block.   \n17 6. create -block: This action allows the robot to create a   \nblock at a depot. The preconditions are that the robot is   \nat the depot and does not have a block. The effect is that   \nthe robot now has a block.   \n18 7. destroy -block: This action allows the robot to destroy a   \nblock at a depot. The preconditions are that the robot is   \nat the depot and has a block. The effect is that the robot   \nno longer has a block.   \n19 \u2018\u2018\u2018   \n20 Problem PDDL:   \n21 \u2018\u2018\u2018pddl   \n22 (define (problem prob)   \n23 (: domain termes)   \n24 ; Initial state:   \n25 ; 0 0 R0D   \n26 ; 0 0 0   \n27 ; 0 0 0   \n28 ; Goal state:   \n29 ; 0 0 0   \n30 ; 0 1 0   \n31 ; 0 0 0   \n32 ; Maximal height: 1   \n33 (: objects   \n34 n0 - numb   \n35 n1 - numb   \n36 pos -0-0 $\\begin{array}{r}{\\begin{array}{c}{\\mathrm{po\\thinspacesit\\thinspacei\\thinspaceo\\thinspacen}}\\\\ {\\mathrm{po\\thinspacesit\\thinspacei\\thinspaceo\\thinspacen}}\\\\ {\\mathrm{po\\thinspacesit\\thinspacei\\thinspaceo\\thinspacen}}\\\\ {\\mathrm{po\\thinspacesit\\thinspacei\\thinspaceo\\thinspacen}}\\\\ {\\mathrm{po\\thinspacesit\\thinspacei\\thinspaceo\\thinspacen}}\\\\ {\\mathrm{po\\thinspacesit\\thinspacei\\thinspaceo\\thinspacen}}\\\\ {\\mathrm{po\\thinspacesit\\thinspacei\\thinspaceo\\thinspacen}}\\\\ {\\mathrm{po\\thinspacesit\\thinspacei\\thinspaceo\\thinspacen}}\\\\ {\\mathrm{po\\thinspacesit\\thinspacei\\thinspaceo\\thinspacen}}\\\\ {\\mathrm{po\\thinspacesit\\thinspacei\\thinspaceo\\thinspacen}}\\end{array}}\\end{array}$   \n37 pos -0-1   \n38 pos -0-2   \n39 pos -1-0   \n40 pos -1-1 -   \n41 pos -1-2 -   \n42 pos -2-0 -   \n43 pos -2-1   \n44 pos -2-2   \n45 )   \n46 (: init   \n47 (height pos -0-0 n0)   \n48 (height pos -0-1 n0)   \n49 (height pos -0-2 n0)   \n50 (height pos -1-0 n0)   \n51 (height pos -1-1 n0)   \n52 (height pos -1-2 n0)   \n53 (height pos -2-0 n0)   \n54 (height pos -2-1 n0)   \n55 (height pos -2-2 n0)   \n56 (at pos -2-0)   \n57 (SUCC n1 n0)   \n58 (NEIGHBOR pos -0-0 pos -1-0)   \n59 (NEIGHBOR pos -0-0 pos -0-1)   \n60 (NEIGHBOR pos -0-1 pos -1-1)   \n61 (NEIGHBOR pos -0-1 pos -0-0)   \n62 (NEIGHBOR pos -0-1 pos -0-2)   \n63 (NEIGHBOR pos -0-2 pos -1-2)   \n64 (NEIGHBOR pos -0-2 pos -0-1)   \n65 (NEIGHBOR pos -1-0 pos -0-0)   \n66 (NEIGHBOR pos -1-0 pos -2-0)   \n67 (NEIGHBOR pos -1-0 pos -1-1)   \n68 (NEIGHBOR pos -1-1 pos -0-1)   \n69 (NEIGHBOR pos -1-1 pos -2-1)   \n70 (NEIGHBOR pos -1-1 pos -1-0)   \n71 (NEIGHBOR pos -1-1 pos -1-2)   \n72 (NEIGHBOR pos -1-2 pos -0-2)   \n73 (NEIGHBOR pos -1-2 pos -2-2)   \n74 (NEIGHBOR pos -1-2 pos -1-1)   \n75 (NEIGHBOR pos -2-0 pos -1-0)   \n76 (NEIGHBOR pos -2-0 pos -2-1)   \n77 (NEIGHBOR pos -2-1 pos -1-1)   \n78 (NEIGHBOR pos -2-1 pos -2-0)   \n79 (NEIGHBOR pos -2-1 pos -2-2)   \n80 (NEIGHBOR pos -2-2 pos -1-2)   \n81 (NEIGHBOR pos -2-2 pos -2-1)   \n82 (IS -DEPOT pos -2-0)   \n83 )   \n84 (: goal   \n85 (and   \n86 (height pos -0-0 n0)   \n87 (height pos -0-1 n0)   \n88 (height pos -0-2 n0)   \n89 (height pos -1-0 n0)   \n90 (height pos -1-1 n1)   \n91 (height pos -1-2 n0)   \n92 (height pos -2-0 n0)   \n93 (height pos -2-1 n0)   \n94 (height pos -2-2 n0)   \n95 (not (has -block))   \n96 )   \n97 )   \n98 )   \n99 \u2018\u2018\u2018   \n100 A:   \n101 \u2018\u2018\u2018markdown   \n102 You control a robot capable of building structures by moving   \nand manipulating blocks.   \n103 The environment consists of a grid of positions and two   \nheights , numbered 0 and 1. The robot can move at the same   \nheight , move up one height , or move down one height. It   \ncan also place or remove a block at a neighboring   \nposition , or create or destroy a block at the depot. A   \nblock \u2019s height increases by one when placed and decreases   \nby one when removed.   \n104 Initially:   \n105 - All positions on the grid have a height of 0.   \n106 The robot is at position pos -2-0, which is the depot.   \n107 The robot does not have a block.   \n108 The positions have the following neighboring relationships:   \n109 pos -0-0 neighbors pos -1-0 and pos -0-1 ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "110 pos -0-1 $\\begin{array}{r l}&{\\;\\mathrm{ne\\,i\\,ghb\\,o\\,r~s}}\\\\ &{\\;\\mathrm{ne\\,i\\,ghb\\,o\\,r~s}}\\\\ &{\\;\\mathrm{ne\\,i\\,ghb\\,o\\,r~s}}\\\\ &{\\;\\mathrm{ne\\,i\\,ghb\\,o\\,r~s}}\\\\ &{\\;\\mathrm{ne\\,i\\,ghb\\,o\\,r~s}}\\\\ &{\\;\\mathrm{ne\\,i\\,ghb\\,o\\,r~s}}\\\\ &{\\;\\mathrm{ne\\,i\\,ghb\\,o\\,r~s}}\\\\ &{\\;\\mathrm{ne\\,i\\,ghb\\,o\\,r~s}}\\\\ &{\\;\\mathrm{ne\\,i\\,ghb\\,o\\,r~s}}\\\\ &{\\;\\mathrm{s\\,suc\\,c\\,e\\,ss\\,o\\,:}}\\end{array}$ pos -1-1, pos -0-0, and pos -0-2   \n111 - pos -0-2 pos -1-2 and pos -0-1   \n112 - pos -1-0 pos -0-0, pos -2-0, and pos -1-1   \n113 - pos -1-1 pos -0-1, pos -2-1, pos -1-0, and pos -1-2 114 - pos -1-2 pos -0-2, pos -2-2, and pos -1-1   \n115 pos -2-0 pos -1-0 and pos -2-1, and is the depot 116 - pos -2-1 pos -1-1, pos -2-0, and pos -2-2   \n117 - pos -2-2 pos -1-2 and pos -2-1   \n118 There is r relationship between the numbers n1 and n0.   \n119 Your goal is to achieve the following configuration:   \n120 - The height at pos -1-1 needs to be 1.   \n121 - All other positions must remain at height 0.   \n122 - The robot should not have a block at the end of the task. 123 \u2018\u2018\u2018   \n124 Q:   \n125 Domain Description:   \n126 \u2018\u2018\u2018markdown   \n127 The satellite domain is designed to model the operation of satellites that can take images of various targets in different modes. Each satellite is equipped with   \ninstruments that can be turned on and off , calibrated , and used to take images. The domain includes actions for   \nturning the satellite to point at different directions , switching instruments on and off , calibrating instruments , and taking images.   \n128 The actions defined in this domain include:   \n129 1. turn_to: This action changes the direction the satellite is pointing. The preconditions are that the satellite must be pointing at a previous direction , and both the new and previous directions are valid. The effect is that the satellite is now pointing at the new direction and no longer pointing at the previous direction.   \n130 2. switch_on: This action turns on an instrument on board the satellite. The preconditions are that the instrument must be on board the satellite and there must be power   \navailable on the satellite. The effect is that the   \ninstrument is powered on , it is no longer calibrated , and the satellite no longer has power available.   \n131 3. switch_off: This action turns off an instrument on board the satellite. The preconditions are that the instrument must be on board the satellite and it must be powered on. The effect is that the satellite has power available and the instrument is no longer powered on.   \n132 4. calibrate: This action calibrates an instrument on board the satellite. The preconditions are that the satellite must be pointing at a calibration target for the   \ninstrument , the instrument must be on board the satellite and powered on. The effect is that the instrument is   \ncalibrated.   \n133 5. take_image: This action uses an instrument on board the satellite to take an image in a specific mode of a   \ndirection the satellite is pointing at. The preconditions are that the satellite must be pointing at the direction , ", "page_idx": 35}, {"type": "text", "text": "the instrument must be calibrated , on board the satellite ,   \nsupport the mode , and be powered on. The effect is that an   \nimage of the direction in the specific mode is now   \navailable.   \n134 \u2018\u2018\u2018   \n135 Problem PDDL:   \n136 \u2018\u2018\u2018pddl   \n137 (define (problem strips -sat -x-1)   \n138 (: domain satellite)   \n139 (: objects   \n140 satellite0   \n141 instrument0   \n142 satellite1   \n143 instrument1   \n144 instrument2   \n145 instrument3   \n146 satellite2   \n147 instrument4   \n148 instrument5   \n149 instrument6   \n150 satellite3   \n151 instrument7   \n152 satellite4   \n153 instrument8   \n154 thermograph2   \n155 image3   \n156 infrared1   \n157 spectrograph4   \n158 infrared0   \n159 Star1   \n160 Star4   \n161 Star0   \n162 GroundStation3   \n163 Star2   \n164 Star5   \n165 Planet6   \n166 Phenomenon7   \n167 Star8   \n168 Phenomenon9   \n169 Star10   \n170 Star11   \n171 Star12   \n172 Planet13   \n173 Planet14   \n174 Phenomenon15   \n175 Planet16   \n176 Star17   \n177 Star18   \n178 Planet19   \n179 )   \n180 (: init   \n181 (satellite satellite0)   \n182 (instrument instrument0)   \n183 (supports instrument0 spectrograph4)   \n184 ( calibration_target instrument0 Star0)   \n185 (on_board instrument0 satellite0)   \n186 (power_avail satellite0)   \n187 (pointing satellite0 Star8)   \n188 (satellite satellite1)   \n189 (instrument instrument1)   \n190 (supports instrument1 infrared0)   \n191 (supports instrument1 infrared1)   \n192 ( calibration_target instrument1 GroundStation3 )   \n193 (instrument instrument2)   \n194 (supports instrument2 infrared1)   \n195 (supports instrument2 infrared0)   \n196 ( calibration_target instrument2 Star2)   \n197 (instrument instrument3)   \n198 (supports instrument3 spectrograph4)   \n199 (supports instrument3 infrared1)   \n200 (supports instrument3 thermograph2)   \n201 ( calibration_target instrument3 Star0)   \n202 (on_board instrument1 satellite1)   \n203 (on_board instrument2 satellite1)   \n204 (on_board instrument3 satellite1)   \n205 (power_avail satellite1)   \n206 (pointing satellite1 GroundStation3)   \n207 (satellite satellite2)   \n208 (instrument instrument4)   \n209 (supports instrument4 infrared1)   \n210 (supports instrument4 image3)   \n211 (supports instrument4 infrared0)   \n212 ( calibration_target instrument4 Star2)   \n213 (instrument instrument5)   \n214 (supports instrument5 thermograph2)   \n215 (supports instrument5 spectrograph4)   \n216 ( calibration_target instrument5 Star0)   \n217 (instrument instrument6)   \n218 (supports instrument6 infrared0)   \n219 ( calibration_target instrument6 GroundStation3 )   \n220 (on_board instrument4 satellite2)   \n221 (on_board instrument5 satellite2)   \n222 (on_board instrument6 satellite2)   \n223 (power_avail satellite2)   \n224 (pointing satellite2 Star4)   \n225 (satellite satellite3)   \n226 (instrument instrument7)   \n227 (supports instrument7 image3)   \n228 ( calibration_target instrument7 Star2)   \n229 (on_board instrument7 satellite3)   \n230 (power_avail satellite3)   \n231 (pointing satellite3 Phenomenon9)   \n232 (satellite satellite4)   \n233 (instrument instrument8)   \n234 (supports instrument8 infrared0)   \n235 (supports instrument8 spectrograph4)   \n236 (supports instrument8 infrared1)   \n237 ( calibration_target instrument8 Star2)   \n238 (on_board instrument8 satellite4)   \n239 (power_avail satellite4)   \n240 (pointing satellite4 Phenomenon9)   \n241 (mode thermograph2)   \n242 (mode image3)   \n243 (mode infrared1)   \n244 (mode spectrograph4)   \n245 (mode infrared0)   \n246 (direction Star1)   \n247 (direction Star4)   \n248 (direction Star0)   \n249 (direction GroundStation3 )   \n250 (direction Star2)   \n251 (direction Star5)   \n252 (direction Planet6)   \n253 (direction Phenomenon7)   \n254 (direction Star8)   \n255 (direction Phenomenon9)   \n256 (direction Star10)   \n257 (direction Star11)   \n258 (direction Star12)   \n259 (direction Planet13)   \n260 (direction Planet14)   \n261 (direction Phenomenon15)   \n262 (direction Planet16)   \n263 (direction Star17)   \n264 (direction Star18)   \n265 (direction Planet19)   \n266 )   \n267 (: goal (and   \n268 (pointing satellite0 Phenomenon9)   \n269 (pointing satellite1 Star4)   \n270 (pointing satellite4 Star11)   \n271 (have_image Star5 image3)   \n272 (have_image Planet6 infrared1)   \n273 (have_image Phenomenon7 infrared1)   \n274 (have_image Star8 image3)   \n275 (have_image Star10 thermograph2)   \n276 (have_image Star11 infrared1)   \n277 (have_image Planet13 spectrograph4)   \n278 (have_image Planet14 thermograph2)   \n279 (have_image Phenomenon15 infrared0)   \n280 (have_image Planet16 image3)   \n281 (have_image Star17 infrared0)   \n282 ))   \n283 )   \n284   \n285 A:   \n286 \u2018\u2018\u2018markdown   \n287 You are operating a constellation of satellites tasked with   \ntaking images of various targets in different modes.   \n288 Initially:   \n289 There are five satellites (satellite0 to satellite4) with   \ncorresponding instruments (instrument0 to instrument8). ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "290 Each instrument supports specific modes and has a calibration target:   \n291 - Instrument0 supports spectrograph4 and targets Star0.   \n292 Instrument1 supports infrared0 and infrared1 , targeting GroundStation3 .   \n293 Instrument2 supports infrared1 and infrared0 , targeting Star2.   \n294 Instrument3 supports spectrograph4 , infrared1 , and thermograph2 , targeting Star0.   \n295 Instrument4 supports infrared1 , image3 , and infrared0 , targeting Star2.   \n296 Instrument5 supports thermograph2 and spectrograph4 , targeting Star0.   \n297 Instrument6 supports infrared0 , targeting GroundStation3.   \n298 Instrument7 supports image3 , targeting Star2.   \n299 Instrument8 supports infrared0 , spectrograph4 , and infrared1 , targeting Star2.   \n300 Instruments are on board their respective satellites , and all satellites have power available.   \n301 Satellites are pointing at various directions:   \n302 Satellite0 is pointing at Star8.   \n303 - Satellite1 is pointing at GroundStation3.   \n304 - Satellite2 is pointing at Star4.   \n305 - Satellite3 is pointing at Phenomenon9.   \n306 Satellite4 is pointing at Phenomenon9.   \n307 There are various modes (thermograph2 , image3 , infrared1 , spectrograph4 , infrared0) and directions (Star1 to Star18 , GroundStation3 , Planet6 , Phenomenon7 , Phenomenon9 , Planet13 , Planet14 , Phenomenon15 , Planet16 , Planet19).   \n308 Your goal is to:   \n309 - Point satellite0 at Phenomenon9.   \n310 Point satellite1 at Star4.   \n311 Point satellite4 at Star11.   \n312 Have images of the following targets in the specified modes:   \n313 - Star5 in image3 mode.   \n314 - Planet6 in infrared1 mode.   \n315 Phenomenon7 in infrared1 mode.   \n316 Star8 in image3 mode.   \n317 Star10 in thermograph2 mode.   \n318 Star11 in infrared1 mode.   \n319 Planet13 in spectrograph4 mode.   \n320 Planet14 in thermograph2 mode.   \n321 Phenomenon15 in infrared0 mode.   \n322 Planet16 in image3 mode.   \n323 Star17 in infrared0 mode.   \n324 To achieve these goals , you will need to turn the satellites to point at the correct directions , switch on and calibrate the necessary instruments , and take images using the calibrated instruments in the supported modes.   \n325 \u2018\u2018\u2018   \n326 Q:   \n327 Domain Description:   \n328 \u2018\u2018\u2018markdown   \n329 [Target Domain Natural Language Description] ", "page_idx": 39}, {"type": "text", "text": "330 ", "page_idx": 40}, {"type": "text", "text": "331 Problem PDDL: ", "page_idx": 40}, {"type": "text", "text": "332 \u2018\u2018\u2018pddl ", "page_idx": 40}, {"type": "text", "text": "333 [Target Problem PDDL Code] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "334 ", "page_idx": 40}, {"type": "text", "text": "35 A: Listing 11: Domain back-translation prompt template, with domain natural language description and problem PDDL placeholders for each target domain. ", "page_idx": 40}, {"type": "text", "text": "Your task is to generate python predicate descriptor for each environment. You are given the natural language description of the domain along with the PDDL code. 2 Q: 3 Domain Description: \u2018\u2018\u2018markdown The robot has four actions: pickup , putdown , stack , and unstack. The domain assumes a world where there are a set of blocks that can be stacked on top of each other , an arm that can hold one block at a time , and a table where blocks can be placed. The actions defined in this domain include: pickup: allows the arm to pick up a block from the table if it is clear and the arm is empty. After the pickup action , the arm will be holding the block , and the block will no longer be on the table or clear. 8 putdown: allows the arm to put down a block on the table if it is holding a block. After the putdown action , the arm will be empty , and the block will be on the table and clear. stack: allows the arm to stack a block on top of another block if the arm is holding the top block and the bottom block is clear. After the stack action , the arm will be empty , the top block will be on top of the bottom block , and the bottom block will no longer be clear. 10 unstack: allows the arm to unstack a block from on top of another block if the arm is empty and the top block is clear. After the unstack action , the arm will be holding the top block , the top block will no longer be on top of the bottom block , and the bottom block will be clear. 11 12 Domain PDDL: 13 \u2018\u2018\u2018pddl 14 (define (domain blocksworld -4ops) 15 (: requirements :strips) 16 (: predicates (clear ?x) 17 (on -table ?x) 18 (arm -empty) 19 (holding ?x) 20 (on ?x ?y)) 21 (: action pickup 22 :parameters (?ob) 23 :precondition (and (clear ?ob) (on -table ?ob) (arm -empty)) 24 :effect (and (holding ?ob) (not (clear ?ob)) (not (on -table ?ob)) 25 (not (arm -empty)))) ", "page_idx": 40}, {"type": "text", "text": "26 (: action putdown   \n27 :parameters (?ob)   \n28 :precondition (holding ?ob)   \n29 :effect (and (clear ?ob) (arm -empty) (on -table ?ob)   \n30 (not (holding ?ob))))   \n31 (: action stack   \n32 :parameters (?ob ?underob)   \n33 :precondition (and (clear ?underob) (holding ?ob))   \n34 :effect (and (arm -empty) (clear ?ob) (on ?ob ?underob)   \n35 (not (clear ?underob)) (not (holding ?ob))))   \n36 (: action unstack   \n37 :parameters (?ob ?underob)   \n38 :precondition (and (on ?ob ?underob) (clear ?ob) (arm -empty))   \n39 :effect (and (holding ?ob) (clear ?underob)   \n40 (not (on ?ob ?underob)) (not (clear ?ob)) (not   \n(arm -empty)))))   \n41 \u2018\u2018\u2018   \n42 A:   \n43 \u2018\u2018\u2018python   \n44 def describe_predicate (predicate_name , predicate_args):   \n45 \"\"\"   \n46 Predicates:   \n47 (clear ?x)   \n48 (on -table ?x)   \n49 (arm -empty)   \n50 (holding ?x)   \n51 (on ?x ?y)   \n52 :param predicate_name : str   \n53 :param predicate_args : Tuple[str]   \n54 :return: Tuple[str , str] (positive , negative)   \n55 \"\"\"   \n56 # (clear ?x)   \n57 if predicate_name $==$ \"clear \":   \n58 $\\begin{array}{r l}{(\\textbf{x},)}&{{}=}\\end{array}$ predicate_args   \n59 return f\"Block {x} is clear.\", f\"Block {x} is not   \nclear .\"   \n60 # (on -table $?\\,\\mathtt{x}$ )   \n61 elif predicate_name $==$ \"on -table \":   \n62 $\\begin{array}{r l}{(\\textbf{x},)}&{{}=}\\end{array}$ predicate_args   \n63 return f\"Block {x} is on the table .\", f\"Block {x} is   \nnot on the table .\"   \n64 # (arm -empty)   \n65 elif predicate_name $==$ \"arm -empty \":   \n66 return \"Arm is empty .\", \"Arm is not empty .\"   \n67 # (holding $?\\,\\mathtt{x}$ )   \n68 elif predicate_name $==$ \"holding \":   \n69 $\\begin{array}{r l}{(\\textbf{x},)}&{{}=}\\end{array}$ predicate_args   \n70 return f\"Arm is holding block {x}.\", f\"Arm is not   \nholding block {x}.\"   \n71 # (on ?x ?y)   \n72 elif predicate_name $==$ \"on\":   \n73 (x, y) $=$ predicate_args   \n74 return f\"Block {x} is on block {y}.\", f\"Block {x} is   \nnot on block {y}.\"   \n75 else:   \n76 raise ValueError(f\"Unknown predicate:   \n{predicate_name }\")   \n77   \n78 Q:   \n79 Domain Description:   \n80 \u2018\u2018\u2018markdown   \n81 [Target Domain Natural Language Description]   \n82   \n83 Domain PDDL:   \n84 \u2018\u2018\u2018pddl   \n85 [Target Domain PDDL Code]   \n86   \n87 A: ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: The claims are supported by experiments. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 43}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Justification: In the conclusion section, we discuss limitations of our work ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 43}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: Our paper does not include theoretical results. Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 44}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 44}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Justification: We describe our method in Algorithm 1, and Appendix B. The code will be made publicly available upon publication. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "page_idx": 44}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 44}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 44}, {"type": "text", "text": "Answer: [No] ", "page_idx": 45}, {"type": "text", "text": "Justification: The code will be made publicly available upon publication. We explain our method in Algorithm 1. For the data, the PDDL flies are publicly available, and we provide examples on how to obtain natural language descriptions. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 45}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: Our method uses pre-trained large language models, and we provide examples in Algorithm 1 on how we prompt the LLMs. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 45}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 45}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 45}, {"type": "text", "text": "Justification: Figure 2b and Figure 4 show standard error. Table 2 does not contain any confidence intervals as the experiments become computationally expensive. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 45}, {"type": "text", "text": "", "page_idx": 46}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: In the experiments section, we reported the number of tokens used to create Table 2. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 46}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 46}, {"type": "text", "text": "Justification: Yes ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 46}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: The setup used in our work does not have direct societal impact as it is centered around generating PDDL code. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 46}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 47}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 47}, {"type": "text", "text": "Justification: Our work is a fundamental work and poses no such risk ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 47}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 47}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Justification: We have cited the sources from which we use library, data, and code. Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 47}, {"type": "text", "text": "", "page_idx": 48}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: The code will be made publicly available upon publication ", "page_idx": 48}, {"type": "text", "text": "Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 48}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 48}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 48}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 48}]