[{"figure_path": "WcIeEtY3AG/figures/figures_1_1.jpg", "caption": "Figure 1: ANN-MoE and Spiking Experts Mixture Mechanism (SEMM). N denotes the length of image patches.", "description": "This figure compares the architecture of the traditional ANN-MoE (Artificial Neural Network Mixture of Experts) with the proposed SEMM (Spiking Experts Mixture Mechanism).  ANN-MoE uses softmax and TopK for routing and sparsification, while SEMM uses a spiking router and element-wise operations.  The core difference lies in how experts' weights are selected.  ANN-MoE employs softmax and then TopK for selection, whereas SEMM uses a spiking router that dynamically adapts to the input and performs element-wise addition of spiking sequences from experts.", "section": "1 Introduction"}, {"figure_path": "WcIeEtY3AG/figures/figures_2_1.jpg", "caption": "Figure 2: (a) The overview of Spiking Transformer. (b) The Experts Mixture Spiking Attention (EMSA). (c) The Experts Mixture Spiking Perceptron (EMSP). EMSA and EMSP can directly replace SSA and MLP in (a).", "description": "This figure illustrates the architecture of the Spiking Transformer and its components:  (a) shows the overall Spiking Transformer architecture, highlighting the Patch Splitting (PS), Relative Position Embedding (RPE), Spiking Self-Attention (SSA), Multi-Layer Perceptron (MLP), and Classification Head (CH). (b) and (c) illustrate the proposed modifications, EMSA and EMSP respectively, which are designed to replace SSA and MLP in the base Spiking Transformer. EMSA uses a Spiking Router and multiple spiking attention experts, while EMSP integrates a channel-wise mixture of experts within the MLP structure. Both utilize the Spiking Experts Mixture Mechanism (SEMM) for spike-driven conditional computation.", "section": "3 Methodology"}, {"figure_path": "WcIeEtY3AG/figures/figures_5_1.jpg", "caption": "Figure 8: Accuracy with different experts number.", "description": "This figure shows the accuracy results for different numbers of experts used in EMSA (Experts Mixture Spiking Attention) and EMSP (Experts Mixture Spiking Perceptron) across CIFAR10 and CIFAR10-DVS datasets. It demonstrates the impact of varying the number of experts on model performance. The baseline and three different spiking transformer models are included in the plot, demonstrating the stable and consistent performance improvements achieved by SEMM (Spiking Experts Mixture Mechanism) across different expert numbers.", "section": "4.2 Results on various Datasets"}, {"figure_path": "WcIeEtY3AG/figures/figures_6_1.jpg", "caption": "Figure 4: Ablation study on the EMSA and EMSP router.", "description": "This figure presents an ablation study on the effectiveness of the router components within the Experts Mixture Spiking Attention (EMSA) and Experts Mixture Spiking Perceptron (EMSP) modules.  It shows the accuracy results on CIFAR10 and CIFAR100 datasets when comparing the baseline models against versions with the router removed for both EMSA and EMSP, as well as the complete EMSA and EMSP models.  The results demonstrate that the router is crucial for achieving high performance in both modules.", "section": "4.1 Sparse Conditional Computation Analysis"}, {"figure_path": "WcIeEtY3AG/figures/figures_6_2.jpg", "caption": "Figure 8: Accuracy with different experts number.", "description": "This figure shows the accuracy of three different spiking transformer models (Spikformer, Spike-driven Transformer, and Spikingformer) on CIFAR10 and CIFAR100 datasets with varying numbers of experts in the experts mixture mechanism.  The x-axis represents the number of experts (2, 4, 6, 8, 12), and the y-axis represents the accuracy in percentage.  The results demonstrate the impact of the number of experts on the overall model performance for each architecture. The figure provides a visual comparison, allowing for easy assessment of the effects of the expert mixture mechanism on classification accuracy.", "section": "4.3 Ablation Study and Hyperparameter Sensitivity"}, {"figure_path": "WcIeEtY3AG/figures/figures_7_1.jpg", "caption": "Figure 5: Visualization of routers as masks. The mask position (black) indicates router of 0 here and the background image is the same for each subplot. We show the dynamic sparsity of spiking router for different experts (horizontal direction) and time steps (vertical direction).", "description": "This figure visualizes the dynamic sparsity of the spiking router in SEMM.  Each subplot shows a different image patch processed by the model, and the black pixels indicate that the router has a value of 0, effectively masking that portion of the input for that particular expert at that particular time step. The figure demonstrates how the spiking router dynamically allocates computation across experts and timesteps, adapting to the image content.", "section": "4.1 Sparse Conditional Computation Analysis"}, {"figure_path": "WcIeEtY3AG/figures/figures_7_2.jpg", "caption": "Figure 6: Average spiking rate of different kinds of images in the ImageNet validation set in the spatial-temporal dimension. The height of the cube is the time step. (a) Japanese spaniel. (b) Volcano.", "description": "This figure visualizes the average spiking rate (ASR) of the spiking router in the spatial-temporal dimension for different image categories from ImageNet.  It demonstrates the dynamic and data-dependent nature of the router's conditional computation.  The height of each cube represents the time dimension, and the color intensity shows the ASR.  The subfigures (a) and (b) show the ASR for different images (Japanese spaniel and volcano respectively). The figure supports the claim that SEMM realizes sparse conditional computation, adapting its resource allocation according to the input data.", "section": "4.1 Sparse Conditional Computation Analysis"}, {"figure_path": "WcIeEtY3AG/figures/figures_7_3.jpg", "caption": "Figure 8: Accuracy with different experts number.", "description": "The figure shows the accuracy of three different Spiking Transformer models (Spikformer, Spike-driven Transformer, and Spikingformer) on CIFAR-10 and CIFAR-100 datasets with varying numbers of experts (4, 6, 8, 12).  It demonstrates the impact of the number of experts within the Experts Mixture Spiking Mechanism (SEMM) on the overall accuracy of the models.", "section": "4.3 Ablation Study and Hyperparameter Sensitivity"}, {"figure_path": "WcIeEtY3AG/figures/figures_16_1.jpg", "caption": "Figure 5: Visualization of routers as masks. The mask position (black) indicates router of 0 here and the background image is the same for each subplot. We show the dynamic sparsity of spiking router for different experts (horizontal direction) and time steps (vertical direction).", "description": "This figure visualizes the dynamic sparsity of the spiking router in SEMM for different experts and time steps.  Each subplot shows the router's mask (black indicates a 0 value) applied to the same image patch. The horizontal direction shows different experts, and the vertical direction displays different time steps, highlighting how the router dynamically allocates computation across experts and time.", "section": "4.1 Sparse Conditional Computation Analysis"}]