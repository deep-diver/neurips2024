---
title: "DiffCut: Catalyzing Zero-Shot Semantic Segmentation with Diffusion Features and Recursive Normalized Cut"
summary: "DiffCut, a novel unsupervised zero-shot semantic segmentation method, leverages diffusion UNet features and recursive normalized cuts to achieve state-of-the-art performance."
categories: ["AI Generated", ]
tags: ["Computer Vision", "Image Segmentation", "üè¢ Thales",]
showSummary: true
date: 2024-09-26
draft: false
---

<br>

{{< keywordList >}}
{{< keyword icon="fingerprint" >}} N0xNf9Qqmc {{< /keyword >}}
{{< keyword icon="writer" >}} Paul Couairon et el. {{< /keyword >}}
 
{{< /keywordList >}}

{{< button href="https://openreview.net/forum?id=N0xNf9Qqmc" target="_self" >}}
‚Üó arXiv
{{< /button >}}
{{< button href="https://huggingface.co/papers/N0xNf9Qqmc" target="_self" >}}
‚Üó Hugging Face
{{< /button >}}



<audio controls>
    <source src="https://ai-paper-reviewer.com/N0xNf9Qqmc/podcast.wav" type="audio/wav">
    Your browser does not support the audio element.
</audio>


### TL;DR


{{< lead >}}

Unsupervised semantic segmentation, crucial for various applications, lags behind supervised methods due to the lack of labeled data.  Existing unsupervised techniques often struggle to produce detailed and accurate segmentations. Some methods rely on extra image-text data, thus limiting their generalizability.  Others struggle with discovering arbitrary numbers of segments, limiting their suitability for diverse scenes. 

DiffCut overcomes these limitations by ingeniously employing diffusion model features and a recursive normalized cut algorithm. This approach results in high-quality segmentations without needing any labeled data or textual descriptions. The **recursive nature of the algorithm dynamically adapts to the complexity of the scene**, generating accurate segmentation maps with intricate detail.  This **significant improvement** in accuracy, alongside the **efficiency and lack of need for extra data**, marks a substantial advancement in the field.

{{< /lead >}}


#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} DiffCut achieves state-of-the-art results in unsupervised zero-shot semantic segmentation. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} The method uses diffusion UNet encoder features and a recursive normalized cut algorithm for accurate and efficient segmentation. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} DiffCut demonstrates the remarkable semantic knowledge embedded within diffusion UNet encoders, highlighting their potential as foundation vision encoders for downstream tasks. {{< /typeit >}}
{{< /alert >}}

#### Why does it matter?
This paper is important because it significantly advances unsupervised zero-shot semantic segmentation, a challenging problem in computer vision.  **DiffCut's superior performance** over existing methods opens up new avenues for research and development in foundation models for downstream tasks. The **method's efficiency and robustness** make it highly relevant to current trends in sustainable AI.

------
#### Visual Insights



![](https://ai-paper-reviewer.com/N0xNf9Qqmc/figures_0_1.jpg)

> üîº This figure shows a comparison of unsupervised zero-shot image segmentation results between the proposed DiffCut method and the DiffSeg method.  Three example images are shown with their corresponding segmentation maps generated by each method.  DiffCut demonstrates improved accuracy and detail in its segmentation compared to DiffSeg, more accurately capturing semantic concepts within the images.
> <details>
> <summary>read the caption</summary>
> Figure 1: Unsupervised zero-shot image segmentation. Our DiffCut method exploits features from a diffusion UNet encoder in a graph-based recursive partitioning algorithm. Compared to DiffSeg [1], DiffCut provides finely detailed segmentation maps that more closely align with semantic concepts.
> </details>





![](https://ai-paper-reviewer.com/N0xNf9Qqmc/tables_5_1.jpg)

> üîº This table presents the mean Intersection over Union (mIoU) scores achieved by various unsupervised semantic segmentation methods on six benchmark datasets (Pascal VOC, Pascal Context, COCO-Object, COCO-Stuff-27, Cityscapes, and ADE20K).  The methods are categorized into those requiring extra training data (Extra-Training) and those that are training-free (Training-free).  The table highlights DiffCut's superior performance compared to existing state-of-the-art methods.  It also indicates whether a method relies on language dependency (LD), auxiliary images (AX), or unsupervised adaptation (UA).
> <details>
> <summary>read the caption</summary>
> Table 1: Unsupervised segmentation results. Best method in bold, second is underlined.
> </details>





### In-depth insights


#### DiffCut's novelty
DiffCut's novelty lies in its **two-pronged approach** to unsupervised zero-shot semantic segmentation. First, it leverages the **semantic richness** embedded within the final self-attention block of a diffusion UNet encoder, bypassing the need for text prompts or cross-attention modules, which are computationally expensive and prone to inaccuracies.  Second, it employs a **recursive normalized cut algorithm**, allowing for adaptive segmentation granularity based on image content, unlike previous methods limited to a pre-defined number of segments. This recursive approach coupled with the unique utilization of diffusion features makes DiffCut exceptionally efficient and robust, leading to more accurate and finely detailed segmentation maps.  The combination of these two innovative aspects represents a significant advancement over existing unsupervised semantic segmentation techniques.

#### Recursive NCut
The heading 'Recursive NCut' suggests a method for image segmentation that leverages the Normalized Cut algorithm in a recursive manner.  This recursive application likely addresses the limitations of standard Normalized Cut, which often struggles with complex images containing multiple objects.  **Recursively applying NCut allows for a hierarchical segmentation process, starting with a coarse partitioning and progressively refining the segments to achieve finer granularity.** This hierarchical approach is particularly valuable for zero-shot semantic segmentation, where prior knowledge about object classes is absent.  The method's ability to adapt the granularity of segmentation suggests robustness across diverse image content.  **A key benefit is the capacity to automatically determine the number of segments rather than relying on pre-defined parameters**, making it well-suited for scenarios with unpredictable visual complexity.

#### Zero-shot seg
Zero-shot semantic segmentation, a challenging task in computer vision, aims to segment images into meaningful regions without any prior training on the specific classes present in those images.  This contrasts with traditional supervised methods that require large labeled datasets.  **A key focus in zero-shot seg is leveraging knowledge from pre-trained models**, such as diffusion models or vision transformers, to generalize to unseen data.  The effectiveness of this approach depends heavily on the richness of the learned representations and the ability of the method to effectively transfer that knowledge.   **Graph-based methods and recursive partitioning algorithms are commonly employed** to handle the unsupervised nature of the problem.  However, zero-shot seg faces the inherent limitation that without class-specific information, segmentation accuracy might not always perfectly align with human semantic understanding, particularly in complex scenes.  **Future improvements in zero-shot seg likely involve developing even more powerful and robust foundation models**  as well as creating more sophisticated algorithms that can better handle ambiguity and noisy inputs.

#### Feature coherence
The concept of 'feature coherence' in the context of zero-shot semantic segmentation is crucial.  It speaks to the **internal consistency and semantic meaningfulness** of the features extracted from a foundation model, such as a diffusion UNet. High feature coherence implies that features representing similar semantic concepts (e.g., different instances of 'dog') will cluster together in feature space, leading to improved segmentation accuracy.  **The paper likely investigates this by measuring the alignment of patch-level features**, comparing the performance of different vision encoders.  Strong feature coherence is essential for the success of the recursive normalized cut algorithm; well-clustered features make the task of partitioning the image graph easier and more accurate.  **A lack of coherence could lead to fragmented or semantically incorrect segmentations.** The analysis of feature coherence, therefore, provides valuable insights into the quality of the chosen foundation model and its suitability for zero-shot segmentation.  It also underscores the importance of selecting an appropriate foundation model with inherent semantic understanding for downstream tasks like semantic segmentation.

#### Future work
The paper's impactful results in zero-shot semantic segmentation using diffusion features open exciting avenues for future research.  **Extending DiffCut to handle more complex scenes and challenging datasets** with diverse object interactions and occlusions is crucial.  **Investigating the influence of different diffusion model architectures** and training strategies on the performance of DiffCut could significantly improve its capabilities.  **Exploring the integration of other modalities,** such as depth or motion information, would enhance the robustness and contextual understanding of the segmentation process.  Furthermore, **developing a more efficient algorithm for recursive normalized cut** is essential to improve scalability and reduce computational cost, particularly for very high-resolution images. Finally, **analyzing the robustness of DiffCut's hyperparameters** and developing automated optimization methods is key to ensuring consistent performance across various datasets. This comprehensive approach will advance the state-of-the-art in zero-shot semantic segmentation.


### More visual insights

<details>
<summary>More on figures
</summary>


![](https://ai-paper-reviewer.com/N0xNf9Qqmc/figures_3_1.jpg)

> üîº This figure illustrates the pipeline of the DiffCut model.  First, features from the final self-attention block of a diffusion UNet encoder are extracted from an input image. These features are used to create an affinity matrix, which is then processed by a recursive normalized cut algorithm to generate a low-resolution segmentation map. Finally, a high-resolution segmentation map is generated by upsampling the low-resolution map and using a concept assignment mechanism.
> <details>
> <summary>read the caption</summary>
> Figure 2: Overview of DiffCut. 1) DiffCut takes an image as input and extracts the features of the last self-attention block of a diffusion UNet encoder. 2) These features are used to construct an affinity matrix that serves in a recursive normalized cut algorithm, which outputs a segmentation map at the latent spatial resolution. 3) A high-resolution segmentation map is produced via a concept assignment mechanism on the features upsampled at the original image size.
> </details>



![](https://ai-paper-reviewer.com/N0xNf9Qqmc/figures_6_1.jpg)

> üîº This ROC curve shows the area under the curve (AUC) scores for different vision encoders, measuring their ability to correctly identify whether pairs of image patches belong to the same semantic class.  The higher the AUC score, the better the semantic coherence of the encoder's feature representations.  The figure demonstrates that the SSD-1B UNet encoder outperforms other models, indicating its strong semantic coherence for image segmentation.
> <details>
> <summary>read the caption</summary>
> Figure 3: ROC curves revealing the semantic coherence of vision encoders.
> </details>



![](https://ai-paper-reviewer.com/N0xNf9Qqmc/figures_6_2.jpg)

> üîº This figure displays the results of an experiment designed to evaluate the semantic coherence of various vision encoders by comparing the patch-level alignment of features extracted from different models.  The experiment focuses on measuring the cosine similarity between patches representing semantically similar concepts across different images. The visualization shows heatmaps illustrating the similarity scores, providing insights into the capacity of each vision encoder to capture and represent semantic information within image patches. The stronger alignment shown by certain models highlights their ability to group semantically related visual elements.
> <details>
> <summary>read the caption</summary>
> Figure 4: Qualitative results on the semantic coherence of various vision encoders. We select a patch (red marker) associated to the dog in REF. IMAGE. Top row shows the cosine similarity heatmap between the selected patch and all patches produced by vision encoders for REF. IMAGE. Bottom row shows the heatmap between the selected patch in REF. IMAGE and all patches produced by vision encoders for TARGET. IMAGE.
> </details>



![](https://ai-paper-reviewer.com/N0xNf9Qqmc/figures_7_1.jpg)

> üîº This figure shows the sensitivity analysis of the hyperparameters Œ± (exponent value) and œÑ (threshold value) on the segmentation performance of the DiffCut method.  It demonstrates that as Œ± increases, the range of œÑ values resulting in competitive performance widens significantly, improving the robustness of DiffCut across various settings.
> <details>
> <summary>read the caption</summary>
> Figure 5: Sensitivity of DiffCut. As Œ± increases, DiffCut shows competitive results for a broad range of œÑ values.
> </details>



![](https://ai-paper-reviewer.com/N0xNf9Qqmc/figures_8_1.jpg)

> üîº This figure shows the effect of the hyperparameter œÑ on the segmentation results.  The hyperparameter œÑ controls the granularity of the segmentation. A smaller value of œÑ leads to finer-grained segmentations, while a larger value results in coarser segmentations. The figure displays segmentation results for three different values of œÑ (0.3, 0.5, and 0.7) on two example images.  In both images, increasing œÑ leads to fewer, larger segments.
> <details>
> <summary>read the caption</summary>
> Figure 6: Effect of œÑ. As œÑ corresponds to the maximum Ncut value, a larger threshold loosens the constraint on the partitioning algorithm and allows it to perform more recursive steps to uncover finer objects. It can be interpreted as the level of granularity of detected objects.
> </details>



![](https://ai-paper-reviewer.com/N0xNf9Qqmc/figures_18_1.jpg)

> üîº This figure showcases the results of unsupervised zero-shot image segmentation using three different methods: a reference image, DiffSeg, and the authors' proposed method, DiffCut.  DiffCut uses features from a diffusion UNet encoder within a recursive graph partitioning algorithm. The comparison highlights that DiffCut produces more detailed and semantically accurate segmentation maps compared to DiffSeg, aligning more closely with the actual semantic concepts in the image.
> <details>
> <summary>read the caption</summary>
> Figure 1: Unsupervised zero-shot image segmentation. Our DiffCut method exploits features from a diffusion UNet encoder in a graph-based recursive partitioning algorithm. Compared to DiffSeg [1], DiffCut provides finely detailed segmentation maps that more closely align with semantic concepts.
> </details>



![](https://ai-paper-reviewer.com/N0xNf9Qqmc/figures_19_1.jpg)

> üîº This figure shows the relationship between the amount of Gaussian noise added to the input image (controlled by the 'timestep' parameter) and the resulting mean Intersection over Union (mIoU) score on the Pascal VOC dataset.  The mIoU, a common metric for evaluating semantic segmentation, measures the accuracy of the segmentation produced.  The graph illustrates that there's an optimal level of noise; a small amount of noise leads to the best performance, demonstrating that the best semantic features are obtained with a slightly noisy input image.  Conversely, adding too much noise significantly reduces performance.
> <details>
> <summary>read the caption</summary>
> Figure 7: mIoU according to the noising timestep on Pascal VOC.
> </details>



![](https://ai-paper-reviewer.com/N0xNf9Qqmc/figures_19_2.jpg)

> üîº This figure shows the performance of DiffCut on the Pascal VOC dataset with respect to different values of the hyperparameters Œ± and œÑ.  The x-axis represents the threshold œÑ, and the y-axis represents the mean Intersection over Union (mIoU).  Different curves represent different values of the hyperparameter Œ±. The figure demonstrates that DiffCut is robust to changes in œÑ across a wide range of Œ± values, indicating that the method is relatively insensitive to the precise tuning of these hyperparameters.
> <details>
> <summary>read the caption</summary>
> Figure 8: Robustness of DiffCut on Pascal VOC.
> </details>



![](https://ai-paper-reviewer.com/N0xNf9Qqmc/figures_20_1.jpg)

> üîº This figure shows a comparison of unsupervised zero-shot image segmentation results between the proposed method, DiffCut, and a baseline method, DiffSeg. DiffCut leverages features from a diffusion UNet encoder in a recursive graph partitioning algorithm, resulting in more accurate and detailed segmentation maps that better align with semantic concepts. The figure visually demonstrates the improved performance of DiffCut by showcasing examples of image segmentation for various scenes.
> <details>
> <summary>read the caption</summary>
> Figure 1: Unsupervised zero-shot image segmentation. Our DiffCut method exploits features from a diffusion UNet encoder in a graph-based recursive partitioning algorithm. Compared to DiffSeg [1], DiffCut provides finely detailed segmentation maps that more closely align with semantic concepts.
> </details>



![](https://ai-paper-reviewer.com/N0xNf9Qqmc/figures_20_2.jpg)

> üîº This figure showcases the performance of DiffCut on unsupervised zero-shot image segmentation. It compares the segmentation maps produced by DiffCut and DiffSeg on several sample images. DiffCut, by leveraging features from a diffusion UNet encoder and a recursive normalized cut algorithm, achieves more accurate and detailed segmentation maps that better align with semantic concepts compared to DiffSeg.
> <details>
> <summary>read the caption</summary>
> Figure 1: Unsupervised zero-shot image segmentation. Our DiffCut method exploits features from a diffusion UNet encoder in a graph-based recursive partitioning algorithm. Compared to DiffSeg [1], DiffCut provides finely detailed segmentation maps that more closely align with semantic concepts.
> </details>



![](https://ai-paper-reviewer.com/N0xNf9Qqmc/figures_21_1.jpg)

> üîº This figure shows a comparison of unsupervised zero-shot image segmentation results between the proposed DiffCut method and the baseline DiffSeg method.  DiffCut uses features from a diffusion UNet encoder within a recursive normalized cut algorithm to produce segmentation maps. The figure visually demonstrates that DiffCut produces more accurate and detailed segmentations that better align with semantic concepts compared to DiffSeg.
> <details>
> <summary>read the caption</summary>
> Figure 1: Unsupervised zero-shot image segmentation. Our DiffCut method exploits features from a diffusion UNet encoder in a graph-based recursive partitioning algorithm. Compared to DiffSeg [1], DiffCut provides finely detailed segmentation maps that more closely align with semantic concepts.
> </details>



![](https://ai-paper-reviewer.com/N0xNf9Qqmc/figures_22_1.jpg)

> üîº This figure showcases the results of unsupervised zero-shot image segmentation using the proposed DiffCut method.  It compares DiffCut's performance against a previous method (DiffSeg). DiffCut utilizes features from a diffusion UNet encoder within a recursive graph partitioning algorithm. The images demonstrate that DiffCut produces finer segmentation maps that more accurately reflect the underlying semantic concepts compared to DiffSeg.
> <details>
> <summary>read the caption</summary>
> Figure 1: Unsupervised zero-shot image segmentation. Our DiffCut method exploits features from a diffusion UNet encoder in a graph-based recursive partitioning algorithm. Compared to DiffSeg [1], DiffCut provides finely detailed segmentation maps that more closely align with semantic concepts.
> </details>



</details>




<details>
<summary>More on tables
</summary>


![](https://ai-paper-reviewer.com/N0xNf9Qqmc/tables_7_1.jpg)
> üîº This ablation study compares the performance of DiffCut against two variations: DiffSeg (using a self-attention merging process) and AutoSC (using automated spectral clustering).  The table shows the mean Intersection over Union (mIoU) scores achieved by each method across six different datasets (VOC, Context, COCO-Object, COCO-Stuff-27, Cityscapes, ADE20K).  It demonstrates that DiffCut's recursive partitioning significantly outperforms both alternatives, highlighting the effectiveness of its approach.
> <details>
> <summary>read the caption</summary>
> Table 2: Ablation Study. The recursive partitioning of DiffCut yields superior results to both the self-attention merging process of DiffSeg and Automated Spectral Clustering. 
> </details>

![](https://ai-paper-reviewer.com/N0xNf9Qqmc/tables_8_1.jpg)
> üîº This table presents ablation study results to show the contribution of different hierarchical features extracted from the UNet encoder to the overall performance of DiffCut.  It shows the mIoU scores achieved on the Pascal VOC validation set when using features from different encoder layers (32x32 and 64x64 resolution). The results indicate that the features from the 32x32 resolution layer provide the best performance, suggesting that lower resolution features are sufficient for optimal semantic segmentation in this context. Combining features from multiple layers does not improve performance, and instead increases computational overhead.
> <details>
> <summary>read the caption</summary>
> Table 3: Features Contribution. Hierarchical features in E32 provide optimal performance (Pascal VOC validation set).
> </details>

![](https://ai-paper-reviewer.com/N0xNf9Qqmc/tables_8_2.jpg)
> üîº This table presents a comparison of unsupervised semantic segmentation methods on six benchmark datasets (Pascal VOC, Pascal Context, COCO-Object, COCO-Stuff-27, Cityscapes, and ADE20K).  The methods are categorized as either requiring extra training or being training-free.  The key metric is mean Intersection over Union (mIoU). DiffCut achieves the highest mIoU scores in most cases, highlighting its effectiveness compared to other state-of-the-art methods for zero-shot image segmentation.
> <details>
> <summary>read the caption</summary>
> Table 1: Unsupervised segmentation results. Best method in bold, second is underlined.
> </details>

![](https://ai-paper-reviewer.com/N0xNf9Qqmc/tables_16_1.jpg)
> üîº This table presents the ablation study results, comparing the performance of DiffCut against two variations: DiffSeg (using self-attention merging) and Automated Spectral Clustering.  It shows that DiffCut's recursive partitioning approach significantly outperforms both alternatives across various datasets (VOC, Context, COCO-Object, COCO-Stuff, Cityscapes, ADE20K), demonstrating the effectiveness of its key design choices.
> <details>
> <summary>read the caption</summary>
> Table 2: Ablation Study. The recursive partitioning of DiffCut yields superior results to both the self-attention merging process of DiffSeg and Automated Spectral Clustering.
> </details>

![](https://ai-paper-reviewer.com/N0xNf9Qqmc/tables_17_1.jpg)
> üîº This table compares the performance of DiffCut and MaskCut on six benchmark datasets (VOC, Context, COCO-Object, COCO-Stuff-27, Cityscapes, ADE20K). It demonstrates that DiffCut's recursive partitioning approach outperforms MaskCut's iterative approach.  The comparison is performed by varying the number of segments (k) detected by MaskCut (k = 3, 5, 20).  DiffCut consistently achieves better mIoU scores across all datasets and k values, highlighting its ability to dynamically adjust the number of segments based on visual content.
> <details>
> <summary>read the caption</summary>
> Table 6: Comparison with MaskCut. DiffCut recursive partitioning algorithm yields superior results than MaskCut iterative partitioning.
> </details>

![](https://ai-paper-reviewer.com/N0xNf9Qqmc/tables_17_2.jpg)
> üîº This table presents a comparison of DiffCut's performance using three different diffusion backbones: SD1.4, SSD-Vega, and SSD-1B.  It shows the mean Intersection over Union (mIoU) scores achieved on six different semantic segmentation benchmark datasets (VOC, Context, COCO-Object, COCO-Stuff-27, Cityscapes, and ADE20K). The results demonstrate DiffCut's robustness across various backbone choices, achieving competitive performance even with smaller models like SD1.4 and SSD-Vega compared to DiffSeg, a state-of-the-art baseline.
> <details>
> <summary>read the caption</summary>
> Table 7: Performance of DiffCut with alternative diffusion backbones.
> </details>

![](https://ai-paper-reviewer.com/N0xNf9Qqmc/tables_17_3.jpg)
> üîº This ablation study compares two different upsampling strategies for generating high-resolution segmentation maps from low-resolution maps produced by the recursive normalized cut algorithm. The 'Concept Assignment' strategy uses a concept assignment mechanism to assign pixel labels based on similarity to upsampled features of the segments, while the 'Nearest Upsampling' strategy simply performs nearest-neighbor upsampling. The results show that the concept assignment strategy significantly outperforms the nearest upsampling approach, achieving a mIoU score of 62.0 compared to 61.2 for nearest upsampling.  This highlights the effectiveness of semantic information preservation in the concept assignment approach.
> <details>
> <summary>read the caption</summary>
> Table 8: Mask Upsampling
> </details>

![](https://ai-paper-reviewer.com/N0xNf9Qqmc/tables_18_1.jpg)
> üîº This table presents the results of DiffCut where the threshold parameter œÑ is tuned using a small subset of annotated images from the target training split.  The tuning process aims to find an optimal œÑ value for each dataset (COCO-Object, COCO-Stuff-27, and Cityscapes) that balances the trade-off between segment granularity and segmentation accuracy.  The results demonstrate the impact of tuning œÑ on the performance of the method across different datasets.
> <details>
> <summary>read the caption</summary>
> Table 10: Threshold tuning. Tuned œÑ is denoted with œÑ*.
> </details>

![](https://ai-paper-reviewer.com/N0xNf9Qqmc/tables_19_1.jpg)
> üîº This table presents a comparison of unsupervised semantic segmentation methods on six benchmark datasets (Pascal VOC, Pascal Context, COCO-Object, COCO-Stuff-27, Cityscapes, and ADE20K).  The table shows the mean Intersection over Union (mIoU) achieved by each method.  The methods are categorized by whether they require language dependency (LD), auxiliary images (AX), or unsupervised training (UA).  The table highlights that DiffCut outperforms other state-of-the-art methods on this task.
> <details>
> <summary>read the caption</summary>
> Table 1: Unsupervised segmentation results. Best method in bold, second is underlined.
> </details>

</details>




### Full paper

{{< gallery >}}
<img src="https://ai-paper-reviewer.com/N0xNf9Qqmc/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/N0xNf9Qqmc/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/N0xNf9Qqmc/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/N0xNf9Qqmc/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/N0xNf9Qqmc/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/N0xNf9Qqmc/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/N0xNf9Qqmc/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/N0xNf9Qqmc/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/N0xNf9Qqmc/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/N0xNf9Qqmc/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/N0xNf9Qqmc/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/N0xNf9Qqmc/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/N0xNf9Qqmc/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/N0xNf9Qqmc/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/N0xNf9Qqmc/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/N0xNf9Qqmc/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/N0xNf9Qqmc/17.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/N0xNf9Qqmc/18.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/N0xNf9Qqmc/19.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/N0xNf9Qqmc/20.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}