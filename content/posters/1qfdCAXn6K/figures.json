[{"figure_path": "1qfdCAXn6K/figures/figures_1_1.jpg", "caption": "Figure 1: Our methodology of Wasserstein Distance (WD) based knowledge distillation. To effectively exploit rich category interrelations (a), we propose discrete WD based logit distillation (WKD-L) (b) that matches predicted distributions between the teacher and student. Besides, we introduce a feature distillation method based on continuous WD (WKD-F) (b), where we let student mimic parametric feature distributions of the teacher. In (a), features of 100 categories are displayed by the corresponding images as per their 2D embeddings obtained by t-SNE; refer to Section A.1 for details on this visualization.", "description": "This figure illustrates the proposed Wasserstein Distance (WD)-based knowledge distillation method.  It shows how the method leverages rich category interrelations. The left panel (a) visualizes real-world category interrelations in feature space using t-SNE on 100 categories. The right panel (b) illustrates the proposed methods: WKD-L (discrete WD for logit distillation) and WKD-F (continuous WD for feature distillation). WKD-L focuses on cross-category comparison of probabilities, while WKD-F uses continuous WD to transfer knowledge from intermediate layers by modeling feature distributions with Gaussians.", "section": "1 Introduction"}, {"figure_path": "1qfdCAXn6K/figures/figures_1_2.jpg", "caption": "Figure 1: Our methodology of Wasserstein Distance (WD) based knowledge distillation. To effectively exploit rich category interrelations (a), we propose discrete WD based logit distillation (WKD-L) (b) that matches predicted distributions between the teacher and student. Besides, we introduce a feature distillation method based on continuous WD (WKD-F) (b), where we let student mimic parametric feature distributions of the teacher. In (a), features of 100 categories are displayed by the corresponding images as per their 2D embeddings obtained by t-SNE; refer to Section A.1 for details on this visualization.", "description": "This figure illustrates the proposed Wasserstein Distance (WD)-based knowledge distillation methods.  Panel (a) shows how real-world categories have complex interrelationships in feature space. Panel (b) details the two proposed methods: WKD-L (logit distillation) uses discrete WD to compare probability distributions across categories, while WKD-F (feature distillation) leverages continuous WD to model and match the feature distributions of the teacher and student in intermediate layers.", "section": "1 Introduction"}, {"figure_path": "1qfdCAXn6K/figures/figures_3_1.jpg", "caption": "Figure 2: KL-Div cannot perform cross-category comparison. Compare to WD in Figure 1b (left).", "description": "This figure illustrates the limitations of Kullback-Leibler Divergence (KL-Div) in knowledge distillation. KL-Div only compares the probabilities of corresponding categories between the teacher and student models, lacking a mechanism for cross-category comparison.  The figure shows this limitation visually.  It contrasts KL-Div's category-to-category comparison (vertical lines between corresponding categories) with the cross-category comparisons enabled by Wasserstein Distance (WD), shown in Figure 1b in the paper. The figure highlights that KL-Div is a category-to-category measure, lacking a way to effectively use rich interrelations (IRs) among categories, unlike WD.", "section": "2.1 Discrete WD for Logit Distillation"}, {"figure_path": "1qfdCAXn6K/figures/figures_5_1.jpg", "caption": "Figure 3: Diagrams of WCORD/EMD+IPOT and NST/ICKD-C.", "description": "This figure compares two different approaches to knowledge distillation using Wasserstein distance. WCORD/EMD-IPOT uses discrete Wasserstein distance to match the distributions of features across instances between teacher and student models. In contrast, NST/ICKD-C uses the Frobenius norm of 2nd-moments of features for distillation, comparing distributions at an instance level. Both methods aim to transfer knowledge from intermediate layers of a deep neural network.", "section": "3 Related Works"}, {"figure_path": "1qfdCAXn6K/figures/figures_15_1.jpg", "caption": "Figure 1: Our methodology of Wasserstein Distance (WD) based knowledge distillation. To effectively exploit rich category interrelations (a), we propose discrete WD based logit distillation (WKD-L) (b) that matches predicted distributions between the teacher and student. Besides, we introduce a feature distillation method based on continuous WD (WKD-F) (b), where we let student mimic parametric feature distributions of the teacher. In (a), features of 100 categories are displayed by the corresponding images as per their 2D embeddings obtained by t-SNE; refer to Section A.1 for details on this visualization.", "description": "This figure illustrates the proposed Wasserstein Distance-based knowledge distillation method (WKD).  It highlights two key aspects: logit distillation (WKD-L) and feature distillation (WKD-F). WKD-L leverages discrete WD to compare probability distributions across categories, considering relationships between categories. WKD-F uses continuous WD with Gaussian distribution modeling to transfer knowledge from intermediate layers.  The visualization in (a) uses t-SNE to show the relationships between categories in feature space. (b) presents schematic diagrams of the methods.", "section": "1 Introduction"}, {"figure_path": "1qfdCAXn6K/figures/figures_15_2.jpg", "caption": "Figure 1: Our methodology of Wasserstein Distance (WD) based knowledge distillation. To effectively exploit rich category interrelations (a), we propose discrete WD based logit distillation (WKD-L) (b) that matches predicted distributions between the teacher and student. Besides, we introduce a feature distillation method based on continuous WD (WKD-F) (b), where we let student mimic parametric feature distributions of the teacher. In (a), features of 100 categories are displayed by the corresponding images as per their 2D embeddings obtained by t-SNE; refer to Section A.1 for details on this visualization.", "description": "This figure illustrates the proposed Wasserstein Distance (WD)-based knowledge distillation methodology.  It shows how the method leverages rich interrelations between categories for both logit distillation (WKD-L) and feature distillation (WKD-F). WKD-L uses discrete WD to compare probability distributions across categories, unlike KL-divergence which only compares corresponding categories. WKD-F uses continuous WD with parametric Gaussian modeling of feature distributions at intermediate layers.  The visualization in (a) uses t-SNE to display 2D embeddings of features from 100 categories represented by their corresponding images, showcasing the rich inter-category relationships. Part (b) shows the overall architecture of the proposed distillation methods.", "section": "1 Introduction"}, {"figure_path": "1qfdCAXn6K/figures/figures_17_1.jpg", "caption": "Figure 1: Our methodology of Wasserstein Distance (WD) based knowledge distillation. To effectively exploit rich category interrelations (a), we propose discrete WD based logit distillation (WKD-L) (b) that matches predicted distributions between the teacher and student. Besides, we introduce a feature distillation method based on continuous WD (WKD-F) (b), where we let student mimic parametric feature distributions of the teacher. In (a), features of 100 categories are displayed by the corresponding images as per their 2D embeddings obtained by t-SNE; refer to Section A.1 for details on this visualization.", "description": "This figure illustrates the proposed Wasserstein Distance (WD)-based knowledge distillation method. It shows how the method leverages rich category interrelations for logit distillation (WKD-L) using discrete WD and for feature distillation (WKD-F) using continuous WD.  The figure also visualizes real-world category interrelations in feature space using t-SNE.", "section": "1 Introduction"}, {"figure_path": "1qfdCAXn6K/figures/figures_18_1.jpg", "caption": "Figure 1: Our methodology of Wasserstein Distance (WD) based knowledge distillation. To effectively exploit rich category interrelations (a), we propose discrete WD based logit distillation (WKD-L) (b) that matches predicted distributions between the teacher and student. Besides, we introduce a feature distillation method based on continuous WD (WKD-F) (b), where we let student mimic parametric feature distributions of the teacher. In (a), features of 100 categories are displayed by the corresponding images as per their 2D embeddings obtained by t-SNE; refer to Section A.1 for details on this visualization.", "description": "This figure illustrates the proposed Wasserstein Distance (WD)-based knowledge distillation methodology.  It shows how the method leverages rich category interrelations (IRs) in a feature space.  The figure highlights two key approaches: WKD-L (logit distillation) which performs cross-category comparison using discrete WD and WKD-F (feature distillation) that uses continuous WD to transfer knowledge from intermediate layers.  It also includes a visualization of real-world category interrelations (IRs) which are used for a more accurate comparison.", "section": "1 Introduction"}, {"figure_path": "1qfdCAXn6K/figures/figures_21_1.jpg", "caption": "Figure 7: Visualization of teacher-student discrepancies for WKD-L (a) and WKD-F (b). Darker color indicates larger difference.", "description": "This figure visualizes the differences between teacher and student models using two methods: WKD-L (logit distillation) and WKD-F (feature distillation).  Panel (a) shows heatmaps representing the discrepancies in the correlation matrices of logits between the teacher and student models for two network settings (ResNet32x4 to ResNet8x4 and VGG13 to VGG8).  Lighter colors indicate higher similarity between the teacher and student, suggesting better knowledge transfer with WKD-L.  Panel (b) shows heatmaps illustrating discrepancies in feature distributions using the WD metric for the same network settings. Again, lighter colors suggest more similar distributions and better knowledge transfer with WKD-F.  The visualization demonstrates that WKD-L and WKD-F achieve more similar results to the teacher models compared to standard methods.", "section": "D Visualization"}, {"figure_path": "1qfdCAXn6K/figures/figures_21_2.jpg", "caption": "Figure 7: Visualization of teacher-student discrepancies for WKD-L (a) and WKD-F (b). Darker color indicates larger difference.", "description": "This figure visualizes the differences in correlation matrices between student and teacher logits (a) and feature distributions between student and teacher (b) for different model architectures.  In part (a), lighter colors in WKD-L compared to KD indicate that WKD-L produces correlation matrices more similar to the teacher. Part (b) shows that WKD-F demonstrates smaller discrepancies with the teacher than FitNet, suggesting it better mimics the teacher's distributions.", "section": "D Visualization"}, {"figure_path": "1qfdCAXn6K/figures/figures_22_1.jpg", "caption": "Figure 8: Visualization of different models via Grad-CAM.", "description": "This figure visualizes class activation maps (CAMs) for three example images using Grad-CAM.  It compares the CAMs generated by the teacher model, a vanilla student model (trained without distillation), a student model trained with the standard KL-divergence based KD method, a student model trained with the proposed WKD-L (logit distillation), a student model trained with the FitNet feature distillation method, and finally a student model trained with the proposed WKD-F (feature distillation) method. The purpose is to show how well each distillation method is able to transfer knowledge from the teacher, as evidenced by how similar the CAMs of the student models are to the CAMs of the teacher model.", "section": "D Visualization"}]