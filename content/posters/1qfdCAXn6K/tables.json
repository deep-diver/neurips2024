[{"figure_path": "1qfdCAXn6K/tables/tables_5_1.jpg", "caption": "Table 1: Comparison with related works.", "description": "This table compares the proposed Wasserstein Distance-based Knowledge Distillation (WKD) method with other related knowledge distillation methods.  The comparison highlights key differences in terms of the type of distribution used (discrete vs. continuous), the dissimilarity measure employed (KL divergence, Mutual Information, or Wasserstein Distance), whether cross-category interrelations are considered, and whether the method utilizes the Riemannian metric of Gaussian distributions. The table provides a concise overview of the strengths and weaknesses of different KD approaches.", "section": "Related Works"}, {"figure_path": "1qfdCAXn6K/tables/tables_6_1.jpg", "caption": "Table 2: Ablation analysis of WKD-L for image classification (Acc, %) on ImageNet.", "description": "This table presents an ablation study on the proposed WKD-L method for image classification on the ImageNet dataset. It compares the performance of WKD-L against the baseline KL-Div method and its variants, and analyzes the impact of different methods for modeling category interrelations (IRs). The results show that WKD-L consistently outperforms KL-Div, and the choice of IR modeling method significantly affects the performance of WKD-L.", "section": "4.2.1 Ablation of WKD-L"}, {"figure_path": "1qfdCAXn6K/tables/tables_6_2.jpg", "caption": "Table 2: Ablation analysis of WKD-L for image classification (Acc, %) on ImageNet.", "description": "This table presents an ablation study for the WKD-L (Wasserstein Distance based Knowledge Distillation for Logits) method on the ImageNet dataset. It compares the performance of WKD-L against KL-Div (Kullback-Leibler Divergence) based methods with and without target-non-target probability separation, showing the improvement achieved by WKD-L.  Different interrelation (IR) modeling methods using CKA (Centered Kernel Alignment) with various kernels (Linear, Polynomial, RBF) and cosine similarity with different category prototypes (classifier weights, class centroids) are also compared.", "section": "4.2.1 Ablation of WKD-L"}, {"figure_path": "1qfdCAXn6K/tables/tables_7_1.jpg", "caption": "Table 3: Ablation analysis of WKD-F for image classification (Acc, %) on ImageNet.", "description": "This table presents an ablation study on the feature distillation method WKD-F. It investigates several aspects of WKD-F, including different ways to model feature distributions (Gaussian, Laplace, exponential, etc.), different matching strategies (instance-wise vs. cross-instance), and different positions and grid schemes for feature distillation.  The results are reported as Top-1 accuracy and the change in accuracy compared to the baseline FitNet.", "section": "4.2 Ablation of WKD-F"}, {"figure_path": "1qfdCAXn6K/tables/tables_8_1.jpg", "caption": "Table 4: Image classification results (Acc, %) on ImageNet. In setting (a), the teacher (T) and student (S) are ResNet34 and ResNet18, respectively, while setting (b) consists of a teacher of ResNet50 and a student of MobileNetV1. We refer to Table 10 in Section C.4 for additional comparison to competitors with different setups.", "description": "This table presents the top-1 and top-5 accuracy results for image classification on the ImageNet dataset using different knowledge distillation methods.  Two experimental settings are shown: (a) where both teacher and student models use ResNet architectures (ResNet34 and ResNet18 respectively) and (b) where a ResNet50 teacher model is paired with a MobileNetV1 student model. The table compares the performance of various knowledge distillation techniques, including the proposed Wasserstein Distance-based methods (WKD-L and WKD-F), against several state-of-the-art techniques.  The results demonstrate the superior performance of the proposed WKD methods, particularly when combined.  Table 10 in section C.4 offers additional comparisons with competitors using varied experimental setups.", "section": "4.3 Image Classification on ImageNet"}, {"figure_path": "1qfdCAXn6K/tables/tables_8_2.jpg", "caption": "Table 5: Training latency on ImageNet.", "description": "This table presents a comparison of training latency (in milliseconds) for different knowledge distillation methods on the ImageNet dataset.  It compares the classical KD method with several variants, including methods that use Wasserstein distance (WD), and shows the impact of combining logit and feature distillation approaches. The model parameters (in millions) are also included to show the model complexity.", "section": "4 Experiments"}, {"figure_path": "1qfdCAXn6K/tables/tables_8_3.jpg", "caption": "Table 6: Image classification results (Top-1 Acc, %) on CIFAR-100 across CNNs and Transformers.", "description": "This table presents the top-1 accuracy results on the CIFAR-100 dataset for image classification using different combinations of CNNs and Transformers as teacher and student models. It compares various knowledge distillation methods, including KD, DKD, DIST, OFA, WKD-L, FitNet, CC, RKD, CRD, and WKD-F.  The results are broken down by whether the teacher is a CNN or a Transformer and the corresponding student model architecture.  The table demonstrates the performance of each method in different settings, allowing for a comparison of the effectiveness of various knowledge distillation approaches when transferring knowledge between different types of neural network architectures.", "section": "4 Experiments"}, {"figure_path": "1qfdCAXn6K/tables/tables_9_1.jpg", "caption": "Table 7: Self-KD results (Acc, %) on ImageNet with ResNet18.", "description": "This table presents the results of self-knowledge distillation (Self-KD) experiments conducted on the ImageNet dataset using the ResNet18 architecture.  It compares different methods for self-KD, including a standard training approach, several variants of knowledge distillation methods using Kullback-Leibler divergence (KL-Div), and the proposed Wasserstein Distance-based method (WKD-L). The table shows the top-1 accuracy achieved by each method, highlighting the improved performance of WKD-L compared to other methods. ", "section": "4.5 Self-Knowledge Distillation on ImageNet"}, {"figure_path": "1qfdCAXn6K/tables/tables_9_2.jpg", "caption": "Table 8: Object detection results on MS-COCO.", "description": "This table presents the results of object detection experiments on the MS-COCO dataset.  It compares various knowledge distillation methods, including both logit and feature distillation, against several baselines (KD, DKD, FitNet, FGFI, ICD, ReviewKD, and FCFD). The table shows the mean Average Precision (mAP), AP50 (average precision at 50% IoU), and AP75 (average precision at 75% IoU) for different methods and settings.  It highlights the performance improvement achieved by the proposed WKD methods, both individually and in combination.", "section": "4.6 Object Detection on MS-COCO"}, {"figure_path": "1qfdCAXn6K/tables/tables_19_1.jpg", "caption": "Table 9: Analysis on different combinations of logit and feature distillation methods.", "description": "This table presents the results of experiments combining different logit and feature distillation methods. It compares the performance of using NKD (logit distillation) with WKD-F (feature distillation), WKD-L (logit distillation) with ReviewKD (feature distillation), and WKD-L with WKD-F, both with and without target/non-target separation.  The results are measured by Top-1 and Top-5 accuracy.", "section": "C.3 More Experiments on Combination of WKD-L or WKD-F"}, {"figure_path": "1qfdCAXn6K/tables/tables_19_2.jpg", "caption": "Table 4: Image classification results (Acc, %) on ImageNet. In setting (a), the teacher (T) and student (S) are ResNet34 and ResNet18, respectively, while setting (b) consists of a teacher of ResNet50 and a student of MobileNetV1. We refer to Table 10 in Section C.4 for additional comparison to competitors with different setups.", "description": "This table presents a comparison of different knowledge distillation methods on the ImageNet dataset. Two experimental settings are used: (a) ResNet34 as the teacher and ResNet18 as the student, and (b) ResNet50 as the teacher and MobileNetV1 as the student. The table shows the top-1 and top-5 accuracy for each method, including the baseline KD, and other state-of-the-art methods. The improvement over the vanilla student is also indicated. A reference to Table 10 in Section C.4 is provided for a more detailed comparison with other methods.", "section": "4.3 Image Classification on ImageNet"}, {"figure_path": "1qfdCAXn6K/tables/tables_20_1.jpg", "caption": "Table 11: Image classification results (Top-1 Acc, %) on CIFAR-100 within CNN architectures.", "description": "This table presents the results of image classification experiments conducted on the CIFAR-100 dataset using various CNN architectures.  It compares the performance of different knowledge distillation methods (KD, DKD, DIST, NKD, WTTM, FitNet, VID, CRD, ReviewKD, CAT, WCORD, EMD+IPOT, DPK, FCFD, DiffKD, ICKD-C, and the proposed WKD-L and WKD-F methods) across different CNN architectures, showing their top-1 accuracy. Both homogeneous (teacher and student architectures are similar) and heterogeneous (teacher and student architectures are different) settings are evaluated. The table helps to understand the relative performance of various knowledge distillation techniques for image classification across diverse network architectures.", "section": "4.4 Image Classification on CIFAR-100"}, {"figure_path": "1qfdCAXn6K/tables/tables_23_1.jpg", "caption": "Table 1: Comparison with related works.", "description": "This table compares the proposed Wasserstein Distance based knowledge distillation (WKD) method with other related knowledge distillation methods.  It highlights the key differences in terms of the dissimilarity measure used (KL divergence, mutual information, Wasserstein distance), whether category interrelations are considered, and the type of distillation (logit or feature).  The table helps to position WKD within the existing literature and emphasizes its unique contributions.", "section": "3 Related Works"}]