[{"type": "text", "text": "Gaussian Graph Network: Learning Efficient and Generalizable Gaussian Representations from Multi-view Images ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Shengjun Zhang, Xin Fei, Fangfu Liu, Haixu Song, Yueqi Duan\u2217 Tsinghua University {zhangsj23, feix21}@mails.tsinghua.edu.cn, duanyueqi@tsinghua.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis performance. While conventional methods require per-scene optimization, more recently several feed-forward methods have been proposed to generate pixel-aligned Gaussian representations with a learnable network, which are generalizable to different scenes. However, these methods simply combine pixel-aligned Gaussians from multiple views as scene representations, thereby leading to artifacts and extra memory cost without fully capturing the relations of Gaussians from different images. In this paper, we propose Gaussian Graph Network (GGN) to generate efficient and generalizable Gaussian representations. Specifically, we construct Gaussian Graphs to model the relations of Gaussian groups from different views. To support message passing at Gaussian level, we reformulate the basic graph operations over Gaussian representations, enabling each Gaussian to benefit from its connected Gaussian groups with Gaussian feature fusion. Furthermore, we design a Gaussian pooling layer to aggregate various Gaussian groups for efficient representations. We conduct experiments on the large-scale RealEstate10K and ACID datasets to demonstrate the efficiency and generalization of our method. Compared to the state-of-the-art methods, our model uses fewer Gaussians and achieves better image quality with higher rendering speed. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Novel view synthesis is a fundamental problem in computer vision due to its widespread applications, such as virtual reality, augmented reality, robotics and so on. Remarkable progress has been made using neural implicit representations [33, 42, 43], but these methods suffer from expensive time consumption in training and rendering [28, 13, 1, 40, 65, 11, 17, 34]. Recently, 3D Gaussian Splatting (3DGS) [21] has drawn increasing attention for explicit Gaussian representations and real-time rendering performance. Beneftiing from rasterization-based rendering, 3DGS avoids dense points querying in scene space, so that it can maintain high efficiency and quality. ", "page_idx": 0}, {"type": "text", "text": "Since 3DGS relies on per-subject [21] or per-frame [31] parameter optimization, several generalizable methods [68, 6, 4, 45] are proposed to directly regress Gaussian parameters with feed-forward networks. Typically, these methods [6, 4] generate pixel-aligned Gaussians with U-Net architectures, epipolar transformers or cost volume representations for depth estimation and parameter predictions, and directly combine Gaussian groups obtained from different views as scene representations. However, such combination of Gaussians leads to superfluous representations, where the overlapped regions are covered by similar Gaussians predicted separately from multiple images. While a simple solution is to delete redundant Gaussians, it ignores the connection among Gaussian groups. As illustrated in Figure 1a, pixelSplat [4] and MVSplat [6] suffer from artifacts with several times as many Gaussians as ours, while the deletion of similar Gaussians hurts rendering quality. ", "page_idx": 0}, {"type": "image", "img_path": "2dfBpyqh0A/tmp/2c57a88adeabdd86e48157fa3bf952046d7b27c8a699a730cd859747c1f36677.jpg", "img_caption": ["Figure 1: Comparison of previous methods and ours. (a) We visualize the rendering results of various methods and report the number of Gaussians in parentheses. (b) Previous pixel-wise methods can be considered as a degraded case of Gaussian Graphs without edges. (c) We report PSNR as well as the number of Gaussians for pixelSplat [4], MVSplat [6] and GNN under different input settings. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To tackle the challenge, we propose Gaussian Graphs to model the relations of Gaussian groups from multiple views. Based on this structure, we present Gaussian Graph Network (GGN), extending conventional graph operations to Gaussian domain, so that Gaussians from different views are not independent but can learn from their neighbor groups. Precisely, we reformulate the scalar weight of an edge to a weight matrix to depict the interactions between two Gaussian groups, and introduce a Gaussian pooling strategy to aggregate Gaussians. Under this definition, previous methods [4, 6] can be considered as a degraded case of Gaussian Graphs without edges. As shown in Figure 1b, our GGN allows message passing and aggregation across Gaussians for efficiency representations. ", "page_idx": 1}, {"type": "text", "text": "We conduct extensive experiments on both indoor and outdoor datasets, including RealEstate10K [69] and ACID [26]. While the performance of previous methods declines as the number of input views increases, our method can benefit from more input views. As shown in Figure 1c, our model outperforms previous methods under different input settings with higher rendering quality and fewer Gaussian representations. Our main contributions can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose Gaussian Graphs to construct the relations of different Gaussian groups, where each node is a set of pixel-aligned Gaussians from an input view. \u2022 We introduce Gaussian Graph Network to process Gaussian Graphs by extending the graph operations to Gaussian domain, bridging the interaction and aggregation across Gaussian groups. \u2022 Experimental results illustrate that our method can generate efficient and generalizable Gaussian representations. Our model requires fewer Gaussians and achieves better rendering quality. ", "page_idx": 1}, {"type": "text", "text": "2 Related Works ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Neural Implicit Representations ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Early researches focus on capturing dense views to reconstruct scenes, while neural implicit representations have significantly advanced neural processing for 3D data and multi-view images, leading to high reconstruction and rendering quality [32, 38, 64, 43]. In particular, Neural Radiance Fields (NeRF) [33] has garnered considerable attention with a fully connected neural network to represent complex 3D scenes. Subsequently, following works have emerged to address NeRF\u2019s limitations and enhance its performance. Some studies aim to solve the long-standing problem of novel view synthesis by improving the speed and efficiency of training and inference [17, 13, 34, 40, 28, 65, 11]. Other research focuses on modeling complex geometry and view-dependent effects to reconstruct dynamic scenes [8, 22, 39, 48, 55, 25, 52, 47, 23, 51]. Additionally, some studies [50, 46, 61, 56] have worked on reconstructing large urban scenes to avoid blurred renderings without fine details, which is a challenge for NeRF-based methods due to their limited model capacity. Furthermore, other works [36, 49, 54, 59] apply NeRF to novel view synthesis with sparse input views by incorporating additional training regularizations, such as depth, correspondence, and diffusion models. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2.2 3D Gaussian Splatting ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "More recently, 3D Gaussian Splatting (3DGS) [21] has drawn significant attention in the field of computer graphics, especially in the realm of novel view synthesis. Different from the expensive volume sampling strategy in NeRF, 3DGS utilizes a much more efficient rasterization-based splatting approach to render novel views from a set of 3D Gaussian primitives. However, 3DGS may suffer from artifacts, which occur during the splatting process. Thus, several works [62, 12, 19, 24] have been proposed to enhance the quality and realness of rendered novel views. Since 3DGS requires millions of parameters to represent a single scene, resulting in significant storage demands, some researches [30, 35, 14, 10, 20] focus on reducing the memory usage to ensure real-time rendering while maintaining the rendering quality. Other works [70, 58] are proposed to reduce the amount of required images to reconstruct scene. Besides, to extend the capability of 3D Gaussians from representing static scenes to 4D scenarios, several works [53, 9, 63, 57] have been proposed to incorporate faithful dynamics which are aligned with real-world physics. ", "page_idx": 2}, {"type": "text", "text": "2.3 Generalizable Novel View Synthesis ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Optimization-based methods train a separate model for each individual scene and require dozens of images to synthesis high-quality novel views. In contrast, feed-forward models learn powerful priors from large-scale datasets, so that 3D reconstruction and view synthesis can be achieved via a single feed-forward inference. A pioneering work, pixelNeRF [66], proposes a feature-based method to employ the encoded features to render novel views. MVSNeRF [5] combines plane-swept cost volumes which is widely used in multi-view stereo with physically based volume rendering to further improve the quality of neural radiance field reconstruction. Subsequently, many researches [2, 3, 15, 18, 37, 41, 27] have developed novel view synthesis methods with generalization and decomposition abilities. Several generalizable 3D Gaussian Splatting methods have also been proposed to leverage sparse view images to reconstruct novel scenes. While Splatter Image [45] and GPS-Gaussian [68] regress pixel-aligned Gaussian parameters to reconstruct single objects or humans instead of complex scenes, pixelSplat [4] takes sparse views as input to predict Gaussian parameters by leveraging epipolar geometry and depth estimation. MVSplat [6] constructs a cost volume structure to directly predict depth from cross-view features, further improving the geometric quality. ", "page_idx": 2}, {"type": "text", "text": "However, these methods follow the paradigm of regressing pixel-aligned Gaussians and combine Gaussians from different views directly, resulting in an excessive number of Gaussians when the model processes multi-view inputs. In comparison, we construct Gaussian Graphs to model the relations of Gaussian groups. Furthermore, we introduce Gaussian Graph Network with specifically designed graph operations to ensure the interaction and aggregation across Gaussian groups. In this manner, we can obtain efficient and generalizable Gaussian representations from multi-view images. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The overall framework is illustrated in Figure 2. After predicting positions and features of Gaussian groups from input views with extracted feature maps, we build a Gaussian Graph to model the relations. Then, we process the graph with Gaussian Graph Network via our designed graph operations on Gaussian domain for information exchange and aggregation across Gaussian groups. We leverage the fused Gaussians features to predict other Gaussian parameters, including opacity $\\alpha$ , covariance matrix $\\Sigma$ and colors $c$ . ", "page_idx": 2}, {"type": "image", "img_path": "2dfBpyqh0A/tmp/f8975566580861c7b9663f8ba5dc3889f839e83aed908b42120dd9221ad3ccf8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: Overview of Gaussian Graph Network. Given multiple input images, we extract image features and predict the means and features of pixel-aligned Gaussians. Then, we construct a Gaussian Graph to model the relations between different Gaussian nodes. We introduce Gaussian Graph Network to process our Gaussian Graph. The parameter predictor generates Gaussians parameters from the output Gaussian features. ", "page_idx": 3}, {"type": "text", "text": "3.1 Building Gaussian Graphs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given $N$ input images $\\mathcal{T}\\;=\\;\\{I_{i}\\}\\;\\in\\;\\mathbb{R}^{N\\times H\\times W\\times3}$ and their corresponding camera parameters $\\mathcal{C}=\\{c_{i}\\}$ , we follow the instructions of pixelSplat [4] and MVSplat [6] to extract image features: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{F}=\\Phi_{i m a g e}(\\mathcal{T}),\\quad\\mathcal{F}=\\{F_{i}\\}\\in\\mathbb{R}^{N\\times\\frac{H}{4}\\times\\frac{W}{4}\\times C},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\Phi_{i m a g e}$ is a 2D backbone. We predict the means and features of pixel-aligned Gaussians: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mu_{i}=\\psi_{u n p r o j}\\left(\\Phi_{d e p t h}(F_{i}),c_{i}\\right)\\in\\mathbb{R}^{H W\\times3},\\quad f_{i}=\\Phi_{f e a t}(F_{i})\\in\\mathbb{R}^{H W\\times D},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\Phi_{d e p t h}$ and $\\Phi_{f e a t}$ stand for neural networks to predict depth maps and Gaussian features, and $\\psi_{u n p r o j}$ is the unprojection operation. ", "page_idx": 3}, {"type": "text", "text": "We build a Gaussian Graph $G$ with nodes $V=\\{v_{i}\\}=\\{(\\mu_{i},f_{i})\\}_{1\\leq i\\leq N}$ . To model the relations between nodes, we define its adjacency matrix $A=[a_{i j}]_{N\\times N}$ as: ", "page_idx": 3}, {"type": "equation", "text": "$$\na_{i j}=\\left\\{\\begin{array}{l l}{1,}&{i=j}\\\\ {\\psi_{o v e r l a p}(v_{i},v_{j}),}&{i\\neq j}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\psi_{o v e r l a p}(v_{i},v_{j})$ computes the overlap ratio between view $i$ and view $j$ . To limit further computational complexity, we prune the graph by preserving edges with top $n$ weights and ignore other possible edges. The degree matrix $D=[d_{i j}]_{N\\times N}$ satisfies $\\begin{array}{r}{d_{i i}=\\sum_{j}a_{i j}}\\end{array}$ . Thus, the scaled adjacency can be formulated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\tilde{A}=D^{-1}A\\quad\\mathrm{or}\\quad\\tilde{A}=D^{-\\frac{1}{2}}A D^{-\\frac{1}{2}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3.2 Gaussian Graph Network ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Linear Layers. Assuming that a conventional graph has $N$ nodes with features $\\{g_{i}\\}\\in\\mathbb{R}^{N\\times C}$ and the scaled adjacency matrix $A=\\left[\\tilde{a}_{i j}\\right]_{N\\times N}$ , the basic linear operation can be formulated as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{g}_{i}=\\sum_{j=1}^{N}\\tilde{a}_{i j}g_{j}W\\in\\mathbb{R}^{D},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $W\\in\\mathbb{R}^{C\\times D}$ is a learnable weight. Different from the vector nodes $g_{i}$ in conventional graphs, each node of our Gaussian Graph contains a set of pixel-aligned Gaussians $v_{i}=\\{\\mu_{i},f_{i}\\}$ . Therefore, we extend the scalar weight $a_{s k}$ of an edge to a matrix ${\\underline{{E}}}^{s\\to k}=[e_{i j}^{s\\to k}]_{H W\\times H W}$ isj\u2192k]HW \u00d7HW , which depicts the detailed relations at Gaussian-level between $v_{s}$ and $v_{k}$ . For the sake of simplicity, we define ", "page_idx": 3}, {"type": "equation", "text": "$$\ne_{i j}^{s\\rightarrow k}=\\binom{1,}{0,}\\ \\ \\psi_{p r o j}(\\mu_{s,i},c_{s})=\\psi_{p r o j}(\\mu_{k,j},c_{s})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Input: Multi-view images $\\mathcal{T}=\\{I_{i}\\}$ , camera parameters $\\mathcal{C}=\\{c_{i}\\}$ , the number of graph layers $h$   \nOutput: Gaussian parameters $(\\mu,\\Sigma,\\alpha,c)$ $\\bar{\\mathcal{F}}\\gets\\Phi_{i m a g e}(\\bar{\\mathcal{L}},\\bar{\\mathcal{C}})$ A $\\begin{array}{r l}&{\\quad\\times^{\\lfloor\\mathit{m a g e}\\lfloor\\bullet\\rfloor,\\vee_{J}}}\\\\ &{\\iota\\leftarrow\\psi_{\\mathit{u n p r o j}}\\left(\\Phi_{\\mathit{d e p t h}}(F_{i}),c_{i}\\right),\\quad f_{i}\\leftarrow\\Phi_{\\mathit{f e a t}}(F_{i})\\quad i=1,2,\\cdots,N}\\\\ &{\\tilde{\\mathfrak{A}}\\leftarrow D^{-1}A}\\\\ &{\\mathbf{or}\\,k\\leftarrow\\mathfrak{1}\\tan b\\,\\mathbf{do}}\\\\ &{\\quad F_{i}\\leftarrow\\sigma\\left(\\sum\\tilde{a}_{i j}E^{j\\rightarrow i}f_{j}W\\right)\\quad i=1,2,\\cdots\\,,N}\\\\ &{\\mathbf{nd}\\,\\mathbf{for}}\\\\ &{\\mu,f)\\leftarrow\\bigcup_{s=1}^{l}\\phi_{\\mathit{p o o l i n g}}(G^{s})}\\\\ &{\\tilde{\\mathfrak{A}},S,\\alpha,c\\leftarrow\\psi_{\\mathit{R}}(f),\\psi_{S}(f),\\psi_{\\alpha}(f),\\psi_{c}(f)}\\\\ &{\\Sigma\\leftarrow R S S^{\\top}R^{\\top}}\\end{array}$ f ( ", "page_idx": 4}, {"type": "text", "text": "where $\\mu_{s,i}$ is the center of the $i$ -th Gaussian in Gaussian group $v_{s}$ , and $\\psi_{p r o j}$ is the projection function which returns coordinates of the occupied pixel. In this manner, the linear operation on a Gaussian Graph can be defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{f}_{i}=\\sum_{j=1}^{N}\\tilde{a}_{i j}E^{j\\to i}f_{j}W\\in\\mathbb{R}^{D}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Pooling layers. If we have a series of connected Gaussian Graphs $\\{G^{s}\\}_{1\\leq s\\leq l}$ , where $G^{s}\\subseteq G$ , then we operate the specific pooling operation on each $G^{s}$ and combine them together: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\phi_{p o o l i n g}(G)=\\bigcup_{s=1}^{l}\\phi_{p o o l i n g}(G^{s}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "If $G^{s}$ only contains one node $v^{s}$ , $\\phi_{p o o l i n g}(G^{s})=v^{s}$ . Otherwise, we start with a random selected node $v_{i}^{s}=(\\mu_{i}^{s},f_{i}^{s})\\in G^{s}$ . Since $G^{s}$ is a connected graph, we can randomly pick up its neighbor $v_{j}^{s}$ with corresponding camera parameters $c_{j}^{s}$ . We define a function $\\psi_{s i m i l a r i t y}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\psi_{s i m i l a r i t y}\\left(v_{j,m}^{s},v_{i}^{s}\\right)=\\operatorname*{max}_{n}e_{m n}^{j\\to i},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $v_{j,m}^{s}$ is the $m$ -th Gaussian in node $v_{j}^{s}$ . We define the merge function $\\psi_{m e r g e}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\psi_{m e r g e}\\left(v_{j,m}^{s},v_{i}^{s}\\right)=\\left\\{\\emptyset,\\ \\begin{array}{l l}{\\psi_{s i m i l a r i t y}\\left(v_{j,m}^{s},v_{i}^{s}\\right)=1}\\\\ {v_{j,m}^{s},}&{\\mathrm{otherwise}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then, we merge two nodes together to get a new node ", "page_idx": 4}, {"type": "equation", "text": "$$\nv_{n e w}^{s}=\\bigcup_{m=1}^{H W}\\psi_{m e r g e}\\left(v_{j,m}^{s},v_{i}^{s}\\right)\\cup v_{i}^{s}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In this manner, we aggregate a connected graph $G^{s}$ step by step to one node. Specifically, previous methods can be considered as a degraded Gaussian Graph without edges: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\phi_{p o o l i n g}(G)=\\bigcup_{s=1}^{N}\\phi_{p o o l i n g}(G^{s})=\\bigcup_{s=1}^{N}v^{s}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\phi_{p o o l i n g}$ degenerates to simple combination of nodes. ", "page_idx": 4}, {"type": "text", "text": "Parameter prediction. After aggregating the Gaussian Graph $(\\mu,f)=\\psi_{p o o l i n g}(G)$ , we predict the rotation matrix $R$ , scale matrix $S$ , opacity $\\alpha$ and color $c$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\nR=\\phi_{R}(f),\\quad S=\\phi_{S}(f),\\quad\\alpha=\\phi_{\\alpha}(f),\\quad c=\\phi_{c}(f)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\phi_{R},\\phi_{S}$ , $\\phi_{\\alpha}$ and $\\phi_{c}$ are prediction heads. The covariance matrix can be formulated as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Sigma={\\cal R}{\\cal S}{\\cal S}^{\\top}{\\cal R}^{\\top}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Our Gaussian Graph Network is illustrated in Algorithm 1, where $\\sigma$ stands for non-linear operations, such as ReLU and GeLU. ", "page_idx": 4}, {"type": "table", "img_path": "2dfBpyqh0A/tmp/abc7a6eeed09c8e9f4f74fd957cae94a7d08466e88deb6b868b9be6e739d4377.jpg", "table_caption": ["Table 1: Quantitative comparison on RealEstate10K (re10K) [69] and ACID [26] benchmarks. We evaluate all models with 4, 8, 16 input views and report PSNR as well as the number of Gaussians (K). \u2020 Models accept multi-view inputs, and only preserve Gaussians from two input views for rendering. "], "table_footnote": [], "page_idx": 5}, {"type": "table", "img_path": "2dfBpyqh0A/tmp/b721e2eb21a8d4889bb5c8bec27c5a703eff67cbf8fbca4dfe9698d06acd3dc3.jpg", "table_caption": ["Table 2: Novel view synthesis results with two-view inputs. We report the average results of PSNR, SSIM and LPIPS on all test scenes. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We validate our proposed Gaussian Graph Network for novel view synthesis. In Section 4.1, we compare various methods on RealEstate10K [69] and ACID [26] under different input settings. In Section 4.2, we analyze the efficiency of our representations. In Section 4.3, we further validate the generalization of our Gaussian representations under cross dataset evaluation. In Section 4.4, we ablate our designed operations of Gaussian Graph Network. ", "page_idx": 5}, {"type": "text", "text": "4.1 Multi-view Scene Reconstruction and Synthesis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets. We conduct experiments on two large-scale datasets, including RealEstate10K [69] and ACID [26]. RealEstate10K dataset comprises video frames of real estate scenes, which are split into 67,477 scenes for training and 7,289 scenes for testing. ACID dataset consists of natural landscape scenes, with 11,075 training scenes and 1,972 testing scenes. For two-view inputs, our model is trained with two views as input, and subsequently tested on three target novel views for each scene. For multi-view inputs, we construct challenging subsets for RealEstate10K and ACID across a broader range of each scenario to evaluate model performance. We select 4, 8 and 16 views as reference views, and evaluate pixelSplat [4], MVSplat [6] and ours on the same target novel views. ", "page_idx": 5}, {"type": "text", "text": "Implementation details. Our model is trained with two input views for each scene on a single A6000 GPU for 300,000 iterations, utilizing the Adam optimizer. Following the instruction of previous methods [4, 6], all experiments are conducted on $256\\times256$ resolutions for fair comparison. The image backbone is initialized by the feature extractor and cost volume representations in MVSplat [6]. The number of graph layer is set to 2. The training loss is a linear combination of MSE and LPIPS [67] losses, with loss weights of 1 and 0.05. ", "page_idx": 5}, {"type": "text", "text": "Results. As shown in Table 1 and Table 2, our Gaussian Graph benefits from increasing input views, whereas both pixelSplat [4] and MVSplat [6] exhibit declines in performance. This distinction highlights the superior efficiency of our approach, which achieves more effective 3D representation with significantly fewer Gaussians compared to pixel-wise methodologies. For 4 view inputs, our method outperforms MVSplat [6] by about 4dB on PSNR with more than $2\\times$ fewer Gaussians. For more input views, the number of Gaussians in previous methods increases linearly, while our GGN only requires a small increase. Considering that previous methods suffer from the redundancy of Gaussians, we adopt these models with multi-view inputs and preserve Gaussians from two input views for rendering. Under this circumstance, pixelSplat [4] and MVSplat [6] achieve better performance than before, because the redundancy of Gaussians is alleviated to some degree. However, the lack of interaction at Gaussian level limits the quality of previous methods. In contrast, our Gaussian Graph can significantly enhance performance by leveraging additional information from extra views. Visualization results in Figure 3 also indicate that pixelSplat [4] and MVSplat [6] tend to suffer from artifacts due to duplicated and unnecessary Gaussians in local areas, which increasingly affects image quality as more input views are added. ", "page_idx": 5}, {"type": "image", "img_path": "2dfBpyqh0A/tmp/b418021ceec9e1c07e17b5307be6430de911ae6744c80dbfde0a04ee0d4d1a01.jpg", "img_caption": ["Figure 3: Visualization results on RealEstate10K [69] and ACID [26] benchmarks. We evaluate all models with 4, 8, 16 views as input and subsequently test on three target novel views. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Table 3: Inference time comparison across different views. We train our model on 2 input views and report the inference time for 4 views, 8 views, and 16 views, respectively. ", "page_idx": 7}, {"type": "image", "img_path": "2dfBpyqh0A/tmp/86674af180465fb8688e4c03ef85e95699de54d431d035752fa980c3f660ef99.jpg", "img_caption": ["Figure 4: Efficiency analysis. We report the number of Gaussians (M), rendering frames per second (FPS) and reconstruction PSNR of pixelSplat [4], MVSplat [6] and our GGN. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.2 Efficiency Analysis. ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In addition to delivering superior rendering quality, our Gaussian Graph network also enhances efficiency in 3D representations. As illustrated in Figure 4, when using 24 input images, our model outperforms MVSplat by 8.6dB on PSNR with approximately one-tenth the number of 3D Gaussians and more than three times faster rendering speed. Additionally, we compare the average inference time of our model with pixel-wise methods in Table 3. Our GGN is able to efficiently remove duplicated Gaussians and enhance Gaussian-level interactions, which allows it to achieve superior reconstruction performance with comparable inference speed to MVSplat. ", "page_idx": 7}, {"type": "text", "text": "4.3 Cross Dataset Generalization. ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To further demonstrate the generalization of Gaussian Graph Network, we conduct cross-dataset experiments. Specifically, all models are trained on RealEstate10K [69] or ACID [26] datasets, and are tested on the other dataset without any fine-tuning. Our method constructs the Gaussian Graph according to the relations of input views. As shown in Table 4, our GGN consistently outperforms pixelSplat [4] and MVSplat [6] on both benchmarks. ", "page_idx": 7}, {"type": "image", "img_path": "2dfBpyqh0A/tmp/0dee42342a21c946d396d02cc6a917a057db0e1383ddb6add97b28746f2bce11.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 5: Visualization of model performance for cross-dataset generalization on RealEstate10K [69] and ACID [26] benchmarks. ", "page_idx": 7}, {"type": "table", "img_path": "2dfBpyqh0A/tmp/2723472c53d63b29d13880fe148ea6e2d3c93a509b6f4facadf6d30c399c6cf5.jpg", "table_caption": ["Table 4: Cross-dataset performance and efficiency comparisons on RealEstate10K [69] and ACID [26] benchmarks. We assign eight views as reference and test on three target views for each scene. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "2dfBpyqh0A/tmp/e24566657e16a1dc1237e13099708395f575e92896002e4a6d011efa9ff39ea5.jpg", "table_caption": ["Table 5: Ablation study results of GGN on RealEstate10K [69] benchmarks. Each scene takes eight reference views and renders three novel views. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.4 Ablations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "To investigate the architecture design of our Gaussian Graph Network, we conduct ablation studies on RealEstate10K [69] benchmark. We first introduce a vanilla model without Gaussian Graph Network. Then, we simply adopt Gaussian Graph linear layer to model relations of Gaussian groups from multiple views. Furthermore, we simply introduce Gaussian Graph pooling layer to aggregate Gaussian groups to obtain efficient representations. Finally, we add the full Gaussian Graph Network model to both remove duplicated Gaussians and enhance Gaussian-level interactions. ", "page_idx": 8}, {"type": "text", "text": "Gaussian Graph linear layer. The Gaussian Graph linear layer serves as a pivotal feature fusion block, enabling Gaussian Graph nodes to learn from their neighbor nodes. The absence of linear layers leads to a performance drop of $0.43\\;\\mathrm{dB}$ on PSNR. ", "page_idx": 8}, {"type": "text", "text": "Gaussian Graph pooling layer. The Gaussian Graph pooling layer is important to avoid duplicate and unnecessary Gaussians, which is essential for preventing artifacts and floaters in reconstructions and speeding up the view rendering process. As shown in Table 5, the introduction of Gaussian Graph pooling layer improves the rendering quality by $4.9\\mathrm{dB}$ on PSNR and reduces the number of Gaussians to nearly one-fourth. ", "page_idx": 8}, {"type": "text", "text": "5 Conclusion and Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we propose Gaussian Graph to model the relations of Gaussians from multiple views. To process this graph, we introduce Gaussian Graph Network by extending the conventional graph operations to Gaussian representations. Our designed layers bridge the interaction and aggregation between Gaussian groups to obtain efficient and generalizable Gaussian representations. Experiments demonstrate that our method achieves better rendering quality with fewer Gaussians and higher FPS. ", "page_idx": 8}, {"type": "text", "text": "Limitations and future works. Although GGN produces compelling results and outperforms prior works, it has limitations. Because we predict pixel-aligned Gaussians for each view, the representations are sensitive to the resolution of input images. For high resolution inputs, e.g. $1024\\times1024$ , we generate over 1 million Gaussians for each view, which will significantly increase the inference and rendering time. GGN does not address generative modeling of unseen parts of the scene, where generative methods, such as diffusion models can be introduced to the framework for extensive generalization. Furthermore, GGN focuses on the color field, which does not fully capture the geometry structures of scenes. Thus, a few directions would be focused in future works. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgements. This work was supported by the National Natural Science Foundation of China under Grant 62206147. We thank David Charatan for his help on experiments in pixelSplat. ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. In ICCV, pages 5855\u20135864, 2021.   \n[2] Shengqu Cai, Anton Obukhov, Dengxin Dai, and Luc Van Gool. Pix2nerf: Unsupervised conditional p-gan for single image to neural radiance fields translation. In CVPR, pages 3981\u20133990, 2022. [3] Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and Gordon Wetzstein. pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis. In CVPR, pages 5799\u20135809, 2021. [4] David Charatan, Sizhe Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction. arXiv preprint arXiv:2312.12337, 2023. [5] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo. In ICCV, pages 14124\u201314133, 2021.   \n[6] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images. arXiv preprint arXiv:2403.14627, 2024.   \n[7] Yilun Du, Cameron Smith, Ayush Tewari, and Vincent Sitzmann. Learning to render novel views from wide-baseline stereo pairs. In CVPR, pages 4970\u20134980, 2023. [8] Yilun Du, Yinan Zhang, Hong-Xing Yu, Joshua B Tenenbaum, and Jiajun Wu. Neural radiance flow for 4d view synthesis and video processing. In ICCV, pages 14304\u201314314. IEEE Computer Society, 2021. [9] Bardienus P Duisterhof, Zhao Mandi, Yunchao Yao, Jia-Wei Liu, Mike Zheng Shou, Shuran Song, and Jeffrey Ichnowski. Md-splatting: Learning metric deformation from 4d gaussians in highly deformable scenes. arXiv preprint arXiv:2312.00583, 2023.   \n[10] Zhiwen Fan, Kevin Wang, Kairun Wen, Zehao Zhu, Dejia Xu, and Zhangyang Wang. Lightgaussian: Unbounded 3d gaussian compression with 15x reduction and ${200+}$ fps. arXiv preprint arXiv:2311.17245, 2023.   \n[11] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In CVPR, pages 5501\u20135510, 2022.   \n[12] Jian Gao, Chun Gu, Youtian Lin, Hao Zhu, Xun Cao, Li Zhang, and Yao Yao. Relightable 3d gaussian: Real-time point cloud relighting with brdf decomposition and ray tracing. arXiv preprint arXiv:2311.16043, 2023.   \n[13] Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, and Julien Valentin. Fastnerf: High-fidelity neural rendering at 200fps. In ICCV, pages 14346\u201314355, 2021.   \n[14] Sharath Girish, Kamal Gupta, and Abhinav Shrivastava. Eagles: Efficient accelerated 3d gaussians with lightweight encodings. arXiv preprint arXiv:2312.04564, 2023.   \n[15] Jiatao Gu, Lingjie Liu, Peng Wang, and Christian Theobalt. Stylenerf: A style-based 3d-aware generator for high-resolution image synthesis. arXiv preprint arXiv:2110.08985, 2021.   \n[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770\u2013778, 2016.   \n[17] Tao Hu, Shu Liu, Yilun Chen, Tiancheng Shen, and Jiaya Jia. Efficientnerf efficient neural radiance fields. In CVPR, pages 12902\u201312911, 2022.   \n[18] Wonbong Jang and Lourdes Agapito. Codenerf: Disentangled neural radiance fields for object categories. In ICCV, pages 12949\u201312958, 2021.   \n[19] Yingwenqi Jiang, Jiadong Tu, Yuan Liu, Xifeng Gao, Xiaoxiao Long, Wenping Wang, and Yuexin Ma. Gaussianshader: 3d gaussian splatting with shading functions for reflective surfaces. arXiv preprint arXiv:2311.17977, 2023.   \n[20] Kai Katsumata, Duc Minh Vo, and Hideki Nakayama. An efficient 3d gaussian representation for monocular/multi-view dynamic scenes. arXiv preprint arXiv:2311.12897, 2023.   \n[21] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00fchler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ToG, 42(4):1\u201314, 2023.   \n[22] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene flow fields for space-time view synthesis of dynamic scenes. In CVPR, pages 6498\u20136508, 2021.   \n[23] Zhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker, and Noah Snavely. Dynibar: Neural dynamic image-based rendering. In CVPR, pages 4273\u20134284, 2023.   \n[24] Zhihao Liang, Qi Zhang, Ying Feng, Ying Shan, and Kui Jia. Gs-ir: 3d gaussian splatting for inverse rendering. arXiv preprint arXiv:2311.16473, 2023.   \n[25] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH Asia, pages 1\u20139, 2022.   \n[26] Andrew Liu, Richard Tucker, Varun Jampani, Ameesh Makadia, Noah Snavely, and Angjoo Kanazawa. Infinite nature: Perpetual view generation of natural scenes from a single image. In ICCV, pages 14458\u2013 14467, 2021.   \n[27] Fangfu Liu, Chubin Zhang, Yu Zheng, and Yueqi Duan. Semantic ray: Learning a generalizable semantic field with cross-reprojection attention. In CVPR, pages 17386\u201317396, 2023.   \n[28] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields. NeurIPS, 33:15651\u201315663, 2020.   \n[29] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, pages 10012\u201310022, 2021.   \n[30] Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, and Bo Dai. Scaffold-gs: Structured 3d gaussians for view-adaptive rendering. arXiv preprint arXiv:2312.00109, 2023.   \n[31] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis. arXiv preprint arXiv:2308.09713, 2023.   \n[32] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space. In CVPR, pages 4460\u20134470, 2019.   \n[33] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99\u2013106, 2021.   \n[34] Thomas M\u00fcller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. ToG, 41(4):1\u201315, 2022.   \n[35] KL Navaneet, Kossar Pourahmadi Meibodi, Soroush Abbasi Koohpayegani, and Hamed Pirsiavash. Compact3d: Compressing gaussian splat radiance field models with vector quantization. arXiv preprint arXiv:2311.18159, 2023.   \n[36] Michael Niemeyer, Jonathan T Barron, Ben Mildenhall, Mehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. Regnerf: Regularizing neural radiance fields for view synthesis from sparse inputs. In CVPR, pages 5480\u20135490, 2022.   \n[37] Michael Niemeyer and Andreas Geiger. Giraffe: Representing scenes as compositional generative neural feature fields. In CVPR, pages 11453\u201311464, 2021.   \n[38] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In CVPR, pages 165\u2013174, 2019.   \n[39] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes. In CVPR, pages 10318\u201310327, 2021.   \n[40] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps. In ICCV, pages 14335\u201314345, 2021.   \n[41] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. Graf: Generative radiance fields for 3d-aware image synthesis. NeurIPS, 33:20154\u201320166, 2020.   \n[42] Vincent Sitzmann, Semon Rezchikov, William T. Freeman, Joshua B. Tenenbaum, and Fr\u00e9do Durand. Light field networks: Neural scene representations with single-evaluation rendering. In NeurIPS, 2021.   \n[43] Vincent Sitzmann, Michael Zollhofer, and Gordon Wetzstein. Scene representation networks: Continuous 3d-structure-aware neural scene representations. NeurIPS, 32, 2019.   \n[44] Mohammed Suhail, Carlos Esteves, Leonid Sigal, and Ameesh Makadia. Generalizable patch-based neural rendering. In ECCV, pages 156\u2013174. Springer, 2022.   \n[45] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea Vedaldi. Splatter image: Ultra-fast single-view 3d reconstruction. arXiv preprint arXiv:2312.13150, 2023.   \n[46] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul P Srinivasan, Jonathan T Barron, and Henrik Kretzschmar. Block-nerf: Scalable large scene neural view synthesis. In CVPR, pages 8248\u20138258, 2022.   \n[47] Fengrui Tian, Shaoyi Du, and Yueqi Duan. Mononerf: Learning a generalizable dynamic radiance field from monocular videos. In ICCV, pages 17903\u201317913, 2023.   \n[48] Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollh\u00f6fer, Christoph Lassner, and Christian Theobalt. Non-rigid neural radiance fields: Reconstruction and novel view synthesis of a dynamic scene from monocular video. In ICCV, pages 12959\u201312970, 2021.   \n[49] Prune Truong, Marie-Julie Rakotosaona, Fabian Manhardt, and Federico Tombari. Sparf: Neural radiance fields from sparse and noisy poses. ieee. In CVPR, volume 1, 2023.   \n[50] Haithem Turki, Deva Ramanan, and Mahadev Satyanarayanan. Mega-nerf: Scalable construction of large-scale nerfs for virtual fly-throughs. In CVPR, pages 12922\u201312931, 2022.   \n[51] Haithem Turki, Jason Y Zhang, Francesco Ferroni, and Deva Ramanan. Suds: Scalable urban dynamic scenes. In CVPR, pages 12375\u201312385, 2023.   \n[52] Liao Wang, Jiakai Zhang, Xinhang Liu, Fuqiang Zhao, Yanshun Zhang, Yingliang Zhang, Minye Wu, Jingyi Yu, and Lan Xu. Fourier plenoctrees for dynamic radiance field rendering in real-time. In CVPR, pages 13524\u201313534, 2022.   \n[53] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. arXiv preprint arXiv:2310.08528, 2023.   \n[54] Jamie Wynn and Daniyar Turmukhambetov. Diffusionerf: Regularizing neural radiance fields with denoising diffusion models. In CVPR, pages 4180\u20134189, 2023.   \n[55] Wenqi Xian, Jia-Bin Huang, Johannes Kopf, and Changil Kim. Space-time neural irradiance fields for free-viewpoint video. In CVPR, pages 9421\u20139431, 2021.   \n[56] Yuanbo Xiangli, Linning Xu, Xingang Pan, Nanxuan Zhao, Anyi Rao, Christian Theobalt, Bo Dai, and Dahua Lin. Bungeenerf: Progressive neural radiance field for extreme multi-scale scene rendering. In ECCV, pages 106\u2013122. Springer, 2022.   \n[57] Tianyi Xie, Zeshun Zong, Yuxin Qiu, Xuan Li, Yutao Feng, Yin Yang, and Chenfanfu Jiang. Physgaussian: Physics-integrated 3d gaussians for generative dynamics. arXiv preprint arXiv:2311.12198, 2023.   \n[58] Haolin Xiong, Sairisheek Muttukuru, Rishi Upadhyay, Pradyumna Chari, and Achuta Kadambi. Sparsegs: Real-time 360 {\\deg} sparse view synthesis using gaussian splatting. arXiv preprint arXiv:2312.00206, 2023.   \n[59] Haofei Xu, Anpei Chen, Yuedong Chen, Christos Sakaridis, Yulun Zhang, Marc Pollefeys, Andreas Geiger, and Fisher Yu. Murf: Multi-baseline radiance fields. arXiv preprint arXiv:2312.04565, 2023.   \n[60] Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, Fisher Yu, Dacheng Tao, and Andreas Geiger. Unifying flow, stereo and depth estimation. TPAMI, 2023.   \n[61] Linning Xu, Yuanbo Xiangli, Sida Peng, Xingang Pan, Nanxuan Zhao, Christian Theobalt, Bo Dai, and Dahua Lin. Grid-guided neural radiance fields for large urban scenes. In CVPR, pages 8296\u20138306, 2023.   \n[62] Zhiwen Yan, Weng Fei Low, Yu Chen, and Gim Hee Lee. Multi-scale 3d gaussian splatting for anti-aliased rendering. arXiv preprint arXiv:2311.17089, 2023.   \n[63] Zeyu Yang, Hongye Yang, Zijie Pan, Xiatian Zhu, and Li Zhang. Real-time photorealistic dynamic scene representation and rendering with 4d gaussian splatting. arXiv preprint arXiv:2310.10642, 2023.   \n[64] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman. Multiview neural surface reconstruction by disentangling geometry and appearance. NeurIPS, 33:2492\u2013 2502, 2020.   \n[65] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. Plenoctrees for real-time rendering of neural radiance fields. In ICCV, pages 5752\u20135761, 2021.   \n[66] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In CVPR, pages 4578\u20134587, 2021.   \n[67] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR, pages 586\u2013595, 2018.   \n[68] Shunyuan Zheng, Boyao Zhou, Ruizhi Shao, Boning Liu, Shengping Zhang, Liqiang Nie, and Yebin Liu. Gps-gaussian: Generalizable pixel-wise 3d gaussian splatting for real-time human novel view synthesis. arXiv preprint arXiv:2312.02155, 2023.   \n[69] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. arXiv preprint arXiv:1805.09817, 2018.   \n[70] Zehao Zhu, Zhiwen Fan, Yifan Jiang, and Zhangyang Wang. Fsgs: Real-time few-shot view synthesis using gaussian splatting. arXiv preprint arXiv:2312.00451, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Gaussian Graph Network ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we explain the definition of several functions in our method. ", "page_idx": 13}, {"type": "text", "text": "Unprojection $\\psi_{u n p r o j}$ and projection $\\psi_{p r o j}$ . The camera parameter $c_{i}$ includes the extrinsic matrix $M_{E}$ , the intrinsic matrix $M_{I}\\,\\,\\in\\,\\,\\mathbb{R}^{3\\times3}$ and camera origin $\\mathbf{o}$ . Assuming that $u_{I}~\\in~\\mathbb{R}^{2}$ is pixel coordinates from $I_{i}$ and $d_{d e p t h}\\,\\in\\,\\mathbb{R}$ is the estimated depth, the mean $\\mu\\,\\in\\,\\mathbb{R}^{3}$ of pixel-aligned Gaussian is: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mu=o+d_{d e p t h}u_{w},\\quad[u_{w},1]^{\\top}=M_{E}[u_{c},1]^{\\top},\\quad[u_{c},1]^{\\top}=M_{I}^{-1}[u,1]^{\\top}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The projection function $\\psi_{p r o j}$ can be considered as the inverse process of unprojection, which projects 3D coordinates to pixel coordinates. ", "page_idx": 13}, {"type": "text", "text": "Overlap $\\psi_{o v e r l a p}$ . For view $v_{i}\\,=\\,(\\mu_{i},f_{i})$ and view $v_{j}\\,=\\,(\\mu_{j},f_{j})$ , we project $\\mu_{i}$ on view $j$ and   \nowbhtearien $M$ s  oac hcyuppeirepd arpaixmeeltse ar.n Td $H W-M$ ou 0n oatc c0u iptieeradt ipoinx,e l0s..1  aWt e1 0d,e0f0in0 ei $\\begin{array}{r}{\\psi_{o v e r l a p}(v_{i},v_{j})=\\beta\\frac{M}{H W}}\\end{array}$ $\\beta$ $\\beta$   \niterations, in order to gradually enhance the influence of neighbor nodes. ", "page_idx": 13}, {"type": "text", "text": "B More Experiment Settings ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Baselines ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We choose NeRF-based methods [66, 44, 7, 59] and Gaussian-based methods [4, 6] as our baselines. We report performance of these methods according to the results in previous articles [6]. For new input settings, we employ their released checkpoints for comparison. ", "page_idx": 13}, {"type": "text", "text": "NeRF-based methods. pixelNeRF [66] is trained for 500,000 iterations with a batch size of 12. The hyperparameters are set to the same as previous experiments in the original paper. The near and far planes is set to be 0.1 and 10.0. GPNR [44] is trained for 250,000 iterations with a batch size of 4,098. The learning rate is set to $10^{-4}$ . AttnRend [7] is trained for 300,000 iterations with a batch size of 32. After 150,000 iterations, LPIPS loss is added to the total loss for further training. ", "page_idx": 13}, {"type": "text", "text": "Gaussian-based methods. pixelSplat [4] uses a pre-trained ResNet-50 and a ViT-B/8 vision transformer for image feature extraction. Is is trained for 300,000 steps using a batch size of 7 with an MSE loss for the first 150,000 iterations and supplement it with an LPIPS loss with weight 0.05 starting at 150,000 steps. pixelSplat predicts 3 Gaussians for each pixel and divides each Gaussian\u2019s opacity value by 3. MVSplat [6] initializes the image backbone with the UniMatch [60] pre-trained weight, and is trained for 300,000 iterations with a batch size of 14. ", "page_idx": 13}, {"type": "text", "text": "B.2 Training Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The image backbone $\\Phi_{i m a g e}$ consists of a ResNet [16] for per-image feature extraction and a Swin Transformer [29] for cross-view interaction. For $\\Phi_{d e p t h}$ , we adopt cost volume representations for depth estimation and a 2D UNet for depth refinement, following the instructions of MVSplat [6]. For $\\Phi_{f e a t}$ , we employ a simple CNN network and upsampling operations. ", "page_idx": 13}, {"type": "table", "img_path": "2dfBpyqh0A/tmp/ee083fd9ccd6bda563a62c2b3c2779534d934a02e776ea4c89aacf68de20d5dc.jpg", "table_caption": ["Table 6: Details of network architecture. ", "Table 7: Details of training settings. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "C Additional Results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We provide additional quantitative results and visualization results. As illustrated in Table 8 and Table 9, our GGN outperforms previous methods on all metrics with high FPS and fewer Gaussians. ", "page_idx": 13}, {"type": "text", "text": "Visualization results draw the same conclusion that our method generates efficient and generalizable Gaussian representations. ", "page_idx": 14}, {"type": "text", "text": "D Societal Impact ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Our method focuses on generalizable novel view synthesis which can be used for application ranging from virtual reality to robotics. However, it can also have potential negative societal impact. Our method relies on large amounts of data and with large computational resources, which potentially has a negative impact on global climate change. What\u2019s more, accurate rendering of a scene may raise privacy concerns that need to be addressed carefully. ", "page_idx": 14}, {"type": "table", "img_path": "2dfBpyqh0A/tmp/3ce4cbdc676f2394ff01cc77144f60d3747dcde3598e9fabc4961606b4bdb5bd.jpg", "table_caption": ["Table 8: Model performance and Gaussian numbers (K) of multi-view inputs on RealEstate10K [69]. \u2020 Models accept multi-view inputs, and only preserve Gaussians from two input views for rendering. "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "2dfBpyqh0A/tmp/fa78d7442f672f917ae062d49ed587d473a411827689ee0d323d152c7b062891.jpg", "table_caption": ["Table 9: Model performance and Gaussian numbers (K) of multi-view inputs on ACID [26]. \u2020 Models accept multi-view inputs, and only preserve Gaussians from two input views for rendering. "], "table_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "2dfBpyqh0A/tmp/4dc9f65e630decc771c96176dba202130392358cff4f0cc29af6f5a74e9c56d1.jpg", "img_caption": ["Figure 6: Additional visualization results on RealEstate10K [69] and ACID [26] benchmarks. We evaluate all models with 4, 8, 16 views as input and subsequently test on three target novel views. ", ""], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "2dfBpyqh0A/tmp/a94ed21782592324ea1ca498dba49620c5454c4aca604b900f6d0c2e1192a2e1.jpg", "img_caption": ["Figure 7: Additional visualization results of model performance for cross-dataset generalization on RealEstate10K [69] and ACID [26] benchmarks. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "2dfBpyqh0A/tmp/bf56d74a3d1ba50c875deedfbf0c7e6c91578afc560fef9d16068784b97d5af9.jpg", "img_caption": ["Figure 8: Visualization results of ablation study on RealEstate10K [69] and ACID [26] benchmarks. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "2dfBpyqh0A/tmp/041e3a3cc260fecde772c1c88483f786e0bcadacc99d89cca42b74713a2ecfb9.jpg", "img_caption": ["Figure 9: Visualization results of large-scale scenes from RealEstate10K and ACID benchmarks. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We provide claims made in the abstract and introduction. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: We discuss the limitations of our work in Section 5. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: We provide assumptions and proofs both in Section 3 and appendix. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: We propose a new network architecture and describe it clearly and fully in Section 3, Section 4 and appendix. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We will release our codes when we prepare it well. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We provide implementation details in both Section 4 and appendix. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [No] ", "page_idx": 20}, {"type": "text", "text": "Justification: We follow the experimental setting in previous studies, which do not include error bars in their experiments. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We report information on the computer resources in Section 4 and appendix. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: We discuss potential societal impacts in appendix D. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our paper poses no such risks. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: Our paper does not use existing asserts. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 22}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our paper does not release new asserts. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}]