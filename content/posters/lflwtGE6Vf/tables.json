[{"figure_path": "lflwtGE6Vf/tables/tables_7_1.jpg", "caption": "Table 1: Evaluation of (FL)\u00b2 compared with existing FSSL methods. We report the average accuracy(%) and standard deviation across three runs with different random seeds. (FL)\u00b2 shows significant performance improvements over existing methods across different settings. Bold indicates the best result and underline indicates the second-best result.", "description": "This table compares the performance of the proposed (FL)\u00b2 method against three other state-of-the-art Federated Semi-Supervised Learning (FSSL) methods: FedMatch, FedCon, and SemiFL.  The comparison is done across three datasets (CIFAR10, SVHN, CIFAR100) under various data distribution settings (balanced IID, unbalanced non-IID with Dir(0.1) and Dir(0.3)), and with varying amounts of labeled data at the server (10, 40, 40, 250, 100, 400).  The table reports the average accuracy and standard deviation across three independent runs for each configuration. Bold values highlight the best performance, and underlined values indicate the second-best performance for each setting. This table demonstrates (FL)\u00b2's superior performance, particularly when labeled data is scarce.", "section": "5.2 Performance comparison with FSSL algorithms"}, {"figure_path": "lflwtGE6Vf/tables/tables_7_2.jpg", "caption": "Table 2: Contribution of each component of (FL)2 on the SVHN dataset (N\u2081 = 40, balanced IID). By applying Client-specific Adaptive Thresholding (CAT) and Sharpness-Aware Consistency Regularization (SACR) to the baseline (FixMatch + FedAvg), performance is boosted. The combination of CAT and SACR further improves the accuracy. Incorporating Learning Status-Aware Aggregation (LSAA) leads to the best performance, finally achieving (FL)2. The result demonstrates the importance of each component in (FL)2.", "description": "This table shows the contribution of each component of the proposed (FL)\u00b2 algorithm on the SVHN dataset. It starts with the baseline FixMatch + FedAvg, then adds components one by one (SACR, CAT, LSAA) to show how much each component improves the accuracy. The final row shows the accuracy of the full (FL)\u00b2 algorithm, which combines all three components.", "section": "5.2 Performance comparison with FSSL algorithms"}, {"figure_path": "lflwtGE6Vf/tables/tables_14_1.jpg", "caption": "Table 3: More evaluation results of (FL)\u00b2 compared with SemiFL on Fashion-MNIST and AGNews dataset. We report the average accuracy(%) and standard deviation across three runs with different random seeds.", "description": "This table presents additional experimental results obtained by applying the proposed (FL)\u00b2 method and comparing it against SemiFL on two different datasets, namely Fashion-MNIST and AGNews.  It shows average accuracy and standard deviation over three independent runs for each method under different data distribution settings (Unbalanced Non-IID and Balanced IID). The number of labeled samples used in training also varies between the datasets.", "section": "5.2 Performance comparison with FSSL algorithms"}, {"figure_path": "lflwtGE6Vf/tables/tables_15_1.jpg", "caption": "Table 4: Hyperparameters in our experiments", "description": "This table lists the hyperparameters used in the experiments conducted in the paper.  It shows the settings used for different aspects of the training process across four different methods: FedMatch, FedCon, SemiFL, and the proposed (FL)\u00b2 method. The hyperparameters are categorized by their role: Server settings, Client settings, and Global settings.  The settings include batch size, number of epochs, optimizer type, learning rate, weight decay, momentum, and other parameters specific to each method, such as loss weight parameters for (FL)\u00b2.", "section": "5 Experiments"}]