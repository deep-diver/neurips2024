[{"type": "text", "text": "Retraction-free optimization over the Stiefel manifold with application to the LoRA fine-tuning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Optimization over the Stiefel manifold has played a significant role in various   \n2 machine learning tasks. Many existing algorithms either use the retraction operator   \n3 to keep each iterate staying on the manifold, or solve an unconstrained quadratic   \n4 penalized problem. The retraction operator in the former corresponds to orthonor  \n5 malization of matrices and can be computationally costly for large-scale matrices.   \n6 The latter approach usually equips with an unknown large penalty parameter. To   \n7 address the above issues, we propose a retraction-free and penalty parameter-free   \n8 algorithm, which lands on the manifold. A key component of the analysis is the   \n9 convex-like property of the quadratic penalty of the Stiefel manifold, which enables   \n0 us to explicitly characterize the penalty parameter. As an application, we introduce   \n11 a new algorithm, Manifold-LoRA, which employs the landing technique and a   \n12 carefully designed step size strategy to accelerate low-rank adaptation (LoRA)   \n3 in fine-tuning large language models. Numerical experiments on the benchmark   \n4 datasets demonstrate the efficiency of our proposed method. ", "page_idx": 0}, {"type": "text", "text": "15 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "16 Optimization over the Stiefel manifold has attracted considerable attention in the context of machine   \n17 learning, e.g., RNN [3], batch normalization [10], and distributionally robust optimization [8]. The   \n18 mathematical formulation of this class of problems is: ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{X\\in\\mathbb R^{d\\times r}}\\ \\ f(X)\\ \\mathrm{~subject~to~}\\ \\ X\\in\\operatorname{St}(d,r):=\\{X\\in\\mathbb R^{d\\times r}:X^{\\top}X=I_{d}\\},\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "19 where $r\\leq d$ and $f:\\mathbb{R}^{d\\times r}\\rightarrow\\mathbb{R}$ is a continuously differentiable function. The most popular methods   \n20 for solving (1) are retraction-based algorithms, which have been extensively studied in the context   \n21 of manifold optimization [2, 23, 6]. Recently, to alleviate the possible computational burden of the   \n22 retraction operator, some retraction-free methods have been developed in [19, 18, 41, 1]. The ideas   \n23 in these papers are based on a combination of the manifold geometry and a penalty function for the   \n24 manifold constraint, which involves an unknown but sufficiently large penalty parameter. For large  \n25 scale machine learning applications, retraction-free algorithms are preferred. However, designing   \n26 retraction-free algorithms with a known penalty parameter for solving (1) remains a challenge.   \n27 Another motivation for studying retraction-free methods arises from its application in the fine-tuning   \n28 of large language models (LLMs). Recently, LLMs have revolutionized the field of natural language   \n29 processing (NLP), achieving unprecedented performance across various applications [33, 32]. To   \n30 tailor pretrained LLMs for specific downstream tasks, the most common approach is full fine-tuning,   \n31 which requires prohibitively large computational resources due to the need to adapt all model weights,   \n32 hindering the deployment of large models. As a result, parameter-efficient fine-tuning (PEFT) has   \n33 gained widespread attention for requiring few trainable parameters while delivering comparable   \n34 or even superior results to full fine-tuning. This paradigm involves inserting learnable modules or   \n35 designating only a small portion of weights as trainable, keeping the main model frozen [21, 26, 44].   \n36 Among fine-tuning methods, low-rank adaptation (LoRA) [22] has become the de facto standard   \n37 among parameter-efficient fine-tuning techniques. It assumes that the change in weights lies in a   \n38 \u201clow intrinsic dimension\u201d, thereby modelling the update $\\Delta W\\in\\mathbb{R}^{d\\times m}$ by two low-rank (not greater   \n39 than a small integer $r$ ) matrices $A\\in\\mathbb{R}^{r\\times\\bar{m}}$ and $\\dot{\\boldsymbol{B}}\\in\\mathbb{R}^{d\\times r}$ , i.e., $\\Delta W\\,=\\,B A$ . Since $r\\ll d.$ , the   \n40 requirements on both storage and computation are significantly reduced. Due to its decompositional   \n41 nature, there is redundancy in the representation of $\\Delta W$ . Traditional optimization methods for LoRA   \n42 do not exploit this redundancy, which consequently undermines model performance. Instead, we   \n43 reformulate LoRA fine-tuning as an optimization problem over the product of Stiefel manifolds   \n44 and Euclidean spaces. Therefore, we propose an algorithmic framework called Manifold-LoRA to   \n45 accelerate the fine-tuning process and enhance model performance. Moreover, by exploiting projected   \n46 gradients and incorporating a parameter-free penalty, the overhead that our method incurs is relatively   \n47 negligible. Our contributions are as follows:   \n48 \u2022 We first prove the existence of explicit choice for the penalty parameter by establishing a   \n49 strong convexity-like condition of the nonconvex penalty problem associated with the Stiefel   \n50 manifold constraint. Furthermore, for the given penalty parameter, under mild conditions,   \n51 we prove that the iterates of our proposed retraction-free gradient descent method eventually   \n52 land on the Stiefel manifold and achieve the optimality of (1).   \n53 \u2022 Building upon the established landing theory of retraction-free and penalty parameter-free   \n54 method and the AdamW framework, we proposed a new method, Manifold-LoRA, which   \n55 employs a carefully designed step size strategy to accelerate the training process of fine  \n56 tuning. Compared with the conventional AdamW method, we use the penalized gradient   \n57 instead of the usual gradient, and the computational overhead is negligible.   \n58 \u2022 Numerical experiments are conducted on a wide range of NLP tasks, demonstrating the   \n59 efficiency of our algorithm. Specifically, compared to the vanilla LoRA, our Manifold-LoRA   \n60 with half the trainable parameters not only delivers fast convergence but also yields improved   \n61 generalization. In particular, Our method converges twice as fast as baseline methods on   \n62 several typical datasets, including the SQuAD 2.0 dataset and the CoLA dataset. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "63 1.1 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "64 Optimization over the Stiefel manifold. Optimization over the Stiefel manifold has attracted lots of   \n65 attention due to its broad applications. Through the use of retraction, known as the generalization of   \n66 the exponential map, the Riemannian gradient descent is proposed [2, 6, 23], where all iterates lie on   \n67 the manifold. When such retraction is computationally costly, the authors [19] develop a retraction  \n68 free algorithm based on the augmented Lagrangian method. More recently, by defining the constraint   \n69 dissolving operator and adding a sufficiently large penalty term, the authors [41] convert the manifold   \n70 constrained problem (1) into an unconstrained problem and then apply unconstrained optimization   \n71 algorithms. In [1], motivated by the convergence of the Oja\u2019s flow, a landing flow, consisting of the   \n72 projected gradient and the gradient of the penalty function, is developed to retraction-free method for   \n73 the squared Stiefel manifold, i.e., $d=r$ . All of these methods rely on an unknown penalty parameter   \n74 to ensure the convergence. This motivates us to design penalty parameter-free algorithms, which   \n75 could significantly reduce the need for tuning parameters in practical implementations.   \n76 LoRA. There are numerous variants of LoRA aiming to improve performance or reduce memory   \n77 usage. AdaLoRA [46], a well-known successor, introduces the idea of adaptively adjusting the rank   \n78 of different layers by incorporating an additional vector $\\textbf{\\textit{g}}$ to serve as the diagonal of a singular   \n79 value matrix. This approach leverages a revised sensitivity-based importance measure to decide   \n80 whether to disable entries in vector $\\textbf{\\textit{g}}$ and in matrices $A$ and $B$ . A similar work, SoRA [15],   \n81 adopts the same model architecture as AdaLoRA, but proposes a different way to update vector   \n82 $\\textbf{\\textit{g}}$ after training. This update rule is the proximal gradient of $\\mathcal{L}_{1}$ loss, acting as a post-pruning   \n83 method. Additionally, a recently emerged method called VeRA [25] significantly reduces memory   \n84 overhead while maintaining competitive performance. Based on the idea that networks with random   \n85 initialization contain subnetworks that are near-optimal or optimal [17], VeRA only uses two frozen   \n86 low-rank matrices shared by all layers, training scaling vectors unique to each layer. Although LoRA   \n87 has gained significant popularity and various variants have been developed, the potential for efficient   \n88 training through leveraging the manifold geometry to reduce redundancy has not been well-explored.   \n90 For a matrix $X\\in\\mathbb{R}^{d\\times r}$ , we use $\\|X\\|$ to denote its Frobenius norm. For a squared matrix $A\\in\\mathbb{R}^{d\\times d}$ ,   \n91 we define $\\textstyle\\operatorname{sym}(A)={\\frac{A+A^{\\top}}{2}}$ and use $\\mathrm{diag}(A)\\in\\mathbb{R}^{d}$ to denote its diagonal part. For two matrices   \n92 $X,Y\\in\\mathbb{R}^{d\\times r}$ , we use $\\begin{array}{r}{\\langle X,Y\\rangle:=\\sum_{i=1}^{d}\\sum_{j=1}^{r}X_{i j}Y_{i j}}\\end{array}$ to denote their Euclidean inner product. For a   \n93 differential function $\\boldsymbol{f}:\\mathbb{R}^{d\\times r}\\rightarrow\\boldsymbol{d}$ , we use $\\nabla f(X)$ to denote its Euclidean gradient at $X$ . ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "94 2 Retraction-free and penalty parameter-free optimization over the Stiefel 95 manifold ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "96 In this section, we focus on the design of retraction-free and penalty parameter-free algorithms for   \n97 solving problem (1). We will first present the retraction-free algorithm and then show how the penalty   \n98 parameter can be explicitly determined by characterizing the landscape of the penalty function. ", "page_idx": 2}, {"type": "text", "text": "99 2.1 Retraction-free algorithms ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "100 Inspired by the retraction-free algorithms [19, 41, 1], we consider the following retraction-free   \n101 gradient descent method for problem (1): ", "page_idx": 2}, {"type": "equation", "text": "$$\nX_{k+1}=X_{k}-\\alpha\\mathrm{grad}f(X_{k})-\\mu X_{k}(X_{k}^{\\top}X_{k}-I_{d}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "102 where $\\alpha,\\mu\\ \\ >\\ \\ 0$ are step sizes and the projected gradient $\\begin{array}{r l r}{\\operatorname{grad}f(X_{k})}&{{}:=}&{\\nabla f(X_{k})\\,\\mathrm{~-~}\\,}\\end{array}$   \n103 $X_{k}\\mathrm{sym}(X_{k}^{\\top}\\nabla f(X_{k}))$ . Note that the tangent space of $\\operatorname{St}(d,r)$ is $T_{X_{k}}\\mathrm{St}(d,r)\\,:=\\,\\{\\xi\\,\\in\\,\\mathbb{R}^{d\\times r}$ :   \n104 $X_{k}^{\\top}\\xi+\\xi^{\\top}X_{k}=0\\}.$ . Then, for $X_{k}\\in\\mathrm{St}(d,r)$ , $\\operatorname{grad}f(X_{k})$ is the projection of the Euclidean gra  \n105 dient $\\nabla f(X_{k})$ to the tangent space, i.e., $\\operatorname{grad}f(X_{k})={\\mathcal{P}}_{T_{X_{k}}\\operatorname{St}(d,r)}(\\nabla f(X_{k}))$ . Note that the term   \n106 $X_{k}(X_{k}^{\\top}X_{k}-I_{d})$ is exactly the gradient of the following quadratic penalty function ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\varphi(X):={\\frac{1}{4}}\\|X^{\\top}X-I\\|^{2}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "107 As will be shown in our theorem, the use of the projected gradient is essential for landing on the   \n108 manifold. This differs with the usual penalty method, which optimizes $f(X)+\\mu\\varphi(X)\\,$ using the   \n109 update $X_{k+1}=X_{k}-\\alpha\\nabla f(X_{k})-\\mu X_{k}(X_{k}^{\\top}X_{k}-I_{d}).$ , needs $\\mu\\to\\infty$ to guarantee the feasibility. ", "page_idx": 2}, {"type": "text", "text": "110 2.2 Explicit choice for the penalty parameter ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "111 It is known that a large penalty parameter yields better feasibility [29, Chapter 17]. To make the   \n112 iterative scheme (2) be penalty parameter-free, we need a careful investigation on the landscape of   \n113 the following optimization problem: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{X\\in\\mathbb{R}^{d\\times r}}\\ \\ \\varphi(X).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "114 It can be easily verified that problem (3) is nonconvex and its the optimal solution set is $\\operatorname{St}(d,r)$ . The   \n115 key of obtaining an explicit formula of $\\mu$ is to establish certain strong convexity-type inequality and   \n116 show the gradient descent method with step size $\\mu$ has linear convergence.   \n117 For any $X\\,\\in\\,\\mathrm{St}(d,r)$ , let us denote ${\\bar{X}}:={\\mathcal{P}}_{\\mathrm{St}(d,r)}(X)$ . Let $\\boldsymbol{X}\\,=\\,\\boldsymbol{U}\\boldsymbol{S}\\boldsymbol{V}^{\\intercal}$ be the singular value   \n118 decomposition with orthogonal matrices $U\\in\\mathbb{R}^{d\\times r}$ , $\\stackrel{\\cdot}{V}\\in\\mathbb{R}^{d\\times d}$ and diagonal matrix $S\\in\\mathbb{R}^{d\\times d}$ , then   \n119 $\\bar{X}=\\dot{U}V^{\\top}$ . Building on these notations, we demonstrate that problem (3) satisfies the restrict secant   \n120 inequality (RSI) [45], which serves as an alternative to the strong convexity in the linear convergence   \n121 analysis of gradient-type methods. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "122 Lemma 1. For any $X\\in\\mathbb{R}^{d\\times r}$ with $\\begin{array}{r}{\\|X-\\bar{X}\\|\\leq\\frac{1}{8}}\\end{array}$ , we have ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\left\\langle\\nabla\\varphi(X),X-{\\bar{X}}\\right\\rangle\\geq\\|X-{\\bar{X}}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "123 With the above RSI, we have the linear convergence of the gradient descent update for (3), i.e., ", "page_idx": 2}, {"type": "equation", "text": "$$\nX_{k+1}=X_{k}-\\mu\\nabla\\varphi(X_{k}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "124 Lemma 2. Let the sequence $\\{X_{k}\\}$ be generated by (5) with $\\begin{array}{r}{\\mu=\\frac{1}{3}}\\end{array}$ . Suppose that $\\begin{array}{r}{\\|X_{0}-\\bar{X}_{0}\\|\\leq\\frac{1}{8}}\\end{array}$ .   \n125 We have ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\|X_{k+1}-\\bar{X}_{k+1}\\|^{2}\\leq\\frac{2}{3}\\|X_{k}-\\bar{X}_{k}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "126 The proofs of Lemmas 1 and 2 can be found in Appendix B. ", "page_idx": 2}, {"type": "text", "text": "127 2.3 Landing on the Stiefel manifold ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "128 Building on the established linear convergence of gradient descent for problem (3), we are now able   \n129 to show that the iterates generated by (2) will land on the Stiefel manifold eventually, and the limiting   \n130 point is a stationary point of (1), i.e., $\\operatorname{grad}f(X_{\\infty})=0$ .   \n131 Let us start with the Lipschitz continuity of $\\operatorname{grad}f(X)$ . For any $X\\ \\in\\ {\\bar{U}}_{\\mathrm{St}(d,r)}{\\big(}{\\frac{1}{8}}{\\big)}$ , we define   \n132 $\\mathcal{P}_{T_{X}\\mathrm{St}(d,r)}(U)=U-X\\mathrm{sym}(X^{\\top}U)$ for $U\\in\\mathbb{R}^{d\\times r}$ . We first have the following quadratic upper   \n133 bound on $f$ from its twice differentiability and the compactness of $\\operatorname{St}(d,r)$ .   \n134 Lemma 3. There exists a constant $L>0$ such that for any $X,Y\\in\\operatorname{St}(d,r)$ , the following quadratic   \n135 upper bound holds: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\nf(Y)\\leq f(X)+\\langle\\operatorname{grad}f(X),Y-X\\rangle+{\\frac{L}{2}}\\|Y-X\\|^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "136 In addition, there exists a constant $\\hat{L}>0$ such that for any $X\\in\\mathrm{{St}}(d,r),Y\\in U_{\\mathcal{M}}(\\frac{1}{8}),$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\|\\mathrm{grad}f(X)-\\mathrm{grad}f(Y)\\|\\leq{\\hat{L}}\\|X-Y\\|.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "137 By the linear convergence result in Lemma 2, we have the following bound on the feasibility error.   \n138 Lemma 4. Let $\\{X_{k}\\}$ be the sequence generated by (2) with $\\begin{array}{r}{\\mu=\\frac{1}{3}}\\end{array}$ and $\\begin{array}{r}{\\|X_{0}-\\bar{X}_{0}\\|\\leq\\frac{1}{8}}\\end{array}$ . We have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\|X_{k+1}-\\bar{X}_{k+1}\\|\\leq\\frac{2}{3}\\|X_{k}-\\bar{X}_{k}\\|+\\alpha\\|\\mathrm{grad}f(X_{k})\\|.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "139 The following one-step descent lemma on $f$ is crucial in establishing the convergence. ", "page_idx": 3}, {"type": "text", "text": "140 Lemma 5. Let $\\{X_{k}\\}$ be the sequence generated by (2) with $\\begin{array}{r}{\\mu=\\frac{1}{3}}\\end{array}$ and $\\begin{array}{r}{\\|X_{0}-\\bar{X}_{0}\\|\\leq\\frac{1}{8}}\\end{array}$ . We have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\bar{X}_{k+1})-f(\\bar{X}_{k})\\leq-\\left(\\alpha-(4\\hat{L}^{2}+4L+1)\\alpha^{2}\\right)\\!\\|\\mathrm{grad}f(X_{k})\\|^{2}+\\displaystyle\\frac{1}{2}\\|X_{k+1}-\\bar{X}_{k+1}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\frac{1}{2}\\left(4\\hat{D}_{f}+16\\hat{L}^{2}+16L+3\\right)\\|X_{k}-\\bar{X}_{k}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "141 From the above lemma, the one-step descrease on $f$ is related to both the gradient norm of $f$ and the   \n142 feasibility error. In terms of convergence, we need both grad $f(X_{k})$ and $\\|\\bar{X}_{k}^{\\top}X_{k}-I\\|$ converge to 0.   \n143 The following theorem demonstrates that the retraction-free and penalty parameter-free update (2)   \n144 converges.   \n145 Theorem 1. Let $\\{X_{k}\\}$ be the sequence generated by (2) with $\\begin{array}{r}{\\mu=\\frac{1}{3}}\\end{array}$ and $\\begin{array}{r}{\\|X_{0}-\\bar{X}_{0}\\|\\leq\\frac{1}{8}}\\end{array}$ . If the   \n146 step size $\\begin{array}{r}{\\alpha<\\frac{1}{2c_{1}}}\\end{array}$ for some $c_{1}$ large enough, then we have ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{k=0,\\ldots,K}\\|\\mathrm{grad}f(X_{k})\\|^{2}\\leq\\frac{1}{K},\\quad\\operatorname*{min}_{k=0,\\ldots,K}\\|X_{k}^{\\top}X_{k}-I\\|^{2}\\leq\\frac{1}{K}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "147 The proofs of the above lemmas and theorem are presented in Appendix B. ", "page_idx": 3}, {"type": "text", "text": "148 3 Accelerate LoRA fine-tuning with landing ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "149 In this section, we will first clarify where the Stiefel manifold constraint comes from in the LoRA   \n150 fine-tuning. Then, we will apply the above developed retraction-free and penalty parameter-free   \n151 method to enhance LoRA fine-tuning. ", "page_idx": 3}, {"type": "text", "text": "152 3.1 Manifold optimization formulation of LoRA fine-tuning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "153 In neural networks, the dense layers perform matrix multiplication, and the weight matrices in these   \n154 layers usually have a full rank. However, when adapting to a specific task, pre-trained language models   \n155 have been shown to have a low intrinsic dimension, allowing them to learn efficiently even with a   \n156 random projection to a smaller subspace. One possible drawback in the current LoRA fine-tuning   \n157 framework is that the low-rank decomposition $\\Delta W$ into product $B A$ is not unique. Specifically,   \n158 for any invertible matrix $C$ , it holds that $B A\\,=\\,(B C)(\\bar{C}^{-1}A)$ . Note that $B C$ shares the same   \n159 column space with $B$ . This suggests us optimizing the subspace generated by $B$ instead of $B$ itself.   \n160 Numerous studies in the field of low-rank optimization, e.g., [7, 13, 12], investigate the manifold   \n161 geometry of the low-rank decomposition and develop efficient algorithms. However, such geometry   \n162 has not been explored in the LoRA fine-tuning.   \n163 To address such redundancy (i.e., the non-uniqueness of $B A$ representations), we regard $B$ as the basis   \n164 through the manifold constraint and $A$ as the coordinate of $\\Delta W$ under $B$ . Hence, the optimization   \n165 problem can be formulated as ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{A,B}\\quad L(B A),\\quad{\\mathrm{subject~to~}}\\quad B\\in\\operatorname{St}(d,r){\\mathrm{~or~}}B\\in\\operatorname{Ob}(d,r),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "166 where $\\operatorname{Ob}(d,r):=\\{B\\in\\mathbb{R}^{d\\times r}:\\operatorname{diag}(B^{\\top}B)={\\bf1}\\}$ . Compared to the Stiefel manifold $\\operatorname{St}(d,r)$ ,   \n167 the oblique manifold $\\operatorname{Ob}(d,r)$ necessitates that the matrix $B$ has unit norms in its columns, without   \n168 imposing requirements for orthogonality between the columns. Problem (12) is an optimization   \n169 problem over the product of manifolds and Euclidean spaces. ", "page_idx": 4}, {"type": "text", "text": "170 3.2 Manifold-LoRA ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "171 The retraction-free method is well-suited to address (12), simultaneously minimizing the loss function   \n172 $L(B A)$ and constraint violation. To control the constraint violation, we use the quadratic penalties   \n173 $R_{s}(B):=\\|B^{\\top}B-I\\|^{2}$ and $R_{o}(B):=\\|\\mathrm{diag}(B^{\\top}B)-1\\|^{2}$ for the Stiefel manifold and oblique   \n174 manifold, respectively. As shown in the landing theory in Section 2, we shall use the projected   \n175 gradient of the loss part instead of the Euclidean gradient. For the Stiefel manifold and the oblique   \n176 manifold, the respective projected gradients are ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname{grad}_{B}L(B A)=\\nabla_{B}L(B A)-B\\operatorname{sym}(B^{\\top}\\nabla_{B}L(B A))\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "177 and ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname{grad}_{B}L(B A)=\\nabla_{B}L(B A)-B\\mathrm{diag}(\\mathrm{diag}(B^{\\top}\\nabla_{B}L(B A))),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "178 where $\\mathrm{sym}(X):=(X+X^{\\top})/2$ . Thus, the gradients of our retraction-free method for $A$ and $B$ are   \n179 $\\nabla_{A}L({\\dot{B}}A)$ and $\\operatorname{grad}_{B}L(B A)\\,\\dot{+}\\,\\mu\\nabla R_{s}(B)\\,\\dot{(}\\operatorname{or}\\nabla R_{o}(B)).$   \n180 Note that $B$ and $A$ represent the basis and the coordinate of $\\Delta W$ , respectively. This results in   \n181 different magnitudes and different Lipschitz constants of their gradient function. In fact, let $X=B A$ .   \n182 It follows ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nabla_{A}L(B A)=B^{\\top}\\nabla_{X}L(X),\\quad\\nabla_{B}L(B A)=\\nabla_{X}L(X)A^{\\top}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "183 Then, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{A}L(B A_{1})-\\nabla L(B A_{2})\\|\\leq\\|B\\|_{2}L_{g}\\|A_{1}-A_{2}\\|,}\\\\ &{\\|\\nabla_{B}L(B_{1}A)-\\nabla L(B_{2}A)\\|\\leq\\|A\\|_{2}L_{g}\\|B_{1}-B_{2}\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "184 where $L_{g}$ is the Lipschitz constant of $\\nabla_{X}L(X)$ and $\\|\\cdot\\|_{2}$ represent the matrix $\\ell_{2}$ norm (i.e., the   \n185 largest singular value). Note that the step size generally should be propositional to the reciprocal of   \n186 Lipschitz constant for the gradient type algorithms [29, 5]. Hence, we schedule the learning rates for   \n187 the two matrices based on their respective $\\ell_{2}$ norms. Having prepared the above, we incorporate the   \n188 AdamW optimizer [28] with our manifold-accelerated technique to enhance the LoRA fine-tuning, as   \n189 presented in Algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "190 4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "191 In this section, we delve into the experimental results and their detailed analysis. This discussion is   \n192 structured around two principal areas: (1) the performance gain compared to other mainstream fine  \n193 tuning methods and accelerated convergence achieved through our manifold-constrained optimization   \n194 approach; (2) the convergence of matrix $B$ onto the manifold, illustrated by the heat map of B\u22a4B.   \n195 Baselines We compare our approach against several baseline methods, including full fine-tuning,   \n196 Adapter [21], BitFit [44] and LoRA [22]. The variants of the Adapter method are excluded from the   \n197 baselines, as their performance are relatively similar.   \n198 Implementation Details Our code is based on Pytorch [31], Huggingface Transformers [40] and an   \n199 open-source plug-and-play library for parameter-efficient fine-tuning opendelta [24]. The bottleneck   \n200 dimension for the Adapter is set to 16 or 32, ensuring that the number of trainable parameters aligns   \n201 closely with that of the LoRA method and the new layers are inserted into the attention layer and   \n202 feed-forward layer. The update of LoRA is scaled by a hyper-parameter $\\alpha$ . This value is typically left   \n203 unmodified, as it is usually set as 16 or 32 and never tuned [22, 43]. The exponential moving average   \n204 parameters $\\beta_{1}$ and $\\beta_{2}$ of AdamW [27] are set to their default values of 0.9 and 0.999, respectively. All   \n205 the experiments are conducted on NVIDIA A800 GPUs. More details are presented in Appendix C. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "image", "img_path": "GP30inajOt/tmp/e13c5754c8cae9ed5c6693aa0c2c03a6c72e2a588899bddaa81635a1be3fefd8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "206 4.1 Natural language understanding ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "207 We first evaluate our backbone model DeBERTaV3-base [20] on GLUE [37] benchmark containing   \n208 nine sub datasets, including MNLI [39], SST-2 [36], CoLA [38], QQP [37], QNLI [35], RTE [4],   \n209 MRPC [16], and STS-B [37].   \n210 Experimental results of the GLUE dataset are recorded in Table 1. It can be seen that our method   \n211 is consistently superior to other baselines. Notably, for RTE and STS-B datasets, both sphere  \n212 constrained (i.e., oblique manifold-constrained) and Stiefel-constrained have an obvious performance   \n213 gain even with only half the trainable parameters compared to the LoRA baseline, i.e., Sphere $r{=}8$ and   \n214 Stiefel $_{r=8}$ beat $\\mathrm{LoRA}_{r=16}$ . In addition, with the help of manifold geometry, the fine-tuning process   \n215 can be significantly accelerated compared to the vanilla AdamW optimizer, achieving a lower training   \n216 loss, as shown in Figure 1. Particularly on the CoLA dataset presented in Figure 1a, our approach   \n217 achieves the same training loss as the standard Adam optimizer but requires nearly half the number   \n218 of epochs. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "219 4.2 Question Answering ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "220 We conduct an evaluation on two question answering datasets: SQuAD v1.1 [35] and SQuADv2.0   \n221 [34]. Manifold-LoRA is used to fine-tune DeBERTaV3-base for these tasks, which are treated as   \n222 sequence labeling problems predicting the probability of each token as the start or end of an answer   \n223 span.   \n224 The main experimental results are presented in Table 2. For LoRA and our algorithms, new layers   \n225 are inserted into $W_{q},W_{k},W_{v},W_{o},F C_{1},F C_{2}$ . Notably, both manifold-regularized LoRA variants   \n226 consistently outperform all fine-tuning methods. Additionally, we plot the training loss, evaluation   \n227 exact match, and evaluation F1 scores against epochs in Figure 2. We conclude that the proposed   \n228 Manifold-LoRA method achieves a $2\\mathbf{x}$ speed-up in training epochs compared to AdamW, while   \n229 simultaneously improving model performance. We also illustrate the heat map of $B^{\\top}B$ in Figure 3,   \n230 which indicates that the matrix $B$ lands on the manifold eventually. This supports our assertion that   \n231 landing on manifold enhances the performance of LoRA. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "table", "img_path": "GP30inajOt/tmp/97891e93798d62b221419384ff5ed55dd96b84c4f236003c9e85feefa38556f0.jpg", "table_caption": ["Table 1: Results with DeBERTaV3-base on GLUE benchmark. We denote the best results in bold. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "232 4.3 Natural Language Generation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "233 The E2E NLG Challenge[30], as introduced by Novikova, provides a dataset for training end-to-end,   \n234 data-driven natural language generation systems, widely used in data-to-text evaluations. The E2E   \n235 dataset comprises approximately 42,000 training examples, 4,600 validation examples, and 4,600   \n236 test examples, all from the restaurant domain. We test our method on the E2E dataset using GPT-2   \n237 Medium and Large models, following the experimental setup outlined by LoRA [22]. For LoRA, we   \n238 set the hyperparameters to match those specified in the original paper.   \n239 The results from the E2E dataset are recorded in Table 3, where we focus on comparing LoRA and   \n240 Manifold-LoRA. The results clearly indicate that our proposed algorithm outperforms the established   \n241 baselines. Also, as shown in Figure 4, the matrix $B$ resides on the manifold even at the early training   \n242 stage, validating the feasibility of our method. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "GP30inajOt/tmp/abeebc8d8791bd0a80034308ff4a9fddff78b41f6317e321c4300a0a73a4bd1b.jpg", "img_caption": ["(a) CoLA train loss "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "GP30inajOt/tmp/f74f4c17760b95089b4a113a237551a9a6ebdf4066a7ae414b68eece4061a0be.jpg", "img_caption": ["(b) QQP train loss "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "GP30inajOt/tmp/e5d1fea39341c8bb39342c4408c1780267bbcf81edc0587a96ee23e5a4229b46.jpg", "img_caption": ["(c) STSB train loss "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "GP30inajOt/tmp/ca75ae868673094dae1f795002a3b10f042350fbaf4600fe577f6c2463233e29.jpg", "img_caption": ["(d) CoLA evaluation matthews correlation "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "GP30inajOt/tmp/0143bcae2d9251e908b43107f2901f23e14bde65738ab19b28c0efce53a62c6a.jpg", "img_caption": ["(e) QQP evaluation accuracy "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "GP30inajOt/tmp/4fbaa10a0b574e3e6f3ecf1fb15d33b20b689739b19e8bbd2ff6faf3abec9d27.jpg", "img_caption": ["(f) STSB evaluation pearson "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 1: The figures illustrate that both sphere constrained and Stiefel constrained manifold-LoRA achieve a faster convergence rate and attain a lower training loss within same optimization steps compared to LoRA method on three distinct datasets CoLA, QQP, STSB. ", "page_idx": 6}, {"type": "text", "text": "Table 2: Results with DeBERTaV3-base on SQuAD v1.1 and SQuADv2.0. We report EM/F1. The best results in each setting are shown in bold. ", "page_idx": 7}, {"type": "table", "img_path": "GP30inajOt/tmp/e9f400b40689d1e24cd840660cf0d5971c992d90c9272272abf7519b4d32341a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "GP30inajOt/tmp/3a8e00921f7539fe4e7f5abb0e7466c9dc492548e6fc9493c0e3fa0f772c160b.jpg", "img_caption": ["(a) SQuADv2.0 Train Loss (b) SQuADv2.0 Eval Exact Match (c) SQuADv2.0 Eval F1 "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 2: The figures compare the training loss, evaluation exact match, and evaluation F1 metrics against the number of epochs for the $\\mathrm{SQuADv}2.0$ dataset. ", "page_idx": 7}, {"type": "image", "img_path": "GP30inajOt/tmp/9635f20e099caa42eccd29fa3922b626630a8e1f91cc0675db8bb3c0a5d6f4a0.jpg", "img_caption": ["Figure 3: The heat map of $B^{\\top}B$ with the Stiefel manifold (the first and second rows) and the oblique manifold (the third and fourth rows) at the end of training on SQuADv2.0 dataset. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "GP30inajOt/tmp/29c5e7806a9a18b4b2b148da67653e56c5873ea7fc60142ab5248b60f08a3eb8.jpg", "table_caption": ["Table 3: GPT-2 medium (M) and large (L) models were evaluated on the E2E NLG Challenge. \\* denotes results from previously published works. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "GP30inajOt/tmp/5e9aba3070b994d50e9286fbaaec6aca0480313a35448abf5ae682a2094c25bb.jpg", "img_caption": ["Figure 4: The heat map of $B^{\\top}B$ with the Stiefel manifold (left) and the oblique manifold (right) on E2E dataset. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "243 5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "244 Optimization over the Stiefel manifold has been widely used in machine learning tasks. In this work,   \n245 we develop a retraction-free and penalty parameter-free gradient method, and prove that the generated   \n246 iterates eventually land on the manifold and achieve the optimality simultaneously. We then apply   \n247 this landing theory to avoid the possible redundancy of LoRA fine-tuning in LLMs. Specifically, we   \n248 reformulate the LoRA fine-tuning as an optimization problem over the Stiefel manifold, and propose   \n249 a new algorithm, Manifold-LoRA, which incorporates a careful analysis of step sizes to enable fast   \n250 training using the landing properties. Extensive experimental results demonstrate that our approach   \n251 not only accelerates the training process but also yields significant performance improvements.   \n252 Our study suggests several potential directions for future research. Although the established landing   \n253 theory focuses on the Stiefel manifold, extending this theory to general manifolds is one potential   \n254 direction. Additionally, evaluating the performance of Manifold-LoRA on LLMs with billions of   \n255 parameters would be valuable. Due to the heterogeneity of different layers, incorporating adaptive   \n256 ranks for $\\Delta W$ across different layers is another possible direction. This may be achievable by adding   \n257 sparsity regularization to the coordinate matrix $A$ . ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "258 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "259 [1] Pierre Ablin and Gabriel Peyr\u00b4e. Fast and accurate optimization on the orthogonal manifold   \n260 without retraction. In International Conference on Artificial Intelligence and Statistics, pages   \n261 5636\u20135657. PMLR, 2022.   \n262 [2] P-A Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization algorithms on matrix   \n263 manifolds. Princeton University Press, 2008.   \n264 [3] Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks.   \n265 In International conference on machine learning, pages 1120\u20131128. PMLR, 2016.   \n266 [4] Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. The fifth pascal recognizing   \n267 textual entailment challenge. TAC, 7(8):1, 2009.   \n268 [5] Le\u00b4on Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine   \n269 learning. SIAM review, 60(2):223\u2013311, 2018.   \n270 [6] Nicolas Boumal. An introduction to optimization on smooth manifolds. Cambridge University   \n271 Press, 2023.   \n272 [7] Nicolas Boumal and Pierre-antoine Absil. Rtrmc: A riemannian trust-region method for   \n273 low-rank matrix completion. Advances in neural information processing systems, 24, 2011.   \n274 [8] Robert S Chen, Brendan Lucier, Yaron Singer, and Vasilis Syrgkanis. Robust optimization for   \n275 non-convex objectives. Advances in Neural Information Processing Systems, 30, 2017.   \n276 [9] Shixiang Chen, Alfredo Garcia, Mingyi Hong, and Shahin Shahrampour. Decentralized Rie  \n277 mannian gradient descent on the Stiefel manifold. In International Conference on Machine   \n278 Learning, pages 1594\u20131605. PMLR, 2021.   \n279 [10] Minhyung Cho and Jaehyung Lee. Riemannian approach to batch normalization. Advances in   \n280 Neural Information Processing Systems, 30, 2017.   \n281 [11] Francis H Clarke, Ronald J Stern, and Peter R Wolenski. Proximal smoothness and the lower-C2   \n282 property. Journal of Convex Analysis, 2(1-2):117\u2013144, 1995.   \n283 [12] Wei Dai, Ely Kerman, and Olgica Milenkovic. A geometric approach to low-rank matrix   \n284 completion. IEEE Transactions on Information Theory, 58(1):237\u2013247, 2012.   \n285 [13] Wei Dai, Olgica Milenkovic, and Ely Kerman. Subspace evolution and transfer (set) for low-rank   \n286 matrix completion. IEEE Transactions on Signal Processing, 59(7):3120\u20133132, 2011.   \n287 [14] Kangkang Deng and Jiang Hu. Decentralized projected riemannian gradient method for smooth   \n288 optimization on compact submanifolds. arXiv preprint arXiv:2304.08241, 2023.   \n289 [15] Ning Ding, Xingtai Lv, Qiaosen Wang, Yulin Chen, Bowen Zhou, Zhiyuan Liu, and   \n290 Maosong Sun. Sparse low-rank adaptation of pre-trained language models. arXiv preprint   \n291 arXiv:2311.11696, 2023.   \n292 [16] Bill Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases.   \n293 In Third international workshop on paraphrasing (IWP2005), 2005.   \n294 [17] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable   \n295 neural networks. arXiv preprint arXiv:1803.03635, 2018.   \n296 [18] Bin Gao, Guanghui Hu, Yang Kuang, and Xin Liu. An orthogonalization-free parallelizable   \n297 framework for all-electron calculations in density functional theory. SIAM Journal on Scientific   \n298 Computing, 44(3):B723\u2013B745, 2022.   \n299 [19] Bin Gao, Xin Liu, Xiaojun Chen, and Ya-xiang Yuan. A new first-order algorithmic framework   \n300 for optimization problems with orthogonality constraints. SIAM Journal on Optimization,   \n301 28(1):302\u2013332, 2018.   \n302 [20] Pengcheng He, Jianfeng Gao, and Weizhu Chen. Debertav3: Improving deberta using   \n303 electra-style pre-training with gradient-disentangled embedding sharing. arXiv preprint   \n304 arXiv:2111.09543, 2021.   \n305 [21] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,   \n306 Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning   \n307 for nlp. In International conference on machine learning, pages 2790\u20132799. PMLR, 2019.   \n308 [22] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,   \n309 Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv   \n310 preprint arXiv:2106.09685, 2021.   \n311 [23] Jiang Hu, Xin Liu, Zai-Wen Wen, and Ya-Xiang Yuan. A brief introduction to manifold   \n312 optimization. Journal of the Operations Research Society of China, 8:199\u2013248, 2020.   \n313 [24] Shengding Hu, Ning Ding, Weilin Zhao, Xingtai Lv, Zhen Zhang, Zhiyuan Liu, and Maosong   \n314 Sun. Opendelta: A plug-and-play library for parameter-efficient adaptation of pre-trained   \n315 models. arXiv preprint arXiv:2307.03084, 2023.   \n316 [25] Dawid Jan Kopiczko, Tijmen Blankevoort, and Yuki Markus Asano. Vera: Vector-based random   \n317 matrix adaptation. arXiv preprint arXiv:2310.11454, 2023.   \n318 [26] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation.   \n319 arXiv preprint arXiv:2101.00190, 2021.   \n320 [27] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint   \n321 arXiv:1711.05101, 2017.   \n322 [28] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International   \n323 Conference on Learning Representations, 2018.   \n324 [29] Jorge Nocedal and Stephen J Wright. Numerical optimization. Springer, 1999.   \n325 [30] Jekaterina Novikova, Ond\u02c7rej Du\u02c7sek, and Verena Rieser. The e2e dataset: New challenges for   \n326 end-to-end generation. arXiv preprint arXiv:1706.09254, 2017.   \n327 [31] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,   \n328 Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative   \n329 style, high-performance deep learning library. Advances in neural information processing   \n330 systems, 32, 2019.   \n331 [32] Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi   \n332 Yang. Is chatgpt a general-purpose natural language processing task solver? arXiv preprint   \n333 arXiv:2302.06476, 2023.   \n334 [33] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.   \n335 Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n336 [34] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don\u2019t know: Unanswerable   \n337 questions for squad. arXiv preprint arXiv:1806.03822, 2018.   \n338 [35] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: $100{,}000{+}$ questions   \n339 for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.   \n340 [36] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y   \n341 Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a   \n342 sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural   \n343 language processing, pages 1631\u20131642, 2013.   \n344 [37] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.   \n345 Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv   \n346 preprint arXiv:1804.07461, 2018.   \n347 [38] Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability   \n348 judgments. Transactions of the Association for Computational Linguistics, 7:625\u2013641, 2019.   \n349 [39] Adina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus   \n350 for sentence understanding through inference. arXiv preprint arXiv:1704.05426, 2017.   \n351 [40] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony   \n352 Moi, Pierric Cistac, Tim Rault, R\u00b4emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,   \n353 Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain   \n354 Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the  \n355 art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods   \n356 in Natural Language Processing: System Demonstrations, pages 38\u201345, Online, October 2020.   \n357 Association for Computational Linguistics.   \n358 [41] Nachuan Xiao, Xin Liu, and Kim-Chuan Toh. Dissolving constraints for riemannian optimiza  \n359 tion. Mathematics of Operations Research, 49(1):366\u2013397, 2024.   \n360 [42] Jinming Xu, Shanying Zhu, Yeng Chai Soh, and Lihua Xie. Augmented distributed gradient   \n361 methods for multi-agent optimization under uncoordinated constant stepsizes. In 2015 54th   \n362 IEEE Conference on Decision and Control (CDC), pages 2055\u20132060. IEEE, 2015.   \n363 [43] Greg Yang and Edward J Hu. Feature learning in infinite-width neural networks. arXiv preprint   \n364 arXiv:2011.14522, 2020.   \n365 [44] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient   \n366 fine-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199,   \n367 2021.   \n368 [45] Hui Zhang and Wotao Yin. Gradient methods for convex minimization: better rates under   \n369 weaker conditions. arXiv preprint arXiv:1303.4645, 2013.   \n370 [46] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen,   \n371 and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh   \n372 International Conference on Learning Representations, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "373 A Proximal smoothness ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "374 The notion of proximal smoothness, as introduced by [11], refers to the characteristic of a closed set   \n375 whereby the nearest-point projection becomes a singleton when the point is in close enough to the set.   \n376 This property facilitates algorithmic and theoretical advancements by endowing nonconvex sets with   \n377 convex-like structural attributes. Specifically, for any positive real number $\\gamma$ , we define the $\\gamma$ -tube   \n378 around $\\mathcal{M}$ as $U_{\\mathcal{M}}(\\gamma):=\\{x:\\mathrm{dist}(x,\\mathcal{M})<\\gamma\\}$ . We say a closed set $\\mathcal{M}$ is $\\gamma$ -proximally smooth if   \n379 the projection operator $\\textstyle{\\mathcal{P}}_{\\mathcal{M}}(x):=\\operatorname{argmin}_{y\\in\\mathcal{M}}\\|y-x\\|^{2}$ is a singleton whenever $x\\in U_{\\mathcal{M}}(\\gamma)$ .   \n380 Obviously, any closed and convex set is proximally smooth for arbitrary $\\gamma\\in(0,\\infty)$ . According to   \n381 [11, Corollary 4.6], a closed set $\\mathcal{M}$ is convex if and only if it is proximally smooth with a radius of $\\gamma$   \n382 for every $\\gamma>0$ . It is worth noting that the Stiefel manifold is 1-proximally smooth. By following the   \n383 proof in [11, Theorem 4.8], ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\mathcal{P}_{\\mathrm{St}(d,r)}(x)-\\mathcal{P}_{\\mathrm{St}(d,r)}(y)\\right\\|\\le2\\|x-y\\|,\\ \\forall x,y\\in\\bar{U}_{\\mathrm{St}(d,r)}(\\frac{1}{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "384 where $\\begin{array}{r}{\\bar{U}_{\\mathrm{St}(d,r)}(\\frac{1}{2}):=\\left\\{x:\\mathrm{dist}(x,\\mathrm{St}(d,r))\\le\\frac{1}{2}\\right\\}}\\end{array}$ is the closure of $U_{\\mathrm{St}(d,r)}\\!\\left({\\frac{1}{2}}\\right)$ . It is worth noting   \n385 that for any closed convex set $\\mathcal{M}\\subset\\mathbb{R}^{d\\times r}$ , the projection operator $\\mathcal{P}_{\\mathcal{M}}$ is 1-Lipschitz continuous   \n386 over Rd\u00d7r. ", "page_idx": 12}, {"type": "text", "text": "387 B Proofs ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "388 Proof of Lemma 1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "389 Proof. Denote the SVD of $X$ by $\\boldsymbol{X}=\\boldsymbol{U}\\boldsymbol{S}\\boldsymbol{V}^{\\intercal}$ . Then, it holds that $\\mathrm{dist}(X,\\mathrm{St}(d,r))=\\|X-{\\bar{X}}\\|=$   \n390 ${\\Vert s-1\\Vert_{2}}$ , where $s=\\mathrm{diag}(S)$ . Furthermore, we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle\\nabla\\varphi(X),X-\\bar{X}\\right\\rangle=\\left\\langle U S V^{\\top}(V S^{2}V^{\\top}-I),U S V^{\\top}-U V^{\\top}\\right\\rangle}\\\\ &{\\quad\\quad\\quad\\quad=\\left\\langle U(S^{3}-S)V^{\\top},U(S-I)V^{\\top}\\right\\rangle}\\\\ &{\\quad\\quad\\quad\\quad=\\mathrm{tr}((S^{3}-S)(S-I))}\\\\ &{\\quad\\quad\\quad\\quad\\geq\\frac{3}{2}\\|s-1\\|_{2}^{2}=\\frac{3}{2}\\|X-\\bar{X}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "391 where the last inequality is from $\\begin{array}{r}{\\operatorname*{min}_{i}s_{i}(s_{i}+1)\\geq\\frac{105}{64}\\geq\\frac{3}{2}}\\end{array}$ . This completes the proof. ", "page_idx": 12}, {"type": "text", "text": "392 Proof of Lemma 2 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "393 Proof. Assume that $\\begin{array}{r}{\\|X_{k}-\\bar{X}_{k}\\|\\leq\\frac{1}{8}}\\end{array}$ . Denote the SVD of $X_{k}$ by $U S V^{\\top}$ . Let $s=\\mathrm{diag}(S)$ . Then,   \n394 we have $\\begin{array}{r}{\\frac78\\leq s_{i}\\leq\\frac98}\\end{array}$ for any $i$ . This implies ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\|\\nabla\\varphi(X_{k})\\|^{2}=\\operatorname{tr}((S^{3}-S)^{2})\\leq6\\|X_{k}-{\\bar{X}}_{k}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "395 Hence, we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|X_{k+1}-\\bar{X}_{k+1}\\|^{2}\\le\\|X_{k+1}-\\bar{X}_{k}\\|^{2}}&{}\\\\ {=\\|X_{k}-\\frac{1}{3}\\nabla\\varphi(X_{k})-\\bar{X}_{k}\\|^{2}}&{}\\\\ {=\\|X_{k}-\\bar{X}_{k}\\|^{2}-\\frac{2}{3}\\left\\langle X_{k}-\\bar{X}_{k},\\nabla\\varphi(X_{k})\\right\\rangle+\\frac{1}{9}\\|\\nabla\\varphi(X_{k})\\|^{2}}&{}\\\\ {\\le(1-1+\\frac{2}{3})\\|X_{k}-\\bar{X}_{k}\\|^{2}}&{}\\\\ {=\\frac{2}{3}\\|X_{k}-\\bar{X}_{k}\\|^{2},}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "396 where the first inequality is from $\\begin{array}{r}{\\bar{X}_{k+1}=\\operatorname*{argmin}_{X\\in\\mathrm{St}(d,r)}\\|X-X_{k}\\|^{2}}\\end{array}$ and the second inequality is   \n397 due to Lemma 1 and (16). \u53e3 ", "page_idx": 12}, {"type": "text", "text": "398 Proof of Lemma 3 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "399 Proof. Due to the twice differentiability of $f$ and the compactness of $\\operatorname{St}(d,r)$ , the inequality (7)   \n400 directly follows from [9, Lemma 2.4] and [14, Lemma 4.2], where $L:=L_{f}+D_{f}$ with $L_{f}$ being the   \n401 Lipschitz constant of $\\nabla f(X)$ over $\\operatorname{St}(d,r)$ and $D_{f}:=\\operatorname*{max}_{X\\in\\mathrm{St}(d,r)}\\|\\nabla\\mathring{f}(X)\\|$ . ", "page_idx": 13}, {"type": "text", "text": "402 For the second argument, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\mathrm{grad}f(X)-\\mathrm{grad}f(Y)\\|}\\\\ &{\\le\\|\\nabla_{T x}\\mathrm{s}_{\\ell(d,r)}(\\nabla f(X))-\\mathcal{P}_{T x}\\mathrm{s}_{\\ell(d,r)}(\\nabla f(Y))\\|+\\|\\mathcal{P}_{T x}\\mathrm{s}_{\\ell(d,r)}(\\nabla f(Y))-\\mathrm{grad}f(Y)\\|}\\\\ &{\\le L_{f}\\|X-Y\\|+\\frac{1}{2}\\|X(X^{\\top}\\nabla f(Y)+\\nabla f(Y)^{\\top}X)-Y(Y^{\\top}\\nabla f(Y)+\\nabla f(Y)^{\\top}Y)\\|}\\\\ &{\\le L_{f}\\|X-Y\\|+\\frac{1}{2}\\|X((X-Y)^{\\top}\\nabla f(Y)+\\nabla f(Y)^{\\top}(X-Y))\\|}\\\\ &{\\quad+\\frac{1}{2}\\|(X-Y)(Y^{\\top}\\nabla f(Y)+\\nabla f(Y)^{\\top}Y)\\|}\\\\ &{\\le L_{f}\\|X-Y\\|+\\frac{1}{2}(2\\hat{D}_{f}+3\\hat{D}_{f})\\|X-Y\\|}\\\\ &{=(L_{f}+\\frac{5}{2}\\hat{D}_{f})\\|X-Y\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "403 where $\\begin{array}{r}{\\hat{D}_{f}:=\\operatorname*{max}_{X\\in\\bar{U}_{\\mathrm{St}(d,r)}(\\frac{1}{8})}\\|\\nabla f(X)\\|}\\end{array}$ , the second inequality is due to the contractive property   \n404 of $\\mathcal{P}_{T_{X}\\mathrm{St}(d,r)}$ , and the last inequality is from the fact that $\\|Y\\|_{2}\\leq\\frac{3}{2}$ . By setting $\\begin{array}{r}{\\hat{L}=L_{f}+\\frac{5}{2}\\hat{D}_{f}}\\end{array}$ ,   \n405 we complete the proof. ", "page_idx": 13}, {"type": "text", "text": "406 Proof of Lemma 4 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "407 Proof. It follows that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|X_{k+1}-\\bar{X}_{k+1}\\|\\leq\\|X_{k+1}-\\bar{X}_{k}\\|}&{}\\\\ {\\leq\\|X_{k}-\\mu\\varphi(X_{k})-\\bar{X}_{k}\\|+\\alpha\\|\\mathrm{grad}f(X_{k})\\|}&{}\\\\ {\\leq\\frac{2}{3}\\|X_{k}-\\bar{X}_{k}\\|+\\alpha\\|\\mathrm{grad}f(X_{k})\\|.}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "408 We complete the proof. ", "page_idx": 13}, {"type": "text", "text": "409 Proof of Lemma 5 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "410 Proof. It follows from (7) that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(X_{i+1})-f(X_{i})\\leq(\\mathrm{out})(\\hat{X}_{i},\\hat{X}_{i+1}-\\hat{X}_{i})+\\frac{\\varepsilon}{3}\\ensuremath{\\varepsilon}_{1}^{*}\\leq\\ensuremath{1}_{2}\\ensuremath{\\varepsilon}_{1}+\\ensuremath{1}_{3}\\ensuremath{\\varepsilon}_{1}^{*}}\\\\ &{\\leq\\ensuremath{\\varepsilon}(\\ensuremath{\\operatorname*{max}}[\\mathcal{X}_{i},\\hat{X}_{i+1}-X_{i+1}+X_{i}-\\mathcal{X}_{i}])+\\ensuremath{\\varepsilon}_{0}\\ensuremath{\\operatorname*{inf}\\left([\\mathcal{X}_{i},\\hat{X}_{i}],X_{i+1}-X_{i}\\right)}}\\\\ &{\\leq(\\ensuremath{\\varepsilon})\\ensuremath{\\operatorname*{inf}\\left(X_{i},\\hat{X}_{i}-X_{i}-X_{i+1}\\right)}+\\ensuremath{\\varepsilon}_{0}\\ensuremath{\\operatorname*{inf}\\left([\\mathcal{X}_{i},\\hat{X}_{i}],X_{i+1}-X_{i}\\right)}}\\\\ &{+\\ensuremath{\\varepsilon}(\\ensuremath{\\operatorname*{inf}\\left([\\mathcal{X}_{i},\\hat{X}_{i}],X_{i}-X_{i+1}\\right)}+\\ensuremath{\\varepsilon}_{0}\\ensuremath{\\operatorname*{inf}\\left([\\mathcal{X}_{i},\\hat{X}_{i}],X_{i+1}-X_{i}\\right)}}\\\\ &{+\\ensuremath{\\varepsilon}_{0}^{*}\\ensuremath{\\operatorname*{inf}\\left([\\mathcal{X}_{i},\\hat{X}_{i}],X_{i}\\right)}+\\ensuremath{\\varepsilon}_{0}^{*}\\ensuremath{\\operatorname*{inf}\\left([\\mathcal{X}_{i},\\hat{X}_{i}],X_{i}\\right)}+\\ensuremath{\\varepsilon}_{0}\\ensuremath{\\operatorname*{inf}\\left([\\mathcal{X}_{i}],X_{i+1}-X_{i}\\right)}}\\\\ &{=\\ensuremath{\\varepsilon}(\\ensuremath{\\operatorname*{inf}\\left([\\mathcal{X}_{i},\\hat{X}_{i}],X_{i}\\right)}+\\ensuremath{\\varepsilon}_{0}^{*})\\ensuremath{\\operatorname*{inf}\\left([\\mathcal{X}_{i},\\hat{X}_{i+1}-X_{i+1}],+|\\mathcal{X}_{i}|\\right)}}\\\\ &{+\\ensuremath{\\varepsilon}_{0}^{*}\\ensuremath{\\operatorname*{inf}\\left([\\mathcal{X}_{i},\\hat{X}_{i}],X_{i}\\right)}}\\\\\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "411 where the second inequality is from the 2-Lipschitz continuity of $\\mathcal{P}_{\\mathrm{St}(d,r)}$ over $\\bar{U}_{\\mathrm{St}(d,r)}\\big(\\frac{1}{8}\\big)$ , the third   \n412 inequality is due to the facts that $X_{k}-\\bar{X}_{k}\\in N_{\\bar{X}_{k}}\\mathrm{St}(d,r)$ and $\\langle A,B\\rangle\\leq{\\textstyle\\frac{1}{2}}(\\|A\\|^{2}+\\|B\\|^{2})$ for any   \n413 $A,B\\in\\mathbb{R}^{n\\times d}$ , and the last inequality comes from ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\mathcal{P}_{T_{X_{k}}\\mathrm{St}(d,r)}(\\nabla\\varphi(X_{k}))\\|=\\|X_{k}(X_{k}^{\\top}X_{k}-I)^{2}\\|\\leq6\\|X_{k}-\\bar{X}_{k}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "414 Plugging $\\begin{array}{r}{\\mu=\\frac{1}{3}}\\end{array}$ into (17) gives (10). ", "page_idx": 14}, {"type": "text", "text": "415 Proof of Theorem. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "416 Proof. Applying [42, Lemma 2] to (9) yields ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{k=0}^{K}\\|X_{k}-{\\bar{X}}_{k}\\|^{2}\\leq18\\alpha^{2}\\sum_{k=0}^{K}\\|\\mathrm{grad}f({\\bar{X}}_{k})\\|^{2}+4.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "417 Then, summing (10) over $k=0,\\ldots,K$ gives ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f(\\bar{X}_{k+1})-f(\\bar{X}_{0})}\\\\ &{\\leq-\\left(\\alpha-(4\\hat{L}^{2}+4L+1)\\alpha^{2}\\right)\\displaystyle\\sum_{k=0}^{K}\\left\\|\\mathrm{grad}f(X_{k})\\right\\|^{2}}\\\\ &{\\quad+\\displaystyle\\frac{1}{2}\\left(4\\hat{D}_{f}+16\\hat{L}^{2}+16L+3\\right)\\displaystyle\\sum_{k=0}^{K+1}\\|X_{k}-\\bar{X}_{k}\\|^{2}}\\\\ &{\\leq-\\left(\\alpha-(4\\hat{L}^{2}+4L+1)\\alpha^{2}+9(4\\hat{D}_{f}+16\\hat{L}^{2}+16L+3)\\alpha^{2}\\right)\\displaystyle\\sum_{k=0}^{K}\\left\\|\\mathrm{grad}f(X_{k})\\right\\|^{2}}\\\\ &{\\quad+\\displaystyle\\frac{1}{2}\\left(4\\hat{D}_{f}+16\\hat{L}^{2}+16L+3\\right)\\left(18\\alpha^{2}\\left\\|\\mathrm{grad}f(X_{k+1})\\right\\|^{2}+4\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "418 Define $c_{1}=148\\hat{L}^{2}+148L+36\\hat{D}_{f}+28$ and $c_{2}=(9\\hat{D}_{f}^{2}+2)(4\\hat{D}_{f}+16\\hat{L}^{2}+16L+4)$ . Then, we   \n419 have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\alpha(1-c_{1}\\alpha)\\sum_{k=0}^{K}\\|\\mathrm{grad}f(X_{k})\\|^{2}\\leq f(\\bar{X}_{0})-f(\\bar{X}_{k+1})+c_{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "420 The\u221erefore, for any $\\begin{array}{l}{\\alpha\\,\\leq\\,\\frac{1}{2c_{1}}}\\end{array}$ , taking $K\\rightarrow\\infty$ gives $\\textstyle\\sum_{k=0}^{\\infty}\\|\\mathrm{grad}f(X_{k})\\|^{2}\\,<\\,\\infty$ . Then by (11),   \n421 $\\begin{array}{r}{\\sum_{k=0}^{\\infty}\\|X_{k}-{\\bar{X}}_{k}\\|^{2}<\\infty}\\end{array}$ . These lead to (11). \u53e3 ", "page_idx": 15}, {"type": "text", "text": "422 C Hyperparameters ", "text_level": 1, "page_idx": 15}, {"type": "table", "img_path": "GP30inajOt/tmp/f039e2eca4335326d69dc537a8dfd7d411995e971903782a44f47bd489c8307b.jpg", "table_caption": ["Table 4: Hyperparameter setup of Manifold-LoRA for question answering tasks. "], "table_footnote": [], "page_idx": 15}, {"type": "table", "img_path": "GP30inajOt/tmp/915d2ea8eb7feea1dbb4c7b42baebc96b95528ef11c97c94596bac0b2e15b2cd.jpg", "table_caption": ["Table 5: Hyperparameter configurations of Manifold-LoRA for GLUE benchmark "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "GP30inajOt/tmp/d96acbf586bb90c16d760f522a7a744d0f55068205ed3ed92d2a1dca1b28b48f.jpg", "table_caption": ["Table 6: Hyperparameter setup of Manifold-LoRA for E2E benchmark. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "423 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "424 1. Claims   \n425 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n426 paper\u2019s contributions and scope?   \n427 Answer: [Yes]   \n428 Justification: Our empirical results in Section 4 justify ours claims.   \n429 Guidelines:   \n430 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n431 made in the paper.   \n432 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n433 contributions made in the paper and important assumptions and limitations. A No or   \n434 NA answer to this question will not be perceived well by the reviewers.   \n435 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n436 much the results can be expected to generalize to other settings.   \n437 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n438 are not attained by the paper.   \n439 2. Limitations   \n440 Question: Does the paper discuss the limitations of the work performed by the authors?   \n441 Answer: [Yes]   \n442 Justification: We discuss our limitations in Section 5.   \n443 Guidelines:   \n444 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n445 the paper has limitations, but those are not discussed in the paper.   \n446 \u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n447 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n448 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n449 model well-specification, asymptotic approximations only holding locally). The authors   \n450 should reflect on how these assumptions might be violated in practice and what the   \n451 implications would be.   \n452 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n453 only tested on a few datasets or with a few runs. In general, empirical results often   \n454 depend on implicit assumptions, which should be articulated.   \n455 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n456 For example, a facial recognition algorithm may perform poorly when image resolution   \n457 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n458 used reliably to provide closed captions for online lectures because it fails to handle   \n459 technical jargon.   \n460 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n461 and how they scale with dataset size.   \n462 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n463 address problems of privacy and fairness.   \n464 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n465 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n466 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n467 judgment and recognize that individual actions in favor of transparency play an impor  \n468 tant role in developing norms that preserve the integrity of the community. Reviewers   \n469 will be specifically instructed to not penalize honesty concerning limitations.   \n470 3. Theory Assumptions and Proofs   \nQuestion: For each theoretical result, does the paper provide the full set of assumptions and   \n471   \n472 a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: We provide complete proofs in Appendix B and full set of assumptions in Section 2 ", "page_idx": 18}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include theoretical results. \u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems. \u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. \u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 18}, {"type": "text", "text": "487 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 18}, {"type": "text", "text": "Justification: We specify the training details in Section 4 and Appendix C Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 18}, {"type": "text", "text": "Justification: We specify the code and dataset in Section 4. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "529   \n530   \n531   \n532   \n533   \n534   \n535   \n536   \n537   \n538   \n539   \n540   \n541   \n542   \n543   \n544   \n545   \n546   \n547   \n548   \n549   \n550   \n551   \n552   \n553   \n554   \n555   \n556   \n557   \n558   \n559   \n560   \n561   \n562   \n563   \n564   \n565   \n566   \n567   \n568   \n569   \n570   \n571   \n572   \n573   \n574   \n575   \n576   \n577   \n578   \n579   \n580 ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We specify training details in Section 4 and hyperparameters in Appendix C. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: We specify these in our Section 4. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean. ", "page_idx": 19}, {"type": "text", "text": "581 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n582 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n583 of Normality of errors is not verified.   \n584 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n585 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n586 error rates).   \n587 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n588 they were calculated and reference the corresponding figures or tables in the text.   \n589 8. Experiments Compute Resources   \n590 Question: For each experiment, does the paper provide sufficient information on the com  \n591 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n592 the experiments?   \n593 Answer: [Yes]   \n594 Justification: We use the Hugging face and opendelta as our base code and make some   \n595 modifications. We use GLUE, E2E, and Suqad three dataset.   \n596 Guidelines:   \n597 \u2022 The answer NA means that the paper does not include experiments.   \n598 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n599 or cloud provider, including relevant memory and storage.   \n600 \u2022 The paper should provide the amount of compute required for each of the individual   \n601 experimental runs as well as estimate the total compute.   \n602 \u2022 The paper should disclose whether the full research project required more compute   \n603 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n604 didn\u2019t make it into the paper).   \n605 9. Code Of Ethics   \n606 Question: Does the research conducted in the paper conform, in every respect, with the   \n607 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n608 Answer: [Yes]   \n609 Justification: Our research is compatible with the NeurIPS Code of Ethics.   \n610 Guidelines:   \n611 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n612 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n613 deviation from the Code of Ethics.   \n614 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n615 eration due to laws or regulations in their jurisdiction).   \n616 10. Broader Impacts   \n617 Question: Does the paper discuss both potential positive societal impacts and negative   \n618 societal impacts of the work performed?   \n619 Answer: [NA]   \n620 Justification: There is no societal impact of our work performed   \n621 Guidelines:   \n622 \u2022 The answer NA means that there is no societal impact of the work performed.   \n623 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n624 impact or why the paper does not address societal impact.   \n625 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n626 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n627 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n628 groups), privacy considerations, and security considerations.   \n629 \u2022 The conference expects that many papers will be foundational research and not tied   \n630 to particular applications, let alone deployments. However, if there is a direct path to   \n631 any negative applications, the authors should point it out. For example, it is legitimate   \n632 to point out that an improvement in the quality of generative models could be used to   \n633 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n634 that a generic algorithm for optimizing neural networks could enable people to train   \n635 models that generate Deepfakes faster.   \n636 \u2022 The authors should consider possible harms that could arise when the technology is   \n637 being used as intended and functioning correctly, harms that could arise when the   \n638 technology is being used as intended but gives incorrect results, and harms following   \n639 from (intentional or unintentional) misuse of the technology.   \n640 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n641 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n642 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n643 feedback over time, improving the efficiency and accessibility of ML).   \n644 11. Safeguards   \n645 Question: Does the paper describe safeguards that have been put in place for responsible   \n646 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n647 image generators, or scraped datasets)?   \n648 Answer: [NA]   \n649 Justification: Our work poses no such risks.   \n650 Guidelines:   \n651 \u2022 The answer NA means that the paper poses no such risks.   \n652 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n653 necessary safeguards to allow for controlled use of the model, for example by requiring   \n654 that users adhere to usage guidelines or restrictions to access the model or implementing   \n655 safety filters.   \n656 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n657 should describe how they avoided releasing unsafe images.   \n658 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n659 not require this, but we encourage authors to take this into account and make a best   \n660 faith effort.   \n661 12. Licenses for existing assets   \n662 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n663 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n664 properly respected?   \n665 Answer: [Yes]   \n666 Justification: We correctly cite the code and datasets we used in Section 4.   \n667 Guidelines:   \n668 \u2022 The answer NA means that the paper does not use existing assets.   \n669 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n670 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n671 URL.   \n672 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n673 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n674 service of that source should be provided.   \n675 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n676 package should be provided. For popular datasets, paperswithcode.com/datasets   \n677 has curated licenses for some datasets. Their licensing guide can help determine the   \n678 license of a dataset.   \n679 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n680 the derived asset (if it has changed) should be provided.   \n681 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n682 the asset\u2019s creators.   \n683 13. New Assets   \n684 Question: Are new assets introduced in the paper well documented and is the documentation   \n685 provided alongside the assets?   \n686 Answer: [NA]   \n687 Justification: Our paper does not release new asset.   \n688 Guidelines:   \n689 \u2022 The answer NA means that the paper does not release new assets.   \n690 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n691 submissions via structured templates. This includes details about training, license,   \n692 limitations, etc.   \n693 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n694 asset is used.   \n695 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n696 create an anonymized URL or include an anonymized zip file.   \n697 14. Crowdsourcing and Research with Human Subjects   \n698 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n699 include the full text of instructions given to participants and screenshots, if applicable, as   \n700 well as details about compensation (if any)?   \n701 Answer: [NA]   \n702 Justification: Our study does not involve crowdsourcing nor research with human subjects.   \n703 Guidelines:   \n704 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n705 human subjects.   \n706 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n707 tion of the paper involves human subjects, then as much detail as possible should be   \n708 included in the main paper.   \n709 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n710 or other labor should be paid at least the minimum wage in the country of the data   \n711 collector.   \n712 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n713 Subjects   \n714 Question: Does the paper describe potential risks incurred by study participants, whether   \n715 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n716 approvals (or an equivalent approval/review based on the requirements of your country or   \n717 institution) were obtained?   \n718 Answer: [NA]   \n719 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n720 Guidelines:   \n721 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n722 human subjects.   \n723 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n724 may be required for any human subjects research. If you obtained IRB approval, you   \n725 should clearly state this in the paper.   \n726 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n727 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n728 guidelines for their institution.   \n729 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n730 applicable), such as the institution conducting the review. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}]