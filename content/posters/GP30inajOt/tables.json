[{"figure_path": "GP30inajOt/tables/tables_6_1.jpg", "caption": "Table 1: Results with DeBERTaV3-base on GLUE benchmark. We denote the best results in bold.", "description": "This table presents the results of the experiments conducted using the DeBERTaV3-base model on the GLUE benchmark.  It compares the performance of several methods, including full fine-tuning (Full FT), Adapter, BitFit, and different variations of LoRA, Sphere, and Stiefel methods (with rank 8 and 16). The metrics shown include accuracy (Acc), Matthews correlation coefficient (Mcc), and F1 score, depending on the specific task in the GLUE benchmark. The best performance for each metric in each task is highlighted in bold.", "section": "4.1 Natural language understanding"}, {"figure_path": "GP30inajOt/tables/tables_7_1.jpg", "caption": "Table 2: Results with DeBERTaV3-base on SQUAD v1.1 and SQUADv2.0. We report EM/F1. The best results in each setting are shown in bold.", "description": "This table presents the experimental results of the proposed Manifold-LoRA and several baseline methods on two question answering datasets: SQUAD v1.1 and SQUAD v2.0.  The results are reported as Exact Match (EM) / F1 scores.  The table shows the performance of different methods with varying numbers of parameters, highlighting the improvement of Manifold-LoRA over other baselines.", "section": "4 Experiments"}, {"figure_path": "GP30inajOt/tables/tables_8_1.jpg", "caption": "Table 3: GPT-2 medium (M) and large (L) models were evaluated on the E2E NLG Challenge. * denotes results from previously published works.", "description": "This table presents the results of evaluating GPT-2 medium and large language models on the E2E NLG Challenge dataset.  The results are compared across several different parameter-efficient fine-tuning methods including different variants of Adapter, PreLayer, and LoRA, as well as the proposed Manifold-LoRA method (Stiefel and Sphere variants).  The metrics used for evaluation include BLEU, NIST, METEOR, ROUGE-L, and CIDEr scores, providing a comprehensive assessment of the generated text quality.  Results marked with an asterisk (*) are taken from previously published work.", "section": "4.3 Natural Language Generation"}, {"figure_path": "GP30inajOt/tables/tables_15_1.jpg", "caption": "Table 4: Hyperparameter setup of Manifold-LoRA for question answering tasks.", "description": "This table presents the hyperparameter settings used in the Manifold-LoRA model for question answering tasks on the SQuADv1.1 and SQuADv2.0 datasets.  It details the warmup ratio, learning rate schedule, weight decay, beta1 and beta2 parameters for the AdamW optimizer, batch size, learning rate, number of epochs, and the specific parameters (\u03bc, Lower, Upper) for the different manifold optimization techniques (Sphere and Stiefel) used with varying rank values (r=8 and r=16).", "section": "C Hyperparameters"}, {"figure_path": "GP30inajOt/tables/tables_16_1.jpg", "caption": "Table 5: Hyperparameter configurations of Manifold-LoRA for GLUE benchmark", "description": "This table shows the hyperparameter settings used for the Manifold-LoRA model when evaluated on the GLUE benchmark.  It includes parameters such as warmup ratio, learning rate schedule, maximum sequence length, weight decay, beta1, beta2, batch size, LoRA layer, epochs, and learning rate.  Separate hyperparameter values are provided for different variants of the model using different ranks (Sphere(r=8), Sphere(r=16), Stiefel(r=8), and Stiefel(r=16)),  and for each of the GLUE sub-tasks (MNLI, SST-2, CoLA, QQP, QNLI, RTE, MRPC, STS-B). The `\u03bc`, `Lower`, and `Upper` parameters likely relate to the Manifold-LoRA algorithm's specific parameters for controlling the optimization process.", "section": "4.1 Natural language understanding"}, {"figure_path": "GP30inajOt/tables/tables_16_2.jpg", "caption": "Table 6: Hyperparameter setup of Manifold-LoRA for E2E benchmark.", "description": "This table presents the hyperparameter settings used for the Manifold-LoRA model in the experiments conducted on the E2E benchmark. It includes parameters for both GPT-2(M) and GPT-2(L) models, such as Warmup Steps, learning rate schedules, weight decay, batch size, and epochs.  Specifically, for the Manifold-LoRA configurations, the table details the parameter \u03bc, as well as the lower and upper bounds for the step size adjustments.", "section": "4.3 Natural Language Generation"}]