[{"figure_path": "FXdMgfCDer/tables/tables_1_1.jpg", "caption": "Table 1: Results (mean\u00b1std) under the GCIL setting on four large datasets. The best performance on each dataset is boldfaced. \u201c\u2191\u201d denotes the higher value represents better performance. Oracle Model can get access to the data of all tasks and task IDs, i.e., it obtains the upper bound performance. \u201c\u221a\u201d in Data Replay indicates the use of data replay in the model, and \u00d7 denotes no data replay involved.", "description": "This table presents the results of different continual learning methods on four large graph datasets under the graph class-incremental learning (GCIL) setting.  It compares the average accuracy (AA) and average forgetting (AF) of various methods, including baseline methods (Fine-tune, Joint), regularization-based methods (EWC, MAS, GEM, LwF, TWP), and replay-based methods (ERGNN, SSM-uniform, SSM-degree, SEM-curvature, CaT, DeLoMe, OODCIL).  The results are shown with standard deviations and highlight the best performance for each dataset.  An Oracle Model (with access to all data and task IDs) provides an upper bound. The table also shows whether each method utilizes data replay or not.", "section": "4.1 Main Results"}, {"figure_path": "FXdMgfCDer/tables/tables_6_1.jpg", "caption": "Table 1: Results (mean\u00b1std) under the GCIL setting on four large datasets. The best performance on each dataset is boldfaced. \u201c\u2191\u201d denotes the higher value represents better performance. Oracle Model can get access to the data of all tasks and task IDs, i.e., it obtains the upper bound performance. \u201c\u221a\u201d in Data Replay indicates the use of data replay in the model, and \u00d7 denotes no data replay involved.", "description": "This table presents the results of various continual learning methods on four graph datasets under the graph class-incremental learning (GCIL) setting.  The methods are evaluated based on average accuracy (AA) and average forgetting (AF).  A comparison is made with a fine-tuning baseline, a joint training baseline (which has access to all data), and an oracle model (which also knows the task IDs). The table shows the performance of each method, indicating whether data replay was used and highlighting the best performance for each dataset.", "section": "4.1 Main Results"}, {"figure_path": "FXdMgfCDer/tables/tables_7_1.jpg", "caption": "Table 2: AA and AF results of enabling existing GCIL methods with our task ID prediction (TP).", "description": "This table shows the average accuracy (AA) and average forgetting (AF) results of several existing graph class-incremental learning (GCIL) methods.  It compares their performance when used alone versus when combined with the proposed task ID prediction method (TP). The results demonstrate the significant improvement in both AA and AF achieved by incorporating the task ID prediction module, highlighting its effectiveness in addressing the inter-task class separation problem common in GCIL.", "section": "4.1 Main Results"}, {"figure_path": "FXdMgfCDer/tables/tables_8_1.jpg", "caption": "Table 3: Results of TPP and its variants on ablating task ID prediction and graph prompting modules.", "description": "This table presents the ablation study results of the Task Profiling and Prompting (TPP) approach. It shows the average accuracy (AA) and average forgetting (AF) results across four datasets (CoraFull, Arxiv, Reddit, Products) when different components of TPP (Task ID prediction, graph prompting: prompt and classification head) are removed.  The results highlight the contribution of each component to the overall performance of the method.  For instance, it demonstrates the significant improvement in performance due to the inclusion of both Task ID prediction and graph prompting.", "section": "4.2 Ablation Study"}, {"figure_path": "FXdMgfCDer/tables/tables_9_1.jpg", "caption": "Table 1: Results (mean\u00b1std) under the GCIL setting on four large datasets. The best performance on each dataset is boldfaced. \u201c\u2191\u201d denotes the higher value represents better performance. Oracle Model can get access to the data of all tasks and task IDs, i.e., it obtains the upper bound performance. \u201c\u221a\u201d in Data Replay indicates the use of data replay in the model, and \u00d7 denotes no data replay involved.", "description": "This table presents a comparison of different continual graph learning methods under the graph class-incremental learning (GCIL) setting on four large benchmark datasets (CoraFull, Arxiv, Reddit, Products).  The results are reported as mean \u00b1 standard deviation of average accuracy (AA) and average forgetting (AF) across multiple runs.  Methods are evaluated based on their average accuracy (AA) and the percentage of forgetting (AF) across all tasks. An Oracle model (which has access to all data and task IDs) provides an upper performance bound. The use of data replay for each method is indicated, with \u2018\u00d7\u2019 signifying that no replay was used.", "section": "4.1 Main Results"}, {"figure_path": "FXdMgfCDer/tables/tables_15_1.jpg", "caption": "Table 1: Results (mean\u00b1std) under the GCIL setting on four large datasets. The best performance on each dataset is boldfaced. \u201c\u2191\u201d denotes the higher value represents better performance. Oracle Model can get access to the data of all tasks and task IDs, i.e., it obtains the upper bound performance. \u201c\u221a\u201d in Data Replay indicates the use of data replay in the model, and \u00d7 denotes no data replay involved.", "description": "This table presents the results of different continual learning methods on four graph datasets under the graph class-incremental learning (GCIL) setting.  The table compares the average accuracy (AA) and average forgetting (AF) of various methods, including both general CIL methods and graph-specific CIL methods.  It also shows the performance of an oracle model which has access to all data and task IDs, and  indicates whether each method uses data replay.  The best performing method for each dataset is highlighted.", "section": "4.1 Main Results"}, {"figure_path": "FXdMgfCDer/tables/tables_15_2.jpg", "caption": "Table 1: Results (mean\u00b1std) under the GCIL setting on four large datasets. The best performance on each dataset is boldfaced. \u201c\u2191\u201d denotes the higher value represents better performance. Oracle Model can get access to the data of all tasks and task IDs, i.e., it obtains the upper bound performance. \u201c\u221a\u201d in Data Replay indicates the use of data replay in the model, and \u00d7 denotes no data replay involved.", "description": "This table presents the results of different continual graph learning methods on four benchmark datasets under the graph class-incremental learning (GCIL) setting.  It compares the average accuracy (AA) and average forgetting (AF) of various methods, including several state-of-the-art (SOTA) methods.  An Oracle Model that has access to all task data and IDs is included as an upper bound performance.  The table also indicates whether each method uses data replay.", "section": "4.1 Main Results"}, {"figure_path": "FXdMgfCDer/tables/tables_16_1.jpg", "caption": "Table 1: Results (mean\u00b1std) under the GCIL setting on four large datasets. The best performance on each dataset is boldfaced. \u201c\u2191\u201d denotes the higher value represents better performance. Oracle Model can get access to the data of all tasks and task IDs, i.e., it obtains the upper bound performance. \u201c\u221a\u201d in Data Replay indicates the use of data replay in the model, and \u00d7 denotes no data replay involved.", "description": "This table presents the average accuracy (AA) and average forgetting (AF) results for various graph continual learning methods on four benchmark datasets under the graph class-incremental learning (GCIL) setting.  The results are shown for several state-of-the-art methods, along with two baseline methods (Fine-tune and Joint), and an Oracle model (which has access to all data and task IDs).  The table highlights the best-performing method for each dataset and indicates whether data replay was used for each method.  The metrics AA and AF are used to evaluate the performance and forgetting of these methods.  A higher AA indicates better overall performance, while an AF of 0 indicates no forgetting.", "section": "4.1 Main Results"}, {"figure_path": "FXdMgfCDer/tables/tables_17_1.jpg", "caption": "Table 6: The accuracy of task prediction with other task formulations.", "description": "This table presents the accuracy of the task prediction method used in the paper under different task formulations.  These formulations vary how the classes are assigned to different tasks (ascending, descending, and random order). The results show consistent high accuracy (100%) across all datasets regardless of the class ordering.", "section": "4.1 Main Results"}, {"figure_path": "FXdMgfCDer/tables/tables_17_2.jpg", "caption": "Table 7: Additional parameters and performance (AA%) of the proposed graph prompting and task-specific models.", "description": "This table compares the proposed graph prompting approach with learning separate task-specific models. It shows the number of additional parameters required for each method and their average accuracy (AA) across four datasets.  The graph prompting method requires significantly fewer parameters while achieving comparable performance to task-specific models, highlighting its efficiency and effectiveness.", "section": "4.1 Main Results"}, {"figure_path": "FXdMgfCDer/tables/tables_18_1.jpg", "caption": "Table 8: Total training time and inference time (seconds) for different methods on CoraFull.", "description": "This table presents the total training and inference times for four different graph continual learning methods on the CoraFull dataset.  The methods compared are TWP, SSM, DeMoLe, and the proposed TPP method. The training time reflects the time taken to train the model across all tasks, and the inference time represents the time needed to process a single test sample.  The results highlight the efficiency of the proposed TPP method in terms of training time, showing a significant reduction compared to other methods. The inference time is relatively consistent across all methods.", "section": "4 Experiments"}]