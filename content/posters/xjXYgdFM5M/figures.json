[{"figure_path": "xjXYgdFM5M/figures/figures_1_1.jpg", "caption": "Fig. 1: This framework outlines the comprehensive approach to understanding the performance decline of edited models. On the left, traditional knowledge editing tasks are categorized into different types, each with distinct editing objectives: yes/no, a/b/c/d, and entity/event. On the right, our experiments are structured from both data and model perspectives. From the data perspective, we conduct three experiments: (a) a comprehensive performance evaluation of the model, (b) the construction of a Multi-Question Dataset (MQD), and (c) an assessment of the impact of editing different target outputs on model performance. From the model perspective, we design four experiments: (d) an evaluation of the edited model\u2019s forgetting ability, (e) an identification of the current knowledge editing method\u2019s bottleneck and an exploration of the correlation between editing probability values and parameter layer norms, and (f) a proposal of a sequence editing method, which effectively enhances the performance of the edited model.", "description": "This figure illustrates the research framework used to investigate the performance decline of edited language models.  The left side categorizes knowledge editing tasks by type and objective, while the right side details the experimental approach from both data and model perspectives.  Data perspective experiments assess model performance, create a new dataset (MQD), and analyze the impact of various editing data. Model perspective experiments focus on the model's forgetting, identify performance bottlenecks, and propose a new sequence editing method (D4S).", "section": "1 Introduction"}, {"figure_path": "xjXYgdFM5M/figures/figures_3_1.jpg", "caption": "Fig. 2: Evaluation results of different editing methods on various types of datasets. The horizontal axis in the image represents the number of edited samples, and the vertical axis represents the performance of the edited model.", "description": "This figure shows the performance of different knowledge editing methods (KN, ROME, MEMIT) across various datasets (zsRE, ELKEN, 20Q, CKD) on multiple downstream tasks (arc_challenge, hellaswag, mmlu, winogrande, truthfulqa, gsm8k). The x-axis represents the number of edited samples, and the y-axis represents the performance of the edited model.  The graph illustrates how the performance changes as more edits are applied, revealing different trends for each editing method and dataset combination.  Some methods show a gradual decline in performance with increased edits, others show relatively stable performance across the edit range, and some exhibit sharp drops in performance after a certain number of edits.", "section": "3.1 The Overall Performance Evaluation"}, {"figure_path": "xjXYgdFM5M/figures/figures_5_1.jpg", "caption": "Fig. 3: The performance of the model after editing data for different question types.", "description": "This figure shows the performance of the model after editing data for three different question types: True/False (T/F), Multiple-choice (MQ), and Directly generated (DG). Each subplot represents a question type and shows the performance on several downstream tasks (arc_challenge, hellaswag, mmlu, truthfulqa_mc, winogrande, gsm8k) as the number of edits increases. The figure illustrates that the performance degradation after editing is correlated with the perplexity (PPL) of the editing objectives, with directly generated questions showing the most significant performance drop.  The different colored lines represent different downstream tasks.", "section": "3.3 The Influence of Editing Objectives"}, {"figure_path": "xjXYgdFM5M/figures/figures_5_2.jpg", "caption": "Fig. 7: The original and current evaluation methods.", "description": "This figure illustrates the difference between the original and current evaluation methods used in the paper. The original method evaluates each sample individually after editing. In contrast, the current method evaluates the model's performance after a sequence of edits, considering the impact on previously edited samples.  This highlights a shift in methodology to better reflect real-world application scenarios where multiple edits are made.", "section": "Appendix /Evaluation Method"}, {"figure_path": "xjXYgdFM5M/figures/figures_6_1.jpg", "caption": "Fig. 5: The bottleneck and L1-norm correspondence in sequence editing are illustrated in the figure. In subfigure (a), the horizontal axis represents the number of edited samples, and the vertical axis represents the probability value of the edited object. The blue line represents the ROME method, while the green line represents the MEMIT method. In subfigures (b) and (c), the horizontal axis represents the number of edits, and the vertical axis represents the L1-norm value of the editing layer.", "description": "This figure shows the performance degradation of two sequence editing methods, ROME and MEMIT.  The first subplot (a) displays the probability of successful edits over the number of edits performed.  It shows a clear decline in the probability of success for both methods as the number of edits increases, highlighting the 'bottleneck' phenomenon. The second and third subplots (b and c) illustrate the L1-norm of various layers within the model during the editing process.  These demonstrate a strong correlation between the increase in L1-norm values and the decrease in edit success rates, providing further evidence of the model's performance degradation.", "section": "4.2 The Bottleneck of Sequence Edit"}, {"figure_path": "xjXYgdFM5M/figures/figures_9_1.jpg", "caption": "Fig. 6: Norms of weight and performance of the edited model.", "description": "This figure shows the L1-norm of the model's weights and the model's performance on downstream tasks after different numbers of edits using various methods (ROME, MEMIT, PMET, and D4S). The left two subfigures (a, b) display the L1-norm of the model's weights for Llama and GPT respectively, illustrating the explosive growth of the norms for some methods like ROME and MEMIT with increasing edits. The right subfigure (c) presents the performance on downstream tasks, demonstrating that while some methods maintain good performance, others show decline after a certain number of edits, and D4S is effective at minimizing the performance drop. The results indicate a strong correlation between the growth of L1-norm and the decline in model performance.", "section": "5.2 Performance of Edited Model"}, {"figure_path": "xjXYgdFM5M/figures/figures_12_1.jpg", "caption": "Fig. 1: This framework outlines the comprehensive approach to understanding the performance decline of edited models. On the left, traditional knowledge editing tasks are categorized into different types, each with distinct editing objectives: yes/no, a/b/c/d, and entity/event. On the right, our experiments are structured from both data and model perspectives. From the data perspective, we conduct three experiments: (a) a comprehensive performance evaluation of the model, (b) the construction of a Multi-Question Dataset (MQD), and (c) an assessment of the impact of editing different target outputs on model performance. From the model perspective, we design four experiments: (d) an evaluation of the edited model's forgetting ability, (e) an identification of the current knowledge editing method's bottleneck and an exploration of the correlation between editing probability values and parameter layer norms, and (f) a proposal of a sequence editing method, which effectively enhances the performance of the edited model.", "description": "This figure illustrates the framework used in the paper to understand why model performance declines after editing. The left side categorizes knowledge editing tasks, while the right side details the experimental approach from data and model perspectives. The data perspective includes evaluating model performance, constructing a new dataset (MQD), and analyzing the effects of different editing data.  The model perspective covers evaluating forgetting, identifying performance bottlenecks, exploring correlations between editing and model layers, and proposing a new sequence editing method (D4S).", "section": "1 Introduction"}, {"figure_path": "xjXYgdFM5M/figures/figures_13_1.jpg", "caption": "Fig. 8: Evaluation metrics change with the number of edits.", "description": "This figure visualizes the changes in three evaluation metrics (Efficacy, Paraphrase, and Specificity) across different editing methods (ROME, MEMIT, PMET, FT, GRACE, and D4S) as the number of edits increases.  It showcases the performance of each method over time, revealing how well they maintain the accuracy and consistency of edits. The x-axis represents the number of edits performed, while the y-axis displays the corresponding metric value (ranging from 0 to 100).  The plots show the performance of the model with the different approaches and how they are affected by the amount of editing done. This allows for a direct comparison of various editing strategies and their effectiveness.", "section": "Appendix /Evaluate Forgetting Ability"}]