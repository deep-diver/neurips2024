[{"type": "text", "text": "Time-Varying LoRA: Towards Effective Cross-Domain Fine-Tuning of Diffusion Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhan Zhuang1,2,\u2217 Yulong Zhang3,\u2217 Xuehao Wang1 ", "page_idx": 0}, {"type": "text", "text": "1Southern University of Science and Technology 2City University of Hong Kong 3Zhejiang University 12250063@mail.sustech.edu.cn {zhangylcse, lujg, ying.wei}@zju.edu.cn {xuehaowangfi, yu.zhang.ust}@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large-scale diffusion models are adept at generating high-fidelity images and facilitating image editing and interpolation. However, they have limitations when tasked with generating images in dynamic, evolving domains. In this paper, we introduce Terra, a novel Time-varying low-rank adapter that offers a fine-tuning framework specifically tailored for domain flow generation. The key innovation of Terra lies in its construction of a continuous parameter manifold through a time variable, with its expressive power analyzed theoretically. This framework not only enables interpolation of image content and style but also offers a generation-based approach to address the domain shift problems in unsupervised domain adaptation and domain generalization. Specifically, Terra transforms images from the source domain to the target domain and generates interpolated domains with various styles to bridge the gap between domains and enhance the model generalization, respectively. We conduct extensive experiments on various benchmark datasets, empirically demonstrate the effectiveness of Terra. Our source code is publicly available on https://github.com/zwebzone/terra. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recently, text-to-image diffusion models [38, 47, 48, 45] have revolutionized computer vision by synthesizing high-quality, creative images. Those models provide a user-friendly method for generating images through text prompts. Furthermore, with advancements in fine-tuning techniques of diffusion models [4], users can easily customize [83], edit [27], and interpolate [88, 80, 7] images. A common approach involves using a low-rank adapter (LoRA) [25] to fine-tune diffusion models with a few images to generate customized images. This inspires a generation-based approach to address a fundamental and classical problem in machine learning known as domain shift. ", "page_idx": 0}, {"type": "text", "text": "Domain shift is commonly studied in the cross-domain learning [70, 82, 61] with two settings: unsupervised domain adaptation (UDA) [40, 12, 94], which aims to transfer knowledge from a source domain to a target domain, and domain generalization (DG) [89, 64], which focuses on training a model on source domains and then generalizing to unseen target domains. Prior methods [91, 15, 68, 90, 73] have demonstrated the effectiveness of image translation and interpolation on the learning paradigms based on mixup [79, 60], generative adversarial networks [16, 92], and diffusion models [24, 36]. Considering the impressive capabilities of diffusion models and the efficiency of fine-tuning techniques like LoRA, it is natural to extend them to generate domain flow, which generates intermediate domains and bridges the source and target domains, as illustrated in Fig. 1(b). ", "page_idx": 0}, {"type": "image", "img_path": "SgODU2mx9T/tmp/d5c754361685422d5a08717d1628385d3601f8ff2aa0fa9f6ee6d81c3da0af3d.jpg", "img_caption": ["Figure 1: Illustration of the proposed Terra. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "However, previous methods [36, 80] require multiple LoRAs to customize multiple domains, since a single LoRA cannot effectively express knowledge of multiple domains with a plugin [77]. To address this limitation, as illustrated in Fig. 1(a), we propose a Time-varying low-rank adapter (Terra), which offers a framework for gradual domain transferring by constructing a continuous parameter manifold. Instead of training multiple LoRAs for different domains, Terra maintains the parameter efficiency. To this end, inspired by the perspective of dynamic flows [75], Terra introduces a time variable $t$ for each domain and incorporates a square matrix that varies with time $t$ within the original low-rank structure. ", "page_idx": 1}, {"type": "text", "text": "As depicted in Fig. 1(b), Terra enables the use of different time values $t$ for various intermediate domains. Consequently, Terra can generate intermediate images that are natural and smooth when morphing in image pairs, subjects, and styles. For UDA tasks, we generate target samples and transform the source samples into the target domain to form an expanded source domain. Due to the smaller domain shifts, transferring from the expanded source domain to the target domain can improve the performance of existing UDA methods. For DG tasks, we interpolate among all source domains to generate images in various styles. Then, the generated samples are combined with the source domain images to improve the performance of existing DG methods. ", "page_idx": 1}, {"type": "text", "text": "In summary, our contributions are four-fold: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce Terra, a novel framework that integrates a square matrix with a time variable $t$ into the original low-rank structure, facilitating effective and flexible knowledge sharing across different domains while maintaining parameter efficiency.   \n\u2022 We provide a theoretical analysis of the expressive power of Terra, comparing it to LoRA.   \n\u2022 We demonstrate the application of Terra in image transformation and generation for UDA tasks and image interpolation for DG tasks via Terra, respectively.   \n\u2022 Extensive experiments validate the effectiveness of Terra across various tasks, including generative interpolation, unsupervised domain adaptation, and domain generalization. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Fine-Tuning of Text-to-Image Diffusion Models. The impressive performance of diffusion models [24, 55] has sparked a surge of interest in text-to-image generation tasks. As the demand for personalized content synthesis grows [83], pioneer works such as Textual Inversion [11] and DreamBooth [49] have proposed optimized text embedding and full fine-tuning frameworks to generate subject images with limited reference samples. Recently, several parameter-efficient methods for fine-tuning diffusion modules have been proposed, including adapters [54], LoRA [17, 50, 52], singular value decomposition on weight matrices [20], subsets of cross-attention [56, 32], and image prompt adapter [81, 76, 37, 66]. Among those methods, several have been developed to address the challenges of multi-concept generation [32, 20, 17] and natural image interpolation [62, 30, 80, 88]. Different from those methods, Terra focuses on generation and interpolation within domain flows. ", "page_idx": 1}, {"type": "text", "text": "Domain Adaptation and Generalization. UDA [74, 34, 13, 67, 87, 63] is designed to address the challenge of adapting models trained on labeled source domains to unlabeled target domains. The central premise of UDA methods is to learn domain-invariant features that minimize the domain gap. UDA approaches primarily fall into two branches: discrepancy-based methods [34, 72, 93, 19] and adversarial-based methods [13, 46, 85]. Conversely, DG [64, 89] seeks to train models that could generalize well to unseen target domains using multiple source domains. Effective DG methods, such as SWAD [5] and SAGM [65] enhance the generalization by identifying and leveraging flatter minima of training losses landscapes. However, the performance of UDA and DG methods can be constrained by the availability of training data. To address this limitation, recent data augmentation techniques [73, 71, 84, 36] have been developed to improve the transfer effects of UDA and DG methods. Those methods can be categorized into feature-level [71, 95, 42] and imagelevel methods [73, 84, 36, 22], which enhance transfer performance through the transformation or generation of auxiliary samples at the feature and image levels. For instance, MSGD [71] and GGF [95] use intermediate domains to gradually reduce the domain shift between the source and target domains, while BDG [73] employs pairs of cross-domain generators to synthesize domainspecific data based on the other domains. Additionally, CDGA [22] leverages the latent diffusion model to generate synthetic samples across domains and Domaindiff [36] trains LoRAs for each source domain to conduct domain fusion. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Preliminary ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "LoRA [25] uses two low-rank matrices, $W_{\\mathrm{down}}\\in\\mathbb{R}^{r\\times n}$ and $W_{\\mathrm{up}}\\in\\mathbb{R}^{m\\times r}$ , where $r\\ll\\operatorname*{min}(m,n)$ , to compute the weight matrix updates $\\Delta W=W_{\\mathrm{up}}W_{\\mathrm{down}}\\in\\mathbb{R}^{m\\times n}$ . The forward pass of the new weights changes from $h=W_{0}{\\pmb x}$ to: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h=W_{0}\\pmb{x}+\\alpha\\Delta W\\pmb{x}=W_{0}\\pmb{x}+\\alpha W_{\\mathrm{up}}W_{\\mathrm{down}}\\pmb{x},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\alpha$ is a scaling factor for the magnitude of the changes applied to the original weights. Although LoRA is primarily used for fine-tuning large language models, it is also employed in diffusion models for personalizing image generators with limited training samples, targeting specific styles or subjects [49, 52, 80]. The objective function in previous studies is expressed as noise matching: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\Delta\\theta)=\\mathbb{E}_{{x_{0}},\\tau\\sim\\mathcal{U}(1,T),c,\\epsilon\\sim\\mathcal{N}(0,1)}\\left[\\left\\|\\epsilon-\\epsilon_{\\theta_{0}+\\Delta\\theta}\\left({x_{\\tau}},\\tau,e(c)\\right)\\right\\|_{2}^{2}\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\theta_{0}$ and $\\Delta\\pmb{\\theta}$ denote the parameters of the text-to-image diffusion model and LoRA, respectively. The function $e$ denotes the text encoder, and $c$ corresponds to the text prompt. During the forward diffusion process, th\u221ae variable $\\mathbf{\\nabla}x_{\\tau}$ is obtained by gradually adding noise to the initial image $\\pmb{x}_{0}$ using the equation ${\\pmb x}_{\\tau}=\\sqrt{\\bar{\\alpha}_{\\tau}}{\\pmb x}_{0}+\\sqrt{1-\\bar{\\alpha}_{\\tau}}\\epsilon$ . Here $\\alpha_{\\tau}$ follows a decreasing schedule, and $\\bar{\\alpha}_{\\tau}$ is calculated as the cumulative product of $\\alpha$ values up to timestep $\\tau$ . In the objection function, the timestep $\\tau$ is sampled from a uniform distribution $\\bar{\\mathcal{U}}(1,T)$ , where $T$ denotes the total number of timesteps. And the model is utilized to predict the noise $\\epsilon_{\\theta_{0}+\\Delta\\theta}$ to estimate the true noise $\\epsilon$ . After training, the well-trained denoiser $\\theta_{0}+\\Delta\\theta$ can denoise noises and generate images within a few sampling steps. ", "page_idx": 2}, {"type": "text", "text": "3.2 Terra: Time-Varying Low-Rank Adapter ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To address the need for fine-tuning diffusion models across multiple domains while maintaining the parameter efficiency, we propose the Terra, as depicted in Fig. 1(a). Terra involves constructing a LoRA flow that provides a parameter manifold by incorporating time-varying updates as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h(t)=W_{0}x+\\Delta W(t)x=W_{0}x+W_{\\mathrm{up}}K(t)W_{\\mathrm{down}}x,\\;\\;K(t)=\\mathcal{F}(W_{\\mathrm{mid}},t)}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $W_{\\mathrm{mid}}\\,\\in\\,\\mathbb{R}^{r\\times r}$ , $t$ is a one-parameter variable, and $\\mathcal{F}$ is a time-dependent function. This formulation enables the differentiable evolution of the parameters $\\Delta W(t)$ based on a middle timevarying matrix $\\kappa(t)$ . A simple form of $\\mathcal{F}(W,t)$ is $t W+I$ , where $\\boldsymbol{\\mathit{I}}$ represents an identity matrix. Since $r\\ll\\operatorname*{min}(m,n)$ , the parameter difference between Terra and LoRA with the same rank is negligible. Furthermore, by setting the parameter $t$ to 0, Terra will degenerate to LoRA. It is worth noting that the form $\\mathcal{F}(W,t)$ here is just one of the possible variations. More forms can be found in Table 5 of Appendix $\\mathrm{B}$ and a comparison with MoE-based LoRA [69] is provided in Appendix E. ", "page_idx": 2}, {"type": "text", "text": "Here, we present a theoretical analysis of the expression power of the proposed Terra. We define $\\scriptstyle{I_{r}}$ as a diagonal matrix with its first $r$ diagonal entries as 1 and the remaining entries as 0. In the following theorem, we prove that Terra can effectively implement two LoRAs for specific downstream tasks by constructing a parameter manifold with reduced parameters. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1. (The Equivariance between Terra and Multiple LoRAs) Assume there exist two LoRAs $\\Delta\\b{W_{A}},\\Delta\\b{W_{B}}\\in\\mathbb{R}^{m\\bar{\\times}n}$ with ranks of $p$ and $q_{\\cdot}$ , respectively, that effectively solve two specific downstream tasks. Let $k=\\mathrm{max}\\{\\mathrm{rank}([\\Delta W_{A}\\ \\Delta W_{B}])$ , rank $\\overset{\\cdot}{\\underset{\\cdot}{\\left[\\Delta W_{A}^{T}\\right.}\\Delta W_{B}^{T}\\overset{\\cdot}{\\underset{\\cdot}{\\left[\\right]}}\\right)}$ , where rank $(\\cdot)$ denotes the rank of a matrix. Then, there exists a Terra with $W_{u p}\\in\\mathbb{R}^{m\\times k}$ , $W_{d o w n}\\in\\mathbb{R}^{k\\times n}$ , $W_{m i d}\\in\\mathbb{R}^{k\\times k}$ , and ${\\boldsymbol{K}}(t)=t{\\boldsymbol{W}}_{m i d}+{\\boldsymbol{I}}_{r}$ , such that the updated matrix $\\mathbf{\\Delta}\\Delta\\mathbf{W}(t)=\\mathbf{W}_{u p}\\boldsymbol{K}(t)\\mathbf{W}_{d o w n},$ , can simultaneously solve the two downstream task, that is, we have $\\Delta W(0)=\\Delta W_{A}$ and $\\Delta W(1)=\\Delta W_{B}$ . ", "page_idx": 3}, {"type": "text", "text": "In Theorem 1, the number of trainable parameters of Terra is governed by $|\\Theta|=(m+n)k+k^{2}$ , contrasting with that of two LoRAs $|\\Theta|=(m+n)(p+q)$ . Note that $k$ represents the maximum rank of the matrices obtained by concatenating the row and column spaces of the two LoRA matrices, which is not greater than the sum of the ranks of the two LoRA matrices, i.e., $k\\leq p+q$ . ", "page_idx": 3}, {"type": "text", "text": "Drawing inspiration from prior research on the expressive power of LoRA [78], we further demonstrate the expressive power of Terra. Here, we focus on the multi-layer feedforward neural network with identity activation functions, and the analysis can be extended to fully connected neural networks and transformer networks [78]. Assuming that the target models $\\bar{f}_{A}$ and $\\overline{{f}}_{B}$ for two specific tasks, as well as the frozen model $f_{0}$ , are linear, they can be represented as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\bar{f}_{A}(\\pmb{x})=\\overline{{W}}_{A}\\pmb{x},\\quad\\bar{f}_{B}(\\pmb{x})=\\overline{{W}}_{B}\\pmb{x},\\quad f_{0}(\\pmb{x})=W_{L}\\cdot\\cdot\\pmb{\\cdot}W_{1}\\pmb{x}=\\left(\\prod_{l=1}^{L}W_{l}\\right)\\pmb{x},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the frozen model has $L$ layers with consistent dimensions. We define the error matrices $\\begin{array}{r}{E_{A}:=\\overline{{W}}_{A}-\\prod_{l=1}^{L}W_{l}}\\end{array}$ , and $\\begin{array}{r}{E_{B}:=\\overline{{W}}_{B}-\\prod_{l=1}^{L}W_{l}}\\end{array}$ , and their ranks as $R_{E_{A}}\\,=\\,\\mathrm{rank}(E_{A})$ and $R_{E_{B}}=\\mathrm{rank}(E_{B})$ . By utilizing Terra $\\Delta W(t)$ , we can modify the pre-trained frozen model to closely approximate the two target models $\\overline{{W}}_{A}$ and $\\overline{{W}}_{B}$ . We denote the $d$ -th largest singular value of $W$ by $\\sigma_{d}(W)$ , and the best rank- $r$ approximation [8] of $W$ by $\\mathrm{LR}_{r}(W)$ . The following theorem presents an upper bound for the approximation error with a rank- $k$ Terra. ", "page_idx": 3}, {"type": "text", "text": "Theorem 2. (The Expressive Power of Terra) For each layer l, the rank- $k$ Terra has updated matrix $\\Delta W(t)_{l},$ and the function of time-varying matrix is ${\\cal K}(t)_{l}=t{\\cal W}_{m i d,l}+\\tilde{{\\cal I}}$ . Assume that all weight matrices of the frozen model $(W_{l})_{l=1}^{L}$ , $\\begin{array}{r}{\\prod_{l=1}^{L}W_{l}+\\mathrm{LR}_{r}(E_{A})}\\end{array}$ , and $\\begin{array}{r}{\\prod_{l=1}^{L}W_{l}+\\mathrm{LR}_{r}(E_{B})}\\end{array}$ are non-singular for all $r\\leq k(L-1)$ . Then the approximation error satisfies ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\Delta W(t)}\\left(\\left\\|\\prod_{l=1}^{L}(W_{l}+\\Delta W(0)_{l})-\\overline{{W}}_{A}\\right\\|_{2}+\\left\\|\\prod_{l=1}^{L}(W_{l}+\\Delta W(1)_{l})-\\overline{{W}}_{B}\\right\\|_{2}\\right)\\leq2\\sigma_{k L+1}^{*},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the $\\sigma_{k L+1}^{*}$ as the $(k L+1)$ - $^{t h}$ largest singular values obtained by merging the singular values of $E_{A}$ and $E_{B}$ . Moreover, when $\\begin{array}{r}{k\\ge\\left\\lceil\\frac{R_{E_{A}}+R_{E_{B}}}{L}\\right\\rceil}\\end{array}$ , the approximation error is zero. ", "page_idx": 3}, {"type": "text", "text": "We compare the approximation errors of Terra and multiple LoRAs with consistent parameter sizes for the above target models. We consider a rank of $2k$ for Terra and two $k$ -rank LoRA in both tasks. Prior work [78] establishes an upper bound on LoRA\u2019s approximation error as $\\sigma_{k L+1}({\\cal E}_{\\cal A})+\\sigma_{k L+1}({\\cal E}_{\\cal B})$ . In Theorem 2, we demonstrate that Terra\u2019s approximation error bound is $2\\sigma_{2k L+1}^{*}$ . Considering the definition of $\\sigma^{*}$ , it is evident that our Terra\u2019s error bound is not greater than LoRA\u2019s. ", "page_idx": 3}, {"type": "text", "text": "Terra is capable of cross-domain generative tasks, where samples from different domains possess different $t$ \u2019s. In the following sections, we show the use of Terra in three different learning problems. ", "page_idx": 3}, {"type": "text", "text": "3.3 Warm Up: Constructing Evolving Visual Domains via Terra ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we show the first application of Terra to construct evolving visual domains for generative interpolation between two image domains $\\mathcal{D}_{S}$ and $\\mathcal{D}_{T}$ characterized by the differences in the style or subject, which is the key to apply Terra to UDA and DG. ", "page_idx": 3}, {"type": "image", "img_path": "SgODU2mx9T/tmp/c60d97eb81043429f00f1e49e4a34899d89d206debc34aa4274a5f51a12e32b8.jpg", "img_caption": ["Figure 2: The illustration of the training process of constructing evolving visual domains via Terra. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Our method is different from existing methods [53, 55, 88, 80] that employ direct interpolation between two images on embedding using spherical linear interpolation (a.k.a slerp). To accomplish this, Terra incorporates a continuous time variable $t$ . Training on the source images involves setting $t$ to 0, yielding the formulation $\\Delta W(0)\\,=\\,W_{\\mathrm{up}}\\chi(0)W_{\\mathrm{down}}\\,$ . Similarly, for the target images, $t$ is set to 1, leading to $\\Delta W(1)\\,=\\,W_{\\mathrm{up}}\\dot{\\cal K}(1)W_{\\mathrm{down}}^{\\cdot}$ . In the context of fine-tuning text-to-image diffusion models, we employ image descriptions to construct prompts for diffusion models, where the corresponding class label is denoted by \u201cA [class]\u201d, where \u201c[class]\u201d denotes the placeholder for the class label. Finally, the training objective, as depicted in Fig. 2, is formulated as follows ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{L}(\\Delta\\theta)=\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(0,1),\\tau\\sim\\mathcal{U}(1,T)}\\left[\\mathbb{E}_{\\mathbf{x}_{0}^{S}\\sim\\mathcal{D}_{S},t=0}\\left\\|\\epsilon-\\epsilon_{\\theta_{0}+\\Delta\\theta}\\left(x_{\\tau}^{S},\\tau,e(c^{S}),t\\right)\\right\\|_{2}^{2}\\right.}&{}\\\\ {\\left.+\\mathbb{E}_{\\mathbf{x}_{0}^{T}\\sim\\mathcal{D}_{T},t=1}\\left\\|\\epsilon-\\epsilon_{\\theta_{0}+\\Delta\\theta}\\left(x_{\\tau}^{T},\\tau,e(c^{T}),t\\right)\\right\\|_{2}^{2}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\Delta\\pmb{\\theta}$ represents the parameters of the Terra, $c^{S}$ and $c^{T}$ denote the text prompts for the source and target, and $x_{0}^{S}$ and $x_{0}^{\\bar{T}}$ represent the source and target samples. Formally, we construct evolving visual domains by the following two stages: (1) Fine-tune the parameters of Terra (i.e., $\\Delta\\pmb{\\theta}\\,=$ $W_{u p}\\cup W_{m i d}\\cup W_{d o w n})$ using Eq. (5), where the first part with $t=0$ uses source samples $\\mathcal{D}_{S}$ and the second part with $t=1$ uses target samples $\\mathcal{D}_{T}$ . (2) Generate an intermediate domain by uniformly sampling $t$ from $[0,1]$ and inputting the text prompt and a random noise into the fine-tuned diffusion model corresponding to domain $t$ for the backward process. ", "page_idx": 4}, {"type": "text", "text": "3.4 Generation-based Unsupervised Domain Adaptation via Terra ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Built on the first application introduced in the previous section, we introduce the second application of Terra in UDA. Under the UDA setting, we have a labeled source domain $\\mathcal{D}_{S}$ and an unlabeled target domain $\\mathcal{D}_{T}$ . To alleviate domain shifts, we propose a two-stage framework utilizing a generationbased approach to augment the source domain. ", "page_idx": 4}, {"type": "text", "text": "Similar to the construction of evolving domains discussed in Section 3.3, the first stage sets out to train the parameters of Terra that accommodate source domain generation with $t=0$ and target domain generation with $t=1$ . This enables the generation of target images according to the class labels and transitive source images into the target domain. However, due to the polysemous words on the class labels, directly generating images with the text prompt may cause unexpected results. For example, \u201cmouse\u201d usually refers to a rodent, but in some datasets, it refers to a computer mouse. Therefore, we leverage the source samples to conduct semantic alignment between images and class labels while the unlabeled target domain samples contribute to learning style information for fine-tuning the diffusion model. To achieve this, we adopt the same objective function as Eq. (5), where we set $t=0$ for source training with the prompt \u201cA [class]\u201d and $t=1$ for target training with the prompt \u201cAn image\u201d. ", "page_idx": 4}, {"type": "text", "text": "The second stage involves synthesizing a transitive source domain that can benefit the learning of UDA methods, as depicted in Fig. 3(a). We employ two approaches to achieve this. First, we set $t=1$ to synthesize target samples from Gaussian noises for each category with the corresponding prompt, i.e., \u201cA [class]\u201d. Those synthesized samples constitute a generated target domain denoted by $\\mathcal{D}_{\\hat{T}}$ . Second, we transform the source samples into the target domain while preserving semantic information. This is achieved by first setting $t=0$ and applying DDIM inversion [55] to convert the source images into noise. Then, with setting $t=1$ , we use the diffusion model equipped with Terra to denoise, resulting in the adapted source domain $\\mathcal{D}_{\\hat{S}}$ . After generating images, we combine the adapted source domain and the generated target domain to form a transitive source domain $\\begin{array}{r}{\\mathcal{D}_{E}=\\mathcal{D}_{\\hat{S}}\\cup\\mathcal{D}_{\\hat{T}}}\\end{array}$ . Here the transitive source domain could have a smaller domain gap to the target domain than the original source domain due to the generation process, which could facilitate the knowledge transfer from the transitive source domain to the target domain. ", "page_idx": 4}, {"type": "image", "img_path": "SgODU2mx9T/tmp/a7e1134a2b9436fc499f4f28402b3582f3f178ee5e0c48c67086ca7f2bfe8ffb.jpg", "img_caption": ["Figure 3: The illustration of the proposed generation-based UDA and DG frameworks via Terra. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Finally, we conduct transfer learning from the transitive source domain to the target domain by using an existing UDA method. The objective function is formulated as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{f}_{u d a}=\\underset{f}{\\arg\\operatorname*{min}}\\ \\frac{1}{|\\mathcal{D}_{E}|}\\sum_{(\\boldsymbol{x},\\boldsymbol{y})\\in\\mathcal{D}_{E}}\\ell_{c e}(f(\\boldsymbol{x}),\\boldsymbol{y})+\\beta\\ell_{u d a}(\\mathcal{D}_{E},\\mathcal{D}_{T}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\ell_{c e}(\\cdot,\\cdot)$ denotes the cross-entropy loss, $\\beta\\,>\\,0$ is a trade-off parameter, and $\\ell_{u d a}(\\cdot,\\cdot)$ is a transfer loss (e.g., domain discrepancy loss [34, 72, 93] and domain discrimination loss [13, 46, 85]) used to alleviate the domain shift. In this manner, our method can be integrated with any off-the-shelf UDA methods to enhance the transfer performance. ", "page_idx": 5}, {"type": "text", "text": "3.5 Generation-based Domain Generalization via Terra ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we study the application of Terra to DG problems. Under the DG setting, we have $K$ source domains $\\{\\mathcal{D}_{k}^{\\enspace\\enspace\\cdot}=\\{(\\dot{\\mathbf{x}_{i}^{k}},y_{i}^{k})\\}_{i=1}^{n_{k}}\\}_{k=1}^{K}$ , where $n_{k}$ denotes the number of samples in $\\mathcal{D}_{k}$ . To enhance the generalization capability, as shown in Fig. 3(b) and detailed as follows, Terra is adopted to synthesize new source domains by interpolating among existing source domains. Consequently, we expect a more generalized learner that well adapts to both existing and synthesized source domains. ", "page_idx": 5}, {"type": "text", "text": "In the first stage, to accommodate the various styles exhibited by multiple source domains, we utilize a network $g(\\cdot)$ to predict sample-level $\\pmb{t}$ for the Terra. The $\\pmb{t}$ -predictor $g(\\cdot)$ aims to generate similar $\\pmb{t}$ values for images from the same domain. Moreover, due to the diverse range of styles in the training set, each $\\pmb{t}=g(\\pmb{x})$ is represented as a vector instead of a scalar value used in previous settings. This allows us to better capture various styles and intra-domain differences. Specifically, we train the network $g(\\cdot)$ via contrastive learning and the loss function to be minimized is formulated as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{c o n}(g)=\\sum_{k=1}^{K}\\sum_{i=1}^{n_{k}}\\left(\\sum_{j=1}^{n_{k}}\\lVert g(\\pmb{x}_{i}^{k})-g(\\pmb{x}_{j}^{k})\\rVert_{2}+\\sum_{l=1}^{K}\\sum_{m=1}^{n_{l}}\\operatorname*{max}(0,\\delta-\\lVert g(\\pmb{x}_{i}^{k})-g(\\pmb{x}_{m}^{l})\\rVert_{2})\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\delta$ is a predefined positive margin and $\\lVert\\cdot\\rVert_{2}$ denotes the Euclidean distance. In Eq. (7), the first term in the sum is to enforce samples from the same domain yield similar outputs, while the second term is to encourage the distance between the outputs corresponding to samples from two domains to be larger than the margin via the hinge loss. ", "page_idx": 5}, {"type": "image", "img_path": "SgODU2mx9T/tmp/ba19aedc799c9abfca7e0c498e465ccf3877a153e68d5467e5d4d91d4bb56040.jpg", "img_caption": ["Figure 4: Qualitative evaluation. The three rows illustrate examples of morphing in image pairs, subjects, and styles, respectively. The text on the left side represents the training prompts, with the red text indicating detailed descriptions used during inference. Additional examples and comparisons with other methods can be found in Appendix C.1. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Upon learning of the $g(\\cdot)$ , we can obtain $\\pmb{t}$ \u2019s for all the samples in all the source domains. Then based on $\\pmb{t}$ \u2019s, we fine-tune the diffusion model using Terra with the prompt \u201cA [class]\u201d, and the training objective is formulated as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\Delta\\theta)=\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(0,1),\\tau\\sim\\mathcal{U}(1,T)}\\left[\\sum_{k=1}^{K}\\mathbb{E}_{x_{0}\\sim\\mathcal{D}_{k},t=g(x_{0})}\\left\\lVert\\epsilon-\\epsilon_{\\theta_{0}+\\Delta\\theta}\\left(x_{\\tau},\\tau,e(c),t\\right)\\right\\rVert_{2}^{2}\\right].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "After fine-tuning, in the second stage, we set $\\pmb{t}$ to various values to generate diverse samples for each category with the corresponding prompt. The generated samples could originate from various domains which may be beyond the original source domains $\\{\\mathcal{D}_{k}\\}_{k=1}^{\\tilde{K}}$ but we do not need to identify their specific domains. We combine these generated samples with the original source domain samples to form expanded domains $\\mathcal{D}_{E}$ , which can improve the generalization capability of models. The objective function of DG based on Terra is formulated as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{f}_{d g}=\\underset{f}{\\arg\\operatorname*{min}}\\frac{1}{|\\mathcal{D}_{E}|}\\sum_{(\\pmb{x},y)\\in\\{\\mathcal{D}_{E}\\}}\\ell_{c e}(f(\\pmb{x}),y)+\\beta\\ell_{d g}(\\mathcal{D}_{E}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\beta>0$ is a trade-off parameter, and $\\ell_{d g}(\\cdot)$ is a domain generalization loss (e.g., SharpnessAware Minimization (SAM)-based loss [10, 5, 65] and representation learning-based loss [1, 13, 2]) used to improve the generalization capabilities. In this manner, our method can be integrated with any off-the-shelf DG methods to enhance their performance. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Experimental Setups ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "For the UDA experiments, we utilize three benchmark datasets, including Office31 [51], which consists of 4,110 images from 31 categories across three domains: Amazon (A), Webcam (W), and Dslr (D); Office-Home [59], containing 15,588 images from 65 categories across four domains: Art (Ar), Clipart (Cl), Product $(\\mathrm{Pr})$ , and Real-World (Rw); and VisDA [43], featuring 207,785 images from 12 categories across two domains: Synthetic and Real. For the DG experiments, we employ the PACS [33], Office-Home, and VLCS [9] datasets. The PACS dataset contains 9,991 images from seven categories across four domains: Art painting (A), Cartoon (C), Photo (P), and Sketch (S), and VLCS contains 10,729 images from five categories across four domains: VOC2007 (V), LabelMe (L), Caltech101 (C), and SUN09 (S). The baselines and implementation details are put in Appendix B. ", "page_idx": 6}, {"type": "text", "text": "4.2 Experiments on Generative Interpolation Tasks ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "For generative interpolation tasks, we conduct qualitative and quantitative evaluations of our method, focusing on morphing in image pairs, subjects, and styles. ", "page_idx": 7}, {"type": "text", "text": "For morphing in image pairs, we train Terra by setting $t=0$ for the first image and $t=1$ for the second one with a text prompt \u201cAn oil painting of a person\u201d. After training, we produce intermediate images by uniformly transitioning $t$ from 0 to 1 with the same text prompt. The experimental results can be found in the first row of Fig. 4. We also provide qualitative comparisons with other baselines in Fig. 8. As can be seen, Terra produces natural and smooth interpolation between two images. ", "page_idx": 7}, {"type": "text", "text": "In addition to its ability to perform image morphing, Terra can perform style and subject morphing, a capability that DiffMorpher [80] lacks. Due to page limit, implementation details are put in Appendix B. As shown in the second and third rows of Fig. 4, Terra is capable of generating a sequence of intermediate images as a seamless transition in styles and subjects. ", "page_idx": 7}, {"type": "text", "text": "To quantitatively evaluate the quality of the intermediate images and the smoothness of the transition, we utilize the Frechet Inception Distance (FID) [23] and Perceptual Path Length (PPL) [29] metrics, following the setting in DiffMorpher [80]. As shown in Table 1, the quantitative results demonstrate that Terra achieves comparable performance to DiffMorpher and outperforms DGP, DDIM, and LoRA Interpolation. Note that DiffMorpher is specifically designed for morphing by customized techniques such as attention interpolation, adaptive normalization, and a new sampling schedule. Equipped with the customized techniques used in DiffMorpher, Terra is even better than DiffMorhper. ", "page_idx": 7}, {"type": "text", "text": "Table 1: Quantitative evaluation of generative interpolation tasks. We evaluate the fidelity and smoothness of the generated intermediate images in terms of FID $\\left(\\downarrow\\right)$ and PPL (\u2193). ", "page_idx": 7}, {"type": "table", "img_path": "SgODU2mx9T/tmp/cacc94b67b9ee84c6c634a7ed0cfec74103b03cf263f30645c22687b5679cedf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "SgODU2mx9T/tmp/85465a4678326c2c62e683c875a7f4b1569abb0a24a44c847feb096dfc2579e3.jpg", "table_caption": ["Table 2: Transfer accuracies $(\\%)$ on the Office-Home and VisDA datasets under UDA setting. The best performance is highlighted in bold. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "4.3 Experiments on Unsupervised Domain Adaptation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we evaluate the proposed generation-based UDA method via Terra as introduced in Section 3.4. The comparison results against state-of-the-art UDA methods on the Office-Home and VisDA datasets are shown in Tables 2. Due to page limit, detailed results for VisDA and Office31 are shown in Tables 7 and 8 of Appendix C.2 and more results with $\\mathrm{{CoVi}}$ and PMTrans are shown in Table 10. The standard deviations from three experiments are presented in Appendix D. ", "page_idx": 7}, {"type": "text", "text": "As can be seen, our method has achieved significant performance improvements of $4.42\\%$ , $3.46\\%$ , and $1.07\\%$ for ELS on the Office-Home, VisDA, and Office31 datasets, respectively, surpassing all the baseline methods. Thus, Terra can serve as a good plugin for existing UDA methods. ", "page_idx": 7}, {"type": "text", "text": "The effectiveness of our method can be further verified through the t-SNE [57] visualizations, as depicted in Fig. 5. The adapted source domain $\\mathcal{D}_{\\hat{S}}$ and the generated target domain $\\mathcal{D}_{\\hat{T}}$ exhibit a smaller domain discrepancy to the target domain $\\mathcal{D}_{T}$ than the original source domain, thereby reducing the domain gaps. Additionally, Fig. 6 presents example images illustrating the transformation from the source domain to the target domain. It can be observed that the style transfer is achieved while preserving the semantic information and subject shapes. ", "page_idx": 7}, {"type": "image", "img_path": "SgODU2mx9T/tmp/479169e24b64737f28ee187e2c06212b8e1e9272fc53c9cb95e933430c1a755a.jpg", "img_caption": ["Figure 5: T-SNE visualization of the source domain, target domain, adapted source domain, and generated target domain in four classes of the $\\mathrm{Pr}{\\rightarrow}\\mathrm{Cl}$ task on Office-Home under UDA setting. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "SgODU2mx9T/tmp/d566e56f0ef92a650e5bed49677e04d87eda652230773fd98e0cf17eb24de25d.jpg", "img_caption": ["Figure 6: Examples of the source images from $\\mathcal{D}_{S}$ and corresponding adapted images from $\\mathcal{D}_{\\hat{S}}$ for the Office-Home tasks under UDA setting. The text prompts are shown on the left. For instance, the first image pair showcases an image from the Art domain and its corresponding generated image to Clipart domain based on the text prompt \u201cAn alarm clock\u201d. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "SgODU2mx9T/tmp/102719ace7c8439050dec4191e37d0c9f332b3e9a405eab981cfa0bc1f1bfb33.jpg", "table_caption": ["Table 3: Ablation studies on the Office-Home dataset under UDA setting. The best is in bold. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "The ablation studies of $\\mathrm{ELS+}$ Terra presented in Table 3 show that the best performance is achieved when transferring from the expanded domain $\\mathcal{D}_{E}$ to the target domain $\\mathcal{D}_{T}$ , validating the necessity and effectiveness of combining the adapted source domain with the generated target domain. To highlight the design advantages, we conduct a comparison with SDXL\u2019s prior knowledge in Appendix C.5. ", "page_idx": 8}, {"type": "text", "text": "4.4 Experiments on Domain Generalization ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we conduct experiments on the PACS, Office-Home, and VLCS datasets to evaluate the effectiveness of our DG method proposed in Section 3.5. The results presented in Table 4 clearly reveal that our method achieves notable performance improvements across all tasks based on three state-of-the-art DG methods (i.e., ERM, SWAD, and SAGM). Furthermore, our method outperforms the stable diffusion generation-based method, DomainDiff, which requires training a separate LoRA for each source domain while our method maintains the parameter efficiency with only one single low-rank structure and different $\\pmb{t}$ to capture diverse styles. ", "page_idx": 8}, {"type": "table", "img_path": "SgODU2mx9T/tmp/63d3d7fc941bc95f780fdba7ac6d0d9ac0bd44698b1c5933ffe51dad2324671b.jpg", "table_caption": ["Table 4: Accuracies $(\\%)$ on the PACS and OfficeHome datasets under DG setting. The best is in bold. "], "table_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "", "img_caption": ["Figure 7: Visualization of learned time variables on the PACS dataset under the DG setting. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Additionally, Fig. 7 shows the learned values of $\\pmb{t}$ . The $\\pmb{t}$ predictor assigns distinct $\\pmb{t}$ values to each domain, enabling Terra to generate different interpolated images among the source domains based on varying $\\pmb{t}$ values. Moreover, the random sampling of $\\pmb{t}$ effectively covers the target domain, offering a clearer understanding of the rationale behind our approach that the generated samples may bring useful information for the target domain. We also show some generated images of the expanded domains on the $P A C S$ dataset in Fig. 10 of Appendix C. As can be seen, using Terra can generate diverse styles of images that are different from the source domains. With the expanded domains, the generalization capability of the source model can be improved. ", "page_idx": 9}, {"type": "text", "text": "Besides, we conduct an ablation study on the form of Terra and the dimensionality of $\\pmb{t}$ in Appendix B, demonstrating that refining Terra\u2019s form can further enhance its expressive power. We also compare Terra with other domain generalization morphing techniques, as shown in Appendix C.4, to verify its effectiveness in expanding source domains for improved generalization. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion and Future Works ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduce Terra, a framework that facilitates effective cross-domain modeling through the construction of a continuous parameter manifold. Terra incorporates a time-varying parameter within the manifold of domains, enabling flexible and smooth interpolations. This approach facilitates effective knowledge sharing across different domains by training only a single low-rank adaptor. Additionally, based on the designed generation-based strategies, Terra can serve as a plugin for existing UDA and DG methods to enhance performance. We also theoretically analyze the expressive capabilities of Terra. Extensive experiments demonstrate the superior performance of Terra in a range of tasks. For future works, we aim to extend Terra to cover more settings, including different modalities, larger datasets, and more complex tasks. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by NSFC key grant 62136005 and NSFC general grant 62076118. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] M. Arjovsky, L. Bottou, I. Gulrajani, and D. Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019.   \n[2] G. Blanchard, A. A. Deshmukh, U. Dogan, G. Lee, and C. Scott. Domain generalization by marginal transfer learning. Journal of Machine Learning Research, 22(2):1\u201355, 2021.   \n[3] A. Brock, J. Donahue, and K. Simonyan. Large scale gan training for high fidelity natural image synthesis. In International Conference on Learning Representations, 2018.   \n[4] H. Cao, C. Tan, Z. Gao, Y. Xu, G. Chen, P.-A. Heng, and S. Z. Li. A survey on generative diffusion models. IEEE Transactions on Knowledge and Data Engineering, 2024.   \n[5] J. Cha, S. Chun, K. Lee, H.-C. Cho, S. Park, Y. Lee, and S. Park. Swad: Domain generalization by seeking flat minima. Advances in Neural Information Processing Systems, 34:22405\u201322418, 2021.   \n[6] J. Cha, K. Lee, S. Park, and S. Chun. Domain generalization by mutual-information regularization with pre-trained models. In European Conference on Computer Vision, pages 440\u2013457. Springer, 2022.   \n[7] Z. Chen, H. Li, F. Wang, O. Zhang, H. Xu, X. Jiang, Z. Song, and E. H. Wang. Rethinking the diffusion models for missing data imputation: A gradient flow perspective. Advances in Neural Information Processing Systems, 38:1\u201350, 2024.   \n[8] C. Eckart and G. Young. The approximation of one matrix by another of lower rank. Psychometrika, 1(3):211\u2013218, 1936.   \n[9] C. Fang, Y. Xu, and D. N. Rockmore. Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In Proceedings of the IEEE International Conference on Computer Vision, pages 1657\u20131664, 2013.   \n[10] P. Foret, A. Kleiner, H. Mobahi, and B. Neyshabur. Sharpness-aware minimization for efficiently improving generalization. In International Conference on Learning Representations, 2020.   \n[11] R. Gal, Y. Alaluf, Y. Atzmon, O. Patashnik, A. H. Bermano, G. Chechik, and D. Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022.   \n[12] Y. Ganin and V. Lempitsky. Unsupervised domain adaptation by backpropagation. In International Conference on Machine Learning, pages 1180\u20131189. PMLR, 2015.   \n[13] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. March, and V. Lempitsky. Domain-adversarial training of neural networks. Journal of Machine Learning Research, 17(59):1\u201335, 2016.   \n[14] R. H. Goldman. Transformations as exponentials. Graphics Gems II, 2:332, 1991.   \n[15] R. Gong, W. Li, Y. Chen, and L. V. Gool. Dlow: Domain flow for adaptation and generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2477\u20132486, 2019.   \n[16] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, volume 27, pages 2672\u20132680, Jan. 2014.   \n[17] Y. Gu, X. Wang, J. Z. Wu, Y. Shi, Y. Chen, Z. Fan, W. Xiao, R. Zhao, S. Chang, W. Wu, et al. Mix-of-show: Decentralized low-rank adaptation for multi-concept customization of diffusion models. Advances in Neural Information Processing Systems, 36, 2024.   \n[18] I. Gulrajani and D. Lopez-Paz. In search of lost domain generalization. In International Conference on Learning Representations, 2021.   \n[19] P. Guo, J. Zhu, and Y. Zhang. Selective partial domain adaptation. In BMVC, page 420, 2022.   \n[20] L. Han, Y. Li, H. Zhang, P. Milanfar, D. Metaxas, and F. Yang. Svdiff: Compact parameter space for diffusion fine-tuning. arXiv preprint arXiv:2303.11305, 2023.   \n[21] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770\u2013778, 2016.   \n[22] S. Hemati, M. Beitollahi, A. H. Estiri, B. A. Omari, X. Chen, and G. Zhang. Cross domain generative augmentation: Domain generalization with latent diffusion models. arXiv preprint arXiv:2312.05387, 2023.   \n[23] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in Neural Information Processing Systems, 30, 2017.   \n[24] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840\u20136851, 2020.   \n[25] E. J. Hu, yelong shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022.   \n[26] T. Huang, J. Liu, S. You, and C. Xu. Active generation for image classification. arXiv preprint arXiv:2403.06517, 2024.   \n[27] Y. Huang, J. Huang, Y. Liu, M. Yan, J. Lv, J. Liu, W. Xiong, H. Zhang, S. Chen, and L. Cao. Diffusion model-based image editing: A survey. arXiv preprint arXiv:2402.17525, 2024.   \n[28] Y. Jin, X. Wang, M. Long, and J. Wang. Minimum class confusion for versatile domain adaptation. In European Conference on Computer Vision, pages 464\u2013480. Springer, 2020.   \n[29] T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8110\u20138119, 2020.   \n[30] B. Kawar, S. Zada, O. Lang, O. Tov, H. Chang, T. Dekel, I. Mosseri, and M. Irani. Imagic: Textbased real image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6007\u20136017, 2023.   \n[31] A. Kumar, T. Ma, and P. Liang. Understanding self-training for gradual domain adaptation. In International conference on machine learning, pages 5468\u20135479. PMLR, 2020.   \n[32] N. Kumari, B. Zhang, R. Zhang, E. Shechtman, and J.-Y. Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1931\u20131941, 2023.   \n[33] D. Li, Y. Yang, Y.-Z. Song, and T. M. Hospedales. Deeper, broader and artier domain generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5542\u20135550, 2017.   \n[34] M. Long, Y. Cao, J. Wang, and M. Jordan. Learning transferable features with deep adaptation networks. In International Conference on Machine Learning, pages 97\u2013105. PMLR, 2015.   \n[35] M. Long, Z. Cao, J. Wang, and M. I. Jordan. Conditional adversarial domain adaptation. Advances in Neural Information Processing Systems, 31, 2018.   \n[36] Q. Miao, J. Yuan, S. Zhang, F. Wu, and K. Kuang. Domaindiff: Boost out-of-distribution generalization with synthetic data. In IEEE International Conference on Acoustics, Speech and Signal Processing, pages 5640\u20135644. IEEE, 2024.   \n[37] C. Mou, X. Wang, L. Xie, Y. Wu, J. Zhang, Z. Qi, and Y. Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 4296\u20134304, 2024.   \n[38] A. Q. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. Mcgrew, I. Sutskever, and M. Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In International Conference on Machine Learning, pages 16784\u201316804. PMLR, 2022.   \n[39] C. C. Paige and M. A. Saunders. Towards a generalized singular value decomposition. SIAM Journal on Numerical Analysis, 18(3):398\u2013405, 1981.   \n[40] S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang. Domain adaptation via transfer component analysis. IEEE transactions on Neural Networks, 22(2):199\u2013210, 2010.   \n[41] X. Pan, X. Zhan, B. Dai, D. Lin, C. C. Loy, and P. Luo. Exploiting deep generative prior for versatile image restoration and manipulation. In European Conference on Computer Vision, 2020.   \n[42] D. Peng, Q. Ke, Y. Lei, and J. Liu. Unsupervised domain adaptation via domain-adaptive diffusion. arXiv preprint arXiv:2308.13893, 2023.   \n[43] X. Peng, B. Usman, N. Kaushik, J. Hoffman, D. Wang, and K. Saenko. Visda: The visual domain adaptation challenge. arXiv preprint arXiv:1710.06924, 2017.   \n[44] R. Penrose. A generalized inverse for matrices. In Mathematical proceedings of the Cambridge philosophical society, volume 51, pages 406\u2013413. Cambridge University Press, 1955.   \n[45] D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dockhorn, J. M\u00fcller, J. Penna, and R. Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023.   \n[46] H. Rangwani, S. K. Aithal, M. Mishra, A. Jain, and V. B. Radhakrishnan. A closer look at smoothness in domain adversarial training. In International Conference on Machine Learning, pages 18378\u201318399. PMLR, 2022.   \n[47] M. D. M. Reddy, M. S. M. Basha, M. M. C. Hari, and M. N. Penchalaiah. Dall-e: Creating images from text. UGC Care Group I Journal, 8(14):71\u201375, 2021.   \n[48] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.   \n[49] N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein, and K. Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 22500\u201322510, 2023.   \n[50] N. Ruiz, Y. Li, V. Jampani, W. Wei, T. Hou, Y. Pritch, N. Wadhwa, M. Rubinstein, and K. Aberman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models. arXiv preprint arXiv:2307.06949, 2023.   \n[51] K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting visual category models to new domains. In European Conference on Computer Vision, volume 6314, pages 213\u2013226. Springer, 2010.   \n[52] V. Shah, N. Ruiz, F. Cole, E. Lu, S. Lazebnik, Y. Li, and V. Jampani. Ziplora: Any subject in any style by effectively merging loras. In European Conference on Computer Vision, pages 422\u2013438. Springer, 2025.   \n[53] K. Shoemake. Animating rotation with quaternion curves. In Proceedings of the 12th annual conference on Computer graphics and interactive techniques, pages 245\u2013254, 1985.   \n[54] K. Sohn, N. Ruiz, K. Lee, D. C. Chin, I. Blok, H. Chang, J. Barber, L. Jiang, G. Entis, Y. Li, et al. Styledrop: Text-to-image generation in any style. arXiv preprint arXiv:2306.00983, 2023.   \n[55] J. Song, C. Meng, and S. Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2020.   \n[56] Y. Tewel, R. Gal, G. Chechik, and Y. Atzmon. Key-locked rank one editing for text-to-image personalization. In ACM SIGGRAPH 2023 Conference Proceedings, pages 1\u201311, 2023.   \n[57] L. Van der Maaten and G. Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(11), 2008.   \n[58] V. Vapnik. The nature of statistical learning theory. Springer science & business media, 1999.   \n[59] H. Venkateswara, J. Eusebio, S. Chakraborty, and S. Panchanathan. Deep hashing network for unsupervised domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5018\u20135027, 2017.   \n[60] V. Verma, A. Lamb, C. Beckham, A. Najaf,i I. Mitliagkas, D. Lopez-Paz, and Y. Bengio. Manifold mixup: Better representations by interpolating hidden states. In International Conference on Machine Learning, pages 6438\u20136447. PMLR, 2019.   \n[61] C. Wang, J. Gao, Y. Hua, and H. Wang. Cross-domain learning with normalizing flow. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1\u20135. IEEE, 2023.   \n[62] C. J. Wang and P. Golland. Interpolating between images with diffusion models. arXiv preprint arXiv:2307.12560, 2023.   \n[63] H. Wang, J. Fan, Z. Chen, H. Li, W. Liu, T. Liu, Q. Dai, Y. Wang, Z. Dong, and R. Tang. Optimal transport for treatment effect estimation. Advances in Neural Information Processing Systems, 36:1\u201315, 2023.   \n[64] J. Wang, C. Lan, C. Liu, Y. Ouyang, T. Qin, W. Lu, Y. Chen, W. Zeng, and S. Y. Philip. Generalizing to unseen domains: A survey on domain generalization. IEEE Transactions on Knowledge and Data Engineering, 35(8):8052\u20138072, 2022.   \n[65] P. Wang, Z. Zhang, Z. Lei, and L. Zhang. Sharpness-aware gradient matching for domain generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3769\u20133778, 2023.   \n[66] Q. Wang, X. Bai, H. Wang, Z. Qin, and A. Chen. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024.   \n[67] X. Wang, P. Guo, and Y. Zhang. Unsupervised domain adaptation via bidirectional crossattention transformer. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 309\u2013325. Springer, 2023.   \n[68] X. Wang, K. Yu, C. Dong, X. Tang, and C. C. Loy. Deep network interpolation for continuous imagery effect transition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1692\u20131701, 2019.   \n[69] X. Wu, S. Huang, and F. Wei. Mixture of loRA experts. In International Conference on Learning Representations, 2024.   \n[70] H. Xia and Z. Ding. Structure preserving generative cross-domain learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4364\u20134373, 2020.   \n[71] H. Xia, T. Jing, and Z. Ding. Maximum structural generation discrepancy for unsupervised domain adaptation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(3):3434\u2013 3445, 2023.   \n[72] R. Xu, G. Li, J. Yang, and L. Lin. Larger norm more transferable: An adaptive feature norm approach for unsupervised domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1426\u20131435, 2019.   \n[73] G. Yang, H. Xia, M. Ding, and Z. Ding. Bi-directional generation for unsupervised domain adaptation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 6615\u20136622, 2020.   \n[74] Q. Yang, Y. Zhang, W. Dai, and S. J. Pan. Transfer learning. Cambridge, U.K.: Cambridge Univ. Press, 2020.   \n[75] F. Ye, X. Wang, Y. Zhang, and I. W. Tsang. Multi-task learning via time-aware neural ode. In International Joint Conference on Artificial Intelligence, pages 4495\u20134503, 2023.   \n[76] H. Ye, J. Zhang, S. Liu, X. Han, and W. Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023.   \n[77] T. Zadouri, A. \u00dcst\u00fcn, A. Ahmadian, B. Ermis, A. Locatelli, and S. Hooker. Pushing mixture of experts to the limit: Extremely parameter efficient moe for instruction tuning. In The Twelfth International Conference on Learning Representations, 2023.   \n[78] Y. Zeng and K. Lee. The expressive power of low-rank adaptation. In International Conference on Learning Representations, 2023.   \n[79] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz. mixup: Beyond empirical risk minimization. In International Conference on Learning Representations, 2018.   \n[80] K. Zhang, Y. Zhou, X. Xu, X. Pan, and B. Dai. Diffmorpher: Unleashing the capability of diffusion models for image morphing. arXiv preprint arXiv:2312.07409, 2023.   \n[81] L. Zhang, A. Rao, and M. Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836\u20133847, 2023.   \n[82] P. Zhang, H. Dou, Y. Yu, and X. Li. Adaptive cross-domain learning for generalizable person re-identification. In European Conference on Computer Vision, pages 215\u2013232. Springer, 2022.   \n[83] X. Zhang, X.-Y. Wei, W. Zhang, J. Wu, Z. Zhang, Z. Lei, and Q. Li. A survey on personalized content synthesis with diffusion models. arXiv preprint arXiv:2405.05538, 2024.   \n[84] Y. Zhang, S. Chen, W. Jiang, Y. Zhang, J. Lu, and J. T. Kwok. Domain-guided conditional diffusion model for unsupervised domain adaptation. arXiv preprint arXiv:2309.14360, 2023.   \n[85] Y. Zhang, J. Liang, Z. Zhang, L. Wang, R. Jin, T. Tan, et al. Free lunch for domain adversarial training: Environment label smoothing. In International Conference on Learning Representations, 2023.   \n[86] Y. Zhang, T. Liu, M. Long, and M. Jordan. Bridging theory and algorithm for domain adaptation. In International Conference on Machine Learning, pages 7404\u20137413. PMLR, 2019.   \n[87] Y. Zhang, Y. Yao, S. Chen, P. Jin, Y. Zhang, J. Jin, and J. Lu. Rethinking guidance information to utilize unlabeled samples: A label encoding perspective. In International Conference on Machine Learning, 2024.   \n[88] P. Zheng, Y. Zhang, Z. Fang, T. Liu, D. Lian, and B. Han. Noisediffusion: Correcting noise for image interpolation with diffusion models beyond spherical linear interpolation. In International Conference on Learning Representations, 2024.   \n[89] K. Zhou, Z. Liu, Y. Qiao, T. Xiang, and C. C. Loy. Domain generalization: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45:4396\u20134415, 2023.   \n[90] K. Zhou, Y. Yang, Y. Qiao, and T. Xiang. Domain generalization with mixstyle. In International Conference on Learning Representations, 2020.   \n[91] J.-Y. Zhu, P. Kr\u00e4henb\u00fchl, E. Shechtman, and A. A. Efros. Generative visual manipulation on the natural image manifold. In European Conference on Computer Vision, pages 597\u2013613. Springer, 2016.   \n[92] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE International Conference on Computer Vision, pages 2223\u20132232, 2017.   \n[93] Y. Zhu, F. Zhuang, J. Wang, G. Ke, J. Chen, J. Bian, H. Xiong, and Q. He. Deep subdomain adaptation network for image classification. IEEE Transactions on Neural Networks and Learning Systems, 32(4):1713\u20131722, 2020.   \n[94] F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong, and Q. He. A comprehensive survey on transfer learning. Proceedings of the IEEE, 109(1):43\u201376, 2020.   \n[95] Z. Zhuang, Y. Zhang, and Y. Wei. Gradual domain adaptation via gradient flow. In International Conference on Learning Representations, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "A Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma 1. Given two matrices $\\mathbf{A},\\mathbf{B}\\in\\mathbb{R}^{m\\times n}$ , and $\\mathrm{{rank}}(\\cdot)$ denotes the rank of a matrix. Then, $\\operatorname{rank}([\\mathbf{A}\\ \\mathbf{B}])\\leq\\operatorname{rank}(\\mathbf{A})+\\operatorname{rank}(\\mathbf{B})$ and $\\operatorname{rank}([\\mathbf{A}^{T}\\mathbf{\\deltaB}^{T}])\\leq\\operatorname{rank}(\\mathbf{A})+\\operatorname{rank}(\\mathbf{B})$ . ", "page_idx": 16}, {"type": "text", "text": "Lemma 2. Given two matrices $\\mathbf{A}\\in\\mathbb{R}^{m\\times n},\\mathbf{B}\\in\\mathbb{R}^{n\\times q},$ , and rank $(\\cdot)$ denotes the rank of a matrix.   \nThen, $\\operatorname{rank}([\\mathbf{AB}])\\leq\\operatorname*{min}\\left(\\operatorname{rank}(\\mathbf{A}),\\operatorname{rank}(\\mathbf{B})\\right)$ . ", "page_idx": 16}, {"type": "text", "text": "Theorem 3. (Generalized Singular Value Decomposition (GSVD) [39]) For given matrices $\\mathbf{A},\\mathbf{B}\\in$ $\\mathbb{R}^{m\\times n}$ , let $\\mathbf{C}^{\\vec{T}}=[\\mathbf{A}^{T}\\mathbf{\\nabla}\\mathbf{B}^{T}]$ and denote its rank by $r=r a n k(\\mathbf{C})$ , there exist orthogonal matrices $\\mathbf{U_{A}},\\mathbf{U_{B}}\\in\\mathbb{R}^{m\\times m}$ , $\\mathbf{Q}\\in\\mathbb{R}^{n\\times n}$ and $\\mathbf{W}\\in\\mathbb{R}^{k\\times k}$ so that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{U_{A}}^{T}\\mathbf{A}\\mathbf{Q}=\\boldsymbol{\\Sigma_{\\mathbf{A}}}\\left[\\mathbf{W}^{T}\\mathbf{R},\\ \\mathbf{0}\\right],\\quad\\mathbf{U_{B}}^{T}\\mathbf{B}\\mathbf{Q}=\\boldsymbol{\\Sigma_{\\mathbf{B}}}\\left[\\mathbf{W}^{T}\\mathbf{R},\\ \\mathbf{0}\\right],\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Sigma_{\\mathbf{A}}=\\left[\\begin{array}{l l l}{\\mathbf{I}_{\\mathbf{A}}}&&\\\\ &{\\mathbf{S}_{\\mathbf{A}}}&\\\\ &&{\\mathbf{O}_{\\mathbf{A}}}\\end{array}\\right],\\quad\\Sigma_{\\mathbf{B}}=\\left[\\begin{array}{l l l}{\\mathbf{O}_{\\mathbf{B}}}&&\\\\ &{\\mathbf{S}_{\\mathbf{B}}}&\\\\ &&{\\mathbf{I}_{\\mathbf{B}}}\\end{array}\\right],\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\mathbf{R}$ is real diagonal contains the nonzero singular values of $\\mathbf{C}$ in decreasing order, $\\mathbf{\\Sigma}\\Sigma_{\\mathbf{A}},\\Sigma_{\\mathbf{B}}\\in$ ${\\mathbb{R}}^{m\\times k}$ are real non-negative block-diagonal matrices, where $\\mathbf{I_{A}}\\in\\mathbb{R}^{r\\times r}$ and $\\mathbf{I_{B}}\\in\\mathbb{R}^{k-r-s\\times k-r-s}$ are identity matrices, $\\breve{\\mathbf{O}}_{\\mathbf{A}}\\in\\mathbb{R}^{m-r-s\\times\\breve{k}-r-s}$ and $\\mathbf{O_{B}}\\in\\mathbb{R}^{m-k-r\\times r}$ are zero matrices with possibly no rows or no columns, and $\\mathbf{S}_{\\mathbf{A}}=\\left[\\alpha_{r+1},\\ldots,\\alpha_{r+s}\\right]$ and $\\mathbf{S_{B}}=\\left[\\beta_{r+1},\\dots,\\beta_{r+s}\\right]$ . And we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n,\\,>\\alpha_{r+1}\\geq\\,\\cdot\\,\\cdot\\,\\geq\\alpha_{r+s}>0,\\quad0<\\beta_{r+1}\\leq\\,\\cdot\\,\\cdot\\,\\leq\\beta_{r+s}<1,\\quad\\alpha_{i}^{2}+\\beta_{i}^{2}=1,\\quad i\\in[r+1,r+s].\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Indeed, GSVD is a powerful tool in numerical linear algebra and data analysis. It can be seen as an extension of the singular value decomposition (SVD). Notably, when the matrix $\\mathbf{B}$ is the identity matrix, the GSVD of matrix A and $\\mathbf{B}$ simplifies to the SVD of matrix A. ", "page_idx": 16}, {"type": "text", "text": "Theorem 1. (The Equivariance between Terra and Multiple LoRAs) Assume there exist two LoRAs $\\Delta{W_{A}},\\Delta{W_{B}}\\in\\mathbb{R}^{m\\bar{\\times}n}$ with ranks of $p$ and $q$ , respectively, that effectively solve two specific downstream tasks. Let $k=\\mathrm{max}\\{\\mathrm{rank}([\\Delta W_{A}\\ \\Delta W_{B}])$ , $\\operatorname{rank}\\([\\Delta\\boldsymbol{W}_{A}^{T}\\mathrm{\\Delta}\\Delta\\boldsymbol{W}_{B}^{T}])\\}$ , where $\\mathrm{{rank}(\\cdot)}$ denotes the rank of a matrix. Then, there exists a Terra with $W_{u p}\\in\\mathbb{R}^{m\\times k}$ , $W_{d o w n}\\in\\mathbb{R}^{k\\times n}$ , $W_{m i d}\\in\\mathbb{R}^{k\\times k}$ , and ${\\boldsymbol{\\mathcal{K}}}(t)=t{\\boldsymbol{W}}_{m i d}+{\\boldsymbol{I}}_{r},$ , such that the updated matrix $\\mathbf{\\dot{\\Delta}}\\mathbf{W}(t)=\\mathbf{W}_{u p}\\boldsymbol{K}(t)\\mathbf{W}_{d o w n},$ , can simultaneously solve the two downstream task, that is, we have $\\Delta W(0)=\\Delta W_{A}$ and $\\Delta W(1)=\\Delta W_{B}$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. Our goal is to find matrices $W_{\\mathrm{up}}$ , $W_{\\mathrm{down}}$ , and $W_{\\mathrm{mid}}$ to satisfy $\\Delta W(0)=W_{\\mathrm{up}}W_{\\mathrm{down}}=$ $\\Delta W_{A}$ , and $\\Delta\\bar{\\pmb{W}}(1)=W_{\\mathrm{up}}(\\pmb{W}_{\\mathrm{mid}}+\\cal I)\\dot{\\pmb{W}}_{\\mathrm{down}}=\\Delta\\pmb{W}_{B}$ . ", "page_idx": 16}, {"type": "text", "text": "From Theorem 3, since $k\\geq\\mathrm{rank}([\\Delta W_{A}^{T}\\Delta W_{B}^{T}])$ , we know GSVD can decompose the two LoRA adapters with a common right generalized singular vectors $\\mathbf{X}\\in\\mathbb{R}^{k\\times n}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Delta W_{A}=\\mathbf{U_{A}}\\Sigma_{\\mathbf{A}}\\mathbf{X},\\quad\\Delta W_{B}=\\mathbf{U_{B}}\\Sigma_{\\mathbf{B}}\\mathbf{X}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Similarly, we can transpose the matrices of the two LoRA adapters, since $k\\geq\\mathrm{rank}([\\Delta W_{A}\\ \\Delta W_{B}])$ , and use GSVD again, then we have a common left generalized singular vectors $\\mathbf{Y}\\in\\mathbb{R}^{m\\times k}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Delta W_{A}=\\mathbf{Y}\\mathbf{Z_{A}}\\mathbf{V_{A}},\\quad\\Delta W_{B}=\\mathbf{Y}\\mathbf{Z_{B}}\\mathbf{V_{B}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For each matrix W, there exists a pseudo-inverse (a.k.a. the Moore-Penrose inverse [44]) ${{\\bf W}}^{+}$ such that $\\mathbf{W}\\mathbf{W}^{+}\\mathbf{W}=\\mathbf{W}$ . Then we have a special decomposition of the LoRA adapters: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta W_{A}=\\mathbf{U}_{\\mathbf{A}}\\mathbf{\\Sigma}_{\\mathbf{A}}\\mathbf{X}}\\\\ &{\\qquad\\quad=\\mathbf{U}_{\\mathbf{A}}\\mathbf{\\Sigma}_{\\mathbf{A}}(\\mathbf{X}\\mathbf{X}^{+}\\mathbf{X})}\\\\ &{\\qquad\\quad=\\Delta W_{A}\\mathbf{X}^{+}\\mathbf{X}}\\\\ &{\\qquad\\quad=\\mathbf{Y}\\mathbf{Y}^{+}\\Delta W_{A}\\mathbf{X}^{+}\\mathbf{X}\\triangleq\\mathbf{Y}\\mathbf{K}_{\\mathbf{A}}\\mathbf{X},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\mathbf{K_{A}}\\in\\mathbb{R}^{k\\times k}$ . Similarly, we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\Delta W_{B}=\\mathbf{Y}(\\mathbf{Y}^{+}\\Delta W_{B}\\mathbf{X}^{+})\\mathbf{X}\\triangleq\\mathbf{Y}\\mathbf{K}_{\\mathbf{B}}\\mathbf{X}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Assume the SVD of $\\mathbf{K_{A}}$ is of the following form: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{K}_{\\mathbf{A}}=\\mathbf{U}_{\\mathbf{K}}\\mathbf{A}\\mathbf{V}_{\\mathbf{K}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\mathbf{U_{K}},\\mathbf{V_{K}}\\in\\mathbb{R}^{k\\times k}$ are orthogonal matrices. ", "page_idx": 17}, {"type": "text", "text": "We can represent the diagonal matrix $\\pmb{\\Lambda}$ as $\\mathbf{A}=\\mathrm{diag}(\\sigma_{1},\\sigma_{2},\\ldots,\\sigma_{r},0,\\ldots,0)$ . Define $\\Lambda^{+}$ whose first $r$ rows have $1/\\sigma_{1},1/\\sigma_{2},\\dots,1/\\sigma_{r}$ on the diagonal, and the product of $\\Lambda$ and $\\pmb{\\Lambda}^{+}$ is a square matrix whose first $r$ diagonal entries are 1 and whose others are 0, i.e. ${\\mathbf{I}}_{r}$ . Then, we can get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta W(t)=\\mathbf{Y}\\left(t(\\mathbf{K}_{\\mathbf{B}}-\\mathbf{K}_{\\mathbf{A}})+\\mathbf{K}_{\\mathbf{A}}\\right)\\mathbf{X}}\\\\ &{\\quad\\quad\\quad=\\mathbf{Y}\\left(t(\\mathbf{K}_{\\mathbf{B}}-\\mathbf{K}_{\\mathbf{A}})+\\mathbf{U}_{\\mathbf{K}}\\mathbf{A}\\mathbf{V}_{\\mathbf{K}}\\right)\\mathbf{X}}\\\\ &{\\quad\\quad\\quad=\\mathbf{Y}\\mathbf{U}_{\\mathbf{K}}\\mathbf{U}_{\\mathbf{K}}^{T}\\left(t(\\mathbf{K}_{\\mathbf{B}}-\\mathbf{K}_{\\mathbf{A}})+\\mathbf{U}_{\\mathbf{K}}\\mathbf{A}\\mathbf{V}_{\\mathbf{K}}\\right)\\mathbf{V}_{\\mathbf{K}}^{T}\\mathbf{V}_{\\mathbf{K}}\\mathbf{X}}\\\\ &{\\quad\\quad\\quad=\\mathbf{Y}\\mathbf{U}_{\\mathbf{K}}\\left(t\\mathbf{U}_{\\mathbf{K}}^{T}(\\mathbf{K}_{\\mathbf{B}}-\\mathbf{K}_{\\mathbf{A}})\\mathbf{V}_{\\mathbf{K}}^{T}+\\mathbf{A}\\right)\\mathbf{V}_{\\mathbf{K}}\\mathbf{X}}\\\\ &{\\quad\\quad\\quad=\\mathbf{Y}\\mathbf{U}_{\\mathbf{K}}\\left(t\\mathbf{U}_{\\mathbf{K}}^{T}(\\mathbf{K}_{\\mathbf{B}}-\\mathbf{K}_{\\mathbf{A}})\\mathbf{V}_{\\mathbf{K}}^{T}\\mathbf{A}^{+}+\\mathbf{I}_{r}\\right)\\mathbf{A}\\mathbf{V}_{\\mathbf{K}}\\mathbf{X}}\\\\ &{\\quad\\quad\\quad=W_{\\mathbf{up}}\\left(t W_{\\mathbf{mid}}+\\mathbf{I}_{r}\\right)W_{\\mathbf{down}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Finally, we can construct the following matrices to prove the theorem: ", "page_idx": 17}, {"type": "equation", "text": "$$\nW_{\\mathrm{up}}=\\mathbf{Y}\\mathbf{U}_{\\mathbf{K}},\\quad\\mathbf{W}_{\\mathrm{mid}}=\\mathbf{U}_{\\mathbf{K}}^{\\mathbf{\\alpha}}T(\\mathbf{K}_{\\mathbf{B}}-\\mathbf{K}_{\\mathbf{A}})\\mathbf{V}_{\\mathbf{K}}^{\\mathbf{\\alpha}}T\\mathbf{\\Lambda}^{\\mathbf{\\alpha}},\\quad W_{\\mathrm{down}}=\\mathbf{\\Lambda}\\mathbf{V}_{\\mathbf{K}}\\mathbf{X}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Theorem 2. (The Expressive Power of Terra) For each layer $l,$ , the rank- $k$ Terra has updated matrix $\\Delta W(t)_{l},$ and the function of time-varying matrix is ${\\cal K}(t)_{l}=t{\\cal W}_{m i d,l}+\\tilde{{\\cal I}}.$ . Assume that all weight matrices of the frozen model $(W_{l})_{l=1}^{L}$ , $\\begin{array}{r}{\\prod_{l=1}^{L}W_{l}+\\mathrm{LR}_{r}(E_{A});}\\end{array}$ , and $\\begin{array}{r}{\\prod_{l=1}^{L}W_{l}+\\mathrm{LR}_{r}(E_{B})}\\end{array}$ are non-singular for all $r\\leq k(L-1)$ . Then the approximation error satisfies ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\Delta W(t)}\\left(\\left\\|\\prod_{l=1}^{L}(W_{l}+\\Delta W(0)_{l})-\\overline{{W}}_{A}\\right\\|_{2}+\\left\\|\\prod_{l=1}^{L}(W_{l}+\\Delta W(1)_{l})-\\overline{{W}}_{B}\\right\\|_{2}\\right)\\leq2\\sigma_{k L+1}^{*},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the $\\sigma_{k L+1}^{*}$ as the $(k L+1)$ -th largest singular values obtained by merging the singular values of $E_{A}$ and $E_{B}$ . Moreover, when $\\begin{array}{r}{k\\ge\\left\\lceil\\frac{R_{E_{A}}+R_{E_{B}}}{L}\\right\\rceil}\\end{array}$ , the approximation error is zero. ", "page_idx": 17}, {"type": "text", "text": "Proof. We first adopt a similar construction consistently with the prior work [78]: ", "page_idx": 17}, {"type": "equation", "text": "$$\nS_{A}:=\\prod_{l=1}^{L}\\left(\\pmb{W}_{l}+\\Delta\\pmb{W}(0)_{l}\\right)-\\prod_{l=1}^{L}\\pmb{W}_{l}\\quad\\pmb{S}_{B}:=\\prod_{l=1}^{L}\\left(\\pmb{W}_{l}+\\Delta\\pmb{W}(1)_{l}\\right)-\\prod_{l=1}^{L}\\pmb{W}_{l}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, the approximate error can be represented as: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\Delta W(t)}{\\operatorname*{min}}\\left(\\left\\|\\prod_{l=1}^{L}\\left(W_{l}+\\Delta W(0)_{l}\\right)-\\overline{{W}}_{A}\\right\\|_{2}+\\left\\|\\prod_{l=1}^{L}\\left(W_{l}+\\Delta W(1)_{l}\\right)-\\overline{{W}}_{B}\\right\\|_{2}\\right)}\\\\ &{=\\underset{\\Delta W(t)}{\\operatorname*{min}}\\left(\\|S_{A}-E_{A}\\|_{2}+\\|S_{B}-E_{B}\\|_{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Following the prior work, we can also decompose $S_{A}$ into an accumulation of $S_{A_{l}}$ as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{A}=\\Delta W(0)_{L}\\displaystyle\\prod_{l=1}^{L-1}(\\Delta W(0)_{l}+W_{l})+W_{L}\\Delta W(0)_{L-1}\\displaystyle\\prod_{l=1}^{L-2}(\\Delta W(0)_{l}+W_{l})}\\\\ &{\\phantom{\\Delta_{A}=\\Delta W(0)_{L}\\displaystyle\\prod_{l=2}^{L}W_{l}}+\\displaystyle\\prod_{l=2}^{L}W_{l})\\left(\\Delta W(0)_{1}+W_{1}\\right)-\\displaystyle\\prod_{l=1}^{L}W_{l}}\\\\ &{\\displaystyle=\\displaystyle\\sum_{l=1}^{L}\\left[\\left(\\displaystyle\\prod_{i=l+1}^{L}W_{i}\\right)\\Delta W(0)_{l}\\left(\\displaystyle\\prod_{i=1}^{l-1}(W_{i}+\\Delta W(0)_{i})\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Similarly, we have $\\begin{array}{r}{{\\pmb S}_{B}=\\sum_{l=1}^{L}{\\pmb S}_{B_{l}},{\\pmb S}_{B_{l}}=\\left(\\prod_{i=l+1}^{L}{\\pmb W}_{i}\\right)\\Delta{\\pmb W}(1)_{l}\\left(\\prod_{i=1}^{l-1}{\\pmb W}_{i}+\\Delta{\\pmb W}(1)_{i}\\right)\\!\\right)\\!.}\\end{array}$ We select the largest $k L$ largest terms of the singular values of $E_{A}$ and $E_{B}$ , and we denote there are $p$ values from $E_{A}$ and $q$ values from $E_{B}$ . To prove the theorem, we need to show the following: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\Delta W(t)}\\left(\\left\\|S_{\\cal A}-E_{\\cal A}\\right\\|_{2}+\\left\\|S_{\\cal B}-E_{\\cal B}\\right\\|_{2}\\right)\\le2\\sigma_{k L+1}^{*}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Define $E_{A}^{\\prime}=\\mathrm{LR}_{p}(E_{A}),E_{B}^{\\prime}=\\mathrm{LR}_{q}(E_{B})$ , based on the Eckart-Young Theorem [8], then we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|{\\boldsymbol E}_{A}^{\\prime}-{\\boldsymbol E}_{A}\\right\\|_{2}+\\left\\|{\\boldsymbol E}_{B}^{\\prime}-{\\boldsymbol E}_{B}\\right\\|_{2}\\leq\\sigma_{p+1}({\\boldsymbol E}_{A})+\\sigma_{q+1}({\\boldsymbol E}_{B})\\leq2\\sigma_{k L+1}^{*}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Based on (22) and (23), if we can construct Terra parameter $\\Delta W(t)$ to make $S_{A}\\,=\\,E_{A}^{\\prime}$ and $S_{B}=E_{B}^{\\prime}$ , then we will finish the proof. We refer to the SVD of $E_{A}^{\\prime}$ and $E_{B}^{\\prime}$ as: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{E_{A}^{\\prime}=\\mathbf{U}_{\\mathbf{A}}\\mathbf{A}_{\\mathbf{A}}\\mathbf{V}_{\\mathbf{A}},\\quad E_{B}^{\\prime}=\\mathbf{U}_{\\mathbf{B}}\\mathbf{A}_{\\mathbf{B}}\\mathbf{V}_{\\mathbf{B}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We introduce $Q_{A,l}$ and $Q_{B,l}$ to divide $E_{A}^{\\prime}$ and $E_{B}^{\\prime}$ into $L$ parts: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{l=1}^{L}\\pmb{E}_{A}^{\\prime}\\pmb{Q}_{A,l}=\\pmb{E}_{A}^{\\prime}\\quad\\sum_{l=1}^{L}\\pmb{E}_{B}^{\\prime}\\pmb{Q}_{B,l}=\\pmb{E}_{B}^{\\prime},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We define $\\textstyle I_{a:b}$ as a diagonal matrix whose diagonal entries from the $a$ -th to $b$ -th position are 1 and others are 0. Here we define the matrices $(Q_{A,l})_{l=1}^{L}$ and $(Q_{B,l})_{l=1}^{L}$ by: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{Q_{A,l}=\\mathbf{V_{A}}\\mathbf{I}_{R(l-1)+1:R l}\\mathbf{V_{A}}^{T},Q_{B,l}=\\mathbf{0},}&{\\mathrm{~for~}R l<p,}\\\\ {Q_{A,l}=\\mathbf{V_{A}}\\mathbf{I}_{R(l-1)+1:p}\\mathbf{V_{A}}^{T},Q_{B,l}=\\mathbf{V_{B}}\\mathbf{I}_{1:R l-p}\\mathbf{V_{B}}^{T},}&{\\mathrm{~for~}p\\leq R l<p+l,}\\\\ {Q_{A,l}=\\mathbf{0},Q_{B,l}=\\mathbf{V_{B}}\\mathbf{I}_{R(l-1)-p+1:R l-p}\\mathbf{V_{B}}^{T},}&{\\mathrm{~for~}p+l\\leq R l.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "It easy to find that rank $(\\mathbf{Q}_{A,l})+\\operatorname{rank}(\\mathbf{Q}_{B,l})\\leq R$ . Based on Lemma 1, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname{rank}\\left(\\left[E_{A}^{\\prime}Q_{A,l}~E_{B}^{\\prime}Q_{B,l}\\right]\\right)\\leq k,~~~\\operatorname{rank}\\left(\\left[\\left(E_{A}^{\\prime}Q_{A,l}\\right)^{T}~\\left(E_{B}^{\\prime}Q_{B,l}\\right)^{T}\\right]\\right)\\leq k.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now, we show a feasible solution to make $S_{A}=E_{A}^{\\prime}$ and $S_{B}=E_{B}^{\\prime}$ follows these conditions: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\widehat{\\Delta W(0)}_{l}=(\\underset{i=l+1}{\\overset{L}{\\prod}}W_{i})^{-1}E_{A}^{\\prime}Q_{A,l}(\\underset{i=1}{\\overset{l-1}{\\prod}}(W_{i}+\\widehat{\\Delta W(0)}_{i}))^{-1},\\quad\\mathrm{~for~all}\\ l\\in[L],}\\\\ &{\\widehat{\\Delta W(1)}_{l}=(\\underset{i=l+1}{\\overset{L}{\\prod}}W_{i})^{-1}E_{B}^{\\prime}Q_{B,l}(\\underset{i=1}{\\overset{l-1}{\\prod}}(W_{i}+\\widehat{\\Delta W(1)}_{i}))^{-1},\\quad\\mathrm{~for~all}\\ l\\in[L],}\\\\ &{\\operatorname{rank}\\left(W_{l}+\\widehat{\\Delta W(0)}_{l}\\right)=\\operatorname{rank}\\left(W_{l}+\\widehat{\\Delta W(1)}_{l}\\right)=D,\\quad\\mathrm{~for~all}\\ l\\in[L-1].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Based on the assumptions of $(\\boldsymbol{W_{l}})_{l=1}^{L}$ , $\\begin{array}{r}{\\prod_{l=1}^{L}W_{l}+\\mathrm{LR}_{r}(E_{A})}\\end{array}$ , and $\\begin{array}{r}{\\prod_{l=1}^{L}W_{l}+\\mathrm{LR}_{r}(E_{B})}\\end{array}$ are non-singular for all $r\\leq k(L-1)$ and the Eq. (28) and Eq. (29), it\u2019s easy to prove that Eq. (30) is satisfied [78]. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname*{sing}\\mathrm{~the~Lemma~2~and~Eq.~}(27),\\mathrm{~we~can~show~rank}\\left(\\left[\\widehat{\\Delta W(0)}_{l}~\\widehat{\\Delta W(1)}_{l}\\right]\\right)\\leq k\\mathrm{~by~}}\\\\ &{\\mathrm{~rank}\\left(\\left[\\widehat{\\Delta W(0)}_{l}~\\widehat{\\Delta W(1)}_{l}\\right]\\right)}\\\\ &{\\ =\\operatorname{rank}\\left(\\left[E_{A}^{\\prime}Q_{A,l}~E_{B}^{\\prime}Q_{B,l}\\right]\\left[\\begin{array}{l}{(\\prod_{i=1}^{l-1}(\\widehat{W(0)}_{i}))^{-1}}\\\\ {\\overset{(1-1)}{\\longrightarrow}}&{\\overset{(1-1)}{\\longrightarrow}}\\end{array}\\widehat{\\Delta W(1)}_{i}\\right]\\right)}\\\\ &{\\ \\ \\overset{l-1}{\\leq}\\operatorname{rank}\\left(\\left[E_{A}^{\\prime}Q_{A,l}~E_{B}^{\\prime}Q_{B,l}\\right]\\right)\\leq k}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Similarly, we can also get rank $\\langle[\\widehat{\\Delta W(0)}_{l}^{T}\\ \\widehat{\\Delta W(1)}_{l}^{T}])\\leq k$ . Then, based on Theorem 1, for each layer $l$ , we can prove that there exists a Terra can satisfies $\\Delta W(0)_{l}=\\widehat{\\Delta W(0)}_{l}$ and $\\Delta W(1)_{l}=$ $\\widehat{\\Delta W(1)}_{l}$ , thereby completing the proof. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "B Baselines and Implementation Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Baselines. For the generative interpolation tasks, we use DGP [41], DDIM [55], DiffMorpher [80], and LoRA Interpolation for comparison. DGP leverages large-scale pre-trained GAN [3] for image morphing. DDIM means the DDIM inversion and latent interpolation as discussed in [55]. DiffMorpher performs image morphing between two images by interpolating corresponding two LoRAs and latent noises. LoRA Interpolation represents directly training two LoRAs and performing interpolation. For the UDA tasks, we compare with ERM [58] and various UDA methods, including AFN [72], MDD [86], MCC [28], DANN [13], CDAN [35], SDAT [46], ELS [85], and MSGD [71]. We integrate the proposed Terra with the state-of-the-art UDA methods, i.e., MCC, and ELS. For the DG tasks, we compare with ERM [58], MIRO [6], CDGA [22], SWAD [5], SAGM [65], and DomainDiff [36]. Note that previous works [18, 5] have found that ERM is effective in DG and outperforms previous DG methods. Thus, we integrate Terra with ERM and the state-of-the-art DG methods, i.e., SWAD, and SAGM. ", "page_idx": 19}, {"type": "text", "text": "Implementation Details. The text-to-image diffusion model used in this paper is the Stable Diffusion XL (SDXL) model [45]. The default resolutions of the generated images are $1024\\!\\times\\!1024$ The rank of LoRA is set as 16 for generative interpolation tasks and 32 for generation-based UDA and DG tasks. All experiments are conducted on an NVIDIA A100 GPU with three random trials. ", "page_idx": 19}, {"type": "text", "text": "For generative interpolation tasks, the training data utilized is sourced from the repository of Diffmorpher3 and the LoRAs from Hugging Face space \u201cLoRA the Explorer\u201d4. The training images can be found in the supplementary materials provided. For morphing in styles, given images in crayon and watercolor styles for training, we set $t=0$ for training on the crayon images and $t=1$ for training on the watercolor images, with the prompt being \u201cAn image\u201d. During the inference phase, by uniformly transitioning $t$ from 0 to 1 and using the text prompt \u201cA high-speed train\u201d, the generated results are shown in the second row of Fig. 4. For morphing in subject, given five images of cats and eight images of dogs, we set $t=0$ for training on the cat images and $t=1$ for training on the dog images, with the prompt being \u201cA pet\u201d. During the inference phase, by uniformly transitioning $t$ from 0 to 1 and using the text prompt $^{\\bullet\\bullet}\\mathrm{A}$ pet on the lawn\u201d, the generated results are shown in the last row of Fig. 4. ", "page_idx": 19}, {"type": "text", "text": "For UDA tasks, We generate 50 images per category for the Office31 and Office-Home, and 1000 images per category for the VisDA datasets. For images translated from the source domain to the target domain, we scale the long side of each source image to 1024 pixels, adjusting the short side proportionally. Following [46], the ResNet-50 is used as the backbone on the Office31 and Office-Home datasets, and the ResNet-101 [21] is used as the backbone on the VisDA-2017 dataset. The learning rate scheduler follows [13]. For MCC+Terra and ELS $+$ Terra, we follow the settings as the original papers [28, 85]. ", "page_idx": 19}, {"type": "text", "text": "For DG tasks, we generate 400, 160, and 400 images per category for the PACS, Office-Home, and VLCS datasets, respectively. The dimension of parameter $\\pmb{t}$ is set as two, with each dimension sampled from -2 to 2 at intervals of 0.1 to generate diverse samples. We employ ResNet-50 as the backbone and adopt the same training, evaluation protocols, and hyperparameter search results as outlined in [5, 65, 6]. ResNet-50 is also used as the backbone for $\\pmb{t}$ prediction network $g(\\cdot)$ . ", "page_idx": 19}, {"type": "table", "img_path": "SgODU2mx9T/tmp/1bd466ffcc78e0d562196f4eb25551fd44f619096301c0ef0824c7e14e16dbe1.jpg", "table_caption": ["Table 5: Possible forms of Terra and corresponding differentiable functions. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "We provide three possible forms of Terra listed in Table 5, i.e., Linear, Exponential, and Cosine. We apply the Linear form of Terra for generative interpolation and UDA tasks and the \u201cCosine-Sine\u201d form, a variant of \u201cCosine\u201d, for DG tasks. Specifically, the form of \u201cCosine-Sine\u201d is $\\cos(t W)$ on the diagonal of $K(t)$ , and $\\sin(t W)$ at other positions. To provide more insights, we elaborate on the guiding principles behind the choice of these forms: ", "page_idx": 20}, {"type": "text", "text": "\u25e6 Linear: The $t W+I$ is the simplest form, related to a straight and steady flow, which is sufficient for two domains according to Theorem 1 and 2. Its constant velocity of weight changes ensures smooth morphing and is suitable for simple interpolating between two domains under the UDA setting. ", "page_idx": 20}, {"type": "text", "text": "\u25e6 Cosine-Sine: This form is adopted because of the bounded range and non-linearity of trigonometric functions, preventing image collapse during generation and enabling a complex parameter manifold to capture relationships between multiple domains. We recommend using this form in complex scenarios, such as interpolating multiple domains in DG. ", "page_idx": 20}, {"type": "text", "text": "\u25e6 sEmxopootnhe nctuiravle: $\\begin{array}{r}{e^{t W}=\\bar{I}+\\sum_{k=1}^{\\bar{\\infty}}\\frac{t^{k}}{k!}\\bar{W}^{k}}\\end{array}$ ,a niimfopllde.m eTnhtiesd f oursimn gi s\u201c tmorocrhe. emxaptrriexs_sievxep \u201da,n adl ssou idtaebflinee fs oar handling multiple domains. Notably, it is related to three types of transformations: scalings, rotations, and shears [14]. ", "page_idx": 20}, {"type": "table", "img_path": "SgODU2mx9T/tmp/52a5cafa92bb8d0a9968fd81e58a7a72d927daab67e2e63605f41b066a675628.jpg", "table_caption": ["Table 6: Evaluation on the dimension of time variable $\\pmb{t}$ and Linear form (dim2) of Terra on the PACS dataset under the DG setting. The best is in bold. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Empirically, the \u201cCosine-Sine\u201d form of Terra brings better performance for DG compared with the Linear form according to the results shown in Table 6. As can be seen, ERM+Terra with dimension 2 achieves the best average performance, thus we use 2 as the default dimension for DG tasks. ", "page_idx": 20}, {"type": "text", "text": "C More Experimental Results ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "C.1 Results on Generative Interpolation Tasks ", "text_level": 1, "page_idx": 20}, {"type": "image", "img_path": "SgODU2mx9T/tmp/fdacde8a7ee40b73d58527925cf7f2f3be705af708412b612c1b8dbb503c54ae.jpg", "img_caption": ["Figure 8: Qualitative results of image morphing using various methods. \u201cTerra $^+$ DiffM.\u201d integrates Terra with DiffMorpher. As shown, our method generates smooth and natural intermediate images. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "The qualitative comparisons of image morphing using various methods are shown in Fig. 8. We perform more qualitative samples of our Terra in Fig. 9. These samples further demonstrate Terra\u2019s ability to handle morphing under various scenarios. ", "page_idx": 20}, {"type": "image", "img_path": "SgODU2mx9T/tmp/d2b07d2e4c18cad0dd4e808eedd59d9894e667aa76b87bb3e6ad8c59a01c87d7.jpg", "img_caption": ["Figure 9: Supplementary samples of qualitative evaluation. The four rows display examples of morphing in image pairs, styles, and objects (purple-to-dog bags, colorful-to-shiny sneakers). "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "SgODU2mx9T/tmp/2a95c5f3b2583bcf7d03d2cab856fde3ddd43d181547540182f2209d9173fdbc.jpg", "img_caption": ["Figure 10: Example images of the expanded domains on the PACS dataset under the DG setting. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "C.2 Results on More Datasets ", "text_level": 1, "page_idx": 21}, {"type": "table", "img_path": "SgODU2mx9T/tmp/f12bae407ba83bee08e244469ac211e40d570a1d44d977899e8f8719845eeba4.jpg", "table_caption": ["Table 7: Transfer accuracies $(\\%)$ on the VisDA dataset under UDA setting. The best is in bold. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "SgODU2mx9T/tmp/66dbd0f296492b1c5bb494f3510297cc1895e099e5beac2492f77f7f60262db5.jpg", "table_caption": ["Table 8: Transfer accuracies $(\\%)$ on the Office31 dataset under the UDA setting. The best is in bold. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "SgODU2mx9T/tmp/f81fa11ff4e24eba0559cb63ba381017ea865ffedd800dba6c9a7ae47a655b06.jpg", "table_caption": ["Table 9: Testing accuracies $(\\%)$ on the VLCS dataset under the DG setting. The best is in bold. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "The complete results on the $V i s D A$ dataset under the UDA setting are shown in Table 7. The results on the Office31 dataset under the UDA setting are shown in Table 8, and the results on the VLCS dataset under the DG setting are shown in Table 9. As can be seen, Terra is still effective in the two datasets. ", "page_idx": 22}, {"type": "text", "text": "C.3 Results with More Baselines ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Moreover, the results with CoVi and PMTrans are shown in Table 10. Notably, Terra consistently improves performance in all tasks with those UDA methods, further verifying the effectiveness of our method. ", "page_idx": 22}, {"type": "table", "img_path": "SgODU2mx9T/tmp/dfdae5cc4f32f235ad79f4048094b414d985c6e1f635629420be5041ab5c5101.jpg", "table_caption": ["Table 10: Comparative analysis with two baseline methods on Office-Home under UDA setting. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "C.4 Comparison of Morphing Works ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In addition, for a fair comparison of Terra\u2019s effectiveness in expanding source domains that generalize better, we include the comparison against off-the-shelf DG $^+$ morphing works on Office-Home. That is, we train a LoRA for each domain and adopt LoRA Interp./DiffMorpher to interpolate. The results shown in Table $11$ verify the effectiveness of Terra, since Terra interpolates between domains instead of images and thus better models the distributions in two domains. ", "page_idx": 22}, {"type": "text", "text": "Table 11: Comparison of morphing works on Office-Home using the off-the-shelf method (SWAD) under DG setting. Note that DiffMorpher exhibits lower performance due to the large gap between image pairs, even within the same class. ", "page_idx": 23}, {"type": "table", "img_path": "SgODU2mx9T/tmp/51a7c5ab64dda4f1b2e0dded22cfa79ad5846f049b79ca46fae9251e070df10c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "C.5 SDXL Prior ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "To further highlight the design advantages of Terra, we conduct a comparison with data augmentation with SDXL\u2019s prior knowledge. Specifically, we design several methods to synthesize data based on the SDXL model and evaluate their effectiveness on UDA tasks: ", "page_idx": 23}, {"type": "text", "text": "(i) SDXL (random): We use the prompt \u201cA [CLASS]\u201d to generate samples for each class, where [CLASS] denotes the placeholder for the label.   \n(ii) SDXL (styles): We first use the prompt \u201cGenerate 50 prompts describing diverse styles for image generation\u201d to ask GPT-4, and then use the prompt \u201cA [CLASS], an everyday object in office and home, in the style of [STYLE]\u201d to generate samples, where [STYLE] denotes the placeholder for style prompts generated by GPT-4 (e.g. \u201cClassic\u201d, \u201cModern\u201d).   \n(iii) SDXL (target): Based on (ii), we use the name of the target domain (e.g. \u201cClipart\u201d) to replace the [STYLE] as the new placeholder for exploring the SDXL prior on the target domain.   \n(iv) SDXL (target styles): We use the prompt \u201cGenerate 50 prompts describing [TARGET] style for image generation\u201d to ask GPT-4 and obtain more detailed style prompts for synthesis.   \n(v) SDXL (selected): Inspired by [26], we use a confidence-based activate learning method to filter out poor-quality and misclassified samples generated in (iv) and select valid samples. ", "page_idx": 23}, {"type": "text", "text": "The comparison results on Office-Home for UDA are shown in Table 12. Terra outperforms the comparison methods, indicating that despite the boost in accuracy from target style design and active learning, the prior knowledge is insufficient to align with the downstream tasks. This issue can be further mitigated through finetuning with Terra, which demonstrates the design advantages of Terra. ", "page_idx": 23}, {"type": "table", "img_path": "SgODU2mx9T/tmp/119643e7642f28620521a4c9c9f743d8e28068064d6765dd0ef93b1ff467aed1.jpg", "table_caption": ["Table 12: Comparison of target-like samples generation by SDXL prior on Office-Home based on ELS under UDA setting. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "D Standard Deviations of Experiments ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The standard deviations of three random experiments on the Office-Home, VisDA, and ablation studies under UDA setting are shown in Tables 13, 14, and 15, respectively. Table 16 presents the standard deviations on the $P A C S$ and OfficeHome datasets under DG setting. ", "page_idx": 23}, {"type": "text", "text": "E Comparison with Other LoRA Variants ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In cross-domain learning based on MoLE [69], the process can be viewed as first training LoRAs on different domains separately, followed by training a gating function to integrate the trained LoRAs. Although both MoLE and Terra are designed for diffusion model customization, they differ in several key aspects: ", "page_idx": 23}, {"type": "table", "img_path": "SgODU2mx9T/tmp/7f1b47dcdc7176a370dd1bb6760bfe0d56d98ecd63d45e1abfa22dc162dc6e4b.jpg", "table_caption": ["Table 13: The standard deviation of three random experiments on Office-Home under UDA setting. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "SgODU2mx9T/tmp/e24906e877572d1a4bc3608f004c59e98580862fc18622a77f6473200c23fd66.jpg", "table_caption": ["Table 14: The standard deviation of three random experiments on VisDA under UDA setting. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "SgODU2mx9T/tmp/7e50141a9d1abf77523af2964d1773fbb2bfe8a2f73f19fb62bfcc6f96b33a71.jpg", "table_caption": ["Table 15: The standard deviation of three random experiments of ablation studies of $\\mathrm{ELS+}$ Terra on Office-Home under UDA setting. "], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "SgODU2mx9T/tmp/be6656a491b5e6a65756708773062e57335aada6862975c3f559f4bc1b0b6223.jpg", "table_caption": ["Table 16: The standard deviation on the PACS and OfficeHome datasets under DG setting. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "Objective: MoLE focuses on combining multiple pre-trained LoRAs to achieve multi-concept customization, whereas Terra aims to learn a single adapter structure that can capture multiple domains and construct a domain flow for generation. ", "page_idx": 24}, {"type": "text", "text": "Training: MoLE only optimizes the gating function to preserve the characteristics of trained LoRAs on different domains, whereas Terra participates in the diffusion fine-tuning stage and aims to learn domain-general knowledge and domain-specific knowledge, allowing for control over different domains through a time variable. ", "page_idx": 24}, {"type": "text", "text": "Expressiveness: MoLE uses a separate gating function for each LoRA layer, which requires entropybased balancing to resolve conflicts when combining multiple LoRAs. In contrast, Terra achieves domain adaptation through a single time variable $t$ , making it more stable. For two-domain interpolation, Terra and MoLE have similar expressiveness. Considering two domains with time variables $t_{1}$ and $t_{2}$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta W(\\alpha t_{1}+(1-\\alpha)t_{2})=B K(\\alpha t_{1}+(1-\\alpha)t_{2})A}\\\\ &{\\qquad\\qquad\\qquad\\qquad=(\\alpha t_{1}+(1-\\alpha)t_{2})B W A+B A}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\alpha\\Delta W(t_{1})+(1-\\alpha)\\Delta W(t_{2}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This is equivalent to the linear arithmetic composition in MoLE. ", "page_idx": 25}, {"type": "text", "text": "Finally, the relation between MoLE and Terra is similar to that between Gaussian Mixture Model (GMM) and Gaussian Process (GP). GMM composes a complex distribution by multiple Gaussian distributions, and GP is a distribution over functions within a continuous domain (such as time). Analogously, MoLE excels at composition capabilities, while Terra excels at constructing a manifold. ", "page_idx": 25}, {"type": "text", "text": "F Broader Impact and Ethics Statements ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The ability to generate realistic images can be misused to create deepfakes or other deceptive content, potentially leading to misinformation and privacy violations. While our work has the potential to advance the field of PEFT and generation-based cross-domain learning, it is crucial to address the associated risks, particularly in terms of ethical considerations. ", "page_idx": 25}, {"type": "text", "text": "G Limitation and Failure Cases ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Despite showing promising results in data-augmentation-based UDA and DG, Terra has some limitations. Generating images via Terra for data augmentation requires additional storage space. For UDA tasks, we generate target domain samples and transform source domain samples into the target domain, without utilizing Terra\u2019s ability to generate intermediate domains. Note that the intermediate domain can be leveraged by using methods in gradual domain adaptation [31], but we have not explored this due to different settings. We leave it for future studies. Additionally, while we have adapted to downstream domains through fine-tuning, our model may still be influenced by the prior of the foundation model to some extent. ", "page_idx": 25}, {"type": "text", "text": "We acknowledge that a small number of generated images may exhibit poor quality due to the confilct between SD prior knowledge and the knowledge required for downstream tasks. We showcase some failure cases in Fig. 11. However, the number of those poor-quality images is small, and it does not affect the overall performance of the model. ", "page_idx": 25}, {"type": "image", "img_path": "SgODU2mx9T/tmp/cb682979cabfd930e7eca5fff289a1cecb49587b61353111d9a70d7c049033b5.jpg", "img_caption": ["Figure 11: Illustration of failure cases in generated samples. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: See Abstract and last two paragraphs in Introduction. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: See Appendix G. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: See Section 3.2 and Appendix A. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: See supplemental material for the code and Appendix B for the experimental details. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The code and data can be found in the supplemental material. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The implementation details can be found in Appendix B. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The standard deviation of three random experiments with different seeds are shown in Table 8, 9 and 13-16. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: See Appendix B. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We have read and complied with the Code of Ethics. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: See Appendix F. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: There are no such risks in this paper. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: See the cited reference and supplemental material. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 30}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This paper dos not release new assets. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: Not human subjects research. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: Not human subjects research. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]