[{"figure_path": "HkC4OYee3Q/tables/tables_3_1.jpg", "caption": "Table 1: Tabular comparison of our proposed threat model to the existing backdoor literature. Filled circles denote features of the attacks and levels of adversarial access which are necessary for implementation. Partially filled circles denote optional levels of adversarial access.", "description": "This table compares the proposed \"SleeperNets\" attack with existing backdoor attacks in reinforcement learning (RL). It highlights the differences in attack type (inner-loop vs. outer-loop), the level of adversarial access required (manipulating the agent's observations, actions, rewards, the entire MDP or having full control), and the attack's objective (influencing the agent's policy or a specific action). Filled circles indicate necessary features or access levels, while partially filled circles show optional ones.", "section": "DRL Poisoning Attack Threat Models"}, {"figure_path": "HkC4OYee3Q/tables/tables_7_1.jpg", "caption": "Table 2: Comparison between BadRL-M, TrojDRL-W, and SleeperNets. Here the Benign Return Ratio (BRR) is measured as each agent's episodic return divided by the episodic return of an unpoisoned agent, and ASR is each attack's success rate. All results are rounded to the nearest tenth and capped at 100%. Standard deviation \u03c3 for all results is provided in their neighboring column.", "description": "This table compares the performance of three backdoor attacks (SleeperNets, TrojDRL-W, and BadRL-M) across six different reinforcement learning environments.  For each attack and environment, it shows the attack success rate (ASR, percentage of times the attack successfully triggered the target action) and the benign return ratio (BRR, the ratio of the poisoned agent's reward to that of a benign agent).  A higher ASR indicates more effective backdoor poisoning while a BRR close to 100% indicates stealth; meaning that the backdoor did not significantly impact the agent's performance in non-poisoned episodes.  Standard deviations are also included.", "section": "Experimental Results"}, {"figure_path": "HkC4OYee3Q/tables/tables_17_1.jpg", "caption": "Table 3: Comparison of the different environments tested in this work. All action spaces were discrete in some form, though for environments like Car Racing, Safety Car, and Trading-Env discretized versions of their continuous action spaces were used. The final column refers to the exact environment Id used when generating each environment through the gymnasium interface [1].", "description": "This table compares six different reinforcement learning environments used in the paper's experiments.  It details the type of task (video game, self-driving, robotics, stock trading), the type of observations provided to the agent (image, lidar and proprioceptive sensors, stock data), and the specific environment ID used in the OpenAI Gym.", "section": "Experimental Setup"}, {"figure_path": "HkC4OYee3Q/tables/tables_17_2.jpg", "caption": "Table 4: Attack and learning parameters used for each environment.  Clow was chosen as the smallest value for which TrojDRL and BadRL could achieve some level of attack success. Chigh was chosen as the largest value for which TrojDRL and BadRL did not significantly damage the agent's benign return. A similar method was used in determining the poisoning budget.", "description": "This table lists the hyperparameters used for each environment in the backdoor poisoning experiments.  It shows the trigger pattern used, the poisoning budget (\u03b2), the low and high reward constants (clow and chigh) for the reward perturbation, and the target action the adversary aimed to induce. The parameters were chosen to balance attack effectiveness and stealth, ensuring the poisoned agent's performance in benign settings wasn't significantly impacted.", "section": "6.1 Experimental Setup"}, {"figure_path": "HkC4OYee3Q/tables/tables_18_1.jpg", "caption": "Table 2: Comparison between BadRL-M, TrojDRL-W, and SleeperNets. Here the Benign Return Ratio (BRR) is measured as each agent's episodic return divided by the episodic return of an unpoisoned agent, and ASR is each attack's success rate. All results are rounded to the nearest tenth and capped at 100%. Standard deviation \u03c3 for all results is provided in their neighboring column.", "description": "This table compares the performance of three different backdoor attacks (SleeperNets, TrojDRL-W, and BadRL-M) across six different environments.  It shows the attack success rate (ASR) and the benign return ratio (BRR). BRR indicates how well the poisoned agent performs compared to a benignly trained agent.  The table highlights SleeperNets' superior performance in achieving high attack success while maintaining near-optimal benign performance.", "section": "Experimental Results"}, {"figure_path": "HkC4OYee3Q/tables/tables_19_1.jpg", "caption": "Table 2: Comparison between BadRL-M, TrojDRL-W, and SleeperNets. Here the Benign Return Ratio (BRR) is measured as each agent's episodic return divided by the episodic return of an unpoisoned agent, and ASR is each attack's success rate. All results are rounded to the nearest tenth and capped at 100%. Standard deviation \u03c3 for all results is provided in their neighboring column.", "description": "This table compares the performance of three different backdoor attacks (SleeperNets, TrojDRL-W, and BadRL-M) across six different reinforcement learning environments.  It shows both the attack success rate (ASR) and the benign return ratio (BRR). The ASR indicates how often the attack successfully induced the target action. The BRR compares the agent's performance on the benign task (without the trigger) to that of a benignly-trained agent.  Lower BRR indicates a more substantial performance reduction caused by the attack.", "section": "Experimental Results"}, {"figure_path": "HkC4OYee3Q/tables/tables_22_1.jpg", "caption": "Table 2: Comparison between BadRL-M, TrojDRL-W, and SleeperNets. Here the Benign Return Ratio (BRR) is measured as each agent's episodic return divided by the episodic return of an unpoisoned agent, and ASR is each attack's success rate. All results are rounded to the nearest tenth and capped at 100%. Standard deviation \u03c3 for all results is provided in their neighboring column.", "description": "This table compares the performance of three different backdoor poisoning attacks (SleeperNets, TrojDRL-W, and BadRL-M) across six different reinforcement learning environments.  The comparison focuses on two key metrics: Attack Success Rate (ASR), representing the percentage of times the attack successfully induces the agent to perform the target action when a trigger is present, and Benign Return Ratio (BRR), which measures the performance of the poisoned agent relative to a benignly trained agent on the original task. The table shows that SleeperNets consistently achieves much higher ASR (100% in all environments) while maintaining comparable BRR to the other approaches. Standard deviations are also reported for all metrics.", "section": "Experimental Results"}, {"figure_path": "HkC4OYee3Q/tables/tables_22_2.jpg", "caption": "Table 2: Comparison between BadRL-M, TrojDRL-W, and SleeperNets. Here the Benign Return Ratio (BRR) is measured as each agent's episodic return divided by the episodic return of an unpoisoned agent, and ASR is each attack's success rate. All results are rounded to the nearest tenth and capped at 100%. Standard deviation \u03c3 for all results is provided in their neighboring column.", "description": "This table compares the performance of three backdoor attacks (SleeperNets, BadRL-M, and TrojDRL-W) across six different reinforcement learning environments.  It shows the Attack Success Rate (ASR), representing the percentage of times the attack successfully induced the target action, and the Benign Return Ratio (BRR), indicating how well the poisoned agent performs on the benign task compared to an unpoisoned agent.  The results highlight SleeperNets' superior performance in terms of both ASR and BRR.", "section": "Experimental Results"}, {"figure_path": "HkC4OYee3Q/tables/tables_23_1.jpg", "caption": "Table 1: Tabular comparison of our proposed threat model to the existing backdoor literature. Filled circles denote features of the attacks and levels of adversarial access which are necessary for implementation. Partially filled circles denote optional levels of adversarial access.", "description": "This table compares the proposed threat model with existing backdoor attack methods, showing the differences in attack type, adversarial access level (information available to the attacker), and objective. It highlights that the proposed method uses an outer-loop model, providing more information to the adversary than inner-loop methods.", "section": "2 Adversarial Attacks in DRL \u2013 Related Work and Background"}]