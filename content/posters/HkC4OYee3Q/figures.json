[{"figure_path": "HkC4OYee3Q/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of the inner and outer-loop threat models. In an outer-loop attack the adversary can utilize information about completed episodes when determining their poisoning strategy. This information is not accessible in an inner-loop attack.", "description": "The figure compares two threat models for adversarial attacks against reinforcement learning agents: inner-loop and outer-loop.  In the inner-loop model, the adversary interferes with the agent's interaction with the environment on a per-timestep basis, having limited access to the episode's information. In contrast, the outer-loop model allows the adversary to observe the complete episode's trajectory (state, action, reward) before making poisoning decisions, using more global information.", "section": "3 Problem Formulation"}, {"figure_path": "HkC4OYee3Q/figures/figures_4_1.jpg", "caption": "Figure 2: (Left) MDP M\u2081 for which static reward poisoning fails to induce the target action a+. (Right) MDP M\u2082 for which static reward poisoning causes the agent to learn a sub-optimal policy.", "description": "This figure presents two counterexample Markov Decision Processes (MDPs) to illustrate the limitations of static reward poisoning.  The left MDP (M\u2081) demonstrates that static reward poisoning cannot always induce the target action (a+) because the reward structure makes a different action (a\u2081) preferable.  The right MDP (M\u2082) shows that static reward poisoning can lead to the agent learning a suboptimal policy for the benign task because the poisoned rewards make a longer path seem more rewarding, even when a shorter optimal path exists in the unpoisoned MDP.", "section": "4.1 Insufficiency of Static Reward Poisoning"}, {"figure_path": "HkC4OYee3Q/figures/figures_7_1.jpg", "caption": "Figure 3: Comparison of the SleeperNets, BadRL-M, and TrojDRL-W attacks on (Top) Highway Merge and (Bottom) Safety Car in terms of (Left) ASR and (Right) episodic return.", "description": "This figure compares the performance of SleeperNets, BadRL-M, and TrojDRL-W on two environments: Highway Merge and Safety Car.  It shows both the Attack Success Rate (ASR) and Episodic Return for each attack.  The top row shows the results for Highway Merge, while the bottom row shows the results for Safety Car. Each column represents a different metric: ASR (left) and episodic return (right). This visualization helps to understand the relative strengths and weaknesses of each attack method in terms of both achieving the backdoor objective and maintaining the agent's overall performance.", "section": "6 Experimental Results"}, {"figure_path": "HkC4OYee3Q/figures/figures_8_1.jpg", "caption": "Figure 4: (Top) Ablation with respect to poisoning budget \u03b2 for each attack given a fixed c = 40. (Bottom) Ablation with respect to c given a fixed poisoning budget of 0.5%. Both experiments were run on Highway Merge with a value of \u03b1 = 0 for SleeperNets.", "description": "This figure presents the results of ablations conducted on the Highway Merge environment to analyze the impact of poisoning budget (\u03b2) and reward constant (c) on the performance of three backdoor attacks: SleeperNets, TrojDRL-W, and BadRL-M. The top row shows the effect of varying \u03b2 while keeping c constant at 40, while the bottom row illustrates the effect of varying c while maintaining \u03b2 at 0.5%.  The \u03b1 parameter for SleeperNets was set to 0 in both experiments.  The results demonstrate SleeperNet's robustness and superiority in achieving high attack success rates even with reduced poisoning budgets and reward magnitudes compared to the other two algorithms.", "section": "6.3 Attack Parameter Ablations"}, {"figure_path": "HkC4OYee3Q/figures/figures_15_1.jpg", "caption": "Figure 2: (Left) MDP M\u2081 for which static reward poisoning fails to induce the target action a+. (Right) MDP M\u2082 for which static reward poisoning causes the agent to learn a sub-optimal policy.", "description": "This figure presents two counter-example Markov Decision Processes (MDPs) used to illustrate the limitations of static reward poisoning.  The left MDP (M\u2081) demonstrates that static reward poisoning can fail to induce the target action (a+) because the reward for the alternative action (a\u2081) becomes more attractive with a higher discount factor (\u03b3). The right MDP (M\u2082) shows that static reward poisoning can cause the agent to learn a sub-optimal policy in benign states because it artificially makes the longer, less efficient path more rewarding.  These examples highlight that static poisoning methods lack the adaptability needed to ensure both high attack success and stealth.", "section": "4.1 Insufficiency of Static Reward Poisoning"}, {"figure_path": "HkC4OYee3Q/figures/figures_19_1.jpg", "caption": "Figure 6: Best results for each attack on Breakout.", "description": "This figure shows a comparison of the SleeperNets, BadRL-M, and TrojDRL-W attacks on the Breakout Atari game.  The left panel displays the attack success rate (ASR) over time, demonstrating that SleeperNets achieves near-perfect attack success, while BadRL-M and TrojDRL-W have significantly lower success rates. The right panel shows the episodic return (a measure of the agent's performance on the game) over time.  This panel shows that SleeperNets maintains comparable episodic return to a non-poisoned agent, demonstrating stealth. In contrast, BadRL-M and TrojDRL-W show some trade-off between attack success and maintaining good performance.", "section": "6.2 SleeperNets Results"}, {"figure_path": "HkC4OYee3Q/figures/figures_20_1.jpg", "caption": "Figure 7: Best results for each attack on Q*bert.", "description": "The figure compares the performance of SleeperNets, BadRL-M, and TrojDRL-W attacks on the Q*bert Atari game environment.  The left subplot shows the attack success rate (ASR) over time, indicating the percentage of times the agent performed the targeted action when presented with a trigger. The right subplot shows the episodic return (cumulative reward) over time for each attack as well as a baseline of no poisoning.  These plots illustrate the effectiveness of each attack in achieving both high attack success and maintaining stealth (similar episodic returns to an unpoisoned agent).", "section": "6.2 SleeperNets Results"}, {"figure_path": "HkC4OYee3Q/figures/figures_20_2.jpg", "caption": "Figure 3: Comparison of the SleeperNets, BadRL-M, and TrojDRL-W attacks on (Top) Highway Merge and (Bottom) Safety Car in terms of (Left) ASR and (Right) episodic return.", "description": "This figure compares the performance of three different backdoor attacks (SleeperNets, BadRL-M, and TrojDRL-W) on two environments: Highway Merge and Safety Car.  The left side shows the Attack Success Rate (ASR), indicating how often the attack successfully triggered the target action.  The right side displays the Episodic Return, which measures the overall reward the agent received during an episode.  The figure highlights that SleeperNets outperforms the other methods in achieving high ASR and maintaining a similar episodic return to an unpoisoned agent.", "section": "Experimental Results"}, {"figure_path": "HkC4OYee3Q/figures/figures_20_3.jpg", "caption": "Figure 3: Comparison of the SleeperNets, BadRL-M, and TrojDRL-W attacks on (Top) Highway Merge and (Bottom) Safety Car in terms of (Left) ASR and (Right) episodic return.", "description": "This figure compares the performance of SleeperNets, BadRL-M, and TrojDRL-W attacks on two environments, Highway Merge and Safety Car.  It shows the attack success rate (ASR) and episodic return for each attack method. The plots clearly illustrate SleeperNets' superior performance in achieving high attack success rates while maintaining near-optimal episodic returns, significantly outperforming the baseline attacks.", "section": "6.2 SleeperNets Results"}, {"figure_path": "HkC4OYee3Q/figures/figures_20_4.jpg", "caption": "Figure 3: Comparison of the SleeperNets, BadRL-M, and TrojDRL-W attacks on (Top) Highway Merge and (Bottom) Safety Car in terms of (Left) ASR and (Right) episodic return.", "description": "This figure compares the performance of SleeperNets, BadRL-M, and TrojDRL-W in terms of attack success rate (ASR) and episodic return on two environments: Highway Merge and Safety Car.  The top row shows the ASR, while the bottom row shows the episodic return for each attack method.  The plots illustrate how SleeperNets achieves significantly higher ASR compared to the other methods while maintaining comparable episodic return. The figure visually demonstrates the effectiveness and stealth of the SleeperNets attack.", "section": "6.1 Experimental Setup"}, {"figure_path": "HkC4OYee3Q/figures/figures_21_1.jpg", "caption": "Figure 11: Best results for each attack on Trade BTC.", "description": "This figure shows the results of three different backdoor attacks (SleeperNets, TrojDRL-W, and BadRL-M) and a no-poisoning baseline on the Trade BTC environment. The left plot shows the attack success rate (ASR) over time, while the right plot shows the episodic return over time.  SleeperNets achieves 100% ASR and maintains a similar episodic return to the no-poisoning baseline, significantly outperforming the other attacks.", "section": "6.2 SleeperNets Results"}, {"figure_path": "HkC4OYee3Q/figures/figures_21_2.jpg", "caption": "Figure 12: Comparison of poisoning rate for each attack on Breakout and Q*bert", "description": "This figure compares the poisoning rates of SleeperNets, BadRL-M, and TrojDRL-W across two Atari games, Breakout and Q*bert.  The x-axis represents the number of timesteps in training, and the y-axis shows the poisoning rate (the fraction of training data poisoned).  SleeperNets demonstrates a much more aggressive annealing (reduction) of its poisoning rate over time compared to the other two methods.  This showcases the effectiveness of SleeperNets' strategy to maintain stealth by reducing its manipulation of the training data as the agent's performance improves. The shaded areas represent confidence intervals around the mean poisoning rate for each algorithm.", "section": "9.6 Poisoning Rate Annealing"}, {"figure_path": "HkC4OYee3Q/figures/figures_22_1.jpg", "caption": "Figure 13: Comparison of poisoning rate for each attack on Car Racing and Highway Merge", "description": "This figure compares the poisoning rates of SleeperNets, TrojDRL-W, and BadRL-M across two environments: Car Racing and Highway Merge.  It shows how the poisoning rate changes over time as the agents are trained.  The shaded regions likely represent confidence intervals or standard deviations, giving a measure of variability in the poisoning rate.  The plot visually demonstrates the effectiveness of SleeperNets' dynamic poisoning strategy in reducing the overall poisoning rate while maintaining high attack success compared to the static poisoning techniques used by TrojDRL-W and BadRL-M. ", "section": "6.2 SleeperNets Results"}, {"figure_path": "HkC4OYee3Q/figures/figures_22_2.jpg", "caption": "Figure 12: Comparison of poisoning rate for each attack on Breakout and Q*bert", "description": "This figure compares the poisoning rate over time for three different backdoor attacks (SleeperNets, BadRL-M, TrojDRL-W) on two Atari games: Breakout and Q*bert.  The poisoning rate represents the fraction of training data points modified by each attack.  It shows how the attacks adjust their poisoning strategy throughout the training process to maintain attack success while trying to remain stealthy.  SleeperNets exhibits a significant decrease in its poisoning rate over time compared to the other attacks, indicating a more efficient and stealthy poisoning approach.", "section": "9.6 Poisoning Rate Annealing"}]