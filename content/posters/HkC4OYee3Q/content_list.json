[{"type": "text", "text": "SleeperNets: Universal Backdoor Poisoning Attacks Against Reinforcement Learning Agents ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Ethan Rathbun\u2217, Christopher Amato\u2020, Alina Oprea\u2020 Khoury College of Computer Sciences, Northeastern University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reinforcement learning (RL) is an actively growing field that is seeing increased usage in real-world, safety-critical applications \u2013 making it paramount to ensure the robustness of RL algorithms against adversarial attacks. In this work we explore a particularly stealthy form of training-time attacks against RL \u2013 backdoor poisoning. Here the adversary intercepts the training of an RL agent with the goal of reliably inducing a particular action when the agent observes a pre-determined trigger at inference time. We uncover theoretical limitations of prior work by proving their inability to generalize across domains and MDPs. Motivated by this, we formulate a novel poisoning attack framework which interlinks the adversary\u2019s objectives with those of finding an optimal policy \u2013 guaranteeing attack success in the limit. Using insights from our theoretical analysis we develop \u201cSleeperNets\u201d as a universal backdoor attack which exploits a newly proposed threat model and leverages dynamic reward poisoning techniques. We evaluate our attack in 6 environments spanning multiple domains and demonstrate significant improvements in attack success over existing methods, while preserving benign episodic return. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Interest in Reinforcement Learning (RL) methods has grown exponentially, in part sparked by the development of powerful Deep Reinforcement Learning (DRL) algorithms such as Deep Q-Networks (DQN) [24] and Proximal Policy Optimization (PPO) [31]. With this effort comes large scale adoption of RL methods in safety and security critical settings, including fine-tuning Large Language Models [26], operating self-driving vehicles [15], organizing robots in distribution warehouses [17], managing stock trading profiles [13], and even coordinating space traffic [5]. ", "page_idx": 0}, {"type": "text", "text": "These RL agents, both physical and virtual, are given massive amounts of responsibility by the developers and researchers allowing them to interface with the real world and real people. Poorly trained or otherwise compromised agents can cause significant damage to those around them [8], thus it is crucial to ensure the robustness of RL algorithms against all forms of adversarial attacks, both at training time and deployment time. ", "page_idx": 0}, {"type": "text", "text": "Backdoor attacks [9] [14] are particularly powerful and difficult to detect training-time attacks. In these attacks the adversary manipulates the training of an RL agent on a given Markov Decision Process (MDP) with the goal of solving two competing objectives. The first is high attack success, i.e., the adversary must be able to reliably induce the agent to take a target action $a^{+}$ whenever they observe the fixed trigger pattern $\\delta$ at inference time \u2013 irrespective of consequence. At the same time, the adversary wishes to maintain attack stealth by allowing the agent to learn a near-optimal policy on the training task \u2013 giving the illusion that the agent was trained properly. ", "page_idx": 0}, {"type": "text", "text": "Prior works on backdoor attacks in RL [14] [4] [35] use static reward poisoning techniques with inner-loop threat models. Here the trigger is embedded into the agent\u2019s state-observation and their reward is altered to a fixed value of $\\pm c$ during each episode. These attacks have proven effective in ", "page_idx": 0}, {"type": "image", "img_path": "HkC4OYee3Q/tmp/4e625b8e701ffb30121013277cdff9bbf0b35e1457f09c49765c185a6775b2f4.jpg", "img_caption": ["Figure 1: Comparison of the inner and outer-loop threat models. In an outer-loop attack the adversary can utilize information about completed episodes when determining their poisoning strategy. This information is not accessible in an inner-loop attack. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Atari domains [1], but lack any formal analysis or justification. In fact, we find that static reward poisoning attacks, by failing to adapt to different states, MDPs, or algorithms, often do not achieve both adversarial objectives. We further find that the inner loop threat model \u2013 by forcing the adversary to make decisions on a per-time-step basis \u2013 greatly limits the amount of information the adversary has access to. These limitations indicate the need for a more dynamic reward poisoning strategy which exploits a more global, yet equally accessible, view of the agent\u2019s training environment. ", "page_idx": 1}, {"type": "text", "text": "Thus, in this work, we take a principled approach towards answering the fundamental question: \u201cWhat poisoning methods are necessary for successful RL backdoor attacks?\u201d. Through this lens we provide multiple contributions: ", "page_idx": 1}, {"type": "text", "text": "1. The first formal analysis of static reward poisoning attacks, highlighting their weaknesses.   \n2. A new \u201couter-loop\u201d threat model in which the adversary manipulates the agent\u2019s rewards and state-observations after each episode \u2013 allowing for better-informed poisoning attacks.   \n3. A novel framework \u2013 utilizing dynamic reward poisoning \u2013 for developing RL backdoor attacks with provable guarantees of attack success and stealth in the limit.   \n4. A novel backdoor attack, SleeperNets, based on insights from our theoretical guarantees that achieves universal backdoor attack success and stealth.   \n5. Comprehensive analysis of our SleeperNets attack in multiple environments including robotic navigation, video game playing, self driving, and stock trading tasks. Our method displays significant improvements in attack success rate and episodic return over the current state-of-the-art at extremely low poisoning rates (less than $0.05\\%$ ). ", "page_idx": 1}, {"type": "text", "text": "2 Adversarial Attacks in DRL \u2013 Related Work and Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Two main threat vectors have been explored for adversarial attacks against RL agents: training time poisoning attacks and inference time evasion attacks. In this work we focus primarily on backdoor poisoning attacks. Thus, in this section we provide a brief overview of the existing backdoor attack literature \u2013 highlighting their contributions. Background on evasion attacks and policy replacement poisoning attacks is provided in Appendix 9.9. ", "page_idx": 1}, {"type": "text", "text": "Backdoor attacks target agents trying to optimize a particular Markov Decision Process (MDP). These MDPs are specified as the tuple $(\\bar{S_{}}\\bar{A},\\bar{R},T,\\bar{\\gamma_{}}\\bar{\\)}$ where $S$ is the state space, $A$ is the set of possible actions, $R:S\\times A\\times S\\to\\mathbb{R}$ is the reward function, $T:S\\times A\\times S\\rightarrow[0,1]$ represents the transition probabilities between states given actions, and $\\gamma$ is the discount factor. Generally, backdoor attacks are designed to target agents trained with state-of-the-art DRL algorithms such as PPO [31], A3C [23], or DQN [24]. ", "page_idx": 1}, {"type": "text", "text": "TrojDRL was one of the first works to explore backdoor attacks in DRL \u2013 poisoning agents trained using A3C [23] to play Atari games. They utilize a static reward poisoning technique in which the agent receives $+1$ reward if they follow the target action in poisoned states, and $-1$ otherwise (Eq 3). BadRL [4] then built upon these initial findings by using a pre-trained Q-network to determine in which states the target action $a^{+}$ is most harmful. This comes at the cost of a much stronger threat model \u2013 the adversary needs full access to the victim\u2019s benign MDP to train their Q-network, which is unlikely when the victim party is training on a proprietary MDP. Both TrojDRL and BadRL also explore attacks which are capable of directly manipulating the agent\u2019s actions during training. Compared to attacks which only perturb state observations and rewards, these attacks are easier to detect, result in more conservative policies, and require a stronger threat model. We additionally find that such adversarial capability is unnecessary for a successful attack \u2013 both attack success and attack stealth can be achieved with reward and state manipulation alone as explained in Section 4. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Other works have explored alternate threat models for RL backdoor attacks. [38] targets agents trained in cooperative settings to induce an agent to move towards a particular state in the state space. [36] poisons the agent for multiple time steps in a row to try and enforce backdoors that work over longer time horizons. [35] targets RNN architectures to induce backdoors which still work after the trigger has disappeared from the observation. Lastly, [34] utilizes total control over the training procedure and imitation learning to induce a fixed policy in the agent upon observing the trigger. In addition to backdoor attacks, adversarial cheap talk [20] has been proposed as an alternative, training time attack which poisons the agent through a \u201ccheap talk\u201d channel external to the agent\u2019s latent observations in the environment. ", "page_idx": 2}, {"type": "text", "text": "Parallel to the study of backdoor attacks, other works have studied the test time detection [25] and stealth [6] of adversarial attacks through an information theoretic lens. While objectives of test time detectability are outside the scope of this paper, we believe studying the test time detectability backdoor attacks is an interesting and important research direction for future works. We additionally note that the framework we present in this work makes no assumptions about the adversary\u2019s test time objectives, allowing for easy adaptations of our proposed methodology to this setting. ", "page_idx": 2}, {"type": "text", "text": "3 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In our setting we consider two parties: the victim and the adversary. The victim party aims to train an agent to solve a benign MDP $M=(S,A,T,R,\\gamma)$ . Here we define $S$ to be a subset of a larger space $\\mathbb{S}$ , for instance $S\\subset\\mathbb{R}^{n\\times m}$ is a subset of possible $n\\times m$ images. The victim agent implements a stochastic learning algorithm, $L(M)$ , taking MDP $M$ as input and returning some potentially stochastic policy $\\pi:S\\times A\\rightarrow[0,1]$ . ", "page_idx": 2}, {"type": "text", "text": "The adversary, in contrast, wants to induce the agent to associate some adversarial trigger pattern $\\delta$ with a target action $a^{+}$ , and choose this action with high probability when they observe $\\delta$ . We refer to this adversarial objective as attack success, similarly to prior work objectives [4]. ", "page_idx": 2}, {"type": "text", "text": "In addition to maximizing attack success, the adversary must also minimize the detection probability during both training and execution. There are multiple ways to define this attack stealth objective, but we choose an objective which draws from existing literature [14] [4] \u2013 the poisoned agent should perform just as well as a benignly trained agent in the unpoisoned MDP $M$ . Specifically, the adversary poisons the training algorithm $L$ such that it instead trains the agent to solve an adversarially constructed MDP $M^{\\prime}$ with the following optimization problems: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbf{Attack~Success:}\\ \\operatorname*{max}_{M^{\\prime}}[\\mathbb{E}_{s\\in S,\\pi^{+}}[\\pi^{+}(\\delta(s),a^{+})]]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $a^{+}\\in A$ is the adversarial target action and $\\delta:\\mathbb{S}\\rightarrow\\mathbb{S}$ is the \u201ctrigger function\u201d \u2013 which takes in a benign state $s\\in S$ and returns a perturbed version of that state with the trigger pattern embedded into it. For simplicity, we refer to the set of states within which the trigger has been embedded as \u201cpoisoned states\u201d or $\\dot{S}_{p}\\doteq\\delta(S)$ . Additionally, each $\\pi^{+}\\sim L(M^{\\prime})$ is a policy trained on the adversarial MDP $M^{\\prime}$ , and each $\\pi\\sim L(M)$ is a policy trained on the benign MDP $M$ . Lastly, $V_{\\pi}^{M}(s)$ is the value function which measures the value of policy $\\pi$ at state $s$ in the benign MDP $M$ . ", "page_idx": 2}, {"type": "text", "text": "3.1 Threat Model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We introduce a novel, outer-loop threat model (Figure 1) which targets the outer, policy evaluation loop of DRL algorithms such as PPO and DQN. In contrast, prior works rely on inner-loop threat models [14] [4] which target the inner environment interaction loop of DRL algorithms. In both attacks the adversary is constrained by a poisoning budget parameter $\\beta$ that specifies the fraction of state observations they can poison in training. Additionally, the adversary is assumed to either have access to the agent\u2019s state observations, rewards, and actions at each time step \u2013 for inner loop attacks \u2013 or have access to trajectories generated after each episode is finished for outer loop attacks. Therefore, for inner loop attacks the adversary must have direct access to both the agent\u2019s environment and training system (server, desktop, etc.). In the case of outer loop attacks against online RL, which is the subject of this paper, the adversary must have direct control over the victim\u2019s training process to be able to poison the agent\u2019s reward in certain states. While the assumption of a server compromise appears to be strong, such breaches are unfortunately common with 5,175 found in 2024 by the most recent Verizon Data Breach Investigation Report (DBIR) [11]. The outer loop threat model can also be easily extended to offilne RL [16, 37], which would require weaker assumptions for the adversary as they must only manipulate a static dataset. We leave evaluations against offline RL algorithms to future work. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "table", "img_path": "HkC4OYee3Q/tmp/02ab9c33af02f74b2614429e1fc5002abd75d59ac121b58569b2bff434970143.jpg", "table_caption": ["Table 1: Tabular comparison of our proposed threat model to the existing backdoor literature. Filled circles denote features of the attacks and levels of adversarial access which are necessary for implementation. Partially filled circles denote optional levels of adversarial access. "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "When implementing an outer-loop attack, the adversary first allows the agent to complete a full episode, generating some trajectory $\\boldsymbol{H}=\\{(s,a,r)_{t}\\}_{t=1}^{\\phi}$ of size $\\phi$ from $M$ given the agent\u2019s c urrent policy $\\pi$ . The adversary can then observe $H$ and use this information to decide which subset $H^{\\prime}\\subset H$ of the trajectory to poison and determine how to poison the agent\u2019s reward. The poisoned trajectory is then either placed in the agent\u2019s replay buffer $\\mathcal{D}-\\mathbf{a}\\mathbf{s}$ in algorithms like DQN \u2013 or directly used in the agent\u2019s policy evaluation \u2013 as in algorithms like PPO. ", "page_idx": 3}, {"type": "text", "text": "In contrast, when implementing an inner-loop attack, the adversary observes the agent\u2019s current state $s_{t}$ before the agent at each time step $t$ . If they choose to poison at step $t$ they can first apply the trigger $s_{t}\\gets\\bar{\\delta(s_{t})}$ , observe the subsequent action $a_{t}$ taken by the agent, and then alter their reward accordingly $r_{t}\\gets\\pm c$ . Optionally, the adversary can also alter the agent\u2019s action before it is executed in the environment, as used in stronger versions of BadRL [4] and TrojDRL [14]. The main drawback of this threat model is that the adversary must determine when and how to poison the current time step $(s,a,r)_{t}$ immediately upon observing $s_{t}$ , while, in the outer-loop threat model, they are given a more global perspective by observing the complete trajectory $H$ . ", "page_idx": 3}, {"type": "text", "text": "Thus, the outer-loop threat model requires the same level of adversarial access to the training process as the inner-loop threat model, but it enables more powerful attacks. This allows us to achieve higher attack success rates \u2013 while modifying only states and rewards \u2013 than inner-loop attacks, which may rely on action manipulation for high attack success [14], [4]. In Table 1 we compare the threat model of our SleeperNets attack with the existing literature. ", "page_idx": 3}, {"type": "text", "text": "4 Theoretical Results ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section we provide theoretical results proving the limitations of static reward poisoning and the capabilities of dynamic reward poisoning. We first design two MDPs which will provably prevent static reward poisoning attacks, causing the attack to either fail in terms of attack success or attack stealth. Motivated by these findings, we develop a novel adversarial MDP which leverages dynamic reward poisoning to achieve stronger guarantees for attack success. ", "page_idx": 3}, {"type": "text", "text": "4.1 Insufficiency of Static Reward Poisoning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Existing backdoor attack techniques [14] [4] [35] use a form of static reward poisoning. Formally, we say a reward poisoning technique is static if the agent\u2019s reward, at any poisoned time step, is altered to some pre-determined, fixed values: ", "page_idx": 3}, {"type": "equation", "text": "$$\nR(\\delta(s),a)=\\mathbb{1}_{c}[a=a^{+}]\\doteq{\\left\\{\\begin{array}{l l}{c}&{{\\mathrm{if}}\\;a=a^{+}}\\\\ {-c}&{{\\mathrm{otherwise}}}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We formally prove that this technique of reward poisoning is insufficient for achieving both attack success and attack stealth, irrespective of how large $c$ or $\\beta$ are. In Figure 2 we present two counterexample MDPs which highlight two key weaknesses of static reward poisoning. In $M_{1}$ we exploit the inability of static reward poisoning to scale with respect to the discount factor $\\gamma$ . The return the agent receives for taking action $a_{1}$ in the poisoned state $\\delta(\\mathrm{Start})$ surpasses $2c$ if $\\gamma>$ .5 \u2013 making the target action $a^{+}$ sub-optimal given the static return of $+c$ . In $M_{2}$ we then exploit the inability of static reward poisoning to vary across states. In the benign version of $M_{2}$ it is optimal for the agent to take the shorter \u201cFast\u201d path to the finish. However, since the agent receives an extra reward of $+c$ in poisoned states, the longer \u201cSlow\u201d path becomes optimal under static reward poisoning. Fully worked out proofs for these claims are given in Appendix 9.2. ", "page_idx": 3}, {"type": "image", "img_path": "HkC4OYee3Q/tmp/630638295d720c0a9d6efe59d932c23ea402b22bf1fc60eee00b2311eded4e5e.jpg", "img_caption": ["Figure 2: (Left) MDP $M_{1}$ for which static reward poisoning fails to induce the target action $a^{+}$ (Right) MDP $M_{2}$ for which static reward poisoning causes the agent to learn a sub-optimal policy. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "4.2 Base Assumptions for Dynamic Reward Poisoning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In our attack formulation we make two additional assumptions needed for our theoretical results, but malleable for our empirical results. First, we assume that the attacker implements purely out of distribution triggers, more formally: $S_{p}\\cap S=\\emptyset$ . Fulfilling this assumption in our theoretical results prevents conflicts between the adversarially induced reward function and the benign MDP\u2019s reward function. In practice, attacks can still be successful if this assumption is relaxed, as long as the poisoned states $s_{p}\\in S_{p}$ are sufficiently rare in the benign MDP during training. ", "page_idx": 4}, {"type": "text", "text": "One additional assumption is that $\\delta$ must be an invertible function between $S$ and $S_{p}$ . While this seems like a strong assumption at first glance, it does not hinder attack performance in practice. In the rest of the paper we frequently use $\\bar{\\delta}^{-1}(s_{p})$ as a shorthand to denote the benign state $s$ from which we received $s_{p}=\\delta(s)$ . In practice, we are given the benign state $s$ first, and then apply the trigger, thus we never need to actually compute the inverse of $\\delta$ . ", "page_idx": 4}, {"type": "text", "text": "4.3 Dynamic Reward Poisoning Attack Formulation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As previously discussed, the adversary\u2019s goal is to design and enforce an adversarial MDP, $M^{\\prime}$ , which optimizes Equations (1) and (2). Our attack influences the agent\u2019s behavior through the learning algorithm $L$ , which approximates an optimal policy for the agent in a given MDP. We leverage this fact by defining an adversarial MDP such that the optimal policy solves both our attack success and attack stealth objectives: ", "page_idx": 4}, {"type": "equation", "text": "$$\nM^{\\prime}\\doteq(S\\cup S_{p},A,T^{\\prime},R^{\\prime},\\gamma)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In $M^{\\prime}$ , $T^{\\prime}$ is an adversarially induced transition function and $R^{\\prime}$ is an adversarially induced reward function. The transition dynamics of this MDP are fairly intuitive \u2013 at any given time step the agent can be in a poisoned state $s_{p}\\in S_{p}$ with probability $\\beta$ , or a benign state $s\\in S$ with probability $(1-\\beta)$ for poisoning rate $\\beta\\in[0,\\bar{1})$ . Transitions otherwise follow the same dynamics of the benign MDP $M$ . Formally, we define $T^{\\prime}$ as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nT^{\\prime}:(S\\cup S_{p})\\times A\\times(S\\cup S_{p})\\rightarrow[0,1]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\nT^{\\prime}(s,a,s^{\\prime})\\doteq\\left\\{\\begin{array}{c}{(1-\\beta)\\cdot T(s,a,s^{\\prime})}\\\\ {\\beta\\cdot T(s,a,\\delta^{-1}(s^{\\prime}))}\\\\ {\\beta\\cdot T(\\delta^{-1}(s),a,\\delta^{-1}(s^{\\prime}))}\\\\ {(1-\\beta)\\cdot T(\\delta^{-1}(s),a,s^{\\prime})}\\end{array}\\right|\\begin{array}{c}{i f s\\in S,\\ s^{\\prime}\\in S}\\\\ {i f s\\in S,\\ s^{\\prime}\\in S_{p}}\\\\ {i f s\\in S_{p},\\ s^{\\prime}\\in S_{p}}\\\\ {i f s\\in S_{p},\\ s^{\\prime}\\in S}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The adversarial reward $R^{\\prime}$ , on the other hand, dynamically leverages the current policy\u2019s value at each state $s\\in S\\cup S_{p}$ to achieve our two-fold adversarial objective. First, $R^{\\prime}$ is designed such that the agent\u2019s actions in poisoned states do not impact its return in benign states \u2013 allowing an optimal policy in $M^{\\prime}$ to also be an optimal policy in $M$ . Second, $R^{\\prime}$ is formulated such that the agent\u2019s return in poisoned states are agnostic to future consequences \u2013 guaranteeing that the target action is optimal in any poisoned state. Formally we define $R^{\\prime}$ as: ", "page_idx": 4}, {"type": "equation", "text": "$$\nR^{\\prime}:(S\\cup S_{p})\\times A\\times(S\\cup S_{p})\\times\\Pi\\to\\mathbb{R}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\nR^{\\prime}(s,a,s^{\\prime},\\pi)\\doteq\\left\\{\\begin{array}{c}{R(s,a,s^{\\prime})}\\\\ {R(s,a,\\delta^{-1}(s^{\\prime}))-\\gamma V_{\\pi}^{M^{\\prime}}(s^{\\prime})+\\gamma V_{\\pi}^{M^{\\prime}}(\\delta^{-1}(s^{\\prime}))}\\\\ {\\mathbb{I}[a=a^{+}]-\\gamma V_{\\pi}^{M^{\\prime}}(s^{\\prime})}\\end{array}\\right|_{\\displaystyle\\begin{array}{c}{i f s\\in S,\\,s^{\\prime}\\in S}\\\\ {i f s\\in S,s^{\\prime}\\in S_{p}}\\\\ {i f s\\in S_{p}}\\end{array}}\\right.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\Pi\\doteq\\{\\pi:(\\pi:S\\cup S_{p}\\times A\\rightarrow[0,1])\\}$ is the set of all valid policies over $M^{\\prime}$ , $R$ is the benign MDP\u2019s reward function, and $a^{+}$ is the target action. $R^{\\prime}$ is a function of not only the current state $s$ , action $a$ , and next state $s^{\\prime}$ , but also of the agent\u2019s current policy $\\pi$ . In our theoretical developments this gives us access to the policy\u2019s value at each state, allowing us to directly cancel out terms in the Bellman optimally equation and thus achieve our desired guarantees. In practice however, we do not have direct access to policy\u2019s value in each state. In Section 5 we show how the outer-loop threat model can be used to find an approximation of $V_{\\pi}(s)$ , which leverages our theoretical findings. ", "page_idx": 5}, {"type": "text", "text": "4.4 Theoretical Guarantees of Dynamic Reward Poisoning ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The capabilities of dynamic reward poisoning become most evident through its theoretical properties. We first prove two strong lemmas which we then leverage to prove that $M^{\\prime}$ optimizes both our objective of attack success in Equation (1) and attack stealth in Equation (2). ", "page_idx": 5}, {"type": "text", "text": "Lemma 1 $V_{\\pi}^{M^{\\prime}}(s_{p})=\\pi(s_{p},a^{+})\\;\\forall s_{p}\\in S_{p},\\pi\\in\\Pi.$ . Thus, the value of a policy $\\pi$ in poisoned states $s_{p}\\in S_{p}$ is equal to the probability with which it chooses action $a^{+}$ . ", "page_idx": 5}, {"type": "text", "text": "Lemma 2 $V_{\\pi}^{M}(s)=V_{\\pi}^{M^{\\prime}}(s)\\;\\forall s\\in S,\\pi\\in\\Pi$ . Therefore, the value of any policy $\\pi$ in the adversarial MDP $M^{\\prime}$ is equal to its value in the benign MDP $M$ for all benign states $s\\in S$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 1 $V_{\\pi^{*}}^{M^{\\prime}}(s_{p})\\,\\geq\\,V_{\\pi}^{M^{\\prime}}(s_{p})\\ \\forall s_{p}\\,\\in\\,S_{p},\\pi\\,\\in\\,\\Pi\\,\\Leftrightarrow\\,\\pi^{*}(s_{p},a^{+})\\,=\\,1\\ \\forall s_{p}\\,\\in\\,S_{p}$ . Thus, $\\pi^{*}$ is optimal in $M^{\\prime}$ if and only $i f\\pi^{*}$ takes action $a^{+}$ with probability 1 in poisoned states $\\bar{s}_{p}\\in S_{p}$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 2 $V_{\\pi^{*}}^{M^{\\prime}}(s)\\geq V_{\\pi}^{M^{\\prime}}(s)\\,\\forall s\\in S,\\pi\\in\\Pi\\Leftrightarrow V_{\\pi^{*}}^{M}(s)\\geq V_{\\pi}^{M}(s)\\,\\forall s\\in S,\\pi\\in\\Pi$ . Therefore, $\\pi^{*}$ is optimal in $M^{\\prime}$ for all benign states $s\\in S$ if and only $i f\\pi^{*}$ is optimal in $M$ . ", "page_idx": 5}, {"type": "text", "text": "Furthermore, we know that the victim party\u2019s training algorithm $L$ is designed to maximize overall return in the MDP it is solving. Thus, in order to maximize this return, and as a result of Theorem 1 and Theorem 2, the algorithm must produce a poisoned policy which optimizes our objectives of attack success and attack stealth. From this we can conclude that $M^{\\prime}$ optimizes Equations (1) and (2). Rigorous proofs of all our lemmas and theorems can be found in Appendix 9.1. ", "page_idx": 5}, {"type": "text", "text": "5 Attack Algorithm ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The goal of the SleeperNets attack is to replicate our adversarial MDP $M^{\\prime}$ as outlined in the previous section, thus allowing us to leverage our theoretical results. Since we do not have direct access to $V_{\\pi}^{M^{\\prime}}(s)$ for any $s\\in S\\cup S_{p}$ , we must find a way of approximating our adversarial reward function $R^{\\prime}$ . To this end we make use of our aforementioned outer-loop threat model \u2013 the adversary alters the agent\u2019s state-observations and rewards after a trajectory is generated by the agent in $M$ , but before any policy update occurs. The adversary can then observe the completed trajectory and use this information to form a better approximation of $V_{\\pi}^{M^{\\prime}}(s)$ for any $s\\in S\\cup S_{p}$ . ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 The SleeperNets Attack ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Initialize Policy $\\pi$ , Replay Memory $\\mathcal{D}$ , max episodes $N$   \nInput poisoning budget $\\beta$ , weighting factor $\\alpha$ , reward constant $c$ , trigger function $\\delta$ , policy update algorithm $L$ , benign MDP $\\bar{M}=\\bar{(}S,A,R,T,\\gamma)$   \n1: for $i\\leftarrow1,N$ do   \n2: Sample trajectory $\\boldsymbol{H}=\\left\\{(\\boldsymbol{s},a,r)_{t}\\right\\}_{t=1}^{\\phi}$ of size $\\phi$ from $M$ given policy $\\pi$   \n3: Sample $H^{\\bar{\\prime}}\\subset H$ uniformly randomly s.t. $|H^{\\prime}|=\\lfloor\\beta\\cdot|H|\\rfloor$   \n4: for all $(s,a,r)_{t}\\in H^{\\prime}$ do   \n5: Compute value estimates $\\hat{V}(s_{t},H)$ , $\\hat{V}(s_{t+1},H)$ using known trajectory $H$   \n6: $\\begin{array}{r l}&{s_{t}\\leftarrow\\delta(s_{t})}\\\\ &{r_{t}\\leftarrow\\mathbb{1}_{c}[a_{t}=a^{+}]-\\alpha\\gamma\\hat{V}(s_{t+1},H)}\\\\ &{r_{t-1}\\leftarrow r_{t-1}-\\gamma r_{t}+\\gamma\\hat{V}(s_{t},H)}\\end{array}$   \n7:   \n8:   \n9: Store $H$ in Replay Memory $\\mathcal{D}$   \n10: Update $\\pi$ According to $L$ given $\\mathcal{D}$ , flush or prune $\\mathcal{D}$ according to $L$ ", "page_idx": 5}, {"type": "text", "text": "Specifically, the learning algorithm $L$ first samples a trajectory $\\boldsymbol{H}=\\{(s,a,r)_{t}\\}_{t=1}^{\\phi}$ of size $\\phi$ from $M$ given the agent\u2019s current policy $\\pi$ . The adversary then samples a random subset ${\\bf\\bar{{\\cal H}^{\\prime}}}\\subset{\\cal H}$ of size $\\lfloor\\beta\\cdot\\left\\lfloor H\\right\\rfloor\\rfloor$ from the trajectory to poison. Given each $(s,a,r)_{t}\\stackrel{\\cdot}{\\in}H^{\\prime}$ the adversary first applies the trigger pattern $\\displaystyle\\delta(s_{t})$ to the agent\u2019s state-observation, and then computes a Monte-Carlo estimate of the value of $s_{t}$ and $s_{t+1}$ as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\hat{V}(s_{t},H)\\doteq\\sum_{i=t}^{|H|}\\gamma^{i-t}r_{t}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "This is an unbiased estimator of $V_{\\pi}^{M^{\\prime}}(s_{t})$ [33] which sees usage in methods like PPO and A2C \u2013 thus allowing us to accurately approximate $V_{\\pi}^{M^{\\prime}}(s)$ for any $s\\in S\\cup S_{p}$ in expectation. We then use these estimates to replicate our adversarial reward function $R^{\\prime}$ as seen in lines 7 and 8 of Algorithm 1. ", "page_idx": 6}, {"type": "text", "text": "We additionally introduce two hyper parameters, $c$ and $\\alpha$ , as a relaxation of $R^{\\prime}$ . Larger values of $\\alpha$ perturb the agent\u2019s reward to a larger degree \u2013 making the attack stronger, but perhaps making generalization more difficult in DRL methods. On the other hand, smaller values of $\\alpha$ perturb the agent\u2019s reward less \u2013 making it easier for DRL methods to generalize, but perhaps weakening the attack\u2019s strength. Similarly, $c$ scales the first term of $R^{\\prime}$ in poisoned states, $\\mathbb{1}_{c}[a=a^{+}]$ , meaning larger values of $c$ will perturb the agent\u2019s reward to a greater degree. In practice we find the SleeperNets attack to be very stable w.r.t. $\\alpha$ and $c$ on most MDPs, however some settings require more hyper parameter tuning to maximize attack success and stealth. ", "page_idx": 6}, {"type": "text", "text": "6 Experimental Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section we evaluate our SleeperNets attack in terms of our two adversarial objectives \u2013 attack success and attack stealth \u2013 on environments spanning robotic navigation, video game playing, self driving, and stock trading tasks. We further compare our SleeperNets attack against attacks from BadRL [4] and TrojDRL [14] \u2013 displaying significant improvements in attack success while more reliably maintaining attack stealth. ", "page_idx": 6}, {"type": "text", "text": "6.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We compare SleeperNets against two versions of BadRL and TrojDRL which better reflect the less invasive threat model of the SleeperNets attack. First is TrojDRL-W in which the adversary does not manipulate the agent\u2019s actions. Second is BadRL-M which uses a manually crafted trigger pattern rather than one optimized with gradient based techniques. These two versions were chosen so we can directly compare each method\u2019s reward poisoning technique and better contrast the inner and outer loop threat models. Further discussion on this topic can be found in Appendix 9.3. ", "page_idx": 6}, {"type": "text", "text": "We evaluate each method on a suite of 6 diverse environments against agents trained using the cleanrl [10] implementation of PPO [31]. First, to replicate and validate the results of [4] and [14] we test all attacks on Atari Breakout and Qbert from the Atari gymnasium suite [1]. In our evaluation we found that these environments are highly susceptible to backdoor poisoning attacks, thus we extend and focus our study towards the following 4 environments: Car Racing from the Box2D gymnasium [1], Safety Car from Safety Gymnasium [12], Highway Merge from Highway Env [18], and Trading BTC from Gym Trading Env [27]. ", "page_idx": 6}, {"type": "text", "text": "In each environment we first train an agent on the benign, unpoisoned MDP until convergence to act as a baseline. We then evaluate each attack on two key metrics \u2013 episodic return and attack success rate (ASR). Episodic return is measured as the agent\u2019s cumulative, discounted return as it is training. This metric will be directly relayed to the victim training party, and thus successful attacks will have training curves which match its respective, unpoisoned training curve. Attack success rate is measured by first generating a benign episode, applying the trigger to every state-observation, and then calculating the probability with which the agent chooses the target action $a^{+}$ . Successful attacks will attain ASR values near $100\\%$ . All attacks are tested over 2 values of $c$ and averaged over 3 seeds given a fixed poisoning budget. c values and poisoning budgets vary per environment. In each plot and table we then present best results from each attack. Further experimental details and ablations can be found in Appendix 9.4, Appendix 9.5, Appendix 9.6, and Appendix 9.7. ", "page_idx": 6}, {"type": "text", "text": "6.2 SleeperNets Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In Table 2 we highlight the universal capabilities of the SleeperNets attack across 6 environments spanning 4 different domains. Our attack is able to achieve an average attack success rate of $100\\%$ over 3 different seeds on all environments \u2013 highlighting the attack\u2019s reliability. Additionally, our attack causes little to no decrease in benign episodic return \u2013 producing policies that perform at least $96.5\\%$ as well as an agent trained with no poisoning. Lastly, since our attack regularly achieves $100\\%$ ASR during training, we are able to anneal our poisoning rate over time \u2013 resulting in extremely low poisoning rates like $0.001\\%$ on Breakout and $0.0006\\%$ on Trade BTC. In practice we anneal poisoning rates for all three methods by only poisoning when the current ASR is less than $100\\%$ . More detailed discussion on annealing can be found in Appendix 9.6. ", "page_idx": 6}, {"type": "table", "img_path": "HkC4OYee3Q/tmp/93e3d06060d4f0f96467809f348c7dd94a53c9e38ff888a794845a45f3e9bf04.jpg", "table_caption": [], "table_footnote": ["Table 2: Comparison between BadRL-M, TrojDRL-W, and SleeperNets. Here the Benign Return Ratio (BRR) is measured as each agent\u2019s episodic return divided by the episodic return of an unpoisoned agent, and ASR is each attack\u2019s success rate. All results are rounded to the nearest tenth and capped at $100\\%$ . Standard deviation $\\sigma$ for all results is provided in their neighboring column. "], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "HkC4OYee3Q/tmp/d2eb52695f481650bd7546d183758dc8f4824c35482449892ea25c8cdc48a5ee.jpg", "img_caption": ["Figure 3: Comparison of the SleeperNets, BadRL-M, and TrojDRL-W attacks on (Top) Highway Merge and (Bottom) Safety Car in terms of (Left) ASR and (Right) episodic return. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "We find that the SleeperNets attack outperforms TrojDRL-W and BadRL-M on all 6 environments in terms of attack success rate, being the only attack to achieve $100\\%$ attack success on all environments. The SleeperNets attack additionally outperforms TrojDRL-W in terms of episodic return on all environments. BadRL-M performs slightly better in terms of episodic return in Highway Merge and Car Racing, but in these cases it fails to achieve high attack success \u2013 resulting in $0.2\\%$ and $44.1\\%$ ASR in contrast to SleeperNets\u2019 $100\\%$ ASR on both environments. ", "page_idx": 7}, {"type": "text", "text": "In Figure 3 we plot the attack success rate and episodic return of the SleeperNets, BadRL-M, and TrojDRL-W attacks on Highway Merge and Safety Car. In both cases the SleeperNets attack is able to quickly achieve near $100\\%$ ASR while maintaining an indistinguishable episodic return curve compared to the \u201cNo Poisoning\u201d agent. In contrast BadRL-M and TrojDRL-W both stagnate in terms of ASR \u2013 being unable to achieve above $60\\%$ on Highway Merge or above $90\\%$ on Safety Car. Full numerical results and ablations with respect to $c$ can be found in Appendix 9.5. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "6.3 Attack Parameter Ablations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In Figure 4 we provide an ablation of the SleeperNets, BadRL-M, and TrojDRL-W attacks with respect to poisoning budget $\\beta$ and reward poisoning constant $c$ on the Highway Merge environment. For both experiments we use $\\alpha=0$ for SleeperNets, meaning the magnitude of reward perturbation is the same between all three attacks. This highlights the capabilities of the outer-loop threat model in enabling better attack performance with weaker attack parameters. ", "page_idx": 8}, {"type": "text", "text": "When comparing each attack at different poisoning budgets we see that SleeperNets is the only attack capable of achieving ${>}95\\%$ ASR with a poisoning budget of $0.25\\%$ and is the only attack able to achieve an ASR of $100\\%$ given any poisoning budget. TrojDRL-W is the only of the other two attacks able to achieve an ASR ${>}90\\%$ , but this comes at the cost of a $11\\%$ drop in episodic return at a poisoning budget of $2.5\\%-10\\up x$ the poisoning budget needed for SleeperNets to be successful. ", "page_idx": 8}, {"type": "image", "img_path": "HkC4OYee3Q/tmp/c538a61658bfd3e74c3d1cdec7cc79a791cdc89fbb6d2840f46fa843ecff5f11.jpg", "img_caption": ["Figure 4: (Top) Ablation with respect to poisoning budget $\\beta$ for each attack given a fixed $c=40$ . (Bottom) Ablation with respect to $c$ given a fixed poisoning budget of $0.5\\%$ . Both experiments were run on Highway Merge with a value of $\\alpha=0$ for SleeperNets. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "We see a similar trend for different values of $c$ - SleeperNets is the only attack which is able to achieve an ASR near $100\\%$ given a poisoning budget of $0.5\\%$ and $c=20$ . TrojDRL-W, in contrast, only achieves an ASR of $57\\%$ given the same poisoning budget and much larger reward constant of $c=40$ . This again comes at the cost of a $10\\%$ drop in episodic return. Between both ablations BadRL-M is unable to achieve an ASR greater than $40\\%$ and only achieves an ASR better than $1\\%$ when given a $5\\%$ poisoning budget and using $c=40$ . Numerical results can be found in Appendix 9.7. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusion and Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper we provided multiple key contributions to research in backdoor attacks against DRL. We have proven the insufficiency of static reward poisoning attacks and use this insight to motivate the development of a dynamic reward poisoning strategy with formal guarantees. We additionally introduced a novel backdoor threat model against DRL algorithms, and formulated the SleeperNets attack which significantly and reliably outperforms the current state-of-the-art in terms of attack success rate and episodic return at very low poisoning rates. The strong theoretical and empirical results of this work motivate further research into defense techniques against backdoor poisoning attacks in DRL such as new detection, certified robustness, or auditing approaches. ", "page_idx": 8}, {"type": "text", "text": "The main limitation of SleeperNets is that the formulation of $R^{\\prime}$ allows the attack\u2019s reward perturbation to grow arbitrarily large given an arbitrary MDP. When $\\alpha$ is large this may make detection of the attack easier if the victim is able to scan for outliers in the agent\u2019s reward. We believe this is an interesting area of future research that may lead to the development of increasingly sensitive detection techniques and, in response, more stealthy attacks. Additionally, while our experiments are expansive with respect to environments and most attack parameters, this paper is not an exhaustive study of all facets of backdoor attacks. We do not study the effectiveness of the adversary\u2019s chosen trigger pattern nor do we consider triggers optimized with gradient or random search based techniques [4]. Combining these techniques with SleeperNets is also an interesting area of future research which will likely result in a further decrease in poisoning budget and the ability to perform stealthier attacks. ", "page_idx": 9}, {"type": "text", "text": "8 Broader Impacts ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper we expose a key vulnerability of Deep Reinforcement Learning algorithms against backdoor attacks. As with any work studying adversarial attacks, there is the possibility that SleeperNets is used to launch a real-world, malicious attack against a DRL system. We hope that, by exploring and defining this threat vector, developers and researchers will be better equipped to design counter measures and prevent any negative outcomes. In particular we believe the implementation of isolated training systems \u2013 which will be harder for the adversary to access \u2013 and the development of active attack detection techniques will both be critical for mitigating attacks. We would also like to highlight the potential of malicious trainers using these backdoor attack approaches against their own agents for malicious purposes. We believe the development of new DRL auditing techniques is necessary to identify these compromised agents. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This Research was developed with funding from the Defense Advanced Research Projects Agency (DARPA), under contract W912CG23C0031, NSF award CNS-2312875, and NSF award FMitF2319500. We thank Evan Rose, Simona Boboila, Peter Chin, Lisa Oakley, and Aditya Vikram Singh for discussing various aspects of this project. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. OpenAI Gym. arXiv preprint arXiv:1606.01540, 2016.   \n[2] J. Chen and Q. Gu. Rays: A ray searching method for hard-label adversarial attack. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1739\u20131747, 2020.   \n[3] J. C. Costa, T. Roxo, H. Proen\u00e7a, and P. R. In\u00e1cio. How deep learning sees the world: A survey on adversarial attacks & defenses. arXiv preprint arXiv:2305.10862, 2023.   \n[4] J. Cui, Y. Han, Y. Ma, J. Jiao, and J. Zhang. Badrl: Sparse targeted backdoor attack against reinforcement learning. arXiv preprint arXiv:2312.12585, 2023.   \n[5] S. Dolan, S. Nayak, and H. Balakrishnan. Satellite navigation and coordination with limited information sharing. In Learning for Dynamics and Control Conference, pages 1058\u20131071. PMLR, 2023.   \n[6] T. Franzmeyer, S. McAleer, J. F. Henriques, J. N. Foerster, P. H. Torr, A. Bibi, and C. S. de Witt. Illusory attacks: Detectability matters in adversarial attacks on sequential decision-makers. arXiv preprint arXiv:2207.10170, 2022.   \n[7] A. Gleave, M. Dennis, C. Wild, N. Kant, S. Levine, and S. Russell. Adversarial policies: Attacking deep reinforcement learning. arXiv preprint arXiv:1905.10615, 2019.   \n[8] S. Gu, L. Yang, Y. Du, G. Chen, F. Walter, J. Wang, Y. Yang, and A. Knoll. A review of safe reinforcement learning: Methods, theory and applications. arXiv preprint arXiv:2205.10330, 2022.   \n[9] T. Gu, B. Dolan-Gavitt, and S. Garg. BadNets: Identifying vulnerabilities in the machine learning model supply chain. arXiv preprint arXiv:1708.06733, 2017.   \n[10] S. Huang, R. F. J. Dossa, C. Ye, J. Braga, D. Chakraborty, K. Mehta, and J. G. Ara\u00fajo. Cleanrl: High-quality single-flie implementations of deep reinforcement learning algorithms. Journal of Machine Learning Research, 23(274):1\u201318, 2022.   \n[11] C. D. Hylender, P. Langlois, A. Pinto, and S. Widup. Verizon 2024 data breach investigations report. https://www.verizon.com/business/resources/reports/dbir/, 2024.   \n[12] J. Ji, B. Zhang, X. Pan, J. Zhou, J. Dai, and Y. Yang. Safety-gymnasium. GitHub repository, 2023.   \n[13] T. Kabbani and E. Duman. Deep reinforcement learning approach for trading automation in the stock market. IEEE Access, 10:93564\u201393574, 2022.   \n[14] P. Kiourti, K. Wardega, S. Jha, and W. Li. Trojdrl: Trojan attacks on deep reinforcement learning agents. arXiv preprint arXiv:1903.06638, 2019.   \n[15] B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. Al Sallab, S. Yogamani, and P. P\u00e9rez. Deep reinforcement learning for autonomous driving: A survey. IEEE Transactions on Intelligent Transportation Systems, 23(6):4909\u20134926, 2021.   \n[16] I. Kostrikov, A. Nair, and S. Levine. Offline reinforcement learning with implicit q-learning. arXiv preprint arXiv:2110.06169, 2021.   \n[17] A. Krnjaic, R. D. Steleac, J. D. Thomas, G. Papoudakis, L. Sch\u00e4fer, A. W. K. To, K.-H. Lao, M. Cubuktepe, M. Haley, P. B\u00f6rsting, and S. V. Albrecht. Scalable multi-agent reinforcement learning for warehouse logistics with robotic and human co-workers, 2023.   \n[18] E. Leurent. An environment for autonomous driving decision-making. https://github.com/ eleurent/highway-env, 2018.   \n[19] Y.-C. Lin, Z.-W. Hong, Y.-H. Liao, M.-L. Shih, M.-Y. Liu, and M. Sun. Tactics of adversarial attack on deep reinforcement learning agents. arXiv preprint arXiv:1703.06748, 2017.   \n[20] C. Lu, T. Willi, A. Letcher, and J. N. Foerster. Adversarial cheap talk. In International Conference on Machine Learning, pages 22917\u201322941. PMLR, 2023.   \n[21] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.   \n[22] K. Mahmood, R. Mahmood, E. Rathbun, and M. van Dijk. Back in black: A comparative evaluation of recent state-of-the-art black-box attacks. IEEE Access, 10:998\u20131019, 2022.   \n[23] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pages 1928\u20131937. PMLR, 2016.   \n[24] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.   \n[25] L. Nasvytis, K. Sandbrink, J. Foerster, T. Franzmeyer, and C. S. de Witt. Rethinking outof-distribution detection for reinforcement learning: Advancing methods for evaluation and detection. arXiv preprint arXiv:2404.07099, 2024.   \n[26] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730\u201327744, 2022.   \n[27] C. Perroud. Gym trading env. https://github.com/ClementPerroud/Gym-Trading-Env, 2023.   \n[28] N. H. Pham, L. M. Nguyen, J. Chen, H. T. Lam, S. Das, and T.-W. Weng. Evaluating robustness of cooperative MARL: A model-based approach. arXiv preprint arXiv:2202.03558, 2022.   \n[29] A. Rakhsha, G. Radanovic, R. Devidze, X. Zhu, and A. Singla. Policy teaching via environment poisoning: Training-time adversarial attacks against reinforcement learning. In International Conference on Machine Learning, pages 7974\u20137984. PMLR, 2020.   \n[30] A. Rangi, H. Xu, L. Tran-Thanh, and M. Franceschetti. Understanding the limits of poisoning attacks in episodic reinforcement learning. arXiv preprint arXiv:2208.13663, 2022.   \n[31] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   \n[32] D. W. Stroock. An introduction to Markov processes, volume 230. Springer Science & Business Media, 2013.   \n[33] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. 2018.   \n[34] L. Wang, Z. Javed, X. Wu, W. Guo, X. Xing, and D. Song. BACKDOORL: Backdoor attack against competitive reinforcement learning. arXiv preprint arXiv:2105.00579, 2021.   \n[35] Z. Yang, N. Iyer, J. Reimann, and N. Virani. Design of intentional backdoors in sequential models. arXiv preprint arXiv:1902.09972, 2019.   \n[36] Y. Yu, J. Liu, S. Li, K. Huang, and X. Feng. A temporal-pattern backdoor attack to deep reinforcement learning. In GLOBECOM 2022-2022 IEEE Global Communications Conference, pages 2710\u20132715. IEEE, 2022.   \n[37] M. Zare, P. M. Kebria, A. Khosravi, and S. Nahavandi. A survey of imitation learning: Algorithms, recent developments, and challenges. IEEE Transactions on Cybernetics, 2024.   \n[38] H. Zheng, X. Li, J. Chen, J. Dong, Y. Zhang, and C. Lin. One4all: Manipulate one agent to poison the cooperative multi-agent reinforcement learning. Computers & Security, 124:103005, 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "9 Supplemental Material ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In the appendix we provide further details and experimental results which could not fti into the main body of the paper. In Appendix 9.1 we provide proofs for Lemma 1, Lemma 2, Theorem 1, and Theorem 2 as provided in Section 4.4. In Appendix 9.2 we prove the insufficiency of static reward poisoning as discussed in Section 4.1. In Appendix 9.3 we provide further discussion on the baselines evaluated in the main body. In Appendix 9.4 we provide further experimental setup details and justification for each hyper parameter we used. In Appendix 9.5 we provide full experimental details used to generate Table 2 in Section 6. In Appendix 9.6 we provide further details and plots relating SleeperNets\u2019 poisoning rate annealing capabilities as referenced in Section 6. In Appendix 9.7 we provide numerical results for our ablations of $c$ and $\\beta$ presented in Section 6.3. In Appendix 9.8 we provide an overview of the computational resources used in the experimental results of this paper. Lastly, in Appendix 9.9 we provide further discussion of related works on non-backdoor, adversarial attacks against DRL. ", "page_idx": 12}, {"type": "text", "text": "9.1 Proofs For Dynamic Reward Poisoning Guarantees ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Lemma 1 $V_{\\pi}^{M^{\\prime}}(s_{p})=\\pi(s_{p},a^{+})\\;\\forall s_{p}\\in S_{p},\\pi\\in\\Pi.$ Thus, the value of a policy \u03c0 in poisoned states $s_{p}\\in S_{p}$ is equal to the probability with which it chooses action $a^{+}$ . ", "page_idx": 12}, {"type": "text", "text": "Proof. Here we proceed with a direct algebraic proof \u2013 simplifying the definition of $V_{\\pi}^{M^{\\prime}}(s_{p})\\;\\forall s_{p}\\in$ $S_{p}$ until we show it is equal to $\\pi(s_{p},a^{+})$ . ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\bf~Let:}s_{p}\\in S_{p},\\pi\\in\\mathrm{II}}\\\\ &{V_{\\pi}^{M^{\\prime}}(s_{p})=\\displaystyle\\sum_{a\\in A}\\pi(s_{p},a)\\sum_{s^{\\prime}\\in S(3S_{p})}T^{\\prime}(s_{p},a,s^{\\prime})[R^{\\prime}(s_{p},a,s^{\\prime},\\pi)+\\gamma V_{\\pi}^{M^{\\prime}}(s^{\\prime})]}\\\\ &{\\quad\\quad\\quad=\\displaystyle\\sum_{a\\in A}\\pi(s_{p},a)\\sum_{s^{\\prime}\\in S(3S_{p})}T^{\\prime}(s_{p},a,s^{\\prime})[\\mathbb{I}[a=a^{+}]-\\gamma V_{\\pi}^{M^{\\prime}}(s^{\\prime})+\\gamma V_{\\pi}^{M^{\\prime}}(s^{\\prime})]}\\\\ &{\\quad\\quad\\quad=\\displaystyle\\sum_{a\\in A}\\pi(s_{p},a)(\\mathbb{I}[a=a^{+}]\\sum_{s^{\\prime}\\in S(3S_{p})}T^{\\prime}(s_{p},a,s^{\\prime}))}\\\\ &{\\quad\\quad\\quad=\\displaystyle\\sum_{a\\in A}\\pi(s_{p},a)(\\mathbb{I}[a=a^{+}])}\\\\ &{\\quad\\quad\\quad=\\alpha(s_{p},a)(\\mathbb{I}[a=a^{+}])}\\\\ &{\\quad\\quad\\quad=\\pi(s_{p},a^{+})}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof. In this proof we will show that $V_{\\pi}^{M}(s)-V_{\\pi}^{M^{\\prime}}(s)\\neq0$ leads to a contradiction $\\forall\\;s\\in S$ , thus $V_{\\pi}^{M}(s)=V_{\\pi}^{M^{\\prime}}(s)$ must be true. We will begin by expanding and subsequently simplifying $V_{\\pi}^{M^{\\prime}}$ . ", "page_idx": 13}, {"type": "text", "text": "Let: $s\\in S,\\pi\\in\\Pi$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{V_{\\pi}^{M^{\\prime}}(s)=\\displaystyle\\sum_{\\alpha\\in A}\\pi(s,a)\\displaystyle\\sum_{\\alpha^{\\prime}\\in S^{\\alpha}(\\pi S)_{D_{\\alpha}}}T^{\\prime}(s,a,s^{\\prime})[F^{\\prime}(s,a,s^{\\prime},\\pi)+\\gamma V_{\\pi}^{M^{\\prime}}(s^{\\prime})]}&{\\quad{\\scriptstyle(\\mathrm{IS})}}\\\\ {=\\displaystyle\\sum_{\\alpha\\in A}\\pi(s,a)\\displaystyle[\\displaystyle\\sum_{\\alpha^{\\prime}\\in S^{\\alpha}(\\pi S)_{D}}T^{\\prime}(s,a,s^{\\prime})[F^{\\prime}(s,a,s^{\\prime},\\pi)+\\gamma V_{\\pi}^{M^{\\prime}}(s^{\\prime})]}&{}\\\\ {+\\displaystyle\\sum_{\\alpha\\in A}\\gamma T^{\\prime}(s,a,s^{\\prime})[{\\alpha(s,a,s^{\\prime},\\pi)+\\gamma V_{\\pi}^{M^{\\prime}}(s^{\\prime})}]}&{\\quad{\\scriptstyle(\\mathrm{IS})}}\\\\ {+\\displaystyle\\sum_{\\alpha\\in S_{D}}T^{\\prime}(s,a,s^{\\prime})[{\\alpha(s,a,s^{\\prime},\\pi)+\\gamma V_{\\pi}^{M^{\\prime}}(s^{\\prime})}]}&{}\\\\ {=\\displaystyle\\sum_{\\alpha\\in A}\\pi(s,a)[(1-\\beta)\\displaystyle\\sum_{\\alpha^{\\prime}\\in S^{\\alpha}(\\pi S)}T(s,a,s^{\\prime})[F(s,a,s^{\\prime})+\\gamma V_{\\pi}^{M^{\\prime}}(s^{\\prime})]}&{}\\\\ {+\\displaystyle\\beta\\displaystyle\\sum_{\\alpha^{\\prime}\\in S_{D}}T(s,a,s^{\\prime}-(1))[R(s,a,s^{-1}(s^{\\prime})-\\gamma V_{\\pi}^{M^{\\prime}}(s^{\\prime})+\\gamma V_{\\pi}^{M^{\\prime}}(s^{\\prime})+\\gamma V_{\\pi}^{M^{\\prime}}(s^{\\prime})]}&{}\\\\ {+\\displaystyle\\sum_{\\alpha\\in A}\\pi(s,a)[(1-\\beta)\\displaystyle\\sum_{\\alpha^{\\prime}\\in S^{\\alpha}(\\pi S)}T(s,a,s^{\\prime})+\\gamma V_{\\pi}^{M^{\\prime}}(s^{\\prime})]}&{}\\\\ {+\\displaystyle\\beta\\displaystyle\\sum_{\\alpha\\in A}\\pi(s,a)[(1-\\beta)\\displaystyle\\sum_{\\alpha^{\\prime}\\in S^{\\alpha}(\\pi S\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "To get to the next step in this simplification we observe that the second sum is over $s^{\\prime}\\in S_{p}$ , but only includes terms $\\delta^{-1}{\\left(s^{\\prime}\\right)}$ . $\\delta$ is an invertible function and is thus bijective \u2013 as explained in Section $4.2-$ therefore the sum is equivalent to one over $s^{\\prime}\\in S$ . ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{\\pi}^{M^{\\prime}}(s)=\\displaystyle\\sum_{a\\in A}\\pi(s,a)[(1-\\beta)\\displaystyle\\sum_{s^{\\prime}\\in S}T(s,a,s^{\\prime})[R(s,a,s^{\\prime})+\\gamma V_{\\pi}^{M^{\\prime}}(s^{\\prime})]}\\\\ &{\\qquad+\\displaystyle\\beta\\sum_{s^{\\prime}\\in S}T(s,a,s^{\\prime})[R(s,a,s^{\\prime})+\\gamma V_{\\pi}^{M^{\\prime}}(s^{\\prime})]]}\\\\ &{\\qquad=\\displaystyle\\sum_{a\\in A}\\pi(s,a)\\sum_{s^{\\prime}\\in S}T(s,a,s^{\\prime})[R(s,a,s^{\\prime})+\\gamma V_{\\pi}^{M^{\\prime}}(s^{\\prime})]}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "From here we are nearly done, but we still have to connect it back to our goal of showing $V_{\\pi}^{M}(s)=$ $V_{\\pi}^{M^{\\prime}}(s)$ , in other words: $^{\\prime}s\\in S,D_{s}\\doteq V_{\\pi}^{M^{\\prime}}(s)-V_{\\pi}^{M}(s)=0$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{D_{s}=\\underset{\\omega\\in A}{\\sum}\\pi(s,a)\\underset{s^{\\prime}\\in B}{\\sum}T(s,a,s^{\\prime})[R(s,a,s^{\\prime})+\\gamma V_{\\pi}^{M^{\\prime}}(s^{\\prime})]}\\\\ &{\\quad-\\underset{a\\in A}{\\sum}\\pi(s,a)\\underset{s^{\\prime}\\in B}{\\sum}T(s,a,s^{\\prime})[R(s,a,s^{\\prime})+\\gamma V_{\\pi}^{M}(s^{\\prime})]}\\\\ &{\\quad=\\underset{a\\in A}{\\sum}\\pi(s,a)[\\underset{s^{\\prime}\\in B}{\\sum}T(s,a,s^{\\prime})[R(s,a,s^{\\prime})+\\gamma V_{\\pi}^{M^{\\prime}}(s^{\\prime})]}\\\\ &{\\quad=\\underset{a\\in A}{\\sum}\\pi(s,a,s^{\\prime})[R(s,a,s^{\\prime})+\\gamma V_{\\pi}^{M^{\\prime}}(s^{\\prime})]}\\\\ &{\\quad-\\underset{s^{\\prime}\\in B}{\\sum}T(s,a,s^{\\prime})[R(s,a,s^{\\prime})+\\gamma V_{\\pi}^{M^{\\prime}}(s^{\\prime})]}\\\\ &{\\quad=\\underset{a\\in A}{\\sum}\\pi(s,a)[\\underset{s^{\\prime}\\in B}{\\sum}T(s,a,s^{\\prime})[[R(s,a,s^{\\prime})+\\gamma V_{\\pi}^{M^{\\prime}}(s^{\\prime})]}\\\\ &{\\quad=\\underset{a\\in A}{\\sum}\\pi(s,a,s^{\\prime})+\\gamma V_{\\pi}^{M}(s^{\\prime})]]}\\\\ &{\\quad=\\underset{a\\in A}{\\sum}\\pi(s,a)[\\underset{s\\in B}{\\sum}T(s,a,s^{\\prime})[\\gamma]\\gamma_{\\pi}^{M^{\\prime}}(s^{\\prime})-\\gamma V_{\\pi}^{M}(s^{\\prime})]]}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "After this we will convert the problem into the much more manageable matrix form: ", "page_idx": 13}, {"type": "text", "text": "We know that $\\mathcal{P}$ is a Markovian matrix through its definition \u2013 every row $\\mathcal{P}_{s}$ represents a probability vector over next states $s^{\\prime}$ given initial state $s-$ thus every row sums to a value of 1. Markovian matricies have many useful properties which are relevant to Reinforcement Learning, but most relevant to us are its properties regarding eigenvalues: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal P D=\\alpha\\mathcal D\\Rightarrow\\alpha\\leq1\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In other words, the largest eigenvalue of a valid Markovian matrix $\\mathcal{P}$ is 1 [32]. Using our above definitions we can rewrite Equation (27) as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\mathcal{D}=\\mathcal{P}(\\gamma\\mathcal{D})}\\\\ {\\Rightarrow\\displaystyle\\frac{1}{\\gamma}\\mathcal{D}=\\mathcal{P}\\mathcal{D}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Let\u2019s assume, for the purpose of contradiction that $\\mathcal{D}\\neq\\hat{0}$ ", "page_idx": 14}, {"type": "text", "text": "Since $\\gamma\\in[0,1)$ this implies $\\mathcal{P}$ has an eigenvalue larger than 1. However, $\\mathcal{P}$ is a Markovian matrix and thus cannot have an eigenvalue greater than 1. Thus $\\mathcal{D}=\\hat{0}$ must be true. QED ", "page_idx": 14}, {"type": "text", "text": "Finite Horizon MDP Case ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. In the finite horizon case we redefine the benign MDP as $M=(S,A,T,R,H,\\gamma)$ and the poisoned MDP as $M^{\\prime}=(S\\cup S_{p},A,T^{\\prime},R^{\\prime},H,\\gamma)$ where $H\\in\\mathbb{N}$ is the horizon of the MDPs: ", "page_idx": 14}, {"type": "equation", "text": "$$\nV_{\\pi,t}^{M}(s)=0\\ \\,\\forall\\pi\\in\\Pi,s\\in S\\cup S_{p}\\quad{\\mathrm{if~}}t>H\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Using this we will proceed with a proof by induction, starting with the base case $V_{\\pi,H}^{M}(s)\\;=\\;$ $V_{\\pi,H}^{M^{\\prime}}(s)\\mathrel{\\phantom{=}}\\forall s\\in S$ . This can quickly be derived by drawing upon the simplification for V \u03c0M, H(s) found in Equation (23). ", "page_idx": 14}, {"type": "equation", "text": "$$\nV_{\\pi,H}^{M^{\\prime}}(s)=\\sum_{a\\in A}\\pi(s,a)\\sum_{s^{\\prime}\\in S}T(s,a,s^{\\prime})R(s,a,s^{\\prime})=V_{\\pi,H}^{M}(s)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Here the term $\\gamma V_{\\pi,H+1}^{M^{\\prime}}(s^{\\prime})$ disappears due to our definition in Equation (33). Thus we have proven the condition is met $\\forall s\\in S$ for the base case . We will then continue to the inductive case: assume the condition holds for some $t+1\\leq H$ , we can then show the following: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{V_{\\pi,t}^{M^{\\prime}}(s)=\\sum_{a\\in A}\\pi(s,a)\\sum_{s^{\\prime}\\in S}T(s,a,s^{\\prime})R(s,a,s^{\\prime})+\\gamma V_{\\pi,t+1}^{M^{\\prime}}(s)}}\\\\ &{}&{\\quad=\\displaystyle\\sum_{a\\in A}\\pi(s,a)\\sum_{s^{\\prime}\\in S}T(s,a,s^{\\prime})R(s,a,s^{\\prime})+\\gamma V_{\\pi,t+1}^{M}(s)}\\\\ &{}&{\\quad=V_{\\pi,t}^{M}(s)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus the inductive hypothesis and base case hold, therefore $V_{\\pi,t}^{M}(s)\\,=\\,V_{\\pi,t}^{M^{\\prime}}(s)$ $\\forall s\\;\\in\\;S,h\\;\\leq$ $H$ . QED ", "page_idx": 14}, {"type": "text", "text": "Theorem 1 $V_{\\pi^{*}}^{M^{\\prime}}(s)\\,\\geq\\,V_{\\pi}^{M^{\\prime}}(s_{p})\\,\\,\\forall s_{p}\\,\\in\\,S_{p},\\pi\\,\\in\\,\\Pi\\,\\Leftrightarrow\\,\\pi^{*}(s_{p},a^{+})\\,=\\,1\\,\\,\\forall s_{p}\\,\\in\\,S_{p}$ . Thus, $\\pi^{*}$ is optimal in $M^{\\prime}$ if and only $i f\\pi^{*}$ takes action $a^{+}$ with probability 1 in poisoned states $\\bar{s}_{p}\\in S_{p}$ . ", "page_idx": 14}, {"type": "text", "text": "Forward Direction $V_{\\pi^{*}}^{M^{\\prime}}(s)\\geq V_{\\pi}^{M^{\\prime}}(s)\\,\\,\\forall s\\in S_{p},\\pi\\in\\Pi\\Rightarrow\\pi^{*}(s,a^{+})=1\\,\\,\\forall s\\in S_{p}$ ", "page_idx": 14}, {"type": "text", "text": "Proof. Assume for the purpose of contradiction that $\\exists\\pi\\,\\in\\,\\Pi$ such that $\\pi$ is optimal in $M^{\\prime}$ but $\\pi(s_{p},a^{+})<1$ for some $s_{p}\\in S_{p}$ . ", "page_idx": 14}, {"type": "text", "text": "Let $\\pi^{\\prime}\\in\\Pi$ be some arbitrary policy such that $\\pi^{\\prime}(s_{p},a^{+})=1$ . ", "page_idx": 14}, {"type": "text", "text": "From Lemma 1 we know that $V_{\\pi^{\\prime}}^{M^{\\prime}}(s_{p})=1$ and $V_{\\pi}^{M^{\\prime}}(s_{p})<1$ . ", "page_idx": 14}, {"type": "text", "text": "Thus it follows that $V_{\\pi}^{M^{\\prime}}(s_{p})<V_{\\pi^{\\prime}}^{M^{\\prime}}(s_{p})$ which contradicts the initial assumption that $\\pi$ is optimal in $M^{\\prime}$ . QED ", "page_idx": 14}, {"type": "text", "text": "Reverse Direction $\\pi^{*}(s,a^{+})=1\\,\\forall s\\in S_{p}\\Rightarrow V_{\\pi^{*}}^{M^{\\prime}}(s)\\geq V_{\\pi}^{M^{\\prime}}(s)\\,\\forall s\\in S_{p},\\pi\\in\\Pi$ ", "page_idx": 15}, {"type": "text", "text": "Proof. Assume for the purpose of contradiction that $\\pi^{*}(s,a^{+})=1\\;\\forall s\\in S_{p}$ but $\\exists\\pi\\in\\Pi$ , $s^{*}\\in S_{p}$ such that $V_{\\pi^{*}}^{M^{\\prime}}(s^{*})<V_{\\pi}^{M^{\\prime}}(s^{*})$ . From Lemma 1 we know that $V_{\\pi^{*}}^{M^{\\prime}}(s)=1\\forall s\\in S_{p}$ , therefore $V_{\\pi}^{M^{\\prime}}(s^{*})>1$ . Again using Lemma 1 this implies that $\\pi(s^{*},a^{+})>1$ which contradicts the fact that $\\pi(s^{*},\\cdot)$ is a valid probability vector \u2013 no action can have probability greater than 1. QED ", "page_idx": 15}, {"type": "text", "text": "Thus we have proven Theorem 1. QED ", "page_idx": 15}, {"type": "text", "text": "Theorem 2 $V_{\\pi^{*}}^{M^{\\prime}}(s)\\geq V_{\\pi}^{M^{\\prime}}(s)\\,\\forall s\\in S,\\pi\\in\\Pi\\Leftrightarrow V_{\\pi^{*}}^{M}(s)\\geq V_{\\pi}^{M}(s)\\,\\forall s\\in S,\\pi\\in\\Pi.$ . Therefore, $\\pi^{*}$ is optimal in $M^{\\prime}$ for all benign states $s\\in S$ if and only if $\\pi^{*}$ is optimal in $M$ . ", "page_idx": 15}, {"type": "text", "text": "Forward Direction $V_{\\pi^{*}}^{M^{\\prime}}(s)\\geq V_{\\pi}^{M^{\\prime}}(s)\\,\\forall s\\in S,\\pi\\in\\Pi\\Rightarrow V_{\\pi^{*}}^{M}(s)\\geq V_{\\pi}^{M}(s)\\,\\forall s\\in S,\\pi\\in\\Pi$ ", "page_idx": 15}, {"type": "text", "text": "Proof. Assume for the purpose of contradiction that $\\pi^{*}$ is optimal in $M^{\\prime}$ but is sub optimal in $M$ .   \nThis implies there is some $\\pi\\in\\Pi$ and $s\\in S$ such that $V_{\\pi}^{M}(s)>V_{\\pi^{*}}^{M}(s).$ . ", "page_idx": 15}, {"type": "text", "text": "From Lemma 2 we know $V_{\\pi}^{M}(s)=V_{\\pi}^{M^{\\prime}}(s)>V_{\\pi^{*}}^{M}(s)=V_{\\pi^{*}}^{M^{\\prime}}(s)$ ", "page_idx": 15}, {"type": "text", "text": "This implies $V_{\\pi}^{M^{\\prime}}(s)>V_{\\pi^{*}}^{M^{\\prime}}(s)$ which contradicts the initial assumption that $\\pi^{*}$ is optimal in $M^{\\prime}$ for all $s\\in S$ . QED ", "page_idx": 15}, {"type": "text", "text": "Reverse Direction $\\begin{array}{r}{\\colon V_{\\pi^{*}}^{M}(s)\\geq V_{\\pi}^{M}(s)\\,\\forall s\\in S,\\pi\\in\\Pi\\Rightarrow V_{\\pi^{*}}^{M^{\\prime}}(s)\\geq V_{\\pi}^{M^{\\prime}}(s)\\,\\forall s\\in S,\\pi\\in\\Pi}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "Proof. Assume for the purpose of contradiction that $\\pi^{*}$ is optimal in $M$ but is sub optimal in $M^{\\prime}$ . ", "page_idx": 15}, {"type": "text", "text": "This implies there is some $\\pi\\in\\Pi$ and $s\\in S$ such that $V_{\\pi}^{M^{\\prime}}(s)>V_{\\pi^{*}}^{M^{\\prime}}(s).$ . ", "page_idx": 15}, {"type": "text", "text": "From Lemma 2 we know $V_{\\pi}^{M^{\\prime}}(s)=V_{\\pi}^{M}(s)>V_{\\pi^{*}}^{M^{\\prime}}(s)=V_{\\pi^{*}}^{M}(s)$ ", "page_idx": 15}, {"type": "text", "text": "This implies $V_{\\pi}^{M}(s)>V_{\\pi^{*}}^{M}(s)$ which contradicts the initial assumption that $\\pi^{*}$ is optimal in $M$ for all $s\\in S$ . QED ", "page_idx": 15}, {"type": "text", "text": "Thus we have proven Theorem 2. QED ", "page_idx": 15}, {"type": "text", "text": "9.2 Proofs For Insufficiency of Static Reward Poisoning ", "text_level": 1, "page_idx": 15}, {"type": "image", "img_path": "HkC4OYee3Q/tmp/027ce36e0a277914c600ba5c4e8c908d6ade1c736b4ba7bcfc32ac5dadd65858.jpg", "img_caption": ["Figure 5: (Left) MDP $M_{1}$ , (Right) MDP $M_{2}$ "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Static Reward Poisoning is Insufficient for Attack Success ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. Our goal here is to show that for all static reward poisoning attacks using a poisoning rate $\\beta\\,\\in\\,[0,1)$ and a reward constant $c$ , there exists an MDP $M_{1}$ such that attack success cannot be achieved. In other words, we want to show the following to be true $Q(\\delta(\\mathrm{Start}),a_{1})>Q(\\delta(\\mathrm{Start}),a^{+})$ given the poisoned MDP $M_{1}^{\\prime}$ using static reward poisoning with constant $c$ and a poisoning rate of $\\beta$ . ", "page_idx": 15}, {"type": "text", "text": "Here we assume the agent always takes action $a^{+}$ in state \u03b4(Survive) as it is the optimal action. We proceed as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\nQ(\\delta(\\mathrm{Start}),a_{1})=-c+\\sum_{i=0}^{\\infty}c\\gamma^{i}=\\frac{c}{1-\\gamma}-c\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "From here we want to show that $\\exists\\gamma\\in[0,1)$ such that $Q(\\delta(\\mathrm{Start}),a_{1})>Q(\\delta(\\mathrm{Start}),a^{+})$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{Q(\\delta(\\mathrm{{Start}}),a_{1})>Q(\\delta(\\mathrm{{Start}}),a^{+})\\Rightarrow\\displaystyle\\frac{c}{1-\\gamma}-c>c}}\\\\ {{\\Rightarrow\\displaystyle\\frac{1}{1-\\gamma}>2\\Rightarrow\\gamma>\\frac{1}{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, we know that there exists some $\\gamma\\in(\\textstyle{\\frac{1}{2}},1)$ such that $Q(\\delta(\\mathrm{Start}),a_{1})>Q(\\delta(\\mathrm{Start}),a^{+})$ . Thus the target action is sub-optimal in the adversarial MDP using static reward poisoning. Therefore static reward poisoning is insufficient for attack success in arbitrary MDPs. QED ", "page_idx": 16}, {"type": "text", "text": "Static Reward Poisoning is Insufficient for Attack Stealth ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. Our goal here is to show that for all static reward poisoning attacks using a poisoning rate $\\beta\\,\\in\\,[0,1)$ and a reward constant $c$ , there exists an MDP $M_{2}$ such that attack stealth cannot be achieved. In other words, we want to show that the agent learns a sub-optimal policy in the benign MDP $M_{2}$ given it is trained in the poisoned MDP $M_{2}^{\\prime}$ . Specifically, we want to show the following to be true: $Q(\\mathrm{Start},a_{1})>Q(\\mathrm{Start},a^{+})$ in $M_{2}^{\\prime}$ \u2013 meaning $a_{1}$ is the optimal action in $M_{2}^{\\prime}$ given state \u201cStart\u201d whereas $a^{+}$ is optimal in $M_{2}$ . Similarly to the last proof we assume the agent chooses the target action $a^{+}$ in poisoned states \u2013 since it\u2019s the optimal action \u2013 and thus receives a reward of $+c$ in all states with probability $\\beta$ . We proceed as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q(\\operatorname{Start},a_{1})=\\gamma\\beta c+\\gamma^{2}\\beta c+\\gamma^{3}c}\\\\ &{Q(\\operatorname{Start},a^{+})=\\gamma\\beta c+\\gamma^{2}c}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "From here we want to show that $\\exists\\gamma\\in[0,1)$ such that $Q(\\mathrm{Start},a_{1})>Q(\\mathrm{Start},a^{+})$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q(\\mathrm{Start},a_{1})>Q(\\mathrm{Start},a^{+})\\Rightarrow\\gamma\\beta c+\\gamma^{2}\\beta c+\\gamma^{3}c>\\gamma\\beta c+\\gamma^{2}c}\\\\ &{\\Rightarrow\\gamma^{2}(\\beta-1)+\\gamma^{3}>0\\Rightarrow\\gamma>1-\\beta}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, we know that there exists some $\\gamma\\in(1-\\beta,1)$ such that $Q(\\mathrm{Start},a_{1})>Q(\\mathrm{Start},a^{+})$ . Thus $a_{1}$ is optimal action in $M_{2}^{\\prime}$ given state \u201cStart\u201d, whereas $a^{+}$ is optimal in $M_{2}$ . Therefore the agent will learn a sub-optimal policy in $M_{2}$ when trained on $M_{2}^{\\prime}$ . QED ", "page_idx": 16}, {"type": "text", "text": "9.3 Further Justification of Baselines Used ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In the TrojDRL paper [14] two main attack techniques are proposed \u2013 TrojDRL-S and TrojDRL-W \u2013 which correspond to attacks where the adversary can and cannot manipulate the agent\u2019s actions during each episode respectively. The core algorithm behind both of these attacks is fundamentally the same however, as the adversary manipulates the agent\u2019s state observations and reward in the same way. As discussed in Section 2 we believe action manipulation attacks are much easier to detect and offer no theoretical beneftis to the attacker. Thus we compare against TrojDRL-W so we can have a more direct comparison between the dynamic reward poisoning of SleeperNets and the static reward poisoning of TrojDRL. ", "page_idx": 16}, {"type": "text", "text": "In BadRL [4] the authors propose a technique for optimizing the adversary\u2019s trigger pattern. They show that this technique can lead to improvements in attack success rate over TrojDRL in Atari domains given low poisoning budgets. This approach comes with a very high cost in terms of threat model however. Not only must the adversary have full access to the MDP to train the attack\u2019s QNetwork, but they must also have gradient access to the agent\u2019s policy network to optimize the trigger pattern. Furthermore, the technique is not exclusive or dependant on any other piece of BadRL\u2019s formulation, it can be generically applied to any backdoor attack. Taking this all into consideration, we believe it is most fair to compare all methods using the same, fixed trigger pattern, hence the usage of BadRL-M. We could otherwise compare all techniques given optimized trigger patterns, but that would go against our paper\u2019s motive of developing a theoretically sound backdoor attack that can be effective within a restrictive threat model. Thus we show that the SleeperNets attack is effective even when using simple, hand-crafted trigger patterns. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "table", "img_path": "HkC4OYee3Q/tmp/27ef1a064d191d2ce5063dd154f118bdb4218f39e23ad169427449c4fee7302b.jpg", "table_caption": ["9.4 Experimental Setup Details "], "table_footnote": ["Table 3: Comparison of the different environments tested in this work. All action spaces were discrete in some form, though for environments like Car Racing, Safety Car, and Trading-Env discretized versions of their continuous action spaces were used. The final column refers to the exact environment Id used when generating each environment through the gymnasium interface [1]. "], "page_idx": 17}, {"type": "text", "text": "In Table 3 we compare each environment in terms of task type and observation type. We additionally provide the exact environment Id. used when generating each environment through the gymnasium interface [1]. Breakout, $\\mathrm{Q^{*}}$ bert, Car Racing, and Highway Merge all use gray scale 86x86 image observations stacked into 4 frames. Safety car uses 16 lidar sensors placed in a circular pattern around the agent to allow the agent to spatially orient itself with respect to the goal and obstacles. It additionally uses proprioceptive information about the agent (agent\u2019s velocity, angle, etc.). Lastly, Trade BTC uses information corresponding to the volume, closing price, opening price, high price, and low price of BTC on a per-day basis with respect to USDT (equivalent to $\\mathbb{S}1$ ). ", "page_idx": 17}, {"type": "table", "img_path": "HkC4OYee3Q/tmp/14b1d66919026c7d6867d31fee7a3b47638a429e7b5c890d541b268038bf1f14.jpg", "table_caption": [], "table_footnote": ["Table 4: Attack and learning parameters used for each environment. $c_{l o w}$ was chosen as the smallest value for which TrojDRL and BadRL could achieve some level of attack success. $c_{h i g h}$ was chosen as the largest value for which TrojDRL and BadRL did not significantly damage the agent\u2019s benign return. A similar method was used in determining the poisoning budget. "], "page_idx": 17}, {"type": "text", "text": "In Table 4 we summarize the trigger pattern, poisoning budget, target action, and values of $c_{l o w}$ and $c_{h i g h}$ used in each environment. As explained in the caption, values of $c_{l o w}$ , $c_{h i g h}$ , and poisoning budget were chosen as appropriate for each environment. Specifically we chose values which laid on the boundaries of performance for both TrojDRL and BadRL \u2013 values of $c$ higher than $c_{h i g h}$ would begin to damage episodic return and values of $c$ lower than $c_{l o w}$ would see a significant drop off in attack success rate. Poisoning budgets were chosen similarly, as lower values would result in significantly degraded attack success rates for all three attacks, though SleeperNets usually fared best at lower poisoning budgets. ", "page_idx": 17}, {"type": "text", "text": "For all image based environments a 6x6, checkerboard pattern was placed at the top of each image frame as the trigger pattern. For both inner and outer loop attacks the trigger was applied to all images in the frame stack. In the case of Safety Car we embed the trigger pattern into the agent\u2019s lidar observation. In particular, the agent has 16 lidar sensors corresponding to its proximity to \u201cvase\u201d objects. We set 4 of these signals to have a value of 1, corresponding to 4 vases placed directly in front of, behind, to the left, and to the right of the agent. This type of observation cannot occur in ", "page_idx": 17}, {"type": "text", "text": "SafetyCarGoal1-v0 as there is only 1 vase object in the environment. Lastly, in Trade BTC we simply added an additional, binary feature to the agent\u2019s observation which is set to 1 in poisoned states and 0 otherwise. This choice was primarily made to ensure the trigger is truly out of distribution while also not ruining the state observation received by the agent. If any relevant piece of information (e.g. opening price, volume, etc.) is altered, one could argue that the target action $a^{+}$ is actually optimal. This would make the attack more of an adversarial example than a backdoor. ", "page_idx": 18}, {"type": "text", "text": "Target actions were chosen as actions which could be cause the largest potential damage if taken in the wrong state. In the case of Breakout and $\\mathrm{Q^{*}}$ bert \u201cMove Right\u201d is chosen as all movement actions are equally likely to be harmful. In Car Racing \u201cTurn Right\u201d was chosen as it is extremely easy for the agent\u2019s car to spin out if they take a wrong turn, particularly when rounding a leftward corner. In Highway Merge \u201cMerge Right\u201d was chosen as the goal of this environment is to merge left, getting out of the way of a vehicle entering the highway. Merging right at the wrong time could easily result in a collision, we believe this is why this environment was the hardest to solve for the inner loop attacks. Lastly in Trading BTC we chose \u201cShort BTC $100\\%^{\\circ}$ as the target action since this environment uses data gathered from the Bitcoin boom between 2020-2024, making an $100\\%$ short of Bitcoin potentially highly costly. In spite of this, most attacks were still able to be successful in this environment. We believe this is because the agent is able to immediately correct the mistake the following day, only running the risk of a single day\u2019s price change. ", "page_idx": 18}, {"type": "text", "text": "9.5 Detailed Experimental Results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section we provide numerical experimental results for those presented in Section 6.2. In Table 5 we present the numerical results we gathered on TrojDRL-W and BadRL-M including further ablations with respect to $c$ which were not directly presented in the paper. All results placed in bold represent those presented as the best result for each attack in the main body of the paper. Additionally, standard deviation values are provided for each result in its neighboring column. ", "page_idx": 18}, {"type": "text", "text": "In Table 6 we give full numerical results for the SleeperNets attack on our 6 chosen environments. Here we evaluated the attack with the same values of $c_{l o w}$ and $c_{h i g h}$ as we did for TrojDRL and BadRL. The only exception is Highway Merge which uses $c_{l o w}=5$ and $c_{h i g h}=10$ respectively. This decision was made because the attack was successful at $c_{l o w}=30$ and $c_{h i g h}=40$ given $\\alpha=0$ , thus further tests of $\\alpha$ were redundant. Instead we decided to test the limits of the attack be evaluating at very low values of $c$ . ", "page_idx": 18}, {"type": "text", "text": "We also evaluate SleeperNets given three values of $\\alpha\\colon0,0.5$ , and 1. Our results show that SleeperNets is generally very stable with respect to $\\alpha$ and $c$ , however for some of the more difficult environments, like Highway Merge, larger parameter values are used. Lastly, we present plots of each attack\u2019s best results with respect to episodic return and attack success rate in Figure 6 through Figure 11. ", "page_idx": 18}, {"type": "table", "img_path": "HkC4OYee3Q/tmp/3c9b067eaef8b5d01069c3c1721b2a2c9a656898603c1509d496bf3c235efd15.jpg", "table_caption": [], "table_footnote": ["Table 5: Full Experimental results of TrojDRL-W and BadRL-M on our 6 environments given $c_{l o w}$ and $c_{h i g h}$ . For each attack we bold the results which were used in the main body of the paper. In all cases but one this was $c_{h i g h}$ . Standard deviations $\\sigma$ for each experiment are also given. "], "page_idx": 18}, {"type": "table", "img_path": "HkC4OYee3Q/tmp/c43513ca40c465eb2212a30a37ea1e444b7cbbecf6c426dadec2a8a0ed301d6d.jpg", "table_caption": [], "table_footnote": ["Table 6: Tabular ablation of SleeperNets with respect to $\\alpha$ and $c$ . All results are presented with standard deviations in their neighboring column. Results placed in bold represent those presented in the main body of the paper. All results use the same values of $c_{l o w}$ and $c_{h i g h}$ seen in Table 4 excluding Highway Merge which uses values of 5 and 10 respectively. "], "page_idx": 19}, {"type": "image", "img_path": "HkC4OYee3Q/tmp/9b369add417a402ce494274bdd715deab1e8695f0ebd019a7f530fe5338fccf9.jpg", "img_caption": ["Figure 6: Best results for each attack on Breakout. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "HkC4OYee3Q/tmp/8122bfd1fd26093c1ee20a5cb84dae1f17b614a36d22cb67c793116d4ec867d6.jpg", "img_caption": ["Figure 7: Best results for each attack on $\\mathrm{Q^{*}}$ bert. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "HkC4OYee3Q/tmp/e920a93ad40267dc6d5e249b82f0f5c426775d71e7339f6e4e557d9e97895d74.jpg", "img_caption": ["Figure 8: Best results for each attack on Car Racing. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "HkC4OYee3Q/tmp/e194edcd900f2b04b6ef2c7526647885efc31977a4b38b0c959ab641ae0bba10.jpg", "img_caption": ["Figure 9: Best results for each attack on Highway Merge. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "HkC4OYee3Q/tmp/e3fd4a9a9d12ca0e05aefc1f7ca95edc893b872d841e9479fb32eba676bf2873.jpg", "img_caption": ["Figure 10: Best results for each attack on Safety Car. "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "HkC4OYee3Q/tmp/f65bf11a910c93f4a6e1f637057b980fc46a16f613b0833043f1ed5f14f1a787.jpg", "img_caption": ["Figure 11: Best results for each attack on Trade BTC. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "9.6 Poisoning Rate Annealing ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section we explore the capabilities of SleeperNets to anneal its poisoning rate on many environments. In particular we implement poisoning rate annealing as follows \u2013 if the adversary\u2019s ASR is currently $100\\%$ , skip poisoning the current time step/trajectory. This method is applied to all three attack including TrojDRL and BadRL. SleeperNets is able to utilize our poisoning rate annealing the most, however, as it regularly achieves $100\\%$ ASR early in training. In Figure 12, Figure 13, and Figure 14 we present plots of the empirical poisoning rate of each attack while targeting each respective environment. We can see that the SleeperNets attack is able to decrease its poisoning rate to be significantly lower than all other attacks on every environment except for Highway Merge. Of particular note we achieve a $0.001\\%$ poisoning rate on Breakout and a $0.0006\\%$ poisoning rate on Trade BTC. ", "page_idx": 21}, {"type": "text", "text": "We can see that the poisoning rate of BadRL-M also often falls below the given poisoning budget, and falls very low in the case of Highway Merge. This is a feature of the attack as directly stated in [4]. One of the attack\u2019s main goals is to utilize a \u201csparse\u201d poisoning strategy which minimizes poisoning rate while maximizing attack success and stealth. In many cases this can work, however in the case of environments like Highway Merge this works against the attack. In particular, the attack only poisons in states it sees as having a \u201chigh attack value\u201d meaning the damage caused by taking $a^{+}$ instead of the optimal action is maximized. In environments like Highway Merge this set of states is very small and thus the attack only poisons occasionally, rather than regularly and randomly like TrojDRL and SleeperNets. We can see something similar in the case of $\\mathrm{O^{*}b e r t}$ , however the attack eventually corrects for the low poisoning rate and ends up with a value near the budget of $0.03\\%$ . ", "page_idx": 21}, {"type": "image", "img_path": "HkC4OYee3Q/tmp/e522896b9a10304b14fa0a01f485cd81221267df663c5d8f62befa0164718b05.jpg", "img_caption": ["Figure 12: Comparison of poisoning rate for each attack on Breakout and $\\mathrm{Q^{*}}$ bert "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "HkC4OYee3Q/tmp/dbe6c97ccbc4a5cc504190a8c5bdbe7ba1e11a9dcad41c808b8403f9fc4eb514.jpg", "img_caption": ["Figure 13: Comparison of poisoning rate for each attack on Car Racing and Highway Merge "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "HkC4OYee3Q/tmp/2514e82eb872f09761ab9b00a913bf8f830c1d6d88ea4b73acb1acbb2e3db49c.jpg", "img_caption": ["Figure 14: Comparison of poisoning rate for each attack on Safety Car and Trade BTC "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "9.7 Numerical Results from $c$ and $\\beta$ Ablations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section we provide full numerical results from the ablations with respect to $c$ and $\\beta$ presented in Section 6.3. All results are given with standard deviations in their neighboring column. The best result from each column is placed in bold. ", "page_idx": 22}, {"type": "table", "img_path": "HkC4OYee3Q/tmp/502683a9ce03c60a809f1b0982382f5029db3e836144296e9f9318844c57b574.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "HkC4OYee3Q/tmp/6b910b5d4123c5054c7c3ff7ca1e2e31359dda2c2e58ef46eaa973b7f55c5eb5.jpg", "table_caption": ["Table 7: Numerical results from the ablation with respect to $c$ . Each result is given with its respective standard deviation in the neighboring column. The best result in each column is placed in bold. "], "table_footnote": ["Table 8: Numerical results from the ablation with respect to $\\vec{\\beta}$ . Each result is given with its respective standard deviation in the neighboring column. The best result in each column is placed in bold. "], "page_idx": 22}, {"type": "text", "text": "9.8 Computational Resources Used ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "All experiments run in this paper are relatively low cost in terms of computational complexity \u2013 many can finish within a few hours running on CPU alone. For this paper all experiments were split between multiple machines including a desktop, laptop, and CPU server. Their specs are summarized in Table 9. ", "page_idx": 23}, {"type": "table", "img_path": "HkC4OYee3Q/tmp/e8b54a942415d991da30aa541dd7145d0f7b649263dccc1ca23f7d25a6e5b27a.jpg", "table_caption": [], "table_footnote": ["Table 9: Summary of machines used for the experiments in this paper. "], "page_idx": 23}, {"type": "text", "text": "9.9 Related Work ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section we discuss two alternate threat vectors against DRL algorithms \u2013 policy replacement attacks and evasion attacks. Note that \u201cpolicy replacement attacks\u201d is a term we are adopting for clarity, in the existing literature they are referred to generically as \u201cpoisoning attacks\u201d [30]. We believe the term \u201cpolicy replacement\u201d is appropriate as will become evident. In policy replacement attacks the adversary\u2019s objective is to poison the agent\u2019s training algorithm such that they learn a specific adversarial policy $\\pi^{+}:S\\to A$ . Note that these attacks, unlike backdoor attacks, do not alter the agent\u2019s state observations at training or deployment time as they have no goals of \u201ctriggering\u201d a target action. They instead only alter the agent\u2019s reward and or actions during each episode. Policy replacement attacks were extensively and formally studied in [30] and [29]. ", "page_idx": 23}, {"type": "text", "text": "Evasion attacks, on the other hand, use either black-box [22] [2], or white-box [21] [3] access to the fully trained agent\u2019s underlying function approximator to craft subtle perturbations to the agent\u2019s state observations with the goal of inducing undesirable actions or behavior. These attacks have proven highly effective in DRL domains with multiple different observation types, such as proprioceptive [7] and image-based observations [19]. These attacks have a few main drawbacks. First, they incur high levels of computational cost at inference time - the adversary must solve complex, multi-step optimization problems in order to craft their adversarial examples [2] [21]. Second, applying the same adversarial noise to a different observation often does not achieve the same attack success, requiring a full re-computation on a per-observation basis. Lastly, many adversarial examples are highly complex and precise, and thus require direct access to the agent\u2019s sensory equipment in order to effectively implement [28]. ", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: In our abstract and introduction we claim: theoretical analysis proving prior methods insufficient, a new framework with provable performance, a novel threat model, a new algorithm using our theoretical results, and detailed empirical analysis of our method in comparison to prior works in multiple domains. These topics are claims are explored and justified in detail in Section 4, Section 4.4, Section 3.1, Section 5, and Section 6 respectively. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: Limitations are given in Section 7. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best ", "page_idx": 24}, {"type": "text", "text": "judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Assumptions for our theory are given in Section 4.2 along with assumptions of our threat model in Section 3.1. Proofs are given in Appendix Section 9.1 and Section 9.2. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: Implementation details are given in Appendix Section 9.4. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Code is attached to the paper submission and provided anonymously here. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: Implementation details are given in Appendix Section 9.4. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Standard deviation is shown on all plots and given in all tables in the appendix. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Computational resources are summarized in Appendix 9.8. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper conforms with the NeurIPS code of ethics. There are no data or privacy concerns generated by this paper. Additionally, broader impacts are considered in Section 8 ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Broader impacts are considered in Section 8 ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not release any data or models that have a high risk of misuse. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: All environments are used under MIT or Apache license with proper credit and references given throughout the main body of the paper and in Appendix Section 9.4. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer:[NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]