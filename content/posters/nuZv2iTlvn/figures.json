[{"figure_path": "nuZv2iTlvn/figures/figures_2_1.jpg", "caption": "Figure 1: Model Architecture. (a) Architecture overview of NMM-GNN, a non-Euclidean Mixture Model with non-Euclidean VAE framework. (b) Illustration of space unification loss of NMM-GNN. z from the hyperbolic space is projected to its corresponding point in the spherical space, z, such that its geodesic distance is ensured to be close to vi's existing spherical space representation, z.", "description": "This figure shows the architecture of the Non-Euclidean Mixture Model (NMM) with a variational autoencoder (VAE) framework.  Panel (a) provides a high-level overview of the model, showing how hyperbolic and spherical graph neural networks (GNNs) are used to encode node embeddings in different spaces, which are then combined using a mixture model. Panel (b) details the space unification loss component, illustrating how embeddings from the hyperbolic space are projected to the spherical space to ensure consistency between the two representations.", "section": "3 Methodology"}, {"figure_path": "nuZv2iTlvn/figures/figures_4_1.jpg", "caption": "Figure 1: Model Architecture. (a) Architecture overview of NMM-GNN, a non-Euclidean Mixture Model with non-Euclidean VAE framework. (b) Illustration of space unification loss of NMM-GNN. z from the hyperbolic space is projected to its corresponding point in the spherical space, z, such that its geodesic distance is ensured to be close to vi's existing spherical space representation, z.", "description": "This figure shows the architecture of the NMM-GNN model, which combines a non-Euclidean mixture model with a variational autoencoder (VAE) framework.  Panel (a) provides a high-level overview of the model, highlighting the encoder (using spherical and hyperbolic GNNs), mixture model, and decoder components. Panel (b) zooms in on the space unification loss, illustrating how embeddings from hyperbolic space are projected into spherical space to ensure consistency and alignment between the different geometric representations.", "section": "3 Methodology"}, {"figure_path": "nuZv2iTlvn/figures/figures_5_1.jpg", "caption": "Figure 1: Model Architecture. (a) Architecture overview of NMM-GNN, a non-Euclidean Mixture Model with non-Euclidean VAE framework. (b) Illustration of space unification loss of NMM-GNN. z from the hyperbolic space is projected to its corresponding point in the spherical space, z, such that its geodesic distance is ensured to be close to vi's existing spherical space representation, z.", "description": "This figure shows the architecture of the NMM-GNN model, which combines a non-Euclidean Mixture Model with a variational autoencoder (VAE) framework.  Panel (a) presents a high-level overview of the model, illustrating the encoder (using spherical and hyperbolic GNNs), the mixture model (combining homophily and social influence factors), and the decoder. Panel (b) focuses on the space unification loss, which ensures alignment between embeddings in the hyperbolic and spherical spaces by minimizing the geodesic distance between the projection of a hyperbolic embedding onto the spherical space and its corresponding embedding in the spherical space. This alignment is crucial for effectively integrating information from both spaces.", "section": "3 Methodology"}, {"figure_path": "nuZv2iTlvn/figures/figures_7_1.jpg", "caption": "Figure 1: Model Architecture. (a) Architecture overview of NMM-GNN, a non-Euclidean Mixture Model with non-Euclidean VAE framework. (b) Illustration of space unification loss of NMM-GNN. z from the hyperbolic space is projected to its corresponding point in the spherical space, z, such that its geodesic distance is ensured to be close to vi's existing spherical space representation, z.", "description": "This figure shows the architecture of the proposed Non-Euclidean Mixture Model (NMM) with a Graph Neural Network (GNN) based Variational Autoencoder (VAE) framework called NMM-GNN.  The figure is divided into two subfigures. (a) shows a general overview of the framework, highlighting its components such as the encoder, decoder and mixture model. The encoder consists of spherical and hyperbolic GNNs. The decoder uses a mixture model to combine the embeddings from the spherical and hyperbolic spaces to predict link probabilities. (b) illustrates the space unification loss component which ensures alignment between the two non-Euclidean spaces. A node's embedding in the hyperbolic space (zL) is projected onto the spherical space (zS) to minimize the geodesic distance between the projected point and the node's existing spherical representation. This ensures consistency between the representations in the two distinct spaces.", "section": "3 Methodology"}, {"figure_path": "nuZv2iTlvn/figures/figures_15_1.jpg", "caption": "Figure 2: Ablation studies. (a) Quality of mixture model, where NMMhom and NMMrank are homophily-only and social influence-only deconstructed NMM components. (b) Inductive reasoning for NMM-GNN and RaRE on LiveJournal. % nodes {10, 30, 50, 70, 90} are sampled ensuring no overlap with test. (c) Ablation study on quality of using space unification loss (SUL) component.", "description": "This figure presents the results of ablation studies conducted to analyze the impact of different components and design choices in the proposed NMM-GNN model. (a) shows a comparison of the AUC scores for the full NMM model and its deconstructed homophily-only (NMMhom) and social influence-only (NMMrank) components across three datasets (BC, LJ, F). (b) illustrates the inductive reasoning capability of NMM-GNN compared to RaRE on the LiveJournal dataset, showing the AUC scores for varying percentages of training nodes. (c) evaluates the effect of the space unification loss (SUL) component on the model's performance across five datasets (WC, BC, LJ, WH, F).", "section": "Ablation Studies"}]