[{"figure_path": "kPBEAZU5Nm/figures/figures_3_1.jpg", "caption": "Figure 1: Target Distributions of Problems. This figure shows the levels of expected generality for each prompt.", "description": "This figure illustrates the hierarchy of problem sets used in the paper's experiments.  It starts with the broadest category, 'All PDDL Problems', encompassing various planning domains.  Within this, 'All Blocksworld Problems' is a subset focusing on the Blocksworld domain.  Further subdivisions include 'Table to Stack Problems' (where all blocks begin on a table and the goal is a single stack), and the most specific subset, 'Table to Lexicographic Stack Problems' (a subset of Table to Stack Problems where the goal stack is in lexicographical order). This visual helps understand how the different prompt engineering techniques were tested on varying levels of problem complexity and generality, reflecting the intended target distribution of each approach.", "section": "4 Chain of Thought Setups for Planning"}, {"figure_path": "kPBEAZU5Nm/figures/figures_5_1.jpg", "caption": "Figure 2: Accuracy of GPT-4-Turbo, Claude-3-Opus and GPT-4 across chain of thought prompting methods in their intended problem distributions with increasing number of blocks.", "description": "This figure presents the accuracy of three LLMs (GPT-4-Turbo, Claude-3-Opus, and GPT-4) across different prompting methods in solving Blocksworld planning problems.  The x-axis represents the number of blocks in the problem, and the y-axis shows the percentage of problems solved correctly. Different lines represent various prompting techniques, including zero-shot, zero-shot chain of thought, progression proof chain of thought, and problem-specific chain of thought approaches. The figure demonstrates how the accuracy of different LLMs changes with increasing problem complexity and across various prompting strategies, highlighting the impact of prompt engineering on LLM performance in planning tasks.  A baseline using a perfect planner (Fast Downward) is also included for comparison.", "section": "5 Blocksworld Results"}, {"figure_path": "kPBEAZU5Nm/figures/figures_7_1.jpg", "caption": "Figure 3: Accuracy of GPT-4-Turbo with chain of thought prompting across variations of our synthetic datasets. \"Direct\" means direct prompting without any CoT.", "description": "This figure presents the accuracy of GPT-4-Turbo model on three synthetic datasets (CoinFlip, LastLetterConcatenation, and One-Digit Arithmetic) using both chain of thought (CoT) prompting and direct prompting methods. The x-axis represents the number of steps or elements in each problem instance (e.g., number of people for CoinFlip, number of words for LastLetterConcatenation, and number of operations for One-Digit Arithmetic). The y-axis shows the percentage of correctly solved instances. For LastLetterConcatenation, different variants of the task are shown (full, foom_clearer, and vowel). For One-Digit Arithmetic, the different prompting techniques are distinguished (CoT, Basic, and Direct).  The figure visually demonstrates how the performance of both CoT and direct prompting methods vary across different problem complexities and dataset types.", "section": "Extension to Scalable Synthetic Benchmarks"}, {"figure_path": "kPBEAZU5Nm/figures/figures_13_1.jpg", "caption": "Figure 2: Accuracy of GPT-4-Turbo, Claude-3-Opus and GPT-4 across chain of thought prompting methods in their intended problem distributions with increasing number of blocks.", "description": "This figure shows the accuracy of three different LLMs (GPT-4-Turbo, Claude-3-Opus, and GPT-4) across various prompting methods on Blocksworld planning problems. The x-axis represents the number of blocks in the problem, and the y-axis represents the percentage of problems solved correctly.  Different lines represent different prompting methods: zero-shot, zero-shot CoT, progression proof CoT, problem-class specific n-shot, Blocksworld universal algorithm, and stacking prompt.  The figure demonstrates how the accuracy of each LLM varies depending on both the number of blocks and the prompting method used. A Fast Downward Planner line is also included as a baseline for comparison.", "section": "5 Blocksworld Results"}, {"figure_path": "kPBEAZU5Nm/figures/figures_14_1.jpg", "caption": "Figure 2: Accuracy of GPT-4-Turbo, Claude-3-Opus and GPT-4 across chain of thought prompting methods in their intended problem distributions with increasing number of blocks.", "description": "This figure displays the accuracy of three different LLMs (GPT-4-Turbo, Claude-3-Opus, and GPT-4) across various prompting methods on Blocksworld planning problems.  The x-axis represents the number of blocks in the problem, indicating increasing problem complexity. The y-axis shows the percentage of problems solved correctly.  Different lines represent different prompting methods: zero-shot, zero-shot CoT (chain of thought), a problem-class-specific n-shot, the Blocksworld universal algorithm prompt, and a stacking prompt. The figure demonstrates how the accuracy of each method changes as the problem complexity increases (more blocks), illustrating the impact of prompting technique on LLM performance in the Blocksworld domain.  The performance of a perfect planner (Fast Downward) is also included as a baseline.", "section": "5 Blocksworld Results"}, {"figure_path": "kPBEAZU5Nm/figures/figures_17_1.jpg", "caption": "Figure 2: Accuracy of GPT-4-Turbo, Claude-3-Opus and GPT-4 across chain of thought prompting methods in their intended problem distributions with increasing number of blocks.", "description": "This figure shows the accuracy of three different LLMs (GPT-4-Turbo, Claude-3-Opus, and GPT-4) across different chain of thought prompting methods on their intended problem distributions.  The x-axis represents the number of blocks in the Blocksworld planning problem, and the y-axis represents the percentage of instances solved correctly.  Each line in the graph represents a different prompting method, allowing for a comparison of performance across various levels of prompt specificity and generality.  The figure demonstrates how performance varies depending on the complexity of the problem and the detail provided in the prompt.", "section": "5 Blocksworld Results"}, {"figure_path": "kPBEAZU5Nm/figures/figures_22_1.jpg", "caption": "Figure 2: Accuracy of GPT-4-Turbo, Claude-3-Opus and GPT-4 across chain of thought prompting methods in their intended problem distributions with increasing number of blocks.", "description": "This figure displays the accuracy of three LLMs (GPT-4-Turbo, Claude-3-Opus, and GPT-4) across different chain-of-thought prompting methods on their intended problem distributions.  The x-axis represents the number of blocks in the Blocksworld problem, and the y-axis represents the percentage of instances solved correctly.  Each line in the graph represents a specific prompting method: Fast Downward Planner (a baseline), zero-shot, zero-shot CoT, progression proof CoT, domain-specific n-shot, Blocksworld universal algorithm, problem class-specific n-shot, and stacking prompt. The results show how the performance of each method changes with increasing problem complexity (number of blocks).", "section": "5 Blocksworld Results"}, {"figure_path": "kPBEAZU5Nm/figures/figures_23_1.jpg", "caption": "Figure 2: Accuracy of GPT-4-Turbo, Claude-3-Opus and GPT-4 across chain of thought prompting methods in their intended problem distributions with increasing number of blocks.", "description": "This figure presents the accuracy of three LLMs (GPT-4-Turbo, Claude-3-Opus, and GPT-4) across different chain of thought prompting methods.  The x-axis represents the number of blocks in the Blocksworld planning problem, and the y-axis represents the percentage of instances solved correctly. Each line represents a different prompting method, showing how accuracy changes as problem complexity increases.  The results highlight a significant drop in accuracy as the number of blocks grows, indicating limitations in the LLMs' ability to generalize learned reasoning strategies.", "section": "5 Blocksworld Results"}, {"figure_path": "kPBEAZU5Nm/figures/figures_23_2.jpg", "caption": "Figure 2: Accuracy of GPT-4-Turbo, Claude-3-Opus and GPT-4 across chain of thought prompting methods in their intended problem distributions with increasing number of blocks.", "description": "This figure presents the accuracy of three different LLMs (GPT-4-Turbo, Claude-3-Opus, and GPT-4) across various chain of thought prompting methods.  The x-axis represents the number of blocks in the Blocksworld planning problem, and the y-axis represents the percentage of instances solved correctly.  Each line represents a different prompting method, illustrating the impact of prompt design on LLM performance as problem complexity increases. The results show a general decrease in accuracy as the number of blocks increases, highlighting limitations in the generalization capabilities of chain of thought prompting.", "section": "5 Blocksworld Results"}, {"figure_path": "kPBEAZU5Nm/figures/figures_23_3.jpg", "caption": "Figure 2: Accuracy of GPT-4-Turbo, Claude-3-Opus and GPT-4 across chain of thought prompting methods in their intended problem distributions with increasing number of blocks.", "description": "This figure presents the accuracy of three LLMs (GPT-4-Turbo, Claude-3-Opus, and GPT-4) across different chain-of-thought prompting methods on their intended problem distributions.  The x-axis shows the number of blocks involved in the Blocksworld problem, representing increasing problem complexity. The y-axis displays the percentage of instances solved correctly by each LLM under various prompting techniques. The prompting methods include zero-shot, zero-shot CoT, progression proof CoT, domain-specific n-shot, Blocksworld universal algorithm, problem class-specific n-shot, and stacking prompt, demonstrating different levels of specificity and generality. The figure illustrates the performance trade-off between prompt generality and problem complexity, revealing the limitations of CoT in generalizing to larger instances beyond what was demonstrated in the prompt examples.", "section": "5 Blocksworld Results"}, {"figure_path": "kPBEAZU5Nm/figures/figures_24_1.jpg", "caption": "Figure 2: Accuracy of GPT-4-Turbo, Claude-3-Opus and GPT-4 across chain of thought prompting methods in their intended problem distributions with increasing number of blocks.", "description": "This figure displays the accuracy of three large language models (LLMs) - GPT-4-Turbo, Claude-3-Opus, and GPT-4 - across various chain-of-thought prompting methods on Blocksworld problems.  The x-axis represents the number of blocks in the problem, and the y-axis shows the percentage of correctly solved instances.  Different colored lines represent the accuracy achieved by each LLM with each prompting method (Zero-shot, Zero-shot CoT, Progression Proof CoT, Domain-Specific n-shot, Blocksworld Universal Algorithm, Problem Class Specific n-shot, Stacking Prompt). The figure showcases how the performance of the models changes as the complexity of the problems (number of blocks) increases.  A comparison line for a Fast Downward Planner (a highly efficient classical planner) is included as a benchmark.", "section": "5 Blocksworld Results"}, {"figure_path": "kPBEAZU5Nm/figures/figures_24_2.jpg", "caption": "Figure 2: Accuracy of GPT-4-Turbo, Claude-3-Opus and GPT-4 across chain of thought prompting methods in their intended problem distributions with increasing number of blocks.", "description": "This figure shows the accuracy of three different LLMs (GPT-4-Turbo, Claude-3-Opus, and GPT-4) across various chain of thought prompting methods.  The x-axis represents the number of blocks in the Blocksworld planning problem, and the y-axis shows the percentage of problems solved correctly.  Different colored lines represent different prompting methods, allowing for a comparison of their effectiveness and scalability as problem complexity increases (more blocks).  A line for a perfect planner is also included for comparison.", "section": "5 Blocksworld Results"}, {"figure_path": "kPBEAZU5Nm/figures/figures_27_1.jpg", "caption": "Figure 2: Accuracy of GPT-4-Turbo, Claude-3-Opus and GPT-4 across chain of thought prompting methods in their intended problem distributions with increasing number of blocks.", "description": "This figure presents the accuracy of three different LLMs (GPT-4-Turbo, Claude-3-Opus, and GPT-4) across various chain of thought prompting methods.  The x-axis represents the number of blocks in the Blocksworld problem, while the y-axis shows the percentage of problems solved correctly.  Different lines represent different prompting methods, ranging from zero-shot prompting to more specific and detailed approaches. The figure demonstrates how performance changes with the increasing complexity of problems (more blocks) and how the level of detail in the prompt affects the LLM's ability to solve these planning problems.", "section": "5 Blocksworld Results"}, {"figure_path": "kPBEAZU5Nm/figures/figures_27_2.jpg", "caption": "Figure 2: Accuracy of GPT-4-Turbo, Claude-3-Opus and GPT-4 across chain of thought prompting methods in their intended problem distributions with increasing number of blocks.", "description": "This figure shows the accuracy of three different LLMs (GPT-4-Turbo, Claude-3-Opus, and GPT-4) across various chain of thought prompting methods.  The x-axis represents the number of blocks in the Blocksworld planning problems, and the y-axis represents the percentage of problems solved correctly.  Different lines represent different prompting methods, ranging from zero-shot prompting to prompts with highly specific examples. The figure demonstrates how accuracy varies based on the complexity of the problem (number of blocks) and the specificity of the prompting strategy.  It visually represents the key finding that CoT's effectiveness diminishes rapidly as problem complexity increases, even with highly-specific prompts.  A baseline 'Fast Downward Planner' is also included to represent perfect accuracy.", "section": "5 Blocksworld Results"}, {"figure_path": "kPBEAZU5Nm/figures/figures_28_1.jpg", "caption": "Figure 2: Accuracy of GPT-4-Turbo, Claude-3-Opus and GPT-4 across chain of thought prompting methods in their intended problem distributions with increasing number of blocks.", "description": "This figure presents the accuracy of three large language models (LLMs), GPT-4-Turbo, Claude-3-Opus, and GPT-4, across different chain of thought prompting methods on Blocksworld problems.  The x-axis represents the number of blocks in the problem, and the y-axis shows the percentage of instances solved correctly. Different colored lines represent the performance under different prompting strategies (Zero-shot, Zero-shot CoT, Progression Proof CoT, Domain Specific n-shot, Blocksworld Universal Algorithm, Problem Class Specific n-shot, and Stacking Prompt), as well as the performance of a Fast Downward Planner (an optimal planner).  The figure demonstrates how accuracy declines as the number of blocks increases, especially for less specific prompting methods, highlighting the limited generalizability of chain of thought prompting.", "section": "5 Blocksworld Results"}]