[{"figure_path": "kPBEAZU5Nm/tables/tables_6_1.jpg", "caption": "Table 1: Accuracy across CoT and example granularities over 261 instances in table-to-stack Blocksworld.", "description": "This table presents the accuracy results of different prompting methods on a specific subset of Blocksworld problems, namely, the table-to-stack problems.  It compares the performance of zero-shot prompting, zero-shot chain of thought (CoT), different levels of example specificity (n-shot) and varying levels of CoT granularity.  The goal is to assess how well the LLMs generalize the provided reasoning strategies to solve increasingly complex problems within the same problem class.", "section": "5.2 Testing only on Table-to-Stack"}, {"figure_path": "kPBEAZU5Nm/tables/tables_7_1.jpg", "caption": "Table 2: Accuracy across CoT types and problem variations over all instances in our synthetic datasets. CF is CoinFlip, LLC is LastLetterConcatenation, LVC is LastVowelConcatenation, FLC is FoomLetterConcatenation, Arithmetic is baseline single-digit Arithmetic, AE is the same problems but with the explanation provided that all intermediate answers are single digit.", "description": "This table presents the accuracy of different prompting methods (Zero-Shot, Zero-Shot CoT, Manual CoT, Incorrect CoT) across five synthetic datasets (CoinFlip, LastLetterConcatenation, LastVowelConcatenation, FoomLetterConcatenation, Arithmetic, and Arithmetic with explanation).  It shows the performance of each method on various variations of the datasets, highlighting the impact of chain-of-thought prompting and its limitations in generalization across different problem types.", "section": "Extension to Scalable Synthetic Benchmarks"}, {"figure_path": "kPBEAZU5Nm/tables/tables_13_1.jpg", "caption": "Table 1: Accuracy across CoT and example granularities over 261 instances in table-to-stack Blocksworld.", "description": "This table presents the accuracy of different prompting methods on a subset of Blocksworld problems, specifically the table-to-stack problems.  The methods compared include zero-shot, zero-shot CoT, problem class specific n-shot, and the stacking prompt. The results show the accuracy of GPT-4-Turbo and Claude-3-Opus across various prompting strategies. The table highlights the performance differences across different levels of prompt specificity.", "section": "5.2 Testing only on Table-to-Stack"}, {"figure_path": "kPBEAZU5Nm/tables/tables_14_1.jpg", "caption": "Table 1: Accuracy across CoT and example granularities over 261 instances in table-to-stack Blocksworld.", "description": "This table presents the accuracy of different prompting methods on a subset of Blocksworld problems, specifically the table-to-stack problems.  It compares the performance of zero-shot, zero-shot with Chain of Thought (CoT), n-shot prompting with varying levels of specificity (domain-specific, problem-class specific, stacking), and CoT prompting with different levels of algorithmic detail (Progression Proof CoT, Blocksworld Universal Algorithm).  The results highlight how prompting effectiveness changes across various levels of generality and specificity. ", "section": "5.2 Testing only on Table-to-Stack"}]