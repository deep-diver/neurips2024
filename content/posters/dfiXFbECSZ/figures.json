[{"figure_path": "dfiXFbECSZ/figures/figures_1_1.jpg", "caption": "Figure 1: LOFIT methodology. LOFIT freezes all pre-trained weights of a transformer language model and uses two sets of lightweight parameters to modify the LLM representations in two steps: Attention Head Selection and Bias Tuning. Only the tuned biases are used in the final model.", "description": "This figure illustrates the LOFIT methodology, a two-step process for modifying LLM representations. First, it selects a subset of attention heads deemed most important for a specific task. This is done by fine-tuning scaling factor vectors and selecting the top-k heads with the largest norm. Then, it fine-tunes bias vectors for these selected heads, which are added to the model's hidden representations during inference. This localized approach uses far fewer parameters compared to other fine-tuning methods.", "section": "1 Introduction"}, {"figure_path": "dfiXFbECSZ/figures/figures_6_1.jpg", "caption": "Figure 2: Test accuracy of using LOFIT heads learned from a different task. Colors reflect relative accuracy with respect to using same-task heads, with same-task heads (diagonals) representing 100% relative accuracy. Different-task results with * are significantly lower than the same-task result at the significance level of 0.05 with a paired bootstrap test and results with + are significantly lower at the level of 0.1. For TruthfulQA, we report MC1 accuracy. Across models, task-specific heads consistently outperform different-task heads for TruthfulQA and MQUAKE.", "description": "This figure shows the results of an experiment evaluating the task specificity of LOFIT's attention head selection.  It compares the performance of using attention heads selected for a specific task versus using heads selected for a different task. The results demonstrate that using task-specific heads consistently leads to significantly higher accuracy compared to using heads from other tasks across multiple models and datasets.  This highlights the importance of LOFIT's task-specific localization.", "section": "Results: Effectiveness of Localization"}, {"figure_path": "dfiXFbECSZ/figures/figures_6_2.jpg", "caption": "Figure 3: Distribution of LOFIT heads over layers for different tasks. Across tasks, LOFIT heads are often located in different parts of the model, and layer selection differs between Llama2 and Gemma.", "description": "This figure shows the distribution of the selected attention heads across different layers of the language models for three different tasks: TruthfulQA, MQuAKE, and CLUTRR.  The graphs illustrate that the selected heads are not uniformly distributed across the layers, and the distribution varies depending on the task and the model used (Llama 2 7B, Llama 2 13B, and Gemma 7B). This supports the paper's claim that LOFIT's head selection is task-specific and not generalized.", "section": "5.1 Importance of LOFIT Heads"}, {"figure_path": "dfiXFbECSZ/figures/figures_8_1.jpg", "caption": "Figure 4: LOFIT performance using different numbers of training examples n on CLUTRR and MQUAKE with Llama 2-7B. For LOFIT, we tune 10% of the attention heads. Results are averaged over two runs. In the low data settings (n < 100), LOFIT is more data efficient than LoRA and RED. For n \u2265 300, LOFIT is still comparable to LoRA and RED with fewer parameters.", "description": "This figure shows the performance of LOFIT against LoRA and RED on CLUTRR and MQUAKE datasets with varying numbers of training examples.  LOFIT demonstrates superior data efficiency in low-data regimes (less than 100 examples) and remains competitive with LoRA and RED using significantly fewer parameters when more data is available (300 or more examples).", "section": "Results: Effectiveness of Localization"}, {"figure_path": "dfiXFbECSZ/figures/figures_17_1.jpg", "caption": "Figure 1: LOFIT methodology. LOFIT freezes all pre-trained weights of a transformer language model and uses two sets of lightweight parameters to modify the LLM representations in two steps: Attention Head Selection and Bias Tuning. Only the tuned biases are used in the final model.", "description": "The figure shows the architecture of LOFIT, which consists of two main steps. First, it selects a subset of attention heads that are important for a specific task by fine-tuning scaling factor vectors on attention heads. Second, it fine-tunes offset bias vectors that are added to the representations of those selected attention heads.  Only the tuned biases are used in the final model. This approach is designed to be parameter-efficient and to localize the impact of fine-tuning on the LLM.", "section": "1 Introduction"}, {"figure_path": "dfiXFbECSZ/figures/figures_17_2.jpg", "caption": "Figure 1: LOFIT methodology. LOFIT freezes all pre-trained weights of a transformer language model and uses two sets of lightweight parameters to modify the LLM representations in two steps: Attention Head Selection and Bias Tuning. Only the tuned biases are used in the final model.", "description": "The figure shows the two-step process of LOFIT. First, attention head selection is performed by fine-tuning scaling factor vectors and selecting the top-k heads with the largest norm. Then, bias tuning is done by fine-tuning bias vectors for these selected heads. The final model only uses the tuned biases.", "section": "1 Introduction"}, {"figure_path": "dfiXFbECSZ/figures/figures_18_1.jpg", "caption": "Figure 1: LOFIT methodology. LOFIT freezes all pre-trained weights of a transformer language model and uses two sets of lightweight parameters to modify the LLM representations in two steps: Attention Head Selection and Bias Tuning. Only the tuned biases are used in the final model.", "description": "The figure shows the two-step framework of LOFIT. First, it selects a subset of attention heads using a learnable scaling factor. Then, it learns offset vectors to add to the hidden representations of the selected heads. The final model only uses the tuned biases.  This localized approach is more parameter-efficient compared to other methods.", "section": "1 Introduction"}, {"figure_path": "dfiXFbECSZ/figures/figures_18_2.jpg", "caption": "Figure 1: LOFIT methodology. LOFIT freezes all pre-trained weights of a transformer language model and uses two sets of lightweight parameters to modify the LLM representations in two steps: Attention Head Selection and Bias Tuning. Only the tuned biases are used in the final model.", "description": "This figure illustrates the LOFIT methodology, which involves two main steps. First, it selects a subset of attention heads that are important for a specific task using a learned scaling factor. Second, it fine-tunes the biases of those selected attention heads by adding offset vectors to the hidden representations.  The pre-trained weights of the transformer language model remain frozen throughout the process. Only the tuned biases are used in the final LOFIT model.", "section": "1 Introduction"}, {"figure_path": "dfiXFbECSZ/figures/figures_23_1.jpg", "caption": "Figure 5: The effects of the percentage of attention heads K used for LOFIT Bias Tuning on LOFIT performance. Results are averaged over two runs. The test accuracy increases with K when K < 10% and plateaus when K reaches 10% - 20%.", "description": "This figure shows how the model's performance changes depending on the percentage of attention heads used for fine-tuning during the bias tuning stage of LOFIT. The x-axis represents the percentage of attention heads used, and the y-axis shows the accuracy. The results show that accuracy improves as more attention heads are used, but only up to a certain point (around 10-20%). After this point, increasing the number of heads does not significantly improve accuracy. This suggests that a limited set of attention heads is most effective for fine-tuning and that including too many heads may lead to diminishing returns or even negative effects.", "section": "Results: Effectiveness of Localization"}, {"figure_path": "dfiXFbECSZ/figures/figures_23_2.jpg", "caption": "Figure 3: Distribution of LOFIT heads over layers for different tasks. Across tasks, LOFIT heads are often located in different parts of the model, and layer selection differs between Llama2 and Gemma.", "description": "This figure shows the distribution of the selected attention heads across different layers for three different tasks (TruthfulQA, MQuAKE, and CLUTRR) and for three different LLMs (Gemma-7B, Llama 2-7B, and Llama 2-13B). Each bar represents the percentage of selected heads in a particular layer. The figure visually demonstrates that the task-specific attention heads selected by LOFIT are not randomly distributed across the layers.  Instead, their distribution varies across different tasks and models, suggesting task-specific localization of the heads.  This supports the paper's claim of task-specificity in LOFIT.", "section": "5.1 Importance of LOFIT Heads"}, {"figure_path": "dfiXFbECSZ/figures/figures_24_1.jpg", "caption": "Figure 2: Test accuracy of using LOFIT heads learned from a different task. Colors reflect relative accuracy with respect to using same-task heads, with same-task heads (diagonals) representing 100% relative accuracy. Different-task results with * are significantly lower than the same-task result at the significance level of 0.05 with a paired bootstrap test and results with + are significantly lower at the level of 0.1. For TruthfulQA, we report MC1 accuracy. Across models, task-specific heads consistently outperform different-task heads for TruthfulQA and MQUAKE.", "description": "This figure displays the results of an experiment testing the impact of using task-specific attention heads versus non-task-specific heads in the LOFIT model.  The diagonal shows the accuracy when the same task heads were used for both training and testing.  Off-diagonal values show the accuracy when heads selected for one task (e.g., TruthfulQA) were used to fine-tune the model for a different task (e.g., MQuAKE). The results demonstrate that using task-specific heads significantly improves model accuracy compared to using heads from a different task or random head selection, highlighting the importance of localization for optimal performance.", "section": "5.2 Task Specificity of Localized Interventions"}, {"figure_path": "dfiXFbECSZ/figures/figures_25_1.jpg", "caption": "Figure 1: LOFIT methodology. LOFIT freezes all pre-trained weights of a transformer language model and uses two sets of lightweight parameters to modify the LLM representations in two steps: Attention Head Selection and Bias Tuning. Only the tuned biases are used in the final model.", "description": "This figure illustrates the LOFIT methodology, which involves two main steps. First, it selects a subset of attention heads deemed most important for a specific task by fine-tuning scaling factors and choosing the heads with the largest norm of these factors. Second, it fine-tunes offset (bias) vectors for these selected heads.  Only the learned biases are incorporated into the final model, making it a parameter-efficient method. The pre-trained weights of the transformer language model remain frozen throughout the process.", "section": "1 Introduction"}]