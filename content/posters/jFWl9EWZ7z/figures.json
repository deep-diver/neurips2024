[{"figure_path": "jFWl9EWZ7z/figures/figures_2_1.jpg", "caption": "Figure 1: Illustration of the overall pipeline. Our D-LISA processes the 3D point cloud through the dynamic visual module (Sec. 3.1) and encodes the text description through a text encoder. The visual and word features are fused through a language informed spatial fusion module (Sec. 3.2).", "description": "This figure illustrates the overall architecture of the D-LISA model. The model consists of two main modules: a dynamic vision module and a language-informed spatial fusion module. The dynamic vision module takes a 3D point cloud as input and generates a set of box proposals, which are then used by the fusion module. The language-informed spatial fusion module combines the text and object features to select the final referred bounding boxes. The figure shows the flow of data through the different components of the model, highlighting the key steps in the process.", "section": "3 Approach"}, {"figure_path": "jFWl9EWZ7z/figures/figures_4_1.jpg", "caption": "Figure 2: Illustration of language informed spatial attention (LISA). We model the object relations through spatial distance D. For each box proposal, a spatial score is predicted to balance the visual attention weights and spatial relations.", "description": "This figure illustrates the Language-Informed Spatial Attention (LISA) module.  It shows how the module integrates visual features (F), sentence features (g), box proposals (B), box centers, and spatial distances (D) to produce a spatial score (B) which is then used to balance visual attention weights and spatial relations for improved contextual understanding. The process involves several steps including computing queries (Q) and keys (K) from the visual features, calculating spatial scores (B) based on the sentence and visual features, and then using a weighted combination of standard attention and spatial attention to refine the visual features.  The final output is the language-informed spatial attention.", "section": "3.2 Language-Informed Spatial Fusion Module"}, {"figure_path": "jFWl9EWZ7z/figures/figures_6_1.jpg", "caption": "Figure 3: Qualitative examples of Multi3DRefer val set. For each scene-text pair, we visualize the predictions of M3DRef-CLIP, M3DRef-CLIP w/NMS, D-LISA and ground truth labels in magenta/blue/green/red separately.", "description": "This figure shows a qualitative comparison of the proposed D-LISA model with two other state-of-the-art methods (M3DRef-CLIP and M3DRef-CLIP w/NMS) on the Multi3DRefer validation set.  Each row represents a different scene and text description. The visualizations demonstrate the ability of each model to accurately locate the target objects mentioned in the text description within the 3D scene. The ground truth is shown for comparison.", "section": "4.1 Multi-object 3D grounding"}, {"figure_path": "jFWl9EWZ7z/figures/figures_8_1.jpg", "caption": "Figure 4: Qualitative results of dynamic multi-view renderer. On the left, we show the learned pose distribution over the Multi3DRefer val set and visualize one camera ray example. On the right, we present examples of comparison between rendering with fixed pose and dynamic learned pose.", "description": "This figure shows the qualitative results of using a dynamic multi-view renderer in comparison to a fixed pose renderer. The left side shows the learned pose distribution, and the right side shows rendered 2D images using both dynamic and fixed camera poses.  The dynamic pose adaptation improves rendering quality.", "section": "4.3 Ablation studies"}, {"figure_path": "jFWl9EWZ7z/figures/figures_8_2.jpg", "caption": "Figure 4: Qualitative results of dynamic multi-view renderer. On the left, we show the learned pose distribution over the Multi3DRefer val set and visualize one camera ray example. On the right, we present examples of comparison between rendering with fixed pose and dynamic learned pose.", "description": "This figure shows the qualitative results obtained by using a dynamic multi-view renderer. The left part of the figure displays the learned pose distribution and a sample camera ray from the Multi3DRefer validation set. The right part compares the rendering results obtained with fixed and dynamic camera poses. This comparison visually demonstrates the benefits of using dynamic camera poses in generating 2D image features.", "section": "3.1 Dynamic Vision Module"}, {"figure_path": "jFWl9EWZ7z/figures/figures_16_1.jpg", "caption": "Figure 3: Qualitative examples of Multi3DRefer val set. For each scene-text pair, we visualize the predictions of M3DRef-CLIP, M3DRef-CLIP w/NMS, D-LISA and ground truth labels in magenta/blue/green/red separately.", "description": "This figure shows qualitative results on the Multi3DRefer validation dataset.  It provides a visual comparison of the bounding box predictions generated by three different models (M3DRef-CLIP, M3DRef-CLIP with Non-Maximum Suppression, and D-LISA) against the ground truth for several scene-text pairs. The color coding helps in differentiating the predictions from each model and the ground truth.", "section": "4.1 Multi-object 3D grounding"}, {"figure_path": "jFWl9EWZ7z/figures/figures_17_1.jpg", "caption": "Figure A2: Additional qualitative examples of Multi3DRefer val set in ST w/D category. For each scene-text pair, we visualize the predictions of M3DRef-CLIP, M3DRef-CLIP w/NMS, D-LISA and ground truth labels in magenta/blue/green/red separately.", "description": "This figure presents qualitative results for the single-target with distractors (ST w/D) category of the Multi3DRefer validation set.  It shows the predictions of three models (M3DRef-CLIP, M3DRef-CLIP with Non-Maximum Suppression (NMS), and the proposed D-LISA model) along with the ground truth labels for three different scene-text pairs. Each pair shows a 3D point cloud of a scene and an associated textual description.  The goal is to visually compare the accuracy of the bounding boxes predicted by the three methods for various objects described in the text. Different colors represent the boxes for different models or the ground truth.", "section": "A5 Additional qualitative results"}]