[{"type": "text", "text": "Efficient Sketches for Training Data Attribution and Studying the Loss Landscape ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Andrea Schioppa Google DeepMind   \nAmsterdam, the Netherlands   \narischioppa@google.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The study of modern machine learning models often necessitates storing vast quantities of gradients or Hessian vector products (HVPs). Traditional sketching methods struggle to scale under these memory constraints. We present a novel framework for scalable gradient and HVP sketching, tailored for modern hardware. We provide theoretical guarantees and demonstrate the power of our methods in applications like training data attribution, Hessian spectrum analysis, and intrinsic dimension computation for pre-trained language models. Our work sheds new light on the behavior of pre-trained language models, challenging assumptions about their intrinsic dimensionality and Hessian properties. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Overview In this work, we investigate gradient and Hessian vector product (HVP) sketching to address the memory constraints inherent in applications that require the storage of numerous such vectors. Examples of such applications include training data attribution (TDA), eigenvalue estimation, and the computation of the intrinsic dimension. While previous studies on the intrinsic dimension have employed the Fastfood Transform to mitigate memory demands, we demonstrate its theoretical limitations for sketching and its persistent memory bottlenecks on modern accelerators. To resolve these issues, we propose novel sketching algorithms designed for modern hardware and underpinned by robust theoretical guarantees. Our experiments on pre-trained language models demonstrate the scalability of our methods while offering new perspectives on the intrinsic dimension and the Hessian of generative language models. ", "page_idx": 0}, {"type": "text", "text": "Motivation Training data attribution (TDA) [20, 11] and Hessian eigenvalue estimation [12] offer powerful insights into neural network behavior. TDA requires storing vectors of the same dimensionality $N$ as the network\u2019s parameters for each training point, scaling linearly with dataset size $(O(N T)$ for $T$ training points). Similarly, numerically stable Hessian eigenvalue estimation algorithms demand repeated Hessian-vector product (HVP) computations and storage, scaling with network size and iterations $(O(N T)$ for $T$ iterations). These memory bottlenecks hinder the study of large-scale models; to address this, sketching [27] provides a compelling solution. By projecting gradients or HVPs into lower-dimensional random subspaces, sketching preserves their essential geometric properties while drastically reducing memory requirements. However, in TDA sketching has been employed with limited effectiveness. Random projections have been carried out with dense matrices, that introduce significant scaling constraints $(O(N D)$ memory for a target dimension $D$ ) which necessitate the restriction of gradients to a subset of layers. Current TDA scaling methods require layer selection, as demonstrated with BERT in [8], where only 15M parameters were used out of 110M. Beyond the computational cost of dense matrices, our experiments show that layer selection introduces substantial distortion in the estimation of TDA scores (Sec. 5.2). Recent work on neural network geometry [14] suggests using the Fastfood Transform for gradient sketching (abbr. FFD [13]) with $O(N)$ memory; however, as the Fastfood Transform is a random feature generation algorithm, it necessitates indirect application in order to sketch gradients. This has both theoretical and practical consequences. On the one hand, our theoretical analysis reveals that FFD fails to fully satisfy the algorithmic requirements of sketching (Thm. 3.1). On the other hand, our experiments (Tab. 3) demonstrate that FFD exhibits unacceptable run-time performance on TPUs. These limitations, both theoretical and practical, underscore the need for novel sketching algorithms. To this end, we investigate new sketching paradigms optimized for modern accelerators like GPUs and TPUs. Our work makes the following contributions: ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "1. Scalable Gradient Sketching for Modern Accelerators and Networks: We introduce algorithms (AFFD, AFJL, QK) (Sec. 3) designed to overcome performance limitations of existing sketching techniques with modern neural networks on architectures like GPUs and TPUs. Our design analysis provides further insights into the effectiveness of our approach.   \n2. Robust Theoretical Foundations: We establish theoretical guarantees for AFFD and QK (Thms. 3.2, 3.3) for sketching and demonstrate limitations in the theoretical basis of the Fastfood Transform (Thm. 3.1). Our analysis further indicates a dimensionality reduction advantage for AFFD over QK, a finding supported by our TDA experimental results (Sec. 5.3).   \n3. Algorithmic Improvements: We propose more efficient algorithms for the intrinsic dimension estimation and Hessian eigenvalue computation (Sec. 4). ", "page_idx": 1}, {"type": "text", "text": "We demonstrate how our methods enable large-scale applications in training data attribution, intrinsic dimension computation, and Hessian spectra analysis with pre-trained language models. This leads to the following insights that advance the understanding of pre-trained language models: ", "page_idx": 1}, {"type": "text", "text": "1. Limitations of Layer Selection: We demonstrate that layer selection methods yield inaccurate influence score estimations in training data attribution, so their usage should be avoided (Sec. 5.2).   \n2. High Intrinsic Dimension: In contrast to assumptions drawn from classification-based studies, we demonstrate that the intrinsic dimension of LLMs can approach their full parameter count (Sec. 5.4). This challenges prevailing beliefs about the intrinsic dimensionality of these models [14, 1, 17].   \n3. LLM Hessian Spectra: Our analysis shows distinct characteristics of LLM Hessian spectra (Sec. 5.5), contrasting with conjectures based on findings related to smaller networks [9, 21, 3, 12]. ", "page_idx": 1}, {"type": "text", "text": "Paper organization Sec. 2 provides a survey of relevant research in the field, contextualizing our contributions. Sec. 3 introduces our novel sketching algorithms. We begin with necessary background material, analyze design choices, and provide a step-by-step implementation tutorial in Appendix B. Sec. 4 outlines our proposed techniques for efficient intrinsic dimension search and Hessian eigenvalue computation. Sec. 5 describes our experimental setup: subsections are aligned with Sections 3 and 4 to enhance the connection between theory and empirical results ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Sketching Sketching algorithms have been extensively studied (see surveys [27, 18, 15]). Our algorithms, AFFD and AFJL, draw inspiration from the seminal FJL algorithm [2] and the FFD approach [13]. However, these techniques were designed before the era of modern accelerators like GPUs and TPUs. Therefore, our work revisits their design, optimizing them for modern neural networks and hardware. Theoretically, while [2] established FJL as a sketching algorithm, their proof relies on independence assumptions that do not hold in our setting. To address this, we employ more sophisticated concentration tools tailored to bi-linear forms (Thm. 3.2) and the special orthogonal group (Thm. 3.3). Recent work on PAC bounds [17] leveraged Kronecker-product decompositions to accelerate gradient sketching when computing intrinsic dimensionality. Our QK algorithm extends the concepts introduced in [17]. Importantly, we provide a proof that QK is a sketching algorithm, absent in [17]. Additionally, we demonstrate that the Kronecker structure used in [17] is not essential for performance gains, highlighting that the true bottleneck in FFD and FJL is memory access. For a comprehensive comparison with [17], please refer to Appendix C. Finally, to support our eigenvalue estimation, we leverage the theoretical guarantees outlined in [25]. ", "page_idx": 1}, {"type": "text", "text": "Intrinsic dimension The concept of intrinsic dimension (ID) offers a valuable metric for understanding the complexity of learning tasks. Originally employed to analyze loss landscapes [14], intrinsic dimension has expanded into the study of language models. [1] demonstrated its role in explaining the generalization power of fine-tuned pre-trained language models. However, their focus was limited to classification tasks. In contrast, our work extends the analysis of ID to generative tasks. This distinction is crucial as we identify scenarios where the task\u2019s intrinsic dimension approaches the full model size, a phenomenon not typically observed in classification settings. Additionally, while FFD has been used for efficient language model fine-tuning [16], memory constraints limited their investigation of the intrinsic dimension (ID) in generative tasks to $500\\mathrm{k}$ ; in other words, because of scalability contraints, [16] could only work with a target sketching dimension $\\leq500\\mathrm{k}$ , which prevented searching for the true value of ID as we demonstrate on a summarization task where ID approaches the model dimension. Therefore, our work overcomes this limitation, allowing us to compute the intrinsic dimension of such tasks. ", "page_idx": 2}, {"type": "text", "text": "Scaling up influence functions Scaling influence functions for training-data attribution remains a crucial research direction. Previous works like [8, 7] have improved index creation, retrieval speed, and Hessian estimation. However, these approaches can still be computationally demanding. Our work takes a distinct path, aiming to make influence function calculations fundamentally more efficient. Our HVP sketching methods seamlessly replace existing HVP and gradient computations in frameworks proposed within [8, 7, 20]. Furthermore, we offer eigenvector sketching to enhance methods like the Arnoldi iteration [24]. [19] has proposed to use dense random projections by materializing them in chunks and on-the-fly; drawbacks of this approach are: (1) the lack of the scalability in the target sketching dimension and (2) the need of hardware-dependent custom implementations; on the other hand, our approach removes the requirement for specialized implementations (e.g., custom CUDA kernels). This flexibility enables easy integration into standard ML workflows using higher-level languages such as Jax. An orthogonal direction to scaling up via sketching is that of using surrogate models, compare [5]. ", "page_idx": 2}, {"type": "text", "text": "Hessian evolution during training Investigating how the Hessian evolves during training has shed light on the dynamics of deep learning, with seminal works [21, 9] offering intriguing findings. These studies suggest the progressive disappearance of negative eigenvalues and the confinement of gradient descent within a small subspace. While [12] developed a numerically stable Hessian analysis algorithm, its computational demands hinder its application to large-scale models over numerous iterations. Our work addresses this limitation by introducing sketching techniques to enable the efficient construction of large Krylov subspaces (e.g., $10^{3}$ -dimensional) for models like GPT-2L (770M parameters). This advancement significantly surpasses the memory constraints of the method utilized by [12]: a 3TB fp32 storage requirement would have been necessary for a comparable analysis using their approach. Consequently, we are uniquely positioned to rigorously examine the conjectures proposed [21, 9] within the context of pre-trained language models. ", "page_idx": 2}, {"type": "text", "text": "3 Design Principles for Efficient Sketching Algorithms ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we explore a design space for more efficient sketching algorithms. To establish a foundation, we first analyze the performance bottlenecks of existing algorithms, specifically FJL and FFD, within the context of modern accelerators. This examination highlights two critical design choices: whether the gradient is sketched implicitly or explicitly, and the kind of pre-conditioner that is used. Informed by this analysis, we propose three novel algorithms: AFFD, AFJL, and QK. We then delve into the theoretical underpinnings of AFFD and QK, providing rigorous proofs for their guarantees. Additionally, we demonstrate that FFD lacks the theoretical foundation required for sketching. Relevant experimental findings are presented in Sections 5.2 and 5.3. ", "page_idx": 2}, {"type": "text", "text": "Dense sketches and FJL A $D$ -dimensional sketch of the gradient of a real-valued function $L(\\theta)$ $(\\theta\\,\\,\\in\\,\\mathbb{R}^{N})$ is a random projection of the gradient $\\nabla L\\,\\in\\,\\bar{\\mathbb{R}}^{N}$ to $\\mathbb{R}^{D}$ . To ensure this projection preserves essential geometric properties, the random projection operator $\\Phi:\\mathbb{R}^{N}\\rightarrow\\mathbb{R}^{D}$ must, with high probability, concentrate the norm of $\\Phi(x)$ around the norm of $x$ . Mathematically, for each $\\varepsilon$ and $\\delta$ there exists a large enough target dimension $D(\\varepsilon,\\delta)$ so that for $D\\geq D(\\varepsilon,\\delta)$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{Prob}\\left(|\\|\\Phi(x)\\|_{2}-\\|x\\|_{2}\\right|\\geq\\varepsilon\\|x\\|_{2}\\right)\\leq\\delta.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This concept generalizes to sketching higher-order derivatives (see Appendix C). For our purposes, consider the Hessian vector product operator $\\mathrm{HVP}:\\mathbb{R}^{N}\\rightarrow\\mathbb{R}^{N}$ defined as $\\mathrm{HVP}(u)=\\bar{\\nabla^{2}}L\\dot{(}\\theta)(u)$ A sketch of the HVP can be obtained as $v\\mapsto\\Phi(\\mathrm{HVP}(\\Phi^{T}v))$ , $\\boldsymbol{v}\\in\\mathbb{R}^{D}$ . This sketch defines a linear mapping $\\mathbb{R}^{D}\\to\\mathbb{R}^{D}$ [25]. While a simple Dense Sketch (using a random $D\\times N$ Gaussian matrix) ensures the norm property, it has $O(D N)$ memory and $O(D\\bar{N}^{2})$ compute requirements. The FJL algorithm [2] addresses the compute cost with a sparser projection matrix: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Phi(x)=\\sqrt{\\frac{N}{D}}\\cdot G_{s}\\cdot H_{N}\\cdot B(x),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where: $\\boldsymbol{B}\\ \\in\\ \\mathbb{R}^{N\\times N}$ is a diagonal matrix with $B_{i,i}~=~\\pm1$ (random signs); $H_{N}$ is the $N$ - dimensional Walsh-Hadamard transform (Appendix $\\mathrm{^C}$ ); $G_{s}$ is a sparse $\\mathbb{R}^{D\\times N}$ Gaussian matrix with (in-expectation) $\\Theta(D\\log^{2}M)$ non-zero entries ( $M$ is a parameter). The $H_{N}\\cdot B$ component preconditions sparse inputs $x$ for which (1) might otherwise fail. Implementing $G_{s}$ efficiently presents challenges on modern hardware and frameworks like Jax that lack native sparsity support. ", "page_idx": 3}, {"type": "text", "text": "FFD: Implicit Gradient sketching The FFD transform, introduced by [13] in the context of kernel machines, provides a computationally efficient way to approximate high-dimensional feature maps. As a random feature generator [15], FFD constructs high-dimensional random features $(\\in\\mathbb{R}^{N})$ from a lower-dimensional input vector $\\left(u\\in\\mathbb{R}^{D}\\right)$ ). Given $\\bar{u}\\in\\mathbb{R}^{D}$ , FFD concatenates $\\frac{N}{D}$ vectors of the form: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Phi_{i}(u)=\\sigma_{F}\\cdot H_{D}\\cdot G_{v}\\cdot\\Pi\\cdot H_{D}\\cdot B(u)\\quad(1\\leq i\\leq\\frac{N}{D}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $B$ and $H_{D}$ are as in (2) (and are both $D\\times D$ -matrices), $\\Pi$ is a permutation matrix, $G_{v}$ is a diagonal $D\\times D$ -matrix with i.i.d. standard Gaussian entries, and $\\sigma_{F}$ is a normalization constant (see [13]; for a practical implementation see the code released by [16]). FFD has the key advantage of constant memory cost, $O(N)$ , regardless of the input dimension $D$ . Since FFD defines a map $\\mathbb{R}^{D}\\to\\mathbb{R}^{N}$ , direct sketching of a gradient is not possible. To address this, [14] perform what we call an Implicit Gradient Sketch: ", "page_idx": 3}, {"type": "equation", "text": "$$\nS(\\nabla_{\\theta|\\theta_{0}}L)=\\nabla_{\\omega|0}L(\\theta_{0}+\\mathbf{F}\\mathbf{F}\\mathbf{D}(\\omega));\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "this formulation effectively applies the transpose of FFD to the gradient. While [13] establish certain properties of FFD, a complete proof of its suitability as a random feature generator is missing. Additionally, whether the FFD satisfies sketching guarantees (1), remains to be fully investigated (we will address it in Thm. 3.1). ", "page_idx": 3}, {"type": "text", "text": "Explicit sketches. In light of equation (4), it\u2019s natural to consider whether a direct sketch of the gradient could be achieved using a map $\\Phi:\\mathbb{R}^{N}\\rightarrow\\mathbb{R}^{D}$ . We define this as an Explicit Gradient Sketch: ", "page_idx": 3}, {"type": "equation", "text": "$$\nS(\\nabla_{\\theta|\\theta_{0}}L)=\\Phi(\\nabla_{\\theta|\\theta_{0}}L).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This approach offers flexibility. For random feature generation methods like FFD, $\\Phi$ needs to be implemented as the transpose; for sketching algorithms like FJL, the explicit sketch can be applied directly, while the transpose would be needed for the implicit form (4). In Appendix B, we provide a Jax-based tutorial on transposing sketching algorithms. Which one is the right approach? Intuitively, implicit sketches may seem more efficient since they avoid direct gradient materialization. However, as we\u2019ll demonstrate in Section 5.3, explicit sketches surprisingly offer significant performance advantages. Table 4 quantifies the substantial wall-time reductions (approximately $70\\%$ on average) across various algorithms when using explicit sketches. ", "page_idx": 3}, {"type": "text", "text": "Removing the lookup bottleneck. In both FJL and FFD algorithms, multiplications by $G_{s}$ and $\\Pi$ introduce a lookup-based memory bottleneck on modern accelerators. This hinders performance, as seen in unacceptable early TPU results (compare Tab. 3). To address this, we propose randomizing the pre-conditioner $H_{N}$ . Efficient implementations of $H_{N}$ leverage the fact that it can be decomposed using Kronecker products; specifically, $H_{A B}=H_{A}\\otimes H_{B}$ , which allows a recursive multiplication by $H_{N}$ in $O(N\\log N)$ -time and $O(N)$ storage. We exploit this by permuting rows/columns within Kronecker factors, reducing memory access costs from $O(A B)$ to $O(A\\!+\\!B)$ in the previous example. For optimal accelerator usage, we limit factors to roughly 1024 columns. Building on this, we modify (3). In the resulting AFFD algorithm (6), we remove $\\Pi$ , use row-permuted factors in $H_{N}^{\\pi_{1}}$ , and column-permuted factors in $H_{N}^{\\pi_{2}}$ : ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Phi(x)=R_{D}(\\sqrt{\\frac{N}{D}}\\cdot H_{N}^{\\pi_{2}}\\cdot G_{v}\\cdot H_{N}^{\\pi_{1}}\\cdot B(x)),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $R_{D}$ denotes restriction to the first $D$ coordinates. While all matrices are now $N\\times N$ , efficient Hadamard implementations avoid large matrix materializations (see Appendix B for our Jax code). We further introduce AFJL (7), where $H_{N}^{\\pi_{2}}$ is removed: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Phi(x)=R_{D}(\\sqrt{\\frac{N}{D}}\\cdot G_{v}\\cdot H_{N}^{\\pi_{1}}\\cdot B(x)).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "This can be seen as replacing $G_{s}$ in FJL with a diagonal Gaussian matrix. Generalizations with multiple $\\left(G_{v},H_{N}^{\\pi_{1}}\\right)$ pairs are possible but weren\u2019t explored due to the vanilla version\u2019s strong results. Empirical results on TPUs (Table 3) show the crucial impact of our changes. AFJL achieves a $100\\mathrm{x}$ wall-time reduction over FJL, AFFD a $64\\mathrm{x}$ reduction over FFD. While GPU wall-time remains similar, peak memory usage significantly improves. Appendix A highlights FJL\u2019s scaling issues beyond $\\dot{D}=2^{20}$ on GPUs. ", "page_idx": 4}, {"type": "text", "text": "Alternative pre-conditioners. To address the smoothing of sparse inputs in sketching algorithms, [2] introduced $H_{N}$ in (2). The theoretical basis lies in the Heisenberg uncertainty principle, leveraging the Fourier Transform on a discrete group (see [2]). However, the computationally efficient Fast Fourier Transform (FFT) shares this desirable property. This raises the question of whether the FFT might yield performance gains for sketching. We find that replacing $H_{N}$ with the FFT offers significant advantages. Experimentally, it reduces wall time by $62\\%$ on GPUs (Tab 4). Inspired by the Kronecker product structure enabling efficient $H_{N}$ implementation, we propose another generalized pre-conditioner, $Q$ . This random $N\\times N$ orthogonal matrix has a Kronecker decomposition of $K$ independent orthogonal matrices of sizes $\\{B_{i}\\times\\overline{{B_{i}}}\\}_{i=1}^{K}$ , sampled according to the Haar measure on $S O(B_{i})$ . Our approach allows direct application of $Q$ without the additional diagonal matrix $B$ , and offers a $40\\%$ wall time reduction on GPUs (Table 4). This $Q$ pre-conditioner has the potential to unlock broader optimizations within sketching algorithm design as discussed in the next subsection. ", "page_idx": 4}, {"type": "text", "text": "Direct usage of the pre-conditioner $Q$ . Inspired by the design of the pre-conditioner $Q$ , we introduce a novel sketching algorithm, QK. This ablation explores the direct use of $Q$ to transform the input, potentially leading to improved efficiency and memory usage. QK is defined as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Phi(x)={\\sqrt{\\frac{N}{D}}}\\cdot Q(x).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, $Q$ is a random $D\\times N$ -orthogonal matrix with a Kronecker product decomposition of $K$ independent orthogonal matrices of sizes $\\{D_{i}\\,\\times\\,B_{i}\\}_{i=1}^{K}$ . Each factor is generated by sampling from $S O(B_{i})$ according to the Haar measure and restricting to the first $D_{i}$ rows. Importantly, QK generalizes the approach of [17] by employing more Kronecker factors. This offers the potential for significant memory reductions (Appendix C). ", "page_idx": 4}, {"type": "text", "text": "Diagrams. Figure 1 illustrates AFFD, AFJL and QK with diagrams. ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Theoretical results We now delve into the theoretical underpinnings of our proposed algorithms, emphasizing the interplay between theory and experimentation. A key finding is a limitation of the FFD algorithm: ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1. There are some inputs x for which FFD does not satisfy the sketching property (1). ", "page_idx": 4}, {"type": "text", "text": "Note that this limitation is specific to FFD and not inherent to implicit-mode algorithms. This distinction is important: the explicit-mode formulation allowed to simplify the theoretical analysis to prove Thm. 3.1. Next, we establish a theoretical guarantee for our AFFD algorithm. Compared to [2] there is added complexity as independence arguments cannot be used for $G_{v}$ ; we thus apply the Hanson-Wright inequality for quadratic forms to prove: ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.2. AFFD satisfies (1) with ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\delta=\\delta_{1}+2\\exp\\left(-C\\varepsilon^{2}\\frac{D}{4\\log^{2}\\frac{2N}{\\delta_{1}}}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "image", "img_path": "8jyCRGXOr5/tmp/0e5b68434991b152914be9c221d674e04b59b95e5b31c958f04992491b1b3b67.jpg", "img_caption": ["Figure 1: Diagram to illustrate our proposed sketching algorithms. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "for a universal constant $C$ and for any $\\delta_{1}>0$ . ", "page_idx": 5}, {"type": "text", "text": "Finally, we analyze QK. Its structure allows for concentration arguments on orthogonal groups, leading to: ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.3. QK satisfies (1) with ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\delta=2\\sum_{i}\\exp(-4C D_{i}((1+\\varepsilon)^{1/K}-1)^{2}),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for a universal constant $C$ . ", "page_idx": 5}, {"type": "text", "text": "Crucially, the bound in Thm. 3.3 is less favorable than that of Thm. 3.2. This is due to the $1/K$ -root, summation over sub-dimensions, and concentration depending on the $D_{i}$ . These theoretical insights support our experimental findings in Section 5.3, where QK requires higher target dimensions $(D)$ to achieve performance comparable to AFFD. Because of space constraints the proofs are included in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "4 Expanding the Utility of Sketching Algorithms ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this Section we expand the usage of sketching to other applications. ", "page_idx": 5}, {"type": "text", "text": "Improving the search for the intrinsic dimension. The intrinsic dimension $(D_{i n t})$ [14], is the minimum dimension $(D)$ of a random subspace $(V)$ where SGD yields at least $90\\%$ of the full model\u2019s performance on a target metric $(\\tau^{1})$ . Sketching algorithms, more efficient than FastFood, already accelerate the search for $D_{i n t}$ [17]. Our memory-efficient algorithms enable us to investigate scenarios where $D_{i n t}$ may approach the model dimension. For instance, [16] applied FFD to finetune generative language models but capped the target dimension $(D)$ to $500\\mathrm{k}$ due to memory limits. We propose a novel search algorithm that estimates $D_{i n t}$ in a single training run. Current methods rely on multiple runs across potential $D_{i n t}$ values. To streamline, consider a binary search approach (assuming $D_{i n t}$ is a power of 2) starting from an initial guess $(D_{m i n})$ up to the model parameter count (N). This would require \u2308log2DmNin \u2309 runs. Instead, we propose a single training run where $D$ increases progressively. The heuristic: a fixed computational budget $\\dot{c}$ steps) should yield an expected improvement of at least $\\delta$ . We start with $D=D_{m i n}$ . If, after $c$ steps, the target metric\u2019s improvement is less than $\\delta$ or hasn\u2019t reached $\\tau_{90}$ , we double $D$ . This yields an estimate $D^{*}$ within a factor of 2 of $D_{i n t}$ . A subsequent run with $D=D^{*}/2$ verifies if this factor can be eliminated. See Appendix B for Python code. In Sec. 5.4, we apply this approach to pre-trained language models on classification and generative tasks. We find that $D_{i n t}\\ll N$ for classification, but for the generative task, $D_{i n t}$ depends on the choice of $\\tau$ and can approach the parameter count $N$ . This finding is significant, as it challenges prevailing assumptions about intrinsic dimensionality in such models [14, 1, 17].\u221a These studies primarily focused on classification tasks, obtaining generalization bounds like $O(\\bar{\\sqrt{D_{i n t}}})$ ([1, Eq. 4], [17, Sec. 3]) where $D_{i n t}$ was presumed much smaller than the model dimension. Our results suggest a need for non-linear projection algorithms to achieve lower intrinsic dimensionality for generative tasks. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Scaling eigenvalue estimation Investigating the Hessian\u2019s spectrum and eigenvectors often relies on memory-bound iterative algorithms. The Arnoldi algorithm, as used in [12], requires full reorthogonalization for stability. This necessitates storing vectors as large as the model itself for each iteration: a constraint that limits the number of estimable eigenvalues (e.g., Krylov subspaces of dimension $\\simeq90$ in [12]). Inspired by the theoretical work of [25], we propose sketching to address this memory bottleneck. With a sketching dimension of $10^{6}$ , we can readily construct 1k-dimensional Krylov subspaces for pre-trained language models like BART, Roberta, and GPT-2L. This represents a breakthrough because the method in [12] would demand an impractical 3TB of storage for GPT-2L alone. Our technique enables exploration of conjectures [21, 12, 9] about Hessian structure in the context of fine-tuning large language models. These conjectures are elaborated in the experimental results on eigenvalue estimation (Sec. 5.5). Crucially, we find that Hessian spectra in these models may deviate significantly from behaviors observed in smaller networks. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To thoroughly evaluate the proposed sketching methods, we present a comprehensive set of experiments. First, we highlight the limitations of existing TDA scaling strategies (Sec. 5.2). Next, we dissect the impact of specific design choices on our sketches (Sec. 5.3). We then introduce and validate an algorithm for intrinsic dimension estimation, enabling computational savings (Sec. 5.4) and show-casing that the intrinsic dimensionality of generative tasks can be large. Finally, we apply our techniques to explore the evolution of the Hessian spectrum during pre-trained language model fine-tuning (Sec. 5.5). ", "page_idx": 6}, {"type": "text", "text": "5.1 How we define the Training-Data Attribution score. ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In TDA there are different ways to measure the similarity score between two examples $x$ and $y$ . In our experiments we opt for the TDA score defined as $\\nabla_{\\theta}\\bar{L(\\theta,x)}\\cdot\\nabla_{\\theta}L(\\theta,z)$ because of its simplicity, allowing us to iterate on multiple algorithms and layer selection schemes, and being a building block for more complicated methods. While high correlation with full gradient dot products may not be the definitive measure of long-term TDA performance [19, 23], it is a practical metric in the short time range and a building block of more computationally intensive methods like TRAK [19]. For example, in the short time range, gradient dot products correlate with loss changes and are relevant to select examples for error correction [23]. Evaluating with the LDS from TRAK would introduce more hyper-parameters and considerable more computation: one would need at least 50 models fine-tuned on different subsets of the data; moreover, TRAK itself relies on accurate gradient sketches, as measured by dot products, as the basic building block as TRAK demonstrates that \u201cpreserving inner products to sufficient accuracy results in a gradient-descent system that approximately preserves the same evolution as the one corresponding to model re-training\u201d [19, C.2]. ", "page_idx": 6}, {"type": "text", "text": "5.2 Shortcomings of previous Training-Data Attribution scaling strategies. ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Past work [20, 28, 8] tackles the memory bottleneck of Training-Data Attribution by calculating gradients restricted to a specific layer and potentially applying a Dense Sketch. Here, we demonstrate that layer selection distorts both influence scores and eigenvalue estimates, while Dense Sketches exhibit poor scaling characteristics. These findings align closely with the first subsection of Sec. 3. Furthermore, advice on layer selection lacks consistency: [20] promotes the last layer, whereas [28] supports using Token Embeddings in NLP tasks. We demonstrate the distortion caused by layer selection on influence scores (the inner product of two gradients). Considering $2^{12}$ pairs of points $(x,z)$ , we compute the Pearson correlation $r$ between the TDA score $\\nabla_{\\boldsymbol{\\theta}}L(\\boldsymbol{\\theta},\\boldsymbol{x})\\cdot\\bar{\\nabla}_{\\boldsymbol{\\theta}}L(\\bar{\\boldsymbol{\\theta}},\\boldsymbol{z})$ estimated using a layer-specific gradient and the ground truth based on the full gradient. We adopt the setup of [6]: a generative task fine-tuning GPT-2 on the WikiText-103 dataset (BART and zsRE results in Appendix A). Our findings indicate the unreliability of layer selection (Table 1). Correlations ", "page_idx": 6}, {"type": "text", "text": "Table 1: Layer selection results in unreliable estimates for influence scores and eigenvalue estimation. The best correlation with ground truth influence scores does not exceed $90\\%$ and is quite low for most layers; the relative error in eigenvalue prediction is always at least $20\\%$ . ", "page_idx": 7}, {"type": "table", "img_path": "8jyCRGXOr5/tmp/f6916ca94bb2a61b32528ac8aa7393cda15ec83bec44341b6f4c2e6da3ee647c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "8jyCRGXOr5/tmp/91c740d9f72c9144820083550927013e06aa74dd124f56e8d64de6b628a33c94.jpg", "table_caption": ["Table 2: Dense projections on the layers do not scale; for each layer we report the wall time for the maximum dimension that does not result in an OOM. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "with ground truth rarely exceed $90\\%$ and are significantly lower for most layers. In contrast, AFJL achieves a $98\\%$ correlation with a compact $\\breve{D}=2^{13}$ . Extending the analysis to Hessian-based influence functions by looking into eigenvalue estimation emphasizes the shortcomings of layer selection. We aim to compute the top 10 eigenvalues of both the full Hessian and those restricted to a chosen layer. Even with potential differences in magnitude and location, layer selection could still be valuable if the true eigenvalues were approximated well by applying an orientation-preserving linear map $\\mathbb{R}^{1}\\to\\mathbb{R}^{1}$ to those computed for a particular layer. However, this is not the case, with the relative mean absolute error at $20\\%$ on the best layer (Table 1). Finally, layer selection coupled with dense projections faces severe scalability limitations. Setting $D=2^{1\\bar{2}}$ within the same setup highlights this issue. Limited memory forces us to divide the computation into $D/k$ projections to $\\mathbf{\\hat{R}}^{k}$ where $k$ is the smallest power of 2 enabling a dense projection without an Out-of-Memory error. For token embeddings, we find $k=32$ , whereas other layers require $k=128$ on a V100 GPU. Projecting to $\\mathbb{R}^{k}$ takes $\\sim31m s$ for token embeddings, resulting in a total of $\\sim4s$ to project to $\\mathbb{R}^{D}$ . Other layers require $\\sim24m s$ per projection to $\\mathbb{R}^{k}$ and $\\sim0.77s$ to project to $\\mathbb{R}^{D}$ , see Table 2 for a per-layer breakdown. Finally, an alternative approach to dense projections is to materialize dense matrices on the fly in chunks [19]; this has two substantial disadvantages: (1) the runtime scales linearly with the target dimension (memory is traded off with compute), and (2) specialized kernels are necessary for efficient implementation, with unclear applicability to TPUs; we include a demonstration of these limitations in Appendix A. ", "page_idx": 7}, {"type": "text", "text": "5.3 Analyzing the Impact of Design Choices ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we analyze the impact of the design choices presented in Sec. 3 on both sketching quality and performance. Regarding sketching quality, we find that small values of $D$ often lead to remarkably accurate reconstructions of influence scores. For TDA scores, achieving correlation $r\\geq0.95$ requires the following dimensions: FJL, FFD, AFFD: $D=2^{10}$ ; AFJL: $D=2^{12}$ ; QK: $D\\,=\\,2^{14}$ . To reach $r\\,\\geq\\,0.99$ , simply increase each dimension by a factor of 8. While memory limitations prevented sketching HVPs with FJL and FFD for eigenvalue estimation, the other algorithms scaled well. We achieved a relative mean absolute error below $5\\%$ with the following dimensions: AFFD: $D=2^{10}$ ; AFJL: $D=2^{12}$ ; QK: $D=2^{13}$ . Note that QK requires larger $D$ , consistent with the theoretical comparisons in Theorems 3.3 and 3.2 (Sec. 3). Regarding perfomance, we outline here the key findings and refer the reader to Appendix A for a comprehensive analysis ", "page_idx": 7}, {"type": "text", "text": "Table 3: Wall-time $T$ and peak memory usage $M$ comparison on gradient sketches for GPT-2.   \nRemoving look-ups is crucial for TPU performance and decreasing GPU memory utilization. ", "page_idx": 8}, {"type": "table", "img_path": "8jyCRGXOr5/tmp/aad7bc29a8cd514e4b84e8db63454227c4782bd8773914bcbacbf673dd9ef9a2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "8jyCRGXOr5/tmp/815b735c25d6d2c33dead572d37416e717601e0d7636f394ffea7e2dc92f5b28.jpg", "table_caption": ["Table 4: Speed-ups (ratio $R$ of the slowest wall-time to the fastest one) corresponding to changing a design choice (e.g. implicit to explicit or $H_{N}$ to the FFT.). "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "across design choices. First, removing look-ups significantly reduces wall-time on TPUs, while on GPUs it substantially lowers peak memory usage (reductions of $2.4\\mathbf{x}$ for FJL to AFJL, and $1.3\\mathrm{x}$ for FFD to AFFD), see Table 3. Second, explicit sketching consistently provides substantial speed-ups (see Table 4); we conjecture this is due to the fact that implicit sketching results in more memory accesses. Finally ,while modifying the pre-conditioner doesn\u2019t affect TPU performance, $i t$ significantly improves GPU performance, see Table 4 under the headings $H_{N}\\rightarrow\\mathrm{FFT}$ and $H_{N}\\rightarrow Q$ . ", "page_idx": 8}, {"type": "text", "text": "5.4 Estimating the intrinsic dimension. ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our experiments evaluate the efficiency and accuracy of our intrinsic dimension estimation algorithm (presented in Sec. 4). We consider two experimental setups: classification, where we fine-tune Roberta on SNLI with accuracy as the target metric; generation, where we fine-tune BART on XSUM for text summarization, using Rouge1 and Rouge2 for evaluation. We employ three projection algorithms: FFD, AFJL (FFT-based variant), and QK. We first validate algorithm consistency by demonstrate that the algorithm produces consistent estimates of the intrinsic dimension across multiple runs; we repeat each search experiment with three random seeds obtaining estimates within a factor of 2 (Appendix B). We then verify that the estimated $D^{*}$ accurately represents the intrinsic dimension $D_{i n t}$ by fine-tuning the model in a $D=D^{*}/2$ -dimensional subspace and ensuring that the target metric remains below the $\\tau_{90}$ threshold (details in Appendix B). We point out that selecting good search parameters $(c,\\delta)$ was relatively straightforward by observing target metric improvement during full fine-tuning . Regarding computational efficiency, our approach requires significantly fewer steps than binary or brute-force search. For instance, the XSUM search required only twice the steps of a full fine-tuning run and the same amount of steps for the classification task. We finally look at how the intrinsic dimension varies between classification and generation tasks. For classification, our findings are consistent with prior work [1], were all algorithms yield $D_{i n t}\\,=\\,2^{13}\\,\\ll\\,N$ . For generation we first observe that the results are metric-dependent: for Rouge1, FFD and QK estimate $D_{i n t}=2^{25}$ while AFJL yields $D=2^{24}$ ; for Rouge2, $D_{i n t}$ equals the full model dimension $N\\sim2^{27}$ . In both cases, however, the intrinsic dimension is close to the full model dimension: this challenges assumptions about intrinsic dimension in the context of generative task\u221as. While prior studies [14, 1, 17] focused on classification with generalization bounds of the form $O(\\sqrt{D_{i n t}})$ , our results indicate that: generative tasks may exhibit higher intrinsic dimension and generative models may require new non-linear projections to uncover significantly lower intrinsic dimensions. ", "page_idx": 8}, {"type": "image", "img_path": "8jyCRGXOr5/tmp/00a780bac96548112e83612eec7503bc6a7d217afee499c9d1048b1cad8a3184.jpg", "img_caption": ["Figure 2: left: ratio (RNEG) of the absolute value of the top negative to the top positive eigenvalue; right: ratio $R$ of the $n$ -th largest positive eigenvalue to the largest positive eigenvalue. We define outliers when $R>20\\%$ , motivated by [12, Fig.2]. Higher-resolution versions for printing can be found in Appendix A. These results disprove conjectures on the Hessian structure, see Sec. 5.5. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5.5 Hessian Analysis During Pre-trained Language Model Fine-Tuning ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this section, we examine the Hessian\u2019s evolution during pre-trained language model fine-tuning. Using sketching, as proposed in Section 4, we estimate the Hessian\u2019s eigenvalues and eigenvectors. We fine-tune Roberta and GPT-2L on SNLI (3 classes) and BART on XSUM, employing AFFD, with a target dimension $D\\,=\\,2^{20}$ , to construct a $1\\mathbf{k}$ -dimensional Krylov subspace. This substantially extends the work of [12] who considered smaller models and smaller subspaces (90- dimensional Krylov subspace). Spectrum estimation is performed every 1k steps, including initial steps $\\{0,10,50;100,150,200,250,500\\}$ . Our goal is to see if observations from previous studies with smaller networks hold in this context: ", "page_idx": 9}, {"type": "text", "text": "\u2022 Obs1: Negative eigenvalues gradually disappear during training [21, 12]. \u2022 Obs2: $K\\,-\\,1$ outlier eigenvalues emerge for $K$ -class classification, with the gradient aligning to their corresponding subspace [9, 21, 3]. Moreover, these outliers, which hinder optimization, stem from training without normalization [12]. ", "page_idx": 9}, {"type": "text", "text": "We find that these obervations don\u2019t fully translate to pre-trained language model fine-tuning. Regarding Obs1, we compute the ratio (RNEG) of the absolute values of the top negative and positive eigenvalues; RNEG shows inconsistent behavior across models (Fig.2 (a)). Roberta maintains a higher RNEG, while GPT-2L and BART see it diminish over time. Regarding Obs2, outlier eigenvalues don\u2019t strictly adhere to the $K-1$ rule. Roberta has more outliers (6), and the gradient-outlier alignment is less pronounced $27\\%$ Roberta, $35\\%$ GPT-2L, $8\\%$ BART) compared to smaller networks [12, 9]. See Fig. 2 (b). Moreover, outliers emerge despite layer normalization. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusions and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we have dissected the theoretical and practical limitations of existing gradient sketching techniques when applied to modern neural networks and accelerators. Our analysis motivated the design of novel sketching algorithms, for which we established theoretical guarantees; additionally, we exposed limitations in the theoretical underpinnings of the Fastfood transform. These methods, along with refined intrinsic dimension estimation and Hessian eigenvalue computation, provide an efficient toolkit for model analysis. We successfully apply this toolkit to pre-trained language models, revealing the need to rethink layer-selection-based influence functions, the high intrinsic dimensionality of a generative task, and the deviation of LLMs\u2019 Hessian spectra from what observed in smaller networks. While in Sec. 5.4 we exhibit an example of a generative task with a large intrinsic dimension, we leave an in-depth study for future work. We tested the efficiency of our sketching algorithms with Transformers in Sec. 5.3, but results might vary for other model architectures. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank Jonathan Heek for helping with optimizing code for TPUs and Katja Filippova for feedback on the draft. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Armen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer. Intrinsic dimensionality explains the effectiveness of language model Fine-Tuning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 7319\u20137328, Online, August 2021. Association for Computational Linguistics.   \n[2] Nir Ailon and Bernard Chazelle. The fast johnson-lindenstrauss transform and approximate nearest neighbors. SIAM Journal on Computing, 2009.   \n[3] Gerard Ben Arous, Reza Gheissari, Jiaoyang Huang, and Aukosh Jagannath. High-dimensional sgd aligns with emerging outlier eigenspaces, 2023.   \n[4] Krzysztof Choromanski and Francois Fagan. Fast nonlinear embeddings via structured matrices, 2016.   \n[5] Andrew Engel, Zhichao Wang, Natalie S. Frank, Ioana Dumitriu, Sutanay Choudhury, Anand Sarwate, and Tony Chiang. Faithful and efficient explanations for neural networks via neural tangent kernel surrogate models, 2024 (also accepted in ICLR24).   \n[6] Jillian Fisher, Lang Liu, Krishna Pillutla, Yejin Choi, and Zaid Harchaoui. Influence diagnostics under self-concordance. In Francisco Ruiz, Jennifer Dy, and Jan-Willem van de Meent, editors, Proceedings of The 26th International Conference on Artificial Intelligence and Statistics, volume 206 of Proceedings of Machine Learning Research, pages 10028\u201310076. PMLR, 2023. [7] Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez, Evan Hubinger, Kamil\u02d9e Luko\u0161i\u00afut\u02d9e, Karina Nguyen, Nicholas Joseph, Sam McCandlish, Jared Kaplan, and Samuel R. Bowman. Studying large language model generalization with influence functions, 2023. [8] Han Guo, Nazneen Rajani, Peter Hase, Mohit Bansal, and Caiming Xiong. FastIF: Scalable influence functions for efficient model interpretation and debugging. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10333\u201310350, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.   \n[9] Guy Gur-Ari, Daniel A. Roberts, and Ethan Dyer. Gradient descent happens in a tiny subspace, 2018.   \n[10] William B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert space, 1984.   \n[11] Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1885\u20131894. PMLR, 06\u201311 Aug 2017.   \n[12] Shankar Krishnan, Behrooz Ghorbani, and Xiao Ying. An investigation into neural net optimization via hessian eigenvalue density. In ICML, 2019.   \n[13] Quoc Le, Tamas Sarlos, and Alex Smola. Fastfood - approximating kernel expansions in loglinear time. In 30th International Conference on Machine Learning (ICML), 2013.   \n[14] Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the intrinsic dimension of objective landscapes. In International Conference on Learning Representations, 2018.   \n[15] Fanghui Liu, Xiaolin Huang, Yudong Chen, and Johan A K Suykens. Random features for kernel approximation: A survey on algorithms, theory, and beyond. IEEE Transactions on Pattern Analysis and Machine Intelligence, April 2020.   \n[16] Haokun Liu, Derek Tam, Muqeeth Mohammed, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. In Advances in Neural Information Processing Systems, 2022.   \n[17] Sanae Lotf,i Marc Anton Finzi, Sanyam Kapoor, Andres Potapczynski, Micah Goldblum, and Andrew Gordon Wilson. PAC-bayes compression bounds so tight that they can explain generalization. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.   \n[18] Michael W. Mahoney. Lecture notes on randomized linear algebra. Arxiv, 2016.   \n[19] Sung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Madry. Trak: Attributing model behavior at scale, 2023 (also accepted in ICML23).   \n[20] Garima Pruthi, Frederick Liu, Satyen Kale, and Mukund Sundararajan. Estimating training data influence by tracing gradient descent. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.   \n[21] Levent Sagun, Utku Evci, V. Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis of the hessian of over-parametrized neural networks, 2018.   \n[22] Tamas Sarlos. Improved approximation algorithms for large matrices via random projections. In 2006 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS\u201906), pages 143\u2013152, 2006.   \n[23] Andrea Schioppa, Katja Filippova, Ivan Titov, and Polina Zablotskaia. Theoretical and practical perspectives on what influence functions do, 2023 (also accepted in Neurips23).   \n[24] Andrea Schioppa, Polina Zablotskaia, David Vilar Torres, and Artem Sokolov. Scaling up influence functions. In AAAI-22, 2022.   \n[25] William Swartworth and David P Woodruff. Optimal eigenvalue approximation via sketching. STOC 2023: Proceedings of the 55th Annual ACM Symposium on Theory of Computing, June 2023.   \n[26] Roman Vershynin. High-Dimensional Probability. Cambridge University Press, 2018.   \n[27] David P Woodruff. Sketching as a tool for numerical linear algebra, November 2014.   \n[28] Chih-Kuan Yeh, Ankur Taly, Mukund Sundararajan, Frederick Liu, and Pradeep Ravikumar. First is better than last for language data influence. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, February 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A Appendix: Additional Experimental Results 13 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Additional results on layer selection 13   \nA.2 Quality of sketches for inner products and eigenvalue estimation 13   \nA.3 A closer look at FJL vs AFJL . . 14   \nA.4 Compute cost of sketching gradients . . 16   \nA.5 Compute cost of sketching HVPs . . . 18   \nA.6 Search for the intrinsic dimension 20   \nA.7 Higher resolution version of Fig. 2 . . 22   \nA.8 Comparison to on-the-fly dense random projections . . 22 ", "page_idx": 12}, {"type": "text", "text": "B Appendix: Implementation details 23 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "B.1 Libraries, Compute resources and implementation of FFD and FJL 23   \nB.2 HVP in Implicit vs Explicit Form . . . . . . 24   \nB.3 Step-by-step didactic implementation 24   \nB.4 Sketching and model parallelism . . . 28   \nB.5 Algorithm for searching the intrinsic dimension . . 30   \nB.6 Hyper-parameters for Sections 5.2 and 5.3 . . . 31   \nB.7 Hyper-parameters for Sec. 5.4 . . 31   \nB.8 Hyper-parameters for Sec. 5.5 31 ", "page_idx": 12}, {"type": "text", "text": "C Appendix: Theory 31 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "C.1 Definition of Higher order sketches . . 31   \nC.2 Guarantees on distorting distances. . . 31   \nC.3 Definition of the Walsh-Hadamard transform. 31   \nC.4 Failure of concentration for FFD. 32   \nC.5 Comparison to Kronecker products in [17] . . 32   \nC.6 Concentration result for QK: Theorem 3.3 . 32   \nC.7 Concentration result for AFFD . . 35 ", "page_idx": 12}, {"type": "text", "text": "A Appendix: Additional Experimental Results ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Additional results on layer selection ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In Table 5 we include the full results of layer selection of GPT-2 and BART: in Sec. 5.2 we restricted the discussion to GPT-2 because of space constraints; for the purpose of this experiment we consider the setup of [6]: NLP tasks which consists in fine-tuning GPT-2 on the WikiText-103 dataset and BART and zsRE. ", "page_idx": 12}, {"type": "text", "text": "A.2 Quality of sketches for inner products and eigenvalue estimation ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "For each algorithm, we report in Table 6 the minimal value of $\\log_{2}D$ necessary to reach a Pearson correlation $>0.9x$ with the ground truth when estimating inner products of gradients using sketches. As expected from the worse concentration bound, QK requires a larger dimension. We conjecture that the fact that FFD is effective might be due to the gradient distribution giving 0-measure to the inputs ", "page_idx": 12}, {"type": "text", "text": "Table 5: Most of the time layer selection results in unreliable estimates for influence scores and eigenvalue estimation. The best layer correlation with ground truth influence scores does not exceed $\\simeq90\\%$ and is quite low for most layers. The relative error in eigenvalue prediction is always at least $\\simeq20\\%$ . ", "page_idx": 13}, {"type": "table", "img_path": "8jyCRGXOr5/tmp/db6b1477e3116ebd8c09676d8e0bd136747a17e6a899ec2e60a1007b05b079b9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "Table 6: For each algorithm the minimal value of $\\log_{2}D$ necessary to reach a Pearson $r>x$ where $x=0.9\\{5,8,9\\}$   \nfor estimating inner products of gradients. ", "page_idx": 13}, {"type": "table", "img_path": "8jyCRGXOr5/tmp/1f81bfc21a358c168274dba65316d4b130d143ad1deaec52919a452c318cb16d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "that FFD would fail to sketch (Theorem 3.1). For AFFD, AFJL and QK we report in Table 7 the minimal value of $\\log_{2}D$ necessary to reach a relative mean absolute error err $<x$ when estimating the top 10 eigenvalues of the Hessian. ", "page_idx": 13}, {"type": "text", "text": "A.3 A closer look at FJL vs AFJL ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In Sec. 5.3 we pointed out that the FJL\u2019s wall time and memory usage increase with the target dimension $D$ . We compare the peak memory usage and the wall time of FJL with that of AFJL on the inner product task of Sec. 5.3 on GPU (V100), see Figures 3,4: FJL\u2019s cost significantly increases with $D$ and does not scale beyond $D=2^{20}$ . ", "page_idx": 13}, {"type": "text", "text": "Table 7: For each algorithm the minimal value of $\\log_{2}D$ necessary to reach a relative error $e r r<x$ where $x=0.2,0.1,0.05$ in reconstructing the top 10 eigenvalues. ", "page_idx": 13}, {"type": "table", "img_path": "8jyCRGXOr5/tmp/922734be6b3648f010eca70d19bcd21254f8bfbfdc6ff08b31d05689f04f10ea.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "image", "img_path": "8jyCRGXOr5/tmp/18b235fd568e86ef88fb6eb7c30ccfdd4a7b2e2e36981cafb5aaedd70d6f9cdc.jpg", "img_caption": ["Figure 3: Peak memory usage comparing FJL with AFJL. Results on GPU (V100); for FJL results with $D>2^{20}$ are not reported as there were Out-of-Memory errors. "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "8jyCRGXOr5/tmp/5fbf210d815b3563ce6948dfcd31d6466935e91ab74ec18afb036f9e4c486ce0.jpg", "img_caption": ["Figure 4: Wall time comparing FJL with AFJL. Results on GPU (V100); for FJL results with $D^{\\bar{\\mathbf{\\Gamma}}}\\!>2^{20}$ are not reported as there were Out-of-Memory errors. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "A.4 Compute cost of sketching gradients ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In Table 8 we report the compute costs of sketching gradients in the setup of Sec. 5.3. ", "page_idx": 15}, {"type": "text", "text": "Table 8: Compute costs of sketching gradients in the setup of Sec. 5.3. $_{\\mathrm{I}=1}$ denotes that a method is implicit, $_{\\mathrm{F}=1}$ that the FFT is used as a pre-conditioner instead of the Walsh-Hadamard Transform, and $\\mathrm{Q}{=}1$ that random orthogonal matrices decomposed as a Kronecker product are used as a preconditioner The lowest time and memory costs are in blue and bold and the highest ones in red and italic. nan indicates a result that was not computed because of XLA compilation errors. For this benchmark we considered a target dimension $D$ in the range $[2^{17},2^{22}]$ . GPU is V100, TPU is TPUv2. ", "page_idx": 16}, {"type": "image", "img_path": "8jyCRGXOr5/tmp/53bc0f6b530f5473f20f06a34f95e54c4aa6aa6cfab7d9e1e0532c5465fbb5cf.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "A.5 Compute cost of sketching HVPs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In Table 9 we report the compute costs of sketching HVPs in the setup of Sec. 5.3. ", "page_idx": 17}, {"type": "text", "text": "Table 9: Compute costs of HVP using sketching. $_{\\mathrm{I}=1}$ denotes that a method is implicit, $_{\\mathrm{F}=1}$ that the Fourier Transform is used instead of the Walsh-Hadamard Transform and $\\scriptstyle{\\mathrm{Q}}=1$ that random orthogonal matrices decomposed as a Kronecker product are used instead of Hadamard matrices. The lowest time and memory costs are in blue and bold and the highest ones in red and italic. GPU is V100, TPU is TPUv2. ", "page_idx": 18}, {"type": "image", "img_path": "8jyCRGXOr5/tmp/280560b518cd22d386eb44181f76c25f82179eaff7fe38955af38ab483eeb124.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "A.6 Search for the intrinsic dimension ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In Table 10 we report the values of $D^{*}$ across the 3 seeds used in each search experiment. We see good agreement within a factor of 2. For SNLI the target accuracy to exceed was $\\tau_{90}\\,=\\,80.1\\%$ ; for XSUM the value of Rouge1 to exceed was 36.6 while that of Rouge2 was 15.69. In the case of Rouge2 compressing BART in half would lead to a search with $D=2^{26}$ ; in that case the final values of Rouge2 did not exceed 14.4, so it stayed well-below the required threshold that defines the intrinsic dimension $D_{i n t}$ using a $90\\%$ target value of the metric obtained by fine-tuning the full model. ", "page_idx": 19}, {"type": "text", "text": "Table 10: Values of $D^{*}$ returned by the search the intrinsic dimension $D_{i n t}$ using 3 different seeds. This shows the stability of our algorithm which doubles the dimension of the fine-tuning subspace after some compute budget if the target metric has not improved enough. ", "page_idx": 20}, {"type": "table", "img_path": "8jyCRGXOr5/tmp/28607bf1a6833b8439bc7d551d11f91cbe931a8d38a233767954fdb8e7a5daec.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "8jyCRGXOr5/tmp/95120678be4a15de2cf4b4f10acf8d2833401cf45330b24e3a3e5c964447bcb0.jpg", "img_caption": ["Figure 5: ratio (RNEG) of the absolute value of the top negative to the top positive eigenvalue "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "8jyCRGXOr5/tmp/4825f0b86041ef587f2dd33d569d743f1365a175be793ef56d97578538db1cb6.jpg", "img_caption": ["Figure 6: ratio $R$ of the $n$ -th largest positive eigenvalue to the largest positive eigenvalue. We define outliers when $R>20\\%$ , motivated by [12, Fig.2] "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "A.7 Higher resolution version of Fig. 2 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Figs. 5 and 6 are high-resolution versions of Fig. 2. ", "page_idx": 21}, {"type": "text", "text": "A.8 Comparison to on-the-fly dense random projections ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "[19] proposes to materialize the full random projection on-the-fly in chunks (we will refer to this sketching method as TRAK, even though the full TRAK attribution method in [19] is more involved). This leads to two significant drawbacks: (1) the run-time scales linearly with the target dimension (memory traded off with compute), and (2) specialized kernels are necessary for efficient implementation, with unclear applicability to TPUs. Our attempts to implement a competitive version using pure JAX were unsuccessful due to the lack of control over memory allocation and placement. We have included a plot (Figure 7) and a table (Table 11) demonstrating the linear runtime growth of TRAK compared to the constant runtimes of AFFD and QK. The implementation challenges of TRAK are further highlighted by the fact that our Triton implementation does not outperform the original CUDA ", "page_idx": 21}, {"type": "image", "img_path": "8jyCRGXOr5/tmp/affdd60191283aa44efc33f1271112ea151de9890da797f7b61be2c1f3968c8e.jpg", "img_caption": ["Figure 7: Our methods exhibit constant wall time with respect to the target dimension $D$ . In contrast, TRAK\u2019s runtime increases with the target dimension. Efficient implementation of dense random projections with recomputed projectors is non-trivial; compare the performance difference between TRAK[CUDA] and TRAK[Triton]. TRAK[CUDA] utilizes the CUDA kernel provided by the original TRAK authors [19]. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Table 11: Wall-time $T$ (ms) Comparison: Our Methods vs. On-the-fly Dense Projections (TRAK) using a ${\\bf V}100$ GPU. TRAK requires custom kernels and is thus restricted to GPU computation. Our methods exhibit constant runtime with respect to the target dimension, whereas TRAK\u2019s runtime increases substantially as the target dimension grows. ", "page_idx": 22}, {"type": "table", "img_path": "8jyCRGXOr5/tmp/9a50fd53698c83bf25a33ed43f4418e6898c4f82ec7ff50a8ad4f9779972b1d1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "kernel released by the TRAK authors, underscoring the difficulty of efficiently implementing random projections in chunks. ", "page_idx": 22}, {"type": "text", "text": "B Appendix: Implementation details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "B.1 Libraries, Compute resources and implementation of FFD and FJL ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We use Jax and Hugging Face libraries; experiments in Sec. 5.2 were carried out using one GPU V100 or a TPUv2 (8 cores). Experiments in Sec. 5.4 used 2 V100s in the classification setting and 2 A100s in the generation setting. Experiments in Sec. 5.5 used 2 A100s. Experiments were performed on cloud infrastructure and the virtual machines employed for each experiment had at most 32GB of RAM. We used the Jax profiler tool to extract information about peak memory usage and wall time. We checked that under multiple runs the wall time estimates reported by the profiler tool stay within a $10\\%$ relative error, while the peak memory usage does not significantly change. For FJL and FFD the lookup operation is implemented as in previous work [1] storing the permutations or entries to sample using a vector. Regarding permutations there is an important implementation detail: if one wants the implementation of FFD to give the same results in implicit and explicit mode the permutation used needs to be inverted between the two setups. For FJL, in implicit mode the lookup operation needs to be transposed and is implemented as an XLA scatter-add. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "B.2 HVP in Implicit vs Explicit Form ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The implicit implementation of the HVP is easy; one changes the loss to be defined on the image of the projection $\\Phi$ as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\nL_{i m p l i c i t}(\\omega)=L(\\theta_{0}+\\Phi^{T}\\omega),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and then one computes the HVP at the origin $\\omega=0$ . For the explicit form of the HVP, one takes a vector $\\nu$ in the image of $\\Phi$ and lifts it to the vector $\\Phi^{T}\\nu$ in the parameter space; then one computes the standard HVP for $\\Phi^{T}\\nu$ and applies $\\Phi$ to the result obtaining again a vector in the image of $\\Phi$ . ", "page_idx": 23}, {"type": "text", "text": "B.3 Step-by-step didactic implementation ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We describe a step-by-step didactic implementation in Jax and Flax. ", "page_idx": 23}, {"type": "text", "text": "First, we rely on the following modules. ", "page_idx": 23}, {"type": "text", "text": "import jax   \nimport jax.numpy as jnp   \nfrom typing import Sequence , Tuple   \nfrom flax.core import apply   \nfrom flax.core import init   \nfrom flax.core import nn   \nimport scipy   \nimport functools ", "page_idx": 23}, {"type": "text", "text": "The core subroutine is applying pre-conditioners using Kronecker products. One can use the Jax\u2019 einsum operation to write it in a few lines of code. ", "page_idx": 23}, {"type": "text", "text": "Listing 1: Kronecker Implemetation ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "def compute_kronecker_shapes(\u2217, dimension: int) \u2212> Tuple[int, ...]: \"\"\"Computes shapes to decompose Kronecker products.\"\"\" # For realistic use cases, bump it up, e.g. 1024 max_block_size = 32 $\\mathrm{~\\bf~n~}=$ dimension # Divide n into block sizes shape $\\mathbf{\\mu}=\\mathbf{\\mu}\\left[\\begin{array}{l}{\\right]\\;\\;\\;$ while $\\texttt{n}>\\texttt{1}$ : shape.append(min(n, max_block_size)) n $//=$ max_block_size shape.reverse() return tuple(shape) ", "page_idx": 23}, {"type": "text", "text": "def kronecker_product( \u2217, x: jnp.ndarray , matrices: Sequence[jnp.ndarray]   \n) $->$ jnp.ndarray: \"\"\"Performs kronecker product using Jax einsum.\"\"\" shape $=$ tuple(map(lambda x: x.shape[0], matrices)) $\\textbf{y}=\\textbf{x}$ .reshape(shape) num_dims $=$ len(shape) # Einsum iterative implementation. for i, m in enumerate(matrices): y_dims $=$ \u2019 \u2019.join(str(j) for j in range(num_dims)) h_dims $=$ f \u2019 { i }{ num_dims + 1} ", "page_idx": 23}, {"type": "text", "text": "out_dims $=$ y_dims.replace(str(i), str(num_dims + 1), 1) operands $=$ f \u2019 { y_ d im s } , { h _ d i m s }\u2212>{ o u t _ d i m s } \u2019 $\\texttt{y}=$ jnp.einsum(operands , y, m) return y.flatten() ", "page_idx": 24}, {"type": "text", "text": "The next step is implementation of QK which is quite straightforward. ", "page_idx": 24}, {"type": "text", "text": "", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Listing 2: QK Implemetation   \ndef q_init(rng, shape, dtype $=$ jnp.float32): \"\"\"Random orthogonal matrix initializer.\"\"\" x = jax.random.normal(rng, shape $=$ shape) q, _ $=$ jnp.linalg.qr(x, mode $=$ \u2019 c o m p l e t e \u2019) return q.astype(dtype)   \n# target_dim $\\mathit{\\mathrm{~\\varepsilon~}}=\\mathrm{~\\mathbb{D}~}$ in paper   \n# input_dim $=$ N in paper   \n# vec is the N\u2212vector to sketch to a D\u2212vector.   \ndef qk(scope, vec, target_dim , input_dim): \"\"\"Implements QK.\"\"\" shapes $=$ compute_kronecker_shapes(dimension $\\equiv$ input_dim) sigma $=$ jnp.sqrt(input_dim/target_dim) params $\\mathrm{~\\textbf~{~\\,~}~}=\\mathrm{~\\textbf~{~\\displaystyle~[~]~}~}$ for i, s in enumerate(shapes): p $=$ scope.param(f \u2019 q_ { i } \u2019 , q_init, shape $=$ (s, s)) params.append(p) return sigma $^*$ kronecker_product( $\\mathtt{X}\\!=\\!\\mathtt{v e c}$ , matrices $=$ params)[: target_dim] ", "page_idx": 24}, {"type": "text", "text": "For using QK in implicit mode we need to transpose $\\mathbf{QK}$ ; we illustrate how this can be carried out in Flax: ", "page_idx": 24}, {"type": "text", "text": "Listing 3: Transpose of QK ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "# The tranpose of QK.   \n# vec is a D\u2212vector to lift to an N\u2212vector.   \ndef qk_transpose(scope, vec, target_dim , input_dim): shapes_ $\\texttt{1}=$ compute_kronecker_shapes(dimension $\\equiv$ input_dim) shapes_ $.2\\ =$ compute_kronecker_shapes(dimension $=$ target_dim) sigma $=$ jnp.sqrt(input_dim/target_dim) params $\\mathrm{~\\textbf~{~\\,~}~}=\\mathrm{~\\textbf~{~\\displaystyle~[~]~}~}$ for i, (s_1, s_2) in enumerate(zip(shapes_1 , shapes_2)): p = scope.param(f \u2019 q_ { i } \u2019 , q_init, shape $=$ (s_1, s_1)) params.append(p.T[:s_2]) return sigma $^*$ kronecker_product( $\\mathtt{X}\\!=\\!\\mathtt{v e c}$ , matrices=params) ", "page_idx": 24}, {"type": "text", "text": "The implementation of other algorithms is more challenging; we include an implementation of AFFD; it is easy to figure out the implementations of the others (note that for FFD one needs to start with the implementation of the transpose because FFD is defined as a random feature generator). ", "page_idx": 24}, {"type": "text", "text": "Listing 4: Implementation of AFFD ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "def init_hadamard(rng, shape: Tuple[int, int], permute_col: bool) \"\"\u2212\">C rSeeaqtueesn crea[njdnopm.lnyd aprerramyu]t:ed hadamard matrices.\"\"\" matrix $=$ jnp.array(scipy.linalg.hadamard(shape[0])) pi $=$ jax.random.permutation(rng, shape[0]) if permute_col: matrix $=$ matrix.at[:, pi].get() else: # permute rows ", "page_idx": 24}, {"type": "text", "text": "matrix $=$ matrix.at[pi, :].get() ", "page_idx": 25}, {"type": "text", "text": "return matrix ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "# target_dim $\\mathit{\\mathrm{~\\varepsilon~}}=\\mathrm{~\\mathbb{D}~}$ in paper   \n# input_dim $\\mathbf{\\mu}=\\mathbf{\\mu}\\mathbb{N}$ in paper   \n# vec is the N\u2212vector to sketch to a D\u2212vector.   \ndef affd(scope, vec, target_dim , input_dim): \"\"\"Implements AFFD.\"\"\" ", "page_idx": 25}, {"type": "text", "text": "shapes $=$ compute_kronecker_shapes(dimension $\\equiv$ input_dim) sigma $=$ jnp.sqrt(input_dim/target_dim) ", "page_idx": 25}, {"type": "text", "text": "def init_ber(rng, shape, dtype $=$ jnp.int32): return jax.random.choice(rng, jnp.array([\u22121, 1], dtype $=$ \u2019 i n t 3 2 \u2019 ), ", "page_idx": 25}, {"type": "text", "text": "shape) ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "b = scope.param( \u2019B \u2019 , init_ber , (input_dim ,))   \nvec $=$ vec \u2217 b   \nh_1_params $=$ []   \nfor i, s in enumerate(shapes): h_1 $=$ scope.param(f \u2019 H_1_ { i } \u2019 , init_hadamard , shape $=$ (s, s), permute_col $=$ False) ", "page_idx": 25}, {"type": "text", "text": "vec $=$ kronecker_product( $\\scriptstyle{\\mathtt{X}}={\\mathtt{V}}\\,\\Theta\\,c$ , matrices $=$ h_1_params) / jnp.sqrt( input_dim) ", "page_idx": 25}, {"type": "text", "text": "def init_gauss(rng, shape, dtype $=$ jnp.float32): return jax.random.normal(rng, shape $=$ shape, dtype $=$ dtype) ", "page_idx": 25}, {"type": "text", "text": "${\\textbf{\\textsf{g}}}=$ scope.param( \u2019G \u2019 , init_gauss , (input_dim ,)) vec $=$ vec \u2217 g ", "page_idx": 25}, {"type": "text", "text": "h_2_params $\\mathbf{\\mu}=\\mathbf{\\mu}\\left[\\begin{array}{l}{\\right]\\;\\;\\;$   \nfor i, s in enumerate(shapes): $\\mathtt{h\\_2}=$ scope.param(f \u2019 H_2_ { i } \u2019 , init_hadamard , shape $=$ (s, s), permute_col $=$ True) h_2_params.append(h_2) ", "page_idx": 25}, {"type": "text", "text": "vec $=$ kronecker_product( $\\scriptstyle{\\mathtt{X}}={\\mathtt{V}}\\,\\Theta\\,c$ , matrices $=$ h_2_params) / jnp.sqrt( input_dim) ", "page_idx": 25}, {"type": "text", "text": "vec $=$ sigma $^*$ vec[:target_dim] ", "page_idx": 25}, {"type": "text", "text": "return vec ", "page_idx": 25}, {"type": "text", "text": "To transpose AFFD, we just need to reverse the above steps and transpose application of the Hadamard matrices. ", "page_idx": 25}, {"type": "text", "text": "Listing 5: Transpose of AFFD ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "# The tranpose of AFFD. # vec is a D\u2212vector to lift to an N\u2212vector. def affd_transpose(scope, vec, target_dim , input_dim): ", "page_idx": 25}, {"type": "text", "text": "shapes_ $\\mathrm{~\\textit~{~1~}~}=$ compute_kronecker_shapes(dimension $\\equiv$ input_dim) shapes_ $\\_2=$ compute_kronecker_shapes(dimension $\\equiv$ target_dim) sigma $=$ jnp.sqrt(input_dim/target_dim) ", "page_idx": 25}, {"type": "text", "text": "h_2_params $\\mathbf{\\mu}=\\mathbf{\\mu}\\left[\\begin{array}{l}{\\right]\\;\\;\\;$ for i, (s_1, s_2) in enumerate(zip(shapes_1 , shapes_2)): h_2 = scope.param(f \u2019 H_2_ { i } \u2019 , init_hadamard , shape $=$ (s_1, s_1), ", "page_idx": 25}, {"type": "text", "text": "permute_col $=$ True) h_2_params.append(h_2.T[:s_2]) ", "page_idx": 26}, {"type": "text", "text": "vec $=$ kronecker_product( $\\scriptstyle{\\mathtt{X}}={\\mathtt{V}}\\,\\Theta\\,c$ , matrices $=$ h_2_params) / jnp.sqrt( input_dim) ", "page_idx": 26}, {"type": "text", "text": "def init_gauss(rng, shape, dtype $=$ jnp.float32): return jax.random.normal(rng, shape $=$ shape, dtype $=$ dtype)   \n${\\textbf{\\textsf{g}}}=$ scope.param( \u2019G \u2019 , init_gauss , (input_dim ,))   \nvec $=$ vec \u2217 g   \nh_1_params $\\mathbf{\\mu}=\\mathbf{\\mu}\\left[\\begin{array}{l}{\\right]\\;\\;\\;$   \nfor i, s in enumerate(shapes_1): $\\mathtt{h\\_1}=$ scope.param(f \u2019 H_1_ { i } \u2019 , init_hadamard , shape $=$ (s, s), permute_col $=$ False) h_1_params.append(h_1.T)   \nvec $=$ kronecker_product( $\\scriptstyle{\\mathtt{X}}={\\mathtt{V}}\\,\\Theta\\,c$ , matrices $=$ h_1_params) / jnp.sqrt( input_dim)   \ndef init_ber(rng, shape, dtype $=$ jnp.int32): return jax.random.choice(rng, jnp.array([\u22121, 1], dtype $=$ \u2019 i n t 3 2 \u2019 ), shape)   \nb = scope.param( \u2019B \u2019 , init_ber , (input_dim ,))   \nvec = vec \u2217 b   \nvec = vec \u2217 sigma ", "page_idx": 26}, {"type": "text", "text": "return vec ", "page_idx": 26}, {"type": "text", "text": "We now turn to a didactic implement of sketching gradients of a loss functions in implicit and explicit mode. We first make an assumption about the signature of loss and sketching functions ", "page_idx": 26}, {"type": "text", "text": "def loss_fn(model_params , batch): \"\"\"Loss function signature.\"\"\" ", "page_idx": 26}, {"type": "text", "text": "pass ", "page_idx": 26}, {"type": "text", "text": "def sketch_fn(sketch_params , vec): \"\"\"Sketch function signature.\"\"\" ", "page_idx": 26}, {"type": "text", "text": "pass ", "page_idx": 26}, {"type": "text", "text": "def transpose_sketch_fn(sketch_params , vec): \"\"\"Transpose of sketch_fn signature.\" 11 11 11 ", "page_idx": 26}, {"type": "text", "text": "pass ", "page_idx": 26}, {"type": "text", "text": "Then here\u2019s how one can implement explicit and implicit gradient sketching in a few lines of code. ", "page_idx": 26}, {"type": "text", "text": "Listing 6: Implementation of Implicit and Explicit Gradient Sketching def explicit_grad_sketch(model_params , sketch_params , batch): \"\"\"Performs an explicit gradient sketch.\"\"\" ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "grad $=$ jax.grad(loss_fn)(model_params , batch) return sketch_fn(sketch_params , grad) ", "page_idx": 26}, {"type": "text", "text": "def implicit_grad_sketch(model_params , sketch_params , batch, target_dim): \"\"\"Performs an implicit gradient sketch.\"\"\" ", "page_idx": 26}, {"type": "text", "text": "def inner_loss_fn(omega): omega $=$ transposed_sketch_fn(sketch_params , omega) return loss_fn(model_params $^+$ omega, batch) ", "page_idx": 26}, {"type": "text", "text": "omega $=$ jnp.zeros((target_dim ,)) grad $=$ jax.grad(inner_loss_fn)(omega) ", "page_idx": 27}, {"type": "text", "text": "return grad ", "page_idx": 27}, {"type": "text", "text": "Here\u2019s how one can do the same for the sketched HVP. ", "page_idx": 27}, {"type": "text", "text": "Listing 7: Implementation of Implicit and Explicit HVP Sketching ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "# Note tangent_params is a D\u2212dimensional vector   \ndef explicit_hvp_sketch(model_params , tangent_params , sketch_params , batch): \"\"\"Performs an explicit HVP sketch.\"\"\"   \ntangent_params $=$ transposed_sketch_fn(sketch_params , tangent_params)   \nloss_ $=$ functools.partial(loss_fn , batch $\\equiv$ batch)   \ngrad_fn $=$ jax.grad(loss_)   \nhvp $=$ jax.jvp(grad_fn , (model_params ,), (tangent_params ,))[1]   \nreturn sketch_fn(sketch_params , hvp) ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "# Note tangent_params is a D\u2212dimensional vector def implicit_hvp_sketch(model_params , tangent_params , sketch_params , batch, target_dim): \"\"\"Performs an implicit HVP sketch.\" 11 1I ", "page_idx": 27}, {"type": "text", "text": "def inner_loss_fn(omega): omega $=$ transposed_sketch_fn(sketch_params , omega) return loss_fn(model_params $^+$ omega, batch) ", "page_idx": 27}, {"type": "text", "text": "B.4 Sketching and model parallelism ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We take the case of AFFD, and show how the single device code may be lifted to code employing model parallelism. ", "page_idx": 27}, {"type": "text", "text": "First, we rely on the additional modules. ", "page_idx": 27}, {"type": "text", "text": "from jax.sharding import NamedSharding   \nfrom jax.experimental import shard_map   \nfrom jax.sharding import Mesh   \nfrom jax.sharding import PartitionSpec as P   \nfrom jax.sharding import NamedSharding as NS   \nfrom jax import lax   \nfrom jax import tree_util as tu   \nimport numpy as np ", "page_idx": 27}, {"type": "text", "text": "We then define the device mesh; we assume 8 cores with 4-way model parallelism and 2-way data parallelism. ", "page_idx": 27}, {"type": "image", "img_path": "8jyCRGXOr5/tmp/29ebee6c17c72300a501290fc27c1d16c092ffd1bc999318eb007562cbf741d6.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "We now lift initialization and application of the FlaX modules to model-parallel code: ", "page_idx": 27}, {"type": "text", "text": "# target_dim $\\mathit{\\mathrm{~\\varepsilon~}}=\\mathrm{~\\mathbb{D}~}$ in paper   \n# input_dim $\\mathbf{\\mu}=\\mathbf{\\mu}\\mathbb{N}$ in paper   \n# vec is the N\u2212vector to sketch to a D\u2212vector. ", "page_idx": 28}, {"type": "text", "text": "def part_fn(pytree): \"\"\"Partitions each parameter on the last dimension.\"\"\" ", "page_idx": 28}, {"type": "text", "text": "def inner_part_fn(p): out $=$ (None,) $^*$ (p.ndim\u22121) + ( \u2019 model \u2019 ,) return P( $^*$ out)   \nreturn tu.tree_map(inner_part_fn , pytree) ", "page_idx": 28}, {"type": "text", "text": "def init_affd_mp(scope, vec, target_dim , input_dim): \"\"\"Initializes AFFD for model\u2212parallel code.\"\"\" ", "page_idx": 28}, {"type": "text", "text": "# bind dimensional arguments to make jax tracer happy with   \n# jax.eval_shape.   \naffd_init_fn $=$ functools.partial( init(affd), target_dim $=$ target_dim , input_dim $=$ input_dim)   \n_, params_shape $=$ jax.eval_shape(affd_init_fn , rng, vec)   \nparams_part $=$ part_fn(params_shape)   \n# We need to redefine the input_dim because the code   \n# is now executed on each model partition.   \naffd_init_fn $=$ functools.partial( init(affd), target_dim $=$ target_dim , input_dim $=$ input_dim // mesh.shape[ \u2019 model \u2019 ]) ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "def init_fn(rng, vec): # different rng on each model slice rng $=$ jax.random.fold_in(rng, lax.axis_index( \u2019 model \u2019)) out, params $=$ affd_init_fn(rng, vec) # The vector output is fully replicated and we need to sum # on the \u2019model\u2019 partitions out $=$ lax.psum(out, axis_name $=$ \u2019 model \u2019) return out, params ", "page_idx": 28}, {"type": "text", "text": "return shard_map.shard_map( init_fn , mesh $=$ mesh, in_specs $=$ (P(None,), part_fn(vec)), out_specs $=$ (P(None,), params_part), )(rng, vec) ", "page_idx": 28}, {"type": "text", "text": "def apply_affd_mp(params, vec, target_dim , input_dim): \"\"\"Applies AFFD for model\u2212parallel code.\"\"\" ", "page_idx": 28}, {"type": "text", "text": "# We need to redefine the input_dim because the code # is now executed on each model partition. ", "page_idx": 28}, {"type": "text", "text": "affd_apply_fn $=$ functools.partial( apply(affd), target_dim $=$ target_dim , input_dim $=$ input_dim // mesh.shape[ \u2019 model \u2019 ]) ", "page_idx": 28}, {"type": "text", "text": "def apply_fn(params, vec): out $=$ affd_apply_fn(params, vec) # The vector output is fully replicated and we need to sum # on the \u2019model\u2019 partitions out $=$ lax.psum(out, axis_name $=$ \u2019 model \u2019) return out ", "page_idx": 28}, {"type": "text", "text": "return shard_map.shard_map( apply_fn , mesh $=$ mesh, in_specs $=$ part_fn((params, vec)), out_specs $\\mathsf{\\Gamma}=\\mathsf{P}$ (None,), )(params, vec) ", "page_idx": 29}, {"type": "text", "text": "We now illustrate how to use the previous code. ", "page_idx": 29}, {"type": "text", "text": "# We create the vector x on a single device and partition   \n# it on the model axis.   \nrng $=$ jax.random.PRNGKey(0)   \ntarget_dim $=~128$   \ninput_dim $=\\ 1024$   \n${\\texttt{x}}=$ jax.random.normal(jax.random.fold_in(rng, 1), shape $=$ (input_dim ,)) ", "page_idx": 29}, {"type": "text", "text": "affd_x_mp , affd_params_mp $=$ mesh(init_affd_mp)( jax.random.fold_in(rng, 3), x_mp, target_dim , input_dim) ", "page_idx": 29}, {"type": "text", "text": "# Consistency check affd_x_mp_2 $=$ mesh(apply_affd_mp)( affd_params_mp , x_mp, target_dim , input_dim) assert affd_x_mp_2.shape $==$ affd_x_mp.shape jnp.linalg.norm(affd_x_mp_2 \u2212 affd_x_mp) ", "page_idx": 29}, {"type": "text", "text": "B.5 Algorithm for searching the intrinsic dimension ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Our algorithm for searching the intrinsic dimension is in Listing 9. Without loss of generality we assume that the target metric needs to be maximized, e.g. for the loss one might use the negative loss. ", "page_idx": 29}, {"type": "text", "text": "Listing 9: An algorithm that searches the intrinsic dimension def finetune(model_params , D_max: int, d: int, c: int): \"\"\"Finetune function signature. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "Finetunes for c steps in D_max dimensional subspace but zeros out the last D_max \u2212 d components of the gradient. Returns the updated model_params.   \n11 11 11 ", "page_idx": 29}, {"type": "text", "text": "pass ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "def evaluate(model_params): \"\"\"Eval function signature.\" 11 11 11 ", "page_idx": 29}, {"type": "text", "text": "pass ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "def search_intrinsic_dimension( model_params , D_min: int, D_max: int,tau_target: float, c: int, delta: float): 11 11 11 ", "page_idx": 29}, {"type": "text", "text": "model_params: initial model parameters.   \nD_min: start value for the intrinsic dimension.   \nD_max: maximum allowed value of the intrinsic dimension.   \ntau_target: desired target metric.   \nc: number of finetuning steps in which we expect improvement.   \ndelta: minimum expected improvement.   \n11 11 1I   \nd = D_min   \ntau_old $=$ evaluate(model_params)   \nwhile True: model_params $=$ finetune(model_params , D_max, d, c) ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "B.6 Hyper-parameters for Sections 5.2 and 5.3 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Models were trained with the released code from [6]; then checkpoints were converted to Jax for benchmarking the sketching algorithms. ", "page_idx": 30}, {"type": "text", "text": "B.7 Hyper-parameters for Sec. 5.4 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Roberta was fine-tuned with a batch size of 32 for 10k steps with Adam and a constant learning rate of $2\\times10^{-5}$ . For the search algorithm 9 the learning rate was increased to $10^{-4}$ , $\\delta=0.1$ and $c=2k$ steps. ", "page_idx": 30}, {"type": "text", "text": "BART was fine-tuned with a batch size of 32 for 20k steps with Adam and a constant learning rate of $2\\times10^{-5}$ . For the search algorithm 9 the learning rate was increased to $10^{-4}$ ; $\\delta_{R o u g e1}=0.5\\$ , $\\delta_{R o u g e2}=0.5\\$ and $c=2k$ steps; the total number of steps was increased to 40k. ", "page_idx": 30}, {"type": "text", "text": "B.8 Hyper-parameters for Sec. 5.5 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We consider the SGD optimizer as in previous work [12]; the batch size was 32, the learning rate set to $10^{-5}$ . ", "page_idx": 30}, {"type": "text", "text": "C Appendix: Theory ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "C.1 Definition of Higher order sketches ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "For higher-order derivatives of $L$ , one can consider sketches of operators. For example the Hessian vector product is the operator $\\mathrm{HVP}:\\mathbb{R}^{N}\\rightarrow\\mathbb{R}^{N}$ given by $\\mathrm{HVP}(\\dot{\\boldsymbol{u}})=\\nabla^{2}{\\cal L}(\\theta)(u)$ , i.e. $\\mathrm{HVP}(u)_{i}=$ $\\textstyle\\sum_{j}{\\partial_{i,j}^{2^{\\cdot}}L(\\theta)u_{j}}$ . A sketch of the Hessian vector product can then obtained as follows: $S(O)(v)=$ $\\Phi(\\mathrm{HVP}(\\Phi^{T}v))$ where $\\boldsymbol{v}\\in\\mathbb{R}^{D}$ so that we obtain an operator mapping $\\mathbb{R}^{D}\\to\\mathbb{R}^{D}$ . Sketches of matrices were extensively studied [22] to speed-up evaluation of matrix products. Extending the HVP-example, for an operator $O$ mapping $\\ensuremath{\\mathbb{R}}^{k N}$ to $\\mathbb{R}^{s N}$ the transpose $\\Phi^{T}$ is applied to the $k$ input indices and $\\Phi$ is applied to the output $s$ indices to obtain a mapping $S(O):\\mathbb{R}^{k\\^{\\bullet}D^{\\bullet}}\\!\\rightarrow\\mathbb{R}^{s D}$ via: ", "page_idx": 30}, {"type": "equation", "text": "$$\n(\\boldsymbol{S}(\\boldsymbol{O})\\boldsymbol{v})_{l_{1}\\cdots l_{s}}=\\sum_{t_{1}\\cdots t_{s}=1}^{N}\\sum_{i_{1}\\cdots i_{k}=1}^{N}\\sum_{j_{1}\\cdots j_{k}=1}^{D}\\prod_{\\beta=1}^{s}\\Phi_{l_{\\beta},t_{\\beta}}\\cdot O_{i_{1}\\cdots i_{k};t_{1}\\cdots t_{s}}\\cdot\\prod_{\\alpha=1}^{k}\\Phi_{j_{\\alpha},i_{\\alpha}}\\cdot v_{j_{1}\\cdots j_{k}}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "C.2 Guarantees on distorting distances. ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "By the method of Johnson-Lindenstrauss [10] one can leverage the equation about concentration of the sketched norm (1) to prove that, given $M$ points in $\\mathbb{R}^{N}$ , the distances between their sketches in $\\mathbb{R}^{D}$ are distorted by at most a multiplicative factor $1\\pm\\varepsilon$ . The point is that concentration arguments establish, for the $\\delta$ appearing in (1), a bound of the form $\\delta=\\mathcal{O}(\\exp(-\\varepsilon^{2}\\beta^{2}))$ : one can thus apply (1) to the $\\textstyle{\\frac{M(M-1)}{2}}$ differences between points by requiring that $\\frac{\\beta}{\\sqrt{\\log M}}$ is sufficiently large; this is a considerable gain replacing a bound in terms of $M^{2}$ with one involving $\\log M$ . ", "page_idx": 30}, {"type": "text", "text": "C.3 Definition of the Walsh-Hadamard transform. ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "The Fastfood paper [13] defines $H_{N}$ without scaling, so it is not an isometry; however we follow the definition with scaling as in Wikipedia so that $H_{N}$ is an isometry. Specifically, $N$ needs to be a power of 2; then for $i\\leq N$ let $\\Delta(i)$ denote the vector of 0s and 1s and of length $\\log_{2}N$ , representing $i$ in its binary form; then $\\begin{array}{r}{(H_{N})_{i,j}=\\frac{1}{\\sqrt{N}}(-1)^{\\left\\langle\\Delta(i),\\Delta(j)\\right\\rangle}}\\end{array}$ , where $\\langle,\\rangle$ denotes the inner product. ", "page_idx": 30}, {"type": "text", "text": "C.4 Failure of concentration for FFD. ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Theorem C.1. There are some inputs x such that $F F D(x)$ does not satisfy (1). ", "page_idx": 31}, {"type": "text", "text": "Proof. Intuitively, the problem with FFD is that transposition fails to apply the pre-conditioner to some bad inputs. Recall that the FFD, when used for sketching, gets transposed because it is applied in implicit form, i.e. generating random features that perturb the model parameters. The transposed operation of concatenation gives rise to a sum; specifically, given a unit vector $\\boldsymbol{x}\\in\\mathbb{R}^{N}$ , we decompose it into $N/D$ -blocks of size $D$ , denoting the $b$ -th block by $x_{b}$ . Then ", "page_idx": 31}, {"type": "equation", "text": "$$\n{\\bf F}{\\bf F}{\\bf D}(x)=\\sigma\\sum_{b=1}^{N/D}B_{b}H\\Pi_{b}G_{b}H(x_{b});\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "then choose $x$ such that all blocks $x_{b}\\,=\\,0$ for $b>1$ and $x_{1}$ is such that $H(x_{1})\\,=\\,e_{1}$ , the first coordinate vector in $\\mathbb{R}^{D}$ . Then ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\|\\mathbf{F}\\mathbf{F}\\mathbf{D}(x)\\|_{2}^{2}=\\sigma^{2}g_{1}^{2},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $g_{1}$ is the first entry of $G_{v}$ ; then we must have $\\sigma^{2}=1$ and $\\|\\mathbf{F}\\mathbf{F}\\mathbf{D}(x)\\|_{2}$ cannot concentrate around 1 because $g_{1}$ has unit variance. \u53e3 ", "page_idx": 31}, {"type": "text", "text": "C.5 Comparison to Kronecker products in [17] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "[17] proposes two projection operators. The first one is ", "page_idx": 31}, {"type": "equation", "text": "$$\nP_{\\oplus}=\\sigma\\cdot(I\\otimes R_{1}+R_{2}\\otimes I)\\,\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $I$ is a \u221avector of ones in $\\mathbb{R}^{\\sqrt{N}}$ and $R_{i}$ is Gaussian of shape $D\\times\\sqrt{N}$ so that the memory cost of $P_{\\oplus}$ is $O(D\\sqrt{N})$ . The second one is ", "page_idx": 31}, {"type": "equation", "text": "$$\nP_{\\otimes}=\\sigma\\cdot Q_{1}\\otimes Q_{2},\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $Q_{i}$ is Gaussian of shape ${\\sqrt{D}}\\times{\\sqrt{N}}$ so that the memory cost of $P_{\\otimes}$ is $O(\\sqrt{D}\\sqrt{N})$ . Our QK proposal is more general as it calls for a more general Kronecker decomposition ", "page_idx": 31}, {"type": "equation", "text": "$$\nQ=\\sigma\\cdot Q^{(1)}\\otimes Q^{(2)}\\otimes\\cdot\\cdot\\cdot\\otimes Q^{(K)};\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $Q^{(i)}$ is of shape $D_{i}\\times B_{i}$ , $\\prod_{i}D_{i}=D$ (reconstruction of the target dime\u221ansion) and $\\prod_{i}B_{i}=N$ (reconstruction of the model dimension). So if we choose $K=2$ , $D_{i}=\\sqrt{D}$ and $B_{i}=\\sqrt{N}$ we recover $P_{\\otimes}$ . Strictly speaking, $P_{\\oplus}$ is different from $P_{\\otimes}$ , but one might reconstruct it by averaging two\u221a of our $Q$ \u2019s (16), both defined with with $K=2$ : indeed, we select $Q_{1}$ where $\\boldsymbol{Q}^{(1)}$ \u221ais the vector $I/\\sqrt{N}$ as in (14) and $Q^{(2)}$ is of shape $D\\times\\sqrt{N}$ ; we then select $Q_{2}$ where $Q^{(2)}$ is $I/\\sqrt{N}$ and $\\boldsymbol{Q}^{(1)}$ is of shape $D\\times\\sqrt{N}$ ; in other words, to get $P_{\\oplus}$ we restrict the sampling of one factor to $I/\\sqrt{N}$ . Note that memory-wise our approach is more efficient than [17] as we allow $K>2$ ; memory cost iAs $O(\\textstyle\\sum_{i=1}^{K}B_{i}D_{i})$ nw dhiifcfhe,r ecnhcoeo swinitgh $B_{i}\\simeq A$ t ahnatd $D_{i}\\simeq A^{\\prime}$ l ea lflroowms  fthore  as pmeecimalo royr tchoosgt $O(A A^{\\prime}\\log N)$ sample a Gaussian of shape $D_{i}\\times B_{i}$ and obtain ${Q}^{(i)}$ using the QR-decomposition. ", "page_idx": 31}, {"type": "text", "text": "C.6 Concentration result for QK: Theorem 3.3 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Theorem C.2. Consider the projection algorithm $\\pmb{Q}\\pmb{K}$ where $Q$ decomposes as $Q^{(1)}\\otimes\\cdot\\cdot\\cdot\\otimes Q^{(K)}$ where $Q^{(i)}$ has shape $D_{i}\\times B_{i}$ ; then ", "page_idx": 31}, {"type": "equation", "text": "$$\n>\\!\\Big(\\sqrt{\\frac{D}{N}}(1-\\varepsilon)\\|x\\|_{2}\\leq\\|Q(x)\\|_{2}\\leq\\sqrt{\\frac{D}{N}}(1+\\varepsilon)\\|x\\|_{2}\\Big)\\geq1-2\\sum_{i}\\exp(-4C D_{i}((1+\\varepsilon)^{1/K}-1)^{2}).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "In particular, as long as each $D_{i}$ is sufficiently large one still obtains a concentration result; the price to pay for the Kronecker product decomposition is that the concentration probability is dampened by the number of factors as $\\overline{{((1+\\varepsilon)^{1/K}-^{\\dag}1)^{2}}}\\simeq(\\frac{\\varepsilon}{K})^{2}$ . ", "page_idx": 31}, {"type": "text", "text": "Proof. Step 1: Reduction to the case of applying a single factor in the Kronecker product representation of $Q$ . We assume that $Q$ decomposes as $Q^{\\big(1)}\\otimes\\cdots\\otimes Q^{(K)}$ where $Q^{(i)}$ has shape $D_{i}\\times B_{i}$ ; if we reshape $x$ into a tensor of shape $\\left(B_{1},\\cdot\\cdot\\cdot,B_{K}\\right)$ indexed by $\\left(a_{1},\\cdot\\cdot\\cdot,a_{K}\\right)$ , the output $Q(x)$ can be represented as a tensor of shape $\\left(D_{1},\\cdot\\cdot\\cdot,D_{K}\\right)$ indexed by $\\left(b_{1},\\cdot\\cdot\\cdot,b_{K}\\right)$ : ", "page_idx": 32}, {"type": "equation", "text": "$$\nQ(x)_{b_{1},\\cdots,b_{K}}=\\sum_{a_{1},\\cdots,a_{K}}Q_{b_{1},a_{1}}^{(1)}Q_{b_{2},a_{2}}^{(2)}\\cdot\\cdot\\cdot Q_{b_{K},a_{K}}^{(K)}x_{a_{1},\\cdots a_{K}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "We now look into applying (18) one step at a time. We reshape $x$ to shape $(B_{1},B_{2}\\cdot\\cdot\\cdot B_{K})$ obtaining a matrix $\\left(D_{1},B_{2}\\cdot\\cdot\\cdot B_{K}\\right)$ . To1 apply $X_{a_{1},c}^{(1)}$ ; we then contract it with $Q_{b_{2},a_{2}}^{(2)}$ we need to first rbe1,sah1ape $Q_{b_{1},a_{1}}^{(1)}$ over $Y^{(1)}$ $a_{1}$ to shape to obtain a matrix $(D_{1},B_{2},\\cdot\\cdot\\cdot,B_{K})$ $Y_{b_{1},c}^{(1)}$ of shape , then twrae ncsapno steh etnh ec ofirnsttr aacnt di ts ewcitohn eos,v earn etsoh oabptea iitn  tao  am amtraitxr $X_{a_{2},c}^{(2)}$ sohf asphea $(B_{2},D_{1}B_{3}\\cdot\\cdot\\cdot B_{K})$ t; $Q_{b_{2},a_{2}}^{(2)}$ $a_{2}$ $Y_{b_{2},c}^{(2)}$ $(D_{2},D_{1}B_{3}\\cdot\\cdot\\cdot B_{K})$ should be clear how this procedure can be continued for each $i\\in\\{3,\\cdots K\\}$ . Assume that for each $i$ we can prove that the Frobenius norm (which is the $l^{2}$ -norm if we reshape it to be a vector) of $Y_{b_{i},c}^{(i)}$ concentrates around that of Xb(i,)c up to a multiplicative factor $\\sqrt{\\frac{D_{i}}{B_{i}}}$ : ", "page_idx": 32}, {"type": "equation", "text": "$$\nP\\Big(\\sqrt{\\frac{D_{i}}{B_{i}}}(1-\\varepsilon_{i})\\|X^{(i)}\\|_{2}\\leq\\|Y^{(i)}\\|_{2}\\leq\\sqrt{\\frac{D_{i}}{B_{i}}}(1+\\varepsilon_{i})\\|X^{(i)}\\|_{2}\\Big)\\geq1-\\delta_{i};\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "by conditional independence of the matrices $Q^{(i)}$ on one another we get that ", "page_idx": 32}, {"type": "equation", "text": "$$\nP\\Big(\\sqrt{\\frac{D}{N}}\\prod_{i=1}^{K}(1-\\varepsilon_{i})\\|x\\|_{2}\\le\\|Q(x)\\|_{2}\\le\\sqrt{\\frac{D}{N}}\\prod_{i=1}^{K}(1+\\varepsilon_{i})\\|x\\|_{2}\\Big)\\ge\\prod_{i=1}^{K}(1-\\delta_{i}),\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where we used $\\textstyle\\prod_{i=1}^{K}D_{i}=D$ and $\\textstyle\\prod_{i=1}^{K}B_{i}=N$ . ", "page_idx": 32}, {"type": "text", "text": "Step 2: Using concentration of measure on the orthogonal group. The entries of $Q^{(i)}$ are not independent because of the orthogonality requirement and the fact that the rows need to have $l_{2}$ -norm equal to 1. We will employ measure concentration without independence; as a reference for notation and theorems we use [26]. From [26, 2.5.2] we recall the definition of the sub-Gaussian norm of a real-valued random variable $X$ as: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|X\\|_{\\psi_{2}}=\\operatorname*{inf}\\{t>0:E\\exp(X^{2}/t^{2})\\leq2\\};\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "obtaining a bound on $\\|X\\|_{\\psi_{2}}$ is equivalent to a concentration inequality: ", "page_idx": 32}, {"type": "equation", "text": "$$\nP(|X|\\geq t)\\leq2\\exp(-c t^{2}/\\|X\\|_{\\psi_{2}}^{2}),\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "for a universal constant $c>0$ . Note that $Q^{(i)}$ can be sampled on the orthogonal group $S O(B_{i})$ by restricting to the first $D_{i}$ -rows in the case in which $D_{i}<B_{i}$ (by sampling from $O(B_{i})$ and changing in case the sign of one of the last $B_{i}-D_{i}$ rows to ensure the determinant is 1), while the result we are proving is trivial if $D_{i}=B_{i}$ because then $Q^{(i)}$ is full-rank. We now invoke the concentration of measure for $S O(B_{i})$ [26, 5.2.7]; if $f:S O(B_{i})\\rightarrow\\mathbb{R}$ is Lipschitz: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|f(Q^{(i)})-E f(Q^{(i)})\\|_{\\psi_{2}}\\leq C\\frac{\\|f\\|_{L i p}}{\\sqrt{B_{i}}},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $C$ is a universal constant and the Lipschitz constant $\\|f\\|_{L i p}$ is computed using the Frobenius norm on the tangent space. We now define: ", "page_idx": 32}, {"type": "equation", "text": "$$\nf(Q^{(i)})=\\|Y^{(i)}\\|_{2}=\\Bigl(\\sum_{b_{i},c}(\\sum_{a_{i}}Q_{b_{i},a_{i}}^{(i)}X_{a_{i},c}^{(i)})^{2}\\Bigr)^{\\frac{1}{2}};\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "which has derivative: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{\\partial f(Q^{(i)})}{\\partial Q_{b_{i},a_{i}}}=\\frac{\\sum_{c}Y_{b_{i},c}^{(i)}X_{a_{i},c}^{(i)}}{||Y^{(i)}||_{2}};\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "the Cauchy-Schwartz inequality implies that: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left|\\frac{\\partial f(Q^{(i)})}{\\partial Q_{b_{i},a_{i}}}\\right|\\leq\\frac{(\\sum_{c}(Y_{b_{i},c}^{(i)})^{2})^{1/2}(\\sum_{c}(X_{a_{i},c}^{(i)})^{2})^{1/2}}{\\|Y^{(i)}\\|_{2}},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "from which it follows that $\\|f\\|_{L i p}\\leq1$ if one assumes that $\\|X^{(i)}\\|_{2}\\leq1$ . Note that to derive (20) we may rescale $x$ by a constant; if we rescale it so that $\\|x\\|_{2}=1$ then all the norms of the intermediate results $\\|X^{(i)}\\|_{2},\\;\\|Y^{(i)}\\|_{2}$ are at most 1 because the matrices $Q^{(i)}$ are orthogonal. We have thus established ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\|f(Q^{(i)})-E f(Q^{(i)})\\|_{\\psi_{2}}\\leq\\frac{C}{\\sqrt{B_{i}}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Step 3: Replacing $E f(Q^{(i)})$ with something simpler to estimate. A drawback of (27) is that the term $E f(Q^{(i)})$ is not easy to estimate. However, using a symmetry argument, it is easy to estimate $E(f(Q^{(i)}))^{2}$ ; indeed the $D_{i}$ variables $\\sum_{c}(Y_{b_{i},c}^{(i)})^{2}$ are identically distributed and if $D_{i}\\,=\\,B_{i}$ one would get an isometry; so ", "page_idx": 33}, {"type": "equation", "text": "$$\nE(f(Q^{(i)}))^{2}=\\frac{D_{i}}{B_{i}}\\|X^{(i)}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "So we would like to replace $E f(Q^{(i)})$ with $\\sqrt{E(f(Q^{(i)}))^{2}}$ ; the intuition why this would work is that concentration around the mean is equivalent to concentration around the median; so $E f(Q^{(i)})$ concentrates around the median $M_{i}$ ; as $f(Q^{(i)})$ is non-negative, the median of $(f(Q^{(i)}))^{2}$ is $M_{i}^{2}$ ; and this variable concentrates both around the mean and the median, and we have a closed form for the mean (28). To make this more precise, by the fact that concentration around the mean is equivalent to concentration around the mean (see [26, 5.1.13]), we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\|f(Q^{(i)})-M_{i}\\|_{\\psi_{2}}\\leq\\frac{C}{\\sqrt{B_{i}}},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the constant $C$ might h\u221aave changed but is universal. We then just need to show that $\\lvert\\sqrt{E(f(Q^{(i)})^{2})}-M_{i}\\rvert$ is $O(1/\\sqrt{B_{i}})$ . By the triangle inequality: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\sqrt{E(f(Q^{(i)})^{2})}-M_{i}|\\leq\\sqrt{E|f(Q^{(i)})-M_{i}|^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and we can compute the right hand side with the layer cake decomposition: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\sqrt{E|f(Q^{(i)})-M_{i}|^{2}}=\\left(\\int_{0}^{\\infty}P(|f(Q^{(i)})-M_{i}|^{2}\\ge u)\\,d u\\right)^{\\frac{1}{2}},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "and we apply the concentration inequality (22) to the right hand side to get ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\sqrt{E|f(Q^{(i)})-M_{i}|^{2}}\\leq\\left(\\int_{0}^{\\infty}2\\exp(-\\tilde{c}B_{i}u)\\,d u\\right)^{\\frac{1}{2}},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "which implies that the right hand size is $O(1/\\sqrt{B_{i}})$ . We have thus established ", "page_idx": 33}, {"type": "equation", "text": "$$\nP\\left(\\left|\\|Y^{(i)}\\|_{2}-\\sqrt{\\frac{D_{i}}{B_{i}}}\\|X^{(i)}\\|_{2}\\right|\\geq t\\right)\\leq2\\exp(-C B_{i}t^{2}).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We now deduce (19) conditional that it holds for $j\\,=\\,1,\\cdot\\cdot\\cdot\\,,i\\,-\\,1$ so that we may assume that $\\begin{array}{r}{\\|X^{(i-1)}\\|_{2}\\geq\\frac{1}{2}}\\end{array}$ ; then we may take $\\begin{array}{r}{t=\\varepsilon_{i}\\sqrt{\\frac{D_{i}}{B_{i}}}\\|X^{(i)}\\|_{2}}\\end{array}$ and get (19) with ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\delta_{i}=2\\exp(-4C D_{i}\\varepsilon_{i}^{2}).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Step 4: Choosing the $\\varepsilon_{i}$ . We just aim for $\\varepsilon_{i}$ to be equal and that $\\textstyle\\prod_{i}(1+\\varepsilon_{i})\\;=\\;1+\\varepsilon$ ; this is achieved by letting $\\begin{array}{r}{\\varepsilon_{i}\\,=\\,(1+\\varepsilon)^{1/K}\\,-\\,1}\\end{array}$ . In this case we may lower bound $\\prod_{i}(1-\\delta_{i})$ by $\\begin{array}{r}{1-2\\sum_{i}\\exp(-4C D_{i}((1+\\varepsilon)^{1/K}-1)^{2}}\\end{array}$ ). \u53e3 ", "page_idx": 33}, {"type": "text", "text": "C.7 Concentration result for AFFD ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Theorem C.3. Consider sketching with AFFD; then ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P\\Big((1-\\varepsilon)\\|x\\|_{2}\\leq\\|A F F D(x)\\|_{2}\\leq(1+\\varepsilon)\\|x\\|_{2}\\Big)\\geq1-\\delta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where for each $\\delta_{1}>0$ we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\delta\\leq\\delta_{1}+\\exp\\left(-C\\varepsilon^{2}\\frac{N}{2\\log\\frac{2N}{\\delta_{1}}}\\right)+\\exp\\left(-C\\varepsilon^{2}\\frac{D}{4\\log^{2}\\frac{2N}{\\delta_{1}}}\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Note that the first two terms on the right hand side (36) can be made arbitrarily small (especially as $N$ ei fs otrympi $\\delta_{1}$ w.ill affect $C$ in the third term); thus the effective bound for $\\delta$ is of $\\begin{array}{r}{\\delta\\leq\\exp\\left(-C\\varepsilon^{2}\\frac{D}{\\log^{2}N}\\right)}\\end{array}$ ", "page_idx": 34}, {"type": "text", "text": "Proof. We will again use the notation and conventions from [26]. Let us recall the definition of AFFD: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\Phi(x)=R_{D}(\\sigma\\cdot H_{2}\\cdot G_{v}\\cdot H_{1}\\cdot B(x));\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "without loss of generality we will assume that $\\|x\\|_{2}=1$ . ", "page_idx": 34}, {"type": "text", "text": "Step 1: Using the pre-conditioner $H_{1}$ to distribute the mass of $x$ . Note that $H_{1}$ is an $N\\times N$ - dimensional matrix with entries of the form $\\pm{\\frac{1}{\\sqrt{N}}}$ ; each entry of the vector $H_{1}B(x)$ is of the form $\\sum_{i}{\\frac{s_{i}b_{i}x_{i}}{\\sqrt{N}}}$ where the $\\{b_{i}\\}_{i=1}^{N}$ are independent Bernoulli and $s_{i}=\\pm1$ ; applying Hoeffding\u2019s inequality [26, Thm.2.2.2] to each entry of $H_{1}B(x)$ we obtain that: ", "page_idx": 34}, {"type": "equation", "text": "$$\nP\\left(\\|H_{1}B(x)\\|_{\\infty}\\ge\\frac{t_{\\infty}}{\\sqrt{N}}\\right)\\le2N\\exp\\left(-\\frac{t_{\\infty}^{2}}{2}\\right);\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "intuitively, the norm of each entry of $H_{1}B(x)$ cannot become much larger than a multiple of its variance $\\frac{1}{\\sqrt{N}}$ : this is the purpose of using a pre-conditioner to distribute the mass of $x$ . ", "page_idx": 34}, {"type": "text", "text": "Step 2: Decomposing $\\|\\Phi(x)\\|_{2}^{2}$ . We now let $u=H_{1}\\cdot B(x)$ so that $\\Phi(x)=\\sigma R_{D}(H_{2}\\cdot G_{v}\\cdot u)$ ; we let $\\Pi$ be the permutation associated with rearranging the columns of $H_{2}$ so that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\|\\Phi(x)\\|_{2}^{2}=\\sigma^{2}\\sum_{i=1}^{D}\\left(\\sum_{j=1}^{N}H_{i,\\Pi(j)}g_{j}u_{j}\\right)^{2};\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "in (39) we decompose the effect of the diagonal and the off-diagonal terms obtaining ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\|\\Phi(x)\\|_{2}^{2}=\\underbrace{\\sigma^{2}\\sum_{i=1}^{D}\\sum_{j=1}^{N}\\frac{1}{N}g_{j}^{2}u_{j}^{2}}_{T_{1}}+\\underbrace{\\sigma^{2}\\sum_{i=1}^{D}\\sum_{k\\neq j}H_{i,\\Pi(k)}H_{i,\\Pi(j)}g_{j}g_{k}u_{j}u_{k}}_{T_{2}}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "We now use the first term $T_{1}$ to compute $\\sigma$ : ", "page_idx": 34}, {"type": "equation", "text": "$$\nE T_{1}=\\sigma^{2}\\sum_{j=1}^{N}\\frac{D}{N}u_{j}^{2},\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where we used the fact the components of $G_{v}$ have unit variance; as $H_{1}B(x)$ is an isometry, to have $E T_{1}=1$ we just need to set $\\begin{array}{r}{\\sigma=\\sqrt{\\frac{N}{D}}}\\end{array}$ . ", "page_idx": 34}, {"type": "text", "text": "Step 3: Concentration for $\\sqrt{T_{1}}$ . If we regard $\\sqrt{T_{1}}$ as a function $f_{1}(G_{v})$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\nf_{1}(G_{v})=\\left(\\sum_{j=1}^{N}g_{j}^{2}u_{j}^{2}\\right)^{1/2};\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "conditional on the good event $E_{g o o d}$ that $\\begin{array}{r}{\\|H_{1}B(x)\\|_{\\infty}\\le\\frac{t_{\\infty}}{\\sqrt{N}}}\\end{array}$ , this function is $\\frac{t_{\\infty}}{\\sqrt{N}}$ -Lipschitz in the $l^{2}$ -norm of $G_{v}$ ; applying concentration for the Gaussian measure on $\\mathbb{R}^{N}$ [26, Sec. 5.2.2] we get that conditional on $E_{g o o d}$ ", "page_idx": 35}, {"type": "equation", "text": "$$\nP\\left(|\\sqrt{T_{1}}-1|\\geq\\varepsilon\\right)\\leq\\exp\\left(-C\\frac{N}{t_{\\infty}^{2}}\\varepsilon^{2}\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Step 4: Concentration for $\\left|{{T_{2}}}\\right|$ . We would like to claim that $T_{2}$ is small with high probability. This is the second point in which we use the pre-conditioner $H$ ; the intuition is that applying the pre-conditioner $H$ to $G_{v}$ further reduces the finite sample correlation of the rows of the resulting matrix. Let us rewrite $T_{2}$ as follows: ", "page_idx": 35}, {"type": "equation", "text": "$$\nT_{2}=\\underset{k\\neq j}{\\sum_{k\\neq j}\\sum_{\\overset{i=1}{\\sum_{B}}}^{D}}\\frac{N}{D}H_{i,\\Pi(k)}H_{i,\\Pi(j)}\\,u_{j}u_{k}g_{j}g_{k};\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "so we have reduced $T_{2}$ to a bi-linear form $\\textstyle\\sum_{j,k}B_{j,k}g_{j}g_{k}$ ; by the Hanson-Wright inequality [26, 6.2.1] we have ", "page_idx": 35}, {"type": "equation", "text": "$$\nP(|T_{2}|\\geq\\varepsilon)\\leq\\exp\\left(-C\\operatorname*{min}\\{\\frac{\\varepsilon^{2}}{\\|B\\|_{F}^{2}},\\frac{\\varepsilon}{\\|B\\|_{S}}\\}\\right),\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $\\|B\\|_{F}$ is the Frobenious norm of $B$ and $\\|B\\|_{S}$ is the spectral norm. Let us look at $B_{j,k}$ conditional on $E_{g o o d}$ : ", "page_idx": 35}, {"type": "equation", "text": "$$\nB_{j,k}=\\sum_{i=1}^{D}\\frac{N}{D}H_{i,\\Pi(k)}H_{i,\\Pi(j)}u_{j}u_{k}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Recall now that $D$ and $N$ are both powers of 2 and the definition of $H$ in Sec. C.3: if $\\Delta(j)$ denotes the binary vector, of length $\\log_{2}N$ , representing an integer $j\\leq N$ , we have ${\\cal H}_{i,\\Pi(k)}=$ $\\begin{array}{r}{\\frac{1}{\\sqrt{N}}(-1)^{\\left\\langle\\Delta(i),\\Delta(\\Pi(k))\\right\\rangle}}\\end{array}$ , where $\\langle a,b\\rangle$ denotes the inner product of two vectors of length $\\log_{2}N$ . We thus obtain the bound: ", "page_idx": 35}, {"type": "equation", "text": "$$\n|B_{j,k}|\\leq\\frac{t_{\\infty}^{2}}{D}\\left|\\frac{1}{N}\\sum_{i=1}^{D}(-1)^{\\langle\\Delta(i),\\Delta(\\Pi(k))+\\Delta(\\Pi(j))\\rangle}\\right|;\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "the sum $\\begin{array}{r}{\\sum_{i=1}^{D}(-1)^{\\langle\\Delta(i),\\Delta(\\Pi(k))+\\Delta(\\Pi(j))\\rangle}}\\end{array}$ is 0 unless the vectors $\\Delta(\\Pi(j))$ and $\\Delta(\\Pi(k))$ agree in the first $\\log_{2}D$ entries, otherwise varying $i\\leq D$ we can always find two terms in the sum that cancel each other by flipping the parity of $i$ in the first slot where the vectors $\\Delta(\\Pi(j))$ and $\\Delta(\\Pi(k))$ differ. So for each $j$ , there are at most $\\frac{N}{D}$ possible $k$ -s such that $|B_{j,k}|$ is non-zero; moreover, as the sum $\\begin{array}{r}{\\sum_{i=1}^{D}(-1)^{\\langle\\Delta(i),\\Delta(\\Pi(k))+\\Delta(\\Pi(j))\\rangle}}\\end{array}$ is at most $D$ in absolute value, we get ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\|B\\|_{F}^{2}=\\sum_{j=1}^{N}\\sum_{k=1}^{N}|B_{j,k}|^{2}\\le N\\frac{t_{\\infty}^{4}}{D^{2}}\\frac{N}{D}\\frac{D^{2}}{N^{2}}\\le\\frac{t_{\\infty}^{4}}{D};\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "note that a bound on the spectral norm $\\|B\\|_{S}$ is trivial from (47) as we can just take the maximum of the absolute values of the $|B_{j,k}|$ which is bounded by $\\frac{t_{\\infty}^{2}}{N}$ . Given the stronger bound for $\\|B\\|_{S}$ and that $\\varepsilon\\ll1$ , the minimum in the exponential in (45) is achieved by the term involving $\\|B\\|_{F}^{2}$ and we thus obtain: ", "page_idx": 35}, {"type": "equation", "text": "$$\nP(|T_{2}|\\geq\\varepsilon)\\leq\\exp\\left(-C\\varepsilon^{2}\\frac{D}{t_{\\infty}^{4}}\\right).\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Step 5: Picking up constants. Conditional on $E_{g o o d}$ and on $\\begin{array}{r}{\\sqrt{T_{1}}\\geq\\frac{1}{2}}\\end{array}$ we have: ", "page_idx": 35}, {"type": "equation", "text": "$$\n|\\sqrt{T_{1}+T_{2}}-1|\\leq|\\sqrt{T_{1}}-1|+|\\sqrt{T_{1}+T_{2}}-\\sqrt{T_{1}}|\\leq|\\sqrt{T_{1}}-1|+2|T_{2}|.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "We can bound the right hand side of (50) by $3\\varepsilon$ if $E_{g o o d}$ and the concentration inequalities for $\\sqrt{T_{1}}$ and $T_{2}$ hold. Thus, by decreasing the constant $C$ by a factor 9, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\nP\\left(|{\\sqrt{T_{1}+T_{2}}}-1|\\geq\\varepsilon\\right)\\leq\\delta,\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\delta=2N\\exp\\left(-\\frac{t_{\\infty}^{2}}{2}\\right)+\\exp\\left(-C\\frac{N}{t_{\\infty}^{2}}\\varepsilon^{2}\\right)\\exp\\left(-C\\varepsilon^{2}\\frac{D}{t_{\\infty}^{4}}\\right);\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "having fixed a small $\\delta_{1}$ , if we set $\\begin{array}{r}{t_{\\infty}=\\sqrt{2\\log\\frac{2N}{\\delta_{1}}}}\\end{array}$ , we get ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\delta\\leq\\delta_{1}+\\exp\\left(-C\\varepsilon^{2}\\frac{N}{2\\log\\frac{2N}{\\delta_{1}}}\\right)+\\exp\\left(-C\\varepsilon^{2}\\frac{D}{4\\log^{2}\\frac{2N}{\\delta_{1}}}\\right).\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Comparison with [4] It seems plausible that Step 4 could be carried out with arguments similar to those of [4, Lem. 16, Lem. 17] by analyzing the chromatic number of the $P$ -system associated with $H$ ; however we think the method that uses the Hanson-Wright inequality is more simple for this specific case. ", "page_idx": 36}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: the theoretical justification is provided in Sec. 3 and Sec. 4; the experimental evidence is provided in Sec. 5.3. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 37}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: Limitations are discussed in Section 6. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 37}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Theorems are stated with the full assumptions in the paper; proofs are provided in Appendix C. As the proofs can be lengthy, the main proof ingredients (e.g. concentration for quadratic forms, analysis via explicit sketching) are explained in the paper. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 38}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: The complete experiment setup is described in Appendix B. Python code of Jax implementations of the algorithms used is also provided in Appendix B. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 38}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 39}, {"type": "text", "text": "Answer: [No] ", "page_idx": 39}, {"type": "text", "text": "Justification: All datasets and models used are public and can be obtained from HuggingFace.   \nPython code to implement the proposed algorithms (in Jax) is provided in Appendix B. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 39}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: The main paper provides the core ideas for each experimental setup and full details to reproduce the results are available in Appendix B. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 39}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [No] ", "page_idx": 39}, {"type": "text", "text": "Justification: Experiments in Sec. 5.4 require multiple runs to check the consistency and accuracy of our estimation method for the intrinsic dimension. As the target quantity is discrete, we have opted to report the values across the random seeds in Appendix A instead of using error bars. For wall time estimates we checked that on different runs the relative errors stay withing $10\\%$ , while estimates did not change for the peak memory usage (discussion about using the Jax profiler in Appendix B). ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 39}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 40}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: See Appendix B. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 40}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We followed the guidelines. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 40}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: We do not foresee societal impacts for this work. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 41}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: No releases of generative models or datasets. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 41}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: For the experiments in Sec. 5.2 we used the setup of [6] which has been cited. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: No new assets are released. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 42}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: No crowdsourcing, no human subjects. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 42}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: No crowdsourcing, no human subjects. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 43}]