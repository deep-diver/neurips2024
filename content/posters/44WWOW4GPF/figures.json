[{"figure_path": "44WWOW4GPF/figures/figures_5_1.jpg", "caption": "Figure 1: Kernel stacks are acquired through a learned weight-sharing scheme applied to a set of flattened base kernels.", "description": "The figure illustrates the process of creating kernel stacks using a learned weight-sharing scheme. It starts with a set of flattened base kernels (represented as a stack of colored layers).  These kernels are then transformed by a set of learnable doubly stochastic matrices (R\u2081, R\u2082, R\u2083, R\u2084), which act as soft permutation matrices. The result of these transformations is a reshaped kernel stack suitable for use in a convolutional layer.  The visual shows the transformation from the original kernel layers to the permuted and then reshaped version. This weight sharing is the core concept of the proposed method to learn symmetries from data.", "section": "4 Method"}, {"figure_path": "44WWOW4GPF/figures/figures_6_1.jpg", "caption": "Figure 3: Comparison of C4 representations and the representation stack learned by the lifting layer on the rotated MNIST dataset. Top: learned representations. Bottom: permutations for C4 on d = 25.", "description": "This figure compares the learned representations from the weight-sharing convolutional neural network (WSCNN) with the ground truth C4 representations for rotated MNIST data.  The top row shows the learned representation matrices from the WSCNN's lifting layer. The bottom row displays the true permutation matrices corresponding to the C4 group (cyclic rotations of 0, 90, 180, and 270 degrees) for a 25-dimensional data point (d=25). The comparison illustrates how well the WSCNN learns to approximate the C4 symmetry through its learnable weight-sharing scheme.", "section": "5 Experiments"}, {"figure_path": "44WWOW4GPF/figures/figures_12_1.jpg", "caption": "Figure 4: Samples of the two tasks.", "description": "This figure shows example data and labels for two different tasks: an equivariant task and an invariant task.  In the equivariant task (a), both the data and labels change according to the group action (in this case cyclic permutations), while in the invariant task (b), the labels remain unchanged regardless of the data transformation. This illustrates the difference in inductive biases used in modeling each scenario.", "section": "B Toy problems"}, {"figure_path": "44WWOW4GPF/figures/figures_13_1.jpg", "caption": "Figure 5: Coefficient responses of learned representations and their base transformations. (a) Shift-dataset with uniformly sampled group elements. (b) Shift-dataset with partially sampled group elements.", "description": "This figure visualizes the learned weight-sharing matrices (representations) and compares them to the ground truth permutation matrices for a dataset with cyclical shifts.  Subfigure (a) shows the results when the model is trained with uniformly sampled group elements, resulting in a close match between the learned and ground truth representations. Subfigure (b) demonstrates the model's ability to learn even when only a subset of the group elements are used during training. Despite the incomplete sampling, the learned representations still capture the underlying symmetry, indicating that the method is robust to partial or incomplete information.", "section": "B.2 Additional results: toy problems"}, {"figure_path": "44WWOW4GPF/figures/figures_13_2.jpg", "caption": "Figure 10: Learned representations for the weight-sharing G-conv layers. Top to bottom: first to last layer learned representation stacks.", "description": "This figure visualizes the learned representation stacks for the weight-sharing G-convolutional layers.  Each image represents a layer's learned weight-sharing scheme. The arrangement from top to bottom corresponds to the order of the layers in the network.  The color intensity within each image indicates the strength of the learned weight-sharing connections.", "section": "B.3 Visualization of G-Conv layers"}, {"figure_path": "44WWOW4GPF/figures/figures_13_3.jpg", "caption": "Figure 7: Samples of the 2D-signal dataset.", "description": "This figure shows sample images from the 2D signal dataset used in the paper's experiments.  The images depict various rotations of a basic pattern, illustrating the types of symmetries the model is trained to recognize. The specific pattern is a stylized snowflake-like shape.  These examples highlight the complexity and variety within the dataset, demonstrating the challenge of learning symmetries without prior knowledge of the exact transformations.", "section": "B Toy problems"}, {"figure_path": "44WWOW4GPF/figures/figures_15_1.jpg", "caption": "Figure 10: Learned representations for the weight-sharing G-conv layers. Top to bottom: first to last layer learned representation stacks.", "description": "This figure shows the learned weight-sharing matrices for the G-convolutional layers of the proposed Weight Sharing Convolutional Neural Networks (WSCNNs). Each matrix represents a learned transformation (permutation matrix) applied to the base kernels of each layer. The figure visualizes how these matrices evolve from the first layer (top) to the last layer (bottom), illustrating how the network dynamically discovers effective weight sharing schemes across different layers.", "section": "B.3 Visualization of G-Conv layers"}, {"figure_path": "44WWOW4GPF/figures/figures_15_2.jpg", "caption": "Figure 10: Learned representations for the weight-sharing G-conv layers. Top to bottom: first to last layer learned representation stacks.", "description": "This figure shows the learned weight-sharing matrices (representations) in the G-convolution layers of the proposed WSCNN model.  The matrices visualize how the model learned to share weights across different group transformations during training.  The progression from top to bottom indicates the evolution of these learned representations across the layers of the network.  The representation matrices are a key element to the WSCNN's ability to adaptively learn symmetries.", "section": "B.3 Visualization of G-Conv layers"}, {"figure_path": "44WWOW4GPF/figures/figures_16_1.jpg", "caption": "Figure 11: Cube dataset with C4 \u00d7 C4 rotations with C4 \u00d7 C4 \u00d7 C4 base elements.", "description": "This figure visualizes the learned representations for a cube dataset subjected to C4 \u00d7 C4 rotations.  The x-axis represents the base elements (transformations) of the C4 \u00d7 C4 \u00d7 C4 group, and the y-axis represents the indices of the learned representation matrix R. The heatmap shows the coefficients of the learned representation, highlighting which base elements are most prominent in each part of the representation. A red line is drawn to show where the learned representation closely aligns with the applied transformations.", "section": "B.4 Additional results: learning partial equivariances"}, {"figure_path": "44WWOW4GPF/figures/figures_16_2.jpg", "caption": "Figure 3: Comparison of C4 representations and the representation stack learned by the lifting layer on the rotated MNIST dataset. Top: learned representations. Bottom: permutations for C4 on d = 25.", "description": "This figure compares the learned representations from the weight-sharing convolutional neural network (WSCNN) model with the ground truth C4 representations. The top row shows the learned representations from the WSCNN model's lifting layer, while the bottom row displays the true C4 group permutations for a data dimension of 25.  The visualization helps to assess how well the learned weight sharing scheme approximates the known symmetries of the C4 group.", "section": "5 Experiments"}, {"figure_path": "44WWOW4GPF/figures/figures_18_1.jpg", "caption": "Figure 13: Memory allocation at inference time for a C4-GCNN layer and a weight sharing layer, for different number of group elements |G| and different kernel sizes. The input is 32 \u00d7 3 \u00d7 100 \u00d7 100 (batch x channels \u00d7 height \u00d7 width)", "description": "This figure compares the memory usage of C4-GCNN and WSCNN layers during inference.  It shows how memory consumption varies with the number of group elements (|G|) and kernel size. The input tensor dimensions are consistent across all scenarios (32 batches, 3 channels, 100x100 height and width).  The plots reveal that the WSCNN layer generally requires less memory than the C4-GCNN layer, especially as the number of group elements and kernel size increases.", "section": "C Architectural details"}, {"figure_path": "44WWOW4GPF/figures/figures_18_2.jpg", "caption": "Figure 13: Memory allocation at inference time for a C4-GCNN layer and a weight sharing layer, for different number of group elements |G| and different kernel sizes. The input is 32 \u00d7 3 \u00d7 100 \u00d7 100 (batch x channels \u00d7 height \u00d7 width)", "description": "This figure compares the memory usage of a C4-GCNN layer and a weight-sharing layer at inference time. The comparison is shown for various numbers of group elements (|G|) and kernel sizes. The input data has dimensions 32 x 3 x 100 x 100, representing batch size, channels, height, and width respectively. The results demonstrate that the memory usage scales differently for the two methods with changes in the number of group elements and kernel size.", "section": "C Architectural details"}]