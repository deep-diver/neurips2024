[{"figure_path": "44WWOW4GPF/tables/tables_3_1.jpg", "caption": "Table 1: Test accuracy on MNIST for both rotation and scaling transformations. Additional parameters induced by weight-sharing are marked (+). Parameter counts denoted in millions (M) or thousands (K). Best-performing models (equivalent within < 1%) marked bold.", "description": "This table presents the test accuracy results on the MNIST dataset for models trained with rotation and scaling transformations.  It compares the performance of a standard CNN, a group convolutional neural network (GCNN), and the proposed weight-sharing convolutional neural network (WSCNN) with and without normalization and entropy regularizers.  The table shows the number of parameters, the type of weight-sharing scheme used (learned vs. predefined), and the accuracy achieved for both rotation and scaling tasks.  Best-performing models (with accuracy within 1% of the top performer) are highlighted in bold. The additional parameters due to the weight-sharing mechanism are indicated with a (+).", "section": "5 Experiments"}, {"figure_path": "44WWOW4GPF/tables/tables_7_1.jpg", "caption": "Table 1: Test accuracy on MNIST for both rotation and scaling transformations. Additional parameters induced by weight-sharing are marked (+). Parameter counts denoted in millions (M) or thousands (K). Best-performing models (equivalent within < 1%) marked bold.", "description": "This table presents the test accuracy results on the MNIST dataset for models trained with rotation and scaling transformations.  It compares a standard CNN, a Group Convolutional Neural Network (GCNN), and the proposed Weight Sharing Convolutional Neural Network (WSCNN) with and without entropy and normalization regularizers.  The number of parameters, the weight-sharing scheme used (learned vs. predefined), and the accuracy for both rotations and scaling are shown.  Best-performing models are highlighted in bold.", "section": "5 Experiments"}, {"figure_path": "44WWOW4GPF/tables/tables_7_2.jpg", "caption": "Table 2: Test accuracy on CIFAR-10. Number of elements denotes the number of group elements used in group convolutional models. Additional parameters induced by weight-sharing are marked (+). Parameter counts denoted in millions (M) or thousands (K). Best-performing models (equivalent within < 1%) marked bold.", "description": "This table presents the test accuracy results on the CIFAR-10 dataset for various models.  It compares a standard CNN with different numbers of parameters, a group convolutional neural network (GCNN), and the proposed weight-sharing convolutional neural network (WSCNN) with and without different regularizations. The number of elements in GCNN and WSCNN indicates the size of the group used for the group convolution. The table highlights that the WSCNN achieves comparable performance to the best-performing CNN while using significantly fewer parameters.", "section": "5.1 Image datasets and equivariance priors"}, {"figure_path": "44WWOW4GPF/tables/tables_8_1.jpg", "caption": "Table 3: Test accuracy on MNIST with partial rotations.", "description": "This table presents the test accuracy results on the MNIST dataset for different rotation ranges.  The model's performance is evaluated using two different rotation ranges: [0, 90\u00b0] and [0, 180\u00b0]. The table compares the accuracy of two models: GCNN and WSCNN, showcasing how well they perform under varying levels of symmetry.", "section": "5.2 Learning partial equivariances"}, {"figure_path": "44WWOW4GPF/tables/tables_8_2.jpg", "caption": "Table 2: Test accuracy on CIFAR-10. Number of elements denotes the number of group elements used in group convolutional models. Additional parameters induced by weight-sharing are marked (+). Parameter counts denoted in millions (M) or thousands (K). Best-performing models (equivalent within < 1%) marked bold.", "description": "This table presents the test accuracy results on the CIFAR-10 dataset for different models: a standard CNN with varying numbers of parameters, a group convolutional neural network (GCNN) with 4 group elements, and the proposed weight-sharing convolutional neural network (WSCNN) also with 4 group elements.  The table highlights the number of parameters for each model, indicating the additional parameters introduced by the weight-sharing mechanism in WSCNN.  The best performing models (within 1% accuracy difference) are marked in bold, allowing for a direct comparison of performance.", "section": "5.1 Image datasets and equivariance priors"}]