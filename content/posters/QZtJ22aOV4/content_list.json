[{"type": "text", "text": "Safe Exploitative Play with Untrusted Type Beliefs ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Tongxin Lil \\* Tinashe Handina2 Shaolei Ren? Adam Wierman2 ", "page_idx": 0}, {"type": "text", "text": "1School of Data Science The Chinese University of Hong Kong, Shenzhen, China litongxin@cuhk.edu.cn ", "page_idx": 0}, {"type": "text", "text": "2Computing $^+$ Mathematical Sciences California Institute of Technology, Pasadena, USA {thandina, adamw}@caltech.edu ", "page_idx": 0}, {"type": "text", "text": "3Electrical & Computer Engineering University of California, Riverside, USA shaolei@ucr.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The combination of the Bayesian game and learning has a rich history, with the idea of controlling a single agent in a system composed of multiple agents with unknown behaviors given a set of types, each specifying a possible behavior for the other agents. The idea is to plan an agent's own actions with respect to those types which it believes are most likely to maximize the payoff. However, the type beliefs are often learned from past actions and likely to be incorrect. With this perspective in mind, we consider an agent in a game with type predictions of other components, and investigate the impact of incorrect beliefs to the agent's payoff. In particular, we formally define a tradeoff between risk and opportunity by comparing the payoff obtained against the optimal payoff, which is represented by a gap caused by trusting or distrusting the learned beliefs. Our main results characterize the tradeoff by establishing upper and lower bounds on the Pareto front for both normal-form and stochastic Bayesian games, with numerical results provided. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "\"When written in Chinese, the word \u2018crisis' is composed of two characters. One represents danger and the other represents opportunity.\"\u2014 John F. Kennedy ", "page_idx": 0}, {"type": "text", "text": "The famous quote above from John F. Kennedy, that the word \u2018crisis\u2019 (known as \u5371\u6a5f in Chinese) captures the dual nature of danger and opportunity, provides an interesting metaphor for the inherent complexities within real-world multi-agent systems. These systems, where risk and opportunity are often inextricably linked, play a pivotal role across diverse domains, ranging from human-AI collaboration [1] and cyber-physical systems [2], to highly competitive environments like real-time strategy games [3] and poker [4]. ", "page_idx": 0}, {"type": "text", "text": "In conventional applied and theoretical frameworks, it is typically assumed that all agents either cooperate or adhere to pre-defined policies [5, 6, 7, 8]. However, real-world scenarios often defy these simplifications, presenting agents that display a spectrum of behaviors ranging from cooperative, to heterogeneous, irrational, or even adversarial [9]. This deviation from expected behavior patterns complicates the dynamics of multi-agent systems, as agents cannot reliably predict the actions of their counterparts. The uncertainty regarding whether to trust or distrust predictions naturally leads to a critical tradeoff between the coexisted risk and opportunity, reflecting the dual aspects highlighted in Kennedy's remark. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "As critical examples, Bayesian games [10, 11] provide an approach for modeling differing types of agents in strategic environments. In these games, players form beliefs about others\u2019 types and update beliefs in response to observed actions and choose their actions accordingly. For example, in competitive settings like poker or even in simple games like matching pennies, deviating from the game theoretic optimal (GTO) strategy [12, 13] to exploit weaker opponents can be beneficial, but this approach also relies on potentially flawed type beliefs [6], making it risky to take advantage of such side-information. Similarly, in real-world problems like security games, having a prior distribution of attackers\u2019 behavioral types in a Bayesian game setting leads to advantages. However, exploiting incorrect types can be risky compared to just using minimax strategies. Despite the ubiquity of incorrect type beliefs in practical scenarios, limited attention has been paid to explore such a tradeoff, with exceptions in designing heuristically safe and exploitative strategies in specific contexts, such as with Byzantine adversaries [9] and sequential games [14]. ", "page_idx": 1}, {"type": "text", "text": "Inaccuracies in beliefs about others\u2019 types may arise from factors such as using out-of-distribution data to generate priors, changes in opponents\u2019 behaviors, or mismatches between hypothesized types and the optimal type space, etc. As noted in [15], it is evident that prior beliefs significantly affect the long-term performance of type-based learning algorithms like the Harsanyi-Bellman Ad Hoc Coordination (HBA). Although algorithms like HBA demonstrate asymptotic convergence to correct predictions or type distributions through various methods of estimating posterior beliefs, and methods exist for detecting inaccuracies in type beliefs using empirical behavioral hypothesis testing [15], the theoretical capabilities of general algorithms remain uncertain. ", "page_idx": 1}, {"type": "text", "text": "In general, relying on learned beliefs presents a fundamental tradeoff between the potential payoffs from exploitative play and the risk of incorrect type beliefs. Given the potential inaccuracies in these type beliefs, relying on them to exploit opponents could lead to high-risk strategies. Conversely, not exploiting these beliefs might result in overly cautious play. Therefore, it is natural to investigate the impact of incorrect beliefs on the agent's payoff, in terms of a tradeoff between trusting or distrusting the beliefs of types provided by type-based learning algorithms (e.g., Bayesian learning [16], best response dynamics [17], and policy iteration with neural networks [18], etc.) in multi-agent systems. To summarize the focus of this paper, we aim to address the following critical question: ", "page_idx": 1}, {"type": "text", "text": "What is the fundamental tradeoff between trusting/distrusting type beliefs in games? ", "page_idx": 1}, {"type": "text", "text": "Contributions. Motivated by the above question, we analyze the following payoff gap that arises from erroneous type beliefs: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\Delta(\\varepsilon;\\pi):=\\operatorname*{max}_{d(\\theta,\\theta^{\\star})\\leq\\varepsilon}\\left(\\operatorname*{max}_{\\phi}\\mathsf{P a y o f f}(\\phi,\\theta^{\\star})-\\mathsf{P a y o f f}(\\pi(\\theta),\\theta^{\\star})\\right),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\theta$ and $\\theta^{\\star}$ are predicted and true type beliefs respectively; $d(\\cdot,\\cdot)$ measures the distance between $\\theta$ and $\\theta^{\\star}$ bounded from above by $\\varepsilon$ and will be formally specified with concrete model contexts in Section 3 and Section 4, together with the payoff, denoted by $\\mathsf{P a y o f f}(\\cdot,\\cdot)$ as a function of the true type $\\theta^{\\star}$ and the used strategy $\\pi$ . The agent uses a strategy $\\pi$ that depends on the type belief $\\theta$ . Overall, the payoff difference (1) above quantifies the worst-case gap between the optimal payoff (obtained using an optimal strategy that maximizes Payoff $(\\cdot,\\theta^{\\star})),$ , and the payoff corresponding to $\\pi$ ", "page_idx": 1}, {"type": "text", "text": "In particular, when the agent's strategy $\\pi$ trusts the belief $\\theta$ it takes the opportunity to close the gap in (1) when the belief error $\\varepsilon$ is small. However, when $\\varepsilon$ increases, such a strategy incurs a high risk since it trusts the incorrect $\\theta$ . Evaluating the payoff gap in (1) naturally yields a tradeoff between opportunity and risk, as illustrated on the right of Figure 1. To be more precise, we measure the tradeoff between two important quantities: ", "page_idx": 1}, {"type": "text", "text": "(Missed) Opportunity: $\\Delta(0;\\pi)$ , corresponding to the case when the type beliefs are correct;   \nRisk: $\\operatorname*{max}_{\\varepsilon>0}\\Delta(\\varepsilon;\\pi)$ measuring the payoff difference incurred by worst-case incorrect beliefs. ", "page_idx": 1}, {"type": "text", "text": "In summary, the (missed) opportunity measures the discrepancy of a strategy $\\pi$ from the optimal strategy, which is aligned with the ground truth belief $\\theta^{\\star}$ of other players in terms of the obtained payoff. Additionally, the risk quantifies how inaccurate beliefs impact the difference in terms of payoffs in the worst case. The goal of this paper is to investigate safe and exploitative strategies in Bayesian games that achieve near-optimal opportunity and risk. ", "page_idx": 1}, {"type": "image", "img_path": "QZtJ22aOV4/tmp/eb7ba0dac1d7f3ff4e7b89fe9913601fda553b51bce537e0750e05464512ed22.jpg", "img_caption": ["Figure 1: Left: A stochastic Bayesian game where an agent interacts with an environment and opponents, with a belief of their types $\\theta\\in\\Theta$ . Right: The tradeoff between trusting and distrusting type beliefs, with trust leading to higher risk and opportunity and distrust resulting in lower risk and opportunity, implying an opportunity-risk tradeoff with varying strategy $\\pi$ "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Our main results are two-fold. Firstly, in normal-form Bayesian games, we characterize a tradeoff between the opportunity and risk. We consider a strategy as a convex combination of a safe strategy and the best response given type beliefs. Upper bounds on opportunity and risk are provided in Theorem 3.1. Conversely, lower bounds that hold for any mixed strategy are shown in Theorem 3.2. Notably, when the game is fair, and the hypothesis set $\\Theta$ is sufficiently large, these bounds tightly converge. Secondly, we explore a dynamic setting in stochastic Bayesian games, where an agent, provided with type beliefs about other players, engages in interactions over time, as illustrated in Figure 1. Unlike the normal-form approach, we utilize a value-based strategy that establishes upper bounds on opportunity and risk, as outlined in Theorem 4.1. Additionally, Theorem 4.2 provides lower bounds on opportunity and risk that differ from the upper bounds by multiplicative constants, yielding a characterization of the opportunity-risk tradeoff. Finally, a case study of a security game, simulating a defender protecting an elephant population from illegal poachers, is provided in Section 5. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Learning Types in Games. The concept of type-based methods dates back to the development of Bayesian games, as first established by Harsanyi in the late 1960s [10, 11]. The concepts of learning and updating beliefs appear in pioneering works like the adaptive learning [19]. While much game theory research, including work by Kalai et al. [20], focuses on equilibrium analysis through Bayesian belief-based learning, other studies, such as those by Nachbar et al. [21, 22] and [23, 24], reveal the challenges players face in making correct predictions while playing optimally under certain game conditions and assumptions. From an application perspective, Southey et al. [25] applied type-based methods to poker, where players\u2019 hands are partially hidden. They demonstrated how to maintain and use beliefs to determine the best strategies in this setting. Besides classic results exemplified above, closely related to our results, the recent work by Milec et al. [14] addresses the limitations of Nash equilibrium strategies in two-player extensive-form games, particularly their inability to exploit the weaknesses of sub-optimal opponents. They defined the exploitability of a strategy as the expected payoff that a fully rational opponent can gain beyond the game's base value and introduced a method that ensures safety, defined as an upper limit on exploitability compared to the payoff obtained using a Nash equilibrium strategy. However, the proposed tradeoff between the exploitation of the opponent given the correct model and safety against an opponent who can deviate arbitrarily from the predicted model is applicable specifically to an algorithm that employs a continual depth-limited restricted Nash response. ", "page_idx": 2}, {"type": "text", "text": "Online Decision-Making with Predictions. The tradeoff between opportunity and risk analyzed in this work is motivated by the recent progress in algorithms with predictions, also known as learning-augmented algorithms [26, 27] for online decision-making problems such as caching [28, 29, 30], bipartite matching [31], online optimization [32, 33], control [34, 35, 36, 37], valued-based reinforcement learning [38, 39, 40], and real-world applications [41, 42, 43, 44]. This line of work investigates the impact of untrusted predictions on two key metrics known as consistency and robustness, which are defined based on the competitive ratio of the considered contexts. It is also worth highlighting that previous works in decision-making often provide best-of-both-worlds guarantees for both stochastic and adversarial environments [45, 46, 46], while the results in this work shed light on studying the intermediate regimes that do not fully align with either stochastic or adversarial settings and delves into interactive environments formed by players whose behaviors may deviate from the type beliefs. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Stochastic Bayesian Games. Our results presented in Section 4 draw on foundational concepts from stochastic Bayesian games as outlined by Albrecht et al. [47, 15], which merge concepts of the Bayesian games [10, 11] and the stochastic games [48], with applications in cooperative multi-agent reinforcement learning [9]. In [15], three methods\u2014-product, sum, and correlated\u2014for integrating observed evidence into posterior beliefs have been explored. Specifically, it has been demonstrated that the Harsanyi-Bellman Ad Hoc Coordination (HBA) eventually make right future predictions under specific conditions using the product posterior beliefs. However, while HBA may eventually align with the correct type distribution, it is not guaranteed to learn it with the product posterior beliefs. Under certain conditions, HBA with the sum and correlated posterior converges to the correct type distribution. Nonetheless, even though methods like empirical behavioral hypothesis testing are available to detect inaccuracies in type beliefs, a comprehensive theoretical analysis is still lacking. ", "page_idx": 3}, {"type": "text", "text": "3 Normal-Form Bayesian Games with Untrusted Type Beliefs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Let $\\|\\cdot\\|_{1}$ denote the $\\ell_{1}$ -norm and $\\|\\cdot\\|_{\\operatorname*{max}}$ denote the element-wise max norm. ", "page_idx": 3}, {"type": "text", "text": "3.1 Problem Setting, Opportunity, and Risk ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Consider a normal-form game with two players: Player 1 possesses a payoff matrix $A\\in\\mathbb{R}^{a\\times b}$ with $\\|A\\|_{\\operatorname*{max}}\\leq\\alpha$ , where the first player has $a$ choices of the rows and the second player has $b$ choices of the columns of $A$ . Player 1 forms a belief about Player 2's mixed strategy, denoted by a probability distribution $\\rho$ over a set of hypothesized strategies $\\Theta$ that contains a ground truth strategy $y^{\\star}$ 2. ", "page_idx": 3}, {"type": "text", "text": "As a concrete example of the payoff gap proposed in (1), the following benchmark characterizes the gap between payoffs obtained by a strategy with machine-learned belief $\\rho$ and an optimal strategy knowing $y^{\\star}$ beforehand: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Delta_{\\mathsf{N F G}}(\\varepsilon;\\pi):=\\operatorname*{max}_{d(\\rho,y^{\\star})\\leq\\varepsilon}\\left(\\operatorname*{max}_{x\\in\\mathsf{P}_{a}}x^{\\top}A y^{\\star}-\\pi(\\rho)^{\\top}A y^{\\star}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "given a fixed policy $\\pi:\\mathsf{P}_{\\Theta}\\to\\mathsf{P}_{a}$ that outputs a mixed strategy of the first player knowing $\\rho$ , where $\\bar{d}\\,(\\rho,y^{\\star}):=\\|\\bar{\\mathbb{E}}_{\\rho}[y]-y^{\\star}\\|_{1},\\,\\mathsf{P}_{\\epsilon}$ and $\\mathsf{P}_{a}$ are sets of probability distributions on $\\Theta$ and the $a$ different choices of rows respectively. ", "page_idx": 3}, {"type": "text", "text": "In particular, we focus on measuring a tradeoff between two important quantities, the (missed) opportunity $\\Delta_{\\mathsf{N F G}}(0;\\pi)$ and the risk $\\begin{array}{r}{\\operatorname*{max}_{\\varepsilon>0}\\Delta_{\\mathsf{N F G}}(\\varepsilon;\\pi)}\\end{array}$ . In addition, a strategy $\\pi$ is $\\beta$ -foregone if $\\Delta_{\\mathsf{N F G}}(0;\\pi)\\le\\beta(\\pi)$ and $\\delta$ -risky if $\\begin{array}{r}{\\operatorname*{max}_{\\varepsilon>0}\\Delta_{\\mathsf{N F G}}(\\varepsilon;\\pi)\\leq\\delta(\\pi)}\\end{array}$ . The former measures how far the considered strategy $\\pi$ is away from the optimal strategy knowing the ground truth type $y^{\\star}$ of Player 2 in terms of the payoff obtained; the latter quantifies the worst-case impact of inaccurate belief on the payoff difference. ", "page_idx": 3}, {"type": "text", "text": "3.2  Motivating Example: Matching Pennies ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Before presenting general results, we illustrate the underlying concepts through a simple normal-form game. Consider the matching pennies game as a classic example, where the mixed strategies for players as defined follows. ", "page_idx": 3}, {"type": "text", "text": "Suppose $a=b=2$ and each of the two players has the option to choose either Heads (H) or Tails (T). Let $y^{\\star}\\in[0,1]$ be the true probability that Player 2 plays $\\mathtt{H}$ and $1-y^{\\star}$ the probability of playing T. Suppose the hypothesis set $\\Theta$ contains all possible mixed strategies, each corresponding to a type of Player 2. Then, Player 1 receives a belief of $y^{\\star}$ , denoted by $y$ Similarly, let $x$ be the probability that Player 1 plays $\\mathtt{H}$ and $1-x$ the probability of playing T. If two players? actions match, Player 1 will receive 1 and $-1$ otherwise. The problem setting is summarized in Figure 2. Given $y^{\\star}$ and $x$ , the expected payoff is therefore $(2y^{\\star}-1)(2x-1)$ . The best response of Player 1, depending on $y$ ,is ", "page_idx": 3}, {"type": "image", "img_path": "QZtJ22aOV4/tmp/d86c321d5934b7a31208161ee135daf88d27f0035ec028e68b7e36403cdefd87.jpg", "img_caption": ["Figure 2: Left: Matching Pennies payoff matrix for Player 1 (row player) with type belief $y$ and Player 2 (column player) whose strategy is defined by $y^{\\star}$ . Right: Opportunity-risk tradeoff that satisfies $\\Delta_{\\mathsf{M P}}(0;\\pi)+\\operatorname*{max}_{\\varepsilon}\\Delta_{\\mathsf{M P}}(\\varepsilon;\\pi)\\overset{\\cdot\\cdot}{=}2$ "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "characterized by: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathsf{B R}(y)=\\left\\{\\!\\!\\begin{array}{l l}{x=0}&{\\mathrm{if}\\;y<\\frac{1}{2},}\\\\ {x\\in[0,1]}&{\\mathrm{if}\\;y=\\frac{1}{2},}\\\\ {x=1}&{\\mathrm{if}\\;y>\\frac{1}{2}.}\\end{array}\\!\\!\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Given a strategy $\\pi:[0,1]\\rightarrow[0,1]$ , the payoff gap in (2) for this example is instantiated as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Delta_{\\mathsf{M P}}(\\varepsilon;\\pi):=\\operatorname*{max}_{|y-y^{\\star}|\\leq\\varepsilon}2\\Bigl((2y^{\\star}-1)\\mathsf{B R}(y^{\\star})-(2y^{\\star}-1)\\pi(y)\\Bigr).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Impossibility. We first consider an arbitrary strategy. $\\pi:[0,1]\\rightarrow[0,1]$ that is $(1-\\lambda)$ -foregone. Suppose $\\pi$ chooses $\\mathtt{H}$ with a probability $\\pi(y)$ and $\\mathbb{T}$ with a probability $1-\\pi(y)$ . Therefore, setting $\\varepsilon\\,=\\,0$ the payoff gap must satisfy $\\begin{array}{r}{\\Delta_{\\mathsf{M P}}(0;\\pi)=\\operatorname*{max}_{y\\in[0,1]}2(2y-1)\\left(\\mathsf{B R}(y)-\\pi(y)\\right)\\leq1-\\lambda}\\end{array}$ which implies $\\stackrel{\\cdot}{\\pi}(0)\\bar{\\leq}\\left(1-\\lambda\\right)/2$ and $\\pi\\left(1\\right)\\geq(1+\\lambda)/2$ by setting $y=0$ and $y=1$ , respectively. Thus, plugging in $y^{\\star}=1$ and $y=0$ , we conclude that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\varepsilon}\\Delta_{\\mathsf{M P}}(\\varepsilon;\\pi)\\geq\\operatorname*{max}_{|y-y^{\\star}|\\leq1}2\\left(\\mathsf{B R}(y^{\\star})-\\pi(y)\\right)\\geq2\\left(\\mathsf{B R}(1)-\\pi(0)\\right)\\geq1+\\lambda,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "since $\\mathsf{B R}(1)=1$ by (3). This implies that $\\pi$ has to be at least $(1+\\lambda)$ -risky. Note that this argument holds for any arbitrarily chosen $\\pi$ ", "page_idx": 4}, {"type": "text", "text": "Mixed Strategy Existence. Now, let us construct a concrete strategy to derive upper bounds on the (missed) opportunity and risk. The idea is to combine the strategy $\\mathsf{B R}(y)$ , which exploits the belief $y$ of types, and the Nash equilibrium strategy of this game, known as $\\overline{{x}}=1/2$ , which is a solution of the minimax problem $\\begin{array}{r}{\\operatorname*{min}_{y\\in[0,1]}\\operatorname*{max}_{x\\in[0,1]}(2y-1)(2x-1)}\\end{array}$ that enhances safety. ", "page_idx": 4}, {"type": "text", "text": "Fix $\\pi(y):=\\lambda\\mathsf{B}\\mathsf{R}(y)+(1-\\lambda)\\overline{{x}}$ a mixed strategy as a convex combination of the two strategies. The payoff gap in (4) satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Delta_{\\mathsf{M P}}(\\varepsilon;\\pi)\\leq\\operatorname*{max}_{|y-y^{*}|\\leq\\varepsilon}2(2y^{\\star}-1)\\Big(\\lambda\\left({\\mathsf{B R}}(y^{\\star})-{\\mathsf{B R}}(y)\\right)+\\left(1-\\lambda\\right)\\left({\\mathsf{B R}}(y^{\\star})-1/2\\right)\\Big).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Opportunity: Therefore, there exists a mixed strategy $\\pi$ such that when $\\varepsilon=0$ (i.e., $\\mathsf{B R}(y)=\\mathsf{B R}(y^{\\star}))$ its (missed) opportunity is bounded by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Delta_{\\mathsf{M P}}(0;\\pi)=\\operatorname*{max}_{y\\in[0,1]}2(2y-1)\\left(1-\\lambda\\right)\\left(\\mathsf{B R}(y)-1/2\\right)\\leq1-\\lambda.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Risk: Moreover, maximizing over $\\varepsilon$ such that $y=0$ $y^{\\star}=1$ $\\mathsf{B R}(y^{\\star})-\\mathsf{B R}(y)=1$ , the risk for $\\pi$ always satisfies $\\begin{array}{r}{\\operatorname*{max}_{\\varepsilon}\\Delta_{\\mathsf{M P}}(\\varepsilon;\\pi)\\leq1+\\lambda}\\end{array}$ ", "page_idx": 4}, {"type": "text", "text": "Pareto Optimality In conclusion, above construction shows that there is a mixed strategy $\\pi$ for the matching pennies that is $(1-\\lambda)$ -foregone and $(1+\\lambda)$ -risky. Conversely, any $(1-\\lambda)$ -foregone strategy must be at least $(1+\\lambda)$ -risky. The segment on the right of Figure 2 represents a Pareto front, and the strategy constructed as a convex combination is confirmed to be Pareto optimal following the arguments above. This motivating example indicates a tight tradeoff between opportunity and risk for matching pennies. In the sequel, we further generalize this result to normal-form games. ", "page_idx": 4}, {"type": "text", "text": "3.3 Opportunity-Risk Tradeoff for Normal-Form Games ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In general, the opportunity-risk tradeoff depends on the hypothesis set $\\Theta$ that contains a subset of candidate strategies and the payoff matrix of the game. We state useful definitions to characterize properties of the hypothesis set $\\Theta$ and the considered normal-form game. ", "page_idx": 5}, {"type": "text", "text": "Definition 1. Given a hypothesis set $\\Theta$ we let the diameter of $\\Theta$ with respect to the $\\ell_{1}$ -normbe $\\eta(\\Theta):=\\operatorname*{max}_{y,z\\in\\Theta}\\|y-z\\|_{1}$ . Define the following type intensity ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\kappa(\\Theta):=\\operatorname*{max}_{y,z\\in\\Theta}\\left(\\sum_{i:y_{i}\\le z_{i}}z_{i}-\\sum_{i:y_{i}>z_{i}}z_{i}\\right)\\,\\,s u b j e c t\\,t o\\,\\sum_{i:y_{i}\\le z_{i}}y_{i}<\\sum_{i:y_{i}>z_{i}}y_{i}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Furthermore,with fixed $\\Theta$ and $A$ we define the maximum and value of the game by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mu_{\\Theta}(A):=\\operatorname*{max}_{y\\in\\Theta}\\operatorname*{max}_{x\\in\\mathsf{P}_{a}}\\left|x^{\\top}A y\\right|\\ (m a x i m u m),\\quad\\nu_{\\Theta}(A):=\\operatorname*{min}_{y\\in\\Theta}\\operatorname*{max}_{x\\in\\mathsf{P}_{a}}x^{\\top}A y\\ \\left(\\nu a l u e\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The type intensity, as defined in (5), quantifies the divergence between two distributions within the set $\\Theta$ , specifically those that maximize the given objective. As the density of $\\Theta$ increases, the value Oof $\\kappa(\\Theta)$ approaches1. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.1 (NFG ExISTENCE). Fix any $\\Theta$ and consider a general-sum normal-form game where Player $^{\\,l}$ has a payoff matrix $\\boldsymbol{A}\\in\\mathbb{R}^{a\\times b}$ with $\\mu_{\\Theta}(A)\\,\\leq\\,\\mu$ and $\\nu_{\\Theta}(A)\\ge\\nu$ For any $0\\leq\\lambda\\leq1$ there exists a mixed strategy $\\pi:\\mathsf{P}_{\\Theta}\\,\\to\\,\\mathsf{P}_{a}$ forPlayer $^{\\,l}$ that is $\\left(1-\\lambda\\right)\\left(\\mu-\\nu\\right)$ -foregoneand $\\left(\\left(1-\\lambda\\right)\\left(\\mu-\\nu\\right)+\\lambda\\mu\\eta(\\Theta)\\right)$ -risky. ", "page_idx": 5}, {"type": "text", "text": "To show Theorem 3.1, we construct a mixed strategy as follows. Denote by $\\overline{{x}}$ as the following safe/minimax strategy of Player 1, which is the Nash equilibrium when the game is zero-sum: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{y\\in\\Theta}\\overline{{x}}^{\\top}A y=\\operatorname*{max}_{x\\in\\mathsf{P}_{a}}\\operatorname*{min}_{y\\in\\Theta}x^{\\top}A y.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Motivated by the matching pennies example in Section 3.2, Player 1 implements a mixed strategy $\\pi(\\rho):=\\lambda\\widetilde{x}+(1-\\lambda)\\overline{{x}}$ given the predicted belief $\\rho$ as a distribution over types in $\\Theta$ ,where $\\widetilde{x}\\in\\mathsf{P}_{a}$ is a best response strategy given $\\rho$ such that $x^{\\top}A\\mathbb{E}_{\\rho}[y]$ is maximized. The detailed proof of Theorem 3.1 is provided in Appendix A. ", "page_idx": 5}, {"type": "text", "text": "Moreover, we further show the following impossibility result, indicating that the tradeoff in Theorem 3.1 is tight. We relegate the proof of Theorem 3.2 to Appendix B. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.2 (NFG IMPOsSIBILITY). For any $\\Theta$ satisfying $\\kappa(\\Theta)\\,\\geq\\,0.$ thereisapayoffmatrix $A\\in\\mathbb{R}^{a\\times b}$ with $\\mu_{\\Theta}(A)\\leq\\mu$ and $\\nu_{\\Theta}(A)\\ge\\nu$ suchthatforany $0\\leq\\lambda\\leq1,$ if anymixedstrategy $\\pi:\\mathsf{P}_{\\Theta}\\to\\mathsf{P}_{a}$ for Player $^{\\,\\,\\,I}$ is $\\left(1-\\lambda\\right)\\left(\\mu-\\nu\\right)$ foregone,then it is at least $\\left(\\kappa(\\Theta)\\mu-\\nu\\right)(1+\\lambda)$ -risky. ", "page_idx": 5}, {"type": "text", "text": "In particular, Theorem 3.1 and 3.2 together imply the following special case when the hypothesis set $\\Theta$ contains all possible mixed strategies in $\\mathsf{P}_{b}$ . This corollary highlights that for a fair game, the tradeoff is tight and the mixed strategy $\\pi(\\rho):=\\lambda\\widetilde{x}+(1-\\lambda)\\overline{{x}}$ with $\\lambda\\in[0,1]$ is Pareto optimal when the hypothesis set contains all possible mixed strategies. ", "page_idx": 5}, {"type": "text", "text": "Corollary 3.1 (NFG PARETO OPTIMALITY). Suppose $\\Theta=\\mathsf{P}_{b}$ and the game is fair, where Player 1 has a payoff matrix $A\\in\\mathbb{R}^{a\\times b}$ with $\\|A\\|_{\\operatorname*{max}}\\leq\\alpha$ For any $0\\leq\\lambda\\leq1,$ there exists a mixed strategy $\\pi$ for Player $^{\\,l}$ that is $(1-\\lambda)\\alpha$ -foregone and $(1+\\lambda)\\alpha$ -risky. Furthermore, there is a payoff matrix $A\\in\\mathbb{R}^{a\\times b}$ with $\\|A\\|_{\\mathrm{max}}\\leq\\alpha$ such that for any $0\\leq\\lambda\\leq1$ if any mixed strategy $\\pi$ for Player 1 is $(1-\\lambda)\\,\\alpha$ -foregone, then it is at least $(1+\\lambda)\\,\\alpha$ -risky. ", "page_idx": 5}, {"type": "text", "text": "It is worth noting that the requirement can be relaxed straightforwardly so that as long as the hypothesis set contains two mixed strategies such that $\\kappa(\\Theta)\\stackrel{=}{=}1$ in (5), the statement holds. The proof of Corollary 3.1 can be found in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "4  Stochastic Bayesian Games with Untrusted Type Beliefs ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In many real-world applications, agents are coupled through a shared state in stochastic Bayesian games [47, 15], which combines standard Bayesian games [10, 11] with the stochastic games [48]. In this section, we consider an infinite-horizon time-varying discounted $n$ player stochastic Bayesian game, represented by a tuple $\\mathcal{M}:=\\langle S,\\mathcal{A},\\Theta,\\sigma,p,r,\\gamma\\rangle$ , and analyze the impact of beliefs on the opportunity-risk tradeoff. ", "page_idx": 5}, {"type": "text", "text": "4.1 Preliminaries: MDP for Stochastic Bayesian Games ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Let $\\boldsymbol{S}$ be a finite set of states. Write ${\\mathcal{N}}:=\\{1,\\ldots,n\\}$ . Let $\\boldsymbol{A}(i)$ be the set of finitely many actions that player $i\\in\\mathcal N$ can take at any state $s\\in S$ . Furthermore, $\\overset{\\vartriangle}{\\boldsymbol{A}}:=\\overset{\\vartriangle}{\\boldsymbol{A}}(1)\\times\\cdot\\cdot\\cdot\\times\\mathcal{A}(n)$ denotes the set of action profiles $a=(a(i):i\\in\\mathcal{N})$ with $a(i)\\in A(i)$ . The types of other players are unknown to. $i$ -th player. The set $\\Theta=\\prod_{\\cdot}_{j\\neq i}\\Theta(j)$ is a product type space where each opponent $j$ chooses types from $\\Theta_{j}$ . We focus on the decision-making of the $i$ -th player, considering stationary (Markov) strategies.3 At each time $t\\geq0$ each player $j\\neq i$ uses a mixed strategy $\\pi_{j}:S\\times\\Theta(j)\\to\\mathsf{P}_{A(j)}$ parameterized by a type that depends on the current state and the true type $\\theta_{j}^{\\star}\\in\\Theta(j)$ . We use $r:S\\times A\\to\\mathbb{R}$ to denote the reward function for the $i$ -th player, which is assumed to be bounded, as formally defined below. ", "page_idx": 6}, {"type": "text", "text": "Assumption 1. The reward $r:S\\times A\\to\\mathbb{R}$ satisfies that $|r(s,a)|\\leq r_{\\operatorname*{max}}$ for all $s\\in S,\\,a\\in A.$ ", "page_idx": 6}, {"type": "text", "text": "For any pair of states $(s,s^{\\prime})$ and action profile $a\\in A$ , we define $p(s^{\\prime}|s,a)$ as a transition probability from $s$ to $s^{\\prime}$ given an action profile $a$ . Finally, let $\\gamma\\in\\left(0,1\\right)$ be a discount factor. We define the expected utility of the $i$ -th player using a strategy $\\pi_{i}$ as the expected discounted total payoff ", "page_idx": 6}, {"type": "equation", "text": "$$\nJ\\left(\\pi_{i};\\theta_{-i}^{\\star}\\right):=\\mathbb{E}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}r\\left(s_{t},a_{t}\\right)\\right],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\theta_{-i}^{\\star}:=(\\theta_{j}^{\\star}\\in\\Theta(j):j\\neq i)$ , with respect to stochastic processes $\\left(s_{t}\\sim p\\left(\\cdot\\mid s_{t-1},a_{t-1}\\right)\\right)_{t>0}$ and $(a_{t}(i)\\sim\\pi_{i}\\left(s_{t}\\right))_{t\\geq0}$ , which generate the state and the action trajectories. The expectation in (8) is taken with respect to all randomness induced by the initial state distribution $s_{0}\\sim p_{0}\\in\\mathsf{P}_{\\mathcal{S}}$ , the state transition kernel $p$ , and strategy profile $\\pi$ (as well as opponents? types in $\\Theta$ ", "page_idx": 6}, {"type": "text", "text": "4.2  Opportunity, Risk, and Type Beliefs ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Suppose the $i$ -th player has beliefs of the other players\u2019 types, provided by a machine-learned forecaster, denoted by $\\theta_{-i}$ . For notational simplicity, we write $\\pi_{i}$ , the strategy for the $i$ -th player as $\\pi$ , and the strategies $\\pi_{-i}$ for the opponents as $\\sigma$ (parameterized by $\\theta_{-i.}$ ). Furthermore, we omit the subscripts and denote the type beliefs and true types as $\\theta$ and $\\theta^{\\star}$ , and write the payoff in (8) as $J\\left(\\pi\\right)$ if there is no ambiguity. The $i$ -th player uses a strategy $\\pi:S\\times\\Theta\\rightarrow\\mathsf{P}_{{A}(i)}$ , which is a function of the type beliefs of the other players. Given a strategy $\\pi$ used by the agent and strategies by other players, denoted by $\\sigma$ , we define the following useful value functions of the game: ", "page_idx": 6}, {"type": "equation", "text": "$$\nV^{\\pi,\\sigma}(s):=\\mathbb{E}_{p,\\pi,\\sigma}\\left[\\sum_{\\tau=t}^{\\infty}\\gamma^{\\tau-t}r\\left(s_{t},a_{t}\\right)\\vert s_{t}=s\\right],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "which satisfies the Bellman equation $V^{\\pi,\\sigma}(s)=\\mathbb{E}_{a\\sim\\pi(s),a^{\\prime}\\sim\\sigma(s)}\\left[\\left(r+\\gamma\\mathbb{P}V^{\\pi,\\sigma}\\right)\\left(s,(a,a^{\\prime})\\right)\\right],$ where we define an operator $\\mathbb{P}V^{\\pi}(s,(a,a^{\\prime})):=\\mathbb{E}_{s^{\\prime}\\sim p(\\cdot|s,(a,a^{\\prime}))}\\left[V^{\\pi,\\sigma}(s^{\\prime})\\right]$ ", "page_idx": 6}, {"type": "text", "text": "The following worst-case payoff gap is defined similarly as the one in (2) for normal-form games, which in our context can be considered as the dynamic regret for a strategy $\\pi_{i}$ against an optimal strategy. Note that an optimal strategy is also a best response strategy $\\pi^{\\star}$ knowing the true types of all players in hindsight maximizing (9). The optimal value function given $\\sigma$ and $\\pi^{\\star}$ is denoted by $\\bar{V}^{\\star,\\bar{\\sigma}}(\\bar{s})$ , whose Bellman optimality equation is $V^{\\star,\\sigma}(s)=\\operatorname*{max}_{a\\in\\cal A}\\left(r+\\gamma\\mathbb{P}^{\\sigma}V^{\\star,\\sigma}\\right)(s,a)$ ", "page_idx": 6}, {"type": "text", "text": "Definition 2. Given a stochastic Bayesian game $\\mathcal{M}=\\langle S,\\mathcal{A},\\Theta,\\sigma,p,r,\\gamma\\rangle$ for a fixed strategy $\\pi$ \uff0c the payoff gap is defined as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\Delta_{\\mathsf{S B G}}(\\varepsilon;\\pi):=\\operatorname*{sup}_{d(\\theta,\\theta^{\\star})\\leq\\varepsilon\\,s_{0}\\in S}\\Big(V^{\\pi^{\\star},\\sigma(\\theta^{\\star})}(s_{0})-V^{\\pi(\\theta),\\sigma(\\theta^{\\star})}(s_{0})\\Big),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $s_{0}$ denotes an initial state and $\\begin{array}{r}{d\\left(\\theta,\\theta^{\\star}\\right):=\\operatorname*{max}_{s\\in\\cal S}\\left\\|\\boldsymbol{\\sigma}(s;\\theta)-\\boldsymbol{\\sigma}(s;\\theta^{\\star})\\right\\|_{1}\\leq\\varepsilon.}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "Similar to normal-form Bayesian games, we define the (missed) opportunity and risk below. ", "page_idx": 6}, {"type": "text", "text": "Definition 3.Given a payoff gap $\\Delta_{\\mathsf{S B G}}(\\varepsilon;\\pi)$ in (10),the (missed) opportunity of $\\pi$ is $\\Delta_{\\mathsf{S B G}}(0;\\pi)$ andtheriskis $\\operatorname*{max}_{\\varepsilon>0}\\Delta_{\\mathsf{S B G}}(\\varepsilon;\\pi)$ Moreover,astrategy $\\pi$ is $\\beta$ -foregone if $\\Delta_{\\mathsf{S B G}}(0;\\pi)\\le\\beta(\\pi)$ and $\\delta$ -risky $i f\\operatorname*{max}_{\\varepsilon>0}\\Delta_{\\mathsf{S B G}}(\\varepsilon;\\pi)\\leq\\delta(\\pi)$ ", "page_idx": 6}, {"type": "image", "img_path": "QZtJ22aOV4/tmp/9486c5711e229c0e8de4f3cf15d9cc4c834e917e8c185ea5ef83fb9ea4ef304f.jpg", "img_caption": ["Figure 3: Comparison of lower and upper bounds with a varying discount factor $\\gamma$ "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Our goal is to characterize a tradeoff between the opportunity and risk for stochastic Bayesian games.   \nFinally, we define the value of the game. ", "page_idx": 7}, {"type": "text", "text": "Definition 4. Given a hypothesis set $\\Theta$ and a stochastic Bayesian game $\\mathcal{M}=\\langle S,\\mathcal{A},\\Theta,\\sigma,p,r,\\gamma\\rangle$ we denote the value of $\\mathcal{M}$ by ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\nu_{\\Theta}(\\mathcal{M}):=\\operatorname*{max}_{\\pi\\in\\Pi}\\operatorname*{min}_{\\theta_{-i}\\in\\Theta}J(\\pi;\\theta_{-i})\\,\\,\\,(\\nu a l u e).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "4.3 Opportunity-Risk Tradeoff for Stochastic Bayesian Games ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Based on the definitions above, our first result states an upper bound on the opportunity and risk for stochastic Bayesian games. Proving the bounds in Theorem 4.1 is nontrivial because the players are coupled by a shared state. Consequently, the analysis used in Section 3 for a simple convex combination of the best response to type beliefs $\\theta$ and a safe strategy in normal-form games cannot be directly applied to the stochastic setting. ", "page_idx": 7}, {"type": "text", "text": "Theorem 4.1 (SBG ExISTENCE). Consider a stochastic Bayesian game $\\mathcal{M}=\\langle S,\\mathcal{A},\\Theta,\\sigma,p,r,\\gamma\\rangle$ With $\\nu_{\\Theta}(\\mathcal{M})\\ge\\nu$ Forany $0\\leq\\lambda\\leq1$ thereexists a strategy $\\pi$ whose (missed) opportunity and risk satisfy ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\beta(\\pi)\\le\\displaystyle\\frac{1}{1-\\lambda\\gamma}\\left(C_{1}(\\gamma)r_{\\operatorname*{max}}-\\gamma\\nu\\right)(1-\\lambda),\\,\\,w i t h\\,C_{1}(\\gamma):=\\displaystyle\\frac{\\gamma^{2}-3\\gamma+6}{(1-\\gamma)^{2}},}&\\\\ &{\\delta(\\pi)\\le\\displaystyle\\frac{1}{1-\\lambda\\gamma}\\left(C_{2}(\\gamma)r_{\\operatorname*{max}}-\\gamma\\nu\\right)(1+\\lambda),\\,\\,w i t h\\,C_{2}(\\gamma):=\\operatorname*{max}\\left(C_{3}(\\gamma),C_{4}(\\gamma)\\right),}&\\\\ &{\\displaystyle\\qquad C_{3}(\\gamma):=\\displaystyle\\frac{\\gamma^{2}-3\\gamma+2}{1-\\gamma}\\,a n d\\,C_{4}(\\gamma):=\\displaystyle\\frac{2-2\\gamma^{2}+1}{(1-\\gamma)^{2}},}&\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $r_{\\mathrm{max}}$ is defined in Assumption $^{\\,I}$ ", "page_idx": 7}, {"type": "text", "text": "The proof of Theorem 4.1 is given in Appendix D, which constructs the following strategy. ", "page_idx": 7}, {"type": "text", "text": "Given type beliefs $\\theta$ , we denote a parameterized strategy ${\\widetilde{\\sigma}}:=\\sigma(\\theta)$ , and let ${\\overline{{\\sigma}}}=\\sigma({\\overline{{\\theta}}})$ be the safe strategies of the opponents, with $\\overline{{\\theta}}$ being an optimal solution of the minimax optimization in (11). Denoting $\\bar{V}:=V^{\\bar{\\star},\\tilde{\\sigma}}$ and $\\overline{{V}}:=V^{\\star,\\overline{{\\sigma}}}$ the optimal value functions with the opponents' strategies being $\\widetilde{\\sigma}$ and $\\overline{{\\sigma}}$ respectively. Unlike the convex combination used to show Theorem 3.1 for normal-form Bayesian games, the strategy constructed for proving Theorem 4.1 is a value-based strategy below ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\pi\\left(\\theta;s\\right)\\in\\underset{a\\in\\mathsf{P}_{\\mathcal{A}(i)}}{\\arg\\operatorname*{max}}\\,a^{\\top}\\left(\\lambda R_{\\widetilde{V}}(s)\\widetilde{\\sigma}(s)+(1-\\lambda)R_{\\overline{{V}}}(s)\\overline{{\\sigma}}(s)\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\lambda$ is a parameter that indicates the level of trusts on type beliefs, and for a fixed state $s\\in S$ and a given value function $V:S\\rightarrow\\mathbb{R}$ $R_{V}$ is an $\\begin{array}{r}{\\mathcal{A}(i)\\times\\prod_{j\\neq i}\\bar{\\mathcal{A}}(j)}\\end{array}$ matrix defined as ", "page_idx": 7}, {"type": "equation", "text": "$$\nR_{V}\\left(\\left(a_{i},a_{-i}\\right);s):=r\\left(s,\\left(a_{i},a_{-i}\\right)\\right)+\\gamma\\sum_{s^{\\prime}\\in{\\cal S}}p\\left(s^{\\prime}|s,(a_{i},a_{-i})\\right)V(s).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "As Theorem 4.1 indicates, this design leads a safe and exploitative play, whose efficacy is validated through case studies in Section 5. Finally, the following lower bounds can be derived, implying that the upperbounds on $\\beta(\\pi)$ and $\\delta(\\pi)$ are tight and the value-based strategy formed in (12) are Pareto optimal up to multiplicative constants. Together, the theoretical bounds are illustrated in Figure 3. ", "page_idx": 7}, {"type": "image", "img_path": "QZtJ22aOV4/tmp/5e44f90053f990cf89eca77230a166470c03207987068551223b98659ae4f09a.jpg", "img_caption": ["Figure 4: Comparison of average payoff for a player when varying values of $\\lambda$ and 6 potential discrete types for an instantiation of a $2\\times2$ game. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Theorem 4.2 (SBG IMPOsSIBILITY). There is a stochastic Bayesian game $\\mathcal{M}=\\langle S,\\mathcal{A},\\Theta,\\sigma,p,r,\\gamma\\rangle$ whose value satisfies $\\nu({\\mathcal{M}},\\Theta)~\\ge~\\nu$ such that for any $0~\\le~\\lambda~\\le~1$ if any strategy $\\pi$ is $\\begin{array}{r}{\\frac{1}{1-\\lambda\\gamma}\\left(r_{\\operatorname*{max}}-\\nu\\right)\\left(\\overset{.}{1}-\\lambda\\right)}\\end{array}$ foregone, then it isat least $\\begin{array}{r}{\\frac{1}{1-\\lambda\\gamma}\\left(r_{\\operatorname*{max}}-\\nu\\right)\\left(1+\\lambda\\right)}\\end{array}$ -risky. ", "page_idx": 8}, {"type": "text", "text": "In Theorem 4.2, the bounds on (missed) opportunity and risk differ from those in Theorem 4.1 by multiplicative constants. Theorem 4.2 can be derived by applying Theorem 3.2 for normal form games to a stateless MDP. We relegate the proof of Theorem 4.2 to Appendix E. ", "page_idx": 8}, {"type": "text", "text": "5 Case Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "$2\\times2$ Game. We provide a set of numerical evaluations to showcase the opportunity-risk tradeoff across a range of game theoretic scenarios. We begin with a focus on $2\\times2$ games and to that end we sample games spanning the topology of $2\\times2$ matrix games as shown in [49]. This topology includes the comprehensive set of 78 strictly ordinal $2\\times2$ games introduced by [50]. We formulate our evaluation as a single state Stochastic Bayesian Game with the state being a specific $2\\times2$ game. This evaluation is helpful in providing a concrete illustration of the tradeoff dynamics on a wide range of games within a particular topology given the comprehensive bench-marking done on the set of $2\\times2$ games. ", "page_idx": 8}, {"type": "text", "text": "We make use of 3 broad type classes (Markovian, Leader-Follower-Trigger-Agents, and Co-evoloved Neural Networks) inspired from work by [51]. It is from these classes that we construct our types which are then used in simulations with a player leveraging a strategy that is a convex combination of a safe strategy and the best response give type beliefs or predictions. Figs 4 and 5 shows the tradeoff in one particular game as an agent moves from being fully robust as indicated by $\\lambda=0$ to fully trusting of the advice when $\\lambda=1$ . In Appendix F, we provide further information on the particular types as well as illustrations on a couple of other games sampled from the topology of $2\\times2$ games to showcase the range of tradeoffs which exist in this landscape. ", "page_idx": 8}, {"type": "text", "text": "Security Game. Extending our empirical study to real world settings, we also provide an evaluation of the opportunity-risk tradeoff using data from wildlife tracking studies. Security games are a clear domain wherein the tradeoff between opportunity and risk is very critical to the application. Building on a large body of work that has sought to understand the implications of game theoretic analysis in the environmental conservation domain, we formulate and explore the opportunity-risk tradeoff using data from real world wildlife movements. In particular, we formulate a green security game motivated by works such as [52] wherein we simulate a defender who is trying to protect an elephant population from attackers who are illegal poachers. The defender and attacker engage a multi-state Stochastic Bayesian Game where the states are constructed from historic elephant movement data sourced from [53]. Our evaluation shows the practicality and wide ranging impact of this investigation. Figure 6 shows the tradeoff for this setting evaluated for each particular type. We include implementation details on this game in Appendix F. ", "page_idx": 8}, {"type": "image", "img_path": "QZtJ22aOV4/tmp/c1386fdc897dfda53ef496c690a356afa301ecd415bbd6966e9de58fa38a7b99.jpg", "img_caption": ["Figure 5: Left: $2\\times2$ games considered in our case study; Right: Opportunity-risk tradeoff in the evaluation of a $2\\times2$ game using an algorithm that has varying trust of type beliefs in 1,0o0 random runs. Fully trusting $\\lambda=1$ ) and distrusting ( $\\lambda=0$ ) type beliefs yield a best response strategy and a minimax strategy correspondingly. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "QZtJ22aOV4/tmp/114552a76d9a85a1a503c26ddbffc5b66323ead7656c078c33b8358823a9980a.jpg", "img_caption": ["Figure 6: Comparison of average payoff for the defender in a security game protecting an elephant population against illegal poachers when varying values of $\\lambda$ and 6 potential discrete attacker types. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Concluding Remarks ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this study, we explored the fundamental limits of safe and exploitative strategies within both normal-form and stochastic Bayesian games, where pre-established type beliefs about opponents are considered. Given that these type beliefs may be inaccurate, relying on them to exploit opponents can result in high-risk strategies. Conversely, not leveraging these beliefs yields overly cautious play, leading to missed opportunities. We have quantified these dynamics by providing upper and lower bounds on the payoff gaps corresponding to different type belief inaccuracies, thereby characterizing the tradeoffs between opportunity and risk. These bounds are consistent up to multiplicative constants. ", "page_idx": 9}, {"type": "text", "text": "Limitations and Future Directions. In our current problem setting, the dynamics of a stochastic Bayesian game is assumed to be stationary, aligning with the canonical models in the related literature (for example [51]). To address this limitation, we plan to extend our framework to include time-varying type beliefs, addressing the absence of analysis for MDPs with dynamic transition probabilities and reward structures. Additionally, refining the opportunity and risk bounds to make them tighter would provide more precise strategic insights. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We would like to thank all the anonymous reviewers for their helpful comments. ", "page_idx": 10}, {"type": "text", "text": "Tongxin Li was supported in part by the GuangDong Basic and Applied Basic Research Foundation, the National Natural Science Foundation of China under grants No.72301234, 62336005, the PengCheng Peacock Supporting Scientific Research Fund Category C (2024TC0024), the GuangDong Provincial Key Laboratory of Mathematical Foundations for Artificial Intelligence (2023B1212010001), and the Shenzhen Key Lab of Crowd Intelligence Empowered Low-Carbon Energy Network (No. ZDSYS20220606100601002). ", "page_idx": 10}, {"type": "text", "text": "Adam Wierman was supported in part by the U.S. NSF under grants CNS-2146814, CPS-2136197, CNS-2106403, NGSDI-2105648. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Xue Yan, Jiaxian Guo, Xingzhou Lou, Jun Wang, Haifeng Zhang, and Yali Du. An efficient endto-end training approach for zero-shot human-ai coordination. Advances in Neural Information Processing Systems, 36, 2024.   \n[2] Shiyong Wang, Jiafu Wan, Daqiang Zhang, Di Li, and Chunhua Zhang. Towards smart factory for industry 4.0: a self-organized multi-agent system with big data based feedback and coordination. Computer networks, 101:158-168, 2016.   \n[3] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiey, et al. Grandmaster level in starcraft i using multi-agent reinforcement learning. Nature, 575(7782):350-354, 2019.   \n[4] Noam Brown and Tuomas Sandholm. Superhuman ai for multiplayer poker. Science, 365(6456):885-890, 2019.   \n[5] Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. Advances in neural information processing systems, 30, 2017.   \n[6]  Stefano V Albrecht and Peter Stone. Autonomous agents modelling other agents: A comprehensive survey and open problems. Artificial Intelligence, 258:66-95, 2018.   \n[7]  Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Monotonic value function factorisation for deep multi-agent reinforcement learning. Journal of Machine Learning Research, 21(178):1-51, 2020.   \n[8] Ioannis Anagnostides, Ioannis Panageas, Gabriele Farina, and Tuomas Sandholm. On the convergence of no-regret learning dynamics in time-varying games. Advances in Neural Information Processing Systems, 36, 2024.   \n[9] Simin Li, Jun Guo, Jingqiao Xiu, Ruixiao Xu, Xin Yu, Jiakai Wang, Aishan Liu, Yaodong Yang, and Xianglong Liu. Byzantine robust cooperative multi-agent reinforcement learning as a bayesian game. In The Twelfth International Conference on Learning Representations, 2023.   \n[10] John C Harsanyi. Games with incomplete information played by \\*\"bayesian\u201d players, i-i part i. the basic model. Management science, 14(3): 159-182, 1967.   \n[11]  John C Harsanyi. Games with incomplete information played by \u201c\"bayesian\" players part ii. bayesian equilibrium points. Management science, 14(5):320-334, 1968.   \n[12] Lawrence Friedman. Optimal bluffing strategies in poker. Management Science, 17(12):B-764, 1971.   \n[13] Norman Zadeh. Computation of optimal poker strategies. Operations Research, 25(4):541-562, 1977.   \n[14]  David Milec, Ondrej Kubicek, and Viliam Lisy. Continual depth-limited responses for computing counter-strategies in sequential games. arXiv preprint arXiv:2112.12594, 2021.   \n[15] Stefano V Albrecht, Jacob W Crandall, and Subramanian Ramamoorthy. Belief and truth in hypothesised behaviours. Artificial Intelligence, 235:63-94, 2016.   \n[16] James S Jordan. Bayesian learning in normal form games. Games and Economic Behavior, 3(1):60-81, 1991.   \n[17]  Martin Bichler, Max Fichtl, and Matthias Oberlechner. Computing bayes-nash equilibrium strategies in auction games via simultaneous online dual averaging. Operations Research, 2023.   \n[18] Martin Bichler, Maximilian Fichtl, Stefan Heidekriger, Nils Kohring, and Paul Sutterer. Learning equilibria in symmetric auction games using artificial neural networks. Nature machine intelligence, 3(8):687-695, 2021.   \n[19] Paul Milgrom and John Roberts. Adaptive and sophisticated learning in normal form games. Games and economic Behavior, 3(1):82-100, 1991.   \n[20] Ehud Kalai and Ehud Lehrer. Rational learning leads to nash equilibrium. Econometrica: Journal of the Econometric Society, pages 1019-1045, 1993.   \n[21] John H Nachbar. Prediction, optimization, and learning in repeated games. Econometrica: Journal of the Econometric Society, pages 275-309, 1997.   \n[22] John H Nachbar. Beliefs in repeated games. Econometrica, 73(2):459-480, 2005.   \n[23]  Dean P Foster and H Peyton Young. On the impossibility of predicting the behavior of rational agents. Proceedings of the National Academy of Sciences, 98(22): 12848-12853, 2001.   \n[24] Eddie Dekel, Drew Fudenberg, and David K Levine. Learning to play bayesian games. Games and economic behavior, 46(2):282-303, 2004.   \n[25] Finnegan Southey, Michael P Bowling, Bryce Larson, Carmelo Piccione, Neil Burch, Darse Billings, and Chris Rayner. Bayes bluff: Opponent modelling in poker. arXiv preprint arXiv:1207.1411, 2012.   \n[26]  Mohammad Mahdian, Hamid Nazerzadeh, and Amin Saberi. Online optimization with uncertain information. ACM Transactions on Algorithms (TALG), 8(1):1-29, 2012.   \n[27] Manish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms via ml predictions. In Advances in Neural Information Processing Systems, pages 9661-9670, 2018.   \n[28]  Dhruv Rohatgi. Near-optimal bounds for online caching with machine learned advice. In Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1834-1845. SIAM, 2020.   \n[29]  Thodoris Lykouris and Sergei Vassilvitski. Competitive caching with machine learned advice. Journal of the ACM (JACM), 68(4):1-25, 2021.   \n[30]  Sungjin Im, Ravi Kumar, Aditya Petety, and Manish Purohit. Parsimonious learning-augmented caching. In International Conference on Machine Learning, pages 9588-9601. PMLR, 2022.   \n[31]  Antonios Antoniadis, Christian Coester, Marek Elias, Adam Polak, and Bertrand Simon. Online metric algorithms with untrusted predictions. In International Conference on Machine Learning, pages 345-355. PMLR, 2020.   \n[32] Nicolas Christianson, Tinashe Handina, and Adam Wierman. Chasing convex bodies and functions with black-box advice. In Conference on Learning Theory, pages 867-908. PMLR, 2022.   \n[33]  Pengfei Li, Jianyi Yang, Adam Wierman, and Shaolei Ren. Robust learning for smoothed online convex optimization with feedback delay. Advances in Neural Information Processing Systems, 36, 2024.   \n[34]  Tongxin Li, Yue Chen, Bo Sun, Adam Wierman, and Steven Low. Information aggregation for constrained online control. ACM SIGMETRICS Performance Evaluation Review, 49(1):7-8, 2021.   \n[35]  Tongxin Li, Ruixiao Yang, Guannan Qu, Guanya Shi, Chenkai Yu, Adam Wierman, and Steven Low. Robustness and consistency in linear quadratic control with untrusted predictions. ACM SIGMETRICS Performance Evaluation Review, 50(1):107-108, 2022.   \n[36]  Yiheng Lin, Yang Hu, Guannan Qu, Tongxin Li, and Adam Wierman. Bounded-regret mpc via perturbation analysis: Prediction error, constraints, and nonlinearity. Advances in Neural Information Processing Systems, 35:36174-36187, 2022.   \n[37] Tongxin Li, Ruixiao Yang, Guannan Qu, Yiheng Lin, Adam Wierman, and Steven H Low. Certifying black-box policies with stability for nonlinear control. IEEE Open Journal of Control Systems, 2:49-62, 2023.   \n[38]  Noah Golowich and Ankur Moitra. Can q-learning be improved with advice? In Conference on Learning Theory, pages 4548-4619. PMLR, 2022.   \n[39]  Tongxin Li, Yiheng Lin, Shaolei Ren, and Adam Wierman. Beyond black-box advice: Learningaugmented algorithms for mdps with q-value predictions. Advances in Neural Information Processing Systems, 36, 2024.   \n[40] Jianyi Yang, Pengfei Li, Tongxin Li, Adam Wierman, and Shaolei Ren. Anytime-competitive reinforcement learning with policy prior. Advances in Neural Information Processing Systems, 36, 2024.   \n[41] Tongxin Li, Bo Sun, Yue Chen, Zixin Ye, Steven H Low, and Adam Wierman. Learningbased predictive control via real-time aggregate fexibility. IEEE Transactions on Smart Grid, 12(6):4897-4913, 2021.   \n[42]  Nicolas Christianson, Christopher Yeh, Tongxin Li, Mahdi Torabi Rad, Azarang Golmohammadi, and Adam Wierman. Robustifying machine-learned algorithms for efficient grid operation. In NeurIPS 2022 Workshop on Tackling Climate Change with Machine Learning, 2022.   \n[43]  Tongxin Li. Learning-Augmented Control and Decision-Making: Theory and Applications in Smart Grids. PhD thesis, California Institute of Technology, 2023.   \n[44]  Tongxin Li and Chenxi Sun. Out-of-distribution-aware electric vehicle charging. IEEE Transactions on Transportation Electrification, 2024.   \n[45] Tiancheng Jin, Longbo Huang, and Haipeng Luo. The best of both worlds: stochastic and adversarial episodic mdps with unknown transition. Advances in Neural Information Processing Systems, 34:20491-20502, 2021.   \n[46] Idan Amir, Guy Azov, Tomer Koren, and Roi Livni. Better best of both worlds bounds for bandits with switching costs. Advances in neural information processing systems, 35:15800-15810, 2022.   \n[47] Stefano V Albrecht and Subramanian Ramamoorthy. A game-theoretic model and bestresponse learning method for ad hoc coordination in multiagent systems. arXiv preprint arXiv:1506.01170, 2015.   \n[48] Lloyd S Shapley. Stochastic games. Proceedings of the national academy of sciences, 39(10):1095-1100, 1953.   \n[49]  Bryan Bruns. Names for games: Locating $2\\times2$ games. Games, 6(4):495-520, October 2015.   \n[50]  Anatol Rapoport and Melvin Guyer. A taxonomy of $2\\times2$ games. General Systems: Yearbook of the Society for General Systems Research, 11:203-214, 1966.   \n[51] Stefano V. Albrecht, Jacob W. Crandall, and Subramanian Ramamoorthyc. Belief and truth in hypothesised behaviours. arXiv preprint arXiv:1507.07688, 2015.   \n[52] Fei Fang, Peter Stone, and Milind Tambe. When security games go green: Designing defender strategies to prevent poaching and illegal fishing. IJCAl, 2015.   \n[53]  Chamaille-Jammes. African elephant (migration) chamaill\u00e9-jammes hwange np, 2009-2017.   \n[54] Satinder P Singh and Richard C Yee. An upper bound on the loss from approximate optimalvalue functions. Machine Learning, 16:227-233, 1994.   \n[55]  Jacob W. Crandall. Towards minimizing disappointment in repeated games. Journal of Artificial Intelligence Research (JAIR), 49:111-142, 2014.   \n[56]John R. Koza. Genetic Programming: On the Programming of Computers by Means of Natural Selection. The MIT Press, 1992. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Broader Impacts. The implications of our research extend beyond theoretical interests and have practical significance in fields such as economics, cybersecurity, and strategic planning, where decision-making under uncertainty is crucial. By improving the understanding of how predictive information can be used safely and effectively in competitive environments, our work supports the development of more robust strategies in these areas. This can lead to better risk management practices and enhance the ability of systems to make informed decisions even when faced with unreliable or incomplete information. However, there are potential negative impacts, such as the risk of misuse of predictive models that may lead to biased or unfair decisions if the underlying data or assumptions are flawed. Future research could focus on reducing these risks, ensuring AI systems remain reliable and safe. ", "page_idx": 14}, {"type": "text", "text": "AProof of Theorem 3.1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We first consider bounding the opportunity, focusing on the case when the predicted belief contains no error, i.e., $\\boldsymbol{y}^{\\star}=\\mathbb{E}_{\\rho}[\\boldsymbol{y}]$ . Then, by the definition of $\\bar{\\pi}(\\rho)$ , for any payoff matrix $A$ with $\\mu_{\\Theta}(A)\\leq\\mu$ and $\\nu_{\\Theta}(A)\\ge\\nu$ ,we get ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{\\mathsf{N F G}}(0;\\pi)=\\displaystyle\\operatorname*{max}_{y^{\\star}\\in\\Theta}\\left(\\displaystyle\\operatorname*{max}_{x\\in\\mathsf{P}_{a}}x^{\\top}A y^{\\star}-\\pi(\\rho)A y^{\\star}\\right)}\\\\ &{\\qquad\\qquad\\leq(1-\\lambda)\\left(\\displaystyle\\operatorname*{max}_{x\\in\\mathsf{P}_{a}}\\operatorname*{max}_{y^{\\star}\\in\\Theta}x^{\\top}A y^{\\star}-\\displaystyle\\operatorname*{min}_{y^{\\star}\\in\\Theta}\\overline{{x}}^{\\top}A y^{\\star}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where we have maximized the two terms $\\operatorname*{max}_{x\\in\\mathsf{P}_{a}}{x^{\\mathsf{T}}A y^{\\star}}$ and $\\overline{{x}}^{\\top}A y^{\\star}$ separately over $y^{\\star}\\in\\Theta$ to obtain (13). Therefore, noting the definition of the safe strategy $\\textstyle{\\overline{{x}}}$ in (7), ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Delta_{\\mathsf{N F G}}(0;\\pi)\\leq\\!(1-\\lambda)\\left(\\operatorname*{max}_{x\\in\\mathsf{P}_{a}}\\operatorname*{max}_{y^{\\star}\\in\\Theta}x^{\\top}A y^{\\star}-\\operatorname*{max}_{x\\in\\mathsf{P}_{a}}\\operatorname*{min}_{y^{\\star}\\in\\Theta}x^{\\top}A y^{\\star}\\right)\\leq(1-\\lambda)\\left(\\mu-\\nu\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Now, we consider bounding the risk. By the definition of the following mixed strategy $\\pi(\\rho)$ used by Player 1, for any payoff matrix $A$ with $[\\mu_{\\Theta}(A)\\leq\\mu,\\nu_{\\Theta}(A)\\geq\\nu.$ and $\\|A\\|_{\\operatorname*{max}}\\leq\\alpha$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\Delta_{\\mathsf{N F G}}(\\varepsilon;\\pi)=\\underset{d(\\rho,y^{\\star})\\leq\\varepsilon}{\\operatorname*{max}}\\left(\\underset{x\\in\\mathsf{P}_{a}}{\\operatorname*{max}}\\,x^{\\top}A y^{\\star}-\\pi(\\rho)^{\\top}A y^{\\star}\\right)}\\\\ &{\\leq\\underset{d(\\rho,y^{\\star})\\leq\\varepsilon}{\\operatorname*{max}}\\left(\\underset{x\\in\\mathsf{P}_{a}}{\\operatorname*{max}}\\,x^{\\top}A\\mathbb{E}_{\\rho}[y]-\\lambda\\widetilde{x}^{\\top}A y^{\\star}\\right)-(1-\\lambda)\\underset{y^{\\star}\\in\\Theta}{\\operatorname*{min}}\\,\\overline{{x}}^{\\top}A y^{\\star}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In (15), we replace $\\operatorname*{max}_{x\\in\\mathsf{P}_{a}}{x^{\\top}A y^{\\star}}$ by $\\operatorname*{max}_{x\\in\\mathsf{P}_{a}}\\boldsymbol{x}^{\\top}A\\mathbb{E}_{\\rho}[y]$ since $y^{\\star}\\in\\Theta$ and there exists a feasible $\\rho$ with $\\mathbb{E}_{\\rho}[y]=y^{\\star}$ that always satisfies the constraint $d\\left({\\dot{\\rho}},y^{\\star}\\right)\\leq\\varepsilon$ ", "page_idx": 14}, {"type": "text", "text": "Since $\\widetilde{x}$ is a best response strategy given $\\rho$ m $\\operatorname{ax}_{x\\in\\mathsf{P}_{a}}\\,x^{\\top}A\\mathbb{E}_{\\rho}[y]\\;=\\;\\widetilde{x}^{\\top}A\\mathbb{E}_{\\rho}[y]$ .Decomposing $\\begin{array}{r}{\\operatorname*{max}_{x\\in\\mathsf{P}_{a}}{x^{\\top}A y^{\\star}}=(1-\\lambda)\\operatorname*{max}_{x\\in\\mathsf{P}_{a}}{x^{\\top}A y^{\\star}}+\\lambda\\operatorname*{max}_{x\\in\\mathsf{P}_{a}}{x^{\\top}A y^{\\star}}\\,.}\\end{array}$ and maximizing the two terms $(1-\\lambda)\\operatorname*{max}_{x\\in\\mathsf{P}_{a}}\\boldsymbol{x}^{\\top}A\\boldsymbol{y}^{\\star}$ and $\\lambda\\operatorname*{max}_{x\\in\\mathsf{P}_{a}}x^{\\top}A y^{\\star}$ over $d\\left(\\rho,y^{\\star}\\right)\\leq\\varepsilon$ respectively, $\\Delta(\\varepsilon;\\alpha,\\pi)$ can be further bounded from above by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{A:\\|A\\|_{\\operatorname*{max}}\\leq\\alpha}\\left((1-\\lambda)\\operatorname*{max}_{x\\in\\mathsf{P}_{a,y^{*}}\\in\\Theta}x^{\\top}A y^{\\star}+\\lambda\\operatorname*{max}_{d(\\rho,y^{*})\\leq\\varepsilon}\\left(\\widetilde{x}^{\\top}A(\\mathbb{E}_{\\rho}[y]-y^{\\star})\\right)-(1-\\lambda)\\nu\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Since $d\\left(\\rho,y^{\\star}\\right):=\\|\\mathbb{E}_{\\rho}[y]-y^{\\star}\\|_{1}\\leq\\varepsilon.$ (16) yields ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\Delta_{\\mathsf{N F G}}(\\varepsilon;\\pi)\\leq\\!\\left(1-\\lambda\\right)(\\mu-\\nu)+\\lambda\\mu\\varepsilon.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Maximizing over $\\varepsilon\\leq\\eta(\\Theta)$ for (17), we conclude the theorem. ", "page_idx": 14}, {"type": "text", "text": "BProof of Theorem 3.2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We first suppose a mixed strategy $\\pi:\\mathsf{P}_{\\Theta}\\to\\mathsf{P}_{a}$ for Player 1 is $\\left(1-\\lambda\\right)\\left(\\mu-\\nu\\right)$ -foregone, given a belief of types $\\rho\\in\\mathsf{P}_{\\Theta}$ ", "page_idx": 14}, {"type": "text", "text": "Let $y^{\\prime}$ and $y^{\\prime\\prime}$ be two mixed strategies in $\\Theta$ that achieve $\\kappa(\\Theta)\\geq0$ (select arbitrary strategies if there are multiple to break the tie), i.e., ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\kappa(\\Theta)=\\sum_{i:y_{i}^{\\prime}\\le y_{i}^{\\prime\\prime}}y_{i}^{\\prime\\prime}-\\sum_{i:y_{i}^{\\prime}>y_{i}^{\\prime\\prime}}y_{i}^{\\prime\\prime}\\,\\mathrm{subject}\\,\\mathrm{to}\\,\\sum_{i:y_{i}^{\\prime}\\le y_{i}^{\\prime\\prime}}y_{i}^{\\prime}<\\sum_{i:y_{i}^{\\prime}>y_{i}^{\\prime\\prime}}y_{i}^{\\prime}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Let $\\mathcal{T}_{+}\\,:=\\,\\{i\\,\\in\\,[b]\\,:\\,y_{i}^{\\prime}\\,>\\,y_{i}^{\\prime\\prime}\\}$ and ${\\mathcal{Z}}_{-}\\,:=\\,\\{i\\,\\in\\,[b]\\,:\\,y_{i}^{\\prime}\\,\\leq\\,y_{i}^{\\prime\\prime}\\}$ be two indices of actions with non-negative and positive coordinates of $z$ where $[b]:=\\{1,\\ldots,b\\}$ . Consider a payoff matrix $A_{\\mu}$ with the following form. Suppose without loss of generality that $a$ is even; otherwise, we append an all-zero row to $A_{\\mu}$ . Let $\\beta:=\\mu-\\nu$ . For the $i$ -th column of $A_{\\mu}$ , we set it as $(\\beta,-\\beta,\\ldots,-\\stackrel{\\ldots}{\\beta})^{\\top}$ if $i\\in{\\mathcal{Z}}_{+}$ and $(-\\beta,\\beta,\\ldots,\\beta)^{\\top}$ if $i\\in\\mathcal{Z}_{-}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\nA=A_{\\mu}=\\left[\\begin{array}{c c c c c c}{\\cdot\\cdot\\cdot}&{\\beta}&{-\\beta}&{\\cdot\\cdot\\cdot}\\\\ {\\cdot\\cdot}&{-\\beta}&{\\beta}&{\\cdot\\cdot\\cdot}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}\\\\ {\\cdot\\cdot}&{\\underbrace{-\\beta}_{i\\in\\mathbb{Z}_{+}}}&{\\underbrace{\\beta}_{i+1\\in\\mathbb{Z}_{-}}}&{\\cdot\\cdot}\\end{array}\\right]+A_{\\nu},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $(A_{\\nu})_{i j}=\\nu$ for all $i\\in[a]$ and $j\\in[b]$ . Clearly, with this $A$ $\\mu_{\\Theta}(A)\\leq\\mu$ and $\\nu_{\\Theta}(A)\\ge\\nu$ ", "page_idx": 15}, {"type": "text", "text": "Then, by definition, setting $\\varepsilon=0$ with $\\mathbb{E}_{\\rho}[y]=y^{\\star}$ (the type belief contains no error), the payoff gap satisfies ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Delta_{\\mathsf{N F G}}(0;\\pi)=\\operatorname*{max}_{y^{\\star}\\in\\Theta}\\left(\\operatorname*{max}_{x\\in\\mathsf{P}_{a}}{x^{\\top}A y^{\\star}}-\\pi(\\rho)A y^{\\star}\\right)\\leq(1-\\lambda)(\\mu-\\nu).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now, plugging in $y^{\\prime}$ into (18) above. This implies ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{y^{\\star}\\in\\Theta}\\left(\\operatorname*{max}_{x\\in\\mathsf{P}_{a}}{x^{\\top}A y^{\\star}}-\\pi(\\rho)^{\\top}A y^{\\star}\\right)\\geq\\operatorname*{max}_{x\\in\\mathsf{P}_{a}}{x^{\\top}A y^{\\prime}}-\\pi(\\rho)^{\\top}A y^{\\prime}=\\mu-\\pi(\\rho)^{\\top}A y^{\\prime}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For notational convenince, dente $\\begin{array}{r}{F(\\Theta):=\\sum_{i:y_{i}^{\\prime}>y_{i}^{\\prime\\prime}}y_{i}^{\\prime}-\\sum_{i:y_{i}^{\\prime}\\leq y_{i}^{\\prime\\prime}}y_{i}^{\\prime}}\\end{array}$ Combining (18) and (19), $\\pi(\\rho)^{\\top}A y^{\\prime}=\\pi(\\rho)^{\\top}(A_{\\mu}+A_{\\nu})y^{\\prime}\\geq(1-\\lambda)\\nu+\\dot{\\lambda}\\mu$ . Thus, $\\pi(\\rho)^{\\dagger}A_{\\mu}y^{\\prime}\\geq\\lambda(\\mu-\\nu)$ . Equivalently, $F\\left(\\Theta\\right)\\left(\\mu-\\nu\\right)\\pi(\\rho)^{\\top}f\\geq\\lambda\\left(\\mu-\\nu\\right)$ , where $f:=(1,-1,1,\\ldots,1,-1)^{\\top}$ . By the construction of $A$ \uff0c we get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\pi(\\rho)^{\\top}A_{\\mu}y^{\\prime\\prime}=-\\left(\\pi(\\rho)^{\\top}f\\right)(\\mu-\\nu)\\cdot\\left(\\sum_{i:y_{i}^{\\prime}\\leq y_{i}^{\\prime\\prime}}y_{i}^{\\prime\\prime}-\\sum_{i:y_{i}^{\\prime}>y_{i}^{\\prime\\prime}}y_{i}^{\\prime\\prime}\\right)\\leq-\\lambda\\left(\\mu-\\nu\\right)\\cdot\\frac{\\kappa(\\Theta)}{F(\\Theta)}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, since $0<F(\\Theta)\\leq1$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\pi(\\rho)^{\\top}(A_{\\mu}+A_{\\nu})y^{\\prime\\prime}=\\nu+\\pi(\\rho)^{\\top}A_{\\mu}y^{\\prime\\prime}\\leq\\nu-\\lambda\\,(\\mu-\\nu)\\,\\kappa(\\Theta)=-\\lambda\\mu\\kappa(\\Theta)+(1+\\lambda\\kappa(\\Theta))\\nu.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Hence, we get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{y^{\\star}\\in\\Theta}{\\operatorname*{max}}\\left(\\underset{x\\in\\mathsf{P}_{a}}{\\operatorname*{max}}\\,x^{\\top}A y^{\\star}-\\pi(\\rho)^{\\top}A y^{\\star}\\right)\\geq\\underset{x\\in\\mathsf{P}_{a}}{\\operatorname*{max}}\\,x^{\\top}A y^{\\prime\\prime}-\\pi(\\rho)^{\\top}A y^{\\prime\\prime}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\geq\\kappa(\\Theta)\\left(1+\\lambda\\right)\\mu-(1+\\lambda\\kappa(\\Theta))\\nu}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq(\\kappa(\\Theta)\\mu-\\nu)\\left(1+\\lambda\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "C Proof of Corollary 3.1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "If $\\Theta=\\mathsf{P}_{b}$ , then $\\kappa(\\Theta)=1$ . Furthermore, by definition, we get ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mu_{A}\\left(\\mathsf{P}_{b}\\right):=\\operatorname*{max}_{y\\in\\mathsf{P}_{b}}\\operatorname*{max}_{x\\in\\mathsf{P}_{a}}\\left|x^{\\top}A y\\right|=\\|A\\|_{\\operatorname*{max}},\\quad\\nu_{A}\\left(\\mathsf{P}_{b}\\right):=\\operatorname*{min}_{y\\in\\mathsf{P}_{b}}\\operatorname*{max}_{x\\in\\mathsf{P}_{a}}x^{\\top}A y=0,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the equality on the RHS holds since the game is fair. Applying Theorem 3.1 and 3.2, we obtain the corollary. ", "page_idx": 15}, {"type": "text", "text": "D Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Given type beliefs, we denote strategies $\\widetilde{\\sigma}\\;:=\\;\\sigma(\\theta)$ \uff0c $\\sigma:=\\,\\sigma\\left(\\theta^{\\star}\\right)$ , and let ${\\overline{{\\sigma}}}\\,=\\,\\sigma({\\overline{{\\theta}}})$ be the safe strategies of the opponents, with $\\overline{{\\theta}}$ being an optimal solution of the minimax optimization in (11). Denoting $\\widetilde{V}:=\\bar{V}^{\\star,\\widetilde{\\sigma}}$ \uff0c $\\overline{{V}}:=V^{\\star,\\overline{{\\sigma}}}$ \uff0c $V^{\\star}:=V^{\\star,\\sigma}$ the value functions with the opponents\u2019 strategies being $\\widetilde{\\sigma},\\overline{{\\sigma}}$ , and $\\sigma$ , respectively. Given the strategy $\\pi$ defined in (12), we first state two useful bounds. Lemma 1. Given the strategy defined in (12), the following bounds on the reward function hold: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{BoUND}\\ 1:\\quad\\mathbb{E}_{\\pi^{\\star},\\sigma}\\left[r\\right]-\\mathbb{E}_{\\pi,\\sigma}\\left[r\\right]\\leq\\gamma\\lambda\\left(\\mathbb{E}_{p,\\pi,\\widetilde{\\sigma}}\\left[V^{\\star}(s)\\right]-\\mathbb{E}_{p,\\pi^{\\star},\\widetilde{\\sigma}}\\left[V^{\\star}(s)\\right]\\right)+2\\lambda\\left(\\varepsilon r_{\\operatorname*{max}}+\\gamma\\eta\\right)}&{}\\\\ {+\\gamma(1-\\lambda)\\left(\\mathbb{E}_{p,\\pi,\\overline{{\\sigma}}}\\left[V(s)\\right]-\\mathbb{E}_{p,\\pi^{\\star},\\overline{{\\sigma}}}\\left[V(s)\\right]\\right)}&{}\\\\ {+(1-\\lambda)\\left(\\mathbb{E}_{\\pi^{\\star},\\sigma}\\left[r\\right]-\\mathbb{E}_{\\pi^{\\star},\\overline{{\\sigma}}}\\left[r\\right]\\right)+(1-\\lambda)\\left(\\mathbb{E}_{\\pi,\\overline{{\\sigma}}}\\left[r\\right]-\\mathbb{E}_{\\pi,\\sigma}\\left[r\\right]\\right),\\quad}&{\\left(20\\pi\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\eta:=\\operatorname*{max}_{s\\in S}\\Big|V^{\\star}(s)-\\widetilde{V}(s)\\Big|,$ and ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\textsf{D2:}\\mathbb{E}_{\\overline{{\\pi}},\\overline{{\\sigma}}}\\left[r\\right]-\\mathbb{E}_{\\pi,\\overline{{\\sigma}}}\\left[r\\right]\\leq\\gamma\\left(\\mathbb{E}_{p,\\pi,\\overline{{\\sigma}}}\\left[\\overline{{V}}(s)\\right]-\\mathbb{E}_{p,\\overline{{\\pi}},\\overline{{\\sigma}}}\\left[\\overline{{V}}(s)\\right]\\right)}\\\\ &{\\qquad\\qquad+\\frac{\\lambda}{1-\\lambda}\\left((\\mathbb{E}_{\\pi,\\tilde{\\sigma}}\\left[r\\right]-\\mathbb{E}_{\\overline{{\\pi}},\\widetilde{\\sigma}}\\left[r\\right])+\\gamma\\left(\\mathbb{E}_{p,\\pi,\\widetilde{\\sigma}}\\left[\\widetilde{V}(s)\\right]-\\mathbb{E}_{p,\\overline{{\\pi}},\\widetilde{\\sigma}}\\left[\\widetilde{V}(s)\\right]\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma $^{\\,l}$ . First, we prove BOUND 1 in (20). ", "page_idx": 16}, {"type": "text", "text": "Based on the definition of the policy $\\pi$ in (12), it follows that for any fixed state $s\\in S$ \uff0c ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\lambda\\mathbb E_{\\pi^{\\star},\\widetilde{\\sigma}}\\left[r\\right]+(1-\\lambda)\\mathbb E_{\\pi^{\\star},\\overline{{\\sigma}}}\\left[r\\right]+\\gamma\\lambda\\mathbb E_{p,\\pi^{\\star},\\widetilde{\\sigma}}\\left[\\widetilde{V}(s)\\right]+\\gamma(1-\\lambda)\\mathbb E_{p,\\pi^{\\star},\\overline{{\\sigma}}}\\left[\\overline{{V}}(s)\\right]}\\\\ &{\\le\\lambda\\mathbb E_{\\pi,\\widetilde{\\sigma}}\\left[r\\right]+(1-\\lambda)\\mathbb E_{\\pi,\\overline{{\\sigma}}}\\left[r\\right]+\\gamma\\lambda\\mathbb E_{p,\\pi,\\widetilde{\\sigma}}\\left[\\widetilde{V}(s)\\right]+\\gamma(1-\\lambda)\\mathbb E_{p,\\pi,\\overline{{\\sigma}}}\\left[\\overline{{V}}(s)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Furthermore, considering the difference between the expected rewards corresponding to implementing $\\pi^{\\star}$ and the strategy $\\pi$ defined in (12), since by Assumption 1, $|r|\\leq r_{\\operatorname*{max}}$ for alli $t\\,\\in\\,[T]$ and any strategy $\\pi\\in\\Pi$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{\\pi,\\sigma}\\left[r\\right]-\\mathbb{E}_{\\pi,\\tilde{\\sigma}}\\left[r\\right]\\right|\\le\\operatorname*{max}_{s\\in\\mathcal{S}}\\left\\|\\sigma(s)-\\tilde{\\sigma}(s)\\right\\|_{1}r_{\\operatorname*{max}}\\le\\varepsilon r_{\\operatorname*{max}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, using (23), ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\pi^{*},\\sigma}\\left[r\\right]-\\mathbb{E}_{\\pi,\\sigma}\\left[r\\right]=\\lambda\\left(\\mathbb{E}_{\\pi^{*},\\sigma}\\left[r\\right]-\\mathbb{E}_{\\pi,\\sigma}\\left[r\\right]\\right)+(1-\\lambda)\\left(\\mathbb{E}_{\\pi^{*},\\sigma}\\left[r\\right]-\\mathbb{E}_{\\pi,\\sigma}\\left[r\\right]\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\le\\lambda\\left(\\mathbb{E}_{\\pi^{*},\\sigma}\\left[r\\right]-\\mathbb{E}_{\\pi,\\widetilde{\\sigma}}\\left[r\\right]\\right)+(1-\\lambda)\\left(\\mathbb{E}_{\\pi^{*},\\sigma}\\left[r\\right]-\\mathbb{E}_{\\pi,\\widetilde{\\sigma}}\\left[r\\right]\\right)+2\\lambda\\varepsilon r_{\\operatorname*{max}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad+\\left(1-\\lambda\\right)\\left(\\mathbb{E}_{\\pi^{*},\\sigma}\\left[r\\right]-\\mathbb{E}_{\\pi^{*},\\overline{{\\sigma}}}\\left[r\\right]\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad+\\left(1-\\lambda\\right)\\left(\\mathbb{E}_{\\pi,\\overline{{\\sigma}}}\\left[r\\right]-\\mathbb{E}_{\\pi,\\sigma}\\left[r\\right]\\right)}\\\\ &{\\quad\\quad\\quad\\quad=(\\lambda\\mathbb{E}_{\\pi^{*},\\widetilde{\\sigma}}\\left[r\\right]+(1-\\lambda)\\mathbb{E}_{\\pi^{*},\\overline{{\\sigma}}}\\left[r\\right])-(\\lambda\\mathbb{E}_{\\pi,\\widetilde{\\sigma}}\\left[r\\right]+(1-\\lambda)\\mathbb{E}_{\\pi,\\overline{{\\sigma}}}\\left[r\\right])}\\\\ &{\\quad\\quad\\quad\\quad+\\left(1-\\lambda\\right)\\left(\\mathbb{E}_{\\pi^{*},\\sigma}\\left[r\\right]-\\mathbb{E}_{\\pi^{*},\\overline{{\\sigma}}}\\left[r\\right]\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\left(1-\\lambda\\right)\\left(\\mathbb{E}_{\\pi,\\overline{{\\sigma}}}\\left[r\\right]-\\mathbb{E}_{\\pi,\\sigma}\\left[r\\right]\\right)+2\\lambda\\varepsilon r_{\\operatorname*{max}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Combining (22) and (24), we obtain ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{\\pi^{\\star},\\sigma}\\left[r\\right]-\\mathbb{E}_{\\pi,\\sigma}\\left[r\\right]-2\\lambda\\varepsilon r_{\\operatorname*{max}}}\\\\ &{\\le\\gamma\\left(\\lambda\\big(\\mathbb{E}_{p,\\pi,\\widetilde{\\sigma}}\\left[\\widetilde{V}(s)\\right]-\\mathbb{E}_{p,\\pi^{\\star},\\widetilde{\\sigma}}\\left[\\widetilde{V}(s)\\right]\\big)+(1-\\lambda)\\big(\\mathbb{E}_{p,\\pi,\\overline{{\\sigma}}}\\left[\\widetilde{V}(s)\\right]-\\mathbb{E}_{p,\\pi^{\\star},\\overline{{\\sigma}}}\\left[\\widetilde{V}(s)\\right]\\big)\\right)}\\\\ &{\\qquad+\\left(1-\\lambda\\right)\\left(\\mathbb{E}_{\\pi^{\\star},\\sigma}\\left[r\\right]-\\mathbb{E}_{\\pi^{\\star},\\overline{{\\sigma}}}\\left[r\\right]\\right)+\\left(1-\\lambda\\right)\\left(\\mathbb{E}_{\\pi,\\overline{{\\sigma}}}\\left[r\\right]-\\mathbb{E}_{\\pi,\\sigma}\\left[r\\right]\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By definition, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{p,\\pi,\\widetilde{\\sigma}}\\big[V^{\\star}(s)\\big]-\\mathbb{E}_{p,\\pi,\\widetilde{\\sigma}}\\big[\\widetilde{V}(s)\\big]\\right|\\le\\sum_{s^{\\prime}\\in\\mathcal{S}}\\sum_{a,a^{\\prime}\\in\\mathcal{A}}p(s^{\\prime}|s,(a,a^{\\prime}))\\pi_{t}(s,a)\\widetilde{\\sigma}_{t}(s,a^{\\prime})\\eta\\le\\eta,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and similarly, $\\begin{array}{r}{\\Big|\\mathbb{E}_{p,\\pi^{\\star},\\widetilde{\\sigma}}\\big[V^{\\star}(s)\\big]-\\mathbb{E}_{p,\\pi^{\\star},\\widetilde{\\sigma}}\\big[\\widetilde{V}(s)\\big]\\Big|\\le\\eta.}\\end{array}$ thus, (25) yields ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\pi^{\\star},\\sigma}\\left[r\\right]-\\mathbb{E}_{\\pi,\\sigma}\\left[r\\right]\\leq\\!\\gamma\\lambda\\left(\\mathbb{E}_{p,\\pi,\\tilde{\\sigma}}\\big[V^{\\star}(s)\\big]-\\mathbb{E}_{p,\\pi^{\\star},\\tilde{\\sigma}}\\big[V^{\\star}(s)\\big]\\right)+2\\lambda\\left(\\varepsilon r_{\\operatorname*{max}}+\\gamma\\eta\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\left.\\gamma(1-\\lambda)\\left(\\mathbb{E}_{p,\\pi,\\overline{{\\sigma}}}\\left[\\overline{{V}}(s)\\right]-\\mathbb{E}_{p,\\pi^{\\star},\\overline{{\\sigma}}}\\left[\\overline{{V}}(s)\\right]\\right)\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left.+\\left(1-\\lambda\\right)\\left(\\mathbb{E}_{\\pi^{\\star},\\sigma}\\left[r\\right]-\\mathbb{E}_{\\pi^{\\star},\\overline{{\\sigma}}}\\left[r\\right]\\right)+\\left(1-\\lambda\\right)\\left(\\mathbb{E}_{\\pi,\\overline{{\\sigma}}}\\left[r\\right]-\\mathbb{E}_{\\pi,\\sigma}\\left[r\\right]\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Next, we show BoUND 2 in (21). ", "page_idx": 17}, {"type": "text", "text": "By the definition of the policy $\\pi$ in (12), it follows that for any fixed state $s\\in S$ , similarly we obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\lambda\\mathbb{E}_{\\overline{{\\pi}},\\widetilde{\\sigma}}\\left[r\\right]+(1-\\lambda)\\mathbb{E}_{\\overline{{\\pi}},\\overline{{\\sigma}}}\\left[r\\right]+\\gamma\\lambda\\mathbb{E}_{p,\\overline{{\\pi}},\\widetilde{\\sigma}}\\left[\\widetilde{V}(s)\\right]+\\gamma(1-\\lambda)\\mathbb{E}_{p,\\overline{{\\pi}},\\overline{{\\sigma}}}\\left[\\overline{{V}}(s)\\right]}\\\\ &{\\leq\\!\\lambda\\mathbb{E}_{\\pi,\\widetilde{\\sigma}}\\left[r\\right]+(1-\\lambda)\\mathbb{E}_{\\pi,\\overline{{\\sigma}}}\\left[r\\right]+\\gamma\\lambda\\mathbb{E}_{p,\\pi,\\widetilde{\\sigma}}\\!\\left[\\widetilde{V}(s)\\right]+\\gamma(1-\\lambda)\\mathbb{E}_{p,\\pi,\\overline{{\\sigma}}}\\left[\\overline{{V}}(s)\\right]\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Rearranging the terms in (26), ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{\\overline{{\\pi}},\\overline{{\\sigma}}}\\left[r\\right]-\\mathbb{E}_{\\pi,\\overline{{\\sigma}}}\\left[r\\right]\\leq\\gamma\\left(\\mathbb{E}_{p,\\pi,\\overline{{\\sigma}}}\\left[\\overline{{V}}(s)\\right]-\\mathbb{E}_{p,\\overline{{\\pi}},\\overline{{\\sigma}}}\\left[\\overline{{V}}(s)\\right]\\right)}\\\\ &{\\quad\\quad\\quad+\\frac{\\lambda}{1-\\lambda}\\left((\\mathbb{E}_{\\pi,\\tilde{\\sigma}}\\left[r\\right]-\\mathbb{E}_{\\overline{{\\pi}},\\tilde{\\sigma}}\\left[r\\right])+\\gamma\\left(\\mathbb{E}_{p,\\pi,\\tilde{\\sigma}}\\big[\\widetilde{V}(s)\\big]-\\mathbb{E}_{p,\\overline{{\\pi}},\\tilde{\\sigma}}\\big[\\widetilde{V}(s)\\big]\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Lemma 2. The payoff gap for the policy $\\pi$ defined in (12) satisfies ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta_{\\mathsf{S B G}}(\\varepsilon;\\pi)\\leq\\displaystyle\\frac{r_{\\mathrm{max}}}{1-\\lambda\\gamma}\\Bigg(\\displaystyle\\frac{2\\lambda\\varepsilon}{1-\\gamma}+\\operatorname*{min}\\bigg\\{\\displaystyle\\frac{2(1-\\lambda)}{1-\\gamma},\\displaystyle\\frac{2\\lambda}{(1-\\gamma)^{2}}\\bigg\\}\\Bigg.\\Bigg.}\\\\ {\\displaystyle\\left.+\\,\\displaystyle\\frac{(1-\\lambda)(2-\\gamma)}{1-\\gamma}\\right)+\\displaystyle\\frac{\\gamma\\,(2\\lambda\\eta-\\nu)}{1-\\lambda\\gamma}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. By defnition of the payoff gap (see Definition 2) and the value function in (9), we get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\Delta_{\\mathsf{S B G}}(\\varepsilon;\\pi)=\\operatorname*{sup}_{d(\\theta,\\theta^{\\star})\\leq\\varepsilon}\\operatorname*{max}_{s\\in S}\\left(V^{\\star,\\sigma}(s)-V^{\\pi(\\theta),\\sigma}(s)\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Our first goal is to derive an upper bound on $\\Delta_{\\mathsf{S B G}}(\\varepsilon;\\pi)$ , aligning with the classic loss bound of approximate value functions [54]. However, the policy considered in our context is not directly maximizing the approximate value function, but a mixed strategy defined by an optimization as in (12). For notational convenience, we write for $s\\in S$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{E}_{p,\\pi,\\sigma}\\left[V(s)\\right]:=\\sum_{a,a^{\\prime}\\in A}\\sum_{s^{\\prime}\\in S}p(s^{\\prime}|s,(a,a^{\\prime}))\\pi(s,a)\\sigma(s,a^{\\prime})V(s^{\\prime}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For notational simplicity, we denote $V^{\\pi}(s):=V^{\\pi,\\sigma}(s)$ . For any state $s\\in S$ and strategies $(\\sigma,\\widetilde{\\sigma})$ satisfying $\\begin{array}{r}{d\\left(\\theta,\\theta^{\\star}\\right)\\,=\\,\\operatorname*{max}_{s\\in\\mathcal{S}}\\left\\|\\sigma(s;\\theta)-\\sigma(s;\\theta^{\\star})\\right\\|_{1}\\,\\le\\,\\varepsilon}\\end{array}$ , applying BoUND 1 in Lemma 1, the Bellman equations corresponding to $\\pi^{\\star}$ and $\\pi$ imply ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V^{\\star}(s)\\stackrel{\\textstyle\\uparrow}{-}V^{\\pi}(s)=\\mathbb{E}_{\\pi^{\\star},\\sigma}^{\\textstyle\\leftarrow}[r]-\\mathbb{E}_{\\pi,\\sigma}[r]+\\gamma\\left(\\mathbb{E}_{p,\\pi^{\\star},\\sigma}\\big[V^{\\star}(s)\\big]-\\mathbb{E}_{p,\\pi,\\sigma}\\big[V^{\\pi}(s)\\big]\\right)}\\\\ &{\\qquad\\leq\\lambda\\gamma\\Big(\\mathbb{E}_{p,\\pi,\\tilde{\\sigma}}\\big[V^{\\star}(s)\\big]-\\mathbb{E}_{p,\\pi^{\\star},\\tilde{\\sigma}}\\big[V^{\\star}(s)\\big]+\\mathbb{E}_{p,\\pi^{\\star},\\sigma}\\big[V^{\\star}(s)\\big]-\\mathbb{E}_{p,\\pi,\\sigma}\\big[V^{\\pi}(s)\\big]\\Big)}\\\\ &{\\qquad+\\gamma(1-\\lambda)\\left(\\mathbb{E}_{p,\\pi,\\tilde{\\sigma}}\\left[\\overline{{V}}(s)\\right]-\\mathbb{E}_{p,\\pi,\\sigma}\\big[V^{\\pi}(s)\\big]+\\mathbb{E}_{p,\\pi^{\\star},\\sigma}\\big[V^{\\star}(s)\\big]-\\mathbb{E}_{p,\\pi^{\\star},\\tilde{\\sigma}}\\big[\\overline{{V}}(s)\\big]\\right)}\\\\ &{\\qquad+(1-\\lambda)\\left(\\mathbb{E}_{\\pi,\\overline{{\\sigma}}}\\left[r\\right]-\\mathbb{E}_{\\pi,\\sigma}\\left[r\\right]\\right)+(1-\\lambda)\\left(\\mathbb{E}_{\\pi^{\\star},\\sigma}\\left[r\\right]-\\mathbb{E}_{\\pi^{\\star},\\overline{{\\sigma}}}\\left[r\\right]\\right)}\\\\ &{\\qquad\\qquad+2\\lambda\\left(\\varepsilon r_{\\operatorname*{max}}+\\gamma\\eta\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Applying the Bellman equations, for any $s\\in S$ \uff0c ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{V^{\\pi}(s)=\\mathbb{E}_{a\\sim\\pi(s),a^{\\prime}\\sim\\sigma}(s)\\left[\\left(r+\\gamma\\mathbb{P}V^{\\pi}\\right)\\left(s,(a,a^{\\prime})\\right)\\right]}}\\\\ {{=\\mathbb{E}_{\\pi,\\sigma}\\left[r\\right]+\\gamma\\mathbb{E}_{p,\\pi,\\sigma}\\left[V^{\\pi}(s)\\right].}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Similarly, it also holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\nV^{\\star}(s)=\\mathbb{E}_{\\pi^{\\star},\\sigma}\\left[r\\right]+\\gamma\\mathbb{E}_{p,\\pi^{\\star},\\sigma}\\left[V^{\\star}(s)\\right].\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Plugging (28), and (29) into (27), ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{V^{\\star}(s)-V^{\\pi}(s)\\leq\\lambda\\gamma\\Big(\\mathbb{E}_{p,\\pi,\\widetilde{\\sigma}}\\big[V^{\\star}(s)\\big]-\\mathbb{E}_{p,\\pi^{\\star},\\widetilde{\\sigma}}\\big[V^{\\star}(s)\\big]+\\mathbb{E}_{p,\\pi^{\\star},\\sigma}\\big[V^{\\star}(s)\\big]-\\mathbb{E}_{p,\\pi,\\sigma}\\big[V^{\\pi}(s)\\big]\\Big)}\\\\ &{\\quad}&{\\quad+\\left(1-\\lambda\\right)\\Big(\\left(\\mathbb{E}_{\\pi,\\overline{{\\sigma}}}\\left[r\\right]+\\gamma\\mathbb{E}_{p,\\pi,\\overline{{\\sigma}}}\\left[\\overline{{V}}(s)\\right]\\right)-V^{\\pi}(s)\\Big)\\Big)}\\\\ &{\\quad}&{\\quad+\\left(1-\\lambda\\right)\\Big(V^{\\star}(s)-\\left(\\mathbb{E}_{\\pi^{\\star},\\overline{{\\sigma}}}\\left[r\\right]+\\gamma\\mathbb{E}_{p,\\pi^{\\star},\\overline{{\\sigma}}}\\left[\\overline{{V}}(s)\\right]\\right)\\Big)}\\\\ &{\\quad}&{\\quad+2\\lambda\\left(\\varepsilon r_{\\operatorname*{max}}+\\gamma\\eta\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Part I: Proof of Risk Bound ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Let $\\overline{{\\boldsymbol V}}^{\\pi}(s):=\\boldsymbol V^{\\pi,\\overline{\\sigma}}(s)$ for any $s\\in S$ . For the term in (30), we always have $V^{\\pi}(s)\\geq\\overline{{V}}^{\\pi}(s)$ [48], ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\mathbb{E}_{\\pi,\\overline{{\\sigma}}}\\left[r\\right]+\\gamma\\mathbb{E}_{p,\\pi,\\overline{{\\sigma}}}\\left[\\overline{{V}}(s)\\right]\\right)-V^{\\pi}(s)\\leq\\gamma\\left(\\mathbb{E}_{p,\\pi,\\overline{{\\sigma}}}\\left[\\overline{{V}}(s)\\right]-\\mathbb{E}_{p,\\pi,\\overline{{\\sigma}}}\\left[\\overline{{V}}^{\\pi}(s)\\right]\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\gamma\\mathbb{E}_{p,\\pi,\\overline{{\\sigma}}}\\left[\\overline{{V}}(s)-\\overline{{V}}^{\\pi}(s)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Furthermore, the Bellman equation implies ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\overline{{V}}(s)-\\overline{{V}}^{\\pi}(s)=\\left(\\mathbb{E}_{\\overline{{\\pi}},\\overline{{\\sigma}}}\\left[r\\right]-\\mathbb{E}_{\\pi,\\overline{{\\sigma}}}\\left[r\\right]\\right)+\\gamma\\left(\\mathbb{E}_{p,\\overline{{\\pi}},\\overline{{\\sigma}}}\\left[\\overline{{V}}(s)\\right]-\\mathbb{E}_{p,\\pi,\\overline{{\\sigma}}}\\big[\\overline{{V}}^{\\pi}(s)\\big]\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Applying BoUND 2 in Lemma 1, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overline{{V}}(s)-\\overline{{V}}^{\\pi}(s)\\leq\\gamma\\left(\\mathbb{E}_{p,\\pi,\\overline{{\\sigma}}}\\left[\\overline{{V}}(s)\\right]-\\mathbb{E}_{p,\\pi,\\overline{{\\sigma}}}\\left[\\overline{{V}}^{\\pi}(s)\\right]+\\mathbb{E}_{p,\\pi,\\overline{{\\sigma}}}\\left[\\overline{{V}}(s)\\right]-\\mathbb{E}_{p,\\overline{{\\pi}},\\overline{{\\sigma}}}\\left[\\overline{{V}}(s)\\right]\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\displaystyle\\frac{\\lambda}{1-\\lambda}\\left((\\mathbb{E}_{\\pi,\\pi}\\left[r\\right]-\\mathbb{E}_{\\pi,\\widetilde{\\sigma}}\\left[r\\right])+\\gamma\\left(\\mathbb{E}_{p,\\pi,\\widetilde{\\sigma}}\\left[\\widetilde{V}(s)\\right]-\\mathbb{E}_{p,\\overline{{\\pi}},\\overline{{\\sigma}}}\\left[\\widetilde{V}(s)\\right]\\right)\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leq\\gamma\\left(\\mathbb{E}_{p,\\pi,\\overline{{\\sigma}}}\\left[\\overline{{V}}(s)\\right]-\\mathbb{E}_{p,\\pi,\\overline{{\\sigma}}}\\left[\\overline{{V}}^{\\pi}(s)\\right]\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\displaystyle\\frac{2\\lambda}{1-\\lambda}\\left(\\left(1+\\frac{\\gamma}{1-\\gamma}\\right)\\right)r_{\\operatorname*{max}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since above holds for any state $s\\in S$ , let $s^{\\prime}$ be a state such that the value gap $V^{\\star}(s)-V^{\\pi}(s)$ is maximized. Therefore, (34) implies ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\overline{{V}}(s)-\\overline{{V}}^{\\pi}(s)\\leq\\frac{2}{(1-\\gamma)^{2}}\\frac{\\lambda}{1-\\lambda}r_{\\mathrm{max}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then, for the term in (31), by definition $V^{\\star}(s)\\leq r_{\\operatorname*{max}}/(1-\\gamma)$ , and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\pi^{\\star},\\overline{{\\sigma}}}\\left[r\\right]+\\gamma\\mathbb{E}_{p,\\pi^{\\star},\\overline{{\\sigma}}}\\left[\\overline{{V}}(s)\\right]\\geq\\gamma\\nu-r_{\\operatorname*{max}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Combing the bounds in (35) and (36) with (32), we conclude that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{V^{\\star}(s)-V^{\\pi}(s)\\leq\\lambda\\gamma\\Big(\\mathbb{E}_{p,\\pi,\\widetilde{\\sigma}}\\big[V^{\\star}(s)\\big]-\\mathbb{E}_{p,\\pi^{\\star},\\widetilde{\\sigma}}\\big[V^{\\star}(s)\\big]+\\mathbb{E}_{p,\\pi^{\\star},\\sigma}\\big[V^{\\star}(s)\\big]-\\mathbb{E}_{p,\\pi,\\sigma}\\big[V^{\\pi}(s)\\big]\\Big)}\\\\ {+\\;\\displaystyle\\frac{2}{(1-\\gamma)^{2}}\\lambda r_{\\operatorname*{max}}+(1-\\lambda)\\left(\\frac{2-\\gamma}{1-\\gamma}r_{\\operatorname*{max}}-\\gamma\\nu\\right)+2\\lambda\\left(\\varepsilon r_{\\operatorname*{max}}+\\gamma\\eta\\right).~~~(33)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now, noting that by definition we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|\\mathbb{E}_{p,\\pi,\\sigma}\\left[V^{\\star}\\right]-\\mathbb{E}_{p,\\pi,\\widetilde{\\sigma}}\\left[V^{\\star}\\right]\\right|}\\\\ &{\\le\\displaystyle\\sum_{a,a^{\\prime}\\in A,s^{\\prime}\\in S}p(s^{\\prime}|s,(a,a^{\\prime}))\\pi(s,a)\\left|\\sigma(s,a^{\\prime})-\\widetilde{\\sigma}(s,a^{\\prime})\\right|V^{\\star}(s^{\\prime})\\le\\varepsilon\\frac{r_{\\operatorname*{max}}}{1-\\gamma},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and similarly, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{p,\\pi^{\\star},\\sigma}\\left[V^{\\star}\\right]-\\mathbb{E}_{p,\\pi^{\\star},\\widetilde{\\sigma}}\\left[V^{\\star}\\right]\\right|\\le\\varepsilon\\frac{r_{\\mathrm{max}}}{1-\\gamma}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, rearranging the terms and simplifying above, (37) becomes ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V^{\\star}(s)-V^{\\pi}(s)\\leq\\!\\!\\frac{2\\lambda}{(1-\\gamma)^{2}}r_{\\operatorname*{max}}+(1-\\lambda)\\left(\\frac{2-\\gamma}{1-\\gamma}r_{\\operatorname*{max}}-\\gamma\\nu\\right)+2\\lambda\\left(\\varepsilon r_{\\operatorname*{max}}+\\gamma\\eta\\right)}\\\\ &{\\phantom{V^{\\pi}(s)}\\;\\;+\\frac{2\\gamma\\lambda r_{\\operatorname*{max}}}{1-\\gamma}\\varepsilon+\\gamma\\lambda\\left(\\mathbb{E}_{p,\\pi,\\sigma}\\left[V^{\\star}\\right]-\\mathbb{E}_{p,\\pi,\\sigma}[V^{\\pi}]\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since above holds for any state $s\\in S$ , let $s^{*}$ be a state such that the value gap $V^{\\star}(s)-V^{\\pi}(s)$ is maximized. Thus, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbb{E}_{p,\\pi,\\sigma}\\bigl[V^{\\star}(s^{*})\\bigr]-\\mathbb{E}_{p,\\pi,\\sigma}[V^{\\pi}(s^{*})]}\\\\ &{=\\displaystyle\\sum_{a,a^{\\prime}\\in A}\\sum_{s^{\\prime}\\in\\mathcal{S}}p\\left(s^{\\prime}|s^{*},(a,a^{\\prime})\\right)\\pi\\left(s^{*},a\\right)\\sigma\\left(s^{*},a^{\\prime}\\right)(V^{\\star}(s^{\\prime})-V^{\\pi}(s^{\\prime}))\\le V^{\\star}(s^{*})-V^{\\pi}(s^{*}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Continuing from (38), ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left(V^{\\star}(s^{*})-V^{\\pi}(s^{*})\\right)-\\lambda\\gamma\\left(V^{\\star}(s^{*})-V^{\\pi}(s^{*})\\right)}\\\\ &{\\leq\\!2r_{\\operatorname*{max}}\\left(\\displaystyle\\frac{\\lambda}{(1-\\gamma)^{2}}+\\displaystyle\\frac{\\gamma\\lambda}{1-\\gamma}\\varepsilon+\\lambda\\varepsilon\\right)+2\\lambda\\gamma\\eta+(1-\\lambda)\\left(\\displaystyle\\frac{2-\\gamma}{1-\\gamma}r_{\\operatorname*{max}}-\\gamma\\nu\\right)}\\\\ &{=\\!2r_{\\operatorname*{max}}\\left(\\displaystyle\\frac{\\lambda}{(1-\\gamma)^{2}}+\\displaystyle\\frac{\\lambda\\varepsilon}{1-\\gamma}\\right)+2\\lambda\\gamma\\eta+(1-\\lambda)\\left(\\displaystyle\\frac{2-\\gamma}{1-\\gamma}r_{\\operatorname*{max}}-\\gamma\\nu\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, rearranging the terms we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Delta_{\\mathsf{S B G}}(\\varepsilon;\\pi)\\leq\\frac{r_{\\mathrm{max}}}{1-\\lambda\\gamma}\\left(\\frac{2\\lambda\\varepsilon}{1-\\gamma}+\\frac{2\\lambda}{(1-\\gamma)^{2}}+\\frac{(1-\\lambda)(2-\\gamma)}{1-\\gamma}\\right)+\\frac{\\gamma\\left(2\\lambda\\eta-(1-\\lambda)\\nu\\right)}{1-\\lambda\\gamma}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Part II: Proof of Opportunity Bound ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Now, the terms in (30) can be bounded alternatively as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\displaystyle(1-\\lambda)\\Big(\\left(\\big(\\mathbb{E}_{\\pi,\\overline{{\\sigma}}}\\left[r\\right]+\\gamma\\mathbb{E}_{p,\\pi,\\overline{{\\sigma}}}\\left[\\overline{{V}}(s)\\right]\\big)-V^{\\pi}(s)\\right)\\Big)\\le2\\frac{1-\\lambda}{1-\\gamma}r_{\\mathrm{max}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Using this and following the same steps, we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Delta_{\\mathsf{S B G}}(\\varepsilon;\\pi)\\leq\\frac{r_{\\mathrm{max}}}{1-\\lambda\\gamma}\\left(\\frac{2\\lambda\\varepsilon}{1-\\gamma}+\\frac{2(1-\\lambda)}{(1-\\gamma)^{2}}+\\frac{(1-\\lambda)(2-\\gamma)}{1-\\gamma}\\right)+\\frac{\\gamma\\left(2\\lambda\\eta-(1-\\lambda)\\nu\\right)}{1-\\lambda\\gamma}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Finally, setting $\\varepsilon=\\eta=0$ , we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Delta_{\\mathsf{S B G}}(0;\\pi)\\leq\\frac{1}{1-\\lambda\\gamma}\\left(r_{\\mathrm{max}}\\left(\\frac{\\gamma^{2}-3\\gamma+6}{(1-\\gamma)^{2}}\\right)-\\gamma\\nu\\right)(1-\\lambda).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since $\\begin{array}{r}{d\\left(\\theta,\\theta^{\\star}\\right):=\\operatorname*{max}_{s\\in\\cal S}\\left\\|\\boldsymbol{\\sigma}(s;\\theta)-\\boldsymbol{\\sigma}(s;\\theta^{\\star})\\right\\|_{1}\\le2}\\end{array}$ , setting the worst $\\varepsilon\\,=\\,2$ , and noticing that $\\begin{array}{r}{\\eta\\leq\\frac{2r_{\\operatorname*{max}}}{1-\\gamma}}\\end{array}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Delta_{\\mathsf{S B G}}(\\varepsilon;\\pi)\\leq\\!\\frac{r_{\\mathrm{max}}}{1-\\lambda\\gamma}\\left(\\frac{\\left(4(1-\\gamma^{2})+2\\right)\\lambda}{(1-\\gamma)^{2}}+\\frac{(1-\\lambda)(2-\\gamma)}{1-\\gamma}\\right)-\\frac{\\gamma(1-\\lambda)\\nu}{1-\\lambda\\gamma}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "EProof of Theorem 4.2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We prove the theorem by considering a stateless MDP, which reduces a stochastic Bayesian game $\\mathcal{M}\\ =\\ \\langle S,\\mathcal{A},\\Theta,\\bar{\\sigma_{}}p,r,\\gamma\\rangle$ to a normal-form game in Theorem 3.2. Furthermore, let $\\begin{array}{r}{\\mathcal{L}:=\\left\\{1,\\dots,\\prod_{j\\neq i}|A(j)\\right\\}}\\end{array}$ Wefix a strategy kernel $\\sigma$ such that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\kappa(\\Theta):=\\operatorname*{max}_{s\\in\\mathcal{S},\\theta,\\theta^{\\prime}\\in\\Theta}\\left(\\sum_{\\substack{l\\in\\mathcal{L}:\\sigma_{l}(s;\\theta)=0}}\\sigma_{l}(s;\\theta^{\\prime})-\\sum_{\\substack{l\\in\\mathcal{L}:\\sigma_{l}(s;\\theta)>0}}\\sigma_{l}(s;\\theta^{\\prime})\\right)=1.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore,the construction of the payoffmatix $A\\in\\mathbb{R}^{|A(i)|\\times\\prod_{j\\neq i}|A(j)|}$ in the proof of Theorem 3.2 with $\\begin{array}{r}{\\mu_{\\Theta}(A)\\,\\leq\\,\\frac{r_{\\operatorname*{max}}}{1-\\gamma}}\\end{array}$ and $\\nu_{\\Theta}(A)\\,\\geq\\,\\nu$ yields that for any $0\\leq\\lambda\\leq1$ if any mixed strategy $\\pi$ is $\\begin{array}{r}{\\frac{1}{1-\\lambda\\gamma}\\left(r_{\\operatorname*{max}}-\\nu\\right)\\left(1-\\lambda\\right)}\\end{array}$ -foregone, then since ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{1-\\lambda\\gamma}\\left(r_{\\mathrm{max}}-\\nu\\right)(1-\\lambda)\\leq\\left(\\frac{r_{\\mathrm{max}}}{1-\\gamma}-\\nu\\right)(1-\\lambda),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "then $\\pi$ is at least $\\begin{array}{r}{\\left(\\frac{r_{\\operatorname*{max}}}{1-\\gamma}-\\nu\\right)(1+\\lambda)}\\end{array}$ -risky. Furthermore, since ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left(\\frac{r_{\\operatorname*{max}}}{1-\\gamma}-\\nu\\right)(1+\\lambda)\\geq\\left(\\frac{r_{\\operatorname*{max}}}{1-\\lambda\\gamma}-\\frac{\\nu}{1-\\lambda\\gamma}\\right)(1+\\lambda)\\,,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "$\\pi$ is at least $\\begin{array}{r}{\\frac{1}{1-\\lambda\\gamma}\\left(r_{\\operatorname*{max}}-\\nu\\right)\\left(1+\\lambda\\right)}\\end{array}$ -risky. ", "page_idx": 19}, {"type": "text", "text": "F  Details on Experiments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "F.1 Details on $2\\times2$ Game simulations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "F.1.1 Game definition ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "\u00b7 We define a Stochastic Bayesian game with one state: a particular $2\\times2$ game sampled from the topology of $2\\times2$ games. \u00b7 The payoffs for each player are stipulated by the $2\\times2$ game. We set the time horizon for the game be 1,000 giving us an empirical evaluation of the expected payoff for each strategy ", "page_idx": 20}, {"type": "text", "text": "F.1.2  Type definitions ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "As mentioned we have 3 classes of types and below we provide a description of the general classes as well as the specific types we used in our evaluation. ", "page_idx": 20}, {"type": "text", "text": "I. Markovian Types: These are types whose strategy only depends on the current state. We made use of 4 types of Markovian strategies \u00b7 Type 1: Always play action 0 \u00b7 Type 2: Always play action 1 \u00b7 Type 3: The minimax strategy for the player \u00b7 Type 4: Returns a random strategy ", "page_idx": 20}, {"type": "text", "text": "2. Leader- Follower-Trigger Agents: Inspired by [55], these agents have a preferred sequence of play they seek to enforce. Importantly they have access to history of play and when the other player does not play according to their preferred sequence the alter their strategy by engaging in \u201cpunishing\u201d behavior such as playing the minimax strategy or by resetting to a previous action. In our case, we evaluated against simple versions of such agents, wherein our agent had a preferred mixed strategy and when the empirical observed strategy over a preset number of previous plays from the opponent did not match their preferred strategy, they \u201cpunished\u2019\" the opponent by playing a minimax strategy. In particular, we had an agent look at the previous 4 actions of the opponent and if they selected action 1 more than twice, they chose to play a minimax strategy for the next round. ", "page_idx": 20}, {"type": "text", "text": "3. Co-evolved Neural Networks: Inspired by work in [51] we use ideas of genetic programming [56], to generate agents from neural networks. We randomly initialized 10 neural networks with a single hidden layer for both the row and column player. All networks would take as input the previous 4 actions of both players and the corresponding state information. We sample randomly from populations of the row and column players and simulate the game. After simulation we calculate a fitness score based on average payoffs for each agent and a similarity score so as to ensure diversity in the models. We \u201cevolve\u201d the populations by selecting random portions of both populations to mutate whilst also having cross-over between members of the populations selected by fitness (this is done for both populations hence \u201cco-evolve\"). We then proceed to create new populations using the most fit agents from a previous generation (i.e., we take the top $50\\%$ of a population pre-evolution and $50\\%$ post evolution and then create a new population if the average fitness of this new constructed population is greater than the average fitness of the previous population.) ", "page_idx": 20}, {"type": "text", "text": "F.1.3 Experimental ilustration of tightness of bounds ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We have included additional adversarial examples to demonstrate the tightness of the bounds presented in Theorems 3.1 and 3.2. In particular, we consider the zero-sum Matching Pennies (MP) and an Adjusted Matching Pennies (AMP) as our two examples. We assume the hypothesis set $\\Theta$ forPlayer 2 contains 6 behavioral types: ", "page_idx": 20}, {"type": "text", "text": "\u00b7 Markovian Types: - Type 1: Always play action 0 - Type 2: Always play action 1 - Type 3: Minimax strategy - Type 4: Random strategy ", "page_idx": 20}, {"type": "text", "text": "\u00b7 Type 5: Leader-Follower-Trigger Agents \u00b7 Type 6: Co-evolved Neural Networks ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The payoff matrices for the MP and AMP, respectively are ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left[{\\begin{array}{r r}{1}&{-1}\\\\ {-1}&{1}\\end{array}}\\right],{\\mathrm{and}}\\;\\left[{\\begin{array}{r r}{1.2}&{-0.8}\\\\ {-0.8}&{1.2}\\end{array}}\\right]=\\left[{\\begin{array}{r r}{1}&{-1}\\\\ {-1}&{1}\\end{array}}\\right]+\\left[{\\boldsymbol{0.2}}&{0.2}\\right].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "It's worth mentioning that the AMP payoff matrix constructed above is an adversarial example that follows the same construction of the adversarial payoff matrix $A$ used in the proof of Theorem 3.2. Given the considered $\\Theta$ , we know that $\\kappa(\\Theta)=\\Bar{1}$ and $\\eta(\\Theta)=2$ . The upper and lower bounds in Theorem 3.1 and 3.2 therefore read: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x=(1-\\lambda)(\\mu-\\nu),\\;y=(1-\\lambda)(\\mu-\\nu)+2\\lambda\\mu,\\lambda\\in[0,1],}\\\\ &{x=(1-\\lambda)(\\mu-\\nu),\\;y=(1+\\lambda)(\\mu-\\nu),\\lambda\\in[0,1],}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where for MP, $\\mu=1$ $\\nu=0$ ; for AMP, $\\mu=1.2,\\nu=0.2$ ", "page_idx": 21}, {"type": "text", "text": "Besides the bounds above, we also plot simulated risk and opportunity values in the figures using an algorithm that has varying trust of type beliefs in 100 and 1,000 runs. The algorithm is presented with a type prediction and takes a convex combination of the best response to the type prediction and the minimax strategy with the trust parameter $\\lambda$ determining how much to weigh the best response. It is the same as $\\pi$ , the mixed strategy used to prove Theorem 3.1. Fully trusting ( $\\lambda=1$ ) and distrusting $\\left.\\lambda=0\\right]$ 0 type beliefs yield a best response strategy and a minimax strategy correspondingly. The agent then samples from their resulting mixed strategy an action to play while the opponent also samples from whatever mixed strategy they are using an action to play. ", "page_idx": 21}, {"type": "text", "text": "We sample this interaction for the number of runs and gather empirical payoff information, which we use to plot the opportunity risk tradeoff. The variance of the risk and opportunity values decreases as the number of runs increases, as illustrated by the error bars in the attached figures. Moreover, we include as plots the bounds on the opportunity risk tradeoff and show the tightness or looseness of these bounds in two games we pick. Please note that some of the simulated risk and opportunity values fall outside the bounds. This is because the bounds are applicable only to expected payoff gaps, and individual simulations may deviate from these expected values. ", "page_idx": 21}, {"type": "text", "text": "In the Fig 7, the top two figures display the results from 100 runs, while the bottom two figures present the results from 1,000 runs. The left two figures correspond to the MP example, while the right two figures are for the AMP example, demonstrating the gap between the lower and upper bounds. In particular, in an AMP game, we see that the upper bound is loose, and the empirical opportunity-risk tradeoff matches the lower bound we derive. In future work, we will investigate if this gap can be closed. This ^adversarial example illustrates the looseness in the upper bound we derive. In the canonical Matching Pennies game, we find that the lower and upper bounds are tight with the empirical tradeoff matching these theoretical bounds, coinciding with Corollary 3.1. ", "page_idx": 21}, {"type": "image", "img_path": "QZtJ22aOV4/tmp/9ff0aedfa40d829549a1eb12cf3f60e777b3a73e10032786510ac997a3c1f01f.jpg", "img_caption": ["Figure 7: Opportunity-risk tradeoff for Matching Pennies (MP) and Adjusted Matching Pennies (AMP) games over 100 (top) and 1,000 (bottom) runs. Left: Matching Pennis. Right: Adjusted Matching Pennies. As the number of runs increases, the variance of both risk and opportunity values decreases. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "F.2  Details on Green Security Game simulations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "F.2.1 Data description ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "The data is from a study tracking geo-location information of 32 African elephants in the Hwange National Park in Zimbabwe, Africa. The data was collected from 2009 to 2017 and includes time stamp information as well as longitudinal and latitudinal information of the elephants ", "page_idx": 22}, {"type": "text", "text": "F.2.2   Data processing ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We divided each year into two based on the seasons in Zimbabwe which would affect the location of the elephants as they migrate given changes in the environment. A year was divided into a \u201cRainy? season which runs from October to March and a \u201cDry\u201d season which runs from April to September. ", "page_idx": 22}, {"type": "text", "text": "We divided the area covered in the dataset into 9 locations and thus had a $3\\times3$ gridwhichserved as a surveillance area. For each elephant in the season, we calculated its mean location and so for each of the seasons we had a mean location for the elephants. It is important to note, given migratory nature of elephants across national borders, each season does not necessarily have all 32 elephants as they move as a result of a wide range of factors (e.g., availability of water). Some seasons, also notably, do not have any recorded elephant presence in the area under surveillance. We do not see this as a limitation as the dynamic environment presents changes in the game which the defender has to take into account. ", "page_idx": 22}, {"type": "text", "text": "F.2.3  Green Security Game ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Each of the seasonal elephant information presents us with a state for our Stochastic Bayesian Game. We define the game in the following manner: ", "page_idx": 22}, {"type": "text", "text": "\u00b7 State: We have 16 states from the seasonal information we get from the data \u00b7 Actions: Each player (attacker or defender) has 9 available actions each corresponding to a selection of an area in the $3\\times3$ grid defined above. \u00b7Payoffs: Let $n$ be the number of elephants recorded in a particular grid square: ", "page_idx": 22}, {"type": "text", "text": "- If the attacker and defender select the same grid square, the defender gets $n$ while the attacker gets $-2$ - If the attacker and defender select different grid squares, let $n_{a t t}$ be the number of elephants in the grid square selected by the attacker. The defender gets payoff $-n_{a t t}$ while the attacker gets payoff $n_{a t t}$ These payoffs were designed to model the asymmetric nature of the defending task. An offender often will get a fixed penalty as stipulated by law, whilst the defender always depends on the number of elephants the attacker has access to. Transitions: To take into account the effect of changes in weather whilst also bringing in some stochasticity, we made it such that there is some transition probability between any \u201cRainy\u201d state to any \u201cDry\u201d state and vice-versa. We do however, slightly, increase the probability of transition between adjacent historical states, to reflect historical data. We have the probability of transitioning between any two \u201cRainy\" or \"Dry\" states to be zero. Our transition probabilities do not depend on the actions taken by the agents. ", "page_idx": 23}, {"type": "text", "text": "F.2.4  Type definitions ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We make use of the same types as in the $2\\times2$ game simulations. We make adjustments to the Markovian types in that Types 1 and 2 now select the grid with the highest population of elephants and second highest population of elephants, respectively. The Leader-Follower-Trigger Agents now looks to see if the other player is selecting the grid with the highest number of elephants for more than $50\\%$ of the historic play in which case they turn to play their minimax strategy ", "page_idx": 23}, {"type": "text", "text": "F.3Additional $2\\times2$ game evaluations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We include as an illustration as well as for completeness a couple of other $2\\times2$ gameswealso evaluated against. This is helpful as it shows the variety of tradeoffs that exist within the topology of $2\\times2$ games. In particular, we see some games exhibiting gradation as the agent moves from fully robust to fully trusting whilst in others there does not exist such a tradeoff because of the existence of a dominant strategy for the row player regardless of the type of the column player (e.g., the last game we show in this section). We note that the games provided in this file do not exhaust the entire topologyof $2\\times2$ games. They are however added to show the range of tradeoffs that could exist in games. ", "page_idx": 23}, {"type": "image", "img_path": "QZtJ22aOV4/tmp/cef14314b81de0378c6237bc77d0b9cbe11c93f2d0162b900ff776466b7e435b.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "QZtJ22aOV4/tmp/39e0bb132e06ce6f7fc239e1bc5c6b6a1ad842846d14169575416b11a20882b9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The claims made in the abstract and introduction align with our theoretical and experimental results. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The limitations of this work are discussed in our last concluding remark section. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should refect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We have clearly stated our model assumptions for the derived results to hold. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have detailed the settings and parameters used in the experiments, and we will further release our code after the anonymous review stage. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our contributions focus on the theoretical side. The experimental results shown in the submitted manuscript do not depend on private datasets and can be reproduced using the provided settings and parameters. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ", "page_idx": 27}, {"type": "text", "text": "\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We have clearly stated our experimental settings and details to reproduce the results. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: When there is randomness in our experiments, we characterize the variability. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our contributions focus on the theory side, and the experimental setup is sufficiently basic that it does not require intense computing resources such as GPUs. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The research conducted conform with the NeurIPS Code of Ethics. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We have discussed both positive and negative societal impacts in our concluding remark section. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to acces the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]