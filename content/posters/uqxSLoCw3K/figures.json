[{"figure_path": "uqxSLoCw3K/figures/figures_3_1.jpg", "caption": "Figure 1: The overall process of our MoD framework. Before training, we first assign a set of demonstrations to each of the experts. Then we perform expert-wise training to obtain a retriever model for each of the experts. We ensure that the subset S is optimally selected from all experts to filter out unhelpful demonstrations during training. During inference, multiple experts will provide demonstrations for predictions on the input query.", "description": "This figure illustrates the overall process of the Mixture of Demonstrations (MoD) framework proposed in the paper. It shows three main stages: Demonstration Assignment, Expert-wise Training, and Inference.  In the Demonstration Assignment stage, demonstrations are partitioned into groups, each assigned to an expert. The Expert-wise Training stage involves training individual retriever models for each expert to select helpful demonstrations, mitigating the impact of unhelpful ones.  The Inference stage shows how, during inference, multiple experts collaborate to retrieve demonstrations for an input query, ultimately enhancing the LLM's prediction accuracy.", "section": "3.3 Demonstration Assignment"}, {"figure_path": "uqxSLoCw3K/figures/figures_7_1.jpg", "caption": "Figure 2: The results of MoD performance over CEIL on various datasets with different numbers of demonstrations. We report the absolute gain of the results.", "description": "This figure shows the performance gain of the MoD model over the CEIL baseline across four different datasets (SST-5, CMSQA, GeoQ, MTOP) while varying the number of in-context demonstrations.  The x-axis represents the number of in-context demonstrations used, ranging from 4 to 48. The y-axis shows the absolute performance improvement achieved by MoD compared to CEIL.  Each dataset is represented by a different colored line, showing how the performance difference changes as more demonstrations are provided.  The figure visually depicts the impact of varying demonstration numbers on the MoD model's improved accuracy and the differences across various datasets.", "section": "4.4 Reduction of ICL Demonstrations"}, {"figure_path": "uqxSLoCw3K/figures/figures_8_1.jpg", "caption": "Figure 3: The ablation study result.", "description": "This figure presents the results of an ablation study conducted to evaluate the impact of two key components of the MoD framework: the mixture-of-experts design and the expert-wise training.  The x-axis represents four different datasets (SST5, CMSQA, GeoQ, MTOP), and the y-axis shows the model performance.  Three bars for each dataset represent the performance of the full MoD model, MoD without the mixture-of-experts design (MoD w/o E), and MoD without expert-wise training (MoD w/o C). The error bars represent standard deviations.  The results visually demonstrate the contributions of each component to the overall performance.", "section": "4.6 Ablation Study"}]