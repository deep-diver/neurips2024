[{"figure_path": "G9OJUgKo4B/tables/tables_5_1.jpg", "caption": "Table 1: Performance of task negation averaged across eight datasets. Selected results must maintain at least 95% of the pre-trained accuracy on the control dataset, following previous practice [44]. Best performance in each section is highlighted in bold. Task vector is abbreviated as t.v. Results for each dataset are available in Table 7.", "description": "This table presents the results of task negation experiments conducted on eight image classification datasets.  The task negation method aims to reduce undesired biases on a target task while maintaining performance on a control task (ImageNet).  The table compares different methods, including a baseline (pre-trained model) and two variations of the proposed aTLAS method (using standard and linearized task vectors), with a standard search method. The results show the target and control task accuracies for each method, with the best-performing method highlighted in bold for each dataset and model size.", "section": "4 Task arithmetic"}, {"figure_path": "G9OJUgKo4B/tables/tables_7_1.jpg", "caption": "Table 3: Test-time adaptation accuracy averaged over 22 dataset, with \u00d71 standard error over 3 random seeds. LN refers to tuning the LayerNorm layers. CLIP with the ViT-B/32 backbone is used. Highest performance is highlighted in bold.", "description": "This table shows the results of test-time adaptation experiments using different methods.  Test-time adaptation is a technique where the model adapts to a new task without using labeled data.  The results are averaged across 22 different datasets. The table compares several methods, including tuning the LayerNorm layers (LN) and using aTLAS, and shows the accuracy achieved with each method along with standard error calculated over 3 independent runs.", "section": "5.3 Test-time adaptation"}, {"figure_path": "G9OJUgKo4B/tables/tables_8_1.jpg", "caption": "Table 4: Few-shot recognition performance using standard task vectors or LoRAs as sparse task vectors. Results are averaged across 22 datasets over three seeds, with \u00d71 standard deviation. The memory consumption for ViT-B/32 backbone is annotated under each variant. For standard task vectors, we learn compositions on all parameter blocks or weight matrices only. For LoRAs as task vectors, we report results with rank 4, 16 and 64.", "description": "This table presents the few-shot recognition performance results using different task vector types and methods. The results are averaged across 22 datasets and three random seeds, with standard deviations reported.  The table compares the performance of using standard task vectors (all parameter blocks and weight matrices only) with LoRAs (low-rank adaptation) as sparse task vectors, for ranks 4, 16, and 64. Memory consumption for the ViT-B/32 backbone is also shown.", "section": "Relation to parameter-efficient fine-tuning"}, {"figure_path": "G9OJUgKo4B/tables/tables_15_1.jpg", "caption": "Table 5: Details of the 22 image classification datasets used in experiments, the number of epochs for fine-tuning and the final accuracy for different backbones of the CLIP model.", "description": "This table provides a comprehensive overview of the 22 image classification datasets used in the paper's experiments. For each dataset, it lists the number of classes, the sizes of the training, validation, and testing splits, the number of training epochs, and the fine-tuned accuracy achieved using various backbones of the CLIP model (RN50, RN101, ViT-B/32, ViT-B/16, and ViT-L/14). This information is crucial for understanding the experimental setup and evaluating the performance of the proposed method across diverse datasets.", "section": "A Datasets and task vectors"}, {"figure_path": "G9OJUgKo4B/tables/tables_17_1.jpg", "caption": "Table 6: Learning rates and training epochs for task negation.", "description": "This table shows the learning rates and number of training epochs used for the task negation experiments on eight different datasets.  The learning rate and number of epochs were determined using a hyperparameter search on the validation set for each dataset. This table is essential for reproducing the results of the task negation experiments. ", "section": "4.1 Task negation"}, {"figure_path": "G9OJUgKo4B/tables/tables_17_2.jpg", "caption": "Table 1: Performance of task negation averaged across eight datasets. Selected results must maintain at least 95% of the pre-trained accuracy on the control dataset, following previous practice [44]. Best performance in each section is highlighted in bold. Task vector is abbreviated as t.v. Results for each dataset are available in Table 7.", "description": "This table presents the performance of task negation on eight image classification datasets using different methods.  The results show the accuracy on both the target and control tasks. The control task ensures that the method maintains a certain level of performance on a general dataset.  The best performing method in each dataset and overall is highlighted in bold.  Abbreviations: t.v. = task vector.", "section": "4.1 Task negation"}, {"figure_path": "G9OJUgKo4B/tables/tables_20_1.jpg", "caption": "Table 1: Performance of task negation averaged across eight datasets. Selected results must maintain at least 95% of the pre-trained accuracy on the control dataset, following previous practice [44]. Best performance in each section is highlighted in bold. Task vector is abbreviated as t.v. Results for each dataset are available in Table 7.", "description": "This table presents the performance comparison of different methods on task negation across eight image classification datasets. The results are evaluated based on accuracy on both target and control tasks.  The table compares standard task vector methods (with and without learned anisotropic scaling) and linearized task vector methods (also with and without learned anisotropic scaling).  The best-performing method for each setting is highlighted in bold, and complete results for each dataset are available in another table referenced in the caption.", "section": "4.1 Task negation"}, {"figure_path": "G9OJUgKo4B/tables/tables_22_1.jpg", "caption": "Table 9: Average accuracy for few-shot recognition over 22 datasets. We report accuracy averaged over 3 random n-shot sample selections, with 1\u00d7 standard error. Results are produced using CLIP with ViT-B/32 backbone. For our method, we show results with both standard [28] and linearised [44] task vectors. The best method for each choice of k \u2208 {1, 2, 4, 8, 16} is highlighted in bold.", "description": "This table presents the average accuracy of different few-shot learning methods across 22 image recognition datasets.  The results are obtained using the CLIP model with a ViT-B/32 backbone. The table shows the performance for different numbers of shots (k) including 1, 2, 4, 8, and 16.  It compares the performance of the Tip-Adapter, LP++, and aTLAS methods (with both standard and linearised task vectors). The best-performing method for each shot is highlighted in bold.", "section": "D Few-shot learning"}, {"figure_path": "G9OJUgKo4B/tables/tables_23_1.jpg", "caption": "Table 1: Performance of task negation averaged across eight datasets. Selected results must maintain at least 95% of the pre-trained accuracy on the control dataset, following previous practice [44]. Best performance in each section is highlighted in bold. Task vector is abbreviated as t.v. Results for each dataset are available in Table 7.", "description": "This table presents the results of task negation experiments conducted on eight image classification datasets.  The task negation aims to reduce undesired biases on a target task while maintaining performance on a control dataset (ImageNet). The table compares different methods, including standard task vectors (t.v.), linearised task vectors, and the proposed aTLAS method.  Results include target and control dataset performance metrics and highlights the best-performing method for each. More detailed results for each dataset are provided in a separate table (Table 7).", "section": "4.1 Task negation"}, {"figure_path": "G9OJUgKo4B/tables/tables_23_2.jpg", "caption": "Table 7: Accuracy on target and control tasks of task negation for each of the eight datasets. Highest performance in each section is highlighted in bold. The method search corresponds to model f(x; \u03b80 + \u03b1T\u03c4), where \u03b1 is determined via a hyper-parameter search. Our method aniso. corresponds to model f(x; \u03b80 + A\u03c4\u03c4), where A\u03c4 is a learnable scaling matrix.", "description": "This table presents the results of task negation experiments on eight image classification datasets.  It compares the performance of several methods, including zero-shot, standard task vector search (isotropic), anisotropic task vector scaling (our proposed method), linear task vector search, and linear anisotropic task vector scaling. For each method, target and control task accuracies are shown for three ViT models (ViT-L/14, ViT-B/32, ViT-B/16). The table highlights the best performing method for each dataset and model size.", "section": "4.1 Task negation"}, {"figure_path": "G9OJUgKo4B/tables/tables_24_1.jpg", "caption": "Table 7: Accuracy on target and control tasks of task negation for each of the eight datasets. Highest performance in each section is highlighted in bold. The method search corresponds to model f(x; \u03b80 + \u03b1T ), where \u03b1 is determined via a hyper-parameter search. Our method aniso. corresponds to model f(x; \u03b80 + AT ), where AT is a learnable scaling matrix.", "description": "This table presents the results of task negation experiments on eight image classification datasets.  It compares the performance of different methods, including a baseline (zero-shot), a hyperparameter search method, and the proposed aTLAS method (with both standard and linearised task vectors) on both target and control tasks.  The results are given for three different ViT model sizes. The table highlights the effectiveness of the aTLAS method in achieving strong performance on the target task while maintaining performance on the control task, especially in comparison to the hyperparameter search method.", "section": "4.1 Task negation"}, {"figure_path": "G9OJUgKo4B/tables/tables_25_1.jpg", "caption": "Table 1: Performance of task negation averaged across eight datasets. Selected results must maintain at least 95% of the pre-trained accuracy on the control dataset, following previous practice [44]. Best performance in each section is highlighted in bold. Task vector is abbreviated as t.v. Results for each dataset are available in Table 7.", "description": "This table presents the results of task negation experiments conducted on eight image classification datasets.  The task negation technique aims to reduce undesired biases on a target task while maintaining performance on a control task (ImageNet).  The table compares the performance of several methods: a pre-trained model, a linear standard search method using task vectors, aTLAS (the proposed method), and similar approaches using linearised task vectors.  The results are shown in terms of accuracy on both the target and control tasks, highlighting the best-performing method for each dataset.  More detailed results for each dataset are provided in a separate table (Table 7).", "section": "4 Task arithmetic"}, {"figure_path": "G9OJUgKo4B/tables/tables_26_1.jpg", "caption": "Table 9: Average accuracy for few-shot recognition over 22 datasets. We report accuracy averaged over 3 random n-shot sample selections, with 1\u00d7 standard error. Results are produced using CLIP with ViT-B/32 backbone. For our method, we show results with both standard [28] and linearised [44] task vectors. The best method for each choice of k \u2208 {1, 2, 4, 8, 16} is highlighted in bold.", "description": "This table presents the average accuracy for few-shot recognition across 22 different datasets.  The results are obtained using the CLIP model with the ViT-B/32 backbone. Three different random n-shot sample selections are used, and the standard error is reported alongside the accuracy.  The table compares the performance of the proposed aTLAS method to existing Tip-Adapter and LP++ methods, across various numbers of shots (k=1,2,4,8,16).  Both standard and linearized versions of the aTLAS task vectors are included for comparison.", "section": "D Few-shot learning"}, {"figure_path": "G9OJUgKo4B/tables/tables_27_1.jpg", "caption": "Table 15: Additional few-shot recognition results using LoRAs trained on attention layers, MLP layers or both. Results are averaged across 22 datasets over three seeds, with \u00d71 standard deviation. Rank 16 is used for LoRAs.", "description": "This table presents the few-shot learning performance of aTLAS using different LoRA configurations.  It compares the results of aTLAS using LoRAs trained only on attention layers, only on MLP layers, and on both attention and MLP layers. For each LoRA configuration, it shows the performance of aTLAS alone, and also when combined with LP++ and Tip-Adapter, two other state-of-the-art few-shot learning methods. The results are averaged over 22 datasets and three random seeds.  The table is designed to show the impact of different LoRA training strategies on the overall performance, and how combining aTLAS with other techniques affects the final accuracy.", "section": "D.7 LoRAS as task vectors"}, {"figure_path": "G9OJUgKo4B/tables/tables_27_2.jpg", "caption": "Table 16: Few-shot recognition performance with gradient-free optimization. Results are averaged accuracy over 22 datasets, with \u00d71 standard error over 3 random seeds.", "description": "This table presents the few-shot recognition performance results using gradient-free optimization.  The results are averaged across 22 different datasets and include standard error values, calculated from three separate random seeds. The table shows the performance for different numbers of shots (1, 2, 4, 8, 16) under different scaling methods (anisotropic with gradient and isotropic without gradient). The memory consumption in GB is also specified for each method.", "section": "D.8 Gradient-free optimization"}, {"figure_path": "G9OJUgKo4B/tables/tables_28_1.jpg", "caption": "Table 17: Accuracy after fine-tuning on different percentage of training data for variants of aTLAS \u00d7 K and LoRAs [23]. Results are averaged across 22 datasets. Highest accuracy in each section is highlighted in bold.", "description": "This table shows the accuracy of different aTLAS variants and LoRA, when fine-tuned using different percentages of training data.  The aTLAS variants systematically increase the number of learnable parameters.  The results demonstrate how the accuracy improves with more data and more parameters, although the improvements diminish with larger models.", "section": "6.2 Scalability of aTLAS"}]