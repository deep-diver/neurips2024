[{"type": "text", "text": "Optimizing the coalition gain in Online Auctions with Greedy Structured Bandits ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Dorian Baudry1,2,\\* Hugo Richard2,\\* Maria Cherifa2,\\* Clement Calauzenes2 ", "page_idx": 0}, {"type": "text", "text": "Vianney Perchet? ", "page_idx": 0}, {"type": "text", "text": "1 Department of Statistics, University of Oxford. 2 2 Inria Fairplay Joint team, CREST, ENSAE Paris, Criteo AI Lab ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Motivated by online display advertising, this work considers repeated second-price auctions, where agents sample their value from an unknown distribution with cumulative distribution function $F$ .In each auction $t$ ,a decision-maker bound by limited observations selects $n_{t}$ agents from a coalition of $N$ to compete for a prize with $p$ other agents, aiming to maximize the cumulative reward of the coalition across all auctions. The problem is framed as an $N$ -armed structured bandit, each number of player sent being an arm $n$ , with expected reward $r(n)$ fully characterized by $F$ and $p+n$ . We present two algorithms, Local-Greedy (LG) and Greedy-Grid (GG), both achieving constant problem-dependent regret. This relies on three key ingredients: 1. an estimator of $r(n)$ from feedback collected from any arm $k$ , 2. concentration bounds of these estimates for $k$ within an estimation neighborhood of $n$ and 3. the unimodality property of $r$ under standard assumptions on $F$ . Additionally, GG exhibits problem-independent guarantees on top of best problem-dependent guarantees. However, by avoiding to rely on confidence intervals, LG practically outperforms GG, as well as standard unimodal bandit algorithms such as OsUB or multi-armed bandit algorithms. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The online display advertising has seen remarkable evolution in recent decades [14, 30, 33, 25]. Publishers, who are the suppliers of digital ad space on the internet, sell display spots for ads to advertisers through real-time bidding in spot auctions, with many of these auctions being conducted using first or second-price mechanisms [20]. Due to the technological complexity of online advertising, advertisers usually delegate the task of buying ad placements to demand-side platforms (DSP) that operate many advertising campaigns. This interaction between DSP and the publisher, can be simplified as the publisher acting as multiple ad auctions selling ad impressions (online displays), while the DSP acts as a centralized coalition: at each time step, it determines which campaign(s) from the coalition participate to the auction to maximize their total gain. The chosen campaign(s) then compete with others to secure impressions. The primary goal of advertising companies is then to maximize the cumulative utility: the total value of impressions won minus their costs. This raises a fundamental question: how many ad campaigns should participate in the auction to optimize the overall utility ? In the interim setting, where the DsP observes current bidder values before deciding, it's known that only the highest value bidder should be sent. However, online privacy enhancements in browsers necessitate ex-ante decisions from DSPs [8], without exact value knowledge. Here, the problem becomes challenging: choosing a small number of campaigns can make it difficult to secure impressions, while securing the spot with a large number of bidders inevitably raises the price due to competition. In this paper, this problem is formalized and solved via novel Multi-Armed-Bandit (MAB) algorithms. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Problem statement  Consider a sequence of $T$ ad impressions sold through second price auctions (see [20] for a survey). At auction $t\\in[T]$ , each participant (bidder) bids on the item based on its own (stochastic) value for the item. The highest bidder wins the item and pays a price equal to the second highest bid. The decision maker (the DSP) runs $N\\in\\mathbb{N}^{*}$ advertising campaigns forming a coalition. At time $t$ , two groups of bidders participate: (1) $n_{t}\\in[N]$ bidders from the coalition chosen by the decision maker ex-ante - without knowing the realization of the bidders\u2019 values - and (2) $p\\in\\mathbb{N}^{*}$ other bidders, that we call the competition. When a bidder from the coalition wins the auction, the decision maker observes the realized value for the winner (also called winning bid). In the rest of the paper, the following assumptions about the behavior of bidders is made. ", "page_idx": 1}, {"type": "text", "text": "Assumption 1.All bidders are identical, theirvalues are sampledii.dfroma distribution supported on[o,1] characterized by its cumulative distribution function $(c.d.f.)\\ F$ Allbiddersbid theirvalue. ", "page_idx": 1}, {"type": "text", "text": "Assuming identical bidders with i.i.d values is a strong but widespread assumption in auction theory [20, 22], known as the symmetric bidders case. It is particularly relevant in online advertising, notably in homogeneous impression markets where advertisers compete for similar ad displays due to shared objectives, target demographics, or placement competition. The bounded support assumption is also standard, as letting an automated system bid arbitrarily large values is unrealistic. Finally, bidders bid their value as this is a weakly dominant strategy in this case. Lastly, assuming a known number ofcompetitors $p$ is frequently seen in auction models (see for instance [20] chapter 3.2.2). Under Assumption 1, the expected reward received by the decision maker at time $t$ isgivenby $r(n_{t})$ ,where $r$ is the expected reward function, defined by ", "page_idx": 1}, {"type": "equation", "text": "$$\nr:n\\in[N]\\mapsto r(n):=\\mathbb{E}_{\\mathbf{v}=(v_{i})_{i\\in[n+p]}\\sim F\\times\\cdots\\times F}\\left[(\\mathbf{v}_{(1)}-\\mathbf{v}_{(2)})\\mathbb{I}\\Bigg\\{\\underset{i\\in[n+p]}{\\mathrm{argmax}}\\,v_{i}\\in[n]\\,\\right\\}\\right]\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\mathbf{v}_{(1)}$ and $\\mathbf{v}_{(2)}$ are respectively the first and second maximum of $\\mathbf{v}$ , and $[n]$ is used to abbreviate $\\{1,\\ldots,n\\}$ . The problem therefore reduces to a MAB where the decision maker chooses arms $\\bar{n}_{1},\\ldots,\\bar{n_{T}}\\in[N]$ sequentially and aims to minimize its cumulative expected regret $\\mathcal{R}(T)$ defined by ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathcal{R}(T)=\\sum_{t\\leq T}r(n^{*})-r(n_{t})\\;,\\quad\\mathrm{with}\\quad n^{*}=\\underset{n\\in[N]}{\\mathrm{argmax}}\\,r(n),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "given that privacy constraints from the browser [8] only let the decision maker observes (1) if the coalition won, (2) the realization of the maximum value when winning. ", "page_idx": 1}, {"type": "text", "text": "Related works  Following (2),the problem presented in this paper can be formulated as a Multi-Arm Bandits (MAB, see [21] for a survey). In MAB, a learner repeatedly selects from a set of actions, or \"arms\", each yielding a reward. The goal is to maximize total rewards by striking a balance between exploration (sampling various arms to learn their rewards) and exploitation (picking the arms with the highest anticipated rewards based on collected feedback). While the literature has known a significant development in the last years ([2, 7, 17], to name a few), the most popular approaches arguably remain exponential weights algorithms (ExP3, [4]) in adversarial settings, and optimism in face of uncertainty (UcB, [3]) when rewards are stochastic. ", "page_idx": 1}, {"type": "text", "text": "While UCB and ExP3 can both tackle the regret minimization problem presented here, they inevitably achieve sub-optimal performance due to not using the inherent structure of the expected reward function. Several types of structure have been explored in the bandit literature, some notable examples being linear bandits [1], Lipschitz bandits [23], or unimodal bandits [9, 28, 29]. The problem considered here is novel in the literature of structured bandits, arising from the observability restrictions coming with privacy-enhancing systems. Still, in the next section we show that unimodality - in this paper thefact that $r$ admits only one local (hence global) maximum - is in many cases inherited from this stronger structure. A typical strategy to exploit unimodality - also used in this work - consists in playing a standard bandit policy (such as UCB) on a well chosen subset of arms (OSUB, [9]). ", "page_idx": 1}, {"type": "text", "text": "Last, the use of online learning algorithms to tackle repeated auction problems have been explored in various contexts ([26, 12, 5, 32, 27, 11]). However, none of these works approach the problem through the perspective of a coalition of bidders, and are thus not applicable to this setting. ", "page_idx": 1}, {"type": "table", "img_path": "B74mb0tEY6/tmp/993238e6f8f397d14fe3ccd557db7bc6cfe91bc299d8323aa77b951acd51ba26.jpg", "table_caption": ["Table 1: Comparison of regret guarantees for different algorithms "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "Outline and contributions. Section 3 presents two novel bandit algorithms: LG (Local Greedy) which is inspired by OsUB[9], and GG (Greedy Grid) which combines Local Greedy and a successive elimination strategy. Theorem 2 and Theorem 3 provide upper bounds on the regret of LG and GG respectively, which are summarized in Table 1. Both algorithms achieve problem-dependent regret independent of $T$ . However, their scaling differs: the regret of LG depends on the worst local gap $\\begin{array}{r}{\\Delta=\\operatorname*{min}_{n\\in[N]}|r(n+1)-r(n)|.}\\end{array}$ while for G it only depends on the gaps $\\Delta_{n}=r(n^{*})-r\\bar{(}n\\bar{)}$ Furthermore, w.h.p. GG only suffers regret for arms in a reference grid $\\boldsymbol{S}$ containing $\\mathcal{O}(\\log(N))$ arms and in a neighborhood $B^{\\star}$ of the optimal arm. All these quantities, as well as the notation $\\widetilde O$ and ${\\widetilde{\\mathcal{O}}}_{N}$ (hiding logarithmic factors), are defined in Section 3. These regret upper bounds rely on three key ingredients presented in Section 2: (1) an estimator of $r(n)$ from feedback collected from any arm $k$ (2) novel concentration bounds on these estimates for $k$ within an estimation neighborhood of $n$ (Theorem 1) and (3) the unimodality property of $r$ under standard assumptions on $F$ . Lastly, Appendix D provides an experimental benchmark comparison of the performance of GG, LG and their competitors: LG has the lowest expected regret among the algorithms tested. Indeed, LG avoids the explicit use of the confidence bounds in the algorithm which makes it more practical, even though GG admits better theoretical guarantees. ", "page_idx": 2}, {"type": "text", "text": "2   Estimating the reward function from samples of powers of $F$ ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this part, we put aside the sequential nature of the repeated auction setting that we introduced and consider the problem of estimating the expected reward as a function of the number of bidders, given a stream of collected data. We first present a formulation of the expected reward function in terms of powers of the c.d.f. $F$ . Then, we leverage this formula to introduce power estimates, as a solution to estimate the expected reward of an arm $n\\in[N]$ from samples collected from an arm $k\\in[N]$ . Lastly, we discuss the theoretical properties of these estimates, introducing upper and lower confidence bounds on the expected reward in Theorem 1. ", "page_idx": 2}, {"type": "text", "text": "2.1  Properties of the expected reward ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The expected reward function $r$ (Eq. (1) can be expressed as a function of $n,p$ and the c.d.f. $F$ ", "page_idx": 2}, {"type": "text", "text": "Lemma 1. The expected reward function defined in Equation (1) satisfies, ", "page_idx": 2}, {"type": "equation", "text": "$$\nn\\in[N]\\mapsto r(n)=n\\int_{0}^{1}F^{p+n-1}(x)-F^{p+n}(x)\\mathrm{d}x\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The proof can be found in Appendix A.1 and is based on properties of order statistics. ", "page_idx": 2}, {"type": "text", "text": "This particular definition of $r(n)$ , which is a product of $n$ and a function that decreases with $n$ suggests that $r$ could be unimodal for some choices of $F$ . In the rest of the paper, we restrict ourselves to distributions that guarantees unimodal reward functions. ", "page_idx": 2}, {"type": "text", "text": "Assumption 2. $F$ and $p$ are such that the reward function $r$ in Equation (3) is unimodal ", "page_idx": 2}, {"type": "text", "text": "As the next lemma shows, many classical distributions lead to unimodal rewards for all $p\\in\\mathbb N$ ", "page_idx": 2}, {"type": "text", "text": "Lemma 2.Let $F$ be the cumulative distribution function of a Bernoulli, truncated exponential or Complementary Beta distribution. Then, for any $p\\in\\mathbb{N}^{*}$ $r$ in Equation (3) unimodal. ", "page_idx": 2}, {"type": "text", "text": "The proof of Lemma 2 can be found in Appendix A.2. Note that the Complementary Beta distributions [19], chosen for technical reasons, are similar to Beta distributions and any Beta distribution can be approached by a Complementary Beta. Furthermore, in Appendix A.3 we present experiments suggesting that $r$ is unimodal for all $p\\in\\mathbb{N}^{*}$ if $F$ is the c.d.f of Beta or Kumaraswamy distributions. However, we also show in Appendix A.4 that this is not always the case, by providing a counter example. Nonetheless, we argue that (complementary) beta or truncated exponentials are fexible models for real world data, so Assumption 2 is reasonable in practice. We furthermore discuss in Section 3.2 the adaptation of our algorithms if this was not the case. ", "page_idx": 3}, {"type": "text", "text": "2.2  Estimation of powers of $F$ ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Consider the feedback $\\overline{{W_{k}}}\\:=\\:\\left(w_{k,1},\\ldots,w_{k,m_{k}}\\right)$ gathered after playing arm $k$ and winning the auction $m_{k}$ times. $\\overline{{W_{k}}}$ , represents the sequence of first values (value of the winning bid) which has been collected by arm $k$ ", "page_idx": 3}, {"type": "text", "text": "It is well known that the marginal distribution of any order statistic can be expressed as a function of the c.d.f. $F$ (see Section 2.1 of [1o]). The distribution of any element of $\\overline{{W}}_{k}$ hascumulative distribution function $F_{k}:x\\,\\in\\,[0,1]\\,\\to\\,F^{k+p}(x)$ which clearly exhibits a one-to-one mapping between $F_{k}(x)$ and $F(x)$ . Hence, given $\\overline{{W}}_{k}$ , for any $\\ell\\in\\mathbb{N}$ wecanestimate $F^{\\ell}$ by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\widetilde{F}_{k+p}^{\\ell}:x\\mapsto(\\widehat{F}_{k+p}(x))^{\\frac{\\ell}{k+p}},\\mathrm{~where~}\\widehat{F}_{k+p}:x\\mapsto\\frac{1}{m_{k}}\\sum_{j=1}^{m_{k}}\\mathbb{1}\\{w_{k,j}\\leq x\\}\\mathrm{~(emp.~c.d.f.~of~}\\overline{{W_{k}}}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Estimation of $r$ Consider any arm $n\\in[N]$ . Following Equation (3), it appears that estimating both $F^{n+p}$ and $F^{n+p-1}$ is suficiento construct anestimateof $r(n)$ According to Equation (4), this can be done from samples originated from any arm $k\\in[N]$ , by using the simple estimate ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\widehat{r}_{k}(n)=n\\int_{0}^{1}\\Big(\\widetilde{F}_{k+p}^{n+p-1}(x)-\\widetilde{F}_{k+p}^{n+p}(x)\\Big)\\,\\mathrm{d}x\\ .\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Furthermore, it also clear that any convex combination of estimates can become a new estimate, however in the rest of the paper we focus on simple estimates for simplicity. ", "page_idx": 3}, {"type": "text", "text": "Remark 1 (Adaptation to different feedback). $A$ similar procedure can be derived for a setting where the sequence of second prices would be observed instead.Indeed, their distribution would be $G_{k}:x\\in[0,{\\bar{1}}]\\mapsto(k+p)F({\\bar{x}})^{k+p-1}-(k+p-1)F(x)^{k+p}$ whichcanlead to a reward estimate similar to (5) by using a suitable inversion formula. The same can be said for the case where both first and second prices are observed,with additional complexity because the joint distribution should be considered sincefor each auction the first and second price are dependent variables. ", "page_idx": 3}, {"type": "text", "text": "2.3Concentration of estimates of the reward function ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We now introduce the first theoretical contribution of this paper: confidence bounds on the deviations of an empirical estimate $\\widehat{r}_{k}(n)$ w.r.t. the true expected reward $r(n)$ ", "page_idx": 3}, {"type": "text", "text": "Importance of (relatively) local estimation  In principle, (5) suggests that samples from any arm $k\\in[N]$ can provide a simple estimate of the reward function of any other arm $n\\in[N]$ . However, we establish that the position of $k$ w.r.t. $n$ significantly impacts the concentration of $\\widehat{r}_{k}(n)$ . Intuitively, the ratio $(n+p)/(k+p)$ determines how the uncertainty on $F^{k+p}$ propagates on the reward after performing the inversion to obtain an estimate of $F^{n+p}$ . Indeed, considering any $i\\in\\mathbb N$ , if for some $x\\in[0,1]$ the deviation $F(x)^{i}-{\\widehat{F}}_{i}(x)$ is small then a first order approximation provides that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\forall j\\in\\mathbb{N}\\ :\\ \\left(F(x)^{i}\\right)^{\\frac{j}{i}}-\\widehat{F}_{i}(x)^{\\frac{j}{i}}\\approx\\left(F(x)^{i}-\\widehat{F}_{i}(x)\\right)\\times\\frac{j}{i}F_{i}(x)^{\\frac{j}{i}-1}\\ .\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Hence, a small error on $F(x)^{i}$ is multiplied by ${\\frac{j}{i}}F_{i}(x)^{{\\frac{j}{i}}-1}$ to obtain the resulting error on $F(x)^{j}$ . For $j\\geq i$ this term can be as large as $j/i$ while for $j<i$ it can be arbitrarily large if $F^{i}(x)$ is very small. This observation motivates a restriction on the range of arms that can be used to estimate the reward of a given arm $n$ , that we call its estimation neighborhood. We use the convention that arms smaller than 1 or greater than $N$ exist but have not collected any sample and have a known reward of $0$ ", "page_idx": 3}, {"type": "text", "text": "Definition 1 (Estimation neighborhood of an arm $n$ ). Assume3 that $p\\geq4$ Then, the estimation neighborhoodof $n$ is therange $\\begin{array}{r}{\\mathcal{V}(n)=[v_{\\ell}(n),v_{r}(n)]=\\left\\{k\\in[N]:\\ k+p\\in\\left[\\frac{n+p}{2},\\frac{3}{2}(n+p-1)\\right]\\right\\}}\\end{array}$ We call $v_{\\ell}(n)$ and $v_{r}(n)$ respectively the furthest left and right neighbor of $n$ ", "page_idx": 4}, {"type": "text", "text": "Theorem 1 (Concentration of simple estimates). Consider any $n\\in[N]$ and $k\\in\\mathcal{V}(n)$ . Let $\\widehat{r}_{k}(n)$ be defined according to (5) from $m_{k}$ samples collected by $k$ Then, there exists some constants $\\beta_{k,n}$ (depending on $n,k,p)$ and $\\xi_{k,n,F}$ (additionally depending on $F$ )such that,with probability $1-\\delta$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n|\\widehat{r}_{k}(n)-r(n)|\\leq\\beta_{k,n}\\sqrt{\\frac{\\log\\left(\\frac{2\\lceil n\\sqrt{m_{k}}\\rceil}{\\delta}\\right)}{m_{k}}}+n\\times\\xi_{k,n,F}\\left(\\frac{\\log\\left(\\frac{2\\lceil n\\sqrt{m_{k}}\\rceil}{\\delta}\\right)}{m_{k}}\\right)^{\\frac{n+p-1}{k+p}}~.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Furthermore,the constants admit universal upper bounds for any $n,k,p,F$ Forinstanceif $m_{k}\\geq4$ it holds that $\\beta_{k,n}\\leq33$ and $\\gamma_{k,n,F}\\leq100$ ", "page_idx": 4}, {"type": "text", "text": "Proof sketch (see Appendix $B$ for the detailed proof). The first ingredient consists in approximating the reward formulation (3) by a Riemann sum: for some step size $D^{-1}>0$ , it holds that $\\widehat{r}_{k}(n)\\,\\bar{-}$ $\\begin{array}{r}{r(n)\\,=\\,\\frac{n}{D}\\sum_{s=0}^{D-1}\\mathcal{E}(x_{s})+\\mathrm{err}_{D}}\\end{array}$ , with $x_{s}\\,=\\,s/D$ for all $s\\,\\in\\,\\{0,\\dots,D-1\\}$ . In Lemma 4 we use elementary properties of $F$ to show that the approximation error satisfies $\\mathrm{err}_{D}\\,\\in\\,[0,n D^{-1}]$ Next, we upper and lower bound ${\\mathcal E}(x_{s})$ with different concentration bounds according to the value of $F_{k,s}\\,:=\\,F(x_{s})^{k+p}$ . More precisely, for any $\\delta\\,\\in\\,(0,1)$ the following bounds hold each with probability at least $1-\\delta$ \uff0c ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\vert\\widehat{F}_{k}(x_{s})-F_{k,s}\\vert\\le\\sqrt{F_{k,s}}\\times\\sqrt{\\frac{3\\log\\left(\\frac{2}{\\delta}\\right)}{m_{k}}}\\quad\\mathrm{if}\\ F_{k,s}\\in I_{0}:=\\left[\\frac{3\\log\\left(2/\\delta\\right)}{m_{k}},1\\right]\\quad\\mathrm{(Chernoff)}\\ ,\\right.}\\\\ {\\left.\\left.\\widehat{F}_{k}(x_{s})\\le\\frac{6\\log\\left(2/\\delta\\right)}{m_{k}}\\quad\\mathrm{if}\\ F_{k,s}\\in I_{1}:=\\left(\\frac{\\delta}{m_{k}},\\frac{3\\log\\left(2/\\delta\\right)}{m_{k}}\\right)\\quad\\qquad\\qquad\\quad\\mathrm{(Chernoff)}\\ ,\\right.}\\\\ {\\left.\\widehat{F}_{k}(x_{s})=0\\quad\\mathrm{if}\\ F_{k,s}\\in I_{2}:=\\left[0,\\frac{\\delta}{m_{k}}\\right]\\quad\\qquad\\qquad\\quad\\qquad\\quad\\quad\\quad\\mathrm{(union~bound)}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "These results are derived in Lemma 5 from a well-known multiplicative form of the Chernoff bound for Bernoulli random variables [16]. Then, the analysis consists in using the appropriate bound for each point $s\\in\\{0,\\ldots,D-1\\}$ . The interval $I_{0}$ provides the first term in (7), which is dominant in terms of $m_{k}$ , and we make $\\beta_{k,n}$ fully independent of $F$ by carefully using some properties of the reward function. The two remaining intervals $I_{1}$ and $I_{2}$ provide the second term in (7), and $\\gamma_{k,n,F}$ depend on $F$ through the boundaries of the interval $I_{1}$ . The corresponding factor in $\\xi_{k,n,F}$ can be bounded by 1 or estimated in practice (see Appendix B.4). \u53e3 ", "page_idx": 4}, {"type": "text", "text": "In Appendix B, we give the expression of $\\beta_{k,n}$ and $\\xi_{k,n,F}$ and provide in (17) and (19) fully explicit upper and lower confidence bounds on $\\widehat{r}_{k}(n)$ , depending on all problem parameters, and that are much tighter than what the universal constants provided in the theorem suggest. These universal constants are purely indicative, in order to assess that $\\beta_{k,n}$ and $\\xi_{k,n,F}$ do not diverge for any value of the problem parameters. We now provide more high-level comments on the derivation of this result. ", "page_idx": 4}, {"type": "text", "text": "Discussion The proof of Theorem 1 is non-trivial, and the careful usage of the Chernoff bounds that we introduced is crucial to obtain tight bounds on $\\widehat{r}_{k}(n)$ for two reasons. First, it seems necessary to concentrate estimates from arms $k>n$ (see the discussion below (6)), which are instrumental to the performance of the bandit algorithms presented in the next section. Secondly, by exhibiting powers of $F$ , they make $\\beta_{k,n}$ not increasing linearly in $n$ , which is not easy to achieve. Indeed, it is clear from the analysis that this cost would be inevitable with standard Hoeffding bounds. However, completely avoiding $n$ seems difficult in general, so our proof provides a way to mitigate its cost by multilying it by a higher power of $m_{k}^{-1}$ at least $m_{k}^{-\\frac{2}{3}}$ $\\begin{array}{r}{k+p=\\frac{3}{2}(n+p-1))}\\end{array}$ . This is the theoretical motivation for the definition of $\\dot{\\mathcal{V}}(n)$ (Definition 1): while $k\\ \\bar{+}\\,p=2(n+p-1)$ would lead to theoretically valid results, it would not ensure that the linear term in $n$ is second-order in $m_{k}$ We now conclude this section by exhibiting a condition on $F$ that allows to reduce the scaling of the confidence bound in $n$ to logarithmic terms. ", "page_idx": 4}, {"type": "text", "text": "Lemma 3 (Improved bound for Lipschitz quantile function). Assume that $k\\in\\mathcal{V}(n)$ and $F^{-1}$ is $L$ -Lipschitz, then there exists an absolute constant $\\xi$ such that with probability $1-\\delta$ itholdsthat ", "page_idx": 5}, {"type": "equation", "text": "$$\n|\\widehat{r}_{k}(n)-r(n)|\\leq\\beta_{k,n}\\sqrt{\\frac{\\log\\left(\\frac{2\\lceil n\\sqrt{m_{k}}\\rceil}{\\delta}\\right)}{m_{k}}}+\\xi L\\log\\left(\\frac{4\\lceil n\\sqrt{m_{k}}\\rceil}{\\delta}\\right)\\left(\\frac{\\log\\left(\\frac{2\\lceil n\\sqrt{m_{k}}\\rceil}{\\delta}\\right)}{m_{k}}\\right)^{\\frac{n+p-1}{k+p}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This result is proved in Appendix B.3, and shows that for some distributions (e.g. \u201cclose\u201d\u2019 to uniform) the confidence bounds converge relatively fast to standard sub-Gaussian type of bounds, even for verylarge $n$ . Whether this result holds in general remains open. ", "page_idx": 5}, {"type": "text", "text": "3 Bandit algorithms ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "3.1Bandit algorithms: Local-Greedy (LG) and Greedy-Grid (GG) ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We now detail the two novel bandit algorithms proposed to tackle the problem presented in Section 1. Both rely on the use of simple estimates of $r(n)$ (see Section 2) by arms present in its estimation neighborhood $\\mathcal{V}(n)$ (see Definition 1) and theoretically motivated by Theorem 1. In this section, for ease of exposition, we describe algorithms as if feedback was collected at every time steps. In Appendix C.1, we show that the algorithms and their guarantees only require a slight adaptation when the feedback is collected only when the auction is won. ", "page_idx": 5}, {"type": "text", "text": "3.1.1 Local-Greedy ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We first present Local-Greedy (LG), which is a natural adaptation of a standard policy in unimodal bandits, OSUB [9].The main idea of OSUB is to play UCB locally around a reference arm, and eventually reach the optimal arm $n^{\\star}$ by gradually moving the reference arm in its direction. With LG, we adapt this principle to efficiently exploit the structure of the problem considered: at each round $t$ LG defines a reference arm $\\ell_{t}$ , called leader, but plays greedily in the neighborhood $\\mathcal{V}(\\ell_{t})$ , based on simple power estimates computed with samples from $\\ell_{t}$ only. In addition a sampling requirement, implemented by a parameter $\\alpha\\in(0,1)$ , is used in order to ensure the good concentration of these estimates. We detail Local-Greedy in Algorithm 1 below. ", "page_idx": 5}, {"type": "table", "img_path": "B74mb0tEY6/tmp/12eb8336b0d732e130af20eeecdcb0123a861850973b6a0c2b9357da68084f85.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "High-level properties of LG First, using Greedy instead of UCB is only possible because of the structure of the problem: when $\\ell_{t}$ is well-explored the estimates of arms in $\\Vdash(\\ell_{t})$ computedwith samplesfrom $\\ell_{t}$ are sufficiently close to the true reward, so that no exploration is needed. The sampling requirement then guarantees that all greedy plays are made when $\\ell_{t}$ is well explored. ", "page_idx": 5}, {"type": "text", "text": "A second property is that since $|\\mathcal{V}(\\ell_{t})|$ grows with $\\ell_{t}$ , a sequence of locally optimal moves (best play in a given neighborhood) allows to reach the optimal arm exponentially fast (in $\\mathcal{O}(\\log(N))$ steps), which is particularly interesting in practice if $N$ is large. On the other hand, LG might suffer from the inherent drawback of any \u201clocal' policy: identifying a high-rewarding arm in a neighborhood can take a long time if the reward curve in this area is flat (depending on how small are the \u201clocal gaps). This problem can be attenuated, but not solved, by adding an initial exploration phase. We propose Greedy-Grid, presented in the next section, as a way to fully address this issue. ", "page_idx": 5}, {"type": "text", "text": "Lastly, requiring only the computation of empirical reward estimates is a strength of Local-Greedy. Indeed, deriving tighter confidence bounds would improve its analysis, but not the practical implementation (and performance) of the algorithm. ", "page_idx": 5}, {"type": "text", "text": "3.1.2  Greedy-grid ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The concept of Greedy-Grid is very intuitive: it plays a Local-Greedy strategy only if it can tell which segment of the reward function contains the best arm with high probability. To implement this idea, GG uses a Successive-Elimination procedure [13] on a subset of arms forming a reference grid, denotedby $\\boldsymbol{S}$ ", "page_idx": 6}, {"type": "text", "text": "Reference grid _The grid $\\boldsymbol{S}$ is designed so that two of its successive arms belong to their respective neighborhood (Definition 1), and can hence mutually estimate themselves and all arms in between (Theorem 1). In particular, the optimal arm can be well-estimated at least by its two closest neighbors on the grid, so its neighborhood can be \u201cdiscovered\u201d with high probability simply by sampling the points in the grid in a round-robin fashion for a sufficiently long time. ", "page_idx": 6}, {"type": "text", "text": "Following Definition 1, we construct $\\boldsymbol{S}\\;:=\\;\\{\\boldsymbol{s}_{i}\\}_{i\\geq1}$ recursively: $s_{1}\\,=\\,1$ , and for $i\\geq2$ weset $s_{i+1}=\\operatorname*{max}\\left\\{s\\geq s_{i}:s\\in[N]\\right.$ $s\\in\\mathcal{V}(s_{i}),s_{i}\\in\\mathcal{V}(\\overline{{s}})\\}$ . We provide an illustrative example below. ", "page_idx": 6}, {"type": "text", "text": "Example 1. For $N=2000$ and $p=100$ the grid is $\\mathcal{S}=\\{1,50,123,233,398,645,1016,1572\\}$ ", "page_idx": 6}, {"type": "text", "text": "Any arm $n\\,\\in\\,[N]$ admits a left and right \u201cneighbor in the grid\", denoted respectively by $v_{l}^{S}(n)$ and $v_{r}^{S}(n)$ and defined by: $v_{l}^{S}(n)~=~0$ if $n~<~\\operatorname*{min}S$ \uff0c $v_{r}^{\\bar{S}}(n)\\;=\\;N\\,+\\,1$ if $n~>~\\operatorname*{max}S$ and $(v_{l}^{S}(n),v_{r}^{S}(n))\\,=\\,\\mathrm{argmin}_{(x,y)\\in S\\backslash\\{n\\}:\\,n\\in[x,y]}(y-x)$ otherwise . We call the \"bin\" of am $n$ all arms between its left and right neighbors: $\\dot{\\mathcal{B}}(n)=\\{n\\in[N],v_{l}^{S}(n^{\\prime})<n<v_{r}^{S}(n^{\\prime})\\}$ . For simplicity we use the notation $B^{\\star}=B(n^{\\star})^{4}$ ", "page_idx": 6}, {"type": "text", "text": "Greedy-Grid  We provide the detailed implementation in Algorithm 2 below, and now describe the general principle of the algorithm. At each round, it operates in two steps. In the first step, it decides whether to play arms on the grid $\\boldsymbol{S}$ (play the grid, to simplify), or to focus on a specific bin (and, as we will see, play greedy). This choice depends on an elimination procedure: an arm $k$ in $\\boldsymbol{S}$ should be eliminated for this round if their upper confidence bound (UCB) is smaller than the best lower confidence bound (LCB) among all other arms. Furthermore, if there exists an eliminated arm whose index is closer to the index $i_{t}^{*}$ of the arm with the best LCB, then the unimodality assumption implies that $k$ should also be eliminated. The set of arms not eliminated at $t$ is called $\\ensuremath{\\mathcal{C}}_{t}$ in Algorithm 2. ", "page_idx": 6}, {"type": "text", "text": "TocomputetheUCB $\\left(U_{n}\\right)$ and LCB $\\left(L_{n}\\right)$ of an arm $n$ , we elect a leader $\\ell_{n}$ which is the arm in $[v_{l}^{S}(n),\\dot{v}_{r}^{S}(n)]$ that was played the most in the last $t$ rounds and then compute the bounds based on $\\ell_{n}$ , using Theorem 1. We show in the proof of Theorem 3 that this procedure ensures that a linear number of samples in $t$ is used to compute the UCB and LCB of arms in $B^{\\star}$ with high probability. ", "page_idx": 6}, {"type": "text", "text": "If at least one arm is not eliminated ( $\\mathcal{C}_{t}$ is not empty), arms in $\\ensuremath{\\mathcal{C}}_{t}$ are played one after the other (Round Robin). If all arms in the grid are eliminated, GG plays greedily in the bin $B(i_{t}^{*})$ of the arm with the highest LCB. The empirical reward of each arm $n\\in B(i_{t}^{*})$ is computed similarly as $U_{n}$ and $L_{n}$ using samples from the leader $\\ell_{n}$ . GG then plays the best empirical arm $\\alpha t$ times which is the same sampling requirement as LG. ", "page_idx": 6}, {"type": "text", "text": "The careful design of Greedy-Grid prevents the main theoretical drawback of Local-Greedy: since the algorithm has a very low probability to play in a sub-optimal bin, it almost never pays \u201clocal gaps\" in a sub-optimal part of the reward function. However this guarantee comes at a cost: if $n^{\\star}$ isnot in the grid, it will never be played until the confidence intervals shrink \u201csufficiently\" to eliminate the entire grid. Hence, GG might be more conservative than LG in practice, while offering better theoretical guarantees. We express this trade-off in the next section. ", "page_idx": 6}, {"type": "text", "text": "3.2  Regret upper bounds ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We now present the theoretical results obtained for the two algorithms presented in Section 3. We first establish the regret bounds and sketch their proofs, before discussing and comparing the results. We introduce some notation, that considerably simplifies the presentation of the results. ", "page_idx": 6}, {"type": "text", "text": "Notation: $\\widetilde O$ and ${\\widetilde{\\mathcal{O}}}_{n}$ For any $x>0$ , we use the notation $\\widetilde O(x)$ to describe a quantity that scales in $x$ , up to logarithmic terms in $x$ and $N$ (hence the notation is linked to the problem). Furthermore, ", "page_idx": 6}, {"type": "text", "text": "Input: Grid $\\boldsymbol{S}$ , confidence levels $(\\delta_{t})_{t\\in\\mathbb{N}}$ , sampling parameter $\\alpha$ Play $n_{1}=\\operatorname*{min}S$ and observe $w\\sim F^{n_{1}+p}$ ", "page_idx": 7}, {"type": "image", "img_path": "B74mb0tEY6/tmp/987ccc0fe947208baa18e84d963fc8c2d0e3a5e77f55eb28e801fb70584afc2a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "for $n\\in[N]$ we also use ${\\widetilde{\\mathcal{O}}}_{n}$ as a shorthand notation for ${\\widetilde{\\mathcal{O}}}(\\{n^{6}\\vee x\\}\\wedge n^{2}x)$ .Thistype of constants emerges from using (7) (Theorem 1) in the analysis. Indeed, we proved that the simple estimate of an arm $n$ by an arm $k\\in\\mathcal{V}(n)$ admit sub-Gaussian (\"square-root\") confidence intervals, independent of $n$ ,when the sample size of $k$ is larger than $\\Omega(n^{6})$ ", "page_idx": 7}, {"type": "text", "text": "Theorem 2 (Regret bound for Local-Greedy). Let $\\begin{array}{r}{\\Delta:=\\operatorname*{min}_{n\\in[N-1]}|r(n+1)-r(n)|}\\end{array}$ (worst local gap). Under Assumption 2 and with $\\alpha=(\\log_{3/2}N+1)^{-1}$ , the regret of LG is upper bounded by $a$ problem-dependent constant: there exists $(C_{n})_{n\\in[N]\\backslash\\{n^{\\star}\\}}$ , each satisfying $\\begin{array}{r}{C_{n}=\\widetilde{O}_{N}\\left(\\frac{\\Delta_{n}}{\\Delta^{2}}\\right)}\\end{array}$ ,such that $\\begin{array}{r}{\\mathcal{R}_{T}\\leq\\sum_{n\\in[N]\\backslash n^{\\star}}C_{n}}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "Additionally, if the arm set forms a single estimation neighborhood, that is $\\forall n\\in[N]:\\,\\mathcal{V}(n)\\supset[N],$ then each constant $C_{n}$ can be refined to $\\widetilde{\\mathcal{O}}_{n}\\left(\\Delta_{n}^{-1}\\right)$ providing $\\mathcal{R}_{T}=\\widetilde{\\mathcal{O}}(\\sqrt{N T})$ which holds even when the reward function is not unimodal. ", "page_idx": 7}, {"type": "text", "text": "Proof sketch (see Appendix C.3 for the detailed proof). We start by the case where the arm set forms a single neighborhood. Since LG is guaranteed that any arm it selects will provide an estimate for all the other arms, this context is very similar to a full information scenario. This explains why GG achieves both constant regret depending on the gaps, and a gap-independent bound in $\\sqrt{N T}$ Furthermore, the hidden logarithmic constants come from carefully using Theorem 1 to separate the linear term in $n$ from the gaps when they are small. ", "page_idx": 7}, {"type": "text", "text": "The general case presents an additional complexity. Indeed, it is possible that playing arm $n\\neq n^{\\star}$ is locallyoptimal,if $n$ is the best arm in the neighborhood of the current leader: playing $n$ in that context would not be unlikely. To tackle that scenario, we prove that pulling arm $n$ at time $t$ necessarily implies a locally sub-optimal play, in some estimation neighborhood, at some point in the past (maximized by the chosen value of $\\alpha$ ). We then show that this cannot happen after some deterministic time w.h.p., leading to constant regret. However, since the sub-optimal play might be any arm the constant now depends on the worst local gap $\\Delta^{2}$ \u53e3 ", "page_idx": 7}, {"type": "text", "text": "Theorem 3 (Regret upper bound for Greedy-Grid). Suppose that GG is tuned with confidence level $\\begin{array}{r}{\\delta_{t}=\\frac{1}{N^{2}t^{3}}}\\end{array}$ and $\\alpha=1/4.$ Then,for any $T\\in\\mathbb N$ itholds that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{R}_{T}=\\widetilde{\\mathcal{O}}_{N}\\left(\\sum_{n\\in\\mathcal{B}^{\\star}}\\frac{1}{\\Delta_{n}}+\\sum_{n\\in\\mathcal{S}}\\frac{\\log(T)}{\\Delta_{n}}\\wedge\\Delta_{n}\\left(\\frac{\\mathbb{1}\\{n<n^{\\star}\\}}{\\Delta_{v_{l}(n^{\\star})}^{2}}+\\frac{\\mathbb{1}\\{n>n^{\\star}\\}}{\\Delta_{v_{r}(n^{\\star})}^{2}}\\right)\\right)\\ .\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Aditionally,it holds that $\\mathcal{R}_{T}=\\tilde{\\mathcal{O}}\\left(\\sqrt{(K+|B^{\\star}|)T}\\right)\\!.$ for $K=\\lfloor\\log_{3/2}(N)\\rfloor$ ", "page_idx": 7}, {"type": "text", "text": "Proof sketch (see Appendix C.4 for the detailed proof). First we prove that, w.h.p., during a linear time range in $t\\ G G$ either played the grid or in $B^{\\star}$ . Hence, arms $n\\in[N]\\backslash\\{S\\cup B^{\\star}\\}$ are played a (universall) constant number of times by G in expectation. Then, for $n\\in S$ the term in $\\frac{\\log(T)}{\\Delta_{n}}$ comes from the standard analysis of UCB [3]; while the constant bound comes from exploiting that after a constant time $n$ the LCB of $n^{\\star}$ eliminates its neighboors w.h.p., and by extension the entire grid. Finally, the constant bound $n\\in B^{\\star}$ is derived similarly as the first bound of Theorem 2. \u53e3 ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Discussion _ First, we show that being able to estimate $r(n)$ from the feedback obtained after playing an arm $k$ in its estimation neighborhood leads to a regret independent of $T$ for both LG and GG. For the former, the bound depends in general on the worst local gap $\\Delta$ , while for the latter only the actual gaps $\\Delta_{n}$ (with $n^{\\star}$ ) are involved. This difference permits to obtain a problem-independent guarantee for GG for any configuration of $p$ and $N$ . Furthermore,its scaling $\\sqrt{K+|B^{*}|}\\leq\\sqrt{2n^{*}+\\lfloor\\log_{3/2}(N)\\rfloor}$ can be much smaller than $\\sqrt{N}$ if $n^{*}$ is small. ", "page_idx": 8}, {"type": "text", "text": "Then, we would like to discuss the impact of the concentration bound presented in Theorem 1 on the regret of both GG and LG. Indeed, a naive approach with Hoeffding bounds would not allow to remove $n$ from the first order term of the concentration bound, because of the multiplicative factor $n$ in the definition of $r(n)$ . A feature of our concentration bound is that the linear scaling in $n$ does not appear in the first order term. Informally, this allows to exhibit terms of order ${\\widetilde{\\mathcal{O}}}_{N}({\\bar{\\Delta}}_{n}^{-1})$ in the regret analysis instead of $\\widetilde{\\mathcal{O}}(N^{2}\\Delta_{n}^{-1})$ , which can be significantly better for small gaps. A remark here is that the size of the grid in GG could be optimized as a larger grid makes the second order term in Theorem 1 smaller but is paid linearly in the regret. ", "page_idx": 8}, {"type": "text", "text": "We nevertheless highlight some potential for improvement in the analysis of LG. First, the local gaps $\\Delta$ in the bound of LG could be replaced by (in spirit, referring to $\\boldsymbol{S}$ for simplicity) $\\mathrm{min}_{n\\in[N]}\\,|r(v_{l}^{S}\\tilde{(n)})-$ $r(v_{r}^{S}(n))|$ . It is clear though that this gap remains \u201clocal' and can be arbitrarily smaller than $\\Delta_{n}$ for some arms $n\\in[N]$ , so the general interpretation of the results would be unchanged. Second, for simplicity, the analysis of LG was carried out using the constant upper bound of $\\beta_{k,n}$ and $\\xi_{k,n,F}$ but a tighter analysis could lead to a better dependency with respect to $N$ ", "page_idx": 8}, {"type": "text", "text": "We now justify the use of simple estimates in GG and LG. In practice, combining estimates would allow to use more samples for the estimation. However, this would make the algorithm slower, and we believe that the sampling requirement implemented in the algorithms makes the use of simple estimates efficient: potential uniform exploration in a neighborhood is replaced by a focus on a single arm, but the same quality of information is accrued. Furthermore, from a theoretical perspective union bounds over the samples collected by each arm might also cost a factor $N$ in the analysis. ", "page_idx": 8}, {"type": "text", "text": "Lastly, while GG admits better theoretical guarantees, LG might be more appealing in practice because it does not require to explicitly compute confidence intervals. This means that the regret bounds provided for LG are conservative, and might be refined with tighter confidence bounds without changing the algorithm. ", "page_idx": 8}, {"type": "text", "text": "Adaptation for non-unimodal rewards While LG relies heavily on Assumption 2, GG can be readily adapted to handle non-unimodal reward functions. This is done by modifying the definition of the set of non-eliminated grid arms $\\ensuremath{\\mathcal{C}}_{t}$ to $\\{s\\,\\in\\,{\\cal S},{\\cal U}_{s}\\,\\geq\\,{\\cal L}_{i_{t}^{*}}\\}$ in Algorithm 2. In that case, the algorithm can no longer eliminate arms on the grid based on the elimination of other arms. This naturally induces that the number of plays of sub-optimal arms is no longer bounded by a constant. In Theorem 4 (see Appendix), we show that only the ${\\mathcal{O}}(\\log(T))$ term persists for $n\\in S$ in Theorem 3, while the problem-independent bound remains unchanged. Although we believe unimodality is necessary for achieving constant regret, this result demonstrates that, even without that assumption, GG can still provide the same logarithmic regret guarantees as UCB. However, it does so on a $|{\\mathcal{S}}|$ -armed bandit, rather than an $N$ -armed bandits with $|S|=\\mathcal{O}(\\log(N))\\ll N$ for large $N$ ", "page_idx": 8}, {"type": "text", "text": "Experimental results In Appendix D we present a benchmark of LG, GG, UCB, EXP3 and OSUB on synthetic data in terms of the expected regret $\\mathcal{R}(T)$ .This benchmark illustrates the strong performance of LG relative to the other approaches. Although GG offers more robust theoretical guarantees, particularly with sub-linear problem-independent bounds, LG proves to be more effective in practice. Several factors may explain this gap between theoretical guarantees and empirical performance. First, as discussed in the previous section, the worst-case local gap in the analysis of Local Greedy (Theorem 2) might be overly conservative. This worst-case scenario could occur under a combination of unfavorable conditions, such as poor initialization far from the optimal arm and a fat reward function, paired with bad luck in exploration. However, such a scenario is likely rare in practice and was not encountered in our experiments. Additionally, Local Greedy benefits from scenarios where it starts playing in the optimal neighborhood only after a few steps, a situation GG cannot exploit due to its need for sufficient statistical evidence to eliminate all suboptimal neighborhoods. While GG's caution leads to stronger theoretical guarantees, this comes at the cost of empirical performance. Moreover, GG's results are tied to the tightness of the confidence intervals in Theorem 2, a limitation that does not apply to LG. An interesting and challenging open problem remains whether LG can be modified to achieve the same theoretical guarantees as GG without sacrificing its performance. We leave this question for future work. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "4 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The bandit problem studied in this work is structured since playing arm $n$ gives a reward $r(n)$ determinedbyn,andteukowc.dfFand with prbabilty an observation of a sample of the distribution with c.d.f $F^{n+p}$ ", "page_idx": 9}, {"type": "text", "text": "While traditional bandit approaches give problem dependent bounds depending on $T$ algorithms GG and LG presented in this work have constant problem dependent bounds. Furthermore, GG and LG avoid a quadratic dependency in $N$ forlarge $T$ thanks to new concentration bounds introduced in Theorem 1. Overall, while GG has the best theoretical guarantees, LG has better constants and is therefore better suited for most practical problems (see the discussion at the end of Section 3 and experimental results in Appendix D). ", "page_idx": 9}, {"type": "text", "text": "Whether an algorithm that has the theoretical guarantees of GG and the practical performance of LG can be designed is an interesting question. We believe that the main leverage to improve the practical performance of GG might be to derive tighter concentration bounds. Possible directions to improve over Theorem 1 might include: further refining the decomposition of the integral in (3) according to thevalueof $F$ , further use \u201cempirical\u201d components (depending on estimates of $F$ ),orevenusing ideas from the proof of the DKW inequality [24] to avoid the union bounds over the points of each interval in the decomposition. We leave these directions for future work. ", "page_idx": 9}, {"type": "text", "text": "To conclude, since in practice, a DSP can launch campaigns through multiple auctions, an interesting question is whether the current analysis could be extended to the case of $A$ auctions where a play at time $t$ .s $(n_{a,t})_{a\\in[A]}$ where $\\sum_{a\\in[A]}n_{a,t}=N$ and the reward is $\\textstyle\\sum_{a\\in[A]}r_{a}(n_{a,t})$ with $r_{a}$ determined by integers $p_{a}$ \uff0c $n_{a,t}$ and $F_{a}$ in the same way that $r$ depends on $p,n_{t}$ and $F$ .Howto explore each auction in parallel in an efficient manner and how to handle the case where some auctions must be assigned zero players are then the main questions to solve. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Dorian Baudry thanks the support of the French National Research Agency: ANR-19-CHIA-02 SCAI, ANR-22-SRSE-0009 Ocean, and ANR-23-CE23-0002 Doom; and of the European Research Council (GTIR project). ", "page_idx": 9}, {"type": "text", "text": "Vianney Perchet's research was supported in part by the French National Research Agency (ANR) in the framework of the PEPR IA FOUNDRY project (ANR-23-PEIA-0003) and through the grant DOOM ANR-23-CE23-0002. It was also funded by the European Union (ERC, Ocean, 101071601). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Y. Abbasi-yadkori, D. Pal, and C. Szepesvari. Improved algorithms for linear stochastic bandits. In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Weinberger, editors, Advances in Neural Information Processing Systems, volume 24. Curran Associates, Inc., 2011. URL https: //proceedings.neurips.cc/paper_files/paper/2011/file/ e1d5be1c7f2f456670de3d53c7b54f4a-Paper.pdf. 2 ", "page_idx": 9}, {"type": "text", "text": "[2]  S. Agrawal and N. Goyal. Analysis of thompson sampling for the multi-armed bandit problem. In Proceedings of the 25th Annual Conference on Learning Theory, 2012. 2 ", "page_idx": 9}, {"type": "text", "text": "[3] P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47:235-256, 2002. URL https : //api.semanticscholar.org/ CorpusID :207609497. 2, 3, 8, 33, 36   \n[4] P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The nonstochastic multiarmed bandit problem. SIAM Journal on Computing, 32(1):48-77, 2002. doi: 10.1137/S0097539701398375. URL https : //doi.org/10.1137/S0097539701398375. 2, 3, 36   \n[5]  M. Babaioff, Y. Sharma, and A. Slivkins. Characterizing truthful multi-armed bandit mechanisms: extended abstract._ In Proceedings of the 1Oth ACM Conference on Electronic Commerce, EC \\*09, page 79-88, New York, NY, USA, 2009. Assciation for Computing Machinery. ISBN 9781605584584. doi: 10.1145/1566374.1566386. URL https: //doi.0rg/10.1145/1566374.1566386.2   \n[6] D. Baudry, N. Merlis, M. B. Molina, H. Richard, and V. Perchet. Multi-armed bandits with guaranteed revenue per arm. In International Conference on Artificial Intelligence and Statistics, 2-4 May 2024, Paiau de Congressos, Valencia, Spain, Proceedings of Machine Learning Research, 2024. 28   \n[7] O. Cappe, A. Garivier, O.-A. Maillard, R. Munos, G. Stoltz, et al. Kullback-leibler upper confidence bounds for optimal sequential allocation. The Annals of Statistics, 41(3):1516-1541, 2013. 2   \n[8] P. Chatterjee and I. Sharf.  Bidding and auction services in the privacy sandbox, https://github.com/privacysandbox/protected-auction-services-docs/ blob/main/bidding-auction_services_api.md, 2024. 1, 2   \n[9] R. Combes and A. Proutiere. Unimodal bandits: Regret lower bounds and optimal algorithms. ArXiv, abs/1405.5096, 2014. URL https: //api.semanticscholar.org/ CorpusID:15210470. 2, 3, 6, 36   \n[10]  H. David and H. Nagaraja. Order Statistics. Wiley Series in Probability and Statistics. Wiley, 2004. ISBN 9780471654018. URL https ://books g0ogle.fr/books?id $\\cdot$ bdhzFXg6xFkC. 4,13   \n[11] Y. Deng, N. Golrezaei, P. Jaillet, J. C. N. Liang, and V. S. Mirrokni. Multi-channel autobidding with budget and roi constraints. ArXiv, abs/2302.01523, 2023. URL https: //api.semanticscholar.org/CorpusID:256598278. 2   \n[12] N. R. Devanur and S. M. Kakade. The price of truthfulness for pay-per-click auctions. In Proceedings of the 10th ACM Conference on Electronic Commerce,EC \\*09,page 99-106, New York, NY, USA, 2009. Association for Computing Machinery. ISBN 9781605584584. doi: 10.1145/1566374.1566388. URL https : //doi . org/10 .1145/1566374.1566388. 2   \n[13] E. Even-Dar, S. Mannor, Y. Mansour, and S. Mahadevan. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. Journal of machine learning research, 7(6), 2006. 7   \n[14] L. Ha. Online advertising research in advertising journals: A review. Journal of Current Issues and Research in Advertising, 30, 05 2012. doi: 10.1080/10641734.2008.10505236. 1   \n[15] C. R. Harrs, K. J. Millman, S. J. van der Walt, R. Gommers, P. Virtanen, D. Cournapeau, E. Wieser, J. Taylor, S. Berg, N. J. Smith, R. Kern, M. Picus, S. Hoyer, M. H. van Kerkwijk, M. Brett, A. Haldane, J. F. del Rio, M. Wiebe, P. Peterson, P. Gerard-Marchant, K. Sheppard, T. Reddy, W. Weckesser, H. Abbasi, C. Gohlke, and T. E. Oliphant. Array programming with NumPy. Nature, 585(7825):357-362, Sept. 2020. doi: 10.1038/s41586-020-2649-2. URL https://doi.org/10.1038/s41586-020-2649-2. 35   \n[16]  W. Hoeffding. Probability inequalities for sums of bounded random variables. The collected works of Wassily Hoeffding, pages 409-426, 1994. 5, 18   \n[17] J. Honda and A. Takemura. Non-asymptotic analysis of a new bandit algorithm for semibounded rewards. Journal of Machine Learning Research, 16:3721-3756, 2015. 2   \n[18] J. D. Hunter. Matplotlib: A 2d graphics environment. Computing in Science & Engineering, 9 (3):90-95, 2007. doi: 10.1109/MCSE.2007.55. 35   \n[19] M. Jones. The complementary beta distribution. Journal of Statistical Planning and Inference, 104(2):329-337, 2002. ISSN 0378-3758. doi: https://doi.org/10.1016/S0378-3758(01)00260-9 URL https: //www.sciencedirect.com/science/article/pii/S0378375801002609. 4   \n[20] V. Krishna. Auction Theory. Academic Press, 2009. 1, 2   \n[21] T. Lattimore and C. Szepesvari. Bandit algorithms. 2017. URL https : //tor-lattimore. com/downloads/book/book.pdf. 2   \n[22] J.Levin. Auction theory. Manuscript available at www. stanford. edu/jdlvin/Econ, 20286, 2004. 2   \n[23]  S. Magureanu, R. Combes, and A. Proutiere. Lipschitz bandits: Regret lower bound and optimal algorithms. In M. F. Balcan, V. Feldman, and C. Szepesvari, editors, Proceedings of The 27th Conference on Learning Theory, volume 35 of Proceedings of Machine Learning Research, pages 975-999, Barcelona, Spain, 13-15 Jun 2014. 2   \n[24] P. Massart. The tight constant in the dvoretzky-kiefer-wolfowitz inequality.  The Annals of Probability, 18(3):1269-1283, 1990. ISSN 00911798. URL http: //www . jstor .org/ stable/2244426. 10   \n[25] S. Muthukrishnan. Ad exchanges: Research issues. In Workshop on Internet and Network Economics, 2009. URL https: //api.semanticscholar.org/CorpusID:10046036. 1   \n[26] H. Nazerzadeh, A. Saberi, and R. Vohra. Dynamic cost-per-action mechanisms and applications to online advertising. www $\\cdot08$ , page 179-188, New York, NY, USA, 2008. Association for Computing Machinery. ISBN 9781605580852. doi: 10.1145/1367497.1367522. URL https://doi.0rg/10.1145/1367497.1367522. 2   \n[27] T. Nedelec, C. Calauzenes, N. El Karoui, and V. Perchet. 2022. 2   \n[28] S. Paladino, F. Trovo, M. Restell, and N. Gatti. Unimodal thompson sampling for graphstructured arms. In S. Singh and S. Markovitch, editors, Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, USA. AAAI Press, 2017. 2   \n[29] H. Saber, P. M'enard, and O.-A. Maillard. Forced-exploration free strategies for unimodal bandits. ArXiv, abs/2006.16569, 2020. URL https ://api .semanticscholar.org/CorpusID: 220265988. 2   \n[30]  A. S. Sayedi-Roshkhar. Real-time bidding in online display advertising. Mark. Sci., 37:553-568, 2018. URL https: //api .semanticscholar.org/CorpusID :52277027. 1   \n[31] P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau, E. Burovski, P. Peterson, W. Weckesser, J. Bright, S. J. van der Walt, M. Brett, J. Wilson, K. J. Millman, N. Mayorov, A. R. J. Nelson, E. Jones, R. Kern, E. Larson, C. J. Carey, I. Polat, Y. Feng, E. W. Moore, J. VanderPlas, D. Laxalde, J. Perktold, R. Cimrman, I. Henriksen, E. A. Quintero, C. R. Harris, A. M. Archibald, A. H. Ribeiro, F. Pedregosa, P. van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17:261-272, 2020. doi: 10.1038/s41592-019-0686-2. 35   \n[32] J. Weed, V. Perchet, and P. Rigollet. Online learning in repeated auctions. In V. Feldman, A. Rakhlin, and O. Shamir, editors, 29th Annual Conference on Learning Theory, volume 49 of Proceedings of Machine Learning Research, pages 1562-1583, Columbia University, New York, New York, USA, 23-26 Jun 2016. PMLR. URL https ://proceedings mlr .press/ v49/weed16.html. 2   \n[33]  Y. Yuan, J. Li, and R. Qin. A survey on real time bidding advertising. Proceedings of 2014 IEEE International Conference on Service Operations and Logistics, and Informatics, SOLI 2014, pages 418-423, 11 2014. doi: 10.1109/SOLI.2014.6960761. 1 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Properties of the expected reward function ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this appendix we prove the results presented in Section 2.1 of the paper, and discuss the shape of the expected reward. ", "page_idx": 12}, {"type": "text", "text": "A.1 Proof of Lemma 1 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Lemma 1. The expected reward function defined in Equation (1) satisfies, ", "page_idx": 12}, {"type": "equation", "text": "$$\nn\\in[N]\\mapsto r(n)=n\\int_{0}^{1}F^{p+n-1}(x)-F^{p+n}(x)\\mathrm{d}x\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof. Given $\\mathbf{v}=(v_{i})_{i\\in[n+p]}\\sim F\\times\\cdots\\times F$ ,we have ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{r(n)=\\mathbb{E}\\bigg[(v_{(1)}-v_{(2)})\\bigg\\mathrm{t}\\bigg\\{\\frac{\\mathrm{armax~sr}_{1}}{\\varepsilon(n+\\mu_{1})}r_{i}\\in[n]\\}\\bigg]}\\\\ &{\\qquad\\stackrel{(i)}{=}\\mathbb{E}\\bigg[(v_{(1)}-v_{(2)})\\bigg]\\mathbb{E}\\bigg[\\bigg\\{\\frac{\\mathrm{armax~sr}_{2}}{(\\varepsilon|n+\\mu_{1}|)}r_{i}\\in[n]\\}\\bigg]}\\\\ &{\\qquad\\stackrel{(i i)}{=}\\bigg(\\mathbb{E}\\bigg[v_{(1)}\\bigg]-\\mathbb{E}\\bigg[\\mathrm{v}_{(2)}\\bigg]\\bigg)\\times\\frac{n}{n+p}}\\\\ &{\\qquad=\\frac{n}{n+p}\\times\\int_{0}^{1}\\mathbb{P}(v_{(1)}>x)-\\mathbb{P}(v_{(2)}>x)\\,\\mathrm{d}x}\\\\ &{\\qquad=\\frac{n}{n+p}\\times\\int_{0}^{1}\\mathbb{P}(v_{(2)}\\leq x)-\\mathbb{P}(v_{(1)}\\leq x)\\,\\mathrm{d}x}\\\\ &{\\qquad\\stackrel{(i i i)}{=}\\frac{n}{n+p}\\times\\int_{0}^{1}((n+p)F^{n+p-1}(x)-(n+p-1)F^{n+p}(x)-F^{n+p}(x))\\,\\mathrm{d}x}\\\\ &{\\qquad=n\\int_{0}^{1}\\bigg(F^{n+p-1}(x)-F^{n+p}(x)\\bigg)\\,\\mathrm{d}x\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "The first equality is the definition of $r(n)$ in Equation (1). Equality $(i)$ follows by independence of the index of the maximum and the value of the maximum and second maximum. This is itself a consequence of the fact that the values are i.i.d.. Then equality $(i i)$ follows since the distribution of the index of the maximum is uniform over $n+p$ . This is also a consequence of the fact that the values are i.i.d. Lastly, equality $(i i i)$ follows from [10] (Equation 2.1.3) where for $k\\in\\{1,2\\}$ , it is shown that ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathbf{v}_{(k)}\\leq x)=\\sum_{i=n+p-k+1}^{n+p}{\\binom{n+p}{i}}(1-F(x))^{n+p-i}F(x)^{i},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "and the proof is concluded by substitution. ", "page_idx": 12}, {"type": "text", "text": "A.2 Proof of Lemma 2 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "As a preliminary, we formally define the non-usual distributions considered in Lemma 2. ", "page_idx": 12}, {"type": "text", "text": "Truncated exponential distribution  Let $a>0$ be some parameter. Then, we define a truncated exponentialdistribution of prameter $a$ as the distribution with e.f. $\\begin{array}{r}{F:x\\mapsto{\\frac{1-e^{-a x}}{1-e^{-a}}}}\\end{array}$ Hence, $F(0)\\,=\\,0$ and $F(1)\\,=\\,1$ , and the density of this distribution is the same as the density of the exponential distribution with same parameter on the segment $[0,1]$ , up to a normalization constant. ", "page_idx": 12}, {"type": "text", "text": "Complementary Beta distribution ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Lemma 2.Let $F$ be the cumulative distribution function of a Bernoulli, truncated exponential or Complementary Beta distribution.Then,for any $p\\in\\mathbb{N}^{*}$ $r$ in Equation (3) unimodal. ", "page_idx": 12}, {"type": "text", "text": "Proof. We consider each family of distributions separately. ", "page_idx": 12}, {"type": "text", "text": "Bernoulli distributions  If $F$ is the c.d.f. of $\\,B(q)$ (a Bernoulli distribution of parameter $q$ ), then $r(n)$ is equal to the probability that exactly one player from the coalition draws a value of 1, and every other player draw a value of 0. Hence, we obtain that $r(n)=n q(1-q)^{n+p-1}$ , which is trivially unimodal and maximized in n\\* = log(1-q) , regardless of the size of the competition. ", "page_idx": 13}, {"type": "text", "text": "Truncated exponential distributions  Let $a>0$ be the parameter of the distribution. Let $Q(x)$ be the inverse function of F (the quantile function), defined by Q(x) = log (1-a(1-e-\u03b1) $\\begin{array}{r}{\\frac{1}{a}\\sum_{k=1}^{+\\infty}\\frac{x^{k}(1-e^{-a})^{k}}{k}.}\\end{array}$ ", "page_idx": 13}, {"type": "text", "text": "$q(x)$ $Q(x)$ $\\begin{array}{r}{q(x)=\\sum_{k=0}^{+\\infty}\\lambda_{k}x^{k}}\\end{array}$ where $\\textstyle\\lambda_{k}={\\frac{1}{a}}(1\\!-\\!e^{-a})^{k}$ $r(n)$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\tau(n)=n\\int_{0}^{\\infty}F(v)^{n+1}(1-F(v))d v}\\\\ {=}&{\\int_{0}^{1}s^{n+1}(1-x)(v)\\Delta x\\quad\\mathrm{out~}F(v)=x}\\\\ {=n\\int_{0}^{1}s^{n+1}(1-x)\\left(\\displaystyle\\sum_{i=0}^{n}\\lambda x^{i}\\right)\\,\\mathrm{d}x}\\\\ {=}&{\\displaystyle\\sum_{k=0}^{\\infty}\\lambda\\left(\\frac{1}{\\rho^{n}+n}-\\frac{n}{p+n+k}\\right)}\\\\ {=}&{\\displaystyle\\frac{1}{\\rho^{n}}\\Delta\\left(\\frac{1}{\\rho^{n}+n}+\\frac{1}{p^{n}+n+k+1}\\right)}\\\\ {=}&{\\displaystyle\\frac{1}{n+p}\\Delta_{i}+\\frac{1}{p^{n}+n}\\frac{1}{p+p+p+2}(\\lambda_{i}-\\lambda_{i})}\\\\ {=}&{\\displaystyle\\lambda_{i}(1-\\frac{p}{n+p})+\\frac{\\lambda\\alpha}{p+1}(1-\\frac{p+j}{n+p+p})(\\lambda_{j}-\\lambda_{i-1}}\\\\ &{-\\lambda_{i})\\left(-\\frac{p}{n+p}+\\frac{\\lambda_{j-1}-\\lambda_{i}}{\\lambda_{i}}\\frac{p+j}{n+p+j}\\right)}\\\\ {=}&{\\displaystyle\\lambda_{i}\\Big(-\\frac{p}{n+p}+\\frac{\\lambda_{j}}{p+n}\\frac{\\lambda_{j}-1}{\\lambda_{i}}\\frac{p+j}{n+p+j}\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the last inequality follows since $\\textstyle\\operatorname*{lim}_{j\\to\\infty}\\lambda_{j}=0$ . Remark that $\\theta_{j}\\geq0$ since $\\lambda_{j}$ is decreasing and $\\textstyle\\sum_{j=1}^{\\infty}\\theta_{j}=1$ ", "page_idx": 13}, {"type": "text", "text": "The derivative of $r(n)$ is given by, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{r^{\\prime}(n)=\\lambda_{0}\\Big(\\displaystyle\\frac{p}{(n+p)^{2}}-\\displaystyle\\sum_{j=1}^{+\\infty}\\theta_{j}\\displaystyle\\frac{p+j}{(n+p+j)^{2}}\\Big)}&{}&\\\\ {=\\lambda_{0}\\Big(\\Theta_{p}(n)-\\displaystyle\\sum_{j=1}^{\\infty}\\theta_{j}\\Theta_{p+j}(n)\\Big)}\\\\ &{=\\lambda_{0}\\Theta_{p}(n)\\Big(1-\\displaystyle\\sum_{j=1}^{\\infty}\\theta_{j}\\Gamma_{p,p+j}(n)\\Big)}&{\\quad\\mathrm{where~}\\Gamma_{p,p+j}(n)=\\displaystyle\\frac{\\Theta_{p+j}(n)}{\\Theta_{p}(n)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "the functions $\\Gamma_{p,p+j}(n)$ are non-decreasing hence it is the same for their convex combination. As $\\begin{array}{r}{\\mathrm{sign}(r^{\\prime}(n))=\\mathrm{sign}\\ddot{(1-\\sum_{j=1}^{\\infty}\\theta_{j}\\Gamma_{p,p+j}(n))}}\\end{array}$ it follows that $\\mathrm{sign}(r^{\\prime}(n))$ is decreasing meaning $r(n)$ is unimodal. ", "page_idx": 13}, {"type": "text", "text": "Complementary beta distributions  Using the same change of variable as in the previous proof $\\langle Y=F(X))$ , we express the reward as follows, ", "page_idx": 13}, {"type": "equation", "text": "$$\nr(n)=n\\times\\mathbb{E}_{Y\\sim Q}\\left[Y^{n+p-1}(1-Y)\\right]\\ ,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $Q$ denotes the quantile function associated with c.d.f $F$ (i.e. $F^{-1}$ ). By definition of $F$ $Y$ follows a Beta distribution of parameters $(a,b)$ . We can thus compute the expected reward by using ", "page_idx": 13}, {"type": "text", "text": "the explicit formula for moments of the Beta distribution, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\ \\ \\mathbb{E}_{X\\sim\\mathtt{B}(a,b)}[X^{n+p-1}-X^{n+p}]=\\displaystyle\\prod_{k=0}^{n+p-2}\\frac{a+k}{a+b+k}-\\displaystyle\\prod_{k=0}^{n+p-1}\\frac{a+k}{a+b+k}}&{}\\\\ {=\\displaystyle\\prod_{k=0}^{n+p-2}\\frac{a+k}{a+b+k}\\times\\left(1-\\frac{a+n+p-1}{a+b+n+p-1}\\right)}&{}\\\\ {=\\displaystyle\\prod_{k=0}^{n+p-2}\\frac{a+k}{a+b+k}\\times\\frac{b}{a+b+n+p-1}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thanks to this expression, we prove the unimodality by analyzing the ratio $\\textstyle{\\frac{r(n+1)}{r(n)}}$ thatwefirstwrite as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\frac{r(n+1)}{r(n)}=\\frac{n+1}{n}\\times\\frac{a+n+p-1}{a+b+n+p-1}\\times\\frac{a+b+n+p-1}{a+b+n+p}}}\\\\ {{\\displaystyle=\\frac{n+1}{n}\\times\\frac{a+n+p-1}{a+b+n+p}\\;,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and then obtain that this ratio is larger than 1 if and only if ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{n+1)(a+n+p-1)\\ge n(a+b+n+p)\\iff n(a+n+p)+a+p-1\\ge n(a+n+p)+b n}\\\\ &{\\iff n\\ge\\frac{a+p-1}{b}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which concludes the proof by showing the unimodality and expressing the value of the critical point. ", "page_idx": 14}, {"type": "text", "text": "The proof of Lemma 2 highlights that the unimodality assumption is satisfied as soon as the quantile function, expressed as a power series, has its coeffcients that slowly decrease (indeed, the $k$ -th coefficient just needs to be smaller than $\\textstyle1-{\\frac{1}{k}}$ timesthe $k-1$ -thone). ", "page_idx": 14}, {"type": "text", "text": "Similarly, the second proof technique highlights (up to standard algebraic manipulations) that unimodularity is guaranteed as soon as the function $n\\,\\mapsto\\,1\\,-\\,E[\\bar{X^{n+p-1}}]/E[\\bar{X^{n+p-2}}]$ is 1ogconcave. ", "page_idx": 14}, {"type": "text", "text": "A.3 Additional discussion on the unimodality of $r$ ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we plot the shape of $r(n)$ for some additional families of distribution that we conjecture to be unimodal from the plots. ", "page_idx": 14}, {"type": "text", "text": "Beta distribution  The following figure, illustrate the unimodal shape of $r(n)$ for different parameters for the Beta distribution and $p$ ", "page_idx": 14}, {"type": "image", "img_path": "B74mb0tEY6/tmp/eba9419945997d24b503dc57021b3e5f7ae44302756ad80fcc7185340a8717cf.jpg", "img_caption": ["Figure 1: Shape of $r(n)$ when $F$ is Beta "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Kumaraswamy distribution  The cumulative distribution is defined by $F(x)=1-(1-x^{a})^{b}$ for some parameters $(a,b)$ (we use the notation $K(a,b))$ . The following figure, illustrate the unimodal shape of $r(n)$ for different parameters of $K(a,b)$ and $p$ ", "page_idx": 15}, {"type": "image", "img_path": "B74mb0tEY6/tmp/291eb49f814c8d90182869e360bab6a97ea3b459127386c8593586c08f154618.jpg", "img_caption": ["Figure 2: Shape of $r(n)$ when $F$ is Kumaraswamy distribution "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "We now provide and discuss an example where Assumption 2 is not satisfied. ", "page_idx": 15}, {"type": "text", "text": "A.4 An example of distribution with non-unimodal rewards ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Let us consider a discrete distribution supported on $\\{0,0.5,1\\}$ . The counter-example emerges from putting all the probability mass in 0.5: let us consider a small $\\epsilon>0$ , identify $F$ With $\\{\\epsilon,1-\\epsilon,1\\}$ and assume that there is no competition ? $(p=0]$ ). Then, we can verify with (3) that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{r(n)=\\displaystyle\\frac{n}{2}(\\epsilon^{n-1}+(1-\\epsilon)^{n-1}-(\\epsilon^{n}+(1-\\epsilon)^{n}))}}\\\\ {{\\qquad=\\displaystyle\\frac{n}{2}(\\epsilon^{n-1}(1-\\epsilon)+\\epsilon(1-\\epsilon)^{n-1})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Consider $\\epsilon=0.15$ , we get up to a precision 0.001 the values: ", "page_idx": 15}, {"type": "equation", "text": "$$\n(r(n))_{n=1}^{7}=(0.5,0.255,0.191,0.190,0.197,0.200,0.198)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "showing that $r(n)$ is not unimodal in this case. ", "page_idx": 15}, {"type": "text", "text": "B  Concentration bounds on simple reward estimates ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "B.1 Auxiliary results ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Before presenting the proof of the theorem, we present two auxiliary results that are essential to its development. ", "page_idx": 16}, {"type": "text", "text": "B.1.1  Riemann sum approximation of the expected reward ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The first result consists in upper bounding the deviation of a Riemann sum approximation of $r(n)$ (for some $n\\in[N])$ with respect to its exact integral formulation. This result is also of practical interest, since it can prevent computing exact integrals at each step of the algorithms without altering their theoretical guarantees with an appropriate tuning of the approximation error. ", "page_idx": 16}, {"type": "text", "text": "Lemma 4 (Riemann sum approximation of $r(n))$ .Let $n~\\in~[N].$ $\\textit{D}\\in\\mathbb{N}$ and define the grid $(x_{s})_{s\\in\\{0,\\dots,D-1\\}}=\\left\\{0,{\\frac{1}{D}},\\dots,{\\frac{D-1}{D}}\\right\\}$ Then,thexpected reward approximation ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\widetilde{r}(n)=n\\times{\\frac{1}{D}}\\sum_{s=0}^{D-1}\\left\\{F(x_{s})^{n+p-1}-F(x_{s})^{n+p}\\right\\}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "satisfies ", "page_idx": 16}, {"type": "equation", "text": "$$\n|r(n)-\\widetilde{r}(n)|\\leq\\frac{n}{D}\\;.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. For any $j\\in\\mathbb N$ we consider ", "page_idx": 16}, {"type": "equation", "text": "$$\nS_{j}={\\frac{1}{D}}\\sum_{s=0}^{D-1}F^{j}(x_{s})\\quad{\\mathrm{as~an~approximation~of}}\\quad I_{j}=\\int_{0}^{1}F^{j}(x)\\mathrm{d}x\\ .\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We recall that since $F^{j}$ is a c.d.f., it is monotone, increasing and satisfies $F^{j}(0)=0$ and $F^{j}(1)=1$ This means that for any $s\\,\\in\\,\\{0,\\dots,D-1\\}$ , it holds that $\\forall x\\,\\in\\,[x_{s},x_{s+1}],F^{j}(x_{s})\\,\\leq\\,F^{j}(x)\\,\\leq$ $F^{j}(x_{s+1})$ . The linearity of the integral first provides that ", "page_idx": 16}, {"type": "equation", "text": "$$\nI_{j}=\\int_{0}^{1}F^{j}(x)\\mathrm{d}x=\\sum_{s=0}^{D-1}\\int_{\\frac{s}{D}}^{\\frac{s+1}{D}}F^{j}(x)\\mathrm{d}x\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, using this decomposition and the monotony of $F^{j}$ we obtain that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\mathrm{y}}_{j}\\leq\\displaystyle\\frac{1}{D}\\displaystyle\\sum_{s=0}^{D-1}F^{j}\\left(\\frac{s}{D}\\right)\\leq\\displaystyle\\int_{0}^{1}F^{j}(x)\\mathrm{d}x\\leq\\displaystyle\\frac{1}{D}\\sum_{s=0}^{D-1}F^{j}\\left(\\frac{s+1}{D}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{1}{D}\\sum_{s=0}^{D-1}F^{j}\\left(\\frac{s}{D}\\right)+\\displaystyle\\frac{1}{D}\\sum_{k=0}^{D-1}\\left(F^{j}\\left(\\frac{s+1}{D}\\right)-F^{j}\\left(\\frac{s}{D}\\right)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{1}{D}\\sum_{s=0}^{D-1}F^{j}\\left(\\frac{s}{D}\\right)+\\frac{F^{j}(1)-F^{j}(0)}{D}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=S_{j}+\\displaystyle\\frac{1}{D}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, we obtained that for any $j\\in\\mathbb N$ it holds that $\\begin{array}{r}{S_{j}\\le I_{j}\\le S_{j}+\\frac{1}{D}}\\end{array}$ . We conclude by using this result after splitting the reward as a difference of two integrals that can be expressed in this form, respectively with $j=n+p-1$ and $j=n+p$ \u53e3 ", "page_idx": 16}, {"type": "text", "text": "B.1.2 Chernoff bounds for Bernoulli random variables ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In the following lemma, we summarize the different concentration bounds that we use in the proof of Theorem1. ", "page_idx": 16}, {"type": "text", "text": "Lemma 5 (Mutltiplicative Chernoff bounds). Let ${\\widehat{\\mu}}_{m}$ be the empirical average of m i.i.d. Bernoulli randomvariables $X_{1},\\ldots,X_{m}$ with expectation $\\mu$ . Then, for any $\\delta>0$ . each of the following bounds holds with probability at least $1-\\delta$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\vert\\widehat{\\mu}_{m}-\\mu\\vert\\leq\\sqrt{\\mu}\\times\\sqrt{\\frac{3\\log\\left(\\frac{2}{\\delta}\\right)}{m}}\\quad i f\\mu\\in I_{0}:=\\left[\\frac{3\\log\\left(2/\\delta\\right)}{m},1\\right]\\ ,\\right.\\ }\\\\ {\\left.\\mu_{m}\\leq\\frac{6\\log\\left(2/\\delta\\right)}{m}\\qquad\\qquad\\qquad\\qquad\\quad i f\\mu\\in I_{1}:=\\left(\\frac{\\delta}{m},\\frac{3\\log\\left(2/\\delta\\right)}{m}\\right)\\ ,}\\\\ {\\mu_{m}=0\\qquad\\qquad\\qquad\\quad i f\\mu\\in I_{2}:=\\left[0,\\frac{\\delta}{m}\\right]\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. We first tackle the case $\\mu\\in I_{2}$ , where the bound is obtained by remarking that $\\mathbb{P}(\\mu_{m}>0)\\leq$ $\\mathbb{P}(\\exists i\\in[m]:X_{i}=1)\\le m\\mu\\le\\delta$ The two other cases are obtained by using the multiplicative form of the well-known Chernoff bounds [16], that provide that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\forall\\gamma>0,\\quad\\mathbb{P}(\\widehat{\\mu}_{m}\\geq(1+\\gamma)\\mu)\\leq e^{-m\\frac{\\gamma^{2}\\mu}{2+\\gamma}}\\quad,\\mathrm{~and}}\\\\ {\\forall\\gamma\\in[0,1],\\;\\mathbb{P}(\\widehat{\\mu}_{m}\\leq(1-\\gamma)\\mu)\\leq e^{-m\\frac{\\gamma^{2}\\mu}{2}}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "When considering $\\gamma\\le1$ we can further write that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}(|\\widehat{\\mu}_{m}-\\mu|\\geq\\gamma\\mu)\\leq2e^{-m\\frac{\\gamma^{2}\\mu}{3}}\\;.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "On the other hand, for $\\gamma\\geq1$ the bound for the lower deviation is trivially O while the bound for the upper deviation can be written as follows, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\gamma\\geq1\\Rightarrow\\mathbb{P}(\\widehat{\\mu}_{m}\\geq(1+\\gamma)\\mu)\\leq e^{-m\\frac{\\gamma^{2}\\mu}{2+\\gamma}}\\leq e^{-m\\frac{\\gamma^{2}\\mu}{3\\gamma}}=e^{-m\\mu\\frac{\\gamma}{3}}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Hence, the case separation between the intervals $I_{0}$ and $I_{1}$ simply consist in identifying the value $\\mu$ for which a probability $1-\\delta$ can be obtained by setting an appropriate $\\gamma\\in[0,1]$ or for $\\gamma>1$ in the above inequaltMre prisl,ein  frst bd pd $\\begin{array}{r}{\\gamma=\\mu^{\\frac{-1}{2}}\\sqrt{\\frac{3\\log\\left(\\frac{2}{\\delta}\\right)}{m}}}\\end{array}$ Which is valid only if $\\begin{array}{r}{\\mu^{\\frac{-1}{2}}\\sqrt{\\frac{3\\log\\left(\\frac{2}{\\delta}\\right)}{m}}\\leq1\\Rightarrow\\mu\\geq\\frac{3\\log\\left(\\frac{2}{\\delta}\\right)}{m}}\\end{array}$ This leads to the frst confidence interval when $\\mu\\in I_{0}$ The same procedure for $\\mu\\in I_{1}$ leads to $\\gamma=\\mu^{-1}{\\frac{3\\log\\left({\\frac{2}{\\delta}}\\right)}{m}}$ which provides he rsult st i the lemma. This concludes the proof. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "B.2Proof of Theorem 1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Theorem 1 (Concentration of simple estimates). Consider any $n\\in[N]$ and $k\\in\\mathcal{V}(n)$ . Let $\\widehat{r}_{k}(n)$ be defined according to (5) from $m_{k}$ samples collected by $k$ Then, there exists some constants $\\beta_{k,n}$ (depending on $n,k,p)$ and $\\xi_{k,n,F}$ (additionally dependingon $F$ )such that,with probability $1-\\delta$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n|\\widehat{r}_{k}(n)-r(n)|\\leq\\beta_{k,n}\\sqrt{\\frac{\\log\\left(\\frac{2\\lceil n\\sqrt{m_{k}}\\rceil}{\\delta}\\right)}{m_{k}}}+n\\times\\xi_{k,n,F}\\left(\\frac{\\log\\left(\\frac{2\\lceil n\\sqrt{m_{k}}\\rceil}{\\delta}\\right)}{m_{k}}\\right)^{\\frac{n+p-1}{k+p}}~.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Furthermore, the constants admit universal upper bounds for any $n,k,p,F$ Forinstanceif $m_{k}\\geq4$ itholds that $\\beta_{k,n}\\leq33$ and $\\gamma_{k,n,F}\\leq100$ ", "page_idx": 17}, {"type": "text", "text": "Proof. We build the proof from the Riemann sum approximation of the reward presented in Lemma 4 and the Chernoff bounds presented in Lemma 5. Defining some parameter $D\\in\\mathbb{N}$ that will be fixed later, we use the first result to consider the following approximation of the empirical reward estimate $\\widehat{r}_{k}(n)$ by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\widetilde r_{k}(n)={\\frac{1}{D}}\\sum_{s=0}^{D-1}\\left(\\widehat{F}_{k+p,m_{k}}\\left({\\frac{s}{D}}\\right)^{\\frac{n+p-1}{k+p}}-\\widehat{F}_{k+p,m_{k}}\\left({\\frac{s}{D}}\\right)^{\\frac{n+p}{k+p}}\\right)\\;.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thanks to Lemma 4, we know that $\\begin{array}{r}{\\widehat{r}_{k}(n)\\,\\in\\,\\left[\\widetilde{r}_{k}(n)-\\frac{n}{D},\\widetilde{r}_{k}(n)+\\frac{n}{D}\\right]}\\end{array}$ . For the rest of proof, we introduce the notation $\\begin{array}{r}{F_{s}^{j}\\;=\\;F\\left(\\frac{s}{D}\\right)^{j}}\\end{array}$ for any $j\\;\\in\\;[N],\\;\\widehat{F}_{s}^{k+p}\\;=\\;\\widehat{F}_{k+p,m_{k}}\\left(\\frac{s}{D}\\right)$ , and $\\widehat{F}_{s,k,n}\\;=\\;$ ", "page_idx": 17}, {"type": "text", "text": "$\\begin{array}{r}{\\widehat{F}_{k+p,m_{k}}\\left(\\frac{s}{D}\\right)^{\\frac{n+p}{k+p}}}\\end{array}$ ,sothat ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\widetilde{r}_{k}(n):=\\frac{1}{D}\\sum_{s=0}^{D-1}\\left(\\widehat{F}_{s,k,n-1}-\\widehat{F}_{s,k,n}\\right)\\;,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "that we want to relate with ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\widetilde r(n):={\\frac{1}{D}}\\sum_{s=0}^{D-1}\\left(F_{s}^{n+p-1}-F_{s}^{n+p}\\right)\\;.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We use that each variable $\\widehat{F}_{s,k,n}$ can be expressed as the expectation of $m_{k}$ i.i.d. Bernoullirandom variables of expetation $F_{s}^{k+p}$ $\\begin{array}{r}{\\widehat{F}_{k+p,m_{k}}\\,=\\,\\frac{1}{m_{k}}\\sum_{j=1}^{m_{k}}\\mathbb{1}\\{X_{k,j}\\le x\\}}\\end{array}$ Hene, we can usethe confidence intervals providing by Lemma 5, according to the value of $F_{s}^{k+p}$ . We define two critical values, corresponding to the switch between the different intervals $I_{0},I_{1},I_{2}$ in the lemma, and their closest upper point in the discretization grid. The first is ", "page_idx": 18}, {"type": "equation", "text": "$$\nx_{0,k}=F^{-1}\\left(\\left(\\frac{\\delta}{m_{k}}\\right)^{\\frac{1}{k+p}}\\right),\\quad\\mathrm{and}\\;s_{0,k}=\\lceil D x_{0,k}\\rceil\\;,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and we recall that below $x_{0,k}$ it holds that $\\widehat{F}_{k,i,n-1}=0$ with probability larger than $1-\\delta$ . Then, we define ", "page_idx": 18}, {"type": "equation", "text": "$$\nx_{1,k}:=F^{-1}\\left(1\\wedge\\left(4\\frac{\\log\\left(\\frac{2}{\\delta}\\right)}{m_{k}}\\right)^{\\frac{1}{k+p}}\\right),\\,\\,\\,\\,\\mathrm{and}\\,\\,s_{1,k}:=\\lceil D x_{1,k}\\rceil\\ .\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We remark that we use a multiplicative factor 4 inside of $F^{-1}$ , while Lemma 5 might suggest to use 3. We do that for technical reasons, that we will motivate at one stage of the proof. These terms depend both on the sample size $m_{k}$ and the confidence level $\\delta$ , but we omit them in the notation for simplicity. Then, for fixed values of these constants we decompose the estimator between the intervals $I_{0}\\overset{.}{=}\\{s_{1,k},\\ldots,D-1\\}$ \uff0c $I_{1}=\\{s_{0,k},\\ldots,s_{1,k}-1\\}$ , and $I_{2}^{\\bar{\\mathbf{\\alpha}}}=\\{0,\\ldots,s_{0,k}-1\\}$ . Note that for the second interval to be non-empty it must hold that $s_{0,k}\\,\\leq\\,s_{1,k}\\,-\\,1$ , that we assume in the following, otherwise we can just remove this interval from the analysis. ", "page_idx": 18}, {"type": "text", "text": "For $s\\geq s_{1,k}$ , Lemma 5 guarantees that with probability larger than $1-\\delta$ it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\widehat{F}_{s}^{k+p}\\in\\left[(1-\\gamma_{s})F_{s}^{k+p},(1+\\gamma_{s})F_{s}^{k+p}\\right]\\quad\\mathrm{for}\\;\\gamma_{s}=F_{s}^{-\\frac{k+p}{2}}\\sqrt{3\\frac{\\log\\left(\\frac{2}{\\delta}\\right)}{m_{k}}}\\;,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "while for $k\\leq s_{1,k}-1$ we can use one of the two other bounds provided in the lemma. Using a union bound, all the confidence intervals hold simultaneously for the points in the sum and in $x_{1,k}$ With probability larger than $1-(D+1)\\delta$ , which defines a \u201cgood\" event ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\mathcal{G}=\\left\\{\\forall k\\in I_{2},\\widehat{F}_{s}^{k+p}=0,\\;\\;\\forall k\\in I_{1},\\;\\widehat{F}_{s}^{k+p}\\leq8\\frac{\\log\\big(\\frac{2}{\\delta}\\big)}{m_{k}},\\right.}}\\\\ {{\\forall k\\in I_{0},\\widehat{F}_{s}^{k+p}\\in\\left[(1-\\gamma_{s})F_{s}^{k+p},(1+\\gamma_{s})F_{s}^{k+p}\\right]\\left.\\right\\}\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For the rest of the analysis, we assume that $\\mathcal{G}$ holds. In particular, in that context there exists $D-s_{1,k}$ constants $(z_{s})_{s\\in\\{s_{1,k},\\ldots,D-1\\}}$ such that $\\forall s\\in I_{0},\\widehat{F}_{s}^{k+p}=(1+z_{s})F_{s}^{k+p}$ and $z_{s}\\in[-\\gamma_{s},\\gamma_{s}]$ .We now upper and lower bound $\\widehat{r}_{k}(n)$ using these constants, first writing that under $\\mathcal{G}$ it holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathit{\\check{r}}_{k}(n)=\\frac{1}{D}\\sum_{s=0}^{D-1}\\left(\\widehat{F}_{s,k,n-1}-\\widehat{F}_{s,k,n}\\right)}\\ ~}\\\\ {{\\displaystyle~~~=\\frac{1}{D}\\sum_{s=s_{1,k}}^{D-1}\\left(\\widehat{F}_{s,k,n-1}-\\widehat{F}_{s,k,n}\\right)+\\frac{1}{D}\\sum_{s=0}^{s_{1,k}-1}\\left(\\widehat{F}_{s,k,n-1}-\\widehat{F}_{s,k,n}\\right)}\\ ~}\\\\ {{\\displaystyle~~~=\\frac{1}{D}\\sum_{s=s_{1,k}}^{D-1}\\left((1+z_{s})^{\\frac{n+p-1}{k+p}}F_{s}^{n+p-1}-(1+z_{s})^{\\frac{n+p}{k+p}}F_{s}^{n+p}\\right)+\\frac{1}{D}\\sum_{s=s_{0,k}}^{s_{1,k}-1}\\left(\\widehat{F}_{s,k,n-1}-\\widehat{F}_{s,k,n}\\right)~,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where we used that all the terms are zero for indices smaller than $s_{0,k}$ . We can thus express $\\widehat{r}_{k}(n)$ as follows, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\widehat{r}_{k}(n)=\\widetilde{r}(n)+n\\mathcal{E}_{0}+n\\mathcal{E}_{1}\\ ,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\mathcal{E}_{0}:=\\displaystyle\\frac{1}{D}\\sum_{s=s_{1,k}}^{D-1}\\big((1+z_{s})^{\\frac{n+p-1}{k+p}}-1\\big)F_{s}^{n+p-1}-\\frac{1}{D}\\sum_{s=s_{1,k}}^{D-1}\\big((1+z_{s})^{\\frac{n+p}{k+p}}-1\\big)F_{s}^{n+p}\\ ,}}&{{\\mathrm{and}}}\\\\ {{\\mathcal{E}_{1}:=\\displaystyle\\frac{1}{D}\\sum_{s=s_{0,k}}^{s_{1,k}-1}\\bigg(\\widehat{F}_{s,k,n-1}-\\widehat{F}_{s,k,n}\\bigg)-\\displaystyle\\sum_{s=0}^{s_{1,k}-1}\\big(F_{s}^{n+p-1}-F_{s}^{n+p}\\big)\\ ,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "so we can upper and lower bound $\\widehat{r_{i}}(n)$ by upper and lower bounding $\\mathcal{E}_{0}$ and ${\\mathcal{E}}_{1}$ separately. ", "page_idx": 19}, {"type": "text", "text": "Bounding the individual terms of $\\mathcal{E}_{0}$ For any $k\\geq s_{1,k}$ we consider the term ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{0,s}:=((1+z_{s})^{\\frac{n+p-1}{k+p}}-1)F_{s}^{n+p-1}-((1+z_{s})^{\\frac{n+p}{k+p}}-1)F_{s}^{n+p}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We first re-arrange it in a more convenient way, remarking that ", "page_idx": 19}, {"type": "equation", "text": "$$\nF_{s}^{n+p-1}=F_{s}^{n+p-1}(F_{s}+(1-F_{s}))=F_{s}^{n+p}+F_{s}^{n+p-1}(1-F_{s}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Using this result, we obtain that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}_{0,s}:=((1+z_{s})^{\\frac{n+p-1}{k+p}}-1)F_{s}^{n+p-1}-((1+z_{s})^{\\frac{n+p}{k+p}}-1)F_{s}^{n+p}}\\\\ &{\\quad=((1+z_{s})^{\\frac{n+p-1}{k+p}}-1)F_{s}^{n+p-1}(F_{s}+(1-F_{s}))-((1+z_{s})^{\\frac{n+p}{k+p}}-1)F_{s}^{n+p}}\\\\ &{\\quad=F_{s}^{n+p}((1+z_{s})^{\\frac{n+p-1}{k+p}}-1)-(1+z_{s})^{\\frac{n+p}{k+p}}+1)+F_{s}^{n+p-1}(1-F_{s})((1+z_{s})^{\\frac{n+p-1}{k+p}}-1)\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which simplifies to ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{E}_{0,s}=\\underbrace{F_{s}^{n+p}((1+z_{s})^{\\frac{n+p-1}{k+p}}-(1+z_{s})^{\\frac{n+p}{k+p}})}_{\\mathcal{E}_{0,s}^{-}}+\\underbrace{F_{s}^{n+p-1}(1-F_{s})((1+z_{s})^{\\frac{n+p-1}{k+p}}-1)}_{\\mathcal{E}_{0,s}^{+}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We remark that these two terms have opposite sign, $\\mathcal{E}_{0,s}^{+}$ having the same sign as $z_{s}$ . We first upper bound $\\mathcal{E}_{0,s}$ , starting with the case $\\ z_{s}>0$ , for which it holds that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{z_{s}\\geq0\\Rightarrow\\mathcal{E}_{0,s}\\leq\\mathcal{E}_{0,s}^{+}\\leq\\underbrace{\\left(\\left(1+\\gamma_{s}\\right)^{\\frac{n+p-1}{k+p}}-1\\right)}_{c_{s}}F_{s}^{n+p-1}(1-F_{s})\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The constant $c_{k}$ is explicit from the definition of $\\gamma_{s}$ , and the bound holds for any $i\\in[N+p]$ without restriction. However, if we only consider the case $\\begin{array}{r}{\\frac{n+p-1}{k+p}\\leq2}\\end{array}$ then we can further write that ", "page_idx": 19}, {"type": "equation", "text": "$$\nc_{s}\\leq(1+\\gamma_{s})^{2}-1\\leq2\\gamma_{s}+\\gamma_{s}^{2}\\leq3\\gamma_{s}\\ ,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "since $\\gamma_{s}\\leq1$ for the values of $s$ considered. Then, for $z_{s}\\leq0$ we use that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{z_{s}\\leq0\\Rightarrow\\mathcal{E}_{0,s}\\leq\\mathcal{E}_{0,s}^{-}\\leq F_{s}^{n+p}((1+z_{s})^{\\frac{n+p-1}{k+p}}-(1+z_{s})^{\\frac{n+p}{k+p}})}\\\\ {=F_{s}^{n+p}(1+z_{s})^{\\frac{n+p-1}{k+p}}\\left(1-(1+z_{s})^{\\frac{1}{k+p}}\\right)\\;.\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Using the notation $y_{s}=-z_{s}$ for convenience, we upper bound the last multiplicative term as follows. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(1-y_{s})^{\\frac{1}{k+p}}=e^{\\frac{\\log(1-y_{s})}{k+p}}\\geq1+\\displaystyle\\frac{\\log(1-y_{s})}{k+p}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=1-\\displaystyle\\frac{1}{k+p}\\log\\left(1+\\displaystyle\\frac{y_{s}}{1-y_{s}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\geq1-\\displaystyle\\frac{1}{k+p}\\times\\displaystyle\\frac{y_{s}}{1-y_{s}}\\:,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which leads to ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{y_{s}:=-z_{s}\\geq0\\Rightarrow\\mathcal{E}_{0,s}\\leq\\mathcal{E}_{0,s}^{-}\\leq F_{s}^{n+p}(1-y_{s})^{\\frac{n+p-1}{k+p}}\\frac{y_{s}}{(k+p)(1-y_{s})}}}\\\\ {{=F_{s}^{n+p}(1-y_{s})^{\\frac{n+p-1}{k+p}-1}\\frac{y_{s}}{k+p}\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Wenow remark that when+p\u2264n+p-1 then the bound simplybecmes\u2264F+p\u00d7 However, when $k{+}p>n{+}p{-}1$ the upper bound is diverging when $y_{s}$ gets close to 1. Since the upper bound is increasing in $\\gamma_{s}$ , and using that $n+p-1\\geq\\frac{2}{3}(\\stackrel{\\smile}{k}+\\stackrel{\\qgtr}{p})$ we obtain that $\\begin{array}{r}{\\mathcal{E}_{0,s}^{-}\\leq F_{s}^{n+p}\\frac{\\gamma_{s}^{-}}{i(1-\\gamma_{s})^{\\frac{1}{3}}}}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "This is the motivation for calibrating the threshold $^{S1,k}$ so that $k\\geq s_{1,k}\\Rightarrow(1-\\gamma_{s})^{\\frac{1}{3}}\\geq\\frac{1}{2}$ , which is done by tuning $s_{1,k}$ so that $\\gamma_{s_{1,k}}\\leq\\frac{7}{8}$ (hence the multiplicative 4 inside of $F^{-1}$ ", "page_idx": 20}, {"type": "text", "text": "We thus obtain that ", "page_idx": 20}, {"type": "equation", "text": "$$\nz_{s}\\le0,k\\ge s_{1,k}\\Rightarrow\\mathcal{E}_{0,s}\\le2F_{s}^{n+p}\\frac{\\gamma_{s}}{k+p}\\;,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which finally leads to ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{E}_{0,s}\\leq\\underbrace{3\\gamma_{s}F_{s}^{n+p-1}(1-F_{s})}_{\\mathrm{if}\\;z_{s}\\geq0}\\vee2\\underbrace{F_{s}^{n+p}\\frac{\\gamma_{s}}{k+p}}_{\\mathrm{if}\\;z_{s}\\leq0}\\;.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We now proceed to lower bound $\\mathcal{E}_{0,s}$ , using again Equation(10). The proof is similar to the proof of the upper bound, for the case $z_{s}\\geq0$ we can write that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{z_{s}\\geq0\\Rightarrow-\\mathcal{E}_{0,s}\\leq-\\mathcal{E}_{0,s}^{-}\\leq F_{s}^{n+p}(1+z_{s})^{\\frac{n+p-1}{k+p}}((1+z_{s})^{\\frac{1}{k+p}}-1)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq F^{n+p}(1+\\gamma_{s})^{\\frac{n+p-1}{k+p}}\\frac{\\gamma_{s}}{k+p}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq2^{\\frac{n+p-1}{k+p}}F^{n+p}\\frac{\\gamma_{s}}{k+p}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "using the concavity of $x\\mapsto(1+x)^{\\frac{1}{k+p}}$ and that $\\gamma_{s}\\leq1$ . Then, for the case $z_{s}\\leq0$ we use that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\begin{array}{c c}{z_{s}\\leq0\\Rightarrow-\\mathcal{E}_{0,s}\\leq-\\mathcal{E}_{0,s}^{+}\\leq F_{s}^{n+p-1}(1-F_{s})\\left(1-(1+z_{s})^{\\frac{n+p-1}{k+p}}\\right)}\\\\ {\\leq F_{s}^{n+p-1}(1-F_{s})\\left(1-(1-\\gamma_{s})^{\\frac{n+p-1}{k+p}}\\right).}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "If $\\begin{array}{r}{\\frac{n+p-1}{k+p}\\leq2}\\end{array}$ we furthermore obtain that $\\begin{array}{r}{(1-\\gamma_{s})^{\\frac{n+p-1}{k+p}}\\geq(1-\\gamma_{s})^{2}\\geq1-2\\gamma_{s},}\\end{array}$ ", "page_idx": 20}, {"type": "equation", "text": "$$\nz_{s}\\leq0\\Rightarrow-\\mathcal{E}_{0,s}^{+}\\leq2\\gamma_{s}F_{s}^{n+p-1}(1-F_{s})\\;.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Combining these results, we can lower bound $\\mathcal{E}_{0,s}$ as follows, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{E}_{0,s}\\geq-\\left\\{\\frac{2^{\\frac{n+p-1}{k+p}}\\gamma_{s}}{k+p}F_{s}^{n+p}\\;\\vee\\;2\\gamma_{s}F_{s}^{n+p-1}(1-F_{s})\\right\\}\\;,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the terms involved in this lower bound are analogous to the terms used in the upper bound up to some multiplicative constants. ", "page_idx": 20}, {"type": "text", "text": "Summary: bounds on $\\mathcal{E}_{0}$ We start with the lower bound. Using Equation (12), we obtain that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{n\\mathcal{E}_{0}\\geq-\\displaystyle\\frac{n}{D}\\sum_{s=s_{1,k}}^{D-1}\\left\\{\\frac{2^{\\frac{n+p-1}{k+p}}\\gamma_{s}}{k+p}F_{s}^{n+p}\\vee2\\gamma_{s}F_{s}^{n+p-1}(1-F_{s})\\right\\}}}\\\\ {{\\qquad\\geq-\\sqrt{\\frac{3\\log\\binom{2}{\\delta}}{m_{k}}}\\times\\left\\{\\frac{n}{D}\\sum_{s=s_{1,k}}^{D-1}2^{\\frac{n+p-1}{k+p}}\\frac{F_{s}^{n+p-\\frac{k+p}{2}}}{k+p}\\ +\\ \\frac{n}{D}\\sum_{s=s_{1,k}}^{D-1}2F_{s}^{n+p-1-\\frac{k+p}{2}}(1-F_{s})\\right\\}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "The first sum can be trivially upper bounded by $2^{\\frac{n+p-1}{k+p}}\\frac{n}{k+p}$   \nrestrictive assumptions on $F$ . For the second term, we use that $k+p\\leq\\,{\\frac{3}{2}}(n+p)$ to exhibit the $\\begin{array}{r}{n^{\\prime}:=n-\\frac{n(k+p)}{2(n+p)}\\geq\\bar{\\frac{n}{4}}}\\end{array}$ and competion of $\\begin{array}{r}{p^{\\prime}:=p-\\frac{p(k+p)}{2(n+p)}\\geq\\frac{p}{4}}\\end{array}$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\frac{n}{D}\\sum_{s=s_{1,k}}^{D-1}F_{s}^{n+p-1-\\frac{k+p}{2}}(1-F_{s})=n\\times\\displaystyle\\frac{1}{D}\\sum_{s=s_{1,k}}^{D-1}F_{s}^{n^{\\prime}+p^{\\prime}-1}(1-F_{s})}\\\\ &{\\phantom{m m m m m m m m m m}\\leq n\\times\\displaystyle\\int_{0}^{1}F(x)^{\\frac{n+p-1}{4}}(1-F(x))\\mathrm{d}x+\\frac{n}{D}}\\\\ &{\\displaystyle=\\frac{n}{n^{\\prime}}\\times n^{\\prime}\\int_{0}^{1}F(x)^{n^{\\prime}+p^{\\prime}-1}(1-F(x))\\mathrm{d}x+\\frac{n}{D}}\\\\ &{\\displaystyle\\leq\\frac{n}{n^{\\prime}+p^{\\prime}-1}+\\frac{n}{D}=\\frac{n}{n+p-\\frac{k+p}{2}}+\\frac{n}{D}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we used that the reward is smaller than the probability that the coalition wins the auction, which is easily generalized even if $i/2$ is not integer. ", "page_idx": 21}, {"type": "text", "text": "We thus conclude the proof of the lower bound by writing that ", "page_idx": 21}, {"type": "equation", "text": "$$\nn\\mathcal{E}_{0}\\ge-4\\left\\{\\left(\\frac{n}{2(n+p)-(k+p)}+\\frac{n}{2D}\\right)\\;+\\;2^{\\frac{n+p-1}{k+p}-2}\\times\\frac{n}{k+p}\\right\\}\\times\\sqrt{\\frac{3\\log\\left(\\frac{2}{\\delta}\\right)}{m_{k}}}\\;,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Wwhere the wost caing forthe lethand term in the maximum is atined in $\\textstyle k+p=3{\\frac{n+p}{2}}$ and provide $2{\\frac{n}{n+p}}$ , while for the right-hand term it is achieved in $\\begin{array}{r}{k+p=\\frac{n+p-1}{2}}\\end{array}$ and provides $2{\\frac{n}{n+p-1}}$ As we already discussed, the upper bound can be expressed very similarly, remarking that the bound involving the terms $\\gamma_{s}/i$ have to be divided by two, and the other term have to be multiplied by $3/2$ We hence directly obtain that ", "page_idx": 21}, {"type": "equation", "text": "$$\nn\\mathcal{E}_{0}\\le6\\left\\{\\left(\\frac{n}{2(n+p)-(k+p)}+\\frac{n}{2D}\\right)\\;+\\;\\frac{n}{3(k+p)}\\right\\}\\times\\sqrt{\\frac{3\\log\\left(\\frac{2}{\\delta}\\right)}{m_{k}}}\\;.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Bounds on ${\\mathcal{E}}_{1}$ We start by upper bounding the second sum by 0. Under $\\mathcal{G}$ we hence obtain that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{n\\mathcal{E}_{1}\\leq\\frac{n}{D}\\sum_{k\\in I_{1}}(\\widehat{F}_{s}^{k+p})^{\\frac{n+p-1}{k+p}}}}\\\\ &{\\leq\\frac{n}{D}\\sum_{k\\in I_{1}}\\left(8\\frac{\\log(2/\\delta)}{m_{k}}\\right)^{\\frac{n+p-1}{k+p}}}\\\\ &{\\leq n\\left(x_{1,k}-x_{0,k}+\\frac{1}{D}\\right)\\left(8\\frac{\\log(2/\\delta)}{m_{k}}\\right)^{\\frac{n+p-1}{k+p}}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which has a worst possible power of $2/3$ when $m_{k}\\ \\geq\\ 8\\log(2/\\delta)$ , corresponding to $k\\,+\\,p\\,=$ ${\\begin{array}{l}{{\\frac{3}{2}}(n+p-1)}\\end{array}}$ . Replacing $x_{1,k}$ by its expression, we further obtain that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\iota\\mathcal{E}_{1}\\leq n\\left(8\\frac{\\log(2/\\delta)}{m_{k}}\\right)^{\\frac{n+p-1}{k+p}}\\times\\left\\{F^{-1}\\left(1\\wedge\\left(\\frac{4\\log\\left(\\frac{2}{\\delta}\\right)}{m_{k}}\\right)^{\\frac{1}{k+p}}\\right)-F^{-1}\\left(\\left(\\frac{\\delta}{m_{k}}\\right)^{\\frac{1}{k+p}}\\right)+\\frac{1}{D}\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For the lower bound on $n\\mathcal{E}_{1}^{1}$ , we apply the exact same steps, remarking that the constant 8 at the very first step can be replaced by 4, since we now use the exact value of $F^{k+p}$ in the upper bound. Furthermore, we have to remove the term $x_{0,k}$ . We finally obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\nn\\mathcal{E}_{1}\\geq-n\\left(4\\frac{\\log(2/\\delta)}{m_{k}}\\right)^{\\frac{n+p-1}{k+p}}\\times\\left\\{F^{-1}\\left(1\\wedge\\left(\\frac{4\\log\\left(\\frac{2}{\\delta}\\right)}{m_{k}}\\right)^{\\frac{1}{k+p}}\\right)+\\frac{1}{D}\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Summary:bounds on $\\widehat{r}_{k}(n)$ We conclude this proof by summarizing the results, and exhibiting the constants introduced in the theorem. First, by combining (13) and (16) we obtain the following lowerbound, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\widehat{r}_{k}(n)\\geq r(n)-\\frac{n}{D}-n\\left(4\\frac{\\log(2/\\delta)}{m_{k}}\\right)^{\\frac{n+p-1}{k+p}}\\times\\left\\{F^{-1}\\left(1\\wedge\\left(\\frac{4\\log\\left(\\frac{2}{\\delta}\\right)}{m_{k}}\\right)^{\\frac{1}{k+p}}\\right)+\\frac{1}{D}\\right\\}}\\\\ &{}&{-\\,4\\left\\{\\left(\\frac{n}{2(n+p)-(k+p)}+\\frac{n}{2D}\\right)\\,+\\,2^{\\frac{n+p-1}{k+p}-2}\\times\\frac{n}{k+p}\\right\\}\\times\\sqrt{\\frac{3\\log\\left(\\frac{2}{\\delta}\\right)}{m_{k}}}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then, by combining (14) and (15) we obtain the following upper bound, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\hat{\\kappa}(n)\\leq r(n)+\\frac{n}{D}+6\\left\\{\\left(\\frac{n}{2(n+p)-(k+p)}+\\frac{n}{2D}\\right)\\,+\\,\\frac{n}{3(k+p)}\\right\\}\\times\\sqrt{\\frac{3\\log\\left(\\frac{2}{\\delta}\\right)}{m_{k}}}}\\\\ {\\displaystyle\\qquad+\\,n\\left(8\\frac{\\log(2/\\delta)}{m_{k}}\\right)^{\\frac{n+p-1}{k+p}}\\times\\left\\{F^{-1}\\left(1\\wedge\\left(\\frac{4\\log\\left(\\frac{2}{\\delta}\\right)}{m_{k}}\\right)^{\\frac{1}{k+p}}\\right)-F^{-1}\\left(\\left(\\frac{\\delta}{m_{k}}\\right)^{\\frac{1}{k+p}}\\right)+\\frac{1}{D}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "As a final step, we recall that in this proof $1-\\delta$ is the confidence level of the point estimate in each of the $D$ points $(x_{s})_{s\\in\\{0,\\ldots,D-1\\}}$ and $x_{1,k}$ . Hence, to obtain a confidence $1-\\delta$ on the full estimate $\\widehat{r}_{k}(n)$ we need to multiply $\\delta$ by $(D+1)$ in the bounds presented above. As a final step, we choose D +1 = [n\u221am], so that the term  \u2264mk becomes a second order term in the bounds. ", "page_idx": 22}, {"type": "text", "text": "After replacing $\\delta$ and $D$ by the appropriate values, we hence obtain that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widehat{r}_{k}(n)\\ge\\ r(n)-\\frac{1}{\\sqrt{m_{k}}}-\\beta_{k,n}^{-}\\sqrt{\\frac{\\log\\left(\\frac{2\\lceil n\\sqrt{m_{k}}\\rceil}{\\delta}\\right)}{m_{k}}}-n\\times\\xi_{k,n,F}^{-}\\left(\\frac{\\log\\left(\\frac{2\\lceil n\\sqrt{m_{k}}\\rceil}{\\delta}\\right)}{m_{k}}\\right)^{\\frac{n+p-1}{k+p}}\\ .\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "with ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\beta_{k,n}^{-}=4\\sqrt{3}\\left\\{\\left(\\frac{n}{2\\left(n+p\\right)-\\left(k+p\\right)}+\\frac{n}{2\\left(\\lceil n\\sqrt{m_{k}}\\rceil-1\\right)}\\right)\\ +\\ 2^{\\frac{n+p-1}{k+p}-2}\\times\\frac{n}{k+p}\\right\\}}}\\\\ {{\\xi_{k,n,F}^{-}=4^{\\frac{n+p-1}{k+p}}\\left\\{F^{-1}\\left(1\\wedge\\left(\\frac{4\\log\\left(\\frac{2}{\\delta}\\right)}{m_{k}}\\right)^{\\frac{1}{k+p}}\\right)+\\frac{1}{\\lceil n\\sqrt{m_{k}}\\rceil-1}\\right\\}\\ .}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Symmetrically, we obtain that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\widehat{r}_{k}(n)\\leq\\ r(n)+\\frac{1}{\\sqrt{m_{k}}}+\\beta_{k,n}^{+}\\sqrt{\\frac{\\log\\left(\\frac{2\\lceil n\\sqrt{m_{k}}\\rceil}{\\delta}\\right)}{m_{k}}}+n\\times\\xi_{k,n,F}^{+}\\left(\\frac{\\log\\left(\\frac{2\\lceil n\\sqrt{m_{k}}\\rceil}{\\delta}\\right)}{m_{k}}\\right)^{\\frac{n+p-1}{k+p}}\\ .\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "with ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\beta_{k,n}^{+}=6\\sqrt{3}\\left\\{\\left(\\frac{n}{2(n+p)-(k+p)}+\\frac{n}{2(\\lceil n\\sqrt{m_{k}}\\rceil-1)}\\right)\\,+\\,\\frac{n}{3(k+p)}\\right\\}\\,,\\;\\mathrm{and}}}\\\\ {{\\displaystyle{\\xi_{k,n,F}^{+}=8^{\\frac{n+p-1}{k+p}}\\left\\{F^{-1}\\left(1\\wedge\\left(\\frac{4\\log\\left(\\frac{2}{\\delta}\\right)}{m_{k}}\\right)^{\\frac{1+p}{k+p}}\\right)-F^{-1}\\left(\\left(\\frac{\\delta}{m_{k}}\\right)^{\\frac{1+p}{k+p}}\\right)+\\frac{1}{\\lceil n\\sqrt{m_{k}}\\rceil-1}\\right\\}\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Hence, we obtain the statemen of (7) by choosing $\\beta_{k,n}=\\beta_{k,n}^{-}\\vee\\beta_{k,n}^{-}$ and $\\xi_{k,n,F}=\\xi_{k,n,F}^{+}\\vee\\xi_{k,n,F}^{-}$ Furthermore, it is clear from their expression and the constraint k + p E [P 3 32 that these two constants are bounded by by absolute constants. Their expression provided in the theorem comes from choosing the worst admissible value of $k$ for each of their components. This concludes the proof of the theorem. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Remark 2 (Improved constants for practical implementations). We can further improve the constants of the bounds according to the position of i with respect to $n+p-1$ ", "page_idx": 23}, {"type": "text", "text": "\u00b7 In Equation (11) (upper bound): the constant 3 can be improved to 1 if $i\\geq n+p-1$ while the constant 2can beimproved to $\\begin{array}{r}{\\frac{n+p-1}{k+p}\\;i f i\\leq n+p-\\bar{1}}\\end{array}$   \n\u00b7 In Equation (12) (lower bound): the constant 2 on the right-hand side can be improved to 1 $n+p-1\\leq i$ ", "page_idx": 23}, {"type": "text", "text": "These improved constants translate easily to the upper and lower bounds presented in (13) and (14). ", "page_idx": 23}, {"type": "text", "text": "B.3Proof of Lemma 3 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this part, we prove the tighter confidence bounds, assuming that the quantile function of the value distribution is Lipschitz. ", "page_idx": 23}, {"type": "text", "text": "Lemma 3 (Improved bound for Lipschitz quantile function). Assume that $k\\in\\mathcal{V}(n)$ and $F^{-1}$ is $L$ -Lipschitz, then there exists an absolute constant $\\xi$ such that with probability $1-\\delta$ itholdsthat ", "page_idx": 23}, {"type": "equation", "text": "$$\n|\\widehat{r}_{k}(n)-r(n)|\\leq\\beta_{k,n}\\sqrt{\\frac{\\log\\left(\\frac{2\\lceil n\\sqrt{m_{k}}\\rceil}{\\delta}\\right)}{m_{k}}}+\\xi L\\log\\left(\\frac{4\\lceil n\\sqrt{m_{k}}\\rceil}{\\delta}\\right)\\left(\\frac{\\log\\left(\\frac{2\\lceil n\\sqrt{m_{k}}\\rceil}{\\delta}\\right)}{m_{k}}\\right)^{\\frac{n+p-1}{k+p}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. As the result indicates, the improvement comes from providing finer upper and lower bound On the term $n\\mathcal{E}_{1}$ in the proof of Theorem 1. We can start the refined analysis from Equations (16) and (15). ", "page_idx": 23}, {"type": "text", "text": "Let us consider first the upper bound. In that case, the main ingredient comes from refining the upper bound of the difference $x_{1,k}-x_{0,k}$ . To do that, we first provide an upper bound on $1\\wedge$ $\\begin{array}{r}{\\left(\\frac{4\\log\\left(\\frac{2}{\\delta}\\right)}{m_{k}}\\right)^{\\frac{1}{k+p}}-\\left(\\frac{\\delta}{m_{k}}\\right)^{\\frac{1}{k+p}}}\\end{array}$ . We recall that, as in the previous proof, this $\\delta$ should be multiplied by $\\lceil n\\sqrt{m_{k}}\\rceil$ at the end of the computation. We omit this term for now for simplicity. We consider a first case where the first term is equal to 1, which leads to ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{1-\\left(\\frac{\\delta}{m_{k}}\\right)^{\\frac{1}{k+p}}=1-e^{-\\frac{1}{k+p}\\log\\left(\\frac{m_{k}}{\\delta}\\right)}}\\quad}&{}\\\\ &{\\leq\\frac{1}{k+p}\\log\\left(\\frac{m_{k}}{\\delta}\\right)}\\\\ &{\\leq\\frac{1}{k+p}\\log\\left(\\frac{4\\log\\left(2/\\delta\\right)}{\\delta}\\right)}\\\\ &{=\\frac{1}{k+p}\\times\\left\\{\\log\\left(\\frac{4}{\\delta}\\right)+\\log\\log\\left(\\frac{2}{\\delta}\\right)\\right\\}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For the alternative case we obtain a similar result ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\frac{4\\log\\left(\\frac{2}{\\delta}\\right)}{m_{k}}\\right)^{\\frac{1}{k+p}}-\\left(\\frac{\\delta}{m_{k}}\\right)^{\\frac{1}{k+p}}=\\left(\\frac{4\\log\\left(\\frac{2}{\\delta}\\right)}{m_{k}}\\right)^{\\frac{1}{k+p}}\\left(1-\\left(\\frac{\\delta}{4\\log\\left(2/\\delta\\right)}\\right)^{\\frac{1}{k+p}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\left(\\frac{4\\log\\left(\\frac{2}{\\delta}\\right)}{m_{k}}\\right)^{\\frac{1}{k+p}}\\times\\frac{1}{k+p}\\times\\left\\{\\log\\left(\\frac{4}{\\delta}\\right)+\\log\\log\\left(\\frac{2}{\\delta}\\right)\\right\\}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "so the two upper bounds simplify to ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\{1\\vee\\left({\\frac{4\\log\\left({\\frac{2}{\\delta}}\\right)}{m_{k}}}\\right)^{\\frac{1}{k+p}}\\right\\}\\times{\\frac{1}{k+p}}\\times\\left\\{\\log\\left({\\frac{4}{\\delta}}\\right)+\\log\\log\\left({\\frac{2}{\\delta}}\\right)\\right\\}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Next, we use the assumption that $F^{-1}$ is $L$ -Lipschitz to obtain that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle n(x_{1,k}-x_{0,k}):=n\\left(F^{-1}\\left(1\\wedge\\left(\\frac{4\\log\\left(\\frac{2}{\\delta}\\right)}{m_{k}}\\right)^{\\frac{1}{k+p}}\\right)-F^{-1}\\left(\\left(\\frac{\\delta}{m_{k}}\\right)^{\\frac{1}{k+p}}\\right)\\right)}}\\\\ {{\\displaystyle\\leq\\frac{L n}{k+p}\\times\\left\\{\\log\\left(\\frac{4}{\\delta}\\right)+\\log\\log\\left(\\frac{2}{\\delta}\\right)\\right\\}}}\\\\ {{\\displaystyle\\leq4\\frac{L n}{n+p}\\times\\log\\left(\\frac{4}{\\delta}\\right)\\,.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By substituting $\\delta$ by $\\delta/(\\lceil n\\sqrt{m_{k}}\\rceil)$ we obtain that the linear dependency in $n$ obtained with the previous analysis is refined to a log (4[nmEl) for the upper bound, which matches the result at this point. ", "page_idx": 24}, {"type": "text", "text": "We now consider the lower bound, and work on refining the upper bound of the term $-n\\mathcal{E}_{1}$ in the proof of Theorem 1. To do that, we consider a new intermediary point $\\begin{array}{r}{x_{0,k}^{\\prime}=F^{-1}\\left(\\frac{\\delta}{m\\times n^{\\frac{k+p}{n+p-1}}}\\right)}\\end{array}$ For the rest of the proof we get back to the discretized formulation of the error (with generic step $D^{-1}$ ), and upper bound ", "page_idx": 24}, {"type": "equation", "text": "$$\n-n\\mathcal{E}_{1}\\leq\\underbrace{\\frac{n}{D}\\sum_{k\\in I_{1}}F_{s}^{n+p-1}}_{n\\mathcal{E}_{1}^{0}}+\\underbrace{\\frac{n}{D}\\sum_{k\\in I_{2}}F_{s}^{n+p-1}}_{n\\mathcal{E}_{1}^{1}}\\ .\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We remark that we can use the upper bound provided for $n(x_{1,k}\\!-\\!x_{0,k})$ to upper bound $n\\mathcal{E}_{1}^{1}$ , obtaining the same result up to some multiplicative constants. Hence, it remains to upper bound ${\\mathcal E}_{1}^{0}$ , for which additional steps are needed. However, we will simply use the exact same trick as before: we consider another sub-interval $I_{2}^{\\prime}$ , for which this time it holds that $\\begin{array}{r}{k\\in I_{2}^{\\prime}\\Rightarrow F_{s}^{k+p}\\leq\\frac{\\delta}{m\\times n^{\\frac{k+p}{n+p-1}}}}\\end{array}$ . Then, we have that ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle n\\mathcal{E}_{1}^{0}=\\frac{n}{D}\\sum_{k\\in{\\cal I}_{2}^{\\prime}}F_{s}^{n+p-1}+\\frac{n}{D}\\sum_{k\\in{\\cal I}_{2}\\backslash{\\cal I}_{2}^{\\prime}}F_{s}^{n+p-1}}\\ ~}\\\\ {{\\displaystyle~~~\\leq\\frac{n}{D}\\sum_{k\\in{\\cal I}_{2}^{\\prime}}\\frac{1}{n}\\left(\\frac{\\delta}{m_{k}}\\right)^{\\frac{n+p-1}{k+p}}+\\frac{n}{D}\\sum_{k\\in{\\cal I}_{2}\\backslash{\\cal I}_{2}^{\\prime}}\\left(\\frac{\\delta}{m_{k}}\\right)^{\\frac{n+p-1}{k+p}}}\\ ~}\\\\ {{\\displaystyle~~~\\leq\\frac{|{\\cal I}_{2}^{\\prime}|}{D}\\left(\\frac{\\delta}{m_{k}}\\right)^{\\frac{n+p-1}{k+p}}+n\\times\\frac{|{\\cal I}_{2}|-|{\\cal I}_{2}^{\\prime}|}{D}\\left(\\frac{\\delta}{m_{k}}\\right)^{\\frac{n+p-1}{k+p}}\\ .}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Just as before, we show that Lol-Iol cannot be too large if the quantile function is Lipschitz. More precisely, we obtain that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left({\\frac{\\delta}{m_{k}}}\\right)^{\\frac{1}{k+p}}-\\left({\\frac{\\delta}{m_{k}n^{\\frac{k+p}{n+p-1}}}}\\right)^{\\frac{1}{k+p}}=\\left({\\frac{\\delta}{m_{k}}}\\right)^{\\frac{1}{k+p}}{\\frac{\\log(n)}{n+p-1}}\\;,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "so that we can finally write that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{n\\mathcal{E}_{1}^{0}\\leq\\bigg(\\displaystyle\\frac{\\delta}{m_{k}}\\bigg)^{\\frac{n+p-1}{k+p}}+n\\times\\bigg(\\bigg(\\displaystyle\\frac{\\delta}{m_{k}}\\bigg)^{\\frac{1}{k+p}}\\times\\frac{\\log(n)}{n+p-1}\\times L+\\frac{1}{D}\\bigg)\\times\\bigg(\\displaystyle\\frac{\\delta}{m_{k}}\\bigg)^{\\frac{n+p-1}{k+p}}}}&{}\\\\ {{\\leq\\bigg(\\displaystyle\\frac{\\delta}{m_{k}}\\bigg)^{\\frac{n+p-1}{k+p}}+n\\times\\bigg(\\displaystyle\\frac{\\log(n)}{n+p-1}\\times L+\\frac{1}{n\\sqrt{m_{k}}}\\bigg)\\times\\bigg(\\displaystyle\\frac{\\delta}{m_{k}}\\bigg)^{\\frac{n+p-1}{k+p}}\\ ,}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which is suffcient to conclude, since the multiplicative constants to mk $m_{k}^{-\\frac{n+p-1}{k+p}}$ are clearly dominated by $\\log\\left({\\frac{4\\lceil n{\\sqrt{m_{k}}}\\rceil}{\\delta}}\\right)$ \u53e3 ", "page_idx": 25}, {"type": "text", "text": "B.4 Empirical UCB and LCB ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The UCB and LCB in Equation (17) and Equation (19) depend explicitly on the unknown $F$ via $\\xi_{k,n,F}^{+}$ and $\\xi_{k,n,F}^{-}$ and therefore cannot be used in the implementation of GG. ", "page_idx": 25}, {"type": "text", "text": "Below, we give empirical UB $(\\widehat{U}_{k}(n,\\delta))$ and LCB $(\\widehat{L}_{k}(n,\\delta))$ by replacing $\\xi_{k,n,F}^{+}$ and $\\xi_{k,n,F}^{-}$ by empirical estimates $\\widehat{\\xi}_{k,n}^{+}$ and $\\widehat{\\xi}_{k,n}^{-}$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\widehat{U}_{k}(n,\\delta)=\\widehat{r}_{k}(n)+\\frac{1}{\\sqrt{m_{k}}}+\\beta_{k,n}^{-}\\sqrt{\\frac{\\log\\Big(\\frac{2\\lceil n\\sqrt{m_{k}}\\rceil}{\\delta}\\Big)}{m_{k}}}+n\\times\\widehat{\\xi}_{k,n}^{-}\\left(\\frac{\\log\\Big(\\frac{2\\lceil n\\sqrt{m_{k}}\\rceil}{\\delta}\\Big)}{m_{k}}\\right)^{\\frac{n+p-1}{k+p}}}\\\\ {\\widehat{L}_{k}(n,\\delta)=\\widehat{r}_{k}(n)-\\frac{1}{\\sqrt{m_{k}}}-\\beta_{k,n}^{+}\\sqrt{\\frac{\\log\\Big(\\frac{2\\lceil n\\sqrt{m_{k}}\\rceil}{\\delta}\\Big)}{m_{k}}}-n\\times\\widehat{\\xi}_{k,n}^{+}\\left(\\frac{\\log\\Big(\\frac{2\\lceil n\\sqrt{m_{k}}\\rceil}{\\delta}\\Big)}{m_{k}}\\right)^{\\frac{n+p-1}{k+p}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\beta_{k,n}^{-}$ and $\\beta_{k,n}^{+}$ are defnedin Equation (18and Equation 2), and ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\xi}_{k,n}^{-}=4^{\\frac{n+p-1}{k+p}}\\left\\{\\frac{\\widehat{d}_{k+p}+1}{\\lceil n\\sqrt{m_{k}}\\rceil-1}\\right\\},}\\\\ &{\\widehat{\\xi}_{k,n}^{+}=8^{\\frac{n+p-1}{k+p}}\\left\\{\\frac{\\widehat{d}_{k+p}+1}{\\lceil n\\sqrt{m_{k}}\\rceil-1}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\hat{d}_{k+p}=\\operatorname*{inf}\\{d\\in\\{0,\\dots,\\lceil n\\sqrt{m_{k}}\\rceil-1\\},\\hat{F}_{d}^{k+p}\\geq8\\frac{\\log(2\\lceil n\\sqrt{m_{k}}\\rceil/\\delta)}{m_{k}}\\}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "if the infimum exists and $\\hat{d}_{k+p}=1$ otherwise. ", "page_idx": 25}, {"type": "text", "text": "It is a corollary of Theorem 1 that $\\widehat{U}_{k}(n)$ and $\\widehat{L}_{k}(n)$ are indeed high probability upper and lower bounds of the true reward: ", "page_idx": 25}, {"type": "text", "text": "Corollary 1 (Explicit upper and lower bounds). It holds that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\widehat{L}_{k}(n,\\delta)\\leq r(n)\\leq\\widehat{U}_{k}(n,\\delta))\\geq1-\\delta\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. With $D\\,=\\,\\lceil n{\\sqrt{m_{k}}}\\rceil$ and $\\begin{array}{r}{x_{1,k}\\,=\\,F^{-1}\\left(1\\wedge\\left(\\frac{4\\log\\left(\\frac{2\\lceil n\\sqrt{m_{k}}\\rceil}{\\delta}\\right)}{m_{k}}\\right)^{\\frac{1}{k+p}}\\right)}\\end{array}$ the good event $\\mathcal{G}$ defined in Equation (9)implies that with probability $1-\\delta$ the following event $\\mathcal{H}$ holds: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathcal{H}=\\left\\{\\forall k\\le\\lceil D x_{1,k}\\rceil,\\widehat{F}_{s}^{k+p}\\le8\\frac{\\log\\left(\\frac{2\\lceil n\\sqrt{m_{k}}\\rceil}{\\delta}\\right)}{m_{k}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Under $\\mathcal{H}$ and since by definition $\\begin{array}{r}{\\hat{F}_{\\hat{d}_{k+p}}^{k+p}\\geq8\\frac{\\log(2\\lceil n\\sqrt{m_{k}}\\rceil/\\delta)}{m_{k}}}\\end{array}$ it hold that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\frac{\\hat{d}_{k+p}}{D}\\geq x_{1,k}=F^{-1}\\left(1\\wedge\\left(\\frac{4\\log\\left(\\frac{2\\lceil n\\sqrt{m_{k}}\\rceil}{\\delta}\\right)}{m_{k}}\\right)^{\\frac{1}{k+p}}\\right)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This implies that $\\widehat{\\xi}_{k,n}^{-}\\geq\\xi_{k,n,F}^{-}$ and $\\widehat{\\xi}_{k,n}^{+}\\geq\\xi_{k,n,F}^{+}$ ", "page_idx": 25}, {"type": "text", "text": "Therefore, we can incorporate in the proof of Theorem 1 the fact that the good event $\\mathcal{G}$ implies $\\widehat{\\xi}_{k,n}^{-}\\geq\\xi_{k,n,F}^{-}$ and $\\widehat{\\xi}_{k,n}^{+}\\geq\\xi_{k,n,F}^{+}$ and obtain the stated result. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "C  Regret analysis of Local-Greedy and Greedy-Grid ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "C.1 Clarification on the feedback received by the algorithms ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In this section, we consider the case where a feedback (in the form of a sample from a power of $F$ )is gathered only when an auction is won. If this is not the case, the decision-maker only knows that the coalition lost the auction. Therefore, if at time $t$ $n_{t}$ agents are assigned to an auction and the auction is lost, it makes sense to continue assigning $n_{t}$ agents to the auction at time $t+1$ , in order to gather the information that the algorithm wanted to obtain. The meta algorithm called CoMAB for coalition multi-armed bandits described in Algorithm 3 implements this strategy. ", "page_idx": 26}, {"type": "table", "img_path": "B74mb0tEY6/tmp/fb6ad58bfe9f94e26c12011c810891abdc0058e191197e6107c7fa0378050d64.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "Local-Greedy and Greedy-Grid are then defined as CoMAB applied on $\\pi_{\\mathtt{L G}}$ and $\\pi_{\\mathtt{G G}}$ for local greedy and greedy-grid respectively. These policies associate a play $n_{m}$ to an history ${\\mathcal I}_{m-1}$ . In Algorithm 1 and Algorithm 2, the policies are called sequentially $T$ times and fedback is observed after each request. This gives an implicit definition of the policies. ", "page_idx": 26}, {"type": "text", "text": "Lemma 6 expresses the regret of CoMAB in function of the behavior of any policy $\\pi$ whenfeedback is observed after each request. ", "page_idx": 26}, {"type": "text", "text": "Lemma 6 (Regret of CoMAB).Consider a policy $\\pi$ that associate to every ${\\mathcal I}_{m-1}$ a play $n_{m}^{\\pi}$ .Define $\\mathcal{I}_{0}=\\mathcal{D}$ and $\\mathcal{J}_{m}=\\mathcal{J}_{m-1}\\cup\\{w_{n_{m}^{\\pi}},n_{m}^{\\pi},m\\}$ where $w_{n_{m}}^{\\pi}$ is a sample from a distribution with c.d.f $F^{n_{m}^{\\pi}+p}$ and $n_{m}^{\\pi}=\\pi({\\mathcal{J}}_{m-1})$ Consider $m_{n}^{\\pi}(m)$ the number of times $\\pi$ returns $n$ after m calls of $\\pi$ ", "page_idx": 26}, {"type": "text", "text": "After $T$ iterations, CoMAB based on $\\pi$ has regret: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{R}_{T}\\leq\\sum_{n=1}^{N}\\mathbb{E}[m_{n}^{\\pi}(T)]\\frac{p+n}{n}(r(n^{*})-r(n))\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. Call $n_{t}$ the play chosen by CoMAB at time $t$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\eta_{t}=\\mathbb{1}\\left\\{\\mathrm{The~auction~is~won~at~time~}t\\right\\},}\\\\ &{}\\\\ &{m_{n}(t)=|\\{\\rho\\leq t,n_{\\rho}=n\\mathrm{~and~}\\eta_{\\rho}=1\\}|}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "the number of times that $n$ is played and the auction is won up to time $t$ and ", "page_idx": 26}, {"type": "equation", "text": "$$\nZ_{n,m}(t)=|\\{\\rho\\leq t,n_{\\rho}=n{\\mathrm{~and~}}m\\leq m_{n}(\\rho)<m+1\\}|\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "the number of times that $n$ has been played between the $m$ -th time $n$ won an auction and the $m+1$ -th time. Note that $m_{n}(t)\\leq m_{n}^{\\pi}(t)$ since at time $t,\\pi$ has been called at most $t$ times. ", "page_idx": 26}, {"type": "text", "text": "The regret of CoMAB satisfies: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathcal{R}_{T}=}&{\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}[r(n^{*})-r(n_{t})]}\\\\ &{\\quad=\\displaystyle\\sum_{n=1}^{N}\\mathbb{E}\\left[\\sum_{t=1}^{T}\\mathbf{1}\\{n_{t}=n\\}\\right](r(n^{*})-r(n))}\\\\ &{\\quad=\\displaystyle\\sum_{t=1}^{N}\\mathbb{E}\\left[\\sum_{n=1}^{m_{t}(T)}Z_{n,m}(T)\\right](r(n^{*})-r(n))}\\\\ &{\\quad=\\displaystyle\\sum_{n=1}^{N}\\mathbb{E}\\left[\\sum_{n=1}^{m_{t}(T)}Z_{n,m}(T)\\right](r(n^{*})-r(n))}\\\\ &{\\quad=\\displaystyle\\sum_{n=1}^{N}\\mathbb{E}\\left[\\sum_{n=1}^{m_{t}(T)}Z_{n,m}(T)\\right](r(n^{*})-r(n))}\\\\ &{\\quad\\le\\displaystyle\\sum_{n=1}^{N}\\mathbb{E}[m_{n}^{\\pi}(T)]\\frac{p+n}{n}(r(n^{*})-r(n))}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where in the second to last inequality, we used the independence between $n_{t}=n$ and $Z_{n,m_{t}(n)}(T)$ as $n_{t}=n$ depends only on the history at times $t<m_{t}(n)$ while $Z_{n,m_{t}(n)}(T)$ depends only on times $t\\geq m_{t}(n)$ ", "page_idx": 27}, {"type": "text", "text": "C.2 Auxiliary result ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Before proving the theorems, we present an auxiliary result from [6] that we to derive upper bounds that can be recovered explicit in the proof of the theorems. Since the proof is simple, we recall it for completeness. ", "page_idx": 27}, {"type": "text", "text": "Lemma 7 (Lemma 4 from [6]). For any $\\zeta\\geq1$ themapping ", "page_idx": 27}, {"type": "equation", "text": "$$\nf_{\\zeta}:x\\in[(\\zeta+2)^{\\zeta}\\vee3,\\infty)\\mapsto\\operatorname*{sup}\\left\\{t\\in\\mathbb{N}:\\frac{t}{\\log(t)^{\\zeta}}\\leq x\\right\\}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "satisfies ", "page_idx": 27}, {"type": "equation", "text": "$$\nf_{\\zeta}(x)\\leq(\\zeta+2)^{\\zeta}\\times\\log(x)^{\\zeta}x.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "ProofWesta brha $\\begin{array}{r}{g(x)=\\frac{x}{\\log(x)^{\\zeta}}}\\end{array}$ is strietly nerasing forall $x\\geq e^{\\zeta}$ Now, consider a value $s=A x\\log(x)^{\\zeta}$ for some $A>0$ , such that $s\\geq3\\vee e^{\\zeta}$ . By the monotonicity f $\\frac{t}{(\\log t)^{\\zeta}}$ , we have that ", "page_idx": 27}, {"type": "equation", "text": "$$\nt>s\\Rightarrow\\frac{t}{(\\log(t)^{\\zeta}}>\\frac{s}{(\\log(s)^{\\zeta}}=x\\times\\frac{A\\log(x)^{\\zeta}}{(\\log(A)+\\log(x)+\\zeta\\log(\\log(x)))^{\\zeta}}\\;.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then, for $x\\geq A\\geq3$ , it holds that l $\\begin{array}{r}{\\operatorname{og}(A)+\\log(x)+\\zeta\\log(\\log(x))\\leq\\left(\\zeta+2\\right)\\log(x)}\\end{array}$ , so we can simply choose $A=(\\zeta+2)^{\\zeta}$ to obtain the result. ", "page_idx": 27}, {"type": "text", "text": "All that is left is to verify that for this choice, $s=(\\zeta+2)^{\\zeta}\\times\\log(x)^{\\zeta}x\\geq3\\lor e^{\\zeta}$ , but this clearly holds for all $x\\geq3$ and $\\zeta>0$ \u53e3 ", "page_idx": 27}, {"type": "text", "text": "C.3 Proof of Theorem 2 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Theorem 2 (Regret bound for Local-Greedy). Let $\\begin{array}{r}{\\Delta:=\\operatorname*{min}_{n\\in[N-1]}|r(n+1)-r(n)|}\\end{array}$ (worst local gap). Under Assumption 2 and with $\\alpha=(\\log_{3/2}N+1)^{-1}$ , the regret of $\\mathit{L G}$ is upper bounded by $a$ problem-dependent constant: there exists $(C_{n})_{n\\in[N]\\backslash\\{n^{\\star}\\}}$ each satisfying $\\begin{array}{r}{C_{n}=\\widetilde{O}_{N}\\left(\\frac{\\Delta_{n}}{\\Delta^{2}}\\right)}\\end{array}$ such that $\\begin{array}{r}{\\mathcal{R}_{T}\\leq\\sum_{n\\in[N]\\backslash n^{\\star}}C_{n}}\\end{array}$ ", "page_idx": 27}, {"type": "text", "text": "Additionally, if the arm set forms a single estimation neighborhood, that s $\\forall n\\in[N]:\\,\\mathcal{V}(n)\\supset[N],$ then each constant $C_{n}$ can be refined to $\\widetilde{\\mathcal{O}}_{n}\\left(\\Delta_{n}^{-1}\\right)$ providing $\\mathcal{R}_{T}=\\widetilde{\\mathcal{O}}(\\sqrt{N T})$ which holds even when the reward function is not unimodal. ", "page_idx": 27}, {"type": "text", "text": "Proof. First, we denote by $\\widetilde{r}_{t}(n)$ the reward estimate used for arm $n$ at time $t$ , and by $\\widehat{r}_{k,t}(n)$ its value when the arm used to compute the estimate is fixed to $k\\in\\mathcal{V}(n)$ . The proofs rely on concentration bounds on $\\widetilde{r}_{t}(n)$ derived from Theorem 1, with a confidence level $\\delta_{t}$ that will be fixed later. However, we will use this result with extra care given that the identity of the arm $k$ used to compute the estimate is a random variable, as well as its sample size $m_{k}(t)$ . This issue is tackled with appropriate union bounds. Furthermore, in order to simplify the presentation we denote by $\\mathcal{E}(\\boldsymbol{m},\\delta)$ the maximal diameter (as a function of $k$ ) of the confidence interval provided by Equation (7), defined by a number of plays $m$ of the arm used to estimate, and by a confidence level $\\delta$ . More precisely, with notation of Theorem 1, for any $(k,n)\\in[N]^{2}$ we write that $|\\widehat{r}_{k,t}(n)-r(n)|\\leq\\mathcal{E}(m_{k}\\bar{(}t),\\delta_{t})$ with probability at least $1-\\delta_{t}$ . Furthermore, $\\mathcal{E}(\\dot{m_{k}}(t),\\delta_{t})$ is increasing in $\\delta_{t}$ and decreasing in $m_{k}(t)$ . Finally, we use the notation K = [log3/2(N)], so that \u03b1 = K+1: ", "page_idx": 28}, {"type": "text", "text": "We now prove the first statement of the theorem, by upper bounding the number of plays of each sub-optimal arm. ", "page_idx": 28}, {"type": "text", "text": "Single neighborhood   Consider any sub-optimal arm $n$ . The main ingredient of the proof is to tackle the forced sampling by using that if $n$ is pulled at time $t$ , then it is either pulled \u201con purpose\" or due to forced sampling. However, it it forced sampled then it must have been selected \u201con purpose\" by being the best empirical arm in the neighborhood at some previous point in time. We hence consider the following good event ", "page_idx": 28}, {"type": "equation", "text": "$$\n{\\mathcal G}_{t}=\\left\\{\\forall s\\in\\{t-\\lfloor\\alpha t\\rfloor,\\ldots,t\\},\\ (\\forall k\\in\\mathcal{V}(n):m_{k}(s)\\geq\\alpha t),\\ |\\widehat{r}_{k,s}(n)-r(n)|\\leq\\mathcal{E}(m_{k}(s),\\delta_{s})\\right\\}\\ .\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Using Theorem 1, $\\mathcal{G}_{t}$ holds with probability at least $1-|\\mathcal{V}(n)|t^{2}\\delta_{t}$ , where we used a crude union bound on the values of $s$ $k$ and $m_{k}(t)$ $t$ could be replaced by $t-\\lfloor\\alpha\\rfloor t\\rfloor$ 0. Using this result, we frst upper bound the number of plays of $n$ up to horizon $T$ as follows, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{\\Sigma}\\Bigg[\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\{n_{t}=n\\}\\Bigg]\\le\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\{\\exists s\\in\\{t-\\lfloor\\alpha t\\rfloor,\\ldots,t\\}:\\,\\widehat{r}_{s}(n)\\geq\\widehat{r}_{s}(n^{\\star})\\}\\right]}\\\\ &{\\phantom{\\quad\\quad}\\quad\\quad\\quad\\quad\\leq\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\{\\exists s\\in\\{t-\\lfloor\\alpha t\\rfloor,\\ldots,t\\}:\\,\\widehat{r}_{s}(n)\\geq\\widehat{r}_{s}(n^{\\star})\\}\\,\\mathbb{1}\\{\\mathcal{G}_{t}\\}\\right]+\\displaystyle\\sum_{t=1}^{T}\\mathbb{P}(\\bar{\\mathcal{G}}_{t})\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "As discussed above, the second term satisfies $\\begin{array}{r}{\\sum_{t=1}^{T}\\mathbb{P}(\\bar{\\mathcal{G}}_{t})\\,\\le\\,\\sum_{t=1}^{+\\infty}|\\mathcal{V}(n)|t\\delta_{t}}\\end{array}$ . In order to make it constant, we cose $\\begin{array}{r}{\\delta_{t}\\,=\\,\\frac{1}{|\\mathcal{V}(n)|t^{4}}}\\end{array}$ For the fies term, we use that $\\forall s\\in[\\alpha t,t],\\mathcal{E}(m_{k}(s),\\delta_{t})\\leq$ $\\mathcal{E}(\\alpha(1-\\alpha)t,\\delta_{t})$ and that $n$ can be played only if the two confidence intervals overlap. Hence, we further obtain that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{E}\\left[\\sum_{t=1}^{T}\\mathbb{1}\\{n_{t}=n\\}\\right]\\le\\mathbb{E}\\left[\\sum_{t=1}^{T}\\mathbb{1}\\{2\\mathcal{E}(\\alpha(1-\\alpha)t,\\delta_{t})\\ge\\Delta_{n}\\}\\right]+\\sum_{t=1}^{+\\infty}|\\mathcal{V}(n)|t^{2}\\delta_{t-\\lfloor\\alpha t\\rfloor}}\\\\ {\\displaystyle=\\mathbb{E}\\left[\\sum_{t=1}^{T}\\mathbb{1}\\{2\\mathcal{E}(\\alpha(1-\\alpha)t,\\delta_{t})\\ge\\Delta_{n}\\}\\right]+\\sum_{t=1}^{+\\infty}(1-\\alpha)^{-4}t^{-2}}\\\\ {\\displaystyle=t_{\\alpha,n}+\\frac{\\pi^{2}}{6(1-\\alpha)^{4}}\\;,}\\\\ {\\displaystyle=t_{\\alpha,n}+\\frac{\\pi^{2}}{6}\\left(1+\\frac{1}{K}\\right)^{4}\\;,\\quad\\mathrm{since}\\;\\alpha=\\frac{1}{K+1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "and for a deterministic constant $t_{\\alpha,n}$ defined by ", "page_idx": 28}, {"type": "equation", "text": "$$\nt_{\\alpha,n}=\\operatorname*{sup}\\{t\\in\\mathbb{N}:2\\mathcal{E}(\\alpha(1-\\alpha)t,\\delta_{t})\\geq\\Delta_{n}\\}\\ .\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We recall that $K=\\lceil\\log_{3/2}(N)\\rceil$ is used to simplify the notation. The last step consists in upper bounding the value of $t_{\\alpha,n}$ explicitly according to $n$ and $\\Delta_{n}$ . Considering some $t\\geq4\\lor n+1$ , from ", "page_idx": 28}, {"type": "text", "text": "Theorem 1 we know that there exist universal constants $\\beta$ and $\\xi$ , and we obtain that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}(\\alpha(1-\\alpha)t,\\delta_{t})\\leq\\beta\\sqrt{\\frac{\\log\\bigg(\\frac{2[n\\sqrt{m_{k}(s)}]}{\\delta_{t}}\\bigg)}{m_{k}(s)}}+n\\times\\xi\\left(\\frac{\\log\\bigg(2[n\\sqrt{m_{k}(s)}]\\bigg)}{m_{k}(s)}\\right)^{\\frac{3}{2}}}\\\\ &{\\qquad\\qquad\\leq\\beta\\sqrt{\\frac{\\log\\big(2t^{4}(n+1)\\sqrt{t}\\big)\\big(\\mathcal{V}(n)\\big)\\big)}{\\alpha(1-\\alpha)t}}+n\\times\\xi\\left(\\frac{\\log\\big(2t^{4}(n+1)\\sqrt{t}\\big)\\big|\\mathcal{V}(n)\\big|}{\\alpha(1-\\alpha)t}\\right)\\bigg)^{\\frac{3}{2}}}\\\\ &{\\qquad\\qquad\\leq\\beta\\sqrt{\\frac{\\log(t^{5}(n+1)^{2})}{\\alpha(1-\\alpha)t}}+n\\times\\xi\\left(\\frac{\\log\\big(t^{5}(n+1)^{2}\\big)}{\\alpha(1-\\alpha)t}\\right)^{\\frac{3}{2}}}\\\\ &{\\qquad\\qquad\\leq\\beta\\sqrt{\\frac{7\\log(t)}{\\alpha(1-\\alpha)t}}+n\\times\\xi\\left(\\frac{7\\log(t)}{\\alpha(1-\\alpha)t}\\right)^{\\frac{3}{2}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Since \u03b1 = $\\begin{array}{r}{\\alpha=\\frac{1}{K+1}}\\end{array}$ it holds that $\\begin{array}{r}{\\alpha(1-\\alpha)=\\frac{1}{K+1}\\frac{K}{K+1}\\geq\\frac{1}{2(K+1)}}\\end{array}$ We thenge that formvel constants $\\beta^{\\prime}$ and $\\xi^{\\prime}$ , it first holds that: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathcal{E}(\\alpha(1-\\alpha)t,\\delta_{t})\\leq\\left(\\beta^{\\prime}\\sqrt{K+1}+n(K+1)^{\\frac{2}{3}}\\xi^{\\prime}\\right)\\sqrt{\\frac{\\log(t)}{t}}\\;,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where we bounded $\\left({\\frac{\\log(t)}{t}}\\right)^{\\frac{2}{3}}$ by (t)Without this simlication, we alsoobtan that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathcal{E}(\\alpha(1-\\alpha)t,\\delta_{t})\\leq(K+1)^{\\frac{2}{3}}\\left\\{\\beta^{\\prime}+n\\left(\\frac{\\log(t)}{t}\\right)^{\\frac{1}{6}}\\xi^{\\prime}\\right\\}\\sqrt{\\frac{\\log(t)}{t}}\\;.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "The different scaling proposed in the theorem then come from using Lemma 7 on (23) and (24), taking the minimum between the two (since both bounds are valid simultaneously), and for the latter sliting casesdepending on ${\\frac{t}{\\log(t)}}\\leq n^{6}$ being satisied or not taking this time the maximum betweenthetwocases). ", "page_idx": 29}, {"type": "text", "text": "We provide the right-hand term of the result using (23), applying Lemma 7 with $\\zeta=1$ and ", "page_idx": 29}, {"type": "equation", "text": "$$\nx=\\frac{4}{\\Delta_{n}^{2}}\\times\\left(\\beta^{\\prime}\\sqrt{K+1}+n(K+1)^{\\frac{2}{3}}\\xi^{\\prime}\\right)^{2}\\ ,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which leads to $t_{\\alpha,n}\\leq3x\\log(x)$ . This prvides the term $\\begin{array}{r}{\\mathcal{O}\\left(\\frac{n^{2}}{\\Delta_{n}^{2}}\\right)}\\end{array}$ of the resut, and constant i the logarithmic terms can be recovered explicitly by recovering the values of $\\beta^{\\prime}$ and $\\xi^{\\prime}$ ", "page_idx": 29}, {"type": "text", "text": "We then obtain the left-hand term of the result by considering (24). Lemma 7 first provides that $\\begin{array}{r}{n\\left(\\frac{\\log(t)}{t}\\right)^{\\frac{1}{6}}\\leq1}\\end{array}$ for $t\\geq18n^{6}\\log(n)$ (first bound). Sill using Lemma 7, this simplification permits touse $\\zeta=1$ and the threshold ", "page_idx": 29}, {"type": "equation", "text": "$$\ny=\\frac{4}{\\Delta_{n}^{2}}\\times\\left(\\beta^{\\prime}\\sqrt{K+1}+(K+1)^{\\frac{2}{3}}\\xi^{\\prime}\\right)^{2}\\;,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "and an upper bound of $t_{\\alpha,n}\\,\\leq\\,3y\\log(y)$ , but only if this term is larger than $18n^{6}\\log(n)$ .This provides the remaining terms of the bound, and again the logarithms can be easily recovered by computing $\\beta^{\\prime}$ and $\\xi^{\\prime}$ ", "page_idx": 29}, {"type": "text", "text": "This concludes the proof for the problem-dependent in the favorable case where all arms are neighbors, remarking that these upper bounds just have to be multiplied by $\\Delta_{n}$ and summed over $n\\in[N]$ to convert into the regret bound. Furthermore, the problem-independent guarantee can be derived from taking the minimum between the bound and $\\Delta_{n}T$ , remarking that the worst case is $\\Delta_{n}=T^{-1/2}$ if we omit the logarithms. ", "page_idx": 29}, {"type": "text", "text": "General case  We now provide the regret bound for the general case, where at least some arms do not include $[N]$ in their neighborhood. We recall that two main ingredients of Local-Greedy are (1) that the arm $n_{t}$ played in $t$ is the best empirical arm in the neighborhood of $\\mathcal{V}(n_{t-1})$ , according to the simple estimates computed with samples from $n_{t-1}$ , and (2) that $n_{t}=n_{t-1}$ if $m_{n_{t-1}}(t)<\\alpha t$ (forced sampling). Hence, similarly to the previous proof we use that $n_{t}$ is either pulled thanks to a \u201cgreedy play\u201d or because of forced sampling. Furthermore, the two cases can be merged because forced sampling can only come after $n_{t}$ being pulled because of a greedy play in the recent rounds. More precisely, we use that ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\{n_{t}=n\\}\\subset\\{\\exists s\\in[t-\\lfloor\\alpha t\\rfloor]:n_{s}=n,m_{s}(\\ell_{s})\\geq\\lceil\\alpha s\\rceil\\}\\ .\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "This argument is at the core of our analysis, but before going further we need to introduce the notion of locallyoptimalplays. ", "page_idx": 30}, {"type": "text", "text": "Definition 2 (Locally optimal plays and optimal path). Given a reference arm $\\ell_{i}$ playing $n\\in\\mathcal{V}(\\ell)$ is locally optimal ij $f n=\\operatorname{argmax}_{k\\in\\mathcal{V}(\\ell)}r(k)$ In that case, n is the best neighbor of $\\ell_{s}$ and we use the notation $n=v^{+}(n)$ ", "page_idx": 30}, {"type": "text", "text": "Furthermore, a sequence of successive locally optimal plays is an optimal path towards $n^{\\star}$ .By constructionof $\\mathcal{V}$ anoptimalpathcontainsatmost $K:=\\mathcal{O}(\\log(N))$ sub-optimalarms. ", "page_idx": 30}, {"type": "text", "text": "The last fact presented in the definition is trivial: in the worst case the path start at one of the extremes of the interval $[N]$ and $n^{\\star}$ is at the other. By design of $\\nu$ (Definition 1) we obtain that $n^{\\star}$ is reached in $\\lceil\\log_{3/2}(N)\\rceil$ steps at most. The rest of the proof is based on the idea that, when $t$ is large enough, the algorithm starts following an optimal path with high probability, so a sub-optimal arm can be played only if it is located on an optimal path from another sub-optimal arm to $n^{\\star}$ . We formalize it with the following result. ", "page_idx": 30}, {"type": "text", "text": "Lemma 8 (Existence of a \u201crecent\u201d sub-optimal play). For any time step $t\\in[T]$ andarm $n\\neq n^{\\star}$ ,it holdsthat ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\{n_{t}=n\\}\\subset\\rightharpoonup\\rightharpoonup\\rightharpoonup\\{\\rightharpoonup\\rightharpoonup\\rightharpoonup\\rightharpoonup\\rightharpoonup\\rightharpoonup\\rightharpoonup\\rightharpoonup\\rightharpoonup\\rightharpoonup\\rightharpoonup\\rightharpoonup\\rightharpoonup\\rightharpoonup\\rightharpoonup\\rightharpoonup\\rightharpoonup\\rightharpoonup\\rightharpoonup\\rightharpoonup\\rightharpoonup\\rightharpoonup\\rightharpoonup\\ r(l):\\ \\widehat{r}_{s}(l)\\leq\\widehat{r}_{s}(l^{\\prime}),}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ m_{s}(l)\\geq\\alpha(1-\\alpha)^{K}t\\quad a n d\\quad r(l)>r(l^{\\prime})\\}\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. Starting from (25), we first use that either $n\\neq v^{+}(\\ell_{s})$ , and in that case playing $n$ is locally sub-optimal so this event belongs to $\\boldsymbol{A}_{t}$ ,or $n=v^{+}(\\ell_{s})$ . Let us now consider this second case: by definition of the leader, $\\ell_{s}$ was played right before the sequence of forced plays of $n$ started, which must have happened at least as recently as $t-\\lfloor\\alpha t\\rfloor-1$ . From that point, the recursion pattern is clear: $\\ell_{s}$ must have been selected in the last $\\lfloor\\alpha(s-1)\\rfloor$ , by either being a locally sub-optimal play or not. The first case is included in $A_{t}$ , why the second requires to add another step in the analysis. Furthermore, the arm used to estimate $\\ell_{s}$ was itself samples at least proportionally to $s$ . Using that this can happen $K$ times, and that the worst number of steps to look into the past at each step is at most a fraction $(1-\\alpha)$ of the number in the previous step, we finally obtain that there have been a sub-optimal greedy play in the last $\\left[(1-\\alpha)^{\\bar{K}}t,t\\right]$ steps. \u53e3 ", "page_idx": 30}, {"type": "text", "text": "Before going further, we justify the tning $\\begin{array}{r}{\\alpha=\\frac{1}{K+1}}\\end{array}$ ,by stating thatit maximizes $\\alpha(1-\\alpha)^{K}$ (used later in the proof). At this step, we can simplify the notation by remarking that ", "page_idx": 30}, {"type": "equation", "text": "$$\n(1-\\alpha)^{K}=\\left(\\frac{K}{K+1}\\right)^{K}\\ge e^{-1}\\;,\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "hence we replace $(1-\\alpha)^{K}$ by $e^{-1}$ in the rest of the proof. ", "page_idx": 30}, {"type": "text", "text": "In words Lemma 8 states that, even if forced sampling slows down the ascension towards $n^{\\star}$ ,since optimal paths contain at most $K$ sub-optimal arm then $n^{\\star}$ is relatively fast to reach from any arm in $[N]$ . Next, we use this result in the regret analysis by considering its occurrences with the following event, ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{H}_{t}=\\left\\{\\forall s\\in\\left[e^{-1}t,t\\right],\\forall n\\in[N],\\,\\forall k\\in\\mathcal{V}(n):m_{k}(s)\\geq\\frac{e^{-1}}{1+K}t,\\right.}\\\\ {\\left.\\left|\\widehat{r}_{k,s}(n)-r(n)\\right|\\leq\\mathcal{E}(m_{k}(s),\\delta_{s})\\right\\}\\,.\\qquad\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Then, for any $t_{K}\\in\\mathbb{N}$ we can upper bound the number of plays of each sub-optimal arm $n\\in[N]$ as follows, ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\displaystyle\\sum_{t=1}^{T}\\mathbb{1}\\{n_{t}=n\\}\\right]\\leq\\mathbb{E}\\left[\\displaystyle\\sum_{t\\geq1}\\mathbb{1}\\{A_{t}\\}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq t_{K}+\\mathbb{E}\\left[\\displaystyle\\sum_{t\\geq t_{K}}\\mathbb{1}\\{A_{t},\\mathcal{H}_{t}\\}\\right]+\\displaystyle\\sum_{t\\geq t_{K}}\\mathbb{P}(\\bar{\\mathcal{H}}_{t})\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "with the slight abuse of notation that $\\mathcal{E}$ is now defined with the coalition size $N$ and not the local value of $n$ considered. The rest of the proof is analogous to the simple case, where all arms are in a single neighborhood. We frst chose $\\begin{array}{r}{\\delta_{t}=\\frac{1}{N^{2}t^{4}}}\\end{array}$ and obain that $\\begin{array}{r}{\\sum_{t\\geq t_{K}}\\mathbb P(\\bar{\\mathcal{H}}_{t})\\leq\\sum_{t=1}^{+\\infty}\\frac{1}{e^{-4}t^{2}}\\leq\\frac{\\pi^{2}}{6e^{-4}}}\\end{array}$ ", "page_idx": 31}, {"type": "text", "text": "Next, we tune $t_{K}$ large enough so that $\\begin{array}{r}{\\mathbb{E}\\left[\\sum_{t\\geq t_{K}}\\mathbb{I}\\{\\mathcal{A}_{t},\\mathcal{H}_{t}\\}\\right]=0.}\\end{array}$ This can be done by choosing ", "page_idx": 31}, {"type": "equation", "text": "$$\nt_{K}=\\operatorname*{sup}\\left\\{t\\in\\mathbb{N}:2\\mathcal{E}\\left(\\frac{e^{-1}}{1+K}t,\\delta_{t})\\right)\\le\\Delta\\right\\}\\ .\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $\\begin{array}{r}{\\Delta=\\operatorname*{min}_{n\\in[N-1]}\\{|r(n+1)-r(n)|\\}}\\end{array}$ ", "page_idx": 31}, {"type": "text", "text": "We then deduce the result by applying the exact same steps as for the upper bound of $t_{\\alpha,n}$ , by carefully replacing $\\begin{array}{r}{\\delta_{t}=\\frac{1}{|\\mathcal{V}(n)|t^{4}}}\\end{array}$ $\\begin{array}{r}{\\delta_{t}=\\frac{1}{N^{2}t^{4}}}\\end{array}$ and $\\alpha(1-\\alpha)t$ $\\textstyle{\\frac{e^{-1}}{1+K}}t$ Lema then alowsto aily obtain the desired scaling, and to make the upper bound explicit by substitution. Since is logarithmic in $N$ , it is clear that it only contributes to the bound only by logarithmic factors. \u53e3 ", "page_idx": 31}, {"type": "text", "text": "C.4 Proof of Theorem 3 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Theorem 3 (Regret upper bound for Greedy-Grid). Suppose that $G G$ is tuned with confidence level $\\begin{array}{r}{\\delta_{t}=\\frac{1}{N^{2}t^{3}}}\\end{array}$ and $\\alpha=1/4$ Then,for any $T\\in\\mathbb N$ it holds that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{R}_{T}=\\widetilde{\\mathcal{O}}_{N}\\left(\\sum_{n\\in\\mathcal{B}^{\\star}}\\frac{1}{\\Delta_{n}}+\\sum_{n\\in\\mathcal{S}}\\frac{\\log(T)}{\\Delta_{n}}\\wedge\\Delta_{n}\\left(\\frac{\\mathbb{1}\\{n<n^{\\star}\\}}{\\Delta_{v_{l}(n^{\\star})}^{2}}+\\frac{\\mathbb{1}\\{n>n^{\\star}\\}}{\\Delta_{v_{r}(n^{\\star})}^{2}}\\right)\\right)\\ .\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Additionally, it holds that $\\mathcal{R}_{T}=\\tilde{\\mathcal{O}}\\left(\\sqrt{(K+|B^{\\star}|)T}\\right)\\!.$ for $K=\\lfloor\\log_{3/2}(N)\\rfloor.$ ", "page_idx": 31}, {"type": "text", "text": "Proof. As the theorem suggests, we will use different arguments depending on the position of $n$ With respect to the grid and the optimal bin $B^{\\star}$ . Before that, we introduce the crucial result of this proof: for eachtime $t$ large enough, thanks to the design of Greedy-Grid, all arms in optimal bin $B^{\\star}$ are estimated with a simple estimate computed with a linear number of samples in $t$ ", "page_idx": 31}, {"type": "text", "text": "Following the implementation of GG, at each time $t$ and for each arm $n\\in[N]$ , a confidence interval $[\\mathrm{LCB}_{t}(n),\\mathsf{U C B}_{t}(n)]$ is computed so that $r(n)\\in[\\mathrm{LCB}_{t}(n),\\mathrm{UCB}_{t}(n)]$ with probability at least $1-\\delta_{t}$ Similarly to the proof of Theorem 2, we consider a \u201cgood event\u201d\u2019 stating that all confidence intervals where valid on a given time range before $t$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\mathcal{G}_{t}=\\left\\{\\forall s\\in\\left[\\left\\lfloor\\frac{3t}{16}\\right\\rfloor,t\\right],\\forall n\\in[N],\\ r_{t}(n)\\in\\left[\\mathrm{LCB}_{t}(n),\\mathtt{U C B}_{t}(n)\\right]\\right\\}\\ .\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Itis clear that $\\begin{array}{r}{\\sum_{t=1}^{+\\infty}\\mathbb{P}(\\bar{\\mathcal{G}}_{t})\\le\\sum_{t=1}^{+\\infty}N^{2}t\\delta_{\\lceil\\frac{3t}{16}\\rceil}}\\end{array}$ where thescond unionboud on $N$ comes from considering the (random) identity of the arm whose samples are used to compute the interval. The following results proves that, under $\\mathcal{G}_{t}$ , there is at least one arm in the bin $B^{\\star}$ that was played a linear number of times in $t$ ", "page_idx": 31}, {"type": "text", "text": "Lemma 9 Linear number of plays in $B^{\\star}$ under $\\mathcal{G}_{t}$ ).Under $\\mathcal{G}_{t}$ there exists an arm $n\\in\\mathsf{B}^{\\star}\\cup$ $\\{v_{\\ell}^{S}(n^{\\star}),v_{r}^{S}(n^{\\star})\\}$ satisfying $\\begin{array}{r}{\\dot{m_{n}}(\\dot{3}t/4)\\geq\\frac{t}{4K}\\mathrm{~\\boldmath~\\wedge~}\\frac{t}{8}}\\end{array}$ where we call $K=|\\mathcal{S}|=\\lfloor\\log_{3/2}(N)\\rfloor$ ", "page_idx": 31}, {"type": "text", "text": "Proof. $\\mathcal{G}_{t}$ guarantees that any play during the interval $\\left[\\left\\lfloor{\\frac{t}{4}}\\right\\rfloor,t\\right]$ (including those due to forced sail $\\frac{3t}{16}$ we are sure that al forced exploration launched before that time is completed in $t/4$ . Furthermore, it is also direct from the design of the algorithms that, if $r_{s}(n)\\in[\\mathrm{LCB}_{s}\\bar{(}n),\\mathrm{UCB}_{s}\\bar{(}n)]$ and Greedy-Grid is not forced to sample the previous arm it must hold that (1) it is playing the grid, or (2) it is playing an arm in $B^{\\star}$ Indeed, no arm from a sub-optimal bin would eliminate its best neighbor. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "We consider two cases. First, if an arm $n\\in B^{\\star}$ was played between rounds $\\left\\lfloor{\\frac{t}{4}}\\right\\rfloor$ and $\\left\\lfloor{\\frac{t}{2}}\\right\\rfloor$ . In that case it has collected at least $\\frac{t}{8}$ samples before $t$ , thanks to forced sampling. In the alternative case, the grid was played between those these two rounds, which incurs $\\frac{t}{4K}$ plays of $\\operatorname{argmax}_{s\\in S}r(s)$ since by $\\mathcal{G},\\mathrm{argmax}_{s\\in S}\\,r(s)$ is not eliminated when the grid is played. Then, notice that argn $\\operatorname{lax}_{s\\in S}r(s)\\subset$ $\\{n^{*},v_{\\ell}^{S}(n^{\\star}),v_{r}^{S}(n^{\\star})\\}$ . The result follows by combining the two cases. ", "page_idx": 32}, {"type": "text", "text": "Without loss of generality, we assume in the following that $K\\geq2$ (if this is not the case, just replace $K$ by $\\operatorname*{max}(K,2))$ . As a direct consequence of Lemma 9, using Theorem 1,under $\\mathcal{G}_{t}$ there exists some constant $\\beta$ and $\\xi$ (coming from the bounds of the theorem multiplied by $2\\sqrt{4}=4$ such that the LCB of arm $n^{\\star}$ satisfies ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\forall s\\in\\left[\\frac{3t}{4},t\\right]:\\ \\mathrm{LCB}_{s}(n^{\\star})\\geq r(n^{\\star})-\\left\\{\\beta\\sqrt{K\\frac{\\log\\left(\\frac{2\\lceil N\\sqrt{t}\\rceil}{\\delta_{t}}\\right)}{t}}+N\\times\\xi\\left(K\\frac{\\log\\left(\\frac{2\\lceil N\\sqrt{t}\\rceil}{\\delta_{t}}\\right)}{t}\\right)^{\\frac{2}{3}}\\right\\}\\ .\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "To simplify the notation, we denote the right-hand term by $\\mathcal{E}(t)$ in the rest of the proof. Using this result, we can now consider all the sub-cases presented in the theorem. We fix a sub-optimal arm $n$ and upper bound $\\begin{array}{r}{\\sum_{t=1}^{T}\\mathbb{1}\\{n_{t}=n,\\mathcal{G}_{t}\\}}\\end{array}$ depending on the position of $n$ with respect to the grid $\\boldsymbol{S}$ Similarly to what we did in the proof of Theorem 2, we relate the pulls due to forced sampling to actual decisions by stating that, if $n_{t}=n$ , then there exists a round $s$ between $t-\\lfloor t/4\\rfloor$ and $t$ such that GG requested a pull of arm $n$ from a \u201cgrid play\u201d or a \u201cgreedy play\". ", "page_idx": 32}, {"type": "text", "text": "Case1: $n\\in S$ In that case if $n$ is pulled then, since there is no forced sampling for the arms of the grid it directly holds that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathtt{U C B}_{t}(n)\\geq\\operatorname*{max}_{n^{\\prime}\\in[N]}\\mathrm{LCB}_{t}(n^{\\prime})\\geq\\mathrm{LCB}_{t}(n^{\\star})\\geq r(n^{\\star})-\\mathcal{E}(t)\\;.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "On the other hand, under $\\mathcal{G}_{t}$ it holds that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbb{U}\\mathbb{C}\\mathbb{B}_{t}(n)\\leq r(n)+\\left\\{\\beta\\sqrt{K\\frac{\\log\\left(\\frac{2\\lceil N\\sqrt{t}\\rceil}{\\delta_{t}}\\right)}{m_{n}(t)}}+N\\times\\xi\\left(K\\frac{\\log\\left(\\frac{2\\lceil N\\sqrt{t}\\rceil}{\\delta_{t}}\\right)}{m_{n}(t)}\\right)^{\\frac{2}{3}}\\right\\},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "so pulling arm $n$ is possible only if $\\mathcal{E}(t)+\\mathcal{E}^{\\prime}(t,m_{n}(t))\\geq\\Delta_{n}$ , that we simplify to $2\\mathcal{E}^{\\prime}(t,m_{n}(t))\\geq$ $\\Delta_{n}$ or $2\\mathcal{E}(t)\\,\\geq\\,\\Delta_{n}$ . Considering the second term leads to a constant problem-dependent bound, analogous to $t_{\\alpha,n}$ in the proof of Theorem 2. Hence, we focus on the first term, that provide the $\\log(T)$ bound. ", "page_idx": 32}, {"type": "text", "text": "This time, we don't know if $m_{n}(t)$ is large or not. This explains why we obtain a UCB-like $(\\widetilde{\\mathcal{O}}(\\log(T)))$ upper bound with this technique. We use that ", "page_idx": 32}, {"type": "equation", "text": "$$\nt\\le T\\Rightarrow\\log\\left(t\\frac{2\\lceil N\\sqrt{t}\\rceil}{\\delta_{t}}\\right)\\le\\log\\left(T\\frac{2\\lceil N\\sqrt{T}\\rceil}{\\delta_{T}}\\right)=\\widetilde{\\mathcal{O}}(\\log(T))\\;.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then, similarly to proof of Theorem 2, we mitigate the asymptotic scaling in $N$ by noticing that if   \nmn(t) = S2(N6) then ) simplifies. In that case, we obtain a sub-Gaussian confidence interval, mn(t) $n$ $\\widetilde{\\mathcal{O}}\\left(\\frac{\\log(T)}{\\Delta_{n}^{2}}\\vee N^{6}\\right)$   \ntimes with high probability. This is the first part of the result for $n\\in S$ ", "page_idx": 32}, {"type": "text", "text": "We then use another analysis to derive the constant problem-dependent bound. We remark that, when the confidence intervals are valid, the best arm between $v_{\\ell}^{s}(n^{\\star})$ and $v_{r}^{S}(n^{\\star})$ can only be eliminated by an arm $i_{t}^{\\star}\\in B^{\\star}$ . By design of the algorithm (exploiting the unimodality assumption), it furthermore holds that if $i_{t}^{\\star}\\in\\mathcal{B}^{\\star}$ and those two arms are eliminated then GG does not play on the grid. Hence, the constant bound in this case comes from upper bounding the time required for this event to happen under $\\mathcal{G}_{t}$ . If $v_{\\ell}^{s}(n^{\\star})$ is not eliminated it must hold that $\\mathtt{U C B}_{t}(v_{l}^{S}(\\dot{n^{\\star}}))\\,\\geq\\,\\mathtt{L C B}_{t}(n^{\\star})$ (if $n\\,\\leq\\,n^{\\star}$ ) Furthermore, Lemma 9 also guarantees that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{UCB}_{t}\\big(v_{\\ell}(n^{\\star})\\big)\\le r\\big(v_{\\ell}(n^{\\star})\\big)+\\mathcal{E}(t)\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "therefore the event that $v_{\\ell}(n^{\\star})$ is notelminatdisonlypossiblef $2\\mathcal{E}(t)\\geq\\Delta_{v_{\\ell}(n^{\\star})}^{2}$ We can then use Lemma 7, following the same as in the proof of Theorem 2 right after (23) and (24). When $T$ is large enough, the derivation provides the scaling $\\begin{array}{r}{\\widetilde{\\mathcal{O}}\\left(\\frac{1}{\\Delta_{v_{\\ell}^{S}\\left(n^{\\star}\\right)}^{2}}\\right)}\\end{array}$ . We can then can follow the same steps for $v_{r}^{S}(n^{\\star})$ , and obtain $\\begin{array}{r}{\\widetilde{\\mathcal{O}}\\left(\\frac{1}{\\Delta_{v_{r}^{S}\\left(n^{\\star}\\right)}^{2}}\\right)}\\end{array}$ Furthermore, t is cear by analogy with the proo of Theorem 2 that for small values of $T$ the upper bounds have to be multiplied by a $N^{2}$ factor. Finally, we can remark that if $n<n^{\\star}$ the first bound is used, while the second is used for $n>n^{\\star}$ . This concludes the derivation of the upper bound for $n\\in S$ \uff1a ", "page_idx": 33}, {"type": "text", "text": "Case 2: $n\\not\\in{\\mathcal{S}},{\\mathcal{B}}(n)\\not={\\mathcal{B}}^{\\star}$ . We prove that this case is actually impossible under the good event, which explains the surprising constant upper bound independent of any gap. Indeed, if $n\\not\\in{\\mathcal{S}}$ is played, then it must hold that its right and left neighbors in the grid are eliminated. Since $B(n)\\overset{\\cdot}{\\neq}B^{\\star}$ then at least one of them has a reward at least as good as any arm in $B(n)$ . However, if playing arm $n$ was possible under $\\mathcal{G}_{t}$ it would hold that ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\exists\\ell\\in B(n):\\;}&{r(\\ell)\\geq\\mathrm{LCB}_{t}(\\ell)}\\\\ &{\\qquad>\\operatorname*{max}\\{\\mathrm{UCB}_{t}(v_{\\ell}^{S}(n)),\\mathrm{UCB}_{t}(v_{r}^{S}(n))\\}}\\\\ &{\\qquad\\geq\\operatorname*{max}\\{r(v_{\\ell}^{S}(n),r(v_{r}^{S}(n))\\}\\geq r(\\ell)\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "which is a contradiction due to the strict inequality in the second line. Hence, the number of time such arm $n$ is played is simply upper bounded by $\\textstyle\\sum_{t=1}^{+\\infty}\\mathbb{P}({\\bar{\\mathcal{G}}}_{t})$ , which is (by design) bounded by a universal constant. ", "page_idx": 33}, {"type": "text", "text": "Case 3 $:n\\notin S,\\mathcal{B}(n)=\\mathcal{B}^{\\star}$ We use that $n_{t}=n$ implies that $n_{s}=n$ due to a greedy play at some round $s\\in[3t/4,t]$ . Under $\\mathcal{G}_{t}$ , we can thus directly use (26), and obtain that if $2\\bar{\\mathcal{E}}(t)\\leq\\bar{\\Delta}_{n}^{2}$ this event is not possible anymore. Using the same derivation as in the other cases (involving Lemma 7), we obtain the upper bound scaling in $\\begin{array}{r}{\\mathcal{O}\\left(\\frac{1}{\\Delta_{n}^{2}}\\right)}\\end{array}$ for $T$ large enough, and by $\\begin{array}{r}{{\\mathcal O}\\left(\\frac{N^{2}}{\\Delta_{n}^{2}}\\right)}\\end{array}$ in general. ", "page_idx": 33}, {"type": "text", "text": "C.5 Regret of Greedy-Grid adapted for non-unimodal rewards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "In this section we develop the result presented in Section 3, regarding the adaptation of Greedy-Grid in the case when the reward function is no longer assumed to be unimodal. We recall that the adaptation consists in simplifying the definition of the set $\\ensuremath{\\mathcal{C}}_{t}$ in Algorithm 2 by ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathcal{C}_{t}=\\{s\\in\\mathcal{S},U_{s}\\geq L_{i_{t}^{*}}\\}\\;.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "We call the resulting algorithm GG-NU, for Greedy-Grid Non Unimodal, to differentiate it from the original version of GG introduced in the paper. In the following, we formalize the upper bound of the regret of GG-NU, and discuss how this result is obtained by adapting the proof of Theorem 3 from the previoussection. ", "page_idx": 33}, {"type": "text", "text": "Theore4Regret ofGG). Supos that GG is ted with confdencel $\\begin{array}{r}{\\delta_{t}\\,=\\,\\frac{1}{N^{2}t^{3}}}\\end{array}$ and $\\alpha=1/4$ Then,for any itholdsthat ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathcal{R}_{T}=\\widetilde{\\mathcal{O}}_{N}\\left(\\sum_{n\\in\\mathcal{B}^{\\star}}\\frac{1}{\\Delta_{n}}+\\sum_{n\\in\\mathcal{S}}\\frac{\\log(T)}{\\Delta_{n}}\\right)\\;.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Additionally, it holds that $\\mathcal{R}_{T}=\\tilde{\\mathcal{O}}\\left(\\sqrt{(K+|B^{\\star}|)T}\\right)$ for $K=\\lfloor\\log_{3/2}(N)\\rfloor$ ", "page_idx": 33}, {"type": "text", "text": "Proof. The proof follows the exact same steps as the proof of Theorem 3 presented in Appendix C.4. To adapt the arguments to GG-NU, we first remark that it sufices to identify which part of the proof uses the definition of the set $\\ensuremath{\\mathcal{C}}_{t}$ . We then find that this is the case when analyzing the Case $^{\\,l}$ in the proof, namely the regret caused by sub-optimal arms in the grid $|{\\cal S}|$ . More precisely, to obtain the bound of Theorem 3 for GG we provide two simultaneously valid upper bounds: a logarithmic $(\\log(T))$ upper bound with a reasoning akin to the standard UCB analysis, and then a constant upper bound that carefully leverages the definition of $\\ensuremath{\\mathcal{C}}_{t}$ . We easily verify that the steps for the logarithmic bound remain valid with the new definition of $\\ensuremath{\\mathcal{C}}_{t}$ , while the second bound clearly does not hold. This completes the adaptation of Theorem 3 (for GG) into Theorem 4 (for GG-NU). \u53e3 ", "page_idx": 34}, {"type": "text", "text": "D Experiments ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "All the code for the experiments is written in Python. We use Matplotlib for plotting [18], Numpy [15] for numerical computing and Scipy [31] for scientific and technical computing. Scipy and Numpy are distributed under the BSD 3-Clause, and Matplotlib is distribution under BSD-style license. Theses licenses allow free use, modification, and distribution of the library. All the experiments were conducted on a single standard laptop, with an execution time shorter than 24 hours. ", "page_idx": 34}, {"type": "text", "text": "In a first simulation, we consider a coalition of size $N=100$ and a competition of size $p=4$ .At each timestep $t$ , the algorithm decide a number of bidders $n_{t}$ to send to the auction and the values of all bidders (coalition and competition) $\\mathbf{v}\\in\\{0,1\\}^{n_{t}+p}$ are sampled according to $B(0.05)$ . With probabity $\\frac{n_{t}}{n_{t}+p}$ .thereward $\\mathbf{v}_{(1)}-\\mathbf{v}_{(2)}$ isreceived and $\\mathbf{v}_{(1)}$ is observed The(pseudo) regret a time is then computed as the sum of reward obtained up to time . The simulation above is repeated 20 times with random seeds and the mean value across seeds is reported as the expected regret $\\mathcal{R}(t)$ in Figure 3. Error bars represent the first and the last decile. ", "page_idx": 34}, {"type": "text", "text": "In this simulation, the parameters are chosen to allow for having a significant number of players while keeping a gap $\\Delta$ large enough (about $2\\times10^{-4}$ ) to be able to observe logarithmic regrets for the baselines. LG practically outperforms other approaches by a large gap. The two algorithms that ignore the structure (UCB and EXP3) end up exhibiting a worse regret than LG, OSUB and GG, which is expected. However GG has a much higher regret than LG and only outperform UCB and ExP3 for horizons greater than $10^{5}$ , when it starts to eliminate points from the logarithmic grid $\\boldsymbol{S}$ . Indeed, due to the explicit use of concentration bounds in the algorithm, which multiplicative constants are not optimized for practical implementations, GG does not practically reach the constant regret regime in the horizon of these simulations. ", "page_idx": 34}, {"type": "text", "text": "To illustrate the practical performance of LG, we perform additional simulations which are identical to the first one except for the parameters $N,p_{!}$ and the distribution of value that are set according to Table 2. The results are plot in Figure 4 where it is shown that LG reaches the constant regret regime after only a couple hundreds or thousands time steps while the other algorithms are still in the transient linear regime. ", "page_idx": 34}, {"type": "table", "img_path": "B74mb0tEY6/tmp/4668b7c4cd05c697a6fac03233cb44d5fb15609e8e46bea480a1db93b83eb314.jpg", "table_caption": ["Table 2: Configuration of additional experiments presented in Figure 4. "], "table_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "B74mb0tEY6/tmp/13da1df7b09633751000c770e094d6ce06a0f6003002329e1eba39819909ca0f.jpg", "img_caption": ["Expected Regret $\\mathcal{R}(T)$ ", "Figure 3: An empirical illustration of Table 1 with simulations in the following setting: values are distributed according to $B(0.05)$ $N\\,=\\,100$ and $p\\,=\\,4$ .We benchmark LG and GG (this paper), OSUB [9], UCB [3] and EXP3 [4] in terms of $\\mathcal{R}(T)$ computed over 20 trajectories. Error bars represent the first and last decile. "], "img_footnote": [], "page_idx": 35}, {"type": "image", "img_path": "B74mb0tEY6/tmp/5cdb4d1a57e952a27d7fad807dd8d9ad16d429ce1ceb0fb07f67500f9e0d0531.jpg", "img_caption": ["Figure 4: Illustration of additional experiments. Details of parameters are provided in Table 2. "], "img_footnote": [], "page_idx": 36}, {"type": "text", "text": "Complexity analysis_  The time-complexity of both GG and LG mainly comes from the computation of reward estimates. They are computed by replacing the integral in Equation (5) by a Riemann sum with $\\lfloor N{\\sqrt{T}}\\rfloor$ terms (the reasoning behind the number of terms needed is the same as in the proof of Theorem 1. Therefore, whenever reward estimates of a neighborhood of size $\\mathcal{O}(N)$ is needed, it costs ${\\mathcal{O}}(N^{2}{\\sqrt{T}})$ operations. Note that during forced exploration steps, reward estimates are not needed and therefore the associated cost is not paid. The total time complexity therefore depends on the number of times reward estimates are needed which itself depends on the trajectory. However, our algorithms could be modified to guarantee that reward estimates are needed at most ${\\mathcal{O}}(\\log(T))$ , times for instance by only performing updates at the end of phases of exponentially increasing length. This would lead to a mean complexity per iteration of $\\bar{\\mathcal{O}}(\\bar{N}\\log(T))$ , and similar theoretical guarantees by a slight adaptation of the analysis. This would reduce the burden of using of incorporating the structure in the algorithm, compared to the $\\mathcal{O}(N)$ cost of the baselines (which is even $O(\\bar{1})$ for OSUB). ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "E Broader Impact ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "The collection of user data should be carried out with the preservation of user privacy in mind. This issue is at the forefront of recent, ongoing developments, such as the European Union's General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA). ", "page_idx": 37}, {"type": "text", "text": "In online advertising, maintaining privacy presents new challenges, as decisions must be made without complete access to user data. ", "page_idx": 37}, {"type": "text", "text": "This paper tackles one such challenge by detailing Multi-Armed Bandit (MAB) algorithms that Demand Side Platforms (DSPs) can use to determine the number of ad campaigns that should partake in the repeated auction for ad placements, without the need for prior knowledge of each campaign's value. This study is therefore a step towards the realization of practical user privacy. It is important to note that the assumptions we make require that the actions of the DSP have a limited impact on the market, which should be carefully verified in practical applications. ", "page_idx": 37}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: The theoretical and experimental results provided in (Theorems 1 to 3 and appendix D ) correspond to the claims made in the abstract and introduction. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 38}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: See the discussion at the end of Section 3 and future work directions in Section4. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 38}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: In Theorems 1 to 3, the assumptions are referenced in the statement of each theorem, sketch of proofs are provided below each theorem and the detailed proofs are available in Appendices B, C.3 and C.4 respectively. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 39}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: Experiments are described in the text in Appendix D and can be reproduced using the code provided in supplementary material. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b)  If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 39}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: yes the paper provide open access to the code (as a zip folder in the supplementary materials) with all the instructions needed to reproduce it . ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 40}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: Yes all the details for the experimental setting are provided by the paper (see AppendixD). ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 40}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: Error bars are reported and defined in Appendix D. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 40}, {"type": "text", "text": "\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 41}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce theexperiments? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: Experiments were run on a laptop in less than a day (see also Appendix D) Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 41}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: Yes the paper is conform with the NeurIPs Code of Ethics. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 41}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 41}, {"type": "text", "text": "Answer:[Yes] ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Justification: The discussion on the Broader Impacts of this work is in Appendix E Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed. \u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 41}, {"type": "text", "text": "\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 42}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 42}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 42}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: The python packages were cited and use open source licenses (see Appendix D). ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 43}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 43}, {"type": "text", "text": "Justification: The code for the experiments is provided in the supplementary materials as a zip folder. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 43}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPs Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 43}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 43}, {"type": "text", "text": "", "page_idx": 44}]