[{"type": "text", "text": "Exploring Molecular Pretraining Model at Scale ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xiaohong Ji1\u2217, Zhen Wang1\u2217, Zhifeng $\\mathbf{Gao}^{1}$ \u2217, Hang Zheng Linfeng Zhang1,2, Guolin Ke1 , Weinan E2,3,4 ", "page_idx": 0}, {"type": "text", "text": "1DP Technology, Beijing, 100080, China. 2AI for Science Institute, Beijing 100080, China. 3School of Mathematical Sciences, Peking University, Beijing, 100871, China. 4Center for Machine Learning Research, Peking University, Beijing 100084, China. {jixh, wangz, gaozf, zhengh, zhanglf, kegl}@dp.tech weinan@math.pku.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent years, pretraining models have made significant advancements in the fields of natural language processing (NLP), computer vision (CV), and life sciences. The significant advancements in NLP and CV are predominantly driven by the expansion of model parameters and data size, a phenomenon now recognized as the scaling laws. However, research exploring scaling law in molecular pretraining models remains unexplored. In this work, we present Uni-Mol2 , an innovative molecular pretraining model that leverages a two-track transformer to effectively integrate features at the atomic level, graph level, and geometry structure level. Along with this, we systematically investigate the scaling law within molecular pretraining models, characterizing the power-law correlations between validation loss and model size, dataset size, and computational resources. Consequently, we successfully scale Uni-Mol2 to 1.1 billion parameters through pretraining on 800 million conformations, making it the largest molecular pretraining model to date. Extensive experiments show consistent improvement in the downstream tasks as the model size grows. The Uni-Mol2 with 1.1B parameters also outperforms existing methods, achieving an average $27\\%$ improvement on the QM9 and $14\\%$ on COMPAS-1D dataset. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "With the exponential growth of available biological data, there arises a critical need for innovative computational methodologies to utilize this wealth of information effectively. While traditional molecular representations like fingerprint-based models [1, 2] lack the ability to capture fine-grained structural features and struggle to handle large or complex molecules effectively. Molecular Representation Learning (MRL) using molecular pretraining emerges as a promising approach, leveraging the power of machine learning to imbue algorithms with a deep understanding of molecular structures and functions. Various modalities of molecular representation by pretraining have been extensively studied in the past. The typical approach for representing molecules involves two main strategies. One strategy is to represent molecules as one-dimensional sequential strings, such as SMILES [3, 4] and InChI [5]. The representative work is SMILES-BERT[3], which learns from large-scale unlabeled data through the masked SMILES recovery task. Another strategy is to represent molecules as two-dimensional graphs [6, 7, 8]. MolCLR [8], a typical method, learns the representations from unlabeled data by contrasting positive molecule graph pairs against negative ones. Additionally, a growing trend is to leverage three-dimensional information in MRL to enable tasks like 3D geometry prediction or generation [9, 10, 11]. The pursuit of molecular pretraining has sparked a wave of exploration and innovation across the field, marking a new era of discovery within the discipline. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "While in the past few years, scaling up pre-trained language models [12, 13, 14, 15, 16, 17, 18] has been achieved remarkable progress in natural language processing (NLP) and computer vision (CV). The exponential growth in model size and the richness of training data have significantly enhanced the capabilities and performance of LLMs across various NLP and CV tasks. Despite extensive research on molecular pretraining, the majority of prior studies have been conducted on a relatively small scale, utilizing limited parameters and datasets. Learning scalable molecular representation learning is rarely explored and remains a challenging problem. The recent [19]\u2019s work conducts a series of data-centric experiments to demonstrate scaling behaviors in various aspects. The exploration of the molecular pretraining model is limited to the GIN [20], SchNet [21], whose model scale and data scale are comparatively small. ", "page_idx": 1}, {"type": "text", "text": "To delve deeper into the scaling of molecular pretraining foundational models, our preliminary investigations have yielded notable insights within this domain. We summarize the contributions of this work as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We have curated and organized a dataset comprising approximately 884 million 3D conformations, which contains 73 million scaffolds for pretraining. To the best of our knowledge, this is the largest dataset of molecules with 3D conformations for molecular pretraining to date, which provides the foundation ingredient for training large-scale molecular models.   \n\u2022 We systematically study the scalability and flexibility of Uni-Mol2 in terms of model parameters, which range from 84M to 1.1B parameters, and characterize the relationship between validation loss and model size, dataset size, and computational resources. It is the first time to demonstrate the scaling law of molecular pretraining and Uni-Mol2 is currently the largest billion-scale molecular pretraining model to date.   \n\u2022 We present an in-depth analysis of scaling trends about fine-tuning on downstream tasks as the results are shown in Table4 and 5, Uni-Mol2 demonstrates consistent improvement in downstream task performance with increasing model parameters. The 1.1 billion parameters model also achieves significant improvement over the existing method. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Molecular representation learning Previous research has extensively investigated various modalities for molecular representation. A range of methods have been proposed based on different types of information utilized during pretraining. SMILES-BERT[3] uses the smiles sequence in pretraining to capture the representation. Due to SMILES representation lack of explicit encoding of molecular structural information. To address this limitation, GROVER integrates Message Passing Networks into a Transformer-style architecture and learns from unlabeled molecular data through carefully designed self-supervised tasks at different levels of molecular topology (node, edge, and graph). MGSSL [22] developed a motif-based graph self-supervised learning strategy that predicts both the topology and label of motifs as they are generated in the motif tree process. Furthermore, GEM[6] incorporates three-dimensional (3D) spatial structure information, atoms, bonds, and bond angles simultaneously to model the molecular representation. SphereNet[23] utilizes spherical coordinates and introduces a spherical message-passing approach, providing an effective framework for 3D molecular learning. The survey [24] discusses GNN shows great efficiency in processing molecular graph structures and their strong capability to capture local relationships within a molecule. However, the locally connected graph fails to adequately represent long-range interactions between atoms. In contrast, transformer-based models have shown exceptional performance in various tasks within the molecular domain, demonstrating remarkable representation capabilities. ", "page_idx": 1}, {"type": "text", "text": "Foundation models Recently, there has been considerable interest in developing foundational models to consolidate and expand representations. The significant advancements in scaling up pre-trained language models[12, 13, 14] have fundamentally reshaped the field of natural language processing. [25, 15, 18, 26] also prove that the foundation model demonstrates strong performance on many NLP datasets, sometimes reaching or exceeding the human performance. Some works in CV[27, 28] demonstrate the potential for \u201cLLM-like\u201d scaling in vision and underscore significant improvement via model and data scaling. And Sora[29, 30], a multi-modal foundation model exhibits the capacity to offer sophisticated understanding regarding the intricate interplay of physical and contextual dynamics within depicted scenes. ", "page_idx": 1}, {"type": "table", "img_path": "64V40K2fDv/tmp/00e1f65d222a5355ecc6870ce8e94a3752e3df19a5a0fb2241d7834210050e49.jpg", "table_caption": ["Table 1: The different scale of Uni-Mol dataset and Uni-Mol2 dataset "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "3 Pretraining ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The pretraining stage of molecular involves learning from vast amounts of molecular data to acquire a comprehensive understanding of molecular representations. By pretraining on a large and diverse unlabeled dataset, the model can develop a rich understanding of molecular structures and properties, which can subsequently be fine-tuned or applied to specific downstream tasks, such as drug discovery, materials design, or chemical synthesis. The section provides details of the data curation process for pretraining, the detailed pretraining architecture, the well-designed self-supervision tasks, and the specific training procedures employed for scaling up the model. ", "page_idx": 2}, {"type": "text", "text": "3.1 Data ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To augment the richness and diversity of the dataset, we integrated the two parts we have collected. One part consists of approximately 19 million molecules sourced from Uni-Mol [11], while the other is derived from ZINC20 [31] which includes 1.4 billion compounds. We downloaded the subset with standard reactivity, which contains 884 million compounds from website 2. Table 1 shows the enrichment compared with Uni-Mol dataset. The overall Uni-Mol2 dataset has increased by over 40 times compared to the Uni-Mol dataset, with the number of scaffold increasing by 17 times, greatly expanding the diversity of the data. Figure $\\mathbf{l}(\\mathbf{Top})$ shows the numeric distributions of the top 40 skeletons in Uni-Mol dataset and the number corresponding in Uni-Mol2 dataset. To prevent data leakage in evaluating pretraining performance, we randomly sampled $520\\mathbf{k}$ molecules from the Uni-Mol2 dataset as the validation set to evaluate the effectiveness and investigate the scaling relationship. ", "page_idx": 2}, {"type": "text", "text": "As illustrated in the visualization depicting the frequency distribution of the top 40 Murcko scaffolds in Uni-Mol2 dataset (refer to Figure 1 (Bottom)), it is observed that the molecular scaffold conforms to a distribution characterized by a long-tail pattern. To create a more balanced training dataset, we categorize the SMILES of Uni-Mol2 training set by Murcko scaffold, resulting in 73,725,454 scaffolds along with frequency distribution. Then, we utilize the temperature-based sampling method [32][33], as described in equation 1 to select molecules from Uni-Mol2 training set. ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{c}{P_{i}=\\frac{N_{s_{i}}}{\\sum N_{s_{i}}},}\\\\ {P_{s c a f f o l d_{i}}=\\mathrm{softmax}(\\frac{P_{i}}{\\tau})}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Where $N_{s i}$ represents the number of molecules with $i$ -th scaffold in Uni-Mol2 training set. The temperature $\\tau$ modulates the smoothness of the molecular distribution across scaffolds. We use an empirical value $\\tau=0.005$ as the temperature to effectively balance the proportion of molecules with high-frequency and low-frequency scaffolds. ", "page_idx": 2}, {"type": "text", "text": "3.2 Architecture ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "As depicted in Figure 2, Uni-Mol2 essentially adheres to the model design of Uni-Mol+[34], acting as a two-track transformer that concurrently processes atom features and pair features. Consistent with ", "page_idx": 2}, {"type": "image", "img_path": "64V40K2fDv/tmp/c73379702eb8bbfb57305ff993632e7f61c569472d80dac26079ed00a0ed6d66.jpg", "img_caption": ["Figure 1: Top: Comparison of scaffold frequency between Uni-Mol and Uni-Mol2 dataset. Bottom: Scaffolds distribution on Uni-Mol2 dataset "], "img_footnote": [], "page_idx": 3}, {"type": "image", "img_path": "64V40K2fDv/tmp/7809b001c3fbbd159c3cb7a921d39c5736a509f6267a0c8eae67d6e62b465c2d.jpg", "img_caption": ["Figure 2: Left: The overall pretraining architecture. Middle: Atom and Pair representation. Right: The details of backbone block "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Uni-Mol[11], Uni-Mol2 employs two self-supervised tasks: masked token prediction and molecule coordinate denoising. The detailed framework is presented as follows: ", "page_idx": 3}, {"type": "text", "text": "Feature Representation and Position Encoding Given molecular $M=(x,e,r)$ , where $\\boldsymbol{x}\\in\\mathbb{R}^{n\\times d_{a}}$ denotes atom features, $\\boldsymbol{e}\\,\\in\\,\\mathbb{R}^{n\\times n\\times d_{e}}$ denotes bond features and $\\boldsymbol{r}\\,\\in\\,\\mathbb{R}^{n\\times3}$ denotes coordinate features. Following Uni- ${\\cdot}{\\bf M o l}{+}$ , we employ RDKit to obtain atom token $x_{\\mathrm{token}}^{i}$ , atom degree $x_{\\mathrm{degree}}^{i}$ and atomic features $x_{\\mathrm{atomic}}^{i}$ for each atom. The atom embedding $x_{\\mathrm{atom}}^{i}$ is then initialized as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nx_{\\mathrm{atom}}^{i}=\\mathrm{Embedding}(x_{\\mathrm{token}}^{i})+\\mathrm{Embedding}(x_{\\mathrm{degree}}^{i})+\\mathrm{Embedding}(x_{\\mathrm{atomic}}^{i})\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "For pair features, we utilize RDKit to obtain bond features xib,ojnd by Embedding $(x_{\\mathrm{bond}}^{i,j})$ . We adopt the method from [35, 34] to encode the shortest path distance $x_{\\mathrm{SPD}}^{i,j}$ of atom pair $(\\mathrm{i},\\mathrm{j})$ in the molecular graph by Embedding $(x_{\\mathrm{SPD}}^{i,j})$ . Additionally, we employ the Gaussian kernel approach with pair type, as described in [36, 11], to encode the Euclidean distance of the atom pair $(\\mathrm{i},\\mathrm{j})$ by $\\psi^{i,j}$ . The pair embedding xpair is then initialized as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nx_{\\mathrm{pair}}^{i,j}=\\mathrm{Embedding}(x_{\\mathrm{bond}}^{i,j})+\\mathrm{Embedding}(x_{\\mathrm{SPD}}^{i,j})+\\psi^{i,j}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Two-track Transformer Layer The backbone of Uni-Mol2 has $N$ blocks, each block handles atom representation and pair representation concurrently. Formally, for the $l$ -th block, Uni-Mol2 update atom representation $x^{l}$ by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{x^{l}=\\mathrm{SelfAttentionPairBias}(\\mathrm{LN}(x^{l-1}),p^{l-1}),}\\\\ &{x^{l}=x^{l-1}+\\mathrm{FFN}(\\mathrm{LN}(x^{l}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For the pair representation $p^{l}$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p^{l}=p^{l-1}+\\mathrm{OuterProduct}(\\mathrm{LN}(p^{l-1})),}\\\\ &{p^{l}=p^{l}+\\mathrm{TriangularUpdate}(\\mathrm{LN}(p^{l})),}\\\\ &{p^{l}=p^{l}+\\mathrm{FFN}(\\mathrm{LN}(p^{l}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The details of SelfAttentionPairBias, OuterProduct, and TriangularUpdate are aligned with those of Uni-Mol+. Additionally, Uni-Mol2 adopts pre-norm layer normalization at atom and pair representation, which differs from Uni-Mol+, to improve stability in the model\u2019s training dynamics. Specifically, we set atom embedding $x_{\\mathrm{atom}}$ as atom representation $x^{0}$ and pair embedding $x_{\\mathrm{pair}}$ as pair representation $p^{0}$ for the first block. ", "page_idx": 4}, {"type": "text", "text": "Pretraining Tasks To effectively model the structure of molecular conformations, we set pretraining tasks basically following Uni-Mol. In detail, for each molecule, we randomly mask $15\\%$ of the atom tokens with the placeholder token [MASK]. We then add the atom token prediction head to optimize masked atom token loss ${\\mathcal{L}}_{\\mathrm{atom}}$ by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{atom}}=H(x_{\\mathrm{atom}}[\\mathrm{mask}],x_{\\mathrm{patom}}[\\mathrm{mask}])\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $_\\mathrm{H}$ denotes the cross entropy function, $x_{\\mathrm{{atom}}}[\\mathrm{{mask}]}$ denotes the masked atom tokens and $x_{\\mathrm{patom}}[\\mathrm{mask}]$ denotes the corresponding predicted atom tokens for the masked positions. ", "page_idx": 4}, {"type": "text", "text": "In the coordinate denoising task, to increase the challenge of the pertaining task, we introduce Gaussian noise with a standard deviation of 0.2 for all the atom coordinates. Additionally, to enhance broader applicability across downstream applications, we mask atomic features $x_{\\mathrm{atomic}}$ , bond features $x_{\\mathrm{{bond}}}$ , and shortest path distance features $x_{\\mathrm{SPD}}$ with a probability of $50\\%$ . Furthermore, we align the conformation of the noised molecule, denoted as rnoised_coor, with that of the raw molecule, denoted as $r_{\\mathrm{coor}}$ , using the Kabsch algorithm. ", "page_idx": 4}, {"type": "text", "text": "In contrast to Uni-Mol, Uni-Mol2 employs the position prediction head to predict the atom coordinates $r_{\\mathrm{pcoor}}$ of molecules. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{p o s}=\\mathrm{Dis}(r_{\\mathrm{noised\\_coor}})}\\\\ &{Q_{p o s}=\\mathrm{FFN}(\\mathrm{LN}(x^{N})),K_{p o s}=\\mathrm{FFN}(\\mathrm{LN}(x^{N}))}\\\\ &{V_{p o s}=\\mathrm{FFN}(\\mathrm{LN}(x^{N})),B_{p o s}=\\mathrm{FFN}(\\mathrm{LN}(p^{N}))}\\\\ &{a t t n_{p o s}=\\mathrm{softmax}(Q_{p o s}K_{p o s}^{T}+B_{p o s})\\circ\\Delta_{p o s}}\\\\ &{~\\Delta_{v p o s}=a t t n_{p o s}V_{p o s},\\Delta_{p p o s}=\\mathrm{FFN}(\\Delta_{v p o s})}\\\\ &{~r_{\\mathrm{poor}}=r_{\\mathrm{noised\\_coor}}+\\Delta_{p p o s}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $D i s$ denotes element-wise subtraction of positions between different noised atoms rnoised_coor. Specifically, the difference in position between atoms $i$ and $j$ is given by $\\Delta_{p o s}(i,j)=r_{\\mathrm{noised\\_coor},i}-$ $r_{\\mathrm{noised\\_coor},j}$ . And $\\circ$ denotes Hadamard product. $L N$ denotes layer normalization. $F F N$ denotes a feed-forward network. In practice, we use multi-head attention; for simplicity in writing, we omitted the notation related to heads here. Once the predicted coordinates $r_{\\mathrm{pcoor}}$ are obtained, the predicted pair-distance $r_{\\mathrm{pdistance}}$ can be derived by calculating the Euclidean distances between each pair of $r_{\\mathrm{pcoor}}$ . We integrated coordinate prediction and pair-distance prediction with $\\ell_{1}$ loss into Uni-Mol2\u2019s optimization process for the coordinate denoising task: ", "page_idx": 4}, {"type": "table", "img_path": "64V40K2fDv/tmp/9fa3407afae342660376e154794386998f71e066f2cbfb52be187f3d0cd1c87d.jpg", "table_caption": ["Table 2: Architecture of Uni-Mol2 at different scale "], "table_footnote": [], "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{\\mathrm{coor}}=||r_{\\mathrm{pcoor}}-r_{\\mathrm{coor}}||_{1},~~~~~~~~~~~~~}\\\\ {\\mathcal{L}_{\\mathrm{distance}}=||r_{\\mathrm{pdistance}}-r_{\\mathrm{distance}}||_{1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We eliminated two stabilizing regularization terms from the Uni-Mol model, yielding the final loss of Uni-Mol2: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{total}}=\\mathcal{L}_{\\mathrm{atom}}+\\mathcal{L}_{\\mathrm{coor}}+\\mathcal{L}_{\\mathrm{distance}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "3.3 Hyperparameter and Training Details ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We study the scalability of Uni-Mol with the scale from 42M to 1.1B, and all the parameters for Uni-Mol2 at different scales are listed in Table 2. And Uni-Mol2 is trained with AdamW optimzer[37, 38], with the following hyper-parameters: $\\beta1=0.9$ and $\\beta2=0.99$ and weight decay $1e-4$ . The gradient clip norm is set to 1.0 for training stability. The learning rate scheduler employed is a polynomial decay scheduler during pretraining. Specifically, all models reach its maximum learning rate value $1e-4$ after 100,000 warm-up steps and decay the learning rate of each parameter group using a polynomial function with power 1.0. All the models are trained with mix-precision[39] for training efficiency. ", "page_idx": 5}, {"type": "text", "text": "Using the temperature-based sampling method outlined in Equation 1, we sample 838 million conformations as training samples from the dataset. All models were subsequently trained on these 838 million samples. All these conformations were generated using the ETKGD method [40] and optimized with the Merck Molecular Force Field (MMFF) [41] in RDKit. For models containing parameters ranging from 42M to 310M, we employed 32 NVIDIA A100 GPU cards, while for models with 570M and 1.1B parameters, we utilized 64 NVIDIA A100 GPU cards. Details on pretrain time complexity across varying sizes are provided in Appendix 12. ", "page_idx": 5}, {"type": "text", "text": "4 Scaling Laws ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Several studies[15, 42, 43] on large language models (LLMs) investigate the power-law connections between model performance, commonly assessed by validation or test loss, and factors such as the number of model parameters, dataset size, and compute budget. Here, we aim to define the power-law of validation loss $\\mathcal{L}$ during the model\u2019s convergence period. In Figure 3, we present the validation loss of Uni-Mol2 models with parameter counts varying from 42 million to 1.1 billion during the training process. We mainly examine the impact factors of three aspects: data scale $N$ , model scale $M$ , and compute budget scale $C$ . Given that a constant batch size $B$ of 1024 is maintained for Uni-Mol2 across various scales, the number of training steps $S$ is considered as a suitable proxy for $D$ , as $D$ can be approximated by the product $B S$ . ", "page_idx": 5}, {"type": "text", "text": "We initially designed a power term for $M$ and $S$ separately. Additionally, we approximate the computed budget $C$ as $M S$ . Notably, we have neglected the intricate relationship between actual computing costs $C$ and $M S$ , instead subsuming it into the parameter estimation. Adhering to the design principles of [42], the loss function $\\mathcal{L}(M,D)$ should exhibit scale invariance, limit consistency, and analyticity to ensure stability and consistency across varying parameters. As a result, we derived the following empirical power-law relationship: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}(M,S,C)=\\alpha_{m}M^{\\beta_{m}}+\\alpha_{s}S^{\\beta_{s}}+\\alpha_{c}C^{\\beta_{c}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "image", "img_path": "64V40K2fDv/tmp/cebe78c03f878b377ac45732fae62266467d51c3d8ff3141be93885edd7a861d.jpg", "img_caption": ["Figure 3: Validation loss curves. Training curves for Uni-Mol2 model from 42M to 1.1B parameters. Models are trained on 0.8B samples. At the convergence stage, the 84M parameters model has a loss of 0.105, and the 1.1B parameters model reaches a loss of 0.087. "], "img_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "64V40K2fDv/tmp/0f33af63ace5b531cd0f2e9be32d73d2b9fd78a77b8935c8d7876fac582abf49.jpg", "img_caption": ["Figure 4: Graph of actual loss and prediction loss across different updates for the 570M (left) and 1.1B (right) models "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "We established the relationship based on the validation loss trajectory of Uni-Mol2 across different scales, as detailed in Table 2. Specifically, we utilized the validation data from Uni-Mol2 42M, 84M, 164M, and 310M, recording the validation loss every 10,000 training steps. Furthermore, to prevent the performance during the transient period from affecting the parameter estimation, we excluded the loss of information from the first 200,000 training steps. Consequently, we have: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}(M,S,C)=2.660M^{-1.137}+1.848S^{-0.225}+0.588C^{-1.479}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "As shown in Fig 4, equation 11 ftis the actual validation loss well for Uni-Mol2 570M and Uni-Mol2 1.1B parameters model, particularly when the model\u2019s performance reaches convergence. To assess the scaling law\u2019s effectiveness, we calculated Relative Mean Absolute Error (RMAE), Mean Square Error (MSE), R-squared, and Pearson Correlation Coefficient by comparing predicted validation loss with actual validation loss over the last 100,000 steps for Uni-Mol2 570M and Uni-Mol2 1.1B on Table 3. The high Pearson Correlation Coefficient and R-squared we computed indicate a strong linear relationship between our predicted values and the actual data. The RMAE values for Uni-Mol2 570M and Uni-Mol2 1.1B are 0.0169 and 0.0095, respectively, indicating that Equation 11 accurately models the loss curve. Specifically, for the Uni-Mol2 570M at 810,000 steps, the actual validation loss was recorded at 0.09, compared to a predicted loss of 0.088, yielding a predicted validation error of $2.22\\%$ . Meanwhile, for Uni-Mol2 1.1B at the same step, the actual validation loss stood at 0.087, slightly below the forecast of 0.0871, with a prediction error of $0.23\\%$ . ", "page_idx": 6}, {"type": "text", "text": "5 Downstream Experiment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Upon pretraining with extensive unlabeled datasets using the predefined task, one should acquire a highly accurate molecular representation for fine-tuning downstream tasks. In this section, we conduct experiments on the ability of scaled models on downstream tasks. ", "page_idx": 6}, {"type": "table", "img_path": "64V40K2fDv/tmp/b7095bb781cf6abb3c8874328ea221c989df7d71f611f5d64c887763975af1fb.jpg", "table_caption": ["Table 3: Metrics about Scaling Law for Uni-Mol2 "], "table_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "64V40K2fDv/tmp/207c0c26e75e64b548b000aa2740cad54f3740937866fa46429038789b6d5cd0.jpg", "table_caption": ["Table 4: Mean absolute error(MAE, $\\downarrow$ results on QM9 Dataset "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "5.1 QM9 Dataset ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We employ QM9 [44, 45] datasets to evaluate the performance of the molecular pretraining model at different scales and compare Uni-Mol2 with representative existing methods. QM9 dataset provides the geometric, energetic, electronic, and thermodynamic properties of the molecule, comprising 134 thousand stable organic molecules with up to nine heavy atoms. Due to QM9 containing several quantum mechanical properties with different quantitative ranges, each property is treated as a separate task. However, the HOMO, LUMO, and HOMO-LUMO GAP, which share similar ranges, are trained together as a single task for simplicity [6]. ", "page_idx": 7}, {"type": "text", "text": "Baselines We evaluate Uni-Mol2 against several baseline models, with a primary emphasis on pretraining baselines. Given that Uni-Mol demonstrates superior performance compared to these baselines in previous work [11], our analysis concentrates on the comparison between Uni-Mol and Uni-Mol2, specifically examining the scalability of Uni-Mol2 at various scales. It is noted that we have shifted the dataset partitioning method from scaffold-based partitioning to scaffold similaritybased partitioning, thereby increasing the task difficulty to evaluate the model\u2019s performance more comprehensively. The dataset is then divided into training, validation, and test sets in proportions of $80\\%$ , $10\\%$ , and $10\\%$ , respectively. Following previous work [6, 11], we report the mean and standard deviation by the results of 3 random seeds. ", "page_idx": 7}, {"type": "text", "text": "Results The results are presented comprehensively in Table 4, where the best results are marked in bold. Uni-Mol still outperforms baselines on almost all downstream datasets. Uni-Mol2 outperforms Uni-Mol in four out of the six tasks examined. But as the model parameters increase, Uni-Mol2 demonstrates significantly improved performance, surpassing Uni-Mol across all tasks at the 1.1 billion parameter level, achieving an average $27\\%$ improvement on the QM9 task for all properties. We systematically investigate the scaling of Uni-Mol2 across parameter sizes ranging from 84 million to 1.1 billion. Except for the $C_{v}$ property prediction task, the results for other properties progressively improve as the model size increases, consistent with the patterns observed in the model\u2019s validation performance. This indicates that enlarging the model consistently enhances downstream performance. However, for properties such as HOMO, LUMO, HOMO-LUMO GAP, and ZPVE, the results converge as the model size increases. This convergence suggests that further increases no longer influence the performance ceiling for these tasks in model size. ", "page_idx": 7}, {"type": "text", "text": "5.2 COMPAS-1D Dataset ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Due to the QM9 dataset only providing the conformation, some molecules failed to generate the atom and bond feature correctly. Therefore, fine-tuning Uni-Mol2 on the existing QM9 dataset to evaluate its effectiveness with bond and edge features presents a non-trivial challenge. To further validate the performance and generalization capabilities of the Uni-Mol2 pretraining model, we utilized COMPAS-1D from COMPAS project [46]. COMPAS-1D offers essential computational properties crucial for comprehending the behaviour of polycyclic aromatic hydrocarbons and other organic molecules across various chemical and physical processes. Modeling the relationships of these properties has significant implications for the field of organic photoelectric materials. ", "page_idx": 7}, {"type": "table", "img_path": "64V40K2fDv/tmp/dab61f5bfc5687507fc144cb7941482c1ecc788a18eaa9e7b9d8daac4349fe1f.jpg", "table_caption": ["Table 5: Mean absolute error(MAE, $\\downarrow$ ) results on COMPAS-1D Dataset. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "64V40K2fDv/tmp/45a9c1c5249a80bbe2431d04ea7052acd5194148af9f77c9877d3fa6fc3d8bd8.jpg", "table_caption": ["Table 6: Mean absolute error(MAE, \u2193) about HOMO-LUMO GAP on QM9 Dataset "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "We still follow the QM9 scaffold similarity-based partition and split it by a ratio of 8:1:1 into the train, validation, and test sets. Table 5 presents the predictive capabilities of Uni-Mol2 regarding photoelectric quantum properties. The model with $\\star$ suffix indicates that they incorporate atom and bond features. The results indicate that Uni-Mol2 excels in all tasks except for aEA property prediction task. Additionally, consistent with findings from the QM9 dataset, Uni-Mol2 demonstrates superior performance across all tasks as the model scales up. The results also show that under the same parameter scale, models incorporating atom and bond features outperform those without these features. Uni-Mol2 1B achieves $4\\%$ improvement over Uni-Mol, while Uni-Mol2 1B with atom and bond feature achieves $14\\%$ improvement over Uni-Mol. This suggests that, in certain scenarios, these features consistently provide a significant advantage. ", "page_idx": 8}, {"type": "text", "text": "5.3 The Performance on Limited QM9 Dataset ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In numerous fields like bio-medicine, acquiring extensive well-annotated molecular data is often expensive and time-consuming. Typically, these datasets include only a limited quantity of data[47, 48]. To evaluate the performance of Uni-Mol2 with restricted data availability, we conducted sampling on the QM9 dataset. We sampled the training set by stratifying it according to the quantile binning of the HOMO-LUMO GAP label from the QM9 test set and then created subsets named train50, train100, and train200 by sampling at $50\\%$ , $100\\%$ , and $200\\%$ of the test set size, respectively. ", "page_idx": 8}, {"type": "text", "text": "We enhanced Uni-Mol2 from 84M to 1.1B parameters using train50, train100, and train200 datasets to predict HOMO, LUMO, and GAP properties on the QM9 test dataset. As illustrated in Table 6, two conclusions emerge from the MAE for predicting HOMO, LUMO, and HOMO-LUMO GAP on the QM9 test set. First, the model\u2019s performance, indicated by a decreasing MAE, progressively improves as the training dataset expands. This is evident from comparing the MAE values between the train50 and train200 rows across different scales of the Uni-Mol2 models. For example, the Uni-Mol2 84M model shows a reduction in MAE from 0.0062 to 0.0046, marking a $25.8\\%$ decrease as the dataset grows from 50 to 200 instances. Secondly, in situations where training data is scarce, the larger Uni-Mol2 models demonstrate enhanced predictive capabilities. This is evidenced by the fact that the Uni-Mol2 1.1B parameters model, which has the largest parameters, consistently records the lowest MAE scores for all sizes of training sets. This is especially apparent in the train50 scenario, where it achieves an MAE of 0.0056, marking the best performance among the models discussed. These results highlight the advantages of enlarging both the training dataset and the model scale to improve predictive accuracy in downstream finetuning tasks with Uni-Mol2. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, to fully investigate the scaling law in the molecular pretraining field, we construct a diverse dataset of molecular structures spanning 884 million instances and present a novel molecular pretraining model Uni-Mol2. We successfully scale the model size to 1.1 billion parameters from 84 million parameters and characterize the power-law relationship between validation loss and model size, dataset size, and computational resources. By empowering the power-law relationship of UniMol2, it can shed light on the performance of the larger model. Our largest 1.1B parameters model also outperforms the existing methods. ", "page_idx": 9}, {"type": "text", "text": "The scaling law paves the way for exploring larger models to achieve higher performance. We hope that our work can open avenues for further exploration of the foundational molecular pretraining model. While larger models yield substantial beneftis, there are still several potential future directions. Firstly, beyond property prediction tasks, it is also worthwhile to explore whether the representation can be effectively utilized to enhance generative tasks. Secondly, even though the Uni-Mol2 has shown excellent results in several domains by increasing model capacity, it remains to be explored whether the advantages of scaling are beneficial for a broader range of tasks. Thirdly, the current mainstream large language models (LLMs) are predominantly based on a decode-only architecture. It is worth investigating whether there are more elegant decode-only architectures for molecular pre-training models. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Minjian Yang et al. \u201cMachine learning models based on molecular fingerprints and an extreme gradient boosting method lead to the discovery of JAK2 inhibitors\u201d. In: Journal of Chemical Information and Modeling 59.12 (2019), pp. 5002\u20135012.   \n[2] David Rogers and Mathew Hahn. \u201cExtended-connectivity fingerprints\u201d. In: Journal of chemical information and modeling 50.5 (2010), pp. 742\u2013754.   \n[3] Sheng Wang et al. \u201cSmiles-bert: large scale unsupervised pre-training for molecular property prediction\u201d. In: Proceedings of the 10th ACM international conference on bioinformatics, computational biology and health informatics. 2019, pp. 429\u2013436. [4] Zheng Xu et al. \u201cSeq2seq fingerprint: An unsupervised deep molecular embedding for drug discovery\u201d. In: Proceedings of the 8th ACM international conference on bioinformatics, computational biology, and health informatics. 2017, pp. 285\u2013294. [5] Robin Winter et al. \u201cLearning continuous and data-driven molecular descriptors by translating equivalent chemical representations\u201d. In: Chemical science 10.6 (2019), pp. 1692\u20131701. [6] Xiaomin Fang et al. \u201cGeometry-enhanced molecular representation learning for property prediction\u201d. In: Nature Machine Intelligence 4.2 (2022), pp. 127\u2013134. [7] Yu Rong et al. \u201cSelf-supervised graph transformer on large-scale molecular data\u201d. In: Advances in neural information processing systems 33 (2020), pp. 12559\u201312571. [8] Yuyang Wang et al. \u201cMolecular contrastive learning of representations via graph neural networks\u201d. In: Nature Machine Intelligence 4.3 (2022), pp. 279\u2013287. [9] Hannes St\u00e4rk et al. \u201c3d infomax improves gnns for molecular property prediction\u201d. In: International Conference on Machine Learning. PMLR. 2022, pp. 20479\u201320502.   \n[10] Shengchao Liu et al. \u201cPre-training molecular graph representation with 3d geometry\u201d. In: arXiv preprint arXiv:2110.07728 (2021).   \n[11] Gengmo Zhou et al. \u201cUni-Mol: A Universal 3D Molecular Representation Learning Framework\u201d. In: The Eleventh International Conference on Learning Representations. 2023. URL: https://openreview. net/forum?id $=$ 6K2RM6wVqKu.   \n[12] Alec Radford et al. \u201cLanguage models are unsupervised multitask learners\u201d. In: OpenAI blog 1.8 (2019), p. 9.   \n[13] Tom Brown et al. \u201cLanguage models are few-shot learners\u201d. In: Advances in neural information processing systems 33 (2020), pp. 1877\u20131901.   \n[14] Josh Achiam et al. \u201cGpt-4 technical report\u201d. In: arXiv preprint arXiv:2303.08774 (2023).   \n[15] DeepSeek-AI. \u201cDeepSeek LLM: Scaling Open-Source Language Models with Longtermism\u201d. In: arXiv preprint arXiv:2401.02954 (2024). URL: https://github.com/deepseek-ai/DeepSeek-LLM.   \n[16] Hugo Touvron et al. \u201cLlama: Open and efficient foundation language models\u201d. In: arXiv preprint arXiv:2302.13971 (2023).   \n[17] Haotian Liu et al. \u201cVisual instruction tuning\u201d. In: Advances in neural information processing systems 36 (2024).   \n[18] Zhe Chen et al. \u201cInternvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks\u201d. In: arXiv preprint arXiv:2312.14238 (2023).   \n[19] Dingshuo Chen et al. \u201cUncovering neural scaling laws in molecular representation learning\u201d. In: Advances in Neural Information Processing Systems 36 (2024).   \n[20] Keyulu Xu et al. \u201cHow powerful are graph neural networks?\u201d In: arXiv preprint arXiv:1810.00826 (2018).   \n[21] Kristof Sch\u00fctt et al. \u201cSchnet: A continuous-filter convolutional neural network for modeling quantum interactions\u201d. In: Advances in neural information processing systems 30 (2017).   \n[22] Zaixi Zhang et al. \u201cMotif-based graph self-supervised learning for molecular property prediction\u201d. In: Advances in Neural Information Processing Systems 34 (2021), pp. 15870\u201315882.   \n[23] Yi Liu et al. \u201cSpherical message passing for 3d molecular graphs\u201d. In: International Conference on Learning Representations (ICLR). 2022.   \n[24] Zhichun Guo et al. \u201cGraph-based molecular representation learning\u201d. In: arXiv preprint arXiv:2207.04869 (2022).   \n[25] Jinze Bai et al. \u201cQwen technical report\u201d. In: arXiv preprint arXiv:2309.16609 (2023).   \n[26] Albert Q Jiang et al. \u201cMistral 7B\u201d. In: arXiv preprint arXiv:2310.06825 (2023).   \n[27] Xiaohua Zhai et al. \u201cScaling vision transformers\u201d. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022, pp. 12104\u201312113.   \n[28] Mostafa Dehghani et al. \u201cScaling vision transformers to 22 billion parameters\u201d. In: International Conference on Machine Learning. PMLR. 2023, pp. 7480\u20137512.   \n[29] OpenAI. Sora: Creating video from text. 2024. URL: https://openai.com/sora.   \n[30] Yixin Liu et al. \u201cSora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models\u201d. In: arXiv preprint arXiv:2402.17177 (2024).   \n[31] John J Irwin et al. \u201cZINC20\u2014a free ultralarge-scale chemical database for ligand discovery\u201d. In: Journal of chemical information and modeling 60.12 (2020), pp. 6065\u20136073.   \n[32] Alexis Conneau and Guillaume Lample. \u201cCross-lingual language model pretraining\u201d. In: Advances in neural information processing systems 32 (2019).   \n[33] Hyung Won Chung et al. \u201cUniMax: Fairer and More Effective Language Sampling for Large-Scale Multilingual Pretraining\u201d. In: The Eleventh International Conference on Learning Representations. 2023. URL: https://openreview.net/forum?id=kXwdL1cWOAi.   \n[34] Shuqi Lu et al. Highly Accurate Quantum Chemical Property Prediction with Uni-Mol+. 2023. arXiv: 2303.16982 [physics.chem-ph].   \n[35] Chengxuan Ying et al. \u201cDo transformers really perform badly for graph representation?\u201d In: Advances in neural information processing systems 34 (2021), pp. 28877\u201328888.   \n[36] Yu Shi et al. \u201cBenchmarking graphormer on large-scale molecular modeling datasets\u201d. In: arXiv preprint arXiv:2203.04810 (2022).   \n[37] Diederik P Kingma and Jimmy Ba. \u201cAdam: A method for stochastic optimization\u201d. In: arXiv preprint arXiv:1412.6980 (2014).   \n[38] Ilya Loshchilov and Frank Hutter. \u201cDecoupled weight decay regularization\u201d. In: arXiv preprint arXiv:1711.05101 (2017).   \n[39] Paulius Micikevicius et al. \u201cMixed precision training\u201d. In: arXiv preprint arXiv:1710.03740 (2017).   \n[40] Sereina Riniker and Gregory A Landrum. \u201cBetter informed distance geometry: using what we know to improve conformation generation\u201d. In: Journal of chemical information and modeling 55.12 (2015), pp. 2562\u20132574.   \n[41] Thomas A Halgren. \u201cMerck molecular force field. I. Basis, form, scope, parameterization, and performance of MMFF94\u201d. In: Journal of computational chemistry 17.5-6 (1996), pp. 490\u2013519.   \n[42] Jared Kaplan et al. \u201cScaling laws for neural language models\u201d. In: arXiv preprint arXiv:2001.08361 (2020).   \n[43] Hui Su et al. Unraveling the Mystery of Scaling Laws: Part I. 2024. arXiv: 2403.06563 [cs.LG].   \n[44] Raghunathan Ramakrishnan et al. \u201cQuantum chemistry structures and properties of 134 kilo molecules\u201d. In: Scientific data 1.1 (2014), pp. 1\u20137.   \n[45] Zhenqin Wu et al. \u201cMoleculeNet: a benchmark for molecular machine learning\u201d. In: Chemical science 9.2 (2018), pp. 513\u2013530.   \n[46] Alexandra Wahab et al. \u201cThe compas project: A computational database of polycyclic aromatic systems. phase 1: cata-condensed polybenzenoid hydrocarbons\u201d. In: Journal of Chemical Information and Modeling 62.16 (2022), pp. 3704\u20133713.   \n[47] Keith T Butler et al. \u201cMachine learning for molecular and materials science\u201d. In: Nature 559.7715 (2018), pp. 547\u2013555.   \n[48] Han Li et al. \u201cImproving molecular property prediction through a task similarity enhanced transfer learning strategy\u201d. In: Iscience 25.10 (2022).   \n[49] Stefan Hougardy. \u201cThe Floyd\u2013Warshall algorithm on graphs with negative cycles\u201d. In: Information Processing Letters 110.8-9 (2010), pp. 279\u2013281.   \n[50] Uni-Core. Uni-Core, an efficient distributed PyTorch framework. 2024. URL: https://github.com/ dptech-corp/Uni-Core.   \n[51] Jason Ansel et al. \u201cPyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation\u201d. In: Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2. 2024, pp. 929\u2013947.   \n[52] Ashish Vaswani et al. \u201cAttention is all you need\u201d. In: Advances in neural information processing systems 30 (2017). ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Implementation Details ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Dataset Description ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "QM9 Dataset The QM9 dataset [44] is a significant resource in the field of quantum chemistry, offering a single equilibrium conformation and 12 labels that include geometric, energetic, electronic, and thermodynamic properties. For the purpose of performance evaluation, we select the following properties: HOMO, LUMO, gap, alpha, $C_{v}$ , mu, $R^{\\dot{2}}$ , and ZPVE. The details of the properties are as follows: ", "page_idx": 12}, {"type": "text", "text": "\u2022 HOMO The HOMO (Highest Occupied Molecular Orbital) is the highest energy molecular orbital that is occupied by electrons in a molecule.   \n\u2022 LUMO The LUMO (Lowest Unoccupied Molecular Orbital) is the lowest energy molecular orbital that is not occupied by electrons.   \n\u2022 gap The gap, often referred to as the HOMO-LUMO gap, is the energy difference between the HOMO and LUMO. It is a measure of the energy required to excite an electron from the HOMO to the LUMO.   \n\u2022 ZPVE ZPVE (Zero-Point Vibrational Energy) is the energy associated with the vibrational motion of atoms in a molecule at absolute zero temperature.   \n\u2022 $\\alpha$ The $\\alpha$ value represents the static polarizability of a molecule.   \n\u2022 $C_{v}$ The $C_{v}$ (Heat Capacity at Constant Volume) is the amount of heat needed to raise the temperature of a given amount of substance by one degree Celsius at constant volume.   \n\u2022 $\\mu$ The $\\mu$ (Dipole Moment) is the measure of the molecule\u2019s permanent electric dipole moment.   \n\u2022 $R^{2}$ The $R^{2}$ (Electronic Spatial Extent) is defined as the expectation value of the square of the electronic distance from the nucleus. ", "page_idx": 12}, {"type": "text", "text": "COMPAS-1D Dataset The COMPAS-1D dataset is a part of the COMPAS Project, which is an acronym for the computational Database of Polycyclic Aromatic Systems. The dataset is specifically focused on data-condensed poly-benzenoid hydrocarbons, which are a type of polycyclic aromatic hydrocarbons (PAHs) with a unique structure where the benzene rings are connected edge-to-edge. The COMPAS-1D [46] contains 8,678 molecules and offers essential computational properties crucial for comprehending the behavior of polycyclic aromatic hydrocarbons and other organic molecules across various chemical and physical processes. The details of the properties used in the downstream tasks are as follows: ", "page_idx": 12}, {"type": "text", "text": "\u2022 aEA aEA (Adiabatic Electron Affinity) measures the tendency of a molecule to gain an electron.   \n\u2022 aIp aIP (Adiabatic Ionization Potential) measures the energy required for a molecule to lose an electron.   \n\u2022 Dispersion Dispersion describes weak inter-molecular forces important for understanding molecular interactions.   \n\u2022 Dipmom Debye Dipmom in Debye indicates the polarity of a molecule, affecting its interactions and solubility. ", "page_idx": 12}, {"type": "text", "text": "A.2 Atom and Bond Feature for Molecules ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The molecular feature used in Uni-Mol2 contains two parts: 1) Atom and bond features, we use RDkit to generate these atom and bond features as input of Uni-Mol2. The detailed features are listed in Table 7 and Table 8. 2) Shortest path $\\mathrm{SPD}_{i,j}$ . We employ the Floyd-Warshall algorithm[49] to calculate the shortest distances between each pair of connected atoms. ", "page_idx": 12}, {"type": "text", "text": "A.3 Hyperparameter Settings ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In line with previous methods, we employ grid search to find the optimal hyper-parameters for tasks within the QM9 and COMPAS-1D datasets. The specific hyper-parameters are detailed in Table 9. In all experiments, we select the checkpoint with the lowest validation loss and report the corresponding test set results based on that checkpoint. For the COMPAS-1D dataset, experiments were conducted using a single A100 GPU, whereas for the QM9 dataset, the experiments were run on eight A100 GPUs. ", "page_idx": 12}, {"type": "table", "img_path": "64V40K2fDv/tmp/92e4b95103cb93105c970cbd7a2d05eec425a9a95d47197b5080ffb0e84952dc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "table", "img_path": "64V40K2fDv/tmp/44d28266154dd5f7e0d69576be9cdf8c9cfb56d4ce53335c7a9c255a4e2ca8cd.jpg", "table_caption": ["Table 8: Bond features "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "table", "img_path": "64V40K2fDv/tmp/4f49f1498273024aa8ccefa08cdc9091f90a8225f0f5b6226ddf83340ff2203f.jpg", "table_caption": ["Table 9: Hyper-paramters for fine-tuning on QM9 and COMPAS-1D Dataset "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "A.4 Evaluation Metrics ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Diverse evaluation metrics can better help us understand and evaluate the effectiveness of our model. In this section, we introduce the evaluation metrics used in this study. Given $n$ samples, where $y_{i}$ is the actual value and $\\hat{y}_{i}$ is the predicted value. ", "page_idx": 13}, {"type": "text", "text": "Mean Absolute Error (MAE) calculates the average of the absolute differences between predicted and actual values in regression tasks, treating errors of different scales equally. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{MAE}=\\frac{1}{n}\\sum_{i=1}^{n}\\left|\\hat{y}_{i}-y_{i}\\right|\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Relative Mean Absolute Error (RMAE) measures the average absolute prediction error relative to the actual values, providing a dimensionless indication of model accuracy. By normalizing with the actual values, it removes the effect of the data scale, making it possible to compare data with different scales. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{RMAE}=\\frac{1}{n}\\sum_{i=1}^{n}\\frac{|\\hat{y}_{i}-y_{i}|}{|y_{i}|}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Mean Square Error (MSE) calculates the average of the squared differences between predicted and actual values, heavily penalizing larger errors. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{MSE}=\\frac{\\sum_{i=1}^{n}(\\hat{y}_{i}-y_{i})^{2}}{n}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "R-squared measures the proportion of variance in the dependent variable that can be predicted by the independent variables, highlighting the goodness of fit for a regression model. A higher $\\mathbf{R}$ -squared indicates that the independent variables explain a significant portion of the variance in the dependent variable, while a lower R-squared indicates that the model explains less. ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{R-squared}=1-\\frac{\\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^{2}}{\\sum_{i=1}^{n}(y_{i}-\\bar{y}_{i})^{2}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The Pearson Correlation Coefficient (r) measures the linear correlation between two variables, ranging from -1 to 1. Larger absolute values signify a stronger linear relationship between the two variables, while values near 0 indicate a weak or non-existent linear relationship. ", "page_idx": 14}, {"type": "equation", "text": "$$\nr={\\frac{\\sum_{i=1}^{n}(x_{i}-{\\bar{x}})(y_{i}-{\\bar{y}})}{\\sqrt{\\sum_{i=1}^{n}(x_{i}-{\\bar{x}})^{2}\\sum_{i=1}^{n}(y_{i}-{\\bar{y}})^{2}}}}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "B Extra QM9 Property and ADME Experiment ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Additional QM9 Property Experiment Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We have undertaken additional experiments utilizing the QM9 dataset to assess molecular properties 10. Our findings indicate that the model\u2019s performance consistently improves with an increase in model size. This scalability suggests that employing larger models will further enhance the accuracy and reliability of molecular property predictions, thereby increasing the utility of our approach for practical applications in the field. ", "page_idx": 14}, {"type": "table", "img_path": "64V40K2fDv/tmp/463a2d0ffc026c8639d9bbccfdce15d2c60e617943b9a52de731ed24e1c92d35.jpg", "table_caption": ["Table 10: Mean absolute error(MAE, $\\downarrow$ ) results on QM9 Dataset "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "B.2 Biogen ADME Dataset ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The Biogen ADME dataset focuses on the evaluation of drug metabolism and pharmacokinetics (DMPK) properties, specifically assessing absorption, distribution, metabolism, and excretion (ADME) characteristics of potential drug candidates. Table 11 illustrates the predictive performance of the Uni-Mol2 model regarding these ADME properties. ", "page_idx": 14}, {"type": "table", "img_path": "64V40K2fDv/tmp/8fb49845a7519a18757569bf92e04145d05908f2f949372972b4cc511b660b86.jpg", "table_caption": ["Table 11: Mean absolute error(MAE, $\\downarrow$ ) results on Biogen ADME Dataset "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "C Infrastructures ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We utilize an efficient distributed PyTorch framework called Uni-Core [50], specifically designed for swiftly developing high-performance PyTorch models [51], particularly those based on Transformer architectures[52]. Given the variability in molecule lengths, padding inputs to match the maximum molecular length is necessary during training. Consequently, the batch size for model training is influenced by the longest molecule in each batch. However, since molecule lengths follow a longtail distribution (with the majority falling within a specific range), we employ dynamic batching techniques to enhance GPU utilization. By adjusting batch sizes according to the maximum lengths of different batches, we can significantly boost GPU utilization with minimal effort. ", "page_idx": 14}, {"type": "text", "text": "The time consumption of reading data from distributed storage is often overlooked. We employ a singular, dedicated process on each computational node to asynchronously replicate the training dataset of each epoch onto the host machine. This strategy effectively mitigates time overheads, thereby obscuring the duration spent on data reading from distributed storage. To resume the corruption due to the infra and other factors effectively, we save model weight and optimizer state for every 1k step asynchronously. This means we will lose 1k step training resources in the worst case of hardware instability or loss spike during training. Meanwhile, any checkpoints exceeding the most recent ten files will be deleted to avoid consuming too much storage space. ", "page_idx": 15}, {"type": "text", "text": "D Pretrain Time Complexity and GPU Resources ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We utilized a computational cluster comprising 64 NVIDIA A100 GPUs, each equipped with 80GB of HBM2 memory. The GPUs were interconnected via a high-speed Nvidia Infiniband fabric, offering 400 Gbps bandwidth for inter-GPU communication. The details of each model size are listed in table 12. ", "page_idx": 15}, {"type": "table", "img_path": "64V40K2fDv/tmp/4a46cc7b3fa70ea8b4adade6036cd44962aaeb9a07ac7620fef5e8659e46431e.jpg", "table_caption": ["Table 12: Training Time of Uni-Mol2 at different scale "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "E Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The major limitation of our study pertains to the absence of an exploration of the optimal batch size and learning rate. Our investigation primarily focuses on analyzing and delineating the power-law relationships among validation loss, model size, dataset size, and computational resources. The predictive accuracy of performance aligns well with the scaling curve, indicating that the current optimal learning rate and batch size approximate the near-optimal values. However, existing research suggests a progressive increase in the optimal batch size with augmented computing resources, while the optimal learning rate tends to decrease gradually. It is necessary to note that as we further increase the model\u2019s parameters, the final optimal values for learning rate and batch size may fall outside the currently identified range. Consequently, investigating the scaling law for optimal batch size and learning rate is also paramount. ", "page_idx": 15}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 16}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 16}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] . ", "page_idx": 16}, {"type": "text", "text": "\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available. \u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 16}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 16}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 16}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 16}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 16}, {"type": "text", "text": "1. Claims ", "page_idx": 16}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The main claims articulated in the abstract and introduction accurately mirror the paper\u2019s contributions and scope. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 16}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The paper outlines the limitations of the research and discusses the future direction to explore the limitations. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 17}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: The paper does not include theoretical results. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 17}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The paper provides comprehensive details necessary for reproducing the main experimental results. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. ", "page_idx": 17}, {"type": "text", "text": "\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. ", "page_idx": 18}, {"type": "text", "text": "\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. ", "page_idx": 18}, {"type": "text", "text": "\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 18}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: The code, model, and data are made publicly available upon acceptance. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 18}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The paper meticulously details all aspects of the training and testing process. It specifies the data splits used for training, validation, and testing, along with a thorough description of the hyperparameters and their selection criteria. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: he paper appropriately reports error bars. All the results reported with the mean and standard deviation by the results of 3 random seeds. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 19}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper provided the detailed information of compute resources. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 19}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: There is no societal impact of the work performed in this paper. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 20}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 21}, {"type": "text", "text": "Guidelines:   \n\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 21}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 21}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 21}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 22}]