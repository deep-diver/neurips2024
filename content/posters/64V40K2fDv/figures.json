[{"figure_path": "64V40K2fDv/figures/figures_3_1.jpg", "caption": "Figure 1: Top: Comparison of scaffold frequency between Uni-Mol and Uni-Mol2 dataset. Bottom: Scaffolds distribution on Uni-Mol2 dataset", "description": "This figure compares the frequency distributions of the top 40 Murcko scaffolds in the Uni-Mol and Uni-Mol2 datasets. The top panel shows a bar chart comparing the counts of each scaffold in both datasets. The bottom panel presents a detailed bar chart illustrating the frequency distribution of scaffolds within the Uni-Mol2 dataset alone.", "section": "3.1 Data"}, {"figure_path": "64V40K2fDv/figures/figures_3_2.jpg", "caption": "Figure 2: Left: The overall pretraining architecture. Middle: Atom and Pair representation. Right: The details of backbone block", "description": "This figure illustrates the Uni-Mol2 model architecture. The left panel shows the overall architecture, including the two-track transformer that processes atom and pair features concurrently. The middle panel highlights how atom and pair representations are generated.  The right panel provides details of the backbone block, outlining the specific components and processing steps for atom and pair features within each block.", "section": "3.2 Architecture"}, {"figure_path": "64V40K2fDv/figures/figures_6_1.jpg", "caption": "Figure 3: Validation loss curves. Training curves for Uni-Mol2 model from 42M to 1.1B parameters. Models are trained on 0.8B samples. At the convergence stage, the 84M parameters model has a loss of 0.105, and the 1.1B parameters model reaches a loss of 0.087.", "description": "This figure displays the training curves for the Uni-Mol2 model with parameter counts varying from 42 million to 1.1 billion. The left panel shows the curves in a linear scale, highlighting the decrease in validation loss as more samples are processed and the model size increases. The right panel shows the same data using a log-log scale, making the power-law relationship between training samples and validation loss more evident. The figure also shows that at the convergence stage, the model with 84 million parameters has a validation loss of 0.105, while the 1.1 billion parameter model achieves a loss of 0.087.", "section": "4 Scaling Laws"}, {"figure_path": "64V40K2fDv/figures/figures_6_2.jpg", "caption": "Figure 4: Graph of actual loss and prediction loss across different updates for the 570M (left) and 1.1B (right) models", "description": "This figure displays two graphs illustrating the relationship between actual and predicted validation loss over the course of training for the Uni-Mol2 model with 570 million (left graph) and 1.1 billion (right graph) parameters.  The x-axis represents the number of training samples processed, and the y-axis represents the validation loss. The graphs visually compare the model's actual performance during training (blue dots) against a predicted validation loss (pink line), which was calculated using a power-law relationship developed in the paper. The graphs aim to demonstrate the accuracy of the proposed scaling law in predicting the model's validation loss during training.", "section": "4 Scaling Laws"}]