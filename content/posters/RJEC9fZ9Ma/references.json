{"references": [{"fullname_first_author": "Alex Krizhevsky", "paper_title": "Imagenet classification with deep convolutional neural networks", "publication_date": "2012-01-01", "reason": "This paper introduced AlexNet, a groundbreaking deep convolutional neural network architecture that significantly advanced the field of computer vision and deep learning."}, {"fullname_first_author": "Kaiming He", "paper_title": "Deep residual learning for image recognition", "publication_date": "2015-01-01", "reason": "This paper introduced ResNet, a deep residual network that addressed the vanishing gradient problem and enabled the training of significantly deeper and more accurate networks."}, {"fullname_first_author": "Vardan Papyan", "paper_title": "Prevalence of neural collapse during the terminal phase of deep learning training", "publication_date": "2020-01-01", "reason": "This paper first introduced the concept of neural collapse, a phenomenon observed in the final stages of deep learning training where the features and weights of the network converge to specific geometric structures."}, {"fullname_first_author": "Cong Fang", "paper_title": "Exploring deep neural networks via layer-peeled model: Minority collapse in imbalanced training", "publication_date": "2021-01-01", "reason": "This paper investigated the phenomenon of minority collapse in imbalanced datasets, showing that the classifier weights of minority classes tend to collapse together, which has implications for understanding neural collapse in real-world applications."}, {"fullname_first_author": "Dustin G Mixon", "paper_title": "Neural collapse with unconstrained features", "publication_date": "2020-01-01", "reason": "This paper analyzed neural collapse using a simplified unconstrained features model, providing theoretical insights into the phenomenon and its relationship to optimization and generalization."}]}