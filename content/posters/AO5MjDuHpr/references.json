{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021", "reason": "This paper introduces CLIP, the foundational vision-language model upon which the current work builds, making it a crucial foundational reference."}, {"fullname_first_author": "Kaiyang Zhou", "paper_title": "Learning to prompt for vision-language models", "publication_date": "2022", "reason": "This paper introduces the concept of prompt learning for vision-language models, which is directly extended and improved in the current work."}, {"fullname_first_author": "Mohammad Mahdi Derakhshani", "paper_title": "Bayesian prompt learning for image-language model generalization", "publication_date": "2023", "reason": "This paper addresses the challenge of generalizing vision-language models, a key focus of the current work, making it an important comparative study."}, {"fullname_first_author": "Adrian Bulat", "paper_title": "LASP: Text-to-text optimization for language-aware soft prompting of vision & language models", "publication_date": "2023", "reason": "This paper explores text-to-text optimization for prompt tuning, a related technique that is compared to the approach in the current work."}, {"fullname_first_author": "Muhammad Uzair Khattak", "paper_title": "Self-regulating prompts: Foundational model adaptation without forgetting", "publication_date": "2023", "reason": "This paper focuses on improving the generalization of prompt learning, a key aspect also addressed in the current work."}]}