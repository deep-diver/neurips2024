[{"figure_path": "lxuXvJSOcP/tables/tables_6_1.jpg", "caption": "Table 1: Comparison of Domain Generalization performance with existing SOTA techniques. The bold values indicate the best performance. Note that all methods are evaluated on \u2018car\u2019 category.", "description": "This table compares the proposed Unified Domain Generalization and Adaptation (UDGA) method's performance with other state-of-the-art (SOTA) domain generalization techniques across three benchmark datasets (Lyft, nuScenes, and Waymo).  It shows the performance metrics NDS*, mAP, mATE, mASE, MAOE, and Closed Gap for each method and dataset combination, focusing on the \u2018car\u2019 object category.  The bold values highlight the best performing method for each task. The Closed Gap metric provides a relative performance improvement compared to a direct transfer approach (without domain adaptation).", "section": "4. Experimental Results"}, {"figure_path": "lxuXvJSOcP/tables/tables_7_1.jpg", "caption": "Table 2: Comparison of UDGA performance on BEVDepth with various PEFT modules, SSF [50], and Adapter [48]. We construct six different target data splits from 1% to 100%. Additionally, # Params denote the number of parameters for training. Note that  represents \u2018Do not support\u2019.", "description": "This table compares the performance of the proposed Unified Domain Generalization and Adaptation (UDGA) method against other Parameter-Efficient Fine-Tuning (PEFT) methods on the BEVDepth model. It shows how the performance changes based on the amount of target data available (from 1% to 100%) and the number of parameters used for training. The table also includes the performance of an oracle model (which has access to all target data) and a baseline model that performs direct transfer from the source domain without adaptation.", "section": "4.2 Experiment Results"}, {"figure_path": "lxuXvJSOcP/tables/tables_8_1.jpg", "caption": "Table 3: Ablation studies on UDGA (10% Adaptation). B and A represents pre-trained blocks and LEDA blocks, respectively. Note that we train B and A, alternatively (i.e., pre-train and fine-tune).", "description": "This table presents the ablation study results for the Unified Domain Generalization and Adaptation (UDGA) framework. It shows the impact of different components of the UDGA framework (pre-trained blocks B, LEDA blocks A, and the loss functions) on the model's performance in terms of NDS* and mAP for two cross-domain tasks: Lyft to nuScenes and nuScenes to Lyft. The study uses 10% of the target domain data for adaptation. Each row represents a different configuration of the framework, with checkmarks indicating the inclusion of specific components. The results demonstrate the effectiveness of the UDGA components in improving the model's performance.", "section": "4.3 Ablation studies"}, {"figure_path": "lxuXvJSOcP/tables/tables_8_2.jpg", "caption": "Table 4: Ablation studies on Domain Generalization with our novel depth constraint modules,  Lov and Lp. Lidar and SS each represents LiDAR depth supervision and Self-Supervised overlap depth.", "description": "This table presents the ablation study results on the impact of the proposed depth constraint modules (Lov and Lp) on domain generalization performance. It compares the results using LiDAR depth supervision, self-supervised overlap depth, and different combinations of Lov and Lp,  along with additional external augmentation.  The performance metrics (NDS*, mAP, mATE, mASE, mAOE) are reported for two domain generalization tasks: Lyft \u2192 nuScenes and nuScenes \u2192 Lyft.", "section": "4.3 Ablation studies"}, {"figure_path": "lxuXvJSOcP/tables/tables_15_1.jpg", "caption": "Table 1: Comparison of Domain Generalization performance with existing SOTA techniques. The bold values indicate the best performance. Note that all methods are evaluated on \u2018car\u2019 category.", "description": "This table compares the proposed method's performance with state-of-the-art (SOTA) techniques in domain generalization for multi-view 3D object detection.  It shows the results on three different cross-domain scenarios (Lyft to nuScenes, nuScenes to Lyft, and Waymo to nuScenes).  The metrics used for comparison include NDS, mAP, mATE, mASE, and MAOE. The 'Oracle' and 'Direct Transfer' rows provide upper and lower bound performance, respectively. The table highlights that the proposed method significantly outperforms existing SOTA techniques in the challenging cross-domain scenarios.", "section": "4. Experiment Results"}, {"figure_path": "lxuXvJSOcP/tables/tables_16_1.jpg", "caption": "Table 1: Comparison of Domain Generalization performance with existing SOTA techniques. The bold values indicate the best performance. Note that all methods are evaluated on \u2018car\u2019 category.", "description": "This table compares the performance of the proposed Unified Domain Generalization and Adaptation (UDGA) method with other state-of-the-art (SOTA) domain generalization techniques.  The comparison is done across several benchmark datasets (Lyft, nuScenes, and Waymo) and focuses on the \u2018car\u2019 object category.  The results are presented using several metrics (NDS*, mAP, mATE, mASE, MAOE) to assess the overall performance and a \"Closed Gap\" metric, which shows the relative improvement of each method compared to a simple direct transfer approach.  The \"Oracle\" row provides an upper bound representing the ideal performance achievable with full supervision on the target domain.", "section": "4.2 Experiment Results"}, {"figure_path": "lxuXvJSOcP/tables/tables_17_1.jpg", "caption": "Table 7: Performance under CALRA-simulated domain changes. The model is trained exclusively on Source. The diff shows the Source-Target difference. The bold values indicate the worst difference.", "description": "This table presents the results of experiments conducted using CARLA simulation to evaluate the model's performance under simulated domain changes. The model was trained exclusively on the source domain data and tested on three different target domains. The \"diff\" row shows the difference in performance metrics between the source and each target domain. The bold values highlight the most significant performance drops in the target domains.", "section": "4.2 Experiment Results"}, {"figure_path": "lxuXvJSOcP/tables/tables_17_2.jpg", "caption": "Table 1: Comparison of Domain Generalization performance with existing SOTA techniques. The bold values indicate the best performance. Note that all methods are evaluated on \u2018car\u2019 category.", "description": "This table compares the performance of the proposed Unified Domain Generalization and Adaptation (UDGA) method with other state-of-the-art (SOTA) domain generalization techniques on the task of multi-view 3D object detection.  The results are presented for three different domain adaptation scenarios (Lyft \u2192 nuScenes, nuScenes \u2192 Lyft, and Waymo \u2192 nuScenes), each showing the NDS (NuScenes Detection Score), mAP (mean Average Precision), mATE (mean Average Translation Error), mASE (mean Average Scale Error), and mAOE (mean Average Orientation Error) metrics for the 'car' category.  The 'Oracle' row represents the upper bound performance achievable with full supervision on the target domain, while 'Direct Transfer' represents the lower bound performance achieved without any domain adaptation.  The 'Closed Gap' metric shows the improvement achieved by each method compared to direct transfer.", "section": "4. Experimental Results"}, {"figure_path": "lxuXvJSOcP/tables/tables_18_1.jpg", "caption": "Table 9: Performance comparison for each module (UDGA 5%). Gray highlight denotes 'Ours'.", "description": "This table presents an ablation study analyzing the contribution of each module (backbone, view transformer, BEV encoder, and detection head) to the overall performance of the UDGA method with 5% target domain adaptation.  The results are shown separately for the Lyft to nuScenes and nuScenes to Lyft domain adaptation tasks.  The gray highlighted row indicates the full UDGA model's performance, serving as a benchmark for comparison against models with individual modules removed.", "section": "4.3 Ablation studies"}, {"figure_path": "lxuXvJSOcP/tables/tables_18_2.jpg", "caption": "Table 10: Comparison with various adapter structures (UDGA 10%). Gray highlight denotes \u2018Ours\u2019.", "description": "This table presents a comparison of the performance of different adapter structures within the UDGA framework when using 10% of the target data for adaptation.  Different combinations of convolutional and linear layers were tested for the \"Project Down\" and \"Project Up\" modules of the adapter. The results are presented in terms of NDS* and mAP for both Lyft\u2192nuScenes and nuScenes\u2192Lyft domain adaptation tasks.  The table highlights that using a convolutional layer for the Project Down module and a linear layer for the Project Up module (Ours) yields the best overall performance.", "section": "4.2 Experiment Results"}, {"figure_path": "lxuXvJSOcP/tables/tables_18_3.jpg", "caption": "Table 2: Comparison of UDGA performance on BEVDepth with various PEFT modules, SSF [50], and Adapter [48]. We construct six different target data splits from 1% to 100%. Additionally, # Params denote the number of parameters for training. Note that  represents 'Do not support'.", "description": "This table compares the performance of the proposed Unified Domain Generalization and Adaptation (UDGA) method against other Parameter-Efficient Fine-Tuning (PEFT) methods such as SSF and Adapter.  It shows the impact of different amounts of target data (1% to 100%) on the performance of the model. The number of parameters used for training is also included for each method, allowing for a comparison of efficiency.", "section": "4.2 Experiment Results"}]