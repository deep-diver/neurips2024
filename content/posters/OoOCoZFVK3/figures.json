[{"figure_path": "OoOCoZFVK3/figures/figures_3_1.jpg", "caption": "Figure 1: The framework of CORY. A traditional RL fine-tuning method can be simply extended to the CORY version with only three steps. First, duplicate the LLM into two LLM agents, one acting as a pioneer and the other as an observer; second, combine the task rewards of the two LLM agents to replace the original task reward; third, periodically exchange the roles of the two LLM agents during training. After training, either the LLM agent can perform the task independently.", "description": "This figure illustrates the framework of the CORY method and compares it to traditional RL fine-tuning.  The left side shows the standard approach where a single LLM agent interacts with a reward model to generate a response.  The right side shows the CORY method which duplicates the LLM into two agents (pioneer and observer). The pioneer generates a response, which is then used by the observer to generate another response. Both responses contribute to the overall reward.  The roles of pioneer and observer are periodically exchanged during training to enhance co-evolution. This figure visually demonstrates how the CORY approach builds upon and extends traditional RL fine-tuning with multi-agent learning and role exchange.", "section": "3 Method"}, {"figure_path": "OoOCoZFVK3/figures/figures_4_1.jpg", "caption": "Figure 2: The empirical demonstration of why CORY surpasses single-agent RL fine-tuning. In (c), the values of \u03b7 from left to right are 1e-5, 1e-4, 1e-3, and 1e-2.", "description": "This figure demonstrates empirically why the proposed method CORY surpasses single-agent RL fine-tuning methods.  It illustrates the concept of a Pareto frontier in multi-objective reinforcement learning, where an ideal policy maximizes task reward while minimizing KL divergence.  The plots show that CORY achieves policies closer to the Pareto frontier than PPO, indicating better performance.", "section": "3.2 Understanding CORY"}, {"figure_path": "OoOCoZFVK3/figures/figures_5_1.jpg", "caption": "Figure 3: Training curves under subjective rewards on IMDB Review.", "description": "This figure shows the training curves for task reward, KL divergence, and combined reward during the fine-tuning of GPT-2 models on the IMDB Review dataset using both single-agent PPO and the proposed CORY method.  The results demonstrate that CORY achieves comparable task reward to PPO while maintaining significantly lower KL divergence, indicating better resistance to distribution collapse and improved policy optimality. The combined reward curves further highlight the superiority of CORY over single-agent PPO in balancing these two objectives.", "section": "4.1 Subjective Rewards on IMDB Review"}, {"figure_path": "OoOCoZFVK3/figures/figures_6_1.jpg", "caption": "Figure 3: Training curves under subjective rewards on IMDB Review.", "description": "This figure visualizes the training process of three different models (PPO-GPT-2-1, PPO-GPT-2-xl, CORY-LLM1, and CORY-LLM2) on the IMDB Review dataset using subjective rewards.  It shows three subplots: (a) Task reward, (b) KL divergence, and (c) Combined reward. The plots illustrate the performance of the models over 100 training iterations.  The combined reward combines the task reward and the KL divergence to represent a balance between performance and maintaining a distribution close to the pre-trained model. The figure helps in comparing the performance and stability of different models during the training.", "section": "4.1 Subjective Rewards on IMDB Review"}, {"figure_path": "OoOCoZFVK3/figures/figures_7_1.jpg", "caption": "Figure 5: Evaluation results on GSM8K test dataset.", "description": "This figure shows the performance comparison among Llama-2-7b-chat, PPO, and CORY on the GSM8K test dataset, using the Pass@k metric.  The x-axis represents the k value (number of attempts), and the y-axis represents the accuracy. The plot illustrates the relative performance of the three methods across various numbers of attempts, showing how the accuracy changes as the number of tries increases. CORY demonstrates superior performance compared to PPO and Llama-2-7b-chat across all k values.", "section": "4.2 Objective Rewards on GSM8K"}, {"figure_path": "OoOCoZFVK3/figures/figures_7_2.jpg", "caption": "Figure 6: Training curves for ablations experiments.", "description": "This figure presents the ablation study results, showing the impact of model size, knowledge transfer, and role exchange on the model's performance.  Three subfigures display the task reward, KL divergence, and combined reward across different ablation settings.  The results illustrate the contribution of each mechanism to the overall performance of CORY.  For instance, the results reveal that removing knowledge transfer substantially impacts task reward and increases the KL divergence while removing role exchange leads to higher KL divergence for one agent, revealing the important effect of each individual component.", "section": "4.3 Ablations"}, {"figure_path": "OoOCoZFVK3/figures/figures_19_1.jpg", "caption": "Figure 3: Training curves under subjective rewards on IMDB Review.", "description": "This figure shows the training curves for task reward, KL divergence, and combined reward during the fine-tuning process using PPO and CORY on the IMDB Review dataset.  The results demonstrate that CORY achieves comparable task reward with significantly lower KL divergence, indicating better policy optimality and resistance to distribution collapse compared to PPO. The combined reward curve further highlights CORY's superior performance in balancing task reward and KL divergence.", "section": "4.1 Subjective Rewards on IMDB Review"}, {"figure_path": "OoOCoZFVK3/figures/figures_19_2.jpg", "caption": "Figure 3: Training curves under subjective rewards on IMDB Review.", "description": "This figure shows three plots visualizing the training process of different models on the IMDB Review dataset under subjective rewards.  The plots display the task reward, KL divergence, and a combined reward (incorporating both task reward and KL divergence) over 100 training iterations.  Four different models are compared:  PPO-GPT-2-1, PPO-GPT-2-xl, CORY-LLM1, and CORY-LLM2. The plots illustrate the performance of the single-agent PPO method against CORY, highlighting CORY's better performance in terms of policy optimality, robustness, and resistance to distribution collapse.", "section": "4.1 Subjective Rewards on IMDB Review"}, {"figure_path": "OoOCoZFVK3/figures/figures_19_3.jpg", "caption": "Figure 2: The empirical demonstration of why CORY surpasses single-agent RL fine-tuning. In (c), the values of \u03b7 from left to right are 1e-5, 1e-4, 1e-3, and 1e-2.", "description": "This figure demonstrates empirically why the proposed CORY method outperforms single-agent RL fine-tuning.  It visually represents the multi-objective optimization inherent in RL fine-tuning with a KL penalty.  The Pareto frontier (optimal trade-off between task reward and KL divergence) is difficult to achieve perfectly.  The figure shows that CORY's sub-optimal frontier (the achievable policy trade-off) lies closer to the true Pareto frontier than that achieved by single-agent RL, indicating a more optimal policy and better resistance to distribution collapse. The plots (a) and (b) illustrate idealized Pareto and sub-optimal frontiers, while (c) presents the empirical results, showing CORY's consistently better performance across different KL penalty parameters (\u03b7).", "section": "3.2 Understanding CORY"}, {"figure_path": "OoOCoZFVK3/figures/figures_20_1.jpg", "caption": "Figure 2: The empirical demonstration of why CORY surpasses single-agent RL fine-tuning. In (c), the values of \u03b7 from left to right are 1e-5, 1e-4, 1e-3, and 1e-2.", "description": "This figure shows an empirical analysis of why the proposed method CORY outperforms single-agent RL fine-tuning methods. It demonstrates that RL fine-tuning with KL penalty inherently involves a multi-objective reinforcement learning problem; the LLM agent tries to maximize the task reward and minimize the KL divergence simultaneously.  Subplots (a) and (b) illustrate the Pareto frontier and the suboptimal frontier, respectively, representing the ideal and achievable trade-offs between task reward and KL divergence. Subplot (c) displays empirical results obtained by fine-tuning Llama-2-7b-chat on the GSM8K dataset using both PPO and CORY, showing that CORY's suboptimal frontier is closer to the true Pareto frontier compared to PPO's.", "section": "3.2 Understanding CORY"}, {"figure_path": "OoOCoZFVK3/figures/figures_20_2.jpg", "caption": "Figure 11: Cooperative and competitive settings between two LLMs. The figure only displays the performance curve of LLM1 for clarity.", "description": "This figure shows the results of experiments comparing cooperative and competitive settings between two LLMs on IMDB and GSM8K datasets. The x-axis represents training iterations, and the y-axis represents task reward and KL divergence.  The curves show that cooperative settings (positive coefficients) lead to better performance (higher task reward and lower KL divergence) than competitive settings (negative coefficients). The figure only shows the results for LLM1 for clarity, but similar trends were observed for LLM2.", "section": "E.2 Different Reward Settings"}, {"figure_path": "OoOCoZFVK3/figures/figures_21_1.jpg", "caption": "Figure 1: The framework of CORY. A traditional RL fine-tuning method can be simply extended to the CORY version with only three steps. First, duplicate the LLM into two LLM agents, one acting as a pioneer and the other as an observer; second, combine the task rewards of the two LLM agents to replace the original task reward; third, periodically exchange the roles of the two LLM agents during training. After training, either the LLM agent can perform the task independently.", "description": "This figure illustrates the framework of the CORY method, which extends traditional RL fine-tuning of LLMs to a cooperative multi-agent reinforcement learning setting.  It shows how a single LLM is duplicated into two agents (pioneer and observer) that interact and exchange roles periodically during training. The combined reward from both agents replaces the original reward signal. This collaborative approach enhances the effectiveness of fine-tuning.", "section": "3 Method"}]