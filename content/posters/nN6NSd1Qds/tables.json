[{"figure_path": "nN6NSd1Qds/tables/tables_6_1.jpg", "caption": "Table 1: Summary of run-time in seconds averaged over 5 runs to reduce the graph to 50%.", "description": "This table presents a comparison of the computational time taken by different graph coarsening methods to reduce the size of various graph datasets by 50%.  The runtimes are averaged over five runs for each method and dataset.  It highlights the significant speedup achieved by the proposed UGC method compared to existing techniques.", "section": "4 Experiments"}, {"figure_path": "nN6NSd1Qds/tables/tables_8_1.jpg", "caption": "Table 4: This table illustrates the accuracy of GCN model when trained with 50% coarsen graph. UGC demonstrated superior performance compared to existing methods in 7 out of the 9 datasets.", "description": "This table presents the node classification accuracy achieved by different graph coarsening methods when training a Graph Convolutional Network (GCN) on a 50% coarsened graph.  The methods compared include various established techniques and the proposed Universal Graph Coarsening (UGC) approach, both with and without adjacency information. The table highlights the superior performance of UGC, especially when incorporating adjacency information, across most datasets.", "section": "4 Experiments"}, {"figure_path": "nN6NSd1Qds/tables/tables_8_2.jpg", "caption": "Table 4: This table illustrates the accuracy of GCN model when trained with 50% coarsen graph. UGC demonstrated superior performance compared to existing methods in 7 out of the 9 datasets.", "description": "This table presents the node classification accuracy achieved by different Graph Neural Network (GNN) models trained on graphs coarsened to 50% using various methods.  It compares the performance of Universal Graph Coarsening (UGC) against other graph coarsening techniques, highlighting UGC's superior accuracy in most cases.", "section": "4 Experiments"}, {"figure_path": "nN6NSd1Qds/tables/tables_15_1.jpg", "caption": "Table 1: Summary of run-time in seconds averaged over 5 runs to reduce the graph to 50%.", "description": "This table presents a comparison of the computational time taken by different graph coarsening methods to reduce the size of various benchmark datasets by 50%.  The datasets used represent a mix of sizes and characteristics, allowing for a comprehensive assessment of each method's efficiency in different contexts. The time reported is averaged over five runs for each method and dataset to provide a reliable estimate and reduce the impact of variance.", "section": "Run-Time Analysis"}, {"figure_path": "nN6NSd1Qds/tables/tables_15_2.jpg", "caption": "Table 1: Summary of run-time in seconds averaged over 5 runs to reduce the graph to 50%.", "description": "This table presents a comparison of the computation time required by different graph coarsening methods to reduce the size of various graphs by 50%. The methods compared include: Variable Neighborhood, Variable Edges, Variable Clique, Heavy Edge, Algebraic Distance, Affinity, Kronecker product, and the proposed UGC method.  The datasets used are diverse and include both small and large graphs, showcasing the scalability of UGC. The results demonstrate UGC's significant speed advantage over existing methods.", "section": "4 Experiments"}, {"figure_path": "nN6NSd1Qds/tables/tables_16_1.jpg", "caption": "Table 1: Summary of run-time in seconds averaged over 5 runs to reduce the graph to 50%.", "description": "This table presents a comparison of the computation time taken by different graph coarsening methods to reduce the size of various benchmark graph datasets by 50%.  The datasets include PubMed, DBLP, Physics, Flickr, Reddit, Yelp, Squirrel, Chameleon, Cora, Texas, Film, and Citeseer, showcasing the performance differences across diverse graph sizes and characteristics. The table highlights the significant speed advantage of the proposed UGC method compared to existing techniques.", "section": "4 Experiments"}, {"figure_path": "nN6NSd1Qds/tables/tables_17_1.jpg", "caption": "Table 7: Evaluation of node classification accuracy of different GNN models when trained with 50% coarsen graph.", "description": "This table compares the node classification accuracy achieved by four different Graph Neural Network (GNN) models (GCN, GraphSage, GIN, and GAT) when trained on graph datasets that have been coarsened to 50% of their original size using the Universal Graph Coarsening (UGC) method.  The table shows that the performance of the GNN models can still be quite good even when trained on a much smaller graph, and that UGC is effective at preserving important information when coarsening graphs.", "section": "4 Experiments"}, {"figure_path": "nN6NSd1Qds/tables/tables_17_2.jpg", "caption": "Table 9: We report the accuracy of GCN on node classification after coarsening by UGC at different ratios.", "description": "This table presents the accuracy of Graph Convolutional Networks (GCNs) for node classification after applying Universal Graph Coarsening (UGC) at different coarsening ratios (30%, 50%, and 70%).  The accuracy is measured across four benchmark datasets: Cora, DBLP, Pubmed, and Physics.  The table allows comparison of GCN performance on coarsened graphs against the original graphs. ", "section": "4 Experiments"}]