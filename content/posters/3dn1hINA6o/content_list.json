[{"type": "text", "text": "The Edge-of-Reach Problem in Offline Model-Based Reinforcement Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anya Sims University of Oxford anya.sims@stats.ox.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Cong Lu University of Oxford ", "page_idx": 0}, {"type": "text", "text": "Jakob N. Foerster FLAIR, University of Oxford ", "page_idx": 0}, {"type": "text", "text": "Yee Whye Teh University of Oxford ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Offilne reinforcement learning (RL) aims to train agents from pre-collected datasets. However, this comes with the added challenge of estimating the value of behaviors not covered in the dataset. Model-based methods offer a potential solution by training an approximate dynamics model, which then allows collection of additional synthetic data via rollouts in this model. The prevailing theory treats this approach as online RL in an approximate dynamics model, and any remaining performance gap is therefore understood as being due to dynamics model errors. In this paper, we analyze this assumption and investigate how popular algorithms perform as the learned dynamics model is improved. In contrast to both intuition and theory, if the learned dynamics model is replaced by the true error-free dynamics, existing model-based methods completely fail. This reveals a key oversight: The theoretical foundations assume sampling of full horizon rollouts in the learned dynamics model; however, in practice, the number of model-rollout steps is aggressively reduced to prevent accumulating errors. We show that this truncation of rollouts results in a set of edge-of-reach states at which we are effectively \u201cbootstrapping from the void.\u201d This triggers pathological value overestimation and complete performance collapse. We term this the edge-of-reach problem. Based on this new insight, we flil important gaps in existing theory, and reveal how prior model-based methods are primarily addressing the edge-of-reach problem, rather than modelinaccuracy as claimed. Finally, we propose Reach-Aware Value Learning (RAVL), a simple and robust method that directly addresses the edge-of-reach problem and hence - unlike existing methods - does not fail as the dynamics model is improved. Since world models will inevitably improve, we believe this is a key step towards future-proofing offline RL.1 ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Standard online reinforcement learning (RL) requires collecting large amounts of on-policy data. This can be both costly and unsafe, and hence represents a significant barrier against applying RL in domains such as healthcare [28, 32] or robotics [2, 4, 20], and also against scaling RL to more complex problems. Offilne RL [6, 23] aims to remove this need for online data collection by enabling agents to be trained on pre-collected datasets. One hope [21] is that it may facilitate advances in RL similar to those driven by the use of large pre-existing datasets in supervised learning [3]. ", "page_idx": 0}, {"type": "image", "img_path": "3dn1hINA6o/tmp/e990caa56e0cbad034b012f38520a1c9ceb734f57b73a8ca60a559310ed3e4b8.jpg", "img_caption": ["Accuracy of dynamics model: Interpolating from random to learned to perfect ", "Figure 1: Existing offilne model-based RL methods fail if the accuracy of the dynamics model is increased (with all else kept the same). Results shown are for MOPO [36], but note that this failure indicates the failure of all existing uncertainty-based methods since each of their specific penalty terms disappear under the true dynamics as \u2018uncertainty\u2019 is zero. By contrast, our method is much more robust to changes in dynamics model. The $x$ -axis shows linearly interpolating next states and rewards of the learned model with the true model (center $\\rightarrow$ right) and random model (center $\\rightarrow$ left), with results on the D4RL W2d-medexp benchmark (min/max over 4 seeds). The full set of results and experimental setup are provided in Table 1 and Appendix C.2 respectively. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "The central challenge in offilne RL is estimating the value of actions not present in the dataset, known as the out-of-sample action problem [17].2 A na\u00efve approach results in extreme value overestimation due to bootstrapping using inaccurate values at out-of-sample state-actions [18]. There have been many proposals to resolve this, with methods largely falling into one of two categories: modelfree [1, 8, 10, 17\u201319] or model-based [14, 24, 29, 36]. ", "page_idx": 1}, {"type": "text", "text": "Model-free methods typically address the out-of-sample action problem by applying a form of conservatism or constraint to avoid using out-of-sample actions in the Bellman update. In contrast, the solution proposed by model-based methods is to allow the collection of additional data at any previously out-of-sample actions. This is done by first training an approximate dynamics model on the offline dataset [13, 30], and then allowing the agent to collect additional synthetic data in this model via $k$ -step rollouts (see Algorithm 1). The prevailing understanding is that this can be viewed as online RL in an approximate dynamics model, with the instruction being to then simply \u201crun any RL algorithm onM until convergence\u201d[36], whereM is the learned dynamics model with some form of dynamics uncertainty penalty. Existing methods propose various forms of dynamics penalties (see Table 7), based on the assumption that the remaining performance gap compared to online RL is solely due to inaccuracies in the learned dynamics model. ", "page_idx": 1}, {"type": "text", "text": "This understanding naturally implies that improving the dynamics model should also improve performance. Surprisingly, we find that existing offline model-based methods completely fail if the learned dynamics model is replaced with the true, error-free dynamics model, while keeping everything else the same (see Figure 1). Under the true dynamics, the only difference to online RL is that in online RL, data is sampled as full-length episodes, while in offilne model-based RL, data is instead sampled as $k$ -step rollouts, starting from a state in the original offline dataset, with rollout length $k$ limited to avoid accumulating model errors. Failure under the true model therefore highlights that truncating rollouts has critical and previously-overlooked consequences. ", "page_idx": 1}, {"type": "text", "text": "We find that this rollout truncation leads to a set of states which, under any policy, can only be reached in the final rollout step (see red in Figure 2). The existence of these edge-of-reach states is problematic as it means Bellman updates (see Equation (1)) use target values that are never themselves trained. This is illustrated in Figure 2, and described in detail in Section 3. This effective \u201cbootstrapping from the void\u201d triggers a catastrophic breakdown in $Q$ -learning. Concisely, this issue can be viewed as all actions from edge-of-reach states remaining out-of-reach over training. Hence, contrary to common understanding, the out-of-sample action problem central to model-free methods is not fully resolved by a model-based approach. In fact, in Section 3.4 we provide detailed analysis suggesting that this is the predominant source of issues on the standard D4RL benchmark. In Section 6.5 we consequently reexamine how existing methods work and find they are indirectly and unintentionally addressing this issue (rather than model errors as claimed). ", "page_idx": 1}, {"type": "text", "text": "Algorithm 1: Pseudocode for the base procedure used in offline model-based methods. ", "page_idx": 2}, {"type": "text", "text": "In summary: (1) Train a dynamics model $\\widehat{M}$ on $\\mathcal{D}_{o f f i n e}$ , then (2) Train an agent (often $S A C.$ ) with $k$ -step rollouts in the learned model starting from $s\\in\\mathcal{D}_{o f\\!\\mathscr{H}i n e}$ . Existing methods consider issues to be due to errors in $\\widehat{M}$ and hence introduce dynamics uncertainty penalties (see Table 7). We find the predominant source of issues to be the edge-of-reach problem, and hence instead propose using value pessimism (RAVL) (see Section 5). ", "page_idx": 2}, {"type": "text", "text": "1: Require: Offline dataset $\\mathcal{D}_{\\mathrm{offine}}$   \n2: Require: Dynamics model $\\widehat{M}=(\\widehat{T},\\widehat{R})$ trained on $\\mathcal{D}_{\\mathrm{offine}}$ . Augment $\\widehat{M}$ with an uncertainty penalty\\*   \n3: Specify: Rollout length $k\\geq1$ , rea l d ata ratio $r\\in[0,1]$   \n4: Initialize: Replay buffer $\\mathcal{D}_{\\mathrm{rollouts}}=\\emptyset$ , policy $\\pi_{\\theta}$ , value function $Q_{\\phi}$ (both from random)   \n5: for epochs $\\mathbf{\\Sigma}=1,\\ldots.$ do   \n6: (Collect data) Starting from states in $\\mathcal{D}_{\\mathrm{offline}}$ , collect $k$ -step rollouts in $\\widehat{M}$ with $\\pi_{\\theta}$ . Store data in $\\mathcal{D}_{\\mathrm{rollouts}}$   \n7: (Train agent) Train $\\pi_{\\theta}$ and $Q_{\\phi}$ on Drollouts $\\cup\\,\\mathcal{D}_{\\mathrm{offine}}$ (mixed with ratio $r$ ) Add Q-value pessimism   \n8: end for \\*This uncertainty penalty collapses to zero with the true error-free dynamics ${\\widehat{M}}=M)$ ) (see experiments Table 2). ", "page_idx": 2}, {"type": "text", "text": "We have the following problem: Indirectly addressing the edge-of-reach problem means existing model-based methods have a fragile and unforeseen dependence on model quality and fail catastrophically as dynamics models improve. Since model improvements are inevitable, and since tuning of hyperparameters is particularly impractical in offline RL, this is a substantial practical barrier. ", "page_idx": 2}, {"type": "table", "img_path": "3dn1hINA6o/tmp/e7a20978dce532c0592a5dcf03908c695ef036e7d6b516964d2f31bda1eab17b.jpg", "table_caption": ["Algorithm 1 Base model-based algorithm (MBPO) $^+$ Additions in existing methods and RAVL (ours) "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "In light of this, we propose Reach-Aware Value Learning (RAVL), a simple and robust method that directly addresses the edge-of-reach problem. RAVL achieves strong performance on standard benchmarks, and moreover continues to perform even with error-free, uncertainty-free dynamics models (Section 6). Our model-based method bears close connections to model-free approaches, and hence in Appendix A.2 we present a unified perspective of these two previously disjoint subfields. ", "page_idx": 2}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Reinforcement Learning. We consider the standard RL framework [31], in which the environment is formulated as a Markov Decision Process, $M=(S,\\mathcal{A},T,R,\\mu_{0},\\gamma)$ , where $\\boldsymbol{S}$ and $\\boldsymbol{\\mathcal{A}}$ denote the state and action spaces, $T(s^{\\prime}|s,a)$ and $R(s,a)$ denote the transition and reward dynamics, $\\mu_{0}$ the initial state distribution, and $\\gamma\\in(0,1)$ is the discount factor. The goal in reinforcement learning is to learn a policy $\\pi(a|s)$ that maximizes the expected discounted return $\\begin{array}{r}{\\mathbb{E}_{\\mu_{0},\\pi,T}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R(s_{t},\\tilde{a_{t}})\\right]}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "Actor-Critic Algorithms. The broad class of algorithms we consider are actor-critic [15] methods which jointly optimize a policy $\\pi$ and state-action value function ( $Q$ -function). Given a dataset $\\mathcal{D}$ of (state, action, reward, nextstate) transitions, actor-critic algorithms iterate between two steps: ", "page_idx": 2}, {"type": "text", "text": "2. (Policy improvement) Update $\\pi_{\\theta}$ to increase the current $Q$ -value predictions: $Q_{\\phi}(s,\\pi_{\\theta}(s))^{3}$ . ", "page_idx": 2}, {"type": "text", "text": "Offilne RL and the Out-of-Sample Action Problem. In offilne RL, training must rely on only a fixed dataset of transitions $\\mathcal{D}_{\\mathrm{offine}}=\\bar{\\{}(s,a,r,s^{\\prime})^{(i)}\\}_{i=1,...,N}$ collected by some policy $\\pi^{\\beta}$ . The central problem in this offline setting is the out-of-sample4 action problem: The values of $Q_{\\phi}$ being updated are at any state-actions that appear as $(s,a)$ in $\\mathcal{D}_{\\mathrm{offine}}$ . However, the targets of these updates rely on values at $(s^{\\prime},a^{\\prime})$ where $a^{\\prime}\\sim\\pi_{\\theta}$ (see Equation (1)). Since $a^{\\prime}\\sim\\pi_{\\theta}$ (on-policy) whereas $a\\sim\\pi^{\\beta}$ (offpolicy), $(s^{\\prime},a^{\\prime})$ may not appear in $\\mathcal{D}_{\\mathrm{offline}}$ , and hence may itself have never been updated. This means that updates can involve bootstrapping from \u2018out-of-sample\u2019 and hence arbitrarily misestimated values and thus lead to misestimation being propagated over the entire state-action space. The max operation (implicit in the policy improvement step) further acts to exploit any misestimation, converting misestimation into extreme pathological overestimation. As a result $Q$ -values often tend to increase unboundedly over training, while performance collapses entirely [18, 23]. ", "page_idx": 2}, {"type": "text", "text": "Model-Based Offline RL. Model-based methods [30] aim to solve the out-of-sample issue by allowing the agent to collect additional synthetic data in a learned dynamics model. They generally share the same base procedure as described in Algorithm 1. This involves first training an approximate dynamics model $\\widehat{M}=(\\mathcal{S},\\mathcal{A},\\widehat{T},\\widehat{R},\\mu_{0},\\gamma)$ on $\\mathcal{D}_{\\mathrm{offine}}$ . Here, $\\widehat{T}(s^{\\prime}|s,a)$ and $\\widehat{R}(s,a)$ denote the learned transition and reward functions, commonly realized as a deep ensemble [5, 22]. Following this an agent is trained using an online RL algorithm (typically SAC [11]), for which data is sampled as $k$ -step trajectories (termed rollouts) under the current policy, starting from states in the offilne dataset $\\mathcal{D}_{\\mathrm{offline}}$ . The base procedure of training a SAC agent with model rollouts does not work out of the box. Existing methods attribute this as due to dynamics model errors. Consequently, a broad class of methods propose augmenting the learned dynamics modelM with some form of dynamics uncertainty penalty, often based on variance over the ensemble dynamics model [2, 14, 24, 29, 36]. We present some explicit examples in Table 7. Crucially, all of these methods assume that, under the true error-free model, no intervention should be needed, and hence all the uncertainty penalties collapse to zero. In the following section, we show that this assumption leads to catastrophic failure. ", "page_idx": 3}, {"type": "image", "img_path": "3dn1hINA6o/tmp/297f1d45e06bd3f89b4a76cf1c068052bb26bcd021ab99e4b2032ffc2c7c3f4f.jpg", "img_caption": ["Figure 2: The previously unnoticed edge-of-reach problem. Left illustrates the base procedure used in offline model-based RL, whereby synthetic data is sampled as $k$ -step trajectories \u201crollouts\u201d starting from a state in the original offline dataset. Edge-of-reach states are those that can be reached in $k$ -steps, but which cannot (under any policy) be reached in less than $k$ -steps. We depict the data collected with two rollouts, one ending in $s_{k}=D$ , and the other with $s_{k}=C$ . Right then shows this data arranged into a dataset of transitions as used in $Q$ -updates. State $D$ is edge-of-reach and hence appears in the dataset as $s^{\\prime}$ but never as s. Bellman updates therefore bootstrap from $D$ , but never update the value at $D$ (see Equation (1)). (For comparison consider state $C$ : $C$ is also sampled at $s_{k}$ , but unlike $D$ it is not edge-of-reach, and hence is also sampled at $s_{i<k}$ meaning it is updated and hence does not cause issues.) "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3 The Edge-of-Reach Problem ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the following section, we formally introduce the edge-of-reach problem. We begin by showing the empirical failure of SOTA offilne model-based methods on the true environment dynamics. Next, we present our edge-of-reach hypothesis for this, including intuition, empirical evidence on the main D4RL benchmark [7], and theoretical proof of its effect on offline model-based training. ", "page_idx": 3}, {"type": "text", "text": "3.1 Surprising Failure with the True Dynamics ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "As described in Sections 1 and 2, prior works view offline model-based RL as online RL in an approximate dynamics model. Based on this understanding they propose various forms of dynamics uncertainty penalties to address model errors (see Table 7). This approach is described simply as: \u201ctwo steps: (a) learning a pessimistic MDP $^P$ -MDP) using the offline dataset; (b) learning a near-optimal policy in this P-MDP\u201d[14]. This shared base procedure is shown in Algorithm 1, where \u201cP-MDP\u201d refers to the learned dynamics model with a penalty added to address model errors. ", "page_idx": 3}, {"type": "text", "text": "This assumption of issues being due to dynamics model errors naturally leads to the belief that the ideal case would be to have a perfect error-free dynamics model. However, in Figure 1 and Table 1 we demonstrate that, if the learned dynamics model is replaced with the true dynamics, all dynamics-penalized methods completely fail on most environments.5 Note that under the true dynamics, all existing dynamics penalty-based methods assume no intervention is needed since there are no model errors. As a result, their penalties all become zero (see Table 7) and they all collapse to exactly the same procedure. Therefore the results shown in Table 1 indicate the failure of all existing dynamics-penalized methods. In the following section, we investigate why having perfectly accurate dynamics leads to failure. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.2 The Edge-Of-Reach Hypothesis (Illustrated in Figure 2) ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Failure under the error-free dynamics reveals that \u201cmodel errors\u201d cannot explain all the problems in offilne model-based RL. Instead, it highlights that there must be a second issue. On investigation we find this to be the \u201cedge-of-reach problem.\u201d We begin with the main intuition: in offilne model-based RL, synthetic data $\\mathcal{D}_{\\mathrm{rollouts}}$ is generated as short $k$ -step rollouts starting from states in the original dataset $\\mathcal{D}_{\\mathrm{offine}}$ (see Algorithm 1). Crucially, Figure 2 (left) illustrates how, under this procedure, there can exist some states which can be reached in the final rollout step, but which cannot - under any policy - be reached earlier. These edge-of-reach states triggers a breakdown in learning, since even with the ability to collect unlimited data, the agent is never able to reach these states \u2018in time\u2019 to try actions from them, and hence is free to \u2018believe that these edge-of-reach states are great.\u2019 ", "page_idx": 4}, {"type": "text", "text": "More concretely: Figure 2 (right) illustrates how being sampled only at the final rollout step means edge-of-reach states will appear in the resulting dataset $\\mathcal{D}_{\\mathrm{rollouts}}$ as nextstates $s^{\\prime}$ , but will never appear in the dataset as states $s$ . Crucially, in Equation (1), we see that values at states $s$ are updated, while values at $s^{\\prime}$ are used for the targets of these updates. Edge-of-reach states are therefore used for targets, but are never themselves updated, meaning that their values can be arbitrarily misestimated. Updates consequently propagate misestimation over the entire state-action space. Furthermore, the max operation in the policy improvement step (see Section 2) exploits any misestimation by picking out the most heavily overestimated values [23]. The misestimation is therefore turned into overestimation, resulting in the value explosion seen in Figure 3. Thus, contrary to common understanding, the out-of-sample action problem key in model-free offilne RL can be seen to persist in model-based RL. ", "page_idx": 4}, {"type": "text", "text": "Edge-of-Reach Hypothesis: Limited horizon rollouts from a fixed dataset leads to \u2018edge-ofreach\u2019 states: states which are used as targets for Bellman-based updates, but which are never themselves updated. The resulting maximization over misestimated values in Bellman updates causes pathological value overestimation and a breakdown in $Q$ -learning. ", "page_idx": 4}, {"type": "text", "text": "When is this an issue in practice? Failure via this mode requires (a) the existence of edge-of-reach states, and $(b)$ such edge-of-reach states to be sampled. The typical combination of $k\\ll H$ along with a limited pool of starting states $s\\in{\\mathcal{D}}_{\\mathrm{offine}})$ ) means the rollout distribution is unlikely to sufficiently cover the full state space $\\boldsymbol{S}$ , thus making $(a)$ likely. Moreover, we observe pathological \u2018edge-of-reach seeking\u2019 behavior in which the agent appears to \u2018seek out\u2019 edge-of-reach states due to this being the source of overestimation, thus making $(b)$ likely. This behaviour is discussed further in Section 4. ", "page_idx": 4}, {"type": "text", "text": "In Appendix A, we give a thorough and unified view of model-free and model-based offline RL, dividing the problem into independent conditions for states and actions and examining when we can expect the edge-of-reach problem to be significant. ", "page_idx": 4}, {"type": "text", "text": "3.3 Definitions and Formalization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Definition 1 (Edge-of-reach states). Consider a deterministic transition model $T:S\\times A\\to S$ , rollout length $k$ , and some distribution over starting states $\\nu_{0}$ . For some policy $\\pi:{\\mathcal{S}}\\rightarrow A,$ , rollouts are then generated according to $s_{0}\\sim\\nu_{0}(\\cdot)$ , $a_{t}\\sim\\pi(\\cdot|s_{t})$ and $s_{t+1}\\sim T(\\cdot|s_{t},a_{t})$ for $t=0,\\ldots,k-1$ , giving $\\left(s_{0},a_{0},s_{1},\\ldots,s_{k}\\right)$ . Let us use $\\rho_{t,\\pi}(s)$ to denote the marginal distributions over $s_{t}$ . We define a state $s\\in S$ edge-of-reach with respect to $(T,\\,k,\\,\\nu_{0})$ if: for $t=k$ , $\\exists~\\pi~s.t.$ . $\\rho_{t,\\pi}(s)>0$ , but, for $t=1,\\ldots,k-1$ and $\\forall\\,\\pi$ , $\\rho_{t,\\pi}(s)=0$ . In our case, $\\nu_{0}$ is the distribution of states in $\\mathcal{D}_{\\mathrm{offine}}$ . In Appendix B, we include an extension to stochastic transition models, proof of how errors can consequently propagate to all states, and a discussion of the practical implications. ", "page_idx": 4}, {"type": "text", "text": "3.4 Empirical evidence on the D4RL benchmark ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "There are two potential issues in offline model-based RL: $(I)$ dynamics model errors and subsequent model exploitation, and (2) the edge-of-reach problem and subsequent pathological value overestimation. We ask: Which is the true source of the issues observed in practice? While $(I)$ is stated as the sole issue and hence the motivation in prior methods, the \u201cfailure\u201d results presented in Figure 1 and Table 1 are strongly at odds with this explanation. Furthermore, in Table 8, we examine the model rewards sampled by the agent over training. For $(I)$ to explain the $Q$ -values seen in Figure 3, the sampled rewards in Table 8 would need to be on the order of $10^{8}$ . Instead, however, they remain less than 10, and are not larger than the true rewards, meaning $(I)$ does not explain the observed value explosion (see Appendix D.2). By contrast, the edge-of-reach-induced pathological overestimation mechanism of (2) exactly predicts this value explosion, with the observations very closely resembling those of the analogous model-free out-of-sample problem [18]. Furthermore, (2) is consistent with the \u201cfailure\u201d observations in Table 1. Finally, we again highlight the discussion in Section 3.2 where we explain why the edge-of-reach problem can be expected to occur in practice. ", "page_idx": 5}, {"type": "text", "text": "Figure 3: The base procedure results in poor performance (left) with exponential increase in $Q$ - values (right) on the D4RL benchmark. Approx $Q^{*}$ indicates the $Q$ -value for a normalized score of 100 (with $\\gamma=0.99_{.}$ ). Results are shown for Walker2d-medexp (6 seeds), but we note similar trends across other D4RL datasets. ", "page_idx": 5}, {"type": "image", "img_path": "3dn1hINA6o/tmp/33eedea67ff95c98a00ca7f9b3bb91a68f3455edea636586db209eab7fe3fc16.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "4 Analysis with a Simple Environment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In the previous section, we presented the edge-of-reach hypothesis as an explanation of why existing methods fail under the true dynamics. In this section, we construct a simple environment to empirically confirm this hypothesis. We first reproduce the observation seen in Figures 1 and 3 and Table 1 of the base offilne model-based procedure resulting in exploding $Q$ -values and failure to learn despite using the true dynamics model. Next, we verify that edge-of-reach states are the source of this problem by showing that correcting value estimates only at these states is sufficient to resolve the issues. ", "page_idx": 5}, {"type": "text", "text": "4.1 Setup", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We isolate failure observed in Section 3.1 with the following setup: Reward is defined as in Figure 4a, the transitions function is simply $\\mathbf{s}^{\\circ}=\\mathbf{s}+\\mathbf{a}\\,\\in\\mathbb{R}^{2}$ , and initial states (analogous to $s\\in\\mathcal{D}_{\\mathrm{offine}})$ are sampled from $\\mu_{0}=U([-2,2]^{\\bar{2}})$ (the area shown in the navy blue box Figure 4). ", "page_idx": 5}, {"type": "text", "text": "In applying the offilne model-based procedure (see Algorithm 1) to this (true) environment we have precisely the same setup as in Section 3.1, where again the only difference compared to online $R L$ is the use of truncated $k$ -step rollouts with $k=10$ (compared to full horizon $H=30$ ). This small change results in the existence of edge-of-reach states (those between the red and orange boxes). ", "page_idx": 5}, {"type": "text", "text": "4.2 Observing Pathological Value Overestimation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Exactly as with the benchmark experiments in Section 3.1, the base model-based procedure fails despite using the true dynamics (see blue Figure 4, Base), and the $Q$ -values grow unboundedly over training (compare Figure 4f and Figure 3b). ", "page_idx": 5}, {"type": "text", "text": "Looking at the rollouts sampled over training (see Figure 7) we see the following behavior: ", "page_idx": 5}, {"type": "text", "text": "(Before 25 epochs) Performance initially increases. (Between 25 and 160 epochs) Value misestimation takes over, and the policy begins to aim toward unobserved state-actions (since their values can be misestimated and hence overestimated). (After 160 epochs) This \u2018edge-of-reach seeking\u2019 behavior compounds with each epoch, leading the agent to eventually reach edge-of-reach states. From this point onwards, the agent samples edge-of-reach states at which it never receives any corrective feedback. The consequent pathological value overestimation results in a complete collapse in performance. In Figure 4b we visualize the final policy and see that it completely ignores the reward function, aiming instead towards an arbitrary edge-of-reach state. ", "page_idx": 5}, {"type": "image", "img_path": "3dn1hINA6o/tmp/e88d889ac0d2134ec8c975743956f7ee5875b8902dd1fcf9e6d880490eaef3d9.jpg", "img_caption": ["Figure 4: Experiments on the simple environment, illustrating the edge-of-reach problem and potential solutions. (a) Reward function, (b) final (failed) policy with na\u00efve application of the base procedure (see Algorithm 1), (c) final (successful) policy with patching in oracle $Q$ -values for edge-of-reach states, (d) final (successful) policy with RAVL, (e) returns evaluated over training, (f) mean $Q$ -values evaluated over training. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.3 Verifying the Hypothesis Using Value Patching ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our hypothesis is that the source of this failure is value misestimation at edge-of-reach states. Our Base-OraclePatch experiments (see yellow Figure 4) verify this by showing that patching in the correct values solely at edge-of-reach states is sufficient to completely solve the problem. This is particularly compelling as in practice we only corrected values at $0.4\\%$ of states over training. In Section 5 we introduce our practical method RAVL, which Figures 4 green and 7 show has an extremely similar effect to that of the ideal but practically impossible Base-OraclePatch intervention. ", "page_idx": 6}, {"type": "text", "text": "5 RAVL: Reach-Aware Value Learning ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "As verified in Section 4.3, issues stem from value overestimation at edge-of-reach states. To resolve this, we therefore need to (A) detect and $(B)$ prevent overestimation at edge-of-reach states. ", "page_idx": 6}, {"type": "text", "text": "(A) Detecting edge-of-reach states: As illustrated in Figure 2 (right), edge-of-reach states are states at which the $Q$ -values are never updated, i.e., those that are out-of-distribution (OOD) with respect to the training distribution of the $Q$ -function, $s\\in\\mathcal{D}_{\\mathrm{rollouts}}$ . A natural solution for OOD detection is measuring high variance over a deep ensemble [22]. We can therefore detect edge-of-reach states using an ensemble of $Q$ -functions. We demonstrate that this is effective in Figure 5. ", "page_idx": 6}, {"type": "text", "text": "$(B)$ Preventing overestimation at these states: Once we have detected edge-of-reach states, we may simply apply value pessimism methods from the offline model-free literature. Our choice of an ensemble for part $(A)$ conveniently allows us to minimize over an ensemble of $Q$ -functions, which effectively adds value pessimism based on ensemble variance. ", "page_idx": 6}, {"type": "text", "text": "Our resulting proposal is Reach-Aware Value Learning (RAVL). Concretely, we take the standard offilne model-based RL procedure (see Algorithm 1), and simply exchange the dynamics pessimism penalty for value pessimism using minimization over an ensemble of $N\\,Q$ -functions: ", "page_idx": 6}, {"type": "equation", "text": "$$\nQ_{\\phi}^{n}(s,a)\\gets r+\\gamma\\operatorname*{min}_{i=1,\\ldots,N}Q_{\\phi}^{i}(s^{\\prime},\\pi_{\\theta}(s^{\\prime}))\\quad\\mathrm{~for~}n=1,\\ldots,N\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We include EDAC\u2019s [1] ensemble diversity regularizer, and in Section 7 we discuss how the impact of value pessimism differs significantly in the model-based (RAVL) vs model-free (EDAC) settings. ", "page_idx": 6}, {"type": "text", "text": "6 Empirical Evaluation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we begin by analyzing RAVL on the simple environment from Section 4. Next, we look at the standard D4RL benchmark, first confirming that RAVL solves the failure seen with the true dynamics, before then demonstrating that RAVL achieves strong performance with the learned dynamics. In Appendix F, we include additional results on the challenging pixel-based V-D4RL benchmark on which RAVL now represents a new state-of-the-art. Finally in Section 6.5 we reexamine prior model-based methods and explain why they may work despite not explicitly addressing the edge-of-reach problem. We provide full hyperparameters in Appendix C.2 and ablations showing that RAVL is stable over hyperparameter choice in Appendix G.2. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "6.1 Simple Environment ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Testing on the simple environment from Section 4 (see green Figure 4), we observe that RAVL behaves the same as the theoretically optimal but practically impossible Base-OraclePatch method. Moreover, in Figure 5, we see that the $Q$ -value variance over the ensemble is significantly higher for edge-of-reach states, meaning RAVL is detecting and penalizing edge-of-reach states exactly as intended. ", "page_idx": 7}, {"type": "text", "text": "6.2 D4RL with the True Dynamics ", "text_level": 1, "page_idx": 7}, {"type": "image", "img_path": "3dn1hINA6o/tmp/f998dfce2e8052e04c5d4539ae12e23ae84b7384fade884773f0084f89916980.jpg", "img_caption": ["Figure 5: RAVL\u2019s effective penalty of $Q$ -ensemble variance on the environment in Section 4, showing that - as intended - edge-of-reach states have significantly higher penalty than within-reach states. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Next, we demonstrate that RAVL works without dynamics uncertainty and solves the \u2018failure\u2019 observed in Section 3.1. Table 1 shows results on the standard offilne benchmark D4RL [7] MuJoCo [34] v2 datasets with the true (zero error, zero uncertainty) dynamics. We see that RAVL learns the near-optimal policies, while existing methods using the base model-based procedure (Algorithm 1) completely fail. In Section 6.5 we examine how this is because existing methods overlook the critical edge-of-reach problem and instead only accidentally address it using dynamics uncertainty metrics. In the absence of model uncertainty, these methods have no correction for edge-of-reach states and hence fail dramatically. ", "page_idx": 7}, {"type": "text", "text": "Table 1: True dynamics (zero error, zero uncertainty) Existing model-based methods are presented as different approaches for dealing with dynamics model errors. Surprisingly, however, all existing methods fail in the absence of dynamics errors (when the learned approximate model is replaced by the true model). This reveals that existing methods are unintentionally using their dynamics uncertainty estimates to address the previously unnoticed edge-of-reach problem. By contrast, RAVL directly addresses the edge-of-reach problem and hence does not fail when dynamics uncertainty is zero. Experiments are on the D4RL MuJoCo v2 datasets. Statistical significance highlighted (6 seeds). \\*\\*Note that while labeled as \u2018MOBILE\u2019, the results with the true dynamics will be identical for any other dynamics penalty-based method since penalties under the true model are all zero (see Table 7). Hence these results indicate the failure of all existing dynamics uncertainty-based methods. ", "page_idx": 7}, {"type": "table", "img_path": "3dn1hINA6o/tmp/982c155b85faf1c4c6abbd26ec6ba3eb7a2449ffaae5e792e9dc1f3d8eb2d01a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "6.3 D4RL with Learned Dynamics ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Next, we show that RAVL also performs well with a learned dynamics model. Figure 3 shows RAVL successfully stabilizes the $Q$ -value explosion of the base procedure, and Table 2 shows RAVL largely matches SOTA, while having significantly lower runtime (see Section 6.4). RAVL gives much higher performance on the Halfcheetah mixed and medium datasets than its model-free counterpart EDAC, and in Section 6.5, we discuss why we would expect the effect of value pessimism in the model-based setting (RAVL) to inherently offer much more flexibility than in the model-free setting (EDAC). ", "page_idx": 7}, {"type": "text", "text": "Table 2: A comprehensive evaluation of RAVL over the standard D4RL MuJoCo benchmark. We show the mean and standard deviation of the final performance averaged over 6 seeds. Our simple approach largely matches the state-of-the-art without any explicit dynamics penalization and hence works even in the absence of model uncertainty (where dynamics uncertainty-based methods fail) (see Table 1). ", "page_idx": 8}, {"type": "table", "img_path": "3dn1hINA6o/tmp/a9af09d248095ca1a2c4c897585590aacff3e5477e1179b489ed44f76546d48d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "We additionally include results for the challenging pixel-based V-D4RL benchmark for which latentspace models are used (in Appendix F), and accompanying ablation experiments (in Appendix G.2). In this setting, RAVL represents a new SOTA, giving a performance boost of more than $20\\%$ for some environments. These results are particularly notable as the pixel-based setting means the base algorithm (DreamerV2) uses model rollouts in an imagined latent space (rather than the original state-action space as in MBPO). The results therefore give promising evidence that RAVL is able to generalize well to different representation spaces. ", "page_idx": 8}, {"type": "text", "text": "6.4 Runtime Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Vectorized ensembles can be scaled with extremely minimal effect on the runtime (see Table 10). This means that, per epoch, RAVL is approximately $13\\%$ faster than the SOTA (MOBILE, due to MOBILE needing multiple extra forward passes to compute its uncertainty penalty). Furthermore, in total, we find that RAVL reliably requires $3\\times$ fewer epochs to converge on all but the medexp datasets, meaning the total runtime is approximately $70\\%$ faster than SOTA (see Appendix G.1). ", "page_idx": 8}, {"type": "text", "text": "6.5 How can prior methods work despite overlooking the edge-of-reach problem? ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "While ostensibly to address model errors, we find that existing dynamics penalties accidentally address the edge-of-reach problem: In Figure 6 we see a positive correlation between the penalties used in dynamics uncertainty methods and RAVL\u2019s effective penalty of value ensemble variance. This may be expected, as dynamics uncertainty will naturally be higher further away from $\\mathcal{D}_{\\mathrm{offine}}$ , which is also where edge-of-reach states are more likely to lie. In Section 3.4 we present evidence suggesting that the edge-of-reach problem is likely the dominant source of issues on the main D4RL benchmark, thus indicating that the dynamics uncertainty penalties of existing methods are likely indirectly addressing the edge-of-reach problem. In general, dynamics errors are a second orthogonal problem and RAVL can be easily combined with appropriate dynamics uncertainty penalization [24, 29, 36] for environments where this is a significant issue. ", "page_idx": 8}, {"type": "text", "text": "7 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Model-Based Methods. Existing offline model-based methods present dynamics model errors and consequent model exploitation as the sole source of issues. A broad class of methods therefore propose reward penalties based on the estimated level of model uncertainty [2, 14, 24, 29, 36], typically using variance over a dynamics ensemble. Rigter et al. [27] aim to avoid model exploitation by setting up an adversarial two-player game between the policy and model. Finally, most related to our method, COMBO [37], penalizes value estimates for state-actions outside model rollouts. However, similarly to Yu et al. [36], COMBO is theoretically motivated by the assumption of infinite horizon model rollouts, which we show overlooks serious implications. Critically, in contrast to our approach, none of these methods address the edge-of-reach problem and thus they fail as environment models become more accurate. A related phenomenon of overestimation stemming from hallucinated states has been observed in online model-based RL[12]. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Offline Model-Free Methods. Model-free methods can broadly be divided into two approaches to solving the out-of-sample action problem central to offline model-free RL (see Section 2): action constraint methods and value pessimism-based methods. Action constraint methods [8, 17, 18] aim to avoid using out-of-sample actions in the Bellman update by ensuring selected actions are close to the dataset behavior policy $\\pi^{\\beta}$ . By contrast, value pessimism-based methods aim to directly regularize the value function to produce low-value estimates for out-of-sample state-actions [1, 16, 19]. The edge-of-reach problem is the model-based equivalent of the out-of-sample action problem, and this unified understanding allows us to transfer ideas directly from model-free literature. RAVL is based on EDAC\u2019s [1] use of minimization over a $Q$ -ensemble [9, 35], and applies this to model-based offline RL. ", "page_idx": 9}, {"type": "text", "text": "What is the effect of value pessimism in the model-based vs model-free settings? ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "EDAC [1] can be seen as RAVL\u2019s model-free counterpart, however, the impact of value pessimism in the model-free vs the model-based settings is notably different. Recall that, with the ensemble $Q$ -function trained on $(s,a,r,s^{\\prime})\\sim\\ensuremath{\\mathcal{D}_{\\perp}}$ , the state-actions that are penalized (due to being outside the training distribution) are any $(s^{\\prime},a^{\\prime})$ that are out-of-distribution with respect to the $(s,a)$ \u2019s in the dataset $\\mathcal{D}_{\\boxed{\\Pi}}$ . Recall also that updates use values at $(s^{\\prime},a^{\\prime})$ where $a^{\\prime}\\sim\\pi_{\\theta}$ (see Equation (1)). ", "page_idx": 9}, {"type": "text", "text": "In the model-free case $(E D A C)$ : The dataset is $\\mathcal{D}_{\\square}=\\mathcal{D}_{o f f l i n e}$ , meaning the actions $a$ are effectively sampled from the dataset behavior policy (off-policy), whereas the actions $a^{\\prime}$ are sampled on-policy. This means that EDAC penalizes any $(s^{\\prime},a^{\\prime})$ where $a^{\\prime}$ differs significantly from the behavior policy. ", "page_idx": 9}, {"type": "text", "text": "In the model-based case $(R A V L)$ : The dataset is $\\begin{array}{r}{D_{\\square}=D_{r o l l o u t s}}\\end{array}$ , meaning now both $a$ and $a^{\\prime}$ are sampled on-policy. As a result, the only $(s^{\\prime},a^{\\prime})$ which will now be out-of-distribution with respect to $(s,a)$ (and hence penalized) are those where the state $s^{\\prime}$ is out-of-distribution with respect to the states $s$ in $\\mathcal{D}_{r o l l o u t s}$ . This happens when $s^{\\prime}$ is reachable only in the final step of rollouts, i.e. exactly when $s^{\\prime}$ is \u201cedge-of-reach\u201d (as illustrated in Figure 2). Compared to EDAC, RAVL can therefore be viewed as \u201crelaxing\u201d the penalty and giving the agent freedom to learn a policy that differs significantly from the dataset behavior policy. This distinction is covered in detail in Appendix A. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper investigates how offilne model-based methods perform as dynamics models become more accurate. As an interesting hypothetical extreme, we test existing methods with the true error-free dynamics. Surprisingly, we find that all existing methods fail. This reveals that using truncated rollout horizons (as per the shared base procedure) has critical and previously overlooked consequences stemming from the consequent existence of \u2018edge-of-reach\u2019 states. We show that existing methods are indirectly and accidentally addressing this edge-of-reach problem (rather than addressing model errors as stated), and hence explain why they fail catastrophically with the true dynamics. ", "page_idx": 9}, {"type": "text", "text": "This problem reveals close connections between model-based and model-free approaches and leads us to present a unified perspective for offilne RL. Based on this, we propose RAVL, a simple and robust method that achieves strong performance across both proprioceptive and pixel-based benchmarks. Moreover, RAVL directly addresses the edge-of-reach problem, meaning that - unlike existing methods - RAVL does not fail under the true environment model, and has the practical benefti of not requiring dynamics uncertainty estimates. Since improvements to dynamics models are inevitable, we believe that resolving the brittle and unanticipated failure of existing methods under dynamics model improvements is an important step towards \u2018future-proofing\u2019 offline RL. ", "page_idx": 9}, {"type": "text", "text": "9 Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Since dynamics models for the main offline RL benchmarks are highly accurate, the edge-of-reach effects dominate, and RAVL is sufficient to stabilize model-based training effectively without any explicit dynamics uncertainty penalty. In general, however, edge-of-reach issues could be mixed with dynamics error, and understanding how to balance these two concerns would be useful future work. Further, we believe that studying the impact of the edge-of-reach effect in a wider setting could be an exciting direction, for example investigating its effect as an implicit exploration bias in online RL. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Anya Sims and Cong Lu are funded by the Engineering and Physical Sciences Research Council (EPSRC). The authors would like to thank the anonymous Agent Learning in Open-Endedness Workshop at NeurIPS 2023 and ICLR 2024 reviewers for positive and constructive feedback which helped to improve the paper. We would also like to thank Philip J. Ball and Shimon Whiteson for helpful feedback and discussions on earlier drafts of this work. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offline reinforcement learning with diversified q-ensemble. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, 2021. URL https://proceedings.neurips.cc/paper/2021/file/ 3d3d286a8d153a4a58156d0e02d8570c-Paper.pdf. 2, 7, 10, 16, 20 ", "page_idx": 10}, {"type": "text", "text": "[2] Philip J Ball, Cong Lu, Jack Parker-Holder, and Stephen Roberts. Augmented world models facilitate zero-shot dynamics generalization from a single offline environment. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 619\u2013629. PMLR, 18\u201324 Jul 2021. URL https://proceedings.mlr.press/v139/ball21a.html. 1, 4, 9   \n[3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. 1 [4] Yevgen Chebotar, Karol Hausman, Yao Lu, Ted Xiao, Dmitry Kalashnikov, Jake Varley, Alex Irpan, Benjamin Eysenbach, Ryan Julian, Chelsea Finn, et al. Actionable models: Unsupervised offline reinforcement learning of robotic skills. arXiv preprint arXiv:2104.07749, 2021. 1   \n[5] Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. In Advances in Neural Information Processing Systems, 2018. URL https://proceedings.neurips.cc/paper_ files/paper/2018/file/3de568f8597b94bda53149c7d7f5958c-Paper.pdf. 4, 16   \n[6] Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning. Journal of Machine Learning Research (JMLR), 6(18):503\u2013556, 2005. 1   \n[7] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning, 2020. 4, 8, 17, 22   \n[8] Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offilne reinforcement learning. In Thirty-Fifth Conference on Neural Information Processing Systems, 2021. 2, 10   \n[9] Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research. PMLR, 2018. URL https://proceedings.mlr.press/v80/fujimoto18a.html. 10   \n[10] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 2052\u20132062. PMLR, 09\u201315 Jun 2019. URL https://proceedings. mlr.press/v97/fujimoto19a.html. 2   \n[11] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 1861\u20131870. PMLR, 10\u201315 Jul 2018. URL https://proceedings.mlr.press/v80/haarnoja18b.html. 4, 16   \n[12] Taher Jafferjee, Ehsan Imani, Erin Talvitie, Martha White, and Micheal Bowling. Hallucinating value: A pitfall of dyna-style planning with imperfect environment models. arXiv preprint arXiv:2006.04363, 2020. 10   \n[13] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy optimization. In Advances in Neural Information Processing Systems, 2019. 2, 16, 19   \n[14] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. MOReL: Model-based offline reinforcement learning. In Advances in Neural Information Processing Systems, volume 33, 2020. URL https://proceedings.neurips.cc/paper/2020/file/ f7efa4f864ae9b88d43527f4b14f750f-Paper.pdf. 2, 4, 9, 16, 18   \n[15] Vijay Konda and John Tsitsiklis. Actor-critic algorithms. Advances in neural information processing systems, 1999. 3   \n[16] Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. Offline reinforcement learning with fisher divergence critic regularization. In Proceedings of the 38th International Conference on Machine Learning, Proceedings of Machine Learning Research. PMLR, 2021. URL https://proceedings.mlr.press/v139/kostrikov21a.html. 10   \n[17] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning. In International Conference on Learning Representations, 2022. URL https: //openreview.net/forum?id=68n2s9ZJWF8. 2, 10   \n[18] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via bootstrapping error reduction. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32, 2019. 2, 3, 6, 10, 15, 16   \n[19] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. In Advances in Neural Information Processing Systems, volume 33, 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 0d2b2061826a5df3221116a5085a6052-Paper.pdf. 2, 10   \n[20] Aviral Kumar, Anikait Singh, Stephen Tian, Chelsea Finn, and Sergey Levine. A workflow for offline model-free robotic reinforcement learning. arXiv preprint arXiv:2109.10813, 2021. 1   \n[21] Aviral Kumar, Anikait Singh, Frederik Ebert, Mitsuhiko Nakamoto, Yanlai Yang, Chelsea Finn, and Sergey Levine. Pre-training for robots: Offilne rl enables learning new tasks from a handful of trials, 2023. 1   \n[22] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS\u201917, 2017. 4, 7   \n[23] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems, 2020. URL https://arxiv.org/abs/ 2005.01643. 1, 3, 5   \n[24] Cong Lu, Philip Ball, Jack Parker-Holder, Michael Osborne, and Stephen J. Roberts. Revisiting design choices in offline model based reinforcement learning. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=zz9hXVhf40. 2, 4, 9, 16, 19   \n[25] Cong Lu, Philip J. Ball, Tim G. J. Rudner, Jack Parker-Holder, Michael A Osborne, and Yee Whye Teh. Challenges and opportunities in offline reinforcement learning from visual observations. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id $=$ 1QqIfGZOWu. 17, 20   \n[26] Rafael Rafailov, Tianhe Yu, Aravind Rajeswaran, and Chelsea Finn. Offline reinforcement learning from images with latent space models. In Proceedings of the 3rd Conference on Learning for Dynamics and Control, volume 144 of Proceedings of Machine Learning Research, pages 1154\u20131168. PMLR, 2021. 20   \n[27] Marc Rigter, Bruno Lacerda, and Nick Hawes. RAMBO-RL: Robust adversarial model-based offline reinforcement learning. In Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=nrksGSRT7kX. 9   \n[28] Chamani Shiranthika, Kuo-Wei Chen, Chung-Yih Wang, Chan-Yun Yang, BH Sudantha, and Wei-Fu Li. Supervised optimal chemotherapy regimen based on offilne reinforcement learning. IEEE Journal of Biomedical and Health Informatics, 26(9):4763\u20134772, 2022. 1   \n[29] Yihao Sun, Jiaji Zhang, Chengxing Jia, Haoxin Lin, Junyin Ye, and Yang Yu. ModelBellman inconsistency for model-based offline reinforcement learning. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 33177\u201333194. PMLR, 23\u201329 Jul 2023. URL https: //proceedings.mlr.press/v202/sun23q.html. 2, 4, 9, 16, 18   \n[30] Richard S. Sutton. Dyna, an integrated architecture for learning, planning, and reacting. SIGART Bull., 2(4):160\u2013163, jul 1991. ISSN 0163-5719. doi: 10.1145/122344.122377. URL https://doi.org/10.1145/122344.122377. 2, 4   \n[31] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press, second edition, 2018. URL http://incompleteideas.net/book/the-book-2nd. html. 3   \n[32] Shengpu Tang and Jenna Wiens. Model selection for offline reinforcement learning: Practical considerations for healthcare settings. In Machine Learning for Healthcare Conference, pages 2\u201335. PMLR, 2021. 1   \n[33] Denis Tarasov, Alexander Nikulin, Dmitry Akimov, Vladislav Kurenkov, and Sergey Kolesnikov. CORL: Research-oriented deep offline reinforcement learning library. In 3rd Offline RL Workshop: Offline RL as a \u201dLaunchpad\u201d, 2022. URL https://openreview.net/forum? id=SyAS49bBcv. 17   \n[34] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026\u20135033. IEEE, 2012. doi: 10.1109/IROS.2012.6386109. 8   \n[35] Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning, 2015. 10   \n[36] Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. MOPO: Model-based offilne policy optimization. In Advances in Neural Information Processing Systems, 2020. URL https://proceedings.neurips.cc/paper/ 2020/file/a322852ce0df73e204b7e67cbbef0d0a-Paper.pdf. 2, 4, 9, 16, 18, 19   \n[37] Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn. Combo: Conservative offline model-based policy optimization. In Advances in Neural Information Processing Systems, volume 34, pages 28954\u201328967, 2021. URL https://proceedings.neurips.cc/paper/2021/file/ f29a179746902e331572c483c45e5086-Paper.pdf. 9 ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Supplementary Material ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A A Unified Perspective of Model-Based and Model-Free RL 15 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Definitions . 15   \nA.2 Q-Learning Conditions . 15   \nA.3 Comparison Between Model-Free and Model-Based Methods . . 15 ", "page_idx": 13}, {"type": "text", "text": "B Error Propagation Result 16 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Proof 16   \nB.2 Extension to Stochastic Environments and Practical Implementation 16 ", "page_idx": 13}, {"type": "text", "text": "C Implementation Details 16 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "C.1 Algorithm 16   \nC.2 Hyperparameters . 17   \nC.3 Pixel-Based Hyperparameters 17 ", "page_idx": 13}, {"type": "text", "text": "D Additional Tables 18 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "D.1 Existing Methods\u2019 Penalties All Collapse to Zero Under the True Dynamics . . . . 18   \nD.2 Model Errors Cannot Explain the $Q$ -value Overestimation . . 18 ", "page_idx": 13}, {"type": "text", "text": "E Additional Visualizations 19 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "E.1 Comparison to Prior Approaches 19   \nE.2 Behaviour of Rollouts Over Training . . 19 ", "page_idx": 13}, {"type": "text", "text": "F Evaluation on the Pixel-Based V-D4RL Benchmark 20 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "G Runtime and Hyperparameter Sensitivity Ablations 20 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "G.1 Runtime 20   \nG.2 Hyperparameter Sensitivity Ablations 21 ", "page_idx": 13}, {"type": "text", "text": "H Summary of Setups Used in Comparisons 21 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A A Unified Perspective of Model-Based and Model-Free RL ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We supplement the discussion in Section 3.2 with a more thorough comparison of the out-of-sample and edge-of-reach problems, including how they relate to model-free and model-based approaches. ", "page_idx": 14}, {"type": "text", "text": "A.1 Definitions ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Consider a dataset of transition tuples $\\mathcal{D}=\\{(s_{i},a_{i},r_{i},s_{i}^{\\prime},d_{i})\\}_{i=1,\\ldots,N}$ collected according to some dataset policy $\\pi^{\\mathcal{D}}(\\cdot|s)$ . Compared to Section 2, we include the addition of a done indicator $d_{i}$ , where $d_{i}\\,=\\,1\\$ indicates episode termination (and $d_{i}\\,=\\,0$ otherwise). Transition tuples thus consist of state, action, reward, nextstate, done. Consider the marginal distribution over state-actions $\\rho_{s,a}^{D}(\\cdot,\\cdot)$ , over states $\\rho_{s}^{\\mathcal{D}}(\\cdot)$ , and conditional action distribution $\\rho_{a|s}^{D}(\\cdot|s)$ . Note that $\\rho_{a|s}^{\\mathcal{D}}(\\cdot|s)=\\pi^{\\mathcal{D}}(\\cdot|s)$ . We abbreviate $x$ is in distribution with respect to $\\rho$ as $\\boldsymbol{x}\\in^{\\mathrm{dist}}\\rho$ . ", "page_idx": 14}, {"type": "text", "text": "A.2 Q-Learning Conditions ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "As described in Section 2, given some policy $\\pi$ , we can attempt to learn the corresponding $Q$ -function with the following iterative process: ", "page_idx": 14}, {"type": "equation", "text": "$$\nQ^{k+1}\\gets\\underset{Q}{\\mathrm{arg\\,min}}\\;\\mathbb{E}_{(s,a,r,s^{\\prime})\\sim\\mathcal{D},a^{\\prime}\\sim\\pi(\\cdot|s^{\\prime})}[(\\underset{\\underset{\\mathrm{input}}{\\mathrm{Q}}}{Q}(s,a)-\\underset{\\underset{\\mathrm{Bellman\\,target}}{\\mathrm{V}}}{\\underbrace{[r+\\gamma(1-d)Q^{k}(s^{\\prime},a^{\\prime})]}})^{2}]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "$Q$ -learning relies on bootstrapping, hence to be successful we need to be able to learn accurate estimates of the Bellman targets for all $(s,a)$ inputs. Bootstrapped estimates of $Q(s^{\\prime},a^{\\prime})$ are used in the targets whenever $d\\neq1$ . Therefore, for all $(\\bar{s}^{\\prime},a^{\\prime})$ , we require: ", "page_idx": 14}, {"type": "text", "text": "In the main paper, we use this combined state-action perspective for simplicity, however, we can equivalently divide this state-action condition into independent requirements on the state and action as follows: ", "page_idx": 14}, {"type": "text", "text": "State condition: $s^{\\prime}\\in^{\\mathrm{dist}}\\rho_{s}^{D}$ or $d=1$ , Action condition: $a^{\\prime}\\in^{\\mathrm{dist}}\\rho_{a|s}^{\\mathcal{D}}(s^{\\prime})$ (given the above condition is met and $d\\neq1$ ). ", "page_idx": 14}, {"type": "text", "text": "Informally, the state condition may be violated if $\\mathcal{D}$ consists of partial or truncated trajectories, and the action condition may be violated if there is a significant distribution shift between $\\bar{\\pi}^{\\mathcal{D}}$ and $\\pi$ . ", "page_idx": 14}, {"type": "text", "text": "A.3 Comparison Between Model-Free and Model-Based Methods ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In offline model-free RL, $\\mathcal{D}=\\mathcal{D}_{\\mathrm{offine}}$ , with $\\pi^{\\mathcal{D}}=\\pi^{\\beta}$ . For the settings we consider, $\\mathcal{D}_{\\mathrm{offine}}$ consists of full trajectories and therefore will not violate the state condition. However, this may happen in a more general setting with $\\mathcal{D}_{\\mathrm{offine}}$ containing truncated trajectories. By contrast, the mismatch between $\\pi$ (used to sample $a^{\\prime}$ in $Q$ -learning) and $\\pi^{\\beta}$ (used to sample $a$ in the dataset $\\mathcal{D}_{\\mathrm{offline}})$ often does lead to significant violation of the action condition. This exacerbates the overestimation bias in $Q$ -learning (see Section 7), and can result in pathological training dynamics and $Q$ -value explosion over training [18]. ", "page_idx": 14}, {"type": "text", "text": "On the other hand, in offline model-based RL, the dataset $\\mathcal{D}~=~\\mathcal{D}_{\\mathrm{rollouts}}$ is collected on-policy according to the current (or recent) policy such that $\\pi^{\\mathcal{D}}\\approx\\pi$ . This minimal mismatch between $\\pi^{{\\check{D}}}$ and $\\pi$ means the action condition is not violated and can be considered to be resolved due to the collection of additional data. However, the procedure of generating the data $\\mathcal{D}=\\mathcal{D}_{\\mathrm{rollouts}}$ can be seen to significantly exacerbate the state condition problem, as the use of short truncated-horizon trajectories means the resulting dataset $\\mathcal{D}_{\\mathrm{rollouts}}$ is likely to violate the state condition. Due to lack of exploration, certain states may temporarily violate the state condition. Our paper then considers the pathological case of edge-of-reach states, which will always violate the state condition. ", "page_idx": 14}, {"type": "text", "text": "This comparison between model-based and model-free is summarized in Table 3 ", "page_idx": 14}, {"type": "text", "text": "Table 3: A summary of the comparison between model-free and model-based offline RL in relation to the conditions on $Q$ -learning as described in Appendix A. ", "page_idx": 15}, {"type": "table", "img_path": "3dn1hINA6o/tmp/0809a36831ad4cc5805fc0c18648abc515456aea7a446eafb27c420b89419dd0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "B Error Propagation Result ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proposition 1 (Error propagation from edge-of-reach states). Consider a rollout of length $k$ , $\\left(s_{0},a_{0},s_{1},\\ldots,s_{k}\\right)$ . Suppose that the state $s_{k}$ is edge-of-reach and the approximate value function $Q^{j}(s_{k},\\pi(s_{k}))$ has error \u03f5. Then, standard value iteration will compound error $\\gamma^{k-t}\\epsilon$ to the estimates of $Q^{j+1}(s_{t},a_{t})$ for $t=1,\\ldots,k-1$ . (Proof in Appendix B.) ", "page_idx": 15}, {"type": "text", "text": "B.1 Proof ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we provide a proof of Proposition 1. Our proof follows analogous logic to the error propagation result of Kumar et al. [18]. ", "page_idx": 15}, {"type": "text", "text": "Proof. Let us denote $Q^{*}$ as the optimal value function, $\\zeta_{j}(s,a)=|Q_{j}(s,a)-Q^{*}(s,a)|$ the error at iteration $j$ of Q-Learning, and $\\delta_{j}\\bar{(s,a)}=|Q_{j}(s,a)-\\mathcal{T}\\dot{Q}_{j-1}(s,a)|$ the current Bellman error. Then first considering the $t=k-1$ case, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\zeta_{j}(s_{t},a_{t})=|Q_{j}(s_{t},a_{t})-Q^{*}(s_{t},a_{t})|}\\\\ &{\\phantom{\\zeta_{j}(s_{t},a_{t})=}=|Q_{j}(s_{t},a_{t})-T Q_{j-1}(s_{t},a_{t})+T Q_{j-1}(s_{t},a_{t})-Q^{*}(s_{t},a_{t})|}\\\\ &{\\phantom{\\zeta_{j}(s_{t},a_{t})=}\\leq|Q_{j}(s_{t},a_{t})-T Q_{j-1}(s_{t},a_{t})|+|T Q_{j-1}(s_{t},a_{t})-Q^{*}(s_{t},a_{t})|}\\\\ &{\\phantom{\\zeta_{j}(s_{t},a_{t})=}=\\delta_{j}(s_{t},a_{t})+\\gamma\\zeta_{j-1}(s_{t+1},a_{t+1})}\\\\ &{\\phantom{\\zeta_{j}(s_{t},a_{t})=}=\\delta_{j}(s_{t},a_{t})+\\gamma\\epsilon}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus the errors at edge-of-reach states are discounted and then compounded with new errors at $Q^{j}\\big(s_{k-1},a_{k-1}\\big)$ . For $t<k-1$ , the result follows from repeated application of Equation (7) along the rollout. ", "page_idx": 15}, {"type": "text", "text": "B.2 Extension to Stochastic Environments and Practical Implementation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "With stochastic transition models (e.g. Gaussian models), we may have the case that no state will truly have zero density, in which case we relax the definition of edge-of-reach states slightly (see Section 3.3) from $\\rho_{t,\\pi}\\dot{(s)}=0$ to $\\rho_{t,\\pi}(s)<\\epsilon$ for some small $\\epsilon$ . ", "page_idx": 15}, {"type": "text", "text": "During optimization, in practice, model rollouts are sampled in minibatches and thus the above error propagation effect will occur on average throughout training. An analogous model-free statement has been given in Kumar et al. [18]; however, its significance in the context of model-based methods was previously not considered. ", "page_idx": 15}, {"type": "text", "text": "C Implementation Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we provide full implementation details for RAVL. ", "page_idx": 15}, {"type": "text", "text": "C.1 Algorithm ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We use the base model-based procedure as given in Algorithm 1 and shared across model-based offline RL methods [14, 24, 29, 36]. This involves using short MBPO-style [13] model rollouts to train an agent based on SAC [11]. We modify the SAC agent with the value pessimism losses of EDAC [1]. Our dynamics model follows the standard setup in model-based offilne algorithms, being realized as a deep ensemble [5] and trained via maximum likelihood estimation. ", "page_idx": 15}, {"type": "text", "text": "C.2 Hyperparameters ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For the D4RL [7] MuJoCo results presented in Table 2, we sweep over the following hyperparameters and list the choices used in Table 4. \u201cBase\u201d refers to the shared base procedure in model-based offilne RL shown in Algorithm 1. ", "page_idx": 16}, {"type": "text", "text": "\u2022 (EDAC) Number of $Q$ -ensemble elements $N_{\\mathrm{critic}}$ , in the range $\\{10,50\\}$ \u2022 (EDAC) Ensemble diversity weight $\\eta$ , in the range $\\{1,10,100\\}$ \u2022 (Base) Model rollout length $k$ , in the range $\\{1,5\\}$ \u2022 (Base) Real-to-synthetic data ratio $r$ , in the range $\\{0.05,0.5\\}$ ", "page_idx": 16}, {"type": "text", "text": "The remaining model-based and agent hyperparameters are given in Table 5. Almost all environments use a small $N_{\\mathrm{critic}}=10$ , with only the Hopper-medexp dataset needing $N_{\\mathrm{critic}}=30$ . ", "page_idx": 16}, {"type": "text", "text": "For the uncertainty-free dynamics model experiments in Table 1 we take the same hyperparameters as for the learned dynamics model for $N,k,$ , and $r$ , and try different settings for $\\eta\\in\\dot{\\{1,10,100,200\\}}$ . The choices used are shown in Table 4. Note that for existing methods based on dynamics penalties, the analogous hyperparameter (penalty weighting) has no effect under the true dynamics model as the penalties will always be zero (since dynamics uncertainty is zero). For the experiments in Figure 1 with intermediate dynamics model accuracies, we tune the ensemble diversity coefficient $\\eta$ over $\\eta\\in\\{1,10,100,200\\}$ for RAVL, and analogously we tune the uncertainty penalty coefficient $\\lambda$ over $\\lambda\\in\\{1,5,10,100,200\\}$ for MOPO. ", "page_idx": 16}, {"type": "text", "text": "Our implementation is based on the Clean Offilne Reinforcement Learning (CORL, Tarasov et al. [33]) repository, released at https://github.com/tinkoff-ai/CORL under an Apache-2.0 license. Our algorithm takes on average 6 hours to run using a V100 GPU for the full number of epochs. ", "page_idx": 16}, {"type": "table", "img_path": "3dn1hINA6o/tmp/70cfa7126e81f70ecf4385ba7e54ba477ede1840b0c787db937e6125d5b12cd0.jpg", "table_caption": ["Table 4: Variable hyperparameters for RAVL used in D4RL MuJoCo locomotion tasks with the learned dynamics (see Table 2). With the uncertainty-free dynamics (see Table 1), we tune only $\\eta$ , with the settings used shown in brackets. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "3dn1hINA6o/tmp/b4869e07f1f7ae27e34a368b6165f62aebb994c044ab1c27879e8f727fcd51c9.jpg", "table_caption": ["Table 5: Fixed hyperparameters for RAVL used in D4RL MuJoCo locomotion tasks. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "C.3 Pixel-Based Hyperparameters ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For the V-D4RL [25] DeepMind Control Suite datasets presented in Table 9, we use the default hyperparameters for the Offilne DV2 algorithm, which are given in Table 6. We found keeping the ", "page_idx": 16}, {"type": "text", "text": "uncertainty weight $\\lambda=10$ improved performance over $\\lambda=0$ which shows RAVL can be combined with dynamics penalty-based methods. ", "page_idx": 17}, {"type": "table", "img_path": "3dn1hINA6o/tmp/e1c5d7437862716a7dfbc30b420a2a99717b253b71f94fdc10a2e51e72887e28.jpg", "table_caption": ["Table 6: Hyperparameters for RAVL used in V-D4RL DeepMind Control Suite tasks. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "We used a hyperparameter sweep over $\\{2,5,20\\}$ for $\\mathbf{N}_{\\mathrm{critic}}$ but found a single value of 5 worked well for all environments we consider. ", "page_idx": 17}, {"type": "text", "text": "D Additional Tables ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "D.1 Existing Methods\u2019 Penalties All Collapse to Zero Under the True Dynamics ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "For completeness, in Table 7 we include the expressions for the various dynamics penalties proposed by the most popular model-based approaches. These penalties are all based on estimates of dynamics model uncertainty. Since there is no uncertainty under the true model, all these penalties collapse to zero with the true dynamics. ", "page_idx": 17}, {"type": "table", "img_path": "3dn1hINA6o/tmp/7643d8686bb7ea12d8895d307148d9e083665402e518cef79cc6cbaccfabfea0.jpg", "table_caption": ["Table 7: We show here how all existing dynamics penalized offline MBRL algorithms reduce to the same base procedure when estimated (epistemic) model uncertainty is zero (as it is with the true dynamics). "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "D.2 Model Errors Cannot Explain the $Q$ -value Overestimation ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In Figure 3 we observe that the base offline model-based procedure results in $Q$ -values exploding unboundedly to beyond $10^{10}$ over training. Existing methods attribute this to dynamics model errors and the consequent exploitation of state-action space areas where the model incorrectly predicts high reward. ", "page_idx": 17}, {"type": "text", "text": "In Table 8 we examine the rewards collected by an agent in the learned dynamics model. Under the \u2018model errors\u2019 explanation, we would expect to see rewards sampled by the agent in the learned model to be significantly higher than under the true environment. Furthermore, to explain the $Q$ -values being on the order of $\\mathrm{\\dot{10^{10}}}$ , with $\\gamma=0.99$ the agent would need to sample rewards on the order of $10^{8}$ . However, the per-step rewards sampled by the agent are on the order of $10^{0}$ (and are not higher than with the true environment). ", "page_idx": 17}, {"type": "text", "text": "Model errors and model exploitation therefore cannot explain the explosion in $Q$ -values seen over training. By contrast, the edge-of-reach problem exactly predicts this extreme value overestimation behavior. ", "page_idx": 17}, {"type": "text", "text": "Table 8: Per-step rewards with the base offline model-based procedure (see Algorithm 1). Rewards in model rollouts are close to those with the true dynamics, showing that model exploitation could not explain the subsequent value overestimation. Further, it indicates (as in Janner et al. [13], Lu et al. [24]) that the model is largely accurate for short rollouts and hence is unlikely to be vulnerable to model exploitation. ", "page_idx": 18}, {"type": "table", "img_path": "3dn1hINA6o/tmp/715ffd25f00920560114decf38af8841523bc3b0a14133f497f085ca7b906679.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "E Additional Visualizations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "E.1 Comparison to Prior Approaches ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Figure 6, we plot the dynamics uncertainty-based penalty used in MOPO [36] against the effective penalty in RAVL of variance of the value ensemble. The positive correlation between MOPO\u2019s penalty and the edge-of-reach states targeting penalty of RAVL indicates why prior methods may work despite not considering the crucial edge-of-reach problem. ", "page_idx": 18}, {"type": "image", "img_path": "3dn1hINA6o/tmp/6674073f8201587cfc69cbc9365cb210338841173818a3f977810e58c868b0a5.jpg", "img_caption": ["Figure 6: We find that the dynamics uncertainty-based penalty used in MOPO [36] is positively correlated with the variance of the value ensemble of RAVL, suggesting prior methods may unintentionally address the edge-ofreach problem. Pearson correlation coefficients are 0.49, 0.43, and 0.27 for Hopper-mixed, Walker2d-medexp, and Halfcheetah-medium respectively. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "E.2 Behaviour of Rollouts Over Training ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In Figure 7, we provide a visualization of the rollouts sampled over training in the simple environment for each of the algorithms analyzed in Figure 4 (see Section 4). This accompanies the discussion in Section 4.2 of the behavior over training. ", "page_idx": 18}, {"type": "image", "img_path": "3dn1hINA6o/tmp/415f8e2db03238d09de3bd8edbcc063417addbb1c19498cd583c2ae7bd159d16.jpg", "img_caption": ["Figure 7: A visualization of the rollouts sampled over training on the simple environment in Section 4. We note the pathological behavior of the baseline, and then the success of the ideal intervention Base-OraclePatch, and our practically realizable method RAVL. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "F Evaluation on the Pixel-Based V-D4RL Benchmark ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The insights from RAVL also lead to improvements in performance on the challenging pixel-based benchmark V-D4RL [25]. The base procedure in this pixel-based setting uses the Offilne DreamerV2 algorithm of training on trajectories in a learned latent space. ", "page_idx": 19}, {"type": "text", "text": "We observe in Table 9 that RAVL gives a strong boost in performance on the medium and medexp level datasets, while helping less in the more diverse random and mixed datasets. This observation fits with our intuition of the edge-of-reach problem: the medium and medexp level datasets are likely to have less coverage of the state space, and thus we would expect them to suffer more from edge-of-reach issues and the \u2018edge-of-reach seeking behavior\u2019 demonstrated in Section 4.2. The performance improvements over Offline DreamerV2 and other model-based baselines including LOMPO [26] therefore suggests that the edge-of-reach problem is general and widespread. ", "page_idx": 19}, {"type": "text", "text": "We also note that these results are without the ensemble diversity regularizer from EDAC [1] (used on the D4RL benchmark), which we anticipate may further increase performance. ", "page_idx": 19}, {"type": "text", "text": "Table 9: We show that RAVL extends to the challenging pixel-based V-D4RL benchmark, suggesting the out-of-reach problem is also present in the latent-space setting used by the base procedure for pixel-based algorithms. Mean and standard deviation given over 6 seeds. ", "page_idx": 19}, {"type": "table", "img_path": "3dn1hINA6o/tmp/4020cd3436650c80f562136f1d15d53c4c064b53e871ece82a26ad31a3d95b70.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "G Runtime and Hyperparameter Sensitivity Ablations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "G.1 Runtime ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We compare RAVL to SOTA method MOBILE. The additional requirement for RAVL compared to MOBILE is that RAVL requires increasing the $Q$ -ensemble size: from $N=2$ in MOBILE to $N=10$ in RAVL (for all environments except one where we use $N=30$ ). The additional requirement for MOBILE compared to RAVL is that MOBILE requires multiple additional forwards passes through the model for each update step (for computing their dynamics uncertainty penalty). ", "page_idx": 19}, {"type": "text", "text": "In Table 10 we show that ensemble size can be scaled with very minimal effect on the runtime. Even increasing the ensemble size to $N=100$ (far beyond the maximum ensemble size used by RAVL) only increases the runtime by at most $1\\%$ . Table 7 of MOBILE reports that MOBILE\u2019s requirement of additional forward passes increases runtime by around $14\\%$ .6 ", "page_idx": 20}, {"type": "text", "text": "Table 10: Timing experiments showing that ensemble size for standard vectorized ensembles can be scaled with extremely minimal effect on runtime. Runtime increases by just $1\\%$ with $Q$ -ensemble size increased from $N=2$ (as used in the base procedure SAC agent) to $N=100$ . RAVL uses $N=10$ (for all environments except one where we use $N=30$ ), meaning the runtime increase compared to the base model-based procedure is almost negligible. Other methods add more computationally expensive changes to the base procedure, hence making RAVL significantly faster than SOTA (see Appendix G.1). ", "page_idx": 20}, {"type": "table", "img_path": "3dn1hINA6o/tmp/0e6e901add99b7146db0e8b6c4eb4c6902346efc8d47eb023dc95011fa7def72.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "G.2 Hyperparameter Sensitivity Ablations ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In Table 11 we include results with different settings of RAVL\u2019s ensmble diversity regularizer hyperparameter $\\eta$ . We note a minimal change in performance with a sweep across two orders of magnitude. This is an indication that RAVL may be applied to new settings without much tuning. ", "page_idx": 20}, {"type": "text", "text": "For RAVL\u2019s second hyperparameter of $Q$ -ensemble size $N$ , we note that in the main benchmarking of RAVL against other offline methods (see Table 2) we use the same value of $N=10$ across all except one environment. This again indicates that RAVL is robust and should transfer to new settings with minimal hyperparameter tuning. ", "page_idx": 20}, {"type": "table", "img_path": "3dn1hINA6o/tmp/2175088c3544e1d07de2416ff342a0379b3afa5cd7b9f428d557e369b277fc37.jpg", "table_caption": ["Table 11: Ablations results over RAVL\u2019s diversity regularizer hyperparameter $\\eta$ . "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "H Summary of Setups Used in Comparisons ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Throughout the paper, we compare several different setups in order to identify the true underlying issues in model-based offilne RL. We provide a summary of them in Table 12. More comprehensive descriptions of each are given in the relevant positions in the main text and table and figure captions. ", "page_idx": 20}, {"type": "text", "text": "Table 12: We summarize the various setups used for comparisons throughout the paper. \u2018\\*\u2019 denotes application to the simple environment (see Section 4). All methods use $k$ -step rollouts from the offline dataset (or from a fixed starting state distribution in the case of the simple environment). ", "page_idx": 20}, {"type": "table", "img_path": "3dn1hINA6o/tmp/0cf368b91c313ed647d97d2219019974e450269a4fcbae79f9f4cfae2f06c1fa.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The abstract and introduction clearly define the scope of the paper and its contributions. For our main result (of fixing the failure under the true dynamics model) we provide full results over the standard D4RL benchmark [7] with 6 seeds (see Table 1). For our central hypothesis, we include extensive analysis, both on the standard D4RL benchmark (Section 3.4) and through the construction of a simple environment (Section 4). ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We have a dedicated section for this (Section 9) where we discuss the limitations of our analysis and how our work may be extended, in particular how it would be interesting to examine the interactions of the edge-of-reach problem in settings with less accurate dynamics models. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We have one proposition in Appendix B. For this we clearly state all the assumptions, provide a full proof, and also include a discussion of the practical implications and limitations. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provide complete details of hyperparameters and training settings in Appendix C. For all the analysis experiments in particular, we aim for the reader to understand exactly what we have done such that they can appreciate the results fully. As such we have described them in detail (for example Section 4), including further clarification of the terms we have used in Table 12. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 22}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We have open-sourced our code at https://anonymous.4open.science/ r/edge-of-reach-8096, and have provided a full set of hyperparameters and training settings in Appendix C. The code is clean and has all the defaults set to the exact training settings used. Furthermore, the README contains simple setup instructions and the exact bash command needed to reproduce the main results. All the datasets used are standard open-sourced benchmarks. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: For the benchmark experiments we have provided full detail of the hyperparameters and training settings used in Appendix C. Note also, that we have made sure to match all the default settings exactly in line with the base procedure of other papers in this field to allow for meaningful comparison. For each of the analysis experiments, we have aimed for the reader to completely understand the exact setup such that they can fully appreciate the results. As such we have described the setup of each in detail. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: All the results are over at least 4 seeds, and we use 6 seeds for the main benchmarking results. We have made sure to provide error bars in all relevant tables and figures, and in each case have stated what they show in the accompanying caption. In tables, we have highlighted statistical significance appropriately where relevant. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We have provided average runtime statistics in Appendix C.2 for the main benchmarking results (including the type of machine), and have also provided detailed comparative timing results and discussion in Appendix G.1. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We have read and adhere to the code of ethics. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper discusses the general potential impact of offline RL in Section 1. In terms of societal impact, the paper is on simulated control and we do not believe it has relevance in the way meant by this question. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper poses no such risk, we do not release any models or datasets. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have provided the appropriate reference to the repository that our code is based on in Appendix C including the accompanying license. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We release our code (currently anonymized) at the link provided. It is well documented, with clear straightforward instructions on how to reproduce the experiments. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}]