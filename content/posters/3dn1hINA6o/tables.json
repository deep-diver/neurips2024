[{"figure_path": "3dn1hINA6o/tables/tables_2_1.jpg", "caption": "Table 2: A comprehensive evaluation of RAVL over the standard D4RL MuJoCo benchmark. We show the mean and standard deviation of the final performance averaged over 6 seeds. Our simple approach largely matches the state-of-the-art without any explicit dynamics penalization and hence works even in the absence of model uncertainty (where dynamics uncertainty-based methods fail) (see Table 1).", "description": "This table presents a comprehensive comparison of the proposed RAVL algorithm against other state-of-the-art methods on the standard D4RL MuJoCo benchmark.  The results demonstrate that RAVL achieves comparable performance to the best existing methods, while being significantly more robust to the absence of model uncertainty. The table highlights the mean and standard deviation of the final performance for each algorithm across six different seeds, providing strong statistical support for the claims made in the paper. ", "section": "6.2 D4RL with the True Dynamics"}, {"figure_path": "3dn1hINA6o/tables/tables_7_1.jpg", "caption": "Table 1: True dynamics (zero error, zero uncertainty) Existing model-based methods are presented as different approaches for dealing with dynamics model errors. Surprisingly, however, all existing methods fail in the absence of dynamics errors (when the learned approximate model is replaced by the true model). This reveals that existing methods are unintentionally using their dynamics uncertainty estimates to address the previously unnoticed edge-of-reach problem. By contrast, RAVL directly addresses the edge-of-reach problem and hence does not fail when dynamics uncertainty is zero. Experiments are on the D4RL MuJoCo v2 datasets. Statistical significance highlighted (6 seeds). **Note that while labeled as 'MOBILE', the results with the true dynamics will be identical for any other dynamics penalty-based method since penalties under the true model are all zero (see Table 7). Hence these results indicate the failure of all existing dynamics uncertainty-based methods.", "description": "This table presents the performance of existing model-based offline reinforcement learning methods and the proposed RAVL method when using the true dynamics (zero error, zero uncertainty).  It demonstrates the failure of existing methods under perfect dynamics, highlighting the importance of addressing the edge-of-reach problem.  RAVL, in contrast, maintains high performance even with perfect dynamics, illustrating its robustness.", "section": "6.2 D4RL with the True Dynamics"}, {"figure_path": "3dn1hINA6o/tables/tables_8_1.jpg", "caption": "Table 2: A comprehensive evaluation of RAVL over the standard D4RL MuJoCo benchmark. We show the mean and standard deviation of the final performance averaged over 6 seeds. Our simple approach largely matches the state-of-the-art without any explicit dynamics penalization and hence works even in the absence of model uncertainty (where dynamics uncertainty-based methods fail) (see Table 1).", "description": "This table presents a comparison of the performance of RAVL against other state-of-the-art offline reinforcement learning algorithms on the D4RL MuJoCo benchmark.  The results are averaged over six seeds and show the mean and standard deviation of the final performance.  A key highlight is that RAVL performs competitively even in the absence of model uncertainty, unlike other methods that rely on dynamics penalization, showcasing its robustness and effectiveness.", "section": "6.2 D4RL with the True Dynamics"}, {"figure_path": "3dn1hINA6o/tables/tables_15_1.jpg", "caption": "Table 1: True dynamics (zero error, zero uncertainty) Existing model-based methods are presented as different approaches for dealing with dynamics model errors. Surprisingly, however, all existing methods fail in the absence of dynamics errors (when the learned approximate model is replaced by the true model). This reveals that existing methods are unintentionally using their dynamics uncertainty estimates to address the previously unnoticed edge-of-reach problem. By contrast, RAVL directly addresses the edge-of-reach problem and hence does not fail when dynamics uncertainty is zero. Experiments are on the D4RL MuJoCo v2 datasets. Statistical significance highlighted (6 seeds). **Note that while labeled as 'MOBILE', the results with the true dynamics will be identical for any other dynamics penalty-based method since penalties under the true model are all zero (see Table 7). Hence these results indicate the failure of all existing dynamics uncertainty-based methods.", "description": "This table presents the results of experiments conducted on the D4RL MuJoCo v2 datasets using both approximate and true dynamics models. It compares the performance of RAVL and existing model-based methods that rely on dynamics uncertainty penalties. A key finding is that existing methods fail when using the true dynamics, unlike RAVL which performs well. This demonstrates RAVL's robustness and its ability to address the edge-of-reach problem, a novel issue identified in the paper.", "section": "6.2 D4RL with the True Dynamics"}, {"figure_path": "3dn1hINA6o/tables/tables_16_1.jpg", "caption": "Table 4: Variable hyperparameters for RAVL used in D4RL MuJoCo locomotion tasks with the learned dynamics (see Table 2). With the uncertainty-free dynamics (see Table 1), we tune only \u03b7, with the settings used shown in brackets.", "description": "This table shows the hyperparameter settings used for RAVL in the D4RL MuJoCo locomotion experiments.  It lists the values for four hyperparameters:  Ncritic (number of Q-ensemble elements), \u03b7 (ensemble diversity weight), k (model rollout length), and r (real-to-synthetic data ratio).  Different values were used for the experiments with learned dynamics (Table 2) and for experiments with uncertainty-free dynamics (Table 1), where only \u03b7 was tuned.  The values in parentheses indicate the settings used for the uncertainty-free dynamics experiments.", "section": "Implementation Details"}, {"figure_path": "3dn1hINA6o/tables/tables_16_2.jpg", "caption": "Table 5: Fixed hyperparameters for RAVL used in D4RL MuJoCo locomotion tasks.", "description": "This table lists the fixed hyperparameters used in the Reach-Aware Value Learning (RAVL) method for the Deep Data-Driven Reinforcement Learning (D4RL) MuJoCo locomotion tasks.  These are the hyperparameters that were held constant across all experiments in the paper.  These hyperparameters include the number of training epochs, the discount factor (gamma), the learning rate, batch size, buffer retain epochs, and the number of rollouts used in the training process. The values of these hyperparameters were determined through preliminary experiments and are reported as fixed for reproducibility and to ensure a fair comparison between RAVL and other methods.", "section": "Implementation Details"}, {"figure_path": "3dn1hINA6o/tables/tables_17_1.jpg", "caption": "Table 6: Hyperparameters for RAVL used in V-D4RL DeepMind Control Suite tasks.", "description": "This table lists the hyperparameters used for the Reach-Aware Value Learning (RAVL) algorithm when applied to the V-D4RL DeepMind Control Suite tasks.  The hyperparameters cover various aspects of the model, including ensemble size, imagination horizon, batch size, sequence length, action repeat, observation size, discount factor, optimizer, learning rates, training epochs for both the model and agent, uncertainty penalty type, uncertainty weight, and the number of critics used.  These settings are crucial for the algorithm's performance and stability.", "section": "Implementation Details"}, {"figure_path": "3dn1hINA6o/tables/tables_17_2.jpg", "caption": "Table 7: We show here how all existing dynamics penalized offline MBRL algorithms reduce to the same base procedure when estimated (epistemic) model uncertainty is zero (as it is with the true dynamics).", "description": "This table presents the penalty terms used by three popular offline model-based reinforcement learning algorithms: MOPO, MOREL, and MOBILE.  The table shows that all three algorithms utilize penalties based on estimated dynamics model uncertainty. Importantly, the table demonstrates that when the dynamics model is the true model (and thus there is no uncertainty), the penalty terms all collapse to zero.", "section": "D.1 Existing Methods' Penalties All Collapse to Zero Under the True Dynamics"}, {"figure_path": "3dn1hINA6o/tables/tables_18_1.jpg", "caption": "Table 8: Per-step rewards with the base offline model-based procedure (see Algorithm 1). Rewards in model rollouts are close to those with the true dynamics, showing that model exploitation could not explain the subsequent value overestimation. Further, it indicates (as in Janner et al. [13], Lu et al. [24]) that the model is largely accurate for short rollouts and hence is unlikely to be vulnerable to model exploitation.", "description": "This table compares the per-step rewards obtained using the base offline model-based method with those from the true dynamics. The similarity in rewards suggests that model exploitation is not the primary cause of the value overestimation problem, as existing methods suggest. This supports the paper's argument that the edge-of-reach problem is a more significant factor.", "section": "Additional Visualizations"}, {"figure_path": "3dn1hINA6o/tables/tables_19_1.jpg", "caption": "Table 9: We show that RAVL extends to the challenging pixel-based V-D4RL benchmark, suggesting the out-of-reach problem is also present in the latent-space setting used by the base procedure for pixel-based algorithms. Mean and standard deviation given over 6 seeds.", "description": "This table presents the results of RAVL and baseline methods on the pixel-based V-D4RL benchmark.  It shows that RAVL improves performance on the medium and medexp level datasets, which are less diverse, suggesting that the edge-of-reach problem is more prominent in less diverse environments.  The results also demonstrate that RAVL generalizes well to latent-space settings, which are used by the baseline DreamerV2 algorithm.", "section": "F Evaluation on the Pixel-Based V-D4RL Benchmark"}, {"figure_path": "3dn1hINA6o/tables/tables_20_1.jpg", "caption": "Table 2: A comprehensive evaluation of RAVL over the standard D4RL MuJoCo benchmark. We show the mean and standard deviation of the final performance averaged over 6 seeds. Our simple approach largely matches the state-of-the-art without any explicit dynamics penalization and hence works even in the absence of model uncertainty (where dynamics uncertainty-based methods fail) (see Table 1).", "description": "This table compares the performance of RAVL against other state-of-the-art methods on the D4RL MuJoCo benchmark.  It shows mean and standard deviation of final performance across six seeds for various environments (Halfcheetah, Hopper, Walker2d) and dataset conditions (random, medium, mixed, medexp). Notably, it highlights RAVL's performance even without explicit dynamics penalization, demonstrating its effectiveness in the absence of model uncertainty, a scenario where other methods fail.", "section": "6.2 D4RL with the True Dynamics"}, {"figure_path": "3dn1hINA6o/tables/tables_20_2.jpg", "caption": "Table 11: Ablations results over RAVL's diversity regularizer hyperparameter \u03b7.", "description": "This table shows the results of ablations on RAVL's diversity regularizer hyperparameter \u03b7.  The results show minimal change in performance with a sweep across two orders of magnitude, indicating that RAVL is robust and can be applied to new settings without much tuning.  The table includes results for Halfcheetah medium and Halfcheetah mixed environments.", "section": "G.2 Hyperparameter Sensitivity Ablations"}, {"figure_path": "3dn1hINA6o/tables/tables_20_3.jpg", "caption": "Table 12: We summarize the various setups used for comparisons throughout the paper. '*' denotes application to the simple environment (see Section 4). All methods use k-step rollouts from the offline dataset (or from a fixed starting state distribution in the case of the simple environment).", "description": "This table summarizes the different experimental setups used throughout the paper to compare various offline model-based reinforcement learning methods.  It specifies for each experiment the name used, the type of dynamics model (approximate ensemble or true dynamics), the presence of any penalty (dynamics uncertainty penalty, or none), and the hyperparameters of the agent (size of the Q-ensemble and ensemble diversity regularizer).  Different setups are used to isolate the edge-of-reach problem and test the effectiveness of the proposed method (RAVL) under different conditions.", "section": "H Summary of Setups Used in Comparisons"}]