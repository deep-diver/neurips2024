[{"figure_path": "VaXnxQ3UKo/figures/figures_2_1.jpg", "caption": "Figure 1: Our approach involves iterating through three distinct stages. (1) Collect a mathematical dataset that comprises questions and their corresponding final answers. (2) Employ MCTS on the policy and the value model to generate both correct and incorrect solution paths along with state values. (3) Optimize the policy and the value model with generated data from MCTS.", "description": "This figure illustrates the iterative training process of AlphaMath.  It starts with a mathematical dataset containing questions and answers.  The AlphaMath framework then uses Monte Carlo Tree Search (MCTS) with a policy model (LLM) and a value model to generate solution paths. These paths, including both correct and incorrect ones, along with their associated state values, are used to iteratively train and improve the policy and value models. This iterative process allows AlphaMath to learn and enhance its mathematical reasoning capabilities without explicit process supervision.", "section": "3 AlphaMath"}, {"figure_path": "VaXnxQ3UKo/figures/figures_3_1.jpg", "caption": "Figure 2: An overview of the four key operations in MCTS", "description": "This figure illustrates the four key operations of the Monte Carlo Tree Search (MCTS) algorithm used in the AlphaMath framework.  These steps are Selection, Expansion, Evaluation, and Backpropagation.  Selection involves choosing a node based on a variant of the PUCT algorithm. Expansion involves expanding the selected leaf node, generating new partial solutions from the LLM. Evaluation involves assessing the leaf nodes using a value model or reward function. Backpropagation involves updating the Q-values and visit counts of the nodes along the path from the leaf node to the root.", "section": "3 AlphaMath"}, {"figure_path": "VaXnxQ3UKo/figures/figures_8_1.jpg", "caption": "Figure 1: Our approach involves iterating through three distinct stages. (1) Collect a mathematical dataset that comprises questions and their corresponding final answers. (2) Employ MCTS on the policy and the value model to generate both correct and incorrect solution paths along with state values. (3) Optimize the policy and the value model with generated data from MCTS.", "description": "This figure illustrates the iterative training process of the AlphaMath framework.  It shows three main stages: data collection (question-answer pairs), MCTS application to generate solution paths with state values (both correct and incorrect), and model optimization using the generated data. The process is cyclical, with improved models feeding into subsequent iterations.", "section": "3 AlphaMath"}, {"figure_path": "VaXnxQ3UKo/figures/figures_8_2.jpg", "caption": "Figure 5: (Left) Fitted distribution of Q-values of 3rd round MCTS on the training set. (Right) Fitted distribution of Q-values via MCTS inference on the test set.", "description": "The figure shows the distribution of Q-values obtained from the Monte Carlo Tree Search (MCTS) algorithm. The left panel displays the distribution of Q-values from the training set in the third round of MCTS, while the right panel displays the distribution of Q-values obtained during MCTS inference on the test set.  The distribution for correct solutions in both cases shows a strong skew towards a value of 1, while the distribution for incorrect solutions is more spread out with a smaller skew towards -1, which reflects how well the value model is able to distinguish between correct and incorrect solution paths.", "section": "Analysis 3: Value model"}, {"figure_path": "VaXnxQ3UKo/figures/figures_8_3.jpg", "caption": "Figure 6: The Effects of Temperature on the performance of SBS.", "description": "The figure shows the performance of step-level beam search (SBS) with different beam sizes (B1 = 1 and B1 = 3) under various temperature settings.  The results are compared against a greedy approach.  It demonstrates that SBS significantly outperforms the greedy approach across all temperatures, and that higher temperatures generally lead to better performance for SBS, although there are diminishing returns at the highest temperature.", "section": "4.7 Analysis 5: The Effects of Temperature on Step-level Beam Search"}, {"figure_path": "VaXnxQ3UKo/figures/figures_14_1.jpg", "caption": "Figure 1: Our approach involves iterating through three distinct stages. (1) Collect a mathematical dataset that comprises questions and their corresponding final answers. (2) Employ MCTS on the policy and the value model to generate both correct and incorrect solution paths along with state values. (3) Optimize the policy and the value model with generated data from MCTS.", "description": "This figure illustrates the iterative training process of the AlphaMath framework. The framework begins by collecting a dataset of mathematical questions and answers (Stage 1).  Then, it uses Monte Carlo Tree Search (MCTS) along with a policy and value model to generate numerous solution paths, some correct and some incorrect, capturing the intermediate steps and their associated values (Stage 2). Finally, the policy and value models are jointly trained using this generated data, enabling them to learn effective mathematical reasoning strategies (Stage 3). This iterative process refines the models' abilities to autonomously generate and evaluate high-quality solutions.", "section": "3 AlphaMath"}, {"figure_path": "VaXnxQ3UKo/figures/figures_15_1.jpg", "caption": "Figure 1: Our approach involves iterating through three distinct stages. (1) Collect a mathematical dataset that comprises questions and their corresponding final answers. (2) Employ MCTS on the policy and the value model to generate both correct and incorrect solution paths along with state values. (3) Optimize the policy and the value model with generated data from MCTS.", "description": "This figure illustrates the iterative training process of the AlphaMath framework.  It begins with collecting a dataset of mathematical questions and answers. Then, the Monte Carlo Tree Search (MCTS) algorithm is used with the policy and value models to generate solution paths, which are labeled as correct or incorrect based on their final answers and given a state value. Finally, these generated paths and values are used to train and optimize the policy and value models, improving the accuracy of mathematical reasoning.", "section": "3 AlphaMath"}, {"figure_path": "VaXnxQ3UKo/figures/figures_16_1.jpg", "caption": "Figure 1: Our approach involves iterating through three distinct stages. (1) Collect a mathematical dataset that comprises questions and their corresponding final answers. (2) Employ MCTS on the policy and the value model to generate both correct and incorrect solution paths along with state values. (3) Optimize the policy and the value model with generated data from MCTS.", "description": "This figure illustrates the iterative training process of AlphaMath.  It starts with collecting a dataset of mathematical questions and their answers. Then, it uses Monte Carlo Tree Search (MCTS) with a policy model (LLM) and a value model to generate many solution paths, some correct and some incorrect. Finally, it uses these generated paths and their associated state values to train and improve both the policy and value models. This iterative process helps AlphaMath learn to generate high-quality solutions without human-provided process annotations.", "section": "3 AlphaMath"}, {"figure_path": "VaXnxQ3UKo/figures/figures_17_1.jpg", "caption": "Figure 1: Our approach involves iterating through three distinct stages. (1) Collect a mathematical dataset that comprises questions and their corresponding final answers. (2) Employ MCTS on the policy and the value model to generate both correct and incorrect solution paths along with state values. (3) Optimize the policy and the value model with generated data from MCTS.", "description": "The figure illustrates the iterative training process of the AlphaMath framework.  It begins with a mathematical dataset containing questions and answers (Stage 1). Then, Monte Carlo Tree Search (MCTS) is employed, using a policy model (LLM) and a value model to generate both correct and incorrect solution paths, along with their associated state values (Stage 2). Finally, these generated data (questions, solution paths, state values) are used to optimize both the policy and value models (Stage 3). This process is repeated iteratively to enhance the model's mathematical reasoning abilities.", "section": "3 AlphaMath"}, {"figure_path": "VaXnxQ3UKo/figures/figures_19_1.jpg", "caption": "Figure 1: Our approach involves iterating through three distinct stages. (1) Collect a mathematical dataset that comprises questions and their corresponding final answers. (2) Employ MCTS on the policy and the value model to generate both correct and incorrect solution paths along with state values. (3) Optimize the policy and the value model with generated data from MCTS.", "description": "This figure illustrates the iterative training process of AlphaMath.  It starts with a mathematical dataset containing questions and answers.  Then, Monte Carlo Tree Search (MCTS) is used with a policy model (LLM) and a value model to generate solution paths, labeled as correct or incorrect based on their final answers. Finally, the policy and value models are jointly optimized using the generated data from the MCTS process.", "section": "3 AlphaMath"}, {"figure_path": "VaXnxQ3UKo/figures/figures_20_1.jpg", "caption": "Figure 1: Our approach involves iterating through three distinct stages. (1) Collect a mathematical dataset that comprises questions and their corresponding final answers. (2) Employ MCTS on the policy and the value model to generate both correct and incorrect solution paths along with state values. (3) Optimize the policy and the value model with generated data from MCTS.", "description": "This figure illustrates the iterative training process of the AlphaMath framework. The process involves three main stages: data collection, MCTS-based solution path generation, and model optimization.  First, a dataset of mathematical questions and answers is collected.  Then, Monte Carlo Tree Search (MCTS) is used in conjunction with a policy model (LLM) and value model to generate numerous solution paths, along with associated state values representing the quality of intermediate steps. Finally, these generated paths and values are used to iteratively train and refine both the policy and value models, improving the overall accuracy and effectiveness of mathematical reasoning.", "section": "3 AlphaMath"}, {"figure_path": "VaXnxQ3UKo/figures/figures_23_1.jpg", "caption": "Figure 1: Our approach involves iterating through three distinct stages. (1) Collect a mathematical dataset that comprises questions and their corresponding final answers. (2) Employ MCTS on the policy and the value model to generate both correct and incorrect solution paths along with state values. (3) Optimize the policy and the value model with generated data from MCTS.", "description": "This figure illustrates the iterative training process of the AlphaMath framework.  It starts with a mathematical dataset containing questions and answers.  Then, Monte Carlo Tree Search (MCTS) is used with a policy model (LLM) and a value model to generate solution paths, including both correct and incorrect ones, along with their associated state values. Finally, these generated data are used to optimize both the policy and value models. This iterative process refines the model's ability to generate high-quality mathematical reasoning solutions.", "section": "3 AlphaMath"}, {"figure_path": "VaXnxQ3UKo/figures/figures_24_1.jpg", "caption": "Figure 1: Our approach involves iterating through three distinct stages. (1) Collect a mathematical dataset that comprises questions and their corresponding final answers. (2) Employ MCTS on the policy and the value model to generate both correct and incorrect solution paths along with state values. (3) Optimize the policy and the value model with generated data from MCTS.", "description": "This figure illustrates the iterative training process of AlphaMath. The process begins by collecting a mathematical dataset (questions and answers).  Then, Monte Carlo Tree Search (MCTS) is used with a policy model (LLM) and a value model to generate correct and incorrect solution paths, along with associated state values. Finally, the policy and value models are optimized using the generated data from MCTS. This iterative process allows AlphaMath to generate high-quality process-supervised solutions without human intervention.", "section": "3 AlphaMath"}]