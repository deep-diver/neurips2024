{"references": [{"fullname_first_author": "Edward J Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2021-06-09", "reason": "This paper introduces the LoRA method, a highly influential low-rank adaptation technique for LLMs, which is directly related to and compared against in the target paper."}, {"fullname_first_author": "Noam Shazeer", "paper_title": "Adafactor: Adaptive learning rates with sublinear memory cost", "publication_date": "2018-00-00", "reason": "This paper presents Adafactor, an adaptive optimizer with low memory cost, which addresses memory efficiency concerns central to the target paper's focus on memory-efficient LLM training."}, {"fullname_first_author": "Guy Gur-Ari", "paper_title": "Gradient descent happens in a tiny subspace", "publication_date": "2018-12-04", "reason": "This paper provides theoretical insights into the behavior of gradient descent, which is highly relevant to the target paper's theoretical analysis of subspace descent methods."}, {"fullname_first_author": "Jiawei Zhao", "paper_title": "Galore: Memory-efficient LLM training by gradient low-rank projection", "publication_date": "2024-03-03", "reason": "This paper is closely related to the target paper, as both focus on memory-efficient LLM training using low-rank gradient projection; the target paper builds upon and improves this work."}, {"fullname_first_author": "Chris J Maddison", "paper_title": "Hamiltonian descent methods", "publication_date": "2018-09-05", "reason": "This paper introduces the Hamiltonian Descent framework, providing a theoretical foundation for analyzing the dynamics of various optimizers, which is crucial to the target paper's theoretical contributions."}]}