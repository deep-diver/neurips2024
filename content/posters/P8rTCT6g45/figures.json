[{"figure_path": "P8rTCT6g45/figures/figures_7_1.jpg", "caption": "Figure 1: Validation perplexity of LLaMA 1B with sequence length 256, rank 512 for 10K steps.", "description": "This figure displays the validation perplexity achieved by three different methods (AdamW (8bit), Ours (AdamW 8bit), and GaLore (AdamW 8bit)) during the training of a 1B parameter LLaMA model with a sequence length of 256 and a rank of 512. The x-axis represents the training steps (up to 10K steps), and the y-axis represents the validation perplexity.  The figure demonstrates how the perplexity changes over the training process for each method. This allows for a comparison of the different approaches' effectiveness in reducing perplexity during model training.", "section": "5.1 Why do we Need Online Subspace Descent?"}, {"figure_path": "P8rTCT6g45/figures/figures_7_2.jpg", "caption": "Figure 2: The execution time of torch.svd and that of a single-step backward() call for online PCA in PyTorch, on matrices of typical shapes in linear layers in the LLAMA 60M to 7B. Thanks to the high speed of single-step online PCA, Pt updates can be executed in parallel with weight updates, adding no overhead to the training process. In contrast, SVD incurs significant overhead as the model and weight tensor sizes increase.", "description": "This figure compares the execution time of singular value decomposition (SVD) and online principal component analysis (PCA) for updating the projection matrix in the Online Subspace Descent algorithm.  It shows that online PCA is significantly faster than SVD, especially for larger matrices, which is crucial for efficient training of large language models.  The speed advantage of online PCA allows for parallel updates, minimizing training overhead.", "section": "5.1 Why do we Need Online Subspace Descent?"}, {"figure_path": "P8rTCT6g45/figures/figures_8_1.jpg", "caption": "Figure 3: From left to right are loss curves of 10K steps on LLaMA 60M: leftmost is the sweep of rank, middle is the sweep of \u03b1 and rightmost is the sweep of \u03bb.", "description": "This figure shows the loss curves for three hyperparameter sweeps during the training of a 60M parameter LLaMA model.  The leftmost plot shows how loss changes with different ranks of the projection matrix. The middle plot demonstrates the impact of the \u03b1 parameter (which controls the update speed of the projection matrix) on the loss. Finally, the rightmost plot illustrates how loss varies based on the \u03bb parameter, which handles regularization in the PCA update of the projection matrix.", "section": "5.3 What are the Best Hyperparameters?"}]