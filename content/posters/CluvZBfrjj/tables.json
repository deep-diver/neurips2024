[{"figure_path": "CluvZBfrjj/tables/tables_5_1.jpg", "caption": "Table 1: Compare the characteristics of all comparison methods and the proposed TAGI. More comparisons can be seen in C.1.", "description": "This table compares the proposed TAGI method with eight other baseline methods across several key characteristics. These characteristics include whether the method involves pre-training, instruction fusion, low inference cost, instruction learning, and the ability to handle unseen tasks.  The table helps to highlight the advantages of TAGI over existing methods in terms of efficiency and generalization capabilities.", "section": "4.2 Baselines"}, {"figure_path": "CluvZBfrjj/tables/tables_6_1.jpg", "caption": "Table 2: RougeL results on Super-Natural Instructions. The best results are in bold, while the second-best are underlined. *, \u2020 means that those results are from HINT [13] and [27] respectively, \"-\" means not reported. \u2021 indicates that there is no parameter alignment loss in the hypernetwork finetuning because the model is too large, leading to a significant amount of time required for LORA tuning for each task. The Average Relative FLOPs cost is calculated relative to Tk-Instruct. We use the number of FLOPs required by each model to process one task (containing 100 examples).", "description": "This table presents the performance of various models on the Super-Natural Instructions dataset, broken down by model size and whether zero-shot or few-shot learning was used.  It compares the performance of TAGI to several baselines, including models without finetuning, hypernetwork-based models, and strong fully finetuned models.  Key metrics include Rouge-L scores and relative FLOPs (floating point operations) cost.", "section": "4.4 Main Results"}, {"figure_path": "CluvZBfrjj/tables/tables_7_1.jpg", "caption": "Table 3: Average accuracy results over TO evaluation tasks after training on the TO P3 train set. a means results are from [41]. trained by us followed the Tk-Instruct (meta-training) [37]. Our method uses only template inputs without demonstrations yet achieves competitive performance with ICL-based methods using 16 shots, with much-reduced inference overhead. The Average Relative Inference Time is calculated relative to the Metatrain. We use the inference time required by each model to process all 11 test tasks with batch_size of 1.", "description": "This table presents the average accuracy results on the TO evaluation tasks of the P3 dataset after training on the TO P3 train set.  It compares the performance of TAGI against several baselines, including zero-shot, full finetuning, meta-training, and other ICL-based and hypernetwork-based methods.  The results show the average accuracy across the 11 meta-test tasks, and the average relative inference time, which is calculated relative to the meta-training baseline.", "section": "4.4 Main Results"}, {"figure_path": "CluvZBfrjj/tables/tables_9_1.jpg", "caption": "Table 4: Ablation study of TAGI model. All models utilized are T5-LM-XL (3B) and training for 20,000 steps. The P3 dataset was selected by the HyperT5 evaluation.", "description": "This table presents the results of an ablation study on the TAGI model, showing the impact of removing various components on the model's performance.  The study used the T5-LM-XL (3B) model and trained for 20,000 steps. The evaluation was done on the P3 dataset as selected by the HyperT5 evaluation. The table shows the performance of the full TAGI model and also shows the effect of removing several key components: pretraining, instruction fusion, various loss functions, and the hypernetwork itself.", "section": "4.7 Ablation Study"}, {"figure_path": "CluvZBfrjj/tables/tables_13_1.jpg", "caption": "Table 5: Number of samples in given splits for each dataset.", "description": "This table presents the number of training and testing samples used for each dataset in the experiments.  It shows the number of examples per task and the total number of training and testing samples for Super-Natural Instructions, P3, and a sampled version of P3.", "section": "4.1 Datasets"}, {"figure_path": "CluvZBfrjj/tables/tables_14_1.jpg", "caption": "Table 6: Hyperparameters for Training TAGI Models and LoRA Tuning.", "description": "This table lists the hyperparameters used for training the TAGI models and tuning the LoRA modules.  It shows the settings for various parameters including maximum input and output lengths, optimizers, learning rates, precision, number of training and warmup steps, batch sizes, gradient accumulation steps, and LoRA rank. These settings are broken down separately for the LoRA tuning process, model pretraining, and finetuning on both the SNI and P3 datasets, with separate settings for different model sizes (Base, XL, XXL).  The table provides a detailed view of the hyperparameter choices made for each stage of the model training process and aids in understanding and replicating the experimental results. ", "section": "A.6 Hyperparameter"}, {"figure_path": "CluvZBfrjj/tables/tables_16_1.jpg", "caption": "Table 7: Performance variation due to different hyperparameters. The base model is T5-LM-Base, and all experiments follow the previous hyperparameter settings, changing only the target hyperparameter, where underlines indicate experimental defaults.", "description": "This table presents the ablation study on hyperparameters. It shows the performance variation of the T5-LM-Base model under different hyperparameter settings for the SNI and P3 datasets.  The table displays results for different learning rates, LoRA ranks, training steps, and warmup ratios, highlighting how these factors affect performance on both datasets. The results are useful in determining the optimal hyperparameters for the TAGI model. The underlined values indicate the default hyperparameters used in the experiments.", "section": "4.7 Ablation Study"}, {"figure_path": "CluvZBfrjj/tables/tables_17_1.jpg", "caption": "Table 8: Ablation study on \u03bb hyperparameters. The backbone model is T5-Base.", "description": "This table presents the results of an ablation study on the hyperparameters \u03bb1 and \u03bb2 used in the TAGI model.  Different values for these hyperparameters were tested, and the resulting RougeL scores are reported. The study aims to determine the optimal balance between the two loss functions (knowledge distillation and parameter alignment) for improved performance.", "section": "4.7 Ablation Study"}, {"figure_path": "CluvZBfrjj/tables/tables_17_2.jpg", "caption": "Table 1: Compare the characteristics of all comparison methods and the proposed TAGI. More comparisons can be seen in C.1.", "description": "This table compares the characteristics of different methods, including the proposed TAGI method, used in the paper for cross-task generalization. It provides a concise overview of each method's approach and performance, highlighting key features like whether they use pre-training, instruction fusion, instruction learning, and their ability to generalize to unseen tasks.", "section": "4.2 Baselines"}, {"figure_path": "CluvZBfrjj/tables/tables_18_1.jpg", "caption": "Table 10: Main Full P3 Results. \"-\" means not reported. \u2020 and \u2021 mean the results are from FiD-ICL [41] and Hypertuning [27] respectively. \u25c7 Computed as the average of R1/R2/R3 (except for HyperT5 rows where the numbers are quoted). More ICL-based results and details can be seen FiD-ICL [41].", "description": "This table presents a detailed comparison of the performance of various models on the P3 dataset.  It shows the average accuracy scores across different tasks, including ANLI (R1, R2, R3), Hswag, CB, COPA, RTE, WiC, WSC, WGD, SCloze, and the overall average across the 11 meta-test tasks (MTest11).  The models compared include different baselines such as random, full finetuning (Full FT), meta-training (Metatrain), FiD-ICL, and Hypernetwork-based methods. The table also shows the results for different sizes of the T5-LM and TO models and HyperT5 model variations with different LoRA parameters, enabling a comprehensive evaluation of model performance and efficiency.", "section": "4.4 Main Results"}, {"figure_path": "CluvZBfrjj/tables/tables_19_1.jpg", "caption": "Table 1: Compare the characteristics of all comparison methods and the proposed TAGI. More comparisons can be seen in C.1.", "description": "This table compares the proposed TAGI method with eight other baseline methods across several key characteristics. These characteristics include whether the method uses pretraining, instruction fusion, low-rank adaptation, low inference cost, instruction learning, and its ability to handle unseen tasks. The table helps to understand the advantages and disadvantages of TAGI compared to existing techniques.", "section": "4.2 Baselines"}]