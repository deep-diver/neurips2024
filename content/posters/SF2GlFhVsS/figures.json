[{"figure_path": "SF2GlFhVsS/figures/figures_1_1.jpg", "caption": "Figure 1: (Left) Challenges and Solutions of Contrast Decoding Strategy. Visual Contrastive Decoding, despite introducing perturbations to induce hallucinations, fails to effectively enlarge the logits gap between hallucinatory and targeted tokens, resulting in unsatisfactory outputs. On the contrary, our method addresses the issue by significantly amplifying the logits gap between hallucinatory and targeted tokens. (Right) The performance of various methods on CHAIR metrics. Our HIO generates descriptions with fewer hallucination tokens compared to other visual contrastive decoding methods, achieving lower scores on the CHAIRs and CHAIRI metrics.", "description": "This figure presents a comparison of different decoding strategies for large vision-language models (LVLMs).  The left panel illustrates the challenges of existing contrast decoding methods, which struggle to effectively widen the logits gap between hallucinated and correct tokens, leading to inaccurate outputs. In contrast, the proposed Hallucination-Induced Optimization (HIO) method significantly amplifies this gap, resulting in improved accuracy. The right panel shows a bar chart comparing the performance of various methods, including HIO, on the CHAIR metric, a benchmark for evaluating hallucinations in image captioning.  HIO demonstrates superior performance by generating descriptions with fewer hallucination tokens.", "section": "1 Introduction"}, {"figure_path": "SF2GlFhVsS/figures/figures_4_1.jpg", "caption": "Figure 2: An overview of Hallucination-Induced Optimization (HIO). Our approach comprises two phases: the training stage and inference decoding. During the training stage, given an input image, a query, and a manually annotated correction, the Large Visual Language Model (LVLM) produces multiple instances of hallucinated content. We then apply our Hallucination-Induced Optimization (HIO) method to train an 'Evil' LVLM by inducing hallucinations from the original LVLM. In the inference phase, the logits from the trained 'Evil' LVLM are used to contrast with those generated by the original LVLM, effectively reducing the presence of hallucinations.", "description": "This figure illustrates the two-stage process of the proposed Hallucination-Induced Optimization (HIO) method.  In the training stage, an original LVLM generates outputs with hallucinations, which are then used to train an 'Evil' LVLM that amplifies these hallucinations. During inference, logits from both the original and 'Evil' LVLMs are combined to reduce hallucinations in the final output.", "section": "4 Method"}, {"figure_path": "SF2GlFhVsS/figures/figures_14_1.jpg", "caption": "Figure 1: (Left) Challenges and Solutions of Contrast Decoding Strategy. Visual Contrastive Decoding, despite introducing perturbations to induce hallucinations, fails to effectively enlarge the logits gap between hallucinatory and targeted tokens, resulting in unsatisfactory outputs. On the contrary, our method addresses the issue by significantly amplifying the logits gap between hallucinatory and targeted tokens. (Right) The performance of the state-of-the-art methods compared with Ours.", "description": "The figure's left panel illustrates the challenges of existing contrast decoding strategies, comparing greedy decoding, visual contrastive decoding, and the proposed Hallucination-Induced Optimization (HIO).  It highlights how HIO effectively widens the logit gap between hallucinated and correct tokens, leading to improved accuracy. The right panel presents a bar chart comparing the performance of various methods, including the proposed HIO, on CHAIR metrics. HIO achieves the lowest scores, showing its superior performance in reducing hallucinations.", "section": "1 Introduction"}]