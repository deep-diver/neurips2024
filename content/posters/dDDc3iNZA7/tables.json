[{"figure_path": "dDDc3iNZA7/tables/tables_6_1.jpg", "caption": "Table 1: Performance comparison of multi-modal domain adaptive and domain generalized 3D semantic segmentation methods in four typical scenarios. We report the mIoU results (with best and 2nd best) on the target testing set for each network as well as the ensemble result (i.e., xM) by averaging the predicted probabilities from the 2D and 3D networks.", "description": "This table compares the performance of various multi-modal domain adaptive and generalized 3D semantic segmentation methods across four different scenarios.  It shows the mean Intersection over Union (mIoU) scores for 2D and 3D networks, as well as an ensemble result combining both. The scenarios represent different challenges in adapting and generalizing 3D semantic segmentation models, such as changes in lighting, scene layout, and sensor setup.", "section": "4.3 Quantitative and Qualitative Comparison"}, {"figure_path": "dDDc3iNZA7/tables/tables_8_1.jpg", "caption": "Table 1: Performance comparison of multi-modal domain adaptive and domain generalized 3D semantic segmentation methods in four typical scenarios. We report the mIoU results (with best and 2nd best) on the target testing set for each network as well as the ensemble result (i.e., xM) by averaging the predicted probabilities from the 2D and 3D networks.", "description": "This table compares the performance of various multi-modal domain adaptive and domain generalized 3D semantic segmentation methods across four scenarios.  It shows mean Intersection over Union (mIoU) results for 2D and 3D networks separately, and also an ensemble result ('xM') that combines their predictions.  The scenarios represent different domain adaptation and generalization challenges involving changes in lighting, scene layout, and sensor configurations.", "section": "4.3 Quantitative and Qualitative Comparison"}, {"figure_path": "dDDc3iNZA7/tables/tables_8_2.jpg", "caption": "Table 3: Ablation study on the effectiveness of significant components in UniDSeg with the ViT-B backbone for DG3SS task.", "description": "This ablation study analyzes the impact of Modal Transitional Prompting (MTP) and Learnable Spatial Tunability (LST) on the performance of UniDSeg using the ViT-B backbone for domain generalized 3D semantic segmentation (DG3SS). It shows the 2D, 3D, and ensemble (xM) mIoU scores for two scenarios: nuScenes:Sing./USA and A2D2/sKITTI. The results demonstrate that both MTP and LST contribute significantly to the overall performance, with the combination of both yielding the best results.", "section": "4 Experiments"}, {"figure_path": "dDDc3iNZA7/tables/tables_9_1.jpg", "caption": "Table 1: Performance comparison of multi-modal domain adaptive and domain generalized 3D semantic segmentation methods in four typical scenarios. We report the mIoU results (with best and 2nd best) on the target testing set for each network as well as the ensemble result (i.e., xM) by averaging the predicted probabilities from the 2D and 3D networks.", "description": "This table compares the performance of various multi-modal domain adaptive and generalized 3D semantic segmentation methods across four different scenarios (nuScenes: USA/Sing., nuScenes: Day/Night, VKITTI/SKITTI, A2D2/SKITTI).  It shows the mean Intersection over Union (mIoU) for 2D and 3D networks individually, as well as a combined (ensemble) result (xM). The scenarios represent different challenges in domain adaptation and generalization, allowing for a comprehensive evaluation of the methods' capabilities.", "section": "4 Experiments"}, {"figure_path": "dDDc3iNZA7/tables/tables_9_2.jpg", "caption": "Table 7: The parameters and computational costs of CLIP-based and SAM-based 2D backbones. \"Cost\" means the percentage of trainable parameters in MTP and LST compared to fine-tuning the whole encoder consumed.", "description": "This table compares the number of parameters in different 2D backbones (CLIP:ViT-B, CLIP:ViT-L, SAM:ViT-L) used in the UniDSeg model. It shows the total number of parameters, the number of trainable parameters (only in MTP and LST modules), and the cost (percentage of trainable parameters relative to fine-tuning the entire encoder). The table highlights the efficiency of the proposed method, which requires only a small fraction of the parameters to be trained compared to fine-tuning the whole encoder.", "section": "4.4 Ablation Study"}, {"figure_path": "dDDc3iNZA7/tables/tables_14_1.jpg", "caption": "Table 1: Performance comparison of multi-modal domain adaptive and domain generalized 3D semantic segmentation methods in four typical scenarios. We report the mIoU results (with best and 2nd best) on the target testing set for each network as well as the ensemble result (i.e., xM) by averaging the predicted probabilities from the 2D and 3D networks.", "description": "This table presents a comparison of different methods for 3D semantic segmentation, categorized into domain adaptive and domain generalized tasks.  It shows the mean Intersection over Union (mIoU) scores achieved by various methods on four different scenarios (nuScenes: USA/Sing, nuScenes: Day/Night, vKITTI/SKITTI, A2D2/SKITTI). The results are broken down by 2D and 3D networks, and an ensemble result (xM) is also provided, combining the predictions of both networks. This allows for a comprehensive comparison of different techniques across various datasets and scenarios.", "section": "4 Experiments"}, {"figure_path": "dDDc3iNZA7/tables/tables_16_1.jpg", "caption": "Table 9: Fully-supervised 3D semantic segmentation results on the SemanticKITTI validation set. We report per-class IoU. \u201c\u2020\u201d denotes the reproduced result referring to the official codebase. \u201cw/ TTA\u201d means using test-time augmentation in the inference stage.", "description": "This table presents a comparison of fully-supervised 3D semantic segmentation methods on the SemanticKITTI validation set.  It shows the Intersection over Union (IoU) for each class (car, bicycle, motorcycle, truck, bus, person, bicyclist, motorcyclist, road, parking, sidewalk, other-ground, building, fence, vegetation, trunk, terrain, pole, traffic-sign) for different methods.  The methods compared include MinkowskiNet, SPVCNN, Cylinder3D, and 2DPASS (with and without Test-Time Augmentation). The table also includes results for the proposed 'Ours' method, showing its performance with and without test-time augmentation.", "section": "4.3 Quantitative and Qualitative Comparison"}, {"figure_path": "dDDc3iNZA7/tables/tables_16_2.jpg", "caption": "Table 10: Performance comparison of SFDA3SS methods in three typical scenarios. \u201c\u2020\u201d denotes the reproduced result referring to the official codebase, as the different category splits applied in the same adaptation scenario.", "description": "This table compares the performance of different Source-Free Domain Adaptive 3D Semantic Segmentation (SFDA3SS) methods across three scenarios: nuScenes: USA/Sing, nuScenes: Day/Night, and A2D2/sKITTI.  The performance is measured using mean Intersection over Union (mIoU) for 2D, 3D, and a combined (xM) approach.  The table also includes a baseline, consistency, and pseudo-label methods for comparison, along with the results for the SUMMIT method from prior work.  The xM results combine the probabilities from both 2D and 3D networks. It highlights the performance of UniDSeg, both with and without access to source-free data.", "section": "4.3 Quantitative and Qualitative Comparison"}, {"figure_path": "dDDc3iNZA7/tables/tables_17_1.jpg", "caption": "Table 11: Performance comparison of multi-modal domain adaptive 3D semantic segmentation methods with pseudo-label (\"PL\") re-training on four typical scenarios.", "description": "This table compares the performance of several multi-modal domain adaptive 3D semantic segmentation methods when re-trained with pseudo-labels.  It shows the mean Intersection over Union (mIoU) results for different methods across four scenarios: nuScenes:USA/Sing, nuScenes:Day/Night, vKITTI/SKITTI, and A2D2/SKITTI.  The scenarios represent different domain adaptation challenges.  The results are presented separately for 2D and 3D segmentation, as well as a combined (XM) result.", "section": "4.3 Quantitative and Qualitative Comparison"}, {"figure_path": "dDDc3iNZA7/tables/tables_17_2.jpg", "caption": "Table 1: Performance comparison of multi-modal domain adaptive and domain generalized 3D semantic segmentation methods in four typical scenarios. We report the mIoU results (with best and 2nd best) on the target testing set for each network as well as the ensemble result (i.e., xM) by averaging the predicted probabilities from the 2D and 3D networks.", "description": "This table compares the performance of various multi-modal domain adaptive and domain generalized 3D semantic segmentation methods across four different scenarios.  The mIoU (mean Intersection over Union) is reported for 2D and 3D networks separately, as well as a combined (ensemble) result.  The scenarios represent different domain adaptation and generalization challenges.", "section": "4.3 Quantitative and Qualitative Comparison"}]