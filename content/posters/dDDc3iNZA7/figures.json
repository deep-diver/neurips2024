[{"figure_path": "dDDc3iNZA7/figures/figures_3_1.jpg", "caption": "Figure 1: Overall framework of UniDSeg for DG3SS and DA3SS. The backbone of the VFM-based Encoder is frozen and only trains several learnable modules. \u201cSamp.\u201d means sampling of 2D features. Only the points falling into the intersected field of view are geometrically associated with multi-modal data.", "description": "The figure shows the overall framework of the UniDSeg model, which is designed for both domain generalized (DG3SS) and domain adaptive (DA3SS) 3D semantic segmentation tasks.  It consists of two main branches: a 2D network and a 3D network. The 2D network processes images and sparse depth maps from a camera and LiDAR, respectively, using a VFM-based encoder with several trainable modules. The 3D network processes LiDAR point clouds directly. Both branches feed into a cross-modal learning module that combines their predictions to generate the final 3D semantic segmentation results. The VFM-based encoder utilizes pre-trained Visual Foundation Models (VFMs) as the backbone to leverage their knowledge, while the trainable modules adapt to different domains.  Point sampling is employed to ensure that only the visible points are processed in the cross-modal learning step.", "section": "3.2 Learnable-parameter-inspired Mechanism"}, {"figure_path": "dDDc3iNZA7/figures/figures_4_1.jpg", "caption": "Figure 2: The architecture of VFM-based Encoder. We explore two layer-wise learnable blocks: (a) Modal Transitional Prompting and (b) Learnable Spatial Tunability. During training, only the parameters of two modules are updated while the whole ViT encoder layer is frozen.", "description": "This figure shows the architecture of the VFM-based encoder used in UniDSeg.  It highlights two key learnable modules: Modal Transitional Prompting (MTP) and Learnable Spatial Tunability (LST). MTP utilizes sparse depth information and a low-frequency filtered image to generate 3D-to-2D transitional prompts. LST introduces learnable tokens to capture spatial relationships between instances.  The VFM backbone's parameters remain frozen during training, with only the parameters within these two modules being updated, making it a parameter-efficient approach.", "section": "3.2 Learnable-parameter-inspired Mechanism"}, {"figure_path": "dDDc3iNZA7/figures/figures_7_1.jpg", "caption": "Figure 3: Qualitative results of DG3SS. We showcase the ensembling result of four scenarios.", "description": "This figure shows a qualitative comparison of the results obtained by the UniDSeg model on four different scenarios for Domain Generalized 3D Semantic Segmentation (DG3SS). Each scenario presents a different combination of source and target datasets.  For each scenario, the image, the 2D and 3D segmentation results of the UniDSeg method are displayed side-by-side, along with the ground truth. The visual comparison highlights the model's ability to generalize across diverse and unseen scenarios and achieve good segmentation performance in various scenes.", "section": "4.3 Quantitative and Qualitative Comparison"}, {"figure_path": "dDDc3iNZA7/figures/figures_8_1.jpg", "caption": "Figure 4: Effect of the learnable token length.", "description": "This figure shows the effect of the learnable token length on the 2D mIoU for two different scenarios: nuScenes: USA/Sing. and A2D2/sKITTI.  The x-axis represents the token length, and the y-axis represents the 2D mIoU.  The results show that increasing the token length leads to an increase in 2D mIoU, but beyond a certain point, increasing the token length does not result in further improvements. This suggests an optimal token length exists for maximizing performance.", "section": "4 Experiments"}, {"figure_path": "dDDc3iNZA7/figures/figures_15_1.jpg", "caption": "Figure 5: Qualitative results of our method on the validation set of SemanticKITTI. The misclassification points are signed in red.", "description": "This figure shows a qualitative comparison of the proposed UniDSeg method against the 2DPASS method on the SemanticKITTI validation dataset.  Three sample scenes are presented, each displaying the ground truth segmentation, the segmentation results from the 2DPASS method highlighting errors in red, and the segmentation results from the UniDSeg method, again showing errors in red. The comparison focuses on the accuracy of segmenting trees, bicyclists, and trucks, demonstrating that UniDSeg achieves better results.", "section": "A.2 Fully-supervised 3D Semantic Segmentation"}, {"figure_path": "dDDc3iNZA7/figures/figures_17_1.jpg", "caption": "Figure 2: The architecture of VFM-based Encoder. We explore two layer-wise learnable blocks: (a) Modal Transitional Prompting and (b) Learnable Spatial Tunability. During training, only the parameters of two modules are updated while the whole ViT encoder layer is frozen.", "description": "This figure illustrates the architecture of the VFM-based encoder used in UniDSeg.  It shows two learnable blocks added to a frozen Vision Transformer (ViT) backbone. The first block, Modal Transitional Prompting (MTP), leverages 3D-to-2D transitional prior information from sparse depth and image inputs to create visual prompts. The second block, Learnable Spatial Tunability (LST), learns deep queries that interact with the prompts to improve feature generalization. Only the parameters of these two modules are updated during training; the ViT backbone remains frozen to preserve pre-trained knowledge.", "section": "3.2 Learnable-parameter-inspired Mechanism"}, {"figure_path": "dDDc3iNZA7/figures/figures_18_1.jpg", "caption": "Figure 7: Additional qualitative results of nuScenes:Day/Night scenario for DA3SS.", "description": "This figure shows additional qualitative results for the nuScenes:Day/Night scenario in the domain adaptive 3D semantic segmentation task (DA3SS). It presents comparisons between the image, 2D prediction, 3D prediction, the average of 2D and 3D predictions, and the ground truth. Red rectangles highlight specific regions of interest where the predictions are compared to the ground truth, providing a visual assessment of the model's performance in various lighting conditions.", "section": "A.6 Additional Visualization Results"}, {"figure_path": "dDDc3iNZA7/figures/figures_19_1.jpg", "caption": "Figure 7: Additional qualitative results of nuScenes:Day/Night scenario for DA3SS.", "description": "This figure shows additional qualitative results for the Day/Night scenario of the nuScenes dataset, focusing on domain adaptive 3D semantic segmentation.  It presents comparisons between the input image, 2D and 3D prediction outputs from the UniDSeg model, and the ground truth. Red boxes highlight areas where differences are notable, illustrating the model's performance in handling variations in lighting conditions. The legend provides a color-coded key to the different semantic classes.", "section": "A.6 Additional Visualization Results"}, {"figure_path": "dDDc3iNZA7/figures/figures_20_1.jpg", "caption": "Figure 7: Additional qualitative results of nuScenes:Day/Night scenario for DA3SS.", "description": "This figure shows additional qualitative results for the Day/Night scenario of the nuScenes dataset in the Domain Adaptive 3D Semantic Segmentation task. It presents a comparison between the image, 2D prediction, 3D prediction, the average of 2D and 3D predictions, and the ground truth.  Red boxes highlight areas of misclassification, enabling a visual assessment of the model's performance in different conditions.", "section": "4.3 Quantitative and Qualitative Comparison"}, {"figure_path": "dDDc3iNZA7/figures/figures_20_2.jpg", "caption": "Figure 7: Additional qualitative results of nuScenes:Day/Night scenario for DA3SS.", "description": "This figure shows additional qualitative results of the nuScenes:Day/Night scenario for Domain Adaptive 3D Semantic Segmentation (DA3SS).  It presents comparisons between the 2D and 3D predictions of the UniDSeg model against ground truth.  The images highlight areas where the model's predictions differ from the ground truth, indicating the model's strengths and weaknesses in various aspects of semantic segmentation.", "section": "4.3 Quantitative and Qualitative Comparison"}]