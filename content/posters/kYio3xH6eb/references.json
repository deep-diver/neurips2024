{"references": [{"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-31", "reason": "This paper is foundational to the RLHF method, which is the central focus of the current paper."}, {"fullname_first_author": "Yuntao Bai", "paper_title": "Training a helpful and harmless assistant with reinforcement learning from human feedback", "publication_date": "2022-12-31", "reason": "This paper introduces the Anthropic HH dataset, used in the current paper's experiments, and is highly relevant to the topic of RLHF."}, {"fullname_first_author": "Leo Gao", "paper_title": "Scaling laws for reward model overoptimization", "publication_date": "2023-07-01", "reason": "This paper directly addresses the problem of reward overoptimization, a key challenge that the current paper aims to mitigate."}, {"fullname_first_author": "Thomas Coste", "paper_title": "Reward model ensembles help mitigate overoptimization", "publication_date": "2023-10-26", "reason": "This paper proposes a mitigation strategy for reward overoptimization using reward model ensembles, providing a comparison point for the current paper's approach."}, {"fullname_first_author": "Jacob Eisenstein", "paper_title": "Helping or herding? Reward model ensembles mitigate but do not eliminate reward hacking", "publication_date": "2023-12-15", "reason": "This paper investigates reward model ensembles, which is a related approach to the one explored in the current paper, providing further context and comparison."}]}