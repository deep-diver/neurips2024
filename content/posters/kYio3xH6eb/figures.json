[{"figure_path": "kYio3xH6eb/figures/figures_6_1.jpg", "caption": "Figure 1: Comparison among lightweight uncertainty estimations. In Figure 1a and 1c, the blue lines with shaded areas depict the reward dynamics concerning optimization steps in PPO, where the solid and dashed lines represent gold and proxy rewards, respectively. The lines with dots denote the results from different uncertainty estimation methods. The reward values are indexed on the left y-axis, while the uncertainty is indexed on the right y-axis. In Figure 1b and 1d, we plot the correlation between uncertainty and the difference between gold and proxy rewards.", "description": "This figure compares different methods for estimating reward uncertainty during reinforcement learning from human feedback (RLHF).  The left-hand side plots (a and c) show the reward dynamics over training steps for the Anthropic HH and TL;DR datasets, comparing the gold-standard reward (human judgments) with proxy reward estimates from different methods (including the proposed lightweight method 'CI'). The right-hand side plots (b and d) show the correlation between the estimated uncertainty and the difference between the gold-standard and proxy rewards for each method.", "section": "5.1 Empirical effectiveness of lightweight uncertainty estimation"}, {"figure_path": "kYio3xH6eb/figures/figures_8_1.jpg", "caption": "Figure 2: Experimental results demonstrating the mitigation of overoptimization in RLHF with ADVPO. The gold reward is represented by the solid line, while the dashed line corresponds to the proxy reward. The x-axis of Figure 2b and Figure 2d have a square-root scale.", "description": "This figure shows the results of experiments comparing the performance of standard Proximal Policy Optimization (PPO) and the proposed Adversarial Policy Optimization (ADVPO) method on two datasets: Anthropic HH and TL;DR.  The graphs illustrate the dynamics of gold rewards (true human preference) and proxy rewards (estimated reward from the model) over training steps (left panels) and KL divergence (right panels). ADVPO demonstrates mitigation of the reward overoptimization problem by more closely tracking the gold reward and showing smaller KL divergence.", "section": "5 Experiments"}, {"figure_path": "kYio3xH6eb/figures/figures_17_1.jpg", "caption": "Figure 3: Comparison among lightweight uncertainty estimations. (left) The blue lines with shaded areas depict the reward dynamics concerning optimization steps in PPO, where the solid and dashed lines represent gold and proxy rewards, respectively. The lines with dots denote the results from different uncertainty estimation methods. The reward values are indexed on the left y-axis, while the uncertainty is indexed on the right y-axis. (Right) We plot the correlation between uncertainty and the difference between gold rewards and proxy rewards.", "description": "This figure compares different methods for estimating reward uncertainty during policy optimization.  The left side shows reward dynamics over optimization steps, comparing gold-standard rewards, proxy rewards, and results from various uncertainty estimation methods.  The right side plots the correlation between uncertainty estimates and the differences between gold-standard and proxy rewards. The results illustrate the effectiveness of the lightweight uncertainty estimation method in capturing discrepancies between gold and proxy rewards.", "section": "5.1 Empirical effectiveness of lightweight uncertainty estimation"}, {"figure_path": "kYio3xH6eb/figures/figures_18_1.jpg", "caption": "Figure 2: Experimental results demonstrating the mitigation of overoptimization in RLHF with ADVPO. The gold reward is represented by the solid line, while the dashed line corresponds to the proxy reward. The x-axis of Figure 2b and Figure 2d have a square-root scale.", "description": "This figure shows the mitigation of reward overoptimization by ADVPO in comparison to PPO.  The plots show the gold reward (solid line), proxy reward (dashed line), and KL divergence over optimization steps on two datasets (Anthropic HH and TL;DR).  ADVPO effectively prevents the proxy reward from diverging significantly from the gold reward and keeps the KL divergence lower than PPO, indicating better policy optimization and mitigation of reward hacking.", "section": "5 Experiments"}, {"figure_path": "kYio3xH6eb/figures/figures_18_2.jpg", "caption": "Figure 1: Comparison among lightweight uncertainty estimations. In Figure 1a and 1c, the blue lines with shaded areas depict the reward dynamics concerning optimization steps in PPO, where the solid and dashed lines represent gold and proxy rewards, respectively. The lines with dots denote the results from different uncertainty estimation methods. The reward values are indexed on the left y-axis, while the uncertainty is indexed on the right y-axis. In Figure 1b and 1d, we plot the correlation between uncertainty and the difference between gold and proxy rewards.", "description": "This figure compares different methods for estimating reward uncertainty during reinforcement learning from human feedback (RLHF).  Subfigures (a) and (c) show the reward dynamics (gold standard vs. proxy) over training steps for two datasets (Anthropic HH and TL;DR).  Subfigures (b) and (d) show the correlation between the estimated uncertainty and the difference between gold and proxy rewards.  The goal is to evaluate how well each uncertainty estimation method captures discrepancies between the estimated and true rewards, indicating potential overoptimization.", "section": "5.1 Empirical effectiveness of lightweight uncertainty estimation"}, {"figure_path": "kYio3xH6eb/figures/figures_23_1.jpg", "caption": "Figure 1: Comparison among lightweight uncertainty estimations. In Figure 1a and 1c, the blue lines with shaded areas depict the reward dynamics concerning optimization steps in PPO, where the solid and dashed lines represent gold and proxy rewards, respectively. The lines with dots denote the results from different uncertainty estimation methods. The reward values are indexed on the left y-axis, while the uncertainty is indexed on the right y-axis. In Figure 1b and 1d, we plot the correlation between uncertainty and the difference between gold and proxy rewards.", "description": "This figure compares four different methods for estimating reward uncertainty during reinforcement learning from human feedback (RLHF).  The top row shows the reward dynamics (gold vs proxy) over training steps on two datasets, Anthropic HH (left) and TL;DR (right), along with the uncertainty estimates from each method. The bottom row shows the correlation between the estimated uncertainty and the difference between gold and proxy rewards.  It demonstrates that a lightweight uncertainty estimation method (CI) is effective at identifying when proxy rewards diverge from ground truth, indicating over-optimization. The comparison with ensemble-based methods shows that CI achieves a comparable performance with significantly fewer computational requirements.", "section": "5.1 Empirical effectiveness of lightweight uncertainty estimation"}, {"figure_path": "kYio3xH6eb/figures/figures_23_2.jpg", "caption": "Figure 1: Comparison among lightweight uncertainty estimations. In Figure 1a and 1c, the blue lines with shaded areas depict the reward dynamics concerning optimization steps in PPO, where the solid and dashed lines represent gold and proxy rewards, respectively. The lines with dots denote the results from different uncertainty estimation methods. The reward values are indexed on the left y-axis, while the uncertainty is indexed on the right y-axis. In Figure 1b and 1d, we plot the correlation between uncertainty and the difference between gold and proxy rewards.", "description": "This figure compares four methods for estimating reward uncertainty during reinforcement learning from human feedback (RLHF): a lightweight method (CI), and three ensemble methods (ENS-3B, ENS-7B).  The top row shows the reward dynamics over training steps (left) and the correlation between uncertainty and the difference between gold (human) and proxy rewards (right) for the Anthropic HH dataset.  The bottom row shows the same for the TL;DR dataset.  The lightweight method (CI) demonstrates a strong correlation between increasing uncertainty and reward over-optimization, performing comparably to the larger ensemble methods.", "section": "5.1 Empirical effectiveness of lightweight uncertainty estimation"}]