[{"type": "text", "text": "Score-based generative models are provably robust: an uncertainty quantification perspective ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Nikiforos Mimikos-Stamatopoulos ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Department of Mathematics Universit\u00e9 C\u00f4te d\u2019Azur nmimikos@unice.fr ", "page_idx": 0}, {"type": "text", "text": "Benjamin J. Zhang   \nDivision of Applied Mathematics Brown University   \nbenjamin_zhang@brown.edu ", "page_idx": 0}, {"type": "text", "text": "Markos A. Katsoulakis Department of Mathematics and Statistics University of Massachusetts Amherst markos@umass.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Through an uncertainty quantification (UQ) perspective, we show that score-based generative models (SGMs) are provably robust to the multiple sources of error in practical implementation. Our primary tool is the Wasserstein uncertainty propagation (WUP) theorem, a model-form $U\\mathcal{Q}$ bound that describes how the $L^{2}$ error from learning the score function propagates to a Wasserstein-1 $(\\mathbf{d}_{1})$ ball around the true data distribution under the evolution of the Fokker-Planck equation. We show how errors due to (a) finite sample approximation, (b) early stopping, (c) score-matching objective choice, (d) score function parametrization expressiveness, and (e) reference distribution choice, impact the quality of the generative model in terms of a ${\\bf d}_{1}$ bound of computable quantities. The WUP theorem relies on Bernstein estimates for Hamilton-Jacobi-Bellman partial differential equations (PDE) and the regularizing properties of diffusion processes. Specifically, $P D E$ regularity theory shows that stochasticity is the key mechanism ensuring SGM algorithms are provably robust. The WUP theorem applies to integral probability metrics beyond ${\\bf d}_{1}$ , such as the total variation distance and the maximum mean discrepancy. Sample complexity and generalization bounds in ${\\bf d}_{1}$ follow directly from the WUP theorem. Our approach requires minimal assumptions, is agnostic to the manifold hypothesis and avoids absolute continuity assumptions for the target distribution. Additionally, our results clarify the trade-offs among multiple error sources in SGMs. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Score-based generative models (SGMs) [1, 2] are highly effective [3], producing high quality samples, with more stable and less computationally intensive training methods than generative adversarial nets and normalizing flows [4]. The models are empirically robust to approximations and errors in learning the score function. While SGM generalization properties have been studied for idealized conditions [5, 6, 7, 8], analyses of their robustness in practical settings remains underexplored. This paper analyzes SGMs through the regularity theory of nonlinear partial differential equations (PDEs) [9], specifically Hamilton-Jacobi-Bellman (HJB) equations [10]. Our main result is the Wasserstein uncertainty propagation (WUP) theorem (Theorem 3.1), a versatile model-form uncertainty quantification (UQ) bound which we use to theoretically explain the robustness of SGMs to approximation errors in practical implementation. Generalization bounds for integral probability metrics (IPMs), such as the Wasserstein-1 $(\\mathbf{d}_{1})$ and the total variation (TV) distance follow directly from the WUP theorem. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In the context of SGMs, the WUP theorem shows how an $L^{2}$ neighborhood around the true score function propagates to an IPM neighborhood around the true data distribution. By relating how various approximations in SGMs contributes to $L^{2}$ error with respect to the true score function, we establish how well the resulting SGM generalizes. Theorem 3.2 shows how the error in the learned score-function with respect to the explicit score-matching objective, or uniform-in-time $L^{2}$ error, and the choice of initial condition define the radii of ${\\bf d}_{1}$ and TV neighborhoods, respectively. Additionally, Theorem 3.3 addresses the case where the score function is learned from finite data using the denoising score-matching (DSM) objective and incorporates early stopping. ", "page_idx": 1}, {"type": "text", "text": "Our bounds capture trade-offs among the errors. The ability of SGMs to generalize depends on choices such as the early stopping time and how overtraining to the DSM objective and neglecting early stopping lead to SGMs that memorize and overfit to the data. Notably, our approach relies on minimal assumptions on the data distribution, being agnostic to the manifold hypothesis. This contrasts with existing convergence results which assume the hypothesis [8] or not [5, 7] a priori. Furthermore, unlike previous work, we obtain our generalization bounds with respect to the ${\\bf d}_{1}$ distance directly, without appealing to the Girsanov theorem, the Kullback-Leibler divergence, the $\\chi^{2}$ divergence, or Pinsker\u2019s inequality [5, 11, 7]. This suggests our bounds may be sharper than those which bound stronger norms and divergences. A notable feature of our DSM generalization bound is that the error bound is a computable function of the DSM objective, contrasting with prior results typically assume the learned score is close to the truth with respect to the ESM objective [5]. ", "page_idx": 1}, {"type": "text", "text": "The WUP theorem also enables robust uncertainty quantification (UQ) for score-based generative models, a rare capability in generative models. Robust UQ recognizes that learning complex models involves multiple sources of uncertainty due to modeling choices and imperfect data. The distributionally robust perspective [12] quantifies a uncertainty set based on divergences and probability metrics [13, 14, 15, 16] to measure the impact of model uncertainty around a baseline model. The drawback is that these sets are typically difficult to find computationally. The WUP theorem is an example of a robust UQ bound for IPMs that is computable. ", "page_idx": 1}, {"type": "text", "text": "1.1 Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "\u2022 We introduce the use of regularity theory of nonlinear PDEs for analyzing generative flows [9, 10]. Our key idea is using the Kolmogorov backward equation to study the evolution of IPMs under a generative flow, yielding generalization bounds. WUP is one particular application of this idea, and while we use it to study SGMs, this analysis is not limited to only SGMs. We show that the regularizing properties of the underlying Fokker-Planck equation imply SGM\u2019s robustness to errors with few assumptions on the data distribution. An intuitive explanation of our main technical contribution is provided in Section 4. ", "page_idx": 1}, {"type": "text", "text": "\u2022 The WUP theorem (Theorem 3.1), which we state and prove, describes how an $L^{2}$ ball centered around the true score function maps to a neighborhood of IPMs (such as ${\\bf d}_{1}$ , TV, and MMD [17]). WUP is a model-form uncertainty quantification bound, which maps uncertainties introduced by modeling choices when practically implementing SGM. The resulting generalization bounds WUP produces demonstrate that with properly chosen model parameters, SGM is robust to errors due to score-matching, finite sample approximation, early stopping time, and choice of the reference distribution. (Theorems 3.2, 3.3, and C.1). ", "page_idx": 1}, {"type": "text", "text": "\u2022 When applied to SGM, WUP produces generalization bounds under minimal hypotheses on the data distribution and score function, and explains the impact of implementation errors has on generalization. Our approach is agnostic to the manifold hypothesis, applying whether or not the data distribution has a density. Moreover, it is adaptable to additional assumptions about the target distribution to yield improved generalization bounds. ", "page_idx": 1}, {"type": "text", "text": "1.2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Convergence and generalization of SGMs have been well-studied. Many approaches [5, 7, 18] assume the target distribution is absolutely continuous with respect to a Gaussian, and obtain generalization bounds for TV, $\\chi^{2}$ , and ${\\bf d}_{1}$ by bounding the KL divergence, a stronger divergence, via the Girsanov theorem [11, 6, 18], Pinsker\u2019s inequality [5], functional inequalities [7], or other means [11, 19]. ", "page_idx": 1}, {"type": "text", "text": "While [20] shows that kernel estimators for the score function are minimax optimal distribution estimators, they make no connections to deep learning-based SGMs, and they assume the target distribution is sub-Gaussian and is in a Sobolev space. Meanwhile, [6] has comprehensive sample complexity results that considers neural net approximations in the finite sample regime. Their results, however, are specialized to distributions in a Besov space, and, similar to other work [5, 11, 7, 19], rely on bounding the KL divergence, which breakdown under the manifold hypothesis. ", "page_idx": 2}, {"type": "text", "text": "Our results are similar to [8] which derives ${\\bf d}_{1}$ bounds directly, proving SGM convergence under the manifold hypothesis. However, the analysis assumes a particular discretization of the SGM. In [21], an uncertainty propagation bound for the Wasserstein-2 distance is proven using a similar approach to this work. Their work, however, relies on strong, uncheckable assumptions on the score function and target measure, does not address the errors we study here, and is not extendable to IPMs. ", "page_idx": 2}, {"type": "text", "text": "2 Background and notation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let dimension $d\\in\\mathbb{N}$ , and terminal time $T>0$ . Denote $\\mathbb{T}^{d}\\subset\\mathbb{R}^{d}$ to be the unit torus and $R\\mathbb{T}^{d}\\subset\\mathbb{R}^{d}$ to be the torus of radius $R>0$ . Let ${\\mathcal{P}}(\\Omega)$ be the space of probability distributions on $\\Omega$ , where $\\Omega$ is $\\mathbb{R}^{d}$ or $R\\mathbb{T}^{d}$ . Let $\\pi\\in{\\mathcal{P}}(\\Omega)$ be the target data distribution, known only through a finite number of samples $\\{z_{j}\\}_{j=1}^{N}\\sim\\pi$ . Let $f:[0,T]\\times\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ be a vector field and $\\sigma:\\mathbb{R}\\rightarrow\\mathbb{R}$ be a positive function. Score-based generative modeling [1, 2] is based on considering two Stochastic Differential Equations (SDEs for short) whose flow maps are inverses of each other. Define $Y(s),X(t)$ to be the forward and reverse diffusion processes over time interval $s,t\\in[0,T]$ that evolve according to ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathrm{d}Y(s)=-f(T-s,Y(s))\\mathrm{d}s+\\sigma(T-s)\\mathrm{d}W(s),~~Y(0)\\sim\\pi\n$$", "text_format": "latex", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{d}X(t)=\\left[f(t,X(t))+\\sigma(t)^{2}\\nabla\\log\\eta^{\\pi}(T-t,X(t))\\right]\\mathrm{d}t+\\sigma(t)\\mathrm{d}W(t),\\ \\ X(0)\\sim m_{0},}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $Y(s)\\sim\\eta^{\\pi}(s,\\cdot)$ , and $W(t)$ is a standard Brownian motion in $\\mathbb{R}^{d}$ . Heres, $\\eta^{\\pi}(s,\\cdot)$ is the density of $Y(s)$ at time $s$ where the initial distribution was $\\pi$ . From [22], it is known that if $\\begin{array}{r}{{\\dot{m}}_{0}={\\eta}^{\\pi}(T,\\cdot)}\\end{array}$ , then $X(t)\\sim\\eta^{\\pi}(T-t,\\cdot)$ . SGMs generated new samples from $\\pi$ simulating trajectories of the reverse process (2) using an approximate score function $\\mathbf{s}_{\\theta}\\approx\\mathbf{s}=\\nabla\\log\\eta^{\\pi}$ . Typically $f$ and $\\sigma$ are chosen such that $\\eta^{\\pi}(T,\\cdot)$ is well-approximated by a normal distribution, which is then used as the initial distribution. ", "page_idx": 2}, {"type": "text", "text": "The score function is learned via samples $\\{z_{j}\\}_{j=1}^{N}$ from $\\pi$ by minimizing one of several scorematching objectives. Let $\\mathbf{s}_{\\theta}$ be some function where the parameters are learned via one of three score-matching objectives. For a path $\\rho:[0,T]\\to\\mathscr{P}(\\Omega)$ , we define the functionals ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{I}(\\rho,\\theta)=\\int_{0}^{T}\\int_{\\Omega}\\left|\\mathbf{s}_{\\theta}-\\nabla\\log\\rho\\right|^{2}\\mathrm{d}\\rho(s)\\mathrm{d}s\\ \\ \\mathcal{I}_{L}(\\rho,\\theta)=\\int_{0}^{T}\\int_{\\Omega}\\left(|\\mathbf{s}_{\\theta}|^{2}+2\\nabla\\cdot\\mathbf{s}_{\\theta}\\right)\\mathrm{d}\\rho(s)\\mathrm{d}s,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the subscript $L$ highlights the linear dependence of $\\mathcal{I}_{L}$ on the underlying measure $\\rho$ . The explicit score matching objective (ESM) is $J_{\\mathrm{ESM}}(\\eta^{\\pi},\\theta)=\\mathcal{I}(\\eta^{\\pi},\\theta)$ . As evaluation of the true score function $\\nabla\\log\\eta^{\\pi}$ is typically inaccessible, the implicit (ISM) $J_{\\mathrm{ISM}}(\\eta^{\\pi},\\theta)=\\mathcal{I}_{L}(\\eta^{\\pi},\\theta)$ [23] or the denoising (DSM) score-matching objectives [24, 1] are computed in practice ", "page_idx": 2}, {"type": "equation", "text": "$$\nJ_{\\mathrm{DSM}}(\\eta^{\\pi},\\theta)=\\int_{0}^{T}\\int_{\\Omega}\\int_{\\Omega}\\left|\\mathbf{s}_{\\theta}-\\nabla\\log\\eta^{x^{\\prime}}\\right|^{2}\\mathrm{d}\\eta^{x^{\\prime}}(s)\\mathrm{d}\\pi(x^{\\prime})\\mathrm{d}s.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Here $\\eta^{x^{\\prime}}(s)$ denotes the probability transition kernel from $x^{\\prime}$ to $x$ of (1) at time $s$ . The DSM objective is most frequently used in practice as it does not require computing derivatives of $\\mathbf{s}_{\\theta}$ . It does, however, require the evaluation of $\\bar{\\eta^{x^{\\prime}}}(s)$ in closed form, which is typically only accessible for linear SDEs. ", "page_idx": 2}, {"type": "text", "text": "Remark 2.1 (Choice of domain $\\Omega$ ). While our approach will also produce bounds when $\\Omega=\\mathbb{R}^{d}$ , we primarily focus on the torus $R\\mathbb{T}^{d}$ , which is equivalent to a bounded domain with periodic boundary conditions. This choice ensures that the long-time behavior of the noising\u221a process (1) converges to the uniform distribution on $R\\mathbb{T}^{d}$ . Therefore, we set $\\mathbf{\\nabla}f=\\mathbf{0}$ and $\\sigma(t)=\\sqrt{2}$ instead of using the Ornstein-Uhlenbeck process. We make this choice for mathematical clarity, however our results generally apply, with minor modifications, to the entire space $\\mathbb{R}^{d}$ and for other noising processes. ", "page_idx": 2}, {"type": "text", "text": "2.1 Equivalence of score-matching objectives in the finite sample regime ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Finite sample approximations of ESM, ISM, and DSM are not generally equivalent. However, [24, 25], show that DSM becomes equivalent to ESM and ISM in the finite sample regime when $\\eta^{\\pi}(s)$ is replaced with its kernel density estimate $\\eta^{N}(s)$ , where $\\begin{array}{r}{\\eta^{N}(0)=\\pi^{N}=\\frac{1}{N}\\sum_{i=1}^{N}\\delta_{z_{i}}}\\end{array}$ . The kernel estimate, however, does not have a well-defined score at $s=0$ , so the DSM objective is often integrated only for $s\\in[\\epsilon,T]$ , an example of early-stopping in SGM [1, 26]. In continuous time, this has the effect of score-matching for the mollified distribution $\\pi^{N,\\epsilon}=\\pi^{N}\\star\\rho_{\\epsilon}$ , where $\\rho_{\\epsilon}$ is the transition probability kernel for $s=\\epsilon^{1}$ . Then it can be shown that ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{I}(\\eta^{N,\\epsilon},\\theta)=J_{\\mathrm{DSM}}(\\eta^{N,\\epsilon},\\theta)=J_{\\mathrm{ESM}}(\\eta^{N,\\epsilon},\\theta)=J_{\\mathrm{ISM}}(\\eta^{N,\\epsilon},\\theta)+4\\|\\nabla\\sqrt{\\eta^{N,\\epsilon}}\\|_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3 An uncertainty quantification approach to generalization in SGMs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We describe our UQ approach to generalization in SGMs and overview our main results. Discussion of the proof methods is deferred to Sections 4 and 5. Our primary goal is to study how practical and approximation errors made in implementing SGMs translate into errors in the generative distribution relative to the true distribution. ", "page_idx": 3}, {"type": "text", "text": "3.1 Source of errors in score-based generative modeling ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We attribute errors of SGM to the following six sources: ", "page_idx": 3}, {"type": "text", "text": "\u2022 Data distribution is only accessible via samples $e_{1}$ : The target distribution is only known through a finite set of samples. In practice, the regularity of the score function and data distribution, such as Lipschitzianity or whether the distribution has a density, is unknowable. ", "page_idx": 3}, {"type": "text", "text": "\u2022 Choice of score-matching objective $e_{2}$ : In practice, score-matching objectives are approximated via samples. The DSM objective is most frequently used as it avoids computing derivatives of the score function. While DSM and ISM are equivalent given the exact density evolution $\\eta(x,s)$ , they differ when approximated with finite samples. Previous analysis typically assumes that the learned score function is close to the true score function within some $L^{\\tilde{2}}$ distance, i.e., a ball determined by $J_{\\mathrm{ESM}}$ . In contrast to previous work [11, 7, 5], we show in Theorem B.5 how training through DSM translates into a bound for ESM. Moreover we prove Proposition B.8 which states that if there exists a neural net that well-approximates the true score then that the target density is necessarily regular. ", "page_idx": 3}, {"type": "text", "text": "\u2022 Expressiveness of the score function $e_{3}$ . The expressivity of neural net approximations to the score function will depend on the particular expressivity of the parametrization. ", "page_idx": 3}, {"type": "text", "text": "\u2022 Choice of reference distribution $e_{4}$ . With access to the exact score function, the denoising process (2) produces trajectories that sample from $\\pi$ only if the initial condition starts at $\\eta^{\\pi}(\\cdot,T)$ . In practice, however, the initial distribution is usually a Gaussian approximation of $\\eta^{\\pi}$ or, in the case of the OU noising process, its corresponding stationary distribution. ", "page_idx": 3}, {"type": "text", "text": "\u2022 Early stopping of the denoising process $e_{5}$ . In practice, the score-matching objective is integrated from $\\bar{s}\\in[\\epsilon,T]$ , for small $\\epsilon$ , instead of $s=0$ [1, 26]. This prevents the SGM from memorizing the training data [27, 28], and is equivalent to running the denoising process for $t\\in[0,\\bar{T}-\\epsilon]$ . Early stopping is crucial when the data distribution is supported on a low-dimensional manifold, where it has no density with respect to Lebesgue measure. Previous analyses of SGM often adjust $\\epsilon$ to optimize generalization bounds [6, 20]. ", "page_idx": 3}, {"type": "text", "text": "\u2022 Discretization error $e_{6}$ . The denoising SDE must be solved via a numerical scheme. Previous work [5, 11, 7, 8] have considered the impact of discretization error on the generalization abilities of SGM. While our analysis does not consider the discretization error, our approach can be extended to study discretization error through the use of modified equations [29]. ", "page_idx": 3}, {"type": "text", "text": "3.2 Model-form uncertainty quantification ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Let d be an integral probability metric (IPM) between measures $\\nu_{1},\\nu_{2}\\in\\mathscr{P}(\\Omega)$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{d}(\\nu_{1},\\nu_{2})=\\operatorname*{sup}_{\\psi\\in\\mathcal{X}}\\int\\psi(x)d(\\nu_{2}-\\nu_{1}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "1Here, $\\star$ denotes convolution. ", "page_idx": 3}, {"type": "text", "text": "for some function space $\\mathcal{X}$ . We assume $\\mathcal{X}=\\{\\psi:\\Omega\\rightarrow\\mathbb{R},||\\nabla\\psi||_{\\infty}\\leq1\\}$ or $\\{\\psi:\\Omega\\rightarrow\\mathbb{R},\\|\\psi\\|_{\\infty}\\leq$ $1\\}$ , corresponding to the Wasserstein-1 $(\\mathbf{d}_{1})$ and total variation distances, respectively. Recall that if $\\nu_{1},\\nu_{2}$ have densities, then their TV distance is equivalent to the $L^{1}$ distance between their densities. ", "page_idx": 4}, {"type": "text", "text": "Let $\\pi$ be the target data distribution and take the stationary distribution of the noising process on the torus $\\begin{array}{r}{m_{g}(0)=\\dot{\\frac{1}{v o l(R\\mathbb{T}^{d})}}}\\end{array}$ = vol(1RTd) as the initial condition. Given a learned score function s\u03b8, let mg(T) be the generative distribution produced by SGM. We study how IPMs between $\\pi$ and $m_{g}(T)$ relate to errors from the first five sources of errors discussed above, i.e., we seek bounds of the form ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{d}(m_{g}(T),\\pi)\\leq\\mathcal{F}(e_{1},e_{2},e_{3},e_{4},e_{5}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Our key insight is that for score-based generative modeling, we can derive bounds of the form (6) via a model-form uncertainty quantification bound for the generative Fokker-Planck equation. The Wasserstein Uncertainty Propagation theorem formalizes this insight. ", "page_idx": 4}, {"type": "text", "text": "3.3 Wasserstein uncertainty propagation theorem ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The WUP theorem is a general statement about how $L^{2}$ neighborhoods in the space of drift functions map to neighborhoods in ${\\mathcal{P}}(\\Omega)$ defined by IPMs. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1 (Wasserstein Uncertainty Propagation). Let $\\Omega=R\\ensuremath{\\mathbb{T}}^{d}\\,o r\\,\\ensuremath{\\mathbb{R}}^{d}$ . Let $b^{1},b^{2}:[0,T]\\times\\Omega\\rightarrow$ $\\mathbb{R}^{d}$ be given with $\\|\\nabla b^{1}\\|_{\\infty}<\\infty,$ , and $m_{1},m_{2}\\in\\mathscr{P}(\\Omega)$ . If $m^{i}$ for $i=1,2$ are given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\partial_{t}m^{i}-\\Delta m^{i}-\\mathrm{div}(m^{i}b^{i})=0,\\ \\ m^{i}(0)=m_{i}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "then, up to a universal constant $C>0$ , we have the following: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\bullet\\ I\\!\\!f\\ \\operatorname*{sup}_{0\\leq t\\leq T}\\|(b^{2}-b^{1})(t)\\|_{L^{2}(m^{2}(t))}:=\\operatorname*{sup}_{0\\leq t\\leq T}\\left(\\int_{\\Omega}\\big|(b^{2}-b^{1})(t,x)\\big|^{2}\\,m^{2}(t,x)d x\\right)^{1/2}\\leq\\varepsilon_{1},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\|m^{2}(T)-m^{1}(T)\\|_{L^{1}(\\Omega)}\\leq C(\\sqrt{T\\|\\nabla b^{1}\\|_{\\infty}}+1)\\left(\\frac{1}{\\sqrt{T}}\\mathbf{d}_{1}(m_{1},m_{2})+\\sqrt{T}\\varepsilon_{1}\\right),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|m^{2}(T)-m^{1}(T)\\|_{L^{1}(\\Omega)}\\leq C(\\sqrt{T\\|\\nabla b^{1}\\|_{\\infty}}+1)\\left(\\|m_{1}-m_{2}\\|_{L^{1}(\\Omega)}+\\sqrt{T}\\varepsilon_{1}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\bullet\\ F o r\\,\\Omega=R\\mathbb{T}^{d},\\,i f\\Vert b^{2}-b^{1}\\Vert_{L^{2}(m^{2})}:=\\left(\\int_{0}^{T}\\int_{\\Omega}\\left|(b^{2}-b^{1})(t,x)\\right|^{2}m^{2}(t,x)d x d t\\right)^{1/2}\\leq\\varepsilon_{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{d}_{1}(m^{2}(T),m^{1}(T))\\leq C R^{\\frac{3}{2}}(1+\\sqrt{\\|\\nabla b^{1}\\|_{\\infty}})\\left(\\mathbf{d}_{1}(m_{1},m_{2})+\\varepsilon_{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Equation (8) is a particularly notable result as we bound the TV distance between $m^{1}(T)$ and $m^{2}(T)$ in terms of a weaker ${\\bf d}_{1}$ metric between $m_{1}$ and $m_{2}$ . This is due to the regularizing effects of diffusion processes, which is showcased in the proof. See Section 4 and Section A.1 for full details. ", "page_idx": 4}, {"type": "text", "text": "3.4 Robustness of errors under ESM ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We use WUP to produce generalization bounds when the only errors are due to the choice of the initial condition and the approximation of the score function with respect to the ESM objective. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.2 (ESM bounds). Let $\\pi\\in{\\mathcal{P}}(\\Omega)$ where $\\Omega=R\\mathbb{T}^{d}$ for some $R>0$ . Moreover let $e_{n n}>0$ . Assume that the learned score function $\\mathbf{s}_{\\theta}$ is such that $\\mathcal{I}(\\eta^{\\pi},\\theta)\\le e_{n n}$ . Then for $\\mathbf{b}_{\\theta}=\\mathbf{s}_{\\theta}(T-t,x)$ the generated distribution $m_{g}(T)\\approx\\pi$ where ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\partial_{t}m_{g}-\\Delta m_{g}-\\mathrm{div}(m_{g}\\mathbf{b}_{\\theta})=0\\,i n\\left(0,T\\right]\\times\\Omega,\\ \\ m_{g}(0)=\\frac{1}{\\nu o l(R\\mathbb{T}^{d})}\\,i n\\,\\Omega,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "satisfies ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{d}_{1}(\\pi,m_{g}(T))\\leq C R^{\\frac{3}{2}}(1+\\sqrt{\\|\\nabla\\mathbf{s}_{\\theta}\\|_{\\infty}})\\Big(\\underbrace{R e^{-\\frac{\\omega T}{R^{2}}}\\mathbf{d}_{1}\\Big(\\pi,\\frac{1}{\\nu o l(R\\mathbb{T}^{d})}\\Big)}_{E r r o r f r o m\\:c h o i c e\\:\\sigma f r e r n c e\\:m e a s u r e\\:(e_{4})}+\\underbrace{\\sqrt{e_{n n}}}_{S c o r e\\:f i u n c t i o n\\:e r m r\\:(e_{3})}\\Big).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "If the stronger estimate $\\operatorname*{sup}_{1\\leq t\\leq T}\\|\\mathbf{s}_{\\theta}-\\nabla\\log(\\eta^{\\pi})\\|_{L^{2}(\\eta^{\\pi}(t))}^{2}\\leq e_{n n},$ , holds, then ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\|m_{g}(T)-\\pi\\|_{L^{1}(\\Omega)}\\leq C(\\sqrt{T\\|\\nabla\\mathbf{s}_{\\theta}\\|_{\\infty}}+1)\\left(\\frac{R^{2}e^{-\\frac{\\omega T}{R^{2}}}}{\\sqrt{T}}\\mathbf{d}_{1}\\Big(\\pi,\\frac{1}{\\nu o l(R\\mathbb{T}^{d})}\\Big)+\\sqrt{T e_{n n}}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here, the constants $C,\\omega$ depend only on the dimension $d,$ . ", "page_idx": 5}, {"type": "text", "text": "Applying WUP for $b^{1}$ and $b^{2}$ to be the true and learned score function, respectively, with Proposition D.3 on the convergence of the noising process to the stationary measure immediately yields the estimates above. Notice that the TV estimate (13) is comparable to Theorem 2 of [5], which also assumes a uniform-in-time $L^{2}$ -accurate score function. Again, notice that we are able to bound the TV distance between $m_{g}(T)$ and $\\pi$ using the weaker ${\\bf d}_{1}$ distance between $\\pi$ and $\\frac{1}{\\mathrm{vol}(R\\mathbb{T}^{d})}$ ", "page_idx": 5}, {"type": "text", "text": "3.5 Robustness of errors under DSM ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In practice, the score is learned through samples using the DSM objective with an early stopping parameter [1, 26]. SGM is effective at producing samples from distributions supported on (or near) low dimensional manifolds. Our ${\\bf d}_{1}$ generalization bound describes how early stopping aids in generalization. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3.3. (Pointwise DSM generalization) Let $\\mathbf{b}_{\\theta}=\\mathbf{s}_{\\theta}(T\\!-\\!t,x)$ and $m_{g}:[0,T]\\times R\\mathbb{T}^{d}\\rightarrow\\mathbb{R}\\,b\\epsilon$ 2 given by (11). Assume the learned score function is such that $J_{D S M}(\\eta^{N,\\epsilon},\\theta)=\\mathcal{I}(\\eta^{N,\\epsilon},\\theta)<e_{n n}$ . Let $0<\\delta<\\epsilon$ be such that $\\delta\\le\\hat{\\pi}^{N,\\epsilon}$ , $\\delta<\\pi^{\\epsilon}$ . Then up to a dimensional constant $C=C(d)>0$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{d}_{1}(\\pi,m_{g}(T))\\lesssim\\underbrace{\\sqrt{\\epsilon}}_{E a r l y\\,s t o p p i n g\\,(e_{S})}+R^{3/2}(1+\\sqrt{\\|\\nabla\\mathbf{s}_{\\theta}\\|_{\\infty}})\\biggl(\\underbrace{R e^{-\\frac{\\omega T}{R^{2}}}\\mathbf{d}_{1}\\left(\\pi,\\frac{1}{\\nu o l(R\\mathbb{T}^{d})}\\right)}_{\\substack{r_{1},\\ldots,\\ldots,\\ldots,r_{e}\\longrightarrow\\mathbf{d}_{N}\\,,}}+\\sqrt{e_{n n}^{\\prime}}\\biggr),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where ", "page_idx": 5}, {"type": "text", "text": "The pointwise DSM generalization bound applies to every finite training sample of size $N$ . A crucial part of this theorem relates the error in the practical DSM objective function to the ESM error needed to apply the WUP theorem. We connect the DSM objective early stopping to the ESM error with the mollified distribution $\\pi^{\\epsilon}$ in Lemma B.5. This result is agnostic to the manifold hypothesis for $\\pi$ , as long as $\\epsilon>0$ . Such bounds will blow up if the KL divergence were used instead. In fact, previous results that use the KL divergence to bound the TV distance [5, 19, 11, 6] will produce vacuous bounds for the ${\\bf d}_{1}$ distance under the manifold hypothesis as their ${\\bf d}_{1}$ generalization bounds are derived by bounding the KL divergence. ", "page_idx": 5}, {"type": "text", "text": "Remark 3.4 (Density lower bound). Similar to [6], our DSM generalization bound relies on a iadneneynq sriuataynl idltooy srwa bem opculoen $\\{z_{i}\\}_{i=1}^{N}\\sim\\pi$ p.. e TcWtheies d s dihtioys twlaoenwvceeerr ,  bbtehotauwtn edoe unar D uaSmnMpdt l  ibozeva etrireo mrna obnveodduo nmvid a e hJmoelpndisrsie fcnoa\u2019rsl $i f$ ${\\bf d}_{1}$ $\\pi$ $m_{g}(T)$ measures. See Theorem C.1 and its proof in Section $C$ . ", "page_idx": 5}, {"type": "text", "text": "Remark 3.5 (Trade-offs and memorization). Theorem 3.3 captures trade-offs when training SGMs and memorization effects. To minimize the error from early stopping, we can let $\\epsilon\\rightarrow0$ . However, empirically, without early stopping, SGMs overfit to the kernel approximation and memorize the training data [27, 28, 25]. The bound is vacuous when $\\epsilon=0$ regardless of whether the distribution lies on a low-dimensional manifold. As $\\epsilon\\,\\rightarrow\\,0$ , training the DSM to be small implies that the score function must approximate a rough function with large Lipschitz constant, which will increase the bound. This shows that overtraining the DSM objective may not necessarily yield a better generative model. Moreover, suppose that $\\pi^{N}=\\pi$ , then as $\\epsilon\\,\\rightarrow\\,0$ and $e_{n n}\\,\\rightarrow\\,0$ , we have that $\\mathbf{d}_{1}(\\pi,m_{g}(T))\\to0$ , indicating the model memorizes the training data. Our results corroborate those of [25]. ", "page_idx": 5}, {"type": "text", "text": "4 Regularity theory of Hamilton-Jacobi-Bellman PDEs enables uncertainty quantification in SGMs ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "A recent result in [30] established connections between generative flows with the theory of PDEs, more specifically the theory of mean field games. We continue investigating these connections and showcase how one may study generative modeling algorithms by obtaining stability estimates for the Fokker-Planck equation. We provide a proof sketch for our Wasserstein Uncertainty Quantification theorem (Theorem 3.1). Our strategy involves (1) constructing test functions for the IPMs using the Kolmogorov backward equation, (2) choosing the suitable function space for the terminal data depending on the desired IPM, and (3) obtaining gradient estimates of the test functions via Bernstein estimates. The theorem relies on the gradient of the test function remaining bounded for $t\\in[0,T)$ , which is guaranteed by the regularizing properties of diffusion processes. See A.1 for full details about the proof. ", "page_idx": 6}, {"type": "text", "text": "4.1 Kolmogorov backward equation determines suitable test functions ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "From (7), we aim to compute bounds for $\\begin{array}{r}{\\mathbf{d}\\big(m^{1}(T),m^{2}(T)\\big)=\\operatorname*{sup}_{\\psi(x)\\in\\mathcal{X}}\\int\\psi\\big(m^{1}(T)-m^{2}(T)\\big)d x.}\\end{array}$ The measure $\\lambda=m^{1}-m^{2}$ satisfies the PDE ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\partial_{t}\\lambda-\\Delta\\lambda-\\operatorname{div}(\\lambda b^{1}+m^{2}(b^{1}-b^{2}))=0\\operatorname{in}\\left(0,T\\right)\\times\\Omega,\\ \\ \\lambda(0)=m_{2}-m_{1}\\operatorname{in}\\Omega.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Let $\\phi:[0,T]\\times\\Omega\\rightarrow\\mathbb{R}$ be a test function in space and time. We integrate in space and time against the PDE (16) and integrate by parts to move the derivatives on to $\\phi$ which yields ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\int_{\\Omega}\\lambda(T,x)\\phi(T,x)-\\lambda(0,x)\\phi(0,x)d x+\\int_{0}^{T}\\int_{\\Omega}\\lambda\\left(-\\partial_{t}\\phi-\\Delta\\phi+b^{1}\\cdot\\nabla\\phi\\right)d x d t}&{{}}\\\\ {\\displaystyle+\\int_{0}^{T}\\int_{\\Omega}m^{2}\\nabla\\phi\\cdot(b^{1}-b^{2})d x d t=0}&{{}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Notice that if we choose the test function $\\phi$ to satisfy the Kolmogorov backward equation (KBE) ", "page_idx": 6}, {"type": "equation", "text": "$$\n-\\partial_{t}\\phi-\\Delta\\phi+b^{1}\\cdot\\nabla\\phi=0\\;\\mathrm{in}\\left[0,T\\right)\\times\\Omega,\\;\\;\\phi(T,x)=\\psi(x)\\;\\mathrm{in}\\;\\Omega\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with terminal condition $\\psi\\in\\mathcal X$ , then from (17), we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\int_{\\Omega}\\lambda(T,x)\\psi(x)d x=\\int_{\\Omega}\\lambda(0,x)\\phi(0,x)d x+\\int_{0}^{T}\\int_{\\Omega}m^{2}(t)\\nabla\\phi(t,x)\\cdot(b^{2}-b^{1})(t)d x d t.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The equality above is valid for any terminal condition $\\psi$ . Depending on the choice of function space $\\mathcal{X}$ for $\\psi$ , we obtain bounds on different IPMs. Taking the supremum over $\\mathcal{X}$ we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbf{d}(m^{1}(T),m^{2}(T))\\leq\\operatorname*{sup}_{\\psi\\in\\mathcal{X}}\\left|\\int_{\\Omega}\\lambda(0,x)\\phi(0,x)d x\\right|+\\operatorname*{sup}_{\\psi\\in\\mathcal{X}}\\left|\\int_{0}^{T}\\!\\!\\!\\int_{\\Omega}m^{2}\\nabla\\phi\\cdot(b^{2}-b^{1})d x d t\\right|.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Recall that $\\phi$ is related to $\\psi$ via the KBE (18) To obtain bounds for $\\mathbf{d}$ , we need to bound the two terms in (20), which depend on the choice of function space $\\mathcal{X}$ and assumptions on the drift terms. ", "page_idx": 6}, {"type": "text", "text": "4.2 IPM bounds depend on choice of terminal function space and gradient estimates ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We split our proof sketch into two parts; the first part focuses on deriving $L^{1}$ estimates, while the second derives ${\\bf d}_{1}$ estimates. ", "page_idx": 6}, {"type": "text", "text": "$L^{1}$ estimates. We first note that because of the regularizing properties of (18) we can obtain bounds on $\\textstyle\\int_{\\Omega}\\lambda(0,x)\\phi(0,x)d x$ with weaker norms on $\\phi$ . Notice that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\int_{\\Omega}\\lambda(0,x)\\phi(0,x)d x\\leq\\|\\lambda(0)\\|y\\prime\\|\\phi(0)\\|y,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\boldsymbol{\\wp}$ denotes a generic space of functions, and $\\mathcal{V}^{\\prime}$ is its dual. In what follows, we show that regularizing effects of (18) takes functions in $\\mathcal{X}$ to functions with more regularity $\\boldsymbol{\\wp}$ such that $\\boldsymbol{\\wp}$ is compactly embedded in $\\mathcal{X}$ . This in turn implies that $\\|\\cdot\\|_{\\mathcal{V}^{\\prime}}$ is weaker than $\\mathbf{d}$ and so we are able to bound the stronger norm d by the weaker norm. ", "page_idx": 6}, {"type": "text", "text": "\u2022 To prove (8), we $\\mathbf{d}=\\|\\cdot\\|_{L^{1}(\\Omega)}$ , which corresponds with $\\mathcal{X}=\\{\\psi:\\Omega\\rightarrow\\mathbb{R},\\|\\psi\\|_{\\infty}\\leq1\\}$ . Observe that we can take $\\mathcal{V}=\\{\\psi:\\Omega\\rightarrow\\mathbb{R},\\|\\nabla\\psi\\|_{\\infty}\\leq1\\}$ , in which case we obtain ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\int_{\\Omega}\\lambda(0,x)\\phi(0,x)d x=\\|\\nabla\\phi(0,x)\\|_{\\infty}\\int_{\\Omega}(m_{1}-m_{2})\\frac{\\phi(0,x)}{\\|\\nabla\\phi(0,x)\\|_{\\infty}}d x}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\leq\\mathbf{d}_{1}(m_{1},m_{2})\\|\\nabla\\phi(0,x)\\|_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Notice that this bound crucially depends on showing that $\\phi(0,x)$ is Lipschitz. ", "page_idx": 7}, {"type": "text", "text": "\u2022 To prove (9), we can simply take $\\mathcal{X}=\\mathcal{V}$ , and obtain ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\int_{\\Omega}\\lambda(0,x)\\phi(0,x)\\leq\\|\\lambda(0,x)\\|_{L^{1}(\\Omega)}\\|\\phi(0,x)\\|_{\\infty}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "For (8) and (9), Cauchy-Schwarz on the spatial integral shows the second term can be bounded as ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\int_{0}^{T}\\displaystyle\\int_{\\Omega}m^{2}\\nabla\\phi\\cdot(b^{2}-b^{1})d x d t\\le\\displaystyle\\int_{0}^{T}\\|(b^{1}-b^{2})(t)\\|_{L^{2}(m^{2}(t))}\\|\\nabla\\phi(t,x)\\|_{L^{2}(m^{2}(t))}d t}\\\\ {\\displaystyle\\le\\operatorname*{sup}_{0\\le t\\le T}\\|(b^{1}-b^{2})(t)\\|_{L^{2}(m^{2}(t))}\\displaystyle\\int_{0}^{T}\\|\\nabla\\phi(t,\\cdot)\\|_{\\infty}d t.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Notice here that it is crucial to produce estimates for $\\nabla\\phi$ . ", "page_idx": 7}, {"type": "text", "text": "Wasserstein-1 $\\left(\\mathbf{d}_{1}\\right)$ estimates. To prove (10), we choose $\\mathcal{X}=\\mathcal{Y}=\\{\\boldsymbol{\\psi}:\\Omega\\rightarrow\\mathbb{R}$ , $\\|\\nabla\\psi\\|_{\\infty}\\leq1\\}$ , the space of 1-Lipschitz functions. We obtain (22) again, except the terminal data $\\psi$ also has Lipschitz constant equal to 1. Applying Cauchy-Schwarz in space and time implies the second term is bounded ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\int_{0}^{T}\\int_{\\Omega}m^{2}\\nabla\\phi\\cdot(b^{2}-b^{1})d x d t\\le\\left(\\int_{0}^{T}\\int_{\\Omega}|\\nabla\\phi|^{2}m^{2}d x d t\\right)^{\\frac{1}{2}}\\|b^{1}-b^{2}\\|_{L^{2}(m^{2})}}}\\\\ &{}&{\\le T\\displaystyle\\operatorname*{sup}_{0\\le t\\le T}\\|\\nabla\\phi(t,\\cdot)\\|_{\\infty}\\|b^{1}-b^{2}\\|_{L^{2}(m^{2})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "We again see that we require estimates for $\\nabla\\phi$ , and we need them to be bounded. ", "page_idx": 7}, {"type": "text", "text": "4.3 Bernstein estimates from HJB theory provide gradient estimates ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To obtain estimates for $\\nabla\\phi$ , we could differentiate (18) to derive a PDE for $\\nabla\\phi$ . The resulting PDE, however, will have $\\partial_{t}(\\nabla\\phi)$ grow linearly with $\\nabla\\phi$ , and so if we apply (reverse) Gronwall\u2019s inequality, the resulting estimates for $\\nabla\\phi$ will grow exponentially in time. To avoid this exponential time dependence, we first perform a Hopf-Cole transform $u=-2\\log\\phi$ on (18) to derive the HJB equation for $u$ [9] ", "page_idx": 7}, {"type": "equation", "text": "$$\n-\\partial_{t}u-\\Delta u+\\frac{1}{2}|\\nabla u|^{2}+b\\cdot\\nabla u=0,\\ \\ u(T,x)=-2\\log(\\psi(x)).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Here we provide an example of classical Bernstein estimates (Proposition D.1 and Corollary D.2) to obtain bounds for $\\nabla\\phi$ without using Gronwall\u2019s inequality [10]. The main idea is to derive a PDE (60) for the function $\\begin{array}{r}{z=\\frac{1}{2}|\\nabla u|^{2}}\\end{array}$ by taking the gradient of (26) and then taking the inner product with $\\nabla u$ . Then by showing that the function $w(\\bar{t},x)=z-C u$ for sufficiently large $C$ attains its maximum at $(T,x_{0})$ , we can show that ", "page_idx": 7}, {"type": "equation", "text": "$$\nz(t,x)\\leq w(T,x_{0})+C u(t,x)\\leq\\frac{1}{2}|2\\nabla\\log\\psi|^{2}+C\\|2\\log\\psi\\|_{\\infty}+C\\|u\\|_{\\infty}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Using the maximum principle for (18), we find $\\|u\\|_{\\infty}=\\|2\\log\\psi\\|_{\\infty}.$ , yielding $z(t,x)\\leq C\\|\\log\\psi\\|_{C^{1}}$ . Assuming that $\\psi$ is Lipschitz continuous implies boundedness of $z$ and therefore $\\nabla\\phi$ for all time. A similar result holds when $\\psi\\in L^{\\infty}$ only (see (59)). Detailed proofs and related boundes are provided in Proposition D.1. Applications of the bounds to derive the WUP is provided in Section A.1. ", "page_idx": 7}, {"type": "text", "text": "Remark 4.1 (The regularizing role of stochasticity). In our analysis, the stochasticity in SGMs provides two types of regularizing effects. The first is early stopping, which adds a small amount of Gaussian noise of the data distribution. This, in effect, is equivalent to running the noising process for a short amount of time and immediately mollifies the initial empirical distribution so that it has a smooth density. Second, the Laplacian is the key mechanism that regularizes the test function in (18) [9], which then allows us to bound, for example, the stronger $\\Vert\\cdot\\Vert_{L^{1}}$ norm by the weaker ${\\bf d}_{1}$ -norm. Recall that in PDE theory, the stochasticity of a generative flow manifests as the Laplacian operator in the Fokker-Planck and Kolmogorov backward equations [9]. Without the Laplacian the regularizing the test functions in (18) would not be possible in general and, in fact, we would not have access to long time behavior results. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "5 Proof sketches \u2014 Score-based generative models are robust to errors ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We now provide sketches of the proofs for the main generalization bounds in Theorems 3.2 and 3.3.   \nThe full proofs are provided in Sections A.2 and A.3, respectively. ", "page_idx": 8}, {"type": "text", "text": "5.1 Theorem 3.2: ESM generalization bound ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Theorem 3.2 is a generalization bound with respect to error from the score function approximation with respect to the ESM objective $(e_{3})$ and the choice of reference measure $(e_{4})$ . Assuming we have an (uniform-in-time) $L^{2}$ -close approximation of the score function, first apply the WUP Theorem 3.1 with $\\begin{array}{r}{m_{1}=\\frac{1}{\\mathrm{vol}(R\\mathbb{T}^{d})}}\\end{array}$ and $m_{2}=\\eta^{\\pi}(T,\\cdot)$ . The distance between $m_{1}$ and $m_{2}$ can be expressed in terms of $\\pi$ by studying the long time behavior of periodic solutions to the heat equation. Proposition D.3 shows the heat equation is a contraction under ${\\bf d}_{1}$ . Applying it to $m^{1}$ and $m^{\\bar{2}}$ yields the desired result. ", "page_idx": 8}, {"type": "text", "text": "While Theorem 3.2 has no explicit assumptions on $\\pi$ , the assumption that the approximate score function $\\mathbf{s}_{\\theta}$ is close in $L^{2}$ to the true score implicitly implies regularity of $\\pi$ . Specifically, if $\\mathbf{s}_{\\theta}$ is Lipschitz (which is true for neural networks in practice) and satisfies $\\mathcal{I}(\\pi,\\theta)<e_{n n}$ , then $\\pi$ must have finite entropy. This implies that $\\pi$ necessarily has a density. See Proposition B.8 for the formal statement and proof. ", "page_idx": 8}, {"type": "text", "text": "5.2 Theorem 3.3: Pointwise DSM generalization bound ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In practice, the score function is learned via a Monte Carlo approximation of the DSM objective (2). To avoid overfitting to the kernel estimator and memorizing the dataset [25], the time integral is taken only over $s\\in[\\epsilon,T]$ where $\\epsilon$ is the early stopping parameter. This is equivalent to training with the ESM objective with the true score replaced with the kernel approximation at time $\\epsilon$ [20, 27]. To derive generalization bounds of SGMs trained via DSM, we establish the relationships between (1) the mollified distribution $\\pi^{\\epsilon}=\\Gamma(\\epsilon)\\star\\pi$ and the true distribution $\\pi$ , and (2) the DSM objective $J_{D S M}(\\eta^{N,\\epsilon},\\theta)\\,=\\,\\mathcal{I}(\\eta^{N,\\epsilon},\\theta)$ and the ESM objective with respect to the score function of the mollified distribution $\\mathcal{I}(\\eta^{\\pi^{\\epsilon}},\\theta)$ . Formally, this strategy involves bounding the following two terms ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\underbrace{{\\bf d}_{1}(\\pi,m_{g}(T))}_{\\mathrm{ation\\:bound\\:w.r.t\\:tue\\:distribution}}\\le\\underbrace{{\\bf d}_{1}(\\pi,\\pi^{\\epsilon})}_{\\mathrm{Early\\:stopping\\:error}\\:(e_{5})}+\\underbrace{{\\bf d}_{1}(\\pi^{\\epsilon},m_{g}(T)).}_{\\mathrm{Generalization\\:bound\\:w.r.t.\\:mollified}}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "For the early st\u221aopping error, we use the regularizing properties of the heat equation to show that $\\mathbf{d}_{1}(\\pi,\\pi^{\\epsilon})\\leq C\\sqrt{\\epsilon}$ , where $C$ only depen\u221ads on the dimension $d$ . This implies that, measured in ${\\bf d}_{1}$ , early stopping only incurs a nominal $C\\sqrt\\epsilon$ error even if the $\\pi$ does not admit a density. This result would not possible if we were to study generalization error in terms of $\\mathrm{KL}$ or TV directly. ", "page_idx": 8}, {"type": "text", "text": "A bound for the second term is obtained by comparing the ESM objective value between the learned score function and the true score function of the early stopped distribution, $\\mathcal{I}(\\eta^{\\pi^{\\epsilon}},\\theta)$ to the DSM objective $\\mathcal{I}(\\eta^{N,\\epsilon},\\theta)$ . We present Theorem B.5 and its corollary, which under the assumption that $\\eta^{N,\\epsilon}$ and $\\eta^{\\pi^{\\epsilon}}$ have a lower bound $\\delta$ , state that if $\\mathcal{I}(\\eta^{N,\\epsilon},\\mathbf{s}_{\\theta})<e_{n n}$ , then ${\\mathcal{I}}(\\eta^{\\pi^{\\epsilon}},\\mathbf{s}_{\\theta})<e_{n n}^{\\prime}$ with ", "page_idx": 8}, {"type": "equation", "text": "$$\ne_{n n}^{\\prime}=e_{n n}+C\\Big(1+\\frac{|\\log(\\delta)|}{\\sqrt{\\epsilon}}+\\frac{1}{\\sqrt{T}}+T\\|\\mathbf{s}_{\\theta}\\|_{C^{2}([0,T]\\times\\Omega)}^{2}\\Big)\\mathbf{d}_{1}\\big(\\pi^{N},\\pi\\big).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "The main idea behind Theorem B.5 is to note that the difference between the ESM and DSM objective functions can be written as a difference in ISM objective functions plus the entropy difference between $\\eta^{N,\\epsilon}$ and $\\eta^{\\pi^{\\epsilon}}$ . Details for bounding this term is provided in Proposition B.2 and Lemma B.3. ", "page_idx": 8}, {"type": "text", "text": "To arrive at the final result, apply the WUP theorem to derive generalization bounds for $\\mathbf{d}_{1}(\\pi^{\\epsilon},m_{g}(T))$ under the assumption that $\\mathcal{I}(\\eta^{\\pi^{\\epsilon}},\\theta)<e_{n n}^{\\prime}$ , along with (29). Finally, combine this result with the error due to the early stopping $\\mathbf{d}_{1}(\\pi,\\pi^{\\epsilon})$ . Full details of this proof is provided in Section A.3. ", "page_idx": 8}, {"type": "text", "text": "6 Discussion: PDE regularity theory and UQ for generative modeling ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our main contribution is the study of generalization in score-based generative models from the perspective of uncertainty quantification. The regularity theory of nonlinear PDEs is the key technical tool that produces our results. We emphasize that the tools we use here can be used generative models beyond SGMs, and that we have not pushed our analysis to the limits of our tools in SGMs. Moreover, we also emphasize some downstream UQ applications for SGMs that may be of future interest. ", "page_idx": 9}, {"type": "text", "text": "6.1 The significance of the regularizing properties of SGMs ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "A surprising result of our work is deriving bounds for the stronger $L^{1}$ distance in terms of the weaker ${\\bf d}_{1}$ distance (see (22) and Theorem 3.2). The key insight (Equation (21)) is that the evolution of observables defined by the KBE (18) regularizes the test function. We have not fully exploited the regularizing effects of (18) in this paper, as we only focused on $L^{1}$ and ${\\bf d}_{1}$ estimates. ", "page_idx": 9}, {"type": "text", "text": "Improved bounds in Sobolev spaces $H^{s}$ . To illustrate other extensions and choices of $\\boldsymbol{\\wp}$ in (21), observe that in the trivial case when $b^{1}\\,=\\,b^{2}\\,=\\,0$ , (19) simplifies to $\\textstyle\\int\\lambda(T,x)\\psi(x)d x\\,=$ $\\textstyle\\int\\lambda(0,x)(\\Gamma(T)\\star\\psi)(x)d x$ , where $\\Gamma$ is the heat kernel. If $\\|\\psi\\|_{\\infty}\\leq1$ , then $\\Gamma\\star\\psi(T)\\in C^{\\infty}$ and we have estimates of the form $\\mathbf{d}_{1}(m^{1}(T),m^{2}(T))\\leq C(s,T,d)\\|m_{1}-m_{2}\\|_{H^{-s}}$ for all $s\\in\\mathbb N$ . When $b^{1},b^{2}$ are not identically zero we still expect such estimates, though they will depend on the regularity of $b^{1}$ . To highlight the importance of regularizing effects, note that by [31], if $\\pi\\in\\mathcal{P}(\\mathbb{T}^{d})$ , then for the empirical measure $\\pi^{N}$ , we expect $\\begin{array}{r}{\\mathbf{d}_{1}(\\pi,\\pi^{N})\\lesssim\\frac{1}{N^{\\frac{1}{d}}}}\\end{array}$ . However if $s>\\frac{d}{2}$ , $\\begin{array}{r}{\\|\\pi-\\pi^{N}\\|_{H^{-s}}\\lesssim\\frac{1}{\\sqrt{N}}}\\end{array}$ . This suggests that improved regularity may influence overcoming the curse of dimensionality. ", "page_idx": 9}, {"type": "text", "text": "6.2 A connection to likelihood-free inference ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Computing expectations with respect to posterior distributions is a key task in Bayesian inference. Generative modeling, in particular, has a key role in future developments of likelihood-free inference [32, 33, 34, 35]. For generative models to be trustworthy for inference, they must to be shown to be robust. The WUP theorem provides error bounds for approximating expectations with respect to some true unknown distribution, and may be significant for SGMs in likelihood-free inference. ", "page_idx": 9}, {"type": "text", "text": "For example, suppose we wish to estimate $\\mathbb{E}_{\\pi}h$ for some distribution $\\pi$ and observable $h$ , and an SGM $m_{g}(T)$ is constructed to approximate the expectation. Bounds of the form (6), such as the WUP theorem, translate into guarantees on the expectations: ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left|\\mathbb{E}_{\\pi}h-\\mathbb{E}_{m_{g}(T)}h\\right|\\le{\\bf d}(m_{g}(T),\\pi)\\le{\\mathcal F}(e_{1},e_{2},e_{3},e_{4},e_{5}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "In the context of SGM, the inequalities are a posteriori bounds, meaning they can be computed after learning the model. Additional regularity on $h$ may yield improved guarantees. ", "page_idx": 9}, {"type": "text", "text": "6.3 Enabling distributionally robust optimization (DRO) ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Robust UQ methods are based on the perspective that learning any complex model will typically involve multiple sources of uncertainty due to modeling choices, model reduction or learning from imperfect data. These uncertainties are not just in parameters but are inherently present in the mathematical model itself and will propagate to any predictions. There is substantial related work in recent years using a distributional robustness perspective, [12], using divergences or probability metrics and their variational representations to quantify the impact of model uncertainty around a baseline model that may be either learned (e.g. a generative model) or could be just an empirical distribution from an unknown true distribution. The approach can generally be described as quantifying an uncertainty set around the baseline model that the worst-case distribution belongs to via some neighborhood defined in terms of a probability divergence [13, 14], a Wasserstein distance [15] or maximum mean discrepancy [16]. There are, however, drawbacks to each of these approaches: a divergence ball contains only distributions with the same support as the baseline, while the uncertainty set may be hard to determine practically. The WUP Theorem is a related robust UQ notion where the uncertainty ball is in an IPM, e.g. 1-Wasserstein, MMD, or TV. However, it allows us to bypass the robust UQ for stochastic processes relying on restrictive, path-space probability divergence-based approaches or Girsanov\u2019s Theorem, [36, 37]. Furthermore, the WUP Theorem bounds use PDE theory to provide a computable uncertainty set, as we also demonstrate in the case of DSM, see Theorem 3.3. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "M.K. and B.Z. are partially funded by AFOSR grant FA9550-21-1-0354. M.K. is partially funded by NSF DMS-2307115 and NSF TRIPODS CISE-1934846. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.   \n[2] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840\u20136851, 2020.   \n[3] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780\u20138794, 2021.   \n[4] Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma with denoising diffusion GANs. arXiv preprint arXiv:2112.07804, 2021.   \n[5] Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. arXiv preprint arXiv:2209.11215, 2022.   \n[6] Kazusato Oko, Shunta Akiyama, and Taiji Suzuki. Diffusion models are minimax optimal distribution estimators. In International Conference on Machine Learning, pages 26517\u201326582. PMLR, 2023.   \n[7] Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence for score-based generative modeling with polynomial complexity. Advances in Neural Information Processing Systems, 35:22870\u201322882, 2022.   \n[8] Valentin De Bortoli. Convergence of denoising diffusion models under the manifold hypothesis. arXiv preprint arXiv:2208.05314, 2022.   \n[9] Lawrence C Evans. Partial differential equations, volume 19. American Mathematical Society, 2022.   \n[10] Hung V Tran. Hamilton\u2013Jacobi equations: theory and applications, volume 213. American Mathematical Soc., 2021.   \n[11] Hongrui Chen, Holden Lee, and Jianfeng Lu. Improved analysis of score-based generative modeling: User-friendly bounds under minimal smoothness assumptions. In International Conference on Machine Learning, pages 4735\u20134763. PMLR, 2023.   \n[12] Hamed Rahimian and Sanjay Mehrotra. Distributionally robust optimization: A review. ArXiv, abs/1908.05659, 2019.   \n[13] Kamaljit Chowdhary and Paul Dupuis. Distinguishing and integrating aleatoric and epistemic variation in uncertainty quantification. ESAIM: Mathematical Modelling and Numerical Analysis, 47(03):635\u2013662, 2013.   \n[14] Jun ya Gotoh, Michael Jong Kim, and Andrew E.B. Lim. Robust empirical optimization is almost the same as mean\u2013variance optimization. Operations Research Letters, 46(4):448\u2013452, 2018.   \n[15] Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust optimization using the Wasserstein metric: performance guarantees and tractable reformulations. Mathematical Programming, 171(1):115\u2013166, Sep 2018.   \n[16] Matthew Staib and Stefanie Jegelka. Distributionally robust optimization and generalization in kernel methods. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.   \n[17] Erich Novak, Mario Ullrich, Henryk Woz\u00b4niakowski, and Shun Zhang. Reproducing kernels of Sobolev spaces on $\\mathbb{R}^{d}$ and applications to embedding constants and tractability. Analysis and Applications, 16(05):693\u2013715, 2018.   \n[18] Giovanni Conforti, Alain Durmus, and Marta Gentiloni Silveri. Score diffusion models without early stopping: finite Fisher information is all you need. arXiv preprint arXiv:2308.12240, 2023.   \n[19] Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence of score-based generative modeling for general dat a distributions. In International Conference on Algorithmic Learning Theory, pages 946\u2013985. PMLR, 2023.   \n[20] Kaihong Zhang, Heqi Yin, Feng Liang, and Jingbo Liu. Minimax optimality of score-based diffusion models: Beyond the density lower bound assumptions. arXiv preprint arXiv:2402.15602, 2024.   \n[21] Dohyun Kwon, Ying Fan, and Kangwook Lee. Score-based generative modeling secretly minimizes the wasserstein distance. Advances in Neural Information Processing Systems, 35:20205\u201320217, 2022.   \n[22] Brian DO Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3):313\u2013326, 1982.   \n[23] Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach to density and score estimation. In Uncertainty in Artificial Intelligence, pages 574\u2013584. PMLR, 2020.   \n[24] Pascal Vincent. A connection between score matching and denoising autoencoders. Neural computation, 23(7):1661\u20131674, 2011.   \n[25] Jakiw Pidstrigach. Score-based generative models detect manifolds. Advances in Neural Information Processing Systems, 35:35852\u201335865, 2022.   \n[26] Yang Song. Score-based generative modeling through stochastic differential equations (sdes) - PyTorch implementation. https://github.com/yang-song/score_sde_pytorch, 2020. Accessed: May 2, 2024.   \n[27] Sixu Li, Shi Chen, and Qin Li. A good score does not lead to a good generative model. arXiv preprint arXiv:2401.04856, 2024.   \n[28] Benjamin J. Zhang, Siting Liu, Wuchen Li, Markos A. Katsoulakis, and Stanley J. Osher. Wasserstein proximal operators describe score-based generative models and resolve memorization, 2024.   \n[29] Assyr Abdulle, David Cohen, Gilles Vilmart, and Konstantinos C Zygalakis. High weak order methods for stochastic differential equations based on modified equations. SIAM Journal on Scientific Computing, 34(3):A1800\u2013A1823, 2012.   \n[30] Benjamin J Zhang and Markos A Katsoulakis. A mean-field games laboratory for generative modeling. arXiv preprint arXiv:2304.13534, 2023.   \n[31] Nicolas Fournier and Arnaud Guillin. On the rate of convergence in wasserstein distance of the empirical measure. Probability theory and related fields, 162(3):707\u2013738, 2015.   \n[32] Ricardo Baptista, Bamdad Hosseini, Nikola B Kovachki, and Youssef Marzouk. Conditional sampling with monotone GANs: from generative models to likelihood-free inference. arXiv preprint arXiv:2006.06755, 2020.   \n[33] Ricardo Baptista, Lianghao Cao, Joshua Chen, Omar Ghattas, Fengyi Li, Youssef M Marzouk, and J Tinsley Oden. Bayesian model calibration for block copolymer self-assembly: Likelihoodfree inference and expected information gain computation via measure transport. Journal of Computational Physics, page 112844, 2024.   \n[34] Yuyang Shi, Valentin De Bortoli, George Deligiannidis, and Arnaud Doucet. Conditional simulation using diffusion schr\u00f6dinger bridges. In Uncertainty in Artificial Intelligence, pages 1792\u20131802. PMLR, 2022.   \n[35] Yang Song, Liyue Shen, Lei Xing, and Stefano Ermon. Solving inverse problems in medical imaging with score-based generative models. arXiv preprint arXiv:2111.08005, 2021.   \n[36] Paul Dupuis, Markos A Katsoulakis, Yannis Pantazis, and Petr Plech\u00e1c. Path-space information bounds for uncertainty quantification and sensitivity analysis of stochastic dynamics. SIAM/ASA Journal on Uncertainty Quantification, 4(1):80\u2013111, 2016.   \n[37] Birrell, Jeremiah, Katsoulakis, Markos A., and Rey-Bellet, Luc. Quantification of model uncertainty on path-space via goal-oriented relative entropy. ESAIM: M2AN, 55(1):131\u2013169, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Proofs of main results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Throughout the proofs we will make frequent use of the fact that if $\\Gamma:[0,\\infty)\\times\\Omega\\to\\mathbb{R}$ denotes the heat kernel on $\\Omega=\\mathbb{R}^{d}$ or $\\Omega=R\\mathbb{T}^{d}$ , then there exists a dimensional constant $C=C(d)>0$ such that ", "page_idx": 13}, {"type": "equation", "text": "$$\n|\\nabla\\Gamma(t)\\star g|\\leq C\\frac{\\|g\\|_{\\infty}}{\\sqrt{t}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Furthermore, recall that constants are subject to change from line to line but always maintain a variable dependence as in the corresponding statement. ", "page_idx": 13}, {"type": "text", "text": "A.1 Theorem 3.1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The proof of Theorem 3.1 follows the strategy outlined in Section 4 and uses the different gradient estimates from the HJB equations provided in Section D.1. ", "page_idx": 13}, {"type": "text", "text": "Proof. Let $\\lambda=m^{1}-m^{2}$ , which satisfies ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\{\\partial_{t}\\lambda-\\Delta\\lambda-\\operatorname{div}(\\lambda b^{1}+m^{2}(b^{2}-b^{1}))=0\\mathrm{~in~}(0,T)\\times R\\mathbb{T}^{d},\\right.}\\\\ &{\\left.\\left(\\lambda(0)=m_{1}-m_{2}\\mathrm{~in~}R\\mathbb{T}^{d}.\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "For a fixed $\\psi:\\Omega\\rightarrow\\mathbb{R}$ , for which we will specify bounds latter, let $\\phi:[0,T]\\times\\Omega\\rightarrow\\mathbb{R}$ be given by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{-\\partial_{t}\\phi-\\Delta\\phi+b^{1}\\cdot\\nabla\\phi=0\\mathrm{~in}\\left[0,T\\right)\\times\\Omega,\\right.}\\\\ {\\left.\\phi(T,x)=\\psi(x)\\mathrm{~in~}\\Omega.\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Testing against $\\phi$ in (32) and using (33), we obtain ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\int\\lambda(T,x)\\psi(x)d x=\\int\\lambda(0)\\phi(0,x)d x-\\int_{0}^{T}\\int m^{2}(s)\\nabla\\phi(s)\\cdot(b^{2}-b^{1})(s)d x d s.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "\u2022 Proof of (8) and (9): Assume that $\\|\\psi\\|_{\\infty}\\leq1$ which then implies ", "page_idx": 13}, {"type": "equation", "text": "$$\n-1\\leq\\phi\\leq1.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Since $\\begin{array}{r}{\\int d\\lambda(t)=0}\\end{array}$ for all $t\\in[0,T]$ up to adding a constant we can assume without loss of generality in (34), that ", "page_idx": 13}, {"type": "equation", "text": "$$\n1\\leq\\psi\\leq3{\\mathrm{~and~}}1\\leq\\phi\\leq3.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then from estimate (63) in Corollary D.2, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(T-t)\\|\\nabla\\phi(t)\\|_{\\infty}^{2}\\leq C(T\\|\\nabla b\\|_{\\infty}+1)}\\\\ &{\\implies\\|\\nabla\\phi(t)\\|_{\\infty}\\leq C\\frac{\\sqrt{T\\|\\nabla b^{1}\\|_{\\infty}+1}}{\\sqrt{T-t}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Next, we note that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\ \\ \\int\\lambda(T,x)\\psi(x)d x=\\int\\lambda(0)\\phi(0,x)d x-\\int_{0}^{T}\\int m^{2}(s)\\nabla\\phi(s)\\cdot(b^{2}-b^{1})(s)d x d s}\\\\ &{\\leq{\\bf d}_{1}(m^{1},m^{2})\\|\\nabla\\phi(0)\\|_{\\infty}+\\int_{0}^{T}\\Big(\\int|\\nabla\\phi(s)|^{2}m^{2}(s)d x\\Big)^{\\frac{1}{2}}\\Big(\\int m^{2}(s)|b^{2}-b^{1}|^{2}(s)d x\\Big)^{\\frac{1}{2}}d s}\\\\ &{\\quad\\ \\ \\leq{\\bf d}_{1}(m^{1},m^{2})\\||\\nabla\\phi(0)|_{\\infty}+\\ \\underset{0\\leq t\\leq T}{\\operatorname*{sup}}\\|(b^{2}-b^{1})(t)\\|_{L^{2}(m^{2}(t))}\\int_{0}^{T}\\|\\nabla\\phi(s)\\|_{\\infty}d s,}\\\\ &{\\quad\\ \\ \\leq C(\\sqrt{T\\|\\nabla b^{1}\\|_{\\infty}}+1)\\Big(\\frac{{\\bf d}_{1}(m^{1},m^{2})}{\\sqrt{T}}+\\sqrt{T}\\underset{0\\leq t\\leq T}{\\operatorname*{sup}}\\|(b^{2}-b^{1})(t)\\|_{L^{2}(m^{2}(t))}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Taking the supremum over $\\|\\psi\\|_{\\infty}\\leq1$ yields ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{[\\|m^{2}(T)-m^{1}(T)\\|_{L^{1}(\\Omega)}\\leq C(\\sqrt{T}\\|\\nabla b^{1}\\|_{\\infty}+1)\\times}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left(\\displaystyle\\frac{\\mathbf{d}_{1}(m^{1},m^{2})}{\\sqrt{T}}+\\sqrt{T}\\operatorname*{sup}_{0\\leq t\\leq T}\\|(b^{2}-b^{1})(t)\\|_{L^{2}(m^{2}(t))}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Finally, if instead we used the bound ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\int\\lambda(0)\\phi(0,x)d x\\leq\\|\\lambda(0)\\|_{L^{1}(\\Omega)}\\|\\phi\\|_{\\infty}\\leq C\\|m_{1}-m_{2}\\|_{L^{1}(\\Omega)}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "we obtain (9). ", "page_idx": 14}, {"type": "text", "text": "\u2022 Proof of (10): Let $\\Omega=R\\mathbb{T}^{d}$ and consider a $\\psi$ with $\\|\\nabla\\psi\\|_{\\infty}\\leq1$ . Without loss of generality we can assume $\\psi(0)=R+1$ and thus ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\psi(x)=\\psi(x)-\\psi(0)+\\psi(0)\\geq-|x-0|+R+1\\geq-R+R+1=1\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and similarly ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\psi(x)\\leq2R+1.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore by maximum principle we have that ", "page_idx": 14}, {"type": "equation", "text": "$$\n1\\leq\\phi(t,x)\\leq2R+1,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and so ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|\\psi\\|_{C^{1}}=\\|\\psi\\|_{\\infty}+\\|\\nabla\\psi\\|_{\\infty}\\leq C(1+R).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "From (34) we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\int\\lambda(T,x)\\psi(x)d x\\leq\\mathbf{d}_{1}(m_{1},m_{2})\\|\\nabla\\phi(0)\\|_{\\infty}+\\operatorname*{sup}_{0\\leq t\\leq T}\\|\\nabla\\phi(t)\\|_{\\infty}\\|b^{1}-b^{1}\\|_{L^{2}(m^{2})}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus by Corollary D.2, estimate (62) we obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\int\\lambda(T,x)\\psi(x)d x\\leq C\\sqrt{(1+\\|\\nabla b\\|_{\\infty})\\|v\\|_{C^{1}}^{3}}(\\mathbf{d}_{1}(m_{1},m_{2})+\\|b^{1}-b^{1}\\|_{L^{2}(m^{2})})}}\\\\ &{}&{\\leq C R^{\\frac{3}{2}}(1+\\sqrt{\\|\\nabla b^{1}\\|_{\\infty}})\\Big(\\mathbf{d}_{1}(m_{1},m_{2})+\\|b^{1}-b^{1}\\|_{L^{2}(m^{2})}\\Big),~~~~~~~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which after taking the supremum over $\\|\\nabla\\psi\\|_{\\infty}\\leq1$ yields (10). ", "page_idx": 14}, {"type": "text", "text": "A.2 Theorem 3.2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The proof of Theorem 3.2 follows by an application of Theorem 3.1. ", "page_idx": 14}, {"type": "text", "text": "Proof. Let $\\begin{array}{r}{b^{1}=\\mathbf{s}_{\\theta},m_{1}=\\,\\frac{1}{\\mathrm{vol}(R\\mathbb{T}^{d})}}\\end{array}$ and $b^{2}=\\nabla\\log(\\eta^{\\pi}),m_{2}=\\eta^{\\pi}(T)$ . Note that if $m^{i},i=1,2$ solve ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\partial_{t}m^{i}-\\Delta m^{i}-\\mathrm{div}(m^{i}b^{i})=0\\mathrm{~in~}[0,T]\\times R\\mathbb{T}^{d},}\\\\ {m^{i}(0)=m_{i}\\mathrm{~in~}[0,T]\\times R\\mathbb{T}^{d},\\qquad\\qquad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "then $m^{1}=m_{g},m^{2}(t)=\\eta^{\\pi}(T-t)$ . Thus, by an application of Theorem 3.1 we obtain ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\boldmath~\\lambda~}),\\pi)\\leq C R^{\\frac{3}{2}}(1+\\sqrt{\\|\\nabla\\mathbf{s}_{\\theta}\\|_{\\infty}})({\\bf d}_{1}(\\frac{1}{\\mathrm{vol}(R\\mathbb{T}^{d})},\\eta^{\\pi})+\\|\\mathbf{s}_{\\theta}-\\nabla\\log(\\eta^{\\pi})\\|_{L^{2}(\\eta^{\\pi})})}\\\\ &{\\quad\\leq C R^{\\frac{3}{2}}(1+\\sqrt{\\|\\nabla\\mathbf{s}_{\\theta}\\|_{\\infty}})\\Big((R e^{-\\frac{\\omega T}{R^{2}}}{\\bf d}_{1}\\Big(\\pi,\\frac{1}{\\mathrm{vol}(R\\mathbb{T}^{d})}\\Big)+\\sqrt{e_{n n}}\\Big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where in the last inequality we used Proposition D.3. The second claim follows again by Theorem 3.1 and the fact that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{d}_{1}(\\nu_{1},\\nu_{2})=\\operatorname*{sup}_{\\|\\nabla g\\|_{\\infty}\\leq1}\\int_{R\\mathbb{T}^{d}}g d(\\nu_{1}-\\nu_{1})\\leq R\\|\\nu_{2}-\\nu_{1}\\|_{L^{1}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "A.3 Theorem 3.3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The proof of Theorem 3.3 follows the same strategy as in Theorems 3.1 and 3.2. The main technical difference is that we only have a bound between our generated score function $\\mathbf{s}_{\\theta}$ and the score function generated by the sample $\\nabla\\log(\\eta^{N,\\epsilon})$ of the form ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\int_{0}^{T}\\int|\\mathbf{s}_{\\theta}-\\nabla\\log(\\eta^{N,\\epsilon})|^{2}d\\eta^{N,\\epsilon}(s)d s\\leq e_{n n}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and so we need to produce a bound between $\\mathbf{s}_{\\theta}$ and the true score function $\\nabla\\log(\\eta^{\\epsilon})$ . This last step is shown in Section $\\mathbf{B}$ and in particular Theorem B.5. ", "page_idx": 15}, {"type": "text", "text": "Proof of Theorem 3.3. Consider the function $\\eta^{\\pi,\\epsilon}:[0,T]\\times R\\mathbb{T}^{d}\\to\\mathbb{R}$ given by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\{\\partial_{t}\\eta^{\\pi,\\epsilon}-\\Delta\\eta^{\\pi,\\epsilon}=0\\mathrm{~in~}(0,T)\\times R\\mathbb{T}^{d},\\right.}\\\\ &{\\left.\\left[\\eta^{\\pi,\\epsilon}(0)=\\pi^{\\epsilon}\\mathrm{~in~}R\\mathbb{T}^{d}.\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Define the drifts ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{b}_{\\theta^{*}}(t,x):=\\mathbf{s}_{\\theta^{*}}(T-t,x)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{b}^{\\pi^{\\epsilon}}(t,x):=\\nabla\\log(\\eta^{\\pi^{\\epsilon}})(T-t,x),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and let $m^{\\epsilon}(t,x)=\\eta^{\\pi,\\epsilon}(T-t,x)$ which satisfies ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\partial_{t}m^{\\epsilon}-\\Delta m^{\\epsilon}-\\mathrm{div}(m^{\\epsilon}\\mathbf{b}^{\\pi^{\\epsilon}}(t,x))=0,\\right.}\\\\ {m^{\\epsilon}=\\eta^{\\pi,\\epsilon}(T,x).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Finally, we consider the distribution of our generated sample $m_{g}:[0,T]\\times\\Omega\\rightarrow\\mathbb{R}$ which is given by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\partial_{t}m_{g}-\\Delta m_{g}-\\mathrm{div}(m_{g}\\mathbf{b}_{\\theta^{*}})=0\\;\\mathrm{in}\\;(0,T]\\times\\Omega,\\;}\\\\ {m_{g}(0)=\\frac{1}{\\mathrm{vol}(R\\mathbb{T}^{d})}\\;\\mathrm{in}\\;\\Omega.\\;\\;}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We have the following ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{d}_{1}(\\pi,m_{g}(T))\\leq\\Big[\\mathbf{d}_{1}(\\pi,\\pi^{\\epsilon})\\Big]_{1}+\\Big[\\mathbf{d}_{1}(\\pi^{\\epsilon},m_{g}(T))\\Big]_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We will bound each of the terms separately. ", "page_idx": 15}, {"type": "text", "text": "We recall that $\\pi^{\\epsilon}=\\Gamma(\\epsilon)\\star\\pi$ where $\\Gamma$ is the heat kernel. Therefore, by the dual formulation of ${\\bf d}_{1}$ we have that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{d}_{1}(\\pi,\\pi^{\\epsilon})=\\operatorname*{sup}_{\\|\\nabla g\\|_{\\infty}\\leq1}\\int g(x)d(\\pi^{\\epsilon}-\\pi)=\\operatorname*{sup}_{\\|\\nabla g\\|_{\\infty}\\leq1}\\int(\\Gamma(\\epsilon)\\star g(x)-g(x))d\\pi(x).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Although estimates on $(\\Gamma(\\epsilon)\\star g(x)-g(x))$ for a function $\\|\\nabla g\\|_{\\infty}\\leq1$ are classical for the readers convenience we provide a quick proof. Let $v(t)=\\Gamma(t)\\star g$ , which solves ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{{\\partial_{t}v-\\Delta v=0},\\right.}\\\\ {\\left.v(0)=g.\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then for all $x\\in\\Omega$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n|v(\\epsilon,x)-v(0,x)|\\leq\\int_{0}^{\\epsilon}|\\partial_{t}v(s,x)|d s=\\int_{0}^{\\epsilon}|\\Delta v(s,x)|d s\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n=\\int_{0}^{\\epsilon}|(\\nabla\\Gamma(s)\\otimes\\star\\nabla g)(x)|d s\\leq C\\|\\nabla g\\|_{\\infty}\\int_{0}^{\\epsilon}\\frac{1}{\\sqrt{s}}d s\\leq C\\sqrt{\\epsilon}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Where in the above we used estimate (31). Thus, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{d}_{1}(\\pi,\\pi^{\\epsilon})\\leq C\\sqrt{\\epsilon},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $C=C(d)>0$ . ", "page_idx": 15}, {"type": "text", "text": "2. First we note that $\\pi^{\\epsilon}=m^{\\epsilon}(T)$ . Thus applying Theorem 3.1 for ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{m^{1}=m_{g},b^{1}=\\mathbf{b}_{\\theta},}\\\\ {m^{2}=m^{\\epsilon},b^{2}=\\mathbf{b}^{\\pi^{\\epsilon}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "yields ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{d}_{1}\\big(m_{g}(T),\\pi^{\\epsilon}\\big)=\\mathbf{d}_{1}\\big(m_{g}(T),m^{\\epsilon}(T)\\big)}\\\\ &{\\qquad\\qquad\\qquad\\lesssim R^{\\frac{3}{2}}(1+\\sqrt{\\|\\nabla b^{1}\\|_{\\infty}})(\\mathbf{d}_{1}(m^{\\epsilon}(0),\\frac{1}{\\mathrm{vol}(R\\mathbb{T}^{d})})+\\sqrt{T}\\|\\mathbf{b}^{\\pi^{\\epsilon}}-\\mathbf{b}_{\\theta^{*}}\\|_{L^{2}(m^{\\epsilon})}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By Proposition D.3, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf d_{1}(m^{\\epsilon}(0),\\frac{1}{\\mathrm{vol}(R\\mathbb{T}^{d})})=\\mathbf d_{1}(\\eta^{\\pi,\\epsilon}(T),\\frac{1}{\\mathrm{vol}(R\\mathbb{T}^{d})})\\le\\!C R e^{-\\frac{\\omega t}{R^{2}}}\\mathbf d_{1}(\\pi^{\\epsilon},\\frac{1}{\\mathrm{vol}(R\\mathbb{T}^{d})})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\le\\!C R^{2}e^{-\\frac{\\omega t}{R^{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Finally, from Theorem B.5 ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\|\\mathbf{b}^{\\pi^{\\epsilon}}-\\mathbf{b}_{\\theta^{*}}\\|_{L^{2}(m^{\\epsilon})}^{2}=\\mathcal{J}(\\eta^{\\pi^{\\epsilon}},\\theta^{*})\\leq e_{n n}^{\\prime}}\\\\ {\\displaystyle=e_{n n}+C\\Big(1+\\frac{|\\log(\\delta)|}{\\sqrt{\\epsilon}}+\\frac{1}{\\sqrt{T}}+T\\|\\mathbf{s}_{\\theta}\\|_{C^{2}([0,T]\\times\\Omega)}^{2}\\Big)\\mathbf{d}_{1}(\\pi^{N},\\pi)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, if $T\\geq1$ , putting everything together, we have shown that up to a dimensional constant ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{1}_{1}(\\pi,m_{g}(T))\\leq C\\Bigg(\\sqrt{\\epsilon}+R^{\\frac{3}{2}}\\left(1+\\sqrt{\\|\\nabla b^{1}\\|_{\\infty}}\\right)\\times}\\\\ &{\\qquad\\qquad\\quad\\Bigg[R^{2}e^{-\\frac{\\omega^{T}}{R^{2}}}+\\sqrt{T\\left(e_{n n}+C\\left(1+\\frac{\\displaystyle\\left\\lfloor\\log(\\delta)\\right\\rfloor}{\\displaystyle\\sqrt{\\epsilon}}+T\\|\\mathbf{s}_{\\theta}\\|_{C^{2}([0,T]\\times\\Omega)}^{2}\\right)\\mathbf{d}_{1}(\\pi^{N},\\pi)\\right)}\\Bigg]\\Bigg)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B Analysis of Score Matching functionals ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we gather all the technical results regarding the connections between ISM, DSM, and ESM score matching functionals. For the readers convenience we recall some of our notations. Let $\\Omega\\subset\\mathbb{R}^{d}$ and $m_{0}\\in\\bar{\\mathcal{P}}(\\Omega)$ . We will denote by $\\rho^{m_{0}}:[0,T]\\times\\Omega\\rightarrow[0,\\infty)$ the solution of ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\partial_{t}\\rho^{m_{0}}-\\Delta\\rho^{m_{0}}=0\\mathrm{~in~}(0,T]\\times\\Omega,\\right.}\\\\ {\\left.\\rho^{m_{0}}(0)=m_{0}\\mathrm{~in~}\\Omega,\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and we may drop the superscipt $m_{0}$ when it is clear from context. ", "page_idx": 16}, {"type": "text", "text": "First we state a simple result that justifies the formula ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{I}(\\rho^{m_{0}},\\theta)=\\mathcal{I}_{L}(\\rho^{m_{0}},\\theta)+4\\|\\nabla\\sqrt{\\rho^{m_{0}}}\\|_{2}^{2},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which formally follows by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathcal{I}(\\rho,\\boldsymbol{\\theta})=\\int_{0}^{T}\\int\\Big(|\\mathbf{s}_{\\boldsymbol{\\theta}}|^{2}d\\rho(s)-2\\mathbf{s}_{\\boldsymbol{\\theta}}\\cdot\\nabla\\log(\\rho)+|\\nabla\\log(\\rho)|^{2}\\Big)d\\rho(s)\\mathrm{d}s}\\\\ {\\displaystyle=\\mathcal{I}_{L}(\\rho,\\boldsymbol{\\theta})+\\int_{0}^{T}\\int\\frac{|\\nabla\\rho|^{2}}{\\rho}d x d s}\\\\ {\\displaystyle=\\mathcal{I}_{L}(\\rho)+4\\|\\nabla\\sqrt{\\rho}\\|_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "However $\\|\\nabla\\sqrt{\\rho^{m_{0}}}\\|_{2}$ may not be finite, thus we record the exact statement along with some technical facts for later use in the following Proposition. ", "page_idx": 16}, {"type": "text", "text": "Proposition B.1. Let $m_{0}$ be a probability density in $\\Omega$ such that $m_{0}(x)\\log(m_{0}(x))\\in L^{1}(\\Omega)$ and $\\rho:[0,T]\\times\\Omega\\rightarrow\\mathbb{R}$ be given by (41). Then, the following hold: ", "page_idx": 17}, {"type": "text", "text": "1. There exists a universal constant $C>0$ , such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t\\in[0,T]}\\|\\rho(t)\\log(\\rho(t))\\|_{L^{1}(\\Omega)}+\\|\\nabla\\sqrt{\\rho}\\|_{L^{2}([0,T]\\times\\Omega)}^{2}\\leq C T\\|m_{0}\\log(m_{0})\\|_{L^{1}(\\Omega)},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and thus (42) holds. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{2.\\ 4\\|\\nabla\\sqrt{\\rho}\\|_{2}^{2}=\\int_{\\Omega}m_{0}\\log(m_{0})-\\rho(T)\\log(\\rho(T))d x.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Testing (41) against $f^{\\prime}(\\rho)$ for a smooth function $f$ , we obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\int f(\\rho(T,x))d x+\\int_{0}^{T}\\int f^{\\prime\\prime}(\\rho)|\\nabla\\rho|^{2}d x d t=\\int f(m_{0}(x))d x\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and the choice of $f(x)=x\\log(x)$ yields ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\int\\rho(T,x)\\log(\\rho(T,x))d x+\\int_{0}^{T}\\int\\frac{|\\nabla\\rho|^{2}}{\\rho}d x d t=\\int m_{0}(x)\\log(m_{0}(x))d x,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which shows the second claim. Finally, if $m_{0}\\log(m_{0})\\;\\;\\in\\;\\;L^{1}$ this yields estimates on   \nsup $\\|\\rho(t)\\log(\\rho(t))\\|_{1},\\|\\nabla\\sqrt{\\rho}\\|_{2}^{2}$ since   \n$t\\!\\in\\![0,T]$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int|\\rho(T,x)\\log(\\rho(T,x))|d x=\\int\\rho(T,x)\\log(\\rho(T,x))d x+2\\int(\\rho(T,x)\\log(\\rho(T,x)))^{-}d x}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\displaystyle\\int\\rho(T,x)\\log(\\rho(T,x))d x+2e^{-1}|\\Omega|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we used the fact that $x\\log(x)\\geq-{\\frac{1}{e}}$ . ", "page_idx": 17}, {"type": "text", "text": "The rest of the subsection justifies the steps outlined in Subsection 5.2. Namely starting from a estimate of the form ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{I}(\\eta^{N,\\epsilon},\\theta)\\le e_{n n}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "our goal is to obtain an estimate of the form ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{I}(\\eta^{\\epsilon},\\theta)\\leq e_{n n}^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "For this we use formula (42). In particular we first use the linear dependence of $\\mathcal{I}_{L}$ with respect to the underlying measure to bound ", "page_idx": 17}, {"type": "equation", "text": "$$\n|{\\mathcal{T}}_{L}(\\eta^{N,\\epsilon},\\theta)-{\\mathcal{T}}_{L}(\\eta^{\\epsilon})|.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, instead of comparing the gradients on the difference ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\nabla\\log(\\eta^{N,\\epsilon})\\|_{2}^{2}-\\|\\nabla\\log(\\eta^{\\epsilon})\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "we use Proposition B.1 from which it is enough to bound the entropy terms. The next two Propositions provide these results along with some other technical facts. ", "page_idx": 17}, {"type": "text", "text": "Proposition B.2. Let $T>0$ and $\\pi^{i}$ for $i=1,2$ denote two probability measures in $\\Omega$ such that $\\|\\pi^{i}\\,\\bar{\\log}(\\pi^{i})\\|_{L^{1}}<\\infty$ and $\\rho^{i}$ the corresponding solutions to (41). Then, there exists a dimensional constant $C>0$ such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\mathcal{I}_{L}(\\rho^{2},\\theta)-\\mathcal{I}_{L}(\\rho^{1},\\theta)|\\leq C T\\underset{t\\in[0,T]}{\\operatorname*{sup}}\\ \\mathbf{d}_{1}(\\rho^{2}(t),\\rho^{1}(t))\\|\\mathbf{s}_{\\theta}\\|_{C^{2}([0,T]\\times\\Omega)}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq C T\\mathbf{d}(\\pi^{1},\\pi^{2})\\|\\mathbf{s}_{\\theta}\\|_{C^{2}([0,T]\\times\\Omega)}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In the above $\\mathbf{s}_{\\theta}$ indicates the score function approximation. ", "page_idx": 17}, {"type": "text", "text": "Proof. We recall that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{I}_{L}(\\rho^{i},\\theta)=\\int_{0}^{T}\\mathbb{E}_{\\rho^{i}}\\Big[\\frac{1}{2}\\vert\\mathbf{s}_{\\theta}\\vert^{2}+\\mathrm{div}(\\mathbf{s}_{\\theta})\\Big]d s=\\int_{0}^{T}\\int\\frac{1}{2}\\vert\\mathbf{s}_{\\theta}\\vert^{2}+\\mathrm{div}(\\mathbf{s}_{\\theta})d\\rho^{i}(s)d s.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, if ", "page_idx": 18}, {"type": "equation", "text": "$$\ng_{\\theta}=\\frac{1}{2}|\\mathbf s_{\\theta}|^{2}+\\mathrm{div}(\\mathbf s_{\\theta})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n|\\mathcal{I}_{L}(\\rho^{2},\\mathbf{s}_{\\theta})-\\mathcal{I}_{L}(\\rho^{1},\\mathbf{s}_{\\theta})|=\\Big|\\int_{0}^{T}\\int\\frac{1}{2}|\\mathbf{s}_{\\theta}|^{2}+\\mathrm{div}(\\mathbf{s}_{\\theta})d(\\rho^{2}(s)-\\rho^{1}(s))d s\\Big|\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n=\\Big|\\int_{0}^{T}\\int g_{\\theta}d(\\rho^{2}-\\rho^{1})(s)d s\\Big|\\leq\\operatorname*{sup}_{0\\leq t\\leq T}\\|\\nabla g_{\\theta}\\|_{\\infty}\\int_{0}^{T}\\mathbf{d}_{1}(\\rho^{2}(s),\\rho^{1}(s))d s\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\leq T\\operatorname*{sup}_{0\\leq t\\leq T}\\|\\nabla g_{\\theta}\\|_{\\infty}\\operatorname*{sup}_{0\\leq t\\leq T}\\mathbf{d}_{1}(\\rho^{2}(t),\\rho^{1}(t)).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\nabla g_{\\boldsymbol{\\theta}}\\|_{\\infty}\\leq\\|\\mathbf{s}_{\\boldsymbol{\\theta}}\\|_{C^{2}}^{2}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "to conclude we need to show that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{0\\leq t\\leq T}\\mathbf{d}_{1}(\\rho^{2}(t),\\rho^{1}(t))\\leq\\mathbf{d}_{1}(\\pi^{2},\\pi^{1}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "However this follows from our Lemma 3.1 applied for $b^{1}=b^{2}=\\vec{0}$ and $m^{i}=\\pi^{i}$ for $i=1,2$ . ", "page_idx": 18}, {"type": "text", "text": "Lemma B.3. Let $\\pi^{\\epsilon}$ , $\\hat{\\pi}^{N,\\epsilon}$ , $0<\\delta<\\epsilon$ be as in Theorem 3.3 with $\\epsilon<1$ . There exists a dimensional constant $C=C(d)>0$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{d}_{1}(\\hat{\\pi}^{N,\\epsilon},\\pi^{\\epsilon})\\le\\mathbf{d}_{1}(\\pi^{N},\\pi)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\pi^{\\epsilon}-\\hat{\\pi}^{N,\\epsilon}\\|_{L^{1}(\\Omega)}\\leq C\\frac{\\mathbf{d}_{1}(\\pi^{N},\\pi)}{\\sqrt{\\epsilon}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\log(\\pi^{\\epsilon})\\pi^{\\epsilon}-\\log(\\hat{\\pi}^{N,\\epsilon})\\hat{\\pi}^{N,\\epsilon}\\|_{L^{1}(\\Omega)}\\leq C(1+|\\log(\\delta)|)\\frac{\\mathbf{d}_{1}(\\pi^{N},\\pi)}{\\sqrt{\\epsilon}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Moreover, let $C>0$ denote the constant from Proposition $D.3$ and $T\\geq R^{2}$ large enough such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n2C R^{-d}e^{-\\frac{\\omega T}{R^{2}}}\\leq\\frac{1}{2}\\frac{1}{\\nu o l(R\\mathbb{T}^{d})},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "then up to a dimensional constant $C>0$ we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\int\\log(\\eta^{N,\\epsilon}(T))\\eta^{N,\\epsilon}(T)-\\log\\eta(T))\\eta(T)d x\\leq\\frac{C}{\\sqrt{T}}\\Big(1+d\\log(R)\\Big){\\bf d}_{1}(\\pi,\\hat{\\pi}^{N}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Let $\\rho^{N,\\epsilon},\\rho^{\\epsilon}:[0,T]\\times\\Omega\\rightarrow\\mathbb{R}$ , be solutions to ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\partial_{t}\\rho-\\Delta\\rho=0\\,\\mathrm{in}\\,\\Omega\\times(0,\\epsilon),\\right.}\\\\ {\\left.\\rho(0)=\\rho_{0},\\right.\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for $\\rho_{0}=\\pi,\\rho_{0}=\\pi^{N}$ respectively. Since $\\pi^{\\epsilon}=\\Gamma(\\epsilon)\\!\\star\\!\\pi,\\hat{\\pi}^{N,\\epsilon}=\\Gamma(\\epsilon)\\!\\star\\!\\pi^{N}$ we note that $\\pi^{\\epsilon}=\\rho^{\\epsilon}(\\epsilon,x)$ and $\\hat{\\pi}^{N,\\epsilon}=\\rho^{\\epsilon,N}(\\epsilon,x)$ . Using the same adjoint method as in the proof of Theorem 3.1 for a function $\\psi:\\Omega\\rightarrow\\mathbb{R}$ we consider the function $\\phi:[0,\\epsilon]\\times\\Omega\\rightarrow\\mathbb{R}$ given by ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{-\\partial_{t}\\phi-\\Delta\\phi=0}\\\\ {\\phi(\\epsilon)=\\psi.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Testing against $\\phi$ in the equation for $\\lambda=\\rho^{\\epsilon}-\\rho^{\\epsilon,N}$ yields ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\int(\\hat{\\pi}^{N,\\epsilon}-\\pi^{\\epsilon})\\psi(x)d x=\\int\\phi(0,x)d(\\pi-\\pi^{N})(x).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "First we consider $\\|\\psi\\|_{\\infty}\\leq1$ which from (31), yields ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\nabla\\phi(0)\\|_{\\infty}\\leq\\frac{\\|\\psi\\|_{\\infty}}{\\sqrt{\\epsilon}}\\leq\\frac{C}{\\sqrt{\\epsilon}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, for any $\\|\\psi\\|_{\\infty}\\leq1$ we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\int(\\hat{\\pi}^{N,\\epsilon}-\\pi^{\\epsilon})\\psi(x)d x\\leq\\frac{C\\mathbf{d}_{1}(\\pi,\\pi^{N})}{\\sqrt{\\epsilon}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which implies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\hat{\\pi}^{N,\\epsilon}-\\pi^{\\epsilon}\\|_{L^{1}(\\omega)}\\leq\\frac{C\\mathbf{d}_{1}(\\pi,\\pi^{N})}{\\sqrt{\\epsilon}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Next, by considering $\\mathrm{Lip}(\\psi)\\leq1$ which implies ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\nabla\\phi(0)\\|_{\\infty}\\leq1\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\bf d}_{1}\\big(\\hat{\\pi}^{N,\\epsilon},\\pi^{\\epsilon}\\big)\\leq{\\bf d}_{1}\\big(\\pi,\\pi^{N}\\big).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For estimate (45) we note that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lvert\\hat{\\pi}^{N,\\epsilon}\\log(\\hat{\\pi}^{N,\\epsilon})-\\pi^{\\epsilon}\\log(\\pi^{\\epsilon})\\rvert=\\left\\lvert\\int_{0}^{1}(1+\\log(s\\hat{\\pi}^{N,\\epsilon}+(1-s)\\pi^{\\epsilon}))d s(\\hat{\\pi}^{N,\\epsilon}-\\pi)\\right\\rvert}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq(1+\\lvert\\log(\\delta)\\rvert)\\lvert\\hat{\\pi}^{N,\\epsilon}-\\pi^{\\epsilon}\\rvert.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, ", "text_level": 1, "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\widehat{\\pi}^{N,\\epsilon}\\log(\\widehat{\\pi}^{N,\\epsilon})-\\pi^{\\epsilon}\\log(\\pi^{\\epsilon})\\|_{L^{1}(\\Omega)}\\leq(1+|\\log(\\delta)|)\\|\\widehat{\\pi}^{N,\\epsilon}-\\pi^{\\epsilon}\\|_{L^{1}}\\leq C(1+|\\log(\\delta)|)\\frac{\\mathbf{d}_{1}(\\pi^{N},\\pi)}{\\sqrt{\\epsilon}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "For estimate (46) we note that for any convex function $h$ ", "page_idx": 19}, {"type": "equation", "text": "$$\nh(x)-h(y)\\geq\\nabla h(y)\\cdot(x-y)\\implies h(y)-h(x)\\leq\\nabla h(y)\\cdot(y-x).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence by applying the above to $h(x)=x\\log(x)$ we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\int\\log(\\eta^{N,\\epsilon}(T))\\eta^{N,\\epsilon}(T)-\\log\\eta(T))\\eta(T)d x\\leq\\int(1+\\log(\\eta^{N,\\epsilon}(T)))d(\\eta^{N,\\epsilon}(T)-\\eta^{\\epsilon}(T))\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\leq\\|1+\\log(\\eta^{N,\\epsilon}(T))\\|_{\\infty}\\|\\eta^{N,\\epsilon}(T)-\\eta^{\\epsilon}(T)\\|_{L^{1}}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We now note that for any $g:R\\mathbb{T}^{d}\\rightarrow\\mathbb{R},\\|g\\|_{\\infty}\\leq1$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\int g(x)d(\\eta^{N,\\epsilon}-\\eta^{\\epsilon})(T)=\\int\\Gamma(T)\\star g d(\\hat{\\pi}^{N,\\epsilon}-\\pi^{\\epsilon})\\le\\|\\nabla\\Gamma(T)\\star g\\|_{\\infty}\\mathbf{d}_{1}(\\hat{\\pi}^{N,\\epsilon},\\pi^{\\epsilon})}}\\\\ &{}&{\\le\\displaystyle\\frac{C}{\\sqrt{T}}\\mathbf{d}_{1}(\\hat{\\pi}^{N,\\epsilon},\\pi^{\\epsilon}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Taking the supremum yields ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\eta^{N,\\epsilon}(T)-\\eta^{\\epsilon}(T)\\|_{L^{1}}\\leq\\frac{C}{\\sqrt{T}}\\mathbf{d}_{1}(\\hat{\\pi}^{N,\\epsilon},\\pi^{\\epsilon})\\leq\\mathbf{d}_{1}(\\hat{\\pi}^{N,\\epsilon},\\pi^{\\epsilon}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Finally, from Proposition D.3 above applied to $\\begin{array}{r}{\\rho=\\eta^{N,\\epsilon}-\\frac{1}{\\mathrm{vol}(R\\mathbb{T}^{d})}}\\end{array}$ vol(R1Td) we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\eta^{N,\\epsilon}(T,x)\\geq-\\|\\rho(T)\\|_{\\infty}+\\frac{1}{\\mathrm{vol}(R\\mathbb{T}^{d})}\\geq\\frac{1}{2}\\frac{1}{\\mathrm{vol}(R\\mathbb{T}^{d})}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "as well as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\eta^{N,\\epsilon}(T,x)\\leq\\frac{1}{\\mathrm{vol}(R\\mathbb{T}^{d})}+\\|\\rho(T)\\|_{\\infty}\\leq2\\frac{1}{\\mathrm{vol}(R\\mathbb{T}^{d})}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "by our assumptions on $T>0$ . Therefore, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|1+\\log(\\eta^{N,\\epsilon}(T))\\|_{\\infty}\\leq C(1+d\\log(R))\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and the result follows. ", "page_idx": 19}, {"type": "text", "text": "Remark B.4. It is worth noting that the empirical sample $\\pi^{N}$ could have been regularized by any smooth kernel $\\rho$ . However the choice of the heat kernel which provides the same result as early stopping behaves quite nicely with respect to the metric ${\\bf d}_{1}$ since it is a contraction. This is evident in Lemma B.3 above where we see that we do not pay any additional cost due to the mollification in (43). ", "page_idx": 20}, {"type": "text", "text": "In fact the above techniques show the following general result regarding ESM to DSM bounds.   \nCombining the above we can state the following general result, about ESM to DSM bounds. ", "page_idx": 20}, {"type": "text", "text": "Theorem B.5. Let $\\tau^{1},\\pi^{2}\\in\\mathscr{P}(\\Omega)$ denote two probability densities, such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Vert\\pi^{i}\\log(\\pi^{i})\\Vert_{L^{1}(\\Omega)}<\\infty f o r\\,i=1,2\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and $\\delta>0$ such that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\pi^{i}(x)\\geq\\delta.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Define $\\rho^{i}:[0,T]\\times\\Omega\\rightarrow\\mathbb{R}$ as the solutions to ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\{\\partial_{t}\\rho^{i}-\\Delta\\rho^{i}=0\\,i n\\left(0,T\\right]\\times\\Omega,\\right.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then, up to a dimensional constant $C=C(d)>0$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Big|\\mathcal{I}(\\rho^{2},\\theta)-\\mathcal{I}(\\rho^{1},\\theta)\\Big|\\leq C\\Big(T\\|\\mathbf{s}_{\\theta}\\|_{C^{2}}^{2}\\mathbf{d}_{1}(\\pi^{1},\\pi^{2})+(1+|\\log(\\delta)|)\\|\\pi^{2}-\\pi^{1}\\|_{L^{1}}\\Big).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "B.1 Proof of Theorem 3.3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Now we can prove Theorem 3.3. ", "page_idx": 20}, {"type": "text", "text": "Proof. From Proposition B.1 we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\qquad\\qquad\\mathcal{I}(\\eta^{\\epsilon},\\mathbf{s}_{\\theta^{*}})=\\mathcal{I}_{L}(\\eta^{\\epsilon},\\mathbf{s}_{\\theta^{*}})+2\\|\\nabla\\sqrt{\\eta^{\\epsilon}}\\|_{2}^{2}}\\\\ &{=\\mathcal{I}(\\eta^{N,\\epsilon},\\mathbf{s}_{\\theta^{*}})+2\\Big(\\|\\nabla\\sqrt{\\eta^{\\epsilon}}\\|_{2}^{2}-\\|\\nabla\\sqrt{\\eta^{N,\\epsilon}}\\|_{2}^{2}\\|\\Big)+\\Big(\\mathcal{I}_{L}(\\eta^{\\epsilon},\\mathbf{s}_{\\theta^{*}})-\\mathcal{I}_{L}(\\eta^{N,\\epsilon},\\mathbf{s}_{\\theta^{*}})\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By assumption we have the bound ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{I}(\\eta^{N,\\epsilon},\\mathbf{s}_{\\theta^{*}})\\leq e_{n n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By Proposition B.1 ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle2\\Big(\\|\\nabla\\sqrt{\\eta^{\\varepsilon}}\\|_{2}^{2}-\\|\\nabla\\sqrt{\\eta^{N,\\epsilon}}\\|_{2}^{2}\\Big)}\\\\ {\\displaystyle=\\int_{\\Omega}\\pi^{\\epsilon}(x)\\log(\\pi^{\\epsilon}(x))-\\hat{\\pi}^{N,\\epsilon}(x)\\log(\\hat{\\pi}^{N,\\epsilon}(x))d x-\\int_{\\Omega}\\eta^{\\epsilon}(T)\\log(\\eta^{\\epsilon}(T))-\\eta^{N,\\epsilon}(T)\\log(\\eta^{N,\\epsilon}(T))d x.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "From Lemma B.3 we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|\\pi^{\\epsilon}\\log(\\pi^{\\epsilon})-\\hat{\\pi}^{N,\\epsilon}\\log(\\hat{\\pi}^{N,\\epsilon})\\|_{L^{1}(\\Omega)}\\leq C(1+|\\log(\\delta)|)\\frac{\\mathbf{d}_{1}(\\pi^{N},\\pi)}{\\sqrt{\\epsilon}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Using the same argument and the fact that from maximum principle $\\eta^{\\epsilon}(T),\\eta^{N,\\epsilon}\\geq\\delta>0$ , we can show ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\|\\eta(T)\\log(\\eta(T))-\\eta^{N,\\epsilon}\\log(\\eta^{N,\\epsilon}(T))\\|_{L^{1}(\\Omega)}\\leq(1+|\\log(\\delta)|)\\|\\eta^{\\epsilon}(T)-\\eta^{N,\\epsilon}(T)\\|_{L^{1}(\\Omega)}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\displaystyle\\frac{C}{\\sqrt{T}}\\mathbf{d}_{1}(\\pi^{\\epsilon},\\hat{\\pi}^{N,\\epsilon})\\leq\\displaystyle\\frac{C}{\\sqrt{T}}\\mathbf{d}_{1}(\\pi,\\pi^{N}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Furthermore, by Proposition B.2 we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{I}_{L}(\\eta^{\\epsilon},\\mathbf{s}_{\\theta^{*}})\\!-\\!\\mathcal{I}_{L}(\\eta^{N,\\epsilon},\\mathbf{s}_{\\theta^{*}})\\Big|\\leq C T\\mathbf{d}_{1}(\\hat{\\pi}^{N,\\epsilon},\\pi^{\\epsilon})\\|\\mathbf{s}_{\\theta}\\|_{C^{2}([0,T]\\times\\Omega)}^{2}\\leq C T\\mathbf{d}_{1}(\\pi^{N},\\pi)\\|\\mathbf{s}_{\\theta}\\|_{C^{2}([0,T]\\times\\Omega)}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Putting everything together we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{I}(\\eta,\\mathbf{s}_{\\theta^{*}})\\leq e_{n n}+C\\left(1+\\frac{|\\log(\\delta)|}{\\sqrt{\\epsilon}}+\\frac{1}{\\sqrt{T}}+T\\|\\mathbf{s}_{\\theta}\\|_{C^{2}([0,T]\\times\\Omega)}^{2}\\right)\\mathbf{d}_{1}(\\pi^{N},\\pi).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "As an immediate Corollary we have the following. ", "page_idx": 21}, {"type": "text", "text": "Corollary B.6. Using the same notation as in Section 3.5, if $e_{n n}>0$ is such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n0\\le\\mathcal{I}(\\eta^{N,\\epsilon},\\theta^{*})<e_{n n}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "then ", "page_idx": 21}, {"type": "equation", "text": "$$\n0\\leq\\mathcal{I}(\\eta^{\\pi^{\\epsilon}},\\theta^{*})<e_{n n}^{\\prime},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where for a dimensional constant $C=C(d)>0$ ", "page_idx": 21}, {"type": "equation", "text": "$$\ne_{n n}^{\\prime}=e_{n n}+C\\Big(1+\\frac{|\\log(\\delta)|}{\\sqrt{\\epsilon}}+\\frac{1}{\\sqrt{T}}+T\\|\\mathbf{s}_{\\theta}\\|_{C^{2}([0,T]\\times\\Omega)}^{2}\\Big)\\mathbf{d}_{1}\\big(\\pi^{N},\\pi\\big).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Remark B.7 (On regularity implications of NN approximation). It is important to note that the norm of our NN approximation $\\|\\nabla\\mathbf{s}_{\\theta}\\|_{\\infty}$ is in fact dependent on $\\epsilon>0$ as we can see from the following ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\lVert\\mathbf{s}_{\\theta}\\rVert_{C^{1}}\\gtrsim\\lVert\\mathbf{s}_{\\theta}\\rVert_{L^{2}(\\eta^{N,\\epsilon})}\\gtrsim\\lVert\\nabla\\log(\\eta^{N,\\epsilon})\\rVert_{L^{2}(\\eta^{N,\\epsilon})}-\\sqrt{e_{n n}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and $\\|\\nabla\\log(\\eta^{N,\\epsilon})\\|_{L^{2}(\\eta^{N,\\epsilon})}$ blows up as $\\epsilon\\to0$ . ", "page_idx": 21}, {"type": "text", "text": "We finish this subsection of technical results with an observation about the implications on the existence of a smooth NN-approximation. ", "page_idx": 21}, {"type": "text", "text": "Proposition B.8 (ESM implies regularity). Let $\\pi,\\mathbf{s}_{\\theta}$ and $e_{n n}>0$ be as above for $\\Omega=R\\mathbb{T}^{d}$ . Assume moreover, that $\\|\\nabla\\mathbf{s}_{\\theta}\\|_{\\infty}<\\infty$ . Then, $\\pi=\\pi(x)$ admits a density and in fact $\\|\\pi\\log(\\pi)\\|_{L^{1}(\\Omega)}<\\infty$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. Since $\\|\\nabla\\mathbf{s}_{\\theta}\\|_{\\infty}<\\infty$ , for some constant $C=C(R,d,e_{n n})>0$ it holds ", "page_idx": 21}, {"type": "equation", "text": "$$\n2\\|\\nabla\\sqrt{\\eta^{\\pi}}\\|_{L^{2}}^{2}=\\int_{0}^{T}\\int|\\nabla\\log(\\eta^{\\pi})|^{2}d\\eta^{\\pi}(s,x)d s\\leq C.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Moreover, by the regularizing properties of the diffusion we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\int\\eta^{\\pi}(T)\\log(\\eta^{\\pi}(T))d x<\\infty\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and the result follows from Proposition B.1. ", "page_idx": 21}, {"type": "text", "text": "C Average DSM generalization ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Using the previous results we can in fact show a result about the average error in Theorem B.5. The main observation here is that by Jensen\u2019s inequality when we take expectation with respect to the sample $\\pi^{N}$ we no longer require a lower bound on our densities. However, as we will see we require a very restrictive assumption on the norm of our score function approximation ", "page_idx": 21}, {"type": "text", "text": "Theorem C.1. (Average DSM generalization) $\\mathcal{L}e t\\,e_{n n},A>0$ and assume that for each sample $\\hat{\\pi}^{N}$ from \u03c0 there exists a $\\mathbf{s}_{\\theta^{*}}$ such that ", "page_idx": 21}, {"type": "equation", "text": "$$\nJ(\\eta^{N,\\epsilon},\\theta^{*})\\leq e_{n n},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "with ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\mathbf{s}_{\\theta^{*}}\\|_{C^{2}}\\leq A.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Let $m_{g}(T)$ be the generated distribution from $\\hat{\\pi}^{N}$ (which is also random since $\\hat{\\pi}^{N}$ is random). Let $C>0$ be the dimensional constant appearing in Proposition $D.3$ and $T\\geq R^{2}$ large enough such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n2C R^{-d}e^{-\\frac{\\omega T}{R^{2}}}\\leq\\frac{1}{2}\\frac{1}{\\nu o l(R\\mathbb{T}^{d})},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "then ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{E}\\Big[\\mathbf{d}_{1}\\big(\\pi,m_{g}(T)\\big)\\Big]\\leq C R^{\\frac{3}{2}}(1+\\sqrt{A})(R^{2}e^{-\\frac{\\omega T}{R^{2}}}+\\sqrt{T e_{n n}^{\\prime}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where ", "page_idx": 21}, {"type": "equation", "text": "$$\ne_{n n}^{\\prime}=\\epsilon+e_{n n}+\\frac{C}{N^{\\frac{1}{2d}}}\\Big(T R A^{2}+\\frac{1}{\\sqrt{T}}(1+d\\log(R))\\Big).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Remark C.2. It is important to emphasize in the above that we are assuming that we may pick both an error $e_{n n}>0$ and $a$ bound $A>0$ on the gradient of s\u03b8 globally for all random samples $\\hat{\\pi}^{N}$ . Moreover, $\\mathbf{s}_{\\theta}$ is a weak approximation of $\\nabla\\log(\\eta^{N,\\epsilon})$ which although smooth for all $\\epsilon>0$ it does exhibit a blow-up as $\\epsilon>0$ tends to zero. Therefore, since by triangular inequality up to a constant $C=C(R,d)>0$ ", "page_idx": 22}, {"type": "equation", "text": "$$\nC A\\geq\\|\\mathbf{s}_{\\theta}\\|_{C^{2}}\\geq\\|\\mathbf{s}_{\\theta}\\|_{L^{2}(\\eta^{N,\\epsilon})}\\geq\\|D\\sqrt{\\eta^{N,\\epsilon}}\\|_{L^{2}}-e_{n n}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "the constant $A>0$ will also exhibit a blow-up which can make the approximation worse. Therefore there could be a potential trade-off between NN approximation $e_{n n}$ becoming small and the value of the norm $A$ . ", "page_idx": 22}, {"type": "text", "text": "First we require a preliminary lemma. ", "page_idx": 22}, {"type": "text", "text": "Lemma C.3. Under the same assumptions as in Theorem C.1 we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\displaystyle{\\bf E}\\Big[\\mathcal{J}(\\eta^{\\epsilon},{\\bf s}_{\\theta}^{N})\\Big]\\leq e_{n n}+\\frac{C}{N^{\\frac1d}}\\Big(T A^{2}+\\frac{1}{\\sqrt T}(1+d\\log(R))\\Big)}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof. We note that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\qquad\\qquad\\mathcal{I}(\\eta^{\\epsilon},\\mathbf{s}_{\\theta^{*}})=\\mathcal{I}_{L}(\\eta^{\\epsilon},\\mathbf{s}_{\\theta^{*}})+4\\|D\\sqrt{\\eta^{\\epsilon}}\\|_{2}^{2}}\\\\ &{=\\mathcal{I}(\\eta^{N,\\epsilon},\\mathbf{s}_{\\theta^{*}})+2\\Big(\\|D\\sqrt{\\eta^{\\epsilon}}\\|_{2}^{2}-\\|D\\sqrt{\\eta^{N,\\epsilon}}\\|_{2}^{2}\\|\\Big)+\\Big(\\mathcal{I}_{L}(\\eta,\\mathbf{s}_{\\theta^{*}})-\\mathcal{I}_{L}(\\eta^{N,\\epsilon},\\mathbf{s}_{\\theta^{*}})\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By assumption we have the bound ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{I}(\\eta^{N,\\epsilon},\\mathbf{s}_{\\theta^{*}})\\leq e_{n n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By Proposition B.1 ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle2\\Big(\\|D\\sqrt{\\eta^{\\epsilon}}\\|_{2}^{2}-\\|D\\sqrt{\\eta^{N,\\epsilon}}\\|_{2}^{2}\\Big)}\\\\ {\\displaystyle=\\int_{\\Omega}\\pi^{\\epsilon}(x)\\log(\\pi^{\\epsilon}(x))-\\hat{\\pi}^{N,\\epsilon}(x)\\log(\\hat{\\pi}^{N,\\epsilon}(x))d x-\\displaystyle\\int_{\\Omega}\\eta^{\\epsilon}(T)\\log(\\eta^{\\epsilon}(T))-\\eta^{N,\\epsilon}\\log(\\eta^{N,\\epsilon}(T))d x.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Taking expectations with respect to the sample $\\hat{\\pi}^{N}$ we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\bar{z}}\\Big[\\displaystyle\\int_{\\Omega}\\pi^{\\epsilon}(x)\\log(\\pi^{\\epsilon}(x))\\!-\\!\\bar{\\pi}^{N,\\epsilon}\\log(\\bar{\\pi}^{N,\\epsilon})d x\\Big]=\\displaystyle\\int_{\\Omega}\\pi^{\\epsilon}(x)\\log(\\pi^{\\epsilon}(x))\\!-\\!\\mathbf{E}\\Big[\\hat{\\pi}^{N,\\epsilon}(x)\\log(\\hat{\\pi}^{N,\\epsilon}(x))\\Big]d x}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\int_{\\Omega}\\pi^{\\epsilon}(x)\\log(\\pi^{\\epsilon}(x))-\\mathbf{E}\\Big[\\hat{\\pi}^{N,\\epsilon}(x)\\Big]\\log(\\mathbf{E}\\Big[\\hat{\\pi}^{N,\\epsilon}\\Big])d x=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "In the above we used Jensen\u2019s inequality along with the fact that ", "page_idx": 22}, {"type": "equation", "text": "$$\n{\\bf E}\\Big[\\hat{\\pi}^{N,\\epsilon}\\Big]=\\pi^{\\epsilon}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Moreover, by Lemma B.3 we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\mathbf{E}\\Big[\\eta^{N,\\epsilon}\\log(\\eta^{N,\\epsilon}(T))-\\displaystyle\\int_{\\Omega}\\eta^{\\epsilon}(T)\\log(\\eta^{\\epsilon}(T))d x\\Big]\\leq\\frac{C}{\\sqrt{T}}(1+d\\log(R))\\mathbf{E}\\Big[\\mathbf{d}_{1}(\\pi,\\hat{\\pi}^{N})\\Big]}&{}\\\\ {\\displaystyle\\leq\\frac{C}{\\sqrt{T}N^{\\frac{1}{d}}}(1+d\\log(R)).}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Finally, from Proposition B.2 we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{E}\\Big[\\Big|\\mathcal{I}_{L}\\big(\\eta,\\mathbf{s}_{\\theta^{*}}\\big)-\\mathcal{I}_{L}\\big(\\eta^{N,\\epsilon},\\mathbf{s}_{\\theta^{*}}\\big)\\Big|\\Big]\\leq C T\\mathbf{E}\\Big[\\|\\mathbf{s}_{\\theta}^{N}\\|_{C^{2}}^{2}\\mathbf{d}_{1}(\\pi,\\hat{\\pi}^{N})\\Big]\\leq\\frac{C T}{N^{\\frac{1}{d}}}A^{2}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathbf{E}\\Big[\\mathcal{J}(\\eta^{\\epsilon},\\theta^{*})\\Big]\\leq e_{n n}+\\frac{C}{N^{\\frac{1}{d}}}\\Big(T A^{2}+\\frac{1}{\\sqrt{T}}(1+d\\log(R))\\Big),\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "and the result follows. ", "page_idx": 22}, {"type": "text", "text": "Proof. By Theorem 3.1 we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{d}_{1}(\\pi,m_{g}(T))\\leq\\sqrt{\\epsilon}+\\mathbf{d}_{1}(\\pi^{\\epsilon},m_{g}(T)).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "And ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf d_{1}\\big(m_{g}(T),\\pi^{\\epsilon}\\big)=\\mathbf d_{1}(m_{g}(T),m^{\\epsilon}(T))}\\\\ &{\\qquad\\qquad\\qquad\\leq C R^{\\frac32}(1+\\sqrt{\\|D b^{1}\\|_{\\infty}})\\big(\\mathbf d_{1}(m^{\\epsilon}(0),\\frac1{\\operatorname{vol}(R\\mathbb{T}^{d})})+\\sqrt{T}\\|\\mathbf b^{\\pi^{\\epsilon}}-\\mathbf b_{\\theta^{*}}\\|_{L^{2}(m^{\\epsilon})}\\big)}\\\\ &{\\qquad\\qquad\\qquad\\leq C R^{\\frac32}(1+\\sqrt{A})\\left(R^{2}e^{-\\frac{\\omega T}{R^{2}}}+\\sqrt{T\\mathcal I(\\eta^{\\epsilon},\\theta^{*})}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Taking expectations and using the Lemma above yields the result. ", "page_idx": 23}, {"type": "text", "text": "D Technical results ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "D.1 HJB estimates ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this subsection, we gather all the regularity estimates for the HJB equations. The results are for the most part standard, we include however detailed proofs for the readers convenience. For further study into the techniques and topics we refer to [9, 10]. Although as mentioned in the introduction in the current work we assumed $\\sigma(t)=\\sqrt{2}$ and $f=0$ in the OU processes (1), here we prove the regularity results for a general non-degenerate diffusion $\\alpha(t)$ . The addition of $f$ may be incorporated in the drift $b$ . ", "page_idx": 23}, {"type": "text", "text": "Proposition D.1. Let $\\Omega\\subset\\mathbb{R}^{d}$ , $v:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ and $b:[0,T]\\times\\Omega\\rightarrow\\mathbb{R}$ be given with $\\|\\nabla b\\|_{\\infty}<\\infty$ . Consider the solution $u:[0,T]\\times\\Omega\\rightarrow\\mathbb{R}$ of ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{-\\partial_{t}u-\\alpha(t)\\Delta u+\\frac{\\alpha(t)}{2}|\\nabla u|^{2}+b\\cdot\\nabla u=0\\;i n\\;[0,T)\\times\\Omega,\\right.}\\\\ {u(T,x)=v(x)\\;i n\\;\\Omega.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Assume moreover that for some $M>0$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n0<\\frac{1}{M}\\leq\\alpha(t).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then up to a universal constant $C>0$ the following holds ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\nabla u(t)\\|_{\\infty}^{2}\\leq C(1+\\|\\nabla b\\|_{\\infty}M)\\|v\\|_{C^{1}}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(T-t)\\|\\nabla u(t)\\|_{\\infty}^{2}\\leq C M(T\\|\\nabla b\\|_{\\infty}+1)\\|v\\|_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Let $z:[0,T]\\times\\Omega\\rightarrow\\mathbb{R}$ be given by ", "page_idx": 23}, {"type": "equation", "text": "$$\nz=\\frac{1}{2}|\\nabla u|^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Differentiating the equation for $u$ in $i=1,\\cdots,d$ yields ", "page_idx": 23}, {"type": "equation", "text": "$$\n-\\partial_{t}\\boldsymbol{u}_{i}-\\alpha(t)\\Delta\\boldsymbol{u}_{i}+\\alpha(t)\\nabla\\boldsymbol{u}\\cdot\\nabla\\boldsymbol{u}_{i}+\\boldsymbol{b}\\cdot\\nabla\\boldsymbol{u}_{i}+\\boldsymbol{b}_{i}\\cdot\\nabla\\boldsymbol{u}=0,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and thus multiplying by $u_{i}$ and summing over $i$ , we obtain the following equation for $z$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{\\begin{array}{l l}{-\\partial_{t}z-\\alpha(t)\\Delta z+\\alpha(t)|\\nabla^{2}u|^{2}+b\\cdot\\nabla z+\\alpha(t)\\nabla u\\cdot\\nabla z+\\langle\\nabla u\\nabla b,\\nabla u\\rangle=0,}\\\\ {z(T)=\\frac{1}{2}|\\nabla v|^{2}.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In the above $\\nabla b$ is the matrix with entries ", "page_idx": 23}, {"type": "equation", "text": "$$\n[\\nabla b]_{i,j}=\\frac{\\partial b^{i}}{\\partial x_{j}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for ", "page_idx": 23}, {"type": "equation", "text": "$$\nb=(b^{1},\\cdots\\,,b^{d}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Let $w:[0,T]\\times\\Omega\\rightarrow\\mathbb{R}$ be given by ", "page_idx": 23}, {"type": "equation", "text": "$$\nw(t,x)=z-C u\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $C>0$ is a constant to be determined later. Assume that the function $w$ achieves its maximum at $(s_{0},x_{0})\\in[0,T]\\times\\Omega$ . We will look at the following cases: ", "page_idx": 23}, {"type": "text", "text": "\u2022 Case 1: Assume that $0\\leq s_{0}<T$ and we will show that for $C>0$ large enough this leads to a contradiction. At $(s_{0},x_{0})$ we have the optimality conditions ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla z(s_{0},x_{0})=C\\nabla u(s_{0},x_{0})}\\\\ &{\\Delta z(s_{0},x_{0})\\leq C\\Delta u(s_{0},x_{0})}\\\\ &{\\partial_{t}z(s_{0},x_{0})\\leq C\\partial_{t}u(s_{0},x_{0}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Hence, ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C(-\\partial_{t}u(s_{0},x_{0})-\\alpha(s_{0})\\Delta u(s_{0},x_{0})+b\\cdot\\nabla u(s_{0},x_{0}))}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq(-\\partial_{t}z(s_{0},x_{0})-\\alpha(s_{0})\\Delta z(s_{0},x_{0})+b\\cdot\\nabla z(s_{0},x_{0}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and using the respective equations for $u,z$ we obtain that at $(s_{0},x_{0})$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n-C\\frac{\\alpha(s_{0})}{2}|\\nabla u|^{2}\\leq-\\alpha(s_{0})|\\nabla^{2}u|^{2}-\\langle\\nabla u\\cdot\\nabla b,\\nabla u\\rangle-\\alpha(s_{0})\\nabla u\\cdot\\nabla z.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Using the optimality conditions yields ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\alpha(s_{0})\\frac{C}{2}|\\nabla u|^{2}+\\alpha(s_{0})|\\nabla^{2}u|^{2}\\leq-\\langle\\nabla u\\cdot\\nabla b,\\nabla u\\rangle\\leq\\|\\nabla b\\|_{\\infty}|\\nabla u|^{2},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which is a contradiction if $C\\geq2\\|\\nabla b\\|_{\\infty}M$ . Thus for ", "page_idx": 24}, {"type": "equation", "text": "$$\nC_{0}=2\\|\\nabla b\\|_{\\infty}M\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "the function ", "page_idx": 24}, {"type": "equation", "text": "$$\nz-C_{0}u\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "achieves its maximum at some point $(T,x_{0})$ . ", "page_idx": 24}, {"type": "text", "text": "\u2022 Case 2: Assume that $s_{0}=T$ . Then, for every $(t,x)\\in[0,T]\\times\\Omega$ we have ", "page_idx": 24}, {"type": "equation", "text": "$$\nz(t,x)\\leq w(T,x_{0})+C u(t,x)\\leq\\frac{1}{2}|\\nabla v|^{2}+C\\|v\\|_{\\infty}+C\\|u_{+}\\|_{\\infty}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and by maximum principle we also have that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\|u\\|_{\\infty}\\leq\\|v\\|_{\\infty}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus, up to a universal constant ", "page_idx": 24}, {"type": "equation", "text": "$$\n|\\nabla u(t,x)|^{2}\\lesssim(1+\\|\\nabla b\\|M)\\|v\\|_{C^{1}},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which proves estimate (58). ", "page_idx": 24}, {"type": "text", "text": "Now we prove estimate (59). The proof follows the previous strategy only this time we consider the function ", "page_idx": 24}, {"type": "equation", "text": "$$\nw(t,x)=(T-t)z-C u,\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where again $C>0$ will be determined later. Assume that the maximum occurs at some point $\\left(s_{0},x_{0}\\right)$ . ", "page_idx": 24}, {"type": "text", "text": ". Case 1: Assume that $s_{0}<T$ . Looking again the corresponding optimality conditions and using the equations for $u$ and $z$ we have at $(s_{0},x_{0})$ ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Big(\\frac{C\\alpha(s_{0})}{2}-1\\Big)|\\nabla u|^{2}\\leq-(T-s_{0})\\langle\\nabla u\\cdot\\nabla b,\\nabla u\\rangle\\leq T\\|\\nabla b\\|_{\\infty}|\\nabla u|^{2},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which is a contradiction if $C\\geq2T M\\|\\nabla b\\|_{\\infty}+2M$ . ", "page_idx": 24}, {"type": "text", "text": "2. Case 2: Assume that $s_{0}=T$ , then for all $(t,x)\\in[0,T]\\times\\Omega$ we have ", "page_idx": 24}, {"type": "equation", "text": "$$\nz(t,x)\\leq w(T,x_{0})+C_{0}u(t,x)=-C_{0}v(T,x)+C_{0}u(t,x)\\leq2C_{0}\\|v\\|_{\\infty}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Thus up to a dimensional constant we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n(T-t)|\\nabla u|^{2}(t,x)\\lesssim M(T\\|\\nabla b\\|_{\\infty}+1)\\|v\\|_{\\infty}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Corollary D.2. Let $\\Omega\\subset\\mathbb{R}^{d}$ , $v:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ and $b:[0,T]\\times\\Omega\\rightarrow\\mathbb{R}$ be given with $\\|\\nabla b\\|_{\\infty}<\\infty$ . Given $\\psi:\\Omega\\rightarrow\\mathbb{R}$ , with $\\psi\\geq1$ consider the solution $\\phi:[0,T]\\times\\Omega\\rightarrow\\mathbb{R}$ of ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\{-\\partial_{t}\\phi-\\alpha(t)\\Delta\\phi+b\\cdot\\nabla\\phi=0\\,i n\\left[0,T\\right)\\times\\Omega,\\right.}\\\\ {\\left.\\phi(T,x)=\\psi(x)\\;i n\\,\\Omega.\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Assume moreover that for some $M>0$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n0<\\frac{1}{M}\\leq\\alpha(t).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Then up to a dimensional constant $C>0$ we have the following estimates ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla\\phi(t)\\|_{\\infty}^{2}\\leq C\\|\\psi\\|_{C^{1}}^{3}(1+\\|\\nabla b\\|_{\\infty}M)\\qquad}\\\\ {(T-t)\\|\\nabla\\phi(t)\\|_{\\infty}^{2}\\leq C\\|\\psi\\|_{\\infty}^{3}M(T\\|\\nabla b\\|_{\\infty}+1)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. Since $\\psi\\geq1$ by the Maximum Principle we have that ", "page_idx": 25}, {"type": "equation", "text": "$$\n1\\leq\\phi(t,x)\\leq\\|\\psi\\|_{\\infty}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thus we can define $u:[0,T]\\times\\Omega\\rightarrow\\mathbb{R}$ by the formula ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\phi(t,x)=e^{-{\\frac{u(t,x)}{2}}}\\iff u(t,x)=-2\\log(\\phi(t,x)),\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We then have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\partial_{t}\\phi=-\\frac{1}{2}\\partial_{t}u\\phi,\\nabla\\phi=-\\frac{\\nabla u}{2}\\phi,\\Delta\\phi=\\frac{|\\nabla u|^{2}}{4}\\phi-\\frac{\\Delta u}{2}\\phi,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and so by substituting in the equation for $\\phi$ we obtain ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{-\\partial_{t}u-\\alpha(t)\\Delta u+\\frac{\\alpha(t)}{2}|\\nabla u|^{2}+b\\cdot\\nabla u=0,}\\\\ {u(T,x)=-2\\log(\\psi(x)).}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The results will follow by Proposition D.1 for $v(x)=-2\\log(\\psi(x))$ . First we need the following estimates ", "page_idx": 25}, {"type": "equation", "text": "$$\n|v(x)|=2|\\log(\\psi(x))|\\leq2\\log(\\|\\psi\\|_{\\infty})\\leq2\\|\\psi\\|_{\\infty}{\\mathrm{~since~}}\\psi\\geq1,\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n|\\nabla v(x)|=2\\frac{|\\nabla\\psi(x)|}{|\\psi(x)|}\\leq2\\|\\nabla\\psi\\|_{\\infty}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\implies\\|v\\|_{C^{1}}\\leq2\\|\\psi\\|_{C^{1}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Moreover, we note that ", "page_idx": 25}, {"type": "equation", "text": "$$\n|\\nabla u(t,x)|^{2}=4\\frac{|\\nabla\\phi(t,x)|^{2}}{\\phi^{2}}\\geq4\\frac{|\\nabla\\phi(t,x)|^{2}}{\\|\\psi\\|_{\\infty}^{2}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore, by Proposition D.1 estimate (58) we obtain that up to a dimensional constant $C>0$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\nabla\\phi(t)\\|_{\\infty}^{2}\\leq C\\|\\psi\\|_{\\infty}^{2}(1+\\|\\nabla b\\|_{\\infty}M)\\|\\psi\\|_{C^{1}}\\leq C\\|\\psi\\|_{C^{1}}^{3}(1+\\|\\nabla b\\|_{\\infty}M).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Similarly by Proposition D.1, estimate (59) again up to a dimensional constant $C>0$ we obtain ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(T-t)\\|\\nabla\\phi(t)\\|_{\\infty}^{2}\\leq C\\|\\psi\\|_{\\infty}^{3}M(T\\|\\nabla b\\|_{\\infty}+1).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "D.2 Long time behavior of periodic heat equation. ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The following result is classical and establishes exponential rate of convergence in ${\\bf d}_{1}$ for solutions to the heat equation in the uniform distribution on the torus. We provide a proof for the readers convenience. ", "page_idx": 25}, {"type": "text", "text": "Proposition D.3. Let $\\Omega\\,=\\,R\\mathbb{T}^{d}$ and $m_{i}\\,\\in\\,\\mathscr{P}(\\Omega),i\\,=\\,1,2$ be two probability measures. Define $\\rho^{i}:\\bar{[0,\\infty)}\\times\\Omega\\rightarrow[0,\\infty)$ by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\{\\partial_{t}\\rho^{i}-\\Delta\\rho^{i}=0\\,i n\\left(0,\\infty\\right)\\times\\Omega,\\right.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Then, there exists constants $C=C(d)>0,\\omega=\\omega(d)>0$ such that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{d}_{1}(\\rho^{2}(t),\\rho^{1}(t))\\le C R e^{-\\frac{\\omega t}{R^{2}}}\\mathbf{d}_{1}\\big(m_{2},m_{1}\\big)\\,f o r\\,t\\ge0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "In particular by choosing $\\begin{array}{r}{m_{2}=\\frac{1}{\\nu o l(R\\mathbb{T}^{d})}}\\end{array}$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbf{d}_{1}(\\rho^{1}(t),\\frac{1}{\\nu o l(R\\mathbb{T}^{d})})\\leq C R e^{-\\frac{\\omega t}{R^{2}}}\\mathbf{d}_{1}\\big(m_{1},\\frac{1}{\\nu o l(R\\mathbb{T}^{d})}\\big)f o r\\,t\\geq0.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. First we assume that $R=1$ and recall that if $\\Gamma(t,x)$ denotes the heat kernel on the unit torus $\\mathbb{T}^{d}$ then for a constant $C=C(d)>0$ we have the normalizing effect ", "page_idx": 26}, {"type": "equation", "text": "$$\n|\\nabla\\Gamma(t)\\star g|\\leq C\\frac{\\|g\\|_{\\infty}}{\\sqrt{t}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Fix $\\psi:\\ensuremath{\\mathbb{T}}^{d}\\to\\ensuremath{\\mathbb{R}}$ such that $\\mathrm{Lip}(\\psi)\\leq1$ and $\\psi(0)=0$ . Such a function $\\psi$ will also satisfy ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|\\psi\\|_{\\infty}\\leq1.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Fix $T>0$ and let $\\phi:\\mathbb{T}^{d}\\times[0,T]\\rightarrow\\mathbb{R}$ be given by ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{-\\partial_{t}\\phi-\\Delta\\phi=0\\,\\mathrm{in}\\,[0,T)\\times\\mathbb{T}^{d},}\\\\ {\\phi(T,x)=\\psi(x).}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By testing against $\\phi$ in the equation satisfied by $\\lambda=\\rho^{2}-\\rho^{1}$ we obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\int\\psi(x)d(\\rho^{2}(T)-\\rho^{1}(T))(x)=\\int\\phi(0)d(m_{2}-m_{1})\\leq\\mathbf{d}_{1}(m_{2},m_{1})\\lVert\\nabla\\phi(0)\\rVert_{\\infty}\\leq C\\frac{\\mathbf{d}_{1}(m_{2},m_{1})}{\\sqrt{T}}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Taking the supremum in $\\psi$ yields ", "page_idx": 26}, {"type": "equation", "text": "$$\n{\\bf d}_{1}(\\rho^{2}(T),\\rho^{1}(T))\\le\\frac{C}{\\sqrt{T}}{\\bf d}_{1}(\\rho^{2}(0),\\rho^{1}(0)).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Repeating the same argument as above with initial conditions $m_{i}=\\rho^{i}(T)$ , we obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbf{d}_{1}(\\rho^{2}(2T),\\rho^{1}(2T))\\leq\\frac{C}{\\sqrt{T}}\\mathbf{d}_{1}(\\rho^{2}(T),\\rho^{1}(T))\\leq\\Big(\\frac{C}{\\sqrt{T}}\\Big)^{2}\\mathbf{d}_{1}(m_{2},m_{1})\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and by induction for all $k\\in\\mathbb{N}$ we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n{\\bf d}_{1}(\\rho^{2}(k T),\\rho^{1}(k T))\\le\\Bigl(\\frac{C}{\\sqrt{T}}\\Bigr)^{k}{\\bf d}_{1}(m_{2},m_{1})=C^{k}T^{-\\frac{k}{2}}{\\bf d}_{1}(m_{2},m_{1}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Let $\\theta\\ge0$ and fix a $k\\in\\mathbb{N}$ such that ", "page_idx": 26}, {"type": "equation", "text": "$$\nk T\\leq\\theta<(k+1)T.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Since the heat operator defines a contraction in ${\\bf d}_{1}$ we have that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbf{d}_{1}(\\rho^{2}(\\theta),\\rho^{1}(\\theta))\\leq\\mathbf{d}_{1}(\\rho^{2}(k T),\\rho^{1}(k T))\\leq(C\\sqrt{T})^{-k}\\mathbf{d}_{1}(m_{2},m_{1}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Moreover, we have that ", "page_idx": 26}, {"type": "equation", "text": "$$\n-k\\leq-\\frac{\\theta}{T}+1\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "thus ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbf{d}_{1}(\\rho^{2}(\\theta),\\rho^{1}(\\theta))\\leq(C\\sqrt{T})^{-\\frac{\\theta}{T}+1}\\mathbf{d}_{1}(m_{2},m_{1})\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "and so choosing $T$ large enough so that ", "page_idx": 26}, {"type": "equation", "text": "$$\nC\\sqrt{T}\\geq e\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "yields the result for $R\\,=\\,1$ . For a general $R\\,>\\,0$ , we simply apply the above to the functions $\\lambda^{i}:[0,\\infty)\\times\\ensuremath{\\mathbb{T}}^{d}$ defined by ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\lambda^{i}(t,x)=R^{d}\\rho^{i}(R^{2}t,R x)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "which yields ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{d}_{1}(\\lambda^{2}(t),\\lambda^{1}(t))\\leq C e^{-\\omega t}\\mathbf{d}_{1}(\\lambda^{2}(0),\\lambda^{1}(0)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{d}_{1}(\\lambda^{2}(t),\\lambda^{1}(t))=\\displaystyle\\operatorname*{sup}_{\\psi:\\mathbb{T}^{d}\\to\\mathbb{R},\\mathrm{Lip}(\\psi)\\leq1}\\int_{\\mathbb{T}^{d}}\\psi(x)R^{d}(\\rho^{2}(t R^{2},R x)-\\rho^{1}(t R^{2},R x))}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\operatorname*{sup}_{\\psi}\\int_{R\\mathbb{T}^{d}}\\psi\\Big(\\frac{u}{R}\\Big)(\\rho^{2}(t R^{2},u)-\\rho^{1}(t R^{2},u))}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n=R\\operatorname*{sup}_{\\psi:R\\mathbb{T}^{d}\\rightarrow\\mathbb{R},\\operatorname{Lip}(\\psi)\\leq1}\\int_{R\\mathbb{T}^{d}}\\psi(u)\\big(\\rho^{2}(t R^{2},u)-\\rho^{1}(t R^{2},u)\\big)=R\\mathbf{d}_{1}\\big(\\rho^{2}(R^{2}t),\\rho^{1}(R^{2}t)\\big)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "the result follows. ", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: All our claims, both formal and descriptive explanations of our theorems are carefully written to appropriately reflect the analytical results we obtain. Each theoretical claim is backed by a proof and accompanying supporting arguments in the appendix. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Limitations of the analysis are clear based on the assumptions in each theorem. We have also discussed throughout where the analysis has not been optimized to exhaustion relative to the power of the analysis tools we use. ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The main three theorems in the paper are carefully formulated and stated. The proofs are descriptively outlined in the main body, and the full details with supporting propositions and lemmas are provided in the appendix along with their detailed proofs. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our paper does not include experiments. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our paper does not include experiments requiring code. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our paper does not include experiments. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: Our paper does not include experiments. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our paper does not include experiments. ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] Justification: Yes. The paper is also written to preserve anonymity as much as possible. ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [No] ", "page_idx": 29}, {"type": "text", "text": "Justification: While our paper involves analysis of generative artificial intelligence algorithm, there are no immediate pathways a malicious actor could use our analysis to produce more harmful disinformation or deepfakes. Our analysis primarily exist to understand existing generative AI algorithms. ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper is an analysis paper, and poses none of these acute risks. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: No individual or entity owns the theory of PDEs. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor does it involve research with human subjects. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}]