[{"figure_path": "GB5a0RRYuv/tables/tables_7_1.jpg", "caption": "Table 1: Result of NER, RE, and ER through Fine-tuned LLMs.", "description": "This table presents the performance of different fine-tuned Large Language Models (LLMs) on three tasks: Named Entity Recognition (NER), Relation Extraction (RE), and Entity Resolution (ER).  The models are evaluated using precision, recall, and F1-score metrics for each task.  The results demonstrate the relative effectiveness of each LLM in extracting and resolving entities and relations within the materials science domain.", "section": "2.2 LLMs training, evaluation and inference"}, {"figure_path": "GB5a0RRYuv/tables/tables_7_2.jpg", "caption": "Table 2: Result of the ablation experiment in normalization.", "description": "This table presents the results of an ablation study evaluating the impact of different entity resolution (ER) methods on the overall performance of the named entity recognition (NER), relation extraction (RE), and entity resolution tasks. The baseline is the Darwin model with normalization.  Subsequent rows show the performance when removing components of the ER process: ER-N/F (removing Name/Formula entity resolution), ER-NF/A (removing Name/Formula/Acronym entity resolution), and ER-ED (removing the expert dictionary). The F1 score and the difference (\u0394) from the baseline are reported for each task.", "section": "2.3 Entity resolution"}, {"figure_path": "GB5a0RRYuv/tables/tables_12_1.jpg", "caption": "Table 3: Human evaluation metric on randomly selected triples.", "description": "This table presents the results of a human evaluation performed on a randomly selected subset of 500 triples from the Materials Knowledge Graph (MKG).  The evaluation assesses the accuracy of entity and relation labeling for various categories within the MKG, including 'Formula', 'Name', 'Acronym', 'Descriptor', 'Property', 'Application', 'Structure/Phase', 'Synthesis', and 'Characterization'.  The table shows the total number of instances for each category, the number of entities with disagreements, the percentage disagreement for entities, the number of relations with disagreements, and the percentage disagreement for relations. This data offers insights into the reliability and accuracy of the automated knowledge extraction and normalization processes used to construct the MKG.", "section": "A.1 Validation for MKG"}]