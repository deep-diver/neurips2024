[{"figure_path": "qLnXPVvwLx/tables/tables_4_1.jpg", "caption": "Table 1: Detailed Perception Performance on MMStar under Prism Evaluation Framework. Reasoning module: ChatGPT. Abbreviations: Coarse Perception (CP), Fine-grained Perception (FP); Instance Reasoning (IR); Logical Reasoning (LR); Science&Technology (ST). Overall best scores are marked as bold, and intra-category best scores are marked as underline.", "description": "This table presents a detailed breakdown of the perception performance of various Vision-Language Models (VLMs) on the MMStar benchmark.  The performance is analyzed using the Prism framework, which separates perception and reasoning processes. The table shows the performance of different VLMs across various sub-tasks (Coarse Perception, Fine-grained Perception, Instance Reasoning, Logical Reasoning, Science & Technology, and Math) using both generic and query-specific instructions. The best overall scores and best scores within each category are highlighted.", "section": "3.1 Main Results"}, {"figure_path": "qLnXPVvwLx/tables/tables_6_1.jpg", "caption": "Table 1: Detailed Perception Performance on MMStar under Prism Evaluation Framework. Reasoning module: ChatGPT. Abbreviations: Coarse Perception (CP), Fine-grained Perception (FP); Instance Reasoning (IR); Logical Reasoning (LR); Science&Technology (ST). Overall best scores are marked as bold, and intra-category best scores are marked as underline.", "description": "This table presents a detailed breakdown of the perception performance of various Vision-Language Models (VLMs) on the MMStar benchmark.  The performance is assessed using the Prism framework, with ChatGPT serving as the reasoning module. The results are categorized into five sub-categories: Coarse Perception (CP), Fine-grained Perception (FP), Instance Reasoning (IR), Logical Reasoning (LR), and Science & Technology (ST).  The best overall scores and the best scores within each category are highlighted. This allows for a comparison of perception capabilities across different VLMs, revealing relative strengths and weaknesses in various aspects of visual understanding.", "section": "3.1 Implementation Details"}, {"figure_path": "qLnXPVvwLx/tables/tables_6_2.jpg", "caption": "Table 1: Detailed Perception Performance on MMStar under Prism Evaluation Framework. Reasoning module: ChatGPT. Abbreviations: Coarse Perception (CP), Fine-grained Perception (FP); Instance Reasoning (IR); Logical Reasoning (LR); Science&Technology (ST). Overall best scores are marked as bold, and intra-category best scores are marked as underline.", "description": "This table presents a detailed breakdown of the perception performance of various Vision Language Models (VLMs) on the MMStar benchmark, using ChatGPT as the reasoning module.  It compares the performance of proprietary and open-source VLMs across five categories: Coarse Perception, Fine-grained Perception, Instance Reasoning, Logical Reasoning, and Science & Technology.  The best overall score and best scores within each category are highlighted.", "section": "3.1 Main Results"}, {"figure_path": "qLnXPVvwLx/tables/tables_7_1.jpg", "caption": "Table 4: Ablation on Different Generic Instructions.", "description": "This ablation study investigates the impact of different generic instructions for visual information extraction on the performance of two state-of-the-art VLMs: GPT-40 and LLaVA-NeXT (Yi-34B).  Five different instruction types are compared, including human-written instructions and various GPT-generated instructions that utilize different prompting strategies such as chain-of-thought prompting or decomposition-based prompting. The results show that while different generic instructions lead to some performance variation, the differences are not significant. Therefore, the authors opt to use the \"Human 1\" instruction for subsequent evaluations as a standard.", "section": "4 Ablation Study"}, {"figure_path": "qLnXPVvwLx/tables/tables_7_2.jpg", "caption": "Table 5: Ablation on Using Different LLMs as the Reasoning Module.", "description": "This table presents the ablation study results focusing on the impact of using different Large Language Models (LLMs) as the reasoning module within the Prism framework.  It shows the performance of four different LLMs (GPT-3.5-Turbo-0125, GPT-4-Turbo-0125, Llama-3-70B-Instruct, and DeepSeek-v2-Chat) when paired with various Vision Language Models (VLMs). The results demonstrate the performance variation caused by different LLMs on a range of vision-language tasks.  Higher scores indicate better performance.", "section": "4 Ablation Study"}, {"figure_path": "qLnXPVvwLx/tables/tables_9_1.jpg", "caption": "Table 1: Detailed Perception Performance on MMStar under Prism Evaluation Framework. Reasoning module: ChatGPT. Abbreviations: Coarse Perception (CP), Fine-grained Perception (FP); Instance Reasoning (IR); Logical Reasoning (LR); Science&Technology (ST). Overall best scores are marked as bold, and intra-category best scores are marked as underline.", "description": "This table presents the detailed performance of various Vision-Language Models (VLMs) on the MMStar benchmark, focusing on their perception capabilities.  The results are broken down by different instruction types (Generic and Query-Specific) and various sub-tasks within the benchmark (Coarse Perception, Fine-grained Perception, Instance Reasoning, Logical Reasoning, Science & Technology, and General).  The table highlights the best overall performance and best performance within each category.", "section": "3.1 Implementation Details"}, {"figure_path": "qLnXPVvwLx/tables/tables_9_2.jpg", "caption": "Table 1: Detailed Perception Performance on MMStar under Prism Evaluation Framework. Reasoning module: ChatGPT. Abbreviations: Coarse Perception (CP), Fine-grained Perception (FP); Instance Reasoning (IR); Logical Reasoning (LR); Science&Technology (ST). Overall best scores are marked as bold, and intra-category best scores are marked as underline.", "description": "This table presents a detailed breakdown of the perception performance of various Vision-Language Models (VLMs) on the MMStar benchmark.  The evaluation is conducted using the Prism framework, with ChatGPT as the reasoning module. The table shows the performance for each VLM across several categories (CP, FP, IR, LR, Math, ST) and provides an overall score.  Best scores are highlighted in bold, with the best score within each category underlined.", "section": "3.1 Main Results"}, {"figure_path": "qLnXPVvwLx/tables/tables_17_1.jpg", "caption": "Table 1: Detailed Perception Performance on MMStar under Prism Evaluation Framework. Reasoning module: ChatGPT. Abbreviations: Coarse Perception (CP), Fine-grained Perception (FP); Instance Reasoning (IR); Logical Reasoning (LR); Science&Technology (ST). Overall best scores are marked as bold, and intra-category best scores are marked as underline.", "description": "This table presents the detailed performance results of various Vision-Language Models (VLMs) on the MMStar benchmark, specifically focusing on their perception capabilities. The evaluation was conducted using the Prism framework with ChatGPT as the reasoning module. The table is divided into two sections: Proprietary VLMs and Open-Source VLMs. For each VLM, the performance metrics are provided across several categories: Coarse Perception (CP), Fine-grained Perception (FP), Instance Reasoning (IR), Logical Reasoning (LR), Math, and Science & Technology (ST).  The overall performance is also presented, with the best overall scores and the best scores within each category highlighted.", "section": "3.1 Main Results"}, {"figure_path": "qLnXPVvwLx/tables/tables_18_1.jpg", "caption": "Table 10: More Detailed Performance Results of PrismCaptioners", "description": "This table presents a more detailed breakdown of the performance results for the PrismCaptioner models. It shows the performance across different benchmarks (MMStar, MMMU, MMMU (F), MathVista, MathVista (F), AI2D, and AI2D (F)) when using multiple images as inputs and a maximum output length of 2048 tokens.  The results are broken down by model and language model used (ChatGPT or Llama3). This allows for a more granular understanding of the models' capabilities in various scenarios. Note the use of (F) to signify a filtered subset of data.", "section": "3.1 Implementation Details"}, {"figure_path": "qLnXPVvwLx/tables/tables_26_1.jpg", "caption": "Table 1: Detailed Perception Performance on MMStar under Prism Evaluation Framework. Reasoning module: ChatGPT. Abbreviations: Coarse Perception (CP), Fine-grained Perception (FP); Instance Reasoning (IR); Logical Reasoning (LR); Science&Technology (ST). Overall best scores are marked as bold, and intra-category best scores are marked as underline.", "description": "This table presents the detailed quantitative results of evaluating the perception capabilities of various Vision-Language Models (VLMs) on the MMStar benchmark using the Prism framework.  The evaluation uses ChatGPT as the reasoning module and two types of instructions: generic and query-specific.  The table shows the performance of each VLM across five aspects: Coarse Perception, Fine-grained Perception, Instance Reasoning, Logical Reasoning, and Science & Technology. The best overall scores and the best scores within each category are highlighted.", "section": "3.1 Implementation Details"}]