[{"type": "text", "text": "How does PDE order affect the convergence of PINNs? ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Changhoon $\\mathbf{Song^{1*}}$ Yesom Park1,2\u2217 Myungjoo Kang3 ", "page_idx": 0}, {"type": "text", "text": "1 Research Institute of Mathematics, Seoul National University 2 Department of Mathematics, University of California, Los Angeles 3 Department of Mathematical Sciences, Seoul National University changhoon.song93@gmail.com, yeisom@math.ucla.edu, mkang@snu.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper analyzes the inverse relationship between the order of partial differential equations (PDEs) and the convergence of gradient descent in physics-informed neural networks (PINNs) with the power of ReLU activation. The integration of the PDE into a loss function endows PINNs with a distinctive feature to require computing derivatives of model up to the PDE order. Although it has been empirically observed that PINNs encounter difficulties in convergence when dealing with high-order or high-dimensional PDEs, a comprehensive theoretical understanding of this issue remains elusive. This paper offers theoretical support for this pathological behavior by demonstrating that the gradient flow converges in a lower probability when the PDE order is higher. In addition, we show that PINNs struggle to address high-dimensional problems because the influence of dimensionality on convergence is exacerbated with increasing PDE order. To address the pathology, we use the insights garnered to consider variable splitting that decomposes the high-order PDE into a system of lower-order PDEs. We prove that by reducing the differential order, the gradient flow of variable splitting is more likely to converge to the global optimum. Furthermore, we present numerical experiments in support of our theoretical claims. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Understanding of partial differential equations (PDEs) is fundamental in describing diverse phenomena in science and engineering, including fluid dynamics [60, 17], weather prediction [51], disease progression [3, 46], and quantum mechanics [20, 8]. This underscores the imperative necessity for the effective acquisition of their solutions. Given that analytically solving PDEs is often infeasible or even impossible for numerous practical scenarios due to their complexity, numerical methodologies play a pivotal role in approximating solutions to PDEs, enabling researchers and engineers to address real-world problems effectively. ", "page_idx": 0}, {"type": "text", "text": "The advent of deep learning has led to a surge in attempts to leverage it to solve PDEs [59, 42, 30]. Among these, physics-informed neural networks (PINNs) [39, 21, 38, 58] stand out as a prominent methodology. Coupled with the automatic differentiation technique [7], they integrate the residuals of PDEs and boundary conditions into the loss function, thereby enforcing the approximation of solutions using artificial neural networks. This distinctive incorporation of PDEs into the loss function introduces partial differential operators in calculating the loss, distinguishing PINNs from conventional deep learning models. Renowned for their accessibility and versatility in being capable of easily handling arbitrary PDEs and being mesh-free, PINNs have garnered significant attention and demonstrated promising outcomes across various fields [13, 28, 2, 65]. ", "page_idx": 0}, {"type": "text", "text": "Despite their potential, PINNs frequently encounter difficulties in accurately approximating solutions, particularly when the governing PDE contains high-order derivatives [48, 33]. They also exhibit sensitivity to increasing dimensions [33]. These challenges impede the practicality of PINNs due to the pervasiveness of high-order or high-dimensional PDEs in numerous physical and engineering descriptions, such as control problems [22, 57], finance [5, 53], phase separation [12, 27], and mechanical engineering [4, 32]. Several studies have indicated that neural network architectures possess sufficient expressive power to approximate solutions [31, 41]. However, it has been purported that the inferior performance may be attributed to the difficulty in optimizing PINNs, which arises from including the PDE in the loss function [37, 61, 62]. Despite the widespread use of PINNs, a rigorous mathematical understanding of these pathological behaviors of PINNs has been lacking. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we endeavor to provide a mathematical understanding of the pathological behaviors of PINNs by analyzing the convergence of their gradient flow (GF), which reveals a profound sensitivity of the GF convergence with respect to the PDE order and the power of the activation. Building upon the work of Gao et al. [25], we extend the analysis of the GF of PINNs, composed of two-layer multilayer perceptrons (MLPs), to general kth-order PDEs and the $p_{\\|}$ -th power of Rectified Linear Unit (ReLU) activation function with general $p$ . We achieve tighter bounds than those obtained by Gao et al., shedding light on the underlying causes of the pathological behaviors of PINNs. Our theoretical findings demonstrate that the width size of the network necessary for the convergence of the GF increases exponentially with the power $p$ of ReLUpactivation. Furthermore, our results indicate that the optimal power $p$ is determined by the order $k$ of the governing PDE, specifically to be $k+1$ . We also find that the PDE order impedes the convergence of GF, where this negative impact of the PDE order stems from incorporating the PDE into the PINN loss function, which necessitates network differentiation up to the order of the PDE. Moreover, our theoretical investigation unveils that the GF convergence of PINNs also deteriorates with increasing dimensions, and the differential operators included in the PINN loss further exacerbate the sensitivity of PINNs to dimensionality. This elucidates why PINNs are relatively sensitive to dimensionality compared to conventional deep learning models that do not involve differentiation in the loss function. ", "page_idx": 1}, {"type": "text", "text": "To address these challenges, we mathematically demonstrate the efficacy of a variable splitting strategy [54, 55, 6], which represents derivatives of the solution as additional auxiliary variables. The key point of variable splitting is that learning a high-order PDE boils down to learning a system of lower-order PDEs. Reducing the order of derivatives included in the loss function, the strategy alleviates the difficulties associated with the PDE order. It further enables to utilize more general ReLUpactivation with lower power $p$ than PINNs. The lower differential orders that the network computes, the more likely it is that the GF will converge, so the most suitable one among the various constructions of the variable splitting method is the finest splitting, which separates all the derivatives into auxiliary variables and reformulates the PDE into a system of first-order PDEs. This strategy results in a loss function comprising only first-order derivatives, and the efficacy of this finest variable splitting would be magnified as the order of the governing PDE or dimension increases. Therefore, the finest splitting approach would exhibit a pronounced discrepancy from the vanilla PINNs for high-order PDEs. Moreover, a reduction in the differential orders enhances the resilience of the model with respect to dimensionality. Finally, we present numerical experiments to verify our theoretical findings and validate the effectiveness of the variable splitting. ", "page_idx": 1}, {"type": "text", "text": "1.1 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Characterization of Gradient Descent for PINNs As significant issues have been identified within physics-informed machine learning, numerous mathematical studies have been conducted to elucidate the behavior of PINNs. While studies have been mainly dedicated to examining the generalization capacity of PINNs [19, 49, 23], there has also been work on understanding the difficulty of optimization, which is believed to be the primary source of failure for PINNs. Wang et al. [61] found that PINNs exhibit stiff gradient flow dynamics, resulting in imbalanced gradients during training. Ryck et al. [18] characterized the rate of convergence in terms of the conditioning of an operator and suggested that the difficulty of training PINNs is closely related to the conditioning of the differential operators in the governing PDEs. Another work [62] utilized the neural tangent kernel (NTK) theory to indicate that spectral biases and discrepancies between convergence rates of various loss components can lead to training instabilities. Global convergence properties of PINNs for second-order linear PDEs have also been studied within the NTK regime [34] and using the Rademacher complexity [47]. Most closely related to this paper, Gao et al. [25] demonstrated the convergence of the gradient descent for two-layer PINNs. However, their discussion is limited to second-order linear PDEs. We extend the analysis to general kth-order linear PDEs and $p$ -th power of activation functions and provide tighter bounds than Gao et al.. These advances allow us to observe further the effect of the order and dimensionality of the PDE on the convergence. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Variable Splitting The method of separation of variables, which simplifies differential equations by reformulating them into a more manageable system, is a classical method for solving differential equations [9]. In particular, it has been widely employed when dealing with high-order PDEs as augmenting high-order derivatives as additional variables allows the governing equation to be decoupled into a set of lower-order PDEs that are comparatively easy to solve [16, 26, 63]. Recent endeavors have explored the integration of separable variables within the PINN approach. In this paper, we refer to this approach as variable splitting according to [54, 55, 56]. Augmented variables have been introduced to represent vorticity in the Stokes equation [6], the gradient of the solutions for solving the $p$ -Poisson equation [54], and the eikonal equation [55]. Additionally, second-order derivatives have been separately parameterized to solve bi-harmonic equations effectively [45]. The rationale for introducing auxiliary variables in previous works is to enhance the efficiency and accuracy of PINNs, but they lack a comprehensive theoretical elucidation of its effect. A recent study [56] has theoretically analyzed variable splitting, demonstrating that while PINNs do not guarantee convergence to the PDE solution even when the loss converges to zero, variable splitting does ensure convergence to the solution for second-order linear PDEs. In this study, we analyze the impact of variable splitting for PINNs with ReLUpactivation in terms of the convergence of the GF. ", "page_idx": 2}, {"type": "text", "text": "1.2 Main Contributions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The contribution of the paper is summarized as follows. ", "page_idx": 2}, {"type": "text", "text": "\u2022 We analyze that the GF of PINNs with ReLUpactivation converges to the global minimum for general kth-order linear PDEs. This extends the findings of Gao et al. [25] to encompass a broader range of PDEs and activations and provides an even tighter bound. \u2022 We demonstrate the inverse relation between PDE order and the GF convergence, unveiling the adverse effect of the differentials included in the PINN loss on the GF convergence. \u2022 We provide a theoretical understanding of the reasons why PINNs encounter difficulties in addressing high-dimensional problems. \u2022 We prove that the order reduction of variable splitting, which reformulates the PDEs into a system of lower-order PDEs, results in the convergence enhancement of GF. ", "page_idx": 2}, {"type": "text", "text": "2 Mathematical Setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Arbitrary Order Linear PDEs We consider a general form of kth-order linear partial differential equations $(P D E s)$ defined on a bounded domain $\\bar{\\Omega}\\subset\\mathbb{R}^{d}$ (in which the temporal dimension could be a subcomponent) ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\mathcal{N}\\left[u\\right]\\left(\\pmb{x}\\right)=f\\left(\\pmb{x}\\right),\\quad}&{\\pmb{x}\\in\\Omega,}\\\\ {\\mathcal{B}\\left[u\\right]\\left(\\pmb{x}\\right)=g\\left(\\pmb{x}\\right),\\quad}&{\\pmb{x}\\in\\partial\\Omega,}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathcal{N}\\left[u\\right]=\\sum_{|\\alpha|\\le k}a_{\\alpha}\\frac{\\partial^{\\alpha}}{\\partial{\\pmb x}^{\\alpha}}u}\\end{array}$ is a $k$ th-order linear differential operator with coefficient functions $a_{\\alpha}\\;:\\;\\Omega\\;\\rightarrow\\;\\mathbb{R}$ for each multi-index $\\begin{array}{r}{\\alpha\\,\\in\\,\\mathbb{N}_{0}^{d},\\,\\mathcal{B}\\left[u\\right]\\,=\\,\\sum_{|\\alpha|\\le1}\\tilde{a}_{\\alpha}\\frac{\\partial^{\\alpha}}{\\partial{\\pmb x}^{\\alpha}}u}\\end{array}$ represents the boundary condition operator with coefficient functions $\\tilde{a}_{\\alpha}:\\partial\\Omega\\to\\mathbb{R}$ , which could reflect Dirichlet, Neumann, and Robin conditions 2, $f:\\Omega\\to\\mathbb{R}$ is a given source function, and $g:\\partial\\Omega\\to\\mathbb{R}$ is a given boundary function, and $u:\\bar{\\Omega}\\to\\mathbb{R}$ is the unknown solution of interest. ", "page_idx": 2}, {"type": "text", "text": "Physics-InformedNeural Networks Physics-informed neural networks (PINNs) [58] aim to approximate the solution $u$ of the PDE by neural networks. Following the prior work [25], we approximate the solution $u$ by a two-layer multi-layer perceptron $\\phi:\\mathbb{R}^{d}\\stackrel{}{\\rightarrow}\\mathbb{R}$ of width $m$ , defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\phi\\left(\\pmb{x};\\pmb{w},\\pmb{v}\\right)=\\frac{1}{\\sqrt{m}}\\sum_{r=1}^{m}v_{r}\\cdot\\sigma\\left(\\pmb{w}_{r}^{\\top}\\pmb{y}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\pmb{w}_{r}\\,\\in\\,\\mathbb{R}^{d+2}$ $\\begin{array}{r}{\\mathbb{R}^{d+2},\\,v_{r}\\,\\in\\mathbb{R},\\,w\\,=\\,\\bigl[{\\pmb w}_{0}^{\\top},\\cdot\\cdot\\cdot{\\pmb w}_{m}^{\\top}\\bigr]^{\\top}\\,\\in\\mathbb{R}^{m(d+2)}\\,{\\pmb v}\\,=\\,\\bigl[{\\pmb v}_{0}^{\\top},\\cdot\\cdot\\cdot{\\pmb v}_{m}^{\\top}\\bigr]^{\\top}\\,\\in\\mathbb{R}^{m},}\\end{array}$ $\\pmb{y}=$ $\\left[{\\pmb x}^{\\top}\\quad\\frac{1}{2}\\right]^{\\top}\\in\\mathbb R^{d+1}$ , and $\\sigma\\left(\\cdot\\right)$ is the activation function. For brief notations, we assume that $\\Omega$ is bounded so that $\\|\\pmb{y}\\|_{2}\\leq1$ for $\\pmb{x}\\in\\Omega$ . We consider the case where $\\sigma$ is the $\\mathbf{ReLU}^{p}$ activation function for an integer $p$ , which is also known as Rectified Power(RePU) activation [1, 10, 15]. As it will be clear in the context, the power $p$ necessitates surpassing the order $k$ of the PDE (1) to ensure that the loss function and gradient descent flow are well-defined. Therefore, our analysis is focused on scenarios where $p\\geq k+1$ . PINNs learn the parameters of $\\phi$ by minimizing a composite loss function, comprising the residual of the PDE and the boundary condition of (1), which enforces the network\u2019s compliance with the governing physics. For given the training data $\\{\\pmb{x}_{i},f\\,(\\pmb{x}_{i})\\}_{i=1}^{n_{o}}\\subset\\Omega\\times\\mathbb{R}$ and $\\{\\tilde{\\mathbf{\\{x}}}_{j},g\\,(\\tilde{\\mathbf{x}}_{j})\\}_{j=1}^{n_{b}}\\subset\\partial\\Omega\\times\\mathbb{R}$ of respective sizes $n_{o}\\in\\mathbb{N}$ and $n_{b}\\in\\mathbb N$ , PINN loss function is given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{P I N N}\\left(\\pmb{w},\\pmb{v}\\right)=\\frac{1}{2}\\left(\\left\\|\\pmb{s}\\left(\\pmb{w},\\pmb{v}\\right)\\right\\|^{2}+\\left\\|\\pmb{h}\\left(\\pmb{w},\\pmb{v}\\right)\\right\\|^{2}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{s\\left(w,v\\right)=\\left[s_{1}\\left(w,v\\right)\\right.}&{{}\\cdot\\cdot\\cdot s_{n_{o}}\\left(w,v\\right)\\right]^{\\top}\\,\\mathrm{and}\\;h\\left(w,v\\right)=\\left[h_{1}\\left(w,v\\right)\\right.}&{{}\\cdot\\cdot\\cdot h_{n_{b}}\\left(w,v\\right)\\right]^{\\top}\\,\\mathrm{w}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle s_{i}\\left({\\pmb w},{\\pmb v}\\right)=\\sqrt{\\frac{1}{n_{o}}}\\left({\\cal N}\\left[\\phi\\left(\\cdot;{\\pmb w},{\\pmb v}\\right)\\right]\\left({\\pmb x}_{i}\\right)-f\\left({\\pmb x}_{i}\\right)\\right),}}\\\\ {{\\displaystyle h_{j}\\left({\\pmb w},{\\pmb v}\\right)=\\sqrt{\\frac{\\nu}{n_{b}}}\\left({\\cal B}\\left[\\phi\\left(\\cdot;{\\pmb w},{\\pmb v}\\right)\\right]\\left({\\pmb x}_{j}\\right)-g\\left({\\pmb x}_{j}\\right)\\right),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and $\\nu>0$ is a regularization parameter that relatively balances the two components of the loss. ", "page_idx": 3}, {"type": "text", "text": "Gradient Flow As the limiting dynamics of the gradient descent (GD) with infinitesimal step-sizes [40], gradient flow (GF) is continuous time dynamics that starts at ${\\pmb w}\\left(0\\right)$ and ${\\pmb v}\\left(0\\right)$ and evolves as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\{\\frac{d\\pmb{w}_{r}\\left(t\\right)}{d t}\\right.}&{=-\\frac{\\partial\\mathcal{L}_{P I N N}\\left(\\pmb{w},\\pmb{v}\\right)}{\\partial\\pmb{w}_{r}}=-\\sum_{i=1}^{n_{o}}s_{i}\\left(\\pmb{w},\\pmb{v}\\right)\\cdot\\frac{\\partial s_{i}\\left(\\pmb{w},\\pmb{v}\\right)}{\\partial\\pmb{w}_{r}}-\\sum_{j=1}^{n_{b}}h_{j}\\left(\\pmb{w},\\pmb{v}\\right)\\cdot\\frac{\\partial h_{j}\\left(\\pmb{w},\\pmb{v}\\right)}{\\partial\\pmb{w}_{r}},}\\\\ {\\left\\{\\frac{d\\pmb{v}_{r}\\left(t\\right)}{d t}\\right.}&{=-\\frac{\\partial\\mathcal{L}_{P I N N}\\left(\\pmb{w},\\pmb{v}\\right)}{\\partial\\pmb{v}_{r}}=-\\sum_{i=1}^{n_{o}}s_{i}\\left(\\pmb{w},\\pmb{v}\\right)\\cdot\\frac{\\partial s_{i}\\left(\\pmb{w},\\pmb{v}\\right)}{\\partial\\pmb{v}_{r}}-\\sum_{j=1}^{n_{b}}h_{j}\\left(\\pmb{w},\\pmb{v}\\right)\\cdot\\frac{\\partial h_{j}\\left(\\pmb{w},\\pmb{v}\\right)}{\\partial\\pmb{v}_{r}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Initial weights are supposed to follow the normal and uniform distributions, $\\pmb{w}\\left(0\\right)\\sim N\\left(0,I_{m}\\right)$ and $\\pmb{v}\\sim\\b{U}\\left(\\{-1,1\\}\\right)$ , respectively3. GF can be regarded as a continuous-time analog of GD and is frequently employed to comprehend the behavior of GD optimization algorithm in the limit. By the chain rule in conjunction with (6), the following characterizes how the loss function evolves during training by gradient descent: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\frac{d}{d t}\\left[\\mathfrak{s}\\left(w\\left(t\\right),v\\left(t\\right)\\right)\\right]=-\\left(G_{w}\\left(w\\left(t\\right),v\\left(t\\right)\\right)+G_{v}\\left(w\\left(t\\right),v\\left(t\\right)\\right)\\right)\\left[\\mathfrak{s}\\left(w\\left(t\\right),v\\left(t\\right)\\right)\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\boldsymbol{G}$ and $\\tilde{G}$ are Gram matrices for the dynamics, defined by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l r}&{\\tilde{\\boldsymbol{x}}_{w}\\left(w,v\\right)=D_{w}^{\\top}D_{w},D_{w}=\\left[\\frac{\\partial s_{1}}{\\partial w}\\left(w,v\\right)\\quad\\cdots\\quad\\frac{\\partial s_{n_{2}}}{\\partial w}\\left(w,v\\right)\\quad\\frac{\\partial h_{1}}{\\partial w}\\left(w,v\\right)\\quad\\cdots\\quad\\frac{\\partial h_{n_{b}}}{\\partial w}\\left(w,v\\right)\\right]}&{}&{}\\\\ &{\\qquad}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left(8\\right)}\\\\ &{G_{v}\\left(w,v\\right)=D_{v}^{\\top}D_{v},D_{v}=\\left[\\frac{\\partial s_{1}}{\\partial v}\\left(w,v\\right)\\quad\\cdots\\quad\\frac{\\partial s_{n_{a}}}{\\partial v}\\left(w,v\\right)\\quad\\frac{\\partial h_{1}}{\\partial v}\\left(w,v\\right)\\quad\\cdots\\quad\\frac{\\partial h_{n_{b}}}{\\partial v}\\left(w,v\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We are interested in analyzing the effect of the PDE order on the convergence of the PINN loss, which evolved in accordance with the dynamics (7), to the global minimum zero. ", "page_idx": 3}, {"type": "text", "text": "3 Impact of PDE Order on Convergence of PINNs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Despite the demonstrated promise and versatility of PINNs in addressing a wide range of problems [29, 44, 36], they often encounter difficulties in constructing an accurate approximation to the desired solution of PDEs, particularly with high-order PDEs. Moreover, in contrast to the confirmed efficacy of neural networks in modeling high-dimensional data such as images and text, the exploration of PINNs for high-dimensional PDEs has been apparently limited. While neural network architectures possess sufficient expressive power to approximate solutions [11], inferior performance has been attributed to the difficulty in optimization in practice [61, 62]. Additionally, it has been postulated that the optimization difficulty may stem from the partial differential operators included in the loss function [37, 48, 33]. Nevertheless, despite the significant challenge posed by these pathological phenomena, there remains a paucity of theoretical understanding of them. ", "page_idx": 4}, {"type": "text", "text": "In this section, we theoretically elucidate these pathological phenomena by studying the convergence condition of GF (7) of PINN loss (3). Specifically, we provide a width condition for (7) to converge to global optimum in terms of order $k$ , dimension $d$ , and the power $p$ of ReLU activation. Analyzing how those factors are related to the convergence condition, we explain why optimizing PINNs is harder when the order or degree is higher. ", "page_idx": 4}, {"type": "text", "text": "Following [24] and [25], we first prove the positive definiteness of the limiting Gram matrix of PINNs for general $k$ th-order linear PDE and $p$ without any further strict assumption other than $p>k$ . ", "page_idx": 4}, {"type": "text", "text": "Proposition 3.1 (Special Case). The limiting Gram matrix $G_{v}^{\\infty}\\,=\\,\\mathbb{E}_{w,v}\\left[G_{v}\\left(w,v\\right)\\right]$ is strictly positive definite and independent of $m$ . ", "page_idx": 4}, {"type": "text", "text": "This is a special case of the general state in Proposition C.3 with $L=0$ , and the proof for the general case is provided in Appendix C. We denote the smallest eigenvalue of $G_{v}^{\\infty}$ by $\\lambda_{0}>0$ . The following presents our main theorem in this section, the requisite width size $m$ for the GF of PINN loss to converge to the global minimum with high probability. The result demonstrates that the required width grows exponentially as the PDE order $k$ and the dimension of the domain $d$ increase. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.2 (Special Case). There exists a constant $C$ , independent of $d,\\,k_{i}$ , and $p_{i}$ , such that for any $\\delta\\ll1$ , if ", "page_idx": 4}, {"type": "equation", "text": "$$\nm>C{\\binom{d+k}{d}}^{14}p^{7k+4}2^{6p}\\left(\\log{\\frac{m d}{\\delta}}\\right)^{4p}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "then with probability of at least $1-\\delta$ over the initialization, we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{P I N N}\\left(\\pmb{w}\\left(t\\right),\\pmb{v}\\left(t\\right)\\right)\\leq\\exp\\left(-\\lambda_{0}t\\right)\\mathcal{L}_{P I N N}\\left(\\pmb{w}\\left(0\\right),\\pmb{v}\\left(0\\right)\\right),\\;\\forall t\\geq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "It is a special case of Theorem 4.3 with $L=0$ , the proof of which can be found in Appendix C.1. It extends, inspired by [25], the convergence of the GF of PINNs of second-order linear PDEs to kth-order linear PDEs and the general $p$ -th power of ReLU. It states that even in these general settings, the GF of PINNs converges to the global minimum with a high probability when the width of the network is sufficiently large. Moreover, we obtain a polylogarithmic bound $\\left(\\log{\\frac{1}{\\delta}}\\right)^{4p}$ , which is much tighter than polynomial bound $\\delta^{-3}$ in [25] for $p=3$ . These improvements permit the derivation of the following valuable explanations for the deficiencies observed when optimizing PINNs. ", "page_idx": 4}, {"type": "text", "text": "Optimal Power of ReLU Function in Training Theorem 3.2 sheds light on the suitable choice of activation function for PINNs. In the training process, the activation function plays an important role. However, there are no clues as to which activation function is favorable to the given optimization process. Especially in the case of PINNs, it depends heavily on the PDE at hand. Despite its pervasive use in deep learning due to its numerous advantages and performance benefits, the ReLU activation function is not admissible in the PINN framework, which necessitates the activation function to provide high-order derivatives for optimizing PDE-based constraints. Instead, PINNs harness the $p$ -th power of ReLU as the activation function. It is apparent that $p$ must satisfy $p\\geq k+1$ for the PINN loss and gradient descent to be computed. Theorem 3.2 indicates that the smaller $p$ is, the more likely the gradient descent will converge; that is, it is most optimal4 to adjust $p$ to $k+1$ regarding the training process. ", "page_idx": 4}, {"type": "text", "text": "Understanding Difficulty in High-order PDEs A significant observation of Theorem 3.2 is that it provides a theoretical understanding of why PINNs struggle with high-order PDEs. From (10), we can see that the bound increases exponentially with the order of the PDE. Moreover, for the GF of PINNs to converge with high probability, that is, $\\delta\\ll1$ , a small increment of the power $p$ would contribute to non-negligible degradation in the convergence, which could ultimately prevent the network from reaching a minimizer of the loss. Hence, given that $k$ determines the admissible $p$ by $p\\geq k+1$ , the order $k$ of PDE primarily influences the convergence of PINNs and increasing the exponential term in (10). ", "page_idx": 5}, {"type": "text", "text": "Understanding Difficulty in High-dimensional Problems The above theorem, which shows that the lower bound of $m$ depends on the exponential of $d$ , explains why PINNs cannot completely combat the curse of dimensionality. As PINNs are regarded as a versatile method capable of being mesh-free, they have been expected to be free from the curse of dimensionality [58]. However, the GF of PINNs becomes harder to converge as $d$ increases, requiring the network to be wider. Furthermore, it can be observed that the magnitude of change in $d$ is amplified with respect to the exponent of $k$ . This explains why PINNs are relatively sensitive to increasing dimensionality in comparison to other deep learning models whose loss functions do not contain derivatives. In other words, the presence of derivatives in the loss makes PINNs sensitive to changes in dimensionality, and the larger $k$ is, the more difficult PINNs are for high-dimensionality. ", "page_idx": 5}, {"type": "text", "text": "Combining all crucial observations from our main theorem, we believe that the impact of the PDE order is one of the primary underlying reasons why PINNs often fail to minimize their loss. In light of this theoretical evidence, the next section describes a variable splitting strategy that addresses these pathologies by properly reducing the differential order in the PINN loss function. ", "page_idx": 5}, {"type": "text", "text": "4 Order Reduction through Variable Splitting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The previous section indicates that the PDE order $k$ significantly affects the width requirement for the GF to converge. Concurrently, for kth-order PDEs, it is necessary to increase the ReLU activation to at least the $k+1$ power in order to ensure a well-defined GF for the PINN loss. Consequently, lowering $k$ could potentially lead to better convergence of the GF. In this section, we introduce variable splitting strategy to decrease the differential order by reformulating the given PDE into a system of lower-order PDEs. We then extend Theorem 3.2 to a more general form in Theorem 4.3. ", "page_idx": 5}, {"type": "text", "text": "4.1 Variable Splitting ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The concept of variable splitting [54, 55, 56] is to rewrite a higher-order PDE into a lower-order system, after which the PINN approach is applied to the system. A crucial aspect of the success of such methods is the reduction of the derivative order present in the training loss function. ", "page_idx": 5}, {"type": "text", "text": "Augment Variables For $L\\geq0$ and increasing integers $0=\\xi_{0}<\\xi_{1}<\\cdot\\cdot<\\xi_{L+1}=k.$ , variable   \nsplitting augment the derivatives of the solution\u2202\u2202x\u03be\u03be11 $\\textstyle{\\frac{\\partial^{\\xi_{1}}}{\\partial\\pmb{x}^{\\xi_{1}}}}u,\\ldots,{\\frac{\\partial^{\\xi_{L}}}{\\partial\\pmb{x}^{\\xi_{L}}}}u$ as additional auxiliary variables   \n$\\phi_{1},...,\\phi_{L}$ , respectively. For notational simplicity, we abbreviate the integer set $\\{1,\\cdot\\cdot\\cdot,m\\}$ for   \nam uplotis-iitinvdee ix $m$ $[m]$ $\\ell\\,\\in\\,[L]$ scihz et $\\frac{\\partial^{\\xi_{\\ell}}}{\\partial{\\pmb x}^{\\xi_{\\ell}}}$ .o nTdhse troef $\\frac{\\partial^{\\xi_{\\ell}}}{\\partial x_{1}^{\\alpha_{1}}\\cdots\\partial x_{d}^{\\alpha_{d}}}u$ cotro ra$\\alpha=(\\alpha_{1},\\dots,\\alpha_{d})\\in\\mathbb{N}_{0}^{d}$ $\\begin{array}{r}{|\\alpha|=\\sum_{i=1}^{d}\\alpha_{i}=n}\\end{array}$ $\\phi_{\\ell}$ $|I_{\\xi_{\\ell}}|$ $I$ $\\phi_{\\ell}$ corresponds to $\\frac{\\partial^{\\xi_{\\ell}}}{\\partial{\\pmb x}^{\\alpha}}$ by $\\left(\\phi_{\\ell}\\right)_{\\alpha}$ . ", "page_idx": 5}, {"type": "text", "text": "Reformulate PDE into Lower-order System By replacing each of the differential terms $\\begin{array}{r}{\\frac{\\partial^{\\xi\\ell}}{\\partial\\mathbf{x}^{\\xi\\ell}}u}\\end{array}$ with the corresponding auxiliary variables $\\phi_{\\ell}$ , the differential operator $\\mathcal{N}$ in (1) can be rewritten as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{N}\\left[u\\right]=\\sum_{\\left|\\alpha\\right|\\leq k}a_{\\alpha}\\frac{\\partial^{\\left|\\alpha\\right|}}{\\partial\\pmb{x}^{\\alpha}}u=\\sum_{\\ell=0}^{L}\\sum_{\\left|\\alpha\\right|\\leq\\xi_{\\ell}}\\sum_{\\left|\\beta\\right|\\leq\\Delta\\xi_{\\ell+1}}\\hat{a}_{\\ell,\\alpha,\\beta}\\frac{\\partial^{\\Delta\\xi_{\\ell+1}}}{\\partial\\pmb{x}^{\\beta}}\\left(\\phi_{\\ell}\\right)_{\\alpha},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "not that a network can be easily trained. Indeed, our result leads us to the opposite conclusion that large $p$ could be detrimental to convergence from the optimization perspective. In this paper, we refer to \u2018optimal\u2019 as the sense of being likely trained under the mildest condition rather than approximating the solution with the smallest error. ", "page_idx": 5}, {"type": "text", "text": "for some coefficient functions $\\hat{a}_{\\ell,\\alpha,\\beta}:\\Omega\\rightarrow\\mathbb{R}$ and $\\Delta\\xi_{\\ell}=\\xi_{\\ell}-\\xi_{\\ell-1}$ . Since $\\phi_{\\ell}$ represents a function that differentiates the PDE solution $\\Delta\\xi_{\\ell}$ -times more than $\\phi_{\\ell-1}$ , the components of two consecutive variables $\\phi_{\\ell-1}$ and $\\phi_{\\ell}$ are governed by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{\\partial^{\\Delta\\xi_{\\ell}}}{\\partial{\\pmb x}^{\\beta}}\\left(\\phi_{\\ell-1}\\right)_{\\alpha}\\left({\\pmb x}\\right)=\\left(\\phi_{\\ell}\\right)_{\\alpha+\\beta}\\left({\\pmb x}\\right),\\;\\alpha\\in I_{\\xi_{\\ell-1}},\\;\\beta\\in I_{\\Delta\\xi_{\\ell}}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "From these, the PDE (1) can be identically reformulated by the system of lower-order PDEs: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\hat{\\mathcal{N}}\\left[\\phi_{0},\\cdots,\\phi_{L}\\right]\\left(x\\right)=f\\left(x\\right),}&{x\\in\\Omega,}\\\\ {\\frac{\\partial^{\\beta}}{\\partial x^{\\beta}}\\left(\\phi_{\\ell-1}\\right)_{\\alpha}\\left(x\\right)=\\left(\\phi_{\\ell}\\right)_{\\alpha+\\beta}\\left(x\\right),}&{x\\in\\Omega,\\;\\ell\\in\\left[L\\right],\\;\\alpha\\in I_{\\xi_{\\ell-1}},\\;\\beta\\in I_{\\Delta\\xi_{\\ell}},}\\\\ {\\beta\\left[\\phi_{0}\\right]\\left(x\\right)=g,}&{x\\in\\partial\\Omega.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "It is of paramount importance to note that the maximum differential order of this system of PDEs is the highest difference of derivative order between consecutive auxiliary variables $|\\xi|=$ max $\\{\\Delta\\xi_{\\ell}:\\ell\\in[L+1]\\}$ , which is less than $k$ . This aspect gives rise to notable ramifications in our analysis of VS-PINNs, which will be discussed in the next subsection. ", "page_idx": 6}, {"type": "text", "text": "Variable Splitting for PINNs In this paper, we consider the parameterization of all variables $\\phi_{\\ell}$ with two-layer MLPs with $\\mathbf{ReLU}^{p}$ activation function, in a manner analogous to that described in Section 2 for PINNs. The weights in the first and second layers of $\\phi_{\\ell}$ are denoted by $\\mathbf{\\nabla}w_{\\ell}$ and $\\pmb{v}_{\\ell}$ , respectively.We use $\\pmb{w}=\\left[\\pmb{w}_{1}^{\\top}\\cdot\\cdot\\cdot\\pmb{w}_{L}^{\\top}\\right]^{\\top}$ and $\\pmb{v}=\\left[\\pmb{v}_{1}^{\\top}\\cdots\\pmb{v}_{L}^{\\top}\\right]^{\\top}$ to refer to the respective collections of all weights. Similar to PINNs, Variable Splitting for PINNs (VS-PINNs) employ the linear sum of penalized residuals of each term of the induced system of PDEs (14) as the training loss: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{P I N N}^{V S}\\left(\\pmb{w},\\pmb{v}\\right)=\\displaystyle\\frac{1}{2}\\left(\\frac{1}{n_{o}}\\sum_{i=1}^{n_{o}}\\left(\\hat{N}\\left[\\phi_{0},\\pmb{\\ldots},\\phi_{L}\\right]\\left(\\pmb{x}_{i}\\right)-\\pmb{f}\\left(\\pmb{x}_{i}\\right)\\right)^{2}\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left.+\\left.\\frac{\\nu_{\\ell}}{n_{o}}\\sum_{\\ell=1}^{L}\\displaystyle\\sum_{|\\alpha|\\leq\\xi_{\\ell}}\\sum_{|\\beta|\\leq\\Delta\\xi_{\\ell+1}}\\left(\\frac{\\partial^{\\beta}}{\\partial\\pmb{x}^{\\beta}}\\left(\\phi_{\\ell-1}\\right)_{\\alpha}\\left(\\pmb{x}_{i}\\right)-\\left(\\phi_{\\ell}\\right)_{\\alpha+\\beta}\\left(\\pmb{x}_{i}\\right)\\right)^{2}\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left.+\\left.\\frac{\\nu}{n_{b}}\\left(B\\left[\\phi_{0}\\right]\\left(\\pmb{\\tilde{x}}_{j}\\right)-g\\left(\\pmb{\\tilde{x}}_{j}\\right)\\right)^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\nu,\\nu_{1},\\dots,\\nu_{L}$ are regularization parameters. As the GF of $\\mathcal{L}_{P I N N}$ is characterized by Gram matrices $G_{w}$ and $G_{v}$ induced from the gradients of the residuals of each term in (1), the GF of $\\mathcal{L}_{P I N N}^{V S}$ is characterized by Gram matrices $\\widehat{G}_{w}$ and $\\hat{G}_{v}$ , which is induced from the gradients of the residuals of each term in (14). Appendix A gives more details for $\\widehat{G}_{w}$ and $\\hat{G}_{v}$ . ", "page_idx": 6}, {"type": "text", "text": "Remark 4.1. In order for high-order PDEs with $k>2$ to be well-posed, it is necessary to have more boundary conditions than those defined by the boundary operator $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ in (1). Although our analysis concentrated on $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ that reflect only up to first-order derivatives for the sake of simplicity, our theory can also be applied to more general boundary conditions. Furthermore, the high-order boundary conditions $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ can also be reformulated using the auxiliary variables used for $\\hat{\\mathcal{N}}$ . In that case, relations (13) should hold on the boundary $\\pmb{x}\\in\\partial\\Omega$ . As the reduced system (14) with reformulated boundary condition is equivalent to (1), instability issues were not observed in our numerical experiments even in the absence of artificial boundary conditions on the auxiliary variables unlike to grid-based conventional numerical schemes. ", "page_idx": 6}, {"type": "text", "text": "4.2 Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "A key advantage of VS-PINNs is that the derivative order of the induced system of PDEs (14) is $|\\xi|$ , which is lower than that of the original PDE (1). We prove its effectiveness in this section. As analogous to PINNs, we begin by proving the positive definiteness of the limiting Gram matrix, providing its proof in Appendix C. ", "page_idx": 6}, {"type": "text", "text": "Proposition 4.2 (General Case). The limiting Gram matrix $\\widehat{\\boldsymbol{G}}_{v}^{\\infty}=\\mathbb{E}_{w,v}\\left[\\widehat{\\boldsymbol{G}}_{v}\\left(\\boldsymbol{w},\\boldsymbol{v}\\right)\\right]$ is strictly positive definite and independent of $m$ . ", "page_idx": 6}, {"type": "text", "text": "We denote the smallest eigenvalue of $\\widehat{G}_{v}^{\\infty}$ by $\\lambda_{0}>0$ . We now present our main theorem, which demonstrates the profound impact of or der reduction in variable splitting. The proof of the following theorem can be found in C.1. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.3 (General Case). There exists a constant $C$ , independent of $d,\\,k,\\,|\\xi|,$ , and $p$ , such that for any $\\delta\\ll1$ , $i f$ ", "page_idx": 7}, {"type": "equation", "text": "$$\nm>C{\\binom{d+k}{d}}^{6}{\\binom{d+|\\xi|}{d}}^{8}p^{7|\\xi|+4}2^{6p}\\left(\\log{\\frac{m d}{\\delta}}\\right)^{4p},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "then with probability of at least $1-\\delta$ over the initialization, we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{L}_{P I N N}^{V S}\\left(\\pmb{w}\\left(t\\right),\\pmb{v}\\left(t\\right)\\right)\\leq\\exp\\left(-\\lambda_{0}t\\right)\\mathcal{L}_{P I N N}^{V S}\\left(\\pmb{w}\\left(0\\right),\\pmb{v}\\left(0\\right)\\right),\\;\\forall t\\geq0.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The right-hand-side of (16) grows exponentially with respect to $d,k$ , and $p$ , thereby indicating the substantial influence of these factors on the convergence of VS-PINNs, including PINNs as a specific case $L=0$ ). This analysis reveals several significant advantages of VS-PINNs: ", "page_idx": 7}, {"type": "text", "text": "Improved Convergence: VS-PINNs are more likely to converge to the global optimum than PINNs due to the reduction in the derivative order $|\\xi|<k$ . This also relaxes the condition on $p$ from $p\\geq k\\!+\\!1$ to $p\\geq|\\xi|+1$ . As previously discussed in Section 3, the optimal value of $p$ is $|\\xi|+1$ . Given that $\\hat{\\sf{O}}\\left(\\|\\boldsymbol{\\xi}\\|+1\\right)$ is an exponent of $\\log\\left(1/\\delta\\right)$ , the most dominant term, reducing the order from $k$ to $|\\xi|$ leads to an immense improvement. There is another noteworthy observation we can see here. Given a $k$ th-order PDE, there are numerous possible partitions $\\xi$ that could be employed to decompose it to a system of lower-order PDEs. Consequently, there are a many of potential VS-PINNs that could be constructed. The aforementioned result indicates which of these is the most effective. As the convergence improves dramatically with a reduction in the derivative order, the optimal approach for splitting variables among various ways is to separate the given PDE into a system of first-order PDEs by parameterizing all derivatives of the solution as auxiliary variables. In other words, the finest splitting with $\\xi_{0}=0,\\xi_{1}=1,\\ldots,\\xi_{k-1}=k-1$ would be the most effective in terms of the convergence of GF, as the differential order $|\\xi|$ is reduced the most to 1. Taken all together, the most optimal VS-PINNs that reduce the PDE order $k$ to 1 will markedly enhance the convergence of GF. ", "page_idx": 7}, {"type": "text", "text": "Reduced Dimensional Impact: The reduction of orders in VS-PINNs enhances the resilience of the model to high-dimensionality. From Theorem 3.2, we observed the effect of $d$ being exponentially enlarged for the PDE order $k$ due to the $k$ th-order partial differential operators in the loss function. It can be alleviated by VS-PINNs reducing the order, thereby easing the amplified scale to exponential of $|\\xi|$ . This indicates that VS-PINNs are more effective in combating the curse of dimensionality. Since the curse of dimensionality is a serious issue that is prevalent in various fields, including Hamilton-Jacobi-Bellman equation in control problems, Schrodinger equation in quantum physics, and Black-Scholes equation in finance, it is evident that enhancements to the robustness of VS-PINNs with respect to their dimensionality would facilitate considerable advancements in various fields. ", "page_idx": 7}, {"type": "text", "text": "Memory Efficiency: VS-PINNs are memory-efficient despite the presence of multiple auxiliary networks. As the order of the derivative increases, the complexity in automatic differentiation in modern deep-learning frameworks like PyTorch increases and it becomes computationally expensive [7]. Adopting the order-reduced representation in the proposed variable splitting can overcome the difficulty in calculating the high-order derivative via automatic differentiation. The loss function for the finest VS-PINNs involves only first-order derivatives, which reduces the memory usage and computational requirements. Despite the increase in the number of networks, VS-PINNs exhibit greater efficiency because memory usage and computation scale linearly with the number of networks in contrast to the exponential scaling with the order of derivatives. Table 3 in appendix demonstrates the memory reduction of VS-PINNs. ", "page_idx": 7}, {"type": "text", "text": "Remark 4.4. The current approach to parameterizing the $\\xi_{\\ell}$ -th order differential operator on all axes $\\nabla^{\\xi_{\\ell}}$ as an auxiliary variable may be suboptimal in certain cases. In a given PDE, if the order of the derivative varies significantly along the axes, that is, $a_{\\alpha}\\neq0$ for only a few $\\alpha$ in (1), it may be more efficient to approximate the partial derivatives using auxiliary variables separately for each axis. To illustrate, for the PDE $u_{t t}=u_{x x x x}$ , it is more suitable to parameterize variables $\\phi_{0}\\approx u$ , $\\phi_{1}\\approx(u_{t},u_{x})$ , $\\phi_{2}\\approx u_{x x}$ , and $\\phi_{3}\\approx u_{x x x}$ , rather than approximating all tensors $\\nabla_{(t,x)}^{1},\\nabla_{(t,x)}^{2}$ \u2207(t,x), and $\\nabla_{(t,x)}^{3}$ . The theoretical framework presented in this paper is capable of addressing this scenario by constructing each $\\phi_{\\ell}$ to replace $\\frac{\\partial^{\\xi_{\\ell}}}{\\partial{\\pmb x}^{\\alpha}}$ for only part of $\\alpha$ with $|\\alpha|=\\xi_{\\ell}$ . However, we exclude it due to the intricate nature of the states and the lack of a meaningful impact on the PDE order. ", "page_idx": 7}, {"type": "image", "img_path": "8K6ul0hgtC/tmp/9a97790bd6451147fc36d9ef4bdf79e918048edc563dbf9a20ecf1dbc19e55af.jpg", "img_caption": ["Figure 1: Training losses of PINNs solving (a) bi-harmonic equation and (b) Poisson equation. ", "(b) Convergence behavior of PINNs on Poisson equation "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Remark 4.5. Although the shaprness of the bound in Theorem 4.3 is open, it is important to note that the leading term of the bound is based on conditions necessary for the Gram matrix to be positive definite, which is a crucial property of the Gram matrix for ensuring the convergence of the GF to a global optimizer. Since the Gram matrix is defined by the PDE loss and the network structure, we believe it can still provide valuable insight into how order and power affect convergence. ", "page_idx": 8}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "This section presents experimental results that validate the theory. Throughout numerical experiments, two-layer MLPs with ReLUpactivation function were utilized in order to align with our theoretical framework. Throughout all experiments, the training collocation points consists of uniform grid and regularization parameters are set to $\\nu_{1},\\dots,\\nu_{L}=1$ and $\\nu=10$ . We implement all numerical experiments on a single NVIDIA RTX 3090 GPU. Experimental details are provided in Appendix D. ", "page_idx": 8}, {"type": "text", "text": "Convergence behavior of PINNs To investigate the influence of the activation order $p$ and the PDE order $k$ on the width $m$ required for convergence, we examined both the second-order Poisson equation and the fourth-order bi-harmonic equation, both of which yield the same solution. We trained networks with varying widths m, ranging from $10^{2}$ to $10^{6}$ , for each combination of $p$ and $k$ using GD optimization with a learning rate of $\\bar{10}^{-8}$ . Figure 1 illustrates the training losses at the initial stage on a logarithmic scale, supporting our theoretical findings that a larger width is needed for higher values of $p$ to ensure convergence. Moreover, we can observe that narrower networks tend to converge more readily when solving lower-order PDEs (Poisson) compared to higher-order PDEs (bi-harmonic). This observation aligns with Theorem 4.3 that higher-order PDEs necessitate larger network widths for guaranteed convergence. ", "page_idx": 8}, {"type": "text", "text": "Validation on the effect of $p$ To verify the influence of the power $p$ of the ReLU activation function, we test PINNs with varying $p$ values between 3 and 10. Since the training process became highly unstable as $p$ increases, we consider second-order heat equation (509) [14] to gain a more precise investigation of the effect of $p$ . The results are summarized in Figure 2 (a). We can see that the convergence of loss is enhanced as $p$ decreases, which supports our theoretical finding. ", "page_idx": 8}, {"type": "text", "text": "Comparison between PINNs and VS-PINNs To validate the order reduction effect of VS-PINNs, we conducted an experiment comparing PINNs with VS-PINNs on the second-order heat equation. Each model was run five times with different random seeds, and Figure 2 depicts the training loss for both PINNs and VS-PINNs along with their variance. The results show that the training loss for VS-PINNs converges more effectively than that of PINNs. This indicates that VS-PINNs, which optimize a loss function incorporating lower-order derivatives using networks with smaller $p$ , facilitate convergence of GD, consistent with the theoretical findings in Section 4. Furthermore, we performed a similar experiment on the convection-diffusion equation (511) in the Appendix E and obtained results that were consistent with those observed for the heat equation. ", "page_idx": 8}, {"type": "text", "text": "Effect of splitting level For higher-order PDEs, there are several ways to transform a given PDE into a lower-order system through variable splitting. To investigate this effect, we conducted experiments on the fourth-order elastic beam equation (510) [52] with two cases: (i) $\\phi_{0}\\approx u,\\phi_{1}\\approx u_{t},\\phi_{2}\\approx u_{x x}$ with $|\\xi|=2$ and $p=3$ and (ii) the finest splitting of $\\phi_{0}\\approx u,\\phi_{1}\\approx\\nabla u,\\phi_{2}\\approx u_{x x},\\phi_{3}\\approx u_{x x x}$ with $|\\xi|=1$ and $p=2$ . In order to train PINNs for a fourth-order PDE, $p$ should be at least five, but training such PINNs with GD does not proceed properly, as illustrated in Figure 1. Consequently, the experiments were conducted using the Adam optimizer. In contrast to the underperforming PINNs, VS-PINNs are effectively trained even with GD, as illustrated in Figure 6 of the Appendix E. We run each model five times with different random seeds, and Figure 2 (b) depicts the training loss of PINN and two VS-PINNs with variance. The results show that the model with a lower PDE order $k$ and a smaller power $p$ of the activation exhibits a more pronounced reduction in the loss function, in accordance with our theoretical findings. Furthermore, it can be observed that the variance of the training loss is significantly smaller for the models with smaller values of $k$ and $p$ . This indicates that the learning process is much more stable for a smaller $k$ and $p$ . We also conduct numerical studies on the fourth-order bi-harmonic equation (508). However, the results exhibit a similar trend to that observed in the beam equation and are therefore presented in Appendix E. ", "page_idx": 8}, {"type": "image", "img_path": "8K6ul0hgtC/tmp/5183cd00983466a42166ce34a738a73666b5742516aabcab75f76778a927b279.jpg", "img_caption": ["Figure 2: Loss curves of (a) effect of the power $p$ of ReLUpand (b) comparison between PINNs with VS-PINNs. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we proved that the gradient flow of PINNs converges to a global minimum and provides sufficient width for this convergence. It extends the results in [25] to general PDEs and activation functions and provides even tighter conditions on the width size. The main theorem demonstrates that the PDE order or dimension exponentially increases the width requirement, theoretically indicating that PINNs are challenging to optimize for high-order or high-dimensional PDEs. We also substantiate that the PDE order amplifies the adverse effects of dimensionality, which explains why PINNs are more susceptible to dimensionality than other deep learning losses without differentiation. Furthermore, We showed that the variable splitting strategy improves convergence by reducing the differential order included in the training loss function. ", "page_idx": 9}, {"type": "text", "text": "It is acknowledged that we only provided sufficient conditions for convergence. To fully comprehend the role of these factors in optimizing PINNs, it is also necessary to establish the necessary conditions linking PDE order, dimension, and convergence. Given that the primary goal of PINNs is to approximate the solution of PDEs, it could also be a limitation that all discussions were limited to empirical losses with fixed collocation points. It would therefore be a worthwhile future direction to analyze the conditions under which the expected loss converges when training collocation points are randomly sampled per epoch. Extending our theoretical framework to analyze the impact of the variable splitting strategy on the generalization error of PINNs, as suggested in [64], would also be an interesting and important research direction. Moreover, as our analysis was confined to continuous time flows, a comprehensive understanding of gradient descent would necessitate the analysis of discretized flows, since GF and GD have different dynamics [50]. We expect that our theory could be adapted to GD dynamics by using Theorem 3.3 of [50], which treats GD as GF with a counter term, but we leave it for future work. In a practical context, the convergence of PINNs for adaptive optimizers, such as Adam [35] or L-BFGS [43], and other activation functions, including hyperbolic tangent, remains an open question. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported by the NRF grant [2021R1A2C3010887, RS-2024-00406127, RS-2024- 00343226, RS-2024-00421203] and MSIT/IITP[NO.2021-0-01343Artificial Intelligence Graduate School Program(SNU)]. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Ahmed Abdeljawad and Philipp Grohs. Integral representations of shallow neural network with rectified power unit activation function. Neural Networks, 155:536\u2013550, 2022.   \n[2] Laith Alzubaidi, Jinshuai Bai, Aiman Al-Sabaawi, Jose Santamar\u00eda, Ahmed Shihab Albahri, Bashar Sami Nayyef Al-dabbagh, Mohammed A Fadhel, Mohamed Manoufali, Jinglan Zhang, Ali H Al-Timemy, et al. A survey on deep learning tools dealing with data scarcity: definitions, challenges, solutions, tips, and applications. Journal of Big Data, 10(1):46, 2023.   \n[3] Abdon Atangana and Ernestine Alabaraoye. Solving a system of fractional partial differential equations arising in the model of hiv infection of $\\mathrm{cd4+}$ cells and attractor one-dimensional Keller-Segel equations. Advances in Difference Equations, 2013:1\u201314, 2013.   \n[4] Roberto Ballarini. The Da Vinci-Euler-Bernoulli Beam theory? Mechanical Engineering Magazine Online, 2003.   \n[5] Guy Barles and Halil Mete Soner. Option pricing with transaction costs and a nonlinear black-scholes equation. Finance and Stochastics, 2:369\u2013397, 1998.   \n[6] Shamsulhaq Basir. Investigating and mitigating failure modes in physics-informed neural networks (PINNs). Communications in Computational Physics, 33(5):1240\u20131269, 2023.   \n[7] Atilim Gunes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Automatic differentiation in machine learning: a survey. Journal of machine learning research, 18(153):1\u201343, 2018.   \n[8] Feliks Aleksandrovich Berezin and Mikhail Shubin. The Schr\u00f6dinger Equation, volume 66. Springer Science & Business Media, 2012.   \n[9] Jiang Bo-Nan and CL Chang. Least-squares finite elements for the stokes problem. Computer Methods in Applied Mechanics and Engineering, 78(3):297\u2013311, 1990.   \n[10] Laxman Bokati, Vladik Kreinovich, Joseph Baca, and Natasha Rovelli. Why rectified power (repu) activation functions are efficient in deep learning: A theoretical explanation. In Uncertainty, Constraints, and Decision Making, pages 7\u201313. Springer, 2023.   \n[11] Kurt Izak M. Cabanilla, Rhudaina Z. Mohammad, and Jose Ernie C. Lope. Neural networks with relu powers need less depth. Neural Networks, 172:106073, 2024.   \n[12] John W Cahn and John E Hilliard. Free energy of a nonuniform system. i. interfacial free energy. The Journal of chemical physics, 28(2):258\u2013267, 1958.   \n[13] Shengze Cai, Zhiping Mao, Zhicheng Wang, Minglang Yin, and George Em Karniadakis. Physics-informed neural networks (PINNs) for fluid mechanics: A review. Acta Mechanica Sinica, 37(12):1727\u20131738, 2021.   \n[14] John Rozier Cannon. The one-dimensional heat equation. Number 23. Cambridge University Press, 1984.   \n[15] Charles K Chui, Xin Li, and Hrushikesh Narhar Mhaskar. Neural networks for localized approximation. mathematics of computation, 63(208):607\u2013623, 1994.   \n[16] Bernardo Cockburn and Chi-Wang Shu. The local discontinuous galerkin method for timedependent convection-diffusion systems. SIAM journal on numerical analysis, 35(6):2440\u20132463, 1998.   \n[17] Peter Constantin. Euler equations, Navier-Stokes equations and turbulence. In Mathematical Foundation of Turbulent Viscous Flows: Lectures given at the CIME Summer School held in Martina Franca, Italy, SEptember 1-5, 2003, pages 1\u201343. Springer, 2005.   \n[18] Tim De Ryck, Florent Bonnet, Siddhartha Mishra, and Emmanuel de B\u00e9zenac. An operator preconditioning perspective on training in physics-informed machine learning. arXiv preprint arXiv:2310.05801, 2023.   \n[19] Tim De Ryck and Siddhartha Mishra. Error analysis for physics-informed neural networks (pinns) approximating kolmogorov pdes. Advances in Computational Mathematics, 48(6):79, 2022.   \n[20] Lokenath Debnath and Lokenath Debnath. Nonlinear partial differential equations for scientists and engineers. Springer, 2005.   \n[21] MWMG Dissanayake and Nhan Phan-Thien. Neural-network-based approximations for solving partial differential equations. communications in Numerical Methods in Engineering, 10(3):195\u2013 201, 1994.   \n[22] E Kirk Donald et al. Optimal control theory: an introduction. Mineola, NY: Dover Publications, Inc, 1970.   \n[23] Nathan Doum\u00e8che, G\u00e9rard Biau, and Claire Boyer. Convergence and error analysis of pinns. arXiv preprint arXiv:2305.01240, 2023.   \n[24] Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018.   \n[25] Yihang Gao, Yiqi Gu, and Michael Ng. Gradient descent finds the global optima of two-layer physics-informed neural networks. In International Conference on Machine Learning, pages 10676\u201310707. PMLR, 2023.   \n[26] Roland Glowinski, Hao Liu, Shingyu Leung, and Jianliang Qian. A finite element/operatorsplitting method for the numerical solution of the two dimensional elliptic monge\u2013amp\u00e8re equation. Journal of Scientific Computing, 79:1\u201347, 2019.   \n[27] John B Greer, Andrea L Bertozzi, and Guillermo Sapiro. Fourth order partial differential equations on general geometries. Journal of Computational Physics, 216(1):216\u2013246, 2006.   \n[28] Ehsan Haghighat, Maziar Raissi, Adrian Moure, Hector Gomez, and Ruben Juanes. A physicsinformed deep learning framework for inversion and surrogate modeling in solid mechanics. Computer Methods in Applied Mechanics and Engineering, 379:113741, 2021.   \n[29] QiZhi He, David Barajas-Solano, Guzel Tartakovsky, and Alexandre M Tartakovsky. Physicsinformed neural networks for multiphysics data assimilation with application to subsurface transport. Advances in Water Resources, 141:103610, 2020.   \n[30] Masanobu Horie and Naoto Mitsume. Physics-embedded neural networks: Graph neural pde solvers with mixed boundary conditions. Advances in Neural Information Processing Systems, 35:23218\u201323229, 2022.   \n[31] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks. Neural networks, 3(5):551\u2013560, 1990.   \n[32] K Hosseini, K Sadri, M Mirzazadeh, YM Chu, A Ahmadian, BA Pansera, and S Salahshour. A high-order nonlinear schr\u00f6dinger equation with the weak non-local nonlinearity and its optical solitons. Results in Physics, 23:104035, 2021.   \n[33] Zheyuan Hu, Zekun Shi, George Em Karniadakis, and Kenji Kawaguchi. Hutchinson trace estimation for high-dimensional and high-order physics-informed neural networks. Computer Methods in Applied Mechanics and Engineering, 424:116883, 2024.   \n[34] Deqing Jiang, Justin Sirignano, and Samuel N Cohen. Global convergence of deep galerkin and pinns methods for solving partial differential equations. arXiv preprint arXiv:2305.06000, 2023.   \n[35] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[36] Georgios Kissas, Yibo Yang, Eileen Hwuang, Walter R Witschey, John A Detre, and Paris Perdikaris. Machine learning in cardiovascular flows modeling: Predicting arterial blood pressure from non-invasive 4d flow mri data using physics-informed neural networks. Computer Methods in Applied Mechanics and Engineering, 358:112623, 2020.   \n[37] Aditi Krishnapriyan, Amir Gholami, Shandian Zhe, Robert Kirby, and Michael W Mahoney. Characterizing possible failure modes in physics-informed neural networks. Advances in Neural Information Processing Systems, 34:26548\u201326560, 2021.   \n[38] Isaac E Lagaris, Aristidis Likas, and Dimitrios I Fotiadis. Artificial neural networks for solving ordinary and partial differential equations. IEEE transactions on neural networks, 9(5):987\u2013 1000, 1998.   \n[39] Hyuk Lee and In Seok Kang. Neural algorithm for solving differential equations. Journal of Computational Physics, 91(1):110\u2013131, 1990.   \n[40] Qianxiao Li, Cheng Tai, and E Weinan. Stochastic modified equations and dynamics of stochastic gradient algorithms i: Mathematical foundations. Journal of Machine Learning Research, 20(40):1\u201347, 2019.   \n[41] Xin Li. Simultaneous approximations of multivariate functions and their derivatives by neural networks with one hidden layer. Neurocomputing, 12(4):327\u2013343, 1996.   \n[42] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. arXiv preprint arXiv:2010.08895, 2020.   \n[43] Dong C Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization. Mathematical programming, 45(1):503\u2013528, 1989.   \n[44] Minliang Liu, Liang Liang, and Wei Sun. A generic physics-informed neural network-based constitutive model for soft biological tissues. Computer methods in applied mechanics and engineering, 372:113402, 2020.   \n[45] Yu Liu and Wentao Ma. Gradient auxiliary physics-informed neural network for nonlinear biharmonic equation. Engineering Analysis with Boundary Elements, 157:272\u2013282, 2023.   \n[46] El Mehdi Lotf,i Mehdi Maziane, Khalid Hattaf, and Noura Yousf.i Partial differential equations of an epidemic model with spatial diffusion. International Journal of Partial Differential Equations, 2014:1\u20136, 2014.   \n[47] Tao Luo and Haizhao Yang. Two-layer neural networks for partial differential equations: Optimization and generalization theory. arXiv preprint arXiv:2006.15733, 2020.   \n[48] Revanth Mattey and Susanta Ghosh. A physics informed neural network for time-dependent nonlinear and higher order partial differential equations. arXiv preprint arXiv:2106.07606, 2021.   \n[49] Siddhartha Mishra and Roberto Molinaro. Estimates on the generalization error of physicsinformed neural networks for approximating a class of inverse problems for pdes. IMA Journal of Numerical Analysis, 42(2):981\u20131022, 2022.   \n[50] Taiki Miyagawa. Toward equation of motion for deep neural networks: Continuous-time gradient descent and discretization error analysis. Advances in Neural Information Processing Systems, 35:37778\u201337791, 2022.   \n[51] Eike H M\u00fcller and Robert Scheichl. Massively parallel solvers for elliptic partial differential equations in numerical weather and climate prediction. Quarterly Journal of the Royal Meteorological Society, 140(685):2608\u20132624, 2014.   \n[52] Frithiof I Niordson. On the optimal design of a vibrating beam. Quarterly of Applied Mathematics, 23(1):47\u201353, 1965.   \n[53] Bernt Oksendal. Stochastic differential equations: an introduction with applications. Springer Science & Business Media, 2013.   \n[54] Yesom Park, Taekyung Lee, Jooyoung Hahn, and Myungjoo Kang. $p$ -Poisson surface reconstruction in curl-free flow from point clouds. Advances in Neural Information Processing Systems, 2023.   \n[55] Yesom Park, Chang Hoon Song, Jooyoung Hahn, and Myungjoo Kang. ReSDF: Redistancing implicit surfaces using neural networks. Journal of Computational Physics, 502, 2024.   \n[56] Yesom Park, Changhoon Song, and Myungjoo Kang. Beyond derivative pathology of pinns: Variable splitting strategy with convergence analysis. arXiv preprint arXiv:2409.20383, 2024.   \n[57] Shige Peng. A generalized dynamic programming principle and hamilton-jacobi-bellman equation. Stochastics: An International Journal of Probability and Stochastic Processes, 38(2):119\u2013134, 1992.   \n[58] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational physics, 378:686\u2013707, 2019.   \n[59] Justin Sirignano and Konstantinos Spiliopoulos. DGM: A deep learning algorithm for solving partial differential equations. Journal of computational physics, 375:1339\u20131364, 2018.   \n[60] Roger Temam. Navier-Stokes equations: theory and numerical analysis, volume 343. American Mathematical Soc., 2001.   \n[61] Sifan Wang, Yujun Teng, and Paris Perdikaris. Understanding and mitigating gradient flow pathologies in physics-informed neural networks. SIAM Journal on Scientific Computing, 43(5):A3055\u2013A3081, 2021.   \n[62] Sifan Wang, Xinling Yu, and Paris Perdikaris. When and why pinns fail to train: A neural tangent kernel perspective. Journal of Computational Physics, 449:110768, 2022.   \n[63] Jue Yan and Chi-Wang Shu. Local discontinuous galerkin methods for partial differential equations with higher order derivatives. Journal of Scientific Computing, 17:27\u201347, 2002.   \n[64] Marius Zeinhofer, Rami Masri, and Kent-Andr\u00e9 Mardal. A unified framework for the error analysis of physics-informed neural networks. arXiv preprint arXiv:2311.00529, 2023.   \n[65] Enrui Zhang, Ming Dao, George Em Karniadakis, and Subra Suresh. Analyses of internal structures and defects in materials using physics-informed neural networks. Science advances, 8(7):eabk0644, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Mathematical Notations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We introduce the symbols and mathematical notations that are frequently used in this paper. ", "page_idx": 14}, {"type": "table", "img_path": "8K6ul0hgtC/tmp/93422031bbdafd04a3bc90f0dfa2b596bf8994f9c11e1fdb12d58bedfae221b5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "For a positive integer $m$ , the set $\\{1,\\cdot\\cdot\\cdot,m\\}$ is abbreviated as $[m]$ . The set of multi-indexes whose size is $m$ or is at most $m$ is referred by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{I_{m}=\\left\\{\\alpha=(\\alpha_{1},\\ldots,\\alpha_{d})\\in\\mathbb{N}_{0}^{d}:|\\alpha|=\\displaystyle\\sum_{i=1}^{d}\\alpha_{i}=m\\right\\},}\\\\ {J_{m}=\\left\\{\\alpha=(\\alpha_{1},\\ldots,\\alpha_{d})\\in\\mathbb{N}_{0}^{d}:|\\alpha|=\\displaystyle\\sum_{i=1}^{d}\\alpha_{i}\\leq m\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In this paper, we use two-layer MLPs. For given $k\\in\\mathbb{N}$ and a partition $0=\\xi_{0}<\\xi_{1}<\\cdot\\cdot<\\xi_{L}<$ $\\xi_{L+1}=k$ , we define $\\phi_{\\ell}:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{\\left|I_{\\xi_{\\ell}}\\right|}$ of width $m$ as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\phi_{\\ell}\\left(\\mathbf{x};\\boldsymbol{w}_{\\ell},\\boldsymbol{v}_{\\ell}\\right)=\\frac{1}{\\sqrt{m}}\\sum_{r=1}^{m}\\left(\\boldsymbol{v}_{\\ell}\\right)_{r}\\sigma\\left(\\left[\\left(\\boldsymbol{w}_{\\ell}\\right)_{r,1}\\quad(\\boldsymbol{w}_{\\ell})_{r,2}\\quad\\cdot\\cdot\\quad(\\boldsymbol{w}_{\\ell})_{r,d}\\right]\\boldsymbol{x}+\\frac{1}{2}\\left(\\boldsymbol{w}_{\\ell}\\right)_{r,d+1}\\right)}\\\\ {\\displaystyle=\\frac{1}{\\sqrt{m}}\\sum_{r=1}^{m}\\left(\\boldsymbol{v}_{\\ell}\\right)_{r}\\sigma\\left((\\boldsymbol{w}_{\\ell})_{r}^{\\top}\\,\\boldsymbol{y}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where weights in the first and second layer are ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\pmb{w}_{\\ell}\\right)_{r}=\\left[\\left(\\pmb{w}_{\\ell}\\right)_{r,1}\\quad\\left(\\pmb{w}_{\\ell}\\right)_{r,2}\\quad\\cdot\\cdot\\cdot\\quad\\left(\\pmb{w}_{\\ell}\\right)_{r,d}\\quad\\frac{1}{2}\\left(\\pmb{w}_{\\ell}\\right)_{r,d+1}\\right]^{\\top}\\in\\mathbb{R}^{(d+1)},}\\\\ &{\\left(\\pmb{v}_{\\ell}\\right)_{r}=\\left[\\left(\\pmb{v}_{\\ell}\\right)_{r,1}\\quad\\left(\\pmb{v}_{\\ell}\\right)_{r,2}\\quad\\cdot\\cdot\\quad\\left(\\pmb{v}_{\\ell}\\right)_{r,\\left|I_{\\ell_{\\ell}}\\right|}\\right]^{\\top}\\in\\mathbb{R}^{\\left|I_{\\ell_{\\ell}}\\right|},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and $\\pmb{y}=\\left[\\pmb{x}^{\\top}\\quad\\frac{1}{2}\\right]^{\\top}\\in\\mathbb{R}^{d+1}$ . Note that the output dimension of $\\phi_{\\ell}$ is $\\mathbb{R}^{\\left|I_{\\xi_{\\ell}}\\right|}$ because each component of $\\phi_{\\ell}\\left(\\pmb{x};\\pmb{w}_{\\ell},\\pmb{v}_{\\ell}\\right)$ represents partial derivative $\\textstyle{\\frac{\\partial^{\\xi_{\\ell}}}{\\partial\\mathbf{x}^{\\alpha}}}u$ , for each $\\alpha\\in I_{\\xi_{\\ell}}$ . Hence, even though $(\\boldsymbol{v}_{\\ell})_{r}$ is a flattened vector, it is more convenient to use $\\alpha\\in I_{\\xi_{\\ell}}$ as an index of component of $(\\boldsymbol{v}_{\\ell})_{r}$ . For example, if $d=2$ , $\\xi_{1}=1$ , and $\\alpha=(0,1)\\in I_{1}$ , $\\r(\\boldsymbol{v}_{1})_{r,\\alpha}=\\r(\\boldsymbol{v}_{1})_{r,(0,1)}$ refers a weight between the $r$ -th hidden node and output that represents ${\\frac{\\partial}{\\partial y}}u\\left(x,y\\right)$ . Similarly, we use multi-index itself as an index that is related to $I_{\\xi_{\\ell}}$ , $I_{\\Delta\\xi_{\\ell}}$ , or $J_{\\Delta\\xi_{\\ell}}$ . ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "Moreover, as each $\\left(\\phi_{\\ell}\\right)_{\\alpha}$ is differentiated by some $\\frac{\\partial^{\\Delta\\xi_{\\ell}}}{\\partial{\\pmb x}^{\\beta}}$ for $\\alpha\\,\\in\\,I_{\\xi_{\\ell-1}}$ and $\\beta\\,\\in\\,I_{\\Delta\\xi\\ell}$ , index we use pair of multi-index (\u03b1, \u03b2) to refer a component of \u2202\u2202\u2206x\u03be\u03b2\u2113 $\\frac{\\partial^{\\Delta\\xi_{\\ell}}}{\\partial{\\pmb x}^{\\beta}}\\left(\\phi_{\\ell}\\right)_{r,\\alpha}$ . Sometimes, this indexing by multi-index is used in conjunction with normal indexes, like $(\\mu_{\\ell})_{(\\alpha,\\beta),i}$ for some $i\\in[n_{o}]$ . ", "page_idx": 15}, {"type": "text", "text": "We denote the collection of weights of all $\\phi_{\\ell}$ \u2019s by $\\pmb{w}\\,=\\,\\left[\\pmb{w}_{0}^{\\top},\\cdot\\cdot\\cdot\\pmb{w}_{L}^{\\top}\\right]^{\\top}$ and $\\pmb{v}=\\left[\\pmb{v}_{0}^{\\top},\\cdot\\cdot\\cdot\\pmb{v}_{L}^{\\top}\\right]^{\\top}$ . Similarly, $\\pmb{\\mu}$ is the collection of all $\\pmb{\\mu}_{\\ell}$ \u2019s, $\\pmb{\\mu}\\,=\\,\\left[\\pmb{\\mu}_{0}^{\\top},\\cdots,\\pmb{\\mu}_{L}^{\\top}\\right]$ . With regularization parameters $\\nu,\\nu_{\\ell}>0$ , residuals $s_{i}$ , $(\\pmb{\\mu}_{\\ell})\\,i$ , and $h_{j}$ of variable splitting for each training sample ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{s_{i}\\left(\\boldsymbol{w},\\boldsymbol{v}\\right)=\\sqrt{\\displaystyle\\frac{1}{n_{o}}\\left(\\hat{\\mathcal{N}}\\left[\\phi_{0}\\left(\\cdot;\\boldsymbol{w}_{0},\\boldsymbol{v}_{0}\\right),\\cdots,\\phi_{L}\\left(\\cdot;\\boldsymbol{w}_{L},\\boldsymbol{v}_{L}\\right)\\right]\\left(\\boldsymbol{x}_{i}\\right)-f\\left(\\boldsymbol{x}_{i}\\right)\\right)}}}\\\\ {{\\left(\\mu_{\\ell}\\right)_{i}\\left(\\boldsymbol{w},\\boldsymbol{v}\\right)=\\left[\\left(\\mu_{\\ell}\\right)_{\\left(\\alpha,\\beta\\right),i}\\left(\\boldsymbol{w},\\boldsymbol{v}\\right)\\right]_{\\alpha\\in I_{\\xi_{\\ell-1}},\\beta\\in I_{\\Delta\\xi_{\\ell}}}}}\\\\ {{=\\left[\\frac{\\partial^{\\beta}}{\\partial x^{\\beta}}\\left(\\phi_{\\ell-1}\\right)_{\\alpha}\\left(\\boldsymbol{x}_{i};\\boldsymbol{w}_{\\ell-1},\\boldsymbol{v}_{\\ell-1}\\right)-\\phi_{\\ell}\\left(\\boldsymbol{x}_{i};\\boldsymbol{w}_{\\ell},\\boldsymbol{v}_{\\ell}\\right)_{\\alpha+\\beta}\\right]_{\\alpha\\in I_{\\xi_{\\ell-1}},\\beta\\in I_{\\Delta\\xi_{\\ell}}},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\nh_{j}\\left(\\pmb{w},\\pmb{v}\\right)=\\sqrt{\\frac{\\nu}{n_{b}}}\\left(\\mathcal{B}\\left[\\phi\\left(\\cdot;\\pmb{w}_{0},\\pmb{v}_{0}\\right)\\right]\\left(\\pmb{\\tilde{x}}_{j}\\right)-g\\left(\\pmb{\\tilde{x}}_{j}\\right)\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "define the loss LVP  ISNN for the system, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathcal{L}_{P I N N}^{V S}\\left({\\pmb w},{\\pmb v}\\right)}}\\\\ {{\\displaystyle=\\frac{1}{2}\\left(\\sum_{i=1}^{n_{o}}s_{i}\\left({\\pmb w},{\\pmb v}\\right)^{2}+\\sum_{i=1}^{n_{o}}\\sum_{\\ell=1}^{L}\\sum_{\\alpha\\in I_{\\xi_{\\ell-1}}}\\sum_{\\beta\\in J_{\\Delta\\xi_{\\ell}}}\\left({\\pmb\\mu}_{\\ell}\\right)_{\\left({\\pmb\\alpha},\\beta\\right),i}\\left({\\pmb w},{\\pmb v}\\right)^{2}+\\sum_{j=1}^{n_{b}}h_{j}\\left({\\pmb w},{\\pmb v}\\right)^{2}\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "we define ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a_{\\operatorname*{max}}=\\operatorname*{max}\\left(\\left\\{\\left|\\hat{a}_{\\ell,\\alpha,\\beta}\\left(\\mathbf{x}_{i}\\right)\\right|:\\begin{array}{c}{0\\leq\\ell\\leq L,i\\in\\left[n_{o}\\right],}\\\\ {\\alpha\\in I_{\\xi_{\\ell}},\\beta\\in J_{\\Delta\\xi_{\\ell}}}\\end{array}\\right\\}\\cup\\left\\{\\left|\\tilde{a}_{\\alpha}\\left(\\tilde{\\mathbf{x}}_{j}\\right)\\right|:\\begin{array}{c}{\\alpha\\in J_{1},}\\\\ {j\\in\\left[n_{b}\\right]}\\end{array}\\right\\}\\right),}\\\\ &{\\nu_{\\operatorname*{max}}=\\operatorname*{max}\\left(\\left\\{1,\\nu\\right\\}\\cup\\left\\{\\nu_{\\ell}:\\ell\\in\\left[L\\right]\\right\\}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "B Calculations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The proof of the main theorem includes intricate calculations. To keep the proof clear, we separate some tedious computations that are used frequently. As this section is a reference for the proofs, we use some notation or symbols without any mention if they are defined in the other part of the paper. ", "page_idx": 15}, {"type": "text", "text": "Sizes of index set $I_{m}$ and $J_{m}$ . For any $\\ell\\in[L]$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n|I_{\\xi_{\\ell}}|=\\binom{d+\\xi_{\\ell}-1}{\\xi_{\\ell}},|I_{\\Delta\\xi_{\\ell}}|=\\binom{d+\\Delta\\xi_{\\ell}-1}{\\Delta\\xi_{\\ell}},|J_{\\Delta\\xi_{\\ell}}|=\\binom{d+\\Delta\\xi_{\\ell}}{\\Delta\\xi_{\\ell}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We bound $|I_{\\xi_{\\ell}}|,|\\Delta\\xi_{\\ell}|$ and $J_{\\Delta\\xi_{\\ell}}$ by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\bigl|I_{\\Delta\\xi_{\\ell}}\\bigr|\\leq\\bigl|J_{\\Delta\\xi_{\\ell}}\\bigr|\\leq\\binom{d+|\\xi|}{d},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{\\ell=0}^{L}|I_{\\xi_{\\ell}}|\\leq|J_{\\xi_{L}}|\\leq|J_{k-1}|={\\binom{d+k}{d}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Partial derivative of $\\hat{\\pmb{s}},\\ \\pmb{\\mu}_{:}$ , and $^h$ . Note that $\\frac{\\partial^{\\beta}}{\\partial\\mathbf{x}^{\\beta}}\\left(\\left(\\pmb{w}_{\\ell}\\right)_{r}^{\\top}\\pmb{y}_{i}\\right)_{+}^{p}$ in (34), $\\begin{array}{r}{\\frac{\\partial^{\\beta}}{\\partial\\mathbf{x}^{\\beta}}\\left(\\left(\\pmb{w}_{\\ell-1}\\right)_{r}^{\\top}\\pmb{y}_{i}\\right)_{+}^{p}}\\end{array}$ in (35), $\\left((\\pmb{w}_{\\ell})_{r}^{\\top}\\pmb{y}_{i}\\right)_{+}^{p}$ in (36), and $\\begin{array}{r}{\\frac{\\partial^{\\alpha}}{\\partial\\mathbf{x}^{\\alpha}}\\left(\\left(\\pmb{w}_{0}\\right)_{r}^{\\top}\\tilde{\\pmb{y}}_{j}\\right)_{+}^{p}}\\end{array}$ in (37) are polynomial of $(w_{\\ell})_{r}$ , $(w_{\\ell-1})_{r}$ , $(w_{\\ell})_{r}$ , and $(w_{0})_{r}$ of degree $p$ , respectively. ", "page_idx": 16}, {"type": "text", "text": "For $i\\in[n_{o}],\\ell=0,1,\\ldots,L,r\\in[m]$ , and $\\alpha\\in I_{\\xi_{\\ell}}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{\\partial}{\\partial\\left(\\boldsymbol{v}_{\\ell}\\right)_{r,\\alpha}}s_{i}=\\sqrt{\\frac{1}{n_{o}}}\\displaystyle\\sum_{\\beta\\in J_{\\Delta\\xi_{\\ell+1}}}\\hat{a}_{\\ell,\\alpha,\\beta}\\frac{\\partial^{\\beta}}{\\partial\\boldsymbol{x}^{\\beta}}\\frac{\\partial}{\\partial\\left(\\boldsymbol{v}_{\\ell}\\right)_{r,\\alpha}}\\phi_{\\ell}\\left(\\pmb{x}_{i}\\right)}\\\\ {=\\sqrt{\\frac{1}{m n_{o}}}\\displaystyle\\sum_{\\beta\\in J_{\\Delta\\xi_{\\ell+1}}}\\hat{a}_{\\ell,\\alpha,\\beta}\\frac{\\partial^{\\beta}}{\\partial\\pmb{x}^{\\beta}}\\left(\\left(\\boldsymbol{w}_{\\ell}\\right)_{r}^{\\top}\\pmb{y}_{i}\\right)_{+}^{p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Similarly, for $\\ell\\in[L],r\\in[m],\\alpha_{1}\\in I_{\\xi_{\\ell-1}},\\alpha_{2}\\in I_{\\xi_{\\ell-1}},\\beta\\in I_{\\Delta\\xi_{\\ell}}$ , and $i\\in[n_{o}]$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial\\left(v_{\\ell-1}\\right)_{r,\\alpha_{1}}}\\left(\\mu_{\\ell}\\right)_{\\left(\\alpha_{2},\\beta\\right),i}=\\left\\{\\sqrt{\\frac{\\nu_{\\ell}}{m n_{o}}}\\left(\\frac{\\partial^{\\beta}}{\\partial x^{\\beta}}\\left(\\left({w_{\\ell-1}}\\right)_{r}^{\\top}{y_{i}}\\right)_{+}^{p}\\right)\\right.\\quad\\mathrm{if~}\\alpha_{1}=\\alpha_{2}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For $\\ell\\in[L],r\\in[m],\\alpha_{1}\\in I_{\\xi_{\\ell}},\\alpha_{2}\\in I_{\\xi_{\\ell-1}},\\beta\\in I_{\\Delta\\xi_{\\ell}}.$ , and $i\\in[n_{o}]$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial\\left(\\boldsymbol{v}_{\\ell}\\right)_{r,\\alpha_{1}}}\\left(\\boldsymbol{\\mu}_{\\ell}\\right)_{\\left(\\boldsymbol{\\alpha}_{2},\\beta\\right),i}=\\left\\{\\begin{array}{l l}{-\\sqrt{\\frac{\\nu_{\\ell}}{m n_{o}}}\\left(\\left(\\boldsymbol{w}_{\\ell}\\right)_{r}^{\\top}\\boldsymbol{y}_{i}\\right)_{+}^{p}}&{\\mathrm{~if~}\\boldsymbol{\\alpha}_{1}=\\boldsymbol{\\alpha}_{2}+\\beta,}\\\\ {0}&{\\mathrm{~otherwise.}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For $r\\in[m]$ and $j\\in[n_{b}]$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial\\left(\\boldsymbol{v}_{0}\\right)_{r,1}}h_{j}=\\sqrt{\\frac{\\nu}{m n_{b}}}\\sum_{\\alpha\\in J_{1}}\\tilde{a}_{\\alpha}\\frac{\\partial^{\\alpha}}{\\partial\\pmb{x}^{\\alpha}}\\left(\\left(\\boldsymbol{w}_{0}\\right)_{r}^{\\top}\\tilde{\\pmb{y}}_{j}\\right)_{+}^{p}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Norms of partial derivative of ${\\hat{s}},\\mu$ , and $^h$ . Note that for $i\\in[n_{o}],\\ell=0,1,\\ldots,L,r\\in[m]$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\sqrt{\\frac{1}{m_{m}}}\\left\\|\\frac{\\partial}{\\partial\\left(\\nu\\right)_{r}}\\left(\\sum_{s=1\\atop\\nu_{0}}\\sum_{\\stackrel{k=1}{\\nu_{\\perp}},s=1\\atop\\nu_{0}}\\hat{a}_{\\alpha_{1},\\nu_{\\perp}}\\frac{\\partial^{s}}{\\partial s^{k_{\\nu}}}\\left(\\left(\\nu_{0,\\nu_{0}}^{s}\\left(\\left(\\nu_{0,\\nu}^{\\tau}\\right)^{s}\\right)_{\\right)}^{p}\\right)\\right.\\right.}\\\\ &{=\\sqrt{\\frac{1}{m_{m}}}\\left\\|\\left(\\sum_{s=1\\atop\\nu_{0}}\\sum_{\\stackrel{k=1}{\\nu_{\\perp}},s=1\\atop\\nu_{0}}\\hat{a}_{\\alpha_{2},\\nu_{\\perp}}\\frac{\\partial}{\\partial s^{k_{\\nu}}}\\left(\\frac{\\partial^{s}}{\\partial\\left(\\nu\\right)_{r}}\\left(\\left(\\nu_{0,\\nu}^{s}\\right)^{s}\\right)_{\\right)}^{p}\\right)\\right.}\\\\ &{\\leq\\sqrt{\\frac{1}{m_{m}}}a_{\\alpha_{1},\\nu_{\\perp}}\\sum_{\\stackrel{k=1}{\\nu_{\\perp}},s=1\\atop\\nu_{0}}\\left\\|\\left(\\nu_{0,\\nu_{\\perp}}\\right)_{\\rho_{k},\\nu_{\\perp}}\\right\\|\\frac{\\partial}{\\partial\\left(\\nu\\right)_{r}}\\left(\\left(\\frac{\\partial^{s}}{\\partial\\nu}\\left(\\nu_{0,\\nu}^{s}\\right)^{s}\\right)_{\\right)}^{p}\\right\\|}\\\\ &{\\leq\\sqrt{\\frac{1}{m_{m}}}a_{\\alpha_{1},\\nu_{\\perp}}\\sum_{\\stackrel{k=1}{\\nu_{\\perp}},s=1\\atop\\nu_{0}}\\sum_{\\stackrel{k=1}{\\nu_{\\perp}},s=1\\atop\\nu_{0}}p^{k_{\\nu}\\left(\\nu_{0,\\nu_{\\perp}}\\right)}\\left(\\sum_{s=1\\atop\\nu_{0}}\\hat{a}_{\\alpha_{2},\\nu_{\\perp}}\\right)}\\\\ &{=\\sqrt{\\frac{1}{m_{m}}}a_{\\alpha_{2},\\nu_{\\perp}}\\sum_{\\left(\\nu_{0,\\nu_{\\perp}}\\right),s=1\\atop\\nu_{0}}\\Big|p\\frac{\\partial^{s_{\\nu}}}{\\partial\\left(\\nu_{0,\\nu_{\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for $\\ell\\in[L]$ , $r\\in[m]$ , $\\alpha\\in I_{\\xi_{\\ell-1}}$ , $\\beta\\in I_{\\Delta\\xi\\ell}$ , and $i\\in[n_{o}]$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|\\frac{\\partial\\left(\\mu_{t}\\right)_{i\\in\\mathcal{I}_{3}}(\\mathbf{R}_{t})}{\\partial t}\\right|_{\\mathbf{R}_{t}^{3}}=\\left|\\frac{\\partial}{\\partial t}\\left(\\mathcal{R}_{i\\in\\mathcal{I}_{3}}\\right)\\left(\\sqrt{\\frac{\\pi}{\\mathrm{m}}}\\frac{\\partial^{2}}{\\partial\\alpha_{i}\\partial\\alpha_{j}}\\left(\\left(\\nu_{t-1}\\right)_{i\\in\\mathcal{I}_{3}}^{T}\\right)\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left.-\\sqrt{\\frac{\\pi}{\\mathrm{m}}}\\omega_{i}\\left(\\nu_{t-1}\\right)_{i\\in\\mathcal{I}_{3}}\\right)\\left|\\frac{\\partial^{2}}{\\partial\\alpha_{i}\\partial\\alpha_{j}}\\left(\\left(\\omega_{i-1}\\right)_{i\\in\\mathcal{I}_{3}}^{T}\\right)\\right|_{\\mathbf{R}_{t}^{3}}}\\\\ &{\\qquad\\qquad\\qquad\\times\\sqrt{\\frac{\\pi}{\\mathrm{m}}}\\omega_{i}^{2}\\left(\\nu_{t-1}\\right)_{i\\in\\mathcal{I}_{3}}\\left|\\sqrt{\\frac{\\pi}{\\mathrm{m}}}\\omega_{i}^{3}+\\left[\\left(\\nu_{t-1}\\right)_{i\\in\\mathcal{I}_{3}}^{T}\\right]}\\\\ &{\\qquad\\qquad\\qquad\\times\\sqrt{\\frac{\\pi}{\\mathrm{m}}}\\omega_{i}^{4}\\left(1\\nu_{t-1}\\right)_{i\\in\\mathcal{I}_{3}}\\left|\\int(\\nu_{t-1})_{i\\in\\mathcal{I}_{3}}^{T}\\right|\\frac{\\partial^{2}}{\\partial\\alpha_{i}}\\left(\\nu_{t-1}\\right)_{i\\in\\mathcal{I}_{3}}^{T}}\\\\ {\\left|\\frac{\\partial(\\mu_{t})_{i\\in\\mathcal{I}_{3}}(\\mathbf{R}_{t})}{\\partial t}\\right|_{\\mathbf{R}_{t}^{3}}=\\left|\\frac{\\partial^{2}}{\\partial\\alpha_{i}\\partial\\alpha_{j}}\\left(\\nu_{t-1}\\right)_{i\\in\\mathcal{I}_{3}}^{T}\\right|\\left(\\nu_{t-1}\\right)_{i\\in\\mathcal{I}_{3}}\\left(1\\nu_{t-1}\\right)_{i\\in\\mathcal{I}_{3}}^{T},}\\\\ {\\left|\\frac{\\partial(\\mu_{t})_{i\\in\\mathcal{I}_{3}}(\\mathbf{R}_{t})}{\\partial\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and for $r\\in[m]$ and $j\\in[n_{b}]$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left\\lVert\\frac{\\partial h_{j}}{\\partial\\left(w_{0}\\right)_{r}}\\right\\rVert_{2}=\\left\\lVert\\frac{\\partial}{\\partial\\left(w_{0}\\right)_{r}}\\left(\\sqrt{\\frac{\\nu}{m n_{b}}}\\sum_{\\alpha}\\frac{\\hat{u}_{\\alpha}}{\\partial x^{\\alpha}}\\left(\\iota_{0}\\big)_{r,0}\\cdot\\left(\\left(w_{0}\\right)_{r}^{\\top}\\bar{y}_{j}\\right)_{+}\\right)\\right\\rVert_{2}}&{}\\\\ {\\qquad\\leq\\frac{\\sqrt{\\nu}}{\\sqrt{m n_{b}}}a_{\\mathbf{max}}\\sum_{\\alpha\\in\\bar{J}_{1}}\\left\\lVert\\frac{\\partial}{\\partial\\left(w_{0}\\right)_{r}}\\frac{\\partial^{\\alpha}}{\\partial x^{\\alpha}}\\left(\\left(v_{0}\\right)_{r,0}\\cdot\\left(\\left(w_{0}\\right)_{r}^{\\top}\\bar{y}_{j}\\right)_{+}^{p}\\right)\\right\\rVert_{2}}&{}\\\\ {\\qquad\\leq\\frac{\\sqrt{\\nu}}{\\sqrt{m n_{b}}}a_{\\mathbf{max}}\\sum_{\\alpha\\in\\bar{J}_{1}}p^{2}\\left|\\left(v_{0}\\right)_{r,0}\\right|\\left\\lVert\\left(w_{0}\\right)_{r}\\right\\rVert_{2}^{p-1}}&{}\\\\ {\\qquad\\leq\\sqrt{\\frac{\\nu_{\\mathbf{max}}}{m n_{b}}}a_{\\mathbf{max}}\\left(d+1\\right)p^{2}\\left\\lVert\\left(v_{0}\\right)_{r}\\right\\rVert_{2}\\left\\lVert\\left(w_{0}\\right)_{r}\\right\\rVert_{2}^{p-1},}&{}\\\\ {\\left\\lVert\\frac{\\partial h_{j}}{\\partial\\left(w_{0}\\right)_{r,1}}\\right\\rVert_{2}=0\\mathrm{if~}\\ell^{\\prime}\\in\\left[L\\right].}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Similarly, for $\\ell\\in[L],r\\in[m]$ , and $i\\in[n_{o}]$ , we have the following inequalities: ", "page_idx": 17}, {"type": "text", "text": "for $\\alpha\\in I_{\\xi_{\\ell}}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\left\\|\\frac{\\partial s_{i}\\left(\\pmb{w},\\pmb{v}\\right)}{\\partial\\left(\\pmb{v}_{\\ell}\\right)_{r,\\alpha}}\\right\\|\\leq\\sqrt{\\frac{1}{m n_{o}}}\\sum_{\\beta\\in J_{\\Delta\\xi_{\\ell+1}}}\\left|\\hat{a}_{\\ell,\\alpha,\\beta}\\frac{\\partial^{\\beta}}{\\partial\\mathbf{x}^{\\beta}}\\left(\\left(\\pmb{w}_{\\ell}\\right)_{r}^{\\top}\\pmb{y}_{i}\\right)_{+}^{p}\\right|}\\\\ {\\displaystyle\\leq\\sqrt{\\frac{1}{m n_{o}}}a_{\\mathrm{max}}\\left|J_{\\Delta\\xi_{\\ell+1}}\\right|p^{\\Delta\\xi_{\\ell+1}}\\left\\|\\left(\\pmb{w}_{\\ell}\\right)_{r}\\right\\|_{2}^{p},}\\\\ {\\displaystyle\\left\\|\\frac{\\partial s_{i}\\left(\\pmb{w},\\pmb{v}\\right)}{\\partial\\left(\\pmb{v}_{\\ell}\\right)_{r}}\\right\\|_{2}\\leq\\sqrt{\\frac{\\nu_{\\mathrm{max}}}{m n_{o}}}a_{\\mathrm{max}}\\sqrt{|J_{\\xi_{\\ell}}|}\\left|J_{\\Delta\\xi_{\\ell+1}}\\right|p^{|\\xi|}\\left\\|\\left(\\pmb{w}_{\\ell}\\right)_{r}\\right\\|_{2}^{p}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "for $\\alpha_{1}\\in I_{\\xi_{\\ell-1}}$ , $\\alpha_{2}\\in I_{\\xi_{\\ell-1}}$ , and $\\beta\\in I_{\\Delta\\xi\\ell}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\left|\\frac{\\partial\\left(\\pmb{\\mu}_{\\ell}\\right)_{\\left(\\alpha_{2},\\beta\\right),i}\\left(\\pmb{w}\\left(t\\right),\\pmb{v}\\left(t\\right)\\right)}{\\partial\\left(\\pmb{v}_{\\ell-1}\\right)_{r,\\alpha_{1}}}\\right|=\\mathbf{1}_{\\left\\{\\alpha_{1}=\\alpha_{2}\\right\\}}\\cdot\\sqrt{\\frac{\\nu_{\\ell}}{m n_{o}}}\\left|\\frac{\\partial^{\\beta}}{\\partial\\pmb{x}^{\\beta}}\\left(\\left(\\pmb{w}_{\\ell-1}\\right)_{r}^{\\top}\\pmb{y}_{i}\\right)_{+}^{p}\\right|\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\le\\mathbf{1}_{\\left\\{\\alpha_{1}=\\alpha_{2}\\right\\}}\\cdot\\sqrt{\\frac{\\nu_{\\ell}}{m n_{o}}}p^{\\Delta\\xi_{\\ell}}\\left\\|\\left(\\pmb{w}_{\\ell-1}\\right)_{r}\\right\\|_{2}^{p}}\\\\ &{}&{\\left\\|\\frac{\\partial\\left(\\pmb{\\mu}_{\\ell}\\right)_{\\left(\\alpha_{2},\\beta\\right),i}\\left(\\pmb{w},\\pmb{v}\\right)}{\\partial\\left(\\pmb{v}_{\\ell-1}\\right)_{r}}\\right\\|_{2}\\le\\sqrt{\\frac{\\nu_{\\ell}}{m n_{o}}}p^{\\Delta\\xi_{\\ell}}\\sqrt{\\left|I_{\\xi_{\\ell-1}}\\right|\\left|I_{\\Delta\\xi_{\\ell}}\\right|}\\left\\|\\left(\\pmb{w}_{\\ell-1}\\right)_{r}\\right\\|_{2}^{p},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for $\\alpha_{1}\\in I_{\\xi_{\\ell}}$ , $\\alpha_{2}\\in I_{\\xi_{\\ell-1}}$ , and $\\beta\\in I_{\\Delta\\xi\\ell}$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\frac{\\partial\\left(\\pmb{\\mu}_{\\ell}\\right)_{\\left(\\alpha_{2},\\beta\\right),i}\\left(\\pmb{w},\\pmb{v}\\right)}{\\partial\\left(\\pmb{v}_{\\ell}\\right)_{r,\\alpha_{1}}}\\right|\\leq\\mathbf{1}_{\\left\\{\\alpha_{1}=\\alpha_{2}+\\beta\\right\\}}\\cdot\\sqrt{\\frac{\\nu_{\\ell}}{m n_{o}}}\\left(\\left(\\pmb{w}_{\\ell}\\right)_{r}^{\\top}\\pmb{y}_{i}\\right)_{+}^{p}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\mathbf{1}_{\\left\\{\\alpha_{1}=\\alpha_{2}+\\beta\\right\\}}\\cdot\\sqrt{\\frac{\\nu_{\\ell}}{m n_{o}}}\\left\\|\\left(\\pmb{w}_{\\ell}\\right)_{r}\\right\\|_{2}^{p}}\\\\ &{\\left|\\frac{\\partial\\left(\\pmb{\\mu}_{\\ell}\\right)_{\\left(\\alpha_{2},\\beta\\right),i}\\left(\\pmb{w},\\pmb{v}\\right)}{\\partial\\left(\\pmb{v}_{\\ell}\\right)_{r}}\\right\\|_{2}\\leq\\sqrt{\\frac{\\nu_{\\ell}}{m n_{o}}}\\sqrt{\\left|I_{\\xi\\ell-1}\\right|\\left|I_{\\Delta\\xi_{\\ell}}\\right|}\\left\\|\\left(\\pmb{w}_{\\ell}\\right)_{r}\\right\\|_{2}^{p},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for $\\ell^{\\prime}\\neq\\ell-1,\\ell$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\|\\frac{\\partial\\left(\\pmb{\\mu}_{\\ell}\\right)_{i}\\left(\\pmb{w},\\pmb{v}\\right)}{\\partial\\left(\\pmb{v}_{\\ell^{\\prime}}\\right)_{r}}\\right\\|_{2}=0,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\displaystyle\\frac{\\partial h_{j}\\left(\\pmb{w},\\pmb{v}\\right)}{\\partial\\left(\\pmb{v}_{0}\\right)_{r,0}}\\right|\\leq\\sqrt{\\frac{\\nu}{m n_{b}}}\\sum_{\\alpha\\in J_{1}}\\left|\\tilde{a}_{\\alpha}\\frac{\\partial^{\\alpha}}{\\partial\\pmb{x}^{\\alpha}}\\left(\\left(\\left(\\pmb{w}_{0}\\right)_{r}^{\\top}\\tilde{\\pmb{y}}_{j}\\right)_{+}^{p}\\right)\\right|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\sqrt{\\frac{\\nu_{\\operatorname*{max}}}{m n_{b}}}\\left(d+1\\right)a_{\\operatorname*{max}}p\\left\\|\\left(\\pmb{w}_{0}\\right)_{r}\\right\\|_{2}^{p},}\\\\ &{\\left|\\displaystyle\\left|\\frac{\\partial h_{j}\\left(\\pmb{w},\\pmb{v}\\right)}{\\partial\\left(\\pmb{v}_{\\ell}\\right)_{r}}\\right\\|_{2}=0\\mathrm{~if~}\\ell^{\\prime}\\in\\left[L\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Hoeffding\u2019s inequalities for $s_{i,r},\\left(\\pmb{\\mu}_{\\ell}\\right)_{r,(\\alpha,\\beta),i};$ , and $\\boldsymbol{h}_{j,\\boldsymbol{r}}$ . For each $i\\in[n_{o}]$ and $r\\in[m]$ , let ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{s_{i}\\left(\\pmb{w},\\pmb{v}\\right)=\\sqrt{\\displaystyle{\\frac{1}{n_{o}}}}\\left(\\sum_{{\\ell=0}}^{L}\\sum_{{\\alpha}\\in I_{\\xi_{\\ell}}}\\sum_{{\\beta}\\in J_{{\\Delta}\\xi_{\\ell+1}}}\\hat{a}_{\\ell,{\\alpha},{\\beta}}\\frac{\\partial^{\\beta}}{\\partial\\pmb{x}^{\\beta}}\\left(\\phi_{\\ell}\\right)_{\\alpha}-\\pmb{f}\\left(\\pmb{x}_{i}\\right)\\right)}}\\\\ {{=\\displaystyle{\\sum_{r=1}^{m}s_{i,r}\\left(\\pmb{w},\\pmb{v}\\right)-\\frac{1}{\\sqrt{n_{o}}}\\,f\\left(\\pmb{x}_{i}\\right)}\\,,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where ", "page_idx": 18}, {"type": "equation", "text": "$$\ns_{i,r}\\left(\\boldsymbol{w},\\boldsymbol{v}\\right)=\\frac{1}{\\sqrt{m n_{o}}}\\sum_{\\ell=0}^{L}\\sum_{\\alpha\\in I_{\\xi_{\\ell}}}\\sum_{\\beta\\in J_{\\Delta\\xi_{\\ell+1}}}\\hat{a}_{\\ell,\\alpha,\\beta}\\frac{\\partial^{\\beta}}{\\partial x^{\\beta}}\\left(\\left(\\boldsymbol{v}_{\\ell}\\right)_{r,\\alpha}\\left(\\left(\\boldsymbol{w}_{\\ell}\\right)_{r}^{\\top}\\boldsymbol{y}\\right)_{+}^{p}\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "As $s_{i,r}$ is a $p$ -th degree polynomial of $(w_{\\ell})_{r}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|s_{i,r}\\left(\\boldsymbol{w},\\boldsymbol{v}\\right)\\right|\\leq\\frac{a_{\\operatorname*{max}}}{\\sqrt{m n_{o}}}\\displaystyle\\sum_{\\ell=0}^{L}\\sum_{\\alpha\\in I_{\\xi_{\\ell}}}\\sum_{\\beta\\in J_{\\Delta\\xi_{\\ell+1}}}\\left|\\frac{\\partial^{\\beta}}{\\partial\\boldsymbol{x}^{\\beta}}\\left(\\left(\\boldsymbol{w}_{\\ell}\\right)_{r,\\alpha}\\left(\\left(\\boldsymbol{w}_{\\ell}\\right)_{r}^{\\top}\\boldsymbol{y}\\right)_{+}^{p}\\right)\\right|}\\\\ &{\\qquad\\qquad\\leq\\frac{a_{\\operatorname*{max}}}{\\sqrt{m n_{o}}}\\displaystyle\\sum_{\\ell=0}^{L}\\sum_{\\alpha\\in I_{\\xi_{\\ell}}}\\sum_{\\beta\\in J_{\\Delta\\xi_{\\ell+1}}}\\left|\\left(\\boldsymbol{w}_{\\ell}\\right)_{r,\\alpha}\\right|p^{\\Delta\\xi_{\\ell+1}}\\left\\|\\left(\\boldsymbol{w}_{\\ell}\\right)_{r}\\right\\|_{2}^{p},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for all $i\\in[n_{o}]$ and $r\\in[m]$ . ", "page_idx": 18}, {"type": "text", "text": "Since $\\|(\\pmb{\\omega}_{\\ell})_{r}\\left(0\\right)\\|_{2}<R$ and $\\left|(\\pmb{v}_{\\ell})_{r,\\alpha}\\left(0\\right)\\right|\\leq1$ for all $\\ell=0,\\ldots,L,\\alpha\\in I_{\\xi_{\\ell}}$ , and $r\\in[m]$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left|s_{i,r}\\left(\\boldsymbol{w}\\left(0\\right),\\boldsymbol{v}\\left(0\\right)\\right)\\right|\\leq\\sqrt{\\frac{1}{m n_{o}}}a_{\\operatorname*{max}}\\sum_{\\ell=0}^{L}\\sum_{\\alpha\\in I_{\\xi_{\\ell}}}\\sum_{\\beta\\in J_{\\Delta\\xi_{\\ell+1}}}p^{\\Delta\\xi_{\\ell+1}}R^{p}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\leq\\sqrt{\\frac{\\nu_{\\operatorname*{max}}}{m n_{o}}}a_{\\operatorname*{max}}\\sum_{\\ell=0}^{L}\\left|I_{\\xi\\ell}\\right|\\cdot\\left|J_{\\Delta\\xi_{\\ell+1}}\\right|\\cdot p^{\\left|\\xi\\right|}R^{p}}}\\\\ {{\\displaystyle\\leq\\sqrt{\\frac{\\nu_{\\operatorname*{max}}}{m n_{o}}}a_{\\operatorname*{max}}\\binom{d+k}{d}\\binom{d+\\left|\\xi\\right|}{d}p^{\\left|\\xi\\right|}R^{p},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for all $i\\in[n_{o}]$ and $r\\in[m]$ . ", "page_idx": 19}, {"type": "text", "text": "Using $\\mathbb{E}\\left[(\\pmb{v}_{\\ell})_{r,\\alpha}\\right]=0$ , we attain $\\mathbb{E}\\left[s_{i,r}\\left(\\pmb{w}\\left(0\\right),\\pmb{v}\\left(0\\right)\\right)\\right]=0$ , and Hoeffding\u2019s inequality gives ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|\\sum_{r=1}^{m}s_{i,r}\\left(w\\left(0\\right),v\\left(0\\right)\\right)\\right|>\\frac{\\varepsilon}{\\sqrt{n_{o}}}\\right]\\le2\\exp\\left(-\\frac{2\\left(\\frac{\\varepsilon}{\\sqrt{n_{o}}}\\right)^{2}}{m\\left(2\\sqrt{\\frac{\\nu_{\\operatorname*{max}}}{m n_{o}}}a_{\\operatorname*{max}}\\binom{d+k}{d}\\binom{d+|\\xi|}{d}p^{|\\xi|}R^{p}\\right)^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\leq2\\exp\\left(-\\frac{\\varepsilon^{2}}{2\\nu_{\\operatorname*{max}}a_{\\operatorname*{max}}^{2}\\binom{d+k}{d}^{2}\\binom{d+|\\xi|}{d}^{2}p^{2|\\xi|}R^{2p}}\\right),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for all $\\varepsilon>0$ . ", "page_idx": 19}, {"type": "text", "text": "Similarly, for each $\\ell\\in[L],\\alpha\\in I_{\\xi_{\\ell-1}},\\beta\\in I_{\\Delta\\xi_{\\ell}}$ , and $i\\in[n_{o}]$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{(\\pmb{\\mu}_{\\ell})_{(\\alpha,\\beta),i}\\left({\\pmb w},{\\pmb v}\\right)=\\sqrt{\\frac{\\nu_{\\ell}}{n_{o}}}\\left(\\frac{\\partial^{\\beta}}{\\partial{\\pmb x}^{\\beta}}\\left(\\phi_{\\ell-1}\\right)_{\\alpha}\\left({\\pmb x}_{i};{\\pmb w}_{\\ell-1},{\\pmb v}_{\\ell-1}\\right)-\\phi_{\\ell}\\left({\\pmb x}_{i};{\\pmb w}_{\\ell},{\\pmb v}_{\\ell}\\right)_{\\alpha+\\beta}\\right)}}\\\\ {{\\qquad\\qquad\\qquad\\qquad=\\displaystyle\\sum_{r=1}^{m}\\left(\\pmb{\\mu}_{\\ell}\\right)_{r,(\\alpha,\\beta),i}\\left({\\pmb w},{\\pmb v}\\right),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mu_{\\ell})_{r,(\\alpha,\\beta),i}\\left(w,v\\right)=\\sqrt{\\frac{\\nu_{\\ell}}{m n_{o}}}\\left(\\frac{\\partial^{\\beta}}{\\partial x^{\\beta}}\\left((v_{\\ell-1})_{r,\\alpha}\\left((w_{\\ell-1})_{r}^{\\top}\\,y_{i}\\right)_{+}^{p}\\right)-(v_{\\ell})_{r,\\alpha+\\beta}\\left((w_{\\ell})_{r}^{\\top}\\,y_{i}\\right)_{+}^{p}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, we attain that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|(\\mu_{\\ell})_{r,(\\alpha,\\beta),i}\\left(w,v\\right)\\right|\\leq\\sqrt{\\frac{\\nu_{\\ell}}{m n_{o}}}\\left(\\left|\\frac{\\partial^{\\beta}}{\\partial x^{\\beta}}\\left((v_{\\ell-1})_{r,\\alpha}\\left((w_{\\ell-1})_{r}^{\\top}\\,y_{i}\\right)_{+}^{p}\\right)\\right|+\\left|(v_{\\ell})_{r,\\alpha+\\beta}\\left((w_{\\ell})_{r}^{\\top}\\,y_{i}\\right)_{+}^{p}\\right|\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\times\\left|\\frac{\\nu_{\\ell}}{m n_{o}}\\left(p^{\\Delta\\xi_{\\ell}}\\left|(v_{\\ell-1})_{r,\\alpha}\\right|\\left|(w_{\\ell-1})_{r}^{\\top}\\,y_{i}\\right|_{2}^{p}+\\left|(v_{\\ell})_{r,\\alpha+\\beta}\\right|\\left|(w_{\\ell})_{r}\\right|_{2}^{p}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\vert\\left(\\pmb{\\mu}_{\\ell}\\right)_{r,\\left(\\alpha,\\beta\\right),i}\\left(\\pmb{w}\\left(0\\right),\\pmb{v}\\left(0\\right)\\right)\\right\\vert\\leq\\sqrt{\\frac{\\nu_{\\operatorname*{max}}}{m n_{o}}}\\left(p^{\\left\\vert\\xi\\right\\vert}+1\\right)R^{p}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "$\\mathbb{E}\\left[(\\pmb{v}_{\\ell})_{r,\\alpha}\\right]=0$ implies $\\mathbb{E}\\left[(\\pmb{\\mathscr{\\mu}}_{\\ell})_{r,(\\alpha,\\beta),i}\\right]=0$ and for each $i\\in[n_{o}]$ , Hoeffding\u2019s inequality with the sum over $\\bar{r}\\in[m]$ of variables ", "page_idx": 19}, {"type": "equation", "text": "$$\nX_{r,i}\\left(\\mathbf{w}\\left(0\\right),\\mathbf{v}\\left(0\\right)\\right)=\\sum_{\\alpha\\in I_{\\xi_{\\ell-1}}}\\sum_{\\beta\\in I_{\\Delta\\xi_{\\ell}}}\\left(\\pmb{\\mu}_{\\ell}\\right)_{r,\\left(\\alpha,\\beta\\right),i}\\left(\\pmb{w}\\left(0\\right),\\pmb{v}\\left(0\\right)\\right),\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|X_{r,i}\\left(\\pmb{w}\\left(0\\right),\\pmb{v}\\left(0\\right)\\right)|\\leq\\left|I_{\\xi_{\\ell-1}}\\right|\\cdot\\left|I_{\\Delta\\xi_{\\ell}}\\right|\\sqrt{\\frac{\\nu_{\\operatorname*{max}}}{m n_{o}}}\\left(p^{|\\xi|}+1\\right)R^{p}}\\\\ &{\\qquad\\qquad\\qquad<\\sqrt{\\frac{\\nu_{\\operatorname*{max}}}{m n_{o}}}\\bigg(\\overset{d+k}{d}\\bigg)\\bigg(\\overset{d+}{d}\\bigg)\\left(p^{|\\xi|}\\right)\\left(p^{|\\xi|}+1\\right)R^{p}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "gives ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\left[\\left|\\displaystyle\\sum_{r=1}^{m}(\\mu_{\\ell})_{r,(\\alpha,\\beta),i}\\left(w\\left(0\\right),v\\left(0\\right)\\right)\\right|>\\frac{\\varepsilon}{\\sqrt{n_{o}}}\\right]}\\\\ &{\\le2\\exp\\left(-\\frac{2\\left(\\frac{\\varepsilon}{\\sqrt{n_{o}}}\\right)^{2}}{m\\left(\\sqrt{\\frac{\\nu_{\\operatorname*{max}}}{m n_{o}}}\\left(d_{d}^{+k}\\right)\\left(d_{d}^{+}\\mid\\xi\\mid\\right)\\left(p\\left\\vert\\xi\\mid+1\\right)R^{p}\\right)^{2}}\\right)}\\\\ &{\\le2\\exp\\left(-\\frac{\\varepsilon^{2}}{\\nu_{\\operatorname*{max}}\\left(\\frac{d+k}{d}\\right)^{2}\\left(\\frac{d+\\mid\\xi\\mid}{d}\\right)^{2}\\left(p\\left\\vert\\xi\\mid+1\\right)^{2}R^{2p}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For each $j\\in[n_{b}]$ , let us denote ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle h_{j}\\left(\\pmb{w},\\pmb{v}\\right)=\\sqrt{\\frac{\\nu}{n_{b}}}\\sum_{\\alpha\\in J_{1}}\\tilde{a}_{\\alpha}\\phi_{0}\\left(\\tilde{\\pmb{x}}_{j};\\pmb{w}_{0},\\pmb{v}_{0}\\right)-\\sqrt{\\frac{\\nu}{n_{b}}}g\\left(\\tilde{\\pmb{x}}_{j}\\right)}}\\\\ {{\\displaystyle=\\sum_{r=1}^{m}h_{j,r}\\left(\\left(\\pmb{w}_{0}\\right)_{r},\\left(\\pmb{v}_{0}\\right)_{r}\\right)-\\sqrt{\\frac{\\nu}{n_{b}}}g\\left(\\tilde{\\pmb{x}}_{j}\\right),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where ", "page_idx": 20}, {"type": "equation", "text": "$$\nh_{j,r}\\left(\\pmb{w},\\pmb{v}\\right)=\\sqrt{\\frac{\\nu}{m n_{b}}}\\sum_{\\alpha\\in J_{1}}\\tilde{a}_{\\alpha}\\frac{\\partial^{\\alpha}}{\\partial\\pmb{x}^{\\alpha}}\\left(\\left(\\pmb{v}_{0}\\right)_{r,\\mathbf{0}}\\cdot\\left(\\left(\\pmb{w}_{0}\\right)_{r}^{\\top}\\tilde{\\pmb{y}}_{j}\\right)_{+}^{p}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "It is clear that $h_{j r}$ is expressed as a $p$ -th order polynomial of $(w_{0})_{r}$ and we can deduce that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left|h_{j,r}\\left(\\pmb{w}\\left(0\\right),\\pmb{v}\\left(0\\right)\\right)\\right|\\leq\\sqrt{\\frac{\\nu_{\\operatorname*{max}}}{m n_{b}}}a_{\\operatorname*{max}}\\left(d+1\\right)p R^{p},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for all $j\\in[n_{b}]$ and $r\\in[m]$ . ", "page_idx": 20}, {"type": "text", "text": "Similar to the case of $s_{i,r}$ and $(\\mu_{\\ell})_{r,(\\alpha,\\beta),i}$ , Hoeffding\u2019s inequality gives ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|\\sum_{r=1}^{m}h_{j,r}\\left(\\pmb{w}\\left(0\\right),\\pmb{v}\\left(0\\right)\\right)\\right|>\\frac{\\varepsilon}{\\sqrt{n_{b}}}\\right]\\le2\\exp\\left(-\\frac{\\varepsilon^{2}}{2\\nu_{\\operatorname*{max}}a_{\\operatorname*{max}}^{2}\\left(d+1\\right)^{2}p^{2}R^{2p}}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Components of $Q_{\\ell_{1},\\ell_{2}}$ Each component of $Q_{\\ell_{1},\\ell_{2}}$ is polynomial of order $p$ as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(Q_{0,0}\\right)_{i_{1},i_{2}}}\\\\ &{=\\displaystyle\\sum_{\\ell=0}^{L}\\sum_{r=1}^{m}\\sum_{\\alpha\\in I_{\\ell_{\\ell}}}\\left(\\frac{\\partial}{\\partial\\left(\\boldsymbol{v}_{\\ell}\\right)_{r,\\alpha}}\\boldsymbol{s}_{i_{1}}\\right)\\left(\\frac{\\partial}{\\partial\\left(\\boldsymbol{v}_{\\ell}\\right)_{r,\\alpha}}\\boldsymbol{s}_{i_{2}}\\right)}\\\\ &{=\\displaystyle\\frac{1}{m n_{o}}\\sum_{\\ell=0}^{L}\\sum_{r=1}^{m}\\sum_{\\alpha\\in I_{\\ell_{\\ell}}}^{m}\\left(\\sum_{\\beta\\in J_{\\Delta\\xi\\ell+1}}\\hat{a}_{\\ell,\\alpha,\\beta}\\frac{\\partial^{\\beta}}{\\partial x^{\\beta}}\\left(\\left(\\boldsymbol{w}_{\\ell}\\right)_{r}^{\\top}\\boldsymbol{y}_{i_{1}}\\right)_{+}^{p}\\right)\\left(\\sum_{\\beta\\in J_{\\Delta\\xi\\ell+1}}\\hat{a}_{\\ell,\\alpha,\\beta}\\frac{\\partial^{\\beta}}{\\partial x^{\\beta}}\\left(\\left(\\boldsymbol{w}_{\\ell}\\right)_{r}^{\\top}\\boldsymbol{y}_{i_{2}}\\right)_{-}^{p}\\right)}\\end{array}\\left(\\left(\\boldsymbol{w}_{\\ell}\\right)_{r}^{\\top}\\right)_{i_{1},i_{2}},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\pmb{Q}_{0,\\ell}\\right)_{i_{1},((\\alpha,\\beta),i_{2})}}\\\\ &{=\\displaystyle\\sum_{\\ell^{\\prime}=0}^{L}\\sum_{r=1}^{m}\\sum_{\\alpha^{\\prime}\\in I_{\\xi_{\\ell^{\\prime}}}}\\left(\\frac{\\partial}{\\partial\\left(\\pmb{v}_{\\ell^{\\prime}}\\right)_{r,\\alpha^{\\prime}}}s_{i_{1}}\\right)\\left(\\frac{\\partial}{\\partial\\left(\\pmb{v}_{\\ell^{\\prime}}\\right)_{r,\\alpha^{\\prime}}}\\left(\\pmb{\\mu}_{\\ell}\\right)_{(\\alpha,\\beta),i_{2}}\\right)}\\\\ &{=\\displaystyle\\sum_{r=1}^{m}\\sum_{\\alpha^{\\prime}\\in I_{\\xi_{\\ell-1}}}\\left(\\frac{\\partial}{\\partial\\left(\\pmb{v}_{\\ell-1}\\right)_{r,\\alpha^{\\prime}}}s_{i_{1}}\\right)\\left(\\frac{\\partial}{\\partial\\left(\\pmb{v}_{\\ell-1}\\right)_{r,\\alpha^{\\prime}}}\\left(\\pmb{\\mu}_{\\ell}\\right)_{(\\alpha,\\beta),i_{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\underset{r=1}{\\overset{m}{\\sum}}\\underset{w\\in\\mathcal{U}_{\\ell-1}}{\\sum}\\left(\\frac{\\partial}{\\partial(v_{\\ell})_{r,\\alpha^{r}}}s_{\\ell_{1}}\\right)\\left(\\frac{\\partial}{\\partial(v_{r})_{r,\\alpha^{r}}}(\\mu_{\\ell})_{(\\alpha,\\beta),i_{2}}\\right)}\\\\ &{=\\underset{r=1}{\\overset{m}{\\sum}}\\underset{w\\in\\mathcal{U}_{\\ell-1}}{\\sum}\\left(\\frac{\\partial}{\\partial(v_{\\ell-1})_{r,\\alpha^{r}}}s_{\\ell_{1}}\\right)\\left(\\frac{\\partial}{\\partial(v_{\\ell-1})_{r,\\alpha^{r}}}(\\mu_{\\ell})_{(\\alpha,\\beta),i_{2}}\\right)}\\\\ &{\\qquad+\\underset{r=1}{\\overset{m}{\\sum}}\\left(\\frac{\\partial}{\\partial(v_{\\ell})_{r,\\alpha^{r}}\\partial(s_{1})}\\left(\\frac{\\partial}{\\partial(v_{\\ell})_{r,\\alpha^{r}}\\partial(\\mu_{\\ell})_{(\\alpha,\\beta),i_{2}}}\\right)\\right.}\\\\ &{=\\frac{\\sqrt{\\ell_{\\ell}}}{m\\alpha_{\\mathrm{o}}}\\underset{r=1}{\\overset{m}{\\sum}}\\left(\\left(\\underset{\\rho\\in\\mathcal{U}_{\\ell,\\alpha_{\\ell}}}{\\sum}\\frac{\\partial^{\\ell}}{\\partial\\ell-1_{\\alpha},\\rho^{r}}\\left(\\frac{\\partial^{\\ell}}{\\partial(w_{\\ell-1})_{r}^{\\top}}y_{\\ell_{1}}\\right)_{+}\\right)\\left(\\frac{\\partial^{3}}{\\partial x^{\\beta}}\\left((w_{\\ell-1})_{r}^{\\top}y_{\\ell_{2}}\\right)_{+}^{p}\\right)}\\\\ &{\\qquad\\qquad\\left.-\\left(\\underset{\\rho\\in\\mathcal{U}_{\\ell,\\alpha_{\\ell}}}{\\sum}\\frac{\\partial_{\\ell}}{\\partial\\ell-1_{\\alpha},\\rho^{r}}\\frac{\\partial^{p^{\\prime}}}{\\partial x^{p^{\\prime}}}\\left((w_{\\ell})_{r}^{\\top}y_{\\ell_{1}}\\right)_{+}^{p}\\right)\\left((w_{\\ell})_{r}^{\\top}y_{\\ell_{2}}\\right)_{+}^{p}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c c c}{\\displaystyle Q_{0,L+1}\\big)_{i,j}=\\sum_{\\ell=0}^{L}\\displaystyle\\sum_{r=1}^{m}\\sum_{\\alpha\\in I_{\\xi_{\\ell}}}\\left(\\frac{\\partial}{\\partial\\left(\\boldsymbol{v}_{\\ell}\\right)_{r,\\alpha}}s_{i}\\right)\\left(\\frac{\\partial}{\\partial\\left(\\boldsymbol{v}_{\\ell}\\right)_{r,\\alpha}}h_{j}\\right)}&&\\\\ {\\displaystyle=\\sum_{r=1}^{m}\\left(\\frac{\\partial}{\\partial\\left(\\boldsymbol{v}_{0}\\right)_{r,1}}s_{i}\\right)\\left(\\frac{\\partial}{\\partial\\left(\\boldsymbol{v}_{0}\\right)_{r,1}}h_{j}\\right)}&&{\\displaystyle(111)}\\\\ {\\displaystyle=\\frac{1}{m}\\sqrt{\\frac{\\nu}{n_{o}n_{b}}}\\sum_{r=1}^{m}\\left(\\sum_{\\beta\\in J_{\\Delta\\ell_{1}}}\\hat{a}_{0,1,\\beta}\\frac{\\partial^{\\beta}}{\\partial x^{\\beta}}\\left(\\left(\\boldsymbol{w}_{0}\\right)_{r}^{\\top}\\boldsymbol{y}_{i}\\right)_{+}^{p}\\right)\\left(\\sum_{\\alpha\\in J_{1}}\\Tilde{a}_{\\alpha}\\frac{\\partial^{\\alpha}}{\\partial x^{\\alpha}}\\left(\\left(\\boldsymbol{w}_{0}\\right)_{r}^{\\top}\\Tilde{\\boldsymbol{y}}_{j}\\right)_{+}^{p}\\right).}&&\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|Q_{\\mathcal{X}}\\rangle_{(0,0],(0],(0,1])}}\\\\ &{=\\frac{\\displaystyle\\sum_{t=1}^{T}\\sum_{i=1}^{T}\\sum_{s=t+1}^{T}\\left(\\frac{\\partial}{\\partial(\\tau_{t}\\gamma_{s,t})}\\Big(\\mu\\Big)_{(s,0],(0,1,s),(\\tau_{s})}\\right)\\left(\\frac{\\partial}{\\partial(\\tau_{t}\\gamma_{s,t})}\\Big(\\mu\\Big)_{(0,1,s),(\\tau_{s})}\\right)}\\\\ &{=\\frac{\\displaystyle\\sum_{t=1}^{T}\\sum_{s=t+1}^{T}\\left(\\frac{\\partial}{\\partial(\\tau_{t}\\gamma_{s,t})}\\Big(\\mu\\Big)_{(s,0],(0,1,s),(\\tau_{s})}\\right)\\left(\\frac{\\partial}{\\partial(\\tau_{t}\\gamma_{s,t})}\\Big(\\mu\\Big)_{(0,1,s),(0],(0,1,s)}\\right)}\\\\ &{\\phantom{=}+\\frac{\\displaystyle\\sum_{t=1}^{T}\\sum_{s=t+1}^{T}\\left(\\frac{\\partial}{\\partial(\\tau_{t}\\gamma_{s,t})}\\Big(\\mu\\Big)_{(0,1,s),(0],(0,1,s)}\\right)\\left(\\frac{\\partial}{\\partial(\\tau_{t}\\gamma_{s,t})}\\Big(\\mu\\Big)_{(0,1,s),(0,1,s)}\\right)}\\\\ &{=\\frac{\\displaystyle\\sum_{t=1}^{T}\\left(\\sqrt{\\frac{D^{-}}{\\partial(\\tau_{t}\\gamma_{s,t})}}\\frac{\\partial^{(1)}}{\\partial(\\tau_{s,t})}\\Big(\\left(\\mu\\tau_{t-1}\\right)^{\\tau_{s}}\\Big)_{(0,1,s)}\\frac{\\partial^{\\tau_{t}}}{\\partial\\tau_{s,t}^{\\tau_{s}}}\\left(\\frac{\\gamma\\tau_{t}\\gamma_{s,t}}{\\partial(\\tau_{t},0]}\\Big(\\mu\\tau_{s,t}\\Big)_{(0,1,s)}\\right)1_{{(0,1,s)}}}\\\\ &{\\phantom{=}+\\frac{\\displaystyle\\sum_{t=1}^{T}\\sum_{s=t}^{T}\\left(\\sqrt{\\frac{D^{-}}{\\partial(\\tau_{t}\\gamma_{s,t})}}\\Big(\\mu\\tau_{s,t}\\gamma_{s,t}\\Big)_{(0,1,s)}\\right)\\\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\big(Q_{\\ell,\\ell+1}\\big)_{\\big((\\alpha_{1},\\beta_{1}),i_{1}\\big),\\big((\\alpha_{2},\\beta_{2}),i_{2}\\big)}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "(121) ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle}&{\\displaystyle=\\sum_{\\ell^{\\prime}=0\\,r=1\\,\\alpha\\in t_{\\ell^{\\prime}}}^{L\\,\\,\\,m}\\left(\\frac{\\partial}{\\partial\\left(\\boldsymbol{v}_{\\ell^{\\prime}}\\right)_{r,\\alpha}}\\left(\\mu_{\\ell}\\right)_{\\left(\\alpha_{1},\\beta_{1}\\right),i_{1}}\\right)\\left(\\frac{\\partial}{\\partial\\left(\\boldsymbol{v}_{\\ell^{\\prime}}\\right)_{r,\\alpha}}\\left(\\mu_{\\ell+1}\\right)_{\\left(\\alpha_{2},\\beta_{2}\\right),i_{2}}\\right)}\\\\ {\\displaystyle}&{\\displaystyle=\\sum_{r=1}^{m}\\sum_{\\alpha\\in t_{\\ell}}\\left(\\frac{\\partial}{\\partial\\left(\\boldsymbol{v}_{\\ell}\\right)_{r,\\alpha}}\\left(\\mu_{\\ell}\\right)_{\\left(\\alpha_{1},\\beta_{1}\\right),i_{1}}\\right)\\left(\\frac{\\partial}{\\partial\\left(\\boldsymbol{v}_{\\ell}\\right)_{r,\\alpha}}\\left(\\mu_{\\ell+1}\\right)_{\\left(\\alpha_{2},\\beta_{2}\\right),i_{2}}\\right)}\\\\ {\\displaystyle}&{\\displaystyle=\\sum_{r=1\\,\\alpha\\in t_{\\ell}}^{m}\\left(-\\sqrt{\\frac{\\nu_{\\ell}}{m n_{o}}}\\left(\\left(\\boldsymbol{w}_{\\ell}\\right)_{r}^{\\top}\\boldsymbol{y}_{i_{1}}\\right)_{+}^{p}\\right)\\mathbf{1}_{\\left\\{\\alpha=\\alpha_{1}+\\beta_{1}\\right\\}}\\left(\\sqrt{\\frac{\\nu_{\\ell+1}}{m n_{o}}}\\frac{\\partial^{\\beta_{2}}}{\\partial x^{\\beta_{2}}}\\left(\\left(\\boldsymbol{w}_{\\ell}\\right)_{r}^{\\top}\\boldsymbol{y}_{i_{2}}\\right)_{+}^{p}\\right)\\mathbf{1}_{\\left\\{\\alpha=\\alpha_{2}\\right\\}}}\\\\ {\\displaystyle}&{\\displaystyle=\\frac{\\sqrt{\\nu_{\\ell}\\nu_{\\ell+1}}}{m n_{o}}\\sum_{\\alpha\\in\\mathbf{t}_{\\ell}}^{m}\\left(-\\left(\\left(\\boldsymbol{w}_{\\ell}\\right)_{r}^{\\top}\\boldsymbol{y}_{i_{1}}\\right)_{+}^{p}\\right)\\left(\\frac{\\partial^{\\beta_{2}}}{\\partial x^{\\beta_{2}}}\\left(\\left(\\boldsymbol{w}_{\\ell}\\right)_{r}^{\\top}\\boldsymbol{y}_{i_{2}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(Q_{\\ell,L+1}\\right)_{\\{(\\alpha,\\beta),i\\},j}}\\\\ &{=\\displaystyle\\sum_{\\ell^{\\prime}=0}^{L}\\sum_{n=1}^{m}\\sum_{\\sigma^{\\prime}\\in I_{\\ell^{\\prime}}}\\left(\\frac{\\partial}{\\partial(\\sigma_{\\ell^{\\prime}})_{r,\\sigma^{\\prime}}}\\left(\\mu_{\\ell}\\right)_{(\\alpha,\\beta),i}\\right)\\left(\\frac{\\partial}{\\partial(\\sigma_{\\ell^{\\prime}})_{r,\\sigma^{\\prime}}}h_{j}\\right)}\\\\ &{=\\displaystyle\\sum_{r=1}^{m}\\left(\\frac{\\partial}{\\partial(\\sigma_{0})_{r,1}}\\left(\\mu_{\\ell}\\right)_{(\\alpha,\\beta),i}\\right)\\left(\\frac{\\partial}{\\partial(\\sigma_{0})_{r,1}}h_{j}\\right)}\\\\ &{=\\displaystyle\\sum_{r=1}^{m}\\left(\\sqrt{\\frac{\\nu_{1}}{m n_{o}}}\\frac{\\partial^{3}}{\\partial x^{3}}\\left((w_{0})_{r}^{\\top}y_{i}\\right)_{+}^{r}\\right)\\left(\\sqrt{\\frac{\\nu}{m n_{b}}}\\sum_{\\alpha\\in I_{1}}\\tilde{a}_{\\alpha}\\left((w_{0})_{r}^{\\top}\\;\\tilde{y}_{j}\\right)_{+}^{r}\\right)\\mathbf{1}_{\\{\\ell^{\\prime}1\\}}}\\\\ &{=\\displaystyle\\frac{1}{m}\\sqrt{\\frac{\\nu_{1}\\nu}{n_{o}n_{b}}}\\sum_{r=1}^{m}\\left(\\frac{\\partial^{3}}{\\partial x^{\\beta}}\\left((w_{0})_{r}^{\\top}y_{i}\\right)_{+}^{r}\\right)\\left(\\sum_{\\alpha\\in I_{1}}\\tilde{a}_{\\alpha}\\frac{\\partial^{\\alpha}}{\\partial x^{\\alpha}}\\left((w_{0})_{r}^{\\top}\\;\\tilde{y}_{j}\\right)_{+}^{r}\\right)\\mathbf{1}_{\\{\\ell^{\\prime}1\\}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle(Q_{L+1,L+1})_{j_{1},j_{2}}}\\\\ {\\displaystyle=\\sum_{\\ell=0}^{L}\\sum_{n=1}^{m}\\sum_{\\alpha\\in I_{\\ell}}^{n}\\left(\\frac{\\partial}{\\partial(v_{\\ell})_{r,\\alpha}}h_{j_{1}}\\right)\\left(\\frac{\\partial}{\\partial(v_{\\ell})_{r,\\alpha}}h_{j_{2}}\\right)}\\\\ {\\displaystyle=\\sum_{r=1}^{m}\\left(\\frac{\\partial}{\\partial(v_{0})_{r,1}}h_{j_{1}}\\right)\\left(\\frac{\\partial}{\\partial(v_{0})_{r,1}}h_{j_{2}}\\right)}\\\\ {\\displaystyle=\\sum_{r=1}^{m}\\left(\\sqrt{\\frac{\\nu}{m n}}\\sum_{\\alpha\\in J_{1}}^{\\nu}\\tilde{a}_{\\alpha}\\frac{\\partial^{\\alpha}}{\\partial x^{\\alpha}}\\left((w_{0})_{r}^{\\top}\\tilde{y}_{j_{1}}\\right)_{+}^{p}\\right)\\left(\\sqrt{\\frac{\\nu}{m n}}\\sum_{\\alpha\\in J_{1}}^{\\nu}\\tilde{a}_{\\alpha}\\frac{\\partial^{\\alpha}}{\\partial x^{\\alpha}}\\left((w_{0})_{r}^{\\top}\\tilde{y}_{j_{2}}\\right)_{+}^{p}\\right)}\\\\ {\\displaystyle=\\frac{\\nu}{m n_{b}}\\sum_{r=1}^{m}\\left(\\sum_{\\alpha\\in J_{1}}^{\\nu}\\tilde{a}_{\\alpha}\\frac{\\partial^{\\alpha}}{\\partial x^{\\alpha}}\\left((w_{0})_{r}^{\\top}\\tilde{y}_{j_{1}}\\right)_{+}^{p}\\right)\\left(\\sum_{\\alpha\\in J_{1}}^{\\nu}\\tilde{a}_{\\alpha}\\frac{\\partial^{\\alpha}}{\\partial x^{\\alpha}}\\left((w_{0})_{r}^{\\top}\\tilde{y}_{j_{2}}\\right)_{+}^{p}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The number of possible nonzero elements in $Q_{\\ell_{1},\\ell_{2}}$ and $\\hat{G}_{v}$ . There are possibly $n_{o}^{2}$ , $\\left|{{I_{\\xi_{\\ell-1}}}}\\right|\\left|{{I_{\\Delta\\xi_{\\ell}}}}\\right|n_{o}^{2}$ , and $n_{o}n_{b}$ nonzero elements in $Q_{0,0},Q_{0,\\ell}$ , and $Q_{0,L+1}$ , respectively. For each $(\\alpha_{1},\\beta_{1})\\;\\in\\;I_{\\xi_{\\ell-1}}\\,\\times\\,I_{\\Delta\\xi_{\\ell}}$ , there are at most $\\lvert I_{\\Delta\\xi_{\\ell}}\\rvert$ pairs of $(\\alpha_{2},\\beta_{2})~\\in~I_{\\xi_{\\ell-1}}\\times I_{\\Delta\\xi_{\\ell}}$ such that $\\alpha_{1}+\\beta_{1}\\,=\\,\\alpha_{2}+\\beta_{2}$ and at most $\\lvert I_{\\Delta\\xi_{\\ell}}\\rvert$ pairs of $(\\alpha_{2},\\beta_{2})\\;\\in\\;I_{\\xi_{\\ell-1}}\\,\\times\\,I_{\\Delta\\xi_{\\ell}}$ such that $\\alpha_{1}~=~\\alpha_{2}$ . Thus, there are at most $2\\,|I_{\\Delta\\xi_{\\ell}}|$ pairs of $(\\alpha_{2},\\beta_{2})\\;\\in\\;I_{\\xi_{\\ell-1}}\\,\\times\\,I_{\\bar{\\Delta}\\xi_{\\ell}}$ with $\\dot{\\alpha_{1}}+\\beta_{1}\\,=\\,\\alpha_{2}\\,+\\,\\beta_{2}$ or $\\alpha_{1}\\,=\\,\\alpha_{2}$ , and $Q_{\\ell,\\ell}$ has possibly $\\left(\\left|I_{\\xi_{\\ell-1}}\\right|\\left|I_{\\Delta\\xi_{\\ell}}\\right|n_{o}\\right)(2\\left|I_{\\Delta\\xi_{\\ell}}\\right|n_{o})=2\\left|I_{\\xi_{\\ell-1}}\\right|\\cdot\\left|I_{\\Delta\\xi_{\\ell}}\\right|^{2}n_{o}^{2}$ nonzero elements. Similarly, each $(\\alpha_{1},\\beta_{1})\\;\\in\\;I_{\\xi_{\\ell-1}}\\,\\times\\,I_{\\Delta\\xi_{\\ell}}$ uniquely determines $\\alpha_{2}\\,=\\,\\alpha_{1}\\,+\\,\\beta_{1}\\,\\in\\,I_{\\xi\\ell}$ , there are $\\left|I_{\\Delta\\xi\\ell+1}\\right|$ pairs of $(\\alpha_{2},\\beta_{2})~\\in~I_{\\xi_{\\ell}}\\,\\times\\,I_{\\Delta\\xi_{\\ell+1}}$ with $\\alpha_{2}~=~\\alpha_{1}+\\beta_{1}$ . Hence, $Q_{\\ell,\\ell+1}$ has $\\left|I_{\\xi_{\\ell-1}}\\right|\\cdot\\left|I_{\\Delta\\xi_{\\ell}}\\right|\\cdot\\left|I_{\\Delta\\xi_{\\ell+1}}\\right|n_{o}^{2}$ nonzero elements. $Q_{1,L+1}$ has $|I_{\\Delta\\xi_{1}}|\\,n_{o}n_{b}$ nonzero elements. The number of nonzero elements in $Q_{L+1,L+1}$ is $n_{b}^{2}$ . After all, the number of nonzero elements in $\\hat{G}_{v}$ is ", "page_idx": 22}, {"type": "table", "img_path": "8K6ul0hgtC/tmp/70e26111b79453ee1e28bfa17638bab01adfe8bf2c996b61c3473d20166a6e7c.jpg", "table_caption": [], "table_footnote": ["Table 1: The number of nonzero elements in blocks in $\\pmb{G}$ . "], "page_idx": 23}, {"type": "text", "text": "at most $\\begin{array}{r}{N_{1}=7{\\binom{d+k}{d}}{\\binom{d+|\\xi|}{d}}^{2}n_{o}^{2}+4{\\binom{d+|\\xi|}{d}}n_{o}n_{b}+n_{b}^{2},}\\end{array}$ , because ", "text_level": 1, "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left(\\mathbf{he}\\operatorname{supher}\\mathbf{e}\\operatorname{for}\\mathbf{a}\\mathbf{a}\\right)}\\\\ &{=\\displaystyle\\sum_{k=0}^{t+1}\\left(\\mathbf{i}\\alpha\\operatorname{andner}\\mathbf{e}\\operatorname{for}\\mathbf{a}\\mathbf{e}\\operatorname{for}\\mathbf{a}\\mathbf{e}\\operatorname{for}\\mathbf{a}\\right)}\\\\ &{\\quad+\\nu\\beta\\alpha+\\left(\\mathbf{i}\\alpha+\\eta\\right)\\mathbf{be}\\alpha}\\\\ &{\\quad+2\\sum_{k=0}^{t}\\left(\\mathbf{i}\\alpha+\\eta\\right)\\mathbf{be}\\alpha+\\eta\\mathbf{be}\\alpha+\\eta\\mathbf{a}\\mathbf{e}\\operatorname{for}\\mathbf{a}\\mathbf{e}\\operatorname{for}\\mathbf{a}\\mathbf{e}}\\\\ &{\\quad+2\\sum_{k=0}^{t}\\mathbf{e}^{-\\frac{1}{k}+1}\\left(\\mathbf{i}\\alpha\\mathbf{~a}\\mathbf{a}\\right)\\mathbf{be}\\alpha+\\eta\\mathbf{a}}\\\\ &{\\lesssim\\eta\\mathbf{a}^{2}+\\sum_{k=1}^{t}\\left(\\mathbf{j}\\alpha_{k-1}\\left|\\prod_{\\alpha}\\mathbf{a}\\mathbf{e}\\right|^{2}\\pi_{\\alpha}^{2}\\right)+n_{0}^{2}}\\\\ &{\\quad+2\\left\\{\\sum_{k=1}^{t}\\left(\\|U_{\\alpha-1}\\|\\prod_{\\alpha}\\left|\\eta\\right|_{\\alpha}^{2}+\\eta\\right)+n_{0}\\eta\\cdot\\mathbf{a}\\right.\\Big.+\\displaystyle\\sum_{k=1}^{t}\\left(\\|U_{\\alpha-1}\\|\\prod_{\\alpha}\\left|\\prod_{\\alpha,\\alpha+1}\\left|\\prod_{\\alpha,\\alpha+1}\\left|\\pi_{\\alpha}^{2}\\right|+\\|U_{\\alpha+1}\\|_{\\alpha,\\alpha}\\right|\\right)\\right\\}}\\\\ &{\\quad+\\left.2\\left\\{\\sum_{k=1}^{t}\\left(\\|U_{\\alpha-1}\\|^{2}\\right)^{2}\\frac{L}{\\alpha}\\left|U_{\\alpha-1}\\left|\\eta\\right|_{\\alpha}^{2}+\\eta\\right\\}+\\eta\\mathbf{a}^{2}\\right.}\\\\ &{\\quad+\\left.2\\left\\{\\left(d+1\\right)\\right\\}\\sum_{k=1}^{t}\\left|U_{\\alpha-1}\\left|\\eta\\right|_{\\alpha}^{2}+n_{0}n+\\left(\\begin{array}{l}{4+1}\\\\ {d+1}\\end{array}\\right)^{2}\\sum_{k=1}^{t}\\left|U_{\\alpha-1}\\left|\\eta\\right|_{\\alpha}^{2}+\\left(\\begin{array}{l} \n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\leq\\left(1+2\\{\\pmod{d+|\\xi|}\\left(\\begin{array}{l}{d+k}\\\\ {d}\\end{array}\\right)\\left(\\begin{array}{c}{d+k}\\\\ {d}\\end{array}\\right)+4\\{\\pmod{d+|\\xi|}\\}^{2}{\\binom{d+k}{d}}\\right)n_{o}^{2}+2\\left(1+\\binom{d+|\\xi|}{d}\\right)\\right)n_{o}n_{b}+n_{b}^{2}}\\\\ &{\\leq}&{(1+2\\binom{d+k}{d})\\binom{d+|\\xi|}{d}^{2}n_{o}^{2}+4\\binom{d+|\\xi|}{d}n_{o}n_{b}+n_{b}^{2}}\\\\ &{=N_{1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Norms of partial derivative of $Q_{\\ell_{1},\\ell_{2}}$ . For each $i_{1},i_{2}\\in[n_{o}]$ , $r\\in[m]$ , and $\\ell\\in\\{0\\}\\cup[L]$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\begin{array}{r l r}&{\\left(Q_{0,0}\\right)_{i_{1},i_{2}}}&{\\left(146\\right)}\\\\ &{=\\cfrac{1}{m n_{o}}\\displaystyle\\sum_{\\ell=0}^{L}\\sum_{r=1}^{m}\\sum_{\\alpha\\in I_{\\xi_{\\ell}}}\\left(\\displaystyle\\sum_{\\beta\\in J_{\\Delta\\xi_{\\ell+1}}}\\hat{a}_{\\ell,\\alpha,\\beta}\\frac{\\partial^{\\beta}}{\\partial x^{\\beta}}\\left(({\\boldsymbol w}_{\\ell})_{r}^{\\top}{\\boldsymbol y}_{i_{1}}\\right)_{+}^{p}\\right)\\left(\\displaystyle\\sum_{\\beta\\in J_{\\Delta\\xi_{\\ell+1}}}\\hat{a}_{\\ell,\\alpha,\\beta}\\frac{\\partial^{\\beta}}{\\partial x^{\\beta}}\\left(({\\boldsymbol w}_{\\ell})_{r}^{\\top}{\\boldsymbol y}_{i_{2}}\\right)_{+}^{p}\\right)}\\\\ &{\\left\\Vert\\frac{\\partial}{\\partial({\\boldsymbol w}_{\\ell})_{r}}\\left(Q_{0,0}\\right)_{i_{1},i_{2}}\\right\\Vert_{2}}&{\\left(148\\right)}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n=\\left\\|\\frac{1}{m n_{o}}\\frac{\\partial}{\\partial\\left(w_{\\ell}\\right)_{r}}\\left\\{\\sum_{\\alpha\\in I_{\\xi_{\\ell}}}\\left(\\sum_{\\beta\\in J_{\\Delta\\xi_{\\ell+1}}}\\widehat{a}_{\\ell,\\alpha,\\beta}\\frac{\\partial^{\\beta}}{\\partial x^{\\beta}}\\left(\\left(w_{\\ell}\\right)_{r}^{\\top}y_{i_{1}}\\right)_{+}^{p}\\right)\\left(\\sum_{\\beta\\in J_{\\Delta\\xi_{\\ell+1}}}\\widehat{a}_{\\ell,\\alpha,\\beta}\\frac{\\partial^{\\beta}}{\\partial x^{\\beta}}\\left(\\left(w_{\\ell}\\right)_{r}^{\\top}y_{i_{2}}\\right)_{+}^{p}\\right)\\right\\}\\right\\|_{\\L{\\left(b_{\\ell}\\right)}^{-1}}=0.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\left\\lVert\\frac{\\lambda}{m n_{o}}\\frac{\\mathcal{C}}{\\partial\\left(w\\right)_{r}}\\left\\{\\sum_{\\alpha\\in I_{\\ell}}\\left\\{\\displaystyle\\sum_{\\beta\\in J_{\\Delta\\ell_{1}}}\\left(\\frac{\\partial}{\\partial\\xi_{2}\\alpha_{\\ell+1}}\\frac{\\partial}{\\partial x\\beta}\\left(\\left(w\\ell\\right)_{r}^{\\top}y_{i_{1}}\\right)_{+}^{p}\\right)\\right.\\left.\\right\\}\\left.\\left.\\left(\\underbrace{\\sum_{\\beta\\in J_{\\Delta\\ell_{1}}+\\Delta_{\\ell,\\beta}}\\frac{\\partial}{\\partial x\\beta}\\left(\\left(w\\right)_{r}^{\\top}y_{i_{2}}\\right)}_{\\left(\\Delta\\ell_{2}\\right)_{\\alpha\\left(r\\right)_{r}}}\\left(\\left(w\\right)_{r}^{\\top}y_{i_{1}}\\right)\\right)\\right\\vert^{2}\\right\\}\\left(\\underbrace{\\sum_{\\beta\\in J_{\\Delta\\ell_{1}}+\\Delta_{\\ell,\\beta}}\\frac{\\partial}{\\partial x\\beta}\\left(\\left(w\\right)_{r}^{\\top}y_{i_{2}}\\right)}_{\\left(\\Delta\\ell_{2}\\right)_{\\alpha\\left(r\\right)_{r}}}\\left(\\left(w\\right)_{r}^{\\top}y_{i_{2}}\\right)\\right)\\right\\}\\left(\\underbrace{\\mathcal{C}}_{\\left(\\alpha\\right)_{r}}\\left(\\left(w\\right)_{r}^{\\top}y_{i_{1}}\\right)_{\\left(\\Delta\\ell_{2}\\right)_{r}}\\left(\\left(w\\right)_{r}^{\\top}y_{i_{2}}\\right)}_{\\left(\\Delta\\ell_{3}\\right)_{\\alpha\\left(r\\right)_{r}}}\\left(\\left(w\\right)_{r}^{\\top}y_{i_{1}}\\right)\\right)}\\\\ &{\\leq\\frac{1}{m n_{o}}\\left\\vert I_{\\xi\\ell}\\left\\vert\\left(\\frac{\\partial}{\\partial\\left(w\\right)_{r}}\\right)_{i_{1},i_{2}}\\left\\{\\left(\\frac{\\mathcal{C}}{\\partial\\left(w\\right)_{r}}\\frac{\\partial^{\\beta}}{\\partial x\\beta}\\left(\\left(w\\ell\\right)_{r}^{\\top}y_{i_{1}}\\right)_{+}^{p}\\right)\\left(\\left(w\\ell\\right)_{r}^{\\top}y_{i_{1}}\\right)_{+}^{p}\\right\\}\\left(\\underbrace{\\mathcal{B}^{3}}_{\\beta\\in J_{\\Delta\\ell_{1}+1}}\\frac{\\partial^{3}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For each $\\ell\\in[L],\\alpha\\in I_{\\xi_{\\ell-1}},\\beta\\in I_{\\Delta\\xi_{\\ell}}$ , and $i_{1},i_{2}\\in[n_{o}]$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\left(Q_{0,i}\\right)_{i,\\{0,\\},j_{0}}}&{\\quad\\left(\\mathrm{I}\\leq\\rho\\right)}\\\\ &{=\\frac{\\sqrt{\\pi}}{m n}\\frac{\\sqrt{\\pi}}{m}\\left(\\left(\\int_{\\mathcal{S}^{2}(\\Delta_{t})}\\int_{\\mathcal{A}^{2}}\\left(w\\left(x_{t-1}\\right)^{\\top}y_{t}\\right)_{t}^{p^{\\prime}}\\left((w\\left(x_{t-1}\\right)^{\\top}y_{t}\\right)_{t}^{p^{\\prime}}\\right)\\right)\\left(\\frac{\\partial^{3}}{\\partial x^{3}}\\left(\\left(w\\left(x_{t-1}\\right)^{\\top}y_{t}\\right)_{t}^{p^{\\prime}}\\right)\\right)}&{\\quad\\mathrm{(I}\\leq\\rho)}\\\\ &{\\qquad\\qquad\\quad\\quad\\cdot\\left(\\int_{\\mathcal{S}^{2}(\\Delta_{t})}\\int_{\\mathcal{A}^{2}(\\Delta_{t})}\\int_{\\mathbb{R}^{p^{\\prime}}}\\left((w\\left(x_{t}\\right)^{\\top}y_{t}\\right)_{t}^{p^{\\prime}}\\left((w\\left(x_{t}\\right)^{\\top}y_{t}\\right)_{t}^{p^{\\prime}}\\right)\\,\\mathrm{d}\\left(w_{t}\\right)^{\\top}y_{t}\\right)\\,\\mathrm{d}\\left(\\mathrm{I}\\leq\\rho\\right)}\\\\ &{\\qquad\\qquad\\quad\\cdot\\left(\\int_{\\mathcal{S}^{2}(\\Delta_{t})_{i-1}}\\int_{\\mathbb{R}^{p^{\\prime}}}\\left(w\\left(x_{t-1}\\right)\\int_{\\mathbb{R}^{p^{\\prime}}}\\left(w\\left(x_{t}\\right)^{\\top}y_{t}\\right)_{t}^{p^{\\prime}}\\left((w_{t})^{\\top}y_{t}\\right)_{t}^{p^{\\prime}}\\right)\\,\\mathrm{d}\\left(\\mathrm{I}\\leq\\rho\\right)}\\\\ &{=\\left\\lVert\\frac{\\partial^{2}}{\\partial n\\rho}\\frac{\\partial}{\\partial\\left(w\\left(x_{t-1}\\right)_{t}\\right)_{t}}\\left\\{\\left(\\int_{\\mathcal{S}^{2}(\\Delta_{t})_{i-1}}\\int_{\\mathbb{R}^{p^{\\prime}}}\\left(\\left(w\\left(x_{t-1}\\right)^{\\top}y_{t}\\right)_{t}^{p^{\\prime}}\\left(\\left(w\\left(x_{t-1}\\right)^{\\top}y_{t}\\right)_{t}^{p \n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\lVert\\frac{\\partial}{\\partial(w_{i})}\\frac{\\partial}{\\partial(w_{i})}(Q_{0,i})_{i_{1},((\\alpha,\\beta),i_{2})}\\right\\rVert_{2}}\\\\ &{=\\left\\lVert-\\frac{\\sqrt{w_{i}}}{m n_{0}}\\frac{\\partial}{\\partial(w)_{r}}\\left(\\left(\\underbrace{\\sum_{j\\in\\mathcal{J}_{\\Delta\\ell+1}}\\hat{a}_{\\ell,\\alpha,\\beta^{\\prime}}\\frac{\\partial^{\\prime}}{\\partial x^{\\beta^{\\prime}}}\\left((w_{i})_{r}^{\\top}y_{i_{1}}\\right)_{+}^{p}\\right)\\left((w_{\\ell})_{r}^{\\top}y_{i_{2}}\\right)^{p}}_{\\Delta}\\right)\\right\\rVert_{2}}\\\\ &{\\le\\frac{\\sqrt{w_{i}^{\\prime}}}{m n_{0}}\\operatorname*{max}_{i\\in\\mathcal{J}_{\\Delta\\ell+1}}{p^{\\Delta\\ell+1}}{p^{\\Delta\\ell+1}}{\\left\\lVert(w_{i})_{r}^{\\top}\\right\\rVert}^{2p-1}}\\\\ &{\\le\\frac{\\sqrt{w_{i}^{\\prime}\\operatorname*{max}}}{m n_{0}}\\operatorname*{max}_{i}\\left(\\frac{d+|\\xi|}{d}\\right){p^{2}}^{1/1+1}{\\left\\lVert(w_{i})_{r}\\right\\rVert}^{2p-1},}\\\\ &{\\ \\ \\ \\frac{\\partial}{\\partial(w_{i})_{r}}(Q_{0,i})_{i_{1},((\\alpha,\\beta),i_{2})}=0,\\ \\mathrm{f~}\\ell^{\\prime}\\ne\\ell-1,\\ell,}\\\\ &{\\ \\ \\ \\frac{\\partial}{\\partial(v_{\\ell})_{r}}(Q_{0,i})_{i_{1},((\\alpha,\\beta),i_{2})}=0,\\ \\mathrm{f~}\\ell^{\\prime}\\in[L].}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For each $i\\in[n_{o}]$ and $j\\in[n_{b}]$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left(Q_{0,L+1}\\right)_{i,j}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\frac{1}{m}\\sqrt{\\frac{\\nu}{m_{t}m_{x}}}\\displaystyle\\sum_{c=1\\atop0}^{m}\\left(\\sum_{\\theta\\in\\mathcal{A}_{1}}\\omega_{t,i+1,i}\\frac{\\partial^{\\theta^{\\star}}}{\\partial x^{\\theta^{\\star}}}\\left((\\omega_{0})_{\\gamma}^{\\top}y_{i}\\right)_{+}^{p^{\\prime}}\\right)\\left(\\sum_{\\varphi\\in\\mathcal{A}_{1}}\\frac{\\partial^{\\theta^{\\star}}}{\\partial\\varphi^{\\varphi}}\\left((\\omega_{0})_{\\gamma}^{\\top}\\tilde{y}_{j}\\right)_{+}^{p^{\\prime}}\\right),}\\\\ &{\\bigg|\\left|\\frac{\\partial}{\\partial(\\theta\\varphi)_{r}}\\left(Q_{0,t+1\\rangle_{+},i}\\right)_{2}\\bigg|_{2}}\\\\ &{=\\left|\\frac{1}{m}\\sqrt{\\frac{\\nu}{m_{t}m_{x}}}\\frac{\\partial}{\\partial(\\varphi)_{\\theta}}\\right.\\left\\{\\left(\\sum_{\\varphi\\in\\mathcal{A}_{1}}\\frac{\\partial^{\\theta^{\\star}}}{\\partial x^{\\theta^{\\star}}}\\left((\\omega_{0})_{\\gamma}^{\\top}y_{i}\\right)_{+}^{p^{\\prime}}\\right)\\left(\\sum_{\\varphi\\in\\mathcal{A}_{1}}\\frac{\\partial^{\\theta^{\\star}}}{\\partial x^{\\theta^{\\star}}}\\left((\\omega_{0})_{\\gamma}^{\\top}\\tilde{y}_{j}\\right)_{+}^{p^{\\prime}}\\right)\\left(\\sum_{\\varphi\\in\\mathcal{A}_{1}}\\frac{\\partial^{\\theta^{\\star}}}{\\partial y^{\\varphi}}\\left((\\omega_{0})_{\\gamma}^{\\top}\\tilde{y}_{j}\\right)_{+}^{p^{\\prime}}\\right.}\\\\ &{\\left.\\leq\\frac{1}{m}\\sqrt{\\frac{\\nu}{m_{t}m_{x}}}\\Big(d+\\Delta\\xi_{1}\\right)\\left(d+1\\right)\\alpha_{m\\omega}^{2}p^{\\mathbb{A}+2}\\left[(\\omega_{0})_{\\gamma}\\right]_{-}^{2p-1}}\\\\ &{\\leq\\frac{1}{m}\\sqrt{\\frac{\\nu_{t}\\omega_{0}}{m_{t}}}\\Big(d+\\mathbb{I}\\Big)\\left(d+1\\right)\\alpha_{m\\omega}^{2}p^{\\mathbb{A}+2}\\left[(\\omega_{0})\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For each $\\ell\\in[L],\\alpha_{1},\\alpha_{2}\\in I_{\\xi_{\\ell-1}},\\beta_{1},\\beta_{2}\\in I_{\\Delta\\xi_{\\ell}},$ , and $i_{1},i_{2}\\in[n_{o}]$ , ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r l}&{\\left(Q_{\\mathcal{E}_{i}}\\right)((\\omega_{{t+1}},\\omega_{\\mathcal{E}_{i}}),\\omega_{i})}&&{\\in\\Omega}\\\\ &{=\\frac{\\nu_{\\mathcal{E}_{i}}}{m n_{\\omega_{\\tau}}}\\sum_{i=1}^{m}\\left(\\left(\\frac{\\partial^{3}}{\\partial x^{\\mathcal{E}_{i}}}\\left((w_{t-1})_{i}^{\\top}\\cdot y_{i}\\right)_{i}\\right)^{p}\\right)\\left(\\frac{\\partial^{3}}{\\partial x^{\\mathcal{E}_{i}}}\\left((w_{t-1})_{i}^{\\top}\\cdot y_{i}\\right)_{i}^{p}\\right)\\mathbf{1}_{\\{\\alpha_{1}=\\alpha_{2}\\}}}&&{}\\\\ &{\\qquad\\qquad+\\left((w_{t})_{\\mathcal{E}_{i}}^{\\top}y_{i}\\right)_{i}^{p}\\left((w_{t})_{\\mathcal{E}_{i}}^{\\top}y_{i}\\right)_{i}^{p}\\mathbf{1}_{\\{\\alpha_{1}+\\beta_{1}=\\alpha_{2}+\\beta_{2}\\}}\\right),}&&{\\in\\Omega,}\\\\ &{\\left.\\left.\\left\\Vert\\frac{\\partial}{\\partial(w_{\\mathcal{E}_{i}}-1)_{\\mathcal{E}_{i}}}\\left(Q_{\\mathcal{E}_{i}}\\right)((\\omega_{1},\\beta_{1})_{i},(\\alpha_{2},\\beta_{i}),\\omega_{1})\\right\\Vert_{\\mathcal{E}_{i}}\\right.}&&{\\in\\Omega,}\\\\ &{=\\mathbf{1}_{\\left(\\alpha_{1}=\\alpha_{2}\\right)}\\left\\Vert\\frac{\\nu_{\\mathcal{E}_{i}}}{m n_{\\phi}}\\frac{\\partial}{\\partial(w_{\\mathcal{E}_{i}}-1)_{\\mathcal{E}_{i}}}\\left\\{\\left(\\frac{\\partial^{3}}{\\partial x^{\\mathcal{E}_{i}}}\\left((w_{t-1})_{i}^{\\top}\\cdot y_{i}\\right)_{i}\\right)_{i}^{p}\\right\\}\\left(\\frac{\\partial^{3}}{\\partial x^{\\mathcal{E}_{i}}}\\left((w_{t-1})_{i}^{\\top}\\cdot y_{i}\\right)_{i}^{p}\\right)\\right\\}\\right\\Vert_{\\mathcal{E}_{i}}}\\\\ &{\\le\\mathbf{1}_{\\left(\\alpha_{1}=\\alpha_{2}\\right)} \n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and ", "text_level": 1, "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\displaystyle\\frac{\\partial}{\\partial\\left(w_{\\ell}\\right)_{r}}\\left(Q_{\\ell,\\ell}\\right)_{\\left((\\alpha_{1},\\beta_{1}),i_{1}\\right),\\left((\\alpha_{2},\\beta_{2}),i_{2}\\right)}\\right\\|_{2}}\\\\ &{=\\mathbf{1}_{\\left(\\alpha_{1}+\\beta_{1}=\\alpha_{2}+\\beta_{2}\\right)}\\left\\|\\displaystyle\\frac{\\nu_{\\ell}}{m\\alpha_{0}}\\frac{\\partial}{\\partial\\left(w_{\\ell}\\right)_{r}}\\left\\{\\left(\\left(w_{\\ell}\\right)_{r}^{\\top}y_{i_{1}}\\right)_{+}^{p}\\left(\\left(w_{\\ell}\\right)_{r}^{\\top}y_{i_{2}}\\right)_{+}^{p}\\right\\}\\right\\|_{2}}\\\\ &{\\leq\\mathbf{1}_{\\left(\\alpha_{1}+\\beta_{1}=\\alpha_{2}+\\beta_{2}\\right)}\\displaystyle\\frac{\\nu_{\\ell}}{m\\alpha_{0}}p\\left\\|\\left(w_{\\ell}\\right)_{r}\\right\\|_{2}^{2p-1}}\\\\ &{\\leq\\mathbf{1}_{\\left(\\alpha_{1}+\\beta_{1}=\\alpha_{2}+\\beta_{2}\\right)}\\displaystyle\\frac{\\nu_{\\operatorname*{max}}}{m\\alpha_{0}}p\\left\\|\\left(w_{\\ell}\\right)_{r}\\right\\|_{2}^{2p-1},}\\\\ &{\\displaystyle\\frac{\\partial}{\\partial\\left(w_{\\ell}\\right)_{r}}\\left(Q_{\\ell,\\ell}\\right)_{i_{1},i_{2}}=\\mathbf{0},\\,\\forall\\ell^{\\prime}\\neq\\ell-1,\\ell,}\\\\ &{\\displaystyle\\frac{\\partial}{\\partial\\left(\\nu_{\\ell}\\right)_{r}}\\left(Q_{\\ell,\\ell}\\right)_{i_{1},i_{2}}=0,\\,\\forall\\ell^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Similarly, for each $\\ell\\;\\in\\;\\{1,\\cdots\\;,L-1\\}$ , $\\alpha_{1}~\\in~I_{\\xi_{\\ell-1}}$ , $\\alpha_{2}~\\in~I_{\\xi_{\\ell}}$ , $\\beta_{1}\\ \\in\\ I_{\\Delta\\xi_{\\ell}},\\beta_{2}\\ \\in\\ I_{\\Delta\\xi_{\\ell+1}}$ and $i_{1},i_{2}\\in\\dot{[n_{o}]}$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left(Q_{\\ell,\\ell+1}\\right)_{\\left(\\left(\\alpha_{1},\\beta_{1}\\right),i_{1}\\right),\\left(\\left(\\alpha_{2},\\beta_{2}\\right),i_{2}\\right)}}\\\\ &{=\\!\\frac{\\sqrt{\\nu_{\\ell}\\nu_{\\ell+1}}}{m n_{o}}\\displaystyle\\sum_{r=1}^{m}\\left(-\\left(\\left(w_{\\ell}\\right)_{r}^{\\top}y_{i_{1}}\\right)_{+}^{p}\\right)\\left(\\frac{\\partial^{\\beta_{2}}}{\\partial x^{\\beta_{2}}}\\left(\\left(w_{\\ell}\\right)_{r}^{\\top}y_{i_{2}}\\right)_{+}^{p}\\right)\\mathbf{1}_{\\left\\{\\alpha_{2}=\\alpha_{1}+\\beta_{1}\\right\\}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|\\displaystyle\\frac{\\partial}{\\partial\\left(\\boldsymbol{w}_{\\ell}\\right)_{r}}\\left(\\boldsymbol{Q}_{\\ell,\\ell+1}\\right)_{((\\alpha_{1},\\beta_{1}),i_{1}),((\\alpha_{2},\\beta_{2}),i_{2})}\\right\\|_{2}}\\\\ &{\\le\\mathbf{1}_{\\{\\alpha_{2}=\\alpha_{1}+\\beta_{1}\\}}\\frac{\\sqrt{\\nu_{\\ell}\\nu_{\\ell+1}}}{m n_{o}}p^{\\Delta\\xi_{\\ell+1}+1}\\left\\|(\\boldsymbol{w}_{\\ell})_{r}\\right\\|_{2}^{2p-1}}\\\\ &{\\le\\mathbf{1}_{\\{\\alpha_{2}=\\alpha_{1}+\\beta_{1}\\}}\\frac{\\nu_{\\operatorname*{max}}}{m n_{o}}p^{|\\xi|+1}\\left\\|(\\boldsymbol{w}_{\\ell})_{r}\\right\\|_{2}^{2p-1},}\\\\ &{\\quad\\frac{\\partial}{\\partial\\left(\\boldsymbol{w}_{\\ell}\\right)_{r}}\\left(\\boldsymbol{Q}_{\\ell,\\ell+1}\\right)_{((\\alpha_{1},\\beta_{1}),i_{1}),((\\alpha_{2},\\beta_{2}),i_{2})}=\\mathbf{0},\\,\\forall\\ell^{\\prime}\\neq\\boldsymbol{\\ell},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial\\left(\\pmb{v}_{\\ell^{\\prime}}\\right)_{r}}\\left(\\pmb{Q}_{\\ell,\\ell+1}\\right)_{((\\alpha_{1},\\beta_{1}),i_{1}),((\\alpha_{2},\\beta_{2}),i_{2})}=\\mathbf{0},\\,\\forall\\ell^{\\prime}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For each $\\ell\\in[L],\\alpha=1,\\beta\\in I_{\\Delta\\xi_{1}},i\\in[n_{o}]$ and $j\\in[n_{b}]$ , ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left.\\left(Q_{i,l+1}\\right)_{(n,0),l,j}\\!\\!\\!\\!\\!}\\\\ &{=\\!\\!\\frac{1}{m}\\!\\sum_{n,n,\\gamma}^{\\overline{{\\gamma}},\\overline{{n}}}\\!\\frac{\\left(\\frac{\\partial^{2}}{\\partial x^{2}}\\left(\\left(w_{0}\\right)_{\\gamma}^{\\gamma}y_{j}\\right)\\right)^{\\nu}}{m!}\\!\\left(\\sum_{\\alpha,\\gamma,i}^{\\overline{{\\gamma}},\\overline{{n}}}\\!\\!\\!\\!\\!\\frac{\\partial^{\\nu}}{\\partial x^{\\alpha}}\\!\\left(\\left(w_{0}\\right)_{\\gamma}^{\\gamma}\\frac{\\partial_{j}}{\\partial y}\\right)_{\\gamma,j}\\right)\\!\\!\\!\\right)_{{\\{t:\\}=1\\}}\\!\\!\\!\\!\\!}\\\\ &{\\left.\\left[\\frac{\\partial}{\\partial\\left(\\alpha,\\gamma\\right)}\\!\\left(Q_{i,l+1}\\right)_{((n,0),\\gamma),\\overline{{\\Omega}}_{l}}\\right]\\!\\!\\!\\right)\\!\\!\\!,\\left[2\\!\\!\\!\\!\\!\\!}\\\\ &{=\\!\\!\\!\\left[\\frac{\\partial}{\\partial\\left(\\alpha,\\gamma\\right)\\partial_{i}}\\!\\left(\\!\\frac{\\partial}{\\partial\\left(\\alpha,\\gamma\\right)}\\!\\left(\\frac{\\partial}{\\partial\\gamma_{i}}\\!\\left(\\left(\\omega_{0}\\right)_{\\gamma}^{\\gamma}y_{i}\\right)_{\\gamma}\\right)_{\\gamma}\\!\\!\\left(\\frac{\\partial}{\\partial\\gamma_{i}}\\!\\left(\\frac{\\partial}{\\partial\\gamma_{i}}\\!\\left(w_{0}\\right)_{\\gamma}^{\\gamma}y_{i}\\right)_{\\gamma}\\right)\\!\\!\\left(\\sum_{\\alpha\\in\\mathcal{E}_{j,l}}\\!\\frac{\\partial^{\\nu}}{\\partial\\alpha\\partial_{j}\\gamma^{\\alpha}}\\left(\\left(w_{0}\\right)_{\\gamma}^{\\gamma}\\frac{\\partial}{\\partial y}\\right)_{\\gamma}\\right)\\!\\!\\right)\\right]\\!\\right\\}\\!\\Bigg|_{{\\mathbf{r}_{i}}}}\\\\ &{\\leq\\!\\frac{\\sqrt{\\gamma_{i}\\gamma_{j}}}{m!\\gamma_{i}\\gamma_{j}\\partial_{i}\\alpha_{j}}\\!\\left(\\!-d_{1}\\!\\gamma_{i}^{\\beta+2}\\!\\left(\\left[w_{0}\\right]_{\\gamma}\\right)_{j}\\!\\!\\!\\!\\frac{\\partial^{2-1}}{\\partial\\gamma_{i}}\\!\\left(\\\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "For each $j_{1},j_{2}\\in[n_{b}]$ , the direct calculations lead to ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\left(Q_{L+1,L+1}\\right)_{j_{1},j_{2}}=\\frac{\\nu}{m n_{b}}\\sum_{r=1}^{m}\\left(\\sum_{\\alpha\\in J_{1}}\\Tilde{a}_{\\alpha}\\frac{\\partial^{\\alpha}}{\\partial x^{\\alpha}}\\left(\\left(w_{0}\\right)_{r}^{\\top}\\Tilde{y}_{j_{1}}\\right)_{+}^{p}\\right)\\left(\\sum_{\\alpha\\in J_{1}}\\Tilde{a}_{\\alpha}\\frac{\\partial^{\\alpha}}{\\partial x^{\\alpha}}\\left(\\left(w_{0}\\right)_{r}^{\\top}\\Tilde{y}_{j_{2}}\\right)_{+}^{p}\\right)_{j_{1},j_{2}},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\frac{\\partial}{\\partial\\left(\\pmb{w}_{0}\\right)_{r}}\\left(\\pmb{Q}_{L+1,L+1}\\right)_{j_{1},j_{2}}\\right\\|_{2}\\leq\\frac{\\nu}{m n_{b}}a_{\\operatorname*{max}}^{2}\\left(d+1\\right)^{2}p^{3}\\left\\|\\left(\\pmb{w}_{0}\\right)_{r}\\right\\|_{2}^{2p-1},}\\\\ &{\\qquad\\frac{\\partial}{\\partial\\left(\\pmb{w}_{\\ell}\\right)_{r}}\\left(\\pmb{Q}_{L+1,L+1}\\right)_{j_{1},j_{2}}=\\pmb{0},\\,\\forall\\ell\\neq0,}\\\\ &{\\qquad\\frac{\\partial}{\\partial\\left(\\pmb{v}_{\\ell}\\right)_{r}}\\left(\\pmb{Q}_{L+1,L+1}\\right)_{j_{1},j_{2}}=\\pmb{0},\\,\\forall\\ell.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Hoeffding\u2019s inequalities for $Q_{\\ell_{1},\\ell_{2}}$ . For each $i\\in[n_{o}]$ and $j\\in[n_{b}]$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left|\\displaystyle\\frac{\\partial^{\\beta}}{\\partial x^{\\beta}}\\left((w_{\\ell})_{r}^{\\top}\\,y_{i}\\right)_{+}^{p}\\right|\\leq p^{|\\beta|}\\left\\|(w_{\\ell})_{r}\\right\\|_{2}^{p},}\\\\ &{\\left|\\displaystyle\\frac{\\partial^{\\beta}}{\\partial x^{\\beta}}\\left((w_{\\ell-1})_{r}^{\\top}\\,y_{i}\\right)_{+}^{p}\\right|\\leq p^{|\\beta|}\\left\\|(w_{\\ell-1})_{r}\\right\\|_{2}^{p},}\\\\ &{\\quad\\quad\\quad\\quad\\left|\\left((w_{\\ell})_{r}^{\\top}\\,y_{i}\\right)_{+}^{p}\\right|\\leq\\left\\|(w_{\\ell})_{r}\\right\\|_{2}^{p},}\\\\ &{\\left|\\displaystyle\\frac{\\partial^{\\alpha}}{\\partial x^{\\alpha}}\\left((w_{0})_{r}^{\\top}\\,\\tilde{y}_{j}\\right)_{+}^{p}\\right|\\leq p^{|\\alpha|}\\left\\|(w_{0})_{r}\\right\\|_{2}^{p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then, for each $i_{1},i_{2}\\in[n_{o}]$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\left|\\sum_{\\ell=0}^{L}\\sum_{\\alpha\\in I_{\\xi_{\\ell}}}\\left(\\sum_{\\beta\\in J_{\\Delta\\xi_{\\ell+1}}}\\hat{a}_{\\ell,\\alpha,\\beta}\\frac{\\partial^{\\beta}}{\\partial x^{\\beta}}\\left(\\left(w_{\\ell}\\right)_{r}^{\\top}y_{i_{1}}\\right)_{+}^{p}\\right)\\left(\\sum_{\\beta\\in J_{\\Delta\\xi_{\\ell+1}}}\\hat{a}_{\\ell,\\alpha,\\beta}\\frac{\\partial^{\\beta}}{\\partial x^{\\beta}}\\left(\\left(w_{\\ell}\\right)_{r}^{\\top}y_{i_{2}}\\right)_{+}^{p}\\right)\\right|\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\displaystyle\\sum_{\\ell=0}^{L}\\displaystyle\\sum_{\\alpha\\in I_{\\ell_{\\ell}}}\\left(\\sum_{\\beta\\in J_{\\Delta\\ell_{\\ell+1}}}a_{\\operatorname*{max}}p^{|\\beta|}\\left\\Vert(w_{\\ell})_{r}\\right\\Vert_{2}^{p}\\right)\\left(\\sum_{\\beta\\in J_{\\Delta\\ell_{\\ell+1}}}a_{\\operatorname*{max}}p^{|\\beta|}\\left\\Vert(w_{\\ell})_{r}\\right\\Vert_{2}^{p}\\right)}\\\\ &{\\leq\\displaystyle\\sum_{\\ell=0}^{L}\\sum_{\\alpha\\in I_{\\ell_{\\ell}}}\\left\\Vert J_{\\Delta\\ell_{\\ell+1}}\\right\\Vert^{2}a_{\\operatorname*{max}}^{2}p^{2|\\Delta\\ell_{\\ell+1}|}R^{2p}}\\\\ &{\\leq\\displaystyle\\sum_{\\ell=0}^{L}|I_{\\ell,\\ell}|\\left(\\left\\{\\begin{array}{c}{d+|\\xi|}\\\\ {d}\\end{array}\\right\\}^{2}a_{\\operatorname*{max}}^{2}p^{2|\\xi|}R^{2p}\\right.}\\\\ &{=\\left(\\left.\\frac{d+k}{d}\\right)\\left(\\begin{array}{c}{d+|\\xi|}\\\\ {d}\\end{array}\\right)^{2}a_{\\operatorname*{max}}^{2}p^{2|\\xi|}R^{2p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By Hoeffding\u2019s inequality with ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\left(Q_{0,0}\\right)_{i_{1},i_{2}}}}\\\\ {{\\displaystyle=\\sum_{r=1}^{m}\\sum_{\\ell=0}^{L}\\sum_{\\alpha\\in I_{\\ell_{\\ell}}}\\left(\\frac\\partial{\\partial\\left(\\boldsymbol{v}_{\\ell}\\right)_{r,\\alpha}}s_{i_{1}}\\right)\\left(\\frac\\partial{\\partial\\left(\\boldsymbol{v}_{\\ell}\\right)_{r,\\alpha}}s_{i_{2}}\\right)}}\\\\ {{\\displaystyle=\\frac1{m n_{o}}\\sum_{\\ell=0}^{L}\\sum_{r=1}^{m}\\sum_{\\alpha\\in I_{\\ell_{\\ell}}}\\left(\\sum_{\\beta\\in J_{\\Delta\\xi_{\\ell+1}}}\\hat{a}_{\\ell,\\alpha,\\beta}\\frac{\\partial^{\\beta}}{\\partial\\boldsymbol{x}^{\\beta}}\\left(\\left(\\boldsymbol{w}_{\\ell}\\right)_{r}^{\\top}\\boldsymbol{y}_{i_{1}}\\right)_{+}^{p}\\right)\\left(\\sum_{\\beta\\in J_{\\Delta\\xi_{\\ell+1}}}\\hat{a}_{\\ell,\\alpha,\\beta}\\frac\\partial{\\partial^{\\beta}}{\\partial\\boldsymbol{x}^{\\beta}}\\left(\\left(\\boldsymbol{w}_{\\ell}\\right)_{r}^{\\top}\\boldsymbol{y}_{i_{2}}\\right)_{+}^{p}\\right)\\left(\\sum_{\\beta\\in J_{\\Delta\\xi_{\\ell+1}}}\\hat{a}_{\\ell,\\alpha,\\beta}\\frac{\\partial^{\\beta}}{\\partial\\boldsymbol{x}^{\\beta}}\\left(\\left(\\boldsymbol{w}_{\\ell}\\right)_{r}^{\\top}\\boldsymbol{y}_{i_{2}}\\right)_{-}^{p}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|\\left(Q_{0,0}\\right)_{i_{1},i_{2}}-\\mathbb{E}\\left[\\left(Q_{0,0}\\right)_{i_{1},i_{2}}\\right]\\right|>\\varepsilon\\right]<2\\exp\\left(-\\frac{m n_{o}^{2}\\varepsilon^{2}}{2\\binom{d+k}{d}^{2}\\binom{d+|\\xi|}{d}^{4}a_{\\operatorname*{max}}^{4}p^{4|\\xi|}R^{4p}}\\right),\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "for any $\\varepsilon>0$ . ", "page_idx": 27}, {"type": "text", "text": "Similarly, for each $\\ell\\in[L],\\alpha\\in I_{\\xi_{\\ell-1}},\\beta\\in\\Delta\\xi_{\\ell}.$ , and $i_{1},i_{2}\\in[n_{o}]$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left(\\displaystyle\\sum_{\\beta^{\\prime}\\in J_{\\Delta\\xi_{\\ell}}}\\hat{a}_{\\ell-1,\\alpha,\\beta^{\\prime}}\\frac{\\partial^{\\beta^{\\prime}}}{\\partial{x^{\\beta^{\\prime}}}}\\left(\\left({w_{\\ell-1}}\\right)_{r}^{\\top}{y_{i_{1}}}\\right)_{+}^{p}\\right)\\left(\\frac{\\partial^{\\beta}}{\\partial{x^{\\beta}}}\\left(\\left({w_{\\ell-1}}\\right)_{r}^{\\top}{y_{i_{2}}}\\right)_{+}^{p}\\right)\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\left.-\\left(\\displaystyle\\sum_{\\beta^{\\prime}\\in J_{\\Delta\\xi_{\\ell+1}}}\\hat{a}_{\\ell,\\alpha,\\beta^{\\prime}}\\frac{\\partial^{\\beta^{\\prime}}}{\\partial{x^{\\beta^{\\prime}}}}\\left(\\left({w_{\\ell}}\\right)_{r}^{\\top}{y_{i_{1}}}\\right)_{+}^{p}\\right)\\left(\\left({w_{\\ell}}\\right)_{r}^{\\top}{y_{i_{2}}}\\right)_{+}^{p}\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\leq\\binom{d+\\Delta\\xi_{\\ell}}{\\Delta\\xi_{\\ell}}a_{\\operatorname*{max}}p^{\\Delta\\xi_{\\ell}}\\left\\|(w_{\\ell-1})_{r}\\right\\|_{2}^{p}\\cdot p^{|\\beta|}\\left\\|(w_{\\ell-1})_{r}\\right\\|_{2}^{p}+\\binom{d+\\Delta\\xi_{\\ell+1}}{\\Delta\\xi_{\\ell+1}}a_{\\operatorname*{max}}p^{\\Delta\\xi_{\\ell+1}}\\left\\|(w_{\\ell})_{r}\\right\\|_{2}^{p}\\left\\|\\right\\|_{2}^{2}}\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\left(223\\right)}\\\\ &{}&{\\leq\\binom{d+|\\xi|}{d}\\left(p^{|\\xi|}+1\\right)a_{\\operatorname*{max}}p^{|\\xi|}R^{2p}.\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\left(224\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By Hoeffding\u2019s inequality with ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\boldsymbol{Q}_{0,\\ell}\\right)_{i_{1},((\\alpha,\\beta),i_{2})}}\\\\ &{=\\frac{\\sqrt{\\nu_{\\ell}}}{m n_{o}}\\displaystyle\\sum_{r=1}^{m}\\left(\\left(\\sum_{\\beta^{\\prime}\\in J_{\\Delta\\xi_{\\ell}}}\\hat{a}_{\\ell-1,\\alpha,\\beta^{\\prime}}\\frac{\\partial^{\\beta^{\\prime}}}{\\partial\\boldsymbol{x}^{\\beta^{\\prime}}}\\left(\\left(\\boldsymbol{w}_{\\ell-1}\\right)_{r}^{\\top}\\boldsymbol{y}_{i_{1}}\\right)_{+}^{p}\\right)\\left(\\frac{\\partial^{\\beta}}{\\partial\\boldsymbol{x}^{\\beta}}\\left(\\left(\\boldsymbol{w}_{\\ell-1}\\right)_{r}^{\\top}\\boldsymbol{y}_{i_{2}}\\right)_{+}^{p}\\right)\\right.}\\\\ &{\\qquad\\qquad\\qquad-\\left.\\left(\\sum_{\\beta^{\\prime}\\in J_{\\Delta\\xi_{\\ell+1}}}\\hat{a}_{\\ell,\\alpha,\\beta^{\\prime}}\\frac{\\partial^{\\beta^{\\prime}}}{\\partial\\boldsymbol{x}^{\\beta^{\\prime}}}\\left(\\left(\\boldsymbol{w}_{\\ell}\\right)_{r}^{\\top}\\boldsymbol{y}_{i_{1}}\\right)_{+}^{p}\\right)\\left(\\left(\\boldsymbol{w}_{\\ell}\\right)_{r}^{\\top}\\boldsymbol{y}_{i_{2}}\\right)_{+}^{p}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left.\\begin{array}{r}{\\left[\\left|\\left(Q_{0,\\ell}\\right)_{i_{1},((\\alpha,\\beta),i_{2})}-\\mathbb{E}\\left[\\left(Q_{0,\\ell}\\right)_{i_{1},((\\alpha,\\beta),i_{2})}\\right]\\right|>\\varepsilon\\right]<2\\exp\\left(-\\frac{m n_{o}^{2}\\varepsilon^{2}}{2\\nu_{\\ell}\\left(\\binom{d+|\\xi|}{d}\\right)^{2}\\left(p!\\varepsilon\\mid+1\\right)^{2}a_{\\operatorname*{max}}^{2}p^{2|\\xi|}H}\\right)>2\\exp\\left(-\\frac{2\\sqrt{2}\\varepsilon^{2}}{2\\nu_{\\ell}\\left(\\binom{d+|\\xi|}{d}\\right)^{2}}\\right).}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "for any $\\varepsilon>0$ . ", "page_idx": 28}, {"type": "text", "text": "For each $i\\in[n_{o}]$ and $j\\in[n_{b}]$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left(\\displaystyle\\sum_{\\beta\\in J_{\\Delta\\xi_{1}}}\\hat{a}_{0,1,\\beta}\\frac{\\partial^{\\beta}}{\\partial x^{\\beta}}\\left((\\boldsymbol{w}_{0})_{r}^{\\top}\\boldsymbol{y}_{i}\\right)_{+}^{p}\\right)\\left(\\displaystyle\\sum_{\\alpha\\in J_{1}}\\tilde{a}_{\\alpha}\\frac{\\partial^{\\alpha}}{\\partial x^{\\alpha}}\\left((\\boldsymbol{w}_{0})_{r}^{\\top}\\tilde{\\boldsymbol{y}}_{j}\\right)_{+}^{p}\\right)\\right|}\\\\ &{\\leq\\left(\\displaystyle\\sum_{\\Delta\\xi_{1}}^{d+\\Delta\\xi_{1}}\\right)a_{\\operatorname*{max}}p^{\\Delta\\xi_{1}}\\left\\|(\\boldsymbol{w}_{0})_{r}\\right\\|_{2}^{p}\\left(\\displaystyle\\sum_{1}^{d+1}\\!\\!a_{\\operatorname*{max}}p^{1}\\left\\|(\\boldsymbol{w}_{0})_{r}\\right\\|_{2}^{p}\\right.}\\\\ &{\\leq(d+1)\\left(\\displaystyle\\begin{array}{c}{d+|\\xi|}\\\\ {d}\\end{array}\\right)a_{\\operatorname*{max}}^{2}p^{|\\xi|+1}R^{2p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By Hoeffding\u2019s inequality with ", "page_idx": 28}, {"type": "equation", "text": "$$\nQ_{0,L+1}\\bigr)_{i,j}=\\frac{1}{m}\\sqrt{\\frac{\\nu}{n_{o}n_{b}}}\\sum_{r=1}^{m}\\left(\\sum_{\\beta\\in J_{\\Delta\\xi_{1}}}\\hat{a}_{0,1,\\beta}\\frac{\\partial^{\\beta}}{\\partial x^{\\beta}}\\left(\\left(w_{0}\\right)_{r}^{\\top}y_{i}\\right)_{+}^{p}\\right)\\left(\\sum_{\\alpha\\in J_{1}}\\tilde{a}_{\\alpha}\\frac{\\partial^{\\alpha}}{\\partial x^{\\alpha}}\\left(\\left(w_{0}\\right)_{r}^{\\top}\\tilde{y}_{j}\\right)_{+}^{p}\\right)\\left(\\sum_{\\alpha\\in J_{1}}\\tilde{a}_{\\alpha}\\frac{\\partial^{\\beta}}{\\partial x^{\\alpha}}\\left(\\left(w_{0}\\right)_{r}^{\\top}\\tilde{y}_{j}\\right)_{+}^{p}\\right)\\left(\\sum_{\\alpha\\in J_{1}}\\tilde{a}_{\\alpha}\\frac{\\partial^{\\alpha}}{\\partial x^{\\beta}}\\left(\\left(w_{0}\\right)_{r}^{\\top}\\tilde{y}_{j}\\right)_{+}^{p}\\right)\\left(\\left(y_{0}\\right)_{r}^{\\top}\\tilde{y}_{j}\\right)_{+}^{p}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|\\left(Q_{0,L+1}\\right)_{i,j}-\\mathbb{E}\\left[\\left(Q_{0,L+1}\\right)_{i,j}\\right]\\right|>\\varepsilon\\right]<2\\exp\\left(-\\frac{m n_{o}n_{b}\\varepsilon^{2}}{2\\nu\\left(d+1\\right)^{2}\\left(\\frac{d+\\left|\\xi\\right|}{d}\\right)^{2}a_{\\operatorname*{max}}^{4}p^{2\\left|\\xi\\right|+2}R^{4p}}\\right),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "for any $\\varepsilon>0$ . ", "page_idx": 28}, {"type": "text", "text": "For each $\\ell\\in[L],\\alpha_{1},\\alpha_{2}\\in I_{\\xi_{\\ell-1}},\\beta_{1},\\beta_{2}\\in J_{\\Delta\\xi_{\\ell}},$ , and $i_{1},i_{2}\\in[n_{o}]$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left(\\frac{\\partial^{\\beta_{1}}}{\\partial{x^{\\beta_{1}}}}\\left((w_{\\ell-1})_{r}^{\\top}\\;y_{i_{1}}\\right)_{+}^{p}\\right)\\left(\\frac{\\partial^{\\beta_{2}}}{\\partial{x^{\\beta_{2}}}}\\left((w_{\\ell-1})_{r}^{\\top}\\;y_{i_{2}}\\right)_{+}^{p}\\right)\\mathbf{1}_{\\left\\{\\alpha_{1}=\\alpha_{2}\\right\\}}\\right.}\\\\ &{\\qquad\\qquad\\qquad+\\left.\\left((w_{\\ell})_{r}^{\\top}\\;y_{i_{1}}\\right)_{+}^{p}\\left((w_{\\ell})_{r}^{\\top}\\;y_{i_{2}}\\right)_{+}^{p}\\mathbf{1}_{\\left\\{\\alpha_{1}+\\beta_{1}=\\alpha_{2}+\\beta_{2}\\right\\}}\\right|}\\\\ &{\\leq p^{|\\beta_{1}|+|\\beta_{2}|}R^{2p}\\mathbf{1}_{\\left\\{\\alpha_{1}=\\alpha_{2}\\right\\}}+R^{2p}\\mathbf{1}_{\\left\\{\\alpha_{1}+\\beta_{1}=\\alpha_{2}+\\beta_{2}\\right\\}}}\\\\ &{\\leq\\left(p^{2|\\xi|}+1\\right)R^{2p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By Hoeffding\u2019s inequality with ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left(\\boldsymbol{Q}_{\\ell,\\ell}\\right)_{((\\alpha_{1},\\beta_{1}),i_{1}),((\\alpha_{2},\\beta_{2}),i_{2})}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{=\\frac{\\nu_{\\ell}}{m n_{o}}\\sum_{r=1}^{m}\\left(\\left(\\frac{\\partial^{\\beta_{1}}}{\\partial x^{\\beta_{1}}}\\left(\\left(w_{\\ell-1}\\right)_{r}^{\\top}y_{i_{1}}\\right)_{+}^{p}\\right)\\left(\\frac{\\partial^{\\beta_{2}}}{\\partial x^{\\beta_{2}}}\\left(\\left(w_{\\ell-1}\\right)_{r}^{\\top}y_{i_{2}}\\right)_{+}^{p}\\right)\\mathbf{1}_{\\left\\{\\alpha_{1}=\\alpha_{2}\\right\\}}\\right.}}\\\\ &{\\quad\\quad\\quad+\\left.\\left(\\left(w_{\\ell}\\right)_{r}^{\\top}y_{i_{1}}\\right)_{+}^{p}\\left(\\left(w_{\\ell}\\right)_{r}^{\\top}y_{i_{2}}\\right)_{+}^{p}\\mathbf{1}_{\\left\\{\\alpha_{1}+\\beta_{1}=\\alpha_{2}+\\beta_{2}\\right\\}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left.\\left[\\left|\\left(Q_{\\ell,\\ell}\\right)_{((\\alpha_{1},\\beta_{1}),i_{1}),((\\alpha_{2},\\beta_{2}),i_{2})}-\\mathbb{E}\\left[\\left(Q_{\\ell,\\ell}\\right)_{((\\alpha_{1},\\beta_{1}),i_{1}),((\\alpha_{2},\\beta_{2}),i_{2})}\\right]\\right|>\\varepsilon\\right]<2\\exp\\left(-\\frac{m n_{o}^{2}\\varepsilon^{2}}{2\\nu_{\\ell}^{2}\\left(p^{2|\\xi|}+1\\right)}\\right)\\,.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for any $\\varepsilon>0$ . ", "page_idx": 29}, {"type": "text", "text": "For each $\\ell\\in[L],\\alpha_{1}\\in I_{\\xi_{\\ell-1}},\\alpha_{2}\\in I_{\\xi_{\\ell}},\\beta_{1}\\in I_{\\Delta\\xi_{\\ell}},\\beta_{2}\\in I_{\\Delta\\xi_{\\ell+1}},\\mathrm{and~}i_{1},$ $i_{1},i_{2}\\in[n_{o}]$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left|\\left(\\left(\\boldsymbol{w}_{\\ell}\\right)_{r}^{\\top}\\boldsymbol{y}_{i_{1}}\\right)_{+}^{p}\\left(\\frac{\\partial^{\\beta_{2}}}{\\partial\\mathbf{x}^{\\beta_{2}}}\\left(\\left(\\boldsymbol{w}_{\\ell}\\right)_{r}^{\\top}\\boldsymbol{y}_{i_{2}}\\right)_{+}^{p}\\right)\\mathbf{1}_{\\left\\{\\alpha_{2}=\\alpha_{1}+\\beta_{2}\\right\\}}\\right|\\leq p^{|\\xi|}R^{2p}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "By Hoeffding\u2019s inequality with ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\mathbf{Q}_{\\ell,\\ell+1}\\right)_{((\\alpha_{1},\\beta_{1}),i_{1}),((\\alpha_{2},\\beta_{2}),i_{2})}}\\\\ &{=\\frac{\\sqrt{\\nu_{\\ell}\\nu_{\\ell+1}}}{m n_{o}}\\displaystyle\\sum_{r=1}^{m}\\left(-\\left(\\left(w_{\\ell}\\right)_{r}^{\\top}y_{i_{1}}\\right)_{+}^{p}\\right)\\left(\\frac{\\partial^{\\beta_{2}}}{\\partial x^{\\beta_{2}}}\\left(\\left(w_{\\ell}\\right)_{r}^{\\top}y_{i_{2}}\\right)_{+}^{p}\\right)\\mathbf{1}_{\\left\\{\\alpha_{2}=\\alpha_{1}+\\beta_{2}\\right\\}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "we have ", "text_level": 1, "page_idx": 29}, {"type": "equation", "text": "$$\n\\left.\\left[\\left|\\left(Q_{\\ell,\\ell+1}\\right)_{((\\alpha_{1},\\beta_{1}),i_{1}),((\\alpha_{2},\\beta_{2}),i_{2})}-\\mathbb{E}\\left[\\left(Q_{\\ell,\\ell+1}\\right)_{((\\alpha_{1},\\beta_{1}),i_{1}),((\\alpha_{2},\\beta_{2}),i_{2})}\\right]\\right|>\\varepsilon\\right]<2\\exp\\left(-\\frac{m n\\gamma}{2\\nu_{\\ell}\\nu_{\\ell+1}p_{1}}\\right)\\exp\\left(-\\frac{\\alpha_{1}\\beta_{2}\\nu_{\\ell}}{2\\nu_{\\ell}\\nu_{\\ell+1}}\\right),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for any $\\varepsilon>0$ . ", "page_idx": 29}, {"type": "text", "text": "For each $\\alpha\\in I_{\\xi_{\\ell-1}},\\beta\\in J_{\\Delta\\xi_{\\ell}},i\\in[n_{o}],$ , and $j\\in[n_{b}]$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\displaystyle\\frac{\\partial^{\\beta}}{\\partial\\boldsymbol{x}^{\\beta}}\\left(\\left(\\boldsymbol{w}_{0}\\right)_{r}^{\\top}\\boldsymbol{y}_{i}\\right)_{+}^{p}\\left(\\sum_{\\alpha^{\\prime}\\in J_{1}}\\tilde{a}_{\\alpha^{\\prime}}\\frac{\\partial^{\\alpha^{\\prime}}}{\\partial\\boldsymbol{x}^{\\alpha^{\\prime}}}\\left(\\left(\\boldsymbol{w}_{0}\\right)_{r}^{\\top}\\tilde{\\boldsymbol{y}}_{j}\\right)_{+}^{p}\\right)\\right|}\\\\ &{\\leq p^{|\\beta|}R^{p}\\displaystyle\\sum_{\\alpha^{\\prime}\\in J_{1}}a_{\\mathrm{max}}p^{|\\alpha^{\\prime}|}R^{p}}\\\\ &{\\leq\\left(d+1\\right)a_{\\mathrm{max}}p^{|\\xi|+1}R^{2p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "By Hoeffding\u2019s inequality with ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\boldsymbol{Q}_{1,L+1}\\right)_{\\left(\\left(\\alpha,\\beta\\right),i\\right),j}}\\\\ &{=\\displaystyle\\frac{1}{m}\\sqrt{\\frac{\\nu_{1}\\nu}{n_{o}n_{b}}}\\sum_{r=1}^{m}\\left(\\frac{\\partial^{\\beta}}{\\partial x^{\\beta}}\\left(\\left(\\boldsymbol{w}_{0}\\right)_{r}^{\\top}\\boldsymbol{y}_{i}\\right)_{+}^{p}\\right)\\left(\\sum_{\\alpha^{\\prime}\\in J_{1}}\\Tilde{a}_{\\alpha^{\\prime}}\\frac{\\partial^{\\alpha^{\\prime}}}{\\partial x^{\\alpha^{\\prime}}}\\left(\\left(\\boldsymbol{w}_{0}\\right)_{r}^{\\top}\\Tilde{\\boldsymbol{y}}_{j}\\right)_{+}^{p}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left.\\left>\\left[\\left(Q_{1,L+1}\\right)_{\\left(\\left(\\alpha,\\beta\\right),i\\right),j}-\\mathbb{E}\\left[\\left(Q_{1,L+1}\\right)_{\\left(\\left(\\alpha,\\beta\\right),i\\right),j}\\right]\\right|>\\varepsilon\\right]<2\\exp\\left(-\\frac{m n_{o}n_{b}\\varepsilon^{2}}{2\\nu_{1}\\nu\\left(d+1\\right)^{2}a_{\\operatorname*{max}}^{2}p^{2\\left|\\xi\\right|+2}R^{4p}}\\right)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "for any $\\varepsilon>0$ . ", "page_idx": 29}, {"type": "text", "text": "For $j_{1},j_{2}\\in[n_{b}]$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left(\\sum_{\\alpha\\in J_{1}}\\tilde{a}_{\\alpha}\\frac{\\partial^{\\alpha}}{\\partial x^{\\alpha}}\\left((\\boldsymbol{w}_{0})_{r}^{\\top}\\,\\tilde{\\boldsymbol{y}}_{j_{1}}\\right)_{+}^{p}\\right)\\left(\\sum_{\\alpha\\in J_{1}}\\tilde{a}_{\\alpha}\\frac{\\partial^{\\alpha}}{\\partial x^{\\alpha}}\\left((\\boldsymbol{w}_{0})_{r}^{\\top}\\,\\tilde{\\boldsymbol{y}}_{j_{2}}\\right)_{+}^{p}\\right)\\leq\\left(d+1\\right)^{2}a_{\\operatorname*{max}}^{2}p^{2}R^{2p}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "By Hoeffding\u2019s inequality with ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\left(Q_{L+1,L+1}\\right)_{j_{1},j_{2}}=\\frac{\\nu}{m n_{b}}\\sum_{r=1}^{m}\\left(\\sum_{\\alpha\\in J_{1}}\\Tilde{a}_{\\alpha}\\frac{\\partial^{\\alpha}}{\\partial x^{\\alpha}}\\left(\\left(w_{0}\\right)_{r}^{\\top}\\Tilde{y}_{j_{1}}\\right)_{+}^{p}\\right)\\left(\\sum_{\\alpha\\in J_{1}}\\Tilde{a}_{\\alpha}\\frac{\\partial^{\\alpha}}{\\partial x^{\\alpha}}\\left(\\left(w_{0}\\right)_{r}^{\\top}\\Tilde{y}_{j_{2}}\\right)_{+}^{p}\\right),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|\\left(Q_{L+1,L+1}\\right)_{j_{1},j_{2}}-\\mathbb{E}\\left[\\left(Q_{L+1,L+1}\\right)_{j_{1},j_{2}}\\right]\\right|>\\varepsilon\\right]<2\\exp\\left(-\\frac{m n_{b}^{2}\\varepsilon^{2}}{2\\nu^{2}\\left(d+1\\right)^{4}a_{\\operatorname*{max}}^{4}p^{4}R^{4p}}\\right),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "for any $\\varepsilon>0$ . ", "page_idx": 30}, {"type": "text", "text": "C Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "This section proves the main theorem with some lemmas and propositions. Similar to the [25], we first prove that the Gram matrix $\\hat{G}_{v}^{\\infty}$ is independent of $m$ and strictly positive definite, in Proposition C.3. Second, as $\\widehat{G}_{v}\\left(w\\left(0\\right),v\\left(0\\right)\\right)$ is the sample mean, Proposition C.4 shows that $\\widehat{G}_{v}\\left(w\\left(0\\right),v\\left(0\\right)\\right)$ is close to its expected value $\\hat{G}_{v}^{\\infty}$ with a high probability, if $m$ is large enough. This implies that $\\widehat{G}_{v}\\left(w\\left(0\\right),v\\left(0\\right)\\right)$ is strictly positive definite with high probability. Then, we show that the initial loss is bounded with a high probability if $m$ is large. This is because ${\\pmb v}\\left(0\\right)$ has zero mean, which hinders the output of $m$ variables to diverge as $m$ increases. Finally, we prove that the Gram matrix $\\widehat{\\pmb{G}}_{v}\\left(\\pmb{w}\\left(t\\right),\\pmb{v}\\left(t\\right)\\right)$ remains within a small neighborhood of the initial point, where it maintains its strict positive definiteness. The gradient flow converges within the neighborhood, as the smallest eigenvalue of $\\hat{G}_{v}$ is distinct from zero, resulting in a rapid reduction of the loss. ", "page_idx": 30}, {"type": "text", "text": "Lemma C.1. Let $d,n\\in\\mathbb{N}$ and $\\pmb{y}_{1},\\pmb{\\dots},\\pmb{y}_{n}\\in\\mathbb{R}^{d}$ be vectors such that ${y_{i}}/{y_{j}}$ if and only if $i=j$ .   \nThen, for each $i\\in[n]$ , there exists $\\pmb{w}_{i}$ such that $w_{i}^{\\top}y_{j}=0$ if and only if $i=j$ . ", "page_idx": 30}, {"type": "text", "text": "Proof. Let $\\pmb{y}_{i}^{\\bot}=\\left\\{\\pmb{w}\\in\\mathbb{R}^{d}:\\pmb{w}^{\\top}\\pmb{y}_{i}=0\\right\\}$ . Since ${y_{i}}/{y_{j}}$ for $i\\neq j$ , ${\\pmb y}_{i}\\cap{\\pmb y}_{j}$ is nowhere dense in $\\pmb{y}_{i}^{\\bot}$ . Hence, finite union of $\\cup_{j\\neq i}{\\pmb{y}}_{j}^{\\perp}$ is nowhere dense in $\\pmb{y}_{i}^{\\bot}$ , and in particular, there exists $\\pmb{w}_{i}\\in\\pmb{y}_{i}^{\\perp}$ such that wi \u0338\u2208\u222aj\u0338=iyj\u22a5 . \u53e3 ", "page_idx": 30}, {"type": "text", "text": "Lemma C.2. Let $n,d\\in\\mathbb{N},$ , and $\\psi:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ be a homogeneous polynomial of degree $n$ . For any $i\\in\\mathbb N$ and nonzero $\\pmb{y}=(y_{1},\\dots,y_{d})\\in\\mathbb{R}^{d}$ , if a function $\\psi\\left(\\pmb{w}\\right)\\left(\\pmb{w}^{\\top}\\pmb{y}\\right)_{+}^{i}$ of $\\pmb{w}=(w_{1},\\dots,w_{d})\\in\\mathbb{R}^{d}$ is $(n+i)$ -times continuously differentiable at some $\\pmb{w}^{*}\\in\\mathbb{R}^{d}$ with $\\begin{array}{r}{\\left(\\pmb{w}^{*}\\right)^{\\top}\\pmb{y}=0,}\\end{array}$ , then $\\psi\\equiv0$ . ", "page_idx": 30}, {"type": "text", "text": "Proof. Without loss of the generality, we may assume the $y_{1}\\neq0$ . Write $\\begin{array}{r}{\\psi\\left(\\pmb{w}\\right)=\\sum_{\\alpha\\in I_{n}}a_{\\alpha}\\pmb{w}^{\\alpha}}\\end{array}$ for coefficients $a_{\\alpha}\\in\\mathbb{R}$ and define $\\varphi\\left(\\pmb{w}\\right)=\\psi\\left(\\pmb{w}\\right)\\left(\\pmb{w}^{\\top}\\pmb{y}\\right)_{+}^{i}$ . For any $\\alpha\\in I_{n}$ , the assumption on differentiability implies tha t\u2202\u2202wii\u2202\u2202wn\u03b1 \u03c6 is continuous. Indeed, we have \u03c6 (w) = 0 for w\u22a4y \u22640, and ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{\\partial^{i}}{\\partial{\\pmb w}_{1}^{i}}\\frac{\\partial^{n}}{\\partial{\\pmb w}^{\\alpha}}\\varphi\\left({\\pmb w}\\right)=\\alpha!a_{\\alpha}y_{1}^{i},\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "for $w^{\\top}y>0$ . The continuity induces $a_{\\alpha}=0$ and thus $\\psi\\equiv0$ for all $\\pmb{w}\\in\\mathbb{R}^{d}$ . ", "page_idx": 30}, {"type": "text", "text": "Proposition C.3. $\\widehat{\\boldsymbol{G}}_{v}^{\\infty}=\\mathbb{E}_{w,v}\\left[\\widehat{\\boldsymbol{G}}_{v}\\left(\\boldsymbol{w},\\boldsymbol{v}\\right)\\right]$ is strictly positive definite and independent of $m$ . ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathrm{~\\sum_{m}^{\\mathrm{~\\sum~}}\\sum_{n=1}^{m}\\frac{1}{n_{o}}\\sum_{\\ell=0}^{L}\\sum_{\\alpha\\in I_{\\xi_{\\ell}}}\\left(\\sum_{\\beta\\in J_{\\Delta\\xi_{\\ell+1}}}\\widehat{a}_{\\ell,\\alpha,\\beta}\\frac{\\partial^{\\beta}}{\\partial x^{\\beta}}\\left(\\left(w_{\\ell}\\right)_{r}^{\\top}y_{i_{1}}\\right)_{+}^{p}\\right)\\left(\\sum_{\\beta\\in J_{\\Delta\\xi_{\\ell+1}}}\\widehat{a}_{\\ell,\\alpha,\\beta}\\frac{\\partial^{\\beta}}{\\partial x^{\\beta}}\\left(\\left(w_{\\ell}\\right)_{r}^{\\top}y_{i_{2}}\\right)_{+}^{p}\\right)}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "and $m$ random variables ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{1}{n_{o}}\\sum_{\\ell=0}^{L}\\sum_{\\alpha\\in I_{\\xi_{\\ell}}}\\left(\\sum_{\\beta\\in J_{\\Delta\\xi_{\\ell+1}}}\\hat{a}_{\\ell,\\alpha,\\beta}\\frac{\\partial^{\\beta}}{\\partial x^{\\beta}}\\left(\\left(\\boldsymbol{w}_{\\ell}\\right)_{r}^{\\top}\\boldsymbol{y}_{i_{1}}\\right)_{+}^{p}\\right)\\left(\\sum_{\\beta\\in J_{\\Delta\\xi_{\\ell+1}}}\\hat{a}_{\\ell,\\alpha,\\beta}\\frac{\\partial^{\\beta}}{\\partial x^{\\beta}}\\left(\\left(\\boldsymbol{w}_{\\ell}\\right)_{r}^{\\top}\\boldsymbol{y}_{i_{2}}\\right)_{+}^{p}\\right),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "for $r\\,\\in\\,[m]$ , are independent and identically distributed. Therefore, the expected value of each component of $Q_{0,0}$ is independent of $m$ , as the expected value of the sample mean of independent and identically distributed random variables is independent of the number of samples. Similarly, each component of $\\widehat{G}_{v}\\left(w,v\\right)$ is the sample mean of $m$ independent and identically distributed random variables, and its expected value $\\hat{G}_{v}^{\\infty}$ is independent of $m$ . To show that $\\hat{G}_{v}^{\\infty}$ is strictly positive definite, assume that there exists $z_{0}=\\left[\\left(z_{0}\\right)_{1}\\quad\\cdot\\cdot\\cdot\\quad\\left(z_{0}\\right)_{n_{o}}\\right]\\in\\mathbb{R}^{n_{o}},\\tilde{z}=\\left[\\tilde{z}_{1}\\quad\\cdot\\cdot\\quad\\tilde{z}_{n_{b}}\\right]^{\\intercal}\\in\\mathbb{R}^{n_{b}}$ , and ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r}{z_{\\ell}=\\left[\\left(z_{\\ell}\\right)_{\\left(\\alpha,\\beta\\right),i}\\right]_{\\left(\\alpha\\in I_{\\xi_{\\ell}},\\beta\\in I_{\\Delta\\xi_{\\ell}},i\\in[n_{o}]\\right)}\\in\\mathbb{R}^{|I_{\\xi_{\\ell-1}}|\\cdot|I_{\\Delta\\xi_{\\ell}}|n_{o}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "for $\\ell\\in[L]$ , such tha $\\boldsymbol{z}=\\left[z_{0}^{\\top}\\quad z_{1}^{\\top}\\quad\\cdot\\cdot\\cdot\\quad z_{L}^{\\top}\\quad\\tilde{z}^{\\top}\\right]^{\\top}\\in\\mathbb{R}^{n_{o}+\\sum_{\\ell=1}^{L}\\left(\\left|I_{\\xi_{\\ell-1}}\\right|\\cdot\\left|I_{\\Delta\\xi_{\\ell}}\\right|n_{o}\\right)+n_{b}}$ satisfies $\\widehat{\\boldsymbol{G}}_{v}^{\\infty}z=\\mathbf{0}$ . Since $\\begin{array}{r}{\\widehat{G}_{v}^{\\infty}z=\\mathbb{E}_{w,v}\\left[\\hat{D}_{v}^{\\top}\\hat{D}_{v}\\right]z=0}\\end{array}$ if and only if $\\mathbb{E}_{w,v}\\left[\\left\\|\\hat{D}_{v}z\\right\\|_{2}^{2}\\right]=\\mathbf{0}$ , we show that $\\mathbb{E}_{w,v}\\left[\\left\\|\\hat{D}_{v}z\\right\\|_{2}^{2}\\right]=\\mathbf{0}$ implies $z=0$ and thereby 0 is not an eigenvalue of $\\hat{G}_{v}^{\\infty}$ . ", "page_idx": 31}, {"type": "text", "text": "Now assume that there exists $z\\,\\in\\,\\mathbb{R}^{d}$ such that $\\hat{D}_{v}z=\\mathbf{0}$ for almost all $\\mathbf{\\nabla}w$ and $\\pmb{v}$ . Since $\\hat{D}_{v}$ is continuous with respect to $\\pmb{w}$ and $\\pmb{v}$ , we have $\\hat{D}_{v}z=\\mathbf{0}$ for all $\\pmb{w}$ and $\\pmb{v}$ . Hence, the function $\\hat{D}_{v}z$ of $\\pmb{w}$ and $\\pmb{v}$ , which is identically zero, is smooth with respect to $\\pmb{w}$ and $\\pmb{v}$ . ", "page_idx": 31}, {"type": "text", "text": "In this proof, we denote an index for a component of $\\hat{D}_{v}z$ by $\\ell\\in[L],r\\in[m]$ and $\\alpha\\in I_{\\xi_{\\ell}}$ , because each row of $\\hat{D}_{v}$ corresponds to the partial derivatives with respect to $(\\pmb{v}_{\\ell})_{r,\\alpha}$ for $\\ell\\in[L]$ , $r\\in[m]$ and $\\alpha\\in I_{\\xi_{\\ell}}$ . For instance, $\\Big(\\hat{D}_{v}z\\Big)_{0,1,\\mathbf{0}}$ is the first component of $\\Big(\\hat{D}_{v}z\\Big)$ . First, for $\\ell=L$ and any $r\\in[m]$ and $\\alpha\\in I_{\\xi_{L}}$ , we have ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\hat{D}_{v}z\\right)_{L,r,\\alpha}}\\\\ &{=\\displaystyle\\sum_{i=1}^{n_{o}}\\left(\\frac{\\partial}{\\partial{\\left(v_{L}\\right)}_{r,\\alpha}}s_{i}\\right)\\left(z_{0}\\right)_{i}+\\sum_{i=1}^{n_{o}}\\sum_{\\alpha^{\\prime}\\in I_{\\xi_{L-1}}}\\sum_{\\beta\\in I_{\\Delta\\xi_{L}}}\\left(\\frac{\\partial}{\\partial{\\left(v_{L}\\right)}_{r,\\alpha}}\\left(\\mu_{L}\\right)_{\\left(\\alpha^{\\prime},\\beta\\right),i}\\right)\\left(z_{L}\\right)_{\\left(\\alpha^{\\prime},\\beta\\right),i}}\\\\ &{=\\displaystyle\\sqrt{\\frac{1}{m n_{o}}}\\sum_{i=1}^{n_{o}}\\left(\\sum_{\\beta\\in J_{\\Delta\\xi_{L+1}}}\\hat{a}_{L,\\alpha,\\beta}\\frac{\\partial^{\\beta}}{\\partial x^{\\beta}}\\left(\\left(w_{L}\\right)_{r}^{\\top}y_{i}\\right)_{+}^{p}\\right)\\left(z_{0}\\right)_{i}}\\\\ &{\\phantom{=}-\\sqrt{\\frac{1}{m n_{o}}}\\sum_{i=1}^{n_{o}}\\sum_{\\alpha^{\\prime}\\in I_{\\xi_{L-1}}}\\sum_{\\beta\\in I_{\\Delta\\xi_{L-1}}}\\left(\\left(w_{L}\\right)_{r}^{\\top}y_{i}\\right)_{i}^{p}\\left(z_{L}\\right)_{\\left(\\alpha^{\\prime},\\beta\\right),i}\\mathbf{1}_{\\left\\{(\\alpha=\\alpha^{\\prime}+\\beta\\right)}\\cdot\\mathbf{1}\\right\\}}\\end{array}\\left(2\\right)\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "Fix $i\\in[n_{o}]$ , and by Lemma C.1, there exists $(\\boldsymbol{w}_{L}^{*})_{r}\\in\\mathbb{R}^{d}$ such that $(\\pmb{w}_{L}^{*})_{r}^{\\top}\\,\\pmb{y}_{i}=0,(\\pmb{w}_{L}^{*})_{r}^{\\top}\\,\\pmb{y}_{i^{\\prime}}\\neq0$ for $i\\neq i^{\\prime}\\in[n_{o}]$ and $(\\pmb{w}_{L}^{*})_{r}^{\\top}\\ \\tilde{\\pmb{y}}_{j}\\neq0$ for $j\\in[n_{b}]$ . As a function of $(\\mathbf{\\boldsymbol{w}}_{L})_{r},\\hat{D}_{v}z=0$ implies that ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\displaystyle\\sum_{\\beta\\in J_{\\Delta\\xi_{L+1}}}\\hat{a}_{L,\\alpha,\\beta}\\frac{\\partial^{\\beta}}{\\partial x^{\\beta}}\\left((w_{L})_{r}^{\\top}\\,y_{i}\\right)_{+}^{p}\\right)(z_{0})_{i}}\\\\ &{\\quad-\\sqrt{\\nu_{L}}\\displaystyle\\sum_{\\alpha^{\\prime}\\in J_{\\xi_{L-1}}}\\sum_{\\beta\\in I_{\\Delta\\xi_{L}}}\\left((w_{L})_{r}^{\\top}\\,y_{i}\\right)_{+}^{p}(z_{L})_{(\\alpha^{\\prime},\\beta),i}\\,\\mathbf{1}_{\\left\\{\\alpha=\\alpha^{\\prime}+\\beta\\right\\}}}\\\\ &{=-\\displaystyle\\sum_{i^{\\prime}\\neq i}\\left(\\displaystyle\\sum_{\\beta\\in J_{\\Delta\\xi_{L+1}}}\\hat{a}_{L,\\alpha,\\beta}\\frac{\\partial^{\\beta}}{\\partial x^{\\beta}}\\left((w_{L})_{r}^{\\top}\\,y_{i}\\right)_{+}^{p}\\right)(z_{0})_{i}}\\\\ &{\\quad+\\sqrt{\\nu_{L}}\\displaystyle\\sum_{i^{\\prime}\\neq i}\\sum_{\\alpha^{\\prime}\\in J_{\\xi_{L-1}}}\\sum_{\\beta\\in I_{\\Delta\\xi_{L}}}\\left((w_{L})_{r}^{\\top}\\,y_{i}\\right)_{+}^{p}(z_{L})_{(\\alpha^{\\prime},\\beta),i}\\,\\mathbf{1}_{\\left\\{\\alpha=\\alpha^{\\prime}+\\beta\\right\\}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "is smooth at $(\\pmb{w}_{L})_{r}=(\\pmb{w}_{L}^{*})_{r}$ . Note that the coefficient of $\\left(\\left(\\pmb{\\omega}_{L}\\right)_{r}^{\\top}\\pmb{y}_{i}\\right)_{+}^{p-\\Delta\\xi_{L+1}}$ in (266) is given by ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{\\beta\\in I_{\\Delta\\xi_{L+1}}}\\hat{a}_{L,\\alpha,\\beta}\\left(\\pmb{x}_{i}\\right)\\left(\\pmb{w}_{L}\\right)_{r,1}^{\\beta_{1}}\\cdot\\cdot\\cdot\\left(\\pmb{w}_{L}\\right)_{r,d}^{\\beta_{d}}\\left(\\pmb{z}_{0}\\right)_{i},\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "which is a homogeneous polynomial of $w_{L}$ . Together with the fact that at least one of $\\hat{a}_{L,\\alpha,\\beta}\\left(\\pmb{x}_{i}\\right)$ is nonzero, Lemma C.2 implies $\\left(z_{0}\\right)_{i}=0$ . Similarly, for any $r\\in[m]$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\hat{D}_{v}z\\right)_{0,r,0}=\\displaystyle\\sum_{j=1}^{n_{b}}\\left(\\frac{\\partial}{\\partial\\left(v_{0}\\right)_{r,0}}h_{j}\\right)\\tilde{z}_{j}}\\\\ &{\\qquad\\qquad=\\sqrt{\\frac{\\nu}{m n_{b}}}\\displaystyle\\sum_{j=1}^{n_{b}}\\sum_{\\alpha\\in J_{1}}\\tilde{a}_{\\alpha}\\left(\\tilde{x}_{j}\\right)\\frac{\\partial^{\\alpha}}{\\partial x^{\\alpha}}\\left(\\left(w_{0}\\right)_{r}^{\\top}\\tilde{y}_{j}\\right)_{+}^{p}\\tilde{z}_{j}}\\\\ &{\\qquad\\qquad=\\sqrt{\\frac{\\nu}{m n_{b}}}\\displaystyle\\sum_{j=1}^{n_{b}}\\sum_{\\alpha\\in J_{1}}\\tilde{a}_{\\alpha}\\left(\\tilde{x}_{j}\\right)\\tilde{z}_{j}\\left(w_{0}\\right)_{r,1}^{\\alpha_{1}}\\cdot\\cdot\\cdot\\left(w_{0}\\right)_{r,d}^{\\alpha_{d}}\\left(\\left(w_{0}\\right)_{r}^{\\top}\\tilde{y}_{j}\\right)_{+}^{p-\\left|\\alpha\\right|}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "deduces $\\tilde{z}_{j}=0$ with $\\pmb{w}_{0}^{*}$ , instead of $\\pmb{w}_{L}^{*}$ , such that $\\left(\\pmb{w}_{0}^{*}\\right)^{\\top}\\tilde{\\pmb{y}}_{j}=0$ , $\\left(\\pmb{w}_{0}^{*}\\right)^{\\top}\\tilde{\\pmb{y}}_{j^{\\prime}}\\neq=0$ for $j^{\\prime}\\ne j$ , and $\\left(\\pmb{w}_{0}^{*}\\right)^{\\top}\\pmb{y}_{i}\\neq0$ for $i\\in[n_{o}]$ . Now suppose that we have $\\tilde{z}=0$ and $z_{\\ell^{\\prime}}=0$ for $\\ell^{\\prime}=0,1,\\ldots,\\ell-1$ for some $\\ell\\geq1$ . Then, for any $r\\in[\\bar{m}]$ and $\\alpha\\in I_{\\xi_{\\ell-1}}$ , ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\left(\\hat{D}_{v}z\\right)_{\\ell=1,r,\\alpha}}&{(2^{\\ell})}\\\\ &{=\\displaystyle\\sum_{i=1}^{n_{o}}\\left(\\frac{\\partial}{\\partial\\left(v_{\\ell-1}\\right)_{r,\\alpha}}s_{i}\\right)\\left(z_{0}\\right)_{i}+\\sum_{i=1}^{n_{o}}\\sum_{\\alpha^{\\prime}\\in I_{\\xi_{\\ell-1}}}\\sum_{\\beta\\in I_{\\Delta\\xi_{\\ell}}}\\left(\\frac{\\partial}{\\partial\\left(v_{\\ell-1}\\right)_{r,\\alpha}}\\left(\\mu_{\\ell}\\right)_{(\\alpha^{\\prime},\\beta),i}\\right)\\left(z_{\\ell}\\right)_{(\\alpha^{\\prime},\\beta),i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\displaystyle\\sum_{i=1}^{n_{o}}\\sum_{\\alpha^{\\prime}\\in I_{\\xi_{\\ell-2}}}\\displaystyle\\sum_{\\beta\\in I_{\\Delta\\xi_{\\ell-1}}}\\left(\\frac{\\partial}{\\partial\\left(v_{\\ell-1}\\right)_{r,\\alpha}}\\left(\\mu_{\\ell-1}\\right)_{\\left(\\alpha^{\\prime},\\beta\\right),i}\\right)\\left(\\boldsymbol{z}_{\\ell-1}\\right)_{\\left(\\alpha^{\\prime},\\beta\\right),i}}\\\\ &{=\\displaystyle\\sum_{i=1}^{n_{o}}\\sum_{\\alpha^{\\prime}\\in I_{\\xi_{\\ell-1}}}\\beta\\!\\!\\!\\!\\sum_{\\beta\\in I_{\\Delta\\xi_{\\ell}}}\\left(\\frac{\\partial}{\\partial\\left(v_{\\ell-1}\\right)_{r,\\alpha}}\\left(\\mu_{\\ell}\\right)_{\\left(\\alpha^{\\prime},\\beta\\right),i}\\right)\\left(\\boldsymbol{z}_{\\ell}\\right)_{\\left(\\alpha^{\\prime},\\beta\\right),i}}\\\\ &{=\\!\\!\\sqrt{\\frac{\\nu_{\\ell}}{n_{o}}}\\sum_{i=1}^{n_{o}}\\sum_{\\beta\\in I_{\\Delta\\xi_{\\ell}}}\\left(\\left(w_{\\ell}\\right)_{r,1}^{\\beta_{1}}\\cdot\\cdot\\cdot\\left(w_{\\ell}\\right)_{r,d}^{\\beta_{d}}\\left(\\left(w_{\\ell}\\right)_{r}^{\\top}y_{i}\\right)_{+}^{p-\\Delta\\xi_{\\ell}}\\right)\\left(\\boldsymbol{z}_{\\ell}\\right)_{\\left(\\alpha,\\beta\\right),i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Then the coefficient of $\\left(\\left(\\pmb{w}_{\\ell}\\right)_{r}^{\\top}\\pmb{y}_{i}\\right)^{p-\\Delta\\xi_{\\ell}}$ is a homogeneous polynomial ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sqrt{\\frac{\\nu_{\\ell}}{n_{o}}}\\sum_{\\beta\\in I_{\\Delta\\xi_{\\ell}}}\\left(z_{\\ell}\\right)_{(\\alpha,\\beta),i}\\left(\\pmb{w}_{\\ell}\\right)_{r,d}^{\\beta_{d}}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Lemma C.2 shows $(z_{\\ell})_{(\\alpha,\\beta),i}=0$ for all $\\alpha,\\beta$ . The induction concludes $z=0$ . ", "page_idx": 32}, {"type": "text", "text": "In the sequel, $\\lambda_{0}\\,=\\,\\lambda_{\\mathrm{min}}(\\widehat{\\pmb{G}}_{v})\\,>\\,0$ is the minimal eigenvalue of $\\hat{G}_{v}$ . Furthermore, we assume the occurrence of the foll owing event: there exists $R~>~1$ such  that $\\|(\\pmb{\\omega}_{\\ell})_{r}\\left(0\\right)\\|_{2}\\;<\\;R$ and $\\|(\\pmb{v}_{\\ell})_{r}\\left(0\\right)\\|_{2}<R$ for all $\\ell=0,1,\\ldots,L$ and $r\\in[m]$ . This assumption is employed solely for the purpose of simplifying propositions and their respective proofs. In the main theorem and its proof, we use the following arguments with regard to the conditional probability of the aforementioned event. ", "page_idx": 32}, {"type": "text", "text": "Proposition C.4. Let $\\delta>0$ and ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C_{1}=\\binom{d+k}{k}\\binom{d+\\lfloor\\xi\\rfloor}{\\lfloor\\xi\\rfloor}^{2}\\nu_{\\operatorname*{max}}^{2}\\left(\\frac{1}{n_{o}}+\\frac{1}{n_{b}}\\right)\\sqrt{1+a_{\\operatorname*{max}}^{4}}\\left(p^{2\\lfloor\\xi\\rfloor}+1\\right),}\\\\ &{N_{1}=7\\binom{d+k}{d}\\binom{d+\\lfloor\\xi\\rfloor}{d}^{2}n_{o}^{2}+4\\binom{d+\\lfloor\\xi\\rfloor}{d}n_{o}n_{b}+n_{b}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "be constants. If m is large enough so that ", "page_idx": 33}, {"type": "equation", "text": "$$\nm\\geq\\frac{32N_{1}C_{1}^{2}R^{4p}}{\\lambda_{0}^{2}}\\log\\left(\\frac{2N_{1}}{\\delta}\\right),\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "then with the probability of at least $1-\\delta$ over the initialization, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left\\|\\widehat{G}_{v}\\left(w\\left(0\\right),v\\left(0\\right)\\right)-\\widehat{G}_{v}^{\\infty}\\right\\|_{2}<\\frac{\\lambda_{0}}{4}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. We use Hoeffding\u2019s inequality to measure the error between $\\widehat{G}_{v}\\left(w,v\\right)$ and $\\hat{G}_{v}^{\\infty}$ , as Gram matrix $\\widehat{G}_{v}^{\\infty}$ is the expectation of the sample mean. In the Hoeffding\u2019s inequalities (220), (228), (233), (241),  (245), (251), and (254), we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\frac{1}{n_{0}^{2}}\\Big(d+k\\Big)^{2}\\Big(d+\\frac{1}{d}\\Big)^{4}\\frac{4}{a^{4}}\\alpha_{\\mathrm{max}}^{4}p^{4|\\mathrm{f}|}\\le C_{1}^{2},}\\\\ {\\frac{\\nu_{\\ell}}{n_{0}^{2}}\\Big(d+\\frac{1}{d}\\Big)^{2}\\Big(p^{1|}+1\\Big)^{2}\\,\\alpha_{\\mathrm{max}}^{2}p^{2|\\mathrm{f}|}\\le C_{1}^{2},}\\\\ {\\frac{\\nu}{n_{0}n_{\\ell}}(d+1)^{2}\\Big(d+\\frac{1}{d}\\Big)^{2}\\alpha_{\\mathrm{max}}^{4}p^{2|\\mathrm{f}|+2}\\le C_{1}^{2},}\\\\ {\\frac{\\nu_{\\ell}^{2}}{n_{0}^{2}}\\Big(p^{2|\\mathrm{f}|}+1\\Big)^{2}\\le C_{1}^{2}}\\\\ {\\frac{\\nu_{\\ell}\\nu_{\\ell}}{n_{0}^{2}}\\frac{1}{p^{2}}\\mu^{2|\\mathrm{f}|}\\le C_{1}^{2},}\\\\ {\\frac{\\nu_{\\ell}\\nu_{\\ell}}{n_{0}n_{0}^{2}}(d+1)^{2}\\alpha_{\\mathrm{max}}^{2}p^{2|\\mathrm{f}|+2}\\le C_{1}^{2},}\\\\ {\\frac{\\nu_{\\ell}}{n_{0}n_{\\ell}}(d+1)^{2}\\alpha_{\\mathrm{max}}^{2}p^{2|\\mathrm{f}|}\\le C_{1}^{2},}\\\\ {\\frac{\\nu^{2}}{n_{0}n_{\\ell}}(d+1)^{4}\\alpha_{\\mathrm{max}}^{4}p^{4|\\mathrm{f}|}\\le C_{1}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Consequently, all inequalities induce that each component of $\\left|\\widehat{G}_{v}-\\widehat{G}_{v}^{\\infty}\\right|$ is greater than $\\varepsilon$ , with a probability of at most $\\begin{array}{r}{2\\exp\\left(-\\frac{m\\varepsilon^{2}}{2C_{1}^{2}R^{4p}}\\right)}\\end{array}$ . Since there exists at most $N_{1}$ nonzero elements in $\\left|\\widehat{G}_{v}-\\widehat{G}_{v}^{\\infty}\\right|$ by (145), ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left|\\widehat{G}_{v}\\left(\\boldsymbol{w},\\boldsymbol{v}\\right)-\\widehat{G}_{v}^{\\infty}\\right|\\leq\\varepsilon\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "holds componentwise, with the probability of at least ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left(1-2\\exp\\left(-\\frac{m\\varepsilon^{2}}{2C_{1}^{2}R^{4p}}\\right)\\right)^{N_{1}}\\geq1-2N_{1}\\exp\\left(-\\frac{m\\varepsilon^{2}}{2C_{1}^{2}R^{4p}}\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Then, we have ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\widehat{G}_{v}\\left(w,v\\right)-\\widehat{G}_{v}^{\\infty}\\right\\|_{2}\\leq\\left\\|\\widehat{G}_{v}\\left(w,v\\right)-\\widehat{G}_{v}^{\\infty}\\right\\|_{F}\\leq\\varepsilon\\sqrt{N_{1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Set $\\varepsilon$ and $m$ to satisfy $\\begin{array}{r}{\\varepsilon=\\frac{1}{4\\sqrt{N_{1}}}\\lambda_{0}}\\end{array}$ and $\\begin{array}{r}{2N\\exp\\left(-\\frac{m\\varepsilon^{2}}{2C_{1}^{2}R^{4p}}\\right)<\\delta}\\end{array}$ . In other words, if ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{m\\geq\\frac{2C_{1}^{2}R^{4p}}{\\varepsilon^{2}}\\log\\left(\\frac{2N_{1}}{\\delta}\\right)}\\\\ &{\\quad=\\frac{32N_{1}C_{1}^{2}R^{4p}}{\\lambda_{0}^{2}}\\log\\left(\\frac{2N_{1}}{\\delta}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "then $\\begin{array}{r}{\\left\\lVert\\widehat{G}_{v}\\left(\\boldsymbol{w},\\boldsymbol{v}\\right)-\\widehat{G}_{v}^{\\infty}\\right\\rVert_{2}\\leq\\frac{1}{4}\\lambda_{0}}\\end{array}$ with probability of at least 1-\u03b4. ", "page_idx": 33}, {"type": "text", "text": "By the above proposition, the initial Gram matrix $\\widehat{G}_{v}\\left(w\\left(0\\right),v\\left(0\\right)\\right)$ is likely to be strictly positive definite, and its smallest eigenvalue remains greater than $\\frac{\\lambda_{0}}{2}$ , at the beginning of the flow. The following lemma implies that at the early stage of the flow, for which $\\pmb{w}$ and $\\pmb{v}$ are not far from the initial values, the positive definiteness is preserved, and its smallest eigenvalue remains distinct from zero. ", "page_idx": 33}, {"type": "text", "text": "Lemma C.5. Suppose $\\|\\tilde{\\pmb{w}}_{\\ell}-\\pmb{w}_{\\ell}\\left(0\\right)\\|_{2}\\leq R_{w}<R_{\\mathrm{{\\ell}}}$ , for ", "page_idx": 34}, {"type": "equation", "text": "$$\nR_{w}=\\frac{\\lambda_{0}}{8\\sqrt{2}}\\left(\\nu_{\\mathrm{max}}\\sqrt{1+a_{\\mathrm{max}}^{4}}\\bigg({d+k\\atop d}\\bigg)\\bigg({d+k\\atop d}\\bigg)^{2}p^{2|\\xi|+1}\\left(2R\\right)^{2p-1}\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Then ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\left\\|\\widehat{G}_{v}\\left(\\tilde{w},\\tilde{v}\\right)-\\widehat{G}_{v}\\left(w\\left(0\\right),v\\left(0\\right)\\right)\\right\\|_{2}\\leq\\frac{1}{4}\\lambda_{0}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof. 1. Case of $Q_{\\mathrm{0,0}}$ : By the mean value theorem with (151), we can induce that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\langle Q_{0,0}\\left(\\tilde{w},\\tilde{v}\\right)-Q_{0,0}\\left(w\\left(0\\right),v\\left(0\\right)\\right)\\right\\rangle_{i_{1},i_{2}}}\\\\ {\\displaystyle\\le\\left|Q_{0,0}\\left(\\tilde{w},\\tilde{v}\\right)_{i_{1},i_{2}}-Q_{0,0}\\left(w\\left(0\\right),\\tilde{v}\\right)_{i_{1},i_{2}}\\right|+\\left|Q_{0,0}\\left(\\tilde{w},\\tilde{v}\\right)_{i_{1},i_{2}}-Q_{0,0}\\left(\\tilde{w},v\\left(0\\right)\\right)_{i_{1},i_{2}}\\right|}\\\\ {\\displaystyle=\\left|\\frac{\\partial}{\\partial w}Q_{0,0}\\left(w^{*},v\\left(0\\right)\\right)_{i_{1},i_{2}}\\cdot\\left(\\tilde{w}-w\\left(0\\right)\\right)\\right|+\\left|\\frac{\\partial}{\\partial v}Q_{0,0}\\left(w\\left(0\\right),v^{*}\\right)_{i_{1},i_{2}}\\left(\\tilde{v}-v\\left(0\\right)\\right)\\right|}\\\\ {\\displaystyle\\le\\sum_{\\ell=0}^{L}\\sum_{r=1}^{m}\\left(\\left\\|\\frac{\\partial}{\\partial\\left(w_{\\ell}\\right)_{r}}\\left(Q_{0,0}\\right)\\left(w^{*},v\\left(0\\right)\\right)_{i_{1},i_{2}}\\right\\|_{2}\\cdot\\left\\|\\left(\\tilde{w}_{\\ell}\\right)_{r}-\\left(w_{\\ell}\\right)_{r}\\left(0\\right)\\right\\|_{2}\\right)}\\\\ {\\displaystyle\\le\\frac{1}{m n_{o}}a_{\\mathrm{max}}^{2}p^{2\\left\\|\\xi\\right\\|+1}\\sum_{\\ell=0}^{L}\\sum_{r=1}^{m}\\left|I_{\\xi\\ell}\\right|\\left|J_{\\Delta\\xi\\ell+1}\\right|^{2}\\left(\\left\\|\\left(w_{\\ell}^{*}\\right)_{r}\\right\\|_{2}^{2p-1}\\cdot\\left\\|\\left(\\tilde{w}_{\\ell}\\right)_{r}-\\left(w_{\\ell}\\right)_{r}\\left(0\\right)\\right\\|_{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "for some $\\pmb{w}^{*}\\;=\\;\\left[\\left(\\pmb{w}_{0}^{*}\\right)_{1}^{\\top}\\quad\\cdot\\cdot\\cdot\\quad\\left(\\pmb{w}_{0}^{*}\\right)_{m}^{\\top}\\quad\\cdot\\cdot\\cdot\\quad\\left(\\pmb{w}_{L}^{*}\\right)_{0}^{\\top}\\quad\\cdot\\cdot\\cdot\\quad\\left(\\pmb{w}_{L}^{*}\\right)_{m}^{\\top}\\right]^{\\top}$ lies on a line between $\\tilde{w}$ and ${\\pmb w}\\left(0\\right)$ and for some $\\pmb{v}^{*}$ lies on a line connecting $\\tilde{v}$ and ${\\pmb v}\\left(0\\right)$ . Since $\\left\\|(\\pmb{w}_{\\ell}^{*})_{r}\\right\\|_{2}\\;\\leq$ $\\begin{array}{r}{\\left\\|\\left(\\tilde{\\pmb{w}}_{\\ell}\\right)_{r}-\\left(\\pmb{w}_{\\ell}\\right)_{r}\\left(0\\right)\\right\\|_{2}+\\left\\|\\left(\\pmb{w}_{\\ell}\\right)_{r}\\left(0\\right)\\right\\|_{2}\\leq R_{w}+R\\leq2R}\\end{array}$ for each $\\ell\\in\\{0\\}\\cup[L]$ and $r\\in[m]$ , ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left(Q_{0,0}\\left(\\tilde{w},\\tilde{v}\\right)-Q_{0,0}\\left(w\\left(0\\right),v\\left(0\\right)\\right)\\right)_{i_{1},i_{2}}}\\\\ &{\\le\\displaystyle\\frac{a_{\\operatorname*{max}}^{2}}{m n_{o}}p^{2\\left\\vert\\xi\\right\\vert+1}\\displaystyle\\sum_{\\ell=0}^{L}\\sum_{r=1}^{m}\\left\\vert I_{\\xi_{\\ell}}\\right\\vert\\left\\vert J_{\\Delta\\xi_{\\ell+1}}\\right\\vert^{2}\\left(\\left(2R\\right)^{2p-1}\\left\\Vert\\left(\\tilde{w}_{\\ell}\\right)_{r}-\\left(w_{\\ell}\\right)_{r}\\left(0\\right)\\right\\Vert_{2}\\right)}\\\\ &{\\le\\displaystyle\\frac{a_{\\operatorname*{max}}^{2}}{n_{o}}p^{2\\left\\vert\\xi\\right\\vert+1}\\left(\\displaystyle\\left.\\begin{array}{c}{d+\\left\\vert\\xi\\right\\vert}\\\\ {d}\\end{array}\\right)^{2}\\displaystyle\\sum_{\\ell=0}^{L}\\left\\vert I_{\\xi_{\\ell}}\\right\\vert\\left(2R\\right)^{2p-1}R_{w}}\\\\ &{\\le\\displaystyle\\frac{a_{\\operatorname*{max}}^{2}}{n_{o}}p^{2\\left\\vert\\xi\\right\\vert+1}\\left(\\displaystyle\\left.\\begin{array}{c}{d+k}\\\\ {d}\\end{array}\\right)\\left(\\displaystyle\\begin{array}{c}{d+\\left\\vert\\xi\\right\\vert}\\\\ {d}\\end{array}\\right)^{2}\\left(2R\\right)^{2p-1}R_{w}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Therefore, we can attain that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i_{1}=1}^{n_{o}}\\sum_{i_{2}=1}^{n_{o}}\\left(Q_{0,0}\\left(\\tilde{\\pmb{w}},\\tilde{\\pmb{v}}\\right)-Q_{0,0}\\left({\\pmb{w}\\left(0\\right),\\pmb{v}\\left(0\\right)}\\right)\\right)_{i_{1},i_{2}}^{2}}\\\\ &{\\leq\\!a_{\\operatorname*{max}}^{4}\\!\\left(\\!d+k\\!\\right)^{2}\\!\\left(\\!d+\\left|\\xi\\right|\\!\\right)^{4}\\!2p^{2\\left(2\\left|\\xi\\right|+1\\right)}\\left(2R\\right)^{2\\left(2p-1\\right)}R_{w}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "2. Case of $Q_{0,\\ell}$ : Similar to $Q_{0,0}$ , the mean value theorem with (159) and (163) gives ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left(Q_{0,\\ell}\\left(\\tilde{w},\\tilde{v}\\right)-Q_{0,\\ell}\\left(w\\left(0\\right),v\\left(0\\right)\\right)\\right)_{i_{1},\\left(\\left(\\alpha,\\beta\\right),i_{2}\\right)}}\\\\ &{\\leq\\frac{\\sqrt{\\nu_{\\operatorname*{max}}}}{m n_{o}}a_{\\operatorname*{max}}\\binom{d+\\left\\vert\\xi\\right\\vert}{d}p^{2\\left\\vert\\xi\\right\\vert+1}\\displaystyle\\sum_{r=1}^{m}\\left\\{\\left\\Vert\\left(\\tilde{w}_{\\ell-1}\\right)_{r}\\right\\Vert^{2p-1}\\left\\Vert\\left(\\tilde{w}_{\\ell-1}\\right)_{r}-\\left(w_{\\ell-1}\\right)_{r}\\left(0\\right)\\right\\Vert_{2}\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\left.+\\left\\Vert\\left(\\tilde{w}_{\\ell}\\right)_{r}\\right\\Vert^{2p-1}\\left\\Vert\\left(\\tilde{w}_{\\ell}\\right)_{r}-\\left(w_{\\ell}\\right)_{r}\\left(0\\right)\\right\\Vert_{2}\\right\\}}\\\\ &{\\leq\\frac{\\sqrt{\\nu_{\\operatorname*{max}}}}{m n_{o}}a_{\\operatorname*{max}}\\binom{d+\\left\\vert\\xi\\right\\vert}{d}p^{2\\left\\vert\\xi\\right\\vert+1}\\displaystyle\\sum_{r=1}^{m}2\\left(\\left(2R\\right)^{2p-1}R_{w}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "equation", "text": "$$\n=\\frac{\\sqrt{\\nu_{\\operatorname*{max}}}}{n_{o}}a_{\\operatorname*{max}}\\binom{d+|\\xi|}{d}p^{2|\\xi|+1}2\\left(2R\\right)^{2p-1}R_{w}.\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Consequently, we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{i_{1},i_{2}=1}{\\overset{n_{o}}{\\sum}}\\underset{t=1}{\\overset{L}{\\sum}}\\underset{\\alpha\\in I_{\\xi_{\\ell-1}}}{\\sum}\\underset{\\beta\\in I_{\\Delta\\xi_{\\ell}}}(Q_{0,\\ell}(\\tilde{w},\\tilde{v})-Q_{0,\\ell}(w\\left(0\\right),v\\left(0\\right)))_{i_{1},(\\alpha,\\beta)}^{2}}\\\\ &{\\leq\\frac{\\nu_{\\operatorname*{max}}^{2}}{n_{o}^{2}}\\underset{n_{u}^{2}}{\\overset{\\alpha}{=}}\\left(\\overset{d+1}{d}\\right)^{2}p^{2(2|\\xi|+1)}\\underset{i_{1},i_{2}=1}{\\overset{n_{o}}{\\sum}}\\underset{\\varepsilon=1}{\\overset{L}{\\sum}}\\underset{\\beta\\in I_{\\ell-1}}{\\sum}\\underset{\\beta\\in I_{\\Delta\\xi_{\\ell}}}{\\sum}\\underset{\\beta\\in I_{\\Delta\\xi_{\\ell}}}{\\sum}\\left(2R\\right)^{2(2p-1)}R_{w}^{2}}\\\\ &{\\leq\\nu_{\\operatorname*{max}}^{2}a_{\\operatorname*{max}}^{2}\\bigg(\\overset{d+1}{d}\\bigg)^{2}p^{2(2|\\xi|+1)}\\underset{\\ell=1}{\\overset{L}{\\sum}}\\left|I_{\\xi_{\\ell-1}}\\right|\\left|I_{\\Delta\\xi_{\\ell}}\\right|4\\left(2R\\right)^{2(2p-1)}R_{w}^{2}}\\\\ &{\\leq\\nu_{\\operatorname*{max}}^{2}a_{\\operatorname*{max}}^{2}\\bigg(\\overset{d+k}{d}\\bigg)\\bigg(\\overset{d+1}{d}\\bigg)^{4}4\\left(2R\\right)^{2(2p-1)}R_{w}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "3. Case of $Q_{0,L+1}$ : The mean value theorem with (171) induces ", "text_level": 1, "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left(\\left(Q_{0,L+1}\\right)\\left(\\tilde{w},\\tilde{v}\\right)-\\left(Q_{0,L+1}\\right)\\left(w\\left(0\\right),v\\left(0\\right)\\right)\\right)_{i,j}}\\\\ &{\\le\\displaystyle\\frac{\\sqrt{\\nu_{\\operatorname*{max}}}}{m\\sqrt{n_{o}n_{b}}}a_{\\operatorname*{max}}^{2}\\left(d+1\\right)\\binom{d+|\\xi|}{d}p^{|\\xi|+2}\\displaystyle\\sum_{r=1}^{m}\\left(\\|\\left(\\tilde{w}_{0}\\right)_{r}\\|_{2}^{2p-1}\\left\\|\\left(\\tilde{w}_{0}\\right)_{r}-\\left(w_{0}\\right)_{r}\\left(0\\right)\\right\\|_{2}\\right)}\\\\ &{\\le\\displaystyle\\frac{\\sqrt{\\nu_{\\operatorname*{max}}}}{\\sqrt{n_{o}n_{b}}}a_{\\operatorname*{max}}^{2}\\left(d+1\\right)\\binom{d+|\\xi|}{d}p^{|\\xi|+2}\\left(2R\\right)^{2p-1}R_{w},}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and hence ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{n_{o}}\\sum_{j=1}^{n_{b}}\\left(\\left(Q_{0,L+1}\\right)\\left(\\tilde{\\pmb{w}},\\tilde{\\pmb{v}}\\right)-\\left(Q_{0,L+1}\\right)\\left(\\pmb{w}\\left(0\\right),\\pmb{v}\\left(0\\right)\\right)\\right)_{i,j}^{2}}\\\\ &{\\leq\\nu_{\\operatorname*{max}}a_{\\operatorname*{max}}^{4}\\left(d+1\\right)^{2}\\bigg(\\b{d}+\\left|\\xi\\right|\\bigg)^{2}p^{2\\left(\\left|\\xi\\right|+2\\right)}\\left(2R\\right)^{2\\left(2p-1\\right)}R_{w}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "4. Case of $Q_{\\ell_{1},\\ell_{2}}$ : From the mean value theorem with (180) and (184), we attain ", "text_level": 1, "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left(\\left(Q_{\\ell,\\ell}\\right)\\left(\\tilde{\\pmb{w}},\\tilde{\\pmb{v}}\\right)-\\left(Q_{\\ell,\\ell}\\right)\\left(\\pmb{w}\\left(0\\right),\\pmb{v}\\left(0\\right)\\right)\\right)_{\\left(\\left(\\alpha_{1},\\beta_{1}\\right),i_{1}\\right),\\left(\\left(\\alpha_{2},\\beta_{2}\\right),i_{2}\\right)}}\\\\ &{\\leq\\!\\frac{\\nu_{\\operatorname*{max}}}{m n_{o}}p^{2\\left|\\xi\\right|+1}\\displaystyle\\sum_{r=1}^{m}\\left\\{\\mathbf{1}_{\\left\\{\\alpha_{1}=\\alpha_{2}\\right\\}}\\left\\|\\left(\\tilde{\\pmb{w}}_{\\ell-1}\\right)_{r}\\right\\|_{2}^{2p-1}\\left\\|\\left(\\tilde{\\pmb{w}}_{\\ell-1}\\right)_{r}-\\left(\\pmb{w}_{\\ell-1}\\right)_{r}\\left(0\\right)\\right\\|_{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\mathbf{1}_{\\left\\{\\alpha_{1}+\\beta_{1}=\\alpha_{2}+\\beta_{2}\\right\\}}\\left\\|\\left(\\tilde{\\pmb{w}}_{\\ell}\\right)_{r}\\right\\|_{2}^{2p-1}\\left\\|\\left(\\tilde{\\pmb{w}}_{\\ell}\\right)_{r}-\\left(\\pmb{w}_{\\ell}\\right)_{r}\\left(0\\right)\\right\\|_{2}\\right\\}}\\\\ &{\\leq\\!\\frac{\\nu_{\\operatorname*{max}}}{n_{o}}p^{2\\left|\\xi\\right|+1}\\left\\{\\mathbf{1}_{\\left\\{\\alpha_{1}=\\alpha_{2}\\right\\}}\\left(2R\\right)^{2p-1}+\\mathbf{1}_{\\left\\{\\alpha_{1}+\\beta_{1}=\\alpha_{2}+\\beta_{2}\\right\\}}\\left(2R\\right)^{2p-1}\\right\\}R_{w},}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "and further induces ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\sum_{i_{1},i_{2}=1}^{n_{o}}\\sum_{\\ell=1}^{L}\\sum_{\\alpha_{1},\\alpha_{2}\\in I_{\\xi_{\\ell-1}}}\\sum_{\\beta_{1},\\beta_{2}\\in I_{\\Delta\\xi_{\\ell}}}\\left(\\left(Q_{\\ell,\\ell}\\right)\\left(\\tilde{w},\\tilde{v}\\right)-\\left(Q_{\\ell,\\ell}\\right)\\left(w\\left(0\\right),v\\left(0\\right)\\right)\\right)_{\\left(\\left(\\alpha_{1},\\beta_{1}\\right),i_{1}\\right),\\left(\\left(\\alpha_{2},\\beta_{2}\\right),i_{2}\\right)}^{2}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\nu_{\\operatorname*{max}}^{2}p^{2(2|\\xi|+1)}\\displaystyle\\sum_{\\ell=1}^{L}\\sum_{\\alpha_{1}\\in I_{\\xi_{\\ell-1}}}\\sum_{\\beta_{1}\\in I_{\\Delta\\xi_{\\ell}}}\\sum_{\\alpha_{2}\\in I_{\\xi_{\\ell-1}}}\\sum_{\\beta_{2}\\in I_{\\Delta\\xi_{\\ell}}}\\left\\{\\mathbf{1}_{\\{\\alpha_{1}=\\alpha_{2}\\}}\\left(2R\\right)^{2p-1}\\right.}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\left.+\\mathbf{1}_{\\{\\alpha_{1}+\\beta_{1}=\\alpha_{2}+\\beta_{2}\\}}\\left(2R\\right)^{2(2p-1)}\\right\\}^{2}R_{w}^{2}}\\\\ &{\\leq\\nu_{\\operatorname*{max}}^{2}p^{2(2|\\xi|+1)}\\displaystyle\\sum_{\\ell=1}^{L}\\sum_{\\alpha_{1}\\in I_{\\xi_{\\ell-1}}}\\sum_{\\beta_{1}\\in I_{\\Delta\\xi_{\\ell}}}\\left(\\left(\\left|I_{\\Delta\\xi_{\\ell}}\\right|-1\\right)\\left(2R\\right)^{2(2p-1)}+\\left(\\left|I_{\\Delta\\xi_{\\ell}}\\right|-1\\right)\\left(2R\\right)^{2p-1}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\,+4\\left(2R\\right)^{2(2p-1)}\\right)R_{w}^{2}}\\\\ &{\\leq\\nu_{\\operatorname*{max}}^{2}p^{2(2|\\xi|+1)}\\displaystyle\\sum_{\\ell=1}^{L}\\sum_{\\alpha_{1}\\in I_{\\xi\\ell-1}}\\sum_{\\beta_{1}\\in I_{\\Delta\\xi_{\\ell}}}4\\left|I_{\\Delta\\xi_{\\ell}}\\right|\\left(2R\\right)^{2(2p-1)}R_{w}^{2}}\\\\ &{\\leq4\\nu_{\\operatorname*{max}}^{2}p^{2(2|\\xi|+1)}\\displaystyle\\sum_{\\ell=1}^{L}\\left|I_{\\xi_{\\ell-1}}\\right|\\left|I_{\\Delta\\xi_{\\ell}}\\right|^{2}\\left(2R\\right)^{2p-1}R_{w}^{2}}\\\\ &{\\leq4\\nu_{\\operatorname*{max}}^{2}p^{2(2|\\xi|+1)}\\binom{d+k}{d}\\binom{d+|\\xi|}{d}^{2}\\left(2R\\right)^{2p-1}R_{w}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "(192) follows that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left(\\left(Q_{\\ell,\\ell+1}\\right)(\\tilde{\\boldsymbol{w}},\\tilde{\\boldsymbol{v}})-\\left(Q_{\\ell,\\ell+1}\\right)(\\boldsymbol{w}\\left(0\\right),\\boldsymbol{v}\\left(0\\right))\\right)_{((\\alpha_{1},\\beta_{1}),i_{1}),((\\alpha_{2},\\beta_{2}),i_{2})}}\\\\ &{\\le\\mathbf{1}_{\\left\\{\\alpha_{2}=\\alpha_{1}+\\beta_{1}\\right\\}}\\frac{\\nu_{\\operatorname*{max}}}{m n_{o}}p^{|\\xi|+1}\\displaystyle\\sum_{r=1}^{m}\\left(\\|(\\tilde{\\boldsymbol{w}}_{\\ell})_{r}\\|_{2}^{2p-1}\\left\\|(\\tilde{\\boldsymbol{w}}_{\\ell})_{r}-(\\boldsymbol{w}_{\\ell})_{r}\\left(0\\right)\\right\\|_{2}\\right)}\\\\ &{\\le\\mathbf{1}_{\\left\\{\\alpha_{2}=\\alpha_{1}+\\beta_{1}\\right\\}}\\frac{\\nu_{\\operatorname*{max}}}{n_{o}}p^{|\\xi|+1}\\left(2R\\right)^{2p-1}R_{w}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "It deduces that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\sum_{i_{1},i_{2}=1}^{n_{o}}\\sum_{\\ell=1}^{L-1}\\sum_{\\alpha_{1}\\in I_{\\ell_{\\ell-1}}}\\sum_{\\beta_{1}\\in I_{\\Delta\\xi_{\\ell}}}\\,\\left(\\left(Q_{\\ell,\\ell+1}\\right)\\left(\\tilde{w},\\tilde{v}\\right)-\\left(Q_{\\ell,\\ell+1}\\right)\\left(w\\left(0\\right),v\\left(0\\right)\\right)\\right)_{\\left(\\left(\\alpha_{1},\\beta_{1}\\right),i_{1}\\right),\\left(\\left(\\alpha_{2},\\beta_{2}\\right),i_{2}\\right)}^{2}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\le\\nu_{\\operatorname*{max}}^{2}p^{2\\vert\\xi\\vert+2}\\displaystyle\\sum_{\\ell=1}^{L-1}\\sum_{\\substack{\\alpha_{1}\\in I_{\\xi_{\\ell-1}}}}\\sum_{\\beta_{1}\\in I_{\\Delta\\xi_{\\ell}}}\\sum_{\\alpha_{2}\\in I_{\\xi_{\\ell}}}\\sum_{\\beta_{2}\\in I_{\\Delta\\xi_{\\ell+1}}}\\mathbf{1}_{\\{\\alpha_{2}=\\alpha_{1}+\\beta_{1}\\}}\\left(2R\\right)^{2(2p-1)}R_{w}^{2}}\\\\ &{\\le\\nu_{\\operatorname*{max}}^{2}p^{2\\vert\\xi\\vert+2}\\displaystyle\\sum_{\\ell=1}^{L-1}\\sum_{\\alpha_{1}\\in I_{\\xi_{\\ell-1}}}\\sum_{\\beta_{1}\\in I_{\\Delta\\xi_{\\ell}}}\\left\\vert I_{\\Delta\\xi_{\\ell+1}}\\right\\vert\\left(2R\\right)^{2(2p-1)}R_{w}^{2}}\\\\ &{=\\nu_{\\operatorname*{max}}^{2}p^{2\\vert\\xi\\vert+2}\\displaystyle\\sum_{\\ell=1}^{L-1}\\left\\vert I_{\\xi_{\\ell-1}}\\right\\vert\\left\\vert I_{\\Delta\\xi_{\\ell}}\\right\\vert\\left\\vert I_{\\Delta\\xi_{\\ell+1}}\\right\\vert\\left(2R\\right)^{2(2p-1)}R_{w}^{2}}\\\\ &{\\le\\nu_{\\operatorname*{max}}^{2}p^{2\\vert\\xi\\vert+2}\\binom{d+k}{d}\\binom{d+\\vert\\xi\\vert}{d}^{2}\\left(2R\\right)^{2(2p-1)}R_{w}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "In accordance with the definition of the loss $\\mu_{\\ell}$ , it is clear that $Q_{\\ell_{1},\\ell_{2}}$ is identical to the zero matrix if the difference between $\\ell_{1}$ and $\\ell_{2}$ is greater than 1. Consequently, together with (325) and (333) we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\ell_{1},\\ell_{2}=1}^{L}\\sum_{\\alpha_{1},\\alpha_{2}\\in I_{\\xi_{\\ell-1}}}\\sum_{\\beta_{1},\\beta_{2}\\in I_{\\Delta\\xi_{\\ell}}}\\sum_{i_{1},i_{2}=1}^{n_{o}}\\left(\\left(Q_{\\ell_{1},\\ell_{2}}\\right)\\left(w\\left(0\\right),v\\left(0\\right)\\right)\\right)_{\\left(\\left(\\alpha_{1},\\beta_{1}\\right),i_{1}\\right),\\left(\\left(\\alpha_{2},\\beta_{2}\\right),i_{2}\\right)}^{2}}&{\\left(334\\right)}\\\\ &{=\\displaystyle\\sum_{i_{1},i_{2}=1}^{n_{o}}\\sum_{\\ell=1}^{L}\\sum_{\\alpha_{1},\\alpha_{2}\\in I_{\\xi_{\\ell-1}}}\\sum_{\\beta_{1},\\beta_{2}\\in I_{\\Delta\\xi_{\\ell}}}\\left(\\left(Q_{\\ell,\\ell}\\right)\\left(\\tilde{w},\\tilde{v}\\right)-\\left(Q_{\\ell,\\ell}\\right)\\left(w\\left(0\\right),v\\left(0\\right)\\right)\\right)_{\\left(\\left(\\alpha_{1},\\beta_{1}\\right),i_{1}\\right),\\left(\\left(\\alpha_{2},\\beta_{2}\\right),i_{2}\\right)}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "equation", "text": "$$\n+\\,2\\sum_{i_{1},i_{2}=1}^{n_{o}}\\sum_{\\ell=1}^{L-1}\\sum_{\\alpha_{1}\\in I_{\\ell_{\\ell-1}}}\\ \\sum_{\\beta_{1}\\in I_{\\Delta\\xi_{\\ell}}}\\ \\left(\\left(Q_{\\ell,\\ell+1}\\right)\\left(\\tilde{w},\\tilde{v}\\right)-\\left(Q_{\\ell,\\ell+1}\\right)\\left(w\\left(0\\right),v\\left(0\\right)\\right)\\right)_{\\left(\\left(\\alpha_{1},\\beta_{1}\\right),i_{1}\\right),\\left(\\left(\\alpha_{2},\\beta_{2}\\right),\\ell\\right)}^{2}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "equation", "text": "$$\n\\le\\!4\\nu_{\\mathrm{max}}^{2}p^{2(2|\\xi|+1)}{\\binom{d+k}{d}}{\\binom{d+|\\xi|}{d}}^{2}\\left(2R\\right)^{2p-1}R_{w}^{2}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\,2\\nu_{\\operatorname*{max}}^{2}p^{2|\\xi|+2}\\binom{d+k}{d}\\binom{d+|\\xi|}{d}^{2}\\left(2R\\right)^{2\\left(2p-1\\right)}R_{w}^{2}}\\\\ &{\\leq\\!6\\nu_{\\operatorname*{max}}^{2}\\binom{d+k}{d}\\binom{d+|\\xi|}{d}^{2}p^{2\\left(2|\\xi|+1\\right)}\\left(2R\\right)^{2p-1}R_{w}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "5. Case of $Q_{\\ell,L+1}$ : From (201), we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left(\\left(Q_{1,L+1}\\right)(\\tilde{\\boldsymbol{w}},\\tilde{\\boldsymbol{v}})-\\left(Q_{1,L+1}\\right)(\\boldsymbol{w}\\left(0\\right),\\boldsymbol{v}\\left(0\\right))\\right)_{((\\alpha,\\beta),i),j}}\\\\ &{\\le\\!\\frac{\\nu_{\\operatorname*{max}}}{m\\sqrt{n_{o}n_{b}}}a_{\\operatorname*{max}}\\left(d+1\\right)p^{\\left\\vert\\xi\\right\\vert+2}\\displaystyle\\sum_{r=1}^{m}\\left(\\left\\|\\left(\\tilde{\\boldsymbol{w}}_{0}\\right)_{r}\\right\\|_{2}^{2p-1}\\left\\|\\left(\\tilde{\\boldsymbol{w}}_{0}\\right)_{r}-\\left(\\boldsymbol{w}_{0}\\right)_{r}\\left(0\\right)\\right\\|_{2}\\right)}\\\\ &{\\le\\!\\frac{\\nu_{\\operatorname*{max}}}{\\sqrt{n_{o}n_{b}}}a_{\\operatorname*{max}}\\left(d+1\\right)p^{\\left\\vert\\xi\\right\\vert+2}\\left(2R\\right)^{2p-1}R_{w}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "By the definition of the loss $\\mu_{\\ell}$ , we have $Q_{\\ell,L+1}$ is identical to the zero matrix for every $\\ell>1$ . Therefore, we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{n_{o}}\\displaystyle\\sum_{j=1}^{n_{b}}\\sum_{\\ell=1}^{L}\\sum_{\\beta\\in I_{\\beta}}\\big(\\left(Q_{\\ell,L+1}\\right)(\\Tilde{w},\\Tilde{v})-\\left(Q_{1,L+1}\\right)(w\\left(0\\right),v\\left(0\\right))\\big)_{\\left((1,\\beta),i\\right)}^{2}}\\\\ &{\\displaystyle=\\sum_{i=1}^{n_{o}}\\sum_{j=1}^{n_{b}}\\sum_{\\beta\\in I_{\\beta+1}}\\left(\\left(Q_{1,L+1}\\right)(\\Tilde{w},\\Tilde{v})-\\left(Q_{1,L+1}\\right)(w\\left(0\\right),v\\left(0\\right))\\right)_{\\left((1,\\beta),i\\right),j}^{2}}\\\\ &{\\le\\left|I_{\\Delta\\xi_{1}}\\right|\\nu_{\\operatorname*{max}}^{2}a_{\\operatorname*{max}}^{2}\\left(d+1\\right)^{2}p^{2\\left(\\vert\\xi\\vert+2\\right)}\\left(2R\\right)^{2\\left(2p-1\\right)}R_{w}^{2}}\\\\ &{\\displaystyle\\le\\nu_{\\operatorname*{max}}^{2}a_{\\operatorname*{max}}^{2}\\left(\\begin{array}{c}{d+\\vert\\xi\\vert-1}\\\\ {\\vert\\xi\\vert}\\end{array}\\right)\\left(d+1\\right)^{2}p^{2\\left(\\vert\\xi\\vert+2\\right)}\\left(2R\\right)^{2\\left(2p-1\\right)}R_{w}^{2}}\\\\ &{\\le\\nu_{\\operatorname*{max}}^{2}a_{\\operatorname*{max}}^{2}\\left(d+1\\right)^{2}\\left(\\begin{array}{c}{d+\\vert\\xi\\vert}\\\\ {d}\\end{array}\\right)p^{2\\left(\\vert\\xi\\vert+2\\right)}\\left(2R\\right)^{2\\left(2p-1\\right)}R_{w}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "6. Case of QL+1,L+1: From (205), we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left(\\left(Q_{L+1,L+1}\\right)\\left(\\tilde{\\pmb{w}},\\tilde{\\pmb{v}}\\right)-\\left(Q_{L+1,L+1}\\right)\\left(\\pmb{w}\\left(0\\right),\\pmb{v}\\left(0\\right)\\right)\\right)_{j_{1},j_{2}}}\\\\ &{\\le\\displaystyle\\frac{\\nu}{m n_{b}}a_{\\operatorname*{max}}^{2}\\left(d+1\\right)^{2}p^{3}\\sum_{r=1}^{m}\\left(\\|\\left(\\tilde{\\pmb{w}}_{0}\\right)_{r}\\|_{2}^{2p-1}\\left\\|\\left(\\tilde{\\pmb{w}}_{0}\\right)_{r}-\\left(\\pmb{w}_{0}\\right)_{r}\\left(0\\right)\\right\\|_{2}\\right)}\\\\ &{\\le\\displaystyle\\frac{\\nu_{\\operatorname*{max}}}{n_{b}}a_{\\operatorname*{max}}^{2}\\left(d+1\\right)^{2}p^{3}\\left(2R\\right)^{2p-1}R_{w}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "It further follows that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{j_{1}=1}^{n_{b}}\\sum_{j_{2}=1}^{n_{b}}\\left(\\left(Q_{L+1,L+1}\\right)\\left(\\tilde{\\pmb{w}},\\tilde{\\pmb{v}}\\right)-\\left(Q_{L+1,L+1}\\right)\\left(\\pmb{w}\\left(0\\right),\\pmb{v}\\left(0\\right)\\right)\\right)_{j_{1},j_{2}}^{2}}\\\\ &{\\le\\nu_{\\operatorname*{max}}^{2}a_{\\operatorname*{max}}^{4}\\left(d+1\\right)^{4}p^{6}\\left(2R\\right)^{2\\left(2p-1\\right)}R_{w}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Therefore, combining (302), (311), (313), (339), (347), and (351) concludes that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\vert\\left\\vert\\hat{G}_{v}\\left(\\tilde{w},\\tilde{v}\\right)-\\hat{G}_{v}\\left(w\\left(0\\right),v\\left(0\\right)\\right)\\right\\vert\\right\\vert_{2}^{2}}\\\\ &{\\le\\left\\vert\\left\\vert\\hat{G}_{v}\\left(\\tilde{w},\\tilde{v}\\right)-\\hat{G}_{v}\\left(w\\left(0\\right),v\\left(0\\right)\\right)\\right\\vert\\right\\vert_{F}^{2}}\\\\ &{\\le\\!a_{\\operatorname*{max}}^{4}\\!\\left(\\!\\!\\begin{array}{c}{d+k}\\\\ {d}\\end{array}\\!\\!\\right)^{2}\\!\\!\\left(\\!\\!\\begin{array}{c}{d+\\left\\vert\\xi\\right\\vert}\\\\ {d}\\end{array}\\!\\!\\right)^{4}\\!p^{2\\left(2\\left\\vert\\xi\\right\\vert+1\\right)}\\left(2R\\right)^{2\\left(2p-1\\right)}R_{w}^{2}}\\\\ &{\\quad+\\left.4\\nu_{\\operatorname*{max}}^{2}a_{\\operatorname*{max}}^{2}\\!\\left(\\!\\!\\begin{array}{c}{d+k}\\\\ {d}\\end{array}\\!\\!\\right)\\!\\left(\\!\\!\\begin{array}{c}{d+\\left\\vert\\xi\\right\\vert}\\\\ {d}\\end{array}\\!\\!\\right)^{4}p^{2\\left(2\\left\\vert\\xi\\right\\vert+1\\right)}\\left(2R\\right)^{2\\left(2p-1\\right)}R_{w}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad+\\,\\nu_{\\operatorname*{max}}a_{\\operatorname*{max}}^{4}\\left(d+1\\right)^{2}\\left(\\begin{array}{c}{d+1\\xi}\\\\ {d}\\end{array}\\right)^{2}p^{2\\left(|\\xi|+2\\right)}\\left(2R\\right)^{2(2p-1)}R_{w}^{2}}\\\\ &{\\quad+6\\nu_{\\operatorname*{max}}^{2}p^{2\\left(2|\\xi|+1\\right)}\\left(\\begin{array}{c}{d+k}\\\\ {d}\\end{array}\\right)\\left(\\begin{array}{c}{d+1\\xi}\\\\ {d}\\end{array}\\right)^{2}\\left(2R\\right)^{2p-1}R_{w}^{2}}\\\\ &{\\quad+\\,\\nu_{\\operatorname*{max}}^{2}a_{\\operatorname*{max}}^{2}\\left(d+1\\right)^{2}\\left(\\begin{array}{c}{d+1\\xi}\\\\ {d}\\end{array}\\right)p^{2\\left(|\\xi|+2\\right)}\\left(2R\\right)^{2(2p-1)}R_{w}^{2}}\\\\ &{\\quad+\\,\\nu_{\\operatorname*{max}}^{2}a_{\\operatorname*{max}}^{4}\\left(d+1\\right)^{4}p^{6}\\left(2R\\right)^{2(2p-1)}R_{w}^{2}}\\\\ &{\\leq8\\nu_{\\operatorname*{max}}^{2}\\left(1+a_{\\operatorname*{max}}^{4}\\right)\\left(\\begin{array}{c}{d+k}\\\\ {d}\\end{array}\\right)^{2}\\left(\\begin{array}{c}{d+1\\xi}\\\\ {d}\\end{array}\\right)^{4}p^{2\\left(|\\xi|+1\\right)}\\left(2R\\right)^{2(2p-1)}R_{w}^{2}}\\\\ &{=\\frac{\\lambda_{0}^{2}}{16}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "The preceding lemma indicates that the loss will decrease rapidly in cases where $\\pmb{w}$ is not significantly distant from $\\pmb{w}$ (0). Indeed, the subsequent lemma and proposition demonstrate that $\\pmb{w}\\left(t\\right)$ remains within the designated region for any given value of $t>0$ , provided sufficiently large $m$ . ", "page_idx": 38}, {"type": "text", "text": "Proposition C.6. Set constants $C_{2}$ and $C_{3}$ as ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle C_{2}=\\sqrt{\\nu_{\\mathrm{max}}}\\sqrt{\\left(1+a_{\\mathrm{max}}^{2}\\right)}\\left(\\stackrel{d+k}{d}\\right)\\left(\\stackrel{d+}{d}\\!\\!\\!\\!/\\left(\\stackrel{.}{d}\\!\\!\\!\\mid\\right)\\left(p^{\\mid\\xi\\mid}+1\\right),}}\\\\ {{\\displaystyle C_{3}=\\frac{1}{n_{o}}\\sum_{i=1}^{n_{o}}f\\left(x_{i}\\right)^{2}+\\frac{1}{n_{b}}\\sum_{j=1}^{n_{b}}g\\left(\\tilde{x}_{j}\\right)^{2},}}\\\\ {{\\displaystyle N_{2}=\\binom{d+k}{d}\\binom{d+\\mid\\xi\\mid}{d}n_{o}+n_{b}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "For any $C>0$ and $\\begin{array}{r}{\\delta<2N_{2}\\exp\\left(-\\frac{C_{3}^{2}}{2C_{2}^{2}R^{2p}}\\right)}\\end{array}$ , if m is large enough so that ", "page_idx": 38}, {"type": "equation", "text": "$$\nm\\geq\\frac{2C^{2}}{\\lambda_{0}^{2}}\\left(C_{2}\\left(4+\\binom{d+k}{d}\\binom{d+|\\xi|}{d}\\right)R^{4p}\\log\\left(\\frac{2N_{2}}{\\delta}\\right)+C_{3}R^{2p}\\right),\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "then, with the probability of at least $1-\\delta$ over the initialization, we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{m}}\\left\\|\\left[\\begin{array}{c}{\\pm\\left(\\pmb{w}\\left(0\\right),\\pmb{v}\\left(0\\right)\\right)}\\\\ {\\pmb{\\mu}_{1}\\left(\\pmb{w}\\left(0\\right),\\pmb{v}\\left(0\\right)\\right)}\\\\ {\\vdots}\\\\ {\\pmb{\\mu}_{L}\\left(\\pmb{w}\\left(0\\right),\\pmb{v}\\left(0\\right)\\right)}\\\\ {\\pmb{h}\\left(\\pmb{w}\\left(0\\right),\\pmb{v}\\left(0\\right)\\right)}\\end{array}\\right]\\right\\|_{2}<\\frac{\\lambda_{0}}{C}R^{-p}.\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Proof. Since ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nu_{\\mathrm{max}}a_{\\mathrm{max}}^{2}\\binom{d+k}{d}^{2}\\binom{d+|\\xi|}{d}^{2}p^{2|\\xi|}\\leq C_{2}^{2},}\\\\ {\\nu_{\\mathrm{max}}\\binom{d+k}{d}^{2}\\binom{d+|\\xi|}{d}^{2}\\left(p^{|\\xi|}+1\\right)^{2}\\leq C_{2}^{2},}\\\\ {\\nu_{\\mathrm{max}}a_{\\mathrm{max}}^{2}\\left(d+1\\right)^{2}p^{2}\\leq C_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "we have ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left[\\left|\\displaystyle\\sum_{r=1}^{m}s_{i,r}\\left(\\pmb{w}\\left(0\\right),\\pmb{v}\\left(0\\right)\\right)\\right|>\\frac{\\varepsilon}{\\sqrt{n_{o}}}\\right]\\le2\\exp\\left(-\\frac{\\varepsilon^{2}}{2C_{2}^{2}R^{2p}}\\right),}\\\\ {\\mathbb{P}\\left[\\left|\\displaystyle\\sum_{r=1}^{m}\\left(\\pmb{\\mu}_{\\ell}\\right)_{r,(\\alpha,\\beta),i}\\left(\\pmb{w}\\left(0\\right),\\pmb{v}\\left(0\\right)\\right)\\right|>\\frac{\\varepsilon}{\\sqrt{n_{o}}}\\right]\\le2\\exp\\left(-\\frac{\\varepsilon^{2}}{2C_{2}^{2}R^{2p}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|\\sum_{r=1}^{m}h_{j,r}\\left(\\pmb{w}\\left(0\\right),\\pmb{v}\\left(0\\right)\\right)\\right|>\\frac{\\varepsilon}{\\sqrt{n_{b}}}\\right]\\le2\\exp\\left(-\\frac{\\varepsilon^{2}}{2C_{2}^{2}R^{2p}}\\right).\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Hence, from ", "page_idx": 39}, {"type": "equation", "text": "$$\nn_{o}+\\sum_{\\ell=1}^{L}\\left|I_{\\xi_{\\ell-1}}\\right|\\cdot\\left|I_{\\Delta\\xi_{\\ell}}\\right|\\cdot n_{o}+n_{b}\\le\\binom{d+k}{d}\\binom{d+|\\xi|}{d}n_{o}+n_{b}\\le N_{2},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "with the probability of at least ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\left(1-2\\exp\\left(-\\frac{\\varepsilon^{2}}{2C_{2}^{2}R^{2p}}\\right)\\right)^{N_{2}}\\geq1-2N\\exp\\left(-\\frac{\\varepsilon^{2}}{2C_{2}^{2}R^{2p}}\\right),\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "all the conditions ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\left\\vert\\displaystyle\\sum_{r=1}^{m}s_{i,r}\\left(w\\left(0\\right),v\\left(0\\right)\\right)\\right\\vert\\le\\frac{\\varepsilon}{\\sqrt{n_{o}}},}\\\\ &{}&\\displaystyle\\left\\vert\\displaystyle\\sum_{r=1}^{m}\\left(\\mu_{\\ell}\\right)_{r,\\left(\\alpha,\\beta\\right),i}\\left(w\\left(0\\right),v\\left(0\\right)\\right)\\right\\vert\\le\\frac{\\varepsilon}{\\sqrt{n_{o}}},}\\\\ &{}&\\displaystyle\\left\\vert\\displaystyle\\sum_{r=1}^{m}h_{j,r}\\left(w\\left(0\\right),v\\left(0\\right)\\right)\\right\\vert\\le\\frac{\\varepsilon}{\\sqrt{n_{b}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "are satisfied. Then, the square of the initial loss is bounded as ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|\\alpha(w(0),v(0))\\|_{2}^{2}+\\|h(w(0),v(0))\\|_{2}^{2}+\\displaystyle\\sum_{i=1}^{k}\\big\\|h_{\\ell}(w(0),v(0))\\big\\|_{2}^{2}}\\\\ &{=\\displaystyle\\sum_{i=1}^{k}\\alpha_{i}(w(0),v(0))^{2}+\\displaystyle\\sum_{j=1}^{k}h_{\\ell}(w(0),v(0))^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\displaystyle\\sum_{i=1}^{k}\\beta_{i}}\\\\ &{\\qquad\\qquad+\\displaystyle\\sum_{i=1}^{k}\\sum_{m\\in\\frac1{\\ell}}\\sum_{i=k}^{k}\\Big(\\mu_{i}(w_{i},\\alpha_{i})_{i}(w(0))^{2}}\\\\ &{\\le_{n}\\displaystyle\\sum_{i=1}^{k}\\sum_{m\\in\\frac1{\\ell}}(\\varepsilon+\\vert f(x_{i})\\vert^{2})^{2}+\\displaystyle\\frac1{n}\\displaystyle\\sum_{j=1}^{k}(\\varepsilon+\\vert g(x_{j})\\vert^{2}+\\displaystyle\\sum_{i=1}^{k}\\vert I_{k-1}\\vert\\cdot\\vert I_{k\\alpha_{i}}\\vert^{2}}\\\\ &{\\le_{n}\\displaystyle\\sum_{i=1}^{k}\\sum_{\\ell=1}^{k}(\\varepsilon+\\vert f(x_{i})\\vert^{2})^{2}+\\displaystyle\\frac1{n}\\displaystyle\\sum_{j=1}^{k}(\\varepsilon+\\vert g(x_{j})\\vert^{2})^{2}+\\Big(i+\\frac{k}{d}\\Big)\\Big(i^{-4}\\Big)\\Big)\\sigma}\\\\ &{\\le\\Big(4+\\Big(i^{-4k}\\Big)\\Big(d^{-{1}}\\Big)\\Big)\\sigma^{2}+2\\left(\\displaystyle\\frac1{n}\\sum_{i=1}^{k}f\\left(x_{i}\\right)^{2}+\\displaystyle\\frac1{n}\\displaystyle\\sum_{j=1}^{k}g\\left(z_{j}\\right)^{2}\\right)}\\\\ &{=\\Big(4+\\Big(i^{-4k}\\Big)\\Big(\\frac{d+1}{d}\\Big)\\Big)\\sigma^{2}+2C_{n}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "For $\\delta>0$ and $C>0$ , if $\\varepsilon$ and $m$ satisfy the following inequalities, ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\delta\\geq2N_{2}\\exp\\left(-\\frac{\\varepsilon^{2}}{2C_{2}^{2}R^{2p}}\\right),}}\\\\ {{\\varepsilon>C_{3},}}\\\\ {{m\\geq\\displaystyle\\frac{C^{2}}{\\lambda_{0}^{2}}R^{2p}\\left(\\left(4+{\\binom{d+k}{d}}{\\binom{d+|\\xi|}{d}}\\right)\\varepsilon^{2}+2C_{3}\\right),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "then for enough small $\\delta$ such that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\delta<2N_{2}\\exp\\left(-\\frac{C_{3}^{2}}{2C_{2}^{2}R^{2p}}\\right),\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "we have that with the probability of at least $1-\\delta$ , ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{1}{m}\\left(\\left\\|s\\left(w\\left(0\\right),v\\left(0\\right)\\right)\\right\\|_{2}^{2}+\\left\\|h\\left(w\\left(0\\right),v\\left(0\\right)\\right)\\right\\|_{2}^{2}+\\displaystyle\\sum_{\\ell=1}^{L}\\left\\|\\mu_{\\ell}\\left(w\\left(0\\right),v\\left(0\\right)\\right)\\right\\|_{2}^{2}\\right)}\\\\ &{\\leq\\displaystyle\\frac{1}{m}\\left(\\left(4+\\binom{d+k}{d}\\binom{d+|\\xi|}{d}\\right)\\varepsilon^{2}+2C_{3}\\right)}\\\\ &{\\leq\\displaystyle\\frac{\\lambda_{0}^{2}}{C^{2}}R^{2p}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "In other words, if ", "page_idx": 40}, {"type": "equation", "text": "$$\nm\\geq\\frac{2C^{2}}{\\lambda_{0}^{2}}\\left(C_{2}\\left(4+\\binom{d+k}{d}\\binom{d+|\\xi|}{d}\\right)R^{4p}\\log\\left(\\frac{2N_{2}}{\\delta}\\right)+C_{3}R^{2p}\\right),\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "with the probability of at least $1-\\delta$ , we have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{m}}\\left\\|\\left[\\begin{array}{c}{\\pmb{s}\\left(\\pmb{w}\\left(0\\right),\\pmb{v}\\left(0\\right)\\right)}\\\\ {\\pmb{\\mu}_{1}\\left(\\pmb{w}\\left(0\\right),\\pmb{v}\\left(0\\right)\\right)}\\\\ {\\vdots}\\\\ {\\pmb{\\mu}_{L}\\left(\\pmb{w}\\left(0\\right),\\pmb{v}\\left(0\\right)\\right)}\\\\ {\\pmb{h}\\left(\\pmb{w}\\left(0\\right),\\pmb{v}\\left(0\\right)\\right)}\\end{array}\\right]\\right\\|_{2}<\\frac{\\lambda_{0}}{2C}R^{-p}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Proposition C.7. Suppose the conditions (281) holds and $R_{w}$ are given as in Lemma C.5. For ", "page_idx": 40}, {"type": "equation", "text": "$$\nC_{4}=6\\sqrt{1+a_{\\mathrm{max}}^{2}}\\sqrt{\\nu_{\\mathrm{max}}}\\bigg(\\underset{d}{d}+k\\bigg)\\bigg(\\underset{d}{d}+|\\xi|\\bigg)p^{|\\xi|+1}2^{p}\\frac{1}{R_{w}},\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "if m is large enough so that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{m}}\\left\\|\\left[\\begin{array}{c}{\\pmb{s}\\left(\\pmb{w}\\left(0\\right),\\pmb{v}\\left(0\\right)\\right)}\\\\ {\\pmb{\\mu}_{1}\\left(\\pmb{w}\\left(0\\right),\\pmb{v}\\left(0\\right)\\right)}\\\\ {\\vdots}\\\\ {\\pmb{\\mu}_{L}\\left(\\pmb{w}\\left(0\\right),\\pmb{v}\\left(0\\right)\\right)}\\\\ {\\pmb{h}\\left(\\pmb{w}\\left(0\\right),\\pmb{v}\\left(0\\right)\\right)}\\end{array}\\right]\\right\\|_{2}<\\frac{\\lambda_{0}}{C_{4}}R^{-p},\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "then ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|(\\boldsymbol{w}_{\\ell})_{r}\\left(t\\right)-\\left(\\boldsymbol{w}_{\\ell}\\right)_{r}\\left(0\\right)\\|_{2}<R_{w},}\\\\ {\\|(\\boldsymbol{v}_{\\ell})_{r}\\left(t\\right)-\\left(\\boldsymbol{v}_{\\ell}\\right)_{r}\\left(0\\right)\\|_{2}<R_{w},}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "for all $t>0$ , $r\\in[m]$ and $\\ell=0,\\cdots\\,,L$ . ", "page_idx": 40}, {"type": "text", "text": "Proof. We begin the proof by defining ", "page_idx": 40}, {"type": "text", "text": "T $\\Gamma=\\operatorname*{sup}\\left\\{t\\geq0:\\|(w_{\\ell})_{r}\\left(t\\right)-\\left(w_{\\ell}\\right)_{r}\\left(0\\right)\\|_{2}<R_{w}\\mathrm{~and~}\\|(v_{\\ell})_{r}\\left(t\\right)-\\left(v_{\\ell}\\right)_{r}\\left(0\\right)\\|_{2}<R_{w},\\forall\\ell=0,\\cdots,N.$ L . (398) Since $(w_{\\ell})_{r}$ and $\\left(v_{\\ell}\\right)$ are continuous, the above set in supremum is nonempty. For $t\\in(0,T)$ , ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{d}{d t}\\left\\|\\left[\\begin{array}{c}{s\\left(\\boldsymbol{w}\\left(t\\right),\\boldsymbol{v}\\left(t\\right)\\right)}\\\\ {\\mu_{1}\\left(\\boldsymbol{w}\\left(t\\right),\\boldsymbol{v}\\left(t\\right)\\right)}\\\\ {\\vdots}\\\\ {\\mu_{L}\\left(\\boldsymbol{w}\\left(t\\right),\\boldsymbol{v}\\left(t\\right)\\right)}\\\\ {h\\left(\\boldsymbol{w}\\left(t\\right),\\boldsymbol{v}\\left(t\\right)\\right)}\\end{array}\\right]\\right\\|_{2}}\\\\ &{=-\\,2\\,\\left[\\begin{array}{c}{s\\left(\\boldsymbol{w}\\left(t\\right),\\boldsymbol{v}\\left(t\\right)\\right)}\\\\ {\\mu_{1}\\left(\\boldsymbol{w}\\left(t\\right),\\boldsymbol{v}\\left(t\\right)\\right)}\\\\ {\\vdots}\\\\ {\\mu_{L}\\left(\\boldsymbol{w}\\left(t\\right),\\boldsymbol{v}\\left(t\\right)\\right)}\\end{array}\\right]^{\\top}\\left(\\hat{G}_{\\boldsymbol{w}}+\\hat{G}_{\\boldsymbol{v}}\\right)\\left[\\begin{array}{c}{\\mu\\left(\\boldsymbol{w}\\left(t\\right),\\boldsymbol{v}\\left(t\\right)\\right)}\\\\ {\\mu_{1}\\left(\\boldsymbol{w}\\left(t\\right),\\boldsymbol{v}\\left(t\\right)\\right)}\\\\ {\\vdots}\\\\ {\\mu_{L}\\left(\\boldsymbol{w}\\left(t\\right),\\boldsymbol{v}\\left(t\\right)\\right)}\\\\ {h\\left(\\boldsymbol{w}\\left(t\\right),\\boldsymbol{v}\\left(t\\right)\\right)}\\end{array}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "equation", "text": "$$\n\\leq-\\left.2\\cdot\\frac{1}{2}\\lambda_{0}\\right\\Vert\\left[\\left[\\mu\\left(\\boldsymbol{w}\\left(t\\right),\\boldsymbol{v}\\left(t\\right)\\right)\\right.\\right]\\right]^{2}}\\\\ {\\qquad\\qquad\\qquad\\left.\\left\\Vert\\left[\\mu_{1}\\left(\\boldsymbol{w}\\left(t\\right),\\boldsymbol{v}\\left(t\\right)\\right)\\right]\\right\\Vert_{2}^{2},}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where the last inequality comes from Lemma C.5 and the positive semi-definiteness of $\\widehat{G}_{w}$ . This implies that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\left\\|\\left[\\begin{array}{c}{s\\left(w\\left(t\\right),v\\left(t\\right)\\right)}\\\\ {\\mu_{1}\\left(w\\left(t\\right),v\\left(t\\right)\\right)}\\\\ {\\vdots}\\\\ {\\mu_{L}\\left(w\\left(t\\right),v\\left(t\\right)\\right)}\\\\ {h\\left(w\\left(t\\right),v\\left(t\\right)\\right)}\\end{array}\\right]\\right\\|_{2}\\le\\exp\\left(-\\frac{\\lambda_{0}}{2}t\\right)\\left\\|\\left[\\begin{array}{c}{s\\left(w\\left(0\\right),v\\left(0\\right)\\right)}\\\\ {\\mu_{1}\\left(w\\left(0\\right),v\\left(0\\right)\\right)}\\\\ {\\vdots}\\\\ {\\mu_{L}\\left(w\\left(0\\right),v\\left(0\\right)\\right)}\\\\ {h\\left(w\\left(0\\right),v\\left(0\\right)\\right)}\\end{array}\\right]\\right\\|_{2}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "For $t\\in(0,T)$ and $\\ell\\in\\{0,\\cdots\\,,L\\}$ , from $\\|(\\pmb{\\omega}_{\\ell})_{r}\\left(0\\right)\\|_{2}\\leq R$ and $\\|(\\pmb{v}_{\\ell})_{r}\\left(0\\right)\\|_{2}\\leq|I_{\\xi\\ell}|$ and (398), we have ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\big(\\pmb{w}_{\\ell}\\big)_{r}\\left(t\\right)\\|_{2}\\leq\\|\\big(\\pmb{w}_{\\ell}\\big)_{r}\\left(t\\right)-\\big(\\pmb{w}_{\\ell}\\big)_{r}\\left(0\\right)\\|_{2}+\\|\\big(\\pmb{w}_{\\ell}\\big)_{r}\\left(0\\right)\\|_{2}}\\\\ &{\\qquad\\qquad<R_{w}+R}\\\\ &{\\qquad\\qquad\\leq2R,}\\\\ &{\\|\\big(\\pmb{v}_{\\ell}\\big)_{r}\\left(t\\right)\\|_{2}\\leq\\|\\big(\\pmb{v}_{\\ell}\\big)_{r}\\left(t\\right)-\\big(\\pmb{v}_{\\ell}\\big)_{r}\\left(0\\right)\\|_{2}+\\|\\big(\\pmb{v}_{\\ell}\\big)_{r}\\left(0\\right)\\|_{2}}\\\\ &{\\qquad\\qquad<R_{w}+|I_{\\xi_{\\ell}}|}\\\\ &{\\qquad\\qquad\\qquad\\leq R.}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "From (44), (57), (52), and (48), we can attain ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\left\\|\\displaystyle\\frac{\\partial s_{i}\\left(w\\left(t\\right),v\\left(t\\right)\\right)}{\\partial\\left(w_{\\ell}\\right)_{r}}\\right\\|_{2}\\leq\\sqrt{\\displaystyle\\frac{1}{m n_{o}}}a_{\\operatorname*{max}}\\sqrt{|I_{\\xi_{\\ell}}|}\\left|J_{\\Delta\\xi_{\\ell+1}}\\right|p^{\\Delta\\xi_{\\ell+1}+1}2^{p-1}R^{p},}&\\\\ &{\\left\\|\\displaystyle\\frac{\\partial h_{j}\\left(w\\left(t\\right),v\\left(t\\right)\\right)}{\\partial\\left(w_{0}\\right)_{r}}\\right\\|_{2}\\leq\\sqrt{\\displaystyle\\frac{\\nu_{\\operatorname*{max}}}{m n_{b}}}a_{\\operatorname*{max}}\\left(d+1\\right)p^{2}2^{p-1}R^{p},}&\\\\ &{\\left\\|\\displaystyle\\frac{\\partial\\left(\\mu_{\\ell}\\right)_{\\left(\\alpha,\\beta\\right),i}\\left(w,v\\right)}{\\partial\\left(w_{\\ell}\\right)_{r}}\\right\\|_{2}\\leq\\sqrt{\\displaystyle\\frac{\\nu_{\\operatorname*{max}}}{m n_{o}}}p^{2p-1}R^{p},}&\\\\ &{\\left\\|\\displaystyle\\frac{\\partial\\left(\\mu_{\\ell}\\right)_{\\left(\\alpha,\\beta\\right),i}\\left(w,v\\right)}{\\partial\\left(w_{\\ell-1}\\right)_{r}}\\right\\|_{2}\\leq\\sqrt{\\displaystyle\\frac{\\nu_{\\operatorname*{max}}}{m n_{o}}}p^{|\\xi|+1}2^{p-1}R^{p},}&\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "for all $t\\in(0,T)$ . ", "page_idx": 41}, {"type": "text", "text": "Therefore, we can induce that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\displaystyle\\frac{d}{d t}\\left(w_{\\ell}\\right)_{r}(t)\\right\\|_{2}}\\\\ &{=\\left\\|\\displaystyle\\sum_{i=1}^{n_{\\alpha}}s_{i}\\left(\\{w\\left(t\\right),v\\left(t\\right)\\}\\cdot\\frac{\\partial s_{i}\\left(w\\left(t\\right),v\\left(t\\right)\\right)}{\\partial\\left(w_{\\ell}\\right)_{r}}+\\displaystyle\\sum_{j=1}^{n_{\\beta}}h_{j}\\left(w\\left(t\\right),v\\left(t\\right)\\right)\\cdot\\frac{\\partial h_{j}\\left(w\\left(t\\right),v\\left(t\\right)\\right)}{\\partial\\left(w_{\\ell}\\right)_{r}}\\right.}\\\\ &{\\quad\\left.+\\displaystyle\\sum_{i=1}^{n_{\\alpha}}\\sum_{\\ell=1}^{L}\\sum_{\\alpha\\in I_{\\ell\\prime-1}}^{j}\\beta\\epsilon_{i\\alpha_{\\ell\\prime}}\\frac{\\partial\\left(\\mu_{\\ell}\\right)_{\\left(\\alpha,\\beta\\right),i}\\left(w\\left(t\\right),v\\left(t\\right)\\right)}{\\partial\\left(w_{\\ell}\\right)_{r}}\\cdot\\left(\\mu_{\\ell}\\right)_{\\left(\\alpha,\\beta\\right),i}\\left(w\\left(t\\right),v\\left(t\\right)\\right)\\right\\|_{2}}\\\\ &{\\leq\\displaystyle\\sum_{i=1}^{n_{\\alpha}}\\left\\|\\frac{\\partial s_{i}\\left(w\\left(t\\right),v\\left(t\\right)\\right)}{\\partial\\left(w_{\\ell}\\right)_{r}}\\right\\|_{2}\\cdot\\left|s_{i}\\left(w\\left(t\\right),v\\left(t\\right)\\right)\\right|}\\\\ &{\\quad+\\displaystyle\\mathbf{1}_{\\left\\{\\ell=0\\right\\}}\\cdot\\sum_{j=1}^{n_{\\beta}}\\left\\|\\frac{\\partial h_{j}\\left(w\\left(t\\right),v\\left(t\\right)\\right)}{\\partial\\left(w_{\\ell}\\right)_{r}}\\right\\|_{2}\\cdot\\left|h_{j}\\left(w\\left(t\\right),v\\left(t\\right)\\right)\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "equation", "text": "$$\n+\\left(\\mathbf{1}_{\\left\\{\\ell\\neq0\\right\\}}\\cdot\\sum_{i=1}^{n_{o}}\\sum_{\\alpha\\in I_{\\xi_{\\ell-1}}}\\sum_{\\beta\\in I_{\\Delta\\xi_{\\ell}}}\\left\\|\\frac{\\partial\\left(\\pmb{\\mu}_{\\ell}\\right)_{\\left(\\alpha,\\beta\\right),i}\\left(\\pmb{w}\\left(t\\right),\\pmb{v}\\left(t\\right)\\right)}{\\partial\\left(\\pmb{w}_{\\ell}\\right)_{r}}\\right\\|_{2}\\cdot\\left|\\left(\\pmb{\\mu}_{\\ell}\\right)_{\\left(\\alpha,\\beta\\right),i}\\left(\\pmb{w}\\left(t\\right),\\pmb{v}\\left(t\\right)\\right)\\right|\n$$", "text_format": "latex", "page_idx": 42}, {"type": "equation", "text": "$$\n+\\mathbf{1}_{\\left\\{\\ell\\neq L\\right\\}}\\cdot\\sum_{i=1}^{n_{o}}\\sum_{\\alpha\\in I_{\\xi_{\\ell}}}\\sum_{\\beta\\in I_{\\Delta\\xi_{\\ell+1}}}\\left\\|\\frac{\\partial\\left(\\pmb{\\mu}_{\\ell+1}\\right)_{\\left(\\alpha,\\beta\\right),i}\\left(\\pmb{w}\\left(t\\right),\\pmb{v}\\left(t\\right)\\right)}{\\partial\\left(\\pmb{w}_{\\ell}\\right)_{r}}\\right\\|_{2}\\cdot\\left|\\left(\\pmb{\\mu}_{\\ell+1}\\right)_{\\left(\\alpha,\\beta\\right),i}\\left(\\pmb{w}\\left(t\\right),\\pmb{v}\\left(t\\right)\\right)\\right|\\right).\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "The first term and second term are bounded by ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\underset{i=1}{\\overset{n_{o}}{\\sum}}\\left\\|\\frac{\\partial s_{i}\\left(w\\left(t\\right),v\\left(t\\right)\\right)}{\\partial\\left(w\\right)_{r}}\\right\\|_{2}\\cdot\\left|s_{i}\\left(w\\left(t\\right),v\\left(t\\right)\\right)\\right|}\\\\ &{\\leq\\underset{i=1}{\\overset{n_{o}}{\\sum}}\\sqrt{\\frac{\\nu_{\\operatorname*{max}}}{m n_{o}}}\\alpha_{\\operatorname*{max}}\\sqrt{\\left|I_{\\xi_{\\ell}}\\right|\\left|J_{\\Delta\\xi_{\\ell+1}}\\right|p^{\\left\\vert\\xi\\right|+1}2^{p-1}R^{p}\\cdot\\left|s_{i}\\left(w\\left(t\\right),v\\left(t\\right)\\right)\\right|}}\\\\ &{=\\sqrt{\\frac{\\nu_{\\operatorname*{max}}}{m}}\\alpha_{\\operatorname*{max}}\\sqrt{\\left|I_{\\xi_{\\ell}}\\right|\\left|J_{\\Delta\\xi_{\\ell+1}}\\right|p^{\\left\\vert\\xi\\right|+1}2^{p-1}R^{p}\\cdot\\frac{1}{\\sqrt{n_{o}}}\\underset{i=1}{\\overset{n_{o}}{\\sum}}\\left|s_{i}\\left(w\\left(t\\right),v\\left(t\\right)\\right)\\right|}}\\\\ &{\\leq\\sqrt{\\frac{\\nu_{\\operatorname*{max}}}{m}}\\alpha_{\\operatorname*{max}}\\sqrt{\\left|I_{\\xi_{\\ell}}\\right|\\left|J_{\\Delta\\xi_{\\ell+1}}\\right|p^{\\left\\vert\\xi\\right|+1}2^{p-1}R^{p}\\cdot\\left\\|s\\left(w\\left(t\\right),v\\left(t\\right)\\right)\\right\\|_{2}}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "and ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{j=1}^{n_{b}}\\left\\|\\frac{\\partial h_{j}\\left(w\\left(t\\right),v\\left(t\\right)\\right)}{\\partial\\left(w_{\\ell}\\right)_{r}}\\right\\|_{2}\\cdot\\left|h_{j}\\left(w\\left(t\\right),v\\left(t\\right)\\right)\\right|}\\\\ &{\\leq\\sqrt{\\frac{\\nu_{\\operatorname*{max}}}{m}}a_{\\operatorname*{max}}\\left(d+1\\right)p^{2}2^{p-1}R^{p}\\cdot\\displaystyle\\frac{1}{\\sqrt{n_{b}}}\\sum_{j=1}^{n_{b}}\\left|h_{j}\\left(w\\left(t\\right),v\\left(t\\right)\\right)\\right|}\\\\ &{\\leq\\sqrt{\\frac{\\nu_{\\operatorname*{max}}}{m}}a_{\\operatorname*{max}}\\left(d+1\\right)p^{2}2^{p-1}R^{p}\\left\\|h\\left(w\\left(t\\right),v\\left(t\\right)\\right)\\right\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "The third term is bounded by ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{n_{o}}\\sum_{\\alpha\\in I_{\\xi_{\\ell-1}}}\\sum_{\\beta\\in I_{\\Delta\\xi_{\\ell}}}\\left\\|\\frac{\\partial\\left(\\pmb{\\mu}_{\\ell}\\right)_{\\left(\\alpha,\\beta\\right),i}\\left(\\pmb{w}\\left(t\\right),\\pmb{v}\\left(t\\right)\\right)}{\\partial\\left(\\pmb{w}_{\\ell}\\right)_{r}}\\right\\|_{2}\\cdot\\left|\\left(\\pmb{\\mu}_{\\ell}\\right)_{\\left(\\alpha,\\beta\\right),i}\\left(\\pmb{w}\\left(t\\right),\\pmb{v}\\left(t\\right)\\right)\\right|}\\\\ &{\\le\\sqrt{\\frac{\\nu_{\\operatorname*{max}}}{m n_{o}}}p2^{p-1}R^{p}\\cdot\\displaystyle\\sum_{i=1}^{n_{o}}\\sum_{\\alpha\\in I_{\\xi_{\\ell-1}}}\\sum_{\\beta\\in I_{\\Delta\\xi_{\\ell}}}\\left|\\left(\\pmb{\\mu}_{\\ell}\\right)_{\\left(\\alpha,\\beta\\right),i}\\left(\\pmb{w}\\left(t\\right),\\pmb{v}\\left(t\\right)\\right)\\right|}\\\\ &{\\le\\sqrt{\\frac{\\nu_{\\operatorname*{max}}}{m}}p2^{p-1}R^{p}\\sqrt{\\left|I_{\\xi_{\\ell-1}}\\right|\\cdot\\left|I_{\\Delta\\xi_{\\ell}}\\right|}\\cdot\\left\\|\\pmb{\\mu}_{\\ell}\\right\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "and the fourth term is ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{i=1}^{n_{o}}\\sum_{\\alpha\\in I_{\\xi_{\\ell}}}\\sum_{\\beta\\in I_{\\Delta\\xi_{\\ell+1}}}\\left\\|\\frac{\\partial\\left(\\pmb{\\mu}_{\\ell+1}\\right)_{(\\alpha,\\beta),i}\\left(\\pmb{w}\\left(t\\right),\\pmb{v}\\left(t\\right)\\right)}{\\partial\\left(\\pmb{w}_{\\ell}\\right)_{r}}\\right\\|_{2}\\cdot\\left|\\left(\\pmb{\\mu}_{\\ell+1}\\right)_{(\\alpha,\\beta),i}\\left(\\pmb{w}\\left(t\\right),\\pmb{v}\\left(t\\right)\\right)\\right|}\\\\ &{\\le\\sqrt{\\frac{\\nu_{\\operatorname*{max}}}{m n\\alpha_{o}}}p^{|\\xi|+1}2^{p-1}R^{p}\\cdot\\displaystyle\\sum_{i=1}^{n_{o}}\\sum_{\\alpha\\in I_{\\xi_{\\ell}}}\\sum_{\\beta\\in I_{\\Delta\\xi_{\\ell+1}}}\\left|\\left(\\pmb{\\mu}_{\\ell+1}\\right)_{(\\alpha,\\beta),i}\\left(\\pmb{w}\\left(t\\right),\\pmb{v}\\left(t\\right)\\right)\\right|}\\\\ &{\\le\\sqrt{\\frac{\\nu_{\\operatorname*{max}}}{m}}p^{|\\xi|+1}2^{p-1}R^{p}\\sqrt{|I_{\\xi_{\\ell}}|\\cdot\\left|I_{\\Delta\\xi_{\\ell+1}}\\right|}\\cdot\\left\\|\\pmb{\\mu}_{\\ell+1}\\right\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Therefore, for $t\\in(0,T)$ and $\\ell=\\{0,\\ldots,L\\}$ , ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\left\\|\\frac{d}{d t}\\left(\\pmb{w}_{\\ell}\\right)_{r}(t)\\right\\|_{2}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leqslant\\sqrt{\\frac{{p_{\\mathrm{sm}}}}{m}}\\frac{{{q_{\\mathrm{sm}}}}}{{{q_{\\mathrm{sm}}}}}\\sqrt{\\left(\\frac{d+k}{d}\\right)\\left(\\frac{d+k}{d}\\right)}y^{i+1}\\mathcal{P}^{k}\\{\\alpha(t),\\,\\mathrm{e}^{(t)}\\}\\}_{2}\\eqno\\qquad\\qquad(43\\pi)}\\\\ &{\\quad+{\\tau_{1}}\\langle\\omega\\rangle\\cdot\\frac{{{q_{\\mathrm{sm}}}}}{{{q_{\\mathrm{sm}}}}}\\omega_{\\mathrm{sm}}(4-t)+y^{i}\\mathcal{P}^{k}\\{\\alpha(t),\\,\\mathrm{e}^{(t)}\\}}\\\\ &{\\quad+{\\tau_{1}}\\langle\\omega\\rangle\\cdot\\sqrt{\\frac{{p_{\\mathrm{sm}}}}{m}}\\sqrt{\\left(\\frac{d+k}{d}\\right)\\left(\\frac{d+k}{d}\\right)}y^{i+1}\\mathcal{P}^{k}\\{\\alpha(t),\\,\\mathrm{e}^{(t)}\\}\\}_{2}\\eqno\\qquad\\qquad(43\\pi)}\\\\ &{\\quad+{\\tau_{1}}\\langle\\omega\\rangle\\cdot\\sqrt{{p_{\\mathrm{sm}}}}\\sqrt{\\left(\\frac{d+k}{d}\\right)\\left(\\frac{d+k}{d}\\right)}y^{i+1}\\mathcal{P}^{k}\\{\\alpha(t),\\,\\mathrm{e}^{(t)}\\}\\}_{2}\\xrightarrow{()}}\\\\ &{\\quad+{\\tau_{1}}\\langle\\omega\\rangle\\cdot\\sqrt{{p_{\\mathrm{sm}}}}\\frac{{{q_{\\mathrm{sm}}}}}{{{p_{\\mathrm{sm}}}}}\\sqrt{\\left(\\frac{d+k}{d}\\right)\\left(\\frac{d+k}{d}\\right)}y^{i+1}\\mathcal{P}^{k}\\{\\alpha(t),\\,\\mathrm{e}^{(t)}\\}\\rangle_{2}\\eqno\\qquad\\qquad(43\\pi)}\\\\ &{\\lesssim\\sqrt{\\frac{{p_{\\mathrm{sm}}}}{m}}\\sqrt{\\left(\\frac{d+k}{d}\\right)^{2}+{\\tau_{1}}\\alpha_{\\mathrm{sm}}^{2}}\\sqrt{\\left(\\frac{d+k}{d}\\right)}\\left(\\frac{d+k}{d}\\right)\\beta^{i+1}\\mathcal{P}^{k+2-k}\\frac{{p_{\\mathrm{sm}}}}{{{p_{\\mathrm{sm}}}}}\\left\\|\\frac{y}{{h_{\\mathrm{sm}}}(\\omega)}\\right\\|_{2}\\qquad\\arg\\left(\\frac{1}{{q \n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "On the other hand, ", "text_level": 1, "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\|\\int_{0}^{T}\\frac{d}{d s}\\left(\\boldsymbol{w}_{\\ell}\\right)_{r}\\left(s\\right)\\,d s\\right\\|_{2}^{2}=\\displaystyle\\sum_{q=1}^{d}\\left(\\int_{0}^{T}\\frac{d}{d s}\\left(\\boldsymbol{w}_{\\ell}\\right)_{r,q}\\left(s\\right)\\,d s\\right)^{2}}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\int_{0}^{T}\\frac{d}{q=1}\\left(\\int_{0}^{T}\\frac{d}{d s}\\left(\\boldsymbol{w}_{\\ell}\\right)_{r,q}\\left(s\\right)\\,d s\\right)\\cdot\\frac{d}{d s}\\left(\\boldsymbol{w}_{\\ell}\\right)_{r,q}\\left(s\\right)\\,d s}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\leq\\int_{0}^{T}\\left\\|\\int_{0}^{T}\\frac{d}{d s}\\left(\\boldsymbol{w}_{\\ell}\\right)_{r}\\left(s\\right)\\,d s\\right\\|_{2}\\cdot\\left\\|\\frac{d}{d s}\\left(\\boldsymbol{w}_{\\ell}\\right)\\left(s\\right)\\right\\|_{2}\\,d s}\\\\ {\\displaystyle\\qquad\\qquad\\qquad=\\left\\|\\int_{0}^{T}\\frac{d}{d s}\\left(\\boldsymbol{w}_{\\ell}\\right)_{r}\\left(s\\right)\\,d s\\right\\|_{2}\\cdot\\int_{0}^{T}\\left\\|\\frac{d}{d s}\\left(\\boldsymbol{w}_{\\ell}\\right)\\left(s\\right)\\right\\|_{2}\\,d s,}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "and hence ", "text_level": 1, "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\left\\lVert\\left(w_{\\ell}\\right)_{r}\\left(T\\right)-\\left(w_{\\ell}\\right)_{r}\\left(0\\right)\\right\\rVert_{2}}\\\\ &{}&{=\\left\\lVert\\int_{0}^{T}\\frac{d}{d s}\\left(w_{\\ell}\\right)_{r}\\left(s\\right)\\,d s\\right\\rVert_{2}}\\\\ &{}&{\\stackrel{}{\\le}\\int_{0}^{T}\\left\\lVert\\frac{d}{d s}\\left(w_{\\ell}\\right)_{r}\\left(s\\right)\\right\\rVert_{2}\\,d s}\\\\ &{}&{\\left\\lVert\\left[\\frac{s}{\\eta}\\left(w\\right)_{r}\\left(0\\right)\\right]_{2}\\,,}\\\\ &{}&{\\stackrel{}{\\le}3\\sqrt{1+a_{\\mathrm{max}}^{2}}\\sqrt{\\frac{\\nu_{\\mathrm{max}}}{m}}\\sqrt{\\left(\\frac{d+k}{d}\\right)}\\left(\\frac{d+\\left\\lvert\\xi\\right\\rvert}{d}\\right)p^{\\left\\lvert\\xi\\right\\rvert+1}2^{p-1}R^{p}\\int_{0}^{T}\\exp\\left(-\\frac{\\lambda_{0}}{2}s\\right)\\,d s\\left\\lVert\\left[\\begin{array}{l l}{\\left[\\mu_{1}\\left(w\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\right)}\\\\ {\\mu_{L}\\left(w\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\left(0\\right)_{\\ell}\\\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": ") )) )) ) (447) ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\leq\\!\\frac{6}{\\lambda_{0}}\\sqrt{1+a_{\\operatorname*{max}}^{2}}\\sqrt{\\frac{\\nu_{\\operatorname*{max}}}{m}}\\sqrt{\\binom{d+k}{d}}\\binom{d+|\\xi|}{d}p^{|\\xi|+1}2^{p-1}R^{p}\\left\\|\\left[\\begin{array}{c}{\\displaystyle s\\left(w\\left(0\\right),v\\left(0\\right)\\right)}\\\\ {\\displaystyle\\mu_{1}\\left(w\\left(0\\right),v\\left(0\\right)\\right)}\\\\ {\\vdots}\\\\ {\\displaystyle\\mu_{L}\\left(w\\left(0\\right),v\\left(0\\right)\\right)}\\\\ {\\displaystyle h\\left(w\\left(0\\right),v\\left(0\\right)\\right)}\\end{array}\\right]\\right\\|_{2}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "If $m$ is large enough to satisfy (395), we have ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\left\\|\\left(w_{\\ell}\\right)_{r}(T)-\\left(w_{\\ell}\\right)_{r}(0)\\right\\|_{2}\\leq\\frac{6\\sqrt{1+a_{\\operatorname*{max}}^{2}}\\sqrt{\\nu_{\\operatorname*{max}}}\\sqrt{{\\binom{d+k}{d}}{\\binom{d+|\\xi|}{d}}p^{|\\xi|+1}2^{p-1}}}{\\lambda_{0}}\\cdot\\frac{\\lambda_{0}}{C_{4}}<\\frac{1}{2}R_{w}<R_{w}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Similarly, we obtain ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|\\frac{d}{d t}\\left(\\boldsymbol{v}_{\\ell}\\right)_{r}\\left(t\\right)\\right\\|_{2}}\\\\ &{=\\left\\|\\displaystyle\\sum_{i=1}^{n_{o}}s_{i}\\left(\\boldsymbol{w}\\left(t\\right),\\boldsymbol{v}\\left(t\\right)\\right)\\cdot\\frac{\\partial s_{i}\\left(\\boldsymbol{w}\\left(t\\right),\\boldsymbol{v}\\left(t\\right)\\right)}{\\partial\\left(\\boldsymbol{v}_{\\ell}\\right)_{r}}+\\sum_{j=1}^{n_{b}}h_{j}\\left(\\boldsymbol{w}\\left(t\\right),\\boldsymbol{v}\\left(t\\right)\\right)\\cdot\\frac{\\partial h_{j}\\left(\\boldsymbol{w}\\left(t\\right),\\boldsymbol{v}\\left(t\\right)\\right)}{\\partial\\left(\\boldsymbol{v}_{\\ell}\\right)_{r}}\\right.}\\\\ &{\\quad\\left.\\qquad+\\displaystyle\\sum_{i=1}^{n_{o}}\\sum_{\\ell^{\\prime}=1}^{L}\\sum_{\\alpha\\in I_{\\ell_{\\ell^{\\prime}-1}}}\\beta\\!\\left(\\mu_{\\ell^{\\prime}}\\!\\right)_{\\left(\\alpha,\\beta\\right),i}\\left(\\boldsymbol{w}\\left(t\\right),\\boldsymbol{v}\\left(t\\right)\\right)\\cdot\\left(\\mu_{\\ell^{\\prime}}\\right)_{\\left(\\alpha,\\beta\\right),i}\\left(\\boldsymbol{w}\\left(t\\right),\\boldsymbol{v}\\left(t\\right)\\right)\\right\\|_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\leq\\sum_{i=1}^{n_{o}}\\left|s_{i}\\left(w\\left(t\\right),v\\left(t\\right)\\right)\\right|\\cdot\\left\\|\\frac{\\partial s_{i}\\left(w\\left(t\\right),v\\left(t\\right)\\right)}{\\partial\\left(v_{\\ell}\\right)_{r}}\\right\\|_{2}}\\\\ {\\displaystyle\\quad+\\,{\\bf1}_{\\left\\{\\ell=0\\right\\}}\\cdot\\sum_{j=1}^{n_{b}}\\left|h_{j}\\left(w\\left(t\\right),v\\left(t\\right)\\right)\\right|\\cdot\\left\\|\\frac{\\partial h_{j}\\left(w\\left(t\\right),v\\left(t\\right)\\right)}{\\partial\\left(v_{\\ell}\\right)_{r}}\\right\\|_{2}}\\\\ {\\displaystyle\\quad+\\,{\\bf1}_{\\left\\{\\ell\\neq0\\right\\}}\\cdot\\sum_{i=1}^{n_{o}}\\sum_{\\alpha\\in I_{\\xi_{\\ell-1}}}\\sum_{\\beta\\in I_{\\Delta\\xi_{\\ell}}}\\left\\|\\frac{\\partial\\left(\\pmb{\\mu}_{\\ell}\\right)_{\\left(\\alpha,\\beta\\right),i}\\left(\\pmb{w}\\left(t\\right),v\\left(t\\right)\\right)}{\\partial\\left(v_{\\ell}\\right)_{r}}\\right\\|_{2}\\cdot\\left|\\left(\\pmb{\\mu}_{\\ell}\\right)_{\\left(\\alpha,\\beta\\right),i}\\left(\\pmb{w}\\left(t\\right),v\\left(t\\right)\\right)\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "equation", "text": "$$\n+\\,\\mathbf{1}_{\\left\\{\\ell\\neq L\\right\\}}\\cdot\\sum_{i=1}^{n_{o}}\\sum_{\\alpha\\in I_{\\xi_{\\ell}}}\\sum_{\\beta\\in I_{\\Delta\\xi_{\\ell+1}}}\\left\\|\\frac{\\partial\\left(\\pmb{\\mu}_{\\ell+1}\\right)_{\\left(\\alpha,\\beta\\right),i}\\left(\\pmb{w}\\left(t\\right),\\pmb{v}\\left(t\\right)\\right)}{\\partial\\left(\\pmb{v}_{\\ell}\\right)_{r}}\\right\\|_{2}\\cdot\\left|\\left(\\pmb{\\mu}_{\\ell+1}\\right)_{\\left(\\alpha,\\beta\\right),i}\\left(\\pmb{w}\\left(t\\right),\\pmb{v}\\left(t\\right)\\right)\\right|.\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "The first term and second term are bounded by ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\,\\underset{i=1}{\\overset{n_{o}}{\\sum}}\\left\\vert s_{i}\\left(\\boldsymbol{w}\\left(t\\right),\\boldsymbol{v}\\left(t\\right)\\right)\\right\\vert\\cdot\\left\\Vert\\frac{\\partial s_{i}\\left(\\boldsymbol{w}\\left(t\\right),\\boldsymbol{v}\\left(t\\right)\\right)}{\\partial\\left(\\boldsymbol{v}_{\\ell}\\right)_{r}}\\right\\Vert_{2}}\\\\ &{\\leq\\!\\sqrt{\\frac{\\nu_{\\operatorname*{max}}}{m}}a_{\\operatorname*{max}}\\sqrt{\\binom{d+k}{d}}\\binom{d+\\left\\vert\\xi\\right\\vert}{d}\\boldsymbol{p}^{\\left\\vert\\xi\\right\\vert}\\left(2R\\right)^{p}\\frac{1}{\\sqrt{n_{o}}}\\underset{i=1}{\\overset{n_{o}}{\\sum}}\\left\\vert s_{i}\\left(\\boldsymbol{w}\\left(t\\right),\\boldsymbol{v}\\left(t\\right)\\right)\\right\\vert}\\\\ &{\\leq\\!\\sqrt{\\frac{\\nu_{\\operatorname*{max}}}{m}}a_{\\operatorname*{max}}\\sqrt{\\binom{d+k}{d}}\\binom{d+\\left\\vert\\xi\\right\\vert}{d}\\boldsymbol{p}^{\\left\\vert\\xi\\right\\vert}\\left(2R\\right)^{p}\\left\\vert\\hat{\\boldsymbol{s}}\\right\\vert_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "and ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{j=1}^{n_{b}}\\left|h_{j}\\left(\\pmb{w}\\left(t\\right),\\pmb{v}\\left(t\\right)\\right)\\right|\\cdot\\left\\|\\frac{\\partial h_{j}\\left(\\pmb{w}\\left(t\\right),\\pmb{v}\\left(t\\right)\\right)}{\\partial\\left(\\pmb{v}_{\\ell}\\right)_{r}}\\right\\|_{2}}\\\\ {\\displaystyle\\leq\\sqrt{\\frac{\\nu_{\\operatorname*{max}}}{m}}\\left(d+1\\right)a_{\\operatorname*{max}}p\\left(2R\\right)^{p}\\frac{1}{\\sqrt{n_{b}}}\\sum_{j=1}^{n_{b}}\\left|h_{j}\\left(\\pmb{w}\\left(t\\right),\\pmb{v}\\left(t\\right)\\right)\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "equation", "text": "$$\n\\leq\\!\\sqrt{\\frac{\\nu_{\\operatorname*{max}}}{m}}\\left(d+1\\right)a_{\\operatorname*{max}}p\\left(2R\\right)^{p}\\left\\|h\\right\\|_{2}.\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "The third term is bounded by ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\displaystyle\\sum_{i=1}^{n_{o}}\\sum_{\\alpha\\in I_{\\xi_{\\ell-1}}}\\sum_{\\beta\\in I_{\\Delta\\xi_{\\ell}}}\\left\\|\\frac{\\partial\\left(\\mu_{\\ell}\\right)_{\\left(\\alpha,\\beta\\right),i}\\left(w\\left(t\\right),v\\left(t\\right)\\right)}{\\partial\\left(v\\ell\\right)_{r}}\\right\\|_{2}.\\left|\\left(\\mu_{\\ell}\\right)_{\\left(\\alpha,\\beta\\right),i}\\left(w\\left(t\\right),v\\left(t\\right)\\right)\\right|}\\\\ &{\\leq\\sqrt{\\frac{\\nu_{\\operatorname*{max}}}{m n_{o}}}\\sqrt{\\left(\\frac{d+k}{d}\\right){\\left(\\begin{array}{c}{d}\\\\ {d}\\end{array}\\right)}^{\\left(d+\\lvert\\xi\\rvert\\right)}}\\left(2R\\right)^{p}\\cdot\\displaystyle\\sum_{i=1}^{n_{o}}\\sum_{\\alpha\\in I_{\\ell-1}}\\sum_{\\beta\\in I_{\\Delta\\xi_{\\ell}}}\\left|\\left(\\mu_{\\ell}\\right)_{\\left(\\alpha,\\beta\\right),i}\\left(w\\left(t\\right),v\\left(t\\right)\\right)\\right|}\\\\ &{\\leq\\sqrt{\\frac{\\nu_{\\operatorname*{max}}}{m}}\\sqrt{{\\left(\\begin{array}{c}{d+k}\\\\ {d}\\end{array}\\right)}{\\left(\\begin{array}{c}{d+\\lvert\\xi\\rvert}\\\\ {d}\\end{array}\\right)}^{\\left(d+\\lvert\\xi\\rvert\\right)}}\\left(2R\\right)^{p}\\sqrt{\\lvert I_{\\xi\\ell-1}\\rvert\\cdot\\lvert I_{\\Delta\\xi_{\\ell}}\\rvert}\\cdot\\lVert\\mu_{\\ell}\\rVert_{2}}\\\\ &{\\leq\\sqrt{\\frac{\\nu_{\\operatorname*{max}}}{m}}{\\left(\\begin{array}{c}{d+k}\\\\ {d}\\end{array}\\right)}{\\left(\\begin{array}{c}{d+\\lvert\\xi\\rvert}\\\\ {d}\\end{array}\\right)}^{\\left(d+\\lvert\\xi\\rvert\\right)}\\left(2R\\right)^{p}\\lVert\\mu_{\\ell}\\rVert_{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "and the fourth term is ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n_{o}}\\sum_{\\alpha\\in I_{\\xi_{\\ell}}}\\sum_{\\beta\\in I_{\\Delta\\xi_{\\ell+1}}}\\left\\|\\frac{\\partial\\left(\\pmb{\\mu}_{\\ell+1}\\right)_{\\left(\\alpha,\\beta\\right),i}\\left(\\pmb{w}\\left(t\\right),\\pmb{v}\\left(t\\right)\\right)}{\\partial\\left(\\pmb{v}_{\\ell}\\right)_{r}}\\right\\|_{2}\\cdot\\left|\\left(\\pmb{\\mu}_{\\ell+1}\\right)_{\\left(\\alpha,\\beta\\right),i}\\left(\\pmb{w}\\left(t\\right),\\pmb{v}\\left(t\\right)\\right)\\right|\n$$", "text_format": "latex", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\le\\sqrt{\\frac{\\nu_{\\operatorname*{max}}}{m n_{o}}}p!\\xi|\\sqrt{\\left(\\stackrel{d+k}{d}\\right)\\left(\\stackrel{d+}{d}\\right)}\\left(2R\\right)^{p}\\cdot\\displaystyle\\sum_{\\alpha\\in I_{\\xi_{\\ell}}}\\sum_{\\beta\\in I_{\\Delta\\xi_{\\ell+1}}}\\left|\\left(\\pmb{\\mu}_{\\ell+1}\\right)_{\\left(\\alpha,\\beta\\right),i}\\left(\\pmb{w}\\left(t\\right),\\pmb{v}\\left(t\\right)\\right)\\right|}\\\\ &{\\le\\sqrt{\\frac{\\nu_{\\operatorname*{max}}}{m}}\\left(\\stackrel{d+k}{d}\\right)\\left(\\stackrel{d+}{d}\\right)p^{\\left|\\xi\\right|}\\left(2R\\right)^{p}\\left\\|\\pmb{\\mu}_{\\ell+1}\\right\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Therefore, for $t\\in(0,T)$ and $\\ell=\\{0,\\ldots,L\\}$ , ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\frac{d}{d t}\\left(u_{1}^{\\parallel},(u_{2}^{\\perp})\\right)\\right|_{2}}\\\\ &{\\le\\sqrt{\\frac{\\sin\\alpha_{0}}{\\sin\\alpha_{0}}}\\sqrt{\\left(\\frac{d+k}{d t}\\right)\\left(\\frac{d+k}{d t}\\right)}p^{(1)}(2H^{\\prime}||u_{2}^{\\perp})}\\\\ &{\\quad+\\frac{1}{1+\\left(\\nu_{1}^{\\perp}-\\nu_{0}^{\\perp}\\right)^{2}\\cos\\left(\\left(4+1\\right)u_{0}^{\\perp}\\right)/2}(2H^{\\prime}|R||_{2})}\\\\ &{\\quad+1_{\\{\\nu_{0}/\\nu}}\\sqrt{\\frac{\\sin\\alpha_{1}}{\\sin\\alpha_{1}}}\\left(2+k\\right)\\left(\\frac{d+k}{d t}\\right)\\left(2H^{\\prime}||R|_{2}\\right)}\\\\ &{\\quad+1_{\\{\\nu_{0}/\\nu}}\\sqrt{\\frac{\\sin\\alpha_{1}}{\\sin\\alpha_{1}}}\\left(\\frac{d+k}{d t}\\right)\\left(\\frac{d+k}{d t}\\right)^{2}\\left(2H^{\\prime}||R|_{2}\\right)}\\\\ &{\\quad+1_{\\{\\nu_{0}/\\nu}}\\sqrt{\\frac{\\sin\\alpha_{1}}{\\sin\\alpha_{1}}}\\left(\\frac{d+k}{d t}\\right)\\left(\\frac{d+k}{d t}\\right)p^{(1)}(2H^{\\prime}||u_{1}^{\\perp}|)}\\\\ &{\\quad+3\\sqrt{\\frac{\\sin\\alpha_{1}}{\\sin\\alpha_{1}}}\\sqrt{\\frac{\\sin\\alpha_{1}}{\\sin\\alpha_{1}}}\\left(\\frac{d+k}{d t}\\right)\\left(\\frac{d+k}{d t}\\right)p^{(1)}(2H^{\\prime}||u_{1}^{\\perp}|)}\\\\ &{\\quad+3\\sqrt{\\frac{\\sin\\alpha_{1}}{\\sin\\alpha_{1}}}\\sqrt{\\frac{\\sin\\alpha_{1}}{\\sin\\alpha_{1}}}\\left(\\frac{d+k}{d t}\\right)\\left(\\frac{d+k}{d t}\\right)p^{(1)}(2H^{\\prime}||u_{1}^{\\perp}|)\\left|\\frac{p_{1}^{\\alpha_{1}}(u_{1}^{\\perp})}{\\left|\\left|\\frac{d+k}{d t}\\right|\\left(|u_{1}^{\\perp}|,\\cos\\left(\\left(t\\right)\\right)\\right)}\\right|}\\\\ & \n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "As a consequence, we attain ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\left\\|(\\pmb{v}_{\\ell})_{r}\\left(T\\right)-(\\pmb{v}_{\\ell})_{r}\\left(0\\right)\\right\\|_{2}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\leq\\int_{0}^{T}\\left\\|\\frac{d}{d s}\\left(\\boldsymbol{v}_{\\ell}\\right)_{r}\\left(s\\right)\\right\\|_{2}\\,d s}&{(478)}\\\\ &{\\leq3\\sqrt{\\frac{\\nu_{\\operatorname*{max}}}{m}}\\sqrt{1+a_{\\operatorname*{max}}^{2}}\\binom{d+k}{d}\\binom{d+k}{d}p^{\\left\\vert\\ell\\right\\vert}(2R)^{p}\\int_{0}^{T}\\exp\\left(-\\frac{\\lambda_{0}}{2}s\\right)\\,d s\\left\\|\\left[\\frac{\\displaystyle\\left\\vert\\boldsymbol{s}\\left(\\boldsymbol{w}\\left(0\\right),\\boldsymbol{v}\\left(0\\right)\\right)}{\\displaystyle\\left\\vert\\boldsymbol{s}\\left(\\boldsymbol{w}\\left(0\\right),\\boldsymbol{v}\\left(0\\right)\\right)\\right\\vert}\\right]\\right\\vert}&{}\\\\ &{\\leq3\\sqrt{\\frac{\\nu_{\\operatorname*{max}}}{m}}\\sqrt{1+a_{\\operatorname*{max}}^{2}}\\binom{d+k}{d}\\binom{d+k}{d}p^{\\left\\vert\\ell\\right\\vert}p^{\\left\\vert\\ell\\right\\vert}(2R)^{p}\\int_{0}^{T}\\exp\\left(-\\frac{\\lambda_{0}}{2}s\\right)\\,d s\\left\\|\\left[\\frac{\\displaystyle\\left\\vert\\boldsymbol{s}\\left(\\boldsymbol{v}\\left(0\\right),\\boldsymbol{v}\\left(0\\right)\\right)}{\\displaystyle\\left\\vert\\boldsymbol{s}\\left(\\boldsymbol{w}\\left(0\\right),\\boldsymbol{v}\\left(0\\right)\\right)\\right\\vert}\\right]\\right\\vert}&{}\\\\ &{\\quad\\left\\vert\\boldsymbol{h}\\left(\\boldsymbol{w}\\left(0\\right),\\boldsymbol{v}\\left(0\\right)\\right)\\right\\vert}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "equation", "text": "$$\n\\leq\\!\\frac{6}{\\lambda_{0}}\\sqrt{1+a_{\\operatorname*{max}}^{2}}\\sqrt{\\frac{\\nu_{\\operatorname*{max}}}{m}}\\bigg(\\!\\!\\begin{array}{c}{\\!\\!d+k\\!\\bigg)\\bigg(\\!\\!d+|\\xi|\\bigg)p^{|\\xi|}\\left(2R\\right)^{p}\\left\\|\\left[\\mu_{1}\\left(w\\left(0\\right),v\\left(0\\right)\\right)\\right]\\right\\|}\\\\ {\\vdots}\\\\ {\\left[\\mu_{L}\\left(w\\left(0\\right),v\\left(0\\right)\\right)\\right]\\right\\|_{h}\\left[w\\left(0\\right),v\\left(0\\right)\\right]}\\end{array}\\right)\\!\\!\\!\\right)\\;.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Since $m$ is large enough to satisfy (395), we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\|(v_{\\ell})_{r}\\left(T\\right)-(v_{\\ell})_{r}\\left(0\\right)\\|_{2}\\leq\\frac{6\\sqrt{1+a_{\\operatorname*{max}}^{2}}\\sqrt{\\nu_{\\operatorname*{max}}}\\binom{d+k}{d}\\binom{d+|\\xi|}{d}p^{|\\xi|}2^{p}}{\\lambda_{0}}\\cdot\\frac{\\lambda_{0}}{C_{4}}=\\frac{1}{p}R_{w}<R_{w}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "C.1 Proof of Theorem 4.3 ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Proof. For $R\\in\\mathbb{R}$ , $r\\in[m]$ , and $q\\in[d]$ , each $(w_{\\ell})_{r,q}$ satisfies ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left[\\left|(\\pmb{\\omega}_{\\ell})_{r,q}\\right|>R\\right]<\\exp\\left(-\\frac{1}{2}R^{2}\\right).\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "If we set $\\begin{array}{r}{\\delta_{1}=\\left(L+1\\right)m d\\exp\\left(-\\frac{1}{2}R^{2}\\right)}\\end{array}$ , then $\\delta_{1}$ satisfies ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\left(1-\\exp\\left(-\\frac{1}{2}R^{2}\\right)\\right)^{(L+1)m d}\\geq1-\\left(L+1\\right)m d\\exp\\left(-\\frac{1}{2}R^{2}\\right)=1-\\delta_{1}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Hence, with probability of at least $1-\\delta_{1}$ , we have $\\left|(\\pmb{w}_{\\ell})_{r,q}\\right|\\leq R$ for all $\\ell=\\{0,1,\\ldots,L+1\\}$ , $r\\in[m]$ , and $q\\in[d]$ . ", "page_idx": 46}, {"type": "text", "text": "Provided $\\left|(\\pmb{w}_{\\ell})_{r,q}\\right|\\leq R$ for all $\\ell=\\{0,1,\\ldots,L+1\\}$ , if ", "page_idx": 46}, {"type": "equation", "text": "$$\nm\\ge\\frac{32N_{1}C_{1}^{2}R^{4p}}{\\lambda_{0}^{2}}\\log\\left(\\frac{2N_{1}}{\\delta_{2}}\\right),\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C_{1}=\\binom{d+k}{k}\\binom{d+\\lfloor\\xi\\rfloor}{\\lfloor\\xi\\rfloor}^{2}\\nu_{\\operatorname*{max}}^{2}\\left(\\frac{1}{n_{o}}+\\frac{1}{n_{b}}\\right)\\sqrt{1+a_{\\operatorname*{max}}^{4}}\\left(p^{2\\lfloor\\xi\\rfloor}+1\\right),}\\\\ &{N_{1}=7\\binom{d+k}{d}\\binom{d+\\lfloor\\xi\\rfloor}{d}^{2}n_{o}^{2}+4\\binom{d+\\lfloor\\xi\\rfloor}{d}n_{o}n_{b}+n_{b}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "then by Proposition C.4, with probability of at least $1-\\delta_{2}$ , we have ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\left\\|\\widehat{G}_{v}\\left(w\\left(0\\right),v\\left(0\\right)\\right)-\\widehat{G}_{v}^{\\infty}\\right\\|_{2}<\\frac{\\lambda_{0}}{4}.\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "On the other hand, with the condition $\\left|(\\pmb{w}_{\\ell})_{r,q}\\right|\\leq R$ for all $\\ell=\\{0,1,\\ldots,L+1\\}$ , $r\\in[m]$ , and $q\\in[d]$ , Proposition C.6 shows that if ", "page_idx": 46}, {"type": "equation", "text": "$$\nm\\geq\\frac{2C_{4}^{2}}{\\lambda_{0}^{2}}\\left(C_{2}\\left(4+\\binom{d+k}{d}\\binom{d+|\\xi|}{d}\\right)R^{4p}\\log\\left(\\frac{2N_{2}}{\\delta}\\right)+C_{3}R^{2p}\\right),\n$$", "text_format": "latex", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\delta_{3}\\leq2N_{2}\\exp\\left(-\\displaystyle{\\frac{C_{3}^{2}}{2C_{2}^{2}R^{2p}}}\\right),}}\\\\ {{R_{w}\\leq R,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{R_{w}=\\frac{\\lambda_{0}}{8\\sqrt{2}}\\left(\\nu_{\\mathrm{max}}\\sqrt{1+a_{\\mathrm{max}}^{4}}\\bigg(\\frac{d+k}{d}\\bigg)\\bigg(\\frac{d+|\\xi|}{d}\\bigg)^{2}p^{2|\\xi|+1}\\left(2R\\right)^{2p-1}\\right)^{-1},}\\\\ &{C_{2}=\\sqrt{\\nu_{\\mathrm{max}}}\\sqrt{(1+a_{\\mathrm{max}}^{2})}\\bigg(\\frac{d+k}{d}\\bigg)\\bigg(\\frac{d+|\\xi|}{d}\\bigg)\\left(p^{|\\xi|}+1\\right),}\\\\ &{C_{3}=\\frac{1}{n_{o}}\\displaystyle\\sum_{i=1}^{n_{o}}f\\left(x_{i}\\right)^{2}+\\frac{1}{n_{b}}\\displaystyle\\sum_{j=1}^{n_{b}}g\\left(\\bar{x}_{j}\\right)^{2},}\\\\ &{C_{4}=\\sqrt{\\nu_{\\mathrm{max}}}\\sqrt{1+a_{\\mathrm{max}}^{2}}\\bigg(\\frac{d+k}{d}\\bigg)\\bigg(\\frac{d+|\\xi|}{d}\\bigg)p^{|\\xi|+1}2p^{\\frac{1}{p}}\\frac{1}{R_{w}},}\\\\ &{N_{2}=\\bigg(\\frac{d+k}{d}\\bigg)\\bigg(\\frac{d+|\\xi|}{d}\\bigg)n_{o}+n_{b},}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "then, with the probability of at least $1-\\delta_{3}$ , we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{m}}\\left\\|\\left[\\begin{array}{c}{\\pmb{s}\\left(\\pmb{w}\\left(0\\right),\\pmb{v}\\left(0\\right)\\right)}\\\\ {\\pmb{\\mu}_{1}\\left(\\pmb{w}\\left(0\\right),\\pmb{v}\\left(0\\right)\\right)}\\\\ {\\vdots}\\\\ {\\pmb{\\mu}_{L}\\left(\\pmb{w}\\left(0\\right),\\pmb{v}\\left(0\\right)\\right)}\\\\ {\\pmb{h}\\left(\\pmb{w}\\left(0\\right),\\pmb{v}\\left(0\\right)\\right)}\\end{array}\\right]\\right\\|_{2}<\\frac{\\lambda_{0}}{C_{4}}\\pmb{R}^{-p}.\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Consequently, Proposition C.7 implies ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|(\\pmb{w}_{\\ell})_{r}\\left(t\\right)-\\left(\\pmb{w}_{\\ell}\\right)_{r}\\left(0\\right)\\|_{2}<R_{w},}\\\\ {\\|(\\pmb{v}_{\\ell})_{r}\\left(t\\right)-\\left(\\pmb{v}_{\\ell}\\right)_{r}\\left(0\\right)\\|_{2}<R_{w},}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "for all $t\\geq0$ . Then, we have ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\left\\|\\left[\\begin{array}{c}{s\\left(w\\left(t\\right),v\\left(t\\right)\\right)}\\\\ {\\mu_{1}\\left(w\\left(t\\right),v\\left(t\\right)\\right)}\\\\ {\\vdots}\\\\ {\\mu_{L}\\left(w\\left(t\\right),v\\left(t\\right)\\right)}\\\\ {h\\left(w\\left(t\\right),v\\left(t\\right)\\right)}\\end{array}\\right]\\right\\|_{2}\\le\\exp\\left(-\\frac{\\lambda_{0}}{2}t\\right)\\left\\|\\left[\\begin{array}{c}{s\\left(w\\left(0\\right),v\\left(0\\right)\\right)}\\\\ {\\mu_{1}\\left(w\\left(0\\right),v\\left(0\\right)\\right)}\\\\ {\\vdots}\\\\ {\\mu_{L}\\left(w\\left(0\\right),v\\left(0\\right)\\right)}\\\\ {h\\left(w\\left(0\\right),v\\left(0\\right)\\right)}\\end{array}\\right]\\right\\|_{2}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "from Lemam C.5. ", "page_idx": 47}, {"type": "text", "text": "Set $\\delta_{1}=\\delta_{2}=\\delta_{3}=\\textstyle{\\frac{1}{3}}\\delta$ and consider all inequality conditions. First, for $\\textstyle\\delta_{1}={\\frac{1}{3}}\\delta$ , ", "page_idx": 47}, {"type": "equation", "text": "$$\nR=\\sqrt{\\log\\frac{6\\left(L+1\\right)m d}{\\delta}}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "satisfies the condition (483). Then, (484) is satisfied if ", "page_idx": 47}, {"type": "equation", "text": "$$\nm\\geq\\frac{32N_{1}C_{1}^{2}}{\\lambda_{0}^{2}}\\left(\\log\\frac{6\\left(L+1\\right)m d}{\\delta}\\right)^{2p}\\log\\left(\\frac{6N_{1}}{\\delta}\\right).\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Similarly, if $m$ is large enough so that ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r}{m\\geq\\!\\frac{2C_{2}C_{4}^{2}}{\\lambda_{0}^{2}}\\left(4+\\left(\\!\\!\\begin{array}{c}{d+k}\\\\ {d}\\end{array}\\!\\!\\right)\\!\\left(\\!\\!\\begin{array}{c}{d+|\\xi|}\\\\ {d}\\end{array}\\!\\!\\right)\\right)\\left(\\log\\frac{6\\left(L+1\\right)m d}{\\delta}\\right)^{2p}\\log\\left(\\frac{2N_{2}}{\\delta}\\right)}\\\\ {+\\left.\\frac{2C_{3}C_{4}^{2}}{\\lambda_{0}^{2}}\\left(\\log\\frac{6\\left(L+1\\right)m d}{\\delta}\\right)^{p},\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "then (488) holds. ", "page_idx": 47}, {"type": "text", "text": "Therefore, there exsits ${C_{5}}={C_{5}}\\left({{a_{\\mathrm{{max}}}},{\\nu_{\\mathrm{{max}}}},{n_{o}},{n_{b}},{\\lambda_{0}}}\\right)$ such that if ", "page_idx": 48}, {"type": "equation", "text": "$$\nm\\geq C{\\binom{d+k}{d}}^{6}{\\binom{d+|\\xi|}{d}}^{8}p^{7|\\xi|+4}2^{6p}\\left(\\log{\\frac{m d}{\\delta}}\\right)^{4p}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "then with the probability of at least $\\left(1-\\delta_{1}\\right)\\left(1-\\delta_{2}\\right)\\left(1-\\delta_{3}\\right)\\geq1-\\left(\\delta_{1}+\\delta_{2}+\\delta_{3}\\right)=1-\\delta$ , we have ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\left\\|\\left[\\begin{array}{c}{s\\left(w\\left(t\\right),v\\left(t\\right)\\right)}\\\\ {\\mu_{1}\\left(w\\left(t\\right),v\\left(t\\right)\\right)}\\\\ {\\vdots}\\\\ {\\mu_{L}\\left(w\\left(t\\right),v\\left(t\\right)\\right)}\\\\ {h\\left(w\\left(t\\right),v\\left(t\\right)\\right)}\\end{array}\\right]\\right\\|_{2}\\le\\exp\\left(-\\frac{\\lambda_{0}}{2}t\\right)\\left\\|\\left[\\begin{array}{c}{s\\left(w\\left(0\\right),v\\left(0\\right)\\right)}\\\\ {\\mu_{1}\\left(w\\left(0\\right),v\\left(0\\right)\\right)}\\\\ {\\vdots}\\\\ {\\mu_{L}\\left(w\\left(0\\right),v\\left(0\\right)\\right)}\\\\ {h\\left(w\\left(0\\right),v\\left(0\\right)\\right)}\\end{array}\\right]\\right\\|_{2}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "D Experimental Details ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "D.1 Problem Formulations ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Poisson equation ", "text_level": 1, "page_idx": 48}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{u_{x x}+u_{y y}=f_{1}}&{x,y\\in\\left(0,\\pi\\right)}\\\\ {u\\left(x,0\\right)=u\\left(x,\\pi\\right)=u\\left(0,y\\right)=u\\left(\\pi,y\\right)=0}&{x,y\\in\\left[0,\\pi\\right],}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where $f_{1}:\\left(0,\\pi\\right)^{2}\\rightarrow\\mathbb{R}$ is defined so that the exact solution is given by ", "page_idx": 48}, {"type": "equation", "text": "$$\nu\\left(x,y\\right)=\\frac{1}{5}\\sin^{2}\\left(x\\right)y^{2}\\left(\\pi-y\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Bi-harmonic equation ", "text_level": 1, "page_idx": 48}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{u_{x x x x}+2u_{x x y y}+u_{y y y y}=f_{2}}&{x,y\\in\\left(0,\\pi\\right)}\\\\ {u\\left(x,0\\right)=u\\left(x,\\pi\\right)=u\\left(0,y\\right)=u\\left(\\pi,y\\right)=0}&{x,y\\in\\left[0,\\pi\\right]}\\\\ {\\frac{\\partial}{\\partial\\mathbf{n}}u\\left(x,0\\right)=\\frac{\\partial}{\\partial\\mathbf{n}}u\\left(x,\\pi\\right)=\\frac{\\partial}{\\partial\\mathbf{n}}u\\left(0,y\\right)=\\frac{\\partial}{\\partial\\mathbf{n}}u\\left(\\pi,y\\right)=0}&{x,y\\in\\left(0,\\pi\\right),}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where the flux function $f_{2}:\\left(0,\\pi\\right)^{2}\\rightarrow\\mathbb{R}$ is set to ensure that it has a solution that is identical to that of Poisson equation (507). ", "page_idx": 48}, {"type": "text", "text": "Heat equation ", "text_level": 1, "page_idx": 48}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{u_{t}=u_{x x}}&{x\\in\\left(-1,1\\right),\\;t\\in\\left(0,1\\right)}\\\\ {u\\left(t,-1\\right)=u\\left(t,1\\right)=0}&{t\\in\\left[0,1\\right]}\\\\ {u\\left(0,x\\right)=\\sin\\left(\\pi x\\right)}&{x\\in\\left(-1,1\\right),}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Elastic beam equation ", "text_level": 1, "page_idx": 48}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{u_{t}+u_{x x x x}=0}&{x\\in\\left(0,\\pi\\right),\\;t\\in\\left(0,1\\right)}\\\\ {u\\left(t,0\\right)=u\\left(t,\\pi\\right)=u_{x x}\\left(t,0\\right)=u_{x x}\\left(t,\\pi\\right)=0}&{t\\in\\left[0,1\\right]}\\\\ {u\\left(0,x\\right)=2\\sin\\left(x\\right)}&{x\\in\\left(0,\\pi\\right).}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "The exact solution is given by $u\\left(t,x\\right)=2e^{-t}\\sin\\left(x\\right)$ . ", "page_idx": 48}, {"type": "text", "text": "Convection-diffusion equation ", "text_level": 1, "page_idx": 48}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{u_{t}+u_{x}-\\frac{1}{4}u_{x x}=0}&{\\left(t,x\\right)\\in\\Omega=\\left(0,1\\right)\\times\\left(0,\\pi\\right),}\\\\ {u\\left(0,x\\right)=\\sin\\left(x\\right)}&{x\\in\\left[0,\\pi\\right],}\\\\ {u\\left(t,0\\right)=-e^{-\\frac{1}{4}t}\\sin\\left(t\\right)}&{t\\in\\left[0,1\\right],}\\\\ {u\\left(t,\\pi\\right)=e^{-\\frac{1}{4}t}\\sin\\left(\\pi-t\\right)}&{t\\in\\left[0,1\\right],}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "whose exact solution is $u\\left(t,x\\right)=e^{-\\frac{1}{4}t}\\sin\\left(x-t\\right)$ . ", "page_idx": 48}, {"type": "table", "img_path": "", "table_caption": ["Table 2: Experimental settings for each PDE "], "table_footnote": [], "page_idx": 49}, {"type": "image", "img_path": "8K6ul0hgtC/tmp/c24a2a79c1d46574ccd42e56cc6c0fc27d1cbf7a5a536a5132b7d1fbf843a98a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 49}, {"type": "image", "img_path": "", "img_caption": ["Figure 3: Loss of convection-diffusion equation trained by Adam. "], "img_footnote": [], "page_idx": 49}, {"type": "text", "text": "D.2 Parameter settings for experiments ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Convection-diffusion equation We conducted experiments on a convection-diffusion equation: ", "page_idx": 49}, {"type": "text", "text": "We train 100,000 epochs of PINNs with $p=3$ and VS-PINNs with $p=2$ , using the same settings as represented for the heat equation (509). Figures 4 and 3 show that VS-PINNs reach lower loss and achieve more stable convergence for both GD and Adam. ", "page_idx": 49}, {"type": "text", "text": "Bi-harmonic equation Consider the bi-harmonic equation ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\Delta^{2}u\\left(x,y\\right)=f\\left(x,y\\right)}&{x,y\\in\\left[0,\\pi\\right]}\\\\ {u\\left(x,0\\right)=u\\left(x,\\pi\\right)=u\\left(0,y\\right)=u\\left(\\pi,y\\right)=0}&{x,y\\in\\left[0,\\pi\\right]}\\\\ {\\frac{\\partial}{\\partial\\mathbf{n}}u\\left(x,0\\right)=\\frac{\\partial}{\\partial\\mathbf{n}}u\\left(x,\\pi\\right)=\\frac{\\partial}{\\partial\\mathbf{n}}u\\left(0,y\\right)=\\frac{\\partial}{\\partial\\mathbf{n}}u\\left(\\pi,y\\right)=0}&{x,y\\in\\left[0,\\pi\\right],}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where $f\\left(x,y\\right)$ are defined so that the exact solution is given by $\\begin{array}{r}{u\\left(x,y\\right)=\\frac{1}{5}\\sin^{2}\\left(x\\right)y^{2}\\left(\\pi-y\\right)^{2}}\\end{array}$ . We set $m=1{,}000$ , $n_{o}=10\\small{,}000$ , $n_{b}=400$ , and the training collocation points are fixed once they are randomly selected. We experiment VS-PINNs with two cases: (i) $\\bar{\\phi}_{0}\\approx u,\\,\\phi_{1}\\approx\\left(u_{x x},u_{y y}\\right)$ with $|\\xi|\\;=\\;2$ and $p~=~3$ and (ii) the finest splitting of $\\phi_{0}\\;\\approx\\;u,\\;\\phi_{1}\\;\\approx\\;\\nabla u,\\;\\phi_{2}\\;\\approx\\;(u_{x x},u_{y y}^{\\phantom{}\\displaystyle\\circ}),$ $\\phi_{3}\\approx\\left(u_{x x x},u_{x x y},u_{y y y}\\right)$ with $|\\xi|=1$ and $p=2$ . Regularization parameters are $\\nu_{1}=\\nu_{2}=\\nu_{3}=1$ for derivative matching loss $\\pmb{\\mu}$ and $\\nu=10$ for boundary loss $^h$ . Figure 5 depicts the training loss of PINN and two VS-PINNs. ", "page_idx": 49}, {"type": "text", "text": "E Additional Experimental Results ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "Computational efficiency of VS-PINNs Table 3 measures the GPU memory, running time, and the number of model parameters corresponding to experiments on elastic beam and bi-harmonic equations. Because VS-PINNs need as many networks as auxiliary variables, finer VS-PINN requires more parameters to be trained. However, reducing the differentiation order in the Loss significantly reduces memory consumption, despite the additional loss terms. ", "page_idx": 49}, {"type": "image", "img_path": "8K6ul0hgtC/tmp/8bdf83a138b778d70454e406f1d916a6b010496a6439b21ec23a164d2f653d88.jpg", "img_caption": ["Figure 4: Loss of solving convection-diffusion equation trained by GD. "], "img_footnote": [], "page_idx": 50}, {"type": "image", "img_path": "8K6ul0hgtC/tmp/03b76746bde404ebff7b05fdffad99c25959ac5227d58d57f4f74348958e5142.jpg", "img_caption": ["Figure 5: Loss comparison between PINNs with two VS-PINNs with $|\\xi|=1$ and 2. "], "img_footnote": [], "page_idx": 50}, {"type": "text", "text": "", "page_idx": 50}, {"type": "text", "text": "Error with true solution As the ultimate objective of PINNs is to approximate the solution to the PDE, we also quantified the discrepancy between the exact solution of the predicted solutions of the three models we tested. We tested five times for each PDE and splitting setup from different random seeds, and the averages of the Mean Squared Error (MSE) with the exact solution are reported in Table 4. We can observe that the results do not exhibit the same tendency as depicted in Figure 2. Since there is a discrepancy between optimization ability and the generalization error, it is not within the scope of this paper, and we believe that this is an important area for future research. ", "page_idx": 50}, {"type": "text", "text": "GD results for fourth-order PDEs In order to address fourth-order PDEs (elastic beam and bi-harmonic equations), PINNs should employ $p=5$ of the power of the ReLUp activation. When optimized with GD, this configuration results in a loss value of NaN, indicating that PINNs fail to train. Consequently, we present the results of our experiments with the Adam optimizer in Section 5. However, in contrast to PINNs, which exhibit difficulty in training, variable splitting demonstrates superior performance in terms of loss reduction for GD, as illustrated in Figure 6. This is attributed to the fact that variable splitting employs a lower derivative order and lower power $p$ . ", "page_idx": 50}, {"type": "text", "text": "F Broader Impacts ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "We analyzed the behavior of PINNs when applied to general $k$ th-order PDEs and provided theoretical understanding of the reasons why PINNs encounter difficulties in optimizing when dealing with high-order or high-dimensional problems. This paper has societal impact as it bridges artificial intelligence and physics, and hence has a wide range of potential applications. It could contribute to the advancement in scientific understanding, and foster scientific progress and innovation, beneftiing researchers in physics, mathematics, and artificial intelligence fields. By providing variable splitting which can address the convergence pathologies of PINNs, the paper suggests potential applications in industrial and technological fields, such as engineering, medicine, materials science, and environmental modeling, for complex system modeling, prediction, and optimization. Furthermore, as many problems have arisen in sciences and engineering tied with complex PDE systems, we expect that our work has the potential to be applicable in the enormous area such as climate forecasting, epidemics, molecular simulations, micro-mechanics, and modeling turbulent flows. Additionally, PDEs can be utilized in the development of military equipment. As with all numerical algorithms, however, it is not a work of developing a technique to go to warfare; rather, it is a tool for scientific inquiry. We encourage users of our model to focus on the positive impact of this work. ", "page_idx": 50}, {"type": "table", "img_path": "8K6ul0hgtC/tmp/73be8e2c6cbfbd65acfba7706ae996dff33beccfb6cc6331a264b172ff8b6420.jpg", "table_caption": ["Table 3: Computation costs with width $=\\!1000$ for experiments on the paper. "], "table_footnote": [], "page_idx": 51}, {"type": "table", "img_path": "8K6ul0hgtC/tmp/14a886de11ee4e0a233ff77ff9dfa1d527fb50b8a631dde6fd31250d9f90e582.jpg", "table_caption": ["Table 4: The average of Mean Square Error (MSE) with exact solution. "], "table_footnote": [], "page_idx": 51}, {"type": "text", "text": "", "page_idx": 51}, {"type": "image", "img_path": "8K6ul0hgtC/tmp/71da2cb0f3f796b055559f477b0850647872897faa502e34b9775498f8939369.jpg", "img_caption": ["Figure 6: Loss of VS-PINNs trained by gradient descent with variant learning rates. "], "img_footnote": [], "page_idx": 52}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 53}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 53}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: [Yes] ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 54}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 54}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 54}, {"type": "text", "text": "Justification: [Yes] ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 54}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 54}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Justification: [Yes] Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 55}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Justification: [Yes] Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 55}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Justification: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 55}, {"type": "text", "text": "", "page_idx": 56}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Justification: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 56}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Justification: [Yes] Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 56}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Justification: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to ", "page_idx": 56}, {"type": "text", "text": "generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 57}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 57}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 57}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 57}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 57}, {"type": "text", "text": "Justification: [NA] Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 57}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 58}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 58}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 58}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 58}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 58}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 58}]