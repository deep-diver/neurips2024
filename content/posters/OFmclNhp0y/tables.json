[{"figure_path": "OFmclNhp0y/tables/tables_7_1.jpg", "caption": "Table 1: Performance evaluation on the D4RL dataset. Normalized reward at 3M gradient steps and Area Under the Learning Curve (AULC) (mean\u00b1std) scores are averaged across four repetitions for the MuJoCo domain of the D4RL offline reinforcement learning dataset. The highest means are highlighted in bold and are underlined if they fall within one standard deviation of the best score. The average normalized score is the average across all tasks. The average ranking is based on the rank of the mean.", "description": "This table presents the performance of three offline reinforcement learning algorithms (MOPO, MOBILE, and MOMBO) on the D4RL dataset.  The performance is measured using two metrics: Normalized Reward (the total episode reward, normalized by random and expert policies) and Area Under the Learning Curve (AULC, measuring learning speed). The results are averaged across four repetitions for each of twelve tasks, with the best results highlighted.  The table also includes average scores and rankings across all tasks.", "section": "5 Experiments"}, {"figure_path": "OFmclNhp0y/tables/tables_8_1.jpg", "caption": "Table 1: Performance evaluation on the D4RL dataset. Normalized reward at 3M gradient steps and Area Under the Learning Curve (AULC) (mean\u00b1std) scores are averaged across four repetitions for the MuJoCo domain of the D4RL offline reinforcement learning dataset. The highest means are highlighted in bold and are underlined if they fall within one standard deviation of the best score. The average normalized score is the average across all tasks. The average ranking is based on the rank of the mean.", "description": "This table presents the performance comparison of three offline reinforcement learning algorithms (MOPO, MOBILE, and MOMBO) across various tasks from the D4RL dataset.  The results are evaluated using two metrics: Normalized Reward (the total episode reward normalized by the performance of random and expert policies) and Area Under the Learning Curve (AULC, which measures learning efficiency). The table shows the mean and standard deviation of these metrics across four repetitions of each experiment.  Higher scores indicate better performance.  The table also provides average scores and ranks to summarize the overall performance of the algorithms.", "section": "5 Experiments"}, {"figure_path": "OFmclNhp0y/tables/tables_22_1.jpg", "caption": "Table 1: Performance evaluation on the D4RL dataset. Normalized reward at 3M gradient steps and Area Under the Learning Curve (AULC) (mean\u00b1std) scores are averaged across four repetitions for the MuJoCo domain of the D4RL offline reinforcement learning dataset. The highest means are highlighted in bold and are underlined if they fall within one standard deviation of the best score. The average normalized score is the average across all tasks. The average ranking is based on the rank of the mean.", "description": "This table presents the performance comparison of three offline reinforcement learning algorithms (MOPO, MOBILE, and MOMBO) on the D4RL dataset.  The algorithms are evaluated across 12 tasks using two metrics: Normalized Reward (the total episode reward normalized by random and expert policy performance) and Area Under the Learning Curve (AULC, measuring the average reward over the course of training).  The table shows mean and standard deviation scores averaged across four runs for each task and overall.  High scores are better and bold/underlined scores are within one standard deviation of the best.", "section": "5 Experiments"}, {"figure_path": "OFmclNhp0y/tables/tables_23_1.jpg", "caption": "Table 1: Performance evaluation on the D4RL dataset. Normalized reward at 3M gradient steps and Area Under the Learning Curve (AULC) (mean\u00b1std) scores are averaged across four repetitions for the MuJoCo domain of the D4RL offline reinforcement learning dataset. The highest means are highlighted in bold and are underlined if they fall within one standard deviation of the best score. The average normalized score is the average across all tasks. The average ranking is based on the rank of the mean.", "description": "This table presents the results of the performance evaluation on the D4RL dataset for three MuJoCo environments (halfcheetah, hopper, walker2d) and four levels of expertise (random, medium, medium-replay, and medium-expert).  It shows the normalized reward (higher is better) and the area under the learning curve (AULC) (higher is better) for three algorithms: MOPO, MOBILE, and MOMBO.  The average performance across all tasks, and the average ranking based on mean performance are also included.", "section": "5 Experiments"}, {"figure_path": "OFmclNhp0y/tables/tables_25_1.jpg", "caption": "Table 1: Performance evaluation on the D4RL dataset. Normalized reward at 3M gradient steps and Area Under the Learning Curve (AULC) (mean\u00b1std) scores are averaged across four repetitions for the MuJoCo domain of the D4RL offline reinforcement learning dataset. The highest means are highlighted in bold and are underlined if they fall within one standard deviation of the best score. The average normalized score is the average across all tasks. The average ranking is based on the rank of the mean.", "description": "This table presents the performance comparison of three offline reinforcement learning algorithms (MOPO, MOBILE, and MOMBO) across various D4RL benchmark tasks.  The evaluation metrics are the normalized reward (averaged over 3 million gradient steps and four repetitions) and the Area Under the Learning Curve (AULC).  Higher values indicate better performance.  The table also provides the average normalized reward and average ranking of the algorithms across all tasks.", "section": "5 Experiments"}]