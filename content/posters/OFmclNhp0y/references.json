{"references": [{"fullname_first_author": "Jin", "paper_title": "Is pessimism provably efficient for offline RL?", "publication_date": "2021-00-00", "reason": "This paper provides the theoretical foundation for pessimistic value iteration, a core concept in the paper's approach to offline reinforcement learning."}, {"fullname_first_author": "Yu", "paper_title": "Model-based offline policy optimization", "publication_date": "2020-00-00", "reason": "This paper introduces MOPO, a key model-based offline reinforcement learning algorithm that the paper builds upon and improves."}, {"fullname_first_author": "Sun", "paper_title": "Model-Bellman inconsistency for model-based offline reinforcement learning", "publication_date": "2023-00-00", "reason": "This paper introduces MOBILE, another significant model-based offline reinforcement learning algorithm used as a comparison in the paper's experiments."}, {"fullname_first_author": "Fujimoto", "paper_title": "Addressing function approximation error in actor-critic methods", "publication_date": "2018-00-00", "reason": "This paper addresses function approximation errors in actor-critic methods, a challenge that the current paper also tackles in the context of offline RL."}, {"fullname_first_author": "Kumar", "paper_title": "Conservative Q-learning for offline reinforcement learning", "publication_date": "2020-00-00", "reason": "This paper introduces Conservative Q-learning, a method to address distributional shift in offline RL, which is a key issue that the current paper seeks to improve."}]}