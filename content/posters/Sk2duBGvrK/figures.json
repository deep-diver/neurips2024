[{"figure_path": "Sk2duBGvrK/figures/figures_3_1.jpg", "caption": "Figure 1: Linearity scores of diffusion denoisers. Solid and dashed lines depict the linearity scores across noise variances for models in the generalization and memorization regimes, respectively, where \u03b1 = \u03b2 = 1/\u221a2.", "description": "This figure shows how the linearity of diffusion denoisers changes with different training dataset sizes.  The linearity score is calculated using cosine similarity, measuring how close the denoiser's output is to a linear combination of its inputs.  The solid lines represent models that have generalized well to unseen data, showing increased linearity as the dataset size increases. The dashed lines show models in the memorization phase where they primarily replicate the training data, exhibiting lower linearity. The x-axis represents the noise variance and the y-axis represents the linearity score.  The plot demonstrates a clear trend: models trained on larger datasets exhibit greater linearity and hence better generalization capabilities.", "section": "3 Hidden Linear and Gaussian Structures in Diffusion Models"}, {"figure_path": "Sk2duBGvrK/figures/figures_4_1.jpg", "caption": "Figure 2: Score field approximation error and sampling Trajectory. The left and right figures demonstrate the score field approximation error and the sampling trajectories D(xt; \u03c3(t) of actual diffusion model (EDM), Multi-Delta model, linear model and Gaussian model respectively. Notice that the curve corresponding to the Gaussian model almost overlaps with that of the linear model, suggesting they share similar funciton mappings.", "description": "This figure shows a comparison of score functions and sampling trajectories for four different models: the actual diffusion model (EDM), a multi-delta model, a linear model, and a Gaussian model. The left panel plots the root mean square error (RMSE) between the score functions of each model and the actual diffusion model across different noise levels.  The right panel displays the sampling trajectories, showing how each model generates samples starting from random noise. The close overlap between the linear and Gaussian model curves in the left panel indicates that they generate similar samples.", "section": "Hidden Linear and Gaussian Structures in Diffusion Models"}, {"figure_path": "Sk2duBGvrK/figures/figures_5_1.jpg", "caption": "Figure 4: Linear model shares similar function mapping with Gaussian model. The left figure shows the difference between the linear weights and the Gaussian weights w.r.t. 100 training epochs of the linear distillation process for the 10 discrete noise levels. The right figure shows the correlation matrices between the first 100 singular vectors of the linear weights and Gaussian weights.", "description": "This figure shows two plots. The left plot shows how the weights of the linear model (trained to approximate diffusion denoisers) evolve over 100 training epochs and how they become increasingly similar to the weights of the Gaussian model (optimal denoiser for Gaussian data). The right plot visualizes the correlation between the principal components of the linear model and the Gaussian model for different noise levels. High correlation suggests that linear models effectively capture the Gaussian structure of the data.", "section": "Hidden Linear and Gaussian Structures in Diffusion Models"}, {"figure_path": "Sk2duBGvrK/figures/figures_5_2.jpg", "caption": "Figure 2: Score field approximation error and sampling Trajectory. The left and right figures demonstrate the score field approximation error and the sampling trajectories D(xt; \u03c3(t)) of actual diffusion model (EDM), Multi-Delta model, linear model and Gaussian model respectively. Notice that the curve corresponding to the Gaussian model almost overlaps with that of the linear model, suggesting they share similar funciton mappings.", "description": "This figure compares the score approximation error and sampling trajectories of four different generative models: the actual diffusion model (EDM), a Multi-Delta model, a linear model, and a Gaussian model.  The left panel shows that the linear model and the Gaussian model closely approximate the score function of the actual EDM, especially in intermediate noise levels. The right panel visually demonstrates the similarity between the sampling trajectories of the linear and Gaussian models, further reinforcing their close approximation to the actual diffusion model's behavior.", "section": "Hidden Linear and Gaussian Structures in Diffusion Models"}, {"figure_path": "Sk2duBGvrK/figures/figures_6_1.jpg", "caption": "Figure 2: Score field approximation error and sampling Trajectory. The left and right figures demonstrate the score field approximation error and the sampling trajectories D(xt; \u03c3(t) of actual diffusion model (EDM), Multi-Delta model, linear model and Gaussian model respectively. Notice that the curve corresponding to the Gaussian model almost overlaps with that of the linear model, suggesting they share similar funciton mappings.", "description": "This figure shows a comparison of score field approximation errors and sampling trajectories for four different models: the actual diffusion model (EDM), a Multi-Delta model, a linear model, and a Gaussian model.  The left panel displays the root mean square error (RMSE) between the score functions of each model and the actual diffusion model across various noise levels. The right panel illustrates the sampling trajectories\u2014the sequence of images generated during the denoising process\u2014for each model.  The results indicate that the linear model's score function closely approximates that of the Gaussian model, and both are relatively close to the actual diffusion model's score function, especially at intermediate noise levels. This suggests that the linear and Gaussian models effectively capture the essential aspects of the actual diffusion model's behavior.", "section": "Hidden Linear and Gaussian Structures in Diffusion Models"}, {"figure_path": "Sk2duBGvrK/figures/figures_7_1.jpg", "caption": "Figure 6: Diffusion models learn the Gaussian structure when training dataset is large. Models with a fixed scale (channel size 128) are trained across various dataset sizes. The left and right figures show the score difference and the generated images respectively. \u201cNN\u201d denotes the nearest neighbor in the training dataset to the images generated by the diffusion models.", "description": "This figure shows the results of an experiment where diffusion models with a fixed capacity (128 channels) were trained on datasets of varying sizes (68, 137, 1094, 8750, 35000, 70000 images).  The left panel plots the root mean square error (RMSE) between the score function of the trained model and the score function of a multivariate Gaussian distribution fitted to the training data, across different noise levels. The right panel shows generated images from the trained models, along with the nearest neighbor from the training set, and the images generated by the Gaussian model. The figure demonstrates that as the training dataset size increases, the RMSE decreases, indicating that the model learns the Gaussian structure of the data.  This is further corroborated by the images which shows that as the training dataset increases, images generated by the diffusion model start to resemble those generated by the Gaussian model, rather than simple copies of the training data.", "section": "Conditions for the Emergence of Gaussian Structures and Generalizability"}, {"figure_path": "Sk2duBGvrK/figures/figures_8_1.jpg", "caption": "Figure 6: Diffusion models learn the Gaussian structure when training dataset is large. Models with a fixed scale (channel size 128) are trained across various dataset sizes. The left and right figures show the score difference and the generated images respectively. \"NN\" denotes the nearest neighbor in the training dataset to the images generated by the diffusion models.", "description": "This figure shows that as the training dataset size increases, the score approximation error between the diffusion denoisers and the optimal Gaussian denoisers decreases, especially in the intermediate noise variance regime.  The generated images transition from being simple replications of the training data to novel images that exhibit Gaussian structure. The results suggest that the Gaussian structure of the training data plays a critical role in the generalization capabilities of diffusion models.", "section": "Conditions for the Emergence of Gaussian Structures and Generalizability"}, {"figure_path": "Sk2duBGvrK/figures/figures_8_2.jpg", "caption": "Figure 2: Score field approximation error and sampling Trajectory. The left and right figures demonstrate the score field approximation error and the sampling trajectories D(xt; \u03c3(t) of actual diffusion model (EDM), Multi-Delta model, linear model and Gaussian model respectively. Notice that the curve corresponding to the Gaussian model almost overlaps with that of the linear model, suggesting they share similar funciton mappings.", "description": "This figure displays two subfigures. The left subfigure shows the score approximation error for four different models: EDM (Energy-based Diffusion Model), Multi-Delta, Linear, and Gaussian. The x-axis represents the noise variance, and the y-axis represents the Root Mean Square Error (RMSE).  The plot highlights the close agreement between the linear and Gaussian model approximation errors, particularly in the mid-range noise variances. The right subfigure presents the sampling trajectories for each of the four models, demonstrating similar function mappings between the Linear and Gaussian models. These findings suggest that the function mappings of the effective generalized diffusion models can be well approximated by linear models that capture the Gaussian structures of the dataset.", "section": "Hidden Linear and Gaussian Structures in Diffusion Models"}, {"figure_path": "Sk2duBGvrK/figures/figures_9_1.jpg", "caption": "Figure 9: Diffusion models in the strong generalization regime generate similar images as the Gaussian models. Figure(a) Top: Generated images of Gaussian models; Bottom: Generated images of diffusion models, with model scale 128; S1 and S2 each has 35000 non-overlapping images. Figure(b) Top: Generated images of Gaussian model; Bottom: Generated images of diffusion models in the memorization regime, with model scale 128; S1 and S2 each has 1094 non-overlapping images. Figure(c): Early stopping and reducing model capacity help transition diffusion models from memorization to generalization.", "description": "This figure demonstrates the strong generalization phenomenon observed in diffusion models.  The top row shows images generated by Gaussian models trained on two separate, non-overlapping datasets (S1 and S2). The bottom row shows images generated by diffusion models trained on the same datasets.  (a) shows that when trained on large datasets (35000 images), diffusion models generate images very similar to the Gaussian models. (b) shows that when trained on small datasets (1094 images), the diffusion model's images are closer to those in the training set, demonstrating memorization. (c) Illustrates how early stopping or reducing model capacity can allow the diffusion model to transition from memorization to generalization, producing images more similar to the Gaussian models.", "section": "Connection between Strong Generalizability and Gaussian Structure"}, {"figure_path": "Sk2duBGvrK/figures/figures_15_1.jpg", "caption": "Figure 10: Linearity scores for varying \u03b1 and \u03b2. The diffusion models are trained with the edm-ve configuration [4], which ensures the models are in the generalization regime.", "description": "This figure shows the linearity scores for different values of \u03b1 and \u03b2 across different noise levels.  The linearity score measures how well the diffusion denoisers satisfy additivity and homogeneity properties, indicating how close the denoisers are to linear functions.  The x-axis represents noise variance, while the y-axis represents the cosine similarity between the actual output of the denoiser and the expected output based on linear behavior.  The different lines correspond to different values of \u03b1 and \u03b2 (used to calculate the input for the linearity score). The EDM-VE configuration is used to train the models, ensuring they operate in the generalization regime.", "section": "3 Hidden Linear and Gaussian Structures in Diffusion Models"}, {"figure_path": "Sk2duBGvrK/figures/figures_16_1.jpg", "caption": "Figure 11: Linearity scores and sampling trajectory. The left and right figures demonstrate the linearity scores and the sampling trajectories D(xt; \u03c3(t)) of actual diffusion model (EDM-VE and Baseline-VE), Multi Delta model, linear model, and Gaussian model respectively.", "description": "This figure shows a comparison of linearity scores and sampling trajectories for different models.  The left panel displays the linearity scores across different noise levels for EDM-VE (a well-trained diffusion model), Baseline-VE (a less well-trained diffusion model), a linear model, and a Gaussian model.  The right panel illustrates the generation trajectories of these same models, showing how they progressively denoise a random noise vector into a clean image.  This helps to visualize how linearity affects the models' image generation process.", "section": "Hidden Linear and Gaussian Structures in Diffusion Models"}, {"figure_path": "Sk2duBGvrK/figures/figures_16_2.jpg", "caption": "Figure 12: Memorization and generalization regimes of diffusion models. Figures(a) to (c) show the images generated by diffusion models trained on 70000, 4375, 1094 FFHQ images and their corresponding nearest neighbors in the training dataset respectively. Figures(d) to (f) show the images generated by diffusion models trained on 50000, 12500, 782 CIFAR-10 images and their corresponding nearest neighbors in the training dataset respectively. Notice that when the training dataset size is small, diffusion model can only generate images in the training dataset.", "description": "This figure shows how the generalization ability of diffusion models changes as the training dataset size increases.  With small datasets, the models reproduce training images (memorization). As the dataset size increases, the models generate novel images beyond the training set (generalization). This is demonstrated using both FFHQ and CIFAR-10 datasets.", "section": "Generalization vs. Memorization of Diffusion Models"}, {"figure_path": "Sk2duBGvrK/figures/figures_17_1.jpg", "caption": "Figure 1: Linearity scores of diffusion denoisers. Solid and dashed lines depict the linearity scores across noise variances for models in the generalization and memorization regimes, respectively, where \u03b1 = \u03b2 = 1/\u221a2.", "description": "This figure shows the linearity scores of diffusion denoisers across different noise levels for models trained in both the generalization and memorization regimes.  The linearity score measures how closely the denoiser's output approximates a linear function of the input. The solid lines represent models in the generalization regime (trained on larger datasets), and the dashed lines represent models in the memorization regime (trained on smaller datasets).  The x-axis represents noise variance, and the y-axis represents the linearity score.  The plot demonstrates that as the training dataset size increases (moving from memorization to generalization), the linearity of the diffusion denoiser increases across all noise variance levels.", "section": "3 Hidden Linear and Gaussian Structures in Diffusion Models"}, {"figure_path": "Sk2duBGvrK/figures/figures_18_1.jpg", "caption": "Figure 1: Linearity scores of diffusion denoisers. Solid and dashed lines depict the linearity scores across noise variances for models in the generalization and memorization regimes, respectively, where \u03b1 = \u03b2 = 1/\u221a2.", "description": "This figure shows the linearity scores of diffusion denoisers across different noise levels for models trained in both generalization and memorization regimes. The linearity score measures how close the denoisers are to being linear functions.  The solid lines represent models in the generalization regime (producing novel images), while the dashed lines represent models in the memorization regime (primarily reproducing training data).  The plot demonstrates that as models transition from memorization to generalization, their linearity increases.", "section": "Hidden Linear and Gaussian Structures in Diffusion Models"}, {"figure_path": "Sk2duBGvrK/figures/figures_19_1.jpg", "caption": "Figure 15: Difference between D<sub>\u03b8</sub>(x; \u03c3(t)) and x for various noise variance levels. Figures(a) and (c) show the differences between D<sub>\u03b8</sub>(x; \u03c3(t)) and x across \u03c3(t) \u2208 [0.002, 80], measured by normalized MSE and cosine similarity, respectively. Figures(b) and (d) provide zoomed-in views of (a) and (c). The diffusion models were trained on the FFHQ dataset. Notice that the difference between D<sub>\u03b8</sub>(x; \u03c3(t)) and x quickly converges to near zero in the low noise variance regime. The trend is consistent for various model architectures.", "description": "This figure shows the difference between the output of the diffusion models and the input for various noise levels.  The difference is measured using normalized MSE and cosine similarity. The results show that the difference quickly approaches zero in the low-noise variance regime, regardless of model architecture.", "section": "Hidden Linear and Gaussian Structures in Diffusion Models"}, {"figure_path": "Sk2duBGvrK/figures/figures_22_1.jpg", "caption": "Figure 2: Score field approximation error and sampling Trajectory. The left and right figures demonstrate the score field approximation error and the sampling trajectories D(xt; \u03c3(t) of actual diffusion model (EDM), Multi-Delta model, linear model and Gaussian model respectively. Notice that the curve corresponding to the Gaussian model almost overlaps with that of the linear model, suggesting they share similar funciton mappings.", "description": "This figure shows a comparison of score approximation errors and sampling trajectories for four different models: the actual diffusion model (EDM), the multi-delta model, a linear model, and a Gaussian model.  The left panel displays the root mean squared error (RMSE) between the score functions of each model and the actual diffusion model across various noise levels. The right panel shows the sampling trajectories of the different models, which are visualizations of how the models gradually transform random noise into an image. Notably, the Gaussian model's curve closely matches the linear model's curve, indicating the similarity between their function mappings.", "section": "Hidden Linear and Gaussian Structures in Diffusion Models"}, {"figure_path": "Sk2duBGvrK/figures/figures_23_1.jpg", "caption": "Figure 2: Score field approximation error and sampling Trajectory. The left and right figures demonstrate the score field approximation error and the sampling trajectories D(xt; \u03c3(t)) of actual diffusion model (EDM), Multi-Delta model, linear model and Gaussian model respectively. Notice that the curve corresponding to the Gaussian model almost overlaps with that of the linear model, suggesting they share similar funciton mappings.", "description": "This figure shows a comparison of score approximation errors and sampling trajectories for different models. The left plot displays the root mean square error (RMSE) between the score functions of various models (actual diffusion model, multi-delta model, linear model, and Gaussian model) and the actual diffusion model's score function, across various noise levels.  The right plot shows the image generation trajectories of the four models, starting from the same random noise. The close overlap between the curves for the linear and Gaussian models indicates that these two models have very similar function mappings, suggesting the learned score functions have a hidden Gaussian structure.", "section": "Hidden Linear and Gaussian Structures in Diffusion Models"}, {"figure_path": "Sk2duBGvrK/figures/figures_23_2.jpg", "caption": "Figure 2: Score field approximation error and sampling Trajectory. The left and right figures demonstrate the score field approximation error and the sampling trajectories D(xt; \u03c3(t)) of actual diffusion model (EDM), Multi-Delta model, linear model and Gaussian model respectively. Notice that the curve corresponding to the Gaussian model almost overlaps with that of the linear model, suggesting they share similar funciton mappings.", "description": "This figure shows a comparison of the score approximation error and sampling trajectories for four different models: the actual diffusion model (EDM), a multi-delta model, a linear model, and a Gaussian model.  The left panel displays the root mean squared error (RMSE) between the score functions of each model and the actual diffusion model's score function across different noise levels. The right panel shows the sampling trajectories \u2013 sequences of images generated during the denoising process \u2013 for each model. The key observation is that the Gaussian model closely matches the linear model, indicating that the linear model captures essential aspects of the diffusion process.", "section": "Hidden Linear and Gaussian Structures in Diffusion Models"}, {"figure_path": "Sk2duBGvrK/figures/figures_24_1.jpg", "caption": "Figure 6: Diffusion models learn the Gaussian structure when training dataset is large. Models with a fixed scale (channel size 128) are trained across various dataset sizes. The left and right figures show the score difference and the generated images respectively. \"NN\" denotes the nearest neighbor in the training dataset to the images generated by the diffusion models.", "description": "This figure shows the relationship between dataset size and the tendency for diffusion models to learn Gaussian structures. The left graph shows that as dataset size increases, the error between the score function of the actual diffusion model and the optimal Gaussian score decreases, indicating that larger datasets lead diffusion models to lean towards learning Gaussian-like distributions.  The right side of the figure displays generated images from different models (actual diffusion model, Gaussian model, nearest neighbor model) and training dataset sizes, illustrating that larger datasets lead to generated images which closely resemble those generated by a Gaussian model, demonstrating better generalization. ", "section": "Conditions for the Emergence of Gaussian Structures and Generalizability"}, {"figure_path": "Sk2duBGvrK/figures/figures_24_2.jpg", "caption": "Figure 7: Diffusion model learns the Gaussian structure when model scale is small. Models with different scales are trained on a fixed training dataset of 1094 images. The left and right figures show the score difference and the generated images respectively.", "description": "This figure shows the effect of model capacity on the learning of Gaussian structures in diffusion models.  It demonstrates that when the model capacity (measured by the number of channels) is relatively small compared to the size of the training dataset, diffusion models exhibit an inductive bias towards learning Gaussian structures, leading to better generalization. The left panel shows the RMSE between the score functions of the diffusion model and the Gaussian model, while the right panel illustrates the generated images for models with different scales. This inductive bias is less pronounced when the model has larger capacity, leading to memorization of the training data.", "section": "Conditions for the Emergence of Gaussian Structures and Generalizability"}, {"figure_path": "Sk2duBGvrK/figures/figures_25_1.jpg", "caption": "Figure 6: Diffusion models learn the Gaussian structure when training dataset is large. Models with a fixed scale (channel size 128) are trained across various dataset sizes. The left and right figures show the score difference and the generated images respectively. \"NN\" denotes the nearest neighbor in the training dataset to the images generated by the diffusion models.", "description": "This figure shows the relationship between training dataset size and the inductive bias of diffusion models towards learning Gaussian structures.  The left graph shows that as the dataset size increases, the score approximation error between diffusion denoisers and Gaussian denoisers decreases, especially in the intermediate noise variance regime. This is visually confirmed by the right-hand side of the figure, which demonstrates that as dataset size increases, the generated images increasingly resemble those produced by Gaussian denoisers, moving from memorization of the training data to generalization. The \"NN\" (nearest neighbor) images show that smaller datasets result in images very similar to the training set, while larger datasets produce more novel, realistic images.", "section": "Conditions for the Emergence of Gaussian Structures and Generalizability"}, {"figure_path": "Sk2duBGvrK/figures/figures_25_2.jpg", "caption": "Figure 22: Linear model shares similar function mapping with Gaussian model. The figures demonstrate the evolution of normalized MSE between the linear weights Dl and the Gaussian weights DG w.r.t. linear distillation training epochs. Figures (a), (b) and (c) correspond to diffusion models trained on FFHQ, with EDM-VE, EDM-ADM and EDM-VP network architectures specified in [4] respectively.", "description": "This figure shows the results of training linear models to approximate the non-linear diffusion denoisers from diffusion models.  The x-axis represents the training epochs, and the y-axis shows the normalized mean squared error (NMSE) between the weights of the linear model and the weights of the Gaussian model (the optimal linear model).  Three different diffusion model architectures are shown: EDM-VE, EDM-ADM, and EDM-VP. The figure demonstrates that across different diffusion model architectures, the linear models converge towards the optimal Gaussian model, supporting the finding that diffusion models exhibit a bias towards learning Gaussian structures.", "section": "Hidden Linear and Gaussian Structures in Diffusion Models"}, {"figure_path": "Sk2duBGvrK/figures/figures_26_1.jpg", "caption": "Figure 23: Images sampled from various models. The figure shows the sampled images from diffusion models with different network architectures and those from their corresponding Gaussian models.", "description": "This figure compares the generated images from different diffusion models with varying network architectures (EDM-VE, EDM-VP, and EDM-ADM) and their corresponding Gaussian models.  The comparison shows the similarities and differences between the images generated by the diffusion models and the images generated by approximating the data distribution with a multivariate Gaussian distribution.  This highlights the inductive bias of diffusion models towards capturing and utilizing the Gaussian structure of the training dataset for image generation.", "section": "G Additional Experiment Results"}, {"figure_path": "Sk2duBGvrK/figures/figures_27_1.jpg", "caption": "Figure 2: Score field approximation error and sampling Trajectory. The left and right figures demonstrate the score field approximation error and the sampling trajectories D(xt; \u03c3(t)) of actual diffusion model (EDM), Multi-Delta model, linear model and Gaussian model respectively. Notice that the curve corresponding to the Gaussian model almost overlaps with that of the linear model, suggesting they share similar funciton mappings.", "description": "This figure presents a comparison of score field approximation error and sampling trajectories across four different models: the actual diffusion model (EDM), a Multi-Delta model, a linear model, and a Gaussian model.  The left panel shows the root mean squared error (RMSE) between the score functions of each model and the actual diffusion model at various noise levels. The right panel displays the sampling trajectories, illustrating the evolution of generated samples as each model progresses through the reverse diffusion process. A key observation is that the Gaussian model's trajectory closely resembles that of the linear model, supporting the idea that the Gaussian structure is a significant factor in the functionality of diffusion models.", "section": "Hidden Linear and Gaussian Structures in Diffusion Models"}, {"figure_path": "Sk2duBGvrK/figures/figures_28_1.jpg", "caption": "Figure 2: Score field approximation error and sampling Trajectory. The left and right figures demonstrate the score field approximation error and the sampling trajectories D(xt; \u03c3(t)) of actual diffusion model (EDM), Multi-Delta model, linear model and Gaussian model respectively. Notice that the curve corresponding to the Gaussian model almost overlaps with that of the linear model, suggesting they share similar funciton mappings.", "description": "This figure shows a comparison of score approximation errors and sampling trajectories for four different models: the actual diffusion model (EDM), a Multi-Delta model, a linear model, and a Gaussian model.  The left panel plots the root mean squared error (RMSE) between the score functions of each model and the actual diffusion model, as a function of noise variance. The right panel shows image generation trajectories (i.e., the sequence of images generated during the denoising process) for each model.  The results highlight a strong similarity between the linear model and Gaussian model, indicating that the learned score functions of diffusion models in the generalization regime are close to being optimal for a multivariate Gaussian distribution.", "section": "Hidden Linear and Gaussian Structures in Diffusion Models"}, {"figure_path": "Sk2duBGvrK/figures/figures_29_1.jpg", "caption": "Figure 9: Diffusion models in the strong generalization regime generate similar images as the Gaussian models. Figure(a) Top: Generated images of Gaussian models; Bottom: Generated images of diffusion models, with model scale 128; S1 and S2 each has 35000 non-overlapping images. Figure(b) Top: Generated images of Gaussian model; Bottom: Generated images of diffusion models in the memorization regime, with model scale 128; S1 and S2 each has 1094 non-overlapping images. Figure(c): Early stopping and reducing model capacity help transition diffusion models from memorization to generalization.", "description": "This figure demonstrates the strong generalization phenomenon observed in diffusion models.  The top row shows images generated by Gaussian models trained on two non-overlapping datasets (S1 and S2) of different sizes (35,000 and 1,094 images). The bottom row shows corresponding images from diffusion models trained on the same datasets and model scale 128. In (a), with larger datasets, the diffusion models generalize well producing similar images to the Gaussian models. In (b), with smaller datasets, the diffusion models only reproduce images from the training dataset (memorization), showcasing the difference. (c) provides a strategy to mitigate the memorization effect by using early stopping or reducing the model capacity.", "section": "Connection between Strong Generalizability and Gaussian Structure"}, {"figure_path": "Sk2duBGvrK/figures/figures_29_2.jpg", "caption": "Figure 2: Score field approximation error and sampling Trajectory. The left and right figures demonstrate the score field approximation error and the sampling trajectories D(xt; \u03c3(t)) of actual diffusion model (EDM), Multi-Delta model, linear model and Gaussian model respectively. Notice that the curve corresponding to the Gaussian model almost overlaps with that of the linear model, suggesting they share similar funciton mappings.", "description": "This figure shows a comparison of score approximation errors and sampling trajectories for four different models: the actual diffusion model (EDM), a Multi-Delta model, a linear model, and a Gaussian model. The left panel displays the root mean squared error (RMSE) between the score functions of each model and the actual diffusion model across different noise variance levels. The right panel illustrates the sampling trajectories of the four models, starting from random Gaussian noise and progressing through denoising steps until a final image is generated.  The close overlap between the linear and Gaussian model curves suggests similar function mappings.", "section": "Hidden Linear and Gaussian Structures in Diffusion Models"}, {"figure_path": "Sk2duBGvrK/figures/figures_31_1.jpg", "caption": "Figure 1: Linearity scores of diffusion denoisers. Solid and dashed lines depict the linearity scores across noise variances for models in the generalization and memorization regimes, respectively, where \u03b1 = \u03b2 = 1/\u221a2.", "description": "This figure shows the linearity scores of diffusion denoisers across different noise levels for models trained in both the generalization and memorization regimes. The linearity score measures how close the denoiser's behavior is to a linear function. The results indicate that diffusion models in the generalization regime exhibit a greater degree of linearity, suggesting that linearity is a key aspect of their generalizability.", "section": "3 Hidden Linear and Gaussian Structures in Diffusion Models"}, {"figure_path": "Sk2duBGvrK/figures/figures_32_1.jpg", "caption": "Figure 1: Linearity scores of diffusion denoisers. Solid and dashed lines depict the linearity scores across noise variances for models in the generalization and memorization regimes, respectively, where \u03b1 = \u03b2 = 1/\u221a2.", "description": "The figure shows linearity scores of diffusion denoisers across different noise levels for models trained in both generalization and memorization regimes. The linearity score measures how close a denoiser's output is to a linear combination of its inputs.  The solid lines represent models in the generalization regime, showing increasing linearity as dataset size increases, and the dashed lines represent models in the memorization regime, exhibiting lower linearity. The parameter \u03b1=\u03b2=1/\u221a2 controls the weighting of two inputs when calculating the linearity score.", "section": "3 Hidden Linear and Gaussian Structures in Diffusion Models"}, {"figure_path": "Sk2duBGvrK/figures/figures_32_2.jpg", "caption": "Figure 30: Effects of perturbing xt along Jacobian singular vectors. Figure(a)-(c) demonstrate the effects of perturbing input xt along the first singular vector of the Jacobian matrix (xt \u00b1 \u03b4u1(xt)) on the final generated images. Perturbing xt in high-noise regime (Figure (a)) leads to canonical image changes while perturbation in intermediate-noise regime (Figure (b)) leads to change in details but the overall image structure is preserved. At very low noise variances, perturbation has no significant effect (Figure (c)). Similar effects are observed in concurrent work [46].", "description": "This figure shows the effects of perturbing the input image along the first singular vector of the Jacobian matrix at different noise levels.  In the high-noise regime, this leads to major changes in the generated image (e.g., changing the class of the image). In the intermediate-noise regime, there are changes to details, but the overall structure remains similar. Finally, in the low-noise regime, the perturbation has little to no effect on the output image.", "section": "H.2 GAHB Emerge only in Intermediate-Noise Regime"}]