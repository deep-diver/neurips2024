[{"type": "text", "text": "Understanding Generalizability of Diffusion Models Requires Rethinking the Hidden Gaussian Structure ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xiang Li1, Yixiang Dai1, Qing $\\mathbf{Qu^{1}}$ 1Department of EECS, University of Michigan, forkobe@umich.edu, yixiang@umich.edu, qingqu@umich.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this work, we study the generalizability of diffusion models by looking into the hidden properties of the learned score functions, which are essentially a series of deep denoisers trained on various noise levels. We observe that as diffusion models transition from memorization to generalization, their corresponding nonlinear diffusion denoisers exhibit increasing linearity. This discovery leads us to investigate the linear counterparts of the nonlinear diffusion models, which are a series of linear models trained to match the function mappings of the nonlinear diffusion denoisers. Surprisingly, these linear denoisers are approximately the optimal denoisers for a multivariate Gaussian distribution characterized by the empirical mean and covariance of the training dataset. This finding implies that diffusion models have the inductive bias towards capturing and utilizing the Gaussian structure (covariance information) of the training dataset for data generation. We empirically demonstrate that this inductive bias is a unique property of diffusion models in the generalization regime, which becomes increasingly evident when the model\u2019s capacity is relatively small compared to the training dataset size. In the case that the model is highly overparameterized, this inductive bias emerges during the initial training phases before the model fully memorizes its training data. Our study provides crucial insights into understanding the notable strong generalization phenomenon recently observed in real-world diffusion models. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In recent years, diffusion models [1\u20134] have become one of the leading generative models, powering the state-of-the-art image generation systems such as Stable Diffusion [5]. To understand the empirical success of diffusion models, several works [6\u201312] have focused on their sampling behavior, showing that the data distribution can be effectively estimated in the reverse sampling process, assuming that the score function is learned accurately. Meanwhile, other works [13\u201318] investigate the learning of score functions, showing that effective approximation can be achieved with score matching loss under certain assumptions. However, these theoretical insights, grounded in simplified assumptions about data distribution and neural network architectures, do not fully capture the complex dynamics of diffusion models in practical scenarios. One significant discrepancy between theory and practice is that real-world diffusion models are trained only on a finite number of data points. As argued in [19], theoretically a perfectly learned score function over the empirical data distribution can only replicate the training data. In contrast, diffusion models trained on finite samples exhibit remarkable generalizability, producing high-quality images that significantly differ from the training examples. Therefore, a good understanding of the remarkable generative power of diffusion models is still lacking. ", "page_idx": 0}, {"type": "text", "text": "In this work, we aim to deepen the understanding of generalizability in diffusion models by analyzing the inherent properties of the learned score functions. Essentially, the score functions can be interpreted as a series of deep denoisers trained on various noise levels. These denoisers are then chained together to progressively denoise a randomly sampled Gaussian noise into its corresponding clean image, thus, understanding the function mappings of these diffusion denoisers is critical to demystify the working mechanism of diffusion models. Motivated by the linearity observed in the diffusion denoisers of effectively generalized diffusion models, we propose to elucidate their function mappings with a linear distillation approach, where the resulting linear models serve as the linear approximations of their nonlinear counterparts. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Contributions of this work: Our key findings can be highlighted as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Inductive bias towards Gaussian structures (Section 3). Diffusion models in the generalization regime exhibit an inductive bias towards learning diffusion denoisers that are close (but not equal) to the optimal denoisers for a multivariate Gaussian distribution, defined by the empirical mean and covariance of the training data. This implies the diffusion models have the inductive bias towards capturing the Gaussian structure (covariance information) of the training data for image generation. \u2022 Model Capacity and Training Duration (Section 4) We show that this inductive bias is most pronounced when the model capacity is relatively small compared to the size of the training data. However, even if the model is highly overparameterized, such inductive bias still emerges during early training phases, before the model memorizes its training data. This implies that early stopping can prompt generalization in overparameterized diffusion models. \u2022 Connection between Strong Generalization and Gaussian Structure (Section 5). Lastly, we argue that the recently observed strong generalization [20] results from diffusion models learning certain common low-dimensional structural features shared across non-overlapping datasets. We show that such low-dimensional features can be partially explained through the Gaussian structure. ", "page_idx": 1}, {"type": "text", "text": "Relationship with Prior Arts. Recent research [20\u201324] demonstrates that diffusion models operate in two distinct regimes: $(i)$ a memorization regime, where models primarily reproduce training samples and $(i i)$ a generalization regime, where models generate high-quality, novel images that extend beyond the training data. In the generalization regime, a particularly intriguing phenomenon is that diffusion models trained on non-overlapping datasets can generate nearly identical samples [20]. While prior work [20] attributes this \u201dstrong generalization\u201d effect to the structural inductive bias inherent in diffusion models leading to the optimal denoising basis (geometry-adaptive harmonic basis), our research advances this understanding by demonstrating diffusion models\u2019 inductive bias towards capturing the Gaussian structure of the training data. Our findings also corroborate with observations of earlier study [25] that the learned score functions of well-trained diffusion models closely align with the optimal score functions of a multivariate Gaussian approximation of the training data. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminary ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Basics of Diffusion Models. Given a data distribution $p_{\\mathrm{data}}(x)$ , where $\\pmb{x}\\in\\mathbb{R}^{d}$ , diffusion models [1\u20134] define a series of intermediate states $p(\\mathbf{\\boldsymbol{x}};\\sigma(t))$ by adding Gaussian noise sampled from $\\mathcal{N}(\\mathbf{0},\\sigma(t)^{2}I)$ to the data, where $\\sigma(t)$ is a predefined schedule that specifies the noise level at time $t\\in[0,T]$ , such that at the end stage the noise mollified distribution $\\bar{p}(x;\\sigma(T))$ is indistinguishable from the pure Gaussian distribution. Subsequently, a new sample is generated by progressively denoising a random noise $\\pmb{x}_{T}\\sim\\mathcal{N}(\\mathbf{0},\\sigma(T)^{2}\\bar{\\pmb{I}})$ to its corresponding clean image $\\pmb{x}_{0}$ . ", "page_idx": 1}, {"type": "text", "text": "Following [4], this forward and backward diffusion process can be expressed with a probabilistic ODE: ", "page_idx": 1}, {"type": "equation", "text": "$$\nd\\pmb{x}=-\\dot{\\sigma}(t)\\sigma(t)\\nabla_{\\pmb{x}}\\log p(\\pmb{x};\\sigma(t))d t.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "In practice the score function $\\nabla_{x}\\log{p(x;\\sigma(t))}$ can be approximated by ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\nabla_{x}\\log{p(\\mathbf{{x}};\\sigma(t))}=(\\mathcal{D}_{\\theta}(\\mathbf{{x}};\\sigma(t))-\\mathbf{{x}}(t))/\\sigma^{2},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $D_{\\theta}(\\mathbf{\\boldsymbol{x}};\\sigma(t))$ is parameterized by a deep network with parameters $\\pmb{\\theta}$ trained with the denoising score matching objective: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\mathbb{E}_{{\\pmb x}\\sim p_{\\mathrm{data}}}\\mathbb{E}_{{\\pmb\\epsilon}\\sim{\\mathcal{N}}(\\mathbf{0},\\sigma(t)^{2}{\\cal I})}\\left[\\|\\mathscr{D}_{\\pmb\\theta}({\\pmb x}+{\\boldsymbol\\epsilon};\\sigma(t))-{\\pmb x}\\|_{2}^{2}\\right].\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "In the discrete setting, the reverse ODE in (1) takes the following form: ", "page_idx": 1}, {"type": "equation", "text": "$$\n{\\pmb x}_{i+1}\\gets(1-(t_{i}-t_{i+1})\\frac{\\dot{\\sigma}(t_{i})}{\\sigma(t_{i})}){\\pmb x}_{i}+(t_{i}-t_{i+1})\\frac{\\dot{\\sigma}(t_{i})}{\\sigma(t_{i})}\\mathcal{D}_{\\pmb\\theta}({\\pmb x}_{i};\\sigma(t_{i})),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\pmb{x}_{0}\\sim\\mathcal{N}(\\pmb{0},\\sigma^{2}(t_{0})\\pmb{I})$ . Notice that at each iteration $i$ , the intermediate sample $x_{i+1}$ is the sum of the scaled $\\pmb{x}_{i}$ and the denoising output $\\mathcal{D}_{\\theta}(\\mathbf{\\boldsymbol{x}}_{i};\\sigma(t_{i}))$ . Obviously, the final sampled image is largely determined by the denoiser $D_{\\theta}(\\mathbf{\\boldsymbol{x}};\\sigma(t))$ . If we can understand the function mapping of these diffusion denoisers, we can demystify the working mechanism of diffusion models. ", "page_idx": 2}, {"type": "text", "text": "Optimal Diffusion Denoisers under Simplified Data Assumptions. Under certain assumptions on the data distribution $p_{\\mathrm{data}}(x)$ , the optimal diffusion denoisers $\\mathcal{D}_{\\theta}(\\mathbf{\\boldsymbol{x}};\\sigma(t))$ that minimize the score matching objective (3) can be derived analytically in closed-forms as we discuss below. ", "page_idx": 2}, {"type": "text", "text": "\u2022 Multi-delta distribution of the training data. Suppose the training dataset contains a finite number of data points $\\{y_{1},y_{2},...,y_{N}\\}$ , a natural way to model the data distribution is to represent it as a multi-delta distribution: $\\begin{array}{r}{p({\\pmb x})\\stackrel{}{=}\\frac{1}{N}\\sum_{i=1}^{N}\\delta({\\pmb x}-{\\pmb y}_{i})}\\end{array}$ . In this case, the optimal denoiser is ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{D}_{\\mathrm{M}}(\\pmb{x};\\pmb{\\sigma}(t))=\\frac{\\sum_{i=1}^{N}\\mathcal{N}(\\pmb{x};\\pmb{y}_{i},\\pmb{\\sigma}(t)^{2}\\pmb{I})\\pmb{y}_{i}}{\\sum_{i=1}^{N}\\mathcal{N}(\\pmb{x};\\pmb{y}_{i},\\pmb{\\sigma}(t)^{2}\\pmb{I})},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "which is essentially a softmax-weighted combination of the finite data points. As proved in [24, 17], such diffusion denoisers $D_{\\mathrm{M}}({\\boldsymbol{x}};\\bar{\\sigma}(t))$ can only generate exact replicas of the training samples, therefore they have no generalizability. ", "page_idx": 2}, {"type": "text", "text": "\u2022 Multivariate Gaussian distribution. Recent work [25] suggests modeling the data distribution $p_{\\mathrm{data}}(x)$ as a multivariate Gaussian distribution $p(\\pmb{x})=\\mathcal{N}\\bar{(}\\pmb{\\dot{\\mu}},\\pmb{\\Sigma})$ , where the mean $\\pmb{\\mu}$ and the covariance $\\Sigma$ are approximated by the empirical mean $\\begin{array}{r}{\\pmb{\\mu}=\\frac{1}{N}\\sum_{i=1}^{N}y_{i}}\\end{array}$ and the empirical covariance $\\begin{array}{r}{\\pmb{\\Sigma}=\\frac{1}{N}\\sum_{i=1}^{N}(\\pmb{y}_{i}-\\pmb{\\mu})(\\pmb{y}_{i}-\\pmb{\\mu})^{T}}\\end{array}$ of the training dataset. In this case, the optimal denoiser is: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{D}_{\\mathrm{G}}(\\pmb{x};\\sigma(t))=\\pmb{\\mu}+\\pmb{U}\\tilde{\\Lambda}_{\\sigma(t)}\\pmb{U}^{T}(\\pmb{x}-\\pmb{\\mu}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\pmb{\\Sigma}=\\pmb{U}\\pmb{\\Lambda}\\pmb{U}^{T}$ is the SVD of the empirical covariance matrix, with singular values $\\pmb{\\Lambda}=$ diag (\u03bb1, \u00b7 \u00b7 \u00b7 , \u03bbd) and \u02dc\u039b\u03c3(t) = diag \u03bb1+\u03bb\u03c31(t)2 \u00b7 \u00b7 \u00b7\u03bb . With this linear Gaussian denoiser, as proved in [25], the sampling trajectory of the probabilistic ODE (1) has close form: ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\pmb x}_{t}={\\pmb\\mu}+\\sum_{i=1}^{d}\\sqrt{\\frac{\\sigma(t)^{2}+\\lambda_{i}}{\\sigma(T)^{2}+\\lambda_{i}}}{\\pmb u}_{i}^{T}({\\pmb x}_{T}-{\\pmb\\mu}){\\pmb u}_{i},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\pmb{u}_{i}$ is the $i^{\\mathrm{th}}$ singular vector of the empirical covariance matrix. While [25] demonstrate that the Gaussian scores approximate learned scores at high noise variances, we show that they are nearly the best linear approximations of learned scores across a much wider range of noise variances. ", "page_idx": 2}, {"type": "text", "text": "Generalization vs. Memorization of Diffusion Models. As the training dataset size increases, diffusion models transition from the memorization regime\u2014where they can only replicate its training images\u2014to the generalization regime, where the they produce high-quality, novel images [17]. While memorization can be interpreted as an overfitting of diffusion models to the training samples, the mechanisms underlying the generalization regime remain less well-understood. This study aims to explore and elucidate the inductive bias that enables effective generalization in diffusion models. ", "page_idx": 2}, {"type": "text", "text": "3 Hidden Linear and Gaussian Structures in Diffusion Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we study the intrinsic structures of the learned score functions of diffusion models in the generalization regime. Through various experiments and theoretical investigation, we show that ", "page_idx": 2}, {"type": "text", "text": "Diffusion models in the generalization regime have inductive bias towards learning the Gaussian structures of the dataset. ", "page_idx": 2}, {"type": "text", "text": "Based on the linearity observed in diffusion denoisers trained in the generalization regime, we propose to investigate their intrinsic properties through a linear distillation technique, with which we train a series of linear models to approximate the nonlinear diffusion denoisers (Section 3.1). Interestingly, these linear models closely resemble the optimal denoisers for a multivariate Gaussian distribution characterized by the empirical mean and covariance of the training dataset (Section 3.2). This implies diffusion models have the inductive bias towards learning the Gaussian structure of the training dataset. We theoretically show that the observed Gaussian structure is the optimal solution to the denoising score matching objective under the constraint that the model is linear (Section 3.3). In the subsequent sections, although we mainly demonstrate our results using the FFHQ datasets, our findings are robust and extend to various architectures and datasets, as detailed in Appendix G. ", "page_idx": 2}, {"type": "text", "text": "3.1 Diffusion Models Exhibit Linearity in the Generalization Regime ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our study is motivated by the emerging linearity observed in diffusion models in the generalization regime. Specifically, we quantify the linearity of diffusion denoisers at various noise level $\\sigma(t)$ by jointly assessing their \u201dAdditivity\u201d and \u201dHomogeneity\u201d with a linearity score (LS) defined by the cosine similarity between $\\mathcal{D}_{\\pmb{\\theta}}(\\alpha\\pmb{x}_{1}+\\beta\\pmb{x}_{2};\\pmb{\\sigma}(t))$ and $\\alpha\\mathcal{D}_{\\pmb{\\theta}}(\\pmb{x}_{1};\\pmb{\\sigma}(t))+\\beta\\mathcal{D}_{\\pmb{\\theta}}(\\pmb{x}_{2};\\pmb{\\sigma}(t))$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\boldsymbol{S}(t)\\;=\\;\\mathbb{E}_{\\mathbf{x}_{1},\\mathbf{x}_{2}\\sim p(\\mathbf{x};\\sigma(t))}\\,\\left[\\left|\\left\\langle\\frac{\\mathcal{D}_{\\theta}(\\alpha\\mathbf{x}_{1}+\\beta\\mathbf{x}_{2};\\sigma(t))}{\\|\\mathcal{D}_{\\theta}(\\alpha\\mathbf{x}_{1}+\\beta\\mathbf{x}_{2};\\sigma(t))\\|_{2}},\\frac{\\alpha\\mathcal{D}_{\\theta}(\\mathbf{x}_{1};\\sigma(t))+\\beta\\mathcal{D}_{\\theta}(\\mathbf{x}_{2};\\sigma(t))}{\\|\\alpha\\mathcal{D}_{\\theta}(\\mathbf{x}_{1};\\sigma(t))+\\beta\\mathcal{D}_{\\theta}(\\mathbf{x}_{2};\\sigma(t))\\|_{2}}\\right\\rangle\\right|\\right],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{\\boldsymbol{x}}_{1},\\mathbf{\\boldsymbol{x}}_{2}\\,\\sim\\,p(\\mathbf{\\boldsymbol{x}};\\sigma(t))$ , and $\\alpha\\,\\in\\,\\mathbb{R}$ and $\\beta\\,\\in\\,\\mathbb{R}$ are scalars. In practice, the expectation is approximated with its empirical mean over 100 samples. A more detailed discussion on this choice of measuring linearity is deferred to Appendix A. ", "page_idx": 3}, {"type": "text", "text": "Following the EDM training configuration [4], we set the noise levels $\\sigma(t)$ within the continuous range [0.002,80]. As shown in Figure 1, as diffusion models transition from the memorization regime to the generalization regime (increasing the training dataset size), the corresponding diffusion denoisers $\\mathcal{D}_{\\theta}$ exhibit increasing linearity. This phenomenon persists across diverse datasets1 as well as various training configurations2; see Appendix B for more details. This emerging linearity motivates us to ask the following questions: ", "page_idx": 3}, {"type": "text", "text": "\u2022 To what extent can a diffusion model be approximated by a linear model? \u2022 If diffusion models can be approximated linearly, what are the underlying characteristics of this linear approximation? ", "page_idx": 3}, {"type": "image", "img_path": "Sk2duBGvrK/tmp/1c6e9ad6fd5231b4f484e9cdd128a9f1d81cbdfaf3b06bbdd3797df6485e323a.jpg", "img_caption": ["Linearity across Noise Levels (EDM-VE, FFHQ) ", "Figure 1: Linearity scores of diffusion denoisers. Solid and dashed lines depict the linearity scores across noise variances for models in the generalization and memor\u221aization regimes, respectively, where $\\alpha=\\beta=1/\\sqrt{2}$ . "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Investigating the Linear Structures via Linear Distillation. To address these questions, we investigate the hidden linear structure of diffusion denoisers through linear distillation. Specifically, for a given diffusion denoiser $D_{\\theta}(\\mathbf{\\boldsymbol{x}};\\sigma(t))$ at noise level $\\sigma(t)$ , we approximate it with a linear function (with a bias term) such that: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{D}_{\\mathrm{L}}(\\pmb{x};\\sigma(t)):=W_{\\sigma(t)}\\pmb{x}+b_{\\sigma(t)}\\;\\approx\\;\\mathcal{D}_{\\pmb{\\theta}}(\\pmb{x};\\sigma(t)),\\;\\forall\\pmb{x}\\sim p(\\pmb{x};\\sigma(t)),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the weight $W_{\\sigma(t)}\\in\\mathbb{R}^{d\\times d}$ and bias $\\pmb{b}_{\\sigma(t)}\\in\\mathbb{R}^{d}$ are learned by solving the following optimization problem with gradient descent:3 ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{W_{\\sigma(t)},b_{\\sigma(t)}}\\mathbb{E}_{\\mathbf{x}\\sim p_{\\mathrm{dua}}(\\mathbf{x})}\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(\\mathbf{0},\\sigma(t)^{2}I)}||W_{\\sigma(t)}(\\mathbf{x}+\\epsilon)+b_{\\sigma(t)}-\\mathcal{D}_{\\theta}(\\mathbf{x}+\\epsilon;\\sigma(t))||_{2}^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "If these linear models effectively approximate the nonlinear diffusion denoisers, analyzing their weights can elucidate the generation mechanism. ", "page_idx": 3}, {"type": "text", "text": "While diffusion models are trained on continuous noise variance levels within [0.002,80], we examine the 10 discrete sampling steps specified by the EDM schedule [4]: [80.0, 42.415, 21.108, 9.723, 4.06, 1.501, 0.469, 0.116, 0.020, 0.002] . These steps are considered sufficient for studying the diffusion mappings for two reasons: $(i)$ images generated using these 10 steps closely match those generated with more steps, and $(i i)$ recent research [30] demonstrates that the diffusion denoisers trained on similar noise variances exhibit analogous function mappings, implying that denoiser behavior at discrete variances represents their behavior at nearby variances. ", "page_idx": 3}, {"type": "image", "img_path": "Sk2duBGvrK/tmp/e9e8e06cede2f1151eea561a20919d2169a29fcd861b1659aef0a31097dd2cc5.jpg", "img_caption": ["Figure 2: Score field approximation error and sampling Trajectory. The left and right figures demonstrate the score field approximation error and the sampling trajectories $\\mathcal{D}(\\mathbf{\\boldsymbol{x}}_{t};\\sigma(\\bar{t})$ of actual diffusion model (EDM), Multi-Delta model, linear model and Gaussian model respectively. Notice that the curve corresponding to the Gaussian model almost overlaps with that of the linear model, suggesting they share similar funciton mappings. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "After obtaining the linear models $\\mathcal{D}_{\\mathrm{L}}$ , we evaluate their differences with the actual nonlinear denoisers $\\mathcal{D}_{\\theta}$ with the score field approximation error, calculated using the expectation over the root mean square error (RMSE): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{Score-Difference}(t):=\\mathbb{E}_{\\mathbf{x}\\sim p_{\\mathrm{dat}}(\\mathbf{x}),\\epsilon\\sim\\mathcal{N}(\\mathbf{0};\\sigma(t)^{2}I)}\\sqrt{\\frac{\\|\\mathcal{D}_{\\mathrm{L}}(\\mathbf{x}+\\epsilon;\\sigma(t))-\\mathcal{D}_{\\theta}(\\mathbf{x}+\\epsilon;\\sigma(t))\\|_{2}^{2}}{d}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $d$ represents the data dimension and the expectation is approximated with its empirical mean. While we present RMSE-based results in the main text, our findings remain consistent across alternative metrics, including NMSE, as detailed in Appendix G. ", "page_idx": 4}, {"type": "text", "text": "We perform linear distillation on well trained diffusion models operating in the generalization regime. For comprehensive analysis, we also compute the score approximation error between $\\mathcal{D}_{\\theta}$ and: $(i)$ the optimal denoisers for the multi-delta distribution $\\mathcal{D}_{\\mathrm{M}}$ defined as (5), and $(i i)$ the optimal denoisers for the multivariate Gaussian distribution $\\mathcal{D}_{\\mathrm{G}}$ defined as (6). As shown in Figure 2, our analysis reveals three distinct regimes: ", "page_idx": 4}, {"type": "text", "text": "\u2022 High-noise regime $I20,\\!80J$ . In this regime, only coarse image structures are generated (Figure 2(right)). Quantitatively, as shown in Figure 2(left), the distilled linear model $\\mathcal{D}_{\\mathrm{L}}$ closely approximates its nonlinear counterpart $\\mathcal{D}_{\\theta}$ with RMSE below 0.05. Both Gaussian score $\\mathcal{D}_{\\mathrm{G}}$ and multi-delta score $\\mathcal{D}_{\\mathrm{M}}$ also achieve comparable approximation accuracy. \u2022 Low-noise regime [0.002,0.1]. In this regime, only subtle, imperceptible details are added to the generated images. Here, both $\\mathcal{D}_{\\mathrm{L}}$ and $\\mathcal{D}_{\\mathrm{G}}$ effectively approximate $\\mathcal{D}_{\\theta}$ with RMSE below 0.05. \u2022 Intermediate-noise regime [0.1,20]: This crucial regime, where realistic image content is primarily generated, exhibits significant nonlinearity. While $\\mathcal{D}_{\\mathrm{M}}$ exhibits high approximation error due to rapid convergence to training samples\u2014a memorization effect theoretically proved in [24], both $\\mathcal{D}_{\\mathrm{L}}$ and $\\mathcal{D}_{\\mathrm{G}}$ maintain relatively lower approximation errors. ", "page_idx": 4}, {"type": "text", "text": "Qualitatively, as shown in Figure 2(right), despite the relatively high score approximation error in the intermediate noise regime, the images generated with $\\mathcal{D}_{\\mathrm{L}}$ closely resemble those generated with $\\mathcal{D}_{\\theta}$ in terms of the overall image structure and certain amount of fine details. This implies $(i)$ the underlying linear structure within the nonlinear diffusion models plays a pivotal role in their generalization capabilities and $(i i)$ such linear structure is effectively captured by our distilled linear models. In the next section, we will explore this linear structure by examining the linear models $\\mathcal{D}_{\\mathrm{L}}$ . ", "page_idx": 4}, {"type": "text", "text": "3.2 Inductive Bias towards Learning the Gaussian Structures ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Notably, the Gaussian denoisers $\\mathcal{D}_{\\mathrm{G}}$ exhibit behavior strikingly similar to the linear denoisers $\\mathcal{D}_{\\mathrm{L}}$ . As illustrated in Figure 2(left), they achieve nearly identical score approximation errors, particularly in the critical intermediate variance region. Furthermore, their sampling trajectories are remarkably similar (Figure 2(right)), producing nearly identical generated images that closely match those from the actual diffusion denoisers (Figure 3). These observations suggest that $\\mathcal{D}_{\\mathrm{L}}$ and $\\mathcal{D}_{\\mathrm{G}}$ share similar function mappings across various noise levels, leading us to hypothesize that the intrinsic linear structure underlying diffusion models corresponds to the Gaussian structure of the training data\u2014specifically, its empirical mean and covariance. We validate this hypothesis by empirically showing that $\\mathcal{D}_{\\mathrm{L}}$ is close to $\\mathcal{D}_{\\mathrm{G}}$ through the following three complementary experiments: ", "page_idx": 4}, {"type": "image", "img_path": "Sk2duBGvrK/tmp/2df740f4d9f6d9cda64aa9b5d7ad21d9fe42076a9f1635c177c002795b80d302.jpg", "img_caption": ["Figure 4: Linear model shares similar function mapping with Gaussian model. The left figure shows the difference between the linear weights and the Gaussian weights w.r.t. 100 training epochs of the linear distillation process for the 10 discrete noise levels. The right figure shows the correlation matrices between the first 100 singular vectors of the linear weights and Gaussian weights. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "\u2022 Similarity in weight matrices. As illustrated in Figure 4(left), $W_{\\sigma(t)}$ progressively converge towards $U\\tilde{\\Lambda}_{\\sigma(t)}U^{T}$ throughout the linear distillation process, achieving small normalized MSE (less than 0.2) for most of the noise levels. The less satisfactory convergence behavior at $\\sigma(t)=$ 80.0 is due to inadequate training of the diffusion models at this particular noise level, which is minimally sampled during the training of actual diffusion models (see Appendix G.2 for more details). ", "page_idx": 5}, {"type": "text", "text": "\u2022 Similarity in Score functions. Furthermore, Figure 2(left, gray line) demonstrates that $\\mathcal{D}_{\\mathrm{L}}$ and $\\mathcal{D}_{\\mathrm{G}}$ maintain small score differences (RMSE less than 0.05) across all noise levels, indicating that these denoisers exhibit similar function mappings throughout the diffusion process. ", "page_idx": 5}, {"type": "text", "text": "\u2022 Similarity in principle components. As shown in Figure 4(right), for a wide noise range $(\\sigma(t)\\in[0.116,80.0])$ , the leading singular vectors of the linear weights $W\\sigma(t)$ (denoted $U_{L i n e a r}.$ ) align well with $U$ , the singular vectors of the Gaussian weights.4This implies that $U$ , representing the principal components of the training data, is effectively captured by the diffusion models. In the low-noise regime $(\\sigma(t)\\,\\in\\,[0.002,0.116])$ , however, $\\mathcal{D}_{\\theta}$ approximates the identity mapping, leading to ambiguous singular vectors with minimal impact on image generation. Further analysis of $\\mathcal{D}_{\\theta}$ \u2019s behavior in the low-noise regime is provided in Appendices D and F.1. ", "page_idx": 5}, {"type": "image", "img_path": "Sk2duBGvrK/tmp/663f69b4a1aa23671d713531868897f3d79de63145b852e9598efca77c7978bf.jpg", "img_caption": ["Figure 3: Images sampled from various Models. The figure shows the samples generated using different models starting from the same initial noises. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Since the optimization problem (9) is convex w.r.t. $W_{\\sigma(t)}$ and $b_{\\sigma(t)}$ , the optimal solution $\\mathcal{D}_{\\mathrm{L}}$ represents the unique optimal linear approximation of $\\mathcal{D}_{\\theta}$ . Our analyses demonstrate that this optimal linear approximation closely aligns with $\\mathcal{D}_{\\mathrm{G}}$ , leading to our central finding: diffusion models in the generalization regime exhibit an inductive bias (which we term as the Gaussian inductive bias) towards learning the Gaussian structure of training data. This manifests in two main ways: $(i)$ In the high-noise variance regime, well-trained diffusion models learn $\\mathcal{D}_{\\theta}$ that closely approximate the linear Gaussian denoisers $\\mathcal{D}_{\\mathrm{G}}$ ; $(i i)$ As noise variance decreases, although $\\mathcal{D}_{\\theta}$ diverges from $\\mathcal{D}_{\\mathrm{G}}$ , $\\mathcal{D}_{\\mathrm{G}}$ remains nearly identical to the optimal linear approximation $\\mathcal{D}_{\\mathrm{L}}$ , and images generated by $\\mathcal{D}_{\\mathrm{G}}$ retain structural similarity to those generated by $\\mathcal{D}_{\\theta}$ . ", "page_idx": 5}, {"type": "text", "text": "Finally, we emphasize that the Gaussian inductive bias only emerges in the generalization regime. By contrast, in the memorization regime, Figure 5 shows that $\\mathcal{D}_{\\mathrm{L}}$ significantly diverges from $\\mathcal{D}_{\\mathrm{G}}$ , and both $\\mathcal{D}_{\\mathrm{G}}$ and $\\mathcal{D}_{\\mathrm{L}}$ provide considerably poorer approximations of $\\mathcal{D}_{\\theta}$ compared to the generalization regime. ", "page_idx": 5}, {"type": "image", "img_path": "Sk2duBGvrK/tmp/11545e64d921aab1894d3c24a95524cfdebc573d1355729feda8ec685bfa76b4.jpg", "img_caption": ["Figure 5: Comparison between the diffusion denoisers in memorization and generalization regimes. Figure(a) demonstrates that in the memorization regime (trained on small datasets of size 1094 and 68), $\\mathcal{D}_{\\mathrm{L}}$ significantly diverges from $\\mathcal{D}_{\\mathrm{G}}$ , and both provide substantially poorer approximations of $\\mathcal{D}_{\\theta}$ compared to the generalization regime (trained on larger datasets of size 35000 and 1094). Figure(b) qualitatively shows that the denoising outputs of $\\mathcal{D}_{\\theta}$ closely match those of $\\mathcal{D}_{\\mathrm{G}}$ only in the generalization regime\u2014a similarity that persists even when the denoisers process pure noise inputs. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "3.3 Theoretical Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we demonstrate that imposing linear constraints on diffusion models while minimizing the denoising score matching objective (3) leads to the emergence of Gaussian structure. ", "page_idx": 6}, {"type": "text", "text": "Theorem 1. Consider a diffusion denoiser parameterized as a single-layer linear network, defined as $\\begin{array}{r}{\\mathcal{D}(\\pmb{x}_{t};\\sigma(t))=W_{\\sigma(t)}\\pmb{x}_{t}+\\pmb{b}_{\\sigma(t)}}\\end{array}$ , where $\\dot{\\mathbf{W}}_{\\sigma(t)}\\in\\mathbb{R}^{d\\times d}$ is a linear weight matrix and $\\boldsymbol{b}_{\\sigma(t)}\\in\\mathbb{R}^{d}$ is the bias vector. When the data distribution $p_{d a t a}(\\boldsymbol{x})$ has finite mean $\\pmb{\\mu}$ and bounded positive semidefinite covariance $\\Sigma$ , the optimal solution to the score matching objective (3) is exactly the Gaussian denoiser defined in (6): ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{D}_{\\mathrm{G}}({\\pmb x}_{t};\\sigma(t))\\;=\\;U\\tilde{\\Lambda}_{\\sigma(t)}U^{T}({\\pmb x}_{t}-{\\pmb\\mu})+{\\pmb\\mu},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$\\pmb{b}_{\\sigma(t)}=\\left(\\pmb{I}-\\pmb{U}\\tilde{\\Lambda}_{\\sigma(t)}\\pmb{U}^{T}\\right)\\pmb{\\mu}.$ ", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The detailed proof is postponed to Appendix E. This optimal solution corresponds to the classical Wiener filter [31], revealing that diffusion models naturally learn the Gaussian denoisers when constrained to linear architectures. To understand why highly nonlinear diffusion models operate near this linear regime, it is helpful to model the training data distribution as the multi-delta distribution $\\begin{array}{r}{p({\\pmb x})=\\frac{1}{N}\\,\\bar{\\sum_{i=1}^{N}\\delta({\\pmb x}-\\pmb y_{i})}}\\end{array}$ , where $\\{y_{1},y_{2},...,y_{N}\\}$ is the finite training images. Notice that this formulation better reflects practical scenarios where only a finite number of training samples are available rather than the ground truth data distribution. Importantly, it is proved in [25] that the optimal denoisers $\\mathcal{D}_{\\mathrm{M}}$ in this case is approximately equivalent to $\\mathcal{D}_{\\mathrm{G}}$ for high noise variance $\\sigma(t)$ and query points far from the finite training data. This equivalence explains the strong similarity between $\\mathcal{D}_{\\mathrm{G}}$ and $D_{\\mathrm{M}}$ in the high-noise variance regime, and consequently, why $\\mathcal{D}_{\\theta}$ and $\\mathcal{D}_{\\mathrm{G}}$ exhibit high similarity in this regime\u2014deep networks converge to the optimal denoisers for finite training datasets. ", "page_idx": 6}, {"type": "text", "text": "However, this equivalence between $\\mathcal{D}_{\\mathrm{G}}$ and $\\mathcal{D}_{\\mathrm{M}}$ breaks down at lower $\\sigma(t)$ values. The denoising outputs of $\\mathcal{D}_{\\mathrm{M}}$ are convex combinations of training data points, weighted by a softmax function with temperature $\\sigma(t)^{2}$ . As $\\sigma(t)^{2}$ decreases, this softmax function increasingly approximates an argmax function, effectively retrieving the training point $\\pmb{y}_{i}$ closest to the input $\\textbf{\\em x}$ . Learning this optimal solution requires not only sufficient model capacity to memorize the entire training dataset but also, as shown in [32], an exponentially large number of training samples. Due to these learning challenges, deep networks instead converge to local minima $\\mathcal{D}_{\\theta}$ that, while differing from $\\mathcal{D}_{\\mathrm{M}}$ , exhibit better generalization property. Our experiments reveal that these learned $\\mathcal{D}_{\\theta}$ share similar function mappings with $\\mathcal{D}_{\\mathrm{G}}$ . The precise mechanism driving diffusion models trained with gradient descent towards this particular solution remains an open question for future research. ", "page_idx": 6}, {"type": "text", "text": "Notably, modeling $p_{\\mathrm{data}}(x)$ as a multi-delta distribution reveals a key insight: while unconstrained optimal denoisers (5) perfectly capture the scores of the empirical distribution, they have no generalizability. In contrast, Gaussian denoisers, despite having higher score approximation errors due to the linear constraint, can generate novel images that closely match those produced by the actual diffusion models. This suggests that the generative power of diffusion models stems from the imperfect learning of the score functions of the empirical distribution. ", "page_idx": 6}, {"type": "image", "img_path": "Sk2duBGvrK/tmp/79396268be3e14ad633baf376b240858a75bac790043e5852c8ea402c8b17d46.jpg", "img_caption": ["Figure 6: Diffusion models learn the Gaussian structure when training dataset is large. Models with a fixed scale (channel size 128) are trained across various dataset sizes. The left and right figures show the score difference and the generated images respectively. \u201dNN\u201d denotes the nearest neighbor in the training dataset to the images generated by the diffusion models. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4 Conditions for the Emergence of Gaussian Structures and Generalizability ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In Section 3, we demonstrate that diffusion models exhibit an inductive bias towards learning denoisers that are close to the Gaussian denoisers. In this section, we investigate the conditions under which this bias manifests. Our findings reveal that this inductive bias is linked to model generalization and is governed by (i) the model capacity relative to the dataset size and $(i i)$ the training duration. For additional results, including experiments on CIFAR-10 dataset, see Appendix F. ", "page_idx": 7}, {"type": "text", "text": "4.1 Gaussian Structures Emerge when Model Capacity is Relatively Small ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "First, we find that the Gaussian inductive bias and the generalization of diffusion models are heavily influenced by the relative size of the model capacity compared to the training dataset. In particular, we demonstrate that: ", "page_idx": 7}, {"type": "text", "text": "Diffusion models learn the Gaussian structures when the model capacity is relatively small compared to the size of training dataset. ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "This argument is supported by the following two key observations: ", "page_idx": 7}, {"type": "text", "text": "\u2022 Increasing dataset size prompts the emergence of Gaussian structure at fixed model scale. We train diffusion models using the EDM configuration [4] with a fixed channel size of 128 on datasets of varying sizes [68, 137, 1094, 8750, 35000, 70000] until FID convergence. Figure 6(left) demonstrates that the score approximation error between diffusion denoisers $\\mathcal{D}_{\\theta}$ and Gaussian denoisers $\\mathcal{D}_{\\mathrm{G}}$ decreases as the training dataset size grows, particularly in the crucial intermediate noise variance regime $\\overline{{(\\sigma(t)~\\in~[0.116,20])}}$ . This increasing similarity between $\\mathcal{D}_{\\theta}$ and $\\mathcal{D}_{\\mathrm{G}}$ correlates with a transition in the models\u2019 behavior: from a memorization regime, where generated images are replicas of training samples, to a generalization regime, where novel images exhibiting Gaussian structure5 are produced, as shown in Figure 6(b). This correlation underscores the critical role of Gaussian structure in the generalization capabilities of diffusion models. ", "page_idx": 7}, {"type": "text", "text": "\u2022 Decreasing model capacity promotes the emergence of Gaussian structure at fixed dataset sizes. Next, we investigate the impact of model scale by training diffusion models with varying channel sizes [4, 8, 16, 32, 64, 128], corresponding to [64k, 251k, 992k, 4M, 16M, 64M] parameters, on a fixed training dataset of 1094 images. Figure 7(left) shows that in the intermediate noise variance regime $\\bar{(\\sigma(t)}\\in[0.116,20])$ , the discrepancy between $\\mathcal{D}_{\\theta}$ and $\\mathcal{D}_{\\mathrm{G}}$ decreases with decreasing model scale, indicating that Gaussian structure emerges in low-capacity models. Figure 7(right) demonstrates that this trend corresponds to a transition from data memorization to the generation of images exhibiting Gaussian structure. Here we note that smaller models lead to larger discrepancy between $\\mathcal{D}_{\\theta}$ and $\\mathcal{D}_{\\mathrm{G}}$ in the high-noise regime. This phenomenon arises because diffusion models employ a bell-shaped noise sampling distribution that prioritizes intermediate noise levels, resulting in insufficient training at high noise variances, especially when model capacity is limited. (see more details in Appendix F.2). ", "page_idx": 7}, {"type": "image", "img_path": "Sk2duBGvrK/tmp/e98676846c471fdc56d94c5f57a7b4a48947474cb13c4bb60b052d97db0fe222.jpg", "img_caption": ["Figure 7: Diffusion model learns the Gaussian structure when model scale is small. Models with different scales are trained on a fixed training dataset of 1094 images. The left and right figures show the score difference and the generated images respectively. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "Sk2duBGvrK/tmp/8cdaee68336e94cec1e69c7b2142f0286c82e18d0a6f1c189f78f8082d67608d.jpg", "img_caption": ["Figure 8: Diffusion model learns the Gaussian structure in early training epochs. Diffusion model with same scale (channel size 128) is trained using 1094 images. The left and right figures shows the score difference and the generated images respectively. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "These two experiments collectively suggest that the inductive bias of diffusion models is governed by the relative capacity of the model compared to the training dataset size. ", "page_idx": 8}, {"type": "text", "text": "4.2 Overparameterized Models Learn Gaussian Structures before Memorization ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In the overparameterized regime, where model capacity significantly exceeds training dataset size, diffusion models eventually memorize the training data when trained to convergence. However, examining the learning progression reveals a key insight: ", "page_idx": 8}, {"type": "text", "text": "Diffusion models learn the Gaussian structures with generalizability before they memorize. ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Figure 8(a) demonstrates that during early training epochs (0-841), $\\mathcal{D}_{\\theta}$ progressively converge to $\\mathcal{D}_{\\mathrm{G}}$ in the intermediate noise variance regime, indicating that the diffusion model is progressively learning the Gaussian structure in the initial stages of training. Notably. By epoch 841, the diffusion model generates images strongly resembling those produced by the Gaussian model, as shown in Figure 8(b). However, continued training beyond this point increases the difference between $\\mathcal{D}_{\\theta}$ and $\\mathcal{D}_{\\mathrm{G}}$ as the model transitions toward memorization. This observation suggests that early stopping could be an effective strategy for promoting generalization in overparameterized diffusion models. ", "page_idx": 8}, {"type": "text", "text": "5 Connection between Strong Generalizability and Gaussian Structure ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "A recent study [20] reveals an intriguing \u201dstrong generalization\u201d phenomenon: diffusion models trained on large, non-overlapping image datasets generate nearly identical images from the same initial noise. While this phenomenon might be attributed to deep networks\u2019 inductive bias towards learning the \u201dtrue\u201d continuous distribution of photographic images, we propose an alternative explanation: rather than learning the complete distribution, deep networks may capture certain low-dimensional common structural features shared across these datasets and these features can be partially explained by the Gaussian structure. ", "page_idx": 8}, {"type": "image", "img_path": "Sk2duBGvrK/tmp/bb9d8993d7e757e912483edb3d88d8a8de5c818052d831798d3753aa916d0467.jpg", "img_caption": ["Figure 9: Diffusion models in the strong generalization regime generate similar images as the Gaussian models. Figure(a) Top: Generated images of Gausisan models; Bottom: Generated images of diffusion models, with model scale 128; S1 and S2 each has 35000 non-overlapping images. Figure(b) Top: Generated images of Gausisan model; Bottom: Generated images of diffusion models in the memorization regime, with model scale 128; S1 and S2 each has 1094 non-overlapping images. Figure(c): Early stopping and reducing model capacity help transition diffusion models from memorization to generalization. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "To validate this hypothesis, we examine two diffusion models with channel size 128, trained on non-overlapping datasets S1 and S2 (35,000 images each). Figure 9(a) shows that images generated by these models (bottom) closely match those from their corresponding Gaussian models (top), highlighting the Gaussian structure\u2019s role in strong generalization. ", "page_idx": 9}, {"type": "text", "text": "Comparing Figure 9(a)(top) and (b)(top), we observe that $\\mathcal{D}_{\\mathrm{G}}$ generates nearly identical images whether the Gaussian structure is calculated on a small dataset (1,094 images) or a much larger one (70,000 images). This similarity emerges because datasets of the same class can exhibit similar Gaussian structure (empirical covariance) with relatively few samples\u2014just hundreds for FFHQ. Given the Gaussian structure\u2019s critical role in generalization, small datasets may already contain much of the information for generalization, contrasting previous assertions in [20] that strong generalization requires training on datasets of substantial size (more than $10^{5}$ images). However, smaller datasets increase memorization risk, as shown in Figure 9(b). To mitigate this, as discussed in Section 4, we can either reduce model capacity or implement early stopping (Figure 9(c)). Indeed, models trained on 1094 and 35000 images generate remarkably similar images, though the smaller dataset yields lower perceptual quality. This similarity further demonstrates that small datasets contain substantial generalization-relevant information closely tied to Gaussian structure. Further discussion on the connections and differences between our work and [20] are detailed in Appendix H. ", "page_idx": 9}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this study, we empirically demonstrate that diffusion models in the generalization regime have the inductive bias towards learning diffusion denoisers that are close to the the corresponding linear Gaussian denoisers\u2014the optimal solution under Gaussian data assumptions. While real-world image distributions are significantly different from Gaussian, our findings imply that diffusion models have the bias towards learning and utilizing low-dimensional data structures, such as the data covariance, for image generation. However, the underlying mechanism by which the nonlinear diffusion models, trained with gradient descent, exhibit such linearity remains unclear and warrants further investigation. ", "page_idx": 9}, {"type": "text", "text": "Moreover, the Gaussian structure only partially explains diffusion models\u2019 generalizability. While models exhibit increasing linearity as they transition from memorization to generalization, a substantial gap persists between the linear Gaussian denoisers and the actual nonlinear diffusion models, especially in the intermediate noise regime. As a result, images generated by Gaussian denoisers fall short in perceptual quality compared to those generated by the actual diffusion models especially for complex dataset such as CIFAR-10. This disparity highlights the critical role of nonlinearity in high-quality image generation, a topic we aim to investigate further in future research. ", "page_idx": 9}, {"type": "text", "text": "Data Availability Statement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The code and instructions for reproducing the experiment results will be made available in the following link: https://github.com/Morefre/Understanding-Generalizability-ofDiffusion-Models-Requires-Rethinking-the-Hidden-Gaussian-Structure. ", "page_idx": 10}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We acknowledge funding support from NSF CAREER CCF-2143904, NSF CCF-2212066, NSF CCF- 2212326, NSF IIS 2312842, NSF IIS 2402950, ONR N00014-22-1-2529, a gift grant from KLA, an Amazon AWS AI Award, and MICDE Catalyst Grant. We also acknowledge the computing support from NCSA Delta GPU [33]. We thank Prof. Rongrong Wang (MSU) for fruitful discussions and valuable feedbacks. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 2256\u20132265. PMLR, 2015.   \n[2] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[3] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations.   \n[4] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems, 35:26565\u201326577, 2022.   \n[5] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjo\u00a8rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \n[6] Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. In International Conference on Learning Representations, 2023.   \n[7] Valentin De Bortoli. Convergence of denoising diffusion models under the manifold hypothesis. Transactions on Machine Learning Research, 2022.   \n[8] Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence for score-based generative modeling with polynomial complexity. Advances in Neural Information Processing Systems, 35:22870\u201322882, 2022.   \n[9] Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence of score-based generative modeling for general data distributions. In International Conference on Algorithmic Learning Theory, pages 946\u2013985. PMLR, 2023.   \n[10] Yuchen Wu, Yuxin Chen, and Yuting Wei. Stochastic runge-kutta methods: Provable acceleration of diffusion models. arXiv preprint arXiv:2410.04760, 2024.   \n[11] Gen Li, Yuting Wei, Yuejie Chi, and Yuxin Chen. A sharp convergence theory for the probability flow odes of diffusion models. arXiv preprint arXiv:2408.02320, 2024.   \n[12] Zhihan Huang, Yuting Wei, and Yuxin Chen. Denoising diffusion probabilistic models are optimally adaptive to unknown low dimensionality. arXiv preprint arXiv:2410.18784, 2024.   \n[13] Minshuo Chen, Kaixuan Huang, Tuo Zhao, and Mengdi Wang. Score approximation, estimation and distribution recovery of diffusion models on low-dimensional data. In International Conference on Machine Learning, pages 4672\u20134712. PMLR, 2023.   \n[14] Kazusato Oko, Shunta Akiyama, and Taiji Suzuki. Diffusion models are minimax optimal distribution estimators. In International Conference on Machine Learning, pages 26517\u201326582. PMLR, 2023.   \n[15] Kulin Shah, Sitan Chen, and Adam Klivans. Learning mixtures of gaussians using the ddpm objective. Advances in Neural Information Processing Systems, 36:19636\u201319649, 2023.   \n[16] Hugo Cui, Eric Vanden-Eijnden, Florent Krzakala, and Lenka Zdeborova. Analysis of learning a flow-based generative model from limited sample complexity. In The Twelfth International Conference on Learning Representations, 2023.   \n[17] Huijie Zhang, Jinfan Zhou, Yifu Lu, Minzhe Guo, Liyue Shen, and Qing Qu. The emergence of reproducibility and consistency in diffusion models. In Forty-first International Conference on Machine Learning, 2024.   \n[18] Peng Wang, Huijie Zhang, Zekai Zhang, Siyi Chen, Yi Ma, and Qing Qu. Diffusion models learn low-dimensional distributions via subspace clustering. arXiv preprint arXiv:2409.02426, 2024.   \n[19] Sixu Li, Shi Chen, and Qin Li. A good score does not lead to a good generative model. arXiv preprint arXiv:2401.04856, 2024.   \n[20] Zahra Kadkhodaie, Florentin Guth, Eero P Simoncelli, and St\u00b4ephane Mallat. Generalization in diffusion models arises from geometry-adaptive harmonic representation. In The Twelfth International Conference on Learning Representations, 2023.   \n[21] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Diffusion art or digital forgery? investigating data replication in diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6048\u20136058, 2023.   \n[22] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Understanding and mitigating copying in diffusion models. Advances in Neural Information Processing Systems, 36:47783\u201347803, 2023.   \n[23] TaeHo Yoon, Joo Young Choi, Sehyun Kwon, and Ernest K Ryu. Diffusion probabilistic models generalize when they fail to memorize. In ICML 2023 Workshop on Structured Probabilistic Inference $\\{\\backslash\\mathcal{E}\\}$ Generative Modeling, 2023.   \n[24] Xiangming Gu, Chao Du, Tianyu Pang, Chongxuan Li, Min Lin, and Ye Wang. On memorization in diffusion models. arXiv preprint arXiv:2310.02664, 2023.   \n[25] Binxu Wang and John J Vastola. The hidden linear structure in score-based models and its application. arXiv preprint arXiv:2311.10892, 2023.   \n[26] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4401\u20134410, 2019.   \n[27] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[28] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for multiple domains. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8188\u20138197, 2020.   \n[29] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015.   \n[30] Hyojun Go, Yunsung Lee, Seunghyun Lee, Shinhyeok Oh, Hyeongdon Moon, and Seungtaek Choi. Addressing negative transfer in diffusion models. Advances in Neural Information Processing Systems, 36, 2024.   \n[31] Mallat Ste\u00b4phane. Chapter 11 - denoising. In Mallat Ste\u00b4phane, editor, A Wavelet Tour of Signal Processing (Third Edition), pages 535\u2013610. Academic Press, Boston, third edition edition, 2009.   \n[32] Chen Zeno, Greg Ongie, Yaniv Blumenfeld, Nir Weinberger, and Daniel Soudry. How do minimum-norm shallow denoisers look in function space? Advances in Neural Information Processing Systems, 36, 2024.   \n[33] Timothy J Boerner, Stephen Deems, Thomas R Furlani, Shelley L Knuth, and John Towns. Access: Advancing innovation: Nsf\u2019s advanced cyberinfrastructure coordination ecosystem: Services & support. In Practice and Experience in Advanced Research Computing, pages 173\u2013176. 2023.   \n[34] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780\u20138794, 2021.   \n[35] DP Kingma. Adam: a method for stochastic optimization. In Int Conf Learn Represent, 2014.   \n[36] Alfred O. Hero. Statistical methods for signal processing. 2005.   \n[37] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pages 1096\u20131103, 2008.   \n[38] Pascal Vincent. A connection between score matching and denoising autoencoders. Neural computation, 23(7):1661\u20131674, 2011.   \n[39] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted intervention\u2013MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234\u2013241. Springer, 2015.   \n[40] Sergey Ioffe. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.   \n[41] Andrew L Maas, Awni Y Hannun, Andrew $\\mathbf{Y}\\,\\mathbf{Ng}$ , et al. Rectifier nonlinearities improve neural network acoustic models. In Proc. icml, volume 30, page 3. Atlanta, GA, 2013.   \n[42] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195\u20134205, 2023.   \n[43] Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[44] Sreyas Mohan, Zahra Kadkhodaie, Eero P Simoncelli, and Carlos Fernandez-Granda. Robust and interpretable blind image denoising via bias-free convolutional neural networks. In International Conference on Learning Representations.   \n[45] Hila Manor and Tomer Michaeli. On the posterior distribution in denoising: Application to uncertainty quantification. In The Twelfth International Conference on Learning Representations.   \n[46] Siyi Chen, Huijie Zhang, Minzhe Guo, Yifu Lu, Peng Wang, and Qing Qu. Exploring lowdimensional subspaces in diffusion models for controllable image editing. arXiv preprint arXiv:2409.02374, 2024. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendices ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "1 Introduction 1   \n2 Preliminary 2   \nHidden Linear and Gaussian Structures in Diffusion Models 3   \n3.1 Diffusion Models Exhibit Linearity in the Generalization Regime . 4   \n3.2 Inductive Bias towards Learning the Gaussian Structures 5   \n3.3 Theoretical Analysis 7   \nConditions for the Emergence of Gaussian Structures and Generalizability 8   \nGaussian Structures Emerge when Model Capacity is Relatively Small 8   \n4.2 Overparameterized Models Learn Gaussian Structures before Memorization 9   \nConnection between Strong Generalizability and Gaussian Structure 9   \n6 Discussion 10   \nA Measuring the Linearity of Diffusion Denoisers 15   \nB Emerging Linearity of Diffusion Models 16   \nB.1 Generalization and Memorization Regimes of Diffusion Models 16   \nB.2 Diffusion Models Exhibit Linearity in the Generalization Regime 16   \nC Linear Distillation 16   \nD Diffusion Models in Low-noise Regime are Approximately Linear Mapping 18   \nE Theoretical Analysis 20   \nE.1 Proof of Theorem 1 20   \nE.2 Two Extreme Cases 22   \nF More Discussion on Section 4 23   \nF.1 Behaviors in Low-noise Regime 23   \nF.2 Behaviors in High-noise Regime 23   \nF.3 Similarity between Diffusion Denoiers and Gaussian Denoisers 24   \nF.4 CIFAR-10 Results 25   \nG Additional Experiment Results 25   \nG.1 Gaussian Structure Emerges across Various Network Architectures 26   \nG.2 Gaussian Inductive Bias as a General Property of DAEs 26 ", "page_idx": 13}, {"type": "text", "text": "G.3 Gaussian Structure Emerges across Various datasets . 28 ", "page_idx": 14}, {"type": "text", "text": "G.4 Strong Generalization on CIFAR-10 28   \nG.5 Measuring Score Approximation Error with NMSE 28   \nH Discussion on Geometry-Adaptive Harmonic Bases 30   \nH.1 GAHB only Partially Explain the Strong Generalization . 30   \nH.2 GAHB Emerge only in Intermediate-Noise Regime 31 ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Measuring the Linearity of Diffusion Denoisers ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we provide a detailed discussion on how to measure the linearity of diffusion model. For a diffusion denoiser, $D_{\\theta}(\\mathbf{\\boldsymbol{x}};\\sigma(t))$ , to be considered approximately linear, it must fulfill the following conditions: ", "page_idx": 14}, {"type": "text", "text": "\u2022 Additivity: The function should satisfy $\\begin{array}{r}{\\mathcal{D}_{\\theta}(x_{1}+x_{2};\\sigma(t))\\approx\\mathcal{D}_{\\theta}(x_{1};\\sigma(t))+\\mathcal{D}_{\\theta}(x_{2};\\sigma(t)).}\\end{array}$ \u2022 Homogeneity: It should also adhere to $\\mathcal{D}_{\\theta}(\\alpha\\pmb{x};\\sigma(t))\\approx\\alpha\\mathcal{D}_{\\pmb{\\theta}}(\\pmb{x};\\sigma(t))$ . ", "page_idx": 14}, {"type": "text", "text": "To jointly assess these properties, we propose to measure the difference between $\\mathcal{D}_{\\pmb{\\theta}}(\\alpha\\pmb{x}_{1}\\!+\\!\\beta\\pmb{x}_{2};\\sigma(t))$ and $\\alpha\\mathcal{D}_{\\pmb{\\theta}}(\\pmb{x}_{1};\\pmb{\\sigma}(t))+\\beta D(\\pmb{x}_{2};\\pmb{\\sigma}(t))$ . While the linearity score is introduced as the cosine similarity between $\\mathcal{D}_{\\pmb{\\theta}}(\\alpha\\pmb{x}_{1}+\\beta\\pmb{x}_{2};\\pmb{\\sigma}(t))$ and $\\alpha\\mathcal{D}_{\\pmb{\\theta}}(\\pmb{x}_{1};\\pmb{\\sigma}(t))+\\dot{\\beta}D(\\pmb{x}_{2};\\pmb{\\sigma}(t))$ in the main text: ", "page_idx": 14}, {"type": "equation", "text": "$$\nS(t)\\;=\\;\\mathbb{E}_{{\\pmb{x}}_{1},{\\pmb{x}}_{2}\\sim p({\\pmb{x}};\\sigma(t))}\\,\\left[\\left|\\left\\langle\\frac{\\mathcal{D}_{\\pmb{\\theta}}(\\alpha{\\pmb{x}}_{1}+\\beta{\\pmb{x}}_{2};\\sigma(t))}{\\|\\mathcal{D}_{\\pmb{\\theta}}(\\alpha{\\pmb{x}}_{1}+\\beta{\\pmb{x}}_{2};\\sigma(t))\\|_{2}},\\frac{\\alpha\\mathcal{D}_{\\pmb{\\theta}}({\\pmb{x}}_{1};\\sigma(t))+\\beta\\mathcal{D}_{\\pmb{\\theta}}({\\pmb{x}}_{1};\\sigma(t))}{\\|\\alpha\\mathcal{D}_{\\pmb{\\theta}}({\\pmb{x}}_{1};\\sigma(t))+\\beta\\mathcal{D}_{\\pmb{\\theta}}({\\pmb{x}}_{1};\\sigma(t))\\|_{2}}\\right\\rangle\\right|\\right],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "it can also be defined with the normalized mean square difference (NMSE): ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{E}_{x_{1},x_{2}\\sim p(\\mathbf{x};\\sigma(t))}\\frac{||\\mathcal{D}_{\\theta}(\\alpha x_{1}+\\beta x_{2};\\sigma(t))-(\\alpha\\mathcal{D}_{\\theta}(x_{1};\\sigma(t))+\\beta\\mathcal{D}_{\\theta}(x_{1};\\sigma(t)))||_{2}}{||\\mathcal{D}_{\\theta}(\\alpha x_{1}+\\beta x_{2};\\sigma(t))||_{2}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the expectation is approximated with its empirical mean over 100 randomly sampled pairs of $(x_{1},x_{2})$ . In the next section, we will demonstrate the linearity score with both metrics. ", "page_idx": 14}, {"type": "text", "text": "Since the diffusion denoisers are trained solely on inputs $\\mathbf{\\boldsymbol{x}}\\sim p(\\mathbf{\\boldsymbol{x}};\\sigma(t))$ , their behaviors on outof-distribution inputs can be quite irregular. To produce a denoised output with meaningful image structure, it is critical that the noise component in the input $\\textbf{\\em x}$ matches the correct variance $\\sigma(\\bar{t})^{\\bar{2}}$ . Therefore, our analysis of linearity is restricted to in-distribution inputs $x_{1}$ and $\\pmb{x}_{2}$ , which are randomly sampled images with additive Gaussian noises calibrated to noise variance $\\sigma(t)^{2}$ . We also need to ensure that the values of $\\alpha$ and $\\beta$ are chosen such that $\\alpha^{2}+\\beta^{2}=1$ , maintaining the correct variance for the noise term in the combined input $\\alpha{\\pmb x}_{1}+\\beta{\\pmb x}_{2}$ . We present the linearity scores, calculated with varying values of $\\alpha$ and $\\beta$ , for diffusion models trained on diverse datasets in Figure 10. These models are trained with the EDM-VE configuration proposed in [4], w\u221ahich ensures the resulting models are in the generalization regime. Typically, setting $\\alpha=\\beta=1/\\sqrt{2}$ yields the lowest linearity score; however, even in this scenario, the cosine similarity remains impressively high, exceeding 0.96. This high value underscores the presence of significant linearity within diffusion denoisers. ", "page_idx": 14}, {"type": "text", "text": "We would like to emphasize that for linearity to manifest in diffusion denoisers, it is crucial that they are well-trained, achieving a low denoising score matching loss as indicated in (3). As shown in Figure 11, the linearity notably reduces in a less well trained diffusion model (Baseline-VE) comapred to its well-trained counterpart (EDM-VE). Although both models utilize the same \u2019VE\u2019 network architecture $\\mathcal{F}_{\\theta}(\\mathbf{\\boldsymbol{x}};\\sigma(t))$ [2], they differ in how the diffusion denoisers are parameterized: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{D}_{\\theta}(\\mathbf{\\boldsymbol{x}};\\sigma(t)):=c_{\\mathrm{skip}}(\\sigma(t))\\mathbf{\\boldsymbol{x}}+c_{\\mathrm{out}}(\\mathcal{F}_{\\theta}(\\mathbf{\\boldsymbol{x}};\\sigma(t))),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $c_{\\mathrm{skip}}$ is the skip connection and $c_{\\mathrm{out}}$ modulate the scale of the network output. With carefully tailored $c_{\\mathrm{skip}}$ and $c_{\\mathrm{out}}$ , the EDM-VE configuration achieves a lower score matching loss compared to Baseline-VE, resulting in samples with higher quality as illustrated in Figure 11(right). ", "page_idx": 15}, {"type": "text", "text": "B Emerging Linearity of Diffusion Models ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section we provide a detailed discussion on the observation that diffusion models exhibit increasing linearity as they transition from memorization to generalization, which is briefly described in Section 3.1. ", "page_idx": 15}, {"type": "text", "text": "B.1 Generalization and Memorization Regimes of Diffusion Models ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "As shown in Figure 12, as the training dataset size increases, diffusion models transition from the memorization regime\u2014where they can only replicate its training images\u2014to the generalization regime, where the they produce high-quality, novel images. To measure the generalization capabilities of diffusion models, it is crucial to assess their ability to generate images that are not mere replications of the training dataset. This can be quantitatively evaluated by generating a large set of images from the diffusion model and measuring the average difference between these generated images and their nearest neighbors in the training set. Specifically, let $\\{x_{1},x_{2},...,x_{k}\\}$ represent $k$ randomly sampled images from the diffusion models (we choose $k=100$ in our experiments), and let $Y:=$ $\\{y_{1},y_{2},...,y_{N}\\}$ denote the training dataset consisting of $N$ images. We define the generalization score as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\mathrm{GL~Score:}}={\\frac{1}{k}}\\sum_{i=1}^{k}{\\frac{||\\pmb{x}_{i}-\\mathbf{NN}_{Y}(\\pmb{x}_{i})||_{2}}{||\\pmb{x}_{i}||_{2}}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\mathrm{NN}_{Y}({\\pmb x}_{i})$ represents the nearest neighbor of the sample $\\pmb{x}_{k}$ in the training dataset $Y$ , determined by the Euclidean distance on a per-pixel basis. Empirically, a GL score exceeding 0.6 indicates that the diffusion models are effectively generalizing beyond the training dataset. ", "page_idx": 15}, {"type": "text", "text": "B.2 Diffusion Models Exhibit Linearity in the Generalization Regime ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "As demonstrated in Figure 13(a) and (d), diffusion models transition from the memorization regime to the generalization regime as the training dataset size increases. Concurrently, as depicted in Figure 13(b), (c), (e) and (f), the corresponding diffusion denoisers exhibit increasingly linearity. This phenomenon persists across diverse datasets datasets including FFHQ [26], AFHQ [28] and LSUNChurches [29], as well as various model architectures including EDM-VE [3], EDM-VP [2] and EDM-ADM [34]. This emerging linearity implies that the hidden linear structure plays an important role in the generalizability of diffusion model. ", "page_idx": 15}, {"type": "text", "text": "C Linear Distillation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "As discussed in Section 3.1, we propose to study the hidden linearity observed in diffusion denosiers with linear distillation. Specifically, for a given diffusion denoiser $D_{\\theta}(\\mathbf{\\boldsymbol{x}};\\sigma(t))$ , we aim to approxi", "page_idx": 15}, {"type": "image", "img_path": "Sk2duBGvrK/tmp/3b0f77239357aada450596c6b73c9bb5d0b6dce6e77ebc131e3bb27db35c3a75.jpg", "img_caption": ["Figure 10: Linearity scores for varying $\\alpha$ and $\\beta$ . The diffusion models are trained with the edm-ve configuration [4], which ensures the models are in the generalization regime. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "Sk2duBGvrK/tmp/e6514db6aba9a5fb29b5c1f3142efa2af26ce84bf26d43ed6c26a0138d389afa.jpg", "img_caption": ["Figure 11: Linearity scores and sampling trajectory. The left and right figures demonstrate the linearity scores and the sampling trajectories $\\mathcal{D}(\\mathbf{\\boldsymbol{x}}_{t};\\boldsymbol{\\sigma}(t)$ of actual diffusion model (EDM-VE and Baseline-VE), Multi Delta model, linear model, and Gaussian model respectively. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "Sk2duBGvrK/tmp/35533de194af6e73f82e42eb68ccbb757700a00b591cf0f6853c7ef0df07e025.jpg", "img_caption": ["Figure 12: Memorization and generalization regimes of diffusion models. Figures(a) to (c) show the images generated by diffusion models trained on 70000, 4375, 1094 FFHQ images and their corresponding nearest neighbors in the training dataset respectively. Figures(d) to (f) show the images generated by diffusion models trained on 50000, 12500, 782 CIFAR-10 images and their corresponding nearest neighbors in the training dataset respectively. Notice that when the training dataset size is small, diffusion model can only generate images in the training dataset. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "mate it with a linear function (with a bias term for more expressibility): ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{D}_{\\mathrm{L}}(\\pmb{x};\\sigma(t)):=W_{\\sigma(t)}\\pmb{x}+\\pmb{b}_{\\sigma(t)}\\;\\approx\\;\\mathcal{D}_{\\pmb{\\theta}}(\\pmb{x};\\sigma(t)),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for $\\mathbf{\\boldsymbol{x}}\\sim p(\\mathbf{\\boldsymbol{x}};\\sigma(t))$ . Notice that for three dimensional images with size $(c,h,w),\\,\\pmb{x}\\in\\mathbb{R}^{d}$ represents their vectorized version, where $d=c\\times w\\times h$ . Let ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\pmb{W},\\pmb{b})=\\frac{1}{n}\\sum_{i=1}^{n}\\left\\|\\pmb{W}_{\\sigma(t)}\\{k-1\\}\\big(\\pmb{x}_{i}+\\pmb{\\epsilon}_{i}\\big)+\\pmb{b}_{\\sigma(t)}\\{k-1\\}-\\mathcal{D}_{\\pmb{\\theta}}\\big(\\pmb{x}_{i}+\\pmb{\\epsilon}_{i};\\sigma(t)\\big)\\right\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We train 10 independent linear models for each of the selected noise variance level $\\sigma(t)$ with the procedure summarized in Algorithm 1: ", "page_idx": 16}, {"type": "text", "text": "In practice, the gradients on $W_{\\sigma(t)}$ and $b_{\\sigma(t)}$ are obtained through automatic differentiation. Additionally, we employ the Adam optimizer [35] for updates. Additional linear distillation results are provided in Figure 14. ", "page_idx": 16}, {"type": "image", "img_path": "Sk2duBGvrK/tmp/b16f90b93321685bcc461e03e6c61b6d768ea195afa26fe8a87631c9b65d74ac.jpg", "img_caption": ["Figure 13: Diffusion model exhibit increasing linearity as they transition from memorization to generalization. Figure(a) and (d) demonstrate that for both FFHQ and CIFAR-10 datasets, the generalization score increases with the training dataset size, indicating progressive model generalization. Figure(b), (c), (e), and (f) show that this transition towards generalization is accompanied by increasing denoiser linearity. Specifically, Figure(b) and (e) display linearity scores calculated using cosine similarity (11), while Figure(c) and (f) show scores computed using NMSE (12). Both metrics reveal consistent trends. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "D Diffusion Models in Low-noise Regime are Approximately Linear Mapping ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "It should be noted that the low score difference between $\\mathcal{D}_{\\mathrm{G}}$ and $\\mathcal{D}_{\\theta}$ within the low-noise regime $(\\sigma(t)\\in[0.002,0.116])$ does not imply the diffusion denoisers capture the Gaussian structure, instead, the similarity arises since both of them are converging to the identity mapping as $\\sigma(t)$ decreases. As shown in Figure 15, within this regime, the differences between the noisy input $\\textbf{\\em x}$ and their corresponding denoised outputs $D_{\\theta}(\\mathbf{\\boldsymbol{x}};\\sigma(t))$ quickly approach 0. This indicates that the learned denoisers $\\mathcal{D}_{\\theta}$ progressively converge to the identity function. Additionally, from (6), it is evident that the difference between the Gaussian weights and the identity matrix diminishes as $\\sigma(t)$ decreases, which explains why $\\mathcal{D}_{\\mathrm{G}}$ can well approximate $\\mathcal{D}_{\\theta}$ in the low noise variance regime. ", "page_idx": 17}, {"type": "text", "text": "We hypothesize that $\\mathcal{D}_{\\theta}$ learns the identity function because of the following two reasons: ", "page_idx": 17}, {"type": "text", "text": "$(i)$ within the low-noise regime, since the added noise is negligible compared to the clean image, the identity function already achieves a small denoising error, thus serving as a shortcut which is exploited by the deep network. ", "page_idx": 17}, {"type": "text", "text": "$(i i)$ As discussed in Appendix A, diffusion models are typically parameterized as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{D}_{\\theta}(\\mathbf{\\boldsymbol{x}};\\sigma(t)):=c_{\\mathrm{skip}}(\\sigma(t))\\mathbf{\\boldsymbol{x}}+c_{\\mathrm{out}}(\\mathcal{F}_{\\theta}(\\mathbf{\\boldsymbol{x}};\\sigma(t)))}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\mathcal{F}_{\\theta}$ represents the deep network, and $c_{\\mathrm{skip}}(\\sigma(t))$ and $c_{\\mathrm{out}}(\\sigma(t))$ are adaptive parameters for the skip connection and output scaling, respectively, which adjust according to the noise variance levels. For canonical works on diffusion models [2\u20134, 34], as $\\sigma(t)$ approaches zero, $c_{\\mathrm{skip}}$ and $c_{\\mathrm{out}}$ converge to 1 and 0 respectively. Consequently, at low variance levels, the function forms of diffusion denoisers are approximatly identity mapping: $D_{\\theta}(\\mathbf{\\boldsymbol{x}};\\sigma(t))\\approx\\mathbf{\\boldsymbol{x}}$ . ", "page_idx": 17}, {"type": "text", "text": "This convergence to identity mapping has several implications. First, the weights $W_{\\sigma(t)}$ of the distilled linear models $\\mathcal{D}_{\\mathrm{L}}$ approach the identity matrix at low variances, leading to ambiguous ", "page_idx": 17}, {"type": "text", "text": "Algorithm 1 Linear Distillation ", "page_idx": 18}, {"type": "text", "text": "Require: (i) the targeted diffusion denoiser $\\mathcal{D}_{\\theta}(\\cdot;\\sigma(t))$ , (ii) weights $W_{\\sigma(t)}$ and biases $b_{\\sigma(t)}$ , both initialized to zero, (iii) gradient step size $\\eta$ , (iv) number of training iterations $K$ , (v) training batch size $n$ , (vi) image dataset $S$ . for $k=1$ to $K$ do Randomly sample a batch of training images $\\{x_{1},x_{2},\\ldots,x_{n}\\}$ from $S$ . Randomly sample a batch of noises $\\{\\epsilon_{1},\\epsilon_{2},\\hdots,\\epsilon_{n}\\}$ from $\\mathcal{N}(\\mathbf{\\bar{0}},\\sigma(t)\\mathbf{}I)$ . Update $W_{\\sigma(t)}$ and $b_{\\sigma(t)}$ with gradient descent: $\\begin{array}{r}{\\pmb{W_{\\sigma(t)}}\\{\\pmb{k}\\}=\\pmb{W_{\\sigma(t)}}\\{\\pmb{k}-1\\}-\\eta\\nabla_{\\pmb{W_{\\sigma(t)}}\\{\\pmb{k}-1\\}}\\mathcal{L}(\\pmb{W},\\pmb{\\ell}|)}\\end{array}$ b) $b_{\\sigma(t)}\\{k\\}=b_{\\sigma(t)}\\{k-1\\}-\\eta\\nabla_{b_{\\sigma(t)}\\{k-1\\}}\\mathcal{L}(W,b)$ end for Return $W_{\\sigma(t)}\\{K\\},b_{\\sigma(t)}\\{K\\}$ ", "page_idx": 18}, {"type": "image", "img_path": "Sk2duBGvrK/tmp/ae886b23e99a64b92698b468398d72cd80e240ed176115851115e744d76307db.jpg", "img_caption": ["Figure 14: Additional linear distillation results. Figure(a) demonstrates the gradual symmetrization of linear weights during the distillation process. Figure(b) shows that at convergence, the singular values of the linear weights closely match those of the Gaussian weights. Figure(c) and Figure(d) display the leading singular vectors of both linear and Gaussian weights at $\\bar{\\sigma(t)}=4$ for FFHQ and LSUN-Churches datasets, respectively, revealing a strong correlation. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "singular vectors. This explains the poor recovery of singular vectors for $\\sigma(t)\\in[0.002,0.116]$ ] shown in Figure 4. Second, the presence of the bias term in (8) makes it challenging for our linear model to learn the identity function, resulting in large errors at $\\sigma(t)=0.002$ as shown in Figure 4(a). ", "page_idx": 18}, {"type": "text", "text": "Finally, from (4), we observe that when $\\mathcal{D}_{\\theta}$ acts as an identity mapping, $x_{i+1}$ remains unchanged from $\\pmb{x}_{i}$ . This implies that sampling steps in low-variance regions minimally affect the generated image content, as confirmed in Figure 2, where image content shows negligible variation during these steps. ", "page_idx": 18}, {"type": "image", "img_path": "Sk2duBGvrK/tmp/fd95e839fb94bc4db2e0c93c99092ee05f9b3db461ce18e6847fd29c6b637e77.jpg", "img_caption": ["Figure 15: Difference between $D_{\\theta}(\\mathbf{\\boldsymbol{x}};\\sigma(t))$ and $\\textbf{\\em x}$ for various noise variance levels. Figures(a) and (c) show the differences between $D_{\\theta}(\\mathbf{\\boldsymbol{x}};\\sigma(t))$ and $\\textbf{\\em x}$ across $\\sigma(t)\\,\\in\\,[0.002,80]$ , measured by normalized MSE and cosine similarity, respectively. Figures(b) and (d) provide zoomed-in views of (a) and (c). The diffusion models were trained on the FFHQ dataset. Notice that the difference between $D_{\\theta}(\\mathbf{\\boldsymbol{x}};\\sigma(t))$ and $\\textbf{\\em x}$ quickly converges to near zero in the low noise variance regime. The trend is consistent for various model architectures. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "E Theoretical Analysis ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "E.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we give the proof of Theorem 1 (Section 3.3). Our theorem is based on the following two assumptions: ", "page_idx": 19}, {"type": "text", "text": "Assumption 1. Suppose that the diffusion denoisers are parameterized as single-layer linear networks, defined as $\\bar{\\mathcal{D}}(\\pmb{x};\\sigma(t))\\,=\\,\\dot{\\pmb{W_{\\sigma(t)}}}\\pmb{x}+\\pmb{b}_{\\sigma(t)}$ , where $W_{\\sigma(t)}\\,\\in\\,\\mathbb{R}^{d\\times d}$ is the linear weight and $\\pmb{b}_{\\sigma(t)}\\in\\mathbb{R}^{d}$ is the bias. ", "page_idx": 19}, {"type": "text", "text": "Assumption 2. The data distribution $p_{d a t a}(\\boldsymbol{x})$ has finite mean $\\pmb{\\mu}$ and bounded positive semidefinite covariance $\\Sigma$ ", "page_idx": 19}, {"type": "text", "text": "Theorem 1. Under Assumption 1 and Assumption 2, the optimal solution to the denoising score matching objective (3) is exactly the Gaussian denoiser: $\\begin{array}{r}{\\mathcal{D}_{\\mathrm{G}}(\\pmb{x},\\sigma(t))=\\pmb{\\mu}+\\pmb{U}\\tilde{\\Lambda}_{\\sigma(t)}\\pmb{U}^{T}(\\pmb{x}-\\pmb{\\mu}),}\\end{array}$ , where $\\Sigma={U\\Lambda U^{T}}$ represents the SVD of the covariance matrix, with singular values $\\lambda_{\\{k=1,\\ldots,d\\}}$ and $\\begin{array}{r}{\\tilde{\\mathbf{A}}_{\\sigma(t)}=\\mathrm{diag}[\\frac{\\lambda_{k}}{\\lambda_{k}+\\sigma(t)^{2}}]}\\end{array}$ . Furthermore, this optimal solution can be obtained via gradient descent with a proper learning rate. ", "page_idx": 19}, {"type": "text", "text": "To prove Theorem 1, we first show that the Gaussian denoiser is the optimal solution to the denoising score matching objective under the linear network constraint. Then we will show that such optimal solution can be obtained via gradient descent with a proper learning rate. ", "page_idx": 19}, {"type": "text", "text": "The Global Optimal Solution. Under the constraint that the diffusion denoiser is restricted to a single-layer linear network with bias: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r}{\\mathscr{D}(\\pmb{x};\\sigma(t))=W_{\\sigma(t)}\\pmb{x}+\\pmb{b}_{\\sigma(t)},}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We get the following optimizaiton problem from Equation (3): ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{W^{\\star},b^{\\star}=\\underset{W,b}{\\arg\\operatorname*{min}}\\,\\mathcal{L}(W,b;\\sigma(t)):=\\mathbb{E}_{\\mathbf{x}\\sim p_{\\mathrm{das}}}\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(\\mathbf{0},\\sigma(t)^{2}I)}||W(\\mathbf{x}+\\epsilon)+b-\\mathbf{x}||_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we omit the footnote $\\sigma(t)$ in $W_{\\sigma(t)}$ and $b_{\\sigma(t)}$ for simplicity. Since expectation preserves convexity, the optimization problem Equation (16) is a convex optimization problem. To find the global optimum, we first eliminate $^{b}$ by requiring the partial derivative $\\nabla_{b}\\mathcal{L}(W,b;\\sigma(t))$ to be 0. Since ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{b}\\mathcal{L}(W,b;\\sigma(t))=2*\\mathbb{E}_{x\\sim p_{\\mathrm{dat}}}\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(\\mathbf{0},\\sigma(t)^{2}I)}((W-I)x+W\\epsilon+b)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=2*\\mathbb{E}_{x\\sim p_{\\mathrm{dat}}}((W-I)x+b)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=2*((W-I)\\mu+b),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "we have ", "page_idx": 19}, {"type": "equation", "text": "$$\nb^{\\star}=(I-W^{\\ast})\\mu.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Utilizing the expression for $^{b}$ , we get the following equivalent form of the optimization problem: ", "page_idx": 20}, {"type": "text", "text": "$W^{\\star}=\\underset{W}{\\arg\\operatorname*{min}}\\,\\mathcal{L}(W;\\sigma(t)):=2*\\mathbb{E}_{x\\sim p_{\\mathrm{das}}}\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(\\mathbf{0},\\sigma(t)^{2}I)}||W(x-\\mu+\\epsilon)-(x-\\mu)||_{2}^{2}.$ ", "page_idx": 20}, {"type": "text", "text": "The derivative $\\nabla_{W}\\mathcal{L}(W;\\sigma(t))$ is: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{W}\\mathcal{L}(W;\\sigma(t))=2*\\mathbb{E}_{x}\\mathbb{E}_{\\epsilon}(W(x-\\mu+\\epsilon)(x-\\mu+\\epsilon)^{T}-(x-\\mu)(x-\\mu+\\epsilon)^{T})}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=2*\\mathbb{E}_{x}((W-I)(x-\\mu)(x-\\mu)^{T}+\\sigma(t)^{2}W)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=2*W(\\Sigma+\\sigma(t)^{2}I)-2*\\Sigma.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Suppose $\\pmb{\\Sigma}~=~\\pmb{U}\\pmb{\\Lambda}\\pmb{U}^{T}$ is the SVD of the empirical covariance matrix, with singular values $\\lambda_{\\{k=1,\\ldots,n\\}}$ , by setting $\\nabla_{W}\\mathcal{L}(W;\\sigma(t))$ to $\\mathbf{0}$ , we get the optimal solution: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{W^{\\star}=U\\Lambda U^{T}U(\\mathbf{A}+\\sigma(t)^{2}I)^{-1}U^{T}}\\\\ {=U\\tilde{\\Lambda}_{\\sigma(t)}U^{T},\\qquad\\qquad\\quad\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\begin{array}{r}{\\tilde{\\bf A}_{\\sigma(t)}[i,i]=\\frac{\\lambda_{i}}{\\lambda_{i}+\\sigma(t)^{2}}}\\end{array}$ and $\\lambda_{i}=\\mathbf{A}[i,i]$ . Substitute $W^{\\star}$ back to Equation (20), we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\pmb{b}^{\\star}=(\\pmb{I}-\\pmb{U}\\tilde{\\pmb{\\Lambda}}_{\\sigma(t)}\\pmb{U}^{T})\\pmb{\\mu}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Notice that the expression for $W^{\\star}$ and $b^{\\star}$ is exactly the Gaussian denoiser. Next, we will show this optimal solution can be achieved with gradient descent. ", "page_idx": 20}, {"type": "text", "text": "Gradient Descent Recovers the Optimal Solution. Consider minimizing the population loss: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}(\\pmb{W},\\pmb{b};\\sigma(t)):=\\mathbb{E}_{\\pmb{x}\\sim p_{\\mathrm{dat}}}\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(\\mathbf{0},\\sigma(t)^{2}\\pmb{I})}||\\pmb{W}(\\pmb{x}+\\epsilon)+\\pmb{b}-\\pmb{x}||_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Define $\\tilde{W}:=[W\\quad b],\\tilde{x}:=\\left[\\!\\!\\begin{array}{l l}{x}&{}\\\\ {1}&{}\\end{array}\\!\\!\\begin{array}{l l}{[x]}&{}\\\\ {}&{}\\end{array}\\!\\!\\begin{array}{r l}{x],}\\end{array}$ and $\\tilde{\\epsilon}=\\left[\\epsilon\\right]$ then we can rewrite Equation (28) as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}(\\tilde{W};\\sigma(t)):=\\mathbb{E}_{\\mathbf{x}\\sim p_{\\mathrm{data}}}\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(\\mathbf{0},\\sigma(t)^{2}I)}||\\tilde{W}(\\tilde{\\mathbf{x}}+\\tilde{\\epsilon})-\\mathbf{x}||_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We can compute the gradient in terms of $\\Tilde{W}$ as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla\\mathcal{L}(\\tilde{W})=2*\\mathbb{E}_{x,\\epsilon}(\\tilde{W}(\\tilde{x}+\\tilde{\\epsilon})(\\tilde{x}+\\tilde{\\epsilon})^{T}-x(\\tilde{x}+\\tilde{\\epsilon})^{T})}\\\\ &{\\qquad\\qquad=2*\\mathbb{E}_{x,\\epsilon}(\\tilde{W}(\\tilde{x}\\tilde{x}^{T}+\\tilde{x}\\tilde{\\epsilon}^{T}+\\tilde{\\epsilon}\\tilde{x}^{T}+\\tilde{\\epsilon}\\tilde{\\epsilon}^{T})-x\\tilde{x}^{T}-x\\tilde{\\epsilon}^{T}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $\\mathbb{E}_{\\epsilon}(\\tilde{\\epsilon})=\\mathbf{0}$ and $\\mathbb{E}_{\\epsilon}(\\tilde{\\epsilon}\\tilde{\\epsilon}^{T})=\\left[\\overset{\\sigma(t)^{2}I_{d\\times d}}{\\mathbf{0}_{1\\times d}}\\quad0_{d\\times1}\\right]$ we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\nabla\\mathcal{L}(\\tilde{W})=2*\\mathbb{E}_{x}(\\tilde{W}(\\tilde{x}\\tilde{x}^{T}+\\left[\\sigma(t)^{2}I_{d\\times d}\\quad\\mathbf{0}_{d\\times1}\\right])-x\\tilde{x}^{T}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Since $\\mathbb{E}(\\tilde{\\pmb{x}}\\tilde{\\pmb{x}}^{T})=\\left[\\mathbb{E}(\\pmb{x}\\pmb{x}^{T})\\quad\\mathbb{E}(\\pmb{x})\\right]$ we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\nabla{\\mathcal L}(\\tilde{W})=2\\tilde{W}\\left[\\mathbb{E}_{x}{\\left(x{x^{T}}\\right)}+\\sigma{(t)^{2}}I\\quad\\mu\\right]-2\\left[\\mathbb{E}_{x}{\\left(x^{T}{x}\\right)}\\quad\\mu\\right].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "With learning rate $\\eta$ , we can write the update rule as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\pmb{W}}(t+1)=\\tilde{\\pmb{W}}(t)(1-2\\eta\\left[\\mathbb{E}_{\\mathbf{x}}(\\pmb{x}\\pmb{x}^{T})+\\sigma(t)^{2}\\pmb{I}\\quad\\mu\\right])+2\\eta\\left[\\mathbb{E}_{\\mathbf{x}}(\\pmb{x}^{T}\\pmb{x})\\quad\\mu\\right]}\\\\ &{\\qquad\\qquad=\\tilde{\\pmb{W}}(t)(1-2\\eta\\pmb{A})+2\\eta\\left[\\mathbb{E}_{\\mathbf{x}}(\\pmb{x}^{T}\\pmb{x})\\quad\\mu\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we define $A:=I-2\\eta\\left[\\mathbb{E}_{x}(\\pmb{x}\\pmb{x}^{T})+\\sigma(t)^{2}I\\quad\\mu\\right]$ for simplicity. By recursively expanding the expression for $\\tilde{W}$ , we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\tilde{W}(t+1)=\\tilde{W}(0)A^{t+1}+2\\eta\\left[\\mathbb{E}_{x}(x^{T}x)\\quad\\mu\\right]\\sum_{i=0}^{t}A^{i}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Notice that there exists a $\\eta$ , such that every eigen value of $\\pmb{A}$ is smaller than 1 and greater than 0. In this case, $\\mathbf{\\nabla}A^{t+1}\\rightarrow\\mathbf{0}$ as $t\\to\\infty$ . Similarly, by the property of matrix geometric series, we have $\\begin{array}{r}{\\sum_{i=0}^{t}A^{i}\\to(I-A)^{-1}}\\end{array}$ . Therefore we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\tilde{W}\\rightarrow\\left[\\mathbb{E}_{x}({x}^{T}{x})\\quad\\mu\\right]\\left[\\mathbb{E}_{x}({x}{x}^{T})+\\sigma(t)^{2}I\\quad\\mu\\right]^{-1}}&{}\\\\ {\\mathbf{\\mu}^{T}}&{1\\Bigg]}\\\\ {=\\left[\\mathbb{E}_{x}({x}^{T}{x})\\quad\\mu\\right]\\left[\\underline{{B}}\\quad\\mu\\right]^{-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where we define $\\pmb{B}:=\\mathbb{E}_{\\pmb{x}}(\\pmb{x}\\pmb{x}^{T})+\\sigma(t)^{2}\\pmb{I}$ for simplicity. By the Sherman\u2013Morrison\u2013Woodbury formula, we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left[{\\begin{array}{c c}{B}&{\\mu}\\\\ {\\mu^{T}}&{1}\\end{array}}\\right]^{-1}=\\left[{\\begin{array}{c c}{(B-\\mu\\mu^{T})^{-1}}&{-(B-\\mu\\mu^{T})^{-1}\\mu}\\\\ {-(1-\\mu^{T}B^{-1}\\mu)^{-1}\\mu^{T}B^{-1}}&{(1-\\mu^{T}B^{-1}\\mu)^{-1}}\\end{array}}\\right].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\tilde{W}\\to\\left[\\mathbb{E}_{x}[x x^{T}](B-\\mu\\mu^{T})^{-1}-\\frac{\\mu\\mu^{T}B^{-1}}{1-\\mu^{T}B^{-1}\\mu}}&{-\\mathbb{E}_{x}[x x^{T}](B-\\mu\\mu^{T})^{-1}\\mu+\\frac{\\mu}{1-\\mu^{T}B^{-1}\\mu}\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "from which we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{W\\rightarrow\\mathbb{E}_{x}[x x^{T}](B-\\mu\\mu^{T})^{-1}-\\displaystyle\\frac{\\mu\\mu^{T}B^{-1}}{1-\\mu^{T}B^{-1}\\mu}}}\\\\ {{{\\displaystyle b\\rightarrow-\\mathbb{E}_{x}[x x^{T}](B-\\mu\\mu^{T})^{-1}\\mu+\\displaystyle\\frac{\\mu}{1-\\mu^{T}B^{-1}\\mu}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Since $\\mathbb{E}_{x}[{\\pmb x}{\\pmb x}^{T}]=\\mathbb{E}_{{\\pmb x}}[({\\pmb x}-{\\pmb\\mu})(({\\pmb x}-{\\pmb\\mu})^{T}]+{\\pmb\\mu}{\\pmb\\mu}^{T}$ , we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\pmb W}={\\pmb\\Sigma}({\\pmb\\Sigma}+\\sigma(t)^{2}{\\pmb I})^{-1}+\\mu{\\pmb\\mu}^{T}({\\pmb B}-\\mu{\\pmb\\mu}^{T})^{-1}-\\frac{{\\pmb\\mu}\\pmb\\mu^{T}{\\pmb B}^{-1}}{1-\\mu^{T}{\\pmb B}^{-1}\\mu}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Applying Sherman-Morrison Formula, we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n(B-\\mu\\mu^{T})^{-1}=B^{-1}+\\frac{B^{-1}\\mu\\mu^{T}B^{-1}}{1-\\mu^{T}B^{-1}\\mu},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "therefore ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mu\\mu^{T}(B-\\mu\\mu^{T})^{-1}-\\cfrac{\\mu\\mu^{T}B^{-1}}{1-\\mu^{T}B^{-1}\\mu}=\\cfrac{\\mu\\mu^{T}B^{-1}\\mu\\mu^{T}B^{-1}}{1-\\mu^{T}B^{-1}\\mu}-\\cfrac{\\mu\\mu^{T}B^{-1}\\mu^{T}B^{-1}\\mu}{1-\\mu^{T}B^{-1}\\mu}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\cfrac{\\mu^{T}B^{-1}\\mu}{1-\\mu^{T}B^{-1}\\mu}(\\mu\\mu^{T}B^{-1}-\\mu\\mu^{T}B^{-1})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=0}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": ", which implies ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{W\\rightarrow\\Sigma(\\Sigma+\\sigma(t)^{2}I)^{-1}}\\\\ {=U\\tilde{\\Lambda}_{\\sigma(t)}U^{T}.\\qquad\\qquad\\qquad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Similarly, we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\pmb{b}\\rightarrow(\\pmb{I}-\\pmb{U}\\tilde{\\pmb{\\Lambda}}_{\\sigma(t)}\\pmb{U}^{T})\\pmb{\\mu}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, gradient descent with a properly chosen learning rate $\\eta$ recovers the Gaussian Denoisers when time goes to infinity. ", "page_idx": 21}, {"type": "text", "text": "E.2 Two Extreme Cases ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Our empirical results indicate that the best linear approximation of $\\mathcal{D}_{\\theta}$ is approximately equivalent to $\\mathcal{D}_{\\mathrm{G}}$ . According to the orthogonality principle [36], this requires $\\mathcal{D}_{\\theta}$ to satisfy: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{x\\sim p_{\\mathrm{data}}(x)}\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}({\\bf0};\\sigma(t)^{2}I)}\\{(\\mathcal{D}_{\\theta}({\\pmb x}+{\\epsilon};\\sigma(t))-({\\pmb x}-{\\pmb\\mu}))({\\pmb x}+{\\epsilon}-{\\pmb\\mu})^{T}\\}\\approx{\\bf0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Notice that (51) does not hold for general denoisers. Two extreme cases for this to hold are: ", "page_idx": 21}, {"type": "text", "text": "\u2022 Case 1 $\\begin{array}{r l}&{:\\mathcal{D}_{\\theta}(\\boldsymbol{x}+\\epsilon;\\sigma(t))\\approx x\\mathrm{\\;for\\;}\\;\\forall x\\sim p_{\\mathrm{data}},\\epsilon\\sim\\mathcal{N}(\\mathbf{0},\\sigma(t)^{2}I).}\\\\ &{:\\mathcal{D}_{\\theta}(\\boldsymbol{x}+\\epsilon;\\sigma(t))\\approx\\mathcal{D}_{\\mathrm{G}}(\\boldsymbol{x}+\\epsilon;\\sigma(t))\\mathrm{\\;for\\;}\\;\\forall x\\sim p_{\\mathrm{data}},\\epsilon\\sim\\mathcal{N}(\\mathbf{0},\\sigma(t)^{2}I).}\\end{array}$ \u2022 Case 2 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Case 1 requires $\\mathcal{D}_{\\theta}(\\mathbf{\\boldsymbol{x}}+\\mathbf{\\boldsymbol{\\epsilon}};\\sigma(t))$ to be the oracle denoiser that perfectly recover the ground truth clean image, which never happens in practice except when $\\sigma(t)$ becomes extremely small. Instead, our empirical results suggest diffusion models in the generalization regime bias towards Case 2, where deep networks learn $\\mathcal{D}_{\\theta}$ that approximate (not equal) to $\\mathcal{D}_{\\mathrm{G}}$ . This is evidenced in Figure 5(b), where diffusion models trained on larger datasets (35000 and 7000 images) produce denoising outputs similar to $\\mathcal{D}_{\\mathrm{G}}$ . Notice that this similarity holds even when the denoisers take pure Gaussian noise as input. The exact mechanism driving diffusion models trained with gradient descent towards this particular solution remains an open question and we leave it as future work. ", "page_idx": 22}, {"type": "text", "text": "F More Discussion on Section 4 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "While in Section 4 we mainly focus on the discussion of the behavior of diffusion denoisers in the intermediate-noise regime, in this section we study the denoiser dynamics in both low and high-noise regime. We also provide additional experiment results on CIFAR-10 dataset. ", "page_idx": 22}, {"type": "text", "text": "F.1 Behaviors in Low-noise Regime ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We visualize the score differences between $\\mathcal{D}_{\\mathrm{G}}$ and $\\mathcal{D}_{\\theta}$ in low-noise regime in Figure 16. The left figure demonstrates that when the dataset size becomes smaller than a certain threshold, the score difference at $\\sigma=0$ remains persistently non-zero. Moreover, the right figure shows that this difference depends solely on dataset size rather than model capacity. This phenomenon arises from two key factors: (i) $\\mathcal{D}_{\\theta}$ converges to the identity mapping at low noise levels, independent of training dataset size and model capacity, and (ii) $\\mathcal{D}_{\\mathrm{G}}$ approximates the identity mapping at low noise levels only when the empirical covariance matrix is full-rank, as can be seen from (6). ", "page_idx": 22}, {"type": "text", "text": "Since the rank of the covariance matrix is upper-bounded by the training dataset size, $\\mathcal{D}_{\\mathrm{G}}$ differs from the identity mapping when the dataset size is smaller than the data dimension. This creates a persistent gap between $\\mathcal{D}_{\\mathrm{G}}$ and $\\mathcal{D}_{\\theta}$ , with smaller datasets leading to lower rank and consequently larger score differences. These observations align with our discussion in Appendix D. ", "page_idx": 22}, {"type": "text", "text": "F.2 Behaviors in High-noise Regime ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "As shown in Figure 7(a), while a decreased model scale pushes $\\mathcal{D}_{\\theta}$ in the intermediate noise region towards $\\mathcal{D}_{\\mathrm{G}}$ , their differences enlarges in the high noise variance regime. This phenomenon arises because diffusion models employ a bell-shaped noise sampling distribution that prioritizes intermediate noise levels, resulting in insufficient training at high noise variances. A shown in Figure 17, for high $\\sigma(t)$ , $\\mathcal{D}_{\\theta}$ converge to $\\mathcal{D}_{\\mathrm{G}}$ when trained with sufficient model capacity (Figure 17(b)) and training time (Figure 17(c)). This behavior is consistent irrespective of the training dataset sizes (Figure 17(a)). Convergence in the high-noise variance regime is less crucial in practice, since diffusion steps in ", "page_idx": 22}, {"type": "image", "img_path": "Sk2duBGvrK/tmp/a64089b33dea4b45a1214e7310c9d30e8490e37036217511bea6efbe9bdaca3a.jpg", "img_caption": ["Figure 16: Score differences for low-noise variances. The left and right figures are the zoomed-in views of Figure 6(a) and Figure 7(a) respectively. Notice that when the dataset size is smaller than the dimension of the image, the score differences are always non-zero at $\\sigma=0$ . "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "Denoising Outputs  for $\\sigma(t)=60\\;(\\mathsf{P S N R}=-29.5\\;\\mathrm{dB})$ ", "text_level": 1, "page_idx": 23}, {"type": "image", "img_path": "Sk2duBGvrK/tmp/16accee9226ef3ffad478293842eeb6da6cc9979f26c9de5fe38df344d1a6fe2.jpg", "img_caption": ["Figure 17: $\\mathcal{D}_{\\theta}$ converge to $\\mathcal{D}_{\\mathrm{G}}$ with no overfitting for high noise variances. Figure(a) shows the denoising outputs of $\\mathcal{D}_{\\mathrm{M}}$ , $\\mathcal{D}_{\\mathrm{G}}$ and well-trained (trained with sufficient model capacity till convergence) $\\mathcal{D}_{\\theta}$ . Notice that at high noise variance, the three different denoisers are approximately equivalent despite the training dataset size. Figure(b) shows the denoising outputs of $\\mathcal{D}_{\\theta}$ with different model scales trained until convergence. Notice that $\\mathcal{D}_{\\theta}$ converges to $\\mathcal{D}_{\\mathrm{G}}$ only when the model capacity is large enough. Figure(c) shows the denoising outputs of $\\mathcal{D}_{\\theta}$ with sufficient large model capacity at different training epochs. Notice that $\\mathcal{D}_{\\theta}$ converges to $\\mathcal{D}_{\\mathrm{G}}$ only when the training duration is long enough. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "Sk2duBGvrK/tmp/8caff86c1aa4faff3eaea6f815be48dbbd553047c2ade5c9dbe3e97e225631a2.jpg", "img_caption": ["Figure 18: Denoising outputs of $\\mathcal{D}_{\\mathrm{G}}$ and $\\mathcal{D}_{\\theta}$ at $\\sigma=4.$ . Figure(a) shows the clean image $\\textbf{\\em x}$ (from test set), random noise $\\epsilon$ and the resulting noisy image $\\textit{\\textbf{y}}$ . Figure(b) compares denoising outputs of $\\mathcal{D}_{\\theta}$ across different channel sizes [4, 8, 64, 128] with those of $\\mathcal{D}_{\\mathrm{G}}$ . Figure(c) shows the evolution of $\\mathcal{D}_{\\theta}$ outputs at training epochs [187, 841, 9173, 64210] alongside $\\mathcal{D}_{\\mathrm{G}}$ outputs. All models are trained on a fixed dataset of 1,094 images. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "this regime contribute substantially less than those in the intermediate-noise variance regime\u2014a phenomenon we analyze further in Appendix G.5. ", "page_idx": 23}, {"type": "text", "text": "F.3 Similarity between Diffusion Denoiers and Gaussian Denoisers ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In Section 4, we demonstrate that the Gaussian inductive bias is most prominent in models with limited capacity and during early training stages, a finding qualitatively validated in Figure 18. Specifically, Figure 18(b) shows that larger models (channel sizes 128 and 64) tend to memorize, directly retrieving training data as denoising outputs. In contrast, smaller models (channel sizes 8 and 4) exhibit behavior similar to $\\mathcal{D}_{\\mathrm{G}}$ , producing comparable denoising outputs. Similarly, Figure 18(c) reveals that during early training epochs (0-841), $\\mathcal{D}_{\\theta}$ outputs progressively align with those of $\\mathcal{D}_{\\mathrm{G}}$ . However, extended training beyond this point leads to memorization. ", "page_idx": 23}, {"type": "image", "img_path": "Sk2duBGvrK/tmp/b3ba444db415bd5c2348544c31a1558866fb4c7868f568db6d9064272d10375f.jpg", "img_caption": ["Figure 19: Large dataset size prompts the Gaussian structure. Models with the same scale (channel size 64) are trained on CIFAR-10 datasets with varying sizes. Figure(a) shows that larger dataset size leads to increased similarity between $\\mathcal{D}_{\\mathrm{G}}$ and $\\mathcal{D}_{\\theta}$ , resulting in structurally similar generated images as shown in Figure(b). "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "Sk2duBGvrK/tmp/af3b9cd4d2c07152e378841b338a5e8edc090401a8a3ef88c4e7a33f4f89e0c1.jpg", "img_caption": ["Figure 20: Smaller model scale prompts the Gaussian structure. Models with varying scales are trained on a fixed CIFAR-10 datasets with 782 images. Figure(a) shows that smaller model scale leads to increased similarity between $\\mathcal{D}_{\\mathrm{G}}$ and $\\mathcal{D}_{\\theta}$ in the intermediate noise regime $(\\sigma\\in[0.1,10])$ ), resulting in structurally similar generated images as shown in figure(b). However, smaller scale leads to larger score differences in high-noise regime due to insufficient training from limited model capacity. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "F.4 CIFAR-10 Results ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "The effects of model capacity and training duration on the Gaussian inductive bias, as demonstrated in Figures 19 to 21, extend to the CIFAR-10 dataset. These results confirm our findings from Section 4: the Gaussian inductive bias is most prominent when model scale and training duration are limited. ", "page_idx": 24}, {"type": "text", "text": "G Additional Experiment Results ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "While in the main text we mainly demonstrate our findings using EDM-VE diffusion models trained on FFHQ, in this section we show our results are robust and extend to various model architectures and datasets. Furthermore, we demonstrate that the Gaussian inductive bias is not unique to diffusion models, but it is a fundamental property of denoising autoencoders [37]. Lastly, we verify that our conclusions remain consistent when using alternative metrics such as NMSE instead of the RMSE used in the main text. ", "page_idx": 24}, {"type": "image", "img_path": "Sk2duBGvrK/tmp/a1bda07a6e9e196b131f937a00faf507d792bbaa7414ac60b6830e15b3646c98.jpg", "img_caption": ["Figure 21: Diffusion model learns the Gaussian structure in early training epochs. Models with the same scale (channel size 128) are trained on a fixed CIFAR-10 datasets with 782 images. Figure(a) shows that the similarity between $\\mathcal{D}_{\\mathrm{G}}$ and $\\mathcal{D}_{\\theta}$ progressively increases during early training epochs (0-921) in the intermediate noise regime $(\\sigma\\in[0.1,10])$ , resulting in structurally similar generated images as shown in figure(b). However, continue training beyond this point results in diverged $\\mathcal{D}_{\\mathrm{G}}$ and $\\mathcal{D}_{\\theta}$ , resulting in memorization. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "Sk2duBGvrK/tmp/e1bbe8cd8d6478cea8885e20ccea2ea61c2f991700fb0c50608ebb088e35df6e.jpg", "img_caption": ["Figure 22: Linear model shares similar function mapping with Gaussian model. The figures demonstrate the evolution of normalized MSE between the linear weights $\\mathcal{D}_{\\mathrm{L}}$ and the Gaussian weights $\\mathcal{D}_{\\mathrm{G}}$ w.r.t. linear distillation training epochs. Figures(a), (b) and (c) correspond to diffusion models trained on FFHQ, with EDM-VE, EDM-ADM and EDM-VP network architectures specified in [4] respectively. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "G.1 Gaussian Structure Emerges across Various Network Architectures ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We first demonstrate that diffusion models capture the Gaussian structure of the training dataset, irrespective of the deep network architectures used. As shown in Figure 22 (a), (b), and (c), although the actual diffusion models, $\\mathcal{D}_{\\theta}$ , are parameterized with different architectures, for all noise variances except $\\sigma(t)\\in\\{0.002,80.0\\}$ , their corresponding linear models, $\\mathcal{D}_{\\mathrm{L}}$ , consistently converge towards the common Gaussian models, $\\mathcal{D}_{\\mathrm{G}}$ , determined by the training dataset. Qualitatively, as depicted in Figure 23, despite variations in network architectures, diffusion models generate nearly identical images, matching those generated from the Gaussian models. ", "page_idx": 25}, {"type": "text", "text": "G.2 Gaussian Inductive Bias as a General Property of DAEs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In previous sections, we explored the properties of diffusion models by interpreting them as collections of deep denoisers, which are equivalent to the denoising autoencoders (DAEs) [37] trained on various noise variances by minimizing the denoising score matching objective (3). Although diffusion models and DAEs are equivalent in the sense that both of them are trying to learn the score function of the noise-mollified data distribution [38], the training objective of diffusion models is more complex [4]: ", "page_idx": 25}, {"type": "image", "img_path": "Sk2duBGvrK/tmp/46f8db7a60f6fe7149c3fa5d87832c2e6062e5c7a9bf77ec228741ae650a4fa6.jpg", "img_caption": ["Figure 23: Images sampled from various model.The figure shows the sampled images from diffusion models with different network architectures and those from their corresponding Gaussian models. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\mathbb{E}_{x,\\epsilon,\\sigma}[\\lambda(\\sigma)c_{\\mathrm{out}}(\\sigma)^{2}||\\mathcal{F}_{\\theta}(x+\\epsilon,\\sigma)-\\frac{1}{c_{\\mathrm{out}}(\\sigma)}(x-c_{\\mathrm{skip}}(\\sigma)(x+\\epsilon))\\,||_{2}^{2}],\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\mathbf{\\mathcal{x}}\\sim p_{\\mathrm{data}},\\epsilon\\sim\\mathcal{N}(\\mathbf{0},\\sigma(t)^{2}I)$ and $\\sigma\\sim p_{\\mathrm{train}}$ . Notice that the training objective of diffusion models has a few distinct characteristics: ", "page_idx": 26}, {"type": "text", "text": "\u2022 Diffusion models use a single deep network $\\mathcal{F}_{\\theta}$ to perform denoising score matching across all noise variances while DAEs are typically trained separately for each noise level.   \n\u2022 Diffusion models are not trained uniformly across all noise variances. Instead, during training the probability of sampling a given noise level $\\sigma$ is controlled by a predefined distribution $p_{\\mathrm{train}}$ and the loss is weighted by $\\lambda(\\sigma)$ .   \n\u2022 Diffusion models often utilize special parameterizations (13). Therefore, the deep network $\\mathcal{F}_{\\theta}$ is trained to predict a linear combination of the clean image $\\textbf{\\em x}$ and the noise $\\epsilon$ , whereas DAEs typically predict the clean image directly. ", "page_idx": 26}, {"type": "text", "text": "Given these differences, we investigate whether the Gaussian inductive bias is unique to diffusion models or a general characteristic of DAEs. To this end, we train separate DAEs (deep denoisers) using the vanilla denoising score matching objective (3) on each of the 10 discrete noise variances specified by the EDM schedule [80.0, 42.415, 21.108, 9.723, 4.06, 1.501, 0.469, 0.116, 0.020, 0.002], and compare the score differences between them and the corresponding Gaussian denoisers $\\mathcal{D}_{\\mathrm{G}}$ . We use no special parameterization so that $\\mathcal{D}_{\\theta}=\\mathcal{F}_{\\theta}$ ; that is, the deep network directly predicts the clean image. Furthermore, the DAEs for each noise variance are trained till convergence, ensuring all noise levels are trained sufficiently. We consider the following architectural choices: ", "page_idx": 26}, {"type": "text", "text": "\u2022 DAE-NCSN: In this setting, the network $\\mathcal{F}_{\\theta}$ uses the NCSN architecture [3], the same as that used in the EDM-VE diffusion model.   \n\u2022 DAE-Skip: In this setting, $\\mathcal{F}_{\\theta}$ is a U-Net [39] consisting of convolutional layers, batch normalization [40], leaky ReLU activation [41] and convolutional skip connections. We refer to this network as \u201dSkip-Net\u201d. Compared to NCSN, which adapts the state of the art architecture designs, Skip-Net is deliberately constructed to be as simple as possible to test how architectural complexity affects the Gaussian inductive bias.   \n\u2022 DAE-DiT: In this setting, $\\mathcal{F}_{\\theta}$ is a Diffusion Transformer (DiT) introduced in [42]. Vision Transformers are known to lack inductive biases such as locality and translation equivariance that are inherent to convolutional models [43]. Here we are interested in if this affects the Gaussian inductive bias. ", "page_idx": 26}, {"type": "image", "img_path": "Sk2duBGvrK/tmp/24683a5cbd17892e095c5a70635eed77d5a61d611d09d25922e663067b9f8163.jpg", "img_caption": ["Figure 24: Comparison between DAEs and diffusion models. Figure(a) compares the score field approximation error between Gaussian models and both $(i)$ diffusion models (EDM vs. Gaussian) and $(i i)$ DAEs with varying architectures. Figure(b) illustrates the generation trajectories of different models initialized from the same noise input. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "\u2022 DAE-Linear: In this setting we set $\\mathcal{F}_{\\theta}$ to be a linear model with a bias term as in (8). According to Theorem 1, these models should converge to Gaussian denoisers. ", "page_idx": 27}, {"type": "text", "text": "The quantitative results are shown in Figure 24(a). First, the DAE-linear models well approximate $\\mathcal{D}_{\\mathrm{G}}$ across all 10 discrete steps (RMSE smaller than 0.04), consistent with Theorem 1. Second, despite the differences between diffusion models (EDM) and DAEs, they achieve similar score approximation errors relative to $\\mathcal{D}_{\\mathrm{G}}$ for most noise variances, meaning that they can be similarly approximated by $\\mathcal{D}_{\\mathrm{G}}$ . However, diffusion models exhibit significantly larger deviations from $\\mathcal{D}_{\\mathrm{G}}$ at higher noise variances $(\\sigma\\in\\{42.415,80.0\\})$ since they utilize a bell-shaped noise sampling distribution $p_{\\mathrm{train}}$ that emphasizes training on intermediate noise levels, leading to under-training at high noise variances. Lastly, the DAEs with different architectures achieve comparable score approximation errors, and both DAEs and diffusion models generate images matching those from the Gaussian model, as shown in Figure 24(b). These findings demonstrate that the Gaussian inductive bias is not unique to diffusion models or specific architectures but is a fundamental property of DAEs. ", "page_idx": 27}, {"type": "text", "text": "G.3 Gaussian Structure Emerges across Various datasets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "As illustrated in Figure 25, for diffusion models trained on the CIFAR-10, AFHQ and LSUN-Churches datasets that are in the generalization regime, their generated samples match those produced by the corresponding Gaussian models. Additionally, their linear approximations, $\\mathcal{D}_{\\mathrm{L}}$ , obtained through linear distillation, align closely with the Gaussian models, $\\mathcal{D}_{\\mathrm{G}}$ , resulting in nearly identical generated images. These findings confirm that the Gaussian structure is prevalent across various datasets. ", "page_idx": 27}, {"type": "text", "text": "G.4 Strong Generalization on CIFAR-10 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Figure 26 demonstrates the strong generalization effect on CIFAR-10. Similar to the observations in Section 5, reducing model capacity or early stopping the training process prompts the Gaussian inductive bias, leading to generalization. ", "page_idx": 27}, {"type": "text", "text": "G.5 Measuring Score Approximation Error with NMSE ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "While in Section 3.1 we define the score field approximation error between denoisers $\\mathcal{D}_{1}$ and $\\mathcal{D}_{2}$ with RMSE ( (10)), this error can also be quantified using NMSE: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathrm{Score-Difference}(t):=\\mathbb{E}_{\\mathbf{x}\\sim p_{\\mathrm{data}}(\\mathbf{x}),\\epsilon\\sim\\mathcal{N}(\\mathbf{0};\\sigma(t)^{2}I)}\\frac{||\\mathcal{D}_{1}(\\mathbf{x}+\\epsilon)-\\mathcal{D}_{2}(\\mathbf{x}+\\epsilon)||_{2}}{||\\mathcal{D}_{1}(\\mathbf{x}+\\epsilon)||_{2}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "As shown in Figure 27, while the trend in intermediate-noise and low-noise regimes remains unchanged, NMSE amplifies differences in the high-noise variance regime compared to RMSE. This amplified score difference between $\\mathcal{D}_{\\mathrm{G}}$ and $\\mathcal{D}_{\\theta}$ does not contradict our main finding that diffusion models in the generalization regime exhibit an inductive bias towards learning denoisers approximately equivalent to $\\mathcal{D}_{\\mathrm{G}}$ in the high-noise variance regime. As discussed in Section 3.2 and appendices F.2 and G.2, this large score difference stems from inadequate training in this regime. ", "page_idx": 27}, {"type": "image", "img_path": "Sk2duBGvrK/tmp/846c9737fb5270c1929cb1f22ce2fcadc07a14c3dca850d385847d085c6fa12b.jpg", "img_caption": ["Figure 25: Final generated images and sampling trajectories for various models. Figures(a), (c) and (e) demonstrate the images generated using different models starting from the same noises for LSUN-Churches, AFHQ and CIFAR-10 respectively. Figures(b), (d) and (f) demonstrate the corresponding sampling trajectories. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "Figure 27 (Gaussian vs. DAE) demonstrates that when DAEs are sufficiently trained at specific noise variances, they still converge to $\\mathcal{D}_{\\mathrm{G}}$ . Importantly, the insufficient training in the high-noise variance regime minimally affects final generation quality. Figure 25(f) shows that while the diffusion model (EDM) produces noisy trajectories at early timesteps $(\\sigma\\in\\{80.0,42.415\\})$ , these artifacts quickly disappear in later stages, indicating that the Gaussian inductive bias is most influential in the intermediate-noise variance regime. ", "page_idx": 28}, {"type": "text", "text": "Notably, even when $\\mathcal{D}_{\\theta}$ are inadequately trained in the high-noise variance regime, they remain approximable by linear functions, though these functions no longer match $\\mathcal{D}_{\\mathrm{G}}$ . ", "page_idx": 28}, {"type": "image", "img_path": "Sk2duBGvrK/tmp/4fa0b39a389ed59ff5bfaf7ea7a2fe429fe795f18ca7ffa0ce2cc079fe3e42ed.jpg", "img_caption": ["Figure 26: Strong generalization on CIFAR-10 dataset. Figure(a) Top: Generated images of Gausisan models; Bottom: Generated images of diffusion models, with model scale 64; S1 and S2 each has 25000 non-overlapping images. Figure(b) Top: Generated images of Gausisan model; Bottom: Generated images of diffusion models in the memorization regime, with model scale 128; S1 and S2 each has 782 non-overlapping images. Figure(c): Early stopping and reducing model capacity help transition diffusion models from memorization to generalization. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "Sk2duBGvrK/tmp/b8f0e0c24d7328ca2cc14414e15d495016bfe30e1259609b9807439485bfb8be.jpg", "img_caption": ["Figure 27: Comparison between RMSE and NMSE score differences. Figures(a) and (c) show the score field approximation errors measured with RMSE loss while figures(b) and (d) show these errors measured using NMSE loss. Compared to RMSE, the NMSE metric highlight the score differences in the high-noise regime, where diffusion models receive the least training. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "H Discussion on Geometry-Adaptive Harmonic Bases ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "H.1 GAHB only Partially Explain the Strong Generalization ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Recent work [20] observes that diffusion models trained on sufficiently large non-overlapping datasets (of the same class) generate nearly identical images. They explain this \u201dstrong generalization\u201d phenomenon by analyzing bias-free deep diffusion denoisers with piecewise linear input-output ", "page_idx": 29}, {"type": "text", "text": "mappings: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{D}(\\pmb{x}_{t};\\sigma(t))=\\nabla\\mathcal{D}(\\pmb{x}_{t};\\sigma(t))\\pmb{x}}\\\\ &{\\quad\\quad\\quad\\quad=\\displaystyle\\sum_{k}\\lambda_{k}(\\pmb{x}_{t})\\pmb{u}_{k}(\\pmb{x}_{t})\\pmb{v}_{k}^{T}(\\pmb{x}_{t})\\pmb{x}_{t},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\lambda_{k}({\\pmb x}_{t}),\\,{\\pmb u}_{k}({\\pmb x}_{t})$ , and ${\\pmb v}_{k}({\\pmb x}_{t})$ represent the input-dependent singular values, left and right singular vectors of the network Jacobian $\\nabla\\mathcal{D}(\\mathbf{\\boldsymbol{x}}_{t};\\sigma(t))$ . Under this framework, strong generalization occurs when two denoisers $\\mathcal{D}_{1}$ and $\\mathcal{D}_{2}$ have similar Jacobians: $\\nabla\\mathcal{D}_{1}(\\mathbf{x}_{t};\\sigma(t))\\approx\\overleftarrow{\\nabla}\\mathcal{D}_{2}(\\mathbf{x}_{t};\\sigma(t))$ . The authors conjecture this similarity arises from networks\u2019 inductive bias towards learning certain optimal $\\nabla\\mathcal{D}(\\mathbf{\\boldsymbol{x}}_{t};\\sigma(t))$ that has sparse singular values and the singular vectors of which are the geometry-adaptive harmonic bases (GAHB)\u2014near-optimal denoising bases that adapt to input $\\pmb{x}_{t}$ . ", "page_idx": 30}, {"type": "text", "text": "While [20] provides valuable insights, their bias-free assumption does not reflect real-world diffusion models, which inherently contain bias terms. For feed forward ReLU networks, the denoisers are piecewise affine: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{D}(\\pmb{x}_{t};\\sigma(t))=\\nabla\\mathcal{D}(\\pmb{x}_{t};\\sigma(t))\\pmb{x}_{t}+\\pmb{b}_{\\pmb{x}_{t}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where $\\pmb{b}_{\\pmb{x}_{t}}$ is the network bias that depends on both network parameterization and the noisy input $\\pmb{x}_{t}$ [44]. Here, similar Jacobians alone cannot explain strong generalization, as networks may differ significantly in $\\pmb{b}_{\\pmb{x}_{t}}$ . For more complex network architectures where even piecewise affinity fails, we consider the local linear expansion of $\\mathcal{D}(\\mathbf{\\boldsymbol{x}}_{t};\\boldsymbol{\\sigma}(t))$ : ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{D}(\\pmb{x}_{t}+\\Delta\\pmb{x};\\sigma(t))=\\nabla\\mathcal{D}(\\pmb{x}_{t};\\sigma(t))\\Delta\\pmb{x}_{t}+\\mathcal{D}(\\pmb{x}_{t};\\sigma(t)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "which approximately holds for small perturbation $\\Delta x$ . Thus, although $\\nabla\\mathcal{D}(\\mathbf{\\boldsymbol{x}}_{t};\\sigma(t))$ characterizes $\\mathcal{D}(\\mathbf{\\boldsymbol{x}}_{t};\\bar{\\boldsymbol{\\sigma}}(\\bar{t}))$ \u2019s local behavior around $\\pmb{x}_{t}$ , it does not provide sufficient information on the global properties. ", "page_idx": 30}, {"type": "text", "text": "Our work instead examines global behavior, demonstrating that $\\mathcal{D}(\\mathbf{\\boldsymbol{x}}_{t};\\boldsymbol{\\sigma}(t))$ is close to $\\mathcal{D}_{\\mathrm{{G}}}(\\mathbf{\\boldsymbol{x}}_{t};\\sigma(t))$ \u2014the optimal linear denoiser under the Gaussian data assumption. This implies that strong generalization partially stems from networks learning similar Gaussian structures across non-overlapping datasets of the same class. Since our linear model captures global properties but not local characteristics, it complements the local analysis in [20]. ", "page_idx": 30}, {"type": "text", "text": "H.2 GAHB Emerge only in Intermediate-Noise Regime ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "For completeness, we study the evolution of the Jacobian matrix $\\nabla\\mathcal{D}(\\mathbf{\\boldsymbol{x}}_{t};\\sigma(t))$ across various noise levels $\\sigma(t)$ . The results are presented in Figures 28 and 29, which reveal three distinct regimes: ", "page_idx": 30}, {"type": "text", "text": "\u2022 High-noise regime $I I{\\boldsymbol{0}},\\!80\\boldsymbol{J}$ . In this regime, the leading singular vectors6 of the Jacobian matrix $\\nabla\\mathcal{D}(\\mathbf{\\boldsymbol{x}}_{t};\\sigma(t))$ well align with those of the Gaussian weights (the leading principle components of the training dataset), consistent with our finding that diffusion denoisers approximate linear Gaussian denoisers in this regime. Notice that DAEs trained sufficiently on separate noise levels (Figure 29) show stronger alignment compared to vanilla diffusion models (Figure 28), which suffer from insufficient training at high noise levels.   \n\u2022 Intermediate-noise regime [0.1,10]: In this regime, GAHB emerge as singular vectors of $\\nabla\\mathcal{D}(\\mathbf{\\boldsymbol{x}}_{t};\\sigma(t))$ diverge from the principal components, becoming increasingly adaptive to the geometry of input image.   \n\u2022 Low-noise regime [0.002,0.1]. In this regime, the leading singular vectors of $\\nabla\\mathcal{D}(\\mathbf{\\boldsymbol{x}}_{t};\\sigma(t))$ show no clear patterns, consistent with our observation that diffusion denoisers approach the identical mapping, which has unconstrained singular vectors. ", "page_idx": 30}, {"type": "text", "text": "Notice that the leading singular vectors of $\\nabla\\mathcal{D}(\\mathbf{\\boldsymbol{x}}_{t};\\sigma(t))$ are the input directions that lead to the maximum variation in denoised outputs, thus revealing meaningful information on the local properties of $\\mathcal{D}(\\mathbf{\\boldsymbol{x}}_{t};\\boldsymbol{\\sigma}(t))$ at $\\pmb{x}_{t}$ . As demonstrated in Figure 30, perturbing input $\\pmb{x}_{t}$ along these vectors at difference noise regimes leads to distinct effects on the final generated images: $(i)$ in the high-noise regime where the leading singular vectors align with the principle components of the training dataset, perturbing $\\pmb{x}_{t}$ along these directions leads to canonical changes such as image class, $(i i)$ in the intermediate-noise regime where the GAHB emerge, perturbing $\\pmb{x}_{t}$ along the leading singular vectors modify image details such as colors while preserving overall image structure and $(i i i)$ in the low-noise regime where the leading singular vectors have no significant pattern, perturbing $\\pmb{x}_{t}$ along these directions yield no meaningful semantic changes. ", "page_idx": 30}, {"type": "image", "img_path": "Sk2duBGvrK/tmp/7ff6b8bd0a5396ca5ef99dfc4c0be1f94d2bb2a3f6e9446b0e76be951ebdc104.jpg", "img_caption": ["Figure 28: Evolution of $\\nabla\\mathcal{D}(\\mathbf{\\boldsymbol{x}}_{t};\\sigma(t))$ across varying noise levels. Figure(a) shows the generation trajectory. Figure(b) shows the correlation matrix between Jacobian singular vectors $\\bar{U}({\\bf{\\boldsymbol{x}}}_{t})$ and training dataset principal components $U$ . Notice that the leading singular vectors of ${\\bf\\nabla}U({\\bf x}_{t})$ and $U$ well align in early timesteps but diverge in later timesteps. Figure(c) shows the first three principle components of the training dataset while figures(d-f) show the evolution of Jacobian\u2019s first three singular vectors across noise levels. These singular vectors initially match the principle components but progressively adapt to input image geometry, before losing distinct patterns at very low noise levels. While we present only left singular vectors, right singular vectors exhibit nearly identical behavior and yield equivalent results. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "These results collectively demonstrate that the singular vectors of the network Jacobian $\\nabla\\mathcal{D}(\\mathbf{\\boldsymbol{x}}_{t};\\sigma(t))$ have distinct properties at different noise regimes, with GAHB emerging specifically in the intermediate regime. This characterization has significant implications for uncertainty quantification [45] and image editing [46]. ", "page_idx": 31}, {"type": "image", "img_path": "Sk2duBGvrK/tmp/bdb2676c1e56130198b6427b0c16eda74f153e3f98f2bcab424157010117c46d.jpg", "img_caption": ["Figure 29: Evolution of $\\nabla\\mathcal{D}(\\mathbf{\\boldsymbol{x}}_{t};\\sigma(t))$ across varying noise levels for DAEs. We repeat the experiments in Figure 28 on DAEs that are sufficiently trained on each discrete noise levels. Notice that with sufficient training, the Jacobian singular vectors $U({\\boldsymbol{x}}_{t})$ show a better alignment with principle components $U$ in early timesteps. "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "Sk2duBGvrK/tmp/18995eb040f5935623ffb59326ba8286de64122395fdbaa1a386b6136d5214b3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "Figure 30: Effects of perturbing $\\pmb{x}_{t}$ along Jacobian singular vectors. Figure(a)-(c) demonstrate the effects of perturbing input $\\pmb{x}_{t}$ along the first singular vector of the Jacobian matrix $(\\pmb{x}_{t}\\pmb{\\pm\\lambda}\\pmb{u}_{1}(\\pmb{x}_{t}))$ on the final generated images. Perturbing $\\pmb{x}_{t}$ in high-noise regime (Figure (a)) leads to canonical image changes while perturbation in intermediate-noise regime (Figure (b)) leads to change in details but the overall image structure is preserved. At very low noise variances, perturbation has no significant effect (Figure (c)). Similar effects are observed in concurrent work [46]. ", "page_idx": 32}, {"type": "text", "text": "I Computing Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "All the diffusion models in the experiments are trained on A100 GPUs provided by NCSA Delta GPU [33]. ", "page_idx": 32}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We mention the main contribution in both the abstract and introduction section, listed in the \u201dContributions\u201d. ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 33}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Justification: We discuss the limitations in the \u201dDiscussion\u201d section ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 33}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We provide the assumption and proof in Section 3.3 and Appendix E. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 34}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We include the experiment details in Section 3, Section 4 and Section 5. We will also release our code upon publication. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 34}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 35}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We include the experiment details in Section 3, Section 4 and Section 5. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 35}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: For various plots we specify that the numbers are calculated over 100 random samples. However, since the standard deviations are quite small, the error bar is unnecessary. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We include the computating resources in Appendix I. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 36}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our research doesn\u2019t have potential harm cause by research process and negative societal impact. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 36}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: Our work is an empirical and theoretical work on the inductive bias of the diffusion models. It has more scientific contributions rather than societal impacts. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: Our work is an empirical and theoretical work on how the diffusion model learns the data distribution. It doesn\u2019t have a high risk of misuse. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 37}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We use the public released dataset and models, which properly credited the license. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 37}, {"type": "text", "text": "", "page_idx": 38}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: Our work is an empirical and theoretical work on how the diffusion model learns the data distribution. We don\u2019t release new assets. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 38}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 38}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 38}]