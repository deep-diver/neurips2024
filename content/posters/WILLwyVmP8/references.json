{"references": [{"fullname_first_author": "Riccardo Guidotti", "paper_title": "A survey of methods for explaining black box models", "publication_date": "2018-00-00", "reason": "This paper provides a comprehensive overview of explainable AI (XAI) methods, which is fundamental to the current research on interpretable models like CMR."}, {"fullname_first_author": "Amina Adadi", "paper_title": "Peeking inside the black-box: a survey on explainable artificial intelligence (xai)", "publication_date": "2018-00-00", "reason": "This survey offers a broad perspective on XAI, covering its concepts, taxonomies, and challenges, which are relevant to the goals of making AI models more interpretable and trustworthy."}, {"fullname_first_author": "Pang Wei Koh", "paper_title": "Concept bottleneck models", "publication_date": "2020-00-00", "reason": "This paper introduces concept bottleneck models (CBNMs), a significant innovation in XAI that directly addresses the interpretability challenges tackled by CMR."}, {"fullname_first_author": "Cynthia Rudin", "paper_title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead", "publication_date": "2019-00-00", "reason": "This paper advocates for the use of interpretable models over black-box models, particularly in high-stakes decision-making, which is the core motivation behind the research of concept-based methods and CMR."}, {"fullname_first_author": "Pietro Barbiero", "paper_title": "Interpretable neural-symbolic concept reasoning", "publication_date": "2023-00-00", "reason": "This paper introduces Deep Concept Reasoner (DCR), a state-of-the-art CBM that is directly compared with CMR, highlighting the advancements CMR brings to the field."}]}