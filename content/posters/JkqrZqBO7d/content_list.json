[{"type": "text", "text": "PETRA: Parallel End-to-end Training with Reversible Architectures ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Reversible architectures have been shown to be capable of performing on par with   \n2 their non-reversible architectures, being applied in deep learning for memory sav  \n3 ings and generative modeling. In this work, we show how reversible architectures   \n4 can solve challenges in parallelizing deep model training. We introduce PETRA,   \n5 a novel alternative to backpropagation for parallelizing gradient computations.   \n6 PETRA facilitates effective model parallelism by enabling stages (i.e., a set of   \n7 layers) to compute independently on different devices, while only needing to com  \n8 municate activations and gradients between each other. By decoupling the forward   \n9 and backward passes and keeping a single updated version of the parameters, the   \n0 need for weight stashing is also removed. We develop a custom autograd-like   \n11 training framework for PETRA, and we demonstrate its effectiveness on CIFAR  \n2 10, ImageNet32, and ImageNet, achieving competitive accuracies comparable to   \n13 backpropagation using ResNet-18, ResNet-34, and ResNet-50 models. ", "page_idx": 0}, {"type": "text", "text": "14 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "15 First-order methods using stochastic gradients computed via backpropagation on mini-batches are the   \n16 de-facto standard for computing parameter updates in Deep Neural Networks [25]. As datasets and   \n17 models continue to grow [1] there is an urgent need for memory-efficient and scalable parallelization   \n18 of deep learning training across multiple workers. Data parallelism via mini-batches [25] has been   \n19 widely adopted in deep learning frameworks [26]. This approach computes gradients across model   \n20 replicas distributed among workers, yet it requires frequent synchronization to aggregate gradients,   \n21 leading to high communication costs, as well as substantial memory redundancy. Furthermore, with   \n22 the increasing size and scale of models exceeding that of the growth of on-device memory, the   \n23 forward and backward passes now often exceed a single device\u2019s memory capacity [35]. To further   \n24 address these issues, methods have attempted to mitigate this memory overhead and to parallelize   \n25 the sequential backpropagation steps themselves across devices, while computing exact gradients.   \n26 Techniques like optimizer sharding [34], tensor parallelism [36], activation checkpointing [6], or   \n2 pipelining [15], have been deployed individually or combined, leading for instance to the development   \n28 of 3D parallelism [37], a popular methodology which improves the efficiency of the backpropagation   \n29 implementation. On the other hand, the fundamental inefficiency underlying the parallelization of   \n30 backpropagation has not been addressed by these methods.   \n31 However, the use of exact gradient restricts algorithmic choices and parallel implementations, as   \n32 highlighted by [20]. For instance, backpropagation is backward locked: the inputs of each layer   \n33 must be propagated through the network and preserved until an error signal is retropropagated to the   \n34 layer of origin. This requirement enforces a synchronous dependency among subsequent layers and   \n35 requires them to systematically store intermediary activations, potentially impeding overall resource   \n36 efficiency as workers must wait for each other to continue their computations and release memory   \n37 used for activations. To unlock the potential of backpropagation, inexact backpropagation procedures   \n38 have been proposed. These procedures are generally conceptualized within the context of model   \n39 parallelism, where a neural network is split into stages that can process their activations in parallel,   \n40 potentially on multiple devices. For example, some methods use outdated parameters or activations,   \n41 such as double-buffered pipelining [14] or delayed gradient approaches [44]. However, these methods   \n42 introduce significant memory overhead due to the use of ad hoc buffers for activations, parameters,   \n43 or both. Following an opposite direction, local learning methods [33, 4], which estimate inexact   \n44 gradients via a local auxiliary neural network, pave the way to parallel gradient computations but   \n45 often lead to unrecoverable performance drops [11]. This underscores the need for a robust alternative   \n46 to backpropagation, with limited memory overhead.   \n47 In this work, we introduce PETRA (Parallel End-to-End Training with Reversible Architectures),   \n48 a novel method designed to parallelize gradient computations within reversible architectures with   \n49 minimal computational overhead. Reversible architectures are an ideal candidate for this task,   \n50 as they can significantly reduce memory overhead during standard backpropagation with limited   \n51 communication costs. Furthermore, reversibility is a minor requirement, as many studies have   \n52 demonstrated that standard architectures can be adapted into reversible ones without any performance   \n53 drops [12, 19, 29, 22]. By allowing parameters to evolve in parallel and by computing an approximate   \n54 inversion during backward, we propose an effective alternative to backpropagation which allows   \n55 high model parallelism with a constant communication overhead and no additional buffers. In fact,   \n56 for a constant increase in communication overhead, PETRA achieves a linear speedup compared to   \n57 standard backpropagation with respect to the number $J$ of stages the network is split into. We illustrate   \n58 our approach in Fig. 1, by contrasting the evolution of PETRA with a standard backpropagation pass.   \n59 Contributions. Our contributions are as follows: (1) We introduce PETRA, a streamlined approach   \n60 for parallelizing the training of reversible architectures. This method leverages a delayed, approximate   \n61 inversion of activations during the backward pass, allowing for enhanced computational efficiency. (2)   \n62 Our technique significantly reduces memory overhead by minimizing the necessity to store extensive   \n63 computational graphs. (3) It enables the parallelization of forward and backward pass computations   \n64 across multiple devices, effectively distributing the workload and reducing training time. (4) We   \n65 validate the efficacy of PETRA through rigorous testing on benchmark datasets such as CIFAR-10,   \n66 ImageNet-32, and ImageNet, where it demonstrates robust performance with minimal impact on   \n67 accuracy. (5) Additionally, we provide a flexible reimplementation of the autograd system in PyTorch,   \n68 specifically tailored for our experimental setup, which we make available to the research community. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "JkqrZqBO7d/tmp/fa4a0957ceab8a0346a9c42b09299454476768543ff3acad9e80bd65e491edbe.jpg", "img_caption": ["Figure 1: Comparison of PETRA with standard backpropagation. This approach splits the stages of a model and decouples their forward and backward passes, resulting in a sixfold increase in parallelization speed in this example. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "69 2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "70 Reversible architectures. Reversible DNNs are composed of layers that are invertible, meaning   \n71 that the input of a layer can be computed from its output. This approach allows to avoid the need to   \n72 store intermediary activations during the forward pass by reconstructing them progressively during   \n73 the backward pass [12], at the cost of an extra computation per layer. Invertible networks further   \n74 improve this method by removing dimensionality reduction steps such as downsamplings, making   \n75 the networks fully invertible [18]. Reversibility is not restricted to a type of architecture or tasks   \n76 and has been extensively used for generative models [9], for ResNets [12], and Transformers [29].   \n77 However, as far as we know, reversible architectures have never been used to enhance parallelization   \n78 capabilities.   \n79 Alternatives to backpropagation. Multiple alternatives to backpropagation have been proposed   \n80 previously to improve over its computational efficiency. For instance, DNI [20] is the first to mention   \n81 the backpropagation inefficiency and its inherent synchronization locks. However, they address   \n82 those locks with a method non-competitive with simple baselines. Local (or greedy) learning [33, 3]   \n83 propose to use layerwise losses to decouple the training of layers, allowing them to train in parallel   \n84 [5]. Local learning in videos [28] notably uses the similarity between successive temporal features   \n85 to remove buffer memory. However, the difference in training dynamics between local training and   \n86 backpropagation still limits such approaches [11, 38].   \n87 Pipeline parallelism. Pipelining encompasses a range of model parallel techniques that divide the   \n88 components of a network into stages that compute in parallel, while avoiding idle workers. Initially   \n89 popularized by [15], a batch of data is divided into micro-batches that are processed independently at   \n90 each stage. Although more efficient pipelining schedules have been proposed [10], notably to mitigate   \n91 the peak memory overhead, keeping an exact batch gradient computation requires leaving a bubble of   \n92 idle workers. By alternating one forward and one backward pass for each worker, PipeDream [31]   \n93 can allow to get rid of idleness bubbles, but at the expense of introducing staleness in the gradients   \n94 used. [32] mitigates this staleness to only one optimization step by accumulating gradients, thus also   \n95 reducing the parameter memory overhead to only two versions of the parameters. Nevertheless, these   \n96 approaches still suffer from a quadratic activation memory overhead with regard to the number of   \n97 stages, as micro-batch activations pile up in buffers, especially for early layers. Some implementations   \n98 propose to limit this overhead by combining activation checkpointing [6] with pipelining [21, 27],   \n99 although the memory overhead still scales with the number of stages.   \n100 Delayed gradient. By allowing stale gradients in the update process, these previous methods   \n101 provide the context for our approach. Delayed gradient optimization methods are model parallel   \n102 techniques that aim to decouple and process layers in parallel during backpropagation. In these   \n103 approaches, delays occur stage-wise: the backward pass may be computed with outdated parameters   \n104 or activations compared to the forward pass. For instance, [16] proposes a feature replay approach,   \n105 where a forward pass first stores intermediary activations, which are then \"replayed\" to compute the   \n106 backward pass in parallel. This method still requires heavy synchronization between layers, yielding   \n107 a lock on computations. In [42] and [43], stale gradients are computed from older parameter versions   \n108 differing from the parameters used during the update. This staleness can be mitigated: [43] \u2019shrinks\u2019   \n109 the gradient by the delay value, but more advanced techniques also exist [41, 23]. Still, these methods   \n110 are limited like previous pipelining methods by their memory overhead as the computational graph   \n111 is fully stored. A first step to reduce this, as proposed in Diversely Stale Parameters (DSP) [40],   \n112 PipeMare [41] and [23], is to keep a single set of parameters and approximate the gradients computed   \n113 during the backward pass with the updated parameters, which differ from the ones used in the forward   \n114 pass. This requires, like in activation checkpointing, an additional reconstruction of the computational   \n115 graph. Furthermore, the quadratic activation memory overhead still limits the scalability of these   \n116 methods for a large number of stages. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "117 3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "118 3.1 Standard backpropagation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "119 We consider a DNN composed of $J$ stages (e.g., a layer or a set of layers). An input $x_{0}$ is propagated   \n120 through the network, recursively defined by ", "page_idx": 2}, {"type": "equation", "text": "$$\nx_{j}\\triangleq F_{j}(x_{j-1},\\theta_{j})\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "121 where $F_{j}$ is the $j$ -th stage parameterized by $\\theta_{j}$ . The backpropagation algorithm is the ubiquitous   \n122 algorithm to compute parameter gradients. First, an input is propagated through the network with   \n123 a forward pass, while storing its intermediate activations. A scalar loss $\\mathcal{L}$ is then deduced from the   \n124 corresponding output $x_{J}$ . Parameter gradients are then computed during the backward pass by taking   \n125 advantage of the chain rule: starting from the last stage with $\\delta_{J}=\\nabla_{\\boldsymbol{x}_{J}}\\mathcal{L}$ , the gradients with regard   \n126 to the activations are given by ", "page_idx": 2}, {"type": "image", "img_path": "JkqrZqBO7d/tmp/530ce600096e8fe3ae8773fd2c9159164b5fa72d968254f49308ec6e3fb8db4a.jpg", "img_caption": ["Figure 2: Differences between the residual block of a ResNet and its reversible counterpart. (a) Forward of a residual block. (b) Forward and (c) Reverse forward of a reversible residual block. For reversible blocks, similarly to [12], the input $x_{j}$ is doubled in size and split equally into $\\{x_{j}^{1},x_{j}^{2}\\}$ along the channel dimension. The function ${\\mathcal{F}}_{j}$ includes a skip-connection while $\\tilde{\\mathcal{F}}_{j}$ does not. ", "(a) ResNet (b) RevNet Forward (c) RevNet Reverse "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\delta_{j}\\triangleq\\nabla_{x_{j-1}}\\mathcal{L}=\\partial_{x}F_{j}(x_{j-1},\\theta_{j})^{\\mathrm{T}}\\delta_{j+1}\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "127 and the gradients with regard to the parameters are defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Delta_{j}\\triangleq\\nabla_{\\theta_{j}}\\mathcal{L}=\\partial_{\\theta}F_{j}(x_{j-1},\\theta_{j})^{\\mathrm{T}}\\delta_{j+1}\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "128 Note that these computations follow a synchronous and sequential order. The parameters $\\theta_{j}$ can then   \n129 be updated given their gradient estimate $\\Delta_{j}$ , using any optimizer. ", "page_idx": 3}, {"type": "text", "text": "130 3.2 Reversible architectures ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "131 We focus on the reversible neural networks presented in [12], although our method is not dependent   \n132 on this architecture. In practice, only a few stages which do not preserve feature dimensionality   \n133 are not reversible and correspond to the downsampling blocks in the ResNet. Fig. 2 highlights   \n134 how reversible residual blocks $F_{j}$ differ from their standard counterpart. The input is split into two   \n135 equal-size inputs, along the channel dimension, that are propagated forward according to Fig. 2b   \n136 using an ad-hoc operator $\\tilde{\\mathcal{F}}_{j}$ . It can be reconstructed by reverse propagating the output according to   \n137 Fig. 2c, by subtracting the output of $\\tilde{\\mathcal{F}}_{j}$ rather than adding it like in the previous forward.   \n138 Reversible stages. In order to compute the exact gradients during the backpropagation phase, each   \n139 reversible stage needs to retrieve its output from the stage above. We note $F_{j}^{-1}$ the reverse stage   \n140 function, which reconstructs the input from the output. We recursively apply the reconstruction to the   \n141 final activation $x_{J}$ , such that ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left[\\!\\!\\begin{array}{c}{{x_{j-1}}}\\\\ {{\\delta_{j}}}\\end{array}\\!\\!\\right]=\\left[\\!\\!\\begin{array}{c}{{F_{j}^{-1}(x_{j},\\theta_{j})}}\\\\ {{\\partial_{x}F_{j}(F_{j}^{-1}(x_{j},\\theta_{j}),\\theta_{j})^{\\mathrm{T}}\\delta_{j+1}}}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "142 Note that reconstructing the input in our procedure is computationally equivalent to recomputing the   \n143 activations in activation checkpointing, meaning it is equivalent to a single forward pass. Thus, this   \n144 augmented backward procedure is equivalent to one regular forward call and backward call. However,   \n145 one should observe that since the input $x_{j-1}$ must be sent to the reversible stages, this doubles the   \n146 cost of backward communications.   \n147 Non-reversible stages. In practice, a reversible architecture includes layers that reduce dimension  \n148 ality for computational efficiency, which thus correspond to non-invertible functions. For those very   \n149 few stages, we employ a buffer mechanism to store activations and, like activation checkpointing,   \n150 we recompute the computational graph with a forward pass during the backward pass. Note that   \n151 this would not be the case for invertible (i.e., bijective) architectures [18], which use an invertible   \n152 downsampling. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Table 1: Comparisons with other methods in an ideal setting for one stage. We compare several methods to compute a gradient estimate in a model parallel setting. Here, $J$ is the total number of stages while $j$ is the stage index. For the sake of simplicity, we assume that a backward pass requires approximately 2 times more FLOPs than a forward pass. Full Graph indicates that it is required to store the full computational graph of a local forward pass. With a limited increase in communication volume and FLOPs, PETRA requires the least storage of all methods while being linearly faster than backpropagation. We assume that the forward and backward passes can be executed in parallel for PETRA or delayed gradients, making the backward pass responsible for most of the computation time in parallelizable approaches. ", "page_idx": 4}, {"type": "table", "img_path": "JkqrZqBO7d/tmp/b6031dfb342d3a7f85c2310a5b2fee7bc6e2cb059e3615505fa9d10a06f6801a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "153 3.3 A parallelizable approach: PETRA ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "154 As with any model parallel training technique, PETRA requires to partition the network architecture   \n155 into stages $F_{j}$ that are distributed across distinct devices. Each device $j$ needs only to communicate   \n156 with its neighboring devices $j\\mathrm{~-~}1$ and $j+1$ . The pseudo-code in Alg. 1 details the operations   \n157 performed by each device, and the whole algorithm execution can be summarized as follows. The first   \n158 device sequentially accesses mini-batches, initiating the data propagation process. When receiving its   \n159 input $\\bar{x_{j-1}^{t}}$ from the previous stage, each stage processes it in forward mode and passes it to the next   \n160 stage, until the final stage is reached. The final stage evaluates the loss and computes the gradients   \n161 with regard to its input and parameters, thus initiating the backward process, which is performed in   \n162 parallel of the forward process. In it, each stage processes the input and its associated gradient from   \n163 the next stage. This means first reconstructing the computational graph, either while reconstructing   \n164 the input $\\tilde{x}_{j-1}^{t}$ for reversible stages or with a forward pass as in activation checkpointing otherwise.   \n165 Then, the parameter gradient approximation $\\Delta_{j}^{t+1}$ and the input gradient are computed before passing   \n166 the latter to the previous stage. For intermediary reversible stages, this translates into the following   \n167 equations, where $t$ corresponds to the current time step of the training, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{x_{j}^{t+1}=F_{j}(x_{j-1}^{t},\\theta_{j}^{t})}\\\\ {\\tilde{x}_{j-1}^{t+1}=F_{j}^{-1}(\\tilde{x}_{j}^{t},\\theta_{j}^{t})}\\\\ {\\delta_{j}^{t+1}=\\partial_{x}F_{j}(\\tilde{x}_{j-1}^{t+1},\\theta_{j}^{t})^{\\mathrm{T}}\\delta_{j+1}^{t}}\\\\ {\\Delta_{j}^{t+1}=\\partial_{\\theta}F_{j}(\\tilde{x}_{j-1}^{t+1},\\theta_{j}^{t})^{\\mathrm{T}}\\delta_{j+1}^{t}}\\\\ {\\theta_{j}^{t+1}=\\mathrm{Optimize}_{j}^{t}(\\theta_{j}^{t},\\Delta_{j}^{t+1})\\,.}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "168 Note that this complete set of equations effectively decouples communications, computations, and   \n169 parameter updates between independent devices. Indeed, reversible stages are able to operate without   \n170 maintaining any state between the forward and corresponding backward phase by simply avoiding   \n171 weight stashing, similarly to [40], and by reversing the output into the input during the backward   \n172 phase, removing the need for an input buffer. As parameters are updated between the forward and   \n173 backward phases, the reversible stage produces an approximate input reconstruction, thus evaluating   \n174 gradients with an approximate set of inputs and parameters during the backward phase. We illustrate   \n175 in Fig. 3 the mechanism of PETRA compared to standard delayed gradient approaches that rely on   \n176 additional buffers [44, 42].   \n177 Complexity analysis. We now discuss the beneftis of our method, which are summarized in Tab. 1.   \n178 In this discussion, we assume a homogeneous setting in which almost identical stages are distributed   \n179 across $J$ devices uniformly. First, we consider the backpropagation setting, assuming a model   \n180 parallelism strategy: a standard backpropagation pass requires storing locally both the parameters and   \n181 the computational graph and due to the update lock of backpropagation [20], requires synchronization   \n182 between subsequent layers which impede the speed of computations. Standard Delayed Gradients   \n183 strategies as implemented in [44, 42] allow to unlock this barrier, but they require buffers for storing   \n184 both the computational graph and parameters which can become impractical when using large models.   \n185 In [40], an activation checkpointing strategy removes the need for storing parameters, yet it requires   \n186 a small computational overhead of $33\\%$ (assuming a backward pass is approximatively two times   \n187 slower than a forward pass, see Fig. 6 of [17] and [30]). To avoid storing activations, we rely on   \n188 reversible architectures [12] which increases the amount of forward communications by a factor of 2   \n189 and backward communication by a factor of 4 \u2013 activations sizes double and one has to pass both   \n190 activations and gradients at the same time during backward. None of the aforementioned methods   \n191 scale with the depth $J$ : PETRA combines all the advantages of the previous methods, allowing an   \n192 efficient parallelization, while leading to a limited overhead in computations and communications. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "image", "img_path": "JkqrZqBO7d/tmp/92df550abc6fe39e0f0bbde98abbba5c1a122591982e103cc12a491a858b10dd.jpg", "img_caption": ["Figure 3: Comparison of our PETRA method to a standard Delayed Gradient method [42]. By avoiding weight stashing and reversing the output into the input during the backward phase, we are able to fully decouple the forward and backward phases in all reversible stages, with no memory overhead, compared to standard delayed gradient approaches. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "193 4 Numerical experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "194 4.1 Classification accuracy ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "195 We now describe our experimental setup on CIFAR-10 [24], ImageNet-32 [7], and ImageNet [8]. ", "page_idx": 5}, {"type": "text", "text": "196 Experimental setup. All our experiments use a standard SGD optimizer with a Nesterov momen  \n197 tum factor of 0.9. We train all models for 300 epochs on CIFAR-10 and 90 epochs on ImageNet32   \n198 and ImageNet. We apply standard data augmentation, including horizontal flip, random cropping,   \n199 and standard normalization but we do not follow the more involved training settings of [39], which   \n200 potentially leads to higher accuracy. We perform a warm-up of 5 epochs where the learning rate   \n201 linearly increases from 0 to 0.1, following [13]. Then, the learning rate is decayed by a factor of 0.1   \n202 at epochs 30, 60, and 80 for ImageNet32 and ImageNet \u2013 it is decayed at epochs 150 and 225 for   \n203 CIFAR-10. We use a weight decay of 5e-4 for CIFAR-10 and 1e-4 for ImageNet32 and ImageNet. As   \n204 suggested in [13], we do not apply weight decay on the batch norm learnable parameters and biases   \n205 of affine and convolutional layers. For our standard backpropagation experiments, we follow the   \n206 standard practice and use a batch size of 128 on ImageNet32 and CIFAR-10, and 256 on ImageNet32.   \n207 However, we made a few adaptations to train our models with PETRA. As suggested by [42, 43], we   \n208 employ an accumulation factor $k$ and a batch size of 64, which allows to reduce the effective staleness   \n209 during training: in this case, $k$ batches of data must be successively processed before updating the   \n210 parameters of a stage (see Alg. 1). Such gradient accumulation however also increases the effective   \n211 batch size, and we apply the training recipe used in [13] to adjust the learning rate; note that we use   \n212 the average of the accumulated gradients instead of the sum. The base learning rate is thus given by   \n213 the formula $\\textstyle{\\mathtt{1r}}=0.1{\\frac{64k}{256}}$ , with $k$ the accumulation factor. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 Worker perspective for training in parallel with PETRA, on a stage $j$ , assuming initialized parameters $\\theta_{j}$ and time step $t$ , as well as an accumulation factor $k>1$ . ", "page_idx": 6}, {"type": "text", "text": "1: In parallel on the $j$ -th stage, $1\\leq j<J,$ , perform:   \n2: Forward Communications and Computations:   \n3: If $j=1$ then   \n4: $x_{0}\\gets\\mathbf{Read}_{\\mathrm{dataset}}$   \n5: Else   \n6: $x_{j-1}\\leftarrow$ Wait and Receive from $j\\!-\\!1$   \n7: If stage $j$ is not reversible :   \n8: $\\begin{array}{r}{\\mathbf{Buffer}_{j}\\gets x_{j}\\qquad}\\\\ {x_{j}\\gets F_{j}\\big(x_{j-1},\\theta_{j}\\big)}\\\\ {\\mathbf{Send}_{\\textrm{t o}j+1}(x_{j})\\qquad}\\end{array}$   \n9:   \n10:   \n11: Backward Communications and Computations:   \n12: $(\\tilde{x}_{j},\\delta_{j+1})\\gets^{\\cdot}$ Wait and Receive $\\operatorname{from}j{+}1$   \n13: If stage $j$ is reversible:   \n14: $\\tilde{x}_{j-1}\\gets F_{j}^{-1}(\\tilde{x}_{j},\\theta_{j})$ and keep computational graph in memory   \n15: Else :   \n16: $\\tilde{x}_{j-1}\\leftarrow\\mathbf{Buffer}_{j}$   \n17: $\\bar{x_{j}}\\gets F_{j}(\\tilde{x}_{j-1}\\bar{,}\\theta_{j})$ to recompute the computational graph   \n18: $\\delta_{j}\\gets\\partial_{x}F_{j}(\\tilde{x}_{j-1},\\theta_{j})^{T}\\delta_{j+1}$   \n19: $\\begin{array}{r}{\\Delta_{j}\\leftarrow\\Delta_{j}+\\frac{1}{k}\\partial_{\\theta}F_{j}(\\tilde{x}_{j-1},\\theta_{j})^{T}\\delta_{j+1}}\\end{array}$   \n20: If $t$ mod $k=\\mathrm{\\ddot{0}}$ then:   \n21: Update parameters $\\theta_{j}$ with $\\Delta_{j}$   \n22: $\\begin{array}{c}{\\bar{\\Delta_{j}}\\leftarrow0}\\\\ {t\\leftarrow t+1}\\\\ {\\mathbf{Send}_{\\mathrm{\\Deltato}\\;j-1}(x_{j},\\delta_{j})}\\end{array}$   \n23:   \n24:   \n25:   \n26: In parallel on the final stage $J$ , perform:   \n27: $x_{J-1}\\leftarrow\\mathbf{Wai}$ t and Receive from $J\\!-\\!1$   \n28: $\\begin{array}{r l}&{\\dot{\\mathcal{L}}\\gets F_{J}(x_{J-1},\\theta_{J})}\\\\ &{\\delta_{J}\\gets\\nabla_{x_{J}}\\mathcal{L}}\\\\ &{\\Delta_{J}\\gets\\Delta_{J}+\\frac{1}{k}\\nabla_{\\theta_{J}}\\mathcal{L}}\\\\ &{\\mathbf{r}\\delta}\\end{array}$   \n29:   \n30:   \n31: If t mod $k=0$ then:   \n32: Update parameters $\\theta_{J}$ with $\\Delta_{J}$   \n33: $\\begin{array}{r l}&{\\bar{\\Delta\\mathbf{\\omega}}_{J}\\leftarrow0}\\\\ &{t\\leftarrow t+1}\\\\ &{\\mathbf{Send}_{\\mathrm{\\scriptsize~to}\\:J-1}(x_{J-1},\\delta_{J})}\\end{array}$   \n34:   \n35:   \n214 Model adaptations. For designing our RevNet architectures, we adopt a methodology similar   \n215 to [12]: the number of channels in each stage is multiplied by 2 to account for the second data   \n216 stream according to Fig. 2. However, as the stage function $\\dot{\\tilde{\\mathcal{F}}}_{j}$ operates only on one of the two   \n217 streams, the number of parameters stays almost the same between a residual block and its revertible   \n218 counterpart. Consequently, the DNNs are split to preserve each residual block, resulting in 10 stages   \n219 for RevNet18, and 18 stages for RevNet34 and RevNet50; thus varying the level of staleness between   \n220 configurations. On CIFAR-10, the input layer uses 3x3 convolutions instead of $7\\mathrm{x}7$ convolutions and   \n221 does not perform max-pooling. The running statistics of batch normalization layers are updated when   \n222 recomputing the activations during the backward pass and are then used during model evaluation \u2013   \n223 the running statistics are not updated during the forward pass.   \n224 Performance comparison. Tab. 2 reports our numerical accuracy on several vision datasets,   \n225 comparing a backpropagation performance from an official PyTorch implementation of ResNets   \n226 (the numbers can be found as v1 of https://pytorch.org/hub/pytorch_vision_resnet/),   \n227 for our own implementation of ResNets and RevNets in our custom computational framework, and   \n228 our proposed method, PETRA. For PETRA, we report the best classification accuracy after the last   \n229 learning rate drop, using the best value (picked on the training set) of accumulation steps within   \n230 $\\{1,2,4,8,16,32\\}$ . Our CIFAR-10 accuracies are averaged over 3 runs, with a variance smaller than   \n231 0.1. We observe that while our reversible models have about the same parameter count, they all   \n232 perform in the same range of accuracy as their non-reversible counterparts. Only the RevNet-50   \n233 leads to a small drop in accuracy on ImageNet of about $0.6\\%$ : using different downsampling layers   \n234 removes this gap at the expense of a substantial increase in the parameter count (30.4M to 50M).   \n235 However, we decided not to include this result for the sake of comparison with respect to the original   \n236 ResNets.   \n237 Impact of the accumulation $k$ . We test the impact of the accumulation on a RevNet-18 trained   \n238 via PETRA for various values of accumulations with $k$ spanning $\\{1,2,4,8,16,32\\}$ on the ImageNet   \n239 dataset. Fig. 4 indicates that our method can benefit from large accumulation factors, with the   \n240 well-known trade-off of large batches mentioned in [13]. Increasing the accumulation factor reduces   \n241 the effective staleness during training, and closes the performance gap with standard backpropagation   \n242 with perfect matching for $\\bar{k}=32$ . This confirms that this large-batch training recipe derived for   \n243 synchronous data parallelism is also particularly suited for our model parallel approach. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "JkqrZqBO7d/tmp/d6d2ad504227ea76bc5c21e0af5d2b04345457c8c5e196a7216ccff92d2fd475.jpg", "table_caption": ["Table 2: Classification accuracies using our PETRA method with RevNets, compared to standard backpropagation on ResNets and RevNets on CIFAR-10, ImageNet32, and ImageNet. Our method delivers competitive results with backpropagation, even on ImageNet. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "244 4.2 Technical details ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "245 A note on the implementation. We shortly describe our implementation details. We base our   \n246 method on PyTorch [2], although we require significant modifications to the Autograd framework   \n247 in order to manage delayed first-order quantities consistently with PETRA. We rely heavily on   \n248 the Vector Jacobian Product of PyTorch to compute gradients during the backward pass of each   \n249 stage, but other backends could be used. The backward pass for reversible stages only necessitates a   \n250 reconstruction step and a backward step \u2013 a naive implementation would use a reconstruction step,   \n251 followed by a forward and a backward step. This is because we only need the output gradient as well   \n252 as the computational graph of $\\tilde{\\mathcal{F}}_{j}$ to compute the input and parameter gradients at line 12 and 13 of   \n253 Alg. 1, which can be obtained during the input reconstruction phase. For non-reversible stages, we   \n254 reconstruct the computational graph with a forward pass on the input retrieved from the buffer during   \n255 the backward pass. Our models can run on a single A100, 80GB.   \n256 Memory benefits and training time. To better understand the advantage of our method compared   \n257 to other delayed gradient approaches [14, 40, 23], we emphasize the practical memory savings   \n258 associated with different methods in Tab. 3. We estimate the memory needed in gigabytes, as the   \n259 sum of the model size, the input buffer size, and the parameter buffer size, while excluding the input   \n260 buffer size of the first stage, which corresponds to retrievable dataset inputs. We do not include the   \n261 effect of gradient accumulation since it depends on the value of $k$ and only affects the length of   \n262 the parameter buffer, which is small in our case, i.e., we use $k=1$ . Note that the batch size also   \n263 affects the memory savings, and we set it to 64 for consistency with Tab. 2. Storing both inputs   \n264 and parameters into a buffer corresponds to the PipeDream approach [14]. Only storing inputs into   \n265 buffers would correspond to the approach in [40, 23]. The third and fourth lines are only applicable   \n266 to reversible architectures as they do not store the input into buffers. As can be seen, the input buffer   \n267 has the biggest impact on the total memory needed, being responsible for $52.3\\%$ of the memory   \n268 footprint. Dropping the parameter buffer in PETRA pushes the memory savings further to $54.3\\%$ for   \n269 a RevNet50 on ImageNet. Note that non-reversible stages account for the majority of total memory   \n270 use, meaning that savings would be much higher for fully invertible architectures. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "JkqrZqBO7d/tmp/180fe000f3535d41c03810eeda698b4c90bfdd27d9a7fe8d6d801bf2a90c8506.jpg", "img_caption": ["Figure 4: Validation accuracy of PETRA and backpropagation for a various number of accumulation steps, for a RevNet18 trained on ImageNet with $\\bar{k}\\in\\{1,2,4,8,16,32\\}$ . The validation accuracies are averaged over the last 10 epochs. As the number of accumulation steps increases, the effective staleness in PETRA decreases, closing the gap with standard backpropagation. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "JkqrZqBO7d/tmp/a1f66daed1afaf7a1558b8edf61ed6d4bcc157746297942fd5c9d88eb024067c.jpg", "table_caption": ["Table 3: Memory savings for RevNet50 on ImageNet with our method for different configurations. We indicate the use of memory buffers for inputs or parameters. The savings are computed with respect to the first configuration, where inputs and buffers are stored. Our method achieves $54.3\\%$ memory reduction over the base configuration of Delayed Gradients. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "271 5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "272 In this work, we introduce PETRA, a novel model parallel training technique for reversible ar  \n273 chitectures which is a novel promising alternative to backpropagation. It achieves a significant   \n274 parallelization with a limited overhead compared to standard backpropagation or other competitive   \n275 alternatives to end-to-end training, like delayed gradients approaches. Our method has the potential   \n276 to achieve linear speedup compared to standard backpropagation and allows reversible layers to   \n277 operate without any parameter or activation buffers, effectively decoupling the forward and backward   \n278 phases. Despite using an approximate delayed gradient estimate, our method delivers competitive   \n279 performances compared to standard backpropagation on standard computer vision datasets.   \n280 In future work, we aim to implement and optimize PETRA for Large Language Models (LLMs),   \n281 with a first baseline being Reformers [22], invertible transformers that have been shown to scale. This   \n282 will validate PETRA\u2019s effectiveness and robustness, solidifying its potential as a cutting-edge training   \n283 technique. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "284 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "285 [1] I. M. Alabdulmohsin, B. Neyshabur, and X. Zhai. Revisiting neural scaling laws in language   \n286 and vision. Advances in Neural Information Processing Systems, 35:22300\u201322312, 2022.   \n287 [2] J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard,   \n288 E. Burovski, G. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison,   \n289 W. Feng, J. Gong, M. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. Lazos,   \n290 M. Lezcano, Y. Liang, J. Liang, Y. Lu, C. K. Luk, B. Maher, Y. Pan, C. Puhrsch, M. Reso,   \n291 M. Saroufim, M. Y. Siraichi, H. Suk, S. Zhang, M. Suo, P. Tillet, X. Zhao, E. Wang, K. Zhou,   \n292 R. Zou, X. Wang, A. Mathews, W. Wen, G. Chanan, P. Wu, and S. Chintala. Pytorch 2: Faster   \n293 machine learning through dynamic python bytecode transformation and graph compilation. In   \n294 Proceedings of the 29th ACM International Conference on Architectural Support for Program  \n295 ming Languages and Operating Systems, Volume 2, ASPLOS \u201924, page 929\u2013947, New York,   \n296 NY, USA, 2024. Association for Computing Machinery.   \n297 [3] E. Belilovsky, M. Eickenberg, and E. Oyallon. Greedy layerwise learning can scale to imagenet.   \n298 In International conference on machine learning, pages 583\u2013593. PMLR, 2019.   \n299 [4] E. Belilovsky, M. Eickenberg, and E. Oyallon. Decoupled greedy learning of cnns. In Interna  \n300 tional Conference on Machine Learning, pages 736\u2013745. PMLR, 2020.   \n301 [5] E. Belilovsky, L. Leconte, L. Caccia, M. Eickenberg, and E. Oyallon. Decoupled greedy   \n302 learning of cnns for synchronous and asynchronous distributed learning. arXiv preprint   \n303 arXiv:2106.06401, 2021.   \n304 [6] T. Chen, B. Xu, C. Zhang, and C. Guestrin. Training deep nets with sublinear memory cost,   \n305 2016.   \n306 [7] P. Chrabaszcz, I. Loshchilov, and F. Hutter. A downsampled variant of imagenet as an alternative   \n307 to the cifar datasets, 2017.   \n308 [8] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical   \n309 image database. In 2009 IEEE conference on computer vision and pattern recognition, pages   \n310 248\u2013255. Ieee, 2009.   \n311 [9] L. Dinh, D. Krueger, and Y. Bengio. Nice: Non-linear independent components estimation.   \n312 arXiv preprint arXiv:1410.8516, 2014.   \n313 [10] S. Fan, Y. Rong, C. Meng, Z. Cao, S. Wang, Z. Zheng, C. Wu, G. Long, J. Yang, L. Xia, et al.   \n314 Dapple: A pipelined data parallel approach for training large models. In Proceedings of the   \n315 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, pages   \n316 431\u2013445, 2021.   \n317 [11] L. Fournier, S. Rivaud, E. Belilovsky, M. Eickenberg, and E. Oyallon. Can forward gradient   \n318 match backpropagation? In Fortieth International Conference on Machine Learning, 2023.   \n319 [12] A. N. Gomez, M. Ren, R. Urtasun, and R. B. Grosse. The reversible residual network: Back  \n320 propagation without storing activations. Advances in neural information processing systems, 30,   \n321 2017.   \n322 [13] P. Goyal, P. Doll\u00e1r, R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola, A. Tulloch, Y. Jia,   \n323 and K. He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint   \n324 arXiv:1706.02677, 2017.   \n325 [14] A. Harlap, D. Narayanan, A. Phanishayee, V. Seshadri, N. Devanur, G. Ganger, and P. Gibbons.   \n326 Pipedream: Fast and efficient pipeline parallel dnn training. arXiv preprint arXiv:1806.03377,   \n327 2018.   \n328 [15] Y. Huang, Y. Cheng, A. Bapna, O. Firat, D. Chen, M. Chen, H. Lee, J. Ngiam, Q. V. Le, Y. Wu,   \n329 et al. Gpipe: Efficient training of giant neural networks using pipeline parallelism. Advances in   \n330 neural information processing systems, 32, 2019.   \n331 [16] Z. Huo, B. Gu, and H. Huang. Training neural networks using features replay. Advances in   \n332 Neural Information Processing Systems, 31, 2018.   \n333 [17] Z. Huo, B. Gu, H. Huang, et al. Decoupled parallel backpropagation with convergence guarantee.   \n334 In International Conference on Machine Learning, pages 2098\u20132106. PMLR, 2018.   \n335 [18] J.-H. Jacobsen, A. Smeulders, and E. Oyallon. i-revnet: Deep invertible networks. arXiv   \n336 preprint arXiv:1802.07088, 2018.   \n337 [19] J.-H. Jacobsen, A. W. M. Smeulders, and E. Oyallon. i-revnet: Deep invertible networks. ArXiv,   \n338 abs/1802.07088, 2018.   \n339 [20] M. Jaderberg, W. M. Czarnecki, S. Osindero, O. Vinyals, A. Graves, D. Silver, and   \n340 K. Kavukcuoglu. Decoupled neural interfaces using synthetic gradients. In International   \n341 conference on machine learning, pages 1627\u20131635. PMLR, 2017.   \n342 [21] C. Kim, H. Lee, M. Jeong, W. Baek, B. Yoon, I. Kim, S. Lim, and S. Kim. torchgpipe: On-the-fly   \n343 pipeline parallelism for training giant models, 2020.   \n344 [22] N. Kitaev, \u0141. Kaiser, and A. Levskaya. Reformer: The efficient transformer. arXiv preprint   \n345 arXiv:2001.04451, 2020.   \n346 [23] A. Kosson, V. Chiley, A. Venigalla, J. Hestness, and U. Koster. Pipelined backpropagation at   \n347 scale: training large models without batches. Proceedings of Machine Learning and Systems,   \n348 3:479\u2013501, 2021.   \n349 [24] A. Krizhevsky. Learning multiple layers of features from tiny images. 2009.   \n350 [25] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. nature, 521(7553):436\u2013444, 2015.   \n351 [26] S. Li, Y. Zhao, R. Varma, O. Salpekar, P. Noordhuis, T. Li, A. Paszke, J. Smith, B. Vaughan,   \n352 P. Damania, et al. Pytorch distributed: Experiences on accelerating data parallel training. arXiv   \n353 preprint arXiv:2006.15704, 2020.   \n354 [27] Y. Liu, S. Li, J. Fang, Y. Shao, B. Yao, and Y. You. Colossal-auto: Unified automation of   \n355 parallelization and activation checkpoint for large-scale models, 2023.   \n356 [28] M. Malinowski, D. Vytiniotis, G. Swirszcz, V. Patraucean, and J. Carreira. Gradient forward  \n357 propagation for large-scale temporal video modelling. In Proceedings of the IEEE/CVF Confer  \n358 ence on Computer Vision and Pattern Recognition, pages 9249\u20139259, 2021.   \n359 [29] K. Mangalam, H. Fan, Y. Li, C.-Y. Wu, B. Xiong, C. Feichtenhofer, and J. Malik. Reversible   \n360 vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and   \n361 Pattern Recognition, pages 10830\u201310840, 2022.   \n362 [30] E. Mizutani and S. Dreyfus. On complexity analysis of supervised mlp-learning for algorithmic   \n363 comparisons. In IJCNN\u201901. International Joint Conference on Neural Networks. Proceedings   \n364 (Cat. No.01CH37222), volume 1, pages 347\u2013352 vol.1, 2001.   \n365 [31] D. Narayanan, A. Harlap, A. Phanishayee, V. Seshadri, N. R. Devanur, G. R. Ganger, P. B.   \n366 Gibbons, and M. Zaharia. Pipedream: Generalized pipeline parallelism for dnn training. In   \n367 Proceedings of the 27th ACM Symposium on Operating Systems Principles, pages 1\u201315, 2019.   \n368 [32] D. Narayanan, A. Phanishayee, K. Shi, X. Chen, and M. Zaharia. Memory-efficient pipeline  \n369 parallel dnn training. In International Conference on Machine Learning, pages 7937\u20137947.   \n370 PMLR, 2021.   \n371 [33] A. N\u00f8kland and L. H. Eidnes. Training neural networks with local error signals. In International   \n372 conference on machine learning, pages 4839\u20134850. PMLR, 2019.   \n373 [34] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: Memory optimizations toward training   \n374 trillion parameter models, 2020.   \n375 [35] J. Ren, S. Rajbhandari, R. Y. Aminabadi, O. Ruwase, S. Yang, M. Zhang, D. Li, and Y. He.   \n376 {Zero-offload}: Democratizing {billion-scale} model training. In 2021 USENIX Annual   \n377 Technical Conference (USENIX ATC 21), pages 551\u2013564, 2021.   \n378 [36] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-lm:   \n379 Training multi-billion parameter language models using model parallelism. arXiv preprint   \n380 arXiv:1909.08053, 2019.   \n381 [37] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari, J. Casper, Z. Liu, S. Prabhumoye,   \n382 G. Zerveas, V. Korthikanti, et al. Using deepspeed and megatron to train megatron-turing nlg   \n383 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990, 2022.   \n384 [38] Y. Wang, Z. Ni, S. Song, L. Yang, and G. Huang. Revisiting locally supervised learning: an   \n385 alternative to end-to-end training. arXiv preprint arXiv:2101.10832, 2021.   \n386 [39] R. Wightman, H. Touvron, and H. J\u00e9gou. Resnet strikes back: An improved training procedure   \n387 in timm, 2021.   \n388 [40] A. Xu, Z. Huo, and H. Huang. On the acceleration of deep learning model parallelism with   \n389 staleness. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),   \n390 pages 2085\u20132094, 2019.   \n391 [41] B. Yang, J. Zhang, J. Li, C. R\u00e9, C. Aberger, and C. De Sa. Pipemare: Asynchronous pipeline   \n392 parallel dnn training. Proceedings of Machine Learning and Systems, 3:269\u2013296, 2021.   \n393 [42] H. Zhuang, Z. Lin, and K.-A. Toh. Accumulated decoupled learning: Mitigating gradient   \n394 staleness in inter-layer model parallelization. arXiv preprint arXiv:2012.03747, 2020.   \n395 [43] H. Zhuang, Y. Wang, Q. Liu, and Z. Lin. Fully decoupled neural network learning using delayed   \n396 gradients. IEEE transactions on neural networks and learning systems, 33(10):6013\u20136020,   \n397 2021.   \n398 [44] H. Zhuang, Z. Weng, F. Luo, T. Kar-Ann, H. Li, and Z. Lin. Accumulated decoupled learn  \n399 ing with gradient staleness mitigation for convolutional neural networks. In International   \n400 Conference on Machine Learning, pages 12935\u201312944. PMLR, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "401 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "402 1. Claims   \n403 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n404 paper\u2019s contributions and scope?   \n405 Answer: [Yes]   \n406 Justification: Our novel method is presented in Sec. 3 as described. We provide numerical   \n407 accuracy and ablations in Sec. 4.   \n408 2. Limitations   \n409 Question: Does the paper discuss the limitations of the work performed by the authors?   \n410 Answer: [Yes]   \n411 Justification: We discuss it along with our experimental results in Sec. 4.2.   \n412 3. Theory Assumptions and Proofs   \n413 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n414 a complete (and correct) proof?   \n415 Answer: [NA]   \n416 Justification: We do not provide theoretical results in this paper.   \n417 4. Experimental Result Reproducibility   \n418 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n419 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n420 of the paper (regardless of whether the code and data are provided or not)?   \n421 Answer: [Yes]   \n422 Justification: The experimental results are detailed in the article, and can be reproduced with   \n423 the provided code.   \n424 5. Open access to data and code   \n425 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n426 tions to faithfully reproduce the main experimental results, as described in supplemental   \n427 material?   \n428 Answer: [Yes]   \n429 Justification: The data is openly accessible and properly referenced in the text. The code is   \n430 included in the supplementary material.   \n431 6. Experimental Setting/Details   \n432 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n433 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n434 results?   \n435 Answer: [Yes]   \n436 Justification: The hyperparameters are detailed in the article and the code.   \n437 7. Experiment Statistical Significance   \n438 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n439 information about the statistical significance of the experiments?   \n440 Answer: [Yes]   \n441 Justification: We report accuracy averaged over 3 runs on CIFAR10 which have low variance,   \n442 and we follow to the standard practice on ImageNet and ImageNet32.   \n443 8. Experiments Compute Resources   \n444 Question: For each experiment, does the paper provide sufficient information on the com  \n445 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n446 the experiments?   \n447 Answer: [Yes]   \n448 Justification: Details about resources and executions are presented Sec 4.2.   \n449 9. Code Of Ethics   \n450 Question: Does the research conducted in the paper conform, in every respect, with the   \n451 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n452 Answer: [Yes]   \n453 Justification: Our research conforms to the Code of Ethics.   \n454 10. Broader Impacts   \n455 Question: Does the paper discuss both potential positive societal impacts and negative   \n456 societal impacts of the work performed?   \n457 Answer: [NA]   \n458 Justification: Our article is concerned with allowing general deep learning optimization, and   \n459 does not have particular societal impacts.   \n460 11. Safeguards   \n461 Question: Does the paper describe safeguards that have been put in place for responsible   \n462 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n463 image generators, or scraped datasets)?   \n464 Answer: [NA]   \n465 Justification: This paper does not present such risks.   \n466 12. Licenses for existing assets   \n467 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n468 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n469 properly respected?   \n470 Answer: [Yes]   \n471 Justification: The datasets used are cited, and we credit the author of the code used for our   \n472 implementation.   \n473 13. New Assets   \n474 Question: Are new assets introduced in the paper well documented and is the documentation   \n475 provided alongside the assets?   \n476 Answer: [Yes]   \n477 Justification: The code will be provided with a README file describing how to run   \n478 experiments.   \n479 14. Crowdsourcing and Research with Human Subjects   \n480 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n481 include the full text of instructions given to participants and screenshots, if applicable, as   \n482 well as details about compensation (if any)?   \n483 Answer: [NA]   \n484 Justification: No crowdsourcing or human subjects.   \n485 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n486 Subjects   \n487 Question: Does the paper describe potential risks incurred by study participants, whether   \n488 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n489 approvals (or an equivalent approval/review based on the requirements of your country or   \n490 institution) were obtained?   \n491 Answer: [NA]   \n492 Justification: No potential risks or study participants. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}]