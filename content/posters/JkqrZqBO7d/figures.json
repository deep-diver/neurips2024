[{"figure_path": "JkqrZqBO7d/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of PETRA with standard backpropagation. This approach splits the stages of a model and decouples their forward and backward passes, resulting in a sixfold increase in parallelization speed in this example.", "description": "This figure compares the timing and parallelization of standard backpropagation with the proposed PETRA method.  In backpropagation, the forward and backward passes are sequential, creating a bottleneck. In PETRA, the model is split into stages, and the forward and backward passes are performed independently in parallel across different devices. This allows for significantly faster training due to concurrent computations, as illustrated by the sixfold increase in speed shown in the example.", "section": "1 Introduction"}, {"figure_path": "JkqrZqBO7d/figures/figures_3_1.jpg", "caption": "Figure 2: Differences between the residual block of a ResNet and its reversible counterpart. (a) Forward of a residual block. (b) Forward and (c) Reverse forward of a reversible residual block. For reversible blocks, similarly to [12], the input x\u2c7c is doubled in size and split equally into {x\u2081\u2c7c,x\u2082\u2c7c} along the channel dimension. The function F\u2c7c includes a skip-connection while F\u2c7c does not.", "description": "This figure compares the structure of a standard residual block in ResNet architecture with its reversible counterpart.  The standard residual block (a) shows a simple feedforward operation.  In contrast, the reversible block (b,c) splits the input into two parts, processes them separately through different functions (F\u2c7c and F\u0303\u2c7c), and then combines the results.  The key difference is that the reversible block is designed so its input can be reconstructed from its output, making it memory-efficient for training.", "section": "3.2 Reversible architectures"}, {"figure_path": "JkqrZqBO7d/figures/figures_5_1.jpg", "caption": "Figure 3: Comparison of our PETRA method to a standard Delayed Gradient method [42]. By avoiding weight stashing and reversing the output into the input during the backward phase, we are able to fully decouple the forward and backward phases in all reversible stages, with no memory overhead, compared to standard delayed gradient approaches.", "description": "This figure compares PETRA with standard delayed gradient methods.  PETRA avoids the need to store intermediary activations during the backward pass by reversing the output of each layer into its input. This eliminates the need for buffers, unlike standard delayed gradient methods which require significant buffer space to store intermediary activations and gradients.  This allows PETRA to fully decouple the forward and backward passes, improving efficiency and reducing memory usage.", "section": "3.3 A parallelizable approach: PETRA"}, {"figure_path": "JkqrZqBO7d/figures/figures_8_1.jpg", "caption": "Figure 4: Validation accuracy of PETRA and backpropagation for a various number of accumulation steps, for a RevNet18 trained on ImageNet with k \u2208 {1, 2, 4, 8, 16, 32}. The validation accuracies are averaged over the last 10 epochs. As the number of accumulation steps increases, the effective staleness in PETRA decreases, closing the gap with standard backpropagation.", "description": "This figure shows the validation accuracy of PETRA and standard backpropagation for different accumulation steps (k) when training a RevNet18 model on the ImageNet dataset. The x-axis represents the accumulation steps, while the y-axis shows the validation accuracy. The results are averaged over the last 10 epochs of training.  The graph demonstrates that increasing the accumulation steps reduces the effect of stale gradients in PETRA, leading to improved accuracy and closing the gap in performance with standard backpropagation.", "section": "4 Numerical experiments"}]