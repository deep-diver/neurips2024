{"references": [{"fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020-XX-XX", "reason": "This paper introduces denoising diffusion probabilistic models, a foundational concept for many text-to-image diffusion models discussed in the current paper."}, {"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-XX-XX", "reason": "This paper introduces Stable Diffusion, a significant advancement in text-to-image generation that is frequently referenced and compared to in the current paper."}, {"fullname_first_author": "Rinon Gal", "paper_title": "An image is worth one word: Personalizing text-to-image generation using textual inversion", "publication_date": "2022-XX-XX", "reason": "This paper is among the first to introduce the concept of personalizing text-to-image models for subject-driven generation, a core topic of the current research."}, {"fullname_first_author": "Nataniel Ruiz", "paper_title": "DreamBooth: Fine tuning text-to-image diffusion models for subject-driven generation", "publication_date": "2023-XX-XX", "reason": "This paper details DreamBooth, a widely used and highly relevant technique for personalizing text-to-image models, directly influencing the methods and comparative analysis in the current paper."}, {"fullname_first_author": "Edward J Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2021-XX-XX", "reason": "This paper introduces LoRA, a crucial parameter-efficient fine-tuning technique used in the current paper and frequently compared against in the experimental results."}]}