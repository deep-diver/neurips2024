[{"figure_path": "pASJxzMJb7/figures/figures_1_1.jpg", "caption": "Figure 1: Low-frequent words { } and high-frequent words { } are unevenly distributed in the embedding space [39, 24, 44, 10]. Consequently, the \u201capparent\u201d mean calculated by unweighted averaging often differs from the actual centroid \u2605.", "description": "The figure illustrates the uneven distribution of low-frequency and high-frequency words in the embedding space.  Low-frequency words are sparsely distributed while high-frequency words cluster together.  The unweighted mean (calculated by simply averaging all word vectors) is shown as a gray star, which is far from the true centroid (blue star) that is more representative of the overall distribution.  The difference highlights how the standard (unweighted) mean is misleading when word frequencies are non-uniform, which is common in natural language.", "section": "Motivation: type-token distinction and expected values"}, {"figure_path": "pASJxzMJb7/figures/figures_5_1.jpg", "caption": "Figure 2: The relationship between the 1st-order symmetry (Def. 3, x-axis), the 2nd-order symmetry (Def. 4, y-axis), and task performance (color). Each point represents either pre-trained or post-processed word embeddings (GloVe, word2Vec, and fastText). The Zipfian measure well captures the downstream task performance (right), while the uniform isotropic measure cannot (left).", "description": "This figure shows the relationship between the degree of symmetry (1st and 2nd moments) and the performance on downstream tasks for different word embedding models. The left panel shows the results using uniform word frequency, while the right panel uses Zipfian word frequency. The color of each point represents the task performance, showing that Zipfian weighting correlates better with downstream performance than uniform weighting.", "section": "3.3 Evaluation of embedding symmetry"}, {"figure_path": "pASJxzMJb7/figures/figures_8_1.jpg", "caption": "Figure 3: Relationships between the information content \u2013log p(w) and the vector norms ||w||2 for top 500 frequent words w. The figure in the center represents the pre-trained GloVe model. By using Zipfian whitening, the information content gets encoded in the norm (center to right). Conversely, with uniform whitening, this phenomenon does not occur (center to left).", "description": "This figure compares the relationship between information content (-log p(w)) and vector norms (||w||\u00b2) for the top 500 frequent words using different word embedding methods. The leftmost panel shows the results of uniform whitening; the middle left panel shows the results of uniform centering. The middle panel shows the results of a pre-trained GloVe model. The middle right panel shows the results of Zipfian centering, and the rightmost panel shows the results of Zipfian whitening.  The figure demonstrates how Zipfian whitening encodes information content into vector norms, unlike uniform whitening.", "section": "Emphasis on rare words by Zipfian prior"}]