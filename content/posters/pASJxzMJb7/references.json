{"references": [{"fullname_first_author": "S. Arora", "paper_title": "A Latent Variable Model Approach to PMI-based Word Embeddings", "publication_date": "2016-12-01", "reason": "This paper proposes a latent variable model for PMI-based word embeddings, providing a theoretical foundation for understanding the relationship between word embeddings and word co-occurrence probabilities."}, {"fullname_first_author": "T. Mikolov", "paper_title": "Distributed Representations of Words and Phrases and their Compositionality", "publication_date": "2013-12-01", "reason": "This seminal work introduced word2vec, a widely used method for learning word embeddings, which significantly advanced natural language processing and is deeply connected to the Zipfian distribution of word frequencies."}, {"fullname_first_author": "J. Devlin", "paper_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "publication_date": "2019-06-01", "reason": "BERT is a highly influential contextualized word embedding model that addresses the limitations of static word embeddings by considering word context, and its architecture is related to the uniform versus Zipfian discussion in the paper."}, {"fullname_first_author": "O. Levy", "paper_title": "Neural Word Embedding as Implicit Matrix Factorization", "publication_date": "2014-12-01", "reason": "This paper establishes a connection between neural word embeddings and matrix factorization, which is relevant to understanding the mathematical properties of embedding spaces and how they are affected by word frequencies."}, {"fullname_first_author": "J. Mu", "paper_title": "All-but-the-Top: Simple and Effective Postprocessing for Word Representations", "publication_date": "2018-01-01", "reason": "This paper introduces a simple post-processing technique for improving word embeddings, which is directly compared to the proposed Zipfian whitening approach in the paper."}]}