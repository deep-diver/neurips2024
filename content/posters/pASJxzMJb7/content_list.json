[{"type": "text", "text": "Zipfian Whitening ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sho Yokoi Tohoku University / RIKEN yokoi@tohoku.ac.jp ", "page_idx": 0}, {"type": "text", "text": "Han Bao Kyoto University bao@i.kyoto-u.ac.jp ", "page_idx": 0}, {"type": "text", "text": "Hidetoshi Shimodaira Kyoto University / RIKEN shimo@i.kyoto-u.ac.jp ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The word embedding space in neural models is skewed, and correcting this can improve task performance. We point out that most approaches for modeling, correcting, and measuring the symmetry of an embedding space implicitly assume that the word frequencies are uniform; in reality, word frequencies follow a highly non-uniform distribution, known as Zipf\u2019s law. Surprisingly, simply performing PCA whitening weighted by the empirical word frequency that follows Zipf\u2019s law significantly improves task performance, surpassing established baselines. From a theoretical perspective, both our approach and existing methods can be clearly categorized: word representations are distributed according to an exponential family with either uniform or Zipfian base measures. By adopting the latter approach, we can naturally emphasize informative low-frequency words in terms of their vector norm, which becomes evident from the information-geometric perspective [42], and in terms of the loss functions for imbalanced classification [36]. Additionally, our theory corroborates that popular natural language processing methods, such as skip-gram negative sampling [37], WhiteningBERT [26], and headless language models [23], work well just because their word embeddings encode the empirical word frequency into the underlying probabilistic model. ", "page_idx": 0}, {"type": "text", "text": "$\\circ$ https://github.com/cl-tohoku/zipfian-whitening ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Representing discrete words by continuous vectors is a fundamental and powerful framework of modern deep-learning-based natural language processing (NLP). Static word embeddings [43, 37], dynamic word embeddings [18, 33], and causal language models [45, 12, 54] have caused a paradigm shift\u2014they have greatly improved the performance of virtually all kinds of NLP applications and have been actively used in relevant areas as well. While the embedded units may be characters or subwords instead of words, we simply refer to them collectively as word. ", "page_idx": 0}, {"type": "text", "text": "Recently, the machine learning and NLP communities have discovered that the word embedding space is \u201cskewed\u201d and that correcting this can lead to better performance in downstream tasks [39, 21, 16, 56]. The isotropy of the embedding space would be one factor: vectors dispersing more evenly should be more discriminative than those clustered in the same direction [38, 21, 51]. Typically, such spatial symmetry in the embedding space is enhanced through centering/whitening [39, 16, 26]. ", "page_idx": 0}, {"type": "text", "text": "Nevertheless, we would like to point out that most existing approaches implicitly assume uniform word frequency to formalize spatial symmetry. Consider the classical centering operation as an example: we first calculate the mean of the word vectors, and then subtract it to ensure they are zero-meaned. This method, however, has an unexpected pitfall. Recall that the definition of the centroid or barycenter of a random vector $\\boldsymbol{x}\\sim p$ , assuming it has a finite set of distinct realizations, is given by $\\begin{array}{r}{\\mathbb{E}_{\\pmb{x}\\sim p}[\\pmb{x}]=\\sum_{i}p(\\pmb{x}_{i})\\pmb{x}_{i}}\\end{array}$ . The classical centering, based on the standard (unweighted) mean, implicitly assume s that all words occur uniformly $p(\\bar{\\pmb{w}_{1}})=\\cdot\\cdot\\cdot=p(\\pmb{w}_{n})$ . In reality, however, word frequencies are known to follow a highly non-uniform distribution1, creating a significant gap between the methodology and the actual usage of words. This seemingly obvious issue does not arise when addressing classical statistical estimation problems, as data vectors in our hands are usually representations of observations or instances. In contrast, word vectors used in NLP are representations of types or classes; each of them (such as the vector for \u2018the\u2019) abstracts the numerous instances (such as the tokens of \u2018the\u2019) appearing in the data. This problem of hidden frequencies becomes apparent in the cases where the type-token distinction [58] is crucial, such as when dealing with natural language data $(\\S\\,2)$ . The take-home message of this paper can be summarized as follows: use empirical word frequencies when calculating expected values. Following this very simple guideline leads to strong empirical outcomes $\\overline{{\\S\\ 3.2,\\S\\ 3.3)}}$ and opens a rich theoretical landscape $(\\S\\ 4,\\S\\ 5)$ . ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Notation Let $\\boldsymbol{\\mathcal{V}}=\\{w_{1},\\dots,w_{n}\\}$ denote the vocabulary, i.e., the set of words in interest. Bold-face $\\pmb{w}_{i}\\in\\mathbb{R}^{d}$ denotes the row vector of each word type $w_{i}$ , and $p(w_{i})\\in[0,1]$ denotes its frequency. ", "page_idx": 1}, {"type": "text", "text": "2 Motivation: type-token distinction and expected values ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Why have word frequencies been overlooked when considering the geometric properties of embedding spaces? This can be explained through the concept of type-token distinction [58], which is a fundamental concept in linguistics and related fields but generally not required in statistical machine learning. Here, type represents a class and token represents an instance. For example, the phrase \u2018perform natural language processing in a natural way\u2019 contains eight tokens and seven types. The instances \u2018natural\u2019 appear twice, but as a word type, it is counted only once. ", "page_idx": 1}, {"type": "text", "text": "With the type-token distinction in mind, let us take a fresh look at data matrices and their expected values. Typically, each row in a data matrix represents one observation, i.e., one instance token. If we want to centralize a set of data vectors, computing the unweighted mean is a natural way in the machine learning pipeline. On the other hand, each row of a word embedding matrix, i.e., word vector, is a type embedding. Each word vector abstracts the numerous instances appearing repeatedly in a corpus, though information on the frequency of instances for each word type is not encoded in it. The unweighted mean of word vectors treats type vectors as token vectors, resulting in the complete omission of word frequency information. ", "page_idx": 1}, {"type": "text", "text": "Let us describe the above idea formally. The data matrix $\\pmb{X}\\in\\mathbb{R}^{n\\times d}$ or the set of data vectors $\\{x_{i}\\}_{i=1}^{n}\\subseteq$ $\\mathbb{R}^{d}$ represents a collection of instances, observations, or tokens; then the empirical distribution is $\\mu_{\\pmb{X}}~=$ $\\textstyle\\sum_{i=1}^{n}{\\frac{1}{n}}\\delta(\\pmb{x}_{i})$ , where $\\delta$ is the Dirac delta function. Here, the unweighted mean can be seen as the expectation $\\begin{array}{r}{\\widehat{\\mathbb{E}}_{\\pmb{x}\\sim\\mu_{\\pmb{X}}}[\\pmb{x}]=\\sum_{i=1}^{n}\\frac{1}{n}\\pmb{x}_{i}}\\end{array}$ with the empirical distribution. On the other hand, the word embedding matrix $W\\in\\mathbb{R}^{n\\times d}$ or the set of word vectors $\\{\\pmb{w}_{i}\\}_{i=1}^{n}\\subseteq\\mathbb{R}^{d}$ represents a collection of types. When describing the empirical distribution, the hidden frequency $p$ of tokens is necessary. Given $p$ , the empirical distribution is $\\mu_{W}=p(w_{i})\\delta(\\pmb{w}_{i})$ . From this perspective, the centroid of the word vectors should be written as the expectation $\\begin{array}{r}{\\widehat{\\mathbb{E}}_{{\\pmb{w}}\\sim\\mu_{\\pmb{W}}}[{\\pmb{w}}]=\\sum_{i}p({w_{i}}){\\pmb{w}}_{i}}\\end{array}$ over $p$ . ", "page_idx": 1}, {"type": "image", "img_path": "pASJxzMJb7/tmp/3c0d6be8ae9e3ce7e8f4c87f9a4d0130642e22a7f9c53759a0045367d9c69430.jpg", "img_caption": ["Figure 1: Low-frequent words $\\{\\circ\\}$ and high-frequent words $\\{\\mathbb{O}\\}$ are unevenly distributed in the embedding space [39, 24, 44, 10]. Consequently, the \u201capparent\u201d mean calculated by unweighted averaging $\\star$ often differs from the actual centroid $\\star$ . "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "The distinction is not just \u201ctheoretical.\u201d First, refer to Fig. 1. Word vectors are known to cluster by frequency [39, 24, 44, 10]. In this situation, the centroid $\\star$ weighted by the word frequencies is located near the narrow region where high-frequent words are concentrated (a region with a light blue background), and thus differs from the unweighted mean $\\star$ . Second, see Table 1, which shows ", "page_idx": 1}, {"type": "text", "text": "50 words sampled from each of types and tokens. Uniform sampling from types, corresponding to an unweighted mean, tends to select mostly rare words from the heavy tail. Sampling from tokens clearly captures a more natural representation of language as it typically appears in text. ", "page_idx": 2}, {"type": "table", "img_path": "pASJxzMJb7/tmp/bc0aee86f89b1db5fe4224f197f98f644c05da931f7e45adac00c832bdfe2c75.jpg", "table_caption": ["Table 1: The difference between type-based sampling and token-based sampling. "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "3 Embedding symmetry ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Definition of embedding symmetry ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In mathematical science fields, such as high-dimensional probability theory [55] and the volume of convex bodies [27], there are numerous intriguing definitions of spatial symmetry. Among them, we begin with the definition of the symmetry of random vectors with their frequencies [48, 55]. This is suited for dealing with word vectors because they entail word frequencies, unlike usual data instances. ", "page_idx": 2}, {"type": "table", "img_path": "pASJxzMJb7/tmp/12d3d489846ecf79b869f09a8ac8e1882731f7d139bf370f552ecf21ad29c27c.jpg", "table_caption": ["From these definitions, we will develop methods to adjust given word vectors to be symmetric in $\\S\\ 3.2$ , and to evaluate the symmetry of given word vectors in $\\S\\ 3.3$ . "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "In machine learning and NLP, the spatial symmetry of embedding spaces is a hot topic, and numerous theories and algorithms have been proposed [41, 21, 38, 56]. However, the approach in many researches implicitly treats all vectors equally, ignoring word frequency information. In the following sections, we will detail both the empirical and theoretical issues that a uniform approach can cause, especially when applied to NLP tasks. Furthermore, when embeddings correspond to tokens rather than types\u2014such as in the internal representations of masked or causal language models\u2014a uniform approach tends to be effective. This point will be discussed in $\\S\\ 5.1$ . ", "page_idx": 2}, {"type": "text", "text": "3.2 Enhancement of embedding symmetry ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section proposes Zipfian whitening2, which symmetrizes a given set of word vectors with word frequency. At a glance, the most natural method to achieve Def. 1 and Def. 2 would be PCA whitening, also known as sphering. Notably, each step of whitening\u2014centering, decorrelation, and standardization\u2014implicitly involves calculating expected values. Our approach is simple: each time we calculate an expected value, we should weight it by the empirical word frequency. The specific algorithm is as shown in Algorithm 1. The only difference from general whitening is that it uses word frequency in the part highlighted in blue . Please refer to Appendix A for a formal explanation showing that the word vectors obtained by the proposed algorithm actually satisfy Def. 1 and Def. 2. ", "page_idx": 2}, {"type": "text", "text": "Algorithm 1 Zipfian whitening; a post-processing algorithm on word embeddings. The part highlighted in blue shows the difference from the typical centering and whitening. ", "page_idx": 3}, {"type": "text", "text": "Input: Word embeddings $\\{\\pmb{w}_{i}\\in\\mathbb{R}^{d}\\mid\\boldsymbol{w}_{i}\\in\\mathcal{V}\\}$ , word frequency $p!\\llap{/}\\nu\\to[0,1]$ .   \nOutput: Processed word embeddings. $\\{\\overline{{\\pmb{w}}}_{i}\\in\\mathbb{R}^{d}\\}_{i}$ are centered, $\\{\\widetilde{w}_{i}\\in\\mathbb{R}^{d}\\}_{i}$ are further whitened. Zipfian centering (1st moment):   \n1: $\\begin{array}{r}{\\widehat{\\pmb{\\mu}}\\leftarrow\\sum_{w_{i}\\in\\mathcal{V}}p(w_{i})\\pmb{w}_{i}\\in\\mathbb{R}^{d}}\\end{array}$   \n2: for all $w_{i}\\in\\mathcal{V}$ do   \n3: $\\pmb{\\overline{{w}}}_{i}\\leftarrow\\pmb{w}_{i}-\\pmb{\\widehat{\\mu}}\\in\\mathbb{R}^{d}$   \n4: end for Zipfian decorrelation and standardization (2nd moment):   \n5: $\\pmb{W}_{p}\\gets\\left[\\sqrt{p(w_{1})}\\,\\pmb{\\overline{{w}}}_{1}^{\\top},\\dots,\\sqrt{p(w_{|\\mathcal{V}|})}\\,\\pmb{\\overline{{w}}}_{|\\mathcal{V}|}^{\\top}\\right]^{\\top}\\in\\mathbb{R}^{|\\mathcal{V}|\\times d}$   \n6: $U\\Sigma V^{\\top}\\leftarrow\\mathrm{SVD}(W_{p})$ $\\uplus\\ \\Sigma=\\mathrm{diag}(\\sigma_{1},\\uplus\\dots,\\sigma_{d})\\in\\mathbb{R}^{d\\times d}$ consists of the singular values of $\\boldsymbol{W}_{p}$ .   \n7: for all $w_{i}\\in\\mathcal{V}\\,\\mathbf{do}$   \n8: wi wiV \u03a3\u22121 \u2190\u25b7 \u03a3\u22121 := diag(1/\u03c31, . . . , 1/\u03c3d) \u2208Rd\u00d7d.   \n9: end for ", "page_idx": 3}, {"type": "text", "text": "Table 2: The empirical performance of Zipfian whitening, which exploits the empirical frequency of words during expectation calculations. Each cell shows the STS-B [15] score $\\times100$ . By carefully performing the simple operation of whitening, it consistently outperforms powerful baseline methods. ", "page_idx": 3}, {"type": "table", "img_path": "pASJxzMJb7/tmp/d9adf82ab31eeb851664b965b92f6207b1c848d6d5a87722b3ac038562a4a012.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "Empirical evaluation: We confirm the effectiveness of Zipfian whitening (Algorithm 1) by measuring performance on standard sentence-level downstream tasks using post-processed word vectors. We employed the most standard word embeddings\u2014GloVe [43], word2vec [37], and fastText [11]\u2014and utilized the widely adopted evaluation tasks, including STS-B [15] and related benchmarks. Detailed experimental settings can be found in Appendix B. Table 2 shows the results on the STS-B task. Remarkably, the proposed Zipfian whitening shows significant advantages not only over standard (uniform) centering and whitening but also over the strong baseline method [7] specifically designed to create powerful sentence vectors. Consistent results were obtained with various benchmark datasets, multiple empirical word probabilities, and a language other than English (Appendix $\\mathrm{C})^{3}$ . In $\\S\\ 4.2$ , one reason for this remarkable performance is clarified from the perspective of information geometry. ", "page_idx": 3}, {"type": "text", "text": "3.3 Evaluation of embedding symmetry ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The community is greatly interested not only in making word vector spaces symmetric but also in evaluating how symmetric or asymmetric a space is [21, 49]. Here, we return to Def. 1 and Def. 2 and describe metrics for evaluating the symmetry of word embedding spaces with word frequency. ", "page_idx": 3}, {"type": "text", "text": "Degree of centrality\u2014the 1st moment of symmetry: Recall that, if the barycenter $\\mathbb{E}[v]$ is close to 0, then the random vector $\\pmb{v}$ can be considered symmetric in terms of the first moment (Def. 1). ", "page_idx": 3}, {"type": "text", "text": "Thue, examining the value of $\\|\\mathbb{E}[\\pmb{v}]\\|:=\\mathbb{E}[\\pmb{v}]-\\pmb{0}$ appears to be a reasonable way to measure the symmetry of the first moment. However, random vectors $\\pmb{v}$ and $\\alpha\\mathbf{\\partial}v$ $\\alpha\\in\\mathbb{R}_{>0}.$ ) should be considered equivalent in terms of spatial symmetry. Thus, we define the scale-invariant metric (Def. 3), obtained by dividing $\\|\\mathbb{E}[\\pmb{v}]\\|$ by the average length $\\mathbb{E}[\\|\\pmb{v}\\|]$ . ", "page_idx": 4}, {"type": "table", "img_path": "pASJxzMJb7/tmp/bee101a223bf4fcfe8f7797905e4da71b10b5f8f017e06230980ba3cc32a3b98.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "By definition, $\\mathrm{Sym}_{1}(v)$ takes values in $[0,1]$ , and $\\mathrm{Sym}_{1}(\\pmb{v})=1$ if and only if $\\pmb{v}$ is zero mean. ", "page_idx": 4}, {"type": "text", "text": "Degree of isotropy\u2014the 2nd moment of symmetry: If the covariance matrix $\\mathbb{E}[(\\pmb{v}-\\mathbb{E}[\\pmb{v}])(\\pmb{v}-$ $\\mathbb{E}[{\\pmb v}])^{\\top}]$ is a constant multiple of the identity matrix $\\boldsymbol{I}_{d}$ , i.e., if the random vector $\\pmb{v}$ has an equal spread in all directions, $\\pmb{v}$ is symmetric in terms of the second moment (Def. 2). Following convention, this degree can be confirmed by examining the flatness of the eigenspectrum. ", "page_idx": 4}, {"type": "table", "img_path": "pASJxzMJb7/tmp/ecb31c047d160f8b285417adf013915c1f3557b816f9b284cf822102744578f8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Note that the approach of measuring the entropy of the spectrum to evaluate the flatness of a signal can be found in many fields. For example, similar definitions are seen in probability processes [14] and signal processing [17, 47]. We also follow this standard and powerful line. ", "page_idx": 4}, {"type": "text", "text": "Algorithm: To compute the evaluation metrics of symmetry (Def. 3, Def. 4) for given word vectors, again, one should just use the empirical word frequency when calculating the expectations. A pseudocode for measuring symmetry is provided in Appendix E. ", "page_idx": 4}, {"type": "text", "text": "Empirical evaluation: To what extent does our symmetry score (an intrinsic evaluation of embedding spaces) correlate with downstream task performance (an extrinsic evaluation of those)? As baselines, we use versions of our symmetry score that do not account for word frequency, calculated in a uniform manner. We also compare with popular symmetry scores in NLP, the average of cosine similarity (Ave. Cos.) [21] and the recently proposed IsoScore [49]. Note that all these baselines implicitly assume uniform word frequency. Additional experimental settings can be found in Appendix B. Fig. 2 shows the results. The right side of Fig. 2 demonstrates the superiority of the Zipfian approach. Moving from the bottom-left to the top-right of the figure\u2014i.e. as both the 1st $x$ -axis) and 2nd moments $y$ -axis) of the symmetry score increase\u2014it is clearly visible that the downstream task performance increases (the color becomes more red). In contrast, in the left-hand plot, which assumes uniform word frequency, there is no observed relationship between the symmetry score $x$ and $y$ -axis) and the downstream task performance (color). Table 3 lists the correlation coefficients between the symmetry scores and downstream task performance in more detail. It can be seen that the symmetry scores considering word frequency can \u201cpredict\u201d downstream task performance with remarkably high correlation. On the other hand, the \u201cprediction\u201d performance of other metrics, including Ave. Cos. and IsoScore that implicitly assume uniform word frequency, is unsatisfactory. Surprisingly, when the most popular Ave. Cos. metric shows almost no correlation (0.04) with downstream task performance (STS-B), Zipfian symmetry metric has a strong positive correlation (0.83) with it. ", "page_idx": 4}, {"type": "text", "text": "4 Why is Zipfian whitening better than uniform whitening? ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "A natural question is why the Zipfian approach empirically dramatically outperforms the uniform approach. We provide a theoretical explanation using Table 4. In a nutshell, a significant difference arises depending on whether the base measure of an exponential family is uniform or Zipfian. ", "page_idx": 4}, {"type": "image", "img_path": "pASJxzMJb7/tmp/18aa329454fb2080952e2dbb44e01c318feb0c4601fcce7bed5a605280398143.jpg", "img_caption": ["Figure 2: The relationship between the 1st-order symmetry (Def. 3, $x$ -axis), the 2nd-order symmetry (Def. 4, $y$ -axis), and task performance (color). Each point represents either pre-trained or postprocessed word embeddings (GloVe, word2Vec, and fastText). The Zipfian measure well captures the downstream task performance (right), while the uniform isotropic measure cannot (left). "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Table 3: Spearman\u2019s $\\rho\\times100$ (each cell) between the symmetry scores (each column) and downstream STS-B performance (each row), on pre-trained and post-processed embeddings (GloVe, word2Vec, and fastText). The scores based on the Zipfian prior show a significantly higher correlation with task performance compared to those based on the uniform prior including Ave. Cos. and IsoScore. ", "page_idx": 5}, {"type": "table", "img_path": "pASJxzMJb7/tmp/c2848ecbcaeeabb593b0fa4c2a3d224f56dafdeb5e80936f682c9aa0f7ed21ed.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "4.1 Characterization through generative model, partition function, and whitening ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Exponential families: Hereafter, we interpret two salient generative models from the viewpoint of exponential families: one given by Arora et al. [6] and the other generalizing the Levy\u2013Goldberg formula [32, Eq. (7)]. Details of these models will be provided shortly. An exponential family is a class of probability distributions of a random variable $\\textbf{\\em x}$ parametrized by a parameter $\\pmb{\\theta}$ , written in the following (canonical) form: ", "page_idx": 5}, {"type": "equation", "text": "$$\n{\\begin{array}{l}{\\scriptstyle{\\mathrm{IV~antowning~(callontsal)~anm.}}}\\\\ {p({\\boldsymbol{x}}\\mid{\\boldsymbol{\\theta}})=\\pi({\\boldsymbol{x}})\\exp\\left(\\langle{\\boldsymbol{x}},{\\boldsymbol{\\theta}}\\rangle-\\psi({\\boldsymbol{\\theta}})\\right),\\quad\\psi({\\boldsymbol{\\theta}}):=\\log Z({\\boldsymbol{\\theta}})=\\log\\left(\\sum_{{\\boldsymbol{x}}}\\pi({\\boldsymbol{x}})\\exp(\\langle{\\boldsymbol{x}},{\\boldsymbol{\\theta}}\\rangle)\\right),}\\end{array}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\textbf{\\em x}$ is a sufficient statistic, $\\pmb{\\theta}$ is called a natural parameter, $\\pi$ is the base measure (or \u201cprior\u201d), and $\\psi$ is the log-partition function. Once we specify the base measure $\\pi$ and the canonical pair $(x,\\pmb\\theta)$ , the log-partition function is determined. That being said, the base measure $\\pi$ is the design choice of an exponential family left for us. In the following, we specifically examine an exponential family of distributions in the form $p(w\\mid c)$ , where word $w$ is predicted given context $c$ . Specifically, the context represents a co-occurring word (in static word embeddings), a cloze sentence (in masked language models), or a sentence prefix (in causal language models). In all of these cases, we predict a word with the logit $\\langle{\\pmb w},{\\pmb c}\\rangle$ , making the exponential family a natural probabilistic model. Here, the vector $^c$ represents the vector expression of the context $c$ , known as the \u201ccontext vector.\u201d Note that, even for the same word $t$ , the predicted word vector $w(t)$ and the predicting context vector $c(t)$ are distinct. ", "page_idx": 5}, {"type": "text", "text": "Uniform prior: Arora et al. firstly considered a log-linear generative model of word embeddings given a context (6) and demonstrated that when the generative model is adopted with normalized context vectors and a huge vocabulary, the partition function asymptotically becomes constant (8) [6, Lemma 2.1]. Here, we can regard that this model belongs to the exponential family with the uniform base measure $\\pi(w)=\\pi(c)=1/|\\gamma|\\,^{4}$ . ", "page_idx": 5}, {"type": "text", "text": "Table 4: Through the differences in the underlying generative models, the empirical superiority of Zipfian whitening over uniform whitening can be understood. ", "page_idx": 6}, {"type": "text", "text": "Generative models behind the (whitened) embeddings ", "page_idx": 6}, {"type": "equation", "text": "$$\np(w\\mid c)={\\frac{\\pi(w)\\exp(\\langle w,c\\rangle)}{Z(c)}},\\quad Z(c)=\\sum_{w}\\pi(w)\\exp(\\langle w,c\\rangle);\\quad p(w,c)=p(w\\mid c)\\pi(c)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\np_{\\widehat{\\cup}}(w\\mid c)=\\frac{1\\exp(\\langle w,c\\rangle)}{Z_{\\widehat{\\cup}}(c)},\\pi(c)\\propto1\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\np_{\\bigodot}(w\\mid c)=\\frac{p(w)\\exp(\\langle w,c\\rangle)}{Z_{\\bigodot}(c)},\\pi(c)=p(c)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Partition functions become constant under certain conditions ", "page_idx": 6}, {"type": "text", "text": "At the optimal solution of the corresponding loss, ", "page_idx": 6}, {"type": "equation", "text": "$$\nZ_{\\widehat{\\langle}\\rangle}(\\boldsymbol{c}):=\\sum_{\\boldsymbol{w}}\\exp(\\langle\\boldsymbol{w},\\boldsymbol{c}\\rangle)=\\mathrm{const.\\quad}(8)\\left[6\\right]\\qquad Z_{\\widehat{\\langle}\\rangle}(\\boldsymbol{c}):=\\sum_{\\boldsymbol{w}}p(\\boldsymbol{w})\\exp(\\langle\\boldsymbol{w},\\boldsymbol{c}\\rangle)=\\mathrm{const.}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Whitening coarsely achieves a constant partition function ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Z_{\\bigodot}(c)}\\\\ &{=|\\mathcal{V}|+\\underbrace{\\sum_{w}w}_{\\iff}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Vector norm under generative models ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\|\\pmb{w}\\|_{2}^{2}\\approx2d\\log p(w)-2Z\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\n\\|\\pmb{w}\\|_{\\pmb{G}(w)}^{2}\\approx2\\mathrm{KL}(p(\\cdot)||p(\\cdot\\mid w))\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "long vector $\\leftrightarrow$ frequent (uninformative) word ", "page_idx": 6}, {"type": "text", "text": "Loss and error corresponding to generative models $p(w\\mid c)$ ", "page_idx": 6}, {"type": "text", "text": "softmax cross-entropy loss ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\underset{(w,c)}{\\mathbb{E}}-\\log\\frac{\\exp(\\langle w,c\\rangle)}{Z_{\\ @}(c)}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\n\\underset{(w,c)}{\\mathbb{E}}-\\log\\frac{p(w)\\exp(\\langle w,c\\rangle)}{Z_{\\mathcal{Q}}(c)}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{P}_{(w,c)}[w\\notin\\arg\\operatorname*{max}_{w^{\\prime}}\\langle w^{\\prime},c\\rangle]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "equation", "text": "$$\n\\frac{1}{|\\mathcal{V}|}\\sum_{w\\in\\mathcal{V}}\\mathbb{P}_{c|w}[w\\notin\\underset{w^{\\prime}}{\\arg\\operatorname*{max}}\\langle w^{\\prime},c\\rangle]\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Zipfian prior: An exponential family adopted with the Zipfian measure can be written as (7). This generative model can be naturally derived from the skip-gram model with negative sampling (SGNS) [37]. By assuming that the linear model $c\\mapsto\\langle w,c\\rangle$ is sufficiently capable of discriminating cooccurring words and negative samples (as in the realizable case), we can see that the generative model of the word embeddings must comply with the following formula: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\log{\\frac{p(w,c)}{p(w)p(c)}}-\\log k=\\langle{\\pmb w},c\\rangle,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $k$ is the number of negative samples. This optimality formula owes to Levy and Goldberg [32], and we call (19) the Levy\u2013Goldberg formula. A more concise derivation is later given by Oyama et al. [42]. We can regard the Levy\u2013Goldberg formula as an exponential family with the Zipfian base measure, $\\pi(w)=p(w)$ and $\\pi(c)=p(c)$ , and the constant log-partition function $Z_{\\circled{z}}(c)\\equiv k^{-1}$ . The generative model (7) is a relaxation of the Levy\u2013Goldberg formula since we do not impose the realizability assumption necessary for the derivation of (19). ", "page_idx": 6}, {"type": "text", "text": "What does whitening do? Mu and Viswanath [39] proposed a method to approximately make the partition function of the uniform prior model constant by centering the word vectors and removing the top principal components (10). Our Zipfian whitening corresponds to Mu and Viswanath\u2019s post-processing method, in the sense that ours and theirs make the partition function constant up to the second moment (11) and (10), respectively. In summary, Zipfian whitening (11) transforms a probabilistic model into an exponential family adopted with the Zipfian base measure (7), making it closer to the Levy\u2013Goldberg formula (19). ", "page_idx": 7}, {"type": "text", "text": "4.2 Emphasis on rare words by Zipfian prior ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Let us explore further why the Zipfian prior results in good performance in downstream tasks $(\\S\\ 3.2)$ . In summary, the Zipfian prior approach emphasizes low-frequency words, while the uniform prior approach emphasizes high-frequency words, both from perspectives of vector norms and errors/losses. So far in this paper, we have repeatedly discussed weighting each word according to frequency, so it may seem contradictory that Zipfian approach emphasizes low-frequency words as a result. To illustrate, let us reconsider centering. In centering, the mean vector is subtracted from each vector. Weighting each word vector by frequency when constructing the mean vector means that signals corresponding to high-frequency words are removed more substantially from each vector. The emphasis on low-frequency words has been repeatedly supported throughout the history of NLP and information retrieval, such as Luhn\u2019s hypothesis [34], inverse document frequency (IDF) [53], and smooth inverse frequency (SIF) [7]. For instance, it is reasonable to emphasize the word \u2018isotropy\u2019 when creating a sentence embedding containing both words \u2018the\u2019 and \u2018isotropy\u2019. ", "page_idx": 7}, {"type": "text", "text": "From the perspective of vector norm ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Under the Zipfian prior model, words with larger information content have longer (emphasized) vector representations. Conversely, under the uniform prior model, words with smaller information content have longer (emphasized) vector representations. ", "page_idx": 7}, {"type": "text", "text": "As a representative example of uniform prior models, the norms of word vectors learned by random walk language models are theoretically and empirically proportional to word frequency (12) (see Eq. (2.4) and Fig. 2 in Arora et al. [6]). That is, in such embedding space, words with less information (e.g., \u2018the\u2019) are emphasized. This tendency is consistently observed in dynamic language models and causal language models that adopt the softmax cross-entropy loss, another typical example of the uniform prior family [28]. By contrast, when training word embeddings with skip-gram negative sampling [37], the word embeddings follow the Zipfian prior family, and their norms become larger with greater information, which we show subsequently [50, 60, 42]. Based on the formulation of the exponential family and following Eq. (12) of Oyama et al. [42], we formally describe the norm properties of the word vectors obtained from the Zipfian prior model. ", "page_idx": 7}, {"type": "text", "text": "Theorem 1 (The norm of a word vector learned with empirical Zipfian prior models reflect the information amount of the word; a refined version of [42] Eq. (12)). Assume that word embeddings $\\{w_{i}\\}_{i}$ follow the Zipfian prior model (7), For the same word $t$ , the vector $\\pmb{w}$ on the predicted side and the vector c on the predicting side are shared: ${\\pmb w}(t)\\equiv{\\pmb c}(t)$ (weight tying), and $\\begin{array}{r}{\\dot{\\sum_{t\\in\\mathcal{V}}{p(t)}}\\pmb{w}(t)=\\mathbf{0}}\\end{array}$ (centered w.r.t. Zipfian prior), then each word vector $w(t)$ satisfy ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\|w(t)\\|_{G(t)}^{2}\\approx2\\mathrm{KL}(p(\\cdot)\\|p(\\cdot\\mid t)),\\ \\ G(t):=\\sum_{t^{\\prime}\\in\\mathcal{V}}p(t^{\\prime}\\mid t)c(t^{\\prime})c(t^{\\prime})^{\\top},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\|\\pmb{w}\\|_{A}$ with a positive definite matrix $\\pmb{A}$ denote a norm based on a quadratic form $\\sqrt{w^{\\top}A w^{5}}$ .   \nProof. Refer to Appendix F. ", "page_idx": 7}, {"type": "text", "text": "In Fig. 3, we experimentally confirmed that the norms of informative words become larger with Zipfian whitening (shown from center to the right in Fig. 3), bringing them closer to the ideal Zipfian prior model6. ", "page_idx": 7}, {"type": "image", "img_path": "pASJxzMJb7/tmp/268fe192c3a4cb4e8d5979e730dcd26758fd23c58aaeab4cf8f016ef59510282.jpg", "img_caption": ["Figure 3: Relationships between the information content $-\\log p(w)$ and the vector norms $\\lVert\\pmb{w}\\rVert_{2}$ for top 500 frequent words $w$ . The figure in the center represents the pre-trained GloVe model. By using Zipfian whitening, the information content gets encoded in the norm (center to right). Conversely, with uniform whitening, this phenomenon does not occur (center to left). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "From the perspective of error and loss ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The error and loss functions associated with the Zipfian prior model emphasize low-frequency words. In contrast, the error and loss functions of the uniform prior model focus on the average loss across the entire dataset, resulting in a greater emphasis on high-frequency words. ", "page_idx": 8}, {"type": "text", "text": "The standard classification loss is the softmax cross-entropy loss (14). By taking its expectation over the dataset $\\{(w,c)\\}$ , embeddings associated with higher-frequency words receive more updates because the softmax is the uniform inverse link, corresponding to the uniform prior model. By contrast, the logit-adjusted loss (15) has been proposed to tackle class imbalance [36]. From our viewpoint, the logit adjustment term $p(w)$ makes the inverse link belong to the Zipfian prior model. The softmax and logit-adjusted losses are Fisher consistent to the misclassification (16) and balanced (17) error rates, respectively. As the latter tends to stress minor classes, the logit-adjusted loss and Zipfian prior model are suitable for emphasizing low-frequency words during the learning process. ", "page_idx": 8}, {"type": "text", "text": "Another prominent loss function for representation learning is contrastive loss, with the SGNS loss (word2vec) [37] as a representative example in the context of word representation learning. This loss similarly uses a loss aligned with the Zipfian prior: ", "page_idx": 8}, {"type": "equation", "text": "$$\n-\\underset{(w,c)}{\\mathbb{E}}\\left[\\log\\sigma(\\langle{\\pmb w},{\\pmb c}\\rangle)+\\sum_{w_{i}^{\\prime}\\sim p(w)}^{i=1,\\dots,k}\\log\\sigma(-\\langle{\\pmb w}_{i}^{\\prime},{\\pmb c}\\rangle)\\right],\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\sigma$ is sigmoid function, and $k$ is the number of negative samples. Since high-frequency words are more likely to be sampled as negative examples, the loss has less impact on high-frequency words in positive examples. Consequently, low-frequency positive words are relatively emphasized in representation learning. The Levy\u2013Goldberg formula in the previous section describes the properties of an ideally trained word2vec model, which are essentially the properties of Zipfian prior models. ", "page_idx": 8}, {"type": "text", "text": "5 Unified explanation of the efficacy of existing methods ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Distinguishing the distribution that the base measure follows helps us understand why some existing NLP methods are effective. ", "page_idx": 8}, {"type": "text", "text": "5.1 Uniform whitening of token embeddings $\\approx$ Zipfian whitening of type embeddings ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Masked language models like BERT [18] and RoBERTa [33] produce dynamic (contextualized) token embeddings. Adding up such token embeddings of constituent tokens to create sentence embeddings often leads to poor empirical performance [46]. However, symmetrizing significantly improves their performance; such methods including \u201cbatch centering,\u201d \u201cWhiteningBERT,\u201d and contrastive learning methods [16, 46, 59, 22, 26, 57]. This improvement can also be explained from the perspective of the Zipfian prior. A dataset or corpus is first fed into the model to obtain token embeddings7. Centering/whitening is then applied to this entire set of embeddings. As this token embedding (multi)set has the multiplicity asymptotically proportional to the word frequency, ", "page_idx": 8}, {"type": "text", "text": "Table 5: The empirical performance difference between \u201cuniform\u201d\u2014enforced centering and whitening with a uniform prior for dynamic embeddings, and \u201cZipfian\u201d\u2014conventional uniform centering and whitening over tokens with an implicit Zipfian prior over types. Each cell shows the STS-B [15] score $\\times100$ . This comparison reveals that token-level uniform centering/whitening, corresponding to type-level Zipfian centering/whitening, leads to empirically better performance. ", "page_idx": 9}, {"type": "table", "img_path": "pASJxzMJb7/tmp/72595acac6cccd07d908207364615b4600a498bce240a1c59df751a12ae08bd4.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "this uniform centering/whitening of token embeddings corresponds to the word-frequency-weighted (Zipfian) centering/whitening of type embeddings. For a more formal description of the above explanations, please refer to Appendix H. Additionally, recent work has found that contrastive additive sentence encoders implicitly weight words by their information content [30]. This finding is consistent with the previous discussion on vector norms $(\\S\\,4.2)$ , and can be seen as indirect evidence supporting the idea that these models belong to the Zipfian prior family. ", "page_idx": 9}, {"type": "text", "text": "This idea can also be supported by empirical evidence. This idea is also supported by empirical evidence. To establish a baseline for centering and whitening token embeddings under a uniform prior, we scale each embedding by the reciprocal of its type frequency, ensuring uniform treatment across types. Refer to the Appendix H for the detailed computation of this pseudo uniform approach and a formal explanation of how it achieves type uniformity. Table 5 shows the results. Comparing the pseudo-uniform centering/whitening (which assumes a uniform prior over types) with the conventional token-level uniform centering/whitening (which implicitly assumes a Zipfian prior over types) reveals that the latter approach based on a Zipfian prior empirically achieves better performance. Additional experimental settings and results can be found in Appendix B and Appendix I. ", "page_idx": 9}, {"type": "text", "text": "5.2 Headless causal language model roughly belongs to Zipfian prior family ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "The recently proposed headless language model [23] uses only words within the same batch to predict next tokens with a pseudo-softmax function. This method originally aimed to reduce the computational cost of the softmax function in the $\\vert\\protect\\nu\\vert$ direction, but an interesting side effect is the improvement in the performance. This success can also be explained from the perspective of Zipfian priors. If we repeatedly sample small batches, the sampling frequency of each word will increasingly reflect its true frequency as the batch size approaches 1. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Standard methods for adjusting and measuring symmetries in word embedding spaces\u2014such as centering and whitening\u2014implicitly assume uniformly distributed word frequencies, which is unrealistic. We hypothesize that, based on the type-token distinction, using empirical Zipfian word frequencies is essential when calculating the expectation $(\\S\\ 2)$ . Based on the idea and the definitions of firstand second-order symmetry in random vectors, we derived Zipfian whitening, which enhances the symmetry of the word embedding space. Even though it is nearly identical to standard PCA whitening, Zipfian whitening significantly outperforms existing methods $(\\S\\ 3.2)$ . Similarly, we derived a metric to evaluate the symmetry of word embedding spaces. Our intrinsic metrics showed a strong correlation with extrinsic task performance, even when popular metrics show almost none $\\left(\\S\\ 3.3\\right)$ . We then presented a framework explaining the differences in effect between whitening based on uniform and Zipfian approaches, by attributing them to differences in the base measure of the exponential family $(\\S\\,4.1)$ . By further exploring this viewpoint through information geometry and loss functions, we showed how the Zipfian approach emphasizes the informativeness of low-frequency words (\u00a7 4.2). Lastly, through our proposed viewpoint, we found that popular NLP methods perform well because their word embeddings end up encoding a Zipfian prior; such models include word2vec [37] (Fig. 4.2), WhiteningBERT [26] (\u00a7 5.1), and headless language models [23] (\u00a7 5.2). ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work is supported by JST ACT-X Grant Number JPMJAX200S and JSPS KAKENHI Grant Number 22H05106. We received numerous constructive and valuable comments from the anonymous reviewers of NeurIPS 2024, which have significantly contributed to the quality improvements from the submission version to the camera-ready version. We would also like to thank Hayato Tsukagoshi of Nagoya University for his insightful comments on the handling of dynamic embeddings and on the experimental setup of the SimCSE paper [22], including minor discrepancies between the paper\u2019s description and its actual implementation. We also extend our gratitude to the organizers and participants of MLSS 2024, the Tohoku NLP group, the Shimodaira lab at Kyoto University, and many members of the Japanese NLP and machine learning community, for their constructive feedback and motivating encouragement throughout our research discussions. ", "page_idx": 10}, {"type": "text", "text": "Limitations ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "How these assumptions might be violated in practice ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "In our theoretical analysis concerning norms, and in the discussion on the relationship between whitening and normalization constants, we have proceeded by ignoring the residual terms beyond the second order. Empirically, focusing only on the first and second order has yielded significant results. However, to accurately identify cases where the proposed method might fail, a detailed theoretical and empirical examination of the asymptotic behavior of higher-order moments might be crucial. This remains an important future work. ", "page_idx": 10}, {"type": "text", "text": "The condition that the partition function is constant is only a necessary condition from the perspective of both the generative model\u2019s optimal solution and whitening. The true logical relationship between whitening and the generative model has not been clarified. In particular, verifying whether the projection through whitening allows us to transition between the two model families (the uniform family and the Zipfian family) is an intriguing and valuable direction for both theoretical exploration and practical application. ", "page_idx": 10}, {"type": "text", "text": "The scope of the empirical claims made ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Our experiments primarily focused on static and dynamic word embeddings, as many of their theoretical properties have been understood and they have been central to the rise of isotropization. Admittedly, this paper also advances our understanding of causal language models. However, to make a more significant practical impact in the era of large language models, employing the proposed method as a regularization term for next-token prediction holds great promise for future work. ", "page_idx": 10}, {"type": "text", "text": "The experiments utilized typical downstream NLP tasks, particularly popular datasets for sentencelevel semantic tasks. By scaling up the task set to include word-level tasks or leveraging a broader range of multilingual data, we can more robustly demonstrate the practical utility of the proposed framework. ", "page_idx": 10}, {"type": "text", "text": "The factors that influence the performance of our approach ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The proposed method inherently involves numerically unstable calculations, such as multiplying by the inverse of small singular values. Embeddings for low-frequency words are often far from converged even after extensive pre-training, and the eigenvalues of the embedding space are known to decay. Given these situations, the adverse effects of small singular values are plausible. Considering recent advancements in whitening techniques, developing a more numerically stable algorithm is an important direction for future work. ", "page_idx": 10}, {"type": "text", "text": "Broader Impacts ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Potential impacts to AI alignment Dohmatob et al. [19] reported that repeated sampling from generative AIs may shift word frequency distributions toward lighter-tailed distributions. This may reduce linguistic diversity and lead to cultural homogenization by diminishing region-specific or culturally unique expressions. Our Zipfian whitening and similar regularization methods could enhance output diversity, enriching the linguistic landscape. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Potential negative societal impacts The sentence similarity tasks used in our evaluation experiments are now considered core technologies for RAG (retrieval-augmented generation), which is essential when large language models leverage external resources. If chatbots generate responses tailored to user ideologies or preferred information sources, it may result in negative societal impacts, including political agitation. ", "page_idx": 11}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "[1] E. Agirre, D. Cer, M. Diab, and A. Gonzalez-Agirre. SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity. In E. Agirre, J. Bos, M. Diab, S. Manandhar, Y. Marton, and D. Yuret, editors, \\*SEM 2012: The First Joint Conference on Lexical and Computational Semantics \u2013 Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 385\u2013393, Montr\u00e9al, Canada, 7-8 June 2012. Association for Computational Linguistics. URL https://aclanthology.org/S12-1051. [2] E. Agirre, D. Cer, M. Diab, A. Gonzalez-Agirre, and W. Guo. \\*SEM 2013 shared task: Semantic Textual Similarity. In M. Diab, T. Baldwin, and M. Baroni, editors, Second Joint Conference on Lexical and Computational Semantics ( $\\ast S E M)$ , Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity, pages 32\u201343, Atlanta, Georgia, USA, June 2013. Association for Computational Linguistics. URL https://aclanthology.org/S13-1004. [3] E. Agirre, C. Banea, C. Cardie, D. Cer, M. Diab, A. Gonzalez-Agirre, W. Guo, R. Mihalcea, G. Rigau, and J. Wiebe. SemEval-2014 Task 10: Multilingual Semantic Textual Similarity. In P. Nakov and T. Zesch, editors, Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 81\u201391, Dublin, Ireland, Aug. 2014. Association for Computational Linguistics. doi: 10.3115/v1/S14-2010. URL https://aclanthology.org/S14-2010.   \n[4] E. Agirre, C. Banea, C. Cardie, D. Cer, M. Diab, A. Gonzalez-Agirre, W. Guo, I. Lopez-Gazpio, M. Maritxalar, R. Mihalcea, G. Rigau, L. Uria, and J. Wiebe. SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability. In P. Nakov, T. Zesch, D. Cer, and D. Jurgens, editors, Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 252\u2013263, Denver, Colorado, June 2015. Association for Computational Linguistics. doi: 10.18653/v1/S15-2045. URL https://aclanthology.org/S15-2045.   \n[5] E. Agirre, C. Banea, D. Cer, M. Diab, A. Gonzalez-Agirre, R. Mihalcea, G. Rigau, and J. Wiebe. SemEval2016 Task 1: Semantic Textual Similarity, Monolingual and Cross-Lingual Evaluation. In S. Bethard, M. Carpuat, D. Cer, D. Jurgens, P. Nakov, and T. Zesch, editors, Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 497\u2013511, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/S16-1081. URL https://aclanthology. org/S16-1081.   \n[6] S. Arora, Y. Li, Y. Liang, T. Ma, and A. Risteski. A Latent Variable Model Approach to PMI-based Word Embeddings. TACL, 4:385\u2013399, 12 2016. ISSN 2307-387X. doi: 10.1162/tacl{\\_}a{\\_}00106. URL https://aclweb.org/anthology/papers/Q/Q16/Q16-1028/.   \n[7] S. Arora, Y. Liang, and T. Ma. A Simple but Tough-to-Beat Baseline for Sentence Embeddings. In ICLR, 2017. URL https://openreview.net/forum?id $\\cdot$ SyK00v5xx.   \n[8] A. Bakarov. A survey of word embeddings evaluation methods. arXiv [cs.CL], Jan. 2018. [9] S. Bird and E. Loper. NLTK: The Natural Language Toolkit. In Proceedings of the ACL Interactive Poster and Demonstration Sessions, pages 214\u2013217, 7 2004. URL https://www.aclweb.org/anthology/ P04-3031.   \n[10] D. Bis\u00b4, M. Podkorytov, and X. Liu. Too much in common: Shifting of embeddings in transformer language models and its implications. pages 5117\u20135130, June 2021.   \n[11] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov. Enriching Word Vectors with Subword Information. TACL, 5:135\u2013146, 2017. doi: 10.1162/tacl_a_00051. URL https://www.aclweb.org/anthology/ papers/Q/Q17/Q17-1010/.   \n[12] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language Models are Few-Shot Learners. In NeurIPS, volume 33, pages 1877\u20131901, 2020. URL https://proceedings.neurips.cc/paper_ files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.   \n[13] E. Bruni, G. Boleda, M. Baroni, and N.-K. Tran. Distributional Semantics in Technicolor. In ACL, pages 136\u2013145, 7 2012. URL https://www.aclweb.org/anthology/P12-1015.   \n[14] L. L. Campbell. Minimum coefficient rate for stationary random processes. Information and Control, 3(4): 360\u2013371, Dec. 1960.   \n[15] D. Cer, M. Diab, E. Agirre, I. Lopez-Gazpio, and L. Specia. SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation. In SemEval, pages 1\u201314, 8 2017. doi: 10.18653/v1/S17-2001. URL https://www.aclweb.org/anthology/papers/S/S17/S17-2001/.   \n[16] X. Chen, N. Ding, T. Levinboim, and R. Soricut. Improving Text Generation Evaluation with Batch Centering and Tempered Word Mover Distance. In First Workshop on Evaluation and Comparison of NLP Systems, pages 51\u201359, Online, 11 2020. Association for Computational Linguistics. doi: 10.18653/v1/ 2020.eval4nlp-1.6. URL https://www.aclweb.org/anthology/2020.eval4nlp-1.6.   \n[17] R. R. Coifman and M. V. Wickerhauser. Entropy-based algorithms for best basis selection. IEEE Trans. Inf. Theory, 38(2):713\u2013718, Mar. 1992.   \n[18] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL, pages 4171\u20134186, 2019. doi: 10.18653/v1/N19-1423. URL https://www.aclweb.org/anthology/N19-1423/.   \n[19] E. Dohmatob, Y. Feng, P. Yang, F. Charton, and J. Kempe. A tale of tails: Model collapse as a change of scaling laws. arXiv [cs.LG], Feb. 2024.   \n[20] K. Ethayarajh. Unsupervised Random Walk Sentence Embeddings: A Strong but Simple Baseline. In Rep4NLP, pages 91\u2013100, 7 2018. URL https://aclweb.org/anthology/papers/W/W18/ W18-3012/.   \n[21] K. Ethayarajh. How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings. In EMNLP, pages 55\u201365, 2019. URL http://arxiv.org/abs/ 1909.00512.   \n[22] T. Gao, X. Yao, and D. Chen. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894\u20136910, Online and Punta Cana, Dominican Republic, Nov. 2021. Association for Computational Linguistics.   \n[23] N. Godey, \u00c9. V. de La Clergerie, and B. Sagot. Headless language models: Learning without predicting with contrastive weight tying. arXiv [cs.CL], Sept. 2023.   \n[24] C. Gong, D. He, X. Tan, T. Qin, L. Wang, and T.-Y. Liu. FRAGE: Frequency-agnostic word representation. Adv. Neural Inf. Process. Syst., 31, 2018.   \n[25] P. He, X. Liu, J. Gao, and W. Chen. {DEBERTA}: {DECODING}-{enhanced} {bert} {with} {disentangled} {attention}. In ICLR, 2021. URL https://openreview.net/forum?id=XPZIaotutsD.   \n[26] J. Huang, D. Tang, W. Zhong, S. Lu, L. Shou, M. Gong, D. Jiang, and N. Duan. WhiteningBERT: An easy unsupervised sentence embedding approach. In M.-F. Moens, X. Huang, L. Specia, and S. W.-T. Yih, editors, Findings of the Association for Computational Linguistics: EMNLP 2021, pages 238\u2013244, Punta Cana, Dominican Republic, Nov. 2021. Association for Computational Linguistics.   \n[27] R. Kannan, L. Lov\u00e1sz, and M. Simonovits. Random walks and an $\\mathrm{o}^{*}(\\mathrm{n}5)$ volume algorithm for convex bodies. Random Struct. Algorithms, 11(1):1\u201350, Aug. 1997.   \n[28] G. Kobayashi, T. Kuribayashi, S. Yokoi, and K. Inui. Transformer language models handle word frequency in prediction head. pages 4523\u20134535, July 2023.   \n[29] K. Kurihara, D. Kawahara, and T. Shibata. JGLUE: Japanese general language understanding evaluation. In N. Calzolari, F. B\u00e9chet, P. Blache, K. Choukri, C. Cieri, T. Declerck, S. Goggi, H. Isahara, B. Maegaard, J. Mariani, H. Mazo, J. Odijk, and S. Piperidis, editors, Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 2957\u20132966, Marseille, France, June 2022. European Language Resources Association. URL https://aclanthology.org/2022.lrec-1.317.   \n[30] H. Kurita, G. Kobayashi, S. Yokoi, and K. Inui. Contrastive learning-based sentence encoders implicitly weight informative words. In H. Bouamor, J. Pino, and K. Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 10932\u201310947, Singapore, Dec. 2023. Association for Computational Linguistics.   \n[31] Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, Eytan Ruppin. Placing search in context: the concept revisited. ACM Trans. Inf. Syst. Secur., 20(1):116\u2013131, Jan. 2002.   \n[32] O. Levy and Y. Goldberg. Neural Word Embedding as Implicit Matrix Factorization. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, NIPS, pages 2177\u20132185, 2014. URL http://papers.nips.cc/paper/ 5477-neural-word-embedding-as-implicit-matrix-factorization.   \n[33] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv [cs.CL], July 2019.   \n[34] H. P. Luhn. The automatic creation of literature abstracts. IBM J. Res. Dev., 2(2):159\u2013165, Apr. 1958.   \n[35] M. Marelli, S. Menini, M. Baroni, L. Bentivogli, R. Bernardi, and R. Zamparelli. A SICK cure for the evaluation of compositional distributional semantic models. In LREC, pages 216\u2013223, Reykjavik, Iceland, 5 2014. URL http://www.lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf.   \n[36] A. K. Menon, S. Jayasumana, A. S. Rawat, H. Jain, A. Veit, and S. Kumar. Long-tail learning via logit adjustment. Sept. 2020.   \n[37] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed Representations of Words and Phrases and their Compositionality. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, NIPS, pages 3111\u20133119, 2013. URL http://papers.nips.cc/paper/ 5021-distributed-representations-of-words-and-phrases-and-their-compositionality.   \n[38] D. Mimno and L. Thompson. The strange geometry of skip-gram with negative sampling. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2873\u20132878, Copenhagen, Denmark, Sept. 2017. Association for Computational Linguistics.   \n[39] J. Mu and P. Viswanath. All-but-the-Top: Simple and Effective Postprocessing for Word Representations. In ICLR, 2018. URL https://openreview.net/forum?id $\\bar{.}$ HkuGJ3kCb.   \n[40] N. Muennighoff, N. Tazi, L. Magne, and N. Reimers. MTEB: Massive text embedding benchmark. In EACL, pages 2014\u20132037, Dubrovnik, Croatia, 5 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.eacl-main.148. URL https://aclanthology.org/2023.eacl-main.148.   \n[41] K. Oono and T. Suzuki. Graph neural networks exponentially lose expressive power for node classification. Sept. 2019.   \n[42] M. Oyama, S. Yokoi, and H. Shimodaira. Norm of word embedding encodes information gain. In H. Bouamor, J. Pino, and K. Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 2108\u20132130, Singapore, Dec. 2023. Association for Computational Linguistics.   \n[43] J. Pennington, R. Socher, and C. D. Manning. GloVe: Global Vectors for Word Representation. In EMNLP, pages 1532\u20131543, 2014. doi: 10.3115/v1/D14-1162. URL https://aclweb.org/anthology/papers/ D/D14/D14-1162/.   \n[44] I. Provilkov, D. Emelianenko, and E. Voita. BPE-dropout: Simple and effective subword regularization. In D. Jurafsky, J. Chai, N. Schluter, and J. Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1882\u20131892, Online, July 2020. Association for Computational Linguistics.   \n[45] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language Models are Unsupervised Multitask Learners. Technical report, 2019. URL https://d4mucfpksywv.cloudfront.net/ better-language-models/language_models_are_unsupervised_multitask_learners.pdf.   \n[46] N. Reimers and I. Gurevych. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In EMNLP, pages 3980\u20133990, 11 2019. doi: 10.18653/v1/D19-1410. URL https://www.aclweb.org/ anthology/D19-1410.   \n[47] O. Roy and M. Vetterli. The effective rank: A measure of effective dimensionality. pages 606\u2013610, Sept. 2007.   \n[48] M. Rudelson. Random Vectors in the Isotropic Position. Journal of Functional Analysis, 164(1):60\u201372, 1999. ISSN 0022-1236. doi: https://doi.org/10.1006/jfan.1998.3384. URL http://www.sciencedirect. com/science/article/pii/S0022123698933845.   \n[49] W. Rudman, N. Gillman, T. Rayne, and C. Eickhoff. IsoScore: Measuring the uniformity of embedding space utilization. In S. Muresan, P. Nakov, and A. Villavicencio, editors, Findings of the Association for Computational Linguistics: ACL 2022, pages 3325\u20133339, Dublin, Ireland, May 2022. Association for Computational Linguistics.   \n[50] A. M. J. Schakel and B. J. Wilson. Measuring word significance using distributed representations of words. arXiv [cs.CL], Aug. 2015.   \n[51] H. Shi, J. Gao, H. Xu, X. Liang, Z. Li, L. Kong, S. M. S. Lee, and J. Kwok. Revisiting over-smoothing in BERT from the perspective of graph. Sept. 2021.   \n[52] H. Shimodaira. Improving predictive inference under covariate shift by weighting the log-likelihood function. Journal of statistical planning and inference, 90(2):227\u2013244, 2000.   \n[53] K. Sparck Jones. A statistical interpretation of term specificity and its application in retrieval. In Document retrieval systems, pages 132\u2013142. Taylor Graham Publishing, GBR, Dec. 1988.   \n[54] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. LLaMA: Open and Efficient Foundation Language Models. arXiv:2302.13971v1, 2023. URL https://arxiv.org/abs/2302.13971v1.   \n[55] R. Vershynin. High-Dimensional Probability. Cambridge University Press, 2018.   \n[56] L. Wang, J. Huang, K. Huang, Z. Hu, G. Wang, and Q. Gu. Improving Neural Language Generation with Spectrum Control. In ICLR, 2020. URL https://openreview.net/forum?id=ByxY8CNtvr.   \n[57] L. Wang, N. Yang, X. Huang, B. Jiao, L. Yang, D. Jiang, R. Majumder, and F. Wei. Text Embeddings by Weakly-Supervised Contrastive Pre-training. arXiv:2212.03533v2, 2024. URL https://arxiv.org/ abs/2212.03533v2.   \n[58] L. Wetzel. Types and tokens. The Stanford Encyclopedia of Philosophy (Fall 2018 Edition), Apr. 2006.   \n[59] Y. Yan, R. Li, S. Wang, F. Zhang, W. Wu, and W. Xu. ConSERT: A contrastive framework for selfsupervised sentence representation transfer. In C. Zong, F. Xia, W. Li, and R. Navigli, editors, ACL-IJCNLP, pages 5065\u20135075, Online, 8 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. acl-long.393. URL https://aclanthology.org/2021.acl-long.393.   \n[60] S. Yokoi, R. Takahashi, R. Akama, J. Suzuki, and K. Inui. Word Rotator\u2019s Distance. In EMNLP, pages 2944\u2013 2960, Online, 11 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.236. URL https://www.aclweb.org/anthology/2020.emnlp-main.236. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Explanation fo the Zipfian whitened word vectors will have a zero mean and be in an isotropic position in terms of expectation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The second step of PCA whitening involves decorrelating the dimensions and then normalizing them, which is achieved by transforming the centered random vector $\\overline{{\\pmb{w}}}$ as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\widetilde{\\pmb{w}}:=\\mathbb{E}[\\pmb{\\overline{{w}}}\\,\\pmb{\\overline{{w}}}^{\\top}]^{-1/2}\\pmb{\\overline{{w}}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Actually, $\\mathbb{E}[\\widetilde{\\boldsymbol{w}}\\widetilde{\\boldsymbol{w}}^{\\top}]=\\boldsymbol{I}_{d}$ holds; $\\widetilde{\\pmb{w}}$ satisfies Def. 2. Computationally, the estimation of $\\mathbb{E}[\\overline{{\\boldsymbol{w}}}\\,\\overline{{\\boldsymbol{w}}}^{\\top}]^{-1/2}$ can be performed efficiently via singular value decomposition (SVD) of the centered \u201cdata\u201d matrix $\\overline{{W}}:=[\\overline{{\\pmb{w}}}_{1}^{\\top},\\dots,\\overline{{\\pmb{w}}}_{n}^{\\top}]^{\\top}$ . Note again that, this standard method assumes that the frequency of each word (i.e., each row) is uniform, which presents the issues discussed in $\\S\\ 2$ . To account for word frequency, SVD should be performed on the matrix ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\overline{{\\pmb{W}}}_{p}:=\\left[\\sqrt{p(w_{1})}\\overline{{\\pmb{w}}}_{1}^{\\top},\\ldots,\\sqrt{p(w_{|\\mathcal{V}|})}\\overline{{\\pmb{w}}}_{|\\mathcal{V}|}^{\\top}\\right]^{\\top}\\in\\mathbb{R}^{|\\mathcal{V}|\\times d},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where each word frequency is multiplied by its square root. In fact, $\\overline{{W}}_{p}^{\\top}\\overline{{W}}_{p}$ serves as an estimator for $\\underset{{\\pmb w}\\sim p}{\\mathbb{E}}[\\overline{{{\\pmb w}}}\\,\\overline{{{\\pmb w}}}^{\\top}]$ . This can be confirmed by comparing the $(j,k)$ \u2019th elements of each matrix (or matrix-valued random variable): ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{{\\pmb w}\\sim p}{\\mathbb{E}}[{\\overline{{{\\boldsymbol{w}}}}}\\,{\\overline{{{\\boldsymbol{w}}}}}^{\\top}][j,k]=\\underset{{\\pmb w}\\sim p}{\\mathbb{E}}[{\\overline{{{\\boldsymbol{w}}}}}[j]\\,{\\overline{{{\\pmb{w}}}}}[k]],}\\\\ &{(\\overline{{{\\boldsymbol{W}}}}_{p}^{\\top}\\overline{{{\\pmb{W}}}}_{p})[j,k]=\\displaystyle\\sum_{i}p(w_{i})\\overline{{{\\pmb{w}}}}_{i}[j]\\,\\overline{{{\\pmb{w}}}}_{i}[k],}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $A[j,k]$ denotes the $(j,k)$ \u2019th element of $\\pmb{A}$ , and $\\pmb{v}[j]$ denotes the $j^{;}$ th element of $\\pmb{v}$ . Finally, the estimator for the desired $\\mathbb{E}[\\overline{{\\boldsymbol{w}}}\\,\\overline{{\\boldsymbol{w}}}^{\\top}]^{-1/2}$ can be computed as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\underset{w\\sim n}{\\overset{\\rightharpoonup}{\\mathbb{E}}}[\\overline{{w}}\\,\\overline{{w}}^{\\top}]^{-1/2}=(\\overline{{W}}_{p}^{\\top}\\overline{{W}}_{p})^{-1/2}=((U\\Sigma V^{\\top})^{\\top}(U\\Sigma V^{\\top}))^{-1/2}=(V\\Sigma^{2}V^{\\top})^{-1/2}=V\\Sigma^{-1}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Table 6 shows the correspondence between uniform (normal) whitening and Zipfian whitening. This may be useful for readers familiar with matrix notation. ", "page_idx": 15}, {"type": "text", "text": "B Experimental settings ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To ensure the reproducibility of the experiments conducted in this paper, we provide detailed configurations below. Additionally, the source code has been made publicly available at https: //github.com/cl-tohoku/zipfian-whitening. ", "page_idx": 15}, {"type": "text", "text": "B.1 Word embeddings ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For static embeddings, we used the most standard ones, 300-dim GloVe[43] model trained on Common Crawl8, 300-dim word2vec[37] model trained on Google News9, and 300-dim fastText[11] subword/non-subword models trained on Common Crawl10. For the multilingual experiment, we used fastText-ja [11], a fastText model trained on Japanese Wikipedia and Common Crawl 11. ", "page_idx": 15}, {"type": "table", "img_path": "pASJxzMJb7/tmp/3f6e4ad81255bb62369b42bac93d8aeb11c925c0e8ae8711d6f10d8e77508f38.jpg", "table_caption": ["Table 6: The correspondence between uniform (normal) whitening and Zipfian whitening. "], "table_footnote": ["For dynamic embeddings, we used three most standard masked language models, BERT[18]12, RoBERTa[33]13, and DeBERTa [25]14. All three models are base size. To aggregate the dynamic word embeddings to create sentence embeddings, we follow the first-last average pooling from the prior work [22]. In this setting, we first average the hidden states of first and last dynamic layer of "], "page_idx": 16}, {"type": "text", "text": "the model to get the averaged token embeddings15, then average the token embeddings to get final sentence embeddings16. ", "page_idx": 17}, {"type": "text", "text": "B.2 Empirical word frequency and vocabulary ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "As the empirical word probability $p(w)$ of English words, we used the enwiki dataset preprocessed by Arora et al. $[7]^{17}$ . For the Japanese word probability, we used Japanese Wikipedia word frequency from Wiktionary, denoted as jawiki 18. Furthermore, we also used the frequency of words in the evaluation data itself (test set probability). The word frequency in the test set is implicitly utilized in [7]\u2019s sentence embedding method and is also a natural approach in the context of \u201ccovariate shift\u201d [52]. ", "page_idx": 17}, {"type": "text", "text": "As vocabulary $\\mathcal{V}$ , we used the overlapping entries between the word frequency list and the pre-trained word embedding model\u2019s vocabulary across all settings, including baseline methods. ", "page_idx": 17}, {"type": "text", "text": "B.3 Baseline methods ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "As baselines for post-processing of word vectors, we used ABTT (all-but-the-top) [39], which established the trend of post-processing word vectors; and the strong baseline method by [7], the combination of SIF (smoothed inverse frequency) weighting and CCR (common component removal)19. We followed the hyperparameter choices of the original papers, with the dimensionality reduction parameter for ABTT set to $D:=3$ , and the weighting parameter for SIF set to $a:=10^{-3}$ . ", "page_idx": 17}, {"type": "text", "text": "B.4 Extrinsic tasks ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "As downstream tasks, we used the most commonly utilized ones in the community, STS12-16 [1, 2, 3, 4, 5], STS-B [15] and SICK-R [35]. For the multilingual experiments, we used JSTS (Japanese version of the STS) from JGLUE benchmark [29]. They are sentence-level similarity tasks and are standard for empirically evaluating the performance of word vectors20 21. These datasets consist of pairs of sentences and their semantic similarity rated by annotators. We first tokenized the dataset by NLTK [9] with some post-processing following $[20]^{22}$ , then lowercased all tokens. The typical experimental protocol we followed is to sum the word vectors to form a \u201csentence vector\u201d and then check if the angles (cosine similarity) between them correlate well with the gold scores. We reported Spearman\u2019s rank correlation between the predictions (cosine scores) and human-annotated gold scores23. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "B.5 Computational resources for experiments ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We conducted all experiments using a single NVIDIA RTX 6000 Ada GPU with 48GB VRAM. Each STS task required 10 seconds per model and whitening method, totaling approximately 10 minutes for the entire experiment, excluding the embedding loading time to the GPU. ", "page_idx": 18}, {"type": "text", "text": "For the calculation of the symmetry scores, each setting took one minute, resulting in a total of 5 minutes, again excluding the embedding loading time and the average cosine similarity (Ave. Cos.) setting. The Ave. Cos. score computation took 10 minutes per model, totaling 20 minutes for the two models. ", "page_idx": 18}, {"type": "text", "text": "C Experimental results on all benchmark datasets to evaluate the effects of Zipfian whitening ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In $\\S\\ 3.2$ , we evaluated the empirical performance of Zipfian whitening on the STS-B dataset. In this section, we present experimental results using more comprehensive datasets. Detailed experimental settings can be found in Appendix B. Table 8, Table 9 and Table 10 show the results. Across all datasets, the method incorporating a Zipfian prior consistently outperforms the method employing a uniform prior. ", "page_idx": 18}, {"type": "text", "text": "D Proof of Prop. 1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. We will show the following. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\quad\\mathbb{E}[(\\pmb{v}-\\mathbb{E}[\\pmb{v}])(\\pmb{v}-\\mathbb{E}[\\pmb{v}])^{\\top}]\\propto\\pmb{I}_{d}}\\\\ &{\\stackrel{\\textcircled{\\bot}}{\\Longleftrightarrow}\\lambda_{1}=\\lambda_{2}=\\cdots=\\lambda_{d}}\\\\ &{\\stackrel{\\textcircled{\\bot}}{\\Longleftrightarrow}\\mathrm{Sym}_{2}(\\pmb{v})=\\frac{1}{\\log d}\\cal H\\left(\\frac{\\lambda_{1}}{\\sum_{j}\\lambda_{j}},\\cdots,\\frac{\\lambda_{d}}{\\sum_{j}\\lambda_{j}}\\right)=1}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "$(\\;{\\stackrel{\\oslash}{\\Longrightarrow}}$ ) When $\\mathbb{E}[(\\pmb{v}-\\mathbb{E}[\\pmb{v}])(\\pmb{v}-\\mathbb{E}[\\pmb{v}])^{\\top}]=k\\pmb{I}_{d}$ holds, its eigenvalues are $\\{k,\\ldots,k\\}$ . $:\\mathcal{Q}_{\\mathrm{~\\rightmoon~}}$ Since $\\mathbb{E}[(\\pmb{v}-\\mathbb{E}[\\pmb{v}])(\\pmb{v}-\\mathbb{E}[\\pmb{v}])^{\\top}]$ is symmetric positive definite, it can be represented as $U\\mathbf{A}U^{\\top}$ using a diagonal matrix with eigenvalues $\\mathbf{A}=\\mathrm{diag}(\\lambda_{1},\\dots,\\lambda_{d})\\in\\mathbb{R}^{d\\times d}$ and an orthogonal ", "page_idx": 18}, {"type": "text", "text": "matrix $U$ . Now we have $\\lambda_{1}=\\lambda_{2}=\\ldots\\lambda_{d}=:k$ , then $\\mathbb{E}[(\\pmb{v}-\\mathbb{E}[\\pmb{v}])(\\pmb{v}-\\mathbb{E}[\\pmb{v}])^{\\top}]=\\pmb{U}\\pmb{\\Lambda}\\pmb{U}^{\\top}=$ $\\boldsymbol{U}\\boldsymbol{k}\\boldsymbol{I_{d}}\\boldsymbol{U}^{\\top}=\\boldsymbol{k}\\boldsymbol{I_{d}}$ . ", "page_idx": 19}, {"type": "text", "text": "(\u21d0\u21d2) The Shannon entropy $H(p)$ of a random variable $p$ taking $d$ possible values attains its maximum value $\\log d$ if and only if $p$ follows uniform distribution. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "E Pseudocode for the evaluation metrics of symmetry ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "See Algorithm 2 to measure the degree of symmetry of word embeddings. ", "page_idx": 19}, {"type": "text", "text": "Algorithm 2 Measure the degree of symmetry of word embeddings ", "page_idx": 19}, {"type": "text", "text": "Input: Word embeddings $\\{w_{i}\\}$ , word frequency $p!\\llap{/}\\nu\\to[0,1]$ .   \nOutput: Degree of centrality (the 1st moment) $\\widehat{\\mathrm{Sym}}_{1}(\\{{\\pmb w}_{i}\\},p)$ and isotropy (the 2nd moment) $\\widehat{\\mathrm{Sym}}_{2}(\\{{\\pmb w}_{i}\\},p)$ . Measure the degree of centrality (the 1st moment of symmetry):   \n1: $\\begin{array}{r l}&{\\widehat{\\pmb{\\mu}}\\leftarrow\\displaystyle\\sum_{w_{i}\\in\\mathcal{V}}p(w_{i})\\pmb{w}_{i}\\in\\mathbb{R}^{d}}\\\\ &{\\ell\\leftarrow\\displaystyle\\sum_{w_{i}\\in\\mathcal{V}}p(w_{i})\\|\\pmb{w}_{i}\\|\\in\\mathbb{R}}\\\\ &{\\widehat{\\mathrm{Sym}}_{1}(\\{\\pmb{w}_{i}\\},p)\\leftarrow1-\\|\\widehat{\\pmb{\\mu}}\\|/\\ell}\\end{array}$   \n2:   \n3: Measure the degree of isotropy (the 2nd moment of symmetry):   \n4: $W\\gets\\bigg[\\sqrt{p(w_{1})}(w_{1}-\\widehat{\\pmb{\\mu}})^{\\top},\\ldots,\\sqrt{p(w_{|\\mathcal{V}|})}(w_{|\\mathcal{V}|}-\\widehat{\\pmb{\\mu}})^{\\top}\\bigg]$   \n5: $U\\Sigma V^{\\top}\\leftarrow\\operatorname{SVD}(\\mathcal{W})$ \u25b7 $\\begin{array}{r}{\\Sigma=\\mathrm{diag}(\\sigma_{1},\\dots,\\sigma_{d})\\in\\mathbb{R}^{d\\times d}}\\end{array}$ consists the singular values of $W$ .   \n6: $(\\lambda_{1},.\\,.\\,.\\,,\\lambda_{d})\\gets(\\sigma_{1}^{2},.\\,.\\,.\\,,\\sigma_{d}^{2})$   \n7: $\\widehat{\\mathrm{Sym}}_{2}(\\{\\pmb{w}_{i}\\},p)\\gets-\\frac{1}{\\log d}\\sum_{i}\\frac{\\lambda_{i}}{\\sum_{i}\\lambda_{i}}\\log\\frac{\\lambda_{i}}{\\sum_{i}\\lambda_{i}}$ ", "page_idx": 19}, {"type": "text", "text": "F Proof of Thm. 1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. By the assumption, the word and context vectors for the same word $t\\in\\mathcal{V},w(t)$ and $c(t)$ , are obtained through the linear embedding layer with weight tying, namely, $w(t)\\,=\\,U\\mathbf{1}_{t}$ and $\\pmb{c}(t)\\,=\\,U\\mathbf{1}_{t}$ , where $U\\,\\in\\,\\mathbb{R}^{d\\times|\\mathcal{V}|}$ is the embedding matrix and $\\mathbf{1}_{t}\\,\\in\\,\\mathbb{R}^{|\\mathcal{V}|}$ is the one-hot vector indicating the token $t$ . To derive the KL divergence for the model $p(c\\mid w)$ , we need to begin with the generative model $p(w\\mid c)$ (7) and confirm that $p(c\\mid w)$ belongs to an exponential family. ", "page_idx": 19}, {"type": "equation", "text": "$$\np(c\\mid w)=\\frac{p(w\\mid c)p(c)}{p(w)}=\\frac{\\exp(\\langle w,c\\rangle)}{Z_{\\langle\\widehat{\\omega}\\rangle}(c)}p(c)=\\frac{p(c)\\exp(\\langle U^{\\top}w,\\mathbf{1}_{c}\\rangle)}{Z_{\\langle\\widehat{\\omega}\\rangle}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we used the constancy of the partition function $(\\equiv Z_{\\widehat{(z)}})$ from (9) at the last identity. Hence, $p(c\\mid w)$ is an exponential family parametrized by $\\pmb{U}^{\\top}\\pmb{w}(:=\\pmb{\\theta}\\in\\mathbb{R}^{|\\mathcal{V}|})$ , and its log-partition function is given as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\psi_{c|w}(\\pmb\\theta)=\\log\\Biggl(\\sum_{c\\in\\mathcal{V}}p(c)\\exp(\\langle\\pmb\\theta,\\mathbf{1}_{c}\\rangle)\\Biggr).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "The second-order expansion of the KL divergence can be derived based on the second moment of $p(c\\mid w)$ , which is given by the Hessian of $\\psi_{c|w}$ in the case of exponential families. First, let us derive ", "page_idx": 19}, {"type": "text", "text": "the first moment. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\partial\\psi_{c|w}}{\\partial\\theta}=\\frac{\\sum_{c}\\frac{p(c)}{Z_{\\langle\\rangle}}\\exp(\\langle\\theta,\\mathbf{1}_{c}\\rangle)\\mathbf{1}_{c}}{\\sum_{c^{\\prime}}\\frac{p(c^{\\prime})}{Z_{\\langle\\rangle}}\\exp(\\langle\\theta,\\mathbf{1}_{c^{\\prime}\\rangle})}=\\sum_{c\\in\\mathcal{V}}\\frac{p(c)\\exp(\\langle\\theta,\\mathbf{1}_{c}\\rangle)}{Z_{\\langle\\rangle}}\\mathbf{1}_{c}=\\sum_{c\\in\\mathcal{V}}p(c\\mid w)\\mathbf{1}_{c}=\\left[p(c\\mid w)\\right].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then, the second moment is derived. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial\\psi_{c|w}}{\\partial\\theta\\partial\\theta^{\\top}}=\\displaystyle\\frac{1}{Z_{\\bigodot}}\\sum_{c\\in\\mathcal{V}}p(c)\\left\\{\\displaystyle\\frac{\\partial}{\\partial\\theta}\\exp(\\langle\\pmb{\\theta},\\mathbf{1}_{c}\\rangle)\\right\\}\\mathbf{1}_{c}^{\\top}=\\displaystyle\\frac{1}{Z_{\\bigodot}}\\sum_{c\\in\\mathcal{V}}p(c)\\exp(\\langle\\pmb{\\theta},\\mathbf{1}_{c}\\rangle)\\mathbf{1}_{c}\\mathbf{1}_{c}^{\\top}}\\\\ {\\displaystyle=\\sum_{c\\in\\mathcal{V}}p(c\\mid w)\\mathbf{1}_{c}\\mathbf{1}_{c}^{\\top}=\\mathrm{diag}[\\hdots p(c\\mid w)\\ \\dots],}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which is the $|\\mathcal{V}|\\times|\\mathcal{V}|$ diagonal matrix with $p(c\\mid w)$ being the $(c,c)$ -th diagonal entry. Now, we are ready to derive the KL divergence. For two tokens $w,w^{\\prime}\\in|\\nu|$ , if we write $\\pmb\\theta:=U\\mathbf{1}_{w}$ and $\\pmb\\theta^{\\prime}:=\\pmb U\\dot{\\mathbf{1}}_{w^{\\prime}}$ , the KL divergence of the exponential family can be expanded as the following quadratic form in their parameters $\\pmb{\\theta}$ and $\\pmb{\\theta}^{\\prime}$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{2\\mathrm{KL}(p(\\cdot\\mid w^{\\prime})\\|p(\\cdot\\mid w))\\approx(\\theta^{\\prime}-\\theta)^{\\top}\\left(\\frac{\\partial\\psi_{\\mathrm{c}|w}}{\\partial\\theta\\partial\\theta^{\\top}}\\right)(\\theta^{\\prime}-\\theta)}&{}\\\\ {=(w^{\\prime}-w)^{\\top}U\\left(\\frac{\\partial\\psi_{\\mathrm{c}|w}}{\\partial\\theta\\partial\\theta^{\\top}}\\right)U^{\\top}(w^{\\prime}-w)}&{}\\\\ {=(w^{\\prime}-w)^{\\top}\\Bigg\\{\\underset{c\\in[\\mathcal{V}]}{\\sum}p(c\\mid w)(U1_{c})(U1_{c})^{\\top}\\Bigg\\}(w^{\\prime}-w)}&{}\\\\ {=(w^{\\prime}-w)^{\\top}\\Bigg\\{\\underset{c\\in[\\mathcal{V}]}{\\sum}p(c\\mid w)c c^{\\top}\\Bigg\\}(w^{\\prime}-w)}&{}\\\\ {=(w^{\\prime}-w)^{\\top}G(w)(w^{\\prime}-w)}&{}\\\\ {=|(w^{\\prime}-w)^{\\top}G(w)(w^{\\prime}-w)}&{}\\\\ {=\\|w^{\\prime}-w\\|_{G(w)}^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We can consider a word $w_{0}$ such that $p(\\cdot)=p(\\cdot\\mid w_{0})$ , that is, an uninformative word $w_{0}$ whose presence does not change the marginal distribution at all. Noting from Equation (22) of Oyama et al. [42] that $\\begin{array}{r}{\\pmb{\\overline{{w}}}:=\\sum_{w\\in\\mathcal{V}}\\pmb{\\Bar{p}}(w)\\pmb{w}\\approx\\pmb{\\Bar{w}}_{0}}\\end{array}$ , we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{KL}(p(\\cdot)\\|p(\\cdot\\ |\\ w))=\\mathrm{KL}(p(\\cdot\\ |\\ w_{0})\\|p(\\cdot\\ |\\ w))}\\\\ &{\\qquad\\qquad=\\|w_{0}-w\\|_{G(w)}^{2}}\\\\ &{\\qquad\\qquad\\approx\\|\\overline{{\\pmb{w}}}-\\pmb{w}\\|_{G(w)}^{2}}\\\\ &{\\qquad\\qquad\\quad\\mathrm{Assump.}\\ \\|\\pmb{w}\\|_{G(w)}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "G Experiments with a mix of uniform and Zipfian settings ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Based on the findings that Zipfian whitening positively impacts word vector norms $(\\S\\,4.2)$ , we present experimental results for a baseline: first, uniform whitening is applied, followed by rescaling norms according to information content through Zipfian whitening. ", "page_idx": 20}, {"type": "text", "text": "Table 11 presents the results, with the basic settings identical to those in Table 2, but uses Pearson\u2019s $r$ as the evaluation metric. Here, \u201cUniform $+\\alpha$ \u201d refers to the process of \u201ccorrecting word vectors using a uniform prior, then replacing only the norm with that obtained from Zipfian whitening.\u201d We found that appropriately weighting by norm has a critical effect on task performance. Notably, pure Zipfian centering/whitening performs even better, suggesting that Zipfian correction has two effects: (i) the norm becomes representative of information content $(\\S\\,4.2)$ , and (ii) vectors are more evenly dispersed (isotropic), resulting in appropriate positioning in terms of direction as well. ", "page_idx": 20}, {"type": "text", "text": "H Formal Explanation of \u201cUniform whitening of token embeddings \u2248 Zipfian whitening of type embeddings\u201d ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we provide a more formal explanation of \u201cUniform whitening of token embeddings $\\approx$ Zipfian whitening of type embeddings,\u201d as described in $\\S\\,5.1$ . For intuitive explanations and related discussions, please refer to $\\S\\ 5.1$ . ", "page_idx": 21}, {"type": "text", "text": "H.1 Uniform whitening of token embeddings $\\approx$ Zipfian whitening of type embeddings ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Assume that, when the type word of a token $t$ is $w$ , the token embedding $\\pmb{t}$ aligns with the shared type embedding $\\pmb{w}$ . ", "page_idx": 21}, {"type": "text", "text": "Assumption 1. If $\\dot{\\mathbf{\\nabla}}t y p e(t)=w$ , then $\\pmb{t}=\\pmb{w}$ . ", "page_idx": 21}, {"type": "text", "text": "Note that this is a rough approximation, as token embeddings are dynamic and vary with context. Under this assumption, the unweighted mean of token embeddings $\\widehat{\\mathbb{E}}_{(\\widehat{u})}[t]$ obtained from a dataset $\\mathcal{D}$ is asymptotically equivalent to a word-frequency-weighted (Zipfian) average of type embeddings $\\mathbb{E}_{\\varnothing}[w]$ , as $|\\mathcal{D}|\\to\\infty$ : ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\widehat{\\mathbb{E}}_{\\widehat{\\mathbb{U}}}[t]:=\\frac{1}{|\\mathcal{D}|}\\sum_{t\\in\\mathcal{D}}t\\overset{\\mathrm{Assump.\\,1}}{\\approx}\\frac{1}{|\\mathcal{D}|}\\sum_{w\\in\\mathcal{V}}c_{\\mathcal{D}}(w)w\\xrightarrow{|\\mathcal{D}|\\rightarrow\\infty}\\sum_{w\\in\\mathcal{V}}p(w)w=:\\mathbb{E}_{\\widehat{\\mathbb{Q}}}[w],\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $c_{\\mathcal{D}}$ denotes the count of type $w$ in $\\mathcal{D}$ $:c_{\\mathcal{D}}(w):=\\#\\{t\\in\\mathcal{D}:\\mathrm{type}(t)=w\\}$ . ", "page_idx": 21}, {"type": "text", "text": "H.2 Pseudo-uniform whitening of token embeddings $\\approx$ uniform whitening of type embeddings ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "To establish a baseline for centering/whitening token embeddings under uniform prior, we can apply a coefficient $1\\big/|\\mathcal{V}_{\\mathcal{D}}|\\cdot1\\big/c_{\\mathcal{D}}(\\mathrm{type}(t))$ to each token embedding $t$ , for removing type frequencies that are implicitly referenced. Here, $\\nu_{D}$ denotes the vocabulary contained in $\\mathcal{D}$ : $\\mathcal{V}_{\\mathcal{D}}:=\\left\\{w\\in\\mathcal{V}:\\exists t\\in\\right.$ $\\boldsymbol{D},\\mathrm{type}(t)=\\boldsymbol{w}\\}$ . The \u201cpseudo-uniform\u201d average $\\widehat{\\mathbb{E}}_{(\\widehat{\\underline{{u}}})}[t]$ calculated in this way is asymptotically equivalent to the uniform average of type embeddings $\\mathbb{E}_{\\varnothing}[w]$ , under the previous assumption (Assump. 1) that ignores the dynamic nature of token embeddings: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\widehat{\\mathbb{E}}_{\\widehat{\\mathbb{D}}}[t]:=\\sum_{t\\in\\mathcal{D}}\\frac{1}{|\\mathcal{V}_{\\mathcal{D}}|}\\frac{1}{c_{\\mathcal{D}}(\\mathrm{type}(t))}\\,t^{\\mathrm{\\tiny~\\mathrm{Assump.}~1}}\\sum_{w\\in\\mathcal{V}_{p}}\\frac{1}{|\\mathcal{V}_{\\mathcal{D}}|}\\frac{c_{\\mathcal{D}}(w)}{c_{\\mathcal{D}}(w)}\\,w\\xrightarrow{|\\mathcal{D}|\\rightarrow\\infty}\\sum_{w\\in\\mathcal{V}}\\frac{1}{|\\mathcal{V}|}w=:\\mathbb{E}_{\\widehat{\\mathbb{U}}}[w].\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "I Experimental results on all benchmark datasets to evaluate the effects of uniform whitening on token embeddings ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In $\\S\\,5.1$ , we evaluated the empirical performance of uniform whitening of dynamic token embeddings on the STS-B dataset. In this section, we present experimental results using more comprehensive datasets. Table 12 shows the results. Across all datasets, the methods implicitly incorporating a Zipfian prior consistently outperforms the method employing a uniform prior. ", "page_idx": 21}, {"type": "table", "img_path": "pASJxzMJb7/tmp/e1feca5a3463ffede81dafb829b38c3d50f12580ac52da023d86192be80ad69c.jpg", "table_caption": ["Table 8: Full results of the empirical performance of Zipfian whitening. Each cell shows the STS score $\\times100$ . As empirical word frequency $p(w)$ , we used enwiki. Across all models and tasks, Zipfian whitening outperforms powerful baseline methods. "], "table_footnote": [], "page_idx": 22}, {"type": "table", "img_path": "pASJxzMJb7/tmp/e659542fb69cec71f7bc698fe2456cdf6ef5a4680c570f608c82ba87c29ba2bd.jpg", "table_caption": ["Table 9: Full results of the empirical performance of Zipfian whitening, test set frequency setting. Each cell shows the STS score $\\times100$ . As empirical word frequency $p(w)$ , we used test set frequency. Across all models and tasks, Zipfian whitening outperforms powerful baseline methods. Besides, the test set frequency setting consistently outperforms the enwiki setting in Table 8, demonstrating that the models benefit from using task-specific statistics in line with a covariate shift approach [52]. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table 10: Evaluation results using Japanese fastText. Each cell shows the JSTS [29] score $\\times100$ . Even in the multilingual setting, Zipfian whitening outperforms powerful baseline methods. ", "page_idx": 23}, {"type": "table", "img_path": "pASJxzMJb7/tmp/e4b21c8dab6ddfa01a2138c845b79eff96280cb47ae98a24ea2091fc1e43d125.jpg", "table_caption": ["(a) With jawiki as $p(w)$ "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "pASJxzMJb7/tmp/f290b1f5aec09bed9b09e588b25ad0d1b15fede5ddf1a332419ddef45c455378.jpg", "table_caption": ["(b) With test set frequency as $p(w)$ "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "pASJxzMJb7/tmp/0144fa51f6a9afa6765c1b69d2f4f972bf4286b429a63698d425125b6949e4b6.jpg", "table_caption": ["Table 11: Each cell shows the STS-B [15] score $\\times100$ . "], "table_footnote": [], "page_idx": 23}, {"type": "table", "img_path": "pASJxzMJb7/tmp/3db4896a18cab40aef2708ee59cec7e9bf10dc569513d08366750fd008884bb8.jpg", "table_caption": ["Table 12: Full results of the whitening on dynamic embeddings. Each cell shows the STS score $\\times100$ . Token-level uniform centering/whitening (\"Zipfian\" settings), which corresponds to centering/whitening at the word type level under a Zipfian prior, consistently outperforms the \"Uniform\" setting across all STS tasks. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Details are provided in Limitation $\\S\\ 6)$ . ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 25}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: See e.g. Appendix D, Appendix F, and Appendix H. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: In (Appendix B), we have comprehensively described the experimental settings necessary for reproducing the empirical validations. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 26}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Source code has been made publicly available at https://github.com/ cl-tohoku/zipfian-whitening. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: In (Appendix B), we have comprehensively described the experimental settings necessary for reproducing the empirical validations. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 27}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: We used pre-trained word embeddings and applied PCA-like transformations in all experiments, eliminating the need for training hyperparameters such as seed values. Additionally, we utilized existing data splits for downstream task evaluations using the standard sentence embedding evaluation tool [40]. Consequently, our experiments did not include error bars or statistical significance tests. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The details on the compute resources used on the experiments are shown in Appendix B. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 28}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: Details are provided in Broader Impacts section (\u00a7 6) ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. ", "page_idx": 29}, {"type": "text", "text": "\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 29}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. ", "page_idx": 29}, {"type": "text", "text": "\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We provided sufficient documents for using the implementations for the proposed method in the README file in the supplemental material. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 30}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 30}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]