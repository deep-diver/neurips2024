[{"figure_path": "tRjgapiCpm/figures/figures_3_1.jpg", "caption": "Figure 1: Comparison of our heuristic to baselines in various parameter regimes. Horizontal axis is number of iterations T and vertical axis is \u03b5 such that we have (\u03b5, 10\u22126)-DP.", "description": "This figure compares the heuristic privacy analysis with two baselines: the standard composition-based analysis and the full-batch DP-GD approximation.  Across three different parameter settings (keeping Tq and T\u03c3\u00b2, standard DP, and q and \u03c3 constant respectively), the plots show how the epsilon values (\u03b5) vary with the number of iterations (T) for each method. The heuristic analysis provides an upper bound that is consistently better than the standard analysis, which assumes access to all intermediate iterates.  The full-batch DP-GD serves as an idealized lower bound.", "section": "2.1 Baselines"}, {"figure_path": "tRjgapiCpm/figures/figures_3_2.jpg", "caption": "Figure 2: Black-box gradient space attacks fail to achieve tight auditing when other data points are sampled from the data distribution. Heuristic and standard bounds diverge from empirical results, indicating the attack's ineffectiveness. This contrasts with previous work which tightly auditing with access to intermediate updates.", "description": "This figure compares the heuristic and standard privacy bounds with empirical privacy leakage measured via privacy auditing for black-box gradient space attacks.  The experiment uses realistic deep learning settings where other data points are sampled from the data distribution, in contrast to prior work which assumed access to intermediate training updates. The results show that the heuristic and standard composition-based analysis diverge significantly from the empirical measurements, indicating that these black-box attacks are ineffective in this setting.  This contrasts with previous work, where attacks that use intermediate iterates achieved much tighter results.", "section": "3 Empirical Evaluation via Privacy Auditing"}, {"figure_path": "tRjgapiCpm/figures/figures_4_1.jpg", "caption": "Figure 3: For gradient space attacks with adversarial datasets, the empirical epsilon (\u03b5) closely tracks the final epsilon except for at small step counts, where distinguishing is more challenging. This is evident at both subsampling probability values we study (q = 0.01 and q = 0.1).", "description": "This figure compares the heuristic, standard, and empirical epsilon values (privacy loss) for gradient space attacks with adversarial datasets.  The empirical epsilon values are obtained through privacy auditing. The x-axis represents the number of training steps, and the y-axis represents epsilon (\u03b5) at \u03b4 = 1e-05. The two subfigures show the results for different subsampling probabilities (q = 0.01 and q = 0.1). The results show that the heuristic and standard epsilon values generally provide upper bounds on the empirical epsilon values, with better agreement for larger numbers of steps.", "section": "3.1 Experimental Results"}, {"figure_path": "tRjgapiCpm/figures/figures_4_2.jpg", "caption": "Figure 4: Input space attacks show promising results with both natural and blank image settings, although blank images have higher attack success. These input space attacks achieve tighter results than gradient space attacks in the natural data setting, in contrast to findings from prior work.", "description": "This figure compares the results of input space attacks against gradient space attacks for privacy auditing of differentially private stochastic gradient descent (DP-SGD).  The attacks were performed on both a natural dataset (CIFAR10) and a dataset with all-zero gradient inputs, representing different adversarial scenarios. The results show that input space attacks, which do not require access to intermediate training steps, achieve tighter bounds on privacy leakage compared to gradient space attacks, particularly in the case of CIFAR10.  This contrasts with previous findings which indicated that gradient space attacks were more effective. The higher attack success rate in the blank image scenario implies vulnerabilities that could be further exploited.", "section": "3 Empirical Evaluation via Privacy Auditing"}, {"figure_path": "tRjgapiCpm/figures/figures_7_1.jpg", "caption": "Figure 5: Ratio of upper bound on \u03b5 for quadratic loss with \u03b1 = 0.5 divided by maximum \u03b5 of i iterations on a linear loss. In Figure 5a (resp. Figure 5b), for each choice of q, \u03c3 is set so 1 iteration of DP-SGD satisfies (1, 10\u22126)-DP (resp (2, 10\u22126)-DP).", "description": "This figure shows the ratio of the privacy guarantee (epsilon) obtained using a quadratic loss function (with \u03b1 = 0.5) to the heuristic privacy guarantee (epsilon) obtained for linear loss functions. The ratio is computed for various numbers of iterations (T) and subsampling probabilities (q).  Each row corresponds to a different number of iterations (T), and each column to a different subsampling probability (q).  The heatmap shows that the heuristic generally provides a good approximation, with the ratio being close to 1 in many cases, especially for a higher number of iterations. This suggests that the assumption of linearity in the heuristic analysis may often be a reasonable approximation.", "section": "4.2 Linear Loss + Quadratic Regularizer"}, {"figure_path": "tRjgapiCpm/figures/figures_7_2.jpg", "caption": "Figure 5: Ratio of upper bound on \u03b5 for quadratic loss with a = 0.5 divided by maximum \u03b5 of i iterations on a linear loss. In Figure 5a (resp. Figure 5b), for each choice of q, \u03c3 is set so 1 iteration of DP-SGD satisfies (1, 10\u22126)-DP (resp (2, 10\u22126)-DP).", "description": "This figure shows the ratio of the privacy guarantee (epsilon) obtained using a quadratic loss function (with a regularization parameter a = 0.5) to the maximum privacy guarantee obtained using a linear loss function, across different numbers of iterations (T) and subsampling probabilities (q). The results indicate that the linear loss heuristic provides a reasonable approximation of the privacy guarantee for many convex loss functions.", "section": "4.2 Linear Loss + Quadratic Regularizer"}, {"figure_path": "tRjgapiCpm/figures/figures_8_1.jpg", "caption": "Figure 3: For gradient space attacks with adversarial datasets, the empirical epsilon (\u03b5) closely tracks the final epsilon except for at small step counts, where distinguishing is more challenging. This is evident at both subsampling probability values we study (q = 0.01 and q = 0.1).", "description": "This figure compares the heuristic and standard privacy bounds with empirical results from gradient space attacks on adversarial datasets. The plots show that the empirical epsilon values closely follow the heuristic analysis, especially as the number of training steps increases. The difference observed at low step counts is attributed to the difficulty in distinguishing between output distributions in this regime.", "section": "3.1 Experimental Results"}]