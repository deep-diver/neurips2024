[{"type": "text", "text": "The Last Iterate Advantage: Empirical Auditing and Principled Heuristic Analysis of Differentially Private SGD ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 We propose a simple heuristic privacy analysis of noisy clipped stochastic gradient   \n2 descent (DP-SGD) in the setting where only the last iterate is released and the in  \n3 termediate iterates remain hidden. Namely, our heuristic assumes a linear structure   \n4 for the model.   \n5 We show experimentally that our heuristic is predictive of the outcome of privacy   \n6 auditing applied to various training procedures. Thus it can be used prior to training   \n7 as a rough estimate of the final privacy leakage. We also probe the limitations of   \n8 our heuristic by providing some artificial counterexamples where it underestimates   \n9 the privacy leakage.   \n10 The standard composition-based privacy analysis of DP-SGD effectively assumes   \n11 that the adversary has access to all intermediate iterates, which is often unrealistic.   \n12 However, this analysis remains the state of the art in practice. While our heuristic   \n13 does not replace a rigorous privacy analysis, it illustrates the large gap between   \n14 the best theoretical upper bounds and the privacy auditing lower bounds and sets a   \n15 target for further work to improve the theoretical privacy analyses. ", "page_idx": 0}, {"type": "text", "text": "16 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "17 Differential privacy (DP) [DMNS06] defines a measure of how much private information from the   \n18 training data leaks through the output of an algorithm. The standard differentially private algorithm   \n19 for deep learning is DP-SGD [BST14; ACGMMTZ16], which differs from ordinary stochastic   \n20 gradient descent in two ways: the gradient of each example is clipped to bound its norm and then   \n21 Gaussian noise is added at each iteration.   \n22 The standard privacy analysis of DP-SGD is based on composition [BST14; ACGMMTZ16; Mir17;   \n23 Ste22; KJH20]. In particular, it applies to the setting where the privacy adversary has access to   \n24 all intermediate iterates of the training procedure. In this setting, the analysis is known to be tight   \n25 [NSTPC21; NHSBTJCT23]. However, in practice, potential adversaries rarely have access to the   \n26 intermediate iterates of the training procedure, rather they only have access to the final model. Access   \n27 to the final model can either be through queries to an API or via the raw model weights. The key   \n28 question motivating our work is the following.   \n29 Is it possible to obtain sharper privacy guarantees for DP-SGD when the adversary   \n30 only has access to the final model, rather than all intermediate iterates? ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "31 1.1 Background & Related Work ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "32 The question above has been studied from two angles: Theoretical upper bounds, and privacy auditing   \n33 lower bounds. Our goal is to shed light on this question from a third angle via principled heuristics.   \n34 A handful of theoretical analyses [FMTT18; CYS21; YS22; AT22; BSA24] have shown that asymp  \n35 totically the privacy guarantee of the last iterate of DP-SGD can be far better than the standard   \n36 composition-based analysis that applies to releasing all iterates. In particular, as the number of   \n37 iterations increases, these analyses give a privacy guarantee that converges to a constant (depending   \n38 on the loss function and the scale of the noise), whereas the standard composition-based analysis   \n39 would give a privacy guarantee that increases forever. Unfortunately, these theoretical analyses   \n40 are only applicable under strong assumptions on the loss function, such as (strong) convexity and   \n41 smoothness. We lack an understanding of how well they reflect the \u201creal\u201d privacy leakage.   \n42 Privacy auditing [JUO20; DWWZK18; BGDCTV18; SNJ23; TTSSJC22; ZBWTSRPNK22] com  \n43 plements theoretical analysis by giving empirical lower bounds on the privacy leakage. Privacy   \n44 auditing works by performing a membership inference attack [SSSS17; HSRDTMPSNC08; SOJH09;   \n45 DSSUV15]. That is, it constructs neighbouring inputs and demonstrates that the corresponding   \n46 output distributions can be distinguished well enough to imply a lower bound on the differential   \n47 privacy parameters. In practice, the theoretical privacy analysis may give uncomfortably large values   \n48 for the privacy leakage (e.g., $\\varepsilon>10$ ); in this case, privacy auditing may be used as evidence that   \n49 the \u201creal\u201d privacy leakage is lower. There are settings where the theoretical analysis is matched by   \n50 auditing, such as when all intermediate results are released [NSTPC21; NHSBTJCT23]. However,   \n51 despite significant work on privacy auditing and membership inference [CCNSTT22; BTRKMW24;   \n52 WBKBGGG23; LF20; SDSOJ19; ZLS23], a large gap remains between the theoretical upper bounds   \n53 and the auditing lower bounds [AKOOMS23; NHSBTJCT23] when only the final parameters are   \n54 released. This observed gap is the starting point for our work. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "55 1.2 Our Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "56 We propose a heuristic privacy analysis of DP-SGD in the setting where only the final iterate is   \n57 released. Our experiments demonstrate that this heuristic analysis consistently provides an upper   \n58 bound on the privacy leakage measured by privacy auditing tools in realistic deep learning settings.   \n59 Our heuristic analysis corresponds to a worst-case theoretical analysis under the assumption that the   \n60 loss functions are linear. This case is simple enough to allow for an exact privacy analysis whose   \n61 parameters are can be computed numerically (Theorem 1). Our consideration of linear losses is built   \n62 on the observation that current auditing techniques achieve the highest $\\varepsilon$ values when the gradients   \n63 of the canaries \u2013 that is, the examples that are included or excluded to test the privacy leakage \u2013 are   \n64 fixed and independent from the gradients of the other examples. This is definitely the case for linear   \n65 losses; the linear assumption thus allows us to capture the setting where current attacks are most   \n66 effective. Linear loss functions are also known to be the worst case for the non-subsampled (i.e., full   \n67 batch) case; see Appendix B. Assuming linearity is unnatural from an optimization perspective, as   \n68 there is no minimizer. But, from a privacy perspective, we show that it captures the state of the art.   \n69 We also probe the limitations of our heuristic and give some artificial counterexamples where it   \n70 underestimates empirical privacy leakage. One class of counterexamples exploits the presence of a   \n71 regularizer. Roughly, the regularizer partially zeros out the noise that is added for privacy. However,   \n72 the regularizer also partially zeros out the signal of the canary gradient. These two effects are almost   \n73 balanced, which makes the counterexample very delicate. In a second class of counterexamples, the   \n74 data is carefully engineered so that the final iterate effectively encodes the entire trajectory, in which   \n75 case there is no difference between releasing the last iterate and all iterates.   \n76 Implications: Heuristics cannot replace rigorous theoretical analyses. However, our heuristic can   \n77 serve as a target for future improvements to both privacy auditing as well as theoretical analysis. For   \n78 privacy auditing, matching or exceeding our heuristic is a more reachable goal than matching the   \n79 theoretical upper bounds, although our experimental results show that even this would require new   \n80 attacks. When theoretical analyses fail to match our heuristic, we should identify why there is a gap,   \n81 which builds intuition and could point towards further improvements.   \n82 Given that privacy auditing is computationally intensive and difficult to perform correctly [AZT24],   \n83 we believe that our heuristic can also be valuable in practice. In particular, our heuristic can be used   \n84 prior to training (e.g., during hyperparameter selection) to predict the outcome of privacy auditing   \n85 when applied to the final model. (This is a similar use case to scaling laws.)   \n87 Theorem 1 presents our heuristic differen  \n88 tial privacy analysis of DP-SGD (which we   \n89 present in Algorithm 1 for completeness;   \n90 note that we include a regularizer $r$ whose   \n91 gradient is not clipped, because it does not   \n92 depend on the private data $\\mathbf{x}$ ). We con  \n93 sider Poisson subsampled minibatches and   \n94 add/remove neighbours, as is standard in   \n95 the differential privacy literature.   \n96 Our analysis takes the form of a conditional   \n97 privacy guarantee. Namely, under the as  \n98 sumption that the loss and regularizer are   \n99 linear, we obtain a fully rigorous differen  \n100 tial privacy guarantee. The heuristic is to   \n101 apply this guarantee to loss functions that   \n102 are not linear (such as those that arise in   \n103 deep learning applications). Our thesis is   \n104 that, in most cases, the conclusion of the   \n105 theorem is still a good approximation, even   \n106 when the assumption does not hold.   \n107 Recall that a function $\\ell:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ is linear   \n108 if there exist $\\alpha\\in\\mathbb{R}^{d}$ and $\\beta\\in\\mathbb R$ such that   \n109 $\\ell(\\mathbf{m})=\\langle\\boldsymbol{\\alpha},\\mathbf{m}\\rangle+\\beta$ for all $\\mathbf{m}$ .   \n110 Theorem 1 (Privacy of DP-SGD for linear   \n111 losses). $L e t\\,\\mathbf{x},T,q,\\eta,\\sigma,\\ell,r$ be as in Algo  \n112 rithm 1. Assume $r$ and $\\ell(\\cdot,x)$ , for every   \n113 $x\\in\\mathscr{X}$ , are linear. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Algorithm 1 Noisy Clipped Stochastic Gradient Descent (DP-SGD) [BST14; ACGMMTZ16] ", "page_idx": 2}, {"type": "text", "text": "function DP-SGD( $\\mathbf{x}\\in\\mathcal{X}^{n}$ , $T\\in\\mathbb N$ , $q\\in[0,1]$ , $\\eta\\in$   \n$(0,\\infty)$ , $\\sigma\\in(0,\\infty)$ , $\\ell:\\mathbb R^{d}\\times\\mathcal X\\to\\mathbb R$ , $r:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{}$ ) Initialize model $\\mathbf{m}_{0}\\in\\mathbb{R}^{d}$ . for $t=1\\cdot\\cdot\\cdot T$ do Sample minibatch $B_{t}\\subseteq[n]$ including each element independently with probability $q$ . Compute gradients of the loss $\\nabla_{\\mathbf{m}_{t-1}}\\ell(\\mathbf{m}_{t-1},x_{i})$ for all $\\begin{array}{r l r l}{i}&{{}\\in}&{B_{t}}\\end{array}$ and of the regularizer $\\nabla_{\\mathbf{m}_{t-1}}r(\\mathbf{m}_{t-1})$ . Clip loss gradients: clip $\\left(\\nabla_{\\mathbf{m}_{t-1}}\\ell(\\mathbf{m}_{t-1},x_{i})\\right):=$ $\\begin{array}{r}{\\frac{\\nabla_{\\mathbf{m}_{t-1}}\\ell\\left(\\mathbf{m}_{t-1},x_{i}\\right)}{\\operatorname*{max}\\{1,\\|\\nabla_{\\mathbf{m}_{t-1}}\\ell\\left(\\mathbf{m}_{t-1},x_{i}\\right)\\|_{2}\\}}.}\\end{array}$ Sample noise $\\xi_{t}\\gets\\mathcal{N}(0,\\sigma^{2}I_{d})$ . Update   \n$\\begin{array}{r}{\\mathbf{m}_{t}\\!=\\!\\mathbf{m}_{t-1}\\!-\\eta\\cdot\\!\\left(\\!\\sum_{i\\in B_{t}}\\!\\mathrm{clip}\\!\\left(\\nabla_{\\mathbf{m}_{t-1}}\\ell(\\mathbf{m}_{t-1},x_{i})\\right)\\!\\right)\\!.}\\\\ {+\\nabla_{\\mathbf{m}_{t-1}}r\\!\\left(\\mathbf{m}_{t-1}\\right)\\!+\\!\\xi_{t}\\quad\\quad}\\end{array}$ end for if last_iterate_only then return $\\mathbf{m}_{T}$ else if intermediate_iterates then return m0, m1, \u00b7 \u00b7 \u00b7 , mT \u22121, mT end if   \nend function ", "page_idx": 2}, {"type": "text", "text": "114 Letting ", "page_idx": 2}, {"type": "equation", "text": "$$\nP:=\\mathsf{B i n o m i a l}(T,q)+{\\mathcal{N}}(0,\\sigma^{2}T),Q:={\\mathcal{N}}(0,\\sigma^{2}T),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "115 DP-SGD with last_iterate_only satisfies $(\\varepsilon,\\delta)$ -differential privacy with $\\varepsilon\\ge0$ arbitrary and ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\delta=\\delta_{T,q,\\sigma}(\\varepsilon):=\\operatorname*{max}\\{H_{e^{\\varepsilon}}(P,Q),H_{e^{\\varepsilon}}(Q,P)\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "116 Here, $H_{e^{\\varepsilon}}$ denotes the $e^{\\varepsilon}$ -hockey-stick-divergence $\\begin{array}{r}{H_{e^{\\varepsilon}}(P,Q):=\\operatorname*{sup}_{S}P(S)-e^{\\varepsilon}Q(S).}\\end{array}$ ", "page_idx": 2}, {"type": "text", "text": "117 Equation 1 gives us a value of the privacy failure probability parameter $\\delta$ . But it is more natural to   \n118 work with the privacy loss bound parameter $\\varepsilon$ , which can be computed by inverting the formula: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\varepsilon_{T,q,\\sigma}(\\delta):=\\operatorname*{min}\\{\\varepsilon\\geq0:\\delta_{T,q,\\sigma}(\\varepsilon)\\leq\\delta\\}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "119 Both $\\delta_{T,q,\\sigma}(\\varepsilon)$ and $\\varepsilon_{T,q,\\sigma}(\\delta)$ can be computed using existing open-source DP accounting libraries   \n120 [Goo20]. We also provide a self-contained $\\&$ efficient method for computing them in Appendix A.   \n121 The proof of Theorem 1 is deferred to Appendix A, but we sketch the main ideas: Under the linearity   \n122 assumption, the output of DP-SGD is just a sum of the gradients and noises. We can reduce to   \n123 dimension $d=1$ , since the only relevant direction is that of the gradient of the canary1(which is   \n124 constant). We can also ignore the gradients of the other examples. Thus, by rescaling, the worst case   \n125 pair of output distributions can be represented as in Equation 1. Namely, $\\begin{array}{r}{\\dot{Q}=\\sum_{t=1}^{T}\\dot{\\xi}_{t}}\\end{array}$ is simply the   \n126 noise $\\xi_{t}\\gets\\mathcal{N}(0,\\sigma^{2})$ summed over $T$ iterations; this corresponds to the case where the canary is   \n127 excluded. When the canary is included, it is sampled with probability $q$ in each iteration and thus   \n128 the total number of times it is sampled over $T$ iterations is Binomia $(T,q)$ . Thus $P$ is the sum of the   \n129 contributions of the canary and the noise. Finally the definition of differential privacy lets us compute   \n130 $\\varepsilon$ and $\\delta$ from this pair of distributions. Tightness follows from the fact that there exists a loss function   \n131 and pair of inputs such that the corresponding outputs of DP-SGD matches the pair $P$ and $Q$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "image", "img_path": "tRjgapiCpm/tmp/255276181bbdadc472617a7e7f5f35e177a28a89ea57d1365fae22208e9bd27b.jpg", "img_caption": ["", "Figure 1: Comparison of our heuristic to baselines in various parameter regimes. Horizontal axis is number of iterations $T$ and vertical axis is $\\varepsilon$ such that we have $(\\varepsilon,10^{-6})$ -DP. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "132 2.1 Baselines ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "133 In addition to privacy auditing, we compare our heuristic to two different baselines in Figure 1.   \n134 The first is the standard, composition-based analysis. We use the open-source library from Google   \n135 [Goo20], which computes a tight DP guarantee for DP-SGD with intermediate_iterates. Be  \n136 cause DP-SGD with intermediate_iterates gives the adversary more information than with   \n137 last_iterate_only, this will always give at least as large an estimate for $\\varepsilon$ as our heuristic.   \n138 We also consider approximating DP-SGD by full batch DP-GD. That is, set $q=1$ and rescale the   \n139 learning rate $\\eta$ and noise multiplier $\\sigma$ to keep the expected step and privacy noise variance constant: ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\mathrm{DP-SGD}(\\mathbf{x},T,q,\\eta,\\sigma,\\ell,r)\\,\\approx\\,\\frac{\\mathrm{DP-SGD}(\\mathbf{x},T,1,\\eta\\cdot q,\\sigma/q,\\ell,r)}{}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "140 The latter algorithm is full batch DP-GD since at each step it includes each data point in the batch   \n141 with probability 1. Since full batch DP-GD does not rely on privacy amplification by subsampling,   \n142 it is much easier to analyze its privacy guarantees. Interestingly, there is no difference between   \n143 full batch DP-GD with last_iterate_only and with intermediate_iterates; see Appendix B.   \n144 Full batch DP-GD generally has better privacy guarantees than the corresponding minibatch DP-SGD   \n145 and so this baseline usually (but not always) gives smaller values for the privacy leakage $\\varepsilon$ than our   \n146 heuristic. In practice, full batch DP-GD is too computationally expensive to run. But we can use it as   \n147 an idealized comparison point for the privacy analysis. ", "page_idx": 3}, {"type": "image", "img_path": "tRjgapiCpm/tmp/81e42712a7ccbf2eddd026cf793e8b54d935d687e01f2551a6d7275f460b70a5.jpg", "img_caption": ["Figure 2: Black-box gradient space attacks fail to achieve tight auditing when other data points are sampled from the data distribution. Heuristic and standard bounds diverge from empirical results, indicating the attack\u2019s ineffectiveness. This contrasts with previous work which tightly auditing with access to intermediate updates. "], "img_footnote": [], "page_idx": 3}, {"type": "image", "img_path": "tRjgapiCpm/tmp/d4c74789e873dc51be048f57e37d5934a7994576d47b3b2d23f43661935e5a7b.jpg", "img_caption": ["Figure 3: For gradient space attacks with adversarial datasets, the empirical epsilon $(\\varepsilon)$ closely tracks the final epsilon except for at small step counts, where distinguishing is more challenging. This is evident at both subsampling probability values we study $g=0.01$ and $q=0.1$ ). "], "img_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "tRjgapiCpm/tmp/3d5fb0654acb9ff1c755d674649897f4c4faa3316d3414780a95d7bd560ca738.jpg", "img_caption": ["Figure 4: Input space attacks show promising results with both natural and blank image settings, although blank images have higher attack success. These input space attacks achieve tighter results than gradient space attacks in the natural data setting, in contrast to findings from prior work. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "148 3 Empirical Evaluation via Privacy Auditing ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "149 Setup: We follow the construction of Nasr, Song, Thakurta, Papernot, and Carlini [NSTPC21] where   \n150 we have 3 entities, adversarial crafter, model trainer, and distinguisher. In this paper, we assume   \n151 the distinguisher only has access the final iteration of the model parameters. We use the CIFAR10   \n152 dataset [Ale09] with a WideResNet model [ZK16] unless otherwise specified; in particular, we follow   \n153 the training setup of De, Berrada, Hayes, Smith, and Balle [DBHSB22], where we train and audit   \n154 a model with $79\\%$ test accuracy and, using the standard analysis, $(\\varepsilon=\\overline{{8}},\\delta=10^{-5}$ )-DP. For each   \n155 experiment we trained 512 CIFAR10 models with and without the canary (1024 total). To compute   \n156 the empirical lower bounds we use the PLD approach with Clopper-Pearson confidence intervals   \n157 used by Nasr, Hayes, Steinke, Balle, Tram\u00e8r, Jagielski, Carlini, and Terzis [NHSBTJCT23]. Here we   \n158 assume the adversary knows the sampling rate and the number of iterations and is only estimating the   \n159 noise multiplier used in DP-SGD, from which the reported privacy parameters $\\varepsilon$ and $\\delta$ ) are derived.   \n161 We implement state-of-the-art attacks from prior work [NSTPC21; NHSBTJCT23]. These attacks   \n162 heavily rely on the intermediate steps and, as a result, do not achieve tight results. In the next   \n163 section, we design specific attacks for our heuristic privacy analysis approach to further understand   \n164 its limitations and potential vulnerabilities. We used Google Cloud A2-megagpu- $16\\mathrm{g}$ machines with   \n165 16 Nvidia A100 40GB GPUs. Overall, we use roughly 33,000 GPU hours for our experiments.   \n166 Gradient Space Attack: The most powerful attacks in prior work are gradient space attacks where   \n167 the adversary injects a malicious gradient directly into the training process, rather than an example;   \n168 prior work has shown that this attack can produce tight lower bounds, independent of the dataset   \n169 and model used for training [NHSBTJCT23]. However, these previous attacks require access to all   \n170 intermediate training steps to achieve tight results. Here, we use canary gradients in two settings: one   \n171 where the other data points are non-adversarial and sampled from the real training data, and another   \n172 where the other data points are designed to have very small gradients $(\\approx0)$ ). This last setting was   \n173 shown by [NSTPC21] to result in tighter auditing. In all attacks, we assume the distinguisher has   \n174 access to all adversarial gradient vectors. For malicious gradients, we use Dirac gradient canaries,   \n175 where gradient vectors consist of zeros in all but a single index. In both cases, the distinguishing test   \n176 measures the dot product of the final model checkpoint and the gradient canary.   \n177 Figure 2 summarizes the results for the non-adversarial data setting, with other examples sampled   \n178 from the true training data. In this experiment, we fix noise magnitude and subsampling probability,   \n179 and run for various numbers of training steps. While prior work has shown tight auditing in this   \n180 setting, we find an adversary without access to intermediate updates obtains much weaker attacks.   \n181 Indeed, auditing with this strong attack results even in much lower values than the heuristic outputs.   \n182 Our other setting assumes the other data points are maliciously chosen. We construct an adversarial   \n183 \u201cdataset\u201d of $m+1$ gradients, $m$ of which are zero, and one gradient is constant (with norm equal to   \n184 the clipping norm), applying gradients directly rather than using any examples. As this experiment   \n185 does not require computing gradients, it is very cheap to run more trials, so we run this procedure   \n186 $N=100,000$ times with the gradient canary, and $N$ times without it, and compute an empirical   \n187 estimate for $\\varepsilon$ with these values. We plot the results of this experiment in Figure 3 together with   \n188 the $\\varepsilon$ output by the theoretical analysis and the heuristic, fixing the subsampling probability and   \n189 varying the number of update steps. We adjust the noise parameter to ensure the standard theoretical   \n190 analysis produces a fixed $\\varepsilon$ bound. The empirical measured $\\varepsilon$ is close to the heuristic $\\varepsilon$ except for   \n191 when training with very small step counts: we expect this looseness to be the result of statistical   \n192 effects, as lower step counts have higher relative variance at a fixed number of trials.   \n193 Input Space Attack: In practice, adversaries typically cannot insert malicious gradients freely in   \n194 training steps. Therefore, we also study cases where the adversary is limited to inserting malicious   \n195 inputs into the training set. Label filp attacks are one of the most successful approaches used to audit   \n196 DP machine learning models in prior work [NHSBTJCT23; SNJ23]. For input space attacks, we use   \nthe loss of the malicious input as a distinguisher. Similar to our gradient space attacks, we consider   \n198 two settings for input space attacks: one where other data points are correctly sampled from the   \n199 dataset, and another where the other data points are blank images.   \n200 Figure 4 summarizes the results for this setting. Comparing to Figure 2, input space attacks achieve   \n201 tighter results than gradient space attacks. This finding is in stark contrast to prior work. The reason   \n202 is that input space attacks do not rely on intermediate iterates, so they transfer well to our setting.   \n203 In all the cases discussed so far, the empirical results for both gradient and input attacks fall below   \n204 the heuristic analysis and do not violate the upper bounds based on the underlying assumptions. This   \n205 suggests that the heuristic might serve as a good indicator for assessing potential vulnerabilities.   \n206 However, in the next section, we delve into specific attack scenarios that exploit the assumptions used   \n207 in the heuristic analysis to create edge cases where the heuristic bounds are indeed violated. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "208 4 Counterexamples ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "209 We now test the limits of our heuristic by constructing some artificial counterexamples. That is, we   \n210 construct inputs to DP-SGD with last_iterate_only such that the true privacy loss exceeds the   \n211 bound given by our heuristic. While we do not expect the contrived structures of these examples to   \n212 manifest in realistic learning settings, they highlight the difficulties of formalizing settings where the   \n213 heuristic gives a provable upper bound on the privacy loss. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "214 4.1 Warmup: Zeroing Out The Model Weights ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "215 We begin by noting the counterintuitive fact that our heuristic $\\varepsilon_{T,q,\\sigma}(\\delta)$ is not always monotone in   \n216 the number of steps $T$ when the other parameters $\\sigma,q,\\delta$ are kept constant. This is shown in Figure 1c.   \n217 More steps means there is both more noise and more signal from the gradients; these effects partially   \n218 cancel out, but the net effect can be non-monotone.   \n219 We can use a regularizer $r(\\mathbf{m})=\\|\\mathbf{m}\\|_{2}^{2}/2\\eta$ so that $\\boldsymbol{\\eta}\\cdot\\nabla_{\\mathbf{m}}r(\\mathbf{m})=\\mathbf{m}$ . This regularizer zeros out the   \n220 model from the previous step, i.e., the update of DP-SGD becomes ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\bf{m}}_{t}={\\bf{m}}_{t-1}-\\eta\\cdot\\left(\\sum_{i\\in B_{t}}\\mathsf{c l i p}\\left(\\nabla_{{\\bf{m}}_{t-1}}\\ell({\\bf{m}}_{t-1},x_{i})\\right)+\\nabla_{{\\bf{m}}_{t-1}}r({\\bf{m}}_{t-1})+\\xi_{t}\\right)}\\\\ {\\quad=\\eta\\cdot\\displaystyle\\sum_{i\\in B_{t}}\\mathsf{c l i p}\\left(\\nabla_{{\\bf{m}}_{t-1}}\\ell({\\bf{m}}_{t-1},x_{i})\\right)+\\xi_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "221 This means that the last iterate $\\mathbf{m}_{T}$ is effectively the result of only a single iteration of DP-SGD. In   \n222 particular, it will have a privacy guarantee corresponding to one iteration. Combining this regularizer   \n223 with a linear loss and a setting of the parameters $T,q,\\sigma,\\delta$ such that the privacy loss is non-monotone   \n224 \u2013 i.e., $\\varepsilon_{T,q,\\sigma}(\\delta)<\\varepsilon_{1,q,\\sigma}(\\delta)\\,-$ \u2013 yields a counterexample.   \n225 In light of this counterexample, in the next subsection, we benchmark our counterexample against   \n226 sweeping over smaller values of $T$ . I.e., we consider $\\operatorname*{max}_{t\\leq T}\\varepsilon_{t,q,\\sigma}(\\delta)$ instead of simply $\\varepsilon_{T,q,\\sigma}(\\delta)$ . ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "227 4.2 Linear Loss $^+$ Quadratic Regularizer ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "228 Consider running DP-SGD in one dimension (i.e., $d\\,=\\,1$ ) with a linear loss $\\ell(\\mathbf{m},x)\\,=\\,\\mathbf{m}x$ for   \n229 the canary and a quadratic regularizer $\\begin{array}{r}{r(\\mathbf{m})=\\frac{1}{2}\\alpha\\mathbf{m}^{2}}\\end{array}$ , where $\\alpha\\in[0,1]$ and $x\\in[-1,1]$ and we   \n230 use learning rate $\\eta\\:=\\:1$ . With sampling probability $q$ , after $T$ iterations the privacy guarantee   \n231 is equivalent to distinguishing $\\textstyle Q:={\\bar{N}}({\\bar{0}},{\\bar{\\sigma}}^{2})$ and $\\begin{array}{r}{\\dot{P}:=\\mathcal{N}(\\sum_{i\\in[T]}(1-\\alpha)^{i-1}\\mathsf{B e r n o u}|\\mathsf{I i}(q),\\widehat{\\sigma}^{2})}\\end{array}$ ,   \n232 where $\\begin{array}{r}{\\widehat{\\sigma}^{2}:=\\sigma^{2}\\sum_{i\\in[T]}(1-\\alpha)^{2(i-1)}}\\end{array}$ . When $\\alpha=0$ , this retrieves linear losses. When $\\alpha=1$ , this   \n233 corresponds to distinguishing ${\\mathcal{N}}(0,{\\widehat{\\sigma}}^{2})$ and $\\mathcal{N}(\\mathtt{B e r n o u l l i}(q),\\widehat{\\sigma}^{2})$ or, equivalently, to distinguishing   \n234 linear losses after $T=1$ iteration. If we maximize our heuristic over the number of iterations $\\leq T$ ,   \n235 then our heuristic is tight for the extremes $\\alpha\\in\\{0,1\\}$ .   \n236 A natural question is whether the worst-case privacy guarantee on this quadratic is always given by   \n237 $\\alpha\\in\\{0,1\\}$ . Perhaps surprisingly, the answer is no: we found that for $T=3$ $=3,q=0.1,\\sigma=1,\\alpha=0$ ,   \n238 DP-SGD is $(2.222,10^{-6})$ -DP. For $\\alpha\\ =\\ 1$ instead DP-SGD is $(2.182,10^{-6})$ -DP. However, for   \n239 $\\alpha=0.5$ instead the quadratic loss does not satisfy $(\\varepsilon,10^{-6})$ -DP for $\\varepsilon<2.274$ .   \n240 However, this violation is small, which suggests our heuristic is still a reasonable for this class of   \n241 examples. To validate this, we consider a set of values for the tuple $(T,q,\\sigma)$ . For each setting   \n242 of $T,q,\\sigma$ , we compute $\\operatorname*{max}_{t\\leq T}\\varepsilon_{t,q,\\sigma}(\\delta)$ at $\\delta~=~10^{-6}$ . We then compute $\\varepsilon$ for the linear loss   \n243 with quadratic regularizer example with $\\alpha\\,=\\,1/2$ in the same setting. Since the support of the   \n244 random variable $\\begin{array}{r}{\\sum_{i\\in[T]}(1-\\alpha)^{\\bar{i}-1}8\\mathsf{e r n o u}||\\boldsymbol{i}(q)}\\end{array}$ has size $2^{T}$ for $\\alpha=1/2$ , computing exact $\\varepsilon$ for   \n245 even moderate $T$ is computationally intensive. Instead, let $X$ be the random variable equal to   \n246 $\\begin{array}{r}{\\sum_{i\\in[T]}(1-\\alpha)^{i-1}{\\mathsf{B e r n o u}}||{\\mathsf{i}}(q)}\\end{array}$ , except we round up values in the support which are less than .0005   \n247 up to .0005, and then round each value in the support up to the nearest integer power of 1.05. We then   \n248 compute an exact $\\varepsilon$ for distingushing $\\mathcal{N}(0,\\widehat{\\sigma}^{2})$ vs $\\textstyle{\\hat{\\mathcal{N}}}(X,{\\widehat{\\sigma}}^{2})$ . By Lemma 4.5 of Choquette-Choo,   \n249 Ganesh, Steinke, and Thakurta [CCGST24],  w e know that d i stinguishing $\\mathcal{N}(0,\\widehat{\\sigma}^{2})$ vs. $\\textstyle{\\mathcal{N}}(\\sum_{i\\in[T]}(1-$   \n250 $\\alpha)^{i-1}\\mathsf{B e r n o u l l i}(q),\\widehat{\\sigma}^{2})$ is no harder than distingushing $\\mathcal{N}(0,\\widehat{\\sigma}^{2})$ vs $\\mathcal{N}(X,\\widehat{\\sigma}^{2})$ , and since we increase   \n251 the values in the su p port by no more than 1.05 multiplicative l y, we expect  t hat our rounding does not   \n252 increase $\\varepsilon$ by more than 1.05 multiplicatively.   \n253 In Figure 5, we plot the ratio of $\\varepsilon$ at $\\delta=10^{-6}$ for distingushing between ${\\mathcal{N}}(0,{\\widehat{\\sigma}}^{2})$ and $\\mathcal{N}(X,\\widehat{\\sigma}^{2})$   \n254 divided by the maximum over $i\\ \\in\\ [T]$ of $\\varepsilon$ at $\\delta\\:=\\:10^{\\overline{{-}}6}$ for distinguishing  b etween $\\mathcal{N}(0,i\\sigma^{2})$   \n255 and $\\mathcal{N}(\\mathtt{B i n o m i a l}(i,q),i\\sigma^{2})$ . We sweep over $T$ and $q$ , and for each $q$ In Figure $5\\mathrm{a}$ (resp. Figure   \n256 5b) we set $\\sigma$ such that distinguishing $\\textstyle{\\bar{N}}(0,\\sigma^{2})$ from ${\\mathcal{N}}(\\mathtt{B e r n o u l l i}(q),\\sigma^{2})$ satisfies $(1,10^{-6})$ -DP ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "image", "img_path": "tRjgapiCpm/tmp/8c14d6d00921ee877f1883bbb86f52dd8a3fb9cc3923d7087557f55d7c9666cd.jpg", "img_caption": ["\u03b5 for $\\alpha=1/2\\;/$ heuristic \u03b5 ", "(a) One iteration of DP-SGD satisfies $(1,10^{-6})$ - DP. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "tRjgapiCpm/tmp/2eafe7a7f4765389cf6a31575e73a1c13adaab514906e719009339527204a03a.jpg", "img_caption": ["\u03b5 for $\\alpha=1/2\\;/$ heuristic \u03b5 ", "(b) One iteration of DP-SGD satisfies $(2,10^{-6})$ - DP. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 5: Ratio of upper bound on $\\varepsilon$ for quadratic loss with $\\alpha=0.5$ divided by maximum $\\varepsilon$ of $i$ iterations on a linear loss. In Figure 5a (resp. Figure 5b), for each choice of $q,\\sigma$ is set so 1 iteration of DP-SGD satisfies $(1,10^{-6})$ -DP (resp $(2,\\bar{10}^{-\\bar{6}})$ -DP). ", "page_idx": 7}, {"type": "text", "text": "257 (resp. $(2,10^{-6})$ -DP). In the majority of settings, the linear loss heuristic provides a larger $\\varepsilon$ than   \n258 the quadratic with $\\alpha=1/2$ , and even when the quadratic provides a larger $\\varepsilon$ , the violation is small   \n259 $(\\le3\\%)$ . This is evidence that our heuristic is still a good approximation for many convex losses. ", "page_idx": 7}, {"type": "text", "text": "260 4.3 Pathological Example ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "261 If we allow the regularizer $r$ to be arbitrary \u2013 in particular, not even requiring continuity \u2013 then the   \n262 gradient can also be arbitrary. This flexibility allows us to construct a counterexample such that the   \n263 standard composition-based analysis of DP-SGD with intermediate_iterates is close to tight.   \n264 Specifically, choose the regularizer so that the update $\\mathbf{m}^{\\prime}=\\mathbf{m}-\\eta\\nabla_{\\mathbf{m}}r(\\mathbf{m})$ does the following:   \n265 $\\mathbf{m}_{1}^{\\prime}=0$ and, for $i\\in[d-1]$ , $\\mathbf{m}_{i+1}^{\\prime}=v\\cdot\\mathbf{m}_{i}$ . Here $v>1$ is a large constant. We chose the loss   \n266 so that, for our canary $x_{1}$ , we have $\\nabla_{\\mathbf{m}}\\ell(\\mathbf{m},x_{1})=(1,0,0,\\cdots\\,,0)$ and, for all other examples $x_{i}$   \n267 $(i\\in\\{2,3,\\cdots\\,,n\\})$ , we have $\\nabla_{\\mathbf{m}}\\ell(\\mathbf{m},x_{i})=\\mathbf{0}$ . Then the last iterate is ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{m}_{T}=(A_{T}+\\xi_{T,1},v A_{T-1}+v\\xi_{T-1,1}+\\xi_{T,2},v^{2}A_{T-2}+v^{2}\\xi_{T-2,1}+v\\xi_{T-1,2}+\\xi_{T,3},\\cdots),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "268 where $A_{t}\\gets\\mathsf{B e r n o u l l i}(p)$ indicates whether or not the canary was sampled in the $t$ -th iteration and   \n269 $\\xi_{t,i}$ denotes the $i$ -th coordinate of the noise $\\xi_{t}$ added in the $t$ -th step. Essentially, the last iterate $\\mathbf{m}_{T}$   \n270 contains the history of all the iterates in its coordinates. Namely, the $i$ -th coordinate of $\\mathbf{m}_{T}$ gives a   \n271 scaled noisy approximation to $A_{T-i}$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\nv^{1-i}\\mathbf{m}_{T,i}=A_{T-i}+\\sum_{j=0}^{i-1}v^{j+1-i}\\xi_{T-j,i-j}\\sim\\mathcal{N}\\left(A_{T-i},\\sigma^{2}\\frac{1-v^{-2i}}{1-v^{-2}}\\right).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "272 As $v\\rightarrow\\infty$ , the variance converges to $\\sigma^{2}$ . In other words, if $v$ is large, from the final iterate, we can   \n273 obtain $\\mathcal{N}(A_{i},\\sigma^{2})$ for all $i$ . This makes the standard composition-based analysis of DP-SGD tight. ", "page_idx": 7}, {"type": "text", "text": "274 4.4 Malicious Dataset Attack ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "275 The examples above rely on the regularizer having large unclipped gradients. We now construct a   \n276 counterexample without a regularizer, instead using other examples to amplify the canary signal.   \n277 Our heuristic assumes the adversary does not have access to the intermediate iterations and that the   \n278 model is linear. However, we can design a nonlinear model and specific training data to directly   \n279 challenge this assumption. The attack strategy is to use the model\u2019s parameters as a sort of noisy   \n280 storage, saving all iterations within them. Then with access only to the final model, an adversary   \n281 can still examine the parameters, extract the intermediate steps, and break the assumption. Our   \n282 construction introduces a data point that changes its gradient based on the number of past iterations,   \n283 making it easy to identify if the point was present a given iteration of training. The rest of the   \n284 data points are maliciously selected to ensure the noise added during training doesn\u2019t impact the   \n285 information stored in the model\u2019s parameters. We defer the full details of the attack to Appendix C.   \n286 Figure 6 summarizes the results. As illustrated in the figure, this attack achieves a auditing lower   \n287 bound matching the standard DP-SGD analysis even in the last_iterate_only setting. As a result,   \n288 the attack exceeds our heuristic. However, this is a highly artificial example and it is unlikely to   \n289 reflect real-world scenarios. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "tRjgapiCpm/tmp/ff5c99652d1e90895a69324b81b889a1d9af0b6d89b8d0ca596a56c64f844491.jpg", "table_caption": ["Table 1: Previous works showed that large batch sizes achieve high performing models [DBHSB22]. Using our heuristic analysis it is possible to achieve similar performance for smaller batch sizes. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "tRjgapiCpm/tmp/306d509ef183e33773e832817b818f8873be341f138ed67a0a9856afdb42ab8d.jpg", "img_caption": ["Figure 6: In this adversarial example, the attack encodes all training steps within the final model parameters, thereby violating the specific assumptions used to justify our heuristic analysis. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "290 5 Discussion & Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "291 Both theoretical analysis and privacy auditing are valuable for understanding privacy leakage in   \n292 machine learning, but each has limitations. Theoretical analysis is inherently conservative, while   \n293 auditing procedures evaluate only specific attacks, and may thus underrepresent the privacy leakage.   \n294 Our work introduces a novel heuristic analysis for DP-SGD that focuses on the privacy implications   \n295 of releasing only the final model iterate. This approach is based in the empirical observation that   \n296 linear loss functions accurately model the effectiveness of state of the art membership inference   \n297 attacks. Our heuristic offers a practical and computationally efficient way to estimate privacy leakage   \n298 to complement privacy auditing and the standard composition-based analysis of DP-SGD. As shown   \n299 in Table 1, we trained a series of CIFAR10 models with varying batch sizes that all achieved the   \n300 similar level of heuristic epsilon, albeit with different standard epsilon values. Remarkably, these   \n301 models exhibited similar performance and similar empirical epsilon values.   \n302 We also acknowledge the limitations of our heuristic by identifying specific counterexamples where   \n303 the heuristic underestimates the true privacy leakage.   \n305 [ACGMMTZ16] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar,   \n306 and L. Zhang. \u201cDeep learning with differential privacy\u201d. In: Proceedings of   \n307 the 2016 ACM SIGSAC conference on computer and communications security.   \n308 2016, pp. 308\u2013318. URL: https://arxiv.org/abs/1607.00133 (cit. on   \n309 pp. 1, 3).   \n310 [AKOOMS23] G. Andrew, P. Kairouz, S. Oh, A. Oprea, H. B. McMahan, and V. Suriyakumar.   \n311 \u201cOne-shot Empirical Privacy Estimation for Federated Learning\u201d. In: arXiv   \n312 preprint arXiv:2302.03098 (2023). URL: https://arxiv.org/abs/2302.   \n313 03098 (cit. on p. 2).   \n314 [Ale09] K. Alex. \u201cLearning multiple layers of features from tiny images\u201d. In: https:   \n315 // www. cs. toronto. edu/ kriz/ learning-features-2009-TR. pdf   \n316 (2009) (cit. on pp. 5, 20).   \n317 [AT22] J. Altschuler and K. Talwar. \u201cPrivacy of noisy stochastic gradient descent:   \n318 More iterations without more privacy loss\u201d. In: Advances in Neural Informa  \n319 tion Processing Systems 35 (2022), pp. 3788\u20133800. URL: https://arxiv.   \n320 org/abs/2205.13710 (cit. on p. 2).   \n321 [AZT24] M. Aerni, J. Zhang, and F. Tram\u00e8r. \u201cEvaluations of Machine Learning Privacy   \n322 Defenses are Misleading\u201d. In: arXiv preprint arXiv:2404.17399 (2024). URL:   \n323 https://arxiv.org/abs/2404.17399 (cit. on p. 2).   \n324 [BGDCTV18] B. Bichsel, T. Gehr, D. Drachsler-Cohen, P. Tsankov, and M. Vechev. \u201cDp  \n325 finder: Finding differential privacy violations by sampling and optimization\u201d.   \n326 In: Proceedings of the 2018 ACM SIGSAC Conference on Computer and   \n327 Communications Security. 2018, pp. 508\u2013524 (cit. on p. 2).   \n328 [BSA24] J. Bok, W. Su, and J. M. Altschuler. \u201cShifted Interpolation for Differential   \n329 Privacy\u201d. In: arXiv preprint arXiv:2403.00278 (2024). URL: https://arxiv.   \n330 org/abs/2403.00278 (cit. on p. 2).   \n331 [BST14] R. Bassily, A. Smith, and A. Thakurta. \u201cPrivate empirical risk minimization:   \n332 Efficient algorithms and tight error bounds\u201d. In: 2014 IEEE 55th annual   \n333 symposium on foundations of computer science. IEEE. 2014, pp. 464\u2013473.   \n334 URL: https://arxiv.org/abs/1405.7085 (cit. on pp. 1, 3).   \n335 [BTRKMW24] M. Bertran, S. Tang, A. Roth, M. Kearns, J. H. Morgenstern, and S. Z. Wu.   \n336 \u201cScalable membership inference attacks via quantile regression\u201d. In: Advances   \n337 in Neural Information Processing Systems 36 (2024). URL: https://arxiv.   \n338 org/abs/2307.03694 (cit. on p. 2).   \n339 [CCGST24] C. A. Choquette-Choo, A. Ganesh, T. Steinke, and A. Thakurta. Privacy   \n340 Amplification for Matrix Mechanisms. 2024. arXiv: 2310.15526 [cs.LG]   \n341 (cit. on p. 7).   \n342 [CCNSTT22] N. Carlini, S. Chien, M. Nasr, S. Song, A. Terzis, and F. Tramer. \u201cMembership   \n343 inference attacks from first principles\u201d. In: 2022 IEEE Symposium on Security   \n344 and Privacy (SP). IEEE. 2022, pp. 1897\u20131914. URL: https://arxiv.org/   \n345 abs/2112.03570 (cit. on p. 2).   \n346 [CYS21] R. Chourasia, J. Ye, and R. Shokri. \u201cDifferential privacy dynamics of langevin   \n347 diffusion and noisy gradient descent\u201d. In: Advances in Neural Information   \n348 Processing Systems 34 (2021), pp. 14771\u201314781. URL: https://arxiv.   \n349 org/abs/2102.05855 (cit. on p. 2).   \n350 [DBHSB22] S. De, L. Berrada, J. Hayes, S. L. Smith, and B. Balle. \u201cUnlocking high  \n351 accuracy differentially private image classification through scale\u201d. In: arXiv   \n352 preprint arXiv:2204.13650 (2022) (cit. on pp. 5, 9).   \n353 [DMNS06] C. Dwork, F. McSherry, K. Nissim, and A. Smith. \u201cCalibrating noise to   \n354 sensitivity in private data analysis\u201d. In: Theory of Cryptography: Third Theory   \n355 of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006.   \n356 Proceedings 3. Springer. 2006, pp. 265\u2013284. URL: https://www.iacr.   \n357 org/archive/tcc2006/38760266/38760266.pdf (cit. on p. 1).   \n358 [DRS19] J. Dong, A. Roth, and W. J. Su. \u201cGaussian differential privacy\u201d. In: arXiv   \n359 preprint arXiv:1905.02383 (2019) (cit. on p. 13).   \n360 [DSSUV15] C. Dwork, A. Smith, T. Steinke, J. Ullman, and S. Vadhan. \u201cRobust traceability   \n361 from trace amounts\u201d. In: 2015 IEEE 56th Annual Symposium on Foundations   \n362 of Computer Science. IEEE. 2015, pp. 650\u2013669 (cit. on p. 2).   \n363 [DWWZK18] Z. Ding, Y. Wang, G. Wang, D. Zhang, and D. Kifer. \u201cDetecting violations of   \n364 differential privacy\u201d. In: Proceedings of the 2018 ACM SIGSAC Conference   \n365 on Computer and Communications Security. 2018, pp. 475\u2013489 (cit. on p. 2).   \n366 [FMTT18] V. Feldman, I. Mironov, K. Talwar, and A. Thakurta. \u201cPrivacy amplification   \n367 by iteration\u201d. In: 2018 IEEE 59th Annual Symposium on Foundations of   \n368 Computer Science (FOCS). IEEE. 2018, pp. 521\u2013532. URL: https://arxiv.   \n369 org/abs/1808.06651 (cit. on p. 2).   \n370 [Goo20] Google. Differential Privacy Accounting. https://github.com/google/   \n371 differential-privacy/tree/main/python/dp_accounting. 2020   \n372 (cit. on pp. 3, 4).   \n373 [HSRDTMPSNC08] N. Homer, S. Szelinger, M. Redman, D. Duggan, W. Tembe, J. Muehling,   \n374 J. V. Pearson, D. A. Stephan, S. F. Nelson, and D. W. Craig. \u201cResolving   \n375 individuals contributing trace amounts of DNA to highly complex mixtures   \n376 using high-density SNP genotyping microarrays\u201d. In: PLoS genetics 4.8   \n377 (2008), e1000167 (cit. on p. 2).   \n378 [JUO20] M. Jagielski, J. Ullman, and A. Oprea. \u201cAuditing differentially private ma  \n379 chine learning: How private is private sgd?\u201d In: Advances in Neural Informa  \n380 tion Processing Systems 33 (2020), pp. 22205\u201322216 (cit. on p. 2).   \n381 [KJH20] A. Koskela, J. J\u00e4lk\u00f6, and A. Honkela. \u201cComputing tight differential privacy   \n382 guarantees using fft\u201d. In: International Conference on Artificial Intelligence   \n383 and Statistics. PMLR. 2020, pp. 2560\u20132569. URL: https://arxiv.org/   \n384 abs/1906.03049 (cit. on p. 1).   \n385 [LF20] K. Leino and M. Fredrikson. \u201cStolen memories: Leveraging model memoriza  \n386 tion for calibrated $\\{\\mathrm{White}{-}\\mathrm{Box}\\}$ membership inference\u201d. In: 29th USENIX se  \n387 curity symposium (USENIX Security 20). 2020, pp. 1605\u20131622. URL: https:   \n388 //arxiv.org/abs/1906.11798 (cit. on p. 2).   \n389 [Mir17] I. Mironov. \u201cR\u00e9nyi differential privacy\u201d. In: 2017 IEEE 30th computer secu  \n390 rity foundations symposium (CSF). IEEE. 2017, pp. 263\u2013275. URL: https:   \n391 //arxiv.org/abs/1702.07476 (cit. on p. 1).   \n392 [NHSBTJCT23] M. Nasr, J. Hayes, T. Steinke, B. Balle, F. Tram\u00e8r, M. Jagielski, N. Carlini,   \n393 and A. Terzis. \u201cTight Auditing of Differentially Private Machine Learning\u201d.   \n394 In: arXiv preprint arXiv:2302.07956 (2023). URL: https://arxiv.org/   \n395 abs/2302.07956 (cit. on pp. 1, 2, 5, 6).   \n396 [NSTPC21] M. Nasr, S. Song, A. Thakurta, N. Papernot, and N. Carlini. \u201cAdversary   \n397 instantiation: Lower bounds for differentially private machine learning\u201d. In:   \n398 2021 IEEE Symposium on security and privacy (SP). IEEE. 2021, pp. 866\u2013   \n399 882. URL: https://arxiv.org/abs/2101.04535 (cit. on pp. 1, 2, 5, 6).   \n400 [SDSOJ19] A. Sablayrolles, M. Douze, C. Schmid, Y. Ollivier, and H. J\u00e9gou. \u201cWhite  \n401 box vs black-box: Bayes optimal strategies for membership inference\u201d. In:   \n402 International Conference on Machine Learning. PMLR. 2019, pp. 5558\u20135567.   \n403 URL: https://arxiv.org/abs/1908.11229 (cit. on p. 2).   \n404 [SNJ23] T. Steinke, M. Nasr, and M. Jagielski. \u201cPrivacy auditing with one (1) training   \n405 run\u201d. In: Advances in Neural Information Processing Systems 36 (2023). URL:   \n406 https://arxiv.org/abs/2305.08846 (cit. on pp. 2, 6).   \n407 [SOJH09] S. Sankararaman, G. Obozinski, M. I. Jordan, and E. Halperin. \u201cGenomic   \n408 privacy and limits of individual detection in a pool\u201d. In: Nature genetics 41.9   \n409 (2009), pp. 965\u2013967 (cit. on p. 2).   \n410 [SSSS17] R. Shokri, M. Stronati, C. Song, and V. Shmatikov. \u201cMembership inference   \n411 attacks against machine learning models\u201d. In: 2017 IEEE symposium on   \n412 security and privacy $(S P)$ . IEEE. 2017, pp. 3\u201318 (cit. on p. 2).   \n413 [Ste22] T. Steinke. \u201cComposition of Differential Privacy & Privacy Amplification   \n414 by Subsampling\u201d. In: arXiv preprint arXiv:2210.00597 (2022). URL: https:   \n415 //arxiv.org/abs/2210.00597 (cit. on p. 1).   \n416 [TTSSJC22] F. Tramer, A. Terzis, T. Steinke, S. Song, M. Jagielski, and N. Carlini. \u201cDebug  \n417 ging differential privacy: A case study for privacy auditing\u201d. In: arXiv preprint   \n418 arXiv:2202.12219 (2022). URL: https://arxiv.org/abs/2202.12219   \n419 (cit. on p. 2).   \n420 [WBKBGGG23] Y. Wen, A. Bansal, H. Kazemi, E. Borgnia, M. Goldblum, J. Geiping, and   \n421 T. Goldstein. \u201cCanary in a coalmine: Better membership inference with en  \n422 sembled adversarial queries\u201d. In: ICLR. 2023. URL: https://arxiv.org/   \n423 abs/2210.10750 (cit. on p. 2).   \n424 [YS22] J. Ye and R. Shokri. \u201cDifferentially private learning needs hidden state (or   \n425 much faster convergence)\u201d. In: Advances in Neural Information Processing   \n426 Systems 35 (2022), pp. 703\u2013715. URL: https://arxiv.org/abs/2203.   \n427 05363 (cit. on p. 2).   \n428 [ZBWTSRPNK22] S. Zanella-B\u00e9guelin, L. Wutschitz, S. Tople, A. Salem, V. R\u00fchle, A. Paverd,   \n429 M. Naseri, and B. K\u00f6pf. \u201cBayesian estimation of differential privacy\u201d. In:   \n430 arXiv preprint arXiv:2206.05199 (2022) (cit. on p. 2).   \n431 [ZK16] S. Zagoruyko and N. Komodakis. \u201cWide residual networks\u201d. In: arXiv preprint   \n432 arXiv:1605.07146 (2016) (cit. on pp. 5, 20).   \n433 [ZLS23] S. Zarifzadeh, P. C.-J. M. Liu, and R. Shokri. \u201cLow-Cost High-Power Mem  \n434 bership Inference by Boosting Relativity\u201d. In: (2023). URL: https://arxiv.   \n435 org/abs/2312.03262 (cit. on p. 2). ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "436 A Proof of Theorem 1 ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "437 Proof. Let $x_{i^{*}}$ be the canary, let $D$ be the dataset with the canary and $D^{\\prime}$ be the dataset without the   \n438 canary. Since $\\ell$ and $r$ are linear, wlog we can assume $r=0$ and $\\nabla_{\\mathbf{m}_{t-1}}\\ell(\\mathbf{m}_{t-1},x_{i})=\\mathbf{v}_{i}$ for some   \n439 set of vectors $\\{\\mathbf{v}_{i}\\}$ , such that $\\|\\mathbf{v}_{i}\\|_{2}\\leq1$ . We can also assume wlog $\\|\\mathbf{v}_{i^{*}}\\|=1$ since, if $\\|\\mathbf{v}_{i^{*}}\\|<1$ ,   \n440 the final privacy guarantee we show only improves. ", "page_idx": 11}, {"type": "text", "text": "441 We have the following recursion for ${\\bf m}_{t}$ ", "page_idx": 11}, {"type": "equation", "text": "$$\n{\\bf m}_{t}={\\bf m}_{t-1}-\\eta\\left(\\sum_{i\\in B_{t}}{\\bf v}_{i}+\\xi_{t}\\right),\\qquad\\xi_{t}\\overset{i.i.d}{\\sim}{\\mathcal{N}}(0,\\sigma^{2}I_{d}).\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "442 Unrolling the recursion: ", "page_idx": 11}, {"type": "equation", "text": "$$\n{\\bf m}_{t}={\\bf m}_{0}-\\eta\\left[\\sum_{t\\in[T]}\\sum_{i\\in B_{t}}{\\bf v}_{i}+\\xi\\right],\\qquad\\xi\\sim N(0,T\\sigma^{2}I_{d}).\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "443 By the post-processing property of DP, we can assume that in addition to the final model $\\mathbf{m}_{T}$ , we   \n444 release $\\mathbf{m}_{0}$ and $\\{B_{t}\\setminus\\{x_{i^{*}}\\}\\}_{t\\in[T]}$ , that is we release all examples that were sampled in each batch   \n445 except for the canary. The following $f$ is a bijection, computable by an adversary using the released   \n446 information: ", "page_idx": 11}, {"type": "equation", "text": "$$\nf(\\mathbf{m}_{T}):=-\\left[\\frac{\\mathbf{m}_{T}-\\mathbf{m}_{0}}{\\eta}-\\sum_{t\\in[T]}\\sum_{i\\in B_{t}\\backslash\\{x_{i^{*}}\\}}\\mathbf{v}_{i}.\\right]\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "447 Since $f$ is a bijection, distinguishing $\\mathbf{m}_{T}$ sampled using $D$ and $D^{\\prime}$ is equivalent to distinguishing   \n448 $f(\\mathbf{m}_{T})$ instead. Now we have $f({\\bf m}_{T}^{-})=\\mathcal{N}(0,\\bar{T}\\sigma^{2}I_{d})$ for $D^{\\prime}$ , and $f(\\mathbf{m}\\bar{_{T}})=\\mathcal{N}(0,T\\sigma^{2}I_{d})+k\\mathbf{v}_{i^{*}}$ ,   \n449 $k\\sim$ Binomia $(T,q)$ . For any vector $\\mathbf{u}$ orthogonal to $\\mathbf{v}_{i^{*}}$ , by isotropy of the Gaussian distribution the   \n450 distribution of $\\langle f(\\mathbf{m}_{T}),\\mathbf{u}\\rangle$ is the same for both $D$ and $D^{\\prime}$ and independent of $\\langle f(\\mathbf{m}_{T}),\\mathbf{v}_{i^{*}}\\rangle$ , hence   \n451 distinguishing $f(\\mathbf{m}_{T})$ given $D$ and $D^{\\prime}$ is the same as distingushing $\\langle f(\\mathbf{m}_{T}),{\\dot{\\mathbf{v}}}_{i^{*}}\\rangle$ given $D$ and $D^{\\prime}$ .   \n452 Finally, the distribution of $\\langle f(\\mathbf{m}_{T}),\\mathbf{v}_{i^{*}}\\rangle$ is exactly $P$ for $D$ and exactly $Q$ for $D^{\\prime}$ . By post-processing,   \n453 this gives the theorem.   \n454 We can also see that the function $\\delta_{T,q,\\sigma}$ is tight (i.e., even if we do not release $B_{t}\\setminus\\{x_{i^{*}}\\})$ , by   \n455 considering the 1-dimensional setting, where $\\mathbf{v}_{i}=0$ for $i\\neq i^{*}$ and $\\mathbf{v}_{i^{*}}=-1,\\eta=1,\\mathbf{m}_{0}=0$ . Then,   \n456 the distribution of $\\mathbf{m}_{T}$ given $D$ is exactly $P$ , and given $D^{\\prime}$ is exactly $Q$ . \u53e3 ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "457 A.1 Computing $\\delta$ from $\\varepsilon$ ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "458 Here, we give an efficiently computable expression for the function $\\delta_{T,q,\\sigma}(\\varepsilon)$ . Using $P,Q$ as in   \n459 Theorem 1, let $f(y)$ be the privacy loss for the output $y$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{f(y)=\\log\\left(\\frac{P(y)}{Q(y)}\\right)=\\log\\left(\\displaystyle\\sum_{k=0}^{T}\\binom{T}{k}q^{k}(1-q)^{n-k}\\frac{\\exp(-(y-k)^{2}/2T\\sigma^{2})}{\\exp(-y^{2}/2T\\sigma^{2})}\\right)}&{}&\\\\ {=\\log\\left(\\displaystyle\\sum_{k=0}^{T}\\binom{T}{k}q^{k}(1-q)^{k}\\exp\\left(\\frac{2k y-k^{2}}{2T\\sigma^{2}}\\right)\\right).}&{}&\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "460 Then for any $\\varepsilon$ , using the fact that $S=\\{y:f(y)\\geq\\varepsilon\\}$ maximizes $P(S)-e^{\\varepsilon}Q(S)$ , we have: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle H_{e^{c}}(P,Q)=P(\\{y:f(y)\\geq\\varepsilon\\})-e^{\\varepsilon}Q(\\{y:f(y)\\geq\\varepsilon\\})}\\\\ {\\displaystyle\\qquad\\qquad=P(\\{y:y\\geq f^{-1}(\\varepsilon)\\})-e^{\\varepsilon}Q(\\{y:y\\geq f^{-1}(\\varepsilon)\\})}\\\\ {\\displaystyle\\qquad\\qquad=\\sum_{k=0}^{T}\\binom{T}{k}q^{k}(1-q)^{k}\\operatorname*{Pr}[N(k,T\\sigma^{2})\\geq f^{-1}(\\varepsilon)]-e^{\\varepsilon}\\operatorname*{Pr}[N(0,T\\sigma^{2})\\geq f^{-1}(\\varepsilon)].}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "461 Similarly, $S=\\{y:f(y)\\leq-\\varepsilon\\}$ maximizes $Q(S)-e^{\\varepsilon}P(S)$ so we have: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H_{e^{\\varepsilon}}(Q,P)=Q(\\{y:f(y)\\leq-\\varepsilon\\})-e^{\\varepsilon}P(\\{y:f(y)\\leq-\\varepsilon\\})}\\\\ &{\\qquad\\qquad=Q(\\{y:y\\leq f^{-1}(-\\varepsilon)\\})-e^{\\varepsilon}P(\\{y:y\\leq f^{-1}(-\\varepsilon)\\})}\\\\ &{\\qquad\\qquad=\\operatorname*{Pr}[N(0,T\\sigma^{2})\\leq f^{-1}(-\\varepsilon)]-e^{\\varepsilon}\\displaystyle\\sum_{k=0}^{T}\\binom{T}{k}q^{k}(1-q)^{k}\\operatorname*{Pr}[N(k,T\\sigma^{2})\\leq f^{-1}(-\\varepsilon)].}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "462 These expressions can be evaluated efficiently. Since $f$ is monotone, it can be inverted via binary   \n463 search. We can also use binary search to evaluate $\\varepsilon$ as a function of $\\delta$ . ", "page_idx": 12}, {"type": "text", "text": "464 B Linear Worst Case for Full Batch Setting ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "465 It turns out that in the full-batch setting, the worst-case analyses of DP-GD with   \n466 intermediate_iterates and with last_iterate_only are the same. This phenomenon arises   \n467 because there is no subsampling (because $q=1$ in Algorithm 1) and thus the algorithm is \u201cjust\u201d   \n468 the Gaussian mechanism. Intuitively, DP-GD with intermediate_iterates corresponds to $T$   \n469 calls to the Gaussian mechanism with noise multiplier $\\sigma$ , while DP-GD wit\u221ah last_iterate_only   \n470 corresponds to one call to the Gaussian mechanism with noise multiplier $\\sigma/\\sqrt{T}$ ; these are equivalent   \n471 by the properties of the Gaussian distribution.   \n472 We can formaliz\u221ae this using the language of Gaussian DP [DRS19]: DP-GD (Algorithm 1 with   \n473 $q=1$ ) satisfies ${\\sqrt{T}}/\\sigma$ -GDP. (Each iteration satisfies $1/\\sigma$ -GDP and adaptive composition implies   \n474 the overall guarantee.) This means that\u221a the privacy loss is exactly dominated by that of the Gaussian   \n475 mechanism with noise multiplier $\\sigma/\\sqrt{T}$ . Linear losses give an example such that DP-GD with   \n476 last_iterate_only has exactly this privacy loss, since the final iterate reveals the sum of all the   \n477 noisy gradient estimates. The worst-case privacy of DP-GD with intermediate_iterates is no   \n478 worse than that of DP-GD with last_iterate_only. The reverse is also true (by postprocessing).   \n479 In more detail: For $T$ iterations of (full-batch) DP-GD on a linear losses, if the losses are (wlog)   \n480 1-Lipschitz and we add noise $\\textstyle N(0,{\\frac{\\sigma^{2}}{n^{2}}}\\cdot I)\\,$ to the gradient in every round, distinguishing the last   \n481 iterate of DP-SGD on adjacent databases is equivalent to distinguishing $\\mathcal{N}(0,T\\sigma^{2})$ and $\\mathcal{N}(T,T\\sigma^{2})$ .   \n482 This can be seen as a special case of Theorem 1 for $p=1$ , so we do not a give a detailed argument   \n483 here.   \n484 If instead we are given every iteration ${\\mathbf m}_{t}$ , for any 1-Lipschitz loss, distinguishing the joint distribu  \n485 tions of ${\\bf m}_{t}$ given $\\mathbf{m}_{t-1}$ on adjacent databases is equivalent to distinguishing $\\mathcal{N}(0,\\bar{\\sigma}^{2})$ and ${\\mathcal{N}}(1,\\sigma^{2})$ .   \n486 In turn, distinguishing the distribution of all iterates on adjacent databases is equivalent to distin  \n487 guishing $\\mathcal{N}(\\mathbf{0}^{\\bar{T}},\\sigma^{2}I_{T})$ and $\\mathcal{N}(\\mathbf{1}^{T},\\sigma^{2}I_{T})$ , where $\\mathbf{0}^{T}$ and $\\mathbf{1}^{T}$ are the all-zeros and all-ones vectors in   \n488 $\\mathbb{R}^{T}$ . Because the Gaussian distribution is isotropic, distinguishing $\\mathcal{N}(\\mathbf{0}^{T},\\sigma^{2}I_{T})$ and $\\mathcal{N}(\\mathbf{1}^{T},\\sigma^{2}I_{T})$ is   \n489 equivalent to distinguishing $\\langle\\mathbf{x},\\mathbf{1}^{T}\\rangle$ where $\\mathbf{x}\\sim\\mathcal{N}(\\mathbf{0}^{T},\\sigma^{2}I_{T})$ and $\\langle\\mathbf{x},\\mathbf{1}^{T}\\rangle$ where $\\mathbf{x}\\sim\\mathcal{N}(\\mathbf{1}^{T},\\sigma^{2}I_{T})$ .   \n490 These distributions are $\\mathcal{N}(0,\\dot{T}\\sigma^{2})$ and $\\mathcal{N}(T,T\\sigma^{2})$ , the exact pair of distributions we reduced to for   \n491 last-iterate analysis of linear losses. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "492 C Malicious Dataset Attack Details ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "493 Algorithms 2, 3, and 4 summarizes the construction for the attack. The attack assume the model   \n494 parameters have dimension equal to the number of iterations. It also assumes each data point can   \n495 reference which iteration of training is currently happening (this can be implemented by having   \n496 a single model parameter which increments in each step, independently of the training examples,   \n497 without impacting the privacy of the training process). Then we build our two datasets $D$ and   \n498 $D^{\\prime}=D\\cup\\bar{\\{}x\\}$ so that all points in dataset $D$ (\u201crepeaters\u201d) run Algorithm 3 to compute gradients   \n499 and the canary point in $D^{\\prime}$ runs Algorithm 2 to compute its gradient. Our attack relies heavily on   \n500 DP-SGD\u2019s lack of assumptions on the data distribution and any specific properties of the model or   \n501 gradients. Algorithm 2, which generates the canary data point, is straightforward. Its goal is to store   \n502 in the model parameters whether it was present in iteration $i$ by outputting a gradient that changes   \n503 only the $i$ -th index of the model parameters by 1 (assuming a clipping threshold of 1). ", "page_idx": 13}, {"type": "text", "text": "504 All other data points, the \u201crepeaters\u201d, are present in both datasets ( $D$ and $D^{\\prime}$ ), and have three tasks: ", "page_idx": 13}, {"type": "text", "text": "505 \u2022 Cancel out any noise added to the model parameters at an index larger than the current   \n506 iteration. At iteration $i$ , their gradients for parameters from index $i$ onward will be the same   \n507 as the current value of the model parameter, scaled by the batch size and the learning rate to   \n508 ensure this parameter value will be 0 after the update.   \n509 \u2022 Evaluate whether the canary point was present in the previous iteration by comparing   \n510 the model parameter at index $i-1$ with a threshold, and rewrite the value of that model   \n511 parameter to a large value if the canary was present.   \n512 \u2022 Ensure that all previous decisions are not overwritten by noise by continuing to rewrite them   \n513 with a large value based on their previous value.   \n514 To achieve all of these goals simultaneously, we require that the batch size is large enough that the   \n515 repeaters\u2019 updates are not clipped.   \n516 Finally Algorithm 4 runs DP-SGD, with repeater points computing gradients with Algorithm 3 and   \n517 the canary point, sampled with probability $p$ , computing its gradient using Algorithm 2. In our   \n518 experiments we run Algorithm 4 100,000 times. And to evaluate if the model parameters was from   \n519 dataset $D$ or $D^{\\prime}$ we run a hypothesis test on the values of the model parameters. All constants are   \n520 chosen to ensure all objectives of the repeaters are satisfied. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "table", "img_path": "tRjgapiCpm/tmp/94783bdd2333c8ed2671b4dcfa2a4d74d9dd0cd93bb0ac5bf17eeab812a3f62c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "Require: model parameters $\\mathbf{x}$ , iteration number $i$ , batch size $N$ , learning rate $\\eta$ , previous history threshold $t_{\\mathrm{past}}$ , last iteration threshold $t_{\\mathrm{last}}$ , history amplification value BIG_VAL   \n1: function REPEATER $\\mathbf{\\psi}_{\\mathrm{3}}(\\mathbf{x},\\,i,\\,N,\\,\\eta,\\,t_{\\mathrm{past}},$ $t_{\\mathrm{last}}$ , BIG_VAL)   \n2: $\\mathbf{h}\\gets\\mathbf{x}_{0:i}$ $\\triangleright$ Parameter \u201chistory\u201d up to iteration $i$ , not inclusive   \n3: f \u2190xi:end $\\triangleright$ Future and current parameters, starting from iteration $i$   \n4: $\\mathbf{f}\\leftarrow-\\mathbf{f}/(\\eta\\cdot N)$ $\\triangleright$ Remove noise from last iteration   \n5: base_history $\\leftarrow-\\mathbf{x}_{0:i}/(\\eta\\cdot N)$ $\\triangleright$ By default, zero out entire history   \n6: if length $(\\mathbf{h})>1$ then   \n7: $\\mathbf{\\widetilde{h}}_{0:i-1}^{\\ast}\\left\\langle-\\,\\mathrm{BIG}_{-}\\mathrm{VAL}/(\\eta\\cdot N)\\cdot(2\\mathbb{1}[\\mathbf{h}_{0:i-1}\\geq t_{\\mathrm{past}}]-1\\right)$ \u25b7If an old iteration is large enough, it was a canary iteration, so amplify it   \n8: end if   \n9: if length $(\\mathbf{h})>0$ then   \n10: $\\mathbf{h}_{i-1}\\leftarrow\\mathrm{BIG\\mathrm{\\mathrm{-}V A L}}/(\\eta\\cdot N)\\cdot(2\\mathbb{1}[\\mathbf{h}_{i}\\geq t_{\\mathrm{last}}]-1)\\gg$ If the last iteration is large enough, it was a canary iteration, so amplify it   \n11: end if   \n12: h \u2190h + base_history \u25b7Don\u2019t zero out canary iterations   \n13: $\\mathbf{a}\\gets$ concatenate(h, f)   \n14: return \u2212a   \n15: end function ", "page_idx": 14}, {"type": "text", "text": "Algorithm 4 Encoding Attacking ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Require: add-diff, whether to add the canary, batch size $N$ , sampling rate $p$ , learning rate $(\\eta)$ ,   \niteration count/parameter count $D$   \n1: function RUN_DPSGD(add-diff)   \n2: $C\\gets1$   \n3: Initialize model $\\mathbf m\\gets\\mathbf0$ of dimension $D$   \n4: for $i=0$ to $D$ do   \n5: Generate a uniform random value $q\\in[0,1]$   \n6: $\\mathbf{r}\\gets$ repeaters $(\\mathbf{m},i)$   \n7: Compute norm $c\\gets||\\mathbf{r}||$   \n8: if $c>0$ then   \n9: Normalize $\\mathbf r\\gets\\mathbf r/\\operatorname*{max}(c,C)$   \n10: end if   \n11: Adjusted vector $\\mathbf{z}\\leftarrow\\mathbf{r}\\times N$   \n12: Verify condition on ${\\mathbf{m}}_{i}$   \n13: if $p\\leq q$ and add-diff then   \n14: $\\mathbf{r}\\gets\\mathrm{adv}(\\mathbf{m},i)$   \n15: Normalize and update ${\\bf z}$   \n16: end if   \n17: Apply Gaussian noise to $\\mathbf{z}$   \n18: Update model $\\mathbf{m}\\gets\\mathbf{m}-\\mathbf{z}\\times\\eta$   \n19: end for   \n20: return m   \n21: end function ", "page_idx": 14}, {"type": "text", "text": "521 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "523 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n524 paper\u2019s contributions and scope?   \n525 Answer: [Yes]   \n526 Justification: The abstract and introduction state what we do and then the following sections   \n527 and the appendix provide details.   \n528 Guidelines:   \n529 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n530 made in the paper.   \n531 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n532 contributions made in the paper and important assumptions and limitations. A No or   \n533 NA answer to this question will not be perceived well by the reviewers.   \n534 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n535 much the results can be expected to generalize to other settings.   \n536 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n537 are not attained by the paper. ", "page_idx": 15}, {"type": "text", "text": "538 2. Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "42 Guidelines:   \n43 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n44 the paper has limitations, but those are not discussed in the paper.   \n45 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n46 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n47 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n48 model well-specification, asymptotic approximations only holding locally). The authors   \n49 should reflect on how these assumptions might be violated in practice and what the   \n50 implications would be.   \n51 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n52 only tested on a few datasets or with a few runs. In general, empirical results often   \n53 depend on implicit assumptions, which should be articulated.   \n54 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n55 For example, a facial recognition algorithm may perform poorly when image resolution   \n56 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n57 used reliably to provide closed captions for online lectures because it fails to handle   \n58 technical jargon.   \n59 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n60 and how they scale with dataset size.   \n61 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n62 address problems of privacy and fairness.   \n63 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n64 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n65 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n66 judgment and recognize that individual actions in favor of transparency play an impor  \n67 tant role in developing norms that preserve the integrity of the community. Reviewers   \n68 will be specifically instructed to not penalize honesty concerning limitations.   \n3. Theory Assumptions and Proofs ", "page_idx": 15}, {"type": "text", "text": "69 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "570 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n571 a complete (and correct) proof? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "73 Justification: Theorem 1 is proved in Appendix A.   \n74 Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 16}, {"type": "text", "text": "585 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The setup is described for each experiment we conduct and we reference prior work that these build on. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 16}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 16}, {"type": "text", "text": "28 Answer: [No]   \n29 Justification: We intend to release the code eventually, but we are not able to do so at the   \n30 moment; we refrain from providing a detailed reason, as this could violate anonymity.   \n31 Guidelines:   \n32 \u2022 The answer NA means that paper does not include experiments requiring code.   \n33 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n34 public/guides/CodeSubmissionPolicy) for more details.   \n35 \u2022 While we encourage the release of code and data, we understand that this might not be   \n36 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n37 including code, unless this is central to the contribution (e.g., for a new open-source   \n38 benchmark).   \n39 \u2022 The instructions should contain the exact command and environment needed to run to   \n40 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n41 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n42 \u2022 The authors should provide instructions on data access and preparation, including how   \n43 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n44 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n45 proposed method and baselines. If only a subset of experiments are reproducible, they   \n46 should state which ones are omitted from the script and why.   \n47 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n48 versions (if applicable).   \n49 \u2022 Providing as much information as possible in supplemental material (appended to the   \n50 paper) is recommended, but including URLs to data and code is permitted.   \n51 6. Experimental Setting/Details   \n52 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n53 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n54 results?   \n55 Answer: [Yes]   \n56 Justification: The setup is described for each experiment we conduct and we reference   \n57 prior work that these build on. We use the standard CIFAR10 dataset for deep learning   \n58 experiments.   \n59 Guidelines:   \n60 \u2022 The answer NA means that the paper does not include experiments.   \n61 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n62 that is necessary to appreciate the results and make sense of them.   \n63 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n64 material.   \n65 7. Experiment Statistical Significance   \n66 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n67 information about the statistical significance of the experiments?   \n68 Answer: [Yes]   \n69 Justification: The auditing results present a lower bound which can be viewed as a one-sided   \n70 confidence interval. For the other results the numbers are computed non-statistically (i.e.   \n71 by numerically evaluating a formula); the only potential error here is due to numerical   \n72 precision.   \n73 Guidelines:   \n74 \u2022 The answer NA means that the paper does not include experiments.   \n75 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n76 dence intervals, or statistical significance tests, at least for the experiments that support   \n77 the main claims of the paper.   \n78 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n79 example, train/test split, initialization, random drawing of some parameter, or overall   \n80 run with given experimental conditions).   \n681 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n682 call to a library function, bootstrap, etc.)   \n683 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n684 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n685 of the mean.   \n686 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n687 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n688 of Normality of errors is not verified.   \n689 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n690 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n691 error rates).   \n692 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n693 they were calculated and reference the corresponding figures or tables in the text.   \n694 8. Experiments Compute Resources   \n695 Question: For each experiment, does the paper provide sufficient information on the com  \n696 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n697 the experiments?   \n698 Answer: [Yes]   \n699 Justification: We used A2-megagpu-16g machines from Google cloud which have 16 Nvidia   \n700 A100 40GB GPUs to run the experiments in this paper. Overall we used around 33,000   \n701 hours of GPU to run all of the experiments in the paper.   \n702 Guidelines:   \n703 \u2022 The answer NA means that the paper does not include experiments.   \n704 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n705 or cloud provider, including relevant memory and storage.   \n706 \u2022 The paper should provide the amount of compute required for each of the individual   \n707 experimental runs as well as estimate the total compute.   \n708 \u2022 The paper should disclose whether the full research project required more compute   \n709 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n710 didn\u2019t make it into the paper).   \n711 9. Code Of Ethics   \n712 Question: Does the research conducted in the paper conform, in every respect, with the   \n713 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n714 Answer: [Yes]   \n715 Justification: No human subjects or sensitive data were used.   \n716 Guidelines:   \n717 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n718 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n719 deviation from the Code of Ethics.   \n720 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n721 eration due to laws or regulations in their jurisdiction).   \n722 10. Broader Impacts   \n723 Question: Does the paper discuss both potential positive societal impacts and negative   \n724 societal impacts of the work performed?   \n725 Answer: [NA]   \n726 Justification: This work is primarily theoretical. While it is possible that downstream uses   \n727 of our work could be societally impactful, the precise consequences are difficult to foresee.   \n728 The considerations are similar to any other paper on private machine learning.   \n729 Guidelines:   \n730 \u2022 The answer NA means that there is no societal impact of the work performed.   \n731 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n732 impact or why the paper does not address societal impact.   \n733 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n734 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n735 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n736 groups), privacy considerations, and security considerations.   \n737 \u2022 The conference expects that many papers will be foundational research and not tied   \n738 to particular applications, let alone deployments. However, if there is a direct path to   \n739 any negative applications, the authors should point it out. For example, it is legitimate   \n740 to point out that an improvement in the quality of generative models could be used to   \n741 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n742 that a generic algorithm for optimizing neural networks could enable people to train   \n743 models that generate Deepfakes faster.   \n744 \u2022 The authors should consider possible harms that could arise when the technology is   \n745 being used as intended and functioning correctly, harms that could arise when the   \n746 technology is being used as intended but gives incorrect results, and harms following   \n747 from (intentional or unintentional) misuse of the technology.   \n748 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n749 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n750 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n751 feedback over time, improving the efficiency and accessibility of ML).   \n752 11. Safeguards   \n753 Question: Does the paper describe safeguards that have been put in place for responsible   \n754 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n755 image generators, or scraped datasets)?   \n756 Answer: [NA]   \n757 Justification: Our paper uses standard datasets (CIFAR10) and standard models (WideRes  \n758 Net).   \n759 Guidelines:   \n760 \u2022 The answer NA means that the paper poses no such risks.   \n761 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n762 necessary safeguards to allow for controlled use of the model, for example by requiring   \n763 that users adhere to usage guidelines or restrictions to access the model or implementing   \n764 safety filters.   \n765 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n766 should describe how they avoided releasing unsafe images.   \n767 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n768 not require this, but we encourage authors to take this into account and make a best   \n769 faith effort.   \n770 12. Licenses for existing assets   \n771 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n772 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n773 properly respected?   \n774 Answer: [Yes]   \n775 Justification: We use CIFAR10 [Ale09] and a WideResNet [ZK16].   \n776 Guidelines:   \n777 \u2022 The answer NA means that the paper does not use existing assets.   \n778 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n779 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n780 URL.   \n781 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n782 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n783 service of that source should be provided.   \n784 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n785 package should be provided. For popular datasets, paperswithcode.com/datasets   \n786 has curated licenses for some datasets. Their licensing guide can help determine the   \n787 license of a dataset.   \n788 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n789 the derived asset (if it has changed) should be provided.   \n790 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n791 the asset\u2019s creators.   \n792 13. New Assets   \n793 Question: Are new assets introduced in the paper well documented and is the documentation   \n794 provided alongside the assets?   \n795 Answer: [NA]   \n796 Justification: Our main contribution is a heuristic privacy analysis. This is fully described in   \n797 the paper and can be computed using existing open-source libraries.   \n798 Guidelines:   \n799 \u2022 The answer NA means that the paper does not release new assets.   \n800 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n801 submissions via structured templates. This includes details about training, license,   \n802 limitations, etc.   \n803 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n804 asset is used.   \n805 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n806 create an anonymized URL or include an anonymized zip file.   \n807 14. Crowdsourcing and Research with Human Subjects   \n808 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n809 include the full text of instructions given to participants and screenshots, if applicable, as   \n810 well as details about compensation (if any)?   \n811 Answer: [NA]   \n812 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n813 Guidelines:   \n814 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n815 human subjects.   \n816 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n817 tion of the paper involves human subjects, then as much detail as possible should be   \n818 included in the main paper.   \n819 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n820 or other labor should be paid at least the minimum wage in the country of the data   \n821 collector.   \n822 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n823 Subjects   \n824 Question: Does the paper describe potential risks incurred by study participants, whether   \n825 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n826 approvals (or an equivalent approval/review based on the requirements of your country or   \n827 institution) were obtained?   \n828 Answer: [NA]   \n829 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n830 Guidelines:   \n831 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n832 human subjects.   \n833 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n834 may be required for any human subjects research. If you obtained IRB approval, you   \n835 should clearly state this in the paper. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 21}]