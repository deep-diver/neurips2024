{"references": [{"fullname_first_author": "Tim Dettmers", "paper_title": "LLM.int8(): 8-bit matrix multiplication for transformers at scale", "publication_date": "2022-12-01", "reason": "This paper introduces a highly influential quantization method for LLMs, which is directly relevant to the core attack method presented in the target paper."}, {"fullname_first_author": "Manli Shu", "paper_title": "On the exploitability of instruction tuning", "publication_date": "2023-12-01", "reason": "This paper presents the adversary's attack framework to generate malicious LLMs that are used in the target paper's attack."}, {"fullname_first_author": "Jingxuan He", "paper_title": "Large language models for code: Security hardening and adversarial testing", "publication_date": "2023-11-01", "reason": "This paper provides the benchmark data for the code generation attack."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This foundational paper introduced the Transformer architecture, the basis of most modern LLMs including those used and attacked in the target paper."}, {"fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper introduced the concept of few-shot learning in LLMs, which is fundamental to the design and application of many contemporary LLMs."}]}