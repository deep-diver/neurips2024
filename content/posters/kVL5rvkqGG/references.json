{"references": [{"fullname_first_author": "R. Anil", "paper_title": "Palm 2 technical report", "publication_date": "2023-05-10", "reason": "This paper is a technical report describing the large language model PaLM 2, which is used as a backbone model for creating text embeddings in the main research paper."}, {"fullname_first_author": "D. Biderman", "paper_title": "Pythia: A suite for analyzing large language models across training and scaling", "publication_date": "2023-07-23", "reason": "This paper introduces the Pythia suite of language models, which are used as the basis for the empirical studies in the main paper, providing various sizes for compute-optimal analysis."}, {"fullname_first_author": "J. Hoffmann", "paper_title": "Training compute-optimal large language models", "publication_date": "2022-03-15", "reason": "This paper establishes scaling laws for large language models, which are foundational to the main paper's methodology for determining compute-optimal configurations for text embedding models."}, {"fullname_first_author": "J. Devlin", "paper_title": "BERT: pre-training of deep bidirectional transformers for language understanding", "publication_date": "2019-06-02", "reason": "This paper introduces BERT, a foundational model in the field of text embedding, which is referenced to highlight the shift towards decoder-only transformers in recent research."}, {"fullname_first_author": "N. Muennighoff", "paper_title": "MTEB: Massive Text Embedding Benchmark", "publication_date": "2023-05-02", "reason": "This paper introduces the MTEB benchmark, which is used for downstream task evaluation in the main paper, providing a robust measure of embedding model performance across diverse tasks."}]}