[{"figure_path": "ujE83r50tR/tables/tables_6_1.jpg", "caption": "Table 1: Results on Referring Expression Comprehension benchmarks. We note that Octopus with resolution 224/336 is up to 5x/4\u00d7 faster than a purely sequential counterpart Shikra (resolution 224) and achieves higher accuracy. More details of the inference speed comparison are reported in Sec. 4.5", "description": "This table presents the results of the Octopus model and several other models on the Referring Expression Comprehension (REC) benchmark.  The benchmark measures the ability of a model to identify an object in an image based on a textual description.  The table shows that Octopus achieves higher accuracy and is significantly faster than Shikra, a comparable purely sequential model.  The speed improvement is particularly noticeable at higher resolutions.", "section": "4 Experiments"}, {"figure_path": "ujE83r50tR/tables/tables_7_1.jpg", "caption": "Table 2: Results on Visual Question Answering benchmarks. Note that specialists are fine-tuned on each individual evaluation dataset. We gray out those specialists methods, as well as the fine-tuned results of generalists.", "description": "This table presents the results of several multi-modal large language models (MLLMs) on five visual question answering (VQA) benchmark datasets.  It compares the performance of generalist models (trained on multiple datasets) and specialist models (fine-tuned on individual datasets). The table shows the number of parameters (#LLM Params) in each model, image resolution (Res.), and accuracy scores (VQAv2, OKVQA, GQA, VizWiz, and SciQA) for each model on each dataset. The generalist models are directly comparable, while specialist models are grayed out to highlight that their scores are not directly comparable to the generalist models due to their specialized training.", "section": "4.3 Evaluation on VQA datasets"}, {"figure_path": "ujE83r50tR/tables/tables_7_2.jpg", "caption": "Table 3: Results on popular VL benchmarks. MMB is MMBench [53], LLaVAW is LLaVA-Bench (In-the-Wild) [18] and MM-V is MM-Vet Benchmark[54]. POPE [55] is reported on the average F1 score of three splits (Adersarial, Popular and Random).", "description": "This table compares the performance of Octopus against other state-of-the-art multi-modal large language models (MLLMs) on several popular vision-language benchmarks.  The benchmarks evaluated include MMBench (MMB), LLaVA-Bench (In-the-Wild) (LLaVAW), SEED, MM-Vet (MM-V), and POPE.  The table shows the number of parameters (#LLM Params), resolution, and the scores achieved by each model on each benchmark.  POPE scores are averages across three splits.", "section": "4.3 Evaluation on popular VL benchmarks"}, {"figure_path": "ujE83r50tR/tables/tables_13_1.jpg", "caption": "Table 5: Comparison of Detection Performance.", "description": "This table compares the accuracy of object detection between the final Octopus model results and the intermediate detection results from the DETR component.  It shows that the final Octopus model achieves higher accuracy than relying solely on the top-ranked DETR detection, and highlights the improved recall achieved with the top-k ranked DETR detections.", "section": "4.5 Ablation Study"}, {"figure_path": "ujE83r50tR/tables/tables_14_1.jpg", "caption": "Table 6: Referring segmentation results on RefCOCO.", "description": "This table presents the quantitative evaluation of the referring segmentation performance on the RefCOCO benchmark using cIoU.  The results show the performance of the Octopus model on the validation set and test sets (testA and testB) using a resolution of 224.  The relatively low cIoU scores are attributed to the use of low-resolution CLIP-ViT features (224x224) and the smaller size of the input images for the task. The authors suggest that increasing the input size and the training data would likely improve performance.", "section": "B Application on referring segmentation"}]