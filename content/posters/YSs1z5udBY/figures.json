[{"figure_path": "YSs1z5udBY/figures/figures_1_1.jpg", "caption": "Figure 1: (a) Loss curves on document 1 for cyclic and random shuffled fine-tuning on a pre-trained Pythia-1B model. The black circles indicate points just prior to training on the focal document. The inverted-U loss curves within each epoch demonstrate the anticipatory recovery phenomenon. (b) Shift-averaged loss curve for cyclic fine-tuning. (c) Online loss curves for cyclic and random shuffled fine-tuning with prequential evaluation.", "description": "This figure demonstrates the anticipatory recovery phenomenon observed in the paper. (a) shows the loss curves for document 1 under cyclic and shuffled fine-tuning, highlighting the inverted-U shape indicating anticipatory recovery before encountering the document again. (b) presents the shift-averaged loss curves for cyclic fine-tuning, showing the average loss across different documents and epochs. (c) compares online loss curves for cyclic and shuffled fine-tuning using prequential evaluation, where the model is evaluated on the next document before fine-tuning, demonstrating better performance with cyclic fine-tuning.", "section": "3 Emergent Anticipatory Recovery"}, {"figure_path": "YSs1z5udBY/figures/figures_2_1.jpg", "caption": "Figure 1: (a) Loss curves on document 1 for cyclic and random shuffled fine-tuning on a pre-trained Pythia-1B model. The black circles indicate points just prior to training on the focal document. The inverted-U loss curves within each epoch demonstrate the anticipatory recovery phenomenon. (b) Shift-averaged loss curve for cyclic fine-tuning. (c) Online loss curves for cyclic and random shuffled fine-tuning with prequential evaluation.", "description": "This figure demonstrates the anticipatory recovery phenomenon observed in the cyclic fine-tuning of a large language model (LLM).  Subfigure (a) shows the loss curve for a single document during cyclic and shuffled fine-tuning. The inverted-U shape in the cyclic fine-tuning curve indicates that the model's loss on the document decreases before it is seen again, even without any overlap between documents. Subfigure (b) presents a shift-averaged version of the cyclic loss curves across all documents. Subfigure (c) offers a comparison between online losses for cyclic and shuffled fine-tuning across several model sizes using a prequential evaluation metric.", "section": "3 Emergent Anticipatory Recovery"}, {"figure_path": "YSs1z5udBY/figures/figures_3_1.jpg", "caption": "Figure 3: Models trained from scratch with (a) different width (token embedding size) and (b) different depth (number of transformer blocks).", "description": "This figure shows the results of experiments on models trained from scratch, varying width (token embedding size) and depth (number of transformer blocks) to investigate the impact on anticipatory recovery.  The plots display the training loss curves for cyclic training, showing how the model's ability to recover from catastrophic forgetting is affected by changes in width and depth.  It visually supports the paper's claim that sufficient width and depth are necessary to observe the anticipatory recovery behavior.", "section": "3.2 Anticipatory Recovery is an Emergent Behavior"}, {"figure_path": "YSs1z5udBY/figures/figures_3_2.jpg", "caption": "Figure 4: Effect of data randomization strength. (a) Random masking with probability up to 0.3; (b) Random shift of context window up to 128 tokens.", "description": "This figure shows the effect of adding different levels of noise to the training data on the anticipatory recovery phenomenon.  (a) shows results when a portion of the tokens in each document are randomly masked with a probability ranging from 0 to 0.3. (b) shows the impact of randomly shifting the context window by 0 to 128 tokens. The plots illustrate that while anticipatory recovery is generally weaker with increased data variation, it still exists.", "section": "3.2 Anticipatory Recovery is an Emergent Behavior"}, {"figure_path": "YSs1z5udBY/figures/figures_3_3.jpg", "caption": "Figure 2: Effect of model size for (a) pre-trained models and (b) random initializations. In each subfigure, the left shows shift-averaged loss curves and the right shows the recovery score as a function of model size.", "description": "This figure shows the effect of model size on the anticipatory recovery phenomenon.  The left side of the figure (a) displays shift-averaged loss curves for pre-trained models of different sizes (160M, 410M, 1B, and 2.8B parameters), illustrating how the anticipatory recovery becomes more pronounced as model size increases.  The right side of (a) shows the recovery score for each model size, quantifying the strength of the anticipatory effect.  Similarly, (b) repeats the experiment but using randomly initialized models instead of pre-trained models, demonstrating that the phenomenon is still present but less strong in randomly initialized models.", "section": "3.2 Anticipatory Recovery is an Emergent Behavior"}, {"figure_path": "YSs1z5udBY/figures/figures_5_1.jpg", "caption": "Figure 6: Comparison between Adam and vanilla gradient descent on (a) randomly initialized and (b) pre-trained Pythia-1B models with cyclic training.", "description": "This figure compares the performance of Adam and vanilla gradient descent optimizers on two sets of Pythia-1B language models during cyclic training.  The left panel shows results for models initialized randomly, while the right panel displays results for pre-trained models. Each panel shows loss curves over multiple training epochs. The results show that Adam, a more advanced optimizer, generally leads to better performance than vanilla gradient descent, particularly in the context of cyclic training and pre-trained models.", "section": "3 Emergent Anticipatory Recovery"}, {"figure_path": "YSs1z5udBY/figures/figures_5_2.jpg", "caption": "Figure 8: Heat map visualizations for (a) cosine similarities between the gradient vectors of the attention layer in transformer block 12 of the model for each task; (b) loss recoveries for training on task xi (y-axis) and evaluating on task xj (x-axis); (c) cosine similarities between the flattened model weight residuals at each point in training; (d) cosine similarities between the last layer activations for document x1 at each point in training.", "description": "This figure visualizes several heatmaps to analyze the temporal structure of gradients, model weights, and activations during cyclic training.  Specifically, it shows cosine similarities between: (a) gradients of the attention layer across different tasks, revealing high similarity between proximal documents; (b) pairwise loss recovery, illustrating the amount of loss recovery on one task when training on another, showcasing a cyclical pattern; (c) flattened model weight residuals, demonstrating a cyclical structure in the weight updates; and (d) last layer activations for a specific document, indicating increasing similarity in representations across epochs. These visualizations offer insights into the dynamics and relationships between different parts of the model during cyclic training, contributing to the understanding of anticipatory recovery.", "section": "4 Understanding Cyclic Training Dynamics"}, {"figure_path": "YSs1z5udBY/figures/figures_6_1.jpg", "caption": "Figure 11: Visualization of PCA embeddings of the projected data points (f\u22121(Pxi), where fi(w) = yi w) in the toy model throughout training. Epoch 0 refers to the model before any training.", "description": "This figure visualizes the PCA embeddings of projected data points in a toy model throughout the training process. Each point represents a task (document) in the cyclic training sequence. The function fi(w) = yi w projects the model weights (w) onto the task-specific embedding space. The plot shows how these projected points evolve across multiple training epochs. Epoch 0 shows the initial state before training. The visualization helps to understand the dynamic of cyclic training and how tasks are represented in the model's weight space.", "section": "4.1 Temporal Structure of Gradients"}, {"figure_path": "YSs1z5udBY/figures/figures_6_2.jpg", "caption": "Figure 1: (a) Loss curves on document 1 for cyclic and random shuffled fine-tuning on a pre-trained Pythia-1B model. The black circles indicate points just prior to training on the focal document. The inverted-U loss curves within each epoch demonstrate the anticipatory recovery phenomenon. (b) Shift-averaged loss curve for cyclic fine-tuning. (c) Online loss curves for cyclic and random shuffled fine-tuning with prequential evaluation.", "description": "This figure demonstrates the anticipatory recovery phenomenon observed in cyclic fine-tuning of LLMs.  Panel (a) shows the loss curve for a single document across multiple training epochs, illustrating the initial loss decrease upon training, subsequent increase due to catastrophic forgetting, and a surprising decrease before the document is seen again. The inverted-U shape highlights the anticipatory recovery. Panel (b) presents the average loss curve across all documents, showcasing the same anticipatory recovery behavior across the entire sequence. Finally, panel (c) compares online performance between cyclic and random fine-tuning using prequential evaluation, showing the superior performance of cyclic training.", "section": "3 Emergent Anticipatory Recovery"}, {"figure_path": "YSs1z5udBY/figures/figures_6_3.jpg", "caption": "Figure 11: Visualization of PCA embeddings of the projected data points (f<sub>i</sub><sup>-1</sup>(P<b>x</b><sub>i</sub>), where f<sub>i</sub>(<b>w</b>) = <b>y</b><sub>i</sub><b>w</b>) in the toy model throughout training. Epoch 0 refers to the model before any training.", "description": "This figure visualizes the PCA embeddings of projected data points in a toy model throughout training. Each point represents a task, and its position reflects the task's representation in the model's embedding space. The color of each point indicates the training epoch. The figure demonstrates how the task representations evolve and organize into a circular pattern during cyclic training, illustrating the anticipatory recovery phenomenon observed in the paper.", "section": "4.1 Temporal Structure of Gradients"}, {"figure_path": "YSs1z5udBY/figures/figures_16_1.jpg", "caption": "Figure 2: Effect of model size for (a) pre-trained models and (b) random initializations. In each subfigure, the left shows shift-averaged loss curves and the right shows the recovery score as a function of model size.", "description": "This figure shows the effect of model size on the anticipatory recovery phenomenon.  The left side of the figure displays shift-averaged loss curves for cyclic fine-tuning of pre-trained Pythia models (a) and randomly initialized models (b) of varying sizes (160M, 410M, 1B, 1.4B, and 2.8B parameters).  The right side shows the corresponding recovery scores which quantify how much of the initial loss is recovered before seeing the document again.  Larger models exhibit stronger anticipatory recovery, suggesting it is an emergent property of model scale.", "section": "3.2 Anticipatory Recovery is an Emergent Behavior"}, {"figure_path": "YSs1z5udBY/figures/figures_16_2.jpg", "caption": "Figure 16: (Left) Effect of pre-training steps. The full pre-training process is 143k steps. (Right) Recovery scores for models with different pre-training steps.", "description": "This figure shows the effect of the number of pre-training steps on the anticipatory recovery phenomenon. The left panel shows the loss curves for cyclic fine-tuning with pre-trained Pythia models that have undergone different numbers of pre-training steps (6k, 12k, 24k, 48k, and 96k). The right panel shows the average recovery score for epoch 4 as a function of the number of pre-training steps.  The results indicate that models with more pre-training steps exhibit stronger anticipatory recovery, suggesting that the model's ability to fit each task is crucial for the phenomenon.", "section": "B Additional Experiment Results"}, {"figure_path": "YSs1z5udBY/figures/figures_17_1.jpg", "caption": "Figure 5: Effects of (a) number of documents (b) number of gradient steps (c) context length and (d) number of frozen blocks.", "description": "This figure shows the effects of various hyperparameters on the anticipatory recovery phenomenon.  It demonstrates how the number of documents, gradient steps, context length, and frozen blocks impact the model's ability to recover from catastrophic forgetting.  The results showcase that certain parameters are more conducive to anticipatory recovery than others, highlighting the importance of model architecture and training setup in this phenomenon.", "section": "3 Emergent Anticipatory Recovery"}, {"figure_path": "YSs1z5udBY/figures/figures_18_1.jpg", "caption": "Figure 2: Effect of model size for (a) pre-trained models and (b) random initializations. In each subfigure, the left shows shift-averaged loss curves and the right shows the recovery score as a function of model size.", "description": "This figure shows the effect of model size on anticipatory recovery.  The left panels show the average loss curves over multiple epochs for both pre-trained (a) and randomly initialized models (b).  The right panels show the recovery score, a measure of how much the model recovers from forgetting before encountering a document again.  The results indicate that anticipatory recovery is an emergent behavior that becomes more pronounced as model size increases.", "section": "3.2 Anticipatory Recovery is an Emergent Behavior"}, {"figure_path": "YSs1z5udBY/figures/figures_18_2.jpg", "caption": "Figure 23: Loss recoveries for training on task \u00e6i (y-axis) and evaluating on task \u00e6; (x-axis) for longer document sequences of different lengths.", "description": "This figure shows the pairwise loss recovery matrices for document sequences of different lengths (50, 100, and 200).  The heatmaps illustrate how much the model's loss on a given document (x<sub>j</sub>) decreases after training on a nearby document (x<sub>i</sub>) during cyclic training. The diagonal represents the loss recovery when training and evaluating on the same document. The off-diagonal elements show the effect of training on one document and evaluating on a different document. The results indicate that the anticipatory recovery is not limited to short sequences and persists even with longer document sequences.  The figures highlight a symmetrical pattern of recovery, suggesting the anticipatory behavior is not a strictly sequential phenomenon but involves relational learning between documents within the cyclical sequence.", "section": "C Additional Visualizations"}]