{"references": [{"fullname_first_author": "Ho", "paper_title": "Video Diffusion Models", "publication_date": "2022-06-22", "reason": "This paper is foundational for the field of text-to-video generation, introducing the concept of applying diffusion models to video synthesis."}, {"fullname_first_author": "Singer", "paper_title": "Make-A-Video: Text-to-Video Generation without Text-Video Data", "publication_date": "2022-09-27", "reason": "This work represents a major advancement in text-to-video generation by demonstrating high-quality video generation from text descriptions alone, without relying on paired text-video training data."}, {"fullname_first_author": "Chen", "paper_title": "VideoCrafter1: Open Diffusion Models for High-Quality Video Generation", "publication_date": "2023-10-26", "reason": "This paper presents a significant improvement in video quality and introduces open diffusion models, which are highly relevant to the current work and improve the capabilities of text-to-video generation."}, {"fullname_first_author": "Wang", "paper_title": "LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models", "publication_date": "2023-09-26", "reason": "This paper is important for its advancement in high-quality video generation, using a cascaded latent diffusion approach, which also addresses limitations related to motion synthesis."}, {"fullname_first_author": "Liu", "paper_title": "EvalCrafter: Benchmarking and Evaluating Large Video Generation Models", "publication_date": "2023-10-26", "reason": "This paper introduces a comprehensive benchmark for evaluating large video generation models, which is highly relevant to this paper's experimental evaluation and comparison of different approaches."}]}