[{"figure_path": "nkzSE5KkCA/tables/tables_6_1.jpg", "caption": "Table 1: Results of zero-shot T2V generation on MSR-VTT (Evaluation protocol comparison can be found in the appendix).", "description": "This table presents the quantitative results of zero-shot text-to-video (T2V) generation on the MSR-VTT benchmark dataset.  It compares several different models (Magic Video, Make-A-Video, Show-1, Video LDM, LaVie, PYOCO, VideoFactory, EMU VIDEO, SVD, ModelScopeT2V, a fine-tuned version of ModelScopeT2V, and the proposed DEMO model) using three metrics: FID (Frechet Inception Distance), FVD (Fr\u00e9chet Video Distance), and CLIPSIM (CLIP Similarity). Lower FID and FVD scores indicate better visual quality, while a higher CLIPSIM score indicates better alignment between the generated videos and their corresponding text descriptions. The table shows that the DEMO model outperforms other models in terms of these metrics, achieving superior performance in generating high-quality videos with good text-image alignment.", "section": "4.3 Quantitative Evaluations"}, {"figure_path": "nkzSE5KkCA/tables/tables_7_1.jpg", "caption": "Table 3: Results of T2V generation on WebVid-10M (Val).", "description": "This table presents the quantitative results of zero-shot text-to-video generation on the WebVid-10M validation set.  Three models are compared: the original ModelScopeT2V, a fine-tuned version of ModelScopeT2V, and the proposed DEMO model.  The metrics used for evaluation are FID (Fr\u00e9chet Inception Distance), FVD (Fr\u00e9chet Video Distance), and CLIPSIM (CLIP Similarity). Lower FID and FVD scores indicate better video quality, while a higher CLIPSIM score indicates better alignment between the generated video and the input text description.", "section": "4.3 Quantitative Evaluations"}, {"figure_path": "nkzSE5KkCA/tables/tables_7_2.jpg", "caption": "Table 4: Results of zero-shot T2V generation on EvalCrafter.", "description": "This table presents the quantitative evaluation results of zero-shot text-to-video (T2V) generation on the EvalCrafter benchmark.  It compares the performance of four different models: ModelScopeT2V, a fine-tuned version of ModelScopeT2V, DEMO without video-motion loss (Lvideo-motion), and the full DEMO model. The metrics used assess both video quality (VQAA, VQAT, IS) and motion quality (Action Score, Motion AC-Score, Flow Score).  Higher scores indicate better performance. The results demonstrate that the DEMO model, particularly with the inclusion of video-motion loss, significantly improves motion quality while maintaining good video quality compared to the baseline and other variations.", "section": "4.3 Quantitative Evaluations"}, {"figure_path": "nkzSE5KkCA/tables/tables_7_3.jpg", "caption": "Table 5: Results of zero-shot T2V generation on VBench.", "description": "This table presents a quantitative comparison of the performance of three different models on the VBench benchmark. The models compared are ModelScopeT2V, a fine-tuned version of ModelScopeT2V, and the proposed DEMO model.  The evaluation metrics used are Motion Dynamics, Human Action, Temporal Flickering, and Motion Smoothness. Higher scores generally indicate better performance.  The results show that DEMO outperforms the other two models across all metrics, particularly in Motion Dynamics. ", "section": "4.3 Quantitative Evaluations"}, {"figure_path": "nkzSE5KkCA/tables/tables_8_1.jpg", "caption": "Table 6: Ablation study on additional parameters in motion encoder.", "description": "This ablation study compares the performance of ModelScopeT2V, a fine-tuned version of ModelScopeT2V, ModelScopeT2V with an added motion encoder, and the proposed DEMO model across various benchmarks and metrics.  The metrics include FID, FVD, and CLIPSIM for video quality assessment on MSR-VTT, UCF-101, and WebVid-10M.  For EvalCrafter, video and motion quality are evaluated using VQA-A, VQA-T, IS, Action Score, Motion AC-Score, and Flow Score.  Finally, on VBench, Motion Dynamics, Human Action, Temporal Flickering, and Motion Smoothness are assessed.  The table highlights how the addition of the motion encoder and the complete DEMO model affect these metrics, demonstrating the impact of the proposed model components.", "section": "4.4 Ablation Studies"}, {"figure_path": "nkzSE5KkCA/tables/tables_15_1.jpg", "caption": "Table 7: Training Hyperparameters", "description": "This table lists the hyperparameters used during the training of the proposed DEMO model and its base model, LDM.  It includes hyperparameters related to the LDM model, U-Net, Motion Encoder, Motion Conditioning, and Training process, as well as the inference parameters.  The table details settings for compression rate, latent shape, channel dimensions, attention resolutions, number of parameters, dropout rate, token length, activation functions (e.g., GELU), normalization methods (e.g., GroupNorm), optimizer (Adam), learning rate scheduling (OneCycle), classifier-free guidance scale, loss weightings, and DDIM sampling steps.", "section": "3 Method"}, {"figure_path": "nkzSE5KkCA/tables/tables_16_1.jpg", "caption": "Table 8: Training dataset of current T2V models.", "description": "This table lists various Text-to-Video (T2V) models and specifies the base model and training dataset used for each.  The base model column indicates the foundational model architecture upon which each T2V model was built. The training dataset column details the specific datasets used to train each model, often including a combination of image-text and video-text datasets.  Understanding these base models and datasets helps to contextualize the differences in performance and capabilities observed between various T2V models.", "section": "4 Experiments"}, {"figure_path": "nkzSE5KkCA/tables/tables_16_2.jpg", "caption": "Table 1: Results of zero-shot T2V generation on MSR-VTT (Evaluation protocol comparison can be found in the appendix).", "description": "This table presents a comparison of the performance of different text-to-video (T2V) generation models on the MSR-VTT benchmark.  The comparison focuses on zero-shot generation, meaning the models are not fine-tuned on the MSR-VTT dataset. The table includes several metrics for evaluating video generation quality, including FID, which measures the difference between generated and real images; FVD, which assesses the temporal consistency of generated videos; and CLIPSIM, which quantifies the semantic similarity between text prompts and generated videos. The different evaluation protocols used in other studies are also noted for clarity and comparison.", "section": "4.3 Quantitative Evaluations"}, {"figure_path": "nkzSE5KkCA/tables/tables_17_1.jpg", "caption": "Table 2: Results of zero-shot T2V generation on UCF-101 (Evaluation protocol comparison can be found in the appendix).", "description": "This table presents a comparison of the performance of different Text-to-Video (T2V) generation models on the UCF-101 dataset.  The models are evaluated using Inception Score (IS), which measures the quality of generated images and Fr\u00e9chet Video Distance (FVD), which measures the similarity between real and generated videos. Higher IS indicates better image quality, while lower FVD indicates better video quality. The table shows the results for several models, including MagicVideo, Make-A-Video, Show-1, Video LDM, LaVie, PYoCo, VideoFactory, EMU VIDEO, SVD, ModelScopeT2V, a fine-tuned version of ModelScopeT2V, and DEMO (the proposed method). The evaluation protocol is detailed in the appendix. ", "section": "4.3 Quantitative Evaluations"}, {"figure_path": "nkzSE5KkCA/tables/tables_17_2.jpg", "caption": "Table 11: Quantitative results on ZeroScope.", "description": "This table presents a quantitative comparison of the proposed DEMO model against a baseline model (ZeroScope) across various metrics on three benchmark datasets: MSR-VTT, UCF-101, and WebVid-10M. The metrics include FID, FVD, CLIPSIM (for video quality), and IS (for image quality). It also includes VQAA, VQAT, ActionScore, MotionAC-Score, and FlowScore (for video quality and motion quality) on the EvalCrafter dataset and MotionDynamics, HumanAction, TemporalFlickering, and MotionSmoothness on the Vbench dataset.  The results show that DEMO+ZeroScope generally outperforms ZeroScope across most metrics, particularly in motion quality related metrics, indicating that the proposed method enhances motion synthesis in video generation.", "section": "Extended Quantitative Evaluations"}, {"figure_path": "nkzSE5KkCA/tables/tables_22_1.jpg", "caption": "Table 12: User Study Results: Comparison between DEMO and Other Models", "description": "This table presents the results of a user study comparing the performance of the proposed DEMO model against three other state-of-the-art video generation models: ModelScopeT2V, LaVie, and VideoCrafter2.  The user study evaluated the models across three key aspects: Text-Video Alignment, Visual Quality, and Motion Quality.  Each comparison shows the percentage of times users preferred DEMO over the competing model for each of the three criteria.  Additionally, it shows a comparison of DEMO to DEMO without the video-motion loss, Lvideo-motion, to assess its contribution. The results highlight DEMO's superior performance across the criteria, especially in terms of motion quality.", "section": "4.4 Ablation Studies"}]