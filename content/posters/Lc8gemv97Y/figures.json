[{"figure_path": "Lc8gemv97Y/figures/figures_2_1.jpg", "caption": "Figure 1: Overview of proposed ESRM framework for online CL. The proposed ESRM framework has two main components: Entropy Selection (ES) and Real-synthetic similarity Maximization (RM). Motivated by Obs. 2 and Obs. 3, ES is a buffer management strategy designed to use entropy as a criterion to select more real samples in the memory buffer, thereby alleviating catastrophic forgetting and performance degradation caused by the contamination. RM aims to bridge the embedding gap between synthetic and real data, as noted in Obs. 4, using a contrastive learning technique.", "description": "The figure illustrates the ESRM framework, which consists of two main components: Entropy Selection (ES) and Real-synthetic similarity Maximization (RM). ES is used to select high-quality real samples from the memory buffer based on the entropy, improving the performance of the model by reducing catastrophic forgetting. RM bridges the gap between real and synthetic data embeddings using contrastive learning, enhancing the performance of online continual learning. The framework uses entropy as a selection criterion, maximizing the mutual information between current and past representations.", "section": "5 Proposed Method"}, {"figure_path": "Lc8gemv97Y/figures/figures_3_1.jpg", "caption": "Figure 2: The entropy distribution of the training dataset produced by ER and OnPro on In-100/SDXL (P = 50%) at the end of the training.", "description": "This figure displays the entropy distributions of training data generated by two online continual learning methods, ER and OnPro, when trained on a dataset (In-100/SDXL) with 50% synthetic data contamination.  The histograms show a clear difference in entropy between real and synthetic data, with synthetic data exhibiting much lower entropy values. This observation is key to the proposed ESRM method, which uses entropy as a criterion for selecting more realistic samples.", "section": "4 Synthetic Data Contamination in Online CL"}, {"figure_path": "Lc8gemv97Y/figures/figures_4_1.jpg", "caption": "Figure 4: Overview of the proposed Entropy Selection strategy. The color of the samples indicates the class, and the number in the samples represents the entropy predicted by the learner.", "description": "This figure illustrates the steps involved in the Entropy Selection (ES) strategy used in the ESRM framework.  ES manages the memory buffer by preferentially keeping samples with higher entropy (more likely to be real data).  The four steps are:\n\n1. **Drop low entropy samples:**  Low-entropy samples (likely synthetic) from the incoming batch are dropped.\n2. **Random sampling:**  A random selection of high-entropy samples is made from the incoming batch to potentially fill the buffer. \n3. **Same class with lowest entropy:** If a sample is selected, a sample from the same class as the newly selected sample with the lowest entropy from the buffer is identified to be replaced.\n4. **Replace:** The selected low-entropy sample is replaced by a high-entropy sample from the current batch.  This process ensures that the memory buffer maintains a balance between real and synthetic samples, prioritizing real data to improve the performance of the online continual learning model.", "section": "5 Proposed Method"}, {"figure_path": "Lc8gemv97Y/figures/figures_8_1.jpg", "caption": "Figure 6: The percentage of synthetic data in the memory buffer throughout the training of ESRM on the In-100/SDXL dataset with different contamination ratios (P). The average value of 5 runs is plotted.", "description": "This figure shows how the proportion of synthetic images in the memory buffer of the ESRM model changes over the course of training.  Different lines represent different levels of synthetic data contamination in the training dataset (indicated by the ratio P).  As training progresses, the percentage of synthetic images in the buffer decreases, regardless of the initial contamination level. This demonstrates the effectiveness of the Entropy Selection (ES) component of the ESRM method in filtering out less useful synthetic samples.", "section": "4.3 Synthetic data properties"}, {"figure_path": "Lc8gemv97Y/figures/figures_9_1.jpg", "caption": "Figure 7: The ROC curve of the model trained with ESRM on the In-100/SDXL dataset (P = 50%) in predicting the synthetic status of samples in the training dataset. Real samples are regarded as positives and synthetic samples as negatives.", "description": "This ROC curve shows the performance of ESRM in distinguishing real images from synthetic ones in the training dataset.  The model uses the entropy of the predicted probability distribution to classify whether an image is real or synthetic. An AUC of 0.7098 indicates a moderate level of discrimination ability. The dotted line represents a random classifier, where the model has no ability to distinguish between real and synthetic images.", "section": "7 Discussions"}, {"figure_path": "Lc8gemv97Y/figures/figures_14_1.jpg", "caption": "Figure 8: The entropy distribution of the training set produced by all methods on In-100/SDXL (P = 50%) at the end of the training.", "description": "This figure shows the entropy distribution of training data for different online continual learning methods at the end of training on a dataset (In-100/SDXL) with 50% synthetic data contamination.  Each subplot represents a different method (ER, DER++, ERACE, OCM, GSA, OnPro, and ESRM). For each method, two histograms are shown: one for real data and one for synthetic data. The x-axis represents entropy, and the y-axis represents the count of data samples with that entropy.  The figure illustrates that the synthetic data generally has lower entropy than real data across all methods. This observation is central to the paper's proposed method, ESRM, which uses entropy as a criterion to select more realistic samples during training.", "section": "4.3 Synthetic data properties"}, {"figure_path": "Lc8gemv97Y/figures/figures_14_2.jpg", "caption": "Figure 3: T-SNE visualization of the memory data at the end of training on In-100/SDXL (P = 50%). For clarity, only the first 10 classes are visualized.", "description": "This figure uses t-SNE to visualize the feature embeddings of the memory data from the In-100/SDXL dataset after training with 50% synthetic data contamination.  It shows how the embeddings of real and synthetic data are clustered, highlighting a potential issue of misalignment that affects model performance. Only the first 10 classes are shown for clarity.", "section": "4.3 Synthetic data properties"}, {"figure_path": "Lc8gemv97Y/figures/figures_18_1.jpg", "caption": "Figure 10: Random sampled images from class \u201cn01558993\u201d (Robin) in SDXL-In100 and original ImageNet-100 dataset. For clarity, we have cropped some backgrounds and resized the vanilla ImageNet-100 samples.", "description": "This figure shows a comparison of images of robins from two datasets: SDXL-In100 (synthetic data generated by Stable Diffusion XL) and the original ImageNet-100.  The top row displays images from the synthetic dataset, while the bottom row shows images from the original dataset. The figure highlights the differences in image quality, diversity, and background complexity between the two datasets. The synthetic images exhibit a more consistent style, while the original images have a wider range of styles and backgrounds, suggesting a potential impact of synthetic data contamination on the quality of datasets used to train machine learning models.", "section": "3.2 Simulation of synthetic data contamination"}, {"figure_path": "Lc8gemv97Y/figures/figures_20_1.jpg", "caption": "Figure 11: The average training time of each method trained on CIFAR-100 (M=5k), IN-100 (M=5k), and TinyImageNet (M=10k) dataset. For better readability, the values are plotted on the logarithm scale. The numbers are averaged from 10 runs.", "description": "This figure shows the average training time of different online continual learning methods on three datasets: CIFAR-100, ImageNet-100, and TinyImageNet.  The training times are plotted on a logarithmic scale for better readability, and the values are averages across 10 runs. The methods compared are ER, DER++, ER-ACE, OCM, GSA, OnPro, and ESRM, showcasing the computational efficiency differences between these approaches.", "section": "D.7 Hardware and computation"}, {"figure_path": "Lc8gemv97Y/figures/figures_21_1.jpg", "caption": "Figure 1: Overview of proposed ESRM framework for online CL. The proposed ESRM framework has two main components: Entropy Selection (ES) and Real-synthetic similarity Maximization (RM). Motivated by Obs. 2 and Obs. 3, ES is a buffer management strategy designed to use entropy as a criterion to select more real samples in the memory buffer, thereby alleviating catastrophic forgetting and performance degradation caused by the contamination. RM aims to bridge the embedding gap between synthetic and real data, as noted in Obs. 4, using a contrastive learning technique.", "description": "This figure shows the proposed ESRM framework which consists of two main components: Entropy Selection (ES) and Real-synthetic similarity Maximization (RM).  ES is a buffer management strategy that uses entropy to select more real samples to reduce catastrophic forgetting caused by synthetic data contamination. RM uses contrastive learning to bridge the embedding gap between real and synthetic data. The diagram illustrates the data flow within ESRM, showing how entropy selection and real-synthetic similarity maximization are used to improve the performance of online continual learning models.", "section": "5 Proposed Method"}]