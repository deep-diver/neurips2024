[{"figure_path": "vtRotUd539/figures/figures_4_1.jpg", "caption": "Figure 1: Neural collapse with Deep RFM on (A) CIFAR-10 and (B) MNIST. The matrix of inner products of all pairs of points in X extracted from layers l \u2208 {1, 3, 7, 13, 19} of Deep RFM. The columns show the Gram matrices of feature vectors transformed by the AGOP from Deep RFM, Xl = M1/2(Xl \u2212 \u03bc) \u2295 (Xl \u2212 \u03bc). The data are ordered so that points of the same class are adjacent to one another, arranged from classes 1 to 10. Deep RFM uses non-linearity \u03c3(\u00b7) = cos(\u00b7) in (A) and \u03c3(\u00b7) = ReLU(\u00b7) in (B).", "description": "This figure visualizes neural collapse in Deep RFM on CIFAR-10 and MNIST datasets. It shows Gram matrices (inner product matrices) of feature vectors extracted from different layers (1, 3, 7, 13, 19) of the Deep RFM model. The feature vectors are transformed using the Average Gradient Outer Product (AGOP).  The data is ordered such that points from the same class are grouped together. The figure demonstrates the progressive collapse of within-class variability, a key characteristic of neural collapse, showcasing the different effects of using cosine and ReLU activation functions.", "section": "4 Average gradient outer product induces DNC in Deep RFM"}, {"figure_path": "vtRotUd539/figures/figures_8_1.jpg", "caption": "Figure 2: Feature variability collapse from different singular value decomposition components in (A) an MLP on MNIST, and (B) a ResNet on CIFAR-10. We measure the reduction in the NC1 metric throughout training at each of five fully-connected layers. Each layer is decomposed into its input, \u03a6(X), the projection onto the right singular space of W, S VT \u03a6(X), and then U, the left singular vectors of W, and the application of the non-linearity.", "description": "This figure visualizes how different components of a neural network layer contribute to the reduction of within-class variability (NC1 metric).  It shows the NC1 metric's evolution during training for MLP and ResNet models on MNIST and CIFAR-10 datasets, respectively. The layer is decomposed into three parts: the original input, the projection onto the right singular vectors of the weight matrix, and finally the application of non-linearity after projection onto the left singular vectors.  The results highlight the dominant role of the right singular vectors in reducing within-class variability.", "section": "6 Within-class variability collapse through AGOP in neural networks"}, {"figure_path": "vtRotUd539/figures/figures_18_1.jpg", "caption": "Figure 3: Neural collapse with Deep RFM on additional datasets with \u03c3(\u00b7) = ReLU(\u00b7). We show tr \u03a3w/tr \u03a3\u03b2, our NC1 metric on the left, and ||\u03bc\u03bc\u2122 \u2013 \u03a3ETF||, our NC2 metric, on the right. The first row is CIFAR-10, second is MNIST, third is SVHN. We plot these metrics as a function of depth of Deep RFM for the original data X (green), the data after applying the square root of the AGOP M1/2x (orange), and the data after the AGOP and non-linearity (blue).", "description": "This figure shows the results of applying Deep RFM with ReLU activation function on three different datasets: CIFAR-10, MNIST, and SVHN.  It plots two key metrics of neural collapse (NC1 and NC2) across multiple layers of the Deep RFM model. The plots compare the metrics for the original data, the data after transformation by the AGOP's square root, and the data after both AGOP transformation and the ReLU nonlinearity. The aim is to show that the AGOP is responsible for the collapse in Deep RFM.", "section": "4 Average gradient outer product induces DNC in Deep RFM"}, {"figure_path": "vtRotUd539/figures/figures_19_1.jpg", "caption": "Figure 3: Neural collapse with Deep RFM on additional datasets with \u03c3(\u00b7) = ReLU(\u00b7). We show tr \u03a3w/tr \u03a3\u03b2, our NC1 metric on the left, and ||\u03bc\u03bc\u2122 \u2013 \u03a3\u0395\u03a4F||, our NC2 metric, on the right. The first row is CIFAR-10, second is MNIST, third is SVHN. We plot these metrics as a function of depth of Deep RFM for the original data X (green), the data after applying the square root of the AGOP M\u00b9/2x (orange), and the data after the AGOP and non-linearity (blue).", "description": "This figure visualizes the neural collapse phenomenon in Deep RFM across three datasets (CIFAR-10, MNIST, and SVHN) using the ReLU activation function.  It demonstrates the effect of applying the average gradient outer product (AGOP) on the within-class variability and orthogonality properties of the data representations at different layers of the Deep RFM network.  The plots show that projection onto the AGOP significantly improves the neural collapse metrics, indicating the AGOP's crucial role in the process.", "section": "4 Average gradient outer product induces DNC in Deep RFM"}, {"figure_path": "vtRotUd539/figures/figures_20_1.jpg", "caption": "Figure 1: Neural collapse with Deep RFM on (A) CIFAR-10 and (B) MNIST. The matrix of inner products of all pairs of points in X extracted from layers l \u2208 {1,3,7, 13, 19} of Deep RFM. The columns show the Gram matrices of feature vectors transformed by the AGOP from Deep RFM, Xl = M1/2(Xl \u2212 \u03bc)\u2295Xl \u2212 \u03bc. The data are ordered so that points of the same class are adjacent to one another, arranged from classes 1 to 10. Deep RFM uses non-linearity \u03c3(\u00b7) = cos(\u00b7) in (A) and \u03c3(\u00b7) = ReLU(\u00b7) in (B).", "description": "This figure visualizes neural collapse in Deep RFM on CIFAR-10 and MNIST datasets. It shows Gram matrices (inner product matrices) of feature vectors extracted from different layers (1, 3, 7, 13, 19) of the Deep RFM network. The features are transformed using the Average Gradient Outer Product (AGOP) before computing the Gram matrices. The data is ordered such that points from the same class are adjacent. The color intensity represents the inner product value, with yellow indicating a value of 1 (similar features) and dark blue indicating -1 (dissimilar features). The figure demonstrates how the within-class variability collapses as the network deepens, showing the effectiveness of AGOP in inducing neural collapse.", "section": "4 Average gradient outer product induces DNC in Deep RFM"}, {"figure_path": "vtRotUd539/figures/figures_20_2.jpg", "caption": "Figure 1: Neural collapse with Deep RFM on (A) CIFAR-10 and (B) MNIST. The matrix of inner products of all pairs of points in Xl extracted from layers l \u2208 {1,3,7, 13, 19} of Deep RFM. The columns show the Gram matrices of feature vectors transformed by the AGOP from Deep RFM, (Xl \u2212 \u03bcl) \u2295 (Xl \u2212 \u03bcl). The data are ordered so that points of the same class are adjacent to one another, arranged from classes 1 to 10. Deep RFM uses non-linearity \u03c3(\u00b7) = cos(\u00b7) in (A) and \u03c3(\u00b7) = ReLU(\u00b7) in (B).", "description": "This figure visualizes neural collapse in Deep RFM on CIFAR-10 and MNIST datasets. It shows Gram matrices (inner product matrices) of feature vectors from different layers (1, 3, 7, 13, 19) of the Deep RFM. The feature vectors are transformed using the Average Gradient Outer Product (AGOP).  The data is ordered such that points from the same class are together. Different non-linearities (cosine and ReLU) are used for CIFAR-10 and MNIST, respectively. The figure demonstrates how the within-class variability decreases as the network depth increases, indicating neural collapse.", "section": "4 Average gradient outer product induces DNC in Deep RFM"}, {"figure_path": "vtRotUd539/figures/figures_21_1.jpg", "caption": "Figure 2: Feature variability collapse from different singular value decomposition components in (A) an MLP on MNIST, and (B) a ResNet on CIFAR-10. We measure the reduction in the NC1 metric throughout training at each of five fully-connected layers. Each layer is decomposed into its input, \u03a6(X), the projection onto the right singular space of W, S VT \u03a6(X), and then U, the left singular vectors of W, and the application of the non-linearity.", "description": "This figure visualizes how different singular value decomposition components of a neural network layer contribute to the reduction of within-class variability (NC1 metric) during training.  It shows the NC1 metric's evolution across five fully-connected layers in both an MLP (MNIST dataset) and a ResNet (CIFAR-10 dataset). The decomposition highlights the input (\u03a6(X)), the projection onto the right singular space (SVT\u03a6(X)), and finally the application of the non-linearity to the left singular vectors (ReLU(USVT\u03a6(X))). This helps understand which components are most responsible for the decrease in within-class variability.", "section": "6 Within-class variability collapse through AGOP in neural networks"}, {"figure_path": "vtRotUd539/figures/figures_22_1.jpg", "caption": "Figure 2: Feature variability collapse from different singular value decomposition components in (A) an MLP on MNIST, and (B) a ResNet on CIFAR-10. We measure the reduction in the NC1 metric throughout training at each of five fully-connected layers. Each layer is decomposed into its input, (X), the projection onto the right singular space of W, S VT (X), and then U, the left singular vectors of W, and the application of the non-linearity.", "description": "This figure visualizes the feature variability collapse (NC1) from different singular value decomposition components in an MLP and a ResNet. The reduction in NC1 metric is measured throughout the training process for five fully-connected layers. Each layer is broken down into its input, projection onto the right singular space of the weight matrix, projection onto the left singular vectors of the weight matrix, and finally the application of the non-linearity. This decomposition helps to understand the role of each component in the collapse.", "section": "6 Within-class variability collapse through AGOP in neural networks"}, {"figure_path": "vtRotUd539/figures/figures_23_1.jpg", "caption": "Figure 2: Feature variability collapse from different singular value decomposition components in (A) an MLP on MNIST, and (B) a ResNet on CIFAR-10. We measure the reduction in the NC1 metric throughout training at each of five fully-connected layers. Each layer is decomposed into its input, (X), the projection onto the right singular space of W, SV<sup>T</sup>(X), and then U, the left singular vectors of W, and the application of the non-linearity.", "description": "This figure visualizes how feature variability collapse (NC1 metric) changes throughout the training process for different singular value decomposition components. It shows the NC1 metric for Multilayer Perceptron (MLP) on MNIST dataset and Residual Network (ResNet) on CIFAR-10 dataset across five fully-connected layers, comparing the input (X), projection onto right singular space (SV<sup>T</sup>(X)), and the result after applying left singular vectors and non-linearity (ReLU(USV<sup>T</sup>\u03a6(X))).", "section": "6 Within-class variability collapse through AGOP in neural networks"}, {"figure_path": "vtRotUd539/figures/figures_24_1.jpg", "caption": "Figure 2: Feature variability collapse from different singular value decomposition components in (A) an MLP on MNIST, and (B) a ResNet on CIFAR-10. We measure the reduction in the NC1 metric throughout training at each of five fully-connected layers. Each layer is decomposed into its input, (X), the projection onto the right singular space of W, S VT (X), and then U, the left singular vectors of W, and the application of the non-linearity.", "description": "This figure analyzes feature variability collapse during training of neural networks, specifically MLPs and ResNets. It examines the impact of different components of a fully connected layer (input, right singular space projection, and application of non-linearity) on the NC1 metric (within-class variability) across multiple layers. The results highlight the role of the right singular structure in driving the collapse.", "section": "6 Within-class variability collapse through AGOP in neural networks"}, {"figure_path": "vtRotUd539/figures/figures_25_1.jpg", "caption": "Figure 2: Feature variability collapse from different singular value decomposition components in (A) an MLP on MNIST, and (B) a ResNet on CIFAR-10. We measure the reduction in the NC1 metric throughout training at each of five fully-connected layers. Each layer is decomposed into its input, (X), the projection onto the right singular space of W, SV (X), and then U, the left singular vectors of W, and the application of the non-linearity.", "description": "This figure shows how within-class variability changes during the training of an MLP and a ResNet.  The reduction in the NC1 metric (a measure of within-class variability) is tracked across five fully connected layers.  The figure breaks down each layer's contribution into three components: the original input (X), the projection onto the right singular vectors of the weight matrix (SVT(X)), and finally the effect of the left singular vectors and the non-linearity (ReLU(USVT(X))). This analysis helps determine the primary influence on reducing within-class variability, allowing to highlight the role of the weight matrix in neural collapse.", "section": "6 Within-class variability collapse through AGOP in neural networks"}, {"figure_path": "vtRotUd539/figures/figures_26_1.jpg", "caption": "Figure 2: Feature variability collapse from different singular value decomposition components in (A) an MLP on MNIST, and (B) a ResNet on CIFAR-10. We measure the reduction in the NC1 metric throughout training at each of five fully-connected layers. Each layer is decomposed into its input, (X), the projection onto the right singular space of W, SV<sup>T</sup>(X), and then U, the left singular vectors of W, and the application of the non-linearity.", "description": "This figure shows how feature variability collapse (a key aspect of Neural Collapse) changes across different layers of neural networks (MLP and ResNet) during training.  It breaks down each layer's contribution into three components: the original input features (X), the projection onto the right singular vectors of the weight matrix (SV<sup>T</sup>(X)), and the final output after applying the left singular vectors and non-linearity (ReLU(USV<sup>T</sup>(X))). By comparing the within-class variability (NC1 metric) of these components, the figure illustrates the role of the singular value decomposition in driving the collapse.", "section": "Within-class variability collapse through AGOP in neural networks"}]