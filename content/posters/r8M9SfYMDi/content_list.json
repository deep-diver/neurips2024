[{"type": "text", "text": "Dataset Decomposition: Faster LLM Training with Variable Sequence Length Curriculum ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Hadi Pouransari1,\u25e6 Chun-Liang Li1 Jen-Hao Rick Chang1 Pavan Kumar Anasosalu Vasu1 Cem Koc1 Vaishaal Shankar2,\u2020 Oncel Tuzel1 1Apple 2Anthropic ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) are commonly trained on datasets consisting of fixed-length token sequences. These datasets are created by randomly concatenating documents of various lengths and then chunking them into sequences of a predetermined target length (concat-and-chunk). Recent attention implementations mask cross-document attention, reducing the effective length of a chunk of tokens. Additionally, training on long sequences becomes computationally prohibitive due to the quadratic cost of attention. In this study, we introduce dataset decomposition, a novel variable sequence length training technique, to tackle these challenges. We decompose a dataset into a union of buckets, each containing sequences of the same size extracted from a unique document. During training, we use variable sequence length and batch-size, sampling simultaneously from all buckets with a curriculum. In contrast to the concat-and-chunk baseline, which incurs a fixed attention cost at every step of training, our proposed method incurs a computational cost proportional to the actual document lengths at each step, resulting in significant savings in training time. We train an $8\\mathbf{k}$ context-length 1B model at the same cost as a 2k context-length model trained with the baseline approach. Experiments on a webscale corpus demonstrate that our approach significantly enhances performance on standard language evaluations and long-context benchmarks, reaching target accuracy with up to $6\\times$ faster training compared to the baseline. Our method not only enables efficient pretraining on long sequences but also scales effectively with dataset size. Lastly, we shed light on a critical yet less studied aspect of training large language models: the distribution and curriculum of sequence lengths, which results in a non-negligible difference in performance.\\* ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) are often pretrained autoregressively (i.e., predicting the next token given a context) on large text corpora sourced from the web. Examples include The Pile [19], RefinedWeb [46], RedPajama [14], and DOLMA [57]. Each of these datasets comprises multiple documents, ranging from Wikipedia articles to books and code repositories. While the individual lengths of the documents vary from a few words (e.g., a message) to hundreds of thousands of words (e.g., a book), the training infrastructure often supports only a limited sequence length in a batch. To facilitate efficient training, document chunking is necessary. In this paper, we investigate the influence of document chunking, propose alternative strategies, and evaluate the proposed strategies with careful experiments. ", "page_idx": 0}, {"type": "image", "img_path": "r8M9SfYMDi/tmp/084c237325c0e3d1b6c1d0ff40ba6979668e2da8b994bdc9a380f79cb6a90847.jpg", "img_caption": ["(a) Data Efficiency "], "img_footnote": [], "page_idx": 1}, {"type": "image", "img_path": "r8M9SfYMDi/tmp/02f42fc2ce8485fbe759289962bb707561c1677c5631c05c5f4146cf0153cac7.jpg", "img_caption": ["(b) Computational Efficiency "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Scaling the training of the OpenLM-410M model to 1.1 trillion tokens on the RefinedWeb dataset. Note that each point on the figure represents a separate training from scratch (not different checkpoints of a single run). For the largest run, some tokens are seen more than once (the dataset has approximately 525 billion tokens). For dataset decomposition, we use the Grow-P2 curriculum with 8 cycles. Maximum context length is 8192 and all hyper-parameters are the same for DD and the baseline. (a) Regular metrics average when training with the baseline method and the proposed method. We observe more than $4\\times$ data efficiency compared to the baseline. Also, even at 1.1 trillion tokens, DD has a $+2.4$ accuracy improvement compared to the baseline (which has a plateauing accuracy curve even on a logarithmic ${\\bf X}$ -axis). (b) Comparison of model average accuracy versus training cost (GPU-hours). DD reaches the best accuracy of the baseline more than $6\\times$ faster. This is the combined effect of DD accuracy and speed gains. ", "page_idx": 1}, {"type": "text", "text": "Recent works [43, 37, 59, 60] popularized the concat-and-chunk approach to convert text datasets with variable document lengths into sequences with a fixed target length. In this approach, during a data preparation stage before training, we first randomly shuffle and concatenate all tokenized documents. Consecutive concatenated documents are separated by a special token $<\\tt E O T>$ , allowing the model to detect document boundaries. We then chunk the concatenated sequence into subsequences with a target sequence length. For example, 2048 and 4096 for the Llama-1 and Llama-2 models, respectively. The model is then pretrained on batches of sequences with fixed length. ", "page_idx": 1}, {"type": "text", "text": "The concat-and-chunk approach has several shortcomings. First, randomly concatenating documents can lead to the model attending to a context from an unrelated document to predict the next token. While well-trained models learn to avoid cross-document attention, this is not explicitly enforced, leading to potential spurious modeling. Second, the cross-document attention spends unnecessary computation on attending to unrelated tokens that do not facilitate learning. This is especially crucial due to the quadratic complexity of the attention mechanism. Even with an implementation of attention that supports cross-document attention masking, the computational cost for each optimization step would be bottlenecked by the longest document in the global batch, leading to significant underutilization of devices with shorter documents. Third, even if a document is shorter than the target sequence length, it may still be broken into two chunks when they are at the boundary of two sequences. This results in significantly smaller average chunk lengths compared to the original document length average (see Fig. 3a), which hinders the model\u2019s capability. ", "page_idx": 1}, {"type": "text", "text": "Recent and concurrent works on LLM training try to improve the concat-and-chunk approach: document-masking is possible with recent implementation of attention [29] as adopted in some recent pre-training recipes [39], best-fit packing [17] to reduce document chunking, and concatenating semantically related documents instead of randomly [55]. However, none of them address all three issues mentioned above together. ", "page_idx": 1}, {"type": "text", "text": "In this work, we introduce dataset decomposition (DD), a novel approach to decompose data based on their length and train with variable sequence length (VSL) and length-based curriculum to address the above issues. We obtain significant both significant accuracy improvement and straining speed-up as shown in Fig. 1. DD decomposes a given dataset containing documents of variable lengths into a union of datasets/buckets, each with sequences of a fixed length. Specifically, a dataset $\\mathcal{D}$ is decomposed into buckets $\\cup_{i}\\mathcal{D}_{i}$ , where each bucket $\\mathcal{D}_{i}$ contains sequences of length ${\\dot{2}}^{i}$ , each extracted from a unique document. During training with VSL, at every step of the optimization process, we sample $i$ (based on a curriculum) to form a batch with $b/2^{i}$ sequences from the bucket $\\mathcal{D}_{i}$ , which keeps the total number of tokens in a batch constant $(2^{i}\\times b/2^{i}\\,=\\,b)$ , regardless of which $\\mathcal{D}_{i}$ is sampled. ", "page_idx": 1}, {"type": "image", "img_path": "r8M9SfYMDi/tmp/67b470368617512f401f341d1e9f73e2527552086a8605c1327d95b0db91eda2.jpg", "img_caption": ["Figure 2: Each cell in the figure represents a token. Left: Original documents with variable lengths. Middle: Concat-and-chunk baseline to form sequences with a fixed target length (here $=4$ ). Right: Dataset decomposition method with $\\mathcal{D}_{1}$ , $\\mathcal{D}_{2}$ , and $\\mathcal{D}_{3}$ buckets . "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "This approach gives us several advantages and resolves the aforementioned issues of the concatand-chunk method. First, DD is simple and has negligible computational overhead during the data preparation stage, making it easy to scale to large datasets. Second, tokens in each sequence are ensured to be from the same document by construction, which avoids cross-document attention. Furthermore, we have access to the sequence length distribution (an auxiliary prior knowledge) which can be used to create different mixtures/curricula for training. Finally, our VSL training strategy accelerates training time: the latency for one optimization step is less when sampling from $\\mathcal{D}_{i}$ with smaller $i$ (due to attention\u2019s quadratic complexity). Following is a summary of our contributions: ", "page_idx": 2}, {"type": "text", "text": "\u2022 We introduce DD, a method to efficiently decompose a dataset of variable-length documents into a union of buckets with fixed-length sequences. DD enables efficient and robust training via VSL and length-based curriculum. \u2022 We perform large-scale experimentation using different models, datasets, and evaluation tasks to demonstrate the efficacy of the proposed method. We show (see Fig. 1) significant gains in data efficiency $(>4\\times)$ and compute efficiency ( $11\\%$ to $45\\%$ ), resulting in combined LLM pretraining acceleration of up to $6\\times$ (time to reach certain accuracy compared to baseline). \u2022 Through careful experimentation, we study the importance of sequence length distribution and mixture during pretraining for different natural language and long-context tasks. We show the effect of concatenation and chunking operations to synthetically alter sequence length (Section 3.2). ", "page_idx": 2}, {"type": "text", "text": "2 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Dataset decomposition ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given a dataset $\\mathcal{D}$ of tokenized documents $\\{d_{1},d_{2},\\ldots,d_{n}\\}$ , the goal of dataset decomposition (DD) is to reorganize $\\mathcal{D}$ as a union of buckets, $\\cup_{i}\\mathcal{D}_{i}$ , such that: (1) each bucket $\\mathcal{D}_{i}$ consists of sequences of tokens with length $l_{i}$ ; (2) each sequence $s\\in\\mathcal{D}_{i}$ is a subsequence of one document $d\\in\\mathcal{D}$ ; and (3) each token in $\\mathcal{D}$ appears in exactly one $\\mathcal{D}_{i}$ . This decomposition produces sequences that each belong to a unique document, ensuring no cross-document attention within a sequence during training. Additionally, all sequences in a given bucket $\\mathcal{D}_{i}$ have the same length $l_{i}$ , enabling efficient batching. ", "page_idx": 2}, {"type": "text", "text": "Dataset decomposition as defined above is not unique. We propose a specific decomposition, with $l_{i}=2^{i}$ , to optimally maintain the original document sequence length distribution while also enabling efficient batch pretraining, as explained in Section 2.2. We apply decomposition at the document level, which makes it very easy to integrate the method into any existing data preparation pipeline (a stage before model training) and is scalable to large datasets. For a tokenized document $d\\in\\mathcal D$ with length $l$ , where $l=2^{i_{1}}+2^{i_{2}}+\\ldots+2^{i_{k}}$ represents its binary decomposition, we break $d$ into $k$ adjacent sequences $s_{1},\\ldots,s_{k}$ , with lengths of $2^{\\bar{i_{1}}},\\ldots,2^{i_{k}}$ , respectively. Each sequence $s_{j}$ of length $2^{i_{j}}$ is then assigned to bucket $\\mathcal{D}_{i_{j}}$ . Fig. 2 shows a schematic representation of this method. ", "page_idx": 2}, {"type": "text", "text": "With our proposed dataset decomposition approach, each bucket $\\mathcal{D}_{i}$ contains sequences extracted from an original document $d$ such that the length of $d$ is at least $2^{i}$ . In Fig. 3b, we show the distribution of RefinedWeb dataset tokens over different buckets, where $\\mathcal{D}_{9}$ (corresponding to sequences with length 512) has the maximum tokens. We also highlight the original document lengths from which tokens are extracted. Most tokens in a bucket $\\mathcal{D}_{i}$ are extracted from documents with length $l$ such that $2^{i}\\leq l<2^{i+1}$ , and some tokens are rolled over from documents with length $l\\geq2^{\\breve{i}+1}$ . This demonstrates the efficacy of the method in retaining original document length, especially for long documents, which are scarce. ", "page_idx": 2}, {"type": "image", "img_path": "r8M9SfYMDi/tmp/2406899cc22e0be1fc7cfe69e261e0f37309bda03efb1e2a7fea3d88cff429c8.jpg", "img_caption": ["Figure 3: For the RefinedWeb dataset [46]: (a) Distribution of chunk lengths using different dataset preparation methods. Peaks show the percentage of chunks for each method with the same length as the target sequence length. (b) Distribution of tokens over $\\mathcal{D}_{i}$ \u2019s in DD. Color/pattern shows the $\\lfloor\\log_{2}l\\rfloor$ , where $l$ is the length of the original document each token is extracted from. (c) Probability distribution of context length (number of tokens from the same document a token can attend to) observed during training for the concat-and-chunk baseline with target sequence length 8192 and DD with $\\geq256$ mixture defined in Table 1. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "In Fig. 3a, we show the distribution of original document lengths and chunks within 2048 and 8192 target sequence lengths formed by the concat-and-chunk approach. We also present the length distribution using the bin-packing approximate algorithm introduced by a concurrent work [17]. Additionally, in Fig. 3c, we show the distribution of context length (the number of tokens from the same document a token can attend to during pretraining) when using baselines with a target sequence length of 8192 and DD. See Appendix F for additional discussion on sequence length statistics. ", "page_idx": 3}, {"type": "text", "text": "In contrast to the concat-and-chunk approach, which results in a static dataset, DD enables us to use sequence length distribution as prior knowledge and optimize the best mixture for the target task. In Section 3.2, we show the bias of each target evaluation toward a sequence length and the effect of concatenation and chunking on model performance. In Section 3.3, we study the effect of different sequence mixtures for LLM pretraining, a less-studied topic in LLM pretraining. ", "page_idx": 3}, {"type": "text", "text": "2.2 Variable sequence length training ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Following the setup in Section 2.1, we assume a set of $k$ buckets such that $\\mathcal{D}_{i}$ , containing sequences with length $2^{i}$ , are available. Let $b$ be the target batch size \u2013 the number of tokens used per optimization step. In variable sequence length (VSL) training, at every step of optimization, we first sample $i$ from available choices, then pick $\\bar{b}/2^{i}$ sequences from bucket $\\mathcal{D}_{i}$ . Since $\\mathcal{D}_{i}$ consists of sequences with length $2^{i}$ , the number of seen tokens per optimization step remains $b$ , independent of the choice of $i$ . Training LLMs with the VSL algorithm comes with several advantages. ", "page_idx": 3}, {"type": "text", "text": "First, since the total number of seen tokens per optimization step does not change, VSL does not alter optimization dynamics, and the same hyperparameters as the baseline can be utilized (see Section 3). ", "page_idx": 3}, {"type": "text", "text": "Second, in Section 3.1, we show that the time to complete one optimization step (forward+backward) for a fixed $b$ (tokens per step) varies by sequence length due to the quadratic cost of attention [63]. With VSL training, the cost of every optimization step depends on the bucket $\\mathcal{D}_{i}$ sampled for that step (and hence the sequence length). Thus, the more expensive steps (corresponding to long sequences) are compensated with less expensive steps (corresponding to short sequences). ", "page_idx": 3}, {"type": "image", "img_path": "r8M9SfYMDi/tmp/279bac2680c81a64dc0fecd624333429d0d2a97dfeb097e7fa22340be3d57505.jpg", "img_caption": ["Figure 4: (a) Average time for one optimization step $\\mathit{b}=8\\times8192$ tokens) on an $8\\!\\times\\!\\mathrm{Hl00}$ node with FSDP and FlashAttention2 for different context lengths. (b) OpenLM-1B/3B/7B models trained on 137B tokens. Accuracy and training speed gains are shown. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Finally, the sampling component in VSL (which $\\mathcal{D}_{i}$ to choose at every optimization step) enables different curricula of sequence lengths. In Section 3.4, we show the significance of such curricula on model stability and generalization accuracy. ", "page_idx": 4}, {"type": "text", "text": "3 Experiments and analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we show the efficacy of the proposed method to train LLMs of different sizes on largescale datasets and provide additional analyses. For all experiments, except the results in Section 3.5, we use RefinedWeb [46] flitering of Common Crawl [1] with a total of $\\sim525$ billion tokens using the EleutherAI/gpt-neox [9] tokenizer (vocabulary size is 50,432). Model architectures and training code are based on the OpenLM $[22]^{\\dagger}$ . For all experiments, other than model scaling in Section 3.5, we use the OpenLM-1B model with an 8k context length. Please refer to Appendix B for implementation details of all experiments. ", "page_idx": 4}, {"type": "text", "text": "Positional encoding We use Rotary Positional Embedding (RoPE) [58] to encode positions in queries and keys before the attention module. RoPE rotates the consecutive components of queries and keys with a base frequency $f_{b}=10$ , 000. Recent studies [48, 64, 36] have suggested increasing $f_{b}$ to better adapt a pretrained model for longer sequences through fine-tuning. We find that using a larger $f_{b}$ is also beneficial when training LLMs from scratch. In Table 4, we show that increasing $f_{b}$ to 100,000 improves performance for both the baseline and DD methods. ", "page_idx": 4}, {"type": "text", "text": "Evaluation We evaluate each model on a comprehensive set of standard benchmarks, mainly using LLM Foundry [2]. We report averaged accuracies over each category, as well as the regular average, which is the average of 14 regular language modeling benchmarks detailed below: ", "page_idx": 4}, {"type": "text", "text": "\u2022 Commonsense Reasoning (CSR): PIQA-0-shot [8], COPA-0-shot [52], and OpenBookQA-10- shots [40]. ", "page_idx": 4}, {"type": "text", "text": "\u2022 Language Understanding (LU): Lambada-OpenAI [44], Hellaswag-0-shot [65], Winograd-3- shots [30], and WinoGrande-5-shots [54].   \n\u2022 Reading Comprehension (RC): SQuAD-3-shots [50], BoolQ-0-shot [12], and CoQA-0-shot [51]. \u2022 World Knowledge (WK): Jeopardy-3-shots [3], ArcEasy-3-shots [13], ArcChallenge-3- shots [13], and WikiDataQA-3-shots [4] ", "page_idx": 4}, {"type": "text", "text": "To evaluate model on longer context tasks, we adopt the following real-world benchmarks: ", "page_idx": 4}, {"type": "text", "text": "\u2022 Multi-Document Question Answering (MDQA): We follow the exact setup as in Liu et al. [35], where for each question from NaturalQuestions-Open [28, 27], $r$ Wikipedia documents are retrieved such that one of them has the answer to the question, and the other $r-1$ documents are distractors. We report MDQA-10, MDQA-20, and MDQA-30 accuracy corresponding to $r=10,20$ , and 30, respectively. For each query, we evaluate the model by changing the location of the target document among distractors and report the averaged accuracy. ", "page_idx": 4}, {"type": "image", "img_path": "r8M9SfYMDi/tmp/3e571e2d1740055888e855aac51fe43958d522437532cbdc46f3aa3b2d7cc6b1.jpg", "img_caption": ["Figure 5: (a) Performance of OpenLM-1B model trained on $2^{34}$ tokens from buckets with different sequence lengths. ${\\bf(b)}$ distribution of lengths of documents for different benchmarks. (c) Effect of chunking $\\mathcal{D}_{13\\rightarrow10},$ ) and concatenating $\\mathcal{D}_{7\\rightarrow13.}$ ) sequences during pretraining on model performance. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "\u2022 TOEFL: This dataset is a multiple-choice question answering dataset from An et al. [5]. The dataset contains QA pairs for 15 longest lectures in Tseng et al. [61], Chung et al. [11]. Only one of the choices is the correct response. We estimate the correct choice by picking the choice with the lowest mean log probability value. ", "page_idx": 5}, {"type": "text", "text": "\u2022 QuALITY: This dataset is a multiple-choice question answering dataset from An et al. [5]. The dataset contains a long passage for context, followed by a question with multiple choices. Only one of the choices is the correct response. We estimate the correct choice by picking the choice with the lowest mean log probability value. ", "page_idx": 5}, {"type": "text", "text": "3.1 Training efficiency ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We first verify that VSL training enables a higher throughput than the baseline concat-and-chunk method. We enumerate model sizes (OpenLM-1B/3B/7B) and different context lengths ( $2^{6}$ to $2^{13}$ ) and measure the time to train 100 batches with a fixed global batchsize of $b=8\\times8192$ distributed over 8 GPUs in a single node. We repeat this 5 times and report the average time per optimization step in Fig. 4a (with STD mostly $<1\\mathrm{ms}\\mathrm{}$ ). See Appendix C.1 for additional results with different batchsizes $b$ . For each model, we highlight the training time overhead (due to attention\u2019s quadratic complexity with an optimized FlashAttention2 kernel [15]) when training with 8192 context lengths compared to 64 context lengths: $+35\\%$ , $+88\\%$ , and $+23\\%$ for OpenLM-1B, $-3{\\bf B}^{\\ddagger}$ , and -7B, respectively. Training overhead grows for longer context lengths (see Fig. 7 for results up to 16k context length). ", "page_idx": 5}, {"type": "text", "text": "The concat-and-chunk baseline method always operates at a fixed sequence length. For example, for the OpenLM-1B model, an optimization step with concat-and-chunk takes $243\\mathrm{ms}$ and $304\\mathrm{ms}$ for target context lengths of 2048 and 8192, respectively. The expected time for VSL, on the other hand, is the weighted average over different sequence lengths depending on the mixture. In Table 1, we report the training step time for different mixtures. For example, with the natural length distribution resulting from DD (Fig. 3b), training up to length 8192 sequences takes a similar time $(244\\mathrm{ms})$ as baseline training with length 2048 (with $243\\mathrm{ms}$ per step) per step\u2014equivalent to a $20\\%$ training time reduction compared to baseline training with a fixed length of 8192 (with $304\\mathrm{ms}$ per step). ", "page_idx": 5}, {"type": "text", "text": "3.2 Sequence length bias ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we study the effect of pretraining data sequence length on model performance in isolation. Using a single bucket $\\mathcal{D}_{i}$ as the dataset, we train an LLM from scratch on sequences with length $2^{i}$ for a total of $2^{34}$ seen tokens. Note that the number of tokens per optimization step is fixed at 256, irrespective of sequence length. We use the same training hyperparameters for all runs. In Appendix C.2, we show that our conclusions do not depend on the choice of hyperparameters. To reduce statistical error, we train each model twice from scratch with different random seeds and report the average metric for each benchmark (observing an average standard deviation of $\\sim0.3$ for regular benchmarks and $\\sim1.6$ for multi-document QA). Results are demonstrated in Fig. 5a. ", "page_idx": 5}, {"type": "table", "img_path": "r8M9SfYMDi/tmp/3e97f88f1c273cc7538d6f71bc7433b48cbc796b19c768fd7f864eef57c975ea.jpg", "table_caption": [], "table_footnote": ["Table 1: Effect of pretraining dataset mixture on model performance. Each row corresponds to a model trained on a specific mixture of dataset decomposition buckets. All models are OpenLM-1B, have seen a total of $96\\times2^{30}$ tokens, use RoPE with a base frequency of $10\\mathbf{k}$ , and are trained with the same hyperparameters. The definition of average context length is given in Appendix F. "], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "We show a significant correlation between pretraining sequence length and different benchmarks. Specifically, the accuracy of commonsense reasoning, language understanding, and world knowledge shows an inverted U-shape behavior with respect to pretraining sequence length, while reading comprehension benefits from longer sequences. This behavior can be associated with training-test distribution alignment with respect to sequence length. In Fig. 5b, we show the length distribution for different benchmarks where RC demonstrates a heavier tail compared to CSR, LU, and WK. Multi-document QA benchmarks show a vivid correlation with respect to sequence length: test accuracy is $\\approx0$ unless pretraining sequence length is greater than the test context length, which is $\\sim$ 2k, 4k, and 6k for MDQA-10, -20, and $-30$ , respectively. ", "page_idx": 6}, {"type": "text", "text": "It could be argued that data selection based on sequence lengths could introduce bias since the content (or source) of the documents might change based on the sequence lengths. To better understand the effect of sequence length on common metrics, we created two new buckets, $\\mathcal{D}_{13\\rightarrow10}$ and $\\mathcal{D}_{7\\rightarrow10}$ , from existing buckets $\\mathcal{D}_{13}$ and $\\mathcal{D}_{7}$ , respectively. The bucket $\\mathcal{D}_{13\\rightarrow10}$ contains sequences of length $2^{10}$ created by chunking sequences from $\\mathcal{D}_{13}$ into 8 subsequences and then performing a global shuffle. The bucket $\\ensuremath{\\mathcal{D}}_{7\\rightarrow10}$ also includes sequences of length $\\bar{2}^{10}$ , each formed by concatenating 8 random sequences from $\\mathcal{D}_{7}$ . ", "page_idx": 6}, {"type": "text", "text": "In Fig. 5c, we compare the regular average metric of models pretrained on these buckets; for each bucket, we train two models from scratch using different random seeds and report the averaged results. $\\mathcal{D}_{13\\rightarrow10}$ gains 2.6 points compared to $\\mathcal{D}_{13}$ while including the same content. This demonstrates the pure effect of sequence length on model accuracy. Furthermore, training on $\\mathcal{D}_{13\\rightarrow10}$ underperforms $\\mathcal{D}_{10}$ by 0.9 points, even though they are of the same length, indicating that long documents (used to construct $\\mathcal{D}_{13\\rightarrow10}$ ) correlate less with our benchmarks than short documents (used to construct $\\mathcal{D}_{10}$ ). Finally, we show that concatenation, as opposed to chunking, does not mitigate length correlation. This is evident from the fact that $\\ensuremath{\\mathcal{D}}_{7\\rightarrow10}$ scores the same as $\\mathcal{D}_{7}$ and still significantly worse than $\\mathcal{D}_{10}$ . ", "page_idx": 6}, {"type": "text", "text": "Our analysis suggests that effective base model pretraining requires a mixture of different sequence lengths to perform well on all benchmarks. Next, we systematically study the effect of dataset mixture from the sequence length perspective. ", "page_idx": 6}, {"type": "text", "text": "3.3 Data mixture ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "A key benefit of dataset decomposition is access to and control over sequence length distribution. We form datasets with different mixtures of sequence lengths and explore the performance of a model trained on each mixture. Table 1 shows the results. For all experiments, the total seen tokens and hyperparameters are fixed, and only the distribution over sequence length is changed. First, we observe that mixtures with small average context length (we provide the exact definition in Appendix F) perform poorly on MDQA, which requires long context understanding. For example, as for \u201c1k-only\u201d, $\"\\le\\!2\\mathrm{k}\"$ , and \u201cMid\u201d distributions that do not include long sequences from $\\mathcal{D}_{12}$ and $\\mathcal{D}_{13}$ . Larger average context length (e.g., as in ${\\bf\\nabla}^{66}\\ge1{\\bf k}^{\\circ},$ ) also correlates positively with performance on reading comprehension tasks, consistent with our observation in Fig. 5a, but comes at the cost of a longer training step time. ", "page_idx": 6}, {"type": "table", "img_path": "r8M9SfYMDi/tmp/b740bd83eb64aa087b4acedb08f56fb310290dd83a953d8c5eb07111a5822b1f.jpg", "table_caption": [], "table_footnote": ["Table 2: Effect of length-based curriculum. All models are OpenLM-1B and have seen a total of $96\\times2^{30}$ tokens, with exactly $2^{34}$ tokens from each $\\mathcal{D}_{i}$ for $i=8,\\ldots,13$ . We use RoPE with a base frequency of $100\\mathrm{k}$ and the same default hyperparameters. "], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Furthermore, \u201c1k-only\u201d, that is training using only the best sequence length $(=1024)$ from the study in Section 3.2 results in good performance on regular evaluations, especially for language understanding and world knowledge tasks, but is poor for long context tasks. Finally, we observe that \u201cnatural\u201d mixture, that is aligned with the distribution resulting from dataset decomposition (see Fig. 3b), obtains near-optimal performance on both regular and MDQA tasks, demonstrating the scalability of the proposed approach to large datasets without a need for intervention on the natural underlying length distribution. ", "page_idx": 7}, {"type": "text", "text": "3.4 Length-based curriculum ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We can think of short sequences as being \"easier\" compared to longer ones; hence motivating a curriculum learning [7, 18] that prioritizes short sequences. A similar idea (training with image resolutions from low to high) is explored in vision to train CLIP [49] models more efficiently [33]. In VSL, we can easily implement curriculum learning through sampling designs. At every optimization step, we sample without replacement a batch with $b$ tokens from bucket $\\mathcal{D}_{i}$ with probability $p_{i}$ . If a bucket is empty, we exclude it from sampling. We study different curricula for the $\"\\ge256\"$ mixture (with an equal number of tokens in $\\mathcal{D}_{8},\\dotsc,\\mathcal{D}_{13})$ . Results are shown in Table 2. For each curriculum, we determine the odds of picking a batch from each bucket $(=p_{i}$ \u2019s when normalized). Details of our length-based sampling and curriculum are provided in Algorithm 1. We consider curricula that shift from short to long sequences at different paces controlled by $p_{i}$ \u2019s changing linearly, with powers of 2, and with powers of 100 between buckets. ", "page_idx": 7}, {"type": "text", "text": "Due to the presence of other hyperparameter schedules during the course of training (e.g., learning rate and weight decay), a curriculum on length may result in a potential implicit bias. For example, if we only see long sequences toward the end of training, long sequence learning occurs only when the learning rate is too small. To address this potential issue, we also explore cyclic curricula, where a curriculum is applied in cycles similar to cyclic learning rate schedules [56] as shown in Fig. 6. Note that when we train on a sequence of length $l$ , we have $l$ next-token prediction losses (applied in parallel) with context lengths $0,1,\\dots,l-1$ . This already implies some mixing: when training on a \u201chard\u201d example (i.e., a long sequence), we also include \u201ceasy\u201d examples (its shorter sub-sequences). Therefore, even towards the end of each cycle, we still have some losses with short contexts. ", "page_idx": 7}, {"type": "text", "text": "Our results show that the cyclic \"Grow-P2\" curriculum is near optimal with different metrics. An additional benefit of curriculum is training stability. Li et al. [31] noticed that long sequences contribute to extreme gradient variance, especially at the beginning of training, resulting in instability. We also observe (see Appendix E) that our proposed approach with curriculum results in more stable training dynamics, thus enabling more efficient training with larger batch sizes and learning rates. ", "page_idx": 7}, {"type": "text", "text": "3.5 Scaling ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Dataset scaling In Fig. 1a, we show the performance of models trained with $2^{34},2^{35},2^{36},2^{37}$ , and $2^{38}$ total tokens using DD and baseline. We use the $^{\\bullet\\leftarrow}\\ge256^{\\circ}$ mixture and \u201cGrow-Linear\u201d curriculum with 8 cycles for DD, and a fixed target sequence length 8192 for the baseline. Results show $>2\\times$ data efficiency: our proposed method reaches the same accuracy as the baseline using less than half the tokens. ", "page_idx": 7}, {"type": "table", "img_path": "r8M9SfYMDi/tmp/56f5858ccef3445d575ded675360ca3423599fb5603dcd4b8f4e620663a0219c.jpg", "table_caption": [], "table_footnote": ["Table 3: Comparing baseline training with DD on an alternative pretraining dataset and model sizes. "], "page_idx": 8}, {"type": "table", "img_path": "r8M9SfYMDi/tmp/05bed1cf299c930efc229f879069de09a4cda3c4345da535b689d5e91c83edf4.jpg", "table_caption": [], "table_footnote": ["Table 4: Effect of RoPE base frequency, $f_{b}$ , in pretraining. "], "page_idx": 8}, {"type": "text", "text": "Model scaling We report results on OpenLM-1B, -3B, and -7B trained from scratch for a total of $2^{37}$ tokens in Fig. 4b. We compare baseline training with a fixed target sequence length 8192 and VSL training with a $D D_{\\geq256}$ mixture and the \"Grow-Linear\" curriculum with 8 cycles. Training with DD results in significant accuracy gains and reductions in training wall-clock time at different scales. ", "page_idx": 8}, {"type": "text", "text": "Alternative dataset We demonstrate the efficacy of our proposed method on another large-scale dataset, DataComp-LM [32]. We train models with different numbers of parameters: OpenLM-160M, -410M, and -1B, for a total of 137B tokens. We compare the baseline with a $D D_{\\geq256}$ mixture trained with the \"Grow-P2\" curriculum with 8 cycles. Results are reported in Table 3, demonstrating significant accuracy and training efficiency gains. ", "page_idx": 8}, {"type": "text", "text": "3.6 Comparison with state-of-the-art ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We compare our proposed method, data decomposition, with other approaches for handling various document lengths of pretraining data, including document masking (DM), best-fit sequence packing [17], and in-context pretraining (ICLM) [55]. We describe the details of our implementation of the best-fit packing in Appendix D. For ICLM, we use the official implementation\u00a7 applied to the RefinedWeb dataset. The results are shown in Table 5. ", "page_idx": 8}, {"type": "text", "text": "Pre-training context length is an important factor in determining a model\u2019s long-context performance. We empirically validate this in the results shown in Fig. 5a, where models trained on longer sequences perform better on multi-document QA. Our proposed method has an average context length (as defined in Eq. (2)) of 1,344 for the RefinedWeb dataset, compared to 930 for the baseline (see Fig. 3c) and 1,064 when packing [17] is applied. This explains why the dataset decomposition mixture, even without any length-based curriculum (the first row in Table 2), outperforms Baseline- $\\mathbf{\\nabla}\\cdot8\\mathbf{k}$ -DM and Pack- $\\cdot8\\mathrm{k}+\\mathrm{DM}$ (second and third rows in Table 5). Here, DM refers to applying document masking during training to avoid cross-document attention. ", "page_idx": 8}, {"type": "text", "text": "Document masking improves the baseline on regular evaluations from 51.5 to 52.4 by preventing cross-document attention. However, Xiong et al. [64] demonstrate that including concatenated unrelated documents can still enhance long-context metrics compared to training solely with shorter sequences. Therefore, DM experiences a slight decline in long-context evaluations, dropping from 27.5 to 27.1. Baseline-8k multi-document QA performance is even slightly better than our proposed dataset decomposition mixture when used without length-based curriculum (the first row in Table 2). ", "page_idx": 8}, {"type": "text", "text": "In-context pre-training LMs (ICLM) [55] proposes document sorting based on content similarity. Although the benefits of ICLM with large-scale Common Crawl data (used in our experiments) are marginal in regular evaluation, we observe that ICLM results in slightly better multi-document QA performance than Baseline- $8\\mathbf{k}$ when 30 documents are in the context compared with Baseline-8k $(22.0\\%$ vs. $20.5\\%$ ). The average long-context metric boosts from 27.5 for Baseline- $\\cdot8\\mathbf{k}$ to 28.7 for ICLM. However, the similarity finding step proposed by ICLM is resource-intensive at scale\u00b6. ", "page_idx": 8}, {"type": "text", "text": "Finally, as shown in in Table 2 our proposed cyclic length-based curriculum, for example, Grow-P2 with 8 cycles, results in a significant improvement in the model\u2019s long-context capability. Our proposed method avoids cross-document attention to unrelated content, maintains coherent long sequences, and benefits from a length-based curriculum, effectively improving performance in both regular and long-context evaluations compared to all baselines. We further summarize long-context performance of different methods discussed above in Table 6. ", "page_idx": 8}, {"type": "table", "img_path": "r8M9SfYMDi/tmp/ffa0a4b1e405d2d4b4da280a4748284a4adf7177b6ab25be49cb6aad842dbaff.jpg", "table_caption": [], "table_footnote": ["Table 5: Comparison with baseline and state-of-the-art methods. All models are trained with the same hyperparameters, RoPE with $f_{b}=100k$ , and for 103B tokens. DM denotes training with document masking. DD uses the \"Grow-P2\" curriculum with 8 cycles. Dataset preparation cost is symbolic to compare methods and does not reflect the wall-clock time. "], "page_idx": 9}, {"type": "text", "text": "4 Related works ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Recent works have raised concerns regarding cross-document attention. For example, Llama3 [39], ICLM [55], and [17], which we discussed in Section 3.6. Similarly, [26] discuss challenges with the baseline concat-andchunk approach and propose an approximate bin-packing algorithm. ", "page_idx": 9}, {"type": "text", "text": "Related to our study on sequence length bias, [62] shows the importance of train-vs-test time distribution shift from a sequence length perspective on a string editing task. [6, 66, 25, 36] ", "page_idx": 9}, {"type": "table", "img_path": "r8M9SfYMDi/tmp/485ee56022d8601e8464cdbf6b33f9d55edb99ca739592f2dd66c35dd43f31d9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Table 6: Summary of long-context performance for different methods from Table 2 and Table 5. ", "page_idx": 9}, {"type": "text", "text": "highlight the challenge of generalizing to lengths beyond what the model has seen during training and discuss the importance of positional encoding. Several works [41, 67, 23, 64, 10, 47, 48, 53, 34] address enabling LLM inference with long context (see [45] for an overview). These approaches are orthogonal to our contribution and can be applied post-pretraining to adapt to longer lengths. GrowLength [24] proposes accelerating LLM pretraining by progressively growing context length using the baseline sequence formation method, but does not show results on LLMs. Similarly, increasing sequence length has been shown in BERT model training [42] to improve compute efficiency. ", "page_idx": 9}, {"type": "text", "text": "The idea of dynamic batching has been explored in other domains. In vision, methods like NaViT [16, 38] use images with variable resolutions (a similar concept to context length for LLMs). In seq-to-seq tasks (e.g., automatic speech recognition, text-to-speech, and neural machine translation), the inputs have different lengths. An efficient approach is to sort inputs by their length and form batches of inputs with similar lengths during training (after possible padding). Batchsize is dynamically adjusted inversely proportional to input lengths [20, 21]. Different from these works, in dataset decomposition, we do not simply put documents with similar lengths into the same bucket. Instead, we decompose each document into multiple subsequences and form multiple buckets. We form batches with different lengths during training by sampling from these buckets using a target mixture and curriculum. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion and limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we explore the shortcomings of a popular LLM pretraining approach, concat-andchunk, and introduce dataset decomposition, a method to decompose a dataset of text documents into buckets containing fixed sequence lengths. We show results of variable sequence training using DD with different mixtures, curricula, datasets, and models, demonstrating significant LLM pretraining speedup and a final model accuracy boost on a wide range of benchmarks. Furthermore, we provide analysis on sequence length bias and attention masking. We compare our proposed method with recent works that also address concat-and-chunk shortcomings in a unified experimental setup and show gains in data preparation cost, training time, and final model accuracy. ", "page_idx": 9}, {"type": "text", "text": "Limitations. The training speed gains compared to the baseline are significant only when the target sequence length is long enough. Otherwise, the attention cost is not a dominant fraction of training, and hence no significant training speedup is expected. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank Tatiana Likhomanenko, Jason Ramapuram, Alexander Toshev, Barry Theobald, and Fartash Faghri from Apple for their valuable feedback and suggestions. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Common crawl. https://commoncrawl.org.   \n[2] Llm foundry v0.7.0. https://github.com/mosaicml/llm-foundry.   \n[3] Jeopardy. https://huggingface.co/datasets/jeopardy. [Used custom curated version by LLM Foundry].   \n[4] Big-bench qa wikidata. https://github.com/google/BIG-bench/tree/main/ bigbench/benchmark_tasks/qa_wikidata. [Used through LLM Foundry].   \n[5] C. An, S. Gong, M. Zhong, M. Li, J. Zhang, L. Kong, and X. Qiu. L-eval: Instituting standardized evaluation for long context language models, 2023. [6] C. Anil, Y. Wu, A. Andreassen, A. Lewkowycz, V. Misra, V. Ramasesh, A. Slone, G. Gur-Ari, E. Dyer, and B. Neyshabur. Exploring length generalization in large language models. Advances in Neural Information Processing Systems, 35:38546\u201338556, 2022.   \n[7] Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pages 41\u201348, 2009.   \n[8] Y. Bisk, R. Zellers, J. Gao, Y. Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 7432\u20137439, 2020.   \n[9] S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Golding, H. He, C. Leahy, K. McDonell, J. Phang, et al. Gpt-neox-20b: An open-source autoregressive language model. arXiv preprint arXiv:2204.06745, 2022.   \n[10] S. Chen, S. Wong, L. Chen, and Y. Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023.   \n[11] Y.-A. Chung, H.-Y. Lee, and J. Glass. Supervised and unsupervised transfer learning for question answering. In M. Walker, H. Ji, and A. Stent, editors, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). Association for Computational Linguistics, 2018.   \n[12] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019.   \n[13] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.   \n[14] T. Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023. URL https://github.com/togethercomputer/RedPajama-Data.   \n[15] T. Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023.   \n[16] M. Dehghani, B. Mustafa, J. Djolonga, J. Heek, M. Minderer, M. Caron, A. Steiner, J. Puigcerver, R. Geirhos, I. M. Alabdulmohsin, et al. Patch n\u2019pack: Navit, a vision transformer for any aspect ratio and resolution. Advances in Neural Information Processing Systems, 36, 2024.   \n[17] H. Ding, Z. Wang, G. Paolini, V. Kumar, A. Deoras, D. Roth, and S. Soatto. Fewer truncations improve language modeling. arXiv preprint arXiv:2404.10830, 2024.   \n[18] J. L. Elman. Learning and development in neural networks: The importance of starting small. Cognition, 48(1):71\u201399, 1993.   \n[19] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, et al. The Pile: An 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.   \n[20] Z. Ge, L. Kaushik, M. Omote, and S. Kumar. Speed up training with variable length inputs by efficient batching strategies. In Interspeech, pages 156\u2013160, 2021.   \n[21] P. Gonzalez, T. S. Alstr\u00f8m, and T. May. On batching variable size inputs for training end-toend speech enhancement systems. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1\u20135. IEEE, 2023.   \n[22] S. Gururangan, M. Wortsman, S. Y. Gadre, A. Dave, M. Kilian, W. Shi, J. Mercat, G. Smyrnis, G. Ilharco, M. Jordan, R. Heckel, A. Dimakis, A. Farhadi, V. Shankar, and L. Schmidt. OpenLM: a minimal but performative language modeling (lm) repository, 2023. URL https://github.com/mlfoundations/open_lm/. GitHub repository.   \n[23] C. Han, Q. Wang, W. Xiong, Y. Chen, H. Ji, and S. Wang. Lm-infinite: Simple on-the-fly length generalization for large language models. arXiv preprint arXiv:2308.16137, 2023.   \n[24] H. Jin, X. Han, J. Yang, Z. Jiang, C.-Y. Chang, and X. Hu. Growlength: Accelerating llms pretraining by progressively growing training length. arXiv preprint arXiv:2310.00576, 2023.   \n[25] A. Kazemnejad, I. Padhi, K. Natesan Ramamurthy, P. Das, and S. Reddy. The impact of positional encoding on length generalization in transformers. Advances in Neural Information Processing Systems, 36, 2024.   \n[26] M. M. Krell, M. Kosec, S. P. Perez, and A. Fitzgibbon. Efficient sequence packing without cross-contamination: Accelerating large language models without impacting performance. arXiv preprint arXiv:2107.02027, 2021.   \n[27] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, et al. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453\u2013466, 2019.   \n[28] K. Lee, M.-W. Chang, and K. Toutanova. Latent retrieval for weakly supervised open domain question answering. arXiv preprint arXiv:1906.00300, 2019.   \n[29] B. Lefaudeux, F. Massa, D. Liskovich, W. Xiong, V. Caggiano, S. Naren, M. Xu, J. Hu, M. Tintore, S. Zhang, P. Labatut, D. Haziza, L. Wehrstedt, J. Reizenstein, and G. Sizov. xformers: A modular and hackable transformer modelling library. https://github.com/ facebookresearch/xformers, 2022.   \n[30] H. Levesque, E. Davis, and L. Morgenstern. The winograd schema challenge. In Thirteenth international conference on the principles of knowledge representation and reasoning, 2012.   \n[31] C. Li, M. Zhang, and Y. He. The stability-efficiency dilemma: Investigating sequence length warmup for training gpt models. Advances in Neural Information Processing Systems, 35: 26736\u201326750, 2022.   \n[32] J. Li, A. Fang, G. Smyrnis, M. Ivgi, M. Jordan, S. Gadre, H. Bansal, E. Guha, S. Keh, K. Arora, et al. Datacomp-lm: In search of the next generation of training sets for language models. arXiv preprint arXiv:2406.11794, 2024.   \n[33] X. Li, Z. Wang, and C. Xie. An inverse scaling law for clip training. Advances in Neural Information Processing Systems, 36, 2024.   \n[34] H. Liu, W. Yan, M. Zaharia, and P. Abbeel. World model on million-length video and language with blockwise ringattention. arXiv preprint arXiv:2402.08268, 2024.   \n[35] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and P. Liang. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157\u2013173, 2024.   \n[36] X. Liu, H. Yan, S. Zhang, C. An, X. Qiu, and D. Lin. Scaling laws of rope-based extrapolation. arXiv preprint arXiv:2310.05209, 2023.   \n[37] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.   \n[38] S. Mehta, F. Abdolhosseini, and M. Rastegari. Cvnets: High performance library for computer vision. In Proceedings of the 30th ACM International Conference on Multimedia, pages 7327\u20137330, 2022.   \n[39] Meta. Introducing meta llama 3: The most capable openly available llm to date, 2024. URL https://ai.meta.com/blog/meta-llama-3.   \n[40] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018.   \n[41] A. Mohtashami and M. Jaggi. Random-access infinite context length for transformers. Advances in Neural Information Processing Systems, 36, 2024.   \n[42] K. Nagatsuka, C. Broni-Bediako, and M. Atsumi. Pre-training a bert with curriculum learning by increasing block-size of input text. In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021), pages 989\u2013996, 2021.   \n[43] M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Grangier, and M. Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of NAACL-HLT 2019: Demonstrations, 2019.   \n[44] D. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda, and R. Fern\u00e1ndez. The lambada dataset: Word prediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031, 2016.   \n[45] S. Pawar, S. Tonmoy, S. Zaman, V. Jain, A. Chadha, and A. Das. The what, why, and how of context length extension techniques in large language models\u2013a detailed survey. arXiv preprint arXiv:2401.07872, 2024.   \n[46] G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli, H. Alobeidli, B. Pannier, E. Almazrouei, and J. Launay. The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023. URL https://arxiv.org/abs/2306.01116.   \n[47] B. Peng and J. Quesnelle. Ntk-aware scaled rope allows llama models to have extended $(8\\mathbf{k}+)$ context size without any fine-tuning and minimal perplexity degradation. 2023. URL https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ ntkaware_scaled_rope_allows_llama_models_to_have.   \n[48] B. Peng, J. Quesnelle, H. Fan, and E. Shippole. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023.   \n[49] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[50] P. Rajpurkar, R. Jia, and P. Liang. Know what you don\u2019t know: Unanswerable questions for squad. arXiv preprint arXiv:1806.03822, 2018.   \n[51] S. Reddy, D. Chen, and C. D. Manning. Coqa: A conversational question answering challenge. Transactions of the Association for Computational Linguistics, 7:249\u2013266, 2019.   \n[52] M. Roemmele, C. A. Bejan, and A. S. Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series, 2011.   \n[53] A. Ruoss, G. Del\u00e9tang, T. Genewein, J. Grau-Moya, R. Csord\u00e1s, M. Bennani, S. Legg, and J. Veness. Randomized positional encodings boost length generalization of transformers. arXiv preprint arXiv:2305.16843, 2023.   \n[54] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99\u2013106, 2021.   \n[55] W. Shi, S. Min, M. Lomeli, C. Zhou, M. Li, V. Lin, N. A. Smith, L. Zettlemoyer, S. Yih, and M. Lewis. In-context pretraining: Language modeling beyond document boundaries. arXiv preprint arXiv:2310.10638, 2023.   \n[56] L. N. Smith. Cyclical learning rates for training neural networks. In 2017 IEEE winter conference on applications of computer vision (WACV), pages 464\u2013472. IEEE, 2017.   \n[57] L. Soldaini, R. Kinney, A. Bhagia, D. Schwenk, D. Atkinson, R. Authur, B. Bogin, K. Chandu, J. Dumas, Y. Elazar, V. Hofmann, A. H. Jha, S. Kumar, L. Lucy, X. Lyu, N. Lambert, I. Magnusson, J. Morrison, N. Muennighoff, A. Naik, C. Nam, M. E. Peters, A. Ravichander, K. Richardson, Z. Shen, E. Strubell, N. Subramani, O. Tafjord, P. Walsh, L. Zettlemoyer, N. A. Smith, H. Hajishirzi, I. Beltagy, D. Groeneveld, J. Dodge, and K. Lo. Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research. arXiv preprint, 2024.   \n[58] J. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.   \n[59] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[60] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[61] B.-H. Tseng, S.-S. Shen, H.-Y. Lee, and L.-S. Lee. Towards machine comprehension of spoken content: Initial toef llistening comprehension test by machine, 2016.   \n[62] D. Vari\u0161 and O. Bojar. Sequence length is a domain: Length-based overfitting in transformer models. arXiv preprint arXiv:2109.07276, 2021.   \n[63] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[64] W. Xiong, J. Liu, I. Molybog, H. Zhang, P. Bhargava, R. Hou, L. Martin, R. Rungta, K. A. Sankararaman, B. Oguz, et al. Effective long-context scaling of foundation models. arXiv preprint arXiv:2309.16039, 2023.   \n[65] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.   \n[66] Y. Zhou, U. Alon, X. Chen, X. Wang, R. Agarwal, and D. Zhou. Transformers can achieve length generalization but not robustly. arXiv preprint arXiv:2402.09371, 2024.   \n[67] D. Zhu, N. Yang, L. Wang, Y. Song, W. Wu, F. Wei, and S. Li. Pose: Efficient context window extension of llms via positional skip-wise training. arXiv preprint arXiv:2309.10400, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Broader impacts ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "This work enables faster training of LLMs, which are among the most compute-intensive applications in the field. A positive societal/environmental impact of this work is training LLMs with a smaller carbon footprint. ", "page_idx": 14}, {"type": "text", "text": "Another potential societal advantage of this work is training LLMs with fewer hallucinations. While we did not directly measure this potential benefti, a concurrent work [17] shows such a benefti when cross-document attention is not allowed during LLM pretraining. ", "page_idx": 14}, {"type": "text", "text": "B Implementation details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Training details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Software and hardware details All experiments in this paper are conducted using the OpenLM|| repository, which is based on PyTorch. We use Fully Sharded Data Parallelism (FSDP) with Bfloat16 mixed precision for all experiments. We use the Xformers [29] implementation for attention. For hardware, we use one or more nodes of $8\\times$ NVIDIA H100 GPUs (Hopper architecture), each with 80GB memory, and 192 CPU cores with 2000GB of RAM. Nodes are connected through Elastic Fabric Adapter (EFA) for efficient inter-node communication hosted by AWS. ", "page_idx": 14}, {"type": "text", "text": "Model architecture details We provide details of all architectures used in the paper in Table 7 to Table 11. ", "page_idx": 14}, {"type": "table", "img_path": "r8M9SfYMDi/tmp/32b92a44a3ae7c9d40bdad0b652c670823bbfeb016d0a7b0bfac30d9e400cafa.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "r8M9SfYMDi/tmp/98e4317b6b73233688776c67ba901302271e1878aa0ce534525000513cee5e56.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "r8M9SfYMDi/tmp/78d76daabdd1621ecfd7889e8683ecd1bcd9e17425050b41560826e87cfc1e6f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "r8M9SfYMDi/tmp/e8dd77c9c25be57de9504010fd10ddd3c57486649a140a8e5e068082c590f1ff.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "r8M9SfYMDi/tmp/14d42d399e23adbf3aa5f3e1113c210b3a839df774c3ea7f2486028f2e9d57f0.jpg", "table_caption": ["Table 11: OpenLM-7B. "], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "Baseline hyper parameters We list our baseline hyperparameters in Table 12 and iterate over changes for each section next. Note that we did not explicitly optimize hyperparameters for any of the experiments, and we always use the same hyperparameters when using either the baseline method or ours. ", "page_idx": 14}, {"type": "table", "img_path": "r8M9SfYMDi/tmp/b9793f35423fba39358db2067aa0429e003db6413abae5a828e9947788604187.jpg", "table_caption": ["Table 12: Baseline hyper-parameters. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Implementation details of Section 3.2 experiments Experiments in this section are done using the same hyperparameters as in Table 12 for a total of $2^{34}$ tokens on the OpenLM-1B model. We trained each model twice with different random seeds and report the averaged results. For models in this section, we use RoPE with $f_{b}=10,000$ . In Table 12, we show that our results and conclusions in this section are not sensitive to hyperparameters, including the RoPE base frequency $f_{b}$ . ", "page_idx": 15}, {"type": "text", "text": "Implementation details of Section 3.3 and Section 3.4 experiments Experiments in this section are done with OpenLM-1B model, trained for total of $96\\times\\mathbf{\\dot{1}}0^{34}\\approx103\\mathbf{B}$ tokens. Hyper-parameters are the same as Table 12, except we used 20000 warmup steps for all models presented in this section. We use RoPE with $f_{b}\\,=\\,10,000$ for all models in Section 3.3 and $f_{b}\\,=\\,100,000$ for models in Section 3.4. ", "page_idx": 15}, {"type": "text", "text": "Implementation details of Section 3.5 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Dataset scaling: Experiments in this section are trained with the OpenLM-1B model, RoPE with $f_{b}=100,000$ , and the baseline setup as in Table 12 except for the following changes for different dataset sizes: ", "page_idx": 15}, {"type": "text", "text": "\u2022 total tokens $=2^{34}$ , warmup steps $=5$ , 000 \u2022 total tokens $=2^{35}$ , warmup steps $=5,000$ \u2022 total tokens $=2^{36}$ , warmup steps $=10,000$ \u2022 total tokens $=2^{37}$ , warmup steps $=20$ , 000 \u2022 total tokens $=2^{38}$ , warmup steps $=40,000$ ", "page_idx": 15}, {"type": "text", "text": "Model scaling: Experiments in this section are trained with the OpenLM-1B, OpenLM-3B, and OpenLM-7B models, $2^{37}\\approx137\\mathrm{B}$ total seen tokens, RoPE with $f_{b}=100,000$ , and the baseline setup as in Table 12 except for the following changes for different model sizes: ", "page_idx": 15}, {"type": "text", "text": "\u2022 OpenLM-1B, warmup steps $=20,000$ , max- $\\mathrm{lr}=3\\times10^{-3}$ , batchsize $b=2^{19}$ , with 32 H100 GPUs   \n\u2022 OpenLM-3B, warmup steps $=20,000$ , max- $\\mathrm{lr}=2\\times10^{-3}$ , batchsize $b=2^{20}$ , with 64 H100 GPUs   \n\u2022 OpenLM-7B, warmup steps $=20,000$ , max- $\\cdot\\mathrm{lr}=1\\times10^{-3}$ , batchsize $b=2^{22}$ , with 128 H100 GPUs ", "page_idx": 15}, {"type": "text", "text": "Alternative dataset: Experiments in this section are trained with the OpenLM-160M, OpenLM410M, and OpenLM-1B models, $2^{37}\\approx137\\mathrm{B}$ total seen tokens, RoPE with $f_{b}=100,000$ , and the baseline setup as in Table 12 except for the following changes for different model sizes: ", "page_idx": 15}, {"type": "text", "text": "\u2022 OpenLM-160M, warmup steps $=20$ , 000, max- $\\mathrm{lr}=5\\times10^{-3}$ , weight-decay $=0.033$ , with 16 H100 GPUs   \n\u2022 OpenLM-410M, warmup steps $=20,000$ , max- $\\mathrm{lr}=4\\times10^{-3}$ , weight-decay $=0.066$ , with 16 H100 GPUs   \n\u2022 OpenLM-1B, warmup steps $=20,000$ , max- $\\mathrm{lr}=3\\times10^{-3}$ , weight-decay $=0.1$ , with 32 H100 GPUs ", "page_idx": 15}, {"type": "text", "text": "For the DD experiments we used \u201cGrow-P2\u201d length curriculum which is visualized in Fig. 6. ", "page_idx": 15}, {"type": "image", "img_path": "r8M9SfYMDi/tmp/46305796d815af7e5be63fd67f629990409d368416881734baf387470bad38f5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 6: Comparison of length-based curriculum schedule with learning rate schedule. Sequence length varies between 256 and 8192 based on the Grow-P2 curriculum with 8 cycles. Note that the choice of bucket (and hence the sequence length) is random, with sampling probabilities determined by the curriculum. In the figure, we show the length of the sampled sequence at every 9 optimization steps. For the learning rate, we use a cosine learning rate with a warm-up for 4k steps. The job corresponds to training for a total of $2^{36}$ tokens, with $\\bar{2}^{20}$ tokens seen per optimization step. ", "page_idx": 16}, {"type": "text", "text": "Implementation details of Section 3.6 All experiments in this section are done with the OpenLM1B model, trained for a total of $96\\,\\times\\,10^{34}\\,\\approx\\,\\mathrm{\\dot{1}03B}$ tokens. Hyperparameters are the same as in Table 12, except we used 20,000 warmup steps for all models presented in this section. We use RoPE with $f_{b}=100,000$ . ", "page_idx": 16}, {"type": "text", "text": "B.2 Length based sampling and curriculum algorithm ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We present the details of our length-based sampling and curriculum in Algorithm 1 ", "page_idx": 16}, {"type": "text", "text": "Algorithm 1 Length based sampling and curriculum ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Require: ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "\u2022 $D_{i}$ : list of buckets such that $D_{i}$ includes sequences with length $2^{i}$ \u2022 $n_{i}$ : total number of tokens to be picked from each bucket (see Table 1) \u2022 $o_{i}$ : sampling odd for each bucket (see Table 2) \u2022 $c:$ number of cycles \u2022 $b$ : number of tokens per optimization step   \n$s_{i,j}\\gets$ random subset of $D_{i}$ with $n_{i}/c$ tokens $\\triangleright$ non-overlapping subsets of $D_{i}$   \nfor $j\\in[1,2,\\dots,c]$ do $\\triangleright$ loop over cycles while at least one $s_{i,j}$ is non-empty do $o d d s\\gets[o_{i}$ if $s_{i,j}$ is not empty else 0 for $i=1,2,3,\\ldots.$ $p r o b s\\gets o d d s/o d d s.s u m()$ randomly sample index $i$ with probability probs[i] ${\\mathrm{samp1e~}}b/2^{i}$ sequences from $s_{i,j}\\ \\mathrm{w/o}$ replacement for training end while ", "page_idx": 16}, {"type": "text", "text": "B.3 Evaluation details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Multi Document Question Answering (MDQA) We follow the open-book evaluation setup described in [35]. The document containing the answer is part of the context. The evaluation script provided by the official repository processes the model\u2019s response by using only the text before the first occurrence of a newline character as the answer. We noticed that sometimes the model responds with multiple newline characters before providing any valid text. In view of this behavior, we updated the evaluation script to look for the first non-empty text output from the model instead of the first string after newline character. Apart from this change in processing the model output, the rest of the evaluation follows the official implementation [35]. ", "page_idx": 16}, {"type": "text", "text": "TOEFL We follow the setup described in [5]. As described in Section 3, the dataset contains multiple-choice QA pairs for the 15 longest lectures in [61, 11]. To obtain a response from the model, we follow MMLU-style prompting, where the choices are appended to the original prompt individually and the mean log-probability is computed for each choice. The choice corresponding to the argmax of mean log-probability is then chosen as the model\u2019s response. After we obtain the response, the computation of accuracy follows the official implementation [5]. ", "page_idx": 16}, {"type": "image", "img_path": "r8M9SfYMDi/tmp/f4491dacc0695e9202472529c5d38a6ba2ba3d3f328c9e7760b98faf552e9c38.jpg", "img_caption": ["Figure 7: Top row: Average time (ms) for each node to train one batch on a $8\\!\\times\\!\\mathrm{Hl00}$ machine using FSDP. Bottom row: measured standard deviation for each setup. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "QuALITY We follow the setup described in [5]. The dataset contains long documents with each document containing multiple-choice QA pairs. Sometimes the context for a QA pair can be longer than 8192 tokens. To account for the longer sequence length, we increase the base frequency of RoPE positional encoding from $100\\mathrm{k}$ to $200\\mathrm{k}$ without any fine-tuning. To obtain a response from the model, we follow MMLU-style prompting, where the choices are appended to the original prompt individually and the mean log-probability is computed for each choice. The choice corresponding to the argmax of mean log-probability is then chosen as the model\u2019s response. After we obtain the model output, the rest of the evaluation follows the official implementation [5]. ", "page_idx": 17}, {"type": "text", "text": "C Additional results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.1 Additional results for training efficiency ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We enumerate model sizes (OpenLM-1B, OpenLM-3B, OpenLM-7B), the number of sequences in a batch (from 1 to 256), and sequence lengths $2^{6}$ to $2^{14}$ ) and measure the time to train 100 batches. We repeat this 5 times and report the average and standard deviation time per batch in Fig. 7. Notice that in the figure, each diagonal corresponds to a fixed $b$ (number of tokens seen per optimization step). ", "page_idx": 17}, {"type": "text", "text": "C.2 Additional results for sequence length bias experiments ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we show that changing hyperparameters does not alter our conclusions in Section 3.2. We observed that pretraining on a sequence length of 1024 results in optimal performance with respect to regular metrics, compared to both longer and shorter lengths. For example, the regular average metric is 48.0 when pretraining with a 1024 sequence length, but it is 47.0 when pretraining with a 2048 sequence length. We explore whether this gap can be filled by using potentially better hyperparameters when training with a 2048 sequence length. Results are shown in Table 13, demonstrating that the gap cannot be simply filled by choosing a different hyperparameter and is fundamental to the choice of pretraining sequence length. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "table", "img_path": "r8M9SfYMDi/tmp/943c70be635007ac9d6b96216d98ef43053f36871e74b17ff51617197bb6cf40.jpg", "table_caption": [], "table_footnote": ["Table 13: Sensitivity to hyperparameters for Section 3.2 experiments. All models are trained twice with different random seeds, and averaged results are reported. "], "page_idx": 18}, {"type": "text", "text": "C.3 Additional results for scaling experiments ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we show additional results for the experiments presented in Section 3.5. Table 14 shows results for dataset scaling, Table 15 for model scaling, and Table 16 for experiments on an alternative dataset. ", "page_idx": 18}, {"type": "table", "img_path": "r8M9SfYMDi/tmp/96dad40fa26614b8497021e97a3afdcf2f15e4439670a66a7fa9e61c100bb8d6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "r8M9SfYMDi/tmp/e6d600e817ff00569fccc446c6a9e19dddeebae7dce1948d3d2a852c507f2ecc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "r8M9SfYMDi/tmp/ea641de178672b31061752de5cf399641f8decbb35260b6614f22ec5bc710bc7.jpg", "table_caption": ["Table 14: Dataset scaling for OpenLM-1B. ", "Table 15: Model scaling for total of 137B tokens. "], "table_footnote": ["Table 16: Small model performance trained on an improved refined-web pipeline applied to Common Crawl. All models are trained for a total of $2^{37}$ tokens. "], "page_idx": 18}, {"type": "text", "text": "D Comparison to best-fit sequence packing ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Some recent works have employed a bin packing-based strategy [17] which aims to reduce document cross-attention by minimizing unnecessary document truncation. To achieve this, they implement a known approximation algorithm called best-fit decreasing, which packs document chunks into sequences as tightly as possible. To compare with our method, we created a new dataset based on our implementation of the best-fit decreasing algorithm and trained a new model using this dataset. We present our implementation of the best-fit decreasing algorithm, the dataset we created, and the model we trained for comparison. ", "page_idx": 18}, {"type": "text", "text": "Given a dataset $\\mathcal{D}$ , the input to the algorithm is a list of tokenized document chunks ${\\cal C}=$ $\\{c_{1},c_{2},\\ldots,c_{K}\\}$ such that $\\bar{\\bigcup_{i=1}^{K}c_{i}}=\\mathcal{D}$ , where each chunk is at most context size $n$ (e.g., 2048) in length. The output of the  algorithm is a list of bins $\\boldsymbol{B}=\\left\\{b_{1},b_{2},\\dots,b_{M}\\right\\}$ such that $c_{i}\\in b_{j}$ . As a pre-processing step, we first tokenize the documents and convert them into chunks. Truncation is applied during this step only when necessary. Next, we sort the chunks from largest to smallest and start from the first chunk to pack into bins of size $n$ . We track the remaining capacities for each bin while we iterate over the chunks. In each iteration, the algorithm finds the best bin that is both feasible and optimal for placing the chunk. Feasible bins are those that can accommodate the chunk, and optimal bins are those left with the minimum remaining capacity after placing the chunk. If such a bin is not found, we open a new bin and place the chunk inside. After all the chunks have been placed, we select the bins that have non-zero remaining capacities and flil them with pad tokens $<\\tt P A D>$ . ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "We process the RefinedWeb [46] dataset using the aforementioned procedure and create training sequences by concatenating all chunks in a bin. Figure 3 shows that while best-fti packing results in a higher average context length compared to the baseline concat-and-chunk, it is still much lower compared to our method dataset decomposition. Furthermore, the best-fit packing method does not prevent tokens from different documents from appearing in training sequences, whereas our method does. The presence of padding tokens in best-fit packed sequences also means that some context is wasted during each optimization step. ", "page_idx": 19}, {"type": "text", "text": "E Training stability with VSL and curriculum ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "[31] presents the stability-efficiency dilemma: efficient LLM pretraining with massive data parallelism results in a large batch size and requires a high learning rate. However, such a setup can result in training instability, leading to poor generalization. They observe a correlation between training instability and long sequences, especially at the early stages of training, suggesting that training on long sequences when the model is not well-trained can be a main source of training instability. ", "page_idx": 19}, {"type": "text", "text": "Here, we show that dataset decomposition alleviates this problem when used with a curriculum: starting training by sampling more from short sequence buckets. We empirically demonstrate this by training an OpenLM-1B model from scratch with a high learning rate $\\dot{\\mathrm{(=}}\\,10^{-2}\\$ ) and no gradient clipping, once with baseline-8k and once with DD using the \"Grow-P100\" curriculum. Training loss is shown in Fig. 8, demonstrating the stability of training with DD in comparison to the baseline. This suggests that our proposed method can also be beneficial for large-scale pretraining with large batches and high learning rates in terms of efficiency. ", "page_idx": 19}, {"type": "image", "img_path": "r8M9SfYMDi/tmp/776627f82a9845e4d9e557ac03e4214144940ac47beaef195eeba1e3f0cc28b5.jpg", "img_caption": ["Figure 8: We compare the training loss when training with Baseline-8k versus DD with the \"Grow$\\mathrm{{P100^{\\prime\\prime}}}$ curriculum. Both models are trained with identical hyperparameters, a high learning rate $(=10^{-2})$ , and no gradient clipping. It is evident that DD results in greater stability. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "F Average sequence length vs average context length ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We compute the mean of length (Fig. 3a) and context (Fig. 3c) distributions as follows. Assume a list of sequences with lengths $l_{1},l_{2},\\ldots,l_{N}$ , which are, for example, the chunk lengths in the concat-and-chunk approach or the sequence lengths in different buckets of the dataset decomposition approach. We define the average sequence length as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\mathrm{Average~sequence~length}}={\\frac{1}{N}}\\sum_{i}^{N}l_{i}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In auto-regressive training on a sequence with length $l$ , we apply $l$ losses for next-token prediction on each token in parallel. Hence, for a sequence with length $l$ , we see contexts with lengths equal to ", "page_idx": 19}, {"type": "text", "text": "$0,1,2,\\ldots,l-1$ . We define the average context length, which is different from the average sequence length, as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname{Average\\;context\\;length}=\\left(\\sum_{i=1}^{N}\\sum_{j=0}^{l_{i}-1}j\\right)/\\left(\\sum_{i=1}^{N}l_{i}\\right)=\\left(\\sum_{i=1}^{N}l_{i}(l_{i}-1)\\right)/\\left(2\\sum_{i=1}^{N}l_{i}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In Fig. 3a, Fig. 3c, and Table 1, we report the average sequence length and average context length for original documents, concat-and-chunk, and dataset decomposition with different mixtures. ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Method proposed in Section 2.1 and experiments provided in Section 3 support all claims in the abstract. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We provide limitations in Section 5. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Justification: Our work does not have any theorem/proof. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results. \u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. \u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Justification: We provide all experimental details in Appendix B. Further, we will be releasing the code.   \nGuidelines:   \n\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: Our results are based on OpenLM repository (https://github.com/ mlfoundations/open_lm) and RefinedWeb [46] data, both publicly available. We will be releasing our (small) changes to the repository after acceptance. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. \u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not ", "page_idx": 22}, {"type": "text", "text": "including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We provide all implementation details in Appendix B. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate   \ninformation about the statistical significance of the experiments?   \nAnswer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Training LLM at scale is computationally expensive. We repeat all experiments at 17B total tokens scale twice (with different random seeds) and report mean and variance in Section 3.2. For efficiency benchmarks we repeat 5 times, report standard deviation in Fig. 7. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We provide software/hardware details in Appendix B. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: All experiments fully respect NeurIPS code of ethics. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] Justification: We discuss broader impacts of this work in Appendix A ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 24}, {"type": "text", "text": "Justification: We do not believe the models/code related to this work poses any risk that   \nrequires safeguarding.   \nGuidelines: \u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We do our best to appropriately cite/acknowledge all external code/data used in this work.   \nGuidelines:   \n\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?   \nAnswer: [NA]   \nJustification: Aside from code, this work does not intend to release any asset. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 25}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?   \nAnswer: [NA] Justification: This work does not include any crowdsourcing or research with Human subjects.   \nGuidelines:   \n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "uidelines:   \n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]