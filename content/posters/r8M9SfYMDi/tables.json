[{"figure_path": "r8M9SfYMDi/tables/tables_6_1.jpg", "caption": "Table 1: Effect of pretraining dataset mixture on model performance. Each row corresponds to a model trained on a specific mixture of dataset decomposition buckets. All models are OpenLM-1B, have seen a total of 96 \u00d7 230 tokens, use RoPE with a base frequency of 10k, and are trained with the same hyperparameters. The definition of average context length is given in Appendix F.", "description": "This table shows the impact of different mixtures of dataset decomposition buckets on the performance of the OpenLM-1B model. Each row represents a model trained on a specific mixture, with a total of 96 \u00d7 2<sup>20</sup> tokens.  The model uses Rotary Positional Embedding (RoPE) with a base frequency of 10,000 and consistent hyperparameters across all rows. The average sequence and context lengths are reported, along with performance metrics on common benchmarks (CSR, LU, RC, WK, Avg.) and long-context tasks (MDQA 10, 20, 30, Avg.).", "section": "3.3 Data mixture"}, {"figure_path": "r8M9SfYMDi/tables/tables_7_1.jpg", "caption": "Table 2: Effect of length-based curriculum. All models are OpenLM-1B and have seen a total of 96 \u00d7 230 tokens, with exactly 234 tokens from each Di for i = 8, . . ., 13. We use RoPE with a base frequency of 100k and the same default hyperparameters.", "description": "This table presents the results of experiments using different length-based curricula for training the OpenLM-1B model.  Each row represents a different curriculum, characterized by the sampling odds assigned to each bucket (D8-D13), representing sequence lengths of 2<sup>8</sup> to 2<sup>13</sup> tokens.  The \"Uniform\" row serves as a baseline, while other rows show various strategies to increase the proportion of shorter sequences at the beginning of training and gradually increase the longer ones. The table shows the performance metrics across different evaluation benchmarks (CSR, LU, RC, WK) and multi-document question answering (MDQA) for each curriculum, with both short and long context performance.", "section": "3.4 Length-based curriculum"}, {"figure_path": "r8M9SfYMDi/tables/tables_8_1.jpg", "caption": "Table 3: Comparing baseline training with DD on an alternative pretraining dataset and model sizes.", "description": "This table compares the performance of baseline training with 8k context length and dataset decomposition (DD) on different model sizes (160M, 410M, and 1B parameters) using an alternative dataset.  The number of GPUs used, training time in hours, regular average accuracy, and MDQA average accuracy are reported for each model and method.  The \u0394 column shows the percentage change in training time for DD compared to the baseline.  The table highlights the efficiency gains of DD across different model scales. ", "section": "3 Experiments and analysis"}, {"figure_path": "r8M9SfYMDi/tables/tables_8_2.jpg", "caption": "Table 4: Effect of RoPE base frequency, fb, in pretraining.", "description": "This table presents the results of experiments evaluating the impact of the RoPE (Rotary Positional Embedding) base frequency (fb) on the model's performance. Two different base frequencies (10k and 100k) were tested for both the baseline method (Baseline-8k) and the proposed dataset decomposition method (DD\u2265256).  The table shows that increasing the base frequency from 10k to 100k leads to a significant improvement in the MDQA (Multi-Document Question Answering) average, indicating that a larger base frequency is beneficial for long-context tasks.", "section": "3 Experiments and analysis"}, {"figure_path": "r8M9SfYMDi/tables/tables_9_1.jpg", "caption": "Table 5: Comparison with baseline and state-of-the-art methods. All models are trained with the same hyperparameters, RoPE with fb = 100k, and for 103B tokens. DM denotes training with document masking. DD uses the \"Grow-P2\" curriculum with 8 cycles. Dataset preparation cost is symbolic to compare methods and does not reflect the wall-clock time.", "description": "This table compares the performance of the proposed dataset decomposition method (DD) with several baseline and state-of-the-art methods for handling various document lengths in LLM pretraining.  The performance metrics include standard language understanding benchmarks (CSR, LU, RC, WK) and long context benchmarks (MDQA, TOEFL, QUALITY).  The table shows that DD outperforms the baselines across all metrics, achieving significant improvements in long-context tasks while maintaining comparable or better performance on regular tasks. The data preparation cost is denoted symbolically to indicate the relative effort required for each approach, rather than absolute resource use.", "section": "3.6 Comparison with state-of-the-art"}, {"figure_path": "r8M9SfYMDi/tables/tables_9_2.jpg", "caption": "Table 6: Summary of long-context performance for different methods from Table 2 and Table 5.", "description": "This table summarizes the performance of different methods on long-context tasks, specifically MDQA-30.  It compares the baseline approach with document masking, best-fit packing with document masking, dataset decomposition with uniform sampling, baseline with longer context length, ICLM, and dataset decomposition with Grow-P2 curriculum. The table highlights the impact of different strategies (document masking, context length, curriculum) on long-context performance, showing that dataset decomposition with Grow-P2 curriculum significantly improves the results.", "section": "Comparison with state-of-the-art"}, {"figure_path": "r8M9SfYMDi/tables/tables_14_1.jpg", "caption": "Table 1: Effect of pretraining dataset mixture on model performance. Each row corresponds to a model trained on a specific mixture of dataset decomposition buckets. All models are OpenLM-1B, have seen a total of 96 \u00d7 230 tokens, use RoPE with a base frequency of 10k, and are trained with the same hyperparameters. The definition of average context length is given in Appendix F.", "description": "This table presents the results of experiments evaluating the effect of different mixtures of dataset decomposition buckets on the performance of an OpenLM-1B model.  Each row represents a model trained on a specific combination of buckets, all using the same hyperparameters (except for the mixture of buckets). The table shows the performance across various metrics including  regular evaluation average, commonsense reasoning (CSR), language understanding (LU), reading comprehension (RC), world knowledge (WK), and multi-document question answering (MDQA) with varying numbers of documents.  The average sequence length and average context length used for each model are also reported.  Appendix F provides further detail on the calculation of average context length.", "section": "3.3 Data mixture"}, {"figure_path": "r8M9SfYMDi/tables/tables_14_2.jpg", "caption": "Table 1: Effect of pretraining dataset mixture on model performance. Each row corresponds to a model trained on a specific mixture of dataset decomposition buckets. All models are OpenLM-1B, have seen a total of 96 \u00d7 230 tokens, use RoPE with a base frequency of 10k, and are trained with the same hyperparameters. The definition of average context length is given in Appendix F.", "description": "This table presents the results of experiments evaluating the effect of different mixtures of dataset decomposition buckets on the performance of an OpenLM-1B model.  The model was trained on 96 x 230 tokens total, using Rotary Positional Embedding (RoPE) with a base frequency of 10,000, and consistent hyperparameters. Each row shows a different mixture, indicating which buckets (with sequences of length 2<sup>i</sup>) were used in the training.  The results are evaluated across various metrics, including several standard language understanding benchmarks and three long-context question answering benchmarks (MDQA-10, MDQA-20, MDQA-30). The table shows the performance impact of varying the distribution of sequence lengths used in training.", "section": "3.3 Data mixture"}, {"figure_path": "r8M9SfYMDi/tables/tables_14_3.jpg", "caption": "Table 1: Effect of pretraining dataset mixture on model performance. Each row corresponds to a model trained on a specific mixture of dataset decomposition buckets. All models are OpenLM-1B, have seen a total of 96 \u00d7 230 tokens, use RoPE with a base frequency of 10k, and are trained with the same hyperparameters. The definition of average context length is given in Appendix F.", "description": "This table presents the results of experiments conducted to evaluate the impact of different mixtures of dataset decomposition buckets on the performance of an OpenLM-1B model.  The model was trained on a total of 96*230 tokens using Rotary Positional Embeddings (RoPE) with a base frequency of 10k and consistent hyperparameters across all experiments. The table highlights how varying the proportion of sequences from different length buckets (representing different document lengths) affects overall performance metrics including common sense reasoning, language understanding, reading comprehension, and world knowledge, as well as long-context performance measured by the MDQA metric.", "section": "3.3 Data mixture"}, {"figure_path": "r8M9SfYMDi/tables/tables_14_4.jpg", "caption": "Table 1: Effect of pretraining dataset mixture on model performance. Each row corresponds to a model trained on a specific mixture of dataset decomposition buckets. All models are OpenLM-1B, have seen a total of 96 \u00d7 230 tokens, use RoPE with a base frequency of 10k, and are trained with the same hyperparameters. The definition of average context length is given in Appendix F.", "description": "This table presents the results of experiments evaluating the impact of different mixtures of dataset decomposition buckets on model performance.  Each row represents a model trained on a specific mixture, all using the OpenLM-1B architecture, the same hyperparameters, and a total of 96 x 230 tokens.  The RoPE positional embedding method was used with a base frequency of 10,000. The table shows various performance metrics across different benchmarks for each mixture, highlighting the effect of the specific sequence length distribution in the training dataset.", "section": "3.3 Data mixture"}, {"figure_path": "r8M9SfYMDi/tables/tables_14_5.jpg", "caption": "Table 1: Effect of pretraining dataset mixture on model performance. Each row corresponds to a model trained on a specific mixture of dataset decomposition buckets. All models are OpenLM-1B, have seen a total of 96 \u00d7 230 tokens, use RoPE with a base frequency of 10k, and are trained with the same hyperparameters. The definition of average context length is given in Appendix F.", "description": "This table presents the results of experiments evaluating the impact of different mixtures of dataset decomposition buckets on the performance of an OpenLM-1B model.  Each row represents a model trained on a specific mixture, varying the number of sequences from each bucket. The total number of tokens seen by each model is constant (96 \u00d7 230).  All models used Rotary Positional Embeddings (RoPE) with a base frequency of 10,000 and shared hyperparameters. The average context length, a key metric defined in Appendix F, is also reported, which influences model performance.", "section": "3.3 Data mixture"}, {"figure_path": "r8M9SfYMDi/tables/tables_15_1.jpg", "caption": "Table 1: Effect of pretraining dataset mixture on model performance. Each row corresponds to a model trained on a specific mixture of dataset decomposition buckets. All models are OpenLM-1B, have seen a total of 96 \u00d7 230 tokens, use RoPE with a base frequency of 10k, and are trained with the same hyperparameters. The definition of average context length is given in Appendix F.", "description": "This table presents the results of experiments conducted to evaluate the impact of different mixtures of dataset decomposition buckets on the performance of the OpenLM-1B language model.  Each row shows a different mixture, indicating the number of tokens from buckets of various sequence lengths (D6 to D13). The results are presented in terms of average performance metrics across several benchmarks and the average context length, providing insights into the relationship between training dataset composition and model performance. The hyperparameters, total number of tokens, and RoPE settings are kept constant to isolate the effect of dataset composition.", "section": "3.3 Data mixture"}, {"figure_path": "r8M9SfYMDi/tables/tables_18_1.jpg", "caption": "Table 13: Sensitivity to hyperparameters for Section 3.2 experiments. All models are trained twice with different random seeds, and averaged results are reported.", "description": "This table demonstrates the robustness of the findings in Section 3.2 to changes in hyperparameters.  It shows that the choice of pretraining sequence length is fundamental, as the performance difference between different lengths persists even when hyperparameters like maximum learning rate and RoPE base frequency are adjusted.", "section": "3.2 Sequence length bias"}, {"figure_path": "r8M9SfYMDi/tables/tables_18_2.jpg", "caption": "Table 1: Effect of pretraining dataset mixture on model performance. Each row corresponds to a model trained on a specific mixture of dataset decomposition buckets. All models are OpenLM-1B, have seen a total of 96 \u00d7 230 tokens, use RoPE with a base frequency of 10k, and are trained with the same hyperparameters. The definition of average context length is given in Appendix F.", "description": "This table presents the results of experiments evaluating the effect of different mixtures of dataset decomposition buckets on the performance of an OpenLM-1B model.  Each row represents a model trained on a specific mixture, with the total number of tokens and hyperparameters remaining constant across all experiments. The table shows that different mixtures of sequence lengths impact the performance on various benchmarks.", "section": "3.3 Data mixture"}, {"figure_path": "r8M9SfYMDi/tables/tables_18_3.jpg", "caption": "Table 3: Comparing baseline training with DD on an alternative pretraining dataset and model sizes.", "description": "This table compares the performance of baseline training with 8k context length and dataset decomposition (DD) on OpenLM models of different sizes (160M, 410M, and 1B parameters).  It shows the regular average accuracy and MDQA average accuracy for each model and method, highlighting the gains achieved by using DD in terms of accuracy and training time reduction.  The experiment uses an alternative pretraining dataset.", "section": "3 Experiments and analysis"}, {"figure_path": "r8M9SfYMDi/tables/tables_18_4.jpg", "caption": "Table 1: Effect of pretraining dataset mixture on model performance. Each row corresponds to a model trained on a specific mixture of dataset decomposition buckets. All models are OpenLM-1B, have seen a total of 96 \u00d7 230 tokens, use RoPE with a base frequency of 10k, and are trained with the same hyperparameters. The definition of average context length is given in Appendix F.", "description": "This table presents the results of experiments evaluating the effect of different mixtures of dataset decomposition buckets on the performance of an OpenLM-1B model.  The model was trained on a total of 96 x 230 tokens, using Rotary Positional Embeddings (RoPE) with a base frequency of 10,000, and consistent hyperparameters. Each row represents a different mixture of buckets, demonstrating how varied sequence length distributions influence the model's performance on various benchmarks.", "section": "3.3 Data mixture"}]