[{"figure_path": "jsgYYXaSiS/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of our DPE with zero-shot CLIP [45], TPT [53], and TDA [26]. We denote CLIP's parallel textual and visual encoders as Et and Ev, respectively. While previous methods solely adapt the CLIP model from a single modality, we design our DPE to evolve prototypes from both textual and visual modalities to progressively capture more accurate multi-modal representations for target classes during test time.", "description": "This figure compares four different methods for adapting vision-language models (VLMs) during test time.  Zero-shot CLIP uses hand-crafted prompts, while TPT adapts prompts. TDA uses a dynamic adapter that incorporates high-confidence samples, and the proposed DPE method evolves prototypes from both textual and visual modalities to progressively improve multi-modal representations for target classes.", "section": "1 Introduction"}, {"figure_path": "jsgYYXaSiS/figures/figures_3_1.jpg", "caption": "Figure 2: An overview of our DPE method. We introduce prototypes from both textual and visual modalities and enable prototype-based inference with CLIP. For each test sample, we optimize both prototypes using learnable residual parameters with alignment loss Lalign and self-entropy loss Laug. These prototypes are also progressively evolved over time to capture more accurate and discriminative multi-modal representations for target classes.", "description": "This figure illustrates the Dual Prototype Evolving (DPE) method.  It shows how textual and visual prototypes are initialized, updated iteratively using high-confidence samples and learnable residuals, and used for prototype-based inference with CLIP.  The alignment loss (Lalign) and self-entropy loss (Laug) are also shown, highlighting how the method ensures consistent multi-modal representations and minimizes self-entropy during the optimization process. Separate diagrams illustrate the evolution of both textual and visual prototypes over time.", "section": "3 Method"}, {"figure_path": "jsgYYXaSiS/figures/figures_4_1.jpg", "caption": "Figure 3: t-SNE [60] visualizations of the stored image features in the priority queues. With more samples getting in, the selected image features from each class become more clustered, leading to more representative visual prototypes.", "description": "This figure shows t-SNE visualizations of image features stored in priority queues for different numbers of samples.  The left panel shows the state after processing 1500 samples, and the right after 15000.  The goal is to show that as more samples are added, the features for each class cluster closer together in the embedding space, resulting in better prototypes for classification. Different colors represent different classes. Gray points represent features that were not selected for the prototypes.", "section": "3.2 Dual Prototype Evolving"}, {"figure_path": "jsgYYXaSiS/figures/figures_9_1.jpg", "caption": "Figure 2: An overview of our DPE method. We introduce prototypes from both textual and visual modalities and enable prototype-based inference with CLIP. For each test sample, we optimize both prototypes using learnable residual parameters with alignment loss Lalign and self-entropy loss Laug. These prototypes are also progressively evolved over time to capture more accurate and discriminative multi-modal representations for target classes.", "description": "This figure illustrates the Dual Prototype Evolving (DPE) method.  It shows how textual and visual prototypes are initialized, updated iteratively using high-confidence samples, and refined using learnable residuals. The process aims to create accurate multi-modal representations for better classification.", "section": "3 Method"}]