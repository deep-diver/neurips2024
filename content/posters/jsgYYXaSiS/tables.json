[{"figure_path": "jsgYYXaSiS/tables/tables_6_1.jpg", "caption": "Table 1: Performance comparisons on robustness to natural distribution shifts. We present top-1 accuracy (%) results for all evaluated methods employing both ResNet-50 and ViT-B/16 visual backbones of CLIP. The best results are highlighted in bold.", "description": "This table presents a comparison of the performance of different methods on robustness to natural distribution shifts.  The methods are evaluated using two different visual backbones from CLIP (ResNet-50 and ViT-B/16), and the results are reported as top-1 accuracy.  The table includes various baselines (Ensemble, CoOp, TPT, DiffTPT, TDA, TPS, DMN-ZS) and the proposed DPE method.  The best performance for each dataset is highlighted in bold.", "section": "4 Experiments"}, {"figure_path": "jsgYYXaSiS/tables/tables_7_1.jpg", "caption": "Table 1: Performance comparisons on robustness to natural distribution shifts. We present top-1 accuracy (%) results for all evaluated methods employing both ResNet-50 and ViT-B/16 visual backbones of CLIP. The best results are highlighted in bold.", "description": "This table compares the performance of different methods on robustness to natural distribution shifts using two different visual backbones (ResNet-50 and ViT-B/16) for the CLIP model.  It shows top-1 accuracy results across several ImageNet variations (ImageNet-A, ImageNet-V2, ImageNet-R, ImageNet-S) designed to test out-of-distribution generalization.  The average accuracy across these variations and the average out-of-distribution accuracy are also given. The best performance for each backbone is highlighted in bold.", "section": "4 Experiments"}, {"figure_path": "jsgYYXaSiS/tables/tables_8_1.jpg", "caption": "Table 1: Performance comparisons on robustness to natural distribution shifts. We present top-1 accuracy (%) results for all evaluated methods employing both ResNet-50 and ViT-B/16 visual backbones of CLIP. The best results are highlighted in bold.", "description": "This table presents a comparison of the performance of several methods on the task of adapting vision-language models (VLMs) to handle out-of-distribution data.  The methods are evaluated on several benchmark datasets (ImageNet, ImageNet-A, ImageNet-V2, ImageNet-R, ImageNet-S) that represent different types of distribution shifts. The accuracy is reported using two different visual backbones for the CLIP model (ResNet-50 and ViT-B/16).", "section": "4 Experiments"}, {"figure_path": "jsgYYXaSiS/tables/tables_8_2.jpg", "caption": "Table 1: Performance comparisons on robustness to natural distribution shifts. We present top-1 accuracy (%) results for all evaluated methods employing both ResNet-50 and ViT-B/16 visual backbones of CLIP. The best results are highlighted in bold.", "description": "This table presents a comparison of the proposed Dual Prototype Evolving (DPE) method with other state-of-the-art methods on robustness to natural distribution shifts.  The performance is evaluated using two different visual backbones from CLIP (ResNet-50 and ViT-B/16), and the top-1 accuracy is reported for each method across five different ImageNet datasets (ImageNet, ImageNet-A, ImageNet-V2, ImageNet-R, and ImageNet-S) and an out-of-distribution (OOD) average. The best-performing method for each dataset and average is highlighted in bold.", "section": "4 Experiments"}, {"figure_path": "jsgYYXaSiS/tables/tables_9_1.jpg", "caption": "Table 5: Ablation studies on different update steps in prototype residual learning. We vary the number of update steps from 1 to 5 and report the achieved performance on ImageNet [7].", "description": "This table presents an ablation study on the impact of varying the number of update steps in the prototype residual learning component of the proposed DPE method. The experiment is conducted on the ImageNet dataset.  The table shows that a single update step provides the best performance, with marginal improvements seen when increasing to two steps but a decrease in performance with more than two steps. ", "section": "4.3 Ablation Studies"}, {"figure_path": "jsgYYXaSiS/tables/tables_16_1.jpg", "caption": "Table 1: Performance comparisons on robustness to natural distribution shifts. We present top-1 accuracy (%) results for all evaluated methods employing both ResNet-50 and ViT-B/16 visual backbones of CLIP. The best results are highlighted in bold.", "description": "This table presents a comparison of the performance of different methods on robustness to natural distribution shifts.  It shows top-1 accuracy results for various methods using two different visual backbones (ResNet-50 and ViT-B/16) in CLIP.  The methods compared include baseline CLIP, ensemble methods, prompt learning methods, adapter methods, and the proposed DPE method.  The results are presented for different ImageNet datasets, including ImageNet-A, ImageNet-V2, ImageNet-R, and ImageNet-S, which represent various out-of-distribution scenarios. The best-performing method for each scenario is highlighted in bold.", "section": "4 Experiments"}, {"figure_path": "jsgYYXaSiS/tables/tables_17_1.jpg", "caption": "Table 1: Performance comparisons on robustness to natural distribution shifts. We present top-1 accuracy (%) results for all evaluated methods employing both ResNet-50 and ViT-B/16 visual backbones of CLIP. The best results are highlighted in bold.", "description": "This table compares the performance of different methods on ImageNet and its out-of-distribution variants (ImageNet-A, ImageNet-V2, ImageNet-R, ImageNet-S).  It shows the top-1 accuracy for each method using two different backbones (ResNet-50 and ViT-B/16) of the CLIP model.  The table highlights the best-performing method for each dataset and backbone combination.  It helps to understand the robustness of different approaches to distribution shifts.", "section": "4.2 Results and Discussions"}, {"figure_path": "jsgYYXaSiS/tables/tables_17_2.jpg", "caption": "Table A2: Performance comparisons on robustness to natural distribution shifts. We present top-1 accuracy (%) results for all evaluated methods employing larger-scale ViT-L/14 visual backbones of OpenCLIP [4]. The best results are highlighted in bold.", "description": "This table presents a comparison of the performance of different methods on robustness to natural distribution shifts using the larger-scale ViT-L/14 visual backbone of OpenCLIP.  The methods compared include TDA and the authors' proposed DPE method. The table shows the top-1 accuracy for each method across five ImageNet datasets (ImageNet, ImageNet-A, ImageNet-V2, ImageNet-R, ImageNet-S), along with the average accuracy across these datasets and a separate average OOD (out-of-distribution) accuracy.  The best performance for each dataset is highlighted in bold.", "section": "A.2 Performance Comparisons on Larger-Scale VLMs"}, {"figure_path": "jsgYYXaSiS/tables/tables_18_1.jpg", "caption": "Table A1: Performance comparisons on robustness to natural distribution shifts. We present top-1 accuracy (%) results for all evaluated methods employing both ResNet-50 and ViT-B/16 visual backbones of CLIP. Additionally, we assess the performance using prompts learned by CoOp [84] with 16-shot training data per class on ImageNet [7]. The best results are highlighted in bold.", "description": "This table presents a comparison of different methods on their robustness to natural distribution shifts.  The top-1 accuracy is shown for ResNet-50 and ViT-B/16 backbones of CLIP.  Results are also included for methods using prompts learned by CoOp (a train-time adaptation method) to show the impact of training data.  The best-performing method for each setting is highlighted in bold.", "section": "A Additional Experimental Results"}, {"figure_path": "jsgYYXaSiS/tables/tables_18_2.jpg", "caption": "Table A5: Effectiveness of different algorithm components. VPE, TPE, and PRL refer to visual prototype evolution, textual prototype evolution, and prototype residual learning, respectively.", "description": "This table presents an ablation study evaluating the individual contribution of each component of the Dual Prototype Evolving (DPE) method. It shows the top-1 accuracy on ImageNet when using different combinations of visual prototype evolution (VPE), textual prototype evolution (TPE), and prototype residual learning (PRL). The results demonstrate the significant impact of VPE, while also highlighting the positive contributions of TPE and PRL.", "section": "A.5 Ablation Study on Two Loss Terms"}, {"figure_path": "jsgYYXaSiS/tables/tables_18_3.jpg", "caption": "Table A6: Effects of self-entropy loss and alignment loss.. Specifically, we apply the self-entropy loss (Laug) and alignment loss (Lalign) individually and in combination, and report the accuracy on ImageNet using the ResNet-50 backbone.", "description": "This table shows the ablation study of the two loss functions used in the Dual Prototype Evolving (DPE) method.  It compares the ImageNet top-1 accuracy using only the self-entropy loss, only the alignment loss, and both losses combined. The results demonstrate the contribution of each loss to the overall performance.", "section": "A.6 Ablation Study on Two Loss Terms"}, {"figure_path": "jsgYYXaSiS/tables/tables_19_1.jpg", "caption": "Table 1: Performance comparisons on robustness to natural distribution shifts. We present top-1 accuracy (%) results for all evaluated methods employing both ResNet-50 and ViT-B/16 visual backbones of CLIP. The best results are highlighted in bold.", "description": "This table presents a comparison of the performance of different methods on the task of robustness to natural distribution shifts.  The methods are evaluated using two different backbones (ResNet-50 and ViT-B/16) from the CLIP model. The results, shown as top-1 accuracy percentages, demonstrate how well each method generalizes to out-of-distribution data.  The best-performing method for each dataset is highlighted in bold.", "section": "4 Experiments"}, {"figure_path": "jsgYYXaSiS/tables/tables_19_2.jpg", "caption": "Table 1: Performance comparisons on robustness to natural distribution shifts. We present top-1 accuracy (%) results for all evaluated methods employing both ResNet-50 and ViT-B/16 visual backbones of CLIP. The best results are highlighted in bold.", "description": "This table compares the performance of the proposed Dual Prototype Evolving (DPE) method with other state-of-the-art methods on robustness to natural distribution shifts.  The comparison uses two different backbones for the CLIP model (ResNet-50 and ViT-B/16), and evaluates performance on several ImageNet variants (ImageNet-A, ImageNet-V2, ImageNet-R, and ImageNet-S), as well as an out-of-distribution (OOD) average. The best performing method for each setting is highlighted in bold.  The table demonstrates DPE's improved accuracy compared to other methods.", "section": "4 Experiments"}]