[{"figure_path": "sbsaRj475E/figures/figures_3_1.jpg", "caption": "Figure 1: Overview of the SuperNet and SubNet. Standard diffusion models execute the full inference path step by step. In our framework, we propose a SuperNet based on the original flow and integrate backup connections to facilitate block removal. This allows the partial inference SubNet to efficiently eliminate redundant computational costs.", "description": "This figure illustrates the core idea of the DiP-GO method.  It shows how a standard diffusion model (a) is transformed into a SuperNet (b) by adding backup connections between blocks in adjacent timesteps. This SuperNet allows for the identification of an optimal SubNet (c) by selectively removing less important blocks during inference, leading to significant computational savings. The backup connections ensure that the inference process can continue even after blocks are removed.", "section": "3.2 SuperNet and SubNet of Diffusion Model"}, {"figure_path": "sbsaRj475E/figures/figures_4_1.jpg", "caption": "Figure 2: Overview of our diffusion pruner. a) DiP-GO employs a pruner network to learn the importance scores of blocks in the diffusion sampling process. It takes N \u00d7 T queries as input and passes them through stacked self-attention (SA) and fully connected (FC) layers to capture the structural information in existing diffusion models. The network predicts the partial inference paths based on the N \u00d7 T importance scores and is optimized by consistent and sparse loss. b) Once trained, the pruner network is discarded. We can infer the optimal partial inference path with expected computational costs via post-processing based on the predicted importance scores.", "description": "This figure illustrates the architecture and workflow of the proposed diffusion pruner (DiP-GO). Part (a) shows the training process, where a pruner network learns to assign importance scores to blocks in the diffusion model.  The network uses self-attention and fully connected layers to process input queries, predicting which blocks are essential.  The training is guided by a loss function that balances consistency (maintaining generation quality) and sparsity (reducing computations). Part (b) depicts the post-training inference process. The trained pruner's importance scores are used in a binary search to determine the optimal subset of blocks to execute, thus creating a pruned model for faster inference.", "section": "3.3 Our DiP-GO Approach"}, {"figure_path": "sbsaRj475E/figures/figures_8_1.jpg", "caption": "Figure 3: Visualization of generated images. It shows evolving patterns as pruning ratios increase. Despite these changes, main objects in the images remain consistent with the textual conditions.", "description": "This figure visualizes the generated images at different pruning ratios (0.3 to 0.8).  It shows how the image generation time decreases as the pruning ratio increases (from 5.01s to 1.23s).  While some minor image details change with higher pruning ratios, the main subjects in the images remain consistent with the input text prompts, demonstrating that the model maintains good performance even with significant pruning.", "section": "Qualitative Analysis of Increased Prune Ratio"}, {"figure_path": "sbsaRj475E/figures/figures_13_1.jpg", "caption": "Figure 2: Overview of our diffusion pruner. a) DiP-GO employs a pruner network to learn the importance scores of blocks in the diffusion sampling process. It takes N \u00d7 T queries as input and passes them through stacked self-attention (SA) and fully connected (FC) layers to capture the structural information in existing diffusion models. The network predicts the partial inference paths based on the N \u00d7 T importance scores and is optimized by consistent and sparse loss. b) Once trained, the pruner network is discarded. We can infer the optimal partial inference path with expected computational costs via post-processing based on the predicted importance scores.", "description": "This figure illustrates the DiP-GO diffusion pruner. Part (a) shows the training process where a pruner network learns to assign importance scores to blocks in a diffusion model.  The network uses queries and self-attention/fully-connected layers to capture block relationships and predict which blocks to keep or remove during inference.  The network is trained using consistent and sparse losses. Part (b) demonstrates the post-processing step. After training, the pruner network is discarded. The learned importance scores determine the final pruned model (a subnet of the original model) for efficient inference.", "section": "3. Methodology"}, {"figure_path": "sbsaRj475E/figures/figures_14_1.jpg", "caption": "Figure 4: Visualization of DiT model generated images: samples using DDIM-250 steps (uplink) and pruned 60% MACs (downlink). The speedup ratio here is 2.4\u00d7.", "description": "This figure shows a comparison of images generated by the original DiT model and the pruned DiT model using DDIM with 250 steps.  The pruned model achieved a 2.4x speedup while maintaining image quality comparable to the original model. The pruning was done to reduce Multiply-Accumulate operations (MACs) by 60%.  The figure showcases that the pruned model can still generate high-quality images despite significant computational savings.", "section": "C Additional Experiment Results"}, {"figure_path": "sbsaRj475E/figures/figures_15_1.jpg", "caption": "Figure 5: A qualitative comparison with existing methods is provided. We compare our method (prune 0.75) with DeepCache (N=4).", "description": "This figure compares the image generation quality of three different methods: SD1.5 (baseline), DeepCache, and the proposed DiP-GO method. Three different text prompts were used to generate images, and the resulting images are shown for each method.  The comparison highlights the visual similarity of images generated by DiP-GO and the baseline SD1.5 model, suggesting that DiP-GO can achieve significant speedup without compromising image quality.", "section": "4.2 Comparison with State-of-the-Art Methods on Different Base Models"}, {"figure_path": "sbsaRj475E/figures/figures_15_2.jpg", "caption": "Figure 6: The visualization results of the pruning gates for DiT-XL/2 at 250 steps with a pruning ratio of 0.75.", "description": "This figure visualizes the decisions made by the pruner network regarding which blocks (computational units) to keep or remove during the inference process of the DiT-XL/2 diffusion model.  The x-axis represents the timesteps in the denoising process (250 in total), and the y-axis represents the blocks within the model. Each vertical bar indicates whether a block is kept (green) or removed (no bar) at each timestep.  The pattern shows that the model tends to prune fewer blocks in the middle timesteps, where significant image content generation occurs, and prunes more blocks at the beginning and end.", "section": "3.3 Our DiP-GO Approach"}, {"figure_path": "sbsaRj475E/figures/figures_16_1.jpg", "caption": "Figure 7: Feature similarity across adjacent time steps in fast samplers.", "description": "This figure shows heatmaps visualizing the feature similarity across adjacent time steps for two different fast samplers: DDIM (SD-1.5 with 50 steps) and DPM (SD-2.1 with 25 steps).  The heatmaps represent the average cosine similarity between features of the penultimate upsampling block across all time steps, calculated using 200 samples from the COCO2017 validation set. Warmer colors (red) indicate higher similarity, while cooler colors (blue) represent lower similarity. The figures demonstrate a high degree of similarity between features at consecutive time steps, particularly for DDIM. This supports the paper's argument that there is potential for computational savings by exploiting redundancy in feature computation across the inference process.", "section": "C.2 More Quantitative Results"}]