[{"figure_path": "UvbpbEhGaw/tables/tables_13_1.jpg", "caption": "Table 1: Hyperparameters for Training Runs", "description": "This table presents the hyperparameters used for training the different language models (mistral-7b, mixtral-8x7b, and llama3-70b) in the SAMI experiments.  It details the number of iterations, batch size, initial number of batches, number of gradient steps per batch, how the number of batches increases per iteration, total number of gradient steps per iteration, number of constitutions per query, learning rate, precision, and optimizer used for each model.  Additionally, it specifies the FSDP settings including the number of GPUs, sharding strategy, backward prefetch settings, auto wrap policy, transformer layer class, and whether activation checkpointing was used.", "section": "A.3 Hyperparameters"}, {"figure_path": "UvbpbEhGaw/tables/tables_16_1.jpg", "caption": "Table 2: Length-corrected win rates over iterations for helpful and harmless queries against mistral-7b. Significance levels are indicated by p-values: < 0.001, < 0.01, < 0.05, or > 0.05.", "description": "This table presents the results of significance testing for the win rates obtained in the dialogue experiment using the mistral-7b model. The win rates are corrected for length bias. The table shows the win rates for both helpful and harmless queries across three iterations (1, 2, 3), along with their 95% confidence intervals, t-statistics, and p-values.  The p-values indicate the statistical significance of the differences in win rates compared to the baseline model.", "section": "A.7 Significance Testing"}, {"figure_path": "UvbpbEhGaw/tables/tables_16_2.jpg", "caption": "Table 3: Length-corrected win rates over iterations for helpful and harmless queries against mistral-7b-instruct. Significance levels are indicated by p-values: < 0.001, < 0.01, < 0.05, or > 0.05.", "description": "This table presents the results of significance testing for the length-corrected win rates of helpful and harmless queries against the instruction-finetuned model (mistral-7b-instruct).  For each iteration (0-3) and query type (Helpful, Harmless), it shows the win rate (percentage), 95% confidence interval, t-statistic, and p-value.  The p-values indicate the statistical significance of the win rates compared to a baseline of chance (50%).", "section": "A.7 Significance Testing"}, {"figure_path": "UvbpbEhGaw/tables/tables_17_1.jpg", "caption": "Table 4: Raw win rates for TL;DR using mistral-7b as the principle writer. Each model is compared against the respective base model. Significance levels are indicated by p-values: < 0.001, < 0.01, < 0.05, or > 0.05.", "description": "This table presents the results of an experiment on the TL;DR dataset using mistral-7b to generate summarization principles. It shows the win rates for different models (mistral-7b and mixtral-8x7b) against their respective base models across three iterations.  Win rates represent the proportion of times a model's summarization was judged better than the baseline by a human evaluator (GPT-4). Significance levels (p-values) indicate the statistical significance of the win rates.", "section": "4.2.1 Results: Weak Principle Writer"}, {"figure_path": "UvbpbEhGaw/tables/tables_17_2.jpg", "caption": "Table 5: Raw win rates for TL;DR using claude-opus as the principle writer. Each model is compared against mistral-7b-instruct. Significance levels are indicated by p-values: < 0.001, < 0.01, < 0.05, or > 0.05.", "description": "This table presents the results of a win-rate analysis comparing the performance of different language models on the TL;DR summarization dataset.  The models were fine-tuned using principles generated by the claude-opus language model. Each model's performance is measured against the baseline mistral-7b-instruct model.  The table shows the win rate (percentage of times the model's response was judged better than the baseline), 95% confidence interval, t-statistic, and p-value for each model and iteration.  Statistical significance is indicated by p-values.", "section": "4.2 Experiment 2: Weak and Strong Principle Writer"}, {"figure_path": "UvbpbEhGaw/tables/tables_18_1.jpg", "caption": "Table 6: Raw win rates for TL;DR using claude-opus as the principle writer. Each model is compared against the respective base model. Significance levels are indicated by p-values: < 0.001, < 0.01, < 0.05, or > 0.05.", "description": "This table presents the results of win-rate comparisons for the TL;DR summarization task using the claude-opus model as the principle writer.  It shows the win rates of mistral-7b and mixtral-8x7b models against their respective base models across three iterations.  Statistical significance (p-values) is reported to indicate the reliability of the observed differences.", "section": "4.2.2 Results: Strong Principle Writer"}, {"figure_path": "UvbpbEhGaw/tables/tables_18_2.jpg", "caption": "Table 5: Raw win rates for TL;DR using claude-opus as the principle writer. Each model is compared against mistral-7b-instruct. Significance levels are indicated by p-values: < 0.001, < 0.01, < 0.05, or > 0.05.", "description": "This table presents the results of a summarization experiment using the TL;DR dataset and the claude-opus model as the principle writer.  It shows the win rates of the mistral-7b and mixtral-8x7b models against the mistral-7b-instruct baseline across three iterations. Win rates represent the percentage of times each model's response was judged to be better aligned with the summarization principles defined in the constitution by a GPT-4 judge. The 95% confidence intervals and p-values for statistical significance are also included.", "section": "Experiment 2: Summarization"}, {"figure_path": "UvbpbEhGaw/tables/tables_19_1.jpg", "caption": "Table 8: Raw win rates for TL;DR using llama3-70b without chain-of-thought, comparing In-Distribution and Out-of-Distribution (OOD) principles. Significance levels are indicated by p-values: < 0.001, < 0.01, < 0.05, or > 0.05.", "description": "This table presents the win rates of the llama3-70b model on the TL;DR summarization dataset.  The model was evaluated on two sets of summarization principles: those seen during training (In-Distribution) and those not seen (Out-of-Distribution).  The table shows the win rates across three iterations of the SAMI algorithm, along with 95% confidence intervals, t-statistics and p-values to assess statistical significance.  The p-values indicate whether the win rates are statistically different from chance, demonstrating how well SAMI generalizes to unseen principles.", "section": "Experiment 3: Scaling to Stronger Models and Diverse Principles"}, {"figure_path": "UvbpbEhGaw/tables/tables_19_2.jpg", "caption": "Table 9: Raw win rates for TL;DR using llama3-70b with chain-of-thought, comparing In-Distribution and Out-of-Distribution (OOD) principles. Significance levels are indicated by p-values: < 0.001, < 0.01, < 0.05, or > 0.05.", "description": "This table presents the results of win-rate calculations for the llama3-70b model using chain-of-thought prompting.  The win rates are calculated against a baseline model for both in-distribution (principles seen during training) and out-of-distribution (principles not seen during training) summarization principles.  Statistical significance (p-values) are provided to indicate the confidence of the results.  The data shows the model's performance across three iterations of the SAMI training process.", "section": "Experiment 3: Scaling to Stronger Models and Diverse Principles"}, {"figure_path": "UvbpbEhGaw/tables/tables_28_1.jpg", "caption": "Table 10: Constitutions HH-RLHF written with claude-opus.", "description": "This table presents the constitutions used in the HH-RLHF experiments.  Each constitution is composed of two principles, one focusing on helpfulness and harmlessness, and the other on the opposite (not helpful, not harmless).  There are four constitutions in total, representing all four combinations of helpfulness/harmlessness principles.", "section": "A.13 HH-RLHF Constitutions"}, {"figure_path": "UvbpbEhGaw/tables/tables_29_1.jpg", "caption": "Table 10: Constitutions HH-RLHF written with claude-opus.", "description": "This table presents the constitutions used in the HH-RLHF experiments.  Each constitution is composed of two principles: one promoting helpful and harmless responses, and another that violates these principles.  Four constitutions are shown, representing all combinations of helpful/not helpful and harmless/not harmless principles.", "section": "A.13 HH-RLHF Constitutions"}, {"figure_path": "UvbpbEhGaw/tables/tables_29_2.jpg", "caption": "Table 10: Constitutions HH-RLHF written with claude-opus.", "description": "This table presents the constitutions used in the HH-RLHF experiment.  For each combination of helpful/not helpful and harmless/not harmless principles, the table shows the specific constitution used to guide the language model's responses. Each constitution comprises two principles that aim to guide the model's behavior, one focusing on helpfulness and harmlessness, and the other on the opposite or alternative.", "section": "A.13 HH-RLHF Constitutions"}]