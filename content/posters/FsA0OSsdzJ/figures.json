[{"figure_path": "FsA0OSsdzJ/figures/figures_0_1.jpg", "caption": "Figure 1: Within unit n, actions Dhe interact with\n(latent) random effect parameters \u00dfn to produce\nbehavior Xt represented as a dense graphical\nmodel with square vertices denoting interventions\ndo(dhit) [33, 16]. Further assumptions will be\nrequired for the identifiability of the impact of in-\nterventions and their combination, including how\ntemporal impact takes shape and the number of\nindependent units of observation.", "description": "This figure shows a graphical model representing the causal relationships between actions (interventions), latent variables, and observed outcomes within a single unit (n).  The actions (D1, D2, D3, D4) are shown as square nodes, and the observed outcomes (X1, X2, X3, X4) are depicted as circular nodes.  A latent random effect (\u03b2n) influences both the actions and the outcomes, represented by the gray arrows. The blue arrows indicate the direct effect of each action on subsequent outcomes, highlighting the sequential nature of the interventions. Red curved arrows represent potential temporal dependencies between outcomes. The figure emphasizes the complexity of modeling the combined effects of sequential interventions due to the intricate interplay of actions, latent factors, and temporal dynamics.", "section": "1 Contribution"}, {"figure_path": "FsA0OSsdzJ/figures/figures_9_1.jpg", "caption": "Figure 2: Top: 5-run evaluation of test mean squared error on the fully-synthetic (left) and semi-synthetic cases (case). CSI-3 was removed on the right due to very high errors. Bottom: how errors change as training sizes are increased, CSI-1 vs. GRU-2 (left: fully-synthetic, right: semi-synthetic).", "description": "The figure displays box plots to compare the performance of CSI-VAE and GRU across different datasets.  The top row presents the test mean squared error (MSE) for five different model setups in fully synthetic and semi-synthetic datasets.  The bottom row shows how the MSE changes as the training dataset sizes increase for CSI-VAE-1 and GRU-2, again across the fully synthetic and semi-synthetic datasets. Note that CSI-VAE-3 is excluded from the right plot due to significantly higher errors compared to other models.", "section": "5 Experiments"}, {"figure_path": "FsA0OSsdzJ/figures/figures_21_1.jpg", "caption": "Figure 3: Effect of changing r for the fully-synthetic dataset.", "description": "This boxplot shows the effect of varying the dimensionality parameter 'r' on the mean squared error (MSE) of the fully-synthetic dataset.  The ground truth value of r is 5.  The boxplot shows that using a smaller r (r=3) leads to underfitting (higher MSE), whereas using a larger r (r=10) can lead to overfitting (high variance) but can achieve lower MSE, indicating that the model benefits from a higher dimensional representation.  Regularization techniques (L1 and L2 norms) are also shown to help mitigate the overfitting effect seen when r=10.", "section": "D.1 Effect of the Choice of r"}, {"figure_path": "FsA0OSsdzJ/figures/figures_22_1.jpg", "caption": "Figure 4: Examples of reconstruction of training data for CSI-VAE-1 model.", "description": "This figure shows six plots, each visualizing the reconstruction of a time series from the training data for the CSI-VAE-1 model.  Each plot displays the real time series values (blue line) and the best reconstruction (orange line) for a specific user, along with the time steps on the x-axis. This figure helps illustrate the model's ability to learn and reproduce complex patterns in the data. The visual comparison of the real and reconstructed time series provides a qualitative assessment of the model's performance.", "section": "D.2 Additional Figures"}, {"figure_path": "FsA0OSsdzJ/figures/figures_22_2.jpg", "caption": "Figure 4: Examples of reconstruction of training data for CSI-VAE-1 model.", "description": "This figure displays examples of reconstruction of training data using the CSI-VAE-1 model.  It showcases how well the model reconstructs the original time series data for six different users.  The plots compare the actual time series data (Real) against the model's best reconstruction (Recon (best)), showing a close match between the two.  Additionally, 95% confidence intervals are provided to give a sense of the uncertainty associated with the reconstruction.", "section": "D.2 Additional Figures"}, {"figure_path": "FsA0OSsdzJ/figures/figures_22_3.jpg", "caption": "Figure 6: Demonstration of prediction, in the synthetic data case, for CSI-VAE-1.", "description": "This figure demonstrates the prediction results of the CSI-VAE-1 model on synthetic data. It shows the actual time series of X for six different users (User 1 to User 6), along with their corresponding control paths and predictive paths. The control paths represent the expected behavior under no intervention, while the predictive paths are generated by the model. The vertical dashed lines indicate the timesteps where interventions occurred.  The figure visually compares the model's predictions (in green) against the actual observed values (in blue), and shows how well the model captures the effect of interventions on the system dynamics. The variation of each line represents the uncertainty of the model's predictions.", "section": "3 Algorithm and Statistical Inference"}, {"figure_path": "FsA0OSsdzJ/figures/figures_22_4.jpg", "caption": "Figure 7: Residual distribution examples for CSI-VAE-1. In general, we observe a very long tail effect across model predictions.", "description": "This figure displays histograms of prediction errors at different time steps (T+1 to T+5) for the CSI-VAE-1 model. The histograms show the distribution of the differences between the model's predictions and the true values of the target variable.  The long tails indicate that the model occasionally makes large errors, although most predictions are relatively accurate. This is a common characteristic of many machine learning models, particularly for time series, where complex dependencies can make precise prediction challenging.", "section": "D.2 Additional Figures"}, {"figure_path": "FsA0OSsdzJ/figures/figures_23_1.jpg", "caption": "Figure 8: Plug-in model-based predictive interval and coverage.", "description": "The figure shows the plug-in model-based predictive intervals and true values for a subset of 30 test instances. The coverage rate is 57.75%. This visualization demonstrates the model's ability to predict future values within a certain range. The red dots represent true values, and the blue dots with error bars represent prediction intervals generated by the model. The plot is a scatter plot with the x-axis showing test instance indices and the y-axis showing the values (true and predicted). It shows that the prediction intervals tend to be wider, indicating a higher uncertainty around predictions, which is a common trend in prediction tasks.", "section": "E.1 Plug-in Model-based Uncertainty Quantification"}, {"figure_path": "FsA0OSsdzJ/figures/figures_23_2.jpg", "caption": "Figure 9: Conformal prediction predictive interval and coverage.", "description": "This figure shows the results of model-free uncertainty quantification using conformal prediction.  The plot displays model prediction intervals and true values for a subset of 30 test instances.  A key result is the 95.3% coverage rate, indicating that the true values fall within the predicted intervals in 95.3% of the cases. This demonstrates the effectiveness of conformal prediction compared to the model-based approach (Figure 8).", "section": "E.2 Conformal Prediction"}]