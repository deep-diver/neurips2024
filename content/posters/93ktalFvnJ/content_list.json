[{"type": "text", "text": "Boosting Alignment for Post-Unlearning Text-to-Image Generative Models ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Myeongseob Ko\u2217 Henry Li\u2217 Zhun Wang Virginia Tech Yale University University of California, Berkeley myeongseob@vt.edu henry.li@yale.edu zhun.wang@berkeley.edu ", "page_idx": 0}, {"type": "text", "text": "Jonathan Patsenker Yale University jonathan.patsenker@yale.edu ", "page_idx": 0}, {"type": "text", "text": "Jiachen T. Wang Princeton University tianhaowang@princeton.edu ", "page_idx": 0}, {"type": "text", "text": "Qinbin Li University of California, Berkeley liqinbin1998@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Ming Jin Virginia Tech jinming@vt.edu ", "page_idx": 0}, {"type": "text", "text": "Dawn Song University of California, Berkeley dawnsong@berkeley.edu ", "page_idx": 0}, {"type": "text", "text": "Ruoxi Jia   \nVirginia Tech   \nruoxijia@vt.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large-scale generative models have shown impressive image-generation capabilities, propelled by massive data. However, this often inadvertently leads to the generation of harmful or inappropriate content and raises copyright concerns. Driven by these concerns, machine unlearning has become crucial to effectively purge undesirable knowledge from models. While existing literature has studied various unlearning techniques, these often suffer from either poor unlearning quality or degradation in text-image alignment after unlearning, due to the competitive nature of these objectives. To address these challenges, we propose a framework that seeks an optimal model update at each unlearning iteration, ensuring monotonic improvement on both objectives. We further derive the characterization of such an update. In addition, we design procedures to strategically diversify the unlearning and remaining datasets to boost performance improvement. Our evaluation demonstrates that our method effectively removes target classes from recent diffusion-based generative models and concepts from stable diffusion models while maintaining close alignment with the models\u2019 original trained states, thus outperforming stateof-the-art baselines. Our code will be made available at https://github.com/ reds-lab/Restricted_gradient_diversity_unlearning.git. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large-scale text-to-image generative models have recently gained considerable attention for their impressive image-generation capabilities. Despite being at the height of their popularity, these models, trained on vast amounts of public data, inevitably face concerns related to harmful content generation [Heng and Soh, 2024] and copyright infringement [Zhang et al., 2023b]. Although exact machine unlearning\u2014retraining the model by excluding target data\u2014is a direct solution, its computational challenge has driven continued research on approximate machine unlearning. ", "page_idx": 0}, {"type": "image", "img_path": "93ktalFvnJ/tmp/4b80a3842f57d9623a5b0683e6c660a02c2f740dba260e09756de2813ba4b2f1.jpg", "img_caption": ["Figure 1: Generated images using SalUn [Fan et al., 2023], ESD [Gandikota et al., 2023], and Ours after unlearning given the condition. Each row indicates different unlearning tasks: nudity removal, and Van Gogh style removal. Generated images from our approach and SD [Rombach et al., 2022] are well-aligned with the prompt, whereas SalUn and ESD fail to generate semantically correct images given the condition. On average, across 100 different prompts, SalUn shows the lowest clip alignment scores (0.305 for nudity removal and 0.280 for Van Gogh style removal), followed by ESD (0.329 and 0.330, respectively). Our approach achieves scores of 0.350 and 0.352 for these tasks, closely matching the original SD scores of 0.352 and 0.348. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To address this challenge, recent studies [Fan et al., 2023, Gandikota et al., 2023, Heng and Soh, 2024], have introduced approximate unlearning techniques aimed at boosting efficiency while preserving effectiveness. These approaches have successfully demonstrated the ability to remove target concepts while maintaining the model\u2019s general image generation capabilities, with generation quality assessed using the Fr\u00e9chet Inception Distance. However, these studies generally overlook the impact of unlearning on image-text alignment, which pertains to the semantic accuracy between generated images and their text descriptions [Lee et al., 2024]. While pretrained generative models generally demonstrate high alignment scores, our study reveals a critical gap: state-of-the-art unlearning techniques fall short in achieving comparable text-image alignment scores after unlearning, as illustrated in Figure 1. This could lead to potentially problematic behaviors in real-world deployments, necessitating further investigation. ", "page_idx": 1}, {"type": "text", "text": "We attribute the failure of existing techniques to maintain text-image alignment to two primary factors. Firstly, the unlearning objective often conflicts with the goal of maintaining low loss on the retained data, illustrating the competitive nature of these two objectives. Traditionally, approaches to optimizing these objectives have simply aggregated the gradients from both; however, this method of updating the model typically advances one objective at the expense of the other. Hence, while these approaches may successfully remove target concepts, they often compromise text-image alignment for retained concepts in the process. Secondly, current methods employ a simplistic approach to constructing a dataset for loss minimization on retained concepts. For example, in Fan et al. [2023], this dataset is composed of images generated from a single prompt associated with the concept to be removed. This lack of diversity in the dataset might lead to overfitting, which in turn hampers the text-image alignment. ", "page_idx": 1}, {"type": "text", "text": "To address these issues, we propose a principled framework designed to optimally balance the objectives of unlearning the target data and maintaining performance on the remaining data at each update iteration. Specifically, we introduce the concept of the restricted gradient, which allows for the optimization of both objectives while ensuring monotonic improvement in each objective. Furthermore, we have developed a deliberate procedure to enhance data diversity, preventing the model from overftiting to the limited samples in the remaining dataset. To the best of our knowledge, the strategic design of the forgetting target and remaining sets has not been extensively explored in the existing machine unlearning literature. In our evaluation, we demonstrate the improvement in both forgetting quality and alignment on the remaining data, compared to baselines. For example, our evaluation in nudity removal demonstrates that our method effectively reduces the number of detected body parts to zero, compared to 598 with the baseline stable diffusion (SD) [Rombach et al., 2022], 48 with erased stable diffusion (ESD-u), and 3 with saliency map-based unlearning (SalUn) [Fan et al., 2023]. Particularly, while achieving this effective erasing performance, our method reduces the alignment gap to SD by 11x compared to ESD- $^{\\cdot\\mathrm{u}}$ and by $20\\mathrm{x}$ compared to SalUn on the retained test set. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Machine Unlearning ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Machine unlearning has primarily been propelled by the \"Right to be Forgotten\" (RTBF), which upholds the right of users to request the deletion of their data. Given that large-scale models are often trained on web-scraped public data, this becomes a critical consideration for model developers to avoid the need for retraining models with each individual request. In addition to RTBF, recent concerns related to copyrights and harmful content generation further underscore the necessity and importance of in-depth research in machine unlearning. The principal challenge in this field lies in effectively erasing the target concept from pre-trained models while maintaining performance on other data. Recent studies have explored various approaches to unlearning, including the exact unlearning method [Bourtoule et al., 2021] and approximate methods such as using negative gradients, fine-tuning without the forget data, editing the entire parameter space of the model [Golatkar et al., 2020]. To encourage the targeted impact in the parameter space, [Golatkar et al., 2020, Foster et al., 2024] proposed leveraging the Fisher information matrix, and [Fan et al., 2023] leveraged a gradient-based weight saliency map to identify crucial neurons, thus minimizing the impact on remaining neurons. Furthermore, data-influence-based debiasing and unlearning have also been proposed [Chen et al., 2024, Bae et al., 2023]. Another line of work leverages mathematical tools in differential privacy [Guo et al., 2019, Chien et al., 2024] to ensure that the model\u2019s behavior remains indistinguishable between the retrained and unlearned models. ", "page_idx": 2}, {"type": "text", "text": "2.2 Machine Unlearning in Diffusion Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Recent advancements in text-conditioned generative models [Ho and Salimans, 2022, Rombach et al., 2022], trained on extensive web-scraped datasets like LAION-5B [Schuhmann et al., 2022], have raised significant concerns about the generation of harmful content and copyright violations. A series of studies have addressed the challenge of machine unlearning in diffusion models [Heng and Soh, 2024, Gandikota et al., 2023, Zhang et al., 2023a, Fan et al., 2023]. One approach [Heng and Soh, 2024] interprets machine unlearning as a continual learning problem, showing effective removal results in classification tasks by employing Bayesian approaches to continual learning [Kirkpatrick et al., 2017], which enhance unlearning quality while maintaining model performance using generative reply [Shin et al., 2017]. However, this approach falls short in removing concepts such as nudity compared to other methods [Gandikota et al., 2023]. Another proposed method [Gandikota et al., 2023] guides the pre-trained model toward a prior distribution for the targeted concept but struggles to preserve performance. The most recent work [Fan et al., 2023] proposes selectively damaging neurons based on a saliency map and random labeling techniques, although this method tends to overlook the quality of the remaining set, focusing on improving the forgetting quality, which does not fully address the primary challenges in the machine unlearning community. Although [Bae et al., 2023] presents a similar multi-task learning framework for variational autoencoders, their work does not show the optimality of their solution, and their experiments mainly focus on small-scale models, due to the computational expense associated with influence functions. ", "page_idx": 2}, {"type": "text", "text": "3 Our Approach ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We study the efficacy of our approach in unlearning by removing target classes from class-conditional diffusion models or eliminating specific concepts from text-to-image models while maintaining their general generation capabilities. We will call the set of data points to be removed as the forgetting dataset. To set up the notations, let $D$ denote the training set and $D_{f}\\subset D$ be the forgetting dataset. We will use $D_{r}=D\\backslash D_{f}$ to denote the remaining dataset. Our approach only assumes access to some representative points for $D_{f}$ and $D_{r}$ . As discussed later, depending on specific applications, these data points can be either directly sampled from $D_{f}$ and $D_{r}$ or generated based on the high-level concept of $D_{f}$ to be removed. With a slight abuse of notation, we will use $D_{r}$ and $D_{f}$ to also denote the actual representative samples used to operationalize our proposed approach. Furthermore, we denote the model parameter by $\\theta$ . Let $l$ be a proper learning loss function. The loss of remaining data and that of forgettng data are represented by $\\begin{array}{r}{\\dot{\\mathcal{L}}_{r}(\\theta):=\\dot{\\sum}_{z\\in D_{r}}\\,l(\\theta,z)}\\end{array}$ and $\\begin{array}{r}{\\mathcal{L}_{f}(\\theta):=-\\lambda\\sum_{z\\in D_{f}}l(\\theta,z)}\\end{array}$ , respectively, where $\\lambda$ is a weight adjusting the importance of forgetting loss relative to the remaining data loss. We term ${\\mathcal{L}}_{r}$ and $\\mathcal{L}_{f}$ remaining loss and forgetting loss, respectively. We note that in the context of diffusion models, loss function $l$ is defined as $l=\\mathbb{E}_{t,x_{0},\\epsilon\\sim\\mathcal{N}(0,1)}\\left[\\Vert\\epsilon-e_{\\theta}(x_{t},t)\\Vert^{2}\\right]$ , where $x_{t}$ is a noisy version of $x_{0}$ generated by adding Gaussian noise to the clean image $x_{0}\\sim p_{\\mathrm{data}}(x)$ at time step $t$ with a noise scheduler, and $\\boldsymbol{e}_{\\theta}(\\boldsymbol{x}_{t},t)$ is the model\u2019s estimate of the added noise $\\epsilon$ at time $t$ [Xu et al., 2023, Ho et al., 2020]. For text-to-image generative models, the loss function $l$ is specified as $l=\\mathbb{E}_{t,q_{0},c,\\epsilon}\\left[\\|\\epsilon-\\epsilon_{\\theta}(q_{t},t,\\eta)\\|^{2}\\right]$ , where $q_{0}$ is an encoded latent $q_{0}\\,=\\,\\mathcal{E}(x_{0})$ with encoder $\\mathcal{E}$ , and $q_{t}$ is a noisy latent at time step $t$ . The noise prediction $\\epsilon_{\\theta}(q_{t},t,\\eta)$ is conditioned on the timestep $t$ and a text $\\eta$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Optimizing the Update. Similar to existing work Fan et al. [2023], our objective is to find an unlearned model with parameters $\\theta_{u}$ , starting from a pre-trained model with weights $\\theta_{0}$ , such that the model forgets the target concepts in $D_{f}$ while maintaining its utility on the remaining dataset $D_{r}$ . Formally, we aim to maximize the forget error on $D_{f}$ , represented by $\\mathcal{L}_{f}(\\theta)$ , while minimizing the retain error on $D_{r}$ , represented by $\\displaystyle{\\mathcal{L}}r({\\bar{\\theta}})$ . This can be formulated as min\u03b8 ${\\dot{\\mathcal{L}}}_{r}(\\theta)+{\\mathcal{L}}_{f}(\\theta)$ , where our approach applies iterative updates to achieve both goals simultaneously. A simple approach to optimize this objective, often adopted by existing work, is to calculate the gradient $\\nabla\\bar{\\mathcal{L}}_{r}(\\theta)\\bar{+}\\nabla\\mathcal{L}_{f}(\\theta)$ and use it to update the model parameters at each iteration. However, empirically, we observe that the two gradients usually conflict with each other, i.e., the decrease of one objective is at the cost of increasing the other; therefore, in practice, this approach yields a significant tradeoff between forgetting strength and model utility on the remaining data. In this work, we aim to present a principled approach to designing the update direction at each iteration that more effectively handles the tradeoff between forgetting strength and model utility on the remaining data. Our key idea is to identify a direction that achieves a monotonic decrease of both objectives. ", "page_idx": 3}, {"type": "text", "text": "To describe our algorithm, we briefly review the directional derivative. ", "page_idx": 3}, {"type": "text", "text": "Definition 1 (Directional Derivative). The directional derivative [Spivak, 2006] of a function f at x in the direction of v is written as ", "page_idx": 3}, {"type": "equation", "text": "$$\nD_{\\mathbf{v}}\\mathbf{f}\\left(\\mathbf{x}\\right)=\\operatorname*{lim}_{h\\rightarrow0}\\frac{\\mathbf{f}\\left(\\mathbf{x}+h\\mathbf{v}\\right)}{h}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This special form of the derivative has the useful property that its maximizer can be related to the gradient $\\nabla_{\\mathbf{x}}\\mathbf{f}(\\mathbf{x})$ , which we formally state below. ", "page_idx": 3}, {"type": "text", "text": "Theorem 2 (Directional derivative maximizer is the gradient). Let f be a function on x. Then the maximum value of the directional derivative of f at $\\mathbf{x}$ is $|\\nabla\\mathbf{f}(\\mathbf{x})|$ the $\\ell^{2}$ norm of its gradient. Moreover, the direction v is the gradient itself, i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\underset{\\mathbf{v}}{\\arg\\operatorname*{max}}\\;D_{\\mathbf{v}}\\mathbf{f}=\\nabla\\mathbf{f}(\\mathbf{x}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In unlearning, we are specifically interested in the gradient of two losses, the forgetting loss $\\mathcal{L}_{f}$ and the remaining loss $\\mathcal{L}_{r}$ . Moreover, we seek gradient directions that simultaneously improve on both. This motivates the restricted gradient, which we define below. ", "page_idx": 3}, {"type": "text", "text": "Definition 3 (Restricted gradient). The negative restricted gradient of a pair of objectives $\\mathcal{L}_{\\alpha}$ , $\\mathcal{L}_{\\beta}$ is the direction v at $\\pmb{\\theta}$ that satisfies ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{v}}D_{\\mathbf{v}}({\\mathcal{L}}_{\\alpha}+{\\mathcal{L}}_{\\beta})(\\theta)~~s.t.~~{\\mathcal{L}}_{\\alpha}(\\theta)\\geq{\\mathcal{L}}_{\\alpha}(\\theta+\\mathbf{v})~~a n d~~{\\mathcal{L}}_{\\beta}(\\theta)\\geq{\\mathcal{L}}_{\\beta}(\\theta+\\mathbf{v}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Intuitively, with the restricted gradient we seek to define the ideal direction for unlearning. We would like to optimize the joint loss $\\mathcal{L}=\\mathcal{L}_{r}+\\mathcal{L}_{f}$ subject to the condition that at every parameter update step, ${\\mathcal{L}}_{r}$ and $\\mathcal{L}_{f}$ experience monotonic improvement. This is precisely the step prescribed by the negative restricted gradient. Since the learning rates used to fine-tune the parameters in the unlearning process are typically quite small, we can approximate the updated loss at each iteration via a simple first-order Taylor expansion. In this case, the restricted gradient takes a simple form. ", "page_idx": 3}, {"type": "text", "text": "Theorem 4 (Characterizing the restricted gradient under linear approximation). Given $\\theta$ , suppose that $\\mathcal{L}_{r}(\\theta+\\delta)-\\mathcal{L}_{r}(\\theta)\\approx\\bar{\\delta}\\cdot\\nabla\\mathcal{L}_{r}$ and $\\bar{\\mathcal{L}_{f}}(\\theta+\\delta)-\\mathcal{L}_{f}(\\theta)\\approx\\bar{\\delta}\\cdot\\bar{\\nabla}\\mathcal{L}_{f}$ . The restricted gradient can be written as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\underset{\\mathbf{v}}{\\arg\\operatorname*{min}}\\;D_{\\mathbf{v}}(\\mathcal{L}_{f}+\\mathcal{L}_{r})(\\theta)=\\delta_{f}^{*}+\\delta_{r}^{*},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\delta_{f}^{*}=\\nabla\\mathcal{L}_{f}-\\frac{\\nabla\\mathcal{L}_{f}\\cdot\\nabla\\mathcal{L}_{r}}{\\|\\nabla\\mathcal{L}_{r}\\|^{2}}\\nabla\\mathcal{L}_{r},\\quad\\delta_{r}^{*}=\\ \\nabla\\mathcal{L}_{r}-\\frac{\\nabla\\mathcal{L}_{f}\\cdot\\nabla\\mathcal{L}_{r}}{\\|\\nabla\\mathcal{L}_{f}\\|^{2}}\\nabla\\mathcal{L}_{f},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "when we have conflicting unconstrained gradient terms, i.e. $\\nabla{\\mathcal{L}}_{f}\\cdot\\nabla{\\mathcal{L}}_{r}<0$ . ", "page_idx": 4}, {"type": "text", "text": "The theorem presented demonstrates that the restricted gradient is determined by aggregating the modifications from $\\nabla{\\mathcal{L}}_{f}$ and $\\nabla{\\mathcal{L}}_{r}$ . This modification process involves projecting $\\nabla{\\mathcal{L}}_{f}$ onto the normal vector of $\\nabla{\\mathcal{L}}_{r}$ , yielding $\\delta_{f}^{*}$ , and similarly projecting $\\nabla{\\mathcal{L}}_{r}$ onto the normal vector of $\\nabla{\\mathcal{L}}_{f}$ , resulting in $\\delta_{r}^{*}$ . The optimal update, as derived in Theorem 4, is illustrated in Figure 2. Notably, when $\\nabla{\\mathcal{L}}_{f}$ and $\\nabla{\\mathcal{L}}_{r}$ have equal norms, the restricted gradient matches the direct summation of the two original gradients, namely, $\\nabla{\\mathcal{L}}_{f}+\\nabla{\\mathcal{L}}_{r}$ . However, it is more common for the norm of one gradient to dominate the other, in which case the restricted gradient provides a more balanced update compared to direct aggregation. ", "page_idx": 4}, {"type": "image", "img_path": "93ktalFvnJ/tmp/2cc1c3691b70cd44cdaa5a5c1c4f4284072d2516223570370a486aae54b5aa38.jpg", "img_caption": ["(a) Direct aggregate  (b) Restricted gradient "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 2: Visualization of the update. We show the update direction (gray) obtained by (a) directly summing up the two gradients and (b) our restricted gradient. ", "page_idx": 4}, {"type": "text", "text": "Remark 1. We wish to highlight an intriguing link between the gradient aggregation mechanism presented in Theorem 4 and an existing method to address gradient confilcts across different tasks in multi-task learning. This restricted gradient coincides exactly with the gradient surgery procedure introduced in Yu et al. [2020]. While their original paper presented the procedure from an intuitive perspective, our work offers an alternative viewpoint and rigorously characterizes the objective function that the gradient surgery procedure optimizes. ", "page_idx": 4}, {"type": "text", "text": "Diversify $D_{r}$ . Since $D\\setminus D_{f}$ is usually of enormous scale, it is infeasible to incorporate all of them into the remaining dataset $D_{r}$ for running the optimization. In practice, one can only sample a subset of points from $D_{r}$ . In our experiments, we find that the diversity of $D_{r}$ plays an important role in maintaining the model performance on the remaining dataset, as seen in Section 4.2. Therefore, we propose procedures for forming a diverse $D_{r}$ . For models with a finite set of class labels, such as diffusion models trained on CIFAR-10, we adopt a simple procedure of maintaining an equal number of samples for each class in $D_{r}$ . Our ablation studies in Section 4.4 show that this is more effective in maintaining model performance on the remaining dataset than more sophisticated procedures, such as selecting the most similar examples to the forgetting samples. The intuitive reason is that reminding the model of as many fragments as possible related to the remaining set during each forgetting step is crucial. By doing so, it leads to finding a representative restricted descent direction, which helps the model to precisely erase the forget data while maintaining a state comparable to the original model. When the text input is unconstrained, such as in the stable diffusion model setting, to strategically design diverse information, we propose the following procedure to generate $D_{r}$ based on the concept to be forgotten, denoted by $c$ . Using a large language model (LLM), we first generate diverse text prompts related to concept $c$ , yielding prompt set $\\mathcal{V}_{c}$ . These prompts are then modified by removing all references to $c$ , creating a parallel set $\\boldsymbol{\\wp}$ . By passing both $\\mathcal{V}_{c}$ and $\\boldsymbol{\\wp}$ through the target diffusion model, we obtain corresponding image sets $\\scriptstyle{\\mathcal{X}}_{c}$ and $\\mathcal{X}$ . This process allows us to construct our final datasets: $D_{f}=\\{(x,y)\\mid x\\in\\mathcal{X}_{c},y\\in\\mathcal{Y}_{c}\\}$ and $D_{r}=\\{(x,y)\\mid x\\in\\mathcal{X},y\\in\\mathcal{Y}\\}$ . Example prompts and detailed descriptions are provided in Appendix D. ", "page_idx": 4}, {"type": "text", "text": "4 Experiment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this study, we address the crucial challenge of preventing undesirable outputs in text-to-image generative models. We begin by examining class-wise forgetting with CIFAR-10 diffusion-based generative models, where we demonstrate our method\u2019s ability to selectively prevent the generation of specific class images (Section 4.2). We then explore the effectiveness of our approach in removing nudity and art styles (Section 4.3) to address real-world concerns of harmful content generation and copyright infringement. We further study the impact of data diversity (Section 4.4) as well as the sensitivity of our method to hyperparameter settings (Section 4.4). ", "page_idx": 5}, {"type": "text", "text": "4.1 Experiment Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "For our CIFAR-10 experiments, we leverage the EDM framework [Karras et al., 2022], which introduces some modeling improvements including a nonlinear sampling schedule, direct $\\mathbf{x}_{\\mathrm{0}}$ -prediction, and a second-order Heun solver, achieving the state-of-the-art FID on CIFAR-10. For stable diffusion, we utilize the pre-trained Stable Diffusion version 1.4, following prior works. Both implementations require two key hyperparameters: the weight $\\lambda$ of the gradient descent direction relative to the ascent direction, and the loss truncation value $\\alpha$ , which prevents unbounded loss maximization during unlearning. Detailed hyperparameter configurations are provided in Appendix C. For dataset construction, we used all samples in each class for the CIFAR-10 forgetting dataset and 800 samples for Stable Diffusion experiments. Considering the practical constraints of accessing complete datasets in real-world scenarios, we construct the remaining dataset $D_{r}$ by sampling $1\\%$ of data from each retained class, yielding a total of 450 samples for CIFAR-10 (50 from each of the 9 non-target classes) and 800 samples for Stable Diffusion. ", "page_idx": 5}, {"type": "text", "text": "As our baselines for CIFAR-10 experiments, we consider Finetune [Warnecke et al., 2021], gradient ascent and descent, referred to as GradDiff, and SalUn [Fan et al., 2023]. For concept removal, our baselines include the pretrained diffusion model SD [Rombach et al., 2022], erased stable diffusion ESD [Gandikota et al., 2023], and SalUn [Fan et al., 2023]. To fairly compare, We further consider the variants of ESD, depending on the unlearning task. We note that we do not consider the baseline by [Heng and Soh, 2024] due to its demonstrated limited performance in nudity removal, compared to ESD. Our approach is referred to as RG when applied only with the restricted gradient, and RGD when data diversity is incorporated. ", "page_idx": 5}, {"type": "text", "text": "We evaluate our approach using multiple metrics to assess both forgetting effectiveness and model utility. For CIFAR-10 experiments, we measure: 1) unlearning accuracy (UA), calculated as 1-accuracy of the target class, 2) remaining accuracy (RA), which quantifies the accuracy on non-target classes, and 3) Fr\u00e9chet Inception Distance (FID). We observed that standard CIFAR-10 classifiers demonstrate inherent bias when evaluating generated samples from unlearned classes, predomi", "page_idx": 5}, {"type": "table", "img_path": "93ktalFvnJ/tmp/1f24954d424b5b00a8e788c77b9d2023db0cb2dccd23401d18e02557006d9745.jpg", "table_caption": ["Table 1: Quantitative evaluation of unlearning methods on CIFAR-10 diffusion-based generative models. Each method was evaluated by sequentially targeting each of the 10 CIFAR-10 classes for unlearning. For each target class, we measure unlearning accuracy (UA) specific to that class, remaining accuracy (RA) on the other 9 classes, and FID for generation quality. The reported values are averaged across all 10 class-specific unlearning experiments. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "nantly assigning these noise-like images to a particular class among the ten categories\u2014a limitation arising from their training exclusively on clean class samples. We thus leveraged a CLIP-based zero-shot classifier, implementing text prompts \u201ca photo of a class\u201d for the original ten classes and adding \u201crandom noise\u201d as an additional category, enabling a more reliable assessment of unlearning effectiveness. We generate 50K images for FID calculation. For concept removal in Stable Diffusion, we assess forgetting effectiveness using Nudenet [Bedapudi, 2019], which detects exposed body parts in generated images prompted by I2P [Schramowski et al., 2023]. After flitering prompts with non-zero nudity ratios, we obtain 853 evaluation prompts from an initial set of 4,703. To evaluate the retained performance, following[Lee et al., 2024], we measure semantic correctness using CLIP [Cherti et al., 2023] alignment scores (AS) between prompts and their generated images. We evaluate model performance on both training prompts $(D_{r,\\mathrm{train}})$ used during unlearning and a separate set of held-out test prompts $(D_{r,\\mathrm{test}})$ . These two distinct sets are constructed by carefully splitting semantic dimensions (e.g., activities, environments, moods). Detailed construction procedures for both sets are provided in Appendix D. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4.2 Target Class Removal from Diffusion Models ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We present the CIFAR-10 experiment results in Table 1. To fairly compare, we use the same remaining dataset for other baselines. Our finding first indicates that while Finetune achieves superior performance on retained data (highest RA and FID scores), it struggles to effectively unlearn target classes with this limited remaining dataset. Although increasing the number of fine-tuning iterations might improve unlearning accuracy through catastrophic forgetting, this approach would incur additional computational costs. Secondly, we observe that SalUn has low RA, compared to other baselines even with their comparable FID performance. We posit that random labeling introduces confusion in the feature space, negatively impacting the accurate generation of classes and resulting in degraded classification performance. Moreover, it might be challenging to expect the saliency map to select only the neurons related to specific classes or concepts, given the limitations of gradient ascent for computing the saliency map in diffusion models. ", "page_idx": 6}, {"type": "text", "text": "The Impact of Restricted Gradient and Data Diversity Our observations are as follows. 1) RG outperforms Gradiff and other baselines by decreasing FID and increasing RA, indicating that the restricted gradient leads to an optimally balanced solution for both tasks. 2) RGD shows improvements over RG, suggesting that data diversification, in conjunction with the restricted gradient, further enhances performance in terms of RA and FID. We vary the hyperparameters and provide the results in section 4.4. ", "page_idx": 6}, {"type": "text", "text": "4.3 Target Concept Removal from Diffusion Models ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Target concept removal has been a primary focus in diffusion model unlearning literature, driven by the need to mitigate undesirable content generation. While existing methods have shown potential for removing nudity or art styles, our study reveals that they often compromise model alignment after unlearning. ", "page_idx": 6}, {"type": "text", "text": "Nudity Removal. We summarize our results in Figure 4 and Table 2. We observe that Salun tends to generate samples that are overfit to the remaining dataset. Although Salun shows promising performance in nudity removal\u2014detecting fewer exposed body parts compared to SD and ESD-u, as shown in Figure 4\u2014this success comes at the cost of output diversity. In particular, SalUn often generates semantically similar images (e.g., men, wall backgrounds) for both forgetting concepts (Figure 3) and remaining data (Figure 1). Table 4 quantitatively validates this observation, revealing SalUn\u2019s lowest alignment scores post-unlearning. These results suggest that SalUn\u2019s forgetting performance could stem from overfitting. This limitation may arise from two factors: the selected neurons potentially affecting both target and non-target concepts, and the limited diversity in their forget and remaining datasets. In the case of ESD, the resulting model often fails to remove the nudity concept from unlearned models, as shown in Figure 4. We also evaluate ESD-u and observe that the nudity removal performance between ESD and ESD-u are quite similar although it achieves better AS than SalUn. They suggest using \u201cnudity\u201d as a prompt for unlearning, but it might be difficult to reflect the entire semantic space related to the concept of \u201cnudity,\u201d given that we can describe nudity in many different ways using paraphrasing. ", "page_idx": 6}, {"type": "image", "img_path": "93ktalFvnJ/tmp/5e2b8187cd78695d4321886c438f8d73650465a4f6eacfaebf8ce28ea9111bf9.jpg", "img_caption": ["Figure 3: Generated images using SD, SalUn, ESD-u, and RGD (Ours). Each row indicates generated images with different prompts including nudity-related I2P prompts and samples from $D_{f}$ . Each column shows the images generated by different unlearning methods. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "93ktalFvnJ/tmp/e044b7c8cd6ca664fb9cd2f90545c624255840f5088ec433a38e6401e15123f6.jpg", "img_caption": ["Figure 4: The nudity detection results by Nudenet, following prior works [Fan et al., 2023, Gandikota et al., 2023]. The Y-axis shows the exposed body part in the generated images, given the prompt, and the X-axis denotes the number of images generated by each unlearning method and SD. We exclude bars from the plot if the corresponding value is zero. "], "img_footnote": [], "page_idx": 7}, {"type": "table", "img_path": "93ktalFvnJ/tmp/613323920c36a7463f432250e8dcef6f4842ebb71d179c6de9b14305643859d4.jpg", "table_caption": ["Table 2: Nudity and artist removal: we calculate the clip alignment score (AS), following Lee et al. [2024], to measure the model alignment on the remaining set after unlearning. Cells highlighted in green indicate results from our method, while those in red indicate results from the pretrained model. "], "table_footnote": ["\\* The values in parentheses, $\\Delta$ , refer to the gap between the original SD and the unlearned model with each method. \\* ESD, ESD-u, and ESD- $^\\mathrm{x}$ refer to training on full parameters, non-cross-attention weights, and cross-attention weights, respectively. "], "page_idx": 7}, {"type": "text", "text": "RGD outperforms state-of-the-art baselines in terms of forget quality (i.e., zero detection of exposed body part given I2P prompts as described in Figure 4) and retain quality (i.e., high AS presented in Table 2), effectively mitigating the trade-off between the two tasks. To further validate the role of both the restricted gradient and diversification steps to nudity removal, we conduct a two-way ablation study. Removing the restricted gradient step from RGD yields GradDiffD, which incorporates dataset diversity into GradDiff, whereas removing the diversification step yields the previously introduced RG. RGD\u2019s superior performance over both GradDiffD (Table 7 and Figure 4) and RG (Table 4) underscores the crucial importance of both steps in our proposed unlearning algorithm. ", "page_idx": 7}, {"type": "text", "text": "Art Style Removal. Similar to nudity removal, the task of eliminating specific art styles presents a significant challenge. In order to evaluate whether the unlearning methods inadvertently impact other concepts and semantics beyond the targeted art style, we prompt the model with other artists\u2019 styles (e.g., Monet, Picasso) while targeting to remove Vincent van Gogh\u2019s style. The results of generation examples are shown in Figure 1 and Figure 5, and the average alignment scores are shown in Table 2. It is observed that SalUn cannot follow the prompt to generate other artists\u2019 styles and shows a significant drop in alignment scores (AS) compared with the pre-trained SD. ", "page_idx": 8}, {"type": "text", "text": "We also train $\\mathbf{ESD-x}$ by modifying the cross-attention weights, which is more suitable for erasing artist styles than full-parameter training (shown as plain ESD without any suffix) as proposed in ESD work. Although ESD-x performs similarly to RG in terms of alignment scores, after manual inspection of the generated images, we find ESD-x sometimes generates images ignoring the style instructions as presented in ", "page_idx": 8}, {"type": "image", "img_path": "93ktalFvnJ/tmp/56a96ec75827b9b3cbd30868283d2650aa4047509266a0e46b1569ab8d2ecde0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 5: Art style removal. Each row represents different prompts used to evaluate the alignment and each column indicates generated images from different unlearning methods. ", "page_idx": 8}, {"type": "text", "text": "Figure 1, while RG generates images with lower quality details like noisy backgrounds but adheres well to the style instructions. Consequently, after incorporating gradient surgery to prevent interference between retain and forgot targets, our RGD achieves better image quality and shows the best alignment score, almost equivalent to the performance of the pre-trained SD. ", "page_idx": 8}, {"type": "text", "text": "4.4 Ablation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Ablation in Hyperparameters. We examine our method\u2019s sensitivity to two key parameters described in Section 4.1: the retained gradient weight $\\lambda$ and loss truncation threshold $\\alpha$ . Figure 6 presents the variation over different $\\alpha$ values (y-axis) for a given $\\lambda$ value (x-axis), measuring both remaining accuracy (RA) and generation quality (FID). Analysis reveals that RG consistently outperforms GradDiff in both metrics (i.e. achieving the lower FID, and higher or comparable RA with low variation across different $\\alpha$ ), with RGD showing further improvements. RGD exhibits the lowest variance across ", "page_idx": 8}, {"type": "image", "img_path": "93ktalFvnJ/tmp/02a1d828cf411a4c7a9c859ac329bdcb6714516efea906956195694faebbfa60.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 6: Performance analysis across different hyperparameter settings. Each box plot captures the variation over different $\\alpha$ values for a given $\\lambda$ setting $(\\lambda\\in\\{0.5,1.0,5.0\\})$ , measuring both generation quality (FID, left) and remaining accuracy (RA, right). Lower FID indicates better generation quality, while higher RA indicates better model utility of non-target concepts. ", "page_idx": 8}, {"type": "text", "text": "different $\\alpha$ values and achieves the lowest FID and highest RA. RG\u2019s consistent improvements over GradDiff validate the restricted gradient approach, while RGD\u2019s superior performance underscores the importance of dataset diversity. ", "page_idx": 8}, {"type": "text", "text": "Ablation in Diversity. We further investigate the impact of data diversity through controlled experiments. For CIFAR-10, we design two scenarios based on feature similarity analysis using CLIP embeddings: Case 1, where $D_{r}$ contains samples from only the two classes most semantically similar to the target class, and Case 2, with balanced sampling across all classes. This design stems from our ", "page_idx": 8}, {"type": "text", "text": "Table 3: Comparison of UA, RA, and FID for diversity-controlled experiments in CIFAR-10 diffusion models. In this context, Case 1 represents a scenario where the remaining set lacks diversity (i.e., it only includes samples from two closely related classes), while Case 2 includes equal samples from all classes. We note that we used the same remaining dataset size between both cases. ", "page_idx": 9}, {"type": "table", "img_path": "93ktalFvnJ/tmp/cf5a3a53c100b54d51a61ff4f7a14e4cd8c247b9c09e548f86eac8e7508783cf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "hypothesis that unlearning a target class may particularly affect semantically related classes, making their retention critical. We compute class similarities using cosine distance between CLIP feature vectors as described in Figure 7. Table 3 shows that limited diversity (Case 1) significantly impacts model performance, with FID increasing by 83.803 for RGD. This sensitivity to diversity extends to stable diffusion experiments, where we evaluate the impact of uniform dataset construction following SalUn\u2019s approach. As shown in Table 4, RG with uniform datasets shows a larger performance gap from SD $\\Delta=0.032$ in test alignment scores) compared to RGD $\\Delta=0.001)$ ). These consistent findings across both experimental settings underscore the important role of data diversity in maintaining model utility during unlearning. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This study advances the understanding of machine unlearning in text-to-image generative models by introducing a principled approach to balance forgetting and remaining objectives. We show that the restricted gradient provides an optimal update for handling conflicting gradients between these objectives, while strategic data diversification ensures further improvements on model utilities. Our comprehensive evaluation demonstrates that our method effectively removes diverse target classes from CIFAR-10 diffusion models and concepts from stable diffusion models while maintaining close alignment with the models\u2019 original trained states, outperforming state-of-the-art baselines. ", "page_idx": 9}, {"type": "table", "img_path": "93ktalFvnJ/tmp/6fe4e32948c3af5ffd8a55b0e6acbc749da9a7da95fecc503bf62919ba529164.jpg", "table_caption": ["Table 4: Comparison of alignment score (AS) between RGD and RG. RG, in this table, indicates the case when we have uniform forgetting and remaining datasets but utilize the restricted gradient. "], "table_footnote": ["\\* The values in parentheses, $\\Delta$ , refer to the gap between the original SD and the unlearned model with each method. "], "page_idx": 9}, {"type": "text", "text": "5.1 Limitation and Broader Impacts ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "While our solution introduces computationefficient retain set generation using LLMs, the ", "page_idx": 9}, {"type": "text", "text": "strategic sampling of retain sets for stable diffusion models presents intriguing research directions. Specifically, investigating the effectiveness of different sampling strategies\u2014such as the impact of data proximity to target distribution and optimal mixing ratios between near and far samples\u2014could provide valuable insights for unlearning in stable diffusion models. Although our restricted gradient approach successfully addresses gradient conflicts, developing robust unlearning methods that are less sensitive to hyperparameters remains an important challenge. ", "page_idx": 9}, {"type": "text", "text": "6 Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "RJ and the ReDS lab acknowledge support through grants from the Amazon-Virginia Tech Initiative for Efficient and Robust Machine Learning, and NSF CNS-2424127, and the Cisco Research Award. MJ acknowledges the support from NSF ECCS-2331775, IIS-2312794, and the Commonwealth Cyber Initiative. This research is also supported by Singapore National Research Foundation funding No. 053424, DARPA funding No. 112774-19499, and NSF IIS-2229876. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Seohui Bae, Seoyoon Kim, Hyemin Jung, and Woohyung Lim. Gradient surgery for one-shot unlearning on generative model. arXiv preprint arXiv:2307.04550, 2023. ", "page_idx": 10}, {"type": "text", "text": "P Bedapudi. Nudenet: Neural nets for nudity classification, detection and selective censoring, 2019.   \nLucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In 2021 IEEE Symposium on Security and Privacy (SP), pages 141\u2013159. IEEE, 2021.   \nRuizhe Chen, Jianfei Yang, Huimin Xiong, Jianhong Bai, Tianxiang Hu, Jin Hao, Yang Feng, Joey Tianyi Zhou, Jian Wu, and Zuozhu Liu. Fast model debias with machine unlearning. Advances in Neural Information Processing Systems, 36, 2024.   \nMehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2818\u20132829, 2023.   \nEli Chien, Haoyu Wang, Ziang Chen, and Pan Li. Langevin unlearning: A new perspective of noisy gradient descent for machine unlearning. arXiv preprint arXiv:2401.10371, 2024.   \nChongyu Fan, Jiancheng Liu, Yihua Zhang, Dennis Wei, Eric Wong, and Sijia Liu. Salun: Empowering machine unlearning via gradient-based weight saliency in both image classification and generation. arXiv preprint arXiv:2310.12508, 2023.   \nJack Foster, Stefan Schoepf, and Alexandra Brintrup. Fast machine unlearning without retraining through selective synaptic dampening. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 12043\u201312051, 2024.   \nRohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, and David Bau. Erasing concepts from diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2426\u20132436, 2023.   \nAditya Golatkar, Alessandro Achille, and Stefano Soatto. Eternal sunshine of the spotless net: Selective forgetting in deep networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9304\u20139312, 2020.   \nChuan Guo, Tom Goldstein, Awni Hannun, and Laurens Van Der Maaten. Certified data removal from machine learning models. arXiv preprint arXiv:1911.03030, 2019.   \nAlvin Heng and Harold Soh. Selective amnesia: A continual learning approach to forgetting in deep generative models. Advances in Neural Information Processing Systems, 36, 2024.   \nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.   \nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \nTero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models. In Proc. NeurIPS, 2022.   \nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114 (13):3521\u20133526, 2017.   \nTony Lee, Michihiro Yasunaga, Chenlin Meng, Yifan Mai, Joon Sung Park, Agrim Gupta, Yunzhi Zhang, Deepak Narayanan, Hannah Teufel, Marco Bellagente, et al. Holistic evaluation of text-to-image models. Advances in Neural Information Processing Systems, 36, 2024.   \nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.   \nPatrick Schramowski, Manuel Brack, Bj\u00f6rn Deiseroth, and Kristian Kersting. Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22522\u201322531, 2023.   \nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:25278\u201325294, 2022.   \nHanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative replay. Advances in neural information processing systems, 30, 2017.   \nMichael Spivak. Calculus. Cambridge University Press, 2006.   \nAlexander Warnecke, Lukas Pirch, Christian Wressnegger, and Konrad Rieck. Machine unlearning of features and labels. arXiv preprint arXiv:2108.11577, 2021.   \nYanwu Xu, Mingming Gong, Shaoan Xie, Wei Wei, Matthias Grundmann, Tingbo Hou, et al. Semi-implicit denoising diffusion models (siddms). arXiv preprint arXiv:2306.12511, 2023.   \nTianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. Advances in Neural Information Processing Systems, 33: 5824\u20135836, 2020.   \nEric Zhang, Kai Wang, Xingqian Xu, Zhangyang Wang, and Humphrey Shi. Forget-me-not: Learning to forget in text-to-image diffusion models. arXiv preprint arXiv:2303.17591, 2023a.   \nYang Zhang, Teoh Tze Tzun, Lim Wei Hern, Haonan Wang, and Kenji Kawaguchi. On copyright risks of text-to-image diffusion models. arXiv preprint arXiv:2311.12803, 2023b. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Appendices ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A Proof of Theorem 4 14 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A.1 Lemma and Proofs ", "page_idx": 12}, {"type": "text", "text": "B Preliminaries 16 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "B.1 Denoising Diffusion Probabilistic Models 16   \nB.2 Latent Diffusion Models 16 ", "page_idx": 12}, {"type": "text", "text": "C Implementation Details 16 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "C.1 Class Conditional Diffusion Models 16   \nC.2 Stable Diffusion Models . 16 ", "page_idx": 12}, {"type": "text", "text": "D Dataset Diversification Details 17 ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "D.1 Nudity Removal D.2 Artist Removal 18 ", "page_idx": 12}, {"type": "text", "text": "E Additional Results ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "E.1 Generalization to Different Pretrained Models 18   \nE.2 Impact of Different Sizes in $D_{f}$ and $D_{r}$ 19   \nE.3 Class-wise Feature Similarity 19   \nE.4 Qualitative Results 20 ", "page_idx": 12}, {"type": "text", "text": "A Proof of Theorem 4 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To prove this theorem, we establish the following lemma. We notate the $\\ell^{2}$ norm as $\\|\\cdot\\|$ throughout. Lemma 5 (Projected gradients obtain optimal solution to a constrained objective). Let $\\mathcal{L}_{f}(\\theta)$ , and $\\mathcal{L}_{r}(\\theta)$ be $K$ -Lipschitz smooth negative forget and retain losses under the $\\ell^{2}$ norm respectively. Then, the update \u03b4f\u2217 = \u2207Lf \u2212 \u2225\u2207f Lr\u22252r is the minimizer of ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\underset{||\\delta_{f}||=\\eta}{\\arg\\operatorname*{min}}\\;\\mathcal{L}_{f}(\\theta+\\delta_{f})\\;\\;\\;s.t.\\;\\;\\;\\mathcal{L}_{r}(\\theta)\\geq\\mathcal{L}_{r}(\\theta+\\delta_{f})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "in terms of $\\delta_{f}$ . Similarly, $\\begin{array}{r}{\\delta_{r}^{*}=\\nabla{\\mathcal{L}_{r}}-\\frac{\\nabla{\\mathcal{L}_{f}}\\cdot\\nabla{\\mathcal{L}_{r}}}{\\|\\nabla{\\mathcal{L}_{f}}\\|^{2}}\\nabla{\\mathcal{L}_{f}}}\\end{array}$ \u2207\u2225L\u2207f L\u00b7\u2207\u2225L2r \u2207Lf is the minimizer of ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\underset{||\\delta_{r}||=\\eta}{\\arg\\operatorname*{min}}\\,\\mathcal{L}_{r}(\\theta+\\delta_{r})~~s.t.~~\\mathcal{L}_{f}(\\theta)\\geq\\mathcal{L}_{f}(\\theta+\\delta_{r}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "in terms of $\\delta_{f}$ , for a value $\\eta\\ll\\,\\frac{1}{K}$ when we have conflicting unconstrained gradient terms, i.e.   \n$\\nabla{\\mathcal{L}}_{f}\\cdot\\nabla{\\mathcal{L}}_{r}<0$ . ", "page_idx": 13}, {"type": "text", "text": "Proof of Lemma 5. For $\\delta_{r}$ , $\\delta_{f}$ , both of norm $\\eta$ , we have good approximation by the Taylor expansion due to the Lipschitz condition on $\\mathcal{L}_{f},\\,\\mathcal{L}_{r}$ . Therefore, we have, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathscr{L}_{r}(\\theta+\\delta_{r})-\\mathscr{L}_{r}(\\theta)\\approx\\delta_{r}\\cdot\\nabla\\mathscr{L}_{r}}\\\\ &{\\mathscr{L}_{f}(\\theta+\\delta_{f})-\\mathscr{L}_{f}(\\theta)\\approx\\delta_{f}\\cdot\\nabla\\mathscr{L}_{f}}\\\\ &{\\mathscr{L}_{f}(\\theta+\\delta_{r})-\\mathscr{L}_{f}(\\theta)\\approx\\delta_{r}\\cdot\\nabla\\mathscr{L}_{f}}\\\\ &{\\mathscr{L}_{r}(\\theta+\\delta_{f})-\\mathscr{L}_{r}(\\theta)\\approx\\delta_{f}\\cdot\\nabla\\mathscr{L}_{r}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We can re-express the two objectives as, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\arg\\operatorname*{min}_{\\vec{\\delta}_{f}}\\cdot\\nabla\\mathcal{L}_{f}}&{\\mathrm{s.t.}\\quad\\delta_{f}\\cdot\\nabla\\mathcal{L}_{r}\\leq0}\\\\ {\\quad||\\delta_{f}||=\\eta}\\\\ {\\arg\\operatorname*{min}_{\\vec{\\delta}_{r}}\\cdot\\nabla\\mathcal{L}_{r}}&{\\mathrm{s.t.}\\quad\\delta_{r}\\cdot\\nabla\\mathcal{L}_{f}\\leq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "By the method of Langrangian multipliers, for each objective we create slack variables $\\lambda_{f},\\lambda_{r}$ , and obtain the unconstrained objectives, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{||\\delta_{f}||=\\eta}{\\arg\\operatorname*{min}}\\,\\delta_{f}\\cdot\\nabla\\mathcal{L}_{f}+\\lambda_{f}\\delta_{f}\\cdot\\nabla\\mathcal{L}_{r}=\\underset{||\\delta_{f}||=\\eta}{\\arg\\operatorname*{min}}\\,\\delta_{f}\\cdot\\big(\\nabla\\mathcal{L}_{f}+\\lambda_{f}\\nabla\\mathcal{L}_{r}\\big)}\\\\ &{\\underset{||\\delta_{r}||=\\eta}{\\arg\\operatorname*{min}}\\,\\delta_{r}\\cdot\\nabla\\mathcal{L}_{r}+\\lambda_{r}\\delta_{r}\\cdot\\nabla\\mathcal{L}_{f}=\\underset{||\\delta_{r}||=\\eta}{\\arg\\operatorname*{min}}\\,\\delta_{r}\\cdot\\big(\\nabla\\mathcal{L}_{r}+\\lambda_{r}\\nabla\\mathcal{L}_{f}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We first observe since both are now linear objective, that the minima is trivially observed when $\\delta_{f}^{*}\\propto-(\\nabla{\\mathcal{L}}_{f}+\\lambda_{f}\\nabla{\\mathcal{L}}_{r})$ , and $\\delta_{r}^{*}\\propto-(\\nabla{\\mathcal{L}}_{r}+\\lambda_{r}\\nabla{\\mathcal{L}}_{f})$ . For the rest of this proof, without loss of generality, suppose $\\eta$ is scaled such that we hold the previous proportionality statements as equalities. ", "page_idx": 13}, {"type": "text", "text": "We invoke KKT sufficiency conditions to both confirm if these minima exist, and obtain solutions to the slack variables. In the case of confilcting gradients, since $\\nabla{\\mathcal{L}}_{f}\\cdot\\nabla{\\mathcal{L}}_{r}<0$ , the minimizers of the unconstrained objectives in Equations 7, 8 are not satisfied within the constraints. Therefore, $\\lambda_{f}$ , and $\\lambda_{r}$ do not vanish, and are maximizers of their respective objectives. Taking the gradients in respect to the slack variables and setting to 0, we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{\\lambda_{f}}\\left(\\delta_{f}^{*}\\cdot(\\nabla\\mathcal{L}_{f}+\\lambda_{f}\\nabla\\mathcal{L}_{r})\\right)=-\\nabla_{\\lambda_{f}}\\left(\\delta_{f}^{*}\\cdot\\delta_{f}^{*}\\right)=-2\\nabla\\mathcal{L}_{r}\\cdot\\delta_{f}^{*}=0}\\\\ {\\nabla_{\\lambda_{r}}\\left(\\delta_{r}^{*}\\cdot(\\nabla\\mathcal{L}_{r}+\\lambda_{r}\\nabla\\mathcal{L}_{f})\\right)=-\\nabla_{\\lambda_{r}}\\left(\\delta_{r}^{*}\\cdot\\delta_{r}^{*}\\right)=-2\\nabla\\mathcal{L}_{f}\\cdot\\delta_{r}^{*}=0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We can solve this in a way that satisfies the objective by requiring $\\delta_{r}^{*}$ to be orthogonal to $\\nabla{\\mathcal{L}}_{f}$ , and $\\delta_{f}^{*}$ to be orthogonal to $\\nabla{\\mathcal{L}}_{r}$ . In this case, we have $\\begin{array}{r}{\\lambda_{f}=-\\frac{\\nabla\\mathcal{L}_{f}\\cdot\\nabla\\mathcal{L}_{r}}{\\|\\nabla\\mathcal{L}_{r}\\|^{2}}}\\end{array}$ \u2225\u2207f Lr\u22252r and \u03bbr = \u2212 $\\begin{array}{r}{\\lambda_{r}=-\\frac{\\nabla\\mathcal{L}_{f}\\cdot\\nabla\\mathcal{L}_{r}}{\\|\\nabla\\mathcal{L}_{f}\\|^{2}}}\\end{array}$ as the optima. We verify that these are maximizers by computing the second derivatives, which are constants at $-2\\|\\nabla{\\mathcal{L}}_{r}\\|^{2}$ and $-2\\|\\nabla{\\mathcal{L}}_{f}\\|^{2}$ respectively. Both are strictly negative, confirming the second order sufficient condition for a maximizer. ", "page_idx": 13}, {"type": "text", "text": "Therefore it is precisely the restricted gradient steps, \u03b4f\u2217 = \u2207Lf \u2212\u2207\u2225L\u2207f L\u00b7r\u2207\u2225L2r and $\\delta_{r}^{*}=\\nabla\\mathcal{L}_{r}\\mathrm{~-~}$ $\\frac{\\nabla\\mathcal{L}_{f}\\cdot\\nabla\\mathcal{L}_{r}}{\\|\\nabla\\mathcal{L}_{f}\\|^{2}}\\nabla\\mathcal{L}_{f}$ , which solve the optimization problems in Equations 5, 6 respectively. ", "page_idx": 13}, {"type": "text", "text": "Proof of Theorem 4. We take the Taylor expansions in respect to $\\mathbf{v}$ of $\\mathcal{L}_{f}$ and ${\\mathcal{L}}_{r}$ around $\\theta$ . We have mutatis mutandis for some $h\\in\\mathbb{R}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{L}_{f}(\\boldsymbol{\\theta}+h\\mathbf{v})=\\mathcal{L}_{f}(\\boldsymbol{\\theta})+h\\nabla\\mathcal{L}_{f}(\\boldsymbol{\\theta})\\cdot\\mathbf{v}+\\mathcal{O}(h^{2}\\|\\mathbf{v}\\|^{2})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "It follows that, for $\\mathbf{v},\\,\\mathbf{w}$ , such that $\\mathbf{w}\\cdot\\nabla{\\mathcal{L}}_{f}(\\theta)=0.$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{\\mathbf{v}+\\mathbf{w}}\\mathcal{L}_{f}(\\theta)=\\displaystyle\\operatorname*{lim}_{h\\rightarrow0}\\frac{\\mathcal{L}_{f}(\\theta+h\\mathbf{v}+h\\mathbf{w})}{h}}\\\\ &{\\quad\\quad\\quad\\quad=\\displaystyle\\operatorname*{lim}_{h\\rightarrow0}\\frac{\\mathcal{L}_{f}(\\theta)+h\\nabla\\mathcal{L}_{f}(\\theta)\\cdot(\\mathbf{v}+\\mathbf{w})}{h}}\\\\ &{\\quad\\quad\\quad=\\displaystyle\\operatorname*{lim}_{h\\rightarrow0}\\frac{\\mathcal{L}_{f}(\\theta)+h\\nabla\\mathcal{L}_{f}(\\theta)\\cdot\\mathbf{v}}{h}}\\\\ &{\\quad\\quad\\quad=\\displaystyle\\operatorname*{lim}_{h\\rightarrow0}\\frac{\\mathcal{L}_{f}(\\theta+h\\mathbf{v})}{h}}\\\\ &{\\quad\\quad\\quad=D_{\\mathbf{v}}\\mathcal{L}_{f}(\\theta).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We observe that we can bound the optimization, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\mathbf{v}}{\\operatorname*{min}}^{*}\\,D_{\\mathbf{v}}(\\mathcal{L}_{f}+\\mathcal{L}_{r})(\\theta)\\geq\\underset{\\mathbf{v}}{\\operatorname*{min}}\\,D_{\\mathbf{v}}\\mathcal{L}_{f}(\\theta)}&{\\mathrm{s.t.}\\quad\\mathcal{L}_{r}(\\theta)\\geq\\mathcal{L}_{r}(\\theta+\\mathbf{v})}\\\\ &{+\\underset{\\mathbf{w}}{\\operatorname*{min}}\\,D_{\\mathbf{w}}\\mathcal{L}_{r}(\\theta)\\quad\\mathrm{s.t.}\\quad\\mathcal{L}_{f}(\\theta)\\geq\\mathcal{L}_{f}(\\theta+\\mathbf{w})}\\\\ &{=\\underset{h\\rightarrow0}{\\operatorname*{lim}}\\,\\underset{\\mathbf{v}}{\\operatorname*{min}}^{*}\\frac{1}{h}\\mathcal{L}_{f}(\\theta+h\\mathbf{v})+\\underset{h\\rightarrow0}{\\operatorname*{lim}}\\,\\underset{\\mathbf{w}}{\\operatorname*{min}}^{*}\\frac{1}{h}\\mathcal{L}_{r}(\\theta+h\\mathbf{w}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We use $\\mathrm{\\min^{*}}$ to signify the presence of constraints as previously defined for the respective expression to simplify notation. ", "page_idx": 14}, {"type": "text", "text": "We invoke Lemma 5 to solve each minimization problem above, yielding, $\\mathbf{v}^{*}\\,\\propto\\,\\delta_{f}^{*}\\,=\\,\\nabla{\\mathcal{L}}_{f}\\,-$ \u2207\u2225L\u2207f L\u00b7\u2207\u2225L2r \u2207Lr, and w\u2217\u221d\u03b4r\u2217 = \u2207Lr \u2212 $\\begin{array}{r}{\\mathbf{w}^{*}\\propto\\delta_{r}^{*}=\\nabla\\mathcal{L}_{r}-\\frac{\\nabla\\mathcal{L}_{f}\\cdot\\nabla\\mathcal{L}_{r}}{\\|\\nabla\\mathcal{L}_{f}\\|^{2}}\\nabla\\mathcal{L}_{f}}\\end{array}$ \u2207\u2225L\u2207f L\u00b7f\u2207 \u2225L2r \u2207Lf. Note that since we are taking the limits as $h\\to0$ , the Taylor expansion in Lemma 5 is exact as the relevant constant in the lemma, $\\|\\eta\\|\\rightarrow0$ . We also have that $D_{\\mathbf{v}^{*}}\\mathcal{L}_{f}(\\theta)\\,=\\,D_{\\mathbf{v}^{*}+\\mathbf{w}^{*}}\\mathcal{L}_{f}(\\theta)$ since $\\mathbf{w}^{*}\\cdot\\nabla{\\mathcal{L}}_{f}(\\theta)\\,=\\,0$ (and similarly we have $D_{\\mathbf{w}^{*}}\\mathcal{L}_{r}(\\boldsymbol{\\theta})=D_{\\mathbf{v}^{*}+\\mathbf{w}^{*}}\\mathcal{L}_{r}(\\boldsymbol{\\theta}))$ . ", "page_idx": 14}, {"type": "text", "text": "Now, altogether we can show, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\operatorname*{min}^{*}\\,D_{\\mathbf{v}}(\\mathcal{L}_{f}+\\mathcal{L}_{r})(\\theta)\\ge\\operatorname*{min}^{*}\\,D_{\\mathbf{v}}\\mathcal{L}_{f}(\\theta)+\\operatorname*{min}_{\\mathbf{v}}^{*}\\,D_{\\mathbf{w}}\\mathcal{L}_{r}(\\theta)}}\\\\ &{}&{=D_{\\mathbf{v}^{*}}\\mathcal{L}_{f}(\\theta)+D_{\\mathbf{w}^{*}}\\mathcal{L}_{r}(\\theta)}\\\\ &{}&{=D_{\\mathbf{v}^{*}+\\mathbf{w}^{*}}\\mathcal{L}_{f}(\\theta)+D_{\\mathbf{v}^{*}+\\mathbf{w}^{*}}\\mathcal{L}_{r}(\\theta)}\\\\ &{}&{=D_{\\mathbf{v}^{*}+\\mathbf{w}^{*}}\\left(\\mathcal{L}_{f}(\\theta)+\\mathcal{L}_{r}(\\theta)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "If $\\mathbf{v}^{*}+\\mathbf{w}^{*}$ satisfies the constraints of the original optimization, and bounds the minimizer from below, this is the optimal solution. ", "page_idx": 14}, {"type": "text", "text": "Therefore, we require for both losses, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{f}(\\boldsymbol{\\theta}+\\mathbf{v}^{*}+\\mathbf{w}^{*})\\geq\\mathcal{L}_{f}(\\boldsymbol{\\theta})}\\\\ {\\mathcal{L}_{r}(\\boldsymbol{\\theta}+\\mathbf{v}^{*}+\\mathbf{w}^{*})\\geq\\mathcal{L}_{r}(\\boldsymbol{\\theta})}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By the constraints of the optimization problem, we know that ${\\mathcal{L}}_{f}(\\theta\\!+\\!\\mathbf{v}^{*})\\geq{\\mathcal{L}}(\\theta)$ , and $\\mathcal{L}_{r}(\\boldsymbol{\\theta}+\\mathbf{w}^{*})\\geq$ ${\\mathcal{L}}(\\theta)$ . Again, using the Taylor expansion, mutatis mutandis we have, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\mathcal L}_{f}(\\boldsymbol{\\theta}+\\mathbf{v}^{*}+\\mathbf{w}^{*})={\\mathcal L}_{f}(\\boldsymbol{\\theta}+\\mathbf{v}^{*})+\\nabla{\\mathcal L}_{f}(\\boldsymbol{\\theta}+\\mathbf{v}^{*})\\cdot\\mathbf{w}^{*}+{\\mathcal O}(\\|\\mathbf{w}^{*}\\|^{2})}\\\\ &{\\qquad\\qquad\\qquad\\simeq{\\mathcal L}_{f}(\\boldsymbol{\\theta}+\\mathbf{v}^{*})\\geq{\\mathcal L}_{f}(\\boldsymbol{\\theta}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, $\\eta(\\delta_{f}^{*}+\\delta_{r}^{*})$ , solves the optimization for a small enough constant $\\eta\\in\\mathbb{R}^{+}$ , so $\\delta_{f}^{*}+\\delta_{r}^{*}$ solves the optimization up to a constant. This completes the proof. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "B Preliminaries ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Denoising Diffusion Probabilistic Models Diffusion models consist of a forward diffusion process and a reverse diffusion process. The forward diffusion process progressively deteriorates an initial data point $x_{0}\\sim q\\{x_{0}\\}$ by adding Gaussian noise with a variance schedule $\\beta_{t}\\in(0,1)$ to generate a set of noisy latents $\\{{\\dot{x_{1}}},x_{2},...,x_{T}\\}$ with a Markov transition probability: ", "page_idx": 15}, {"type": "equation", "text": "$$\nq(x_{1:T}|x_{0})=\\prod_{t=1}^{T}q(x_{t}|x_{t-1}),\\quad q(x_{t}|x_{t-1})=\\mathcal{N}(x_{t};\\sqrt{1-\\beta_{t}}x_{t-1},\\beta_{t}\\mathbf{I})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\nq(x_{t}|x_{0})=\\mathcal{N}\\left(x_{t};\\sqrt{\\bar{\\alpha}_{t}}x_{0},(1-\\bar{\\alpha}_{t})\\mathbf{I}\\right),\\quad\\bar{\\alpha}_{t}=\\prod_{n=1}^{t}(1-\\beta_{j}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $T$ indicates the maximum time steps. In the reverse process, we aim to predict the latent representation of the previous time step, which can be written as $p_{\\theta}(x_{t-1}|\\dot{x}_{t})\\;\\;=$ $\\mathcal{N}\\left(x_{t-1};\\mu_{\\theta}(\\bar{x}_{t},t),\\Sigma_{\\theta}(t)\\right)$ . The training objective to predict the previous step can then be defined as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{L}=-\\sum_{t=2}^{T}\\mathbb{E}_{q_{(}x_{t}|x_{0})}\\left[D_{K L}\\big(q(x_{t-1}|x_{t},x_{0})||p_{\\theta}(x_{t-1}|x_{t})\\big)\\right]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $q(x_{t-1}|x_{t},x_{0})=\\mathcal{N}\\left(x_{t-1};\\mu_{q}(x_{t},x_{0}),\\Sigma_{q}(t)\\right)$ . Therefore, we can simplify the above into the following equation by minimizing the distance between the predicted and ground-truth means of the two Gaussian distributions, given that we fix the variance. ", "page_idx": 15}, {"type": "equation", "text": "$$\nL=\\mathbb{E}_{t,x_{0},\\epsilon}\\left[\\|\\epsilon-e_{\\theta}(x_{t},t)\\|^{2}\\right]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\boldsymbol{e}_{\\theta}(\\boldsymbol{x}_{t},t)$ is the model\u2019s estimate of the noise $\\epsilon$ added into the clean image $x_{0}$ at time $t$ [Xu et al., 2023, Ho et al., 2020]. ", "page_idx": 15}, {"type": "text", "text": "Latent Diffusion Models Latent Diffusion Models (LDMs) [Rombach et al., 2022] are probabilistic frameworks used to model the distribution $p_{d a t a}$ by learning on a latent space. Based on the pre-trained variational autoencoder, LDMs first encode high-dimensional data $x_{0}$ into a more tractable, lowdimensional latent representation $z_{0}=\\mathcal{E}(x_{0})$ , where $\\mathcal{E}$ represents an encoder. Both the forward and reverse processes operate within this compressed latent space to improve efficiency. The objective can be described as $L\\,=\\,\\mathbb{E}_{t,z_{0},c,\\epsilon}\\left[\\|\\epsilon-\\dot{\\epsilon}_{\\theta}(z_{t},t,c)\\|^{2}\\right]$ , where the noise prediction $\\epsilon_{\\theta}(z_{t},\\bar{t},c)$ is conditioned on the timestep $t$ and a text $c$ . Classifier-free guidance $[\\mathrm{Ho}$ and Salimans, 2022] can be used during inference to adjust the image generation path. ", "page_idx": 15}, {"type": "text", "text": "C Implementation Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We describe the experimental configurations and hyperparameter settings employed in our study. All experiments were conducted using an NVIDIA H100 GPU. ", "page_idx": 15}, {"type": "text", "text": "Class Conditional Diffusion Models For experiments on CIFAR-10, we implemented our method using hyperparameters $\\alpha=1\\times10^{-1}$ and $\\lambda=5$ . Our EDM implementation used a batch size of 64, a duration parameter of 0.05, and a learning rate of 1e-5. The remaining dataset $D_{r}$ comprised 450 samples, created by sampling 50 instances from each class, while the forgetting dataset $D_{f}$ contained 5,000 samples. ", "page_idx": 15}, {"type": "text", "text": "Stable Diffusion Models For nudity removal experiments with Stable Diffusion, we set $\\alpha=1.6$ and $\\lambda=1.5$ . Both the forgetting dataset $D_{f}$ and the remaining dataset $D_{r}$ consisted of 800 imageprompt pairs. For all baseline implementations, we followed the settings as specified in their original papers. ", "page_idx": 15}, {"type": "text", "text": "D Dataset Diversification Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we present a set of example prompts designed for our $D_{f}$ and $D_{r}$ used for stable diffusion model experiments. To generate these prompts, we leverage the ChatGPT. Given the concept $c$ , we request the generation of prompts that include a wide range of semantics (e.g., environment, time, mood, actions) to describe the concept $c$ for $D_{f}$ . As explained before, we remove the word related to $c$ to generate $D_{r}$ . Once generated, we split the prompts into training and test sets to fairly evaluate our models\u2019 performance. ", "page_idx": 16}, {"type": "text", "text": "D.1 Nudity Removal ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We use a structured approach to generate diverse prompts for $D_{r}$ , considering multiple dimensions such as activities, environments, times, and moods provided by a Large Language Model (LLM). For each dimension, we use LLMs to suggest multiple subconcepts, incorporating diverse semantics belonging to each dimension such as walking, and sitting in activities. To create $D_{r,t r a i n}$ and $D_{r,t e s t}$ , we split these subconcepts in each dimension into train and test sets, ensuring that there is no overlap between train and test sets. We then combine these subconcepts to generate $D_{r}$ . Table 5 shows a set of example prompts we used for nudity removal. ", "page_idx": 16}, {"type": "text", "text": "Table 5: Example prompts for nudity concept removal. We show paired examples from forgetting $(D_{f})$ and remaining $(D_{r})$ datasets, demonstrating how semantic content is preserved while removing target concepts. ", "page_idx": 16}, {"type": "table", "img_path": "93ktalFvnJ/tmp/681107bf8ef479f3de510d298ad3307cbbabe96f12578c325ceba8d89214bc72.jpg", "table_caption": [], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "D.2 Artist Removal ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Similar to nudity removal, we construct some templates with multiple dimensions such as the artist\u2019s name, actions, environments, and moods, then flil in each dimension with the suggestions from LLMs. Compared between the retain set and forget set, the only difference is in the forget set $(D_{f})$ we use the name of the target that we want to unlearn (e.g., Van Gogh), and use other artists\u2019 names or some virtual names in the retain set $(D_{r})$ . We will revise the paper to incorporate these details, providing a clearer and more comprehensive explanation of our method for generating $D_{r,t r a i n}$ and $D_{r,t e s t}$ in each application. Table 6 shows a set of example prompts we used for style removal. ", "page_idx": 17}, {"type": "table", "img_path": "93ktalFvnJ/tmp/6a9a3a4af40323d077bef3cab58c04bc948c910f2c9cf6d5590e19f7848bcad4.jpg", "table_caption": ["Table 6: Example prompts for art style removal. Forgetting dataset $(D_{f})$ targets Van Gogh\u2019s style, while remaining dataset $(D_{r})$ preserves the same semantic content with different artistic styles. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "E Additional Results ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "E.1 Generalization to Different Pretrained Models ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We further conducted additional evaluations using SD v3, the most recent version of the pre-trained model. SD v3 employs a transformer-based architecture (e.g., Diffusion Transformer models) instead of the UNet-based architecture used in previous versions. This significant change allows us to test our method\u2019s performance across different model structures. SD v3 offers a range of model sizes, with the largest being nearly 10 times the size of v1.4. We choose a medium size model with 2B parameters, which is approximately 2 times larger than v1.4. This variability enables us to assess how our method performs across different model capacities. We maintained the same hyperparameter settings as in v1.4 to ensure an easy generalization capability. We evaluated two baselines alongside our method, observing their performance under multiple hyperparameter tunings. We observed high alignment scores for both $D_{r,\\mathrm{train}}$ and $D_{r,\\mathrm{test}}$ splits with SD v3, while effectively mitigating harmful output generation. On the other hand, both baselines showed alignment score drops. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "table", "img_path": "93ktalFvnJ/tmp/a1e22b33ada1211bc2bebfc25062fdeb1792d529873f9b21cc3a0b6b1aa2160b.jpg", "table_caption": ["Table 7: Comparison of nudity removal effectiveness and alignment scores across different methods on Stable Diffusion Model "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "E.2 Impact of Different Sizes in $D_{f}$ and $D_{r}$ ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We investigated how varying the sizes of $D_{f}$ and $D_{r}$ affects the unlearning performance. Our analysis reveals several key findings. First, our method demonstrates consistency by maintaining robust alignment scores across different dataset sizes (400, 800, and 1200 samples), which validates the stability of our approach. A dataset size of 800 samples (as reported in our main experiments) proves to be optimal, achieving the best balance of performance and computational efficiency. Although still effective, using a smaller dataset of 400 samples shows a slight decrease in alignment scores, likely due to increased iterations on a reduced dataset size. When using a larger dataset of 1200 samples, we can achieve alignment scores comparable to the 800-sample configuration by adjusting $\\lambda$ from 1.5 to 1.15, which helps balance the increased gradient ascent steps. Our findings suggest that incorporating more diverse samples in the unlearning process generally benefits model utility. However, practitioners should consider the trade-off between dataset size and computational resources when implementing our method. ", "page_idx": 18}, {"type": "table", "img_path": "93ktalFvnJ/tmp/1fc59a5494521fcb0adfdafce0f752baaabbbd3837926daa04dc4d194dd1ebaf.jpg", "table_caption": ["Table 8: Alignment scores comparison with varying dataset sizes "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "E.3 Class-wise Feature Similarity ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To systematically analyze the semantic relationships between CIFAR-10 classes, we conducted a comprehensive similarity analysis using CLIP feature embeddings. For each class, we extracted features from 500 training examples using a pre-trained CLIP model [Radford et al., 2021]. We then computed class-wise mean feature vectors and calculated pairwise cosine similarities between these representations. ", "page_idx": 18}, {"type": "text", "text": "Figure 7 presents the complete analysis of class-wise similarities, showing the two most similar classes for each target class along with their corresponding similarity scores. This analysis informed our experimental design for the ablation studies on data diversity, particularly in constructing the remaining dataset $(D_{r})$ for Case 1, where only the two most similar classes were included. In our ablation study 4.4, we specifically focused on three target classes (plane, bird, and dog) and their respective most similar classes when constructing the limited diversity scenario (Case 1). ", "page_idx": 18}, {"type": "text", "text": "E.4 Qualitative Results ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We provide qualitative results comparing generations from the unlearned models (Salun, ESD-u, RGD and the pretrained model SD using the retain prompts $D_{r}$ . ", "page_idx": 18}, {"type": "image", "img_path": "93ktalFvnJ/tmp/e14ce6d144c1cac2f031edc1a6cc745da54002942e179721c8db317cb309070e.jpg", "img_caption": ["Figure 7: CIFAR10 class-wise feature similarity based on CLIP [Radford et al., 2021] "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "93ktalFvnJ/tmp/68b4890987973dd724ebad56a47ac0b7724440a9fb2b9df2b545552d2316ad4f.jpg", "img_caption": ["Figure 8: SD given the prompts from $D_{r}$ "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "93ktalFvnJ/tmp/71f72d92f89e38d584f1e7b4fe6c1f661423510fc277165d5430b28f5334a7a4.jpg", "img_caption": ["Figure 9: Salun given the prompts from $D_{r}$ "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "93ktalFvnJ/tmp/5a18742bff910dc2b250d92d18038297d891d8b44291d8f23861587049f06feb.jpg", "img_caption": ["Figure 10: ESD-u given the prompts from $D_{r}$ "], "img_footnote": [], "page_idx": 20}, {"type": "image", "img_path": "93ktalFvnJ/tmp/2291a1a84811459b985f4fc4452990687f73d685a7eb59d8ec8f202172ed00b3.jpg", "img_caption": ["Figure 11: RGD (Ours) given the prompts from $D_{r}$ "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: In the abstract and introduction, we provide the motivation, the current limitations in the existing work, and the contribution and brief evaluation results of our paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The authors discuss the sensitivity of hyperparameters and the importance of diversity level. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: In the main paper, we provide the core part of our proposed theory and assumptions, and also provide complete proof in the appendix. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provide all hyperparameters, datasets, architecture we used in both the main paper and appendix and the way of designing the forgetting and remaining datasets. We also provide the examples of each dataset in the appendix. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper provides open access to the data, and the authors will also release the code publicly. ", "page_idx": 22}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provide all implementation details in the main paper and appendix. ", "page_idx": 22}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We provide the average and standard deviation for CIFAR-10 experiments. ", "page_idx": 22}, {"type": "text", "text": "8. Experiments Compute Resources ", "page_idx": 22}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] Justification: We provide the information on the computation resources in Appendix. ", "page_idx": 23}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We conform with the NeurIPS Code of Ethics in every respect. ", "page_idx": 23}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We provide a broader impact in our paper, and we don\u2019t have negative societal impacts. ", "page_idx": 23}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We properly mask the harmful part in the figure for publication. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We properly credited the original owners of assets used in the paper. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: We do not introduce the new assets. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: We do not involve human subjects in our study. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] Justification: We do not conduct experiments on individuals. ", "page_idx": 23}]