[{"figure_path": "Vhh7ONtfvV/tables/tables_4_1.jpg", "caption": "Table 1: Comparison of different methods to map the representation space of ImageNet-1k pre-trained DeiT-B/16 to CLIP image representation space. The green colored texts are exact matches with the top-10 descriptions obtained from the imagenet pretrained embeddings, while the orange colored texts are approximate matches. The match rate is the average fraction of exact matches across all components, while cosine distance is the average cosine distance between the CLIP representations and the transformed model representations on ImageNet", "description": "This table compares three different methods for aligning the representation space of a pre-trained DeiT-B/16 model with CLIP's image representation space.  The methods are: using only one linear map, COMPALIGN without orthogonality regularization (\u03bb = 0), and COMPALIGN with orthogonality regularization. The table shows the top 10 descriptions generated by each method for a random component, highlighting exact matches (green) and approximate matches (orange) against ImageNet's pre-trained embeddings.  Finally, it reports the average match rate and cosine distance for each method, demonstrating the improved performance of COMPALIGN with orthogonality regularization.", "section": "4 Aligning the component representations to CLIP space"}, {"figure_path": "Vhh7ONtfvV/tables/tables_6_1.jpg", "caption": "Table 2: Spearman's rank correlation between the orderings induced by CLIP score and component score averaged over a selection of common features", "description": "This table presents the Spearman's rank correlation coefficients, which measure the strength and direction of monotonic association between two ranked variables.  In this case, it compares the ranking of components based on a CLIP-derived score (measuring how well they align with CLIP's image representation space) and a component score based on the importance of a component towards a particular image feature.  The average correlation is calculated across various common features (like color, texture, animal, etc.) to gauge the overall consistency between the two ranking methods. Higher values (closer to 1.0) indicate strong agreement between the rankings, suggesting that the proposed scoring function effectively identifies component relevance to image features.", "section": "6 Feature-based component analysis"}, {"figure_path": "Vhh7ONtfvV/tables/tables_8_1.jpg", "caption": "Table 3: Worst group accuracy and average group accuracy for Waterbirds dataset before and after intervention for various models (format is before \u2192 after)", "description": "This table presents the results of an ablation study on the Waterbirds dataset.  The study aimed to mitigate spurious correlations by removing components identified as relevant to the dataset's spurious correlation. The table shows the worst-group accuracy and average group accuracy for six different vision transformer models (DeiT, CLIP, DINO, DINOv2, Swin, MaxVit) before and after the ablation of specific components. The improvement in accuracy after ablation demonstrates the effectiveness of the proposed method in reducing spurious correlations and improving the model's robustness.", "section": "Zero-shot spurious correlation mitigation"}, {"figure_path": "Vhh7ONtfvV/tables/tables_17_1.jpg", "caption": "Table 4: Spearman rank correlation for various common properties", "description": "This table presents the Spearman's rank correlation values between the orderings induced by the CLIP score and the component score for various common image properties. The properties considered are color, texture, animal, person, location, pattern, and shape. The correlations are calculated by averaging over a selection of common features for each model.  The models used are DeiT, DINO, DINOv2, Swin, and MaxViT.", "section": "6 Feature-based component analysis"}, {"figure_path": "Vhh7ONtfvV/tables/tables_23_1.jpg", "caption": "Table 5: Zero-shot segmentation results for different algorithms and models. Chefer at al's code does not support MaxViT and SWIN models.", "description": "This table presents the performance comparison of different zero-shot image segmentation methods on several vision transformer (ViT) models.  The methods compared include the proposed decomposition approach and two established baselines: GradCAM and the method by Chefer et al.  The evaluation metrics used are pixel accuracy (pixAcc), mean intersection over union (mIoU), and mean average precision (mAP).  The results highlight the effectiveness of the proposed approach compared to existing techniques.", "section": "J Zero-shot segmentation"}, {"figure_path": "Vhh7ONtfvV/tables/tables_24_1.jpg", "caption": "Table 6: All group accuracies on the Waterbirds dataset before and after component ablation", "description": "This table presents the group accuracies on the Waterbirds dataset before and after ablating specific model components.  The Waterbirds dataset is a synthesized dataset where images of birds are pasted onto backgrounds of either water or land, creating a spurious correlation between the bird type and the background. The ablation experiment aims to mitigate this spurious correlation by selectively removing components identified as contributing to the spurious association. The results show the improvement in the worst-performing group's accuracy and the overall average group accuracy after this intervention. The table provides a quantitative measure of the effectiveness of the proposed method in reducing the impact of the spurious correlation.", "section": "K Zero-shot spurious correlation mitigation"}]