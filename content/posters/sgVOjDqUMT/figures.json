[{"figure_path": "sgVOjDqUMT/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of our MiniCache strategy and example results: (a) shows the observation that the KV cache states between two adjacent layers are highly similar, particularly across the middle-to-deep layers. (b) compares the performance of MiniCache, and the mean baseline, which simply averages the KV caches of two layers, using the LLaMA-3-70B model [6] on the GSM8K dataset [10]. MiniCache, which begins merging from the half-layer depth, achieves near-lossless performance. (c) highlights the primary difference between MiniCache and previous approaches. MiniCache investigates the inter-layer redundancy of KV caches along the depth dimension of LLMs, an aspect overlooked by intra-layer-based methods. Here, T refers to the last timestamp of pre-filling, and T + 1 des to the first timestamp of decoding.", "description": "This figure provides an overview of the MiniCache strategy and shows example results.  Panel (a) demonstrates the high similarity of KV cache states between adjacent layers, especially in the middle and deep layers of LLMs.  Panel (b) compares MiniCache's performance to a simple averaging baseline on the GSM8K dataset using the LLaMA-3-70B model, showing that MiniCache achieves near-lossless performance when merging layers from the halfway point. Finally, panel (c) illustrates the key difference between MiniCache and prior methods: MiniCache focuses on inter-layer redundancy in the depth dimension, whereas others focus on intra-layer redundancy.", "section": "1 Introduction"}, {"figure_path": "sgVOjDqUMT/figures/figures_1_2.jpg", "caption": "Figure 1: Overview of our MiniCache strategy and example results: (a) shows the observation that the KV cache states between two adjacent layers are highly similar, particularly across the middle to deep layers. (b) compares the performance of MiniCache, and the mean baseline, which simply averages the KV caches of two layers, using the LLaMA-3-70B model [6] on the GSM8K dataset [10]. MiniCache, which begins merging from the half-layer depth, achieves near-lossless performance. (c) highlights the primary difference between MiniCache and previous approaches. MiniCache investigates the inter-layer redundancy of KV caches along the depth dimension of LLMs, an aspect overlooked by intra-layer-based methods. Here, T refers to the last timestamp of pre-filling, and T + 1 des to the first timestamp of decoding.", "description": "This figure illustrates the MiniCache strategy and its results.  Panel (a) shows that KV cache states are highly similar between adjacent layers in LLMs, especially in the middle-to-deep layers. Panel (b) compares MiniCache's performance to a simple averaging baseline on the GSM8K dataset, demonstrating that MiniCache achieves near-lossless performance by merging layers from the middle upwards. Panel (c) highlights the key difference between MiniCache and prior work: MiniCache focuses on the inter-layer redundancy of KV caches, which prior methods overlooked.", "section": "1 Introduction"}, {"figure_path": "sgVOjDqUMT/figures/figures_3_1.jpg", "caption": "Figure 1: Overview of our MiniCache strategy and example results: (a) shows the observation that the KV cache states between two adjacent layers are highly similar, particularly across the middle-to-deep layers. (b) compares the performance of MiniCache, and the mean baseline, which simply averages the KV caches of two layers, using the LLaMA-3-70B model [6] on the GSM8K dataset [10]. MiniCache, which begins merging from the half-layer depth, achieves near-lossless performance. (c) highlights the primary difference between MiniCache and previous approaches. MiniCache investigates the inter-layer redundancy of KV caches along the depth dimension of LLMs, an aspect overlooked by intra-layer-based methods. Here, T refers to the last timestamp of pre-filling, and T + 1 des to the first timestamp of decoding.", "description": "This figure provides a visual overview of MiniCache and its performance. Subfigure (a) demonstrates the high similarity of KV cache states between adjacent layers in LLMs, particularly in the middle and deep layers. Subfigure (b) compares MiniCache's performance to a simple averaging baseline on the GSM8K dataset, showing MiniCache achieves near-lossless performance by merging from the half-layer depth.  Subfigure (c) illustrates the key difference between MiniCache and prior methods: MiniCache focuses on inter-layer redundancy, a factor that previous methods overlooked.", "section": "1 Introduction"}, {"figure_path": "sgVOjDqUMT/figures/figures_5_1.jpg", "caption": "Figure 3: The illustration of the proposed method MiniCache. (a) depicts the cross-layer compression process. We fetch the KV caches, from layers l and l \u2013 1, and merge them into shared states via Eq. (3). Additionally, we compute the l2 norm for the caches to obtain their magnitudes. Furthermore, we select unmergeable tokens for retention, then store merged cache, retention tokens, and magnitudes at layer l in C. (b) illustrates the restoration process for layers l and l \u2013 1, which includes magnitude rescaling in Eq. (2) and retention token recovery.", "description": "This figure illustrates the MiniCache method for cross-layer KV cache compression and restoration. (a) shows the compression process: fetching KV caches from layers *l* and *l-1*, merging them using a reparameterization technique (Eq. 3), calculating magnitudes, selecting unmergeable tokens for retention, and storing the merged cache, retention tokens, and magnitudes. (b) shows the restoration process: retrieving the merged cache and magnitudes, rescaling the magnitudes (Eq. 2), and recovering the retention tokens to reconstruct the original KV caches.", "section": "4 Method"}, {"figure_path": "sgVOjDqUMT/figures/figures_7_1.jpg", "caption": "Figure 1: Overview of our MiniCache strategy and example results: (a) shows the observation that the KV cache states between two adjacent layers are highly similar, particularly across the middle-to-deep layers. (b) compares the performance of MiniCache, and the mean baseline, which simply averages the KV caches of two layers, using the LLaMA-3-70B model [6] on the GSM8K dataset [10]. MiniCache, which begins merging from the half-layer depth, achieves near-lossless performance. (c) highlights the primary difference between MiniCache and previous approaches. MiniCache investigates the inter-layer redundancy of KV caches along the depth dimension of LLMs, an aspect overlooked by intra-layer-based methods. Here, T refers to the last timestamp of pre-filling, and T + 1 des to the first timestamp of decoding.", "description": "This figure provides an overview of the MiniCache strategy and its performance.  Panel (a) shows that adjacent layers in LLMs exhibit high similarity in their KV cache states, especially in the middle to deep layers.  Panel (b) illustrates the effectiveness of MiniCache compared to a simple averaging baseline on the GSM8K dataset, demonstrating near-lossless performance with layer merging starting from the middle layers. Panel (c) highlights the key difference between MiniCache and existing intra-layer methods: MiniCache leverages inter-layer redundancy in the depth dimension of LLMs for compression, an aspect not previously considered.", "section": "1 Introduction"}, {"figure_path": "sgVOjDqUMT/figures/figures_8_1.jpg", "caption": "Figure 5: Memory usage and throughput comparison between our 4-bit MiniCache, 2-bit KIVI, and 16-bit Baseline. MiniCache can achieve higher throughput by enabling a larger batch size while reducing memory footprints via LLaMA-2-7B [5].", "description": "This figure shows the results of an experiment comparing the memory usage and throughput of three different methods: a baseline using FP16, KIVI-2, and MiniCache-4.  The experiment was conducted using the LLaMA-2-7B model and varied the batch size. MiniCache-4 consistently demonstrates lower memory usage and significantly higher throughput compared to the other methods, especially as the batch size increases.  This highlights MiniCache's efficiency in handling large language models.", "section": "5 Experiments"}, {"figure_path": "sgVOjDqUMT/figures/figures_19_1.jpg", "caption": "Figure 3: The illustration of the proposed method MiniCache. (a) depicts the cross-layer compression process. We fetch the KV caches, from layers l and l \u2013 1, and merge them into shared states via Eq. (3). Additionally, we compute the l2 norm for the caches to obtain their magnitudes. Furthermore, we select unmergable tokens for retention, then store merged cache, retention tokens, and magnitudes at layer l in C. (b) illustrates the restoration process for layers l and l \u2013 1, which includes magnitude rescaling in Eq. (2) and retention token recovery.", "description": "This figure illustrates the MiniCache method's cross-layer compression and restoration processes. Part (a) shows how KV caches from adjacent layers are merged into shared states, magnitudes are calculated, and unmergeable tokens are retained and stored. Part (b) demonstrates the restoration of the original states from the merged cache, magnitudes, and retained tokens.", "section": "4 Method"}, {"figure_path": "sgVOjDqUMT/figures/figures_20_1.jpg", "caption": "Figure 1: Overview of our MiniCache strategy and example results: (a) shows the observation that the KV cache states between two adjacent layers are highly similar, particularly across the middle-to-deep layers. (b) compares the performance of MiniCache, and the mean baseline, which simply averages the KV caches of two layers, using the LLaMA-3-70B model [6] on the GSM8K dataset [10]. MiniCache, which begins merging from the half-layer depth, achieves near-lossless performance. (c) highlights the primary difference between MiniCache and previous approaches. MiniCache investigates the inter-layer redundancy of KV caches along the depth dimension of LLMs, an aspect overlooked by intra-layer-based methods. Here, T refers to the last timestamp of pre-filling, and T + 1 des to the first timestamp of decoding.", "description": "This figure provides an overview of MiniCache and its results.  Panel (a) demonstrates the high similarity of KV cache states between adjacent layers in LLMs, especially in deeper layers. Panel (b) shows that MiniCache, by merging KV caches from the middle layer downwards, achieves near lossless performance on the GSM8K benchmark while outperforming a simple averaging baseline.  Panel (c) illustrates how MiniCache uniquely addresses inter-layer redundancy in KV caches\u2014a factor ignored by other methods. It uses a novel approach of disentangling the states into magnitude and direction components, then interpolating the directions and preserving lengths.", "section": "1 Introduction"}]