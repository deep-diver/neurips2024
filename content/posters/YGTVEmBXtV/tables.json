[{"figure_path": "YGTVEmBXtV/tables/tables_5_1.jpg", "caption": "Table 1: Quantified performances of various models on VAL Probing.", "description": "This table presents a quantitative comparison of various language models' performance on the VAL (Various Long-context) Probing tasks.  The VAL Probing tasks evaluate a model's ability to utilize information across various context types (document, code, database) and retrieval patterns (forward, backward, bidirectional). The table shows the average performance (Avg) and the gap between maximum and minimum performance (Gap) across different relative positions of the information within the context.  A smaller gap indicates greater robustness in information retrieval across the context.", "section": "3 Long-Context Probing"}, {"figure_path": "YGTVEmBXtV/tables/tables_6_1.jpg", "caption": "Table 2: Quantified comparison between IN2 training and normal instruction tuning.", "description": "This table compares the performance of the Mistral-7B-Instruct-v0.2 model after undergoing two different training methods: Information-Intensive (IN2) training and normal instruction tuning.  The comparison is quantified using average scores and the difference between the maximum and minimum performance across three probing tasks (Document, Code, Database) and overall.  It highlights the effectiveness of IN2 training in improving the model's performance, particularly in reducing the performance gap between different context positions.", "section": "4 Experiments and Analysis"}, {"figure_path": "YGTVEmBXtV/tables/tables_7_1.jpg", "caption": "Table 3: Performances of various models on real-world long-context tasks. Results of models with * are reported in Bai et al. (2023) and Lv et al. (2024).", "description": "This table presents the performance comparison of various language models on nine real-world long-context tasks.  These tasks cover diverse areas like question answering, multi-hop reasoning, and summarization. The table allows a quantitative assessment of the models' abilities to handle long-context information across different problem types.", "section": "4.1 Experimental Setup"}, {"figure_path": "YGTVEmBXtV/tables/tables_7_2.jpg", "caption": "Table 4: Model performances on few-shot learning tasks.", "description": "This table compares the performance of three different language models (GPT-4-Turbo, Mistral-7B-Instruct-v0.2, and FILM-7B) on three few-shot learning tasks: TREC, TriviaQA, and SAMSum.  The \"Average\" column provides the mean performance across all three tasks.  The results show that FILM-7B, despite being an open-source model, achieves comparable performance to GPT-4-Turbo, a proprietary model, highlighting its effectiveness.", "section": "4.2 Main Results and Analysis"}, {"figure_path": "YGTVEmBXtV/tables/tables_16_1.jpg", "caption": "Table 5: Performance of FILM-7B with different RoPE base \u03b8 during IN2 training.", "description": "This table presents the results of experiments conducted to analyze the impact of different RoPE (Rotary Position Embedding) base values on the performance of the FILM-7B model during IN2 (Information-Intensive) training.  The table shows the average performance (Avg) and the difference between the maximum and minimum performance (Gap) across various probing tasks (Document, Code, Database). The results demonstrate how varying the RoPE base affects the model's ability to handle long-context information, particularly its robustness across different positions in the context.  The experiment uses 20% of the training data for IN2 training and a 4K sliding window.", "section": "Training Strategy Analysis"}, {"figure_path": "YGTVEmBXtV/tables/tables_17_1.jpg", "caption": "Table 6: Performance of FILM-7B with different training data sizes for IN2 training.", "description": "This table presents the results of the FILM-7B model trained with varying amounts of data using the IN2 training method.  It shows the average scores (Avg) and the difference between the maximum and minimum scores (Gap) across three probing tasks: Document, Code, and Database.  The results are broken down by the percentage of the full training data used (1%, 10%, 20%, 50%, and 100%).  The \"Gap\" metric indicates the robustness of the model's performance across different positions within the long context.", "section": "4.1 Experimental Setup"}, {"figure_path": "YGTVEmBXtV/tables/tables_18_1.jpg", "caption": "Table 7: Performances (%) of <7B models on RULER benchmark. The performance exceeding the threshold (i.e., Llama2-7B on 4K length) is underlined.", "description": "This table presents the performance of various 7B parameter scale language models on the RULER benchmark. The RULER benchmark is designed to evaluate the effective context length of language models, which is the maximum context length that a model can effectively utilize. The table shows the claimed context window size of each model and its effective context window size as determined by the RULER benchmark. The performance of each model is reported for various context lengths, from 4K to 128K tokens. The performance exceeding the Llama2-7B performance at context length 4K is underlined.  This table highlights the ability of FILM-7B to handle longer contexts compared to other models.", "section": "3 Long-Context Probing"}]