[{"type": "text", "text": "Improved Guarantees for Fully Dynamic $k$ -Center Clustering with Outliers in General Metric Spaces ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Leyla Biabani Eindhoven University of Technology Eindhoven, The Netherlands l.biabani@tue.nl ", "page_idx": 0}, {"type": "text", "text": "Annika Hennes Heinrich Heine University Du\u00a8sseldorf Du\u00a8sseldorf, Germany annika.hennes@hhu.de ", "page_idx": 0}, {"type": "text", "text": "Denise La Gordt Dillie   \nEindhoven University of Technology Eindhoven, The Netherlands   \nlagordtdilliedenise@gmail.com ", "page_idx": 0}, {"type": "text", "text": "Morteza Monemizadeh Eindhoven University of Technology Eindhoven, The Netherlands m.monemizadeh@tue.nl ", "page_idx": 0}, {"type": "text", "text": "Melanie Schmidt Heinrich Heine University Du\u00a8sseldorf Du\u00a8sseldorf, Germany mschmidt@hhu.de ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The metric $k$ -center clustering problem with $z$ outliers, also known as $(k,z)$ -center clustering, involves clustering a given point set $P$ in a metric space $(M,d)$ using at most $k$ balls, minimizing the maximum ball radius while excluding up to $z$ points from the clustering. This problem holds fundamental significance in various domains, such as machine learning, data mining, and database systems. ", "page_idx": 0}, {"type": "text", "text": "This paper addresses the fully dynamic version of the problem, where the point set undergoes continuous updates (insertions and deletions) over time. The objective is to maintain an approximate $(k,z)$ -center clustering with efficient update times. We propose a novel fully dynamic algorithm that maintains a $(4+\\epsilon)$ -approximate solution to the $(k,z)$ -center clustering problem that covers all but at most $(1+\\epsilon)z$ points at any time in the sequence with probability $1-k/e^{\\Omega(\\log k)}$ . The algorithm achieves an expected amortized update time of ${\\mathcal{O}}(\\epsilon^{-3}{\\dot{k}}^{6}\\log(k)\\log(\\Delta))$ , and is applicable to general metric spaces. Our dynamic algorithm presents a significant improvement over the recent dynamic $(14+\\epsilon)$ -approximation algorithm by Chan, Lattanzi, Sozio, and Wang [5] for this problem. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Clustering problems and algorithms play an important role across a multitude of fields, helping researchers and practitioners in the analysis of data and identification of patterns. These techniques find extensive application in diverse domains, including machine learning, where they help in categorizing and understanding complex datasets. In data mining, clustering methods are utilized to uncover hidden structures and relationships within large datasets, facilitating better decision-making and insight generation. Moreover, in image and signal processing, clustering algorithms assist in segmenting and classifying data, enabling tasks such as image recognition and signal denoising. ", "page_idx": 0}, {"type": "text", "text": "In bioinformatics, clustering techniques are essential for organizing biological data and identifying patterns in genetic sequences, protein structures, and gene expression proflies. Similarly, in anomaly detection, clustering methods are employed to identify unusual or unexpected patterns in data, which may indicate potential anomalies or security breaches. Furthermore, in social network analysis, clustering algorithms help in understanding the structure and dynamics of social networks by identifying communities and influential nodes. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The $k$ -center problem is known as one of the fundamental clustering problems. Given a set of points $P$ in a metric space and a number $k$ , the aim of the $k$ -center problem is to find $k$ centers such that the maximum distance between any point and its closest center is minimized. This can also be equivalently formulated as finding a minimum radius $r$ and centers $c_{1},\\ldots,c_{k}$ such that the balls $\\cup_{i=1}^{k}{\\cal B}_{P}(c_{i},\\dot{r})$ cover the point set. The $k$ -center problem can be 2-approximated, and this is the best possible approximation guarantee [13]. In the last decade, the focus has shifted to analyzing the problem under various complications that arise in applications. ", "page_idx": 1}, {"type": "text", "text": "One line of research is to study the $k$ -center problem (and other clustering problems) in different computational models like streaming or for dynamic point sets. In the first case, points arrive sequentially, and only a summary can be stored in memory. In the second case, the point set is maintained by insertion queries and deletion queries for single points, and algorithms have to update their solution after any such query. ", "page_idx": 1}, {"type": "text", "text": "Another line of research is to study clustering under constraints. For example, capacitated clustering is very popular, i.e., limiting the number of points per cluster. However, lower bounds on cluster sizes have also been studied in the context of anonymity, and newer works have also considered constraints that model societal concerns like fair or diverse composition of clusters. In this paper, we study clustering with outliers. Formulated as a constraint, the $k$ -center problem with outliers allows $k+z$ centers, but $z$ of these have to be singletons, meaning no point may be assigned to them. We can intuitively formulate it with balls as follows: The $k$ -center problem with outliers asks to find a minimum $r$ and $k$ centers $c_{1},\\ldots,c_{k}$ such that the balls $\\cup_{i=1}^{k}\\tilde{B_{P}7}(c_{i},r)$ cover all but $z$ points. ", "page_idx": 1}, {"type": "text", "text": "Solving clustering problems in the presence of outliers is a crucial task due to the common occurrence of measurement errors or other sources of significant deviation from the rest of the data in realworld datasets. Ignoring outliers can severely distort the results of clustering algorithms, leading to inaccurate groupings. To address this challenge, a common approach is to solve a clustering problem while excluding up to $z$ data points considered as outliers. ", "page_idx": 1}, {"type": "text", "text": "The first approximation algorithm for the $k$ -center problem with outliers is due to Charikar et al. [8]. The challenge when designing approximation algorithms in the presence of outliers is that one needs to show that enough points are covered by balls of bounded sizes around the approximate centers. It is not necessary to identify the outliers of an optimal solution exactly as long as the number of uncovered points remains small enough. Due to this, [8] and follow-up papers use charging arguments. Points covered by the solution of the algorithm are mapped to points in optimum clusters, and it is then shown that the number of uncharged points is small enough. ", "page_idx": 1}, {"type": "text", "text": "We explore the $k$ -center clustering problem with $z$ outliers within the fully dynamic model, where the point set experiences continuous updates through insertions and deletions over time. Quite some work on this problem has been done for inputs from metric spaces with bounded doubling dimension [2, 3, 19]. This setting allows for geometric data structures that allow easier navigation through the data set and also bounding of the number of changes that can occur due to a query by dimension-dependent volume arguments. ", "page_idx": 1}, {"type": "text", "text": "We study the general metric setting. General metric spaces represent a key objective for clustering algorithms due to their broad range of distance functions, ensuring applicability to any data type. Chan et al. recently showed that in general metric spaces, any dynamic $\\mathcal{O}(1)$ -approximation algorithm for $k$ -center clustering excluding at most $z$ outliers has an amortized update time of $\\Omega(z)$ [5]. For real-world applications, the fraction of outliers of the data could be arbitrarily large. Therefore, we allow for $(1+\\epsilon)z$ outliers to be excluded to avoid the dependence on $z$ of the update time. ", "page_idx": 1}, {"type": "text", "text": "The only previous work in this setting is the algorithm by Chan et al. [5], which returns a solution with at most $(1+\\epsilon)z$ outliers with probability at least $1\\,-\\,\\delta$ , for $\\bar{0}~\\overset{\\cdot}{<}~\\delta~\\leq~\\frac{1}{k}$ and $\\epsilon\\mathrm{~>~0~}$ . It works by maintaining a clustering with $t:=k\\cdot\\lceil\\log_{1+\\epsilon}{\\frac{k}{\\delta}}\\rceil$ clusters of radius $2r$ and using this to derive a final clustering with $k$ clusters and radius $14r$ . They maintain such a clustering for all $r\\in\\Gamma:=\\{(1+\\tau)^{i}:\\bar{d}_{\\operatorname*{min}}\\leq(1+\\tau)^{i}\\leq(1+\\tau)\\cdot d_{\\operatorname*{max}},i\\in\\dot{\\mathbb{N}}\\}$ , with $d_{\\mathrm{min}}$ and $d_{\\mathrm{max}}$ the minimum and maximum distances, respectively, between any two points ever inserted. It is then shown that there exists an instance $r\\in\\Gamma$ that will approximate the optimal radius to within a factor $(14+\\tau)$ , while allowing for $(1+\\epsilon)z$ outliers, with probability at least $1-\\delta$ . The amortized time per update is $\\mathcal{O}(\\lvert\\Gamma\\rvert\\cdot\\frac{k^{2}}{\\epsilon^{2}}\\log^{2}\\frac{1}{\\delta})$ ,n tws iitnh $\\begin{array}{r}{|\\Gamma|=(\\log\\frac{d_{\\operatorname*{max}}}{d_{\\operatorname*{min}}})/\\tau}\\end{array}$ . The total memory requirement is ${\\mathcal{O}}(|\\Gamma|\\cdot|n|)$ where $n$ is ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "1.1 Our contribution ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We introduce a novel fully dynamic $(4+\\epsilon)$ -approximation algorithm designed to maintain a $k$ -center clustering while allowing for at most $(1+\\epsilon)z$ outliers at any time in the sequence. ", "page_idx": 2}, {"type": "text", "text": "The expected amortized update time is ${\\mathcal{O}}(\\epsilon^{-3}k^{6}\\log(k)\\log(\\Delta))$ per operation (insertion or deletion). This is independent of $z$ and of the number of points currently present in the sequence. This characteristic makes the algorithm applicable to real-world scenarios. ", "page_idx": 2}, {"type": "text", "text": "Notice that our data structure is continually storing an actual solution, so we can produce this solution at any time and do not need to specify additional query times (like, for example [9, 2, 18]). ", "page_idx": 2}, {"type": "text", "text": "Our main technical contribution is a novel combination of the sampling-based level data structure by Chan et al. [5] and the original greedy strategy by Charikar et al. [8] that enables us to achieve a much improved approximation guarantee compared to [5]. ", "page_idx": 2}, {"type": "text", "text": "Theorem 1.1. Let $(M,d)$ be a metric space and $\\epsilon>0$ be an accuracy parameter. The spread ratio $\\begin{array}{r}{\\Delta=\\frac{d_{\\mathrm{max}}}{d_{\\mathrm{min}}}}\\end{array}$ of all points ever inserted is assumed to be bounded. There exists a randomized fully dynamic algorithm that maintains a $k$ -center solution that allows up to $(1+\\epsilon)z$ many outliers on the current set of points. At every point in time, the current clustering is a $(4+\\epsilon)$ -approximation to an optimal solution for the $(k,z)$ -center problem with high probability. Upon insertion or deletion of $a$ point, the data structure is updated in amortized update time ${\\mathcal{O}}(\\epsilon^{-3}k^{6}\\log(k)\\log(\\Delta))$ . ", "page_idx": 2}, {"type": "text", "text": "To achieve this, we make use of a data structure that is described in Section 2.1 and maintained by the respective algorithms handling updates to the point set. The algorithm handling insertions is specified in Section 2.2, and the deletion algorithm is described in Section 2.3. Similar to the approach by Chan et al. [5], we maintain a hierarchical structure consisting of levels, each containing one cluster. Unlike their work, however, we only maintain $\\leq k$ levels. For the correct radius guess, the union of these levels directly gives the desired $(4+\\epsilon)$ -approximation. If a level violates certain properties, this and all higher levels are reclustered. As opposed to [5], we do not need to recluster every time a center gets deleted but only when a cluster does not contain enough points anymore. The algorithm handling the reclustering is described in Section 2.4. ", "page_idx": 2}, {"type": "text", "text": "Our analysis of the approximation ratio follows a similar argument as the proof of the 3-approximation for static $k$ -center with $z$ outliers as given by Charikar et al. [8] and works by charging points from chosen clusters to points in optimal clusters. We follow the same iterative charging method but extend their approach by maintaining a set of artificial outliers in order to adjust the argument to our algorithm specifically. ", "page_idx": 2}, {"type": "text", "text": "Note that we assume that a center can be any point from the underlying metric space, if the point was present in the data set once. Requiring that centers can only be placed at currently active points, we can achieve a 6-approximation. We formalize this in Lemma G.1. ", "page_idx": 2}, {"type": "text", "text": "1.2 Further Related work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The $k$ -center problem is very well understood; two 2-approximation algorithms for it are known [13, 15] and a matching lower bound is also known [16]. The $k$ -center problem with outliers was first studied and 3-approximated in [8]. In 2016, [4] gave a 2-approximation for this problem, but the algorithm is rather complex and less amenable to practical implementation. Charikar et al. [7] developed an elegant algorithm to maintain an 8-approximation for the vanilla $k$ -center problem in the streaming model. Streaming can be seen as a dynamic insertion-only model. McCutchen and Khuller [17] improved this algorithm to a $(2+\\epsilon)$ -approximation and also extended it to a $(4+\\epsilon)$ -approximation for $k$ -center with outliers. The $k$ -center problem with outliers has also been studied in the sliding window model [10, 18] that also bears some similarity with our setting. For bounded doubling dimension, [18] gave a $(3+\\epsilon)$ -approximation, but the algorithm for the general metric case is only stated as a $\\mathcal{O}(1)$ -approximation. ", "page_idx": 2}, {"type": "text", "text": "The fully dynamic $k$ -center problem without outliers in general metrics was studied by [1, 6, 11], and the best-known result is a $(2\\!+\\!\\epsilon)$ -approximation with an amortized update time of $\\mathcal{O}(k\\operatorname{polylog}(n,\\Delta))$ by Bateni et al. [1]. The same paper also shows that any algorithm that provably satisfies a non-trivial approximation guarantee needs $\\Omega(n k)$ queries to the distance function, i.e., the amortized update time is in $\\Omega(k)$ , making their algorithm close to tight. The fully dynamic $k$ -center problem without outliers was also studied for metrics with bounded doubling dimension [14, 19, 12] and in Euclidean space [20]. ", "page_idx": 3}, {"type": "text", "text": "1.3 Preliminaries ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In the preliminaries section, we provide an introduction to the $(k,z)$ -center problem and discuss the dynamic model used in our paper. ", "page_idx": 3}, {"type": "text", "text": "We study the $k$ -center clustering problem with $z$ outliers, formally defined as follows. ", "page_idx": 3}, {"type": "text", "text": "Definition 1.2 $((k,z)$ -center clustering). Let $P$ be a point set in a metric space $(M,d)$ and let $k,z\\in\\mathbb{N}$ be two parameters. The goal is to compute a set $C\\subseteq M$ of size at most $k$ , such that the maximum distance of all but at most $z$ points to their nearest $c\\in C$ is minimized. That is, find $C$ such that $\\begin{array}{r}{\\operatorname*{min}_{Z\\subseteq P,|Z|\\leq z}\\operatorname*{max}_{x\\in P\\setminus Z}\\operatorname*{min}_{c\\in C}d(x,c)}\\end{array}$ , with $|C|\\le k$ . ", "page_idx": 3}, {"type": "text", "text": "We define $d_{\\mathrm{min}}$ and $d_{\\mathrm{max}}$ as the minimum and maximum distances, respectively, between any two points ever inserted. The ratio $\\begin{array}{r}{\\Delta=\\frac{d_{\\mathrm{max}}}{d_{\\mathrm{min}}}}\\end{array}$ is referred to as the spread ratio, and is assumed to be bounded. Let $r_{\\mathrm{OPT}}$ be the optimal radius for the $(k,z)$ -center clustering problem of a point set $P$ . We also define the ball $\\mathcal{B}_{P}(c,\\bar{r})=\\{p\\in P:d(c,\\dot{p})\\leq r\\}$ to be the set of points in $P$ that are within distance $r$ from $c$ . If $P$ is clear from the context, we may drop $P$ from the definition $\\ensuremath{\\boldsymbol{B}}_{P}(c,r)$ . For a non-negative integer $m$ , we denote $\\{1,\\cdot\\cdot\\cdot,m\\}$ by $[m]$ . ", "page_idx": 3}, {"type": "text", "text": "In the version of the $(k,z)$ -center clustering problem that we consider, we allow $(1+\\epsilon)z$ outliers instead of strictly $z$ . Furthermore, we require centers of clusters to be in the metric space $(M,d)$ , but they need not be currently present in $P$ . This is referred to as the non-discrete version of the problem. Lemma G.1 in the Appendix shows how our data structure can also support the discrete version of the problem and provides a 6-approximation solution for it. ", "page_idx": 3}, {"type": "text", "text": "Fully dynamic model. We consider the $(k,z)$ -center clustering problem in the fully dynamic model against an adaptive adversary. In this model, we start with an empty point set, $P=\\emptyset$ , and process a sequence of operations determined by the adversary. We assume the adversary does not know the random bits chosen by our algorithm; however, it can observe the algorithm\u2019s output and adapt its responses in real time (unlike an oblivious adversary, which fixes a sequence of operations in advance). Each operation can be either an insertion, where a point from the metric space $(M,d)$ is added to $P$ , or a deletion, where a point currently in $P$ is removed. We assume only points currently in $P$ may be deleted. Let $P^{t}$ represent the point set $P$ after $t$ operations. In other words, $P^{t}$ consists of all points in $(M,d)$ that have been inserted but not deleted after $t$ operations. ", "page_idx": 3}, {"type": "text", "text": "2 Algorithm ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To explain the main ideas of our algorithm, we start by describing an iterative offline algorithm, which gives a $(4+\\epsilon)$ approximation for the $(k,z)$ -center clustering problem. ", "page_idx": 3}, {"type": "text", "text": "Offline algorithm. We start with a point set $P_{1}=P$ , $\\epsilon>0$ and parameters $k,z,r\\in\\mathbb{N}$ . Here, $r$ is a fixed guess for the optimal radius. For each iteration $i$ , we sample a set of $S_{i}\\subset P_{i}$ of $|S_{i}|=\\psi\\epsilon^{-1}\\bar{k}^{2}\\log k$ points with replacement, uniformly at random. Here, $\\psi\\ge6\\beta$ is a constant, where $\\beta>\\alpha\\geq1$ are constants. We find a point $c_{i}\\in S$ , such that $B(c_{i},2r)$ covers at least $\\phi$ points from $P_{i}$ , where $\\phi$ is some threshold to be defined later. We then create a new cluster $C_{i}=\\mathcal{B}_{P_{i}}(c_{i},4r)$ , let $P_{i+1}=P_{i}\\backslash C_{i}$ , and continue to the next iteration. We stop once $k$ clusters have been created or $P_{i}=\\emptyset$ . Let $\\lambda$ be a random variable representing the number of iterations we complete, where $\\lambda$ can be at most $k$ . After we have done $\\lambda$ iterations we have computed clusters $C_{1},C_{2},...,C_{\\lambda}$ with corresponding centers $c_{1},c_{2},...,c_{\\lambda}$ . The points that are not covered by the union of these clusters will be the set of outliers that our algorithm reports. If the set of outliers is at most $(1+\\epsilon)z$ points, we can report a solution for the $(k,z)$ -center clustering problem. This offilne algorithm will be used as a sub-routine for the fully dynamic algorithm, which will be explained in Section 2.4. The definition of $\\phi$ and the pseudocode of the algorithm will also be given in that section. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Guesses for unknown rOPT. Since the optimal $r$ is usually not known, we can run the algorithm for all $r\\,\\in\\,\\mathcal{R}\\,=\\,\\{(1+\\epsilon)^{i}\\,:\\,d_{\\operatorname*{min}}\\,\\leq\\,(1\\,\\dot{+}\\,\\epsilon)^{i}\\,\\leq\\,(1+\\epsilon)\\cdot d_{\\operatorname*{max}},i\\,\\in\\,\\mathbb{N}\\}$ . We then choose the smallest $r\\in\\mathcal{R}$ such that all but at most $(1+\\epsilon)z$ points are covered. We will show that this gives a $(4+\\epsilon)$ -approximation. ", "page_idx": 4}, {"type": "image", "img_path": "OtYCp1yfbX/tmp/ca65f82f3d69030e84d0a65f06b3738876544ccd1b191838ce897ad145fe841c.jpg", "img_caption": ["Figure 1: The $\\lambda$ levels constructed by our offline algorithm. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Leveling. We can visualize the output of this algorithm as (at most) $\\lambda+1$ levels, where the first $\\lambda\\leq k$ levels each represent a cluster, and the last level represents the outliers (See Figure 1). More precisely, level $i$ represents cluster $C_{i}$ with center $c_{i}$ , which was created in the $i^{t h}$ iteration. Note that the construction of level $i$ only considers the points in $P_{i}$ , which does not include points in the lower levels $\\{1,2,...,i-1\\}$ . ", "page_idx": 4}, {"type": "text", "text": "The main idea for the fully dynamic algorithm is to maintain each level $i$ for $i\\in[\\lambda+1]$ under an arbitrary number of insertions and deletions by updating the cluster $C_{i}$ when necessary. In the rest of the paper, we use subscript $i$ to refer to the level and superscript $t$ to refer to the time. For example, $P_{i}^{t}$ refers to the points present in level $i$ at time $t$ , and $r_{\\mathrm{OPT}}^{t}$ refers to the optimal radius at time $t$ . If the level or time is clear from the context, these may be omitted. For instance, if we define a situation at time $t^{\\prime}$ , we will not repeat the superscript $t^{\\prime}$ on every term. ", "page_idx": 4}, {"type": "text", "text": "2.1 Data structure and invariants ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We will construct a clustering $C_{1},\\dots,C_{\\lambda}$ of the current set of points $P^{t}$ that we store in the data structure $\\mathcal{D}_{r}$ introduced by [6]. If $t$ is clear from the context, we will just write $P$ for $P^{t}$ . As we do not know the current optimal radius, we will maintain one data structure $\\mathcal{D}_{r}$ for every choice of $r\\,\\in\\,\\mathcal{R}\\,=\\,\\{(1+\\epsilon)^{i}:\\hat{d}_{\\operatorname*{min}}\\,\\le\\,(1+\\epsilon)^{i}\\,\\le\\,d_{\\operatorname*{max}},i\\,\\in\\,\\mathbb{N}\\}$ . Every data structure consists of up to $\\lambda\\leq k$ cluster-levels, with each level $i$ containing a cluster $C_{i}$ . In level $i$ , we keep track of the set of the points $P_{i}$ currently not covered, i. e., $P_{i}=P\\backslash\\bigcup_{j<i}C_{j}$ . Level $\\lambda+1$ contains the outliers, i.e., $\\mathcal{Z}_{r}=P\\setminus\\cup_{i\\leq\\lambda}C_{i}$ . We denote $n_{i}=|P_{i}|$ . By $B(p,r)$ we denote the ball with radius $r$ centered at $p$ , and $\\mathcal{B}_{A}(p,r)=\\mathcal{B}(p,r)\\cap A$ for $A\\subseteq P$ . ", "page_idx": 4}, {"type": "text", "text": "Let $\\alpha\\geq1$ . The data structure $\\mathcal{D}_{r}$ consists of the following components: ", "page_idx": 4}, {"type": "text", "text": "Let $\\alpha\\geq1$ be a constant. We have the following invariants for $\\mathcal{D}_{r}$ . ", "page_idx": 4}, {"type": "table", "img_path": "OtYCp1yfbX/tmp/dcc67fa5e4fe528751e83d843e13fe9f1d14295dfa00ffe793257b6104068f84.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "2.2 Insertion ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "When a new point $p$ is inserted at time $t$ , Procedure 3 in the appendix is executed. As input, we have $\\mathcal{D}_{r}$ at time $t$ , $p\\in(M,d)$ , $\\epsilon>0$ and $z,k\\in\\mathbb{N}$ . First, we check whether $p$ is inside one of the existing clusters, in which case we can add $p$ to such a cluster. Otherwise, we add $p$ to ${\\mathcal{Z}}_{r}$ . Next, we check if the dense invariant is maintained after the insertion of the new point. Generally, the dense invariant can be broken in two ways: ", "page_idx": 5}, {"type": "text", "text": "$\\pmb{\\mathrm{\\Omega}}$ If the insertion of a new point results in more than $(1+\\epsilon)z$ outliers, then Lemma 3.7 shows that the dense invariant is no longer valid at some level $i$ .   \n$\\pmb{\\varphi}$ If the addition of a point $p$ to a cluster $C_{i}$ causes the dense invariant to be violated at some lower level j < i, that occurs because ntj+ $n_{j}^{t+1}=n_{j}^{t}+1$ . ", "page_idx": 5}, {"type": "text", "text": "If the dense invariant is broken for some level $i$ , we recluster levels $i,...,\\lambda$ with $\\lambda\\leq k$ by invoking Procedure 5 as a sub-routine. Observe that if there are multiple levels $i$ where the dense invariant is broken, we choose the lowest one. ", "page_idx": 5}, {"type": "text", "text": "2.3 Deletion ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "For the deletion of a point $p$ at time $t$ , Procedure 4 in the appendix is executed. The input is $\\mathcal{D}_{r}$ at time $t$ , $p\\in(M,d)$ , $\\epsilon>0$ and $z,k\\in\\mathbb{N}$ . First we check if $p$ is an outlier, in which case we remove $p$ from ${\\mathcal{Z}}_{r}$ . If $p$ is either a center or a point in a cluster, we find cluster $C_{i}$ which contains $p$ . If cluster $C_{i}=\\ensuremath{\\mathcal{B}}(c_{i},4r)$ contains at least $\\begin{array}{r}{\\operatorname*{min}\\left(z+1,\\frac{|P_{i}|-z}{k-i+1}-\\frac{\\epsilon z}{\\alpha k}\\right)}\\end{array}$ points, we do not re-cluster and simply remove $p$ from $C_{i}$ . Note that the underlying point set exists in the metric space $(M,d)$ . If the center $c_{i}$ of a cluster $C_{i}$ is deleted, provided that the dense invariant remains valid, we can continue to utilize $c_{i}$ as the center of cluster $C_{i}$ , given our knowledge that the point $c_{i}$ is located within the metric space $(M,d)$ . In Lemma G.1, we explain how we can still obtain a 6-approximation if centers have to come from the current point set. If |Ci| < min z + 1, |kP\u2212i|i\u2212+z1 \u2212\u03b1\u03f5zk after the deletion of $p$ , then it also follows that $\\begin{array}{r}{|B(c_{i},2r)\\cap P_{i}|<\\operatorname*{min}\\left(z+1,\\frac{|P_{i}|-z}{k-i+1}-\\frac{\\epsilon z}{\\alpha k}\\right)}\\end{array}$ , i.e., the dense invariant is violated on this level. In this case, we want to redistribute the points in $P_{i}$ such that the levels $i,\\dots,\\lambda$ fulflil the dense invariant. Deletion of a point in level $i$ does not violate the invariants at levels $1,\\dots,i-1$ . We re-cluster the points in $(\\cup_{i\\leq j\\leq k}C_{j})\\cup\\mathcal{Z}_{r}$ using Procedure 5. We finish by updating $\\mathcal{D}_{r}$ . ", "page_idx": 5}, {"type": "text", "text": "2.4 Clustering sub-routine ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The clustering sub-routine is the offline algorithm that was described at the beginning of Section 2. The pseudocode of this sub-routine is shown in Procedure 5. We use it to iteratively build the levels of data structure $\\mathcal{D}_{r}$ . Two cases are distinguished based on whether $z$ is small or large compared to the number of points in level $i$ . In line 4, $\\psi$ is a constant with $\\psi\\ge6\\beta$ , where $\\beta>\\alpha$ , and $\\alpha$ is the constant used in the dense invariant. The threshold $\\phi$ that was introduced in Section 2 is different depending on the size of $z$ compared to $n_{i}$ . If $\\begin{array}{r}{z+1\\leq\\frac{n_{i}-z}{4(k-i+1)}}\\end{array}$ , then 2(kni\u2212\u2212i+z1) nd if z + 1 >4(kni\u2212\u2212i+z1), a then $\\begin{array}{r}{\\phi=\\frac{n_{i}-z}{k-i+1}-\\frac{\\epsilon z}{\\beta k}}\\end{array}$ . If we find multiple points $p^{*}$ in line 6 or 12 we choose one arbitrarily. ", "page_idx": 5}, {"type": "text", "text": "2.5 All aspects combined ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The first step will be to initialize all $\\mathcal{D}_{r}$ such that $\\mathcal{F}_{r},\\mathcal{L}_{r},\\mathcal{Z}_{r}=\\emptyset$ . Then, the algorithm waits for an insertion or deletion operation. If a point is inserted, Procedure 3 is executed, and if a point is deleted, Procedure 4 is executed. The algorithm is ran simultaneously for all $r\\in\\mathcal{R}=\\{(1\\bar{+}\\epsilon)^{i}:d_{\\operatorname*{min}}\\leq$ $(1+\\epsilon)^{i}\\le(1+\\epsilon)\\cdot d_{\\operatorname*{max}},i\\in\\mathbb{N}\\}$ . ", "page_idx": 5}, {"type": "text", "text": "In the next sections, it will be shown that a $(4+\\epsilon)$ -approximation is given by the clustering ${\\mathcal{L}}_{r}$ with $r\\in\\mathcal{R}$ the smallest $r$ for which $\\lvert\\mathcal{Z}_{r}\\rvert\\le(1+\\epsilon)z$ . ", "page_idx": 5}, {"type": "text", "text": "3 Analysis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We begin our analysis of the dynamic algorithm by introducing the concept of a dense cluster and specifying the criteria for a valid solution. ", "page_idx": 5}, {"type": "text", "text": "Definition 3.1 (Dense cluster). A cluster $B_{P_{i}}(c_{i},4r)$ is dense with respect to point set $P_{i}$ if it satisfies the dense invariant. That is, $\\begin{array}{r}{|\\mathcal{B}_{P_{i}}(c_{i},2r)|\\ge\\operatorname*{min}\\left(z+1,\\frac{n_{i}-z}{k-i+1}-\\frac{\\epsilon z}{\\alpha k}\\right)}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "Observe that for the dense invariant, we consider the ball $B_{P_{i}}(c_{i},2r)$ with a radius of $2r$ . However, a cluster (as seen in line 18 of Procedure5) corresponds to the points within the ball $B_{P_{i}}(c_{i},4r)$ , which has a radius of $4r$ . ", "page_idx": 6}, {"type": "text", "text": "Definition 3.2 (Valid solution). A solution $\\mathcal{B}_{P_{1}}(c_{i},4r)\\cup\\cdot\\cdot\\cdot\\cup\\mathcal{B}_{P_{\\lambda}}(c_{\\lambda},4r)$ with $\\lambda\\leq k$ for point set $P_{1}$ is valid if it covers all but at most $(1+\\epsilon)z$ points from $P_{1}$ . We refer to each cluster $B_{P_{j}}(c_{j},4r)$ for $j\\in\\{i,\\ldots,\\lambda\\}$ of a valid solution as a valid cluster. ", "page_idx": 6}, {"type": "text", "text": "Lemma 3.3 (Running time offline algorithm). The running time of the offline algorithm, shown in Procedure 5, is $\\mathcal{O}(n\\bar{\\epsilon}^{-1}k^{3}\\log(k))$ , where $n=|P|$ . ", "page_idx": 6}, {"type": "text", "text": "Proof. In each iteration of the while-loop, we need to compute $|B(p,2r)|$ for all $p\\,\\in\\,S_{i}$ . Since $|S_{i}|=\\mathcal{O}(\\epsilon^{-1}k^{2}\\log k)$ , this takes $O(n\\epsilon^{-\\bar{1}}k^{2}\\log k)$ time. There can be at most $k$ iterations of the while-loop and hence the total running time is $\\breve{O(n\\epsilon^{-1}k^{3}\\log k)}$ . \u53e3 ", "page_idx": 6}, {"type": "text", "text": "3.1 Maintaining invariants ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In Supplementary Section B, we prove the following three lemmas. Lemmas 3.4 and 3.5 show that the dense and level invariants are maintained during the insertion or deletion of a point, respectively. Additionally, Lemma 3.6 shows that both invariants are maintained when invoking Procedure 5 as a subroutine upon the insertion or deletion of an arbitrary point with high probability if $r\\geq r_{\\mathrm{OPT}}$ . The case where $r<r_{\\mathrm{OPT}}$ is considered in Lemmas E.1 and E.2. ", "page_idx": 6}, {"type": "text", "text": "Lemma 3.4 (Procedure 3 maintains invariants). Assume that at time $t$ , we have point set $P^{t}$ , data structure $\\boldsymbol{\\mathcal{D}}_{r}\\,=\\,(\\mathcal{F}_{r},\\mathcal{L}_{r},\\mathcal{Z}_{r})$ . We assume that the level and dense invariants hold at time $t$ and $r\\geq r_{\\mathrm{OPT}}^{t+1}$ d. eAnts te hien vstaarrita notfs t istmilel $t+1$ , wwiteh  ipnrsoebrta bpioliitnyt $p$ iuf siPnrgo cPerdoucree rwe a3s.  nAoftt ecra tllheed i nasnedr tiwoitnh,  tthhee $^{\\,l}$ $^{5}$ probability of at least 1 \u22122(ek\u03a8 \u2212loig+ k1 ), where \u03a8 \u22651 if Procedure 5 was not called. ", "page_idx": 6}, {"type": "text", "text": "Lemma 3.5 (Procedure 4 maintains invariants). Assume that at time $t$ , we have point set $P^{t}$ , instance $\\mathcal{D}_{r}=(\\mathcal{F}_{r},\\mathcal{L}_{r},\\mathcal{Z}_{r}),$ , parameters $k,z\\in\\mathbb{N}$ and $\\epsilon>0$ . We assume that the level and dense invariants hPorlodc eadtu trie m4e. $t$ ftaenrd t $r\\,\\geq\\,r_{\\mathrm{OPT}}^{t+1}$ ,. t hAet l tehveel  satnardt  doefn tsie mien $t+1$ ,t s wheo ldd elweitteh  parno baarbbiiltirtayr 1y  ipf oPirnot $p$ duusrien g5 was not called, and with probability 1 \u22122(ek\u03a8 \u2212loig+ k1) with $\\Psi\\geq1$ if Procedure 5 was called. ", "page_idx": 6}, {"type": "text", "text": "Lemma 3.6 (Procedure 5 maintains invariants with high probability). Suppose the level and dense invariants hold for all levels $j<i$ and we call Procedure $^{5}$ on $P_{i}$ as the result of an insertion or deletion. Let $\\lambda\\leq k$ be a random variable representing the number of levels we have after completing Procedure 5. If $r\\geq r_{\\mathrm{OPT}}$ , Procedure 5 maintains the level and dense invariants for all levels $j$ with i \u2264j \u2264\u03bb with probability at least 1 \u22122(ek\u03a8 \u2212loig+ k1 ), with \u03a8 \u22651. ", "page_idx": 6}, {"type": "text", "text": "3.2 Approximation guarantee ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Next, we prove that if the invariants hold, our data structure $\\boldsymbol{\\mathcal{D}}_{r}\\,=\\,(\\mathcal{F}_{r},\\mathcal{L}_{r},\\mathcal{Z}_{r})$ contains a valid solution to the $k$ -center with $z$ outliers problem. Furthermore, if $r<(1+\\epsilon)r_{\\mathrm{OPT}}$ , $\\mathcal{D}_{r}$ provides a $(4+\\epsilon)$ -approximation. Without loss of generality, for simplicity we assume that $\\mathcal{D}_{r}$ contains $k$ levels. The proof of Lemma 3.7 follows the structure of the proof given in [8]. This proof uses the greediness of the algorithm to argue that the balls in the solution cover enough points to charge to. Here, we have to modify the proof to work with the dense invariant. ", "page_idx": 6}, {"type": "text", "text": "Lemma 3.7 (Approximation guarantee). Let $P$ be the current point set. Assume that the level invariant and the dense invariant hold for all levels $i\\leq k$ . Then we have the following guarantees: ", "page_idx": 6}, {"type": "text", "text": "$\\textcircled{1}$ Valid solution: $I f_{O P T}\\leq r_{+}$ , then $B(c_{1},4r)\\cup...\\cup B(c_{k},4r)$ covers all but at most $(1+\\frac{\\epsilon}{\\alpha})z$ outliers in $P$ . $\\circledcirc$ Approximate solution: I $\\lceil f r_{O P T}\\leq r<(1+\\epsilon)r_{O P T}$ , then $\\mathcal{B}(c_{1},4r)\\cup...\\cup\\mathcal{B}(c_{k},4r)$ gives a $(4+\\epsilon)$ -approximation for the $k$ -center with $z$ outliers problem on $P$ . ", "page_idx": 6}, {"type": "text", "text": "Proof. Recall that the level invariant states that for all $i$ , we have $P_{i+1}=P_{i}\\backslash B(c_{i},4r)$ and the dense invariant states that for all $i$ , we have that $\\begin{array}{r}{|\\mathcal{B}(c_{i},2r)\\cap P_{i}|\\ge\\operatorname*{min}(z+1,\\frac{n_{i}-z}{k-i+1}-\\frac{\\epsilon z}{\\alpha k})}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "Assume that in the optimal solution, we have balls $O_{1},\\ldots,O_{k}$ with radius $\\le\\,r_{\\mathrm{OPT}}$ . The union of these balls covers all but $z$ points in $P$ . In order to prove Part $\\textcircled{1}$ , we aim to charge all but at most $\\frac{\\epsilon}{\\alpha}z$ points of the optimal solution to points in our solution $\\mathcal{B}(c_{1},4r)\\cup...\\cup\\mathcal{B}(c_{k},4r)$ . We prove this by induction, and Part $\\circledcirc$ will then follow easily. In order to construct the charging argument, we need to argue that there are enough points in our solution to charge the points in the optimal balls. To this end, we construct modified optimal balls $O_{1}^{\\prime},\\ldots,O_{k}^{\\prime}$ , where $O_{i}^{\\prime}\\subseteq O_{i}$ for every $i\\leq k$ . ", "page_idx": 7}, {"type": "text", "text": "For the base case, we have $O_{1}^{\\prime}=O_{1},\\cdot\\cdot\\cdot,O_{k}^{\\prime}=O_{k}$ . We will show that we can order the modified balls in such a way, that at the end of time step $i$ , all but at most $\\frac{\\epsilon z}{\\alpha k}\\cdot i$ points from the first $i$ modified balls are charged to distinct points in $\\mathcal{B}(c_{1},4r)\\cup...\\cup\\mathcal{B}(c_{i},4r)$ . This will allow us to prove that our solution covers at least as many points as the optimal solution. ", "page_idx": 7}, {"type": "text", "text": "Assume that all but at most $\\begin{array}{r}{\\frac{\\epsilon z}{\\alpha k}\\cdot\\left(i-1\\right)}\\end{array}$ points in the first $i-1$ modified balls $O_{1}^{\\prime}\\cup O_{2}^{\\prime}\\cup...\\cup O_{i-1}^{\\prime}$ have been charged to distinct points in $\\mathcal{B}(c_{1},4r)\\cup...\\cup\\mathcal{B}(c_{i-1},4r)$ and consider iteration $i$ . We distinguish two cases, namely if $\\mathcal{B}(c_{1},2r)\\cup...\\cup B(c_{i},2r)$ intersects one of the remaining modified balls, or if it does not. The charging argument for each case proceeds as follows: ", "page_idx": 7}, {"type": "text", "text": "Case 1. Case 1 is when $\\mathcal{B}(c_{1},2r)\\cup...\\cup\\mathcal{B}(c_{i},2r)$ intersects a remaining modified ball, call this ball $O_{i}^{\\prime}$ . Note that $O_{i}^{\\prime}$ will be covered entirely by $\\mathcal{B}(c_{1},4r)\\cup...\\cup B(c_{i},4r)$ , since $r\\geq r_{\\mathrm{OPT}}$ . Hence, we charge the points of $O_{i}^{\\prime}$ to themselves and mark these points as covered. (See case 1 in Figure 2.) We call this charging rule $I.$ Since the modified balls are disjoint 1, any point can be charged only once (to itself) by this rule. Next, we update $O_{1}^{\\prime},O_{2}^{\\prime},\\ldots,O_{k}^{\\prime}$ . ", "page_idx": 7}, {"type": "image", "img_path": "OtYCp1yfbX/tmp/cf9636262c66e2a9248bb093ae2cb89b07d9d48a119da6f072dee8571fed3c94.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 2: Visualization of charging rules I and II. For case 1, we see that $B(c_{i},2r)$ and $O_{i}$ intersect. As a result, $B(c_{i},4r)$ covers all points in $O_{i}$ . Points $p,q$ are in set $Z_{i}^{c}$ . For case 2, the balls $B(c_{1},2r)$ and $B(c_{2},2r)$ do not intersect the optimal cluster $O_{i}^{\\prime}$ . The crossed points in $O_{2}^{\\prime}$ are charged to black squared in $B(c_{2},2r)$ . The circle points in $O_{2}^{\\prime}$ are not charged to any point. Points $a,b,c$ are in $Z_{i}^{d}$ . ", "page_idx": 7}, {"type": "text", "text": "We maintain two sets of credit points that we save and may use for future charging purposes. First, the set $Z_{i}^{c}=\\mathcal{B}(c_{i},4r)\\backslash(O_{1}^{\\prime}\\cup\\overset{.}{\\dots}.\\cup O_{k}^{\\prime})$ which is the set of points covered by $B(c_{i},4r)$ that are not covered by $O_{1}^{\\prime}\\cup O_{2}^{\\prime}\\cup...\\cup O_{k}^{\\prime}$ . We let $z_{i}^{c}=|Z_{i}^{c}|$ be the number of such points. In Figure 2, points $p,q$ are such points. Observe that no point is charged to points in $Z_{i}^{c}$ , allowing us to use them later. We refer to these points as credit points. ", "page_idx": 7}, {"type": "text", "text": "There may also be previous modified balls $O_{j}^{\\prime}$ , with $j<i$ that were considered in case 2 and are still present in $P_{i}$ . More specifically, let $Z_{i}^{d}$ be the set of (there may exist) points in $O_{j}^{\\prime}\\cap B(c_{j},4r)$ that have been charged to distinct points in $B(c_{j},4r)$ (See the discussion of case 2, below). For example, points $a,b,c$ in case 2 of Figure 2. Let $z_{i}^{d}=|Z_{i}^{d}|$ be the number of such points. Since no points are charged to points in $Z_{i}^{d}$ , we save them as credit points for future charging purposes. We now update $O_{1}^{\\prime},O_{2}^{\\prime},\\ldots,O_{k}^{\\prime}$ as follows: $O_{1}^{\\prime},\\ldots,O_{i}^{\\prime}$ stay the same and we define $z_{i}^{c}+z_{i}^{d}$ artificial outliers in $(O_{i+1}^{\\prime}\\cup...\\cup O_{k}^{\\prime})\\ncap P_{i+1}$ . ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Case 2. Case 2 occurs if $\\mathcal{B}(c_{1},2r)\\cup...\\cup\\mathcal{B}(c_{i},2r)$ does not intersect any of the remaining modified balls. See Figure 2. Let $O_{i}^{\\prime}$ be one of the remaining modified balls covering $\\leq\\frac{n_{i}-z}{k-i+1}$ points. Note that for finding $O_{i}^{\\prime}$ we do not count points that have been defined as artificial outliers, since these artificial outliers are already covered by balls of radius $4r$ in previous levels and we do not cover them by the remaining modified balls. We prove in Lemma 3.8 that such a ball $O_{i}^{\\prime}$ exists. ", "page_idx": 8}, {"type": "text", "text": "Using the assumption of this lemma that the dense invariant holds, we show in Lemma 3.9 that $\\begin{array}{r}{|B(c_{i},2r)|\\ge\\frac{n_{i}-z}{k-i+1}-\\frac{\\epsilon z}{\\alpha k}}\\end{array}$ . In this way, we find an upper bound for the number of points in $O_{i}^{\\prime}$ and a lower bound for the number of points in $B(c_{i},2r)$ . Now, we charge all but at most $\\frac{\\epsilon z}{\\alpha k}$ points of $O_{i}^{\\prime}$ to the points of $B(c_{i},2r)$ and mark these points as charged. We call this charging rule $I I.$ . Note the difference between charging argument in Case 1 and 2. In Case 1, we charge the points in $O_{i}^{\\prime}$ to themselves, but in Case 2, we charge them to points covered by $B(c_{i},2r)$ . Recall that $B(c_{i},2r)$ and $O_{i}^{\\prime}$ are disjoint. ", "page_idx": 8}, {"type": "text", "text": "Observe that points in balls $O_{i+1}^{\\prime},\\cdots,O_{k}^{\\prime}$ will not be charged to points in $B(c_{i},2r)$ . Indeed, points in balls $O_{i+1}^{\\prime},\\cdot\\cdot\\cdot,O_{k}^{\\prime}$ are charged to the balls that our algorithms find either using rule I or rule II. However, this will not happen based on rule I since $\\bar{B(c_{i},2r)}$ is disjoint from all balls $O_{i+1}^{\\prime},\\cdot\\cdot\\cdot,O_{k}^{\\prime}$ . Moreover, rule II cannot also be applied since among balls $O_{i}^{\\prime};O_{i+1}^{\\prime},\\cdot\\cdot\\cdot,O_{k}^{\\prime}$ , we already charged the points in $O_{i}^{\\prime}$ to $B(c_{i},2r)$ and points in $O_{i+1}^{\\prime},\\cdots\\,,O_{k}^{\\prime}$ will be charged to points in balls $\\mathcal{B}(c_{i+1},2r),\\cdot\\cdot\\cdot\\mathbf{\\Omega},\\mathcal{B}(c_{\\lambda},2\\bar{r})$ . ", "page_idx": 8}, {"type": "text", "text": "Next, we consider how we update $O_{1}^{\\prime},O_{2}^{\\prime},\\ldots,O_{k}^{\\prime}$ . Similar to case 1, we define the size of two sets of credits points. First, the variable $z_{i}^{c}=|\\mathcal{B}(c_{i},4r)\\backslash\\!\\left\\backslash(O_{1}^{\\prime}\\cup...\\cup O_{k}^{\\prime})\\right|$ that corresponds to the number of points covered by $B(c_{i},4r)$ that are not covered by $O_{1}^{\\prime}\\cup O_{2}^{\\prime}\\cup...\\cup O_{k}^{\\prime}$ . There are $z_{i}^{c}-|O_{i}^{\\prime}|$ points from $B(c_{i},4r)$ that are free (i.e., no point is charged to) and can still be charged. We consider these points as credits that we save and may use for future charging purposes. ", "page_idx": 8}, {"type": "text", "text": "There may also be previous modified balls $O_{j}^{\\prime}$ , with $j<i$ that were considered in case 2 and are still present in $P_{i}$ . More specifically, let $Z_{i}^{d}$ be the set of (there may exist) points in $O_{j}^{\\prime}\\cap B(c_{j},4r)$ that have been charged to distinct points in $B(c_{j},4r)$ . For example, points $a,b,c$ in case 2 of Figure 2. Let $z_{i}^{d}=|Z_{i}^{d}|$ be the number of such points. Since no points are charged to points in $Z_{i}^{d}$ , we save them as credit points for future charging purposes. We now update $O_{1}^{\\prime},O_{2}^{\\prime},\\ldots,O_{k}^{\\prime}$ as follows: $O_{1}^{\\prime},O_{2}^{\\prime},\\dots,O_{i}^{\\prime}$ stays the same and we define $\\left(z_{i}^{c}-|{\\cal O}_{i}^{\\prime}|\\right)+z_{i}^{d}$ artificial outliers in $(O_{i+1}^{\\prime}\\cup...\\cup O_{k}^{\\prime})\\cap P_{i+1}$ . ", "page_idx": 8}, {"type": "text", "text": "Now, for both cases, we apply charging rule I to any points in the remaining modified balls $O_{i+1}^{\\prime}\\cup...\\cup$ $O_{k}^{\\prime}$ that are inside $B(c_{i},4r)$ . These points are then marked as covered. See Figure 3 for an example. Note that these points will not be in $P_{i+1}=P_{i}\\backslash\\mathcal{B}(c_{i},4r)$ . We assumed that $O_{1}\\cup O_{2}\\cup...\\cup O_{k}$ covers all but at most $z$ points. After the charging has taken place and the modified clustering $O_{1}^{\\prime}\\cup O_{2}^{\\prime}\\cup...\\cup O_{k}^{\\prime}$ has been constructed, we have that $|O_{1}^{\\prime}\\cup O_{2}^{\\prime}\\cup\\bar{\\ldots}\\cup O_{k}^{\\prime}|=n-z-\\sum_{j}$ in case $\\mathbf{\\Psi}_{1}(z_{j}^{c}\\bar{+}$ $z_{j}^{d})-\\sum_{j}$ in case $_2(z_{j}^{c}-|O_{j}^{\\prime}|+z_{j}^{d})$ . ", "page_idx": 8}, {"type": "text", "text": "Note that $|O_{j}^{\\prime}|$ refers to the number of points covered by the modified ball $O_{j}^{\\prime}$ at the time of iteration $j$ . Then, by the way we have charged $O_{1}^{\\prime}\\cup...\\cup O_{k}^{\\prime}$ to our solution $\\mathcal{B}(c_{1},4\\bar{r})\\cup...\\cup\\mathcal{B}(c_{k},4r)$ , we obtain $\\begin{array}{r}{|\\mathcal{B}(c_{1},4r)\\cup...\\cup\\mathcal{B}(c_{k},4r)|\\ge|O_{1}^{\\prime}\\cup O_{2}^{\\prime}\\cup...\\cup O_{k}^{\\prime}|-\\frac{\\epsilon z}{\\alpha}+\\sum_{i}\\mathcal{B}(c_{i},4r)|}\\end{array}$ in case $\\phantom{}_{1}\\bigl(z_{j}^{c}+z_{j}^{d}\\bigr)$ ", "page_idx": 8}, {"type": "text", "text": "$+\\textstyle\\sum_{j}$ in case $\\begin{array}{r}{{_2}(z_{j}^{c}-|O_{j}^{\\prime}|+z_{j}^{d})=n-(1+\\frac{\\epsilon}{\\alpha})z}\\end{array}$ . This concludes the proof of Part $\\textcircled{1}$ of the lemma. It follows easily that when $r_{\\mathrm{OPT}}\\leq r<(1+\\epsilon)r_{\\mathrm{OPT}}$ , Part $\\circledcirc$ also holds, which completes the proof of this lemma. \u53e3 ", "page_idx": 8}, {"type": "text", "text": "Lemma 3.8. Let i be an iteration of the charging argument above such that we are in case 2. This means that $\\mathcal{B}(c_{1},2r)\\cup...\\cup\\mathcal{B}(c_{i},2r)$ does not intersect any of the remaining modified balls. Then, there must be a remaining modified ball covering $\\leq\\frac{n_{i}-z}{k-i+1}$ points. ", "page_idx": 8}, {"type": "text", "text": "For the proof of Lemma 3.8, see Supplementary Section C. ", "page_idx": 8}, {"type": "text", "text": "Lemma 3.9 (Coverage of $B(c_{i},2r);$ ). When we are in case 2 of the charging argument for some iteration i, we must have that |B(ci, 2r)| \u2265kn\u2212ii\u2212+z1 \u2212\u03b1\u03f5zk. ", "page_idx": 9}, {"type": "text", "text": "For the proof of Lemma 3.9, see Supplementary Section C. ", "page_idx": 9}, {"type": "text", "text": "3.3 Duration of dense clusters ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "After running Procedure 5 as a subroutine, we know that each cluster $C_{i}$ covers at least $\\phi$ points. More specifically, for each level $i$ , if $\\begin{array}{r}{z\\!+\\!1\\leq\\frac{n_{i}-z}{4(k-i+1)}}\\end{array}$ , then $\\begin{array}{r}{|B(c_{i},2r)|\\ge\\frac{n_{i}-z}{2(k-i+1)}}\\end{array}$ and if $\\begin{array}{r}{z\\!+\\!1>\\frac{n_{i}-z}{4(k-i+1)}}\\end{array}$ , then $\\begin{array}{r}{|B(c_{i},2r)|\\ge\\frac{n_{i}-z}{k-i+1}-\\frac{\\epsilon z}{\\beta k}}\\end{array}$ . The following two lemmas show that when $C_{i}$ satisfies one of these two constraints, it will be dense for a significant number of arbitrary operations. This will lead to the amortized update time being independent of $n$ . The remaining lemmas are split into two cases, corresponding to the two cases in Procedure 5. ", "page_idx": 9}, {"type": "text", "text": "Lemma 3.10 (Duration of dense cluster for $z$ is small). Assume that we are currently at time $t$ . Let us consider a level i in which z + 1 \u2264 4(kni\u2212\u2212i+z1). Let $p\\,=\\,a r g\\,m a x_{p^{\\prime}\\in S_{i}}|\\mathcal{B}_{P_{i}}(p^{\\prime},2r)|,$ , and $\\ensuremath{\\mathcal{B}_{\\mathrm{max}}}=\\ensuremath{\\mathcal{B}_{P_{i}}}(p,2r)$ . Assume that $\\begin{array}{r}{\\frac{n_{i}^{t}-z}{2(k-i+1)}\\leq|\\mathcal{B}_{m a x}|}\\end{array}$ . Then, we can add $B_{P_{i}}(p,4r)$ as a cluster in our solution, and this cluster will be dense until time t\u2032 = t + t\u2217, with t\u2217\u2265 4(kni\u2212\u2212i+z1). ", "page_idx": 9}, {"type": "text", "text": "For the proof of Lemma 3.10, see Supplementary Section D. ", "page_idx": 9}, {"type": "text", "text": "Lemma 3.11 (Duration of dense cluster for $z$ is large). Assume that we are currently at time t. Let $\\begin{array}{r}{z+1>\\frac{n_{i}^{t}-z}{4(k-i+1)}}\\end{array}$ for some level $i$ . Let $p=a r g\\,m a x_{p^{\\prime}\\in S_{i}}|\\mathcal{B}_{P_{i}}(p^{\\prime},2r)|$ , and $\\ensuremath{\\mathcal{B}_{\\operatorname*{max}}}=\\ensuremath{\\mathcal{B}_{P_{i}}}(p,2r)$ . Assume that kn\u2212ii\u2212+z1 \u2212\u03b2\u03f5zk \u2264|Bmax|. Then, we can add BPi(p, 4r) as a cluster in our solution, and this cluster will be dense until time $t^{\\prime}=t+t^{*}$ , with $t^{*}=\\Omega(\\frac{\\epsilon z}{k})$ . ", "page_idx": 9}, {"type": "text", "text": "For the proof of Lemma 3.11, see Supplementary Section D. ", "page_idx": 9}, {"type": "text", "text": "3.4 Small radius guesses ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "It has not yet been considered what will happen if Procedure 5 fails on some level $i$ . That is, when there exists no $p\\in S_{i}$ such that $B_{P_{i}}(p,2r)$ covers sufficiently many points. Lemmas E.1 and E.2 show that if Procedure 5 fails at some level, then with high probability the guess of the optimal radius for that specific instance is too small $(r<r_{\\mathrm{OPT}})$ ). In this case, we postpone the construction of that level to a time $t+t^{*}$ , referred to as $t_{r}^{\\prime}$ in the algorithm. ", "page_idx": 9}, {"type": "text", "text": "3.5 Computing update time ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Finally, we prove the update time of our algorithm. Lemma 3.4, Lemma 3.5 and Lemma 3.6 prove that the algorithms maintain the invariants with high probability. Together with Lemma 3.7 and Lemma F.1 this implies Theorem 1.1. ", "page_idx": 9}, {"type": "text", "text": "3.6 Robustness to adversarial inputs ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our dynamic algorithm is robust against an adaptive adversary. Specifically, we do not assume that the adversary has predetermined the sequence of updates in advance, as with an oblivious adversary. Instead, the adversary can query the insertion and deletion updates in an online manner, with knowledge of our solution. ", "page_idx": 9}, {"type": "text", "text": "We only use randomization to generate the sample set $S_{i}$ in Procedure 5, which is then used to construct the levels. This randomness affects only the probability of failure in this procedure. After the insertion or deletion updates, we do not need to reconstruct a level until it no longer satisfies the invariants. The guarantees we provide for how long a level can remain valid are all in the worst case; therefore, they hold even when an adaptive adversary chooses the insertion and deletion operations online. ", "page_idx": 9}, {"type": "text", "text": "3.7 Acknowledgements ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "The authors would like to thank the anonymous reviewers for their insightful comments. Annika Hennes\u2019 and Melanie Schmidt\u2019s research was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) \u2013 project number 456558332. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] MohammadHossein Bateni, Hossein Esfandiari, Hendrik Fichtenberger, Monika Henzinger, Rajesh Jayaram, Vahab Mirrokni, and Andreas Wiese. Optimal fully dynamic $k$ -center clustering for adaptive and oblivious adversaries. In Proceedings of the 2023 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 2677\u20132727. Society for Industrial and Applied Mathematics, 2023.   \n[2] Leyla Biabani, Annika Hennes, Morteza Monemizadeh, and Melanie Schmidt. Faster query times for fully dynamic $k$ -center clustering with outliers. Advances in Neural Information Processing Systems, 36, 2024.   \n[3] Matteo Ceccarello, Andrea Pietracaprina, and Geppino Pucci. Solving k-center clustering (with outliers) in mapreduce and streaming, almost as accurately as sequentially. Proc. VLDB Endow., 12(7):766\u2013778, 2019.   \n[4] Deeparnab Chakrabarty, Prachi Goyal, and Ravishankar Krishnaswamy. The non-uniform $k$ - center problem. In Proceedings of the 43rd International Colloquium on Automata, Languages, and Programming (ICALP), volume 55 of LIPIcs. Leibniz Int. Proc. Inform., pages Art. No. 67, 15. Schloss Dagstuhl. Leibniz-Zent. Inform., Wadern, 2016.   \n[5] T-H Hubert Chan, Silvio Lattanzi, Mauro Sozio, and Bo Wang. Fully dynamic $\\mathbf{k}\\cdot$ -center clustering with outliers. Algorithmica, 86(1):171\u2013193, 2024.   \n[6] TH Hubert Chan, Arnaud Guerqin, and Mauro Sozio. Fully dynamic k-center clustering. In Proceedings of the 2018 World Wide Web Conference, pages 579\u2013587, Lyon, France, 2018. International World Wide Web Conferences Steering Committee.   \n[7] Moses Charikar, Chandra Chekuri, Toma\u00b4s Feder, and Rajeev Motwani. Incremental clustering and dynamic information retrieval. In Proceedings of the twenty-ninth annual ACM symposium on Theory of computing, pages 626\u2013635, 1997.   \n[8] Moses Charikar, Samir Khuller, David M. Mount, and Giri Narasimhan. Algorithms for facility location problems with outliers. In S. Rao Kosaraju, editor, Proceedings of the Twelfth Annual Symposium on Discrete Algorithms, January 7-9, 2001, Washington, DC, USA, pages 642\u2013651. ACM/SIAM, 2001.   \n[9] Mark de Berg, Leyla Biabani, and Morteza Monemizadeh. $k$ -center clustering with outliers in the MPC and streaming model. CoRR, abs/2302.12811, 2023.   \n[10] Mark de Berg, Morteza Monemizadeh, and Yu Zhong. k-center clustering with outliers in the sliding-window model. In Petra Mutzel, Rasmus Pagh, and Grzegorz Herman, editors, 29th Annual European Symposium on Algorithms, ESA 2021, September 6-8, 2021, Lisbon, Portugal (Virtual Conference), volume 204 of LIPIcs, pages 13:1\u201313:13. Schloss Dagstuhl - Leibniz-Zentrum fu\u00a8r Informatik, 2021.   \n[11] Hendrik Fichtenberger, Monika Henzinger, and Andreas Wiese. On fully dynamic constantfactor approximation algorithms for clustering problems. CoRR, abs/2112.07217, 2021.   \n[12] Jinxiang Gan and Mordecai J Golin. Fully dynamic k-center in low dimensions via approximate furthest neighbors. In 2024 Symposium on Simplicity in Algorithms (SOSA), pages 269\u2013278. SIAM, 2024.   \n[13] Teofilo F Gonzalez. Clustering to minimize the maximum intercluster distance. Theoretical computer science, 38:293\u2013306, 1985.   \n[14] Gramoz Goranci, Monika Henzinger, Dariusz Leniowski, Christian Schulz, and Alexander Svozil. Fully dynamic $k$ -center clustering in low dimensional metrics. In 2021 Proceedings of the Workshop on Algorithm Engineering and Experiments (ALENEX), pages 143\u2013153. SIAM, 2021.   \n[15] Dorit S Hochbaum and David B Shmoys. A unified approach to approximation algorithms for bottleneck problems. Journal of the ACM (JACM), 33(3):533\u2013550, 1986.   \n[16] Wen-Lian Hsu and George L Nemhauser. Easy and hard bottleneck location problems. Discrete Applied Mathematics, 1(3):209\u2013215, 1979.   \n[17] Richard Matthew McCutchen and Samir Khuller. Streaming algorithms for $k$ -center clustering with outliers and with anonymity. In Approximation, Randomization and Combinatorial Optimization. Algorithms and Techniques, pages 165\u2013178. Springer, 2008.   \n[18] Paolo Pellizzoni, Andrea Pietracaprina, and Geppino Pucci. k-center clustering with outliers in sliding windows. Algorithms, 15(2):52, 2022.   \n[19] Paolo Pellizzoni, Andrea Pietracaprina, and Geppino Pucci. Fully dynamic clustering and diversity maximization in doubling metrics. In Algorithms and Data Structures Symposium, pages 620\u2013636. Springer, 2023.   \n[20] Melanie Schmidt and Christian Sohler. Fully dynamic hierarchical diameter $k$ -clustering and $k$ -center. arXiv preprint arXiv:1908.02645, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Missing Pseudocode ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this section, we provide the missing pseudocodes. We start by Procedure INITIALIZATION, which is called to start the algorithm. This procedure defines the set $\\mathcal{R}$ and then creates a counter for time $t$ , and the elements of our data structure ${\\mathcal{F}}_{r}$ , $\\mathcal{L}_{r}$ , and ${\\mathcal{Z}}_{r}$ that we discussed before. It also creates the variable $t_{r}^{\\prime}$ , which is used when the radius $r$ is smaller than $r_{\\mathrm{OPT}}$ . If $r\\,<\\,r_{\\mathrm{OPT}}$ , we may fail to find the $p^{*}$ that we desired for a level $i$ in Procedure OFFLINECLUSTER, and then have to stop the level construction. In this case, we set $t_{r}^{\\prime}$ to $t+t^{*}$ to postpone the construction of that level to time $t+t^{*}$ . See Lemmas E.1 and E.2 for the choice of $t^{*}$ . If $t_{r}^{\\prime}$ is set to $-1$ , it means there is currently no construction to be postponed. We assume all the parameters and variables defined in Procedure INITIALIZATION are global, and the other procedures have access to them. ", "page_idx": 12}, {"type": "text", "text": "Then, we state Procedure UPDATE to handle the insertion or deletion of a point $p$ . Depending on whether the query is an insertion or a deletion, it calls $\\mathrm{INSERT}(p,r)$ or DELETE $(p,r)$ for different values of $r$ . Then it updates the counter for time. Next, if there is any level construction that was postponed to the current time $t$ , it handles it. ", "page_idx": 12}, {"type": "table", "img_path": "OtYCp1yfbX/tmp/e67d0e196b95d75e2e0b7e5b772b2dbc800385c9b5d084079507c98727118193.jpg", "table_caption": [], "table_footnote": [], "page_idx": 12}, {"type": "table", "img_path": "OtYCp1yfbX/tmp/98227fd2fa3d95a7e1e7a417dd4787974590f612b578d6e93fc03ea99b0a07ed.jpg", "table_caption": [], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "Procedure $3\\;\\mathrm{INSERT}(p,r)$   \n1: $\\lambda\\leftarrow|\\mathcal{F}_{r}|$ , $\\left\\{c_{1},c_{2},...,c_{\\lambda}\\right\\}\\gets\\mathcal{F}_{r}$ , and $\\{C_{1},C_{2},...,C_{\\lambda}\\}\\gets\\mathcal{L}_{r}$   \n2: if $d(p,c_{i})\\leq4r$ for some $i\\in[\\lambda]$ then   \n3: $C_{i}\\leftarrow C_{i}\\cup p$   \n4: else   \n5: $\\mathcal{Z}_{r}\\gets\\mathcal{Z}_{r}\\cup p$   \n6: end if   \n7: For each level $\\ell$ , $P_{\\ell}\\gets(\\cup_{\\ell\\leq j\\leq\\lambda}C_{j})\\cup\\mathcal{Z}_{r}$   \n8: for $i=1$ to $\\lambda$ do   \n9: if $\\begin{array}{r}{|C_{i}|\\le\\operatorname*{min}\\left(z+1,\\frac{|P_{i}|-z}{k-i+1}-\\frac{\\epsilon z}{\\alpha k}\\right)\\mathbf{the}}\\end{array}$ n   \n10: OFFLINECLUSTER $(P_{i},i,r)$   \n11: break   \n12: end if   \n13: end for   \nProcedure 4 DELETE(p, r)   \n1: $\\lambda\\leftarrow|\\mathcal{F}_{r}|$   \n2: if $p\\in\\mathcal{Z}_{r}$ then   \n3: remove $p$ from ${\\mathcal{Z}}_{r}$   \n4: else   \n5: Let $C_{i}$ be the cluster containing $p$ , where $i\\in[\\lambda]$   \n6: $P_{i}\\gets(\\cup_{i\\leq j\\leq\\lambda}C_{j})\\cup\\mathcal{Z}_{r}$   \n7: if $B(c_{i},2r)$ covers $\\begin{array}{r}{>\\operatorname*{min}\\left(z+1,\\frac{|P_{i}|-z}{k-i+1}-\\frac{\\epsilon z}{\\alpha k}\\right)}\\end{array}$ from $P_{i}$ then   \n8: remove $p$ from $C_{i}$   \n9: else   \n10: OFFLINECLUSTER $(P_{i},i,r)$   \n11: end if   \n12: end if   \nProcedure 5 OFFLINECLUSTER(Pi, i, r)   \n1: $t_{r}^{\\prime}\\gets-1$   \n2: while $i\\leq k$ and $P_{i}\\neq\\emptyset$ do   \n3: $n_{i}\\leftarrow|P_{i}|$   \n4: Pick a uniform sample $S_{i}$ of $\\psi\\epsilon^{-1}k^{2}\\log k$ points from $P_{i}$   \n5: if $\\begin{array}{r}{z+1\\leq\\frac{n_{i}-z}{4(k-i+1)}}\\end{array}$ then   \n6: Find a point p\u2217\u2208Si such that |BPi(p\u2217, 2r)| \u22652(kni\u2212\u2212i+z1)   \n7: if there does not exist such a $p^{*}$ then   \n8: t\u2032r \u2190t +2(kni\u2212\u2212i+z2) // See Lemma E.1   \n9: break   \n10: end if   \n11: else if $\\begin{array}{r}{z+1>\\frac{n_{i}-z}{4(k-i+1)}}\\end{array}$ then   \n12: Find a point $p^{*}\\in S_{i}$ such that $\\begin{array}{r}{|\\mathcal{B}_{P_{i}}(p^{*},2r)|\\ge\\frac{n_{i}-z}{k-i+1}-\\frac{\\epsilon z}{\\beta k}}\\end{array}$   \n13: if there does not exist such a $p^{*}$ then   \n14: $\\begin{array}{r}{t_{r}^{\\prime}\\gets t+\\frac{\\epsilon z}{2\\beta k}}\\end{array}$ // See Lemma E.2   \n15: break   \n16: end if   \n17: end if   \n18: $c_{i}\\gets p^{*}$ , $C_{i}\\gets B(p^{*},4r)$ , and $P_{i+1}\\leftarrow P_{i}\\backslash C_{i}$   \n19: $i\\gets i+1$   \n20: end while   \n21: $\\mathcal{F}_{r}\\gets\\{c_{1},c_{2},...,c_{i-1}\\},\\,\\mathcal{L}_{r}\\gets\\{C_{1},C_{2},...,C_{i-1}\\}\\,,\\mathcal{Z}_{r}\\gets P_{i}$ ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "B Missing proofs Section 3.1 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Lemma 3.4 (Procedure 3 maintains invariants). Assume that at time t, we have point set $P^{t}$ , data structure $\\boldsymbol{\\mathcal{D}}_{r}\\,=\\,(\\mathcal{F}_{r},\\mathcal{L}_{r},\\mathcal{Z}_{r})$ . We assume that the level and dense invariants hold at time t and $r\\geq r_{\\mathrm{OPT}}^{t+1}$ d. eAnts te hien vstaarrita notfs t istmilel $t+1$ , wwiteh  ipnrsoebrta bpioliitnyt $p$ iuf siPnrgo cPerdoucreed 5u rwe a3s.  nAoftt ecra tllheed i nasnedr tiwoitnh,  tthhee probability of at least 1 \u2212 2(ek\u03a8 \u2212loig+ k1 ), where \u03a8 \u22651 if Procedure 5 was not called. ", "page_idx": 13}, {"type": "text", "text": "Proof. If we enter the case in line 2, the new point $p$ will be added to an existing cluster $C_{i}$ . Then, we have that $p\\in P_{j}$ for all $j\\leq i$ and $p\\notin P_{j}$ for all $j>i$ . If we enter the case in line 4, the new point $p$ is added as an outlier. This is the highest level, so we have that $p\\in P_{i}$ for all $i$ . The level invariant is maintained by definition in these cases. If we enter the case in line 9 and recluster levels $i,\\dots,k$ , we recluster the points in $P_{i}$ as defined in line 7, and hence, none of the levels $j<i$ are affected. Then by Lemma 3.6, the level invariant is maintained for all levels. The dense invariant is maintained because of the check we do in line 9 for all levels. Furthermore, we choose the lowest level $i$ where the dense invariant does not hold and recluster from this level upwards. Hence, the dense invariant will hold for all levels $j<i$ . We call Procedure 5 on $P_{i}$ , so by Lemma 3.6, the dense invariant is maintained for levels j \u2265i with probability 1 \u22122(ek\u03a8 \u2212loig+ k1 ), with \u03a8 \u22651. \u53e3 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Lemma 3.5 (Procedure 4 maintains invariants). Assume that at time $t$ , we have point set $P^{t}$ , instance $\\mathcal{D}_{r}=(\\mathcal{F}_{r},\\mathcal{L}_{r},\\mathcal{Z}_{r}),$ , parameters $k,z\\in\\mathbb{N}$ and $\\epsilon>0$ . We assume that the level and dense invariants hPorlodc eadtu trie m4e. $t$ ftaenrd t $r\\,\\geq\\,r_{\\mathrm{OPT}}^{t+1}$ ,. t hAet l tehveel  satnardt  doefn tsie mien $t+1$ ,t s wheo ldd elweitteh  parno baarbbiiltirtayr 1y  ipf oPirnot $p$ duusrien $^{5}$ was not called, and with probability 1 \u22122(ek\u03a8 \u2212loig+ k1) with $\\Psi\\geq1$ if Procedure 5 was called. ", "page_idx": 14}, {"type": "text", "text": "Proof. If we enter the case in line 2, where $p$ is an outlier, it follows easily that the level and dense invariants are maintained. In the second case, starting in line 4, $p$ is in some cluster $C_{i}$ , either as a center or as another point. If $C_{i}$ still covers sufficiently many points after the deletion of $p$ as described in line 7, it follows easily that the level invariant is maintained. For the dense invariant, note that for all levels $j<i$ , $n_{j}$ and with this $\\begin{array}{r}{\\frac{n_{j}-z}{k-j+1}-\\frac{\\epsilon z}{\\alpha k}}\\end{array}$ can only decrease. Furthermore, for all levels $j>i$ , $n_{j}$ remains unchanged. This observation, combined with the fact that for any $i\\neq j$ $|C_{j}|$ is unchanged, means that the dense invariant still holds for all levels. ", "page_idx": 14}, {"type": "text", "text": "If $C_{i}$ is no longer dense after deleting $p$ , and we enter the case in line 9, the clusters in levels $j<i$ remain unchanged. The level invariant will be maintained since $P_{j}$ is updated to $P_{j}\\backslash\\{p\\}$ for all $j<i$ The dense invariant is maintained in levels $j<i$ by the same reasoning as the previous case. On the remaining points, Procedure 5 is called in line 10. Using Lemma 3.6, the level and dense invariants are maintained for the remaining levels with probability at least 1 \u22122(ek\u03a8 \u2212loig+ k1 ), where \u03a8 \u22651. Crucial for maintaining the level invariant is line 6. This line ensures that we do not consider the points in levels $j<i$ when constructing level $i$ and any higher levels. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Lemma 3.6 (Procedure 5 maintains invariants with high probability). Suppose the level and dense invariants hold for all levels $j<i$ and we call Procedure $^{5}$ on $P_{i}$ as the result of an insertion or deletion. Let $\\lambda\\leq k$ be a random variable representing the number of levels we have after completing Procedure 5. If $r\\geq r_{\\mathrm{OPT}}$ , Procedure 5 maintains the level and dense invariants for all levels $j$ with $i\\le j\\le\\lambda$ with probability at least $\\begin{array}{r}{1-\\frac{2(k-i+1)}{{e^{\\Psi\\log k}}}}\\end{array}$ 2(ek\u03a8 \u2212loig+ k1 ), with \u03a8 \u22651. ", "page_idx": 14}, {"type": "text", "text": "Proof. By the while-loop structure in combination with line 18, the level invariant is maintained for all levels $j\\geq i$ . For each newly constructed level $j\\geq i$ , there are two cases within the while-loop. For the first case where $\\begin{array}{r}{z+1\\le\\frac{n_{j}-z}{4(k-j+1)}}\\end{array}$ , we find a ball $B_{P_{j}}(p^{*},2r)$ that covers $\\begin{array}{r}{\\ge\\frac{n_{j}-z}{2(k-j+1)}}\\end{array}$ points with probability at least $1-\\frac{2}{e^{\\Psi\\log k}}$ if $r\\geq r_{\\mathrm{OPT}}$ . For the proof of this, we refer to Lemma E.1. Then, the dense invariant holds for any such level since ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{n_{j}-z}{2(k-j+1)}\\geq\\frac{n_{j}-z}{4(k-j+1)}\\geq z+1\\geq\\operatorname*{min}\\left(z+1,\\frac{n_{j}-z}{k-j+1}-\\frac{\\epsilon z}{\\alpha k}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "In the second case where $\\begin{array}{r}{z+1>\\frac{n_{j}-z}{4(k-j+1)}}\\end{array}$ , we find a ball $B_{P_{j}}(p^{*},2r)$ that covers $\\begin{array}{r}{\\geq\\,\\frac{n_{j}-z}{k-j+1}-\\frac{\\epsilon z}{\\beta k}}\\end{array}$ points with probability $\\begin{array}{r}{1-\\frac{2}{e^{\\Psi\\log k}}}\\end{array}$ if $r\\,\\geq\\,r_{\\mathrm{OPT}}$ . For the proof of this, we refer to Lemma E.2. Since $\\beta>\\alpha$ (see Section 2.4), the dense invariant also holds for every level in this case. Then, the probability that the dense invariant holds for all levels $j$ with $i\\le j\\le\\lambda$ is at least $\\begin{array}{r}{1-\\frac{2(k-i+1)}{{e^{\\Psi\\log k}}}}\\end{array}$ . This is because $\\lambda-i+1\\le k-i+1$ is the number of new levels we constructed during Procedure 5. ", "page_idx": 14}, {"type": "text", "text": "C Missing proofs Section 3.2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Lemma 3.8. Let i be an iteration of the charging argument above such that we are in case 2. This means that $\\mathcal{B}(c_{1},2r)\\cup...\\cup\\mathcal{B}(c_{i},2r)$ does not intersect any of the remaining modified balls. Then, there must be a remaining modified ball covering $\\leq\\frac{n_{i}-z}{k-i+1}$ points. ", "page_idx": 14}, {"type": "text", "text": "Proof. We prove this using strong induction on iterations in case 2. First, let us consider the base case. In the base case, we are in iteration $i$ which is the first iteration to be in case 2 of the charging argument. There have been $i-1$ iterations before $i$ which were charged according to case 1. For any ", "page_idx": 14}, {"type": "image", "img_path": "OtYCp1yfbX/tmp/1437063fc3f42f2cd0e91ad4cd6a5730ae14e7005b995984446bec2285a3aade.jpg", "img_caption": [], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Figure 3: An example to illustrate the extra charging we do at the end of both cases 1 and 2. We are in iteration $i$ , and the ball $B(c_{i},4r)$ covers two points from ball $O_{j}^{\\prime}$ , with $j>i$ . The two points, shown in red, will be charged to themselves as in charging rule $I$ and marked as covered. ", "page_idx": 15}, {"type": "text", "text": "$j<i$ , define $x_{j}$ to be the number of artificial outliers covered by $B(c_{j},4r)$ . Then, for the remaining modified balls we have that: ", "page_idx": 15}, {"type": "equation", "text": "$$\n|O_{i}^{\\prime}\\cup.\\,.\\,.\\cup O_{k}^{\\prime}|=n_{i}-\\left(z-\\sum_{j=1}^{i-1}(z_{j}^{c}-x_{j})\\right)-\\sum_{j=1}^{i-1}(z_{j}^{c}-x_{j})=n_{i}-z\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This is because we assume that there are exactly $z$ points outside our optimal solution $O_{1}\\cup...\\cup O_{k}$ . These $z$ points are also outside $O_{1}^{\\prime}\\cup...\\cup O_{k}^{\\prime}$ since for $1\\leq i\\leq k$ , $O_{i}^{\\prime}\\subseteq O_{i}$ . Of those $z$ points, $\\begin{array}{r}{\\big(z-\\sum_{j=1}^{i-1}(z_{j}^{c}-x_{j})\\big)}\\end{array}$ have not yet been seen in an iteration. Hence, these points are definitely not covered by $O_{i}^{\\prime}\\cup...\\cup O_{k}^{\\prime}$ . Furthermore, we know that $\\textstyle\\sum_{j=1}^{i-1}(z_{j}^{c}-x_{j})$ artificial outliers have been defined in $O_{i}^{\\prime}\\cup...\\cup O_{k}^{\\prime}$ in total. This is because all iterations $j<i$ were in case 1, and any artificial outliers present in $O_{j}^{\\prime}$ will be propagated to $(O_{j+1}^{\\prime}\\cup...\\cup O_{k}^{\\prime})\\cap P_{j+1}$ , as these are included in $z_{j}^{c}$ . We subtract $x_{j}$ because these artificial outliers have already been counted by another $z_{j}^{c}$ , and should not be counted again when computing the total amount of artificial outliers. Note that $z_{j}^{d}=0$ for all iterations $j<i$ as there were no previous iterations before $j$ in case 2. Since at most $n_{i}-z$ points are covered by $O_{i}^{\\prime}\\cup...\\cup O_{k}^{\\prime}$ , there must be at least one of the modified optimal ball which covers $\\leq\\frac{n_{i}-z}{k-i+1}$ . This concludes the proof of the base case. ", "page_idx": 15}, {"type": "text", "text": "Now, consider the inductive step. We are in an iteration $i$ that is in case 2. There have been $i-1$ previous iterations, of which an arbitrary number has been charged by case 2. Assume that for any such iteration $j<i$ that was in case 2, we found an optimal ball $O_{j}^{\\prime}$ that was covering at most $\\frac{n_{j}-z}{k-j+1}$ points. At the time of iteration $j$ , the points of ball $O_{j}^{\\prime}$ were charged to distinct points in $B(c_{j},2r)$ , but not necessarily covered. In the iterations between $j$ and $i$ , however, more points of the ball $O_{j}^{\\prime}$ may have been covered. Define $O_{j,\\mathrm{unc}}^{\\prime}$ as the points of $O_{j}^{\\prime}$ that are still uncovered at the time of iteration . Now, for the remaining modified balls, we should have that: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\displaystyle|O_{i}^{\\prime}\\cup...\\cup O_{k}^{\\prime}|=n_{i}-\\left(z-\\sum_{j=1}^{i-1}(z_{j}^{c}-x_{j})\\right)-\\sum_{j<i\\mathrm{\\'~in\\'\\case\\}1}(z_{j}^{c}+z_{j}^{d}-x_{j})}\\\\ &{}&{\\displaystyle-\\sum_{j<i\\mathrm{\\'~in\\'\\case\\}2}(z_{j}^{c}+z_{j}^{d}-|O_{j}^{\\prime}|-x_{j})-\\sum_{j<i\\mathrm{\\'~in\\'\\case\\}2}|O_{j,\\mathrm{unc}}^{\\prime}|\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Similar to the base case, $\\begin{array}{r}{\\big(z-\\sum_{j=1}^{i-1}(z_{j}^{c}-x_{j})\\big)}\\end{array}$ points will definitely be outside $O_{i}^{\\prime}\\cup...\\cup O_{k}^{\\prime}$ . In the iterations before $j<i$ in case 1, $(z_{j}^{c}+z_{j}^{d})$ points are stored as a credit. We subtract $x_{j}$ from this since we don\u2019t want to recount outliers from previous iterations. In each iteration $j<i$ that was in case 2, $(z_{j}^{c}+z_{j}^{d}-|\\boldsymbol{O}_{j}^{\\prime}|)$ are stored as a credit. Even though $B(c_{j},2r)$ is disjoint from the remaining modified balls in this case, $B(c_{j},2r)$ can still cover some artificial outliers present in the remaining modified balls. We do not want to recount these for the total amount of outliers in $O_{i}^{\\prime}\\cup...\\cup O_{k}^{\\prime}$ , and hence we subtract $x_{j}$ . Lastly, since the modified balls are disjoint, any points of $O_{j}^{\\prime}$ with $j<i$ and iteration $j$ in case 2 that are still in the point set $n_{i}$ are not covered by $O_{i}^{\\prime}\\cup...\\cup O_{k}^{\\prime}$ . We can rewrite ", "page_idx": 15}, {"type": "text", "text": "this as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n|O_{i}^{\\prime}\\cup\\dots\\cup O_{k}^{\\prime}|=n_{i}-z-\\left(\\sum_{j<i}z_{j}^{d}+\\sum_{j<i\\mathrm{~in~case~}2}|O_{j,\\mathrm{unc}}^{\\prime}|\\right)+\\sum_{j<i\\mathrm{~in~case~}2}|O_{j}^{\\prime}|=n_{i}-z\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This is because by the definition of $z_{j}^{d}$ , $\\begin{array}{r}{(\\sum_{j<i}z_{j}^{d}+\\sum_{j<i}}\\end{array}$ in case $\\textstyle{}_{2}\\vert O_{j,\\operatorname{unc}}^{\\prime}\\vert)=\\sum_{j<i}$ in case ${}_{2}|O_{j}^{\\prime}|$ . Since at most $n_{i}-z$ points are covered by $O_{i}^{\\prime}\\cup...\\cup O_{k}^{\\prime}$ , there must be at least one of the remaining modified balls which covers $\\leq\\frac{n_{i}-z}{k-i+1}$ . This concludes the proof. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Lemma 3.9 (Coverage of $B(c_{i},2r);$ ). When we are in case 2 of the charging argument for some iteration i, we must have that |B(ci, 2r)| \u2265kn\u2212ii\u2212+z1 \u2212\u03b1\u03f5zk. ", "page_idx": 16}, {"type": "text", "text": "Proof. We prove the statement by induction on iterations $i\\leq k$ in case 2 of the charging argument. For the base case, iteration $i$ is the first iteration in case 2. This means that for all previous iterations $j<i,O_{j}^{\\prime}$ has been charged and covered by charging rule I. Hence, $B(c_{i},2r)$ cannot intersect any such $O_{j}^{\\prime}$ since the points in $O_{j}^{\\prime}$ are not in $P_{i}$ . We know that $B(c_{i},2r)$ also does not intersect any of the remaining modified balls by our assumption that iteration $i$ is in case 2. Hence, $B(c_{i},2r)$ does not intersect $O_{1}^{\\prime}\\cup...\\cup O_{k}^{\\prime}$ . The ball $B(c_{i},2r)$ can cover $\\begin{array}{r}{x\\le\\sum_{j=1}^{i-1}(z_{j}^{c}-x_{j})}\\end{array}$ artificial outliers, where $x$ is the total number of artificial outliers present in $O_{i}^{\\prime}\\cup...\\cup O_{k}^{\\prime}$ . The ball $B(c_{i},2r)$ can cover at most $\\textstyle z-\\sum_{j=1}^{i-1}(z_{j}^{c}-x_{j})$ other points, since there are exactly $z$ points outside $O_{1}\\cup...\\cup O_{k}$ , of which $\\textstyle\\sum_{j=1}^{i-1}(z_{j}^{c}-x_{j})$ have been seen in a previous ball $B(c_{j},4r)$ , for $j<i$ . Hence, $|B(c_{i},2r)|\\leq z$ . So, in order to satisfy the dense invariant, $\\begin{array}{r}{|B(c_{i},2r)|\\ge\\frac{n_{i}-z}{k-i+1}-\\frac{\\epsilon z}{\\alpha k}}\\end{array}$ . ", "page_idx": 16}, {"type": "text", "text": "Now consider the induction case. We are in an arbitrary iteration $i$ in case 2, and there have been an arbitrary number of iterations $j<i$ in case 2. For iterations $j<i$ in case 2, we assume that $\\begin{array}{r}{|B(c_{j},2r)|\\ge\\frac{n_{j}-z}{k-j+1}-\\frac{\\epsilon z}{\\alpha k}}\\end{array}$ . Even though $B(c_{i},2r)$ is disjoint from the remaining modified balls, it can cover $x$ artificial outliers and $\\textstyle z-\\sum_{j=1}^{i-1}(z_{j}^{c}-x_{j})$ other points. For $x$ we have that: ", "page_idx": 16}, {"type": "equation", "text": "$$\nx\\leq\\sum_{j<i\\;\\mathrm{in}\\;\\mathrm{case}\\;1}(z_{j}^{c}+z_{j}^{d}-x_{j})+\\sum_{j<i\\;\\mathrm{in}\\;\\mathrm{case}\\;2}(z_{j}^{c}+z_{j}^{d}-|O_{j}^{\\prime}|-x_{j})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "$\\sum_{j<i}$ in case $_1(z_{j}^{c}+z_{j}^{d}-x_{j})$ are the total number of artificial outliers from iterations in case 1, and j<i in case 2(zjc + zjd \u2212|O\u2032j| \u2212xj) are the total number of artificial outliers from case 2. Then, for the total amount of points in $B(c_{i},2r)$ , we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n|\\mathcal{B}(c_{i},2r)|\\le z+\\sum_{j<i}z_{j}^{d}-\\sum_{j<i\\;\\mathrm{in}\\;\\mathrm{case}\\;2}|O_{j}^{\\prime}|\\le z\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This is because by the definition of $z_{j}^{d}$ , we have that $\\begin{array}{r}{\\sum_{j<i}z_{j}^{d}\\le\\sum_{j<i}}\\end{array}$ in case ${}_{2}|O_{j}^{\\prime}|$ . So, in order to satisfy the dense invariant, $\\begin{array}{r}{|B(c_{i},2r)|\\ge\\frac{n_{i}-z}{k-i+1}-\\frac{\\epsilon z}{\\alpha k}}\\end{array}$ . This concludes the proof. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "D Missing proofs Section 3.3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma 3.10 (Duration of dense cluster for $z$ is small). Assume that we are currently at time $t$ . Let us consider a level i in which z + 1 \u2264 4(kni\u2212\u2212i+z1). Let $p\\,=\\,a r g\\,m a x_{p^{\\prime}\\in S_{i}}|\\mathcal{B}_{P_{i}}(p^{\\prime},2r)|,$ , and $\\ensuremath{\\mathcal{B}_{\\mathrm{max}}}=\\ensuremath{\\mathcal{B}_{P_{i}}}(p,2r)$ . Assume tha t2(kni\u2212\u2212i+z1) \u2264|Bmax|. Then, we can add BPi(p, 4r) as a cluster in our solution, and this cluster will be dense until time t\u2032 = t + t\u2217, with t\u2217\u2265 4(knit\u2212\u2212i+z1). ", "page_idx": 16}, {"type": "text", "text": "Proof. The subscript of $n_{i}$ will be omitted for simplicity. At time $t$ , we know that $\\begin{array}{r}{\\frac{n^{t}-z}{2(k-i+1)}\\ \\leq}\\end{array}$ $|\\boldsymbol{B}_{\\mathrm{max}}|$ . The resulting cluster $B_{P_{i}}(p,4r)$ will be dense as long as $\\mathcal{B}_{\\mathrm{max}}$ covers more than $\\begin{array}{r}{\\operatorname*{min}\\left(z+1,\\frac{{n^{t^{\\prime}}-z}}{k-i+1}-\\frac{\\epsilon z}{\\alpha k}\\right)}\\end{array}$ points at time $t^{\\prime}$ . Hence, the dense invariant will not be broken as long as $\\mathcal{B}_{\\mathrm{max}}$ is greater than $z+1$ . Let $t^{\\prime}$ be the time at which $\\vert{B}_{\\mathrm{max}}\\vert<z+1$ . We can lower bound the time $t^{*}$ between $t$ and $t^{\\prime}$ using the change in size of $\\mathcal{B}_{\\mathrm{max}}$ . Then, we get the following lower bound: ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "equation", "text": "$$\nt^{*}\\geq\\frac{n^{t}-z}{2(k-i+1)}-z\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then, since $\\begin{array}{r}{z+1\\leq\\frac{n^{t}-z}{4(k-i+1)}}\\end{array}$ by our assumption, we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\nt^{*}>\\frac{n^{t}-z}{2(k-i+1)}-\\frac{n^{t}-z}{4(k-i+1)}\\geq\\frac{n^{t}-z}{4(k-i+1)}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 17}, {"type": "text", "text": "Lemma 3.11 (Duration of dense cluster for $z$ is large). Assume that we are currently at time t. Let $\\begin{array}{r}{z+1>\\frac{n_{i}^{t}-z}{4(k-i+1)}}\\end{array}$ 4(kni\u2212\u2212i+z1) for some level i. Let p = arg maxp\u2032\u2208Si|BPi(p\u2032, 2r)|, and Bmax = BPi(p, 2r). Assume that kn\u2212ii\u2212+z1 \u2212\u03b2\u03f5zk \u2264|Bmax|. Then, we can add BPi(p, 4r) as a cluster in our solution, and this cluster will be dense until time $t^{\\prime}=t+t^{*}$ , with $\\begin{array}{r}{t^{*}=\\Omega(\\frac{\\epsilon z}{k})}\\end{array}$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. The subscript of $n_{i}$ will be omitted for simplicity. At time $t$ , we know that $\\begin{array}{r l r}{\\lefteqn{\\frac{n^{t}-z}{k-i+1}}}&{{}-}&{}\\end{array}$ $\\begin{array}{r}{\\frac{\\epsilon z}{\\beta k}\\;\\leq\\;|\\beta_{\\operatorname*{max}}|}\\end{array}$ . The resulting cluster $B_{P_{i}}(p,4r)$ will be dense as long as $\\mathcal{B}_{\\mathrm{max}}$ covers more than $\\begin{array}{r}{\\operatorname*{min}\\left(z+1,\\frac{n^{t^{\\prime}}-z}{k-i+1}-\\frac{\\epsilon z}{\\alpha k}\\right)}\\end{array}$ points at time $t^{\\prime}$ , with $\\alpha$ a fixed constant smaller than $\\beta$ . Hence, the dense invariant will not be broken as long as $\\mathcal{B}_{\\mathrm{max}}$ covers at least $\\begin{array}{r}{\\frac{n^{t^{\\prime}}-z}{k-i+1}-\\frac{\\epsilon z}{\\alpha k}}\\end{array}$ points. Let $t^{\\prime}$ be the time at which $\\begin{array}{r}{|{\\cal B}_{\\mathrm{max}}|\\,<\\,\\frac{{n^{t}}^{\\prime}-z}{k-i+1}-\\frac{\\epsilon z}{\\alpha k}}\\end{array}$ . We can lower bound $t^{*}$ by examining the change in size of $\\mathcal{B}_{\\mathrm{max}}$ . Using that $n^{t^{\\prime}}\\leq n^{t}+t^{*}$ , we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{t^{*}\\geq(\\cfrac{n^{t}-z}{k-i+1}-\\cfrac{\\epsilon z}{\\beta k})-(\\cfrac{n^{t^{\\prime}}-z}{k-i+1}-\\cfrac{\\epsilon z}{\\alpha k})}}\\\\ {{\\displaystyle~~\\geq\\cfrac{n^{t}-z}{k-i+1}-\\cfrac{n^{t}+t^{*}-z}{k-i+1}-\\cfrac{\\epsilon z}{\\beta k}+\\cfrac{\\epsilon z}{\\alpha k}=\\cfrac{-t^{*}}{k-i+1}-\\cfrac{\\epsilon z}{\\beta k}+\\cfrac{\\epsilon z}{\\alpha k}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now, solving for $t^{*}$ gives: ", "page_idx": 17}, {"type": "equation", "text": "$$\nt^{*}\\geq\\frac{\\epsilon z(k-i+1)}{k(k-i+2)}(\\frac{1}{\\alpha}-\\frac{1}{\\beta})\\geq\\frac{\\epsilon z}{2k}(\\frac{1}{\\alpha}-\\frac{1}{\\beta})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Since $\\alpha$ and $\\beta$ are two fixed constants such that $\\beta>\\alpha$ , this gives $\\begin{array}{r}{t^{*}=\\Omega(\\frac{\\epsilon z}{k})}\\end{array}$ . This concludes the proof. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "E Missing proofs Section 3.4 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Lemma E.1 (Radius guess is small for $z$ is small). Assume that we are currently at time $t,$ . Let us consider a level i for which $\\begin{array}{r}{z+1\\le\\frac{n_{i}^{t}-z}{4(k-i+1)}}\\end{array}$ . Let $p=a r g\\,m a x_{p^{\\prime}\\in S_{i}}|\\mathcal{B}_{P_{i}}(p^{\\prime},2r)|,$ , where $S_{i}$ is the sample chosen in Algorithm $^{5}$ , and $\\ensuremath{\\mathcal{B}_{\\mathrm{max}}}=\\ensuremath{\\mathcal{B}_{P_{i}}}(p,2r)$ . Assume that $\\begin{array}{r}{|B_{m a x}|<\\frac{n_{i}^{t}-z}{2(k-i+1)}}\\end{array}$ . Then, with probability at least e\u03a8 l2og k , for \u03a8 \u22651, we have that r < rOPT and until time t\u2032 = t + t\u2217we do not need to consider the instance for r, with t\u2217\u22652(kni\u2212\u2212i+z2). ", "page_idx": 17}, {"type": "text", "text": "Proof. Let $O_{1},\\ldots,O_{k}$ be the optimal balls at time $t$ . Consider the same charging argument used in Lemma 3.7, where we charged one of the optimal balls to $B_{P_{i}}(c_{i},4r)$ in each level $i$ . Using Equation 4 and the fact that $\\bar{O}_{i}^{\\prime}\\ \\subseteq\\ O_{i}$ for each of the remaining optimal balls, we know that $|\\bar{O_{i}}\\cup...\\cup O_{k}|\\geq n_{i}^{t}-z$ . Then, there must be at least one of the remaining optimal balls covering $\\ge~\\frac{n_{i}^{t}-z}{k-i+1}$ points. Let $O_{i}$ be the largest such ball. Using the Chernoff bound, we show that with high probability, one of the sampled points $p\\in S_{i}$ is in $O_{i}$ . Define independent random variables $X_{1},\\ldots,X_{|O_{i}|}$ , one for each point in $O_{i}$ . Each $X_{j}$ , corresponding to point $j\\in O_{i}$ , will be 1 if $j\\in S_{i}$ and 0 otherwise. Define $\\begin{array}{r}{X=\\sum_{j=1}^{|O_{i}|}X_{j}}\\end{array}$ . We know that $\\begin{array}{r}{{\\bf E}[X_{j}]\\,=\\,\\frac{|S_{i}|}{n_{i}^{t}}}\\end{array}$ . Then, using linearity of expectation, we find: ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{E}[X]={\\frac{|S_{i}|\\cdot|O_{i}|}{n_{i}^{t}}}\\geq{\\frac{|S_{i}|}{n_{i}^{t}}}\\cdot{\\frac{n_{i}^{t}-z}{k-i+1}}\\geq{\\frac{|S_{i}|\\cdot(n_{i}^{t}-z)}{n_{i}^{t}\\cdot k}}\\geq{\\frac{|S_{i}|}{k}}(1-{\\frac{z}{n_{i}^{t}}})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The condition $\\begin{array}{r}{z+1\\,\\leq\\,\\frac{n_{i}^{t}-z}{4(k-i+1)}}\\end{array}$ implies $\\begin{array}{r}{z\\,\\le\\,\\frac{n_{i}^{t}}{4(k-i+1)}\\,\\le\\,\\frac{n_{i}^{t}}{4}}\\end{array}$ . Then, since $|S_{i}|\\,=\\,\\psi\\epsilon^{-1}k^{2}\\log k$ , it follows that $\\begin{array}{r}{\\frac{|S_{i}|}{k}(1-\\frac{z}{n_{i}^{t}})\\,\\geq\\,\\frac{3}{4}\\psi\\epsilon^{-1}k\\log k}\\end{array}$ . Given that $\\psi\\ge6\\beta$ and $\\beta>\\alpha\\geq1$ , this is at least $3\\epsilon^{-1}k\\log k\\geq3\\Psi\\log k$ for $\\Psi\\geq1$ . ", "page_idx": 18}, {"type": "text", "text": "Now, using the Chernoff bound, we find: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\vert X-\\mathbf{E}[X]\\vert\\ge\\mathbf{E}[X]\\right]\\le2e^{-\\frac{\\mathbf{E}[X]}{3}}\\le\\frac{1}{e^{\\Psi\\log k}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\Psi\\geq1$ . ", "page_idx": 18}, {"type": "text", "text": "Hence, with probability of at least $\\begin{array}{r}{1-\\frac{2}{e^{\\Psi\\log k}}}\\end{array}$ , there will be at least 1 point from $O_{i}$ in $S_{i}$ . Let us call this point $p$ . If $r\\geq r_{\\mathrm{OPT}}$ , $B_{P_{i}}(p,2r)$ would cover all points of $O_{i}$ since $p\\in O_{i}$ and $O_{i}$ has radius $\\le r_{\\mathrm{OPT}}$ . However, since $\\begin{array}{r}{|B_{\\mathrm{max}}|<\\frac{n_{i}^{t}-z}{2(k-i+1)}}\\end{array}$ , we must have that $r<r_{\\mathrm{OPT}}$ .   \nLeft to prove is that until time t\u2032 = t + t\u2217we do not need this instance of r, where t\u2217\u2265 2(kni\u2212\u2212i+z2) . To this end, let $\\mathcal{B}_{\\mathrm{max}}^{t}$ be $\\mathcal{B}_{\\mathrm{max}}$ at time $t$ . Consider the situation at time $t$ . We know that $\\begin{array}{r}{|B_{\\mathrm{max}}^{t}|<\\frac{n_{i}^{t}-z}{2(k-i+1)}}\\end{array}$ . Let t\u2032 be some time after t such that |Btm\u2032ax| \u2265 knit\u2212 i\u2212+z1 . Hence, the instance for $r$ becomes valid at time . We want to derive a lower bound for $t^{*}=t^{\\prime}-t$ to complete the proof. By examining the change in the size of $\\mathcal{B}_{\\mathrm{max}}$ , we derive the following lower bound: ", "page_idx": 18}, {"type": "equation", "text": "$$\nt^{*}\\geq\\frac{n_{i}^{t^{\\prime}}-z}{k-i+1}-\\frac{n_{i}^{t}-z}{2(k-i+1)}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Using $n_{i}^{t^{\\prime}}\\geq n_{i}^{t}-t^{*}$ and subsequently solving for $t^{*}$ we find that: ", "page_idx": 18}, {"type": "equation", "text": "$$\nt^{*}\\geq\\frac{n_{i}^{t}-z}{2(k-i+2)}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This completes the proof. ", "page_idx": 18}, {"type": "text", "text": "Lemma E.2 (Radius guess is small for $z$ is large). Assume that we are currently at time $t,$ . Let us consider a level $i$ in which $\\begin{array}{r}{z+1>\\frac{n_{i}^{t}-z}{4(k-i+1)}}\\end{array}$ 4(kni\u2212\u2212i+z1). Let p = arg maxp\u2032\u2208Si|BPi(p\u2032, 2r)|, where Si is the sample chosen in Algorithm $^{5}$ , and $\\ensuremath{\\mathcal{B}_{\\mathrm{max}}}=\\ensuremath{\\mathcal{B}_{P_{i}}}(p,2r)$ . Assume that $\\begin{array}{r}{|B_{m a x}|\\,<\\,\\frac{n_{i}^{t}-z}{k-i+1}-\\frac{\\epsilon z}{\\beta k}}\\end{array}$ . Then, with probability at least $\\begin{array}{r}{1-\\frac{2}{e^{\\Psi\\log k}}}\\end{array}$ , for $\\Psi\\geq1$ , we have that $r<r_{O P T}$ and until time $t^{\\prime}=t+t^{*}$ we do not need to consider the instance for r, with t\u2217\u2265 2\u03f5\u03b2zk. ", "page_idx": 18}, {"type": "text", "text": "Proof. Let $O_{1},\\ldots,O_{k}$ be the optimal balls at time $t$ . As in Lemma E.1, let $O_{i}$ be the largest remaining optimal ball covering $\\begin{array}{r}{\\ge~\\frac{n_{i}^{t}-z}{k-i+1}}\\end{array}$ points. Using the Chernoff bound, we show that with high probability, one of the sampled points $p\\in S_{i}$ is in $O_{i}$ . Define independent random variables $X_{1},\\ldots,X_{|O_{i}|}$ , one for each point in $O_{i}$ . Each $X_{j}$ , corresponding to point $j\\in O_{i}$ , will be 1 if $j\\in S_{i}$ and 0 otherwise. Define $\\begin{array}{r}{X=\\sum_{j=1}^{|O_{i}|}X_{j}}\\end{array}$ . We know that $\\begin{array}{r}{{\\bf E}[X_{j}]\\,=\\,\\frac{|{\\cal S}_{i}|}{n_{i}^{t}}}\\end{array}$ . Then, using linearity of expectation, we find ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname{E}[X]={\\frac{|S_{i}|\\cdot|O_{i}|}{n_{i}^{t}}}\\geq{\\frac{|S_{i}|}{n_{i}^{t}}}\\cdot{\\frac{n_{i}^{t}-z}{k-i+1}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Without loss of generality, we can assume $\\begin{array}{r}{\\frac{n_{i}^{t}-z}{k-i+1}-\\frac{\\epsilon z}{\\beta k}\\geq1}\\end{array}$ , otherwise there would not need to be any points in the ball $B_{P_{i}}(p,2r)$ to satisfy the dense invariant, and level $i$ would be trivial. Using this, Equation (15) can be simplified as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname{E}[X]\\ge{\\frac{|S_{i}|}{n_{i}^{t}}}\\cdot{\\frac{n_{i}^{t}-z}{k-i+1}}\\ge{\\frac{|S_{i}|}{n_{i}^{t}}}\\cdot{\\frac{\\epsilon z}{\\beta k}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We make a case distinction. Either $\\begin{array}{r}{z\\ge\\frac{n_{i}^{t}}{2}}\\end{array}$ . In this case, Equation (15) further simplifies to ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\bf E}[X]\\geq\\frac{\\epsilon|S_{i}|}{2\\beta k}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Recall that $|S_{i}|=\\psi\\epsilon^{-1}k^{2}\\log k$ . Given that $\\psi\\ge6\\beta$ and $\\beta>\\alpha\\geq1$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{E}[X]\\geq{\\frac{1}{2\\beta}}\\psi k\\log k\\geq3k\\log k\\geq3\\Psi\\log k\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for some $\\Psi\\geq1$ . ", "page_idx": 19}, {"type": "text", "text": "In the other case $\\begin{array}{r}{z<\\frac{n_{i}^{t}}{2}}\\end{array}$ , Equation (15) simplifies to ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbf{E}[X]\\ge\\frac{|S_{i}|}{n_{i}^{t}}\\cdot\\frac{n_{i}^{t}/2}{k-i+1}\\ge\\frac{|S_{i}|}{2k}\\ge\\frac{1}{2}\\psi\\epsilon^{-1}k\\log k\\ge3\\Psi\\log k\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "for some $\\Psi\\geq1$ . ", "page_idx": 19}, {"type": "text", "text": "In both cases, using the Chernoff bound, we find that with probability at least $1-\\frac{2}{e^{\\Psi\\log k}}$ , there will be at least one point $p$ from $O_{i}$ in $X_{i}$ . If $r\\geq r_{\\mathrm{OPT}}$ , $B_{P_{i}}(p,2r)$ would cover all points of $O_{i}$ since $p\\in O_{i}$ and $O_{i}$ has radius $\\le\\,r_{\\mathrm{OPT}}$ . However, since $\\begin{array}{r}{|\\mathcal{B}_{\\mathrm{max}}|\\,<\\,\\frac{n_{i}^{t}-z}{k-i+1}\\,-\\,\\frac{\\epsilon z}{\\beta k}\\,<\\,\\frac{n_{i}^{t}-z}{k-i+1}\\,\\le\\,|O_{i}|}\\end{array}$ , we must have that $r<r_{\\mathrm{OPT}}$ . ", "page_idx": 19}, {"type": "text", "text": "Left to prove is that until time $t^{\\prime}=t+t^{*}$ , with $t^{*}=\\Omega(\\frac{\\epsilon z}{k})$ , we do not need this instance of $r$ . To this end, let $\\mathcal{B}_{\\mathrm{max}}^{t}$ be $\\boldsymbol{B}_{\\mathrm{max}}$ at time $t$ . Consider the situation at time $t$ . We know that $\\begin{array}{r}{|B_{\\mathrm{max}}^{t}|<\\frac{n_{i}^{t}-z}{k-i+1}-\\frac{\\epsilon z}{\\beta k}}\\end{array}$ . Let t\u2032 be some time after t such that |Btm\u2032ax| \u2265 knit\u2212 i\u2212+z1. The cluster BPi(p, 2r) in level i is now covering sufficiently many points. We want to derive a lower bound for $t^{*}=t^{\\prime}-t$ to complete the proof. By examining the change in the size of $\\boldsymbol{B}_{\\mathrm{max}}$ and using that $n_{i}^{t^{\\prime}}\\geq n_{i}^{t}-t^{*}$ , we derive ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{t^{*}\\geq\\displaystyle\\frac{n_{i}^{t^{\\prime}}-z}{k-i+1}-(\\frac{n_{i}^{t}-z}{k-i+1}-\\frac{\\epsilon z}{\\beta k})}\\\\ &{\\quad=\\displaystyle\\frac{n_{i}^{t}-t^{*}-z}{k-i+1}-\\frac{n_{i}^{t}-z}{k-i+1}+\\frac{\\epsilon z}{\\beta k}}\\\\ &{\\quad=\\displaystyle\\frac{-t^{*}}{k-i+1}+\\frac{\\epsilon z}{\\beta k}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "as a lower bound for $t^{*}$ . Solving for $t^{*}$ gives ", "page_idx": 19}, {"type": "equation", "text": "$$\nt^{*}\\geq\\frac{\\epsilon z(k-i+1)}{\\beta k(k-i+2)}\\geq\\frac{\\epsilon z}{2\\beta k}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since $\\beta$ is a constant, this is in $\\Omega(\\frac{\\epsilon z}{k})$ . This concludes the proof. ", "page_idx": 19}, {"type": "text", "text": "F Missing proofs from Section 3.5 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Lemma F.1. The amortized update time of our dynamic algorithm is ${\\mathcal{O}}(\\epsilon^{-3}k^{6}\\log(k)\\log(\\Delta))$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. Let us fix an arbitrary time $t$ and assume that at time $t$ , there are $\\lambda\\leq k$ clusters. Let time $t^{\\prime}=t+t^{*}$ be the time at which we need to invoke Procedure 5 on an arbitrary level $i\\leq k$ , due to the dense invariant being violated. We have two cases: ", "page_idx": 19}, {"type": "text", "text": "\u278a $\\begin{array}{r}{z+1\\le\\frac{n_{i}^{t}-z}{4(k-i+1)}}\\end{array}$ : For this case, Lemma 3.10 shows the following: If we find a cluster such that |BPi(ci, 2r)| \u22652(kni\u2212\u2212i+z1) at time $t$ , then this cluster will remain dense for $\\begin{array}{r}{t^{*}\\geq\\frac{n_{i}^{t}-z}{4(k-i+1)}}\\end{array}$ update operations (either insert or delete).   \n\u278b $\\begin{array}{r}{z+1>\\frac{{n_{i}^{t}-z}}{4(k-i+1)}}\\end{array}$ 4(knit\u2212\u2212i+z1): For this case, Lemma 3.11 proves that if we find a cluster such that $\\begin{array}{r}{|\\mathcal{B}_{P_{i}}(c_{i},2r)|\\;\\ge\\;\\frac{n_{i}^{t}-z}{k-i+1}\\,-\\,\\frac{\\epsilon z}{\\beta k}}\\end{array}$ at time $t$ , then this cluster will remain dense for the next $\\begin{array}{r}{t^{*}\\geq\\frac{\\epsilon z}{2k}\\big(\\frac{1}{\\alpha}-\\frac{1}{\\beta}\\big)}\\end{array}$ time steps. Asymptotically, $\\begin{array}{r}{t^{*}=\\Omega\\big(\\frac{\\epsilon z}{k}\\big)}\\end{array}$ . By substituting the lower bound of $z$ in this formula, we obtain $t^{*}=\\Omega\\big(\\frac{\\epsilon n_{i}^{t}}{k^{2}}\\big)$ . ", "page_idx": 19}, {"type": "text", "text": "Observe that in both cases, we have a worst-case guarantee for the number of time steps during which the cluster $B_{P_{i}}(c_{i},2r)$ remains dense. This, in turn, allows us to achieve a worst-case amortized update time, rather than the weaker notion of expected amortized update time. We analyze the update time of each case separately. ", "page_idx": 20}, {"type": "text", "text": "Let us start with the first case which is $\\begin{array}{r}{z+1\\le\\frac{n_{i}^{t}-z}{4(k-i+1)}}\\end{array}$ and suppose that we find a cluster such that $\\begin{array}{r}{|\\mathcal{B}_{P_{i}}(c_{i},2r)|\\ge\\frac{n_{i}^{t}-z}{2(k-i+1)}}\\end{array}$ at time $t$ . We know that $\\begin{array}{r}{t^{*}\\geq\\frac{n_{i}^{t}-z}{4(k-i+1)}}\\end{array}$ . The cost of reclustering levels $i,\\dots,k$ according to Lemma 3.3 is $O(n_{i}^{t^{\\prime}}\\epsilon^{-1}\\cdot k^{3}\\log k)$ , with $n_{i}^{t^{\\prime}}\\leq n_{i}^{t}+t^{*}$ . Then, the amortized update time of an arbitrary update operation is $\\begin{array}{r}{\\mathcal{O}(\\frac{1}{t^{*}}(n_{i}^{t}+t^{*})\\epsilon^{-1}k^{3}\\log k)=\\mathcal{O}(\\frac{n_{i}^{t}}{t^{*}}\\epsilon^{-1}k^{3}\\log k)}\\end{array}$ . Since \u2264 4(k\u2212niti+1), we obtain tn\u2217it \u2264 nit \u00b7 4(knit\u2212\u2212i+z1) $\\begin{array}{r}{\\colon n_{i}^{t}\\cdot\\frac{4(k-i+1)}{n_{i}^{t}-z}\\,\\le\\,4k\\cdot\\frac{n_{i}^{t}}{n_{i}^{t}-z}\\,\\le\\,4k\\cdot\\frac{n_{i}^{t}}{n_{i}^{t}(1-\\frac{1}{4(k-i+1)})}\\,=\\,4k}\\end{array}$ $\\begin{array}{r}{\\frac{4(k-i+1)}{4(k-i+1)-1}=\\mathcal{O}(k)}\\end{array}$ . Thus, $\\begin{array}{r}{\\mathcal{O}(\\frac{n_{i}^{t}}{t^{*}}\\epsilon^{-1}k^{3}\\log k)=\\mathcal{O}(\\epsilon^{-1}k^{4}\\log k)}\\end{array}$ . ", "page_idx": 20}, {"type": "text", "text": "Now, we consider the second case. Using a similar analysis as in the first case, we have $t^{*}=$ $\\Omega\\big(\\frac{\\epsilon n_{i}^{t}}{k^{2}}\\big)$ . Then, the amortized update time of an arbitrary update operation is $\\begin{array}{r l}{\\mathcal{O}(\\frac{n_{i}^{t}}{t^{*}}\\epsilon^{-1}k^{3}\\log k)=}\\end{array}$ $O(\\overleftarrow{\\epsilon}^{-2}k^{5}\\log k)$ . ", "page_idx": 20}, {"type": "text", "text": "After reclustering levels $i,\\dots,k$ , there is no longer a lower bound for $t^{*}$ for levels $j<i$ . Thus, it could happen that such a level $j$ needs to be reclustered soon after time $t^{\\prime}$ . Since $j<k$ , this leads to an extra factor $k$ such that the final amortized cost is $\\mathcal{O}(\\epsilon^{-2}k^{6}\\log k)$ . ", "page_idx": 20}, {"type": "text", "text": "Finally, we need to consider the case that offilne clustering fails for a level $i$ . That is, we fail to find a ball covering sufficiently many points in Procedure 5. If offline clustering fails on some level $i$ in case 1, meaning we fail to find a ball such that $\\begin{array}{r}{|\\mathcal{B}_{P_{i}}(c_{i},2r)|\\ge\\frac{n_{i}^{t}-z}{2(k-i+1)}}\\end{array}$ , then Lemma E.1 proves that with probability $1-{\\frac{1}{e^{\\Omega(\\log k)}}}$ , the guess $r$ is small (i.e., $r<r_{\\mathrm{OPT}})$ ) and indeed, remains small and we do not need to consider this guess until a time t\u2032 = t + t\u2217where t\u2217\u22652(knit\u2212\u2212i+z2). ", "page_idx": 20}, {"type": "text", "text": "At time $t^{\\prime}$ , we recluster levels $i,\\dots,k$ . Then, since we are in the first case, we have $z\\ \\leq$ $\\frac{n_{i}^{t}}{4(k-i+1)}$ . Thus, the amortized update time of an arbitrary update operation in this case is $\\begin{array}{r}{\\mathcal{O}(\\frac{1}{t^{*}}(n_{i}^{t}+t^{*})\\epsilon^{-1}k^{3}\\log k)=\\mathcal{O}(\\frac{n_{i}^{t}}{t^{*}}\\epsilon^{-1}k^{3}\\log k)=\\mathcal{O}(\\epsilon^{-1}k^{4}\\log k)}\\end{array}$ using Lemma 3.3. ", "page_idx": 20}, {"type": "text", "text": "Next, we consider the situation when the offline clustering fails in case 2. Specifically, this occurs when we fail to find a ball such that $\\begin{array}{r}{|\\mathcal{B}_{P_{i}}(c_{i},2r)|\\,\\ge\\,\\frac{n_{i}-z}{k-i+1}\\,-\\,\\frac{\\epsilon z}{\\beta k}}\\end{array}$ kn\u2212ii\u2212+z1 \u2212\u03b2\u03f5zk. According to Lemma E.2, the guess $r$ is small (i.e., $r<r_{\\mathrm{OPT}})$ ) with probability e\u2126(l1og k) and indeed, remains small and we do not need to consider this guess until time $t^{\\prime}=t+t^{*}$ where $\\begin{array}{r}{t^{\\prime}=\\Omega(\\frac{\\epsilon z}{k})}\\end{array}$ . This leads to an amortized update time of $\\mathcal{O}(\\epsilon^{-2}k^{5}\\log k)$ . As mentioned above, the running time needs to be multiplied by an additional factor $k$ to account for the possible need to recluster lower levels. ", "page_idx": 20}, {"type": "text", "text": "Finally, we need to consider (lolgo(g1 +\u2206\u03f5)) guesses for the optimal radii. For small \u03f5, this is in O( log\u03f5 \u2206). Thus, we derive the final amortized update time of $\\mathcal{O}(\\epsilon^{-3}k^{6}\\log k\\log\\Delta)$ . \u53e3 ", "page_idx": 20}, {"type": "text", "text": "G How to support discrete $k$ -center clustering with outliers ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Lemma G.1. Let $(M,d)$ be a metric space and $\\epsilon>0$ be an accuracy parameter. The spread ratio $\\begin{array}{r}{\\Delta=\\frac{d_{\\mathrm{max}}}{d_{\\mathrm{min}}}}\\end{array}$ of all points ever inserted is assumed to be bounded. There exists a randomized fully dynamic algorithm that maintains a discrete $k$ -center solution that allows up to $(1\\!+\\!\\epsilon)z$ many outliers on the current set of points. At every point in time $t$ , the current clustering with centers $c_{1},\\ldots,c_{\\lambda}$ is a $(4+\\epsilon)$ -approximation to an optimal solution for the $(k,z)$ -center problem with high probability and $c_{i}\\in P^{t}$ for all $i\\leq\\lambda$ . Upon insertion or deletion of a point, the data structure is updated in amortized update time $\\mathcal{O}(\\epsilon^{-3}\\mathbf{\\hat{\\boldsymbol{k}}}^{6}\\log(k)\\log(\\Delta))$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. Let $r$ and $i$ be fixed, and $c_{i}$ be the center of cluster $C_{i}=\\mathcal{B}_{P_{i}}(c_{i},4r)$ . As long as $c_{i}$ is not deleted, we report $c_{i}$ as the $i$ -th center. Note that any feasible solution for the discrete version is also a feasible solution for the non-discrete version. Therefore, the optimal radius for the discrete version is not smaller than the optimal radius for the non-discrete version. ", "page_idx": 20}, {"type": "text", "text": "After the deletion of the point $c_{i}$ , we consider two cases: $\\begin{array}{r}{\\operatorname*{min}\\left(z+1,\\frac{n_{i}-z}{k-i+1}-\\frac{\\varepsilon z}{\\alpha k}\\right)\\;\\leq\\;0}\\end{array}$ or $\\begin{array}{r}{\\operatorname*{min}\\left(z+1,\\frac{n_{i}-z}{k-i+1}-\\frac{\\varepsilon z}{\\alpha k}\\right)>0.}\\end{array}$ . If $\\begin{array}{r}{\\operatorname*{min}\\left(z+1,\\frac{n_{i}-z}{k-i+1}-\\frac{\\varepsilon z}{\\alpha k}\\right)\\leq0}\\end{array}$ , then $n_{i}<(1+\\varepsilon)z$ as $i\\geq1$ and $\\alpha>1$ . Hence, we can report all the $n_{i}$ points as outliers and stop. We next consider the second case. The dense invariant states that $\\begin{array}{r}{|\\mathcal{B}_{P_{i}}(c_{i},\\bar{2}r)|\\geq\\operatorname*{min}(z+1,\\frac{n_{i}-z}{k-i+1}^{-}-\\frac{\\varepsilon z}{\\alpha k})}\\end{array}$ . Therefore, $|B_{P_{i}}(c_{i},2r)|>0$ holds in the second case, and there exists a point $\\hat{p}\\in\\mathcal{B}_{P_{i}}(c_{i},2r)$ . Then we report an arbitrary point $\\hat{p}\\in\\mathcal{B}_{P_{i}}(c_{i},2r)$ after the deletion of $c_{i}$ . Note that we do not replace $c_{i}$ by $\\hat{p}$ in our data structure, and $c_{i}$ does not change in our data structure as long as the $i$ -th level is not reconstructed. The point $\\hat{p}$ is just the center that we report for the discrete version. ", "page_idx": 21}, {"type": "text", "text": "We next prove the 6-approximation guarantee. To do this, we show that $C_{i}\\subseteq B(\\hat{p},6r)$ . This implies that any feasible solution with radius $4r$ for the non-discrete version can be used to report a feasible solution for the discrete version. Also, see Figure 4 for a visual representation. Let $q\\in C_{i}$ . We show that $q\\in B(\\hat{p},6r)$ . Since $q\\in C_{i}=B(c_{i},4r)$ , we have $d(q,c_{i})\\leq\\bar{4}r$ . Moreover, $d(c_{i},\\hat{p})\\leq2r$ since $\\hat{p}\\in B(c_{i},2r)$ . Then by the triangle inequality, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\nd(q,\\hat{p})\\leq d(q,c_{i})+d(c_{i},\\hat{p})\\leq4r+2r=6r.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "To report a solution for the discrete version, it is enough to keep $B(c_{i},2r)\\cap P_{i}$ . Note that our data structure already stores $C_{i}\\cap P_{i}$ , and since $\\mathcal{B}(c_{i},2r)\\cap P_{i}\\subseteq C_{i}\\cap P_{i}$ , the space complexity and time complexity remain the same. \u53e3 ", "page_idx": 21}, {"type": "image", "img_path": "OtYCp1yfbX/tmp/4d7519cd115e6c33f8eb085bdb7e8ef109d21675b905dafb167a027128c86298.jpg", "img_caption": ["Figure 4: Illustration of the claim\u2019s proof. Part (i): before deletion of $c_{i}$ , we report $c_{i}$ as the center. Part (ii): after deletion of $c_{i}$ , we report an arbitrary point $\\hat{p}\\in B(c_{i},2r)$ as the center. Then, $B(\\hat{p},6r)$ can cover all the points in the cluster $C_{i}$ . "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The problem setting stated at the beginning of the abstract summarizes this paper\u2019s scope as formalized in Section 1.3. The main statements made in the abstract can be found in Theorem 1.1 in Section 1.1, which is a summarization of Lemma 3.7 in Section 3.2 and Lemma F.1 in Section 3.5. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: As stated in the abstract, Section 1.1, Section 1.3 and the result Lemma 3.7, our algorithm yields a bicriteria approximation. This means that we approximate the objective function and also allow a slight violation of the outlier side constraint while comparing with an optimal solution that fulfills the side constraint exactly. Our results hold with high probability, which is stated formally in statement Lemma 3.6. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The problem setting alongside assumptions are stated in Section 1.3. This paper\u2019s main statements are Lemma 3.7 and Lemma F.1. Lemma 3.8 and Lemma 3.9 are used to prove Lemma 3.7. It uses the assumption that the level and dense invariants hold. Lemma 3.6, Lemma 3.4 and Lemma 3.5 show that running our procedures maintains these invariants. Lemma F.1 uses Lemma 3.3, Lemma 3.11, Lemma 3.10, Lemma E.1, Lemma E.2. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not contain experiments. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 23}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] Justification: The paper does not contain experiments. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not contain experiments. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not contain experiments. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: The paper does not contain experiments. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: As our paper is of theoretical nature, most of the points addressed in the Code of Ethics do not apply. Further, we could not identify any foreseeable dangers or harms directly caused by the utilization of our results. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: There is no foreseeable direct societal impact of the work performed. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: The paper poses no such risks. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 27}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}]