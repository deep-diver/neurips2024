{"references": [{"fullname_first_author": "Alexandr Andoni", "paper_title": "Beyond locality-sensitive hashing", "publication_date": "2014-01-01", "reason": "This paper is foundational to the MemoryFormer's use of locality-sensitive hashing (LSH), providing a theoretical basis for its approach."}, {"fullname_first_author": "Stella Biderman", "paper_title": "Pythia: A suite for analyzing large language models across training and scaling", "publication_date": "2023-01-01", "reason": "Pythia serves as the benchmark framework for the experiments, demonstrating the practical impact of MemoryFormer on various LLMs."}, {"fullname_first_author": "Moses S Charikar", "paper_title": "Similarity estimation techniques from rounding algorithms", "publication_date": "2002-01-01", "reason": "This paper is another fundamental reference for the LSH technique used in MemoryFormer, providing essential theoretical background."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-01-01", "reason": "This is the seminal paper introducing the Transformer architecture, which MemoryFormer modifies to improve computational efficiency."}, {"fullname_first_author": "Krzysztof Choromanski", "paper_title": "Rethinking attention with performers", "publication_date": "2020-01-01", "reason": "Performer is a key comparison point, illustrating an alternative approach to efficient Transformer design and highlighting the unique aspects of MemoryFormer."}]}