[{"figure_path": "PKcCHncbzg/tables/tables_6_1.jpg", "caption": "Table 1: Efficiency comparison with state-of-the-art methods. #Params(M) represents the total number of trainable parameters.", "description": "This table compares the efficiency of the proposed Relationship Prompt Network (RPN) method with two other state-of-the-art methods, ZegFormer and ZegCLIP, in terms of the number of trainable parameters (#Params), the number of floating point operations (FLOPS), and Frames Per Second (FPS) on the PASCAL VOC and COCO datasets.  The results highlight the superior efficiency of RPN, which achieves comparable or better performance using significantly fewer parameters and FLOPS while maintaining a higher FPS.", "section": "4.2 System Level Comparison"}, {"figure_path": "PKcCHncbzg/tables/tables_7_1.jpg", "caption": "Table 2: Performance comparison in the zero-shot setting (unit: %). Here, the best results are shown in bold and the second-best results are underlined. The self-training represents applying self-training via generating pseudo labels on all unlabeled pixels like in [19, 8]. The symbol '\u2020' indicates pseudo labels are merely annotated on unseen classes pixels excluding the ignore part.", "description": "This table compares the performance of different methods for zero-shot semantic segmentation on three datasets: VOC, COCO, and Context.  The comparison is done with and without self-training, showing pixel accuracy (pAcc), mean Intersection over Union (mIoU) for seen classes (mIoUs), unseen classes (mIoUu), and harmonic mean of IoU (hIoU).  The results highlight the effectiveness of the proposed RPN method, especially when self-training is employed.", "section": "4.2 System Level Comparison"}, {"figure_path": "PKcCHncbzg/tables/tables_7_2.jpg", "caption": "Table 3: Performance comparison in the open-vocabulary setting (unit: %). Here, the best results are shown in bold and the second-best results are underlined.", "description": "This table presents a comparison of different methods for open-vocabulary semantic segmentation.  The methods are evaluated on several datasets (A-847, PC-459, A-150, PC-59, PAS-20) using various Vision Language Models (VLMs) and training sets.  The table highlights the performance of each method in terms of pixel-wise accuracy, showcasing the relative strengths and weaknesses of different approaches in handling unseen classes during the segmentation process. The best and second best results for each dataset are highlighted for easy comparison.", "section": "4.2 System Level Comparison"}, {"figure_path": "PKcCHncbzg/tables/tables_8_1.jpg", "caption": "Table 4: Impact of different modules (unit: %). Methods without LPM represent eliminating the linear layers in LPM, i.e., discarding Linear(\u00b7) in Eq.7.", "description": "This table presents the ablation study on the impact of different modules in the proposed Relationship Prompt Network (RPN).  It compares the performance of the RPN with different combinations of the Relationship Prompt Module (RPM) components (M20E, ITP, APG) and the Linear Projection Module (LPM). The results show the contribution of each module and the overall impact of combining them. Removing LPM leads to a significant decrease in performance, while removing M20E leads to a moderate decrease.  The results demonstrate that RPM is essential for high performance.", "section": "4.3 Ablation Study"}, {"figure_path": "PKcCHncbzg/tables/tables_8_2.jpg", "caption": "Table 5: Ablation study on different designs in RPM (unit:%).", "description": "This table presents the ablation study of different designs within the Relationship Prompt Module (RPM). It explores the impact of removing image-level or pixel-level attention from the Image-to-Pixel Semantic Attention (ITP) block, the effect of using different activation functions (nn.Linear vs. nn.Parameter) in the Adaptive Prompt Generation (APG) block, and the impact of varying the dimension (r) and using different modes (Multi-Scale vs. M20E) in the Multi-scale Mixture-of-Experts (M20E) block.  The results show the impact of each component on the overall performance (pAcc, mIoUs, mIoUu, hIoU).", "section": "4.3 Ablation Study"}, {"figure_path": "PKcCHncbzg/tables/tables_9_1.jpg", "caption": "Table 6: Ablation study on three different LPMs using Softmax and Sigmoid losses (unit: %).", "description": "This table presents the ablation study on three different Linear Projection Module (LPM) designs, namely LPMa, LPMb, and LPMC, using two different loss functions: Softmax and Sigmoid.  Each LPM design is evaluated using both loss functions, showcasing the impact of the LPM design and loss function on the performance metrics: pixel accuracy (pAcc), mean Intersection over Union for seen classes (mIoUs), mean Intersection over Union for unseen classes (mIoUu), and their harmonic mean (hIoU). The results highlight the effectiveness of LPMc with the Sigmoid loss, achieving the best performance across all metrics.", "section": "4.3 Ablation Study"}, {"figure_path": "PKcCHncbzg/tables/tables_9_2.jpg", "caption": "Table 7: Ablation study on three different training-free projection modules (unit: %)", "description": "This table presents the ablation study of three different training-free projection modules (TFPMs).  The TFPMs are variants of the Linear Projection Module (LPM) that do not contain any trainable parameters. The table shows the performance (pAcc, mIoUs, mIoUu, and hIoU) achieved by each TFPM variant, indicating the impact of each module on the overall model's performance in semantic segmentation.", "section": "4.3 Ablation Study"}, {"figure_path": "PKcCHncbzg/tables/tables_15_1.jpg", "caption": "Table 2: Performance comparison in the zero-shot setting (unit: %). Here, the best results are shown in bold and the second-best results are underlined. The self-training represents applying self-training via generating pseudo labels on all unlabeled pixels like in [19, 8]. The symbol '\u2020' indicates pseudo labels are merely annotated on unseen classes pixels excluding the ignore part.", "description": "This table compares the performance of various methods on three datasets (VOC, COCO, and Context) in a zero-shot semantic segmentation setting.  It shows pixel accuracy (pAcc), mean IoU (mIoUs), mean IoU for unseen classes (mIoUu), and harmonic mean IoU (hIoU). Results are presented for three scenarios: without self-training, with self-training (using pseudo labels on unseen classes), and fully supervised. The table highlights the relative performance of different methods under each scenario and dataset.", "section": "4.2 System Level Comparison"}, {"figure_path": "PKcCHncbzg/tables/tables_16_1.jpg", "caption": "Table 2: Performance comparison in the zero-shot setting (unit: %). Here, the best results are shown in bold and the second-best results are underlined. The self-training represents applying self-training via generating pseudo labels on all unlabeled pixels like in [19, 8]. The symbol '\u2020' indicates pseudo labels are merely annotated on unseen classes pixels excluding the ignore part.", "description": "This table compares the performance of different methods on three datasets (VOC, COCO, and Context) in a zero-shot semantic segmentation setting.  The results are broken down by several metrics: Pixel Accuracy (pAcc), mean Intersection over Union for seen classes (mIoUs), mean Intersection over Union for unseen classes (mIoUu), and harmonic mean of IoU (hIoU).  The table further shows the results with and without self-training, and with fully supervised training, offering a comprehensive comparison under various conditions.", "section": "4.2 System Level Comparison"}, {"figure_path": "PKcCHncbzg/tables/tables_16_2.jpg", "caption": "Table 2: Performance comparison in the zero-shot setting (unit: %). Here, the best results are shown in bold and the second-best results are underlined. The self-training represents applying self-training via generating pseudo labels on all unlabeled pixels like in [19, 8]. The symbol '\u2020' indicates pseudo labels are merely annotated on unseen classes pixels excluding the ignore part.", "description": "This table presents a comparison of different methods' performance in a zero-shot semantic segmentation setting.  The metrics used are pixel accuracy (pAcc), mean Intersection over Union for seen classes (mIoUs), mean IoU for unseen classes (mIoUu), and harmonic mean IoU (hIoU).  The comparison is done across three datasets (VOC, COCO, and Context) and includes results with and without self-training. The self-training approach involves generating pseudo labels for unseen classes to improve model performance. Results are also provided for a fully supervised scenario for comparison.", "section": "4.2 System Level Comparison"}, {"figure_path": "PKcCHncbzg/tables/tables_17_1.jpg", "caption": "Table 3: Performance comparison in the open-vocabulary setting (unit: %). Here, the best results are shown in bold and the second-best results are underlined.", "description": "This table compares the performance of different methods for open-vocabulary semantic segmentation on various datasets.  The methods are evaluated based on their performance using Vision Language Models (VLMs) and different training sets. The results are presented as mean Intersection over Union (mIoU) scores for several classes in each dataset.  The table highlights the effectiveness of the proposed method compared to existing state-of-the-art techniques.", "section": "4.2 System Level Comparison"}, {"figure_path": "PKcCHncbzg/tables/tables_17_2.jpg", "caption": "Table 10: Performance comparison with PEFT methods (unit:%). Baseline represents the CLIP model with LPM. #Params(M) represents the number of trainable parameters during training.", "description": "This table compares the performance of the proposed Relationship Prompt Network (RPN) method against various parameter-efficient fine-tuning (PEFT) methods. The baseline is the CLIP model with Linear Projection Module (LPM).  The table shows the performance (pAcc, mIoU, mIoUu, hIoU) on the VOC and COCO datasets for each method, along with the number of trainable parameters (#Params(M)). This allows for a direct comparison of efficiency and effectiveness between different approaches to adapting the CLIP model for semantic segmentation.", "section": "4.2 System Level Comparison"}, {"figure_path": "PKcCHncbzg/tables/tables_17_3.jpg", "caption": "Table 11: Performance of combining our method and VPT (unit: %). #Params(M) represents the number of trainable parameters during training.", "description": "This table presents the results of combining the proposed Relationship Prompt Network (RPN) method with Visual Prompt Tuning (VPT). It compares the performance (pAcc, mIoUs, mIoUu, hIoU) on the VOC and COCO datasets, showing that adding VPT to RPN does not significantly improve performance while increasing the number of trainable parameters.", "section": "4.3 Ablation Study"}, {"figure_path": "PKcCHncbzg/tables/tables_17_4.jpg", "caption": "Table 12: Impact of different pre-trained weights for the plain encoder (unit: %).", "description": "This table presents the performance comparison of using different pre-trained weights (ViT, MAE, and CLIP) for the plain encoder in the proposed Relationship Prompt Network (RPN).  The results are shown in terms of pixel accuracy (pAcc), mean IoU for seen classes (mIoUs), mean IoU for unseen classes (mIoUu), and harmonic mean IoU (hIoU) for both VOC and COCO datasets.  It demonstrates the effect of different pre-training strategies on the model's performance in open-vocabulary semantic segmentation.", "section": "4.3 Ablation Study"}, {"figure_path": "PKcCHncbzg/tables/tables_18_1.jpg", "caption": "Table 13: Impact of RPM in different layers (unit:%). 'Number' represent the number of layer at which RPM is applied. Note that 'all' do not include the last layer.", "description": "This ablation study investigates the impact of applying the Relationship Prompt Module (RPM) at different layers of the vision language model. It shows the performance of applying RPM at different layers (1, 11, {1,3,5,7,9,11}, {2,4,6,8,10}, and all) on two datasets (VOC and COCO). The results indicate that applying RPM at multiple layers significantly improves performance compared to applying it at a single layer.", "section": "4.3 Ablation Study"}, {"figure_path": "PKcCHncbzg/tables/tables_18_2.jpg", "caption": "Table 14: Ablation study on three kinds of prefix-tuning operations (unit: %)", "description": "This table presents the ablation study of three different prefix-tuning operations (multiplication, addition, and concatenation) used in the Relationship Prompt Module (RPM). It shows the impact of each operation on the performance metrics (pAcc, mIoUs, mIoUu, hIoU) for both VOC and COCO datasets. The results demonstrate that concatenation is the most effective operation, yielding the best performance across all metrics and both datasets.", "section": "4.3 Ablation Study"}]