{"importance": "This paper is crucial for researchers in online learning and game theory.  It **challenges the common belief** that algorithms like Optimistic Multiplicative Weights Update (OMWU) can achieve fast last-iterate convergence, a highly desirable property. By identifying a class of algorithms that inherently suffer from slow convergence, the paper **opens new avenues of research** focused on designing forgetful algorithms for improved last-iterate performance in game settings. This has important implications for developing efficient AI agents and solving large-scale games.", "summary": "Forgetful algorithms are essential for fast last-iterate convergence in learning games; otherwise, even popular methods like OMWU fail.", "takeaways": ["Optimistic Follow-the-Regularized-Leader (OFTRL) algorithms, including OMWU, generally exhibit slow last-iterate convergence.", "The slow convergence is inherent to a broad class of algorithms that don't forget the past quickly, not just a limitation of current analysis.", "Forgetfulness is key to achieving fast last-iterate convergence, as demonstrated by the superior performance of Optimistic Gradient Descent-Ascent (OGDA)."], "tldr": "Many AI algorithms solve two-player zero-sum games using self-play via online learning. Popular algorithms include Optimistic Multiplicative Weights Update (OMWU) and Optimistic Gradient Descent-Ascent (OGDA). While both have good ergodic convergence, last-iterate convergence (performance of the final iteration) has been a focus of recent research. OGDA shows fast last-iterate convergence, but OMWU's performance depends on game-dependent constants that may be arbitrarily large. This paper investigates whether this is a fundamental limitation of OMWU or a problem with current analysis.\nThe paper proves that this slow convergence is inherent to a class of algorithms that don't 'forget' the past quickly. It shows that for any algorithm in this class, there exists a simple 2x2 game for which the algorithm's last iterate will have a constant duality gap even after many rounds. This class includes OMWU and other optimistic FTRL algorithms.  The paper demonstrates that forgetfulness is necessary for fast last-iterate convergence, and that this is generally needed for fast convergence, as seen in the good performance of OGDA.", "affiliation": "Yale", "categories": {"main_category": "AI Theory", "sub_category": "Optimization"}, "podcast_path": "hK7XTpCtBi/podcast.wav"}