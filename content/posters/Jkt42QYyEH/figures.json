[{"figure_path": "Jkt42QYyEH/figures/figures_0_1.jpg", "caption": "Figure 1: LiveScene enables scene-level reconstruction and control with language grounding. Left: Language-interactive articulated object control in Nerfstudio. Right: LiveScene achieves SOTA rendering quality on OmniSim dataset and exhibits a significant advantage in parameter efficiency.", "description": "This figure demonstrates LiveScene's capabilities in scene-level reconstruction and control using language. The left panel showcases an example of language-interactive articulated object control within the Nerfstudio framework, highlighting the system's ability to manipulate objects using natural language commands. The right panel presents a comparison of LiveScene's rendering quality and parameter efficiency against other state-of-the-art methods on the OmniSim dataset.  The graph shows that LiveScene achieves superior PSNR (Peak Signal-to-Noise Ratio) values, indicating higher rendering quality, while maintaining significantly fewer model parameters, even as the number of interactive objects increases. This illustrates LiveScene's efficiency in handling complex scenes with multiple interactive elements.", "section": "Abstract"}, {"figure_path": "Jkt42QYyEH/figures/figures_2_1.jpg", "caption": "Figure 2: The overview of LiveScene. Given a camera view and control variable K of one specific interactive object, a series of 3D points are sampled in a local deformable field that models the interactive motions of this specific interactive object, and then the interactive object with novel interactive motion state is generated via volume-rendering. Moreover, an interaction-aware language embedding is utilized to localize and control individual interactive objects using natural language.", "description": "This figure provides a detailed overview of the LiveScene architecture. It shows how the system takes a camera view and control variables as input and uses a series of steps to generate a 3D scene representation with interactive objects.  The process involves sampling 3D points within local deformable fields, rendering these points to create the interactive objects' motions, and using an interaction-aware language embedding to localize and control individual objects. This allows for natural language control of the scene.", "section": "3 Methodology"}, {"figure_path": "Jkt42QYyEH/figures/figures_3_1.jpg", "caption": "Figure 3: Illustration of hyperplanar factorization for compact storage. We maintain multiple local deformable fields for each interactive object region R\u1d62, and project high-dimensional interaction features into a compact 4D space, which can be further compressed into multiscale feature planes.", "description": "This figure illustrates the concept of hyperplanar factorization used in LiveScene for efficient storage of high-dimensional interaction data.  It shows how the complex interaction space is decomposed into multiple local 4D deformable fields, one for each interactive object region. High-dimensional interaction features from each region are then projected onto a compact 4D space, allowing for efficient storage and processing.  The multiscale feature planes further compress this data.  The left side shows the local deformable fields and how ray sampling interacts with them. The right side visualizes the compact storage method.", "section": "3.2 Multi-scale Interaction Space Factorization"}, {"figure_path": "Jkt42QYyEH/figures/figures_4_1.jpg", "caption": "Figure 4: Illustration of a) boundary sampling conflicts, b) rendering quality comparison.", "description": "This figure shows a comparison of the rendering quality with and without the proposed repulsion and probability rejection methods. In (a), it illustrates the boundary sampling conflicts that may occur during training when optimizing the interaction probability decoder with varying masks. This can lead to blurred boundaries in the local deformable field, causing sampling conflicts and feature oscillations. In (b), it shows the rendering quality comparison, demonstrating the effectiveness of the proposed methods in alleviating these conflicts and achieving higher rendering quality.", "section": "3.2 Multi-scale Interaction Space Factorization"}, {"figure_path": "Jkt42QYyEH/figures/figures_5_1.jpg", "caption": "Figure 5: Overview of the OmniSim and InterReal datasets.", "description": "The figure shows an overview of the OmniSim and InterReal datasets.  OmniSim is a synthetic dataset generated from OmniGibson, showing various indoor scenes with multiple interactive objects and their states. InterReal is a real-world dataset captured from real scenes, also featuring multiple interactive objects and diverse actions. Both datasets contain RGB images, depth maps, segmentations, camera poses, interaction variables, and object captions, providing rich information for training and evaluating interactive scene reconstruction and control models. The image showcases examples of scenes from both datasets.", "section": "4 Dataset"}, {"figure_path": "Jkt42QYyEH/figures/figures_7_1.jpg", "caption": "Figure 6: View Synthesis Visualization on CoNeRF Controllable Dataset. The proposed method achieves higher-quality rendering results compared with the existing methods.", "description": "This figure compares the novel view synthesis results of LiveScene against other state-of-the-art methods on the CoNeRF Controllable dataset.  The top row shows a toy robot interacting with a vehicle. The middle row shows a person's face, and the bottom row shows a car. The ground truth images (GT) are shown first, followed by results generated using HyperNeRF, CoNeRF, CoGS, and LiveScene (Ours). The figure demonstrates that LiveScene produces significantly higher-quality images with sharper details and more accurate color representation than the other compared methods.", "section": "5.1 View Synthesis Quality Comparison"}, {"figure_path": "Jkt42QYyEH/figures/figures_7_2.jpg", "caption": "Figure 7: View Synthesis Visualization on OmniSim Dataset. We compare our method with SOTA methods on RGB rendering across three scenes: #rs, #ihlen, and #pomaria. Boxes of different colors represent distinct interactive objects within the scene.", "description": "This figure compares the novel view synthesis results of LiveScene against state-of-the-art methods (CoNeRF, MKPlanes*, CoGS) across three scenes from the OmniSim dataset.  The scenes (\"#rs\", \"#ihlen\", and \"#pomaria\") showcase different levels of complexity in terms of interactive object arrangements.  Each row represents a specific scene with the ground truth (GT) image followed by the results of each method.  Colored boxes highlight individual interactive objects. The visualization demonstrates LiveScene's superior ability to accurately reconstruct complex interactive scenes, especially in challenging scenarios.", "section": "5.1 View Synthesis Quality Comparison"}, {"figure_path": "Jkt42QYyEH/figures/figures_8_1.jpg", "caption": "Figure 9: Rendering and Grounding Performance for #1 and #6. above): Multi-scale factorization greatly boosts the performance of RGB rendering and geometry reconstruction. below): Without view consistency, the model struggles when objects have similar appearances.", "description": "This figure compares the performance of LiveScene with and without two key components: multi-scale factorization and interaction-relevant features.  The top row shows a scene (#1) where multi-scale factorization significantly improves both RGB rendering quality and geometry reconstruction accuracy. The bottom row illustrates a scene (#6) where the lack of interaction-relevant features causes LiveScene to struggle when objects share similar appearances.  This demonstrates the effectiveness of these components in handling complex interactive scenes.", "section": "5.3 Ablation Study"}, {"figure_path": "Jkt42QYyEH/figures/figures_8_2.jpg", "caption": "Figure 10: Learning process of the probability fields from 0 to 1000 training steps. The model progressively converges to the vicinity of the interactive objects, establishing interactive regions.", "description": "This figure visualizes the learning process of the probability fields within LiveScene, demonstrating how the model progressively learns to identify and focus on interactive objects over 1000 training steps.  The heatmaps show the probability field at various stages, starting from a diffuse state and gradually converging towards the locations of the interactive elements.  This highlights the model's ability to learn and precisely localize interactive areas in complex scenes.", "section": "5.3 Ablation Study"}]