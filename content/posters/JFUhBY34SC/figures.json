[{"figure_path": "JFUhBY34SC/figures/figures_6_1.jpg", "caption": "Figure 1: Alignment and top eigenvalue for a 6-layer CNN model trained on CIFAR-10. The left panel shows the trend of alignment during SAM training; the shaded area represents the 95% confidence interval. The right panel displays the trend of the top eigenvalue over the course of training.", "description": "This figure consists of two sub-figures. The left sub-figure (a) shows the trend of perturbation-eigenvector alignment during the training process of SAM. The shaded area represents the 95% confidence interval.  The right sub-figure (b) shows the trend of the top eigenvalue of the Hessian matrix during training for SAM, Eigen-SAM, and SGD. The figure demonstrates that the alignment between the perturbation vector and the top eigenvector of the Hessian is often poor during training, which limits the effectiveness of SAM in regularizing sharpness. Eigen-SAM shows a significantly lower top eigenvalue compared to SAM and SGD.", "section": "5 Eigen-SAM: an explicit regularization method for the top eigenvalue of the Hessian"}, {"figure_path": "JFUhBY34SC/figures/figures_6_2.jpg", "caption": "Figure 1: Alignment and top eigenvalue for a 6-layer CNN model trained on CIFAR-10. The left panel shows the trend of alignment during SAM training; the shaded area represents the 95% confidence interval. The right panel displays the trend of the top eigenvalue over the course of training.", "description": "This figure shows the results of training a 6-layer Convolutional Neural Network (CNN) on the CIFAR-10 dataset using three different optimization methods: Sharpness-Aware Minimization (SAM), Eigen-SAM (a proposed variant of SAM), and standard Stochastic Gradient Descent (SGD). The left panel shows the alignment between the perturbation vector used in SAM and the top eigenvector of the Hessian matrix during training.  A high degree of alignment is desirable for effective sharpness reduction. The right panel shows the top eigenvalue of the Hessian matrix over the training process. A smaller top eigenvalue typically indicates better generalization. The figure demonstrates that while SAM shows some improvement over SGD, Eigen-SAM achieves better alignment and consequently reduces the top eigenvalue more effectively.", "section": "5 Eigen-SAM: an explicit regularization method for the top eigenvalue of the Hessian"}, {"figure_path": "JFUhBY34SC/figures/figures_8_1.jpg", "caption": "Figure 2: Training dynamics of discrete SAM, second-order SDE, and third-order SDE during training. Metrics include training loss, test loss, test accuracy, parameter norm, gradient norm, and the top Hessian eigenvalue. Each plot illustrates how each approach affects loss dynamics and key stability metrics.", "description": "This figure compares the training dynamics of three different approaches: discrete SAM, a second-order stochastic differential equation (SDE), and the authors' proposed third-order SDE.  Six key metrics are plotted over training steps: training loss, test loss, test accuracy, parameter norm, gradient norm, and the top eigenvalue of the Hessian.  The plots visually demonstrate the differences in how each method affects the loss landscape and key stability indicators during the training process.", "section": "6 Experiments"}, {"figure_path": "JFUhBY34SC/figures/figures_9_1.jpg", "caption": "Figure 1: Alignment and top eigenvalue for a 6-layer CNN model trained on CIFAR-10. The left panel shows the trend of alignment during SAM training; the shaded area represents the 95% confidence interval. The right panel displays the trend of the top eigenvalue over the course of training.", "description": "This figure displays two sub-figures. The left sub-figure shows the trend of alignment between the perturbation vector and the top eigenvector during the training process of SAM algorithm on a 6-layer CNN model trained on CIFAR-10 dataset. The shaded area indicates the 95% confidence interval. The right sub-figure shows the trend of the top eigenvalue of the Hessian matrix over the training process. Both sub-figures show that the alignment between the perturbation vector and the top eigenvector is poor, and SAM does not effectively minimize the top eigenvalue.", "section": "5 Eigen-SAM: an explicit regularization method for the top eigenvalue of the Hessian"}, {"figure_path": "JFUhBY34SC/figures/figures_13_1.jpg", "caption": "Figure 4: Comparison of training loss and test loss metrics across algorithms.", "description": "This figure compares the training and test loss curves for three different algorithms: SAM, Reverse-SAM, and EGR.  The plots show the loss values over the course of training.  Reverse-SAM uses the negative of the normalized gradient as the perturbation vector, while EGR explicitly regularizes the gradient norm. The goal is to demonstrate the limitations of existing theories in explaining SAM's practical outcomes by showing that the performance of these alternative algorithms differs significantly from SAM. The figure provides empirical evidence highlighting the discrepancies between theoretical analysis and practical observations in the context of sharpness-aware minimization.", "section": "A Additional empirical evidence"}, {"figure_path": "JFUhBY34SC/figures/figures_23_1.jpg", "caption": "Figure 1: Alignment and top eigenvalue for a 6-layer CNN model trained on CIFAR-10. The left panel shows the trend of alignment during SAM training; the shaded area represents the 95% confidence interval. The right panel displays the trend of the top eigenvalue over the course of training.", "description": "This figure shows the results of training a 6-layer CNN model on the CIFAR-10 dataset using three different optimization methods: SAM, Eigen-SAM, and SGD. The left panel shows the alignment between the perturbation vector and the top eigenvector of the Hessian matrix over training steps.  The shaded area indicates the 95% confidence interval for the alignment. The right panel displays the top eigenvalue of the Hessian matrix over training steps.  The plots reveal that SAM struggles to efficiently minimize the top eigenvalue because the perturbation vector and the top eigenvector of the Hessian are poorly aligned.  Eigen-SAM, in contrast, is designed to address this alignment issue, leading to more efficient minimization of the top eigenvalue. ", "section": "5 Eigen-SAM: an explicit regularization method for the top eigenvalue of the Hessian"}]