[{"figure_path": "JFUhBY34SC/tables/tables_7_1.jpg", "caption": "Table 1: Test accuracy on CIFAR-10, CIFAR-100, Fashion-MNIST, SVHN.", "description": "This table presents the test accuracy results achieved by different optimization methods (SGD, SAM, and Eigen-SAM) on four benchmark datasets: CIFAR-10, CIFAR-100, Fashion-MNIST, and SVHN.  Three different network architectures (ResNet18, ResNet50, and WideResNet-28-10) were used to evaluate each method. The results are presented as mean \u00b1 standard deviation, indicating the performance variability.  The table aims to demonstrate the improved generalization capability of Eigen-SAM compared to traditional SGD and SAM.", "section": "6 Experiments"}, {"figure_path": "JFUhBY34SC/tables/tables_8_1.jpg", "caption": "Table 1: Test accuracy on CIFAR-10, CIFAR-100, Fashion-MNIST, SVHN.", "description": "This table presents the test accuracy results achieved by three different optimization methods: SGD, SAM, and Eigen-SAM.  The results are reported for four different image classification datasets: CIFAR-10, CIFAR-100, Fashion-MNIST, and SVHN.  Three different network architectures were used: ResNet18, ResNet50, and WideResNet-28-10.  The table shows that Eigen-SAM consistently outperforms both SGD and SAM across all datasets and architectures, demonstrating its effectiveness in improving generalization performance.", "section": "6 Experiments"}, {"figure_path": "JFUhBY34SC/tables/tables_8_2.jpg", "caption": "Table 2: Test accuracy for fine-tuning ViT-B-16 pretrained on ImageNet-1K on CIFAR-10 and CIFAR-100.", "description": "This table presents the test accuracy results for fine-tuning a Vision Transformer (ViT-B-16) model pre-trained on ImageNet-1K, specifically for the CIFAR-10 and CIFAR-100 datasets.  Three optimization methods are compared: SGD (Stochastic Gradient Descent), SAM (Sharpness-Aware Minimization), and Eigen-SAM (the proposed method). The results show the test accuracy achieved by each method, along with the corresponding 95% confidence intervals, indicating the performance variability of each algorithm across different runs.  The table highlights the performance improvement achieved by Eigen-SAM compared to both SGD and SAM, demonstrating its effectiveness in fine-tuning pre-trained models for improved generalization on image classification tasks.", "section": "6.3 Finetuning"}, {"figure_path": "JFUhBY34SC/tables/tables_22_1.jpg", "caption": "Table 1: Test accuracy on CIFAR-10, CIFAR-100, Fashion-MNIST, SVHN.", "description": "This table presents the test accuracy results for different image classification models (ResNet18, ResNet50, WideResNet-28-10) trained on four benchmark datasets (CIFAR-10, CIFAR-100, Fashion-MNIST, SVHN) using three optimization methods: standard SGD, SAM (Sharpness-Aware Minimization), and Eigen-SAM (the proposed method).  The results are presented as mean \u00b1 standard deviation, illustrating the performance comparison across various architectures and datasets.", "section": "6.2 Image classification from scratch"}, {"figure_path": "JFUhBY34SC/tables/tables_22_2.jpg", "caption": "Table 1: Test accuracy on CIFAR-10, CIFAR-100, Fashion-MNIST, SVHN.", "description": "This table presents the test accuracy results for different image classification datasets (CIFAR-10, CIFAR-100, Fashion-MNIST, and SVHN) using various models (ResNet18, ResNet50, and WideResNet-28-10) and optimization methods (SGD, SAM, and Eigen-SAM).  The table shows the mean test accuracy and standard deviation for each combination, highlighting the performance improvement achieved by Eigen-SAM compared to standard SAM and SGD.", "section": "6 Experiments"}]