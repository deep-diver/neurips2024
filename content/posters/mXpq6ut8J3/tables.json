[{"figure_path": "mXpq6ut8J3/tables/tables_5_1.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table presents the main results of the SWE-agent experiments, comparing its performance on the SWE-bench dataset with various baseline models. The table is broken down into two sections: the full SWE-bench dataset and a smaller subset called SWE-bench Lite. For each dataset, the results for three main approaches are reported: RAG (Retrieval Augmented Generation), the Shell-only agent, and the SWE-agent. For each approach, the percentage of instances resolved and the average cost are listed for both GPT-4 Turbo and Claude 3 Opus models.  The results showcase the improved performance of SWE-agent compared to the baselines, demonstrating the effectiveness of its custom agent-computer interface.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_5_2.jpg", "caption": "Table 2: Pass@1 results on HumanEvalFix [32]. Except for SWE-agent, we use scores as reported in Yu et al. [65].", "description": "This table presents the pass@1 scores achieved by different language models on the HumanEvalFix benchmark.  HumanEvalFix is a code debugging benchmark focusing on short-form code fixes.  The pass@1 metric indicates the percentage of test cases where the model's generated code passes all tests after the fixes are applied.  The table shows that SWE-agent, using GPT-4 Turbo, significantly outperforms other models, achieving a pass@1 rate of 87.7% across Python, JavaScript, and Java tasks.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_5_3.jpg", "caption": "Table 3: SWE-bench Lite performance under ablations to the SWE-agent interface, which is denoted by. We consider different approaches to searching and editing (see Figures 5 and 6, respectively). We also verify how varying the file viewer window size affects performance, and we ablate the effect of different context management approaches.", "description": "This table presents the results of ablation studies performed on the SWE-agent system, which involved modifying different aspects of the agent-computer interface (ACI) and evaluating its impact on performance.  The ablations include modifications to the search interface (summarized, iterative, no search), the file editing interface (edit action w/ linting, no edit), the file viewer (30 lines, 100 lines, full file), and the context management (last 5 obs, full history, w/o demo). The results show the percentage of instances solved under each configuration, highlighting the effect of design choices on the agent's performance.", "section": "Analysis of ACI Design"}, {"figure_path": "mXpq6ut8J3/tables/tables_16_1.jpg", "caption": "Table 4: In addition to the standard Linux Bash commands, we provide SWE-agent with specialized tools, including an interactive file viewer, search functionalities, and edit tools for the open file. Required arguments are enclosed in <> and optional arguments are in []. The last column shows the documentation presented to the LM.", "description": "This table lists the commands available to the SWE-agent.  It categorizes commands into four groups: File Viewer, Search tools, File editing, and Task.  For each command, it provides the command syntax, specifying required and optional arguments, and a description of the command's function and documentation provided to the language model.", "section": "3 SWE-agent: Designing an ACI for Software Engineering"}, {"figure_path": "mXpq6ut8J3/tables/tables_24_1.jpg", "caption": "Table 5: Hyper parameter sweep results on a subset of the SWE-bench dev split. % Resolved shows the mean score across 5 samples.", "description": "This table presents the results of a hyperparameter sweep conducted on a subset of the SWE-bench development dataset.  The sweep involved varying three hyperparameters: temperature, window size, and history length.  The table shows the resulting mean % Resolved rate (percentage of instances solved successfully) for each combination of hyperparameter settings across five samples. This helps identify the best performing hyperparameter combination for the model.", "section": "B.1 Hyperparameter Sweep"}, {"figure_path": "mXpq6ut8J3/tables/tables_25_1.jpg", "caption": "Table 6: % Resolved performance across repositories represented in the SWE-bench Lite dataset. Each row corresponds to a repository while each column is the model's performance for that repository. The numbers in parentheses in the \"Repo\" column is the number of task instances in SWE-bench Lite that are from the corresponding repository.", "description": "This table shows the performance of SWE-agent and RAG baselines on the SWE-bench Lite dataset, broken down by repository.  It shows the percentage of instances successfully resolved for each model and each repository.  The numbers in parentheses indicate the number of instances from each repository in the dataset. This allows for comparison of model performance across different repositories and provides insight into which repositories are more challenging for the models.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_25_2.jpg", "caption": "Table 6: % Resolved performance across repositories represented in the SWE-bench Lite dataset. Each row corresponds to a repository while each column is the model's performance for that repository. The numbers in parentheses in the \"Repo\" column is the number of task instances in SWE-bench Lite that are from the corresponding repository.", "description": "This table shows the success rate of SWE-agent and RAG baselines across 12 different repositories included in SWE-bench Lite.  It demonstrates SWE-agent's improved performance compared to the baselines, especially in repositories where baselines had low success rates. The numbers in parentheses indicate the number of instances from each repository.", "section": "B.2 Model Performance"}, {"figure_path": "mXpq6ut8J3/tables/tables_27_1.jpg", "caption": "Table 8: We present a table of the most frequently occurring action patterns at each turn (\"frequently\" means \u2265 4 times) in trajectories of task instances resolved by SWE-agent w/ GPT-4. For instance, the pattern create,edit,python appears 156 times at the first to third turns. In addition, we also manually assign each entry a category (Reproduction, Localization (File), Localization (Line), Editing, Submission) that generally captures the underlying purpose of such a pattern. \"Reproduction\" refers to the sub-task of recreating the error or request described by the issue. \u201cLocalization\" refers to the sub-task of identifying the code that is the cause of the issue.", "description": "This table shows the frequency of action patterns within resolved trajectories of SWE-agent with GPT-4. Each row shows a sequence of actions (pattern) and its frequency across several consecutive turns.  The table also categorizes each pattern to describe the general step in the problem-solving process: Reproduction (reproducing the problem), Localization (File/Line) (identifying the relevant file or code lines), Editing (making changes to the code), and Submission (submitting the solution).", "section": "Analysis of Agent Behavior"}, {"figure_path": "mXpq6ut8J3/tables/tables_34_1.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table presents the main results of the SWE-agent model's performance on the SWE-bench dataset. It compares the performance of SWE-agent against two baseline models: RAG (Retrieval Augmented Generation) and Shell-only.  The results are shown for both the full SWE-bench dataset and a smaller subset called SWE-bench Lite.  The metrics used to evaluate performance are: the percentage of instances successfully resolved (% Resolved) and the average cost ($ Avg. Cost).  The table highlights the significant improvement in performance achieved by SWE-agent, particularly when using GPT-4 Turbo as the base language model.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_34_2.jpg", "caption": "Table 6: % Resolved performance across repositories represented in the SWE-bench Lite dataset. Each row corresponds to a repository while each column is the model's performance for that repository. The numbers in parentheses in the \"Repo\" column is the number of task instances in SWE-bench Lite that are from the corresponding repository.", "description": "This table shows the percentage of successfully resolved tasks for each of the 12 repositories in the SWE-bench Lite dataset.  The performance is broken down by model (SWE-agent with GPT-4 Turbo and SWE-agent with Claude 3 Opus) and includes a comparison to a retrieval-augmented generation (RAG) baseline and Claude 2.  The numbers in parentheses indicate the number of tasks from each repository in the Lite dataset.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_35_1.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table compares the performance of SWE-agent with different language models on the SWE-bench dataset. It shows the percentage of resolved instances, which represents the successful resolution of software engineering tasks, and the average cost of each run. The table also includes baseline results from RAG and Shell-only agent to showcase the improvement achieved by SWE-agent.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_36_1.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table presents the main results of the SWE-agent model's performance on two versions of the SWE-bench dataset: the full dataset and a smaller \"Lite\" version.  It compares SWE-agent's performance against two baseline models:  a non-interactive retrieval-augmented generation (RAG) model and a model using only the default Linux shell.  The table shows the percentage of tasks successfully solved (% Resolved) and the average cost in USD ($ Avg. Cost) for each model and dataset.  The results highlight the significant improvement achieved by SWE-agent due to the custom ACI compared to the baselines.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_37_1.jpg", "caption": "Table 13: This table showcases the counts for the four ways (\"Submit\", \"Exit Cost (Submit)\", \"Exit Cost (No Submit)\", \"Early Exit\") a task episode could conclude.", "description": "This table shows the number of task instances that ended in each of four ways.  These ways represent how the task episode concluded: by a successful submission, by exceeding the cost budget (and submitting or not submitting changes), or by prematurely terminating the run due to too many invalid responses. The data is split between fully resolved trajectories and all trajectories.  The numbers are broken down by model (SWE-agent w/ GPT-4 Turbo and SWE-agent w/ Claude 3 Opus) and by dataset split (Full and Lite).", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_42_1.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table presents the performance comparison of SWE-agent with different language models on the SWE-bench dataset. It compares SWE-agent's performance against two baseline models: RAG (Retrieval Augmented Generation) and Shell-only agent. The table shows the percentage of resolved issues (% Resolved) and the average cost ($ Avg. Cost) for each model and setting on both the full SWE-bench and the smaller SWE-bench Lite dataset. It highlights SWE-agent's superior performance and improved resolve rate, especially when compared to the RAG baseline.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_44_1.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table presents the performance of different models on the SWE-bench dataset, comparing three different settings: SWE-agent, Basic CLI (Shell-only), and RAG (Retrieval Augmented Generation).  It shows the percentage of tasks successfully resolved (% Resolved) and the average cost ($ Avg. Cost) for both the full SWE-bench dataset and a smaller subset called SWE-bench Lite.  The results highlight the improved performance of SWE-agent, particularly when using GPT-4 Turbo, compared to the baseline methods.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_48_1.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table presents the performance comparison of different models and settings on SWE-bench. It includes SWE-agent with GPT-4 Turbo and Claude 3 Opus, Shell-only agent with GPT-4 Turbo and Claude 3 Opus, and RAG baselines with GPT-4 Turbo and Claude 3 Opus. The metrics used for comparison are \\% Resolved and average cost. The table provides a detailed breakdown of the performance of each method on both the full SWE-bench test set and a subset of 300 instances from the SWE-bench test set known as SWE-bench Lite.  This allows for comparison of the performance in the complete and a representative subset of the benchmark.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_48_2.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table compares the performance of SWE-agent with different language models (GPT-4 Turbo and Claude 3 Opus) against two baseline methods: Retrieval Augmented Generation (RAG) and a Shell-only agent. The performance is measured by the percentage of successfully resolved instances (% Resolved) on both the full SWE-bench dataset and a smaller subset (SWE-bench Lite).  The table also shows the average cost ($) for each method.  The results demonstrate that SWE-agent significantly outperforms the baseline methods, achieving state-of-the-art results on both datasets.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_49_1.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table presents the performance of SWE-agent on the SWE-bench dataset, comparing it to two baselines: RAG and Shell-only.  The results show the percentage of instances resolved (% Resolved) and the average cost ($ Avg. Cost) for both the full SWE-bench dataset and a smaller subset called SWE-bench Lite.  It highlights SWE-agent's superior performance, particularly when using GPT-4 Turbo as the underlying language model.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_55_1.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table presents the performance of SWE-agent, along with baselines, on the SWE-bench dataset.  It compares the percentage of resolved issues (% Resolved) and the average cost ($ Avg. Cost) for both the full SWE-bench dataset and a smaller subset (SWE-bench Lite) across different model and interface settings (SWE-agent, Basic CLI, and RAG).  This helps demonstrate the impact of the custom Agent-Computer Interface (ACI) developed for SWE-agent.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_56_1.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table shows the performance of different models on the SWE-bench dataset. The models are tested using three different settings: SWE-agent, Basic CLI, and RAG. The table shows the percentage of instances that were successfully resolved and the average cost for each setting. SWE-agent shows significantly better results compared to other settings.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_58_1.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table presents the main results of the SWE-agent experiments conducted on the SWE-bench dataset.  It compares the performance (% Resolved) and average cost of SWE-agent using two different large language models (LLMs), GPT-4 Turbo and Claude 3 Opus, against two baseline approaches: RAG (Retrieval Augmented Generation) and a Shell-only agent.  The results are shown for both the full SWE-bench dataset and a smaller subset (SWE-bench Lite) focusing on functional bug fixes.  The table highlights SWE-agent's significantly improved performance compared to previous state-of-the-art approaches, particularly using the GPT-4 Turbo LLM.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_58_2.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table compares the performance of SWE-agent with different language models on the SWE-bench dataset. It shows the percentage of resolved issues (% Resolved) and the average cost ($ Avg. Cost) for both the full SWE-bench dataset and a smaller subset called SWE-bench Lite.  The table also includes results for two baseline approaches: RAG (Retrieval Augmented Generation) and a Shell-only agent, which uses only the default Linux shell.  The comparison highlights the improvement achieved by SWE-agent's custom agent-computer interface (ACI) in solving software engineering tasks.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_60_1.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table presents the main results of the SWE-agent experiments.  It compares the performance of SWE-agent (using GPT-4 Turbo and Claude 3 Opus) against two baselines: a retrieval-augmented generation (RAG) approach and a system that only uses the Linux shell. The performance is measured by the percentage of successfully resolved instances in the full SWE-bench dataset and a smaller subset (SWE-bench Lite). The table also includes the average cost (in USD) for each system.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_66_1.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table presents the main results of the SWE-agent experiments.  It compares the performance of SWE-agent with GPT-4 Turbo and Claude 3 Opus against two baseline models: a retrieval augmented generation (RAG) model and a shell-only agent. The comparison is made on two subsets of the SWE-bench dataset: the full test set and a smaller \"Lite\" subset. The table shows the percentage of instances successfully resolved (% Resolved) and the average cost in USD for each setting.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_71_1.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table presents the performance of SWE-agent, compared to two baselines (RAG and Shell-only), on the SWE-bench dataset. The performance is measured by the percentage of instances where all tests passed after applying the generated patch (% Resolved) and the average API inference cost.  The table shows results for both the full SWE-bench dataset and a smaller subset (SWE-bench Lite) and highlights the performance improvements achieved by SWE-agent's custom ACI.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_72_1.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table presents the main results of the SWE-agent performance evaluation on the SWE-bench dataset.  It compares the percentage of resolved issues and the average cost for three different settings: SWE-agent (with GPT-4 Turbo and Claude 3 Opus), Basic CLI (Shell-only agent with GPT-4 Turbo and without demonstration), and Retrieval Augmented Generation (RAG) with GPT-4 Turbo and Claude 3 Opus.  The comparison shows SWE-agent's significant improvement in problem-solving capability compared to the other methods, especially in the SWE-bench Lite subset.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_72_2.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table presents the results of the SWE-agent model on the SWE-bench dataset, comparing its performance to two baseline models: RAG and Shell-only.  The table shows the percentage of tasks successfully solved (% Resolved) and the average cost ($ Avg. Cost) for both the full SWE-bench dataset and a smaller subset known as SWE-bench Lite.  The results highlight the improvement in performance achieved by SWE-agent, particularly when using the GPT-4 Turbo language model.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_74_1.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table shows the performance of different models on the SWE-bench dataset.  It compares three different settings: SWE-agent, Basic CLI (command-line interface only), and RAG (Retrieval Augmented Generation).  The performance is measured by the percentage of tasks successfully solved (% Resolved) and the average cost (in USD).  It breaks down results for the full SWE-bench and the smaller SWE-bench Lite subsets.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_79_1.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table presents the results of the SWE-agent model's performance on the SWE-bench dataset.  It compares the performance of SWE-agent (using both GPT-4 Turbo and Claude 3 Opus language models) against two baselines: RAG (Retrieval Augmented Generation) and a Shell-only agent.  The table shows the percentage of resolved instances (% Resolved) and the average cost ($ Avg. Cost) for both the full SWE-bench dataset and a smaller subset (SWE-bench Lite).  It highlights the significant improvement in performance achieved by SWE-agent using its custom Agent-Computer Interface (ACI) compared to the baselines.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_79_2.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table presents the performance of different models on the SWE-bench dataset.  It compares the performance of SWE-agent (with GPT-4 Turbo and Claude 3 Opus) against two baselines: a retrieval augmented generation (RAG) approach and a shell-only agent.  The results are shown for both the full SWE-bench dataset and a smaller subset called SWE-bench Lite.  The metrics used are the percentage of successfully resolved issues and the average cost.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_83_1.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table presents the main results of the SWE-agent model's performance on the SWE-bench dataset.  It compares the performance of SWE-agent using GPT-4 Turbo and Claude 3 Opus against two baseline models: a non-interactive RAG baseline and a shell-only agent. The results are shown for both the full SWE-bench dataset and a smaller subset called SWE-bench Lite.  The table includes the percentage of resolved instances (% Resolved) and the average cost ($ Avg. Cost) for each model and setting.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_84_1.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table presents the main results of the SWE-agent experiments. It compares the performance of SWE-agent with different Language Models (LMs) on the full and Lite splits of the SWE-bench dataset.  It also includes results from two baseline methods: RAG (Retrieval Augmented Generation) and a shell-only agent. The table shows the percentage of instances successfully resolved (% Resolved) and the average cost ($ Avg. Cost) for each model and setting. The results demonstrate the superior performance of SWE-agent on both splits compared to the baselines, highlighting the impact of the agent-computer interface.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_84_2.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table presents the results of the SWE-agent model on the SWE-bench dataset. It compares the performance of SWE-agent with different language models (GPT-4 Turbo and Claude 3 Opus) against two baseline models: RAG (Retrieval Augmented Generation) and a Shell-only agent. The table shows the percentage of successfully resolved instances (% Resolved) and the average cost ($ Avg. Cost) for both the full SWE-bench dataset and a smaller subset called SWE-bench Lite.  The results highlight the improved performance of SWE-agent with an LM-friendly ACI compared to the baseline models.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_86_1.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table presents the performance comparison of different models on the SWE-bench dataset.  It shows the percentage of successfully resolved instances (% Resolved) and the average cost ($ Avg. Cost) for each model across two settings: the full SWE-bench test set and a smaller subset called SWE-bench Lite.  Three types of model setups are compared: Retrieval Augmented Generation (RAG), a basic command line interface (Basic CLI), and the proposed SWE-agent approach.  Results are given for GPT-4 Turbo and Claude 3 Opus Language Models.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_87_1.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table shows the performance of different models (SWE-agent with GPT-4 Turbo and Claude 3 Opus, Shell-only agent with GPT-4 Turbo, and RAG with GPT-4 Turbo and Claude 3 Opus) on the SWE-bench dataset.  The performance is measured by the percentage of instances resolved and the average cost.  The table compares the performance of SWE-agent to two baselines: RAG and Shell-only. The SWE-bench dataset contains 2,294 instances for the full test set and 300 instances for the SWE-bench Lite test set.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_87_2.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table presents the performance of different models on two versions of the SWE-bench dataset: the full dataset and a smaller subset called SWE-bench Lite.  The models are evaluated using three different approaches: SWE-agent (the proposed method), Basic CLI (interacting directly with the Linux shell), and Retrieval Augmented Generation (RAG; a non-interactive method). The table shows the percentage of tasks successfully resolved (% Resolved) and the average cost ($ Avg. Cost) for each model and approach on both datasets. This table highlights the significant improvement in performance achieved by SWE-agent compared to the baseline methods.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_87_3.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table compares the performance of SWE-agent with different language models on the SWE-bench dataset.  It contrasts SWE-agent's performance against two baselines: a non-interactive retrieval augmented generation (RAG) method and an agent interacting only with the basic command line interface (CLI). The table shows the percentage of successfully resolved issues (\"% Resolved\") and the average cost in USD (\"$ Avg. Cost\") for both the full SWE-bench dataset and a smaller subset called SWE-bench Lite.  This highlights the impact of the agent-computer interface (ACI) on the performance of language models in complex software engineering tasks.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_88_1.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table presents the main results of the SWE-agent experiments on the SWE-bench dataset, comparing its performance to two baselines: RAG and Shell-only.  It shows the percentage of successfully resolved issues (% Resolved) and the average cost ($ Avg. Cost) for both the full SWE-bench dataset and a smaller subset called SWE-bench Lite.  The results are broken down by model (GPT-4 Turbo and Claude 3 Opus) and experimental setup (SWE-agent, Basic CLI, RAG).", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_90_1.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table presents the performance of SWE-agent on the SWE-bench dataset, broken down by different settings (SWE-agent, Basic CLI, RAG) and models (GPT-4 Turbo, Claude 3 Opus).  The main metric is the percentage of instances resolved successfully (\\% Resolved) and the average cost of API calls ($ Avg. Cost). It compares the performance of SWE-agent with different baselines on both the full SWE-bench dataset and a smaller subset (SWE-bench Lite).  The table highlights SWE-agent's significantly improved performance compared to the baselines.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_92_1.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table shows the performance of SWE-agent and two baseline models (RAG and Shell-only agent) on the SWE-bench dataset.  The results are broken down by dataset split (full SWE-bench and SWE-bench Lite) and model (GPT-4 Turbo and Claude 3 Opus).  For each model and split, the table shows the percentage of instances successfully resolved and the average cost (in USD) per instance.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_95_1.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table presents the main results of the SWE-agent experiments on the SWE-bench dataset.  It compares the performance of SWE-agent with GPT-4 Turbo and Claude 3 Opus against two baseline methods: RAG (Retrieval Augmented Generation) and a shell-only agent.  The results are shown for both the full SWE-bench dataset and a smaller subset called SWE-bench Lite. The table shows the percentage of instances successfully resolved (% Resolved) and the average cost ($ Avg. Cost) for each model and setting.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_96_1.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table presents the main results of the SWE-agent's performance on the SWE-bench dataset, broken down by the full and Lite splits. It compares SWE-agent's performance against two baselines: RAG (Retrieval Augmented Generation) and a shell-only agent. The table shows the percentage of resolved instances (% Resolved) and the average cost ($ Avg. Cost) for each model and setting.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_96_2.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table shows the performance comparison of different models (SWE-agent with GPT-4 Turbo and Claude 3 Opus, Shell-only agent with GPT-4 Turbo, and RAG with GPT-4 Turbo and Claude 3 Opus) on two splits of the SWE-bench dataset (full and Lite).  For each model and dataset split, the table shows the percentage of resolved instances (% Resolved) and the average cost in USD ($ Avg. Cost). The results demonstrate that SWE-agent significantly outperforms the baseline methods (Shell-only and RAG) in terms of the percentage of resolved instances, although at a higher cost.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_97_1.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table presents the main results of the SWE-agent experiments, comparing its performance on the SWE-bench dataset against two baselines: RAG (Retrieval Augmented Generation) and Shell-only.  It shows the percentage of resolved instances (% Resolved) and the average cost ($ Avg. Cost) for each model and setting on both the full SWE-bench dataset and a smaller subset called SWE-bench Lite.  The results highlight the improved performance of SWE-agent, particularly when using GPT-4 Turbo, demonstrating the effectiveness of the custom agent-computer interface (ACI) in enhancing language model agents' ability to solve software engineering tasks.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_101_1.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table shows the performance comparison of different models on the SWE-bench dataset. The dataset is split into 'full' and 'Lite' versions. The models are evaluated under three settings: SWE-agent, basic CLI, and RAG.  For each model and setting, the table shows the percentage of resolved issues (% Resolved) and the average cost ($ Avg. Cost). The results highlight SWE-agent's superior performance compared to the other settings.", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_104_1.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table presents the main results of the SWE-agent experiments on the SWE-bench dataset, comparing its performance against two baseline models: RAG (Retrieval Augmented Generation) and Shell-only.  It shows the percentage of instances successfully resolved and the average API cost for both the full SWE-bench dataset and a smaller subset (SWE-bench Lite).  The results are broken down by model (GPT-4 Turbo and Claude 3 Opus) and setting (SWE-agent, Basic CLI, and RAG).", "section": "5 Results"}, {"figure_path": "mXpq6ut8J3/tables/tables_109_1.jpg", "caption": "Table 1: Main results for SWE-agent performance on the full and Lite splits of the SWE-bench test set. We benchmark models in the SWE-agent, Basic CLI, and Retrieval Augmented Generation (RAG) settings established in SWE-bench [20].", "description": "This table presents the main results of the SWE-agent model's performance on the SWE-bench dataset, broken down by the full and Lite splits. It compares SWE-agent's performance against two baselines: RAG and Shell-only agent. The metrics used are % Resolved and Average Cost.  The table showcases the performance of SWE-agent using two different large language models, GPT-4 Turbo and Claude 3 Opus, highlighting the improvement achieved by the custom Agent-Computer Interface (ACI).", "section": "5 Results"}]