{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-00-00", "reason": "This paper introduced the Transformer architecture, a crucial component of PowerPM's temporal encoder which significantly impacts its performance."}, {"fullname_first_author": "Jacob Devlin", "paper_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "publication_date": "2018-00-00", "reason": "BERT's pre-training techniques inspired PowerPM's self-supervised pre-training framework, enabling the model to capture temporal dependencies and discrepancies across ETS windows effectively."}, {"fullname_first_author": "Yue Cui", "paper_title": "Metro: A generic graph neural network framework for multivariate time series forecasting", "publication_date": "2021-00-00", "reason": "This paper's graph neural network (GNN) approach informed PowerPM's hierarchical encoder design, allowing it to model correlations between different levels of hierarchy in ETS data."}, {"fullname_first_author": "Gerald Woo", "paper_title": "COST: Contrastive learning of disentangled seasonal-trend representations for time series forecasting", "publication_date": "2022-00-00", "reason": "PowerPM's dual-view contrastive learning strategy was partly inspired by this paper's contrastive learning approach for time series, enhancing representation learning."}, {"fullname_first_author": "Yuqi Nie", "paper_title": "A time series is worth 64 words: Long-term forecasting with transformers", "publication_date": "2023-00-00", "reason": "This paper's use of transformers for time series forecasting directly influenced PowerPM's design and helped shape the model's temporal encoder."}]}