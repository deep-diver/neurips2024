[{"figure_path": "JInTfcxH3Q/tables/tables_6_1.jpg", "caption": "Table 1: Performance comparison on private dataset. The result of MAE metric refer to Tab. 6", "description": "This table presents a comprehensive comparison of the PowerPM model's performance against various baseline models across a range of downstream tasks within a private dataset.  The tasks are categorized into three groups: Demand-side Management, Grid Stability, and Consumer Behavior Analysis.  Each task utilizes a specific evaluation metric (MSE for forecasting and imputation, F0.5 for anomaly detection, and Accuracy for classification).  The table highlights PowerPM's superior performance across the board, even when compared to a frozen version of the model (PowerPM freeze).", "section": "3.2 Downstream Tasks"}, {"figure_path": "JInTfcxH3Q/tables/tables_7_1.jpg", "caption": "Table 2: Performance comparison on 4 public dataset.", "description": "This table presents a comparison of the performance of PowerPM and several baseline models on four different public datasets.  The performance is evaluated across various forecasting tasks with different prediction horizons. The datasets represent different geographic locations and hierarchical structures, allowing for an assessment of PowerPM's generalization ability. The metrics used are MSE (Mean Squared Error).", "section": "3.3 Main Results"}, {"figure_path": "JInTfcxH3Q/tables/tables_14_1.jpg", "caption": "Table 3: Private dataset description", "description": "This table provides detailed information about the private datasets used in the research.  It lists the dataset type, the number of cities, districts, and users included, the total number of samples, the length of each output sequence (in terms of time points), the data recording frequency, and the number of classes or categories present in the data.  Each row represents a specific dataset used for a specific task within the study, such as pre-training, load forecasting, or anomaly detection. The information in this table is crucial to understanding the scale and characteristics of the datasets used in the PowerPM model experiments.", "section": "A Dataset Description"}, {"figure_path": "JInTfcxH3Q/tables/tables_15_1.jpg", "caption": "Table 4: Public dataset description", "description": "This table presents a summary of the four public datasets used in the paper's experiments.  For each dataset (CAISO, ISONE, NYISO, PJM), it lists the instance type (#state, #area, #region, #city), the number of samples, the output lengths (time horizons for forecasting tasks), the data frequency (how often data points are recorded), and the overall time span covered by the data.", "section": "3.1 Experiment Setup"}, {"figure_path": "JInTfcxH3Q/tables/tables_19_1.jpg", "caption": "Table 5: The model hyperparameters of PowerPM with different model size.", "description": "This table shows the different hyperparameters used for training four different size variants of the PowerPM model.  It details the configuration for the temporal and hierarchical encoders, including the number of layers, dimensions, heads, mask ratio, time shift, number of clusters, batch size, learning rate, optimizer, and scheduler. These specifications highlight how model size influences training parameters.", "section": "3.1 Experiment Setup"}, {"figure_path": "JInTfcxH3Q/tables/tables_19_2.jpg", "caption": "Table 6: Additional performance comparison on private dataset in terms of MAE metric. Forecasting tasks involve varying forecasting lengths of {4, 96, 288, 672} time points and imputation tasks involve varying mask ratio {0.125, 0.25, 0.375, 0.5}. The length of the input window is 672.", "description": "This table presents a detailed performance comparison using the Mean Absolute Error (MAE) metric on a private dataset.  It breaks down the results for different forecasting horizons (4, 96, 288, and 672 time points) and varying data imputation mask ratios (0.125, 0.25, 0.375, and 0.5).  The input window size used for all tasks was 672.  The comparison includes PowerPM and its variants, along with several other baseline models.", "section": "3.3 Main Results"}, {"figure_path": "JInTfcxH3Q/tables/tables_20_1.jpg", "caption": "Table 7: Detailed performance of ablation study. Forecasting tasks involve varying forecasting lengths of {4, 96, 288, 672} time points, imputation tasks involve varying mask ratio {0.125, 0.25, 0.375, 0.5}. The length of the input window is 672.", "description": "This table presents the results of ablation studies conducted to evaluate the impact of each component in the PowerPM model.  The model's performance across various downstream tasks (forecasting, imputation, anomaly detection, and classification) is compared across several ablation variants. Each variant excludes one component of the model (hierarchical encoder, dual-view contrastive learning, exogenous variables encoding, or the masked ETS modeling module).  The results show the relative importance of each component in achieving the overall model's performance. The table displays the mean squared error (MSE) and mean absolute error (MAE) for forecasting and imputation tasks, the F0.5 score for anomaly detection, and the accuracy (Acc.) for classification tasks.", "section": "3.3 Main Results"}, {"figure_path": "JInTfcxH3Q/tables/tables_21_1.jpg", "caption": "Table 8: Complete results of few-shot learning performance comparison. Models are fine-tuned on {10%, 30% and 60%} of the downstream dataset. Forecasting tasks involve varying forecasting lengths of {4, 96, 288, 672} time points and imputation tasks involve varying mask ratio {0.125, 0.25, 0.375, 0.5}. The length of the input window is 672. We average the result for each task.", "description": "This table presents a comprehensive comparison of the performance of various models in a few-shot learning scenario.  The models are evaluated on forecasting, imputation, anomaly detection, and classification tasks using different proportions of the downstream dataset (10%, 30%, 60%). The performance metrics used are MSE for forecasting and imputation, F0.5 for anomaly detection, and accuracy for classification.  Different time horizons are considered for forecasting.", "section": "3.3 Main Results"}]