[{"type": "text", "text": "Entrywise error bounds for low-rank approximations of kernel matrices ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Alexander Modell Department of Mathematics Imperial College London, U.K. a.modell@imperial.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In this paper, we derive entrywise error bounds for low-rank approximations of kernel matrices obtained using the truncated eigen-decomposition (or singular value decomposition). While this approximation is well-known to be optimal with respect to the spectral and Frobenius norm error, little is known about the statistical behaviour of individual entries. Our error bounds fill this gap. A key technical innovation is a delocalisation result for the eigenvectors of the kernel matrix corresponding to small eigenvalues, which takes inspiration from the field of Random Matrix Theory. Finally, we validate our theory with an empirical study of a collection of synthetic and real-world datasets. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Low-rank approximations of kernel matrices play a central role in many areas of machine learning. Examples include kernel principal component analysis [Sch\u00f6lkopf et al., 1998], spectral clustering $[\\mathrm{Ng}$ et al., 2001, Von Luxburg, 2007] and manifold learning [Roweis and Saul, 2000, Belkin and Niyogi, 2001, Coifman and Lafon, 2006], where they serve as a core component of the algorithms, and support vector machines [Cortes and Vapnik, 1995, Fine and Scheinberg, 2001] and Gaussian process regression [Williams and Rasmussen, 1995, Ferrari-Trecate et al., 1998] where they serve to dramatically speed up computation times. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we derive entrywise error bounds for low-rank approximations of kernel matrices obtained using the truncated eigen-decomposition (or singular value decomposition). Entrywise error bounds are important for a number of reasons. The first is practical: in applications where individual errors carry a high cost, such as system control and healthcare, we should seek methods with low entrywise error. The second is theoretical: good entrywise error bounds can lead to improved analyses of learning algorithms which use them. ", "page_idx": 0}, {"type": "text", "text": "For this reason, a wealth of literature has emerged establishing entrywise error bounds for a variety of matrix estimation problems, such as covariance estimation [Fan et al., 2018, Abbe et al., 2022], matrix completion [Candes and Recht, 2012, Chi et al., 2019], phase synchronisation [Zhong and Boumal, 2018, Ma et al., 2018], reinforcement learning [Stojanovic et al., 2023], community detection [Balakrishnan et al., 2011, Lyzinski et al., 2014, Eldridge et al., 2018, Lei, 2019, Mao et al., 2021] and graph inference [Cape et al., 2019, Rubin-Delanchy et al., 2022] to name a few. ", "page_idx": 0}, {"type": "text", "text": "1.1 Contributions ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "\u2022 Our main result (Theorem 1) is an entrywise error bound for the low-rank approximation of a kernel matrix. Under regularity conditions, we find that for kernels with polynomial eigenvalue decay, $\\lambda_{i}={\\mathcal{O}}(i^{-\\alpha})$ , we require a polynomial-rank approximation, $d=\\Omega(n^{1/\\alpha})$ , to achieve entrywise consistency. For kernels with exponential eigenvalue decay, $\\lambda_{i}=$ $\\mathcal{O}(e^{\\beta i^{\\gamma}})$ , we require a (poly)log-rank approximation, $d^{\\stackrel{\\cdot}{>}}\\log^{1/\\gamma}(n^{\\Bar{1/\\beta}})$ . ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "\u2022 The main technical contribution of this paper is to establish a delocalisation result for the eigenvectors of the kernel matrix corresponding to small eigenvalues (Theorem 3), the proof of which draws on ideas from the Random Matrix Theory literature. To our knowledge, this is the first such result for a random matrix with non-zero mean and dependent entries. \u2022 Along the way, we prove a novel concentration inequality for the distance between a random vector (with a potentially non-zero mean) and a subspace (Lemma 1), which may be of independent interest. \u2022 We complement our theory with an empirical study on the entrywise errors of low-rank approximations of the kernel matrices on a collection of synthetic and real datasets. ", "page_idx": 1}, {"type": "text", "text": "1.2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Some complementary results to ours use the Johnson-Lindenstrauss lemma [Johnson and Lindenstrauss, 1982] to bound the entrywise error of low-rank matrix approximations obtained via random projections [Srebro and Shraibman, 2005, Alon et al., 2013, Udell and Townsend, 2019, Budzinskiy, 2024a,b]. In Section 3.2 we discuss these results in more detail and compare them with ours. ", "page_idx": 1}, {"type": "text", "text": "Our proof strategy draws heavily on ideas from the Random Matrix Theory literature, where delocalisation results have been established for certain classes of zero-mean random matrices with independent entries [Erdo\u02dds et al., 2009b,a, Tao and Vu, 2011, Rudelson and Vershynin, 2015, Vu and Wang, 2015]. In addition, our result is made possible by recent relative eigenvalue concentration bounds for kernel matrices [Braun, 2006, Valdivia, 2018, Barzilai and Shamir, 2023], which improve upon classical absolute concentration bounds [Rosasco et al., 2010] which would not provide sufficient control for our purposes. ", "page_idx": 1}, {"type": "text", "text": "1.3 Big- $\\scriptscriptstyle\\mathcal{O}$ notation and frequent events ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We use the standard big- $\\scriptscriptstyle\\mathcal{O}$ notation where $a_{n}=\\mathcal{O}(b_{n})$ (resp. $a_{n}=\\Omega(b_{n}))$ means that for sufficiently large $n$ , $a_{n}\\leq C b_{n}$ (resp. $a_{n}\\geq C b_{n},$ ) for some constant $C$ which doesn\u2019t depend on the parameters of the problem. We will occasionally write $a_{n}\\lesssim b_{n}$ to mean that $a_{n}=\\mathcal{O}(b_{n})$ . ", "page_idx": 1}, {"type": "text", "text": "In addition, we say that an event $E_{n}$ holds with overwhelming probability if for every $c~>~0$ , $\\mathbb{P}(E_{n})\\geq1-\\mathcal{O}(n^{-c})$ , where the hidden constant is allowed to depend on $c$ . ", "page_idx": 1}, {"type": "text", "text": "2 Setup", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We begin by describing the setup of our problem. We suppose that we observe $n$ , $p$ -dimensional data points $\\{x_{i}\\}_{i=1}^{n}$ , which we assume were drawn i.i.d. from some probability distribution $\\rho$ , supported on a set $\\mathcal{X}$ . Given a symmetric kernel $k:\\mathcal{X}\\times\\mathcal{X}\\to\\mathbb{R}$ , we construct the $n\\times n$ kernel matrix $K$ , with entries ", "page_idx": 1}, {"type": "equation", "text": "$$\nK(i,j):=k(x_{i},x_{j}).\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "We will assume throughout that the kernel is positive-definite, continuous and bounded. Let $\\widehat{K}_{d}$ denote the \u201cbest\u201d rank- $d$ approximation of $K$ , in the sense that $\\widehat{K}_{d}$ satisfies ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\widehat K_{d}:=\\underset{K^{\\prime}:\\mathrm{rank}(K^{\\prime})=d}{\\arg\\operatorname*{min}}\\left\\|K-K^{\\prime}\\right\\|_{\\xi},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\|\\cdot\\|_{\\xi}$ is a rotation-invariant norm, such as the spectral or Frobenius norm. Then by the EckartYoung-Mirsky theorem [Eckart and Young, 1936, Mirsky, 1960], $\\widehat{K}_{d}$ is given by the truncated eigen-decomposition of $K$ , i.e. ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\widehat{K}_{d}=\\sum_{i=1}^{d}\\widehat{\\lambda}_{i}\\widehat{u}_{i}\\widehat{u}_{i}^{\\top}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\{\\widehat{\\lambda}_{i}\\}_{i=1}^{n}$ are the eigenvalues of $K$ (counting multiplicities) in decreasing order, and $\\{\\widehat{u}_{i}\\}_{i=1}^{n}$ are corresponding eigenvectors. ", "page_idx": 1}, {"type": "text", "text": "We now introduce some population quantities which will form the basis of our theory. Let $L_{\\rho}^{2}$ denote the Hilbert space of real-valued square integrable functions with respect to $\\rho$ , with the inner product defined as $\\begin{array}{r}{\\langle\\dot{f},g\\rangle_{\\rho}=\\int f(x)g(x)\\dot{\\mathrm{d}\\rho}(x)}\\end{array}$ . We define the integral operator $K:L_{\\rho}^{2}\\to L_{\\rho}^{2}$ by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\left({K f}\\right)(x)=\\int k(x,y)f(y)\\mathrm{d}\\rho(y)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "which is the infinite sample limit of $\\scriptstyle{\\frac{1}{n}}K$ . The operator $\\kappa$ is self-adjoint and compact [Hirsch and Lacombe, 1999], so by the spectral theorem for compact operators, there exists a sequence of eigenvalues $\\{\\lambda_{i}\\}_{i=1}^{\\infty}$ (counting multiplicities) in decreasing order, with corresponding eigenfunctions $\\{u_{i}\\}_{i=1}^{\\infty}$ which are orthonormal in $L_{\\rho}^{2}$ , such that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\kappa u_{i}=\\lambda_{i}u_{i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Moreover, by the classical Mercer\u2019s theorem [Mercer, 1909, Steinwart and Scovel, 2012], the kernel $k$ can be decomposed into ", "page_idx": 2}, {"type": "equation", "text": "$$\nk(x,y)=\\sum_{i=1}^{\\infty}\\lambda_{i}u_{i}(x)u_{i}(y)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the series converges absolutely and uniformly in $x,y$ . ", "page_idx": 2}, {"type": "text", "text": "3 Entrywise error bounds ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section is devoted to our main theoretical result. We begin by discussing our assumptions, before presenting our main theorem and giving some special cases of kernels which fit within our framework. ", "page_idx": 2}, {"type": "text", "text": "In our asymptotics, we will assume that $k$ and $\\rho$ are fixed, and that the number of samples $n$ goes to infinity. This places us in the so-called low-dimensional regime, in which the dimension $p$ of the input space is considered fixed. ", "page_idx": 2}, {"type": "text", "text": "We shall assume that the eigenvalues of the kernel exhibit either polynomial decay, i.e. $\\lambda_{i}={\\mathcal{O}}(i^{-\\alpha})$ for some $\\alpha>1$ , or (nearly) exponential decay, i.e. $\\lambda_{i}=\\mathcal{O}(e^{-\\beta i^{\\gamma}})$ for some $\\beta>0$ and $0<\\gamma\\leq1$ . We will refer to these two hypotheses as (P) and (E) respectively. We also assume a corresponding hypothesis on the supremum-norm growth of the eigenfunctions. Under (P), we assume that $\\|u_{i}\\|_{\\infty}=$ $\\mathcal{O}(i^{r})$ with $\\alpha>2r+1$ , and under (E), we assume that $\\|u_{i}\\|_{\\infty}=O(e^{s i^{\\gamma}})$ with $\\beta>2s$ . ", "page_idx": 2}, {"type": "text", "text": "Our eigenvalue decay hypothesis is commonplace in the kernel literature [Braun, 2006, Ostrovskii and Rudi, 2019, Xu, 2018, Lei, 2021], and can be related to the smoothness of the kernel. For example, the decay of the eigenvalues is directly implied by a H\u00f6lder or Sobolev-type smoothness hypothesis on the kernel (see, for example, Nicaise [2000], Belkin [2018], Section 2.2 of $\\mathrm{Xu}$ [2018], Section 5 of Valdivia [2018], Scetbon and Harchaoui [2021] and Proposition 2 in this paper). We don\u2019t consider a finite-rank (say, $D$ ) hypothesis, since in this case the maximum entrywise error is trivially zero whenever $d\\geq D$ . ", "page_idx": 2}, {"type": "text", "text": "Our hypothesis on the supremum norm of the eigenfunctions is necessary to control the deviation of the sample eigenvectors from their corresponding population eigenfunctions, and is a requirement of eigenvalue bounds we employ. In the literature, it is common to see much stronger assumptions, such as uniformly bounded eigenfunctions [Williamson et al., 2001, Lafferty et al., 2005, Braun, 2006], which do not hold for many commonly-used kernels (see Mendelson and Neeman [2010], Steinwart and Scovel [2012], Zhou [2002] and Barzilai and Shamir [2023] for discussion). This assumption is reminiscent of the incoherence assumption [Candes and Recht, 2012] in the low-rank matrix estimation literature \u2014 a supremum norm bound on population eigenvectors \u2014 which governs the hardness of many compressed sensing and eigenvector estimation problems [Cand\u00e8s and Tao, 2010, Keshavan et al., 2010, Chi et al., 2019, Abbe et al., 2020, Chen et al., 2021]. ", "page_idx": 2}, {"type": "text", "text": "In addition, we introduce a regularity hypothesis, which we will refer to as (R), which relates to the following two quantities: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Delta_{i}=\\operatorname*{max}_{j\\geq i}\\left\\{\\lambda_{j}-\\lambda_{j+1}\\right\\}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "which measures the largest eigengap after a certain point in the spectrum, and ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Gamma_{i}=\\sum_{j=i+1}^{\\infty}\\left(\\int u_{j}(x)\\mathrm{d}\\rho(x)\\right)^{2}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "ziYC4FHRNr/tmp/b14ad2042b83c0a42b687831be2ec03d1d883338fd45533d572e3fd840b3fc8b.jpg", "img_caption": ["Table 1: Summary of the hypotheses (P), (E) and (R). "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "which measures the squared residual after projecting the unit-norm constant function onto the first $i$ eigenfunctions. Under (R), we assume that $\\overline{{\\Delta_{i}}}=\\Omega\\,\\bar{(}\\lambda_{i}^{a})$ and $\\Gamma_{i}=\\mathcal{O}\\left(\\lambda_{i}^{b}\\right)$ with $1\\leq a<b/16\\leq\\infty$ . ", "page_idx": 3}, {"type": "text", "text": "A sufficient condition for (R) to hold, is that the first eigenfunction is constant. This holds, for example, when $k$ is a dot-product kernel and $\\rho$ is a uniform distribution on a hypersphere. In such scenarios, $\\Gamma_{i}=0$ for all $i\\geq1$ and it is not necessary to make any assumptions on the eigengap quantity $\\Delta_{i}$ . We remark that (R) permits repeated eigenvalues in the spectrum of $\\kappa$ , which occur for many commonly-used kernels, but which are often precluded in the literature [Hall and Horowitz, 2007, Meister, 2011, Lei, 2014, 2021]. ", "page_idx": 3}, {"type": "text", "text": "The hypotheses (P), (E) and (R) are summarised in Table 1. We are now ready to state our main theorem. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1. Suppose that $k$ is a symmetric, positive-definite, continuous and bounded kernel and $\\rho$ is a probability measure which satisfy $(R)$ and one of either $(P)$ or $(E)$ . If the hypothesis $(P)$ holds and $d=\\`\\Omega\\left(n^{1/\\alpha}\\right)$ , then ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left\\|\\widehat{K}_{d}-K\\right\\|_{\\operatorname*{max}}=\\mathcal{O}\\left(n^{-\\frac{\\alpha-1}{\\alpha}}\\log(n)\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with overwhelming probability. If the hypothesis $(E)$ holds and $d>\\log^{1/\\gamma}(n^{1/\\beta}).$ , then ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\left\\|\\widehat{K}_{d}-K\\right\\|_{\\operatorname*{max}}=\\mathcal{O}\\left(n^{-1}\\right)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with overwhelming probability. ", "page_idx": 3}, {"type": "text", "text": "3.1 Special cases ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we provide some examples of kernels which satisfy the assumptions of Theorem 1. Proofs of the propositions in this section are given in Section A of the appendix. We start with a canonical example of a radial basis kernel. ", "page_idx": 3}, {"type": "text", "text": "Proposition 1. Suppose $k(x,y)~=~\\exp\\left(-\\|x-y\\|^{2}/2\\omega^{2}\\right)$ is $a$ radial basis kernel, and $\\rho~\\sim$ $\\mathcal{N}(0,\\sigma^{2}I_{p})$ is a isotropic Gaussian distribution on $\\mathbb{R}^{p}$ . Then the hypotheses $(E)$ and $(R)$ are satisfied with ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\beta=\\log\\left(\\frac{1+v+\\sqrt{1+2v}}{v}\\right),\\qquad\\gamma=1\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\upsilon:=2\\sigma^{2}/\\omega^{2}$ . ", "page_idx": 3}, {"type": "text", "text": "For this example, the eigenvalues and eigenfunctions were explicitly calculated in Zhu et al. [1997] (see also Shi et al. [2008] and Shi et al. [2009]), and we are able to verify the assumptions by direct calculation. ", "page_idx": 3}, {"type": "text", "text": "For our second example, we consider the case that $\\rho$ is the uniform distribution on a hypersphere $\\mathbb{S}^{p-1}$ , and $k$ is a dot-product kernel. In this setting, we are able to replace our assumptions with a smoothness hypothesis on the kernel. Note that this class of kernels includes those which are functions of Euclidean distance, since on the sphere we have the identity $\\lVert x-y\\rVert^{2}=2-2\\left\\langle x,y\\right\\rangle$ . ", "page_idx": 3}, {"type": "text", "text": "Proposition 2. Suppose that ", "page_idx": 3}, {"type": "equation", "text": "$$\nk(x,y)=f(\\langle x,y\\rangle)\\equiv\\sum_{i=0}^{\\infty}b_{i}\\left(\\langle x,y\\rangle\\right)^{i}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "is a dot-product kernel and $\\rho$ is the uniform distribution on the hypersphere $\\mathbb{S}^{p-1}$ with $p\\geq3.$ . If there exists $a\\dot{>}(p^{2}-4p+5)/2$ such that $b_{i}=O(i^{-a})$ , then $(P)$ and $(R)$ are satisfied with ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\alpha={\\frac{2a+p-3}{p-2}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Alternatively, if there exists $0<r<1$ such that $b_{i}=\\mathcal{O}(r^{i})$ , then $(E)$ and $(R)$ are satisfied with ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\beta=\\frac{(p-1)!}{C}\\log(1/r),\\qquad\\gamma=\\frac{1}{p-1}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for some universal constant $C>0$ . ", "page_idx": 4}, {"type": "text", "text": "In this example, the eigenfunctions posses the property that they do not depend on the choice of kernel, and are made up of spherical harmonics [Smola et al., 2000]. In particular, the first eigenfunction is constant, and therefore (R) is satisfied automatically. The eigenvalue bounds are derived in Scetbon and Harchaoui [2021], and we make use of a supremum norm bound for spherical harmonics in Minh et al. [2006]. ", "page_idx": 4}, {"type": "text", "text": "3.2 Comparison with random projections and the Johnson-Lindenstrauss lemma ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We pause here to consider how our entrywise bounds compare with existing bounds in the literature for low-rank matrix obtained via random projections [Srebro and Shraibman, 2005, Alon et al., 2013, Udell and Townsend, 2019, Budzinskiy, 2024a,b]. ", "page_idx": 4}, {"type": "text", "text": "For an $n\\,\\times\\,n$ symmetric, positive semi-definite matrix $M$ with bounded entries, the JohnsonLindenstrauss lemma [Johnson and Lindenstrauss, 1982] can be used to show the existence of a rank- $d$ approximation $\\widehat{M}_{d}$ whose entrywise error is bounded by $\\varepsilon$ when $\\begin{array}{r}{d=\\Omega(\\varepsilon^{-2}\\log(n))}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "The proof is via a probabilistic construction. Let $X$ be an $n\\times n$ matrix such that $M=X X^{\\top}$ , and for some $d\\leq n$ , let $R$ be an $n\\times d$ matrix with i.i.d. entries from ${\\mathcal{N}}(0,1/d)$ . Then, the randomised low-rank approximation ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{M}_{d}:=X R R^{\\top}X^{\\top}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "achieves the desired bound with high probability. Here, we state a adaptation of Theorem 1.4 of Alon et al. [2013] which makes the probabilistic construction from the proof explicit. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2. Let $M$ be an $n\\times n$ positive semi-definite matrix with bounded entries, and $\\widehat{M_{d}}$ be $a$ randomised rank- $d$ approximation of $M$ described in (4). Then ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\left\\|\\widehat{M}_{d}-M\\right\\|_{\\operatorname*{max}}=\\mathcal{O}\\left(\\sqrt{\\frac{\\log(n)}{d}}\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with overwhelming probability. ", "page_idx": 4}, {"type": "text", "text": "To obtain a polynomial entrywise error rate, i.e. $\\mathcal{O}(n^{-c})$ for some $c>0$ , with Theorem 2, requires the rank $d$ to be polynomial in $n$ . In contrast, under our hypothesis (E), we are able to obtain a polynomial entrywise error rate using a spectral low-rank approximation with only poly-logarithmic rank. In addition, while our entrywise error bounds are $o(n^{-1/2})$ for the cases we consider, this rate can never be achieved, regardless of the choice of rank $d$ , by (4) with Theorem 2. ", "page_idx": 4}, {"type": "text", "text": "On the flip side, Theorem 2 holds for arbitrary positive semi-definite matrix with bounded entries, whereas our theorem only holds for kernel matrices satisfying the hypotheses in Table 1. ", "page_idx": 4}, {"type": "text", "text": "4 Proof of Theorem 1 ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we outline the proof of Theorem 1. Without loss of generality, we will assume that $k$ is upper bounded by one. We cover the main details here, and defer some of the technical details to the appendix. By the Eckart-Young-Mirsky theorem, we have that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\left\\|\\widehat{K}_{d}-K\\right\\|_{\\operatorname*{max}}:=\\underset{1\\leq i,j\\leq n}{\\operatorname*{max}}\\left|\\widehat{K}_{d}(i,j)-K(i,j)\\right|=\\Big|\\underset{1\\leq i,j\\leq n}{\\operatorname*{max}}\\sum_{l=d+1}^{n}\\widehat{\\lambda}_{l}\\widehat{u}_{l}(i)\\widehat{u}_{l}(j)\\Big|}\\\\ {\\quad\\leq\\underset{l=d+1}{\\overset{n}{\\sum}}\\left|\\widehat{\\lambda}_{l}\\right|\\cdot\\underset{d<l\\leq n}{\\operatorname*{max}}\\left\\|\\widehat{u}_{l}\\right\\|_{\\infty}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Using a concentration bound due to Valdivia [2018], we are able to show that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\sum_{l=d+1}^{n}\\left\\vert\\widehat{\\lambda}_{l}\\right\\vert=\\left\\{\\mathcal{O}\\left(n^{1/\\alpha}\\log(n)\\right)\\quad\\mathrm{~under~(P)~with~}d=\\Omega\\left(n^{1/\\alpha}\\right)\\right.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with overwhelming probability, the details of which are given in Section B of the appendix. Then, the proof boils down to showing the following result, which we state as an independent theorem. ", "page_idx": 5}, {"type": "text", "text": "Theorem 3. Assume the setting of Theorem $^{\\,l}$ , then simultaneously for all $d+1\\leq l\\leq n,$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\left\\|\\widehat{u}_{l}\\right\\|_{\\infty}=\\mathcal{O}\\left(n^{-1/2}\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with overwhelming probability. ", "page_idx": 5}, {"type": "text", "text": "When a unit eigenvector satisfies (6) (up to log factors), it is said to be completely delocalised. There is a now expansive literature in the field of Random Matrix Theory proving the delocalisation of the eigenvectors of certain mean-zero random matrices with independent entries [Erd\u02ddos et al., $2009\\mathbf{b},\\mathbf{a}$ , Tao and Vu, 2011, Rudelson and Vershynin, 2015, Vu and Wang, 2015]. Theorem 3 may be of independent interest since to our knowledge, it is the first eigenvector delocalisation result for a random matrix with non-zero mean and dependent entries. ", "page_idx": 5}, {"type": "text", "text": "To prove Theorem 3, we take inspiration from a proof strategy employed in Tao and $\\mathrm{Vu}$ [2011] (see also Erdo\u02dds et al. [2009b]) which makes use of an identity relating the eigenvalues and eigenvectors of a matrix with that of its principal minor. The non-zero mean, and dependence between the entries of a kernel matrix present new challenges which require novel technical insights and tools and make up the bulk of our technical contribution. ", "page_idx": 5}, {"type": "text", "text": "Proof of Theorem 3. By symmetry and a union bound, to prove Theorem 3, it suffices to establish the bound for the first coordinate of $\\widehat{u}_{l}$ for some an arbitrary index $d<l\\leq n$ . We shall letK denote the bottom right principal minor of $K$ , that is the $n-1\\times n-1$ matrix such that ", "page_idx": 5}, {"type": "equation", "text": "$$\nK=\\left({z}\\atop{y}\\begin{array}{c c}{{y^{\\top}}}\\\\ {{\\widetilde K}}\\end{array}\\right)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $z=k(x_{1},x_{1})$ and $y=\\left(k(x_{1},x_{2}),\\ldots,k(x_{1},x_{n})\\right)^{\\top}$ . We will denote the ordered eigenvalues and corresponding eigenvectors of $\\widetilde{K}$ by $\\{\\widetilde{\\lambda}_{l}\\}_{l=1}^{n-1}$ and $\\{\\widetilde{u}_{l}\\}_{l=1}^{n-1}$ respectively. By Lemma 41 of Tao and $\\mathrm{Vu}$ [2011], we have the followi ng remarkable identit y: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{u}_{l}(1)^{2}=\\frac{1}{1+\\sum_{j=1}^{n-1}\\bigl(\\widetilde{\\lambda}_{j}-\\widehat{\\lambda}_{i}\\bigr)^{-2}\\bigl(\\widetilde{u}_{j}^{\\top}y\\bigr)^{2}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In addition, Cauchy\u2019s interlacing theorem tells us that the eigenvalues of $K$ andK interlace, i.e. $\\widehat{\\lambda}_{i}\\leq\\widetilde{\\lambda}_{i}\\leq\\widehat{\\lambda}_{i+1}$ for all $1\\leq i\\leq n-1$ . By (5) we have that $|\\widehat{\\lambda}_{i}|=\\mathcal{O}(1)$ for all $d+1\\leq i\\leq n$ with overwhelming probability and so by Cauchy\u2019s interlacing theorem, we can find a set of indices $J\\subset\\{d+1,\\ldots,n-1\\}$ with $\\vert J\\vert\\ge(n-d)/2$ such that $|\\widetilde{\\lambda}_{j}-\\widehat{\\lambda}_{i}|=\\mathcal{O}\\left(1\\right)$ for all $j\\in J$ . Combining this observation with (7), we have that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{n-1}\\!\\left(\\widetilde{\\lambda}_{j}-\\widehat{\\lambda}_{i}\\right)^{-2}\\!\\left(\\widetilde{u}_{j}^{\\top}y\\right)^{2}\\geq\\sum_{j\\in J}\\!\\left(\\widetilde{\\lambda}_{j}-\\widehat{\\lambda}_{i}\\right)^{-2}\\!\\left(\\widetilde{u}_{j}^{\\top}y\\right)^{2}\\gtrsim\\left\\|\\pi_{J}(y)\\right\\|^{2}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\pi_{J}$ denotes the orthogonal projection onto the subspace spanned by $\\{\\widetilde{u}_{j}\\}_{j\\in J}$ . So, to establish (6), it suffices to show that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\left\\|\\pi_{J}(y)\\right\\|^{2}=\\Omega(n)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with overwhelming probability. We condition on $x_{1}$ , so that $y$ is a vector of independent random variables and denote its conditional mean by $\\bar{y}$ , which is a constant vector whose entries are less than one. In addition, each entry of $y$ has common conditional variance which we denote by $\\sigma^{2}=\\mathbb{E}_{x\\sim F}\\{k^{2}(x_{1},x)\\}$ . ", "page_idx": 5}, {"type": "text", "text": "To obtain the lower bound (9), we prove a novel concentration inequality for the distance between a random vector and a subspace, which may be of independent interest. Our lemma generalises a similar result in Tao and $\\mathrm{Vu}$ [2011, Lemma 43] which holds only for random vectors with zero mean and unit variance. The proof is provided in Section $\\mathbf{C}$ of the appendix. ", "page_idx": 5}, {"type": "text", "text": "Lemma 1. Let $y\\in\\mathbb{R}^{n}$ be a random vector with mean ${\\bar{y}}:=\\mathbb{E}y$ whose entries are independent, have common variance $\\sigma^{2}$ and are bounded in $[0,1]$ almost surely. Let $H$ be a subspace of dimension $q\\geq64/\\sigma^{2}$ and $\\pi_{H}$ the orthogonal projection onto $H$ . If $H$ is such that $\\|\\pi_{H}(\\bar{y})\\|\\le2(\\sigma^{2}q)^{1/4}$ , then for any $t\\geq8$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\left|\\left|\\pi_{H}\\left(y\\right)\\right|\\right|-\\sigma q^{1/2}\\right|\\geq t\\right)\\leq4\\exp\\left(-t^{2}/32\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In particular, one has ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\|\\pi_{H}\\left(y\\right)\\|=\\sigma q^{1/2}+\\mathcal{O}\\left(\\log^{1/2}(n)\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with overwhelming probability. ", "page_idx": 6}, {"type": "text", "text": "Returning to the main thread, we claim for the moment that $\\|\\pi_{J}(\\bar{y})\\|\\le2|J|^{1/4}$ . Then, by Lemma 1 we have that, conditional on $x_{1}$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\pi_{J}(y)\\|^{2}\\gtrsim|J|\\geq(n-d)/2=\\Omega(n)}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "with overwhelming probability. This holds for all $x_{1}\\in\\mathcal{X}$ and therefore establishes (9). To complete the proof, then, it remains to prove our claim, and it is here where we require the regularity hypothesis (R). The proof of the claim is quite involved, so we defer the details to Section $\\mathrm{D}$ of the appendix, given which, the proof of Theorem 3 is complete. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Datasets and setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To see how our theory translates into practice, we examine the maximum entrywise error of the low-rank approximations of kernel matrices derived from a synthetic dataset and a collection of five real-world data sets, which are summarised in the following table1. ", "page_idx": 6}, {"type": "table", "img_path": "ziYC4FHRNr/tmp/ca0ed018624c644beb6b5ff988c16e76de3f6af8df702ab6f44c2ea15a3ca1ba.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Additional details about the dataset are provided in Section E of the appendix. ", "page_idx": 6}, {"type": "text", "text": "For the purpose of our experiment, we employ kernels in the class of Mat\u00e9rn kernels, of the form ", "page_idx": 6}, {"type": "equation", "text": "$$\nk_{\\nu}(x,y)=\\frac{2^{1-\\nu}}{\\Gamma(\\nu)}\\left(\\sqrt{2\\nu}\\frac{\\lVert x-y\\rVert}{\\omega}\\right)^{\\nu}K_{\\nu}\\left(\\sqrt{2\\nu}\\frac{\\lVert x-y\\rVert}{\\omega}\\right)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\Gamma$ denotes the gamma function, and $K_{\\nu}$ is the modified Bessel function of the second kind. The class of Mat\u00e9rn kernels is a generalisation of the radial basis kernel, with an additional parameter $\\nu$ which governs the smoothness of the resulting kernel. When $\\nu\\,=\\,1/2$ , we obtain the nondifferentiable exponential kernel, and in the $\\nu\\to\\infty$ limit, we obtain the infinitely-differentiable radial basis kernel. For the intermediate values $\\nu=3/2$ and $\\nu=5/2$ , we obtain, respectively, once and twice-differentiable functions. ", "page_idx": 6}, {"type": "text", "text": "The optimal choice of the bandwidth parameter is problem-dependent, and in supervised settings is typically chosen using cross-validation. In unsupervised settings, it is necessary to rely on heuristics, and for this experiment, we use the popular median heuristic [Flaxman et al., 2016, Mooij et al., 2016, Mu et al., 2016, Garreau et al., 2017], which has been shown to perform well in practice. ", "page_idx": 6}, {"type": "image", "img_path": "ziYC4FHRNr/tmp/5012ea71c10512266f2401b693cc6308f8c003ddba57a7d498ca5e5fa3776ed5.jpg", "img_caption": ["Figure 1: The maximum entrywise error against rank for low-rank approximations of kernel matrices constructed from a collection of datasets. The kernel matrices are constructed using Mat\u00e9rn kernels with a range of smoothness parameters, each of which is represented by a line in each plot. Details of the experiment are provided in Section 5. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "For each dataset, we construct four kernel matrices using Mat\u00e9rn kernels with smoothness parameters $\\begin{array}{r}{\\nu=\\frac{1}{2},\\frac{3}{2},\\frac{5}{2},\\infty}\\end{array}$ , each time selecting the bandwidth using the median heuristic. For each kernel, we compute the best rank- $d$ low-rank approximation of the kernel matrix using the svds function in the SciPy library for Python [Virtanen et al., 2020]. We do this for a range of ranks $d$ from 1 to $n$ , where $n$ is the number of instances in the dataset, and record the entrywise errors. ", "page_idx": 7}, {"type": "text", "text": "5.2 Interpretation of the results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Figure 1 shows the maximum entrywise errors for each dataset and kernel. For comparison, the Frobenius norm errors are plotted in Figure 2 in Section E of the appendix. ", "page_idx": 7}, {"type": "text", "text": "As predicted by our theory, for the four \u201clow-dimensional\u201d datasets, GMM, Abalone, Wine Quality and MNIST, the maximum entrywise decays rapidly as we increase the rank of the approximation, with the exception of the highly non-smooth $v\\doteq{\\frac{1}{2}}$ kernel, for which the maximum entrywise error decays much more slowly. In addition, the decay rates of the maximum entrywise error are in order of the smoothness of the kernels. ", "page_idx": 7}, {"type": "text", "text": "For the \u201chigh-dimensional\u201d datasets, 20 Newsgroups and Zebrafish, a different story emerges. Even for the smooth radial basis kernel $\\mathbf{\\boldsymbol{\\nu}}=\\infty$ ), the maximum entrywise error decays very slowly. This would suggests that our theory does potentially not carry over to the high-dimensional setting, and that caution should be taken when employing low-rank approximations for such data. Interestingly, the 20 Newsgroups dataset exhibits a sharp drop in maximum entrywise error between $d=2500$ and $d=3000$ which is not seen in the decay of the Frobenius norm error (Figure 2 in Section E). ", "page_idx": 7}, {"type": "text", "text": "6 Limitations and open problems ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To conclude, we discuss some of the limitations of our theory, as well as some of the open problems. ", "page_idx": 7}, {"type": "text", "text": "6.1 Limitations of our theory ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Positive semi-definite kernels. One significant limitation of our theory is the assumption that the kernel is positive semi-definite and continuous. This condition is known as Mercer\u2019s condition in the literature and ensures that the spectral decomposition of the kernel (2) converges uniformly, however we don\u2019t actually require such a strong notion of convergence for our theory. Valdivia [2018, Lemma 22] show that the decomposition converges almost surely under a much weaker condition which is implied by our hypotheses (P) and (E). The only other places we need this assumption is to make use of results in Rosasco et al. [2010] and Tang et al. [2013]. These results make heavy use of reproducing kernel Hilbert space technology though it seems plausible that they could be generalised to the indefinite setting using the framework of Krein spaces [Ong et al., 2004, Lei, 2021]. ", "page_idx": 8}, {"type": "text", "text": "Low-dimensional setting. In our asymptotics, we explicitly assume that the dimension of the input space remains fixed as the number of sample increases, which places us in the so-called low-dimensional setting. We do not consider the high-dimensional setting, however our empirical experiments suggest that our conclusions may not carry over. ", "page_idx": 8}, {"type": "text", "text": "Verification of the assumptions. While there is a established literature studying the eigenvalue decay of kernels under general probability measures [K\u00fchn, 1987, Cobos and K\u00fchn, 1990, Ferreira and Menegatto, 2013, Belkin, 2018, Li et al., 2024], except in very specialised settings (such as Propositions 1 and 2), control of the eigenfunctions is typically out of reach. This makes verifying the assumptions of our theory under general probability distributions quite challenging. This is a widespread limitation of many theoretical analyses in the kernel literature, and for an extended discussion, we refer the reader to Barzilai and Shamir [2023]. ", "page_idx": 8}, {"type": "text", "text": "6.2 Open problems ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Randomised low-rank approximations. While the truncated spectral decomposition provides the \u201cideal\u201d low-rank approximation, it requires computing the whole kernel matrix which can be prohibitive for very large datasets. Randomised low-rank approximations, such as the randomised SVD [Halko et al., 2011], the Nystr\u00f6m method [Williams and Seeger, 2000, Drineas et al., 2005] and random Fourier features [Rahimi and Recht, 2007, 2008], have emerged as efficient alternatives, and there is an extensive body of literature examining their statistical performance [Drineas et al., 2005, Rahimi and Recht, 2007, Belabbas and Wolfe, 2009, Boutsidis et al., 2009, Kumar et al., 2009a,b, Gittens, 2011, Gittens and Mahoney, 2013, Altschuler et al., 2016, Derezinski et al., 2020]. However, their primary focus is on classical error metrics such as the spectral and Frobenius norm errors and an entrywise analysis would presumably provide greater insights into these approximations, particularly given recently observed multiple-descent phenomena [Derezinski et al., 2020]. ", "page_idx": 8}, {"type": "text", "text": "Lower bounds. At present, it is unclear whether the bounds we obtain are tight, or indeed whether the truncated spectral decomposition itself is optimal with respect the the entrywise error. An interesting direction for future research would be to investigate lower bounds to understand the fundamental limits of this problem. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The author thanks Nick Whiteley, Yanbo Tang and Mahmoud Khabou for helpful discussions and Annie Gray for providing code to preprocess the 20 Newsgroups and Zebrafish datasets. ", "page_idx": 8}, {"type": "text", "text": "This work was supported by the Engineering and Physical Sciences Research Council [grant EP/X002195/1]. ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Emmanuel Abbe, Jianqing Fan, Kaizheng Wang, and Yiqiao Zhong. Entrywise eigenvector analysis of random matrices with low expected rank. The Annals of Statistics, 48(3):1452, 2020. ", "page_idx": 8}, {"type": "text", "text": "Emmanuel Abbe, Jianqing Fan, and Kaizheng Wang. An $\\ell_{p}$ theory of pca and spectral clustering. The Annals of Statistics, 50(4):2359\u20132385, 2022. ", "page_idx": 8}, {"type": "text", "text": "Noga Alon, Troy Lee, Adi Shraibman, and Santosh Vempala. The approximate rank of a matrix and its algorithmic applications: approximate rank. In Proceedings of the forty-fifth annual ACM Symposium on Theory of Computing, pages 675\u2013684, 2013.   \nJason Altschuler, Aditya Bhaskara, Gang Fu, Vahab Mirrokni, Afshin Rostamizadeh, and Morteza Zadimoghaddam. Greedy column subset selection: New bounds and distributed algorithms. In International Conference on Machine Learning, pages 2539\u20132548. PMLR, 2016.   \nSivaraman Balakrishnan, Min Xu, Akshay Krishnamurthy, and Aarti Singh. Noise thresholds for spectral clustering. Advances in Neural Information Processing Systems, 24, 2011.   \nDaniel Barzilai and Ohad Shamir. Generalization in kernel regression under realistic assumptions. arXiv preprint arXiv:2312.15995, 2023.   \nMohamed-Ali Belabbas and Patrick J Wolfe. Spectral methods in machine learning and new strategies for very large datasets. Proceedings of the National Academy of Sciences, 106(2):369\u2013374, 2009.   \nMikhail Belkin. Approximation beats concentration? an approximation view on inference with smooth radial kernels. In Conference On Learning Theory, pages 1348\u20131361. PMLR, 2018.   \nMikhail Belkin and Partha Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. Advances in Neural Information Processing Systems, 14, 2001.   \nChristos Boutsidis, Michael W Mahoney, and Petros Drineas. An improved approximation algorithm for the column subset selection problem. In Proceedings of the twentieth annual ACM-SIAM symposium on Discrete algorithms, pages 968\u2013977. SIAM, 2009.   \nMikio L Braun. Accurate error bounds for the eigenvalues of the kernel matrix. The Journal of Machine Learning Research, 7:2303\u20132328, 2006.   \nStanislav Budzinskiy. On the distance to low-rank matrices in the maximum norm. Linear Algebra and its Applications, 688:44\u201358, 2024a.   \nStanislav Budzinskiy. Entrywise tensor-train approximation of large tensors via random embeddings. arXiv preprint arXiv:2403.11768, 2024b.   \nEmmanuel Candes and Benjamin Recht. Exact matrix completion via convex optimization. Communications of the ACM, 55(6):111\u2013119, 2012.   \nEmmanuel J Cand\u00e8s and Terence Tao. The power of convex relaxation: Near-optimal matrix completion. IEEE transactions on information theory, 56(5):2053\u20132080, 2010.   \nJoshua Cape, Minh Tang, and Carey E Priebe. The two-to-infinity norm and singular subspace geometry with applications to high-dimensional statistics. The Annals of Statistics, 2019.   \nYuxin Chen, Yuejie Chi, Jianqing Fan, Cong Ma, et al. Spectral methods for data science: A statistical perspective. Foundations and Trends\u00ae in Machine Learning, 14(5):566\u2013806, 2021.   \nYuejie Chi, Yue M Lu, and Yuxin Chen. Nonconvex optimization meets low-rank matrix factorization: An overview. IEEE Transactions on Signal Processing, 67(20):5239\u20135269, 2019.   \nFernando Cobos and Thomas K\u00fchn. Eigenvalues of integral operators with positive definite kernels satisfying integrated h\u00f6lder conditions over metric compacta. Journal of Approximation Theory, 63(1):39\u201355, 1990.   \nRonald R Coifman and St\u00e9phane Lafon. Diffusion maps. Applied and computational harmonic analysis, 21(1):5\u201330, 2006.   \nCorinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning, 20:273\u2013297, 1995.   \nPaulo Cortez, A. Cerdeira, F. Almeida, T. Matos, and J. Reis. Wine Quality. UCI Machine Learning Repository, 2009. DOI: https://doi.org/10.24432/C56S3T.   \nErnesto De Vito, Lorenzo Rosasco, Andrea Caponnetto, Umberto De Giovannini, Francesca Odone, and Peter Bartlett. Learning from examples as an inverse problem. Journal of Machine Learning Research, 6(5), 2005.   \nLi Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine, 29(6):141\u2013142, 2012.   \nMichal Derezinski, Rajiv Khanna, and Michael W Mahoney. Improved guarantees and a multipledescent curve for column subset selection and the nystrom method. Advances in Neural Information Processing Systems, 33:4953\u20134964, 2020.   \nPetros Drineas, Michael W Mahoney, and Nello Cristianini. On the nystr\u00f6m method for approximating a gram matrix for improved kernel-based learning. Journal of Machine Learning Research, 6(12), 2005.   \nCarl Eckart and Gale Young. The approximation of one matrix by another of lower rank. Psychometrika, 1(3):211\u2013218, 1936.   \nJustin Eldridge, Mikhail Belkin, and Yusu Wang. Unperturbed: spectral analysis beyond davis-kahan. In Algorithmic learning theory, pages 321\u2013358. PMLR, 2018.   \nL\u00e1szl\u00f3 Erd\u02ddos, Benjamin Schlein, and Horng-Tzer Yau. Local semicircle law and complete delocalization for wigner random matrices. Communications in Mathematical Physics, 287(2):641\u2013655, 2009a.   \nL\u00e1szl\u00f3 Erdo\u02dds, Benjamin Schlein, and Horng-Tzer Yau. Semicircle law on short scales and delocalization of eigenvectors for wigner random matrices. 2009b.   \nJianqing Fan, Weichen Wang, and Yiqiao Zhong. An $\\ell_{\\infty}$ eigenvector perturbation bound and its application. Journal of Machine Learning Research, 18(207):1\u201342, 2018.   \nGregory E Fasshauer and Michael J McCourt. Stable evaluation of gaussian radial basis function interpolants. SIAM Journal on Scientific Computing, 34(2):A737\u2013A762, 2012.   \nGiancarlo Ferrari-Trecate, Christopher Williams, and Manfred Opper. Finite-dimensional approximation of gaussian processes. Advances in Neural Information Processing Systems, 11, 1998.   \nJC Ferreira and VA3128739 Menegatto. Eigenvalue decay rates for positive integral operators. Annali di Matematica Pura ed Applicata, 192(6):1025\u20131041, 2013.   \nShai Fine and Katya Scheinberg. Efficient svm training using low-rank kernel representations. Journal of Machine Learning Research, 2(Dec):243\u2013264, 2001.   \nSeth Flaxman, Dino Sejdinovic, John P Cunningham, and Sarah Filippi. Bayesian learning of kernel embeddings. arXiv preprint arXiv:1603.02160, 2016.   \nDamien Garreau, Wittawat Jitkrittum, and Motonobu Kanagawa. Large sample analysis of the median heuristic. arXiv preprint arXiv:1707.07269, 2017.   \nAlex Gittens. The spectral norm error of the naive nystrom extension. arXiv preprint arXiv:1110.5305, 2011.   \nAlex Gittens and Michael Mahoney. Revisiting the nystrom method for improved large-scale machine learning. In International Conference on Machine Learning, pages 567\u2013575. PMLR, 2013.   \nI.S. Gradshteyn and I.M. Ryzhik. Table of Integrals, Series, and Products. Academic Press, 8 edition, 2014. ISBN 978-0123849335.   \nNathan Halko, Per-Gunnar Martinsson, and Joel A Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM review, 53(2): 217\u2013288, 2011.   \nPeter Hall and Joel L Horowitz. Methodology and convergence rates for functional linear regression. The Annals of Statistics, 2007. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "Francis Hirsch and Gilles Lacombe. Elements of Functional Analysis. Springer, 1999. ", "page_idx": 11}, {"type": "text", "text": "Jack Indritz. An inequality for hermite polynomials. Proceedings of the American Mathematical Society, 12(6):981\u2013983, 1961.   \nWilliam Johnson and J. Lindenstrauss. Extensions of lipschitz mappings into a hilbert space. Conference in Modern Analysis and Probability, 26:189\u2013206, 01 1982.   \nRaghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a few entries. IEEE transactions on information theory, 56(6):2980\u20132998, 2010.   \nThomas K\u00fchn. Eigenvalues of integral operators with smooth positive definite kernels. Archiv der Mathematik, 49:525\u2013534, 1987.   \nSanjiv Kumar, Mehryar Mohri, and Ameet Talwalkar. Ensemble nystrom method. Advances in Neural Information Processing Systems, 22, 2009a.   \nSanjiv Kumar, Mehryar Mohri, and Ameet Talwalkar. On sampling-based approximate spectral decomposition. In Proceedings of the 26th annual International Conference on Machine Learning, pages 553\u2013560, 2009b.   \nJohn Lafferty, Guy Lebanon, and Tommi Jaakkola. Diffusion kernels on statistical manifolds. Journal of Machine Learning Research, 6(1), 2005.   \nKen Lang. Newsweeder: Learning to fliter netnews. In Machine Learning Proceedings 1995, pages 331\u2013339. Elsevier, 1995.   \nMichel Ledoux. The Concentration of Measure Phenomenon, Mathematical Surveys and Monographs. Number 89. American Mathematical Soc., 2001.   \nJing Lei. Adaptive global testing for functional linear models. Journal of the American Statistical Association, 109(506):624\u2013634, 2014.   \nJing Lei. Network representation using graph root distributions. The Annals of Statistics, 2021.   \nLihua Lei. Unified $\\ell_{2\\to\\infty}$ eigenspace perturbation theory for symmetric random matrices. arXiv preprint arXiv:1909.04798, 2019.   \nYicheng Li, Zixiong Yu, Guhan Chen, and Qian Lin. On the eigenvalue decay rates of a class of neural-network related kernel functions defined on general domains. Journal of Machine Learning Research, 25(82):1\u201347, 2024.   \nVince Lyzinski, Daniel L Sussman, Minh Tang, Avanti Athreya, and Carey E Priebe. Perfect clustering for stochastic blockmodel graphs via adjacency spectral embedding. Electron. J. Statist., 2014.   \nCong Ma, Kaizheng Wang, Yuejie Chi, and Yuxin Chen. Implicit regularization in nonconvex statistical estimation: Gradient descent converges linearly for phase retrieval and matrix completion. In International Conference on Machine Learning, pages 3345\u20133354. PMLR, 2018.   \nXueyu Mao, Purnamrita Sarkar, and Deepayan Chakrabarti. Estimating mixed memberships with sharp eigenvector deviations. Journal of the American Statistical Association, 116(536):1928\u20131940, 2021.   \nAlexander Meister. Asymptotic equivalence of functional linear regression and a white noise inverse problem. The Annals of Statistics, pages 1471\u20131495, 2011.   \nShahar Mendelson and Joseph Neeman. Regularization in kernel learning. The Annals of Statistics, 2010.   \nJames Mercer. Xvi. functions of positive and negative type, and their connection the theory of integral equations. Philosophical Transactions of the Royal Society of London. Series A, 209(441-458): 415\u2013446, 1909.   \nHa Quang Minh, Partha Niyogi, and Yuan Yao. Mercer\u2019s theorem, feature maps, and smoothing. In International Conference on Computational Learning Theory, pages 154\u2013168. Springer, 2006.   \nLeon Mirsky. Symmetric gauge functions and unitarily invariant norms. The Quarterly Journal of Mathematics, 11(1):50\u201359, 1960.   \nJoris M Mooij, Jonas Peters, Dominik Janzing, Jakob Zscheischler, and Bernhard Sch\u00f6lkopf. Distinguishing cause from effect using observational data: methods and benchmarks. Journal of Machine Learning Research, 17(32):1\u2013102, 2016.   \nKrikamol Mu, Bharath Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Sch\u00f6lkopf, et al. Kernel mean shrinkage estimators. Journal of Machine Learning Research, 17(48):1\u201341, 2016.   \nWarwick Nash, Tracy Sellers, Simon Talbot, Andrew Cawthorn, and Wes Ford. Abalone. UCI Machine Learning Repository, 1995. DOI: https://doi.org/10.24432/C55C7W.   \nAndrew Ng, Michael Jordan, and Yair Weiss. On spectral clustering: Analysis and an algorithm. Advances in Neural Information Processing Systems, 14, 2001.   \nSerge Nicaise. Jacobi polynomials, weighted sobolev spaces and approximation results of some singularities. Mathematische Nachrichten, 213(1):117\u2013140, 2000.   \nCheng Soon Ong, Xavier Mary, St\u00e9phane Canu, and Alexander J Smola. Learning with non-positive kernels. In Proceedings of the twenty-first International Conference on Machine Learning, page 81, 2004.   \nDmitrii M Ostrovskii and Alessandro Rudi. Affine invariant covariance estimation for heavy-tailed distributions. In Conference on Learning Theory, pages 2531\u20132550. PMLR, 2019.   \nDavid Pollard. Empirical processes: theory and applications. IMS, 1990.   \nAli Rahimi and Benjamin Recht. Random features for large-scale kernel machines. Advances in Neural Information Processing Systems, 20, 2007.   \nAli Rahimi and Benjamin Recht. Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning. Advances in Neural Information Processing Systems, 21, 2008.   \nLorenzo Rosasco, Mikhail Belkin, and Ernesto De Vito. On learning with integral operators. Journal of Machine Learning Research, 11(2), 2010.   \nSam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323\u20132326, 2000.   \nPatrick Rubin-Delanchy, Joshua Cape, Minh Tang, and Carey E Priebe. A statistical interpretation of spectral embedding: the generalised random dot product graph. Journal of the Royal Statistical Society Series B: Statistical Methodology, 84(4):1446\u20131473, 2022.   \nMark Rudelson and Roman Vershynin. Delocalization of eigenvectors of random matrices with independent entries. Duke Math. J., 2015.   \nMeyer Scetbon and Zaid Harchaoui. A spectral analysis of dot-product kernels. In International Conference on Artificial Intelligence and Statistics, pages 3394\u20133402. PMLR, 2021.   \nBernhard Sch\u00f6lkopf, Alexander Smola, and Klaus-Robert M\u00fcller. Nonlinear component analysis as a kernel eigenvalue problem. Neural Computation, 10(5):1299\u20131319, 1998.   \nTao Shi, Mikhail Belkin, and Bin Yu. Data spectroscopy: Learning mixture models using eigenspaces of convolution operators. In Proceedings of the 25th International Conference on Machine Learning, pages 936\u2013943, 2008.   \nTao Shi, Mikhail Belkin, and Bin Yu. Data spectroscopy: Eigenspaces of convolution operators and clustering. The Annals of Statistics, pages 3960\u20133984, 2009.   \nAlex Smola, Zolt\u00e1n Ov\u00e1ri, and Robert C Williamson. Regularization with dot-product kernels. Advances in Neural Information Processing Systems, 13, 2000.   \nNathan Srebro and Adi Shraibman. Rank, trace-norm and max-norm. In International Conference on Computational Learning Theory, pages 545\u2013560. Springer, 2005. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Ingo Steinwart and Clint Scovel. Mercer\u2019s theorem on general domains: On the interaction between measures, kernels, and rkhss. Constructive Approximation, 35:363\u2013417, 2012. ", "page_idx": 13}, {"type": "text", "text": "Stefan Stojanovic, Yassir Jedra, and Alexandre Proutiere. Spectral entry-wise matrix estimation for low-rank reinforcement learning. Advances in Neural Information Processing Systems, 36: 77056\u201377070, 2023. ", "page_idx": 13}, {"type": "text", "text": "Michel Talagrand. A new look at independence. The Annals of Probability, 24(1):1 \u2013 34, 1996. ", "page_idx": 13}, {"type": "text", "text": "Minh Tang, Daniel L. Sussman, and Carey E. Priebe. Universally consistent vertex classification for latent positions graphs. The Annals of Statistics, 41(3):1406 \u2013 1430, 2013. ", "page_idx": 13}, {"type": "text", "text": "Terence Tao and Van Vu. Random matrices: Universality of local eigenvalue statistics. Acta Mathematica, 206(1):127 \u2013 204, 2011. doi: 10.1007/s11511-011-0061-3. ", "page_idx": 13}, {"type": "text", "text": "Madeleine Udell and Alex Townsend. Why are big data matrices approximately low rank? SIAM Journal on Mathematics of Data Science, 1(1):144\u2013160, 2019. ", "page_idx": 13}, {"type": "text", "text": "Ernesto Araya Valdivia. Relative concentration bounds for the spectrum of kernel matrices. arXiv preprint arXiv:1812.02108, 2018. ", "page_idx": 13}, {"type": "text", "text": "Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, St\u00e9fan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, \u02d9Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antonio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17:261\u2013272, 2020. doi: 10.1038/s41592-019-0686-2. ", "page_idx": 13}, {"type": "text", "text": "Ulrike Von Luxburg. A tutorial on spectral clustering. Statistics and Computing, 17:395\u2013416, 2007. ", "page_idx": 13}, {"type": "text", "text": "Van Vu and Ke Wang. Random weighted projections, random quadratic forms and random eigenvectors. Random Structures & Algorithms, 47(4):792\u2013821, 2015. ", "page_idx": 13}, {"type": "text", "text": "Daniel E Wagner, Caleb Weinreb, Zach M Collins, James A Briggs, Sean G Megason, and Allon M Klein. Single-cell mapping of gene expression landscapes and lineage in the zebrafish embryo. Science, 360(6392):981\u2013987, 2018. ", "page_idx": 13}, {"type": "text", "text": "Christopher Williams and Carl Rasmussen. Gaussian processes for regression. Advances in Neural Information Processing Systems, 8, 1995. ", "page_idx": 13}, {"type": "text", "text": "Christopher Williams and Matthias Seeger. Using the nystr\u00f6m method to speed up kernel machines. Advances in Neural Information Processing Systems, 13, 2000. ", "page_idx": 13}, {"type": "text", "text": "Robert C Williamson, Alexander J Smola, and Bernhard Scholkopf. Generalization performance of regularization networks and support vector machines via entropy numbers of compact operators. IEEE transactions on Information Theory, 47(6):2516\u20132532, 2001. ", "page_idx": 13}, {"type": "text", "text": "Jiaming Xu. Rates of convergence of spectral methods for graphon estimation. In International Conference on Machine Learning, pages 5433\u20135442. PMLR, 2018. ", "page_idx": 13}, {"type": "text", "text": "Yiqiao Zhong and Nicolas Boumal. Near-optimal bounds for phase synchronization. SIAM Journal on Optimization, 28(2):989\u20131016, 2018. ", "page_idx": 13}, {"type": "text", "text": "Ding-Xuan Zhou. The covering number in learning theory. Journal of Complexity, 18(3):739\u2013767, 2002. ", "page_idx": 13}, {"type": "text", "text": "Huaiyu Zhu, Christopher KI Williams, Richard Rohwer, and Michal Morciniec. Gaussian regression and optimal finite dimensional linear models. 1997. ", "page_idx": 13}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Proof of Propositions 1 and 2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "For notational simplicity, in this section we will assume in this section that the eigenvalues and eigenfunctions are indexed from 0 rather than 1. ", "page_idx": 14}, {"type": "text", "text": "A.1 Proof of Proposition 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We will begin by reducing the problem of verifying our assumptions under $\\rho$ to verifying them under the probability measure associated with the univariate Gaussian distribution $\\mathcal{N}(0,\\sigma^{2})$ , which we will denote by $\\mu$ . ", "page_idx": 14}, {"type": "text", "text": "Let $\\underline{{K}}:L_{\\rho}^{2}\\to L_{\\rho}^{2}$ denote the integral operator associated with the kernel $k$ and the measure $\\rho$ , and let $\\{\\underline{{\\lambda}}_{i}\\}$ denote its eigenvalues, arranged in descending order, and $\\{\\underline{{u}}_{i}\\}$ denote their corresponding eigenfunctions. By the rotation invariance of both $k$ and $\\rho$ , the operator $\\underline{{\\boldsymbol{\\kappa}}}$ may be written as the $p$ -fold tensor product ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\underline{{K}}=K\\otimes\\cdots\\otimes K\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $K:L_{\\mu}^{2}\\to L_{\\mu}^{2}$ denotes the integral operator associated with the kernel $k$ and the univariate Gaussian measure $\\mu$ . Let $\\{\\lambda_{i}\\}$ denote its eigenvalues, arranged in descending order, and $\\{u_{i}\\}$ denote their corresponding eigenfunctions. Then, the eigenvalues and eigenfunctions of $\\underline{{\\boldsymbol{\\kappa}}}$ and $\\kappa$ are related in the following way (see Shi et al. [2008] or Fasshauer and McCourt [2012]). For every $i$ , there exists $i_{1},\\ldots,i_{p}$ satisfying $\\textstyle\\sum_{j=1}^{p}i_{j}=i$ such that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\underline{{{\\lambda}}}_{i}=\\prod_{j=1}^{p}\\lambda_{i_{j}}\\qquad\\mathrm{and}\\qquad\\underline{{{u}}}_{i}(x)=\\prod_{j=1}^{p}u_{i_{j}}(x^{j})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for all $x=(x^{1},\\ldots,x^{p})^{\\top}\\in\\mathbb{R}^{p}$ . Now suppose that $\\lambda_{i}=\\Theta\\left(e^{-\\beta i}\\right)$ , then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\underline{{{\\lambda}}}_{i}=\\prod_{j=1}^{p}\\lambda_{i_{j}}\\asymp\\prod_{j=1}^{p}e^{-\\beta i_{j}}=e^{-\\beta\\sum_{j}i_{j}}=e^{-\\beta i},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and suppose that $\\|u_{i}\\|_{\\infty}=\\mathcal{O}(e^{s i})$ for some $s<\\beta/2$ . Then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|\\underline{{u}}_{i}\\|_{\\infty}\\le\\prod_{j=1}^{p}\\left\\|u_{i_{j}}\\right\\|_{\\infty}\\lesssim\\prod_{j=1}^{p}e^{s i_{j}}=e^{s i}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore to prove that (E) hold under $\\rho$ , it suffices to show that it holds under $\\mu$ ", "page_idx": 14}, {"type": "text", "text": "Shi et al. [2008] provide an explicit formula for the eigenvalues and eigenfunctions of $\\kappa$ , which is a refinement of an earlier result of Zhu et al. [1997]. ", "page_idx": 14}, {"type": "text", "text": "Let $\\upsilon:=2\\sigma^{2}/\\omega^{2}$ and let $H_{i}(x)$ be the ith order Hermite polynomial. Then the eigenvalues and eigenfunctions of $\\kappa$ are given by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\lambda_{i}=\\sqrt{\\frac{2}{1+v+\\sqrt{1+2v}}}\\left(\\frac{v}{1+v+\\sqrt{1+2v}}\\right)^{i}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\nu_{i}(x)={\\frac{(1+2\\beta)^{1/8}}{\\sqrt{2^{i}i!}}}\\exp\\left(-{\\frac{x^{2}}{2\\sigma^{2}}}{\\frac{\\sqrt{1+2\\beta}-1}{2}}\\right)H_{i}\\left(\\left({\\frac{1}{4}}+{\\frac{\\beta}{2}}\\right)^{1/4}{\\frac{x}{\\sigma}}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, we have $\\lambda_{i}=C_{1}e^{-\\beta i}$ where ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\beta=\\log\\left(\\frac{1+v+\\sqrt{1+2v}}{v}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We will now show that each $u_{i}$ is uniformly bounded. By a change of variables, we can write $u_{i}$ as ", "page_idx": 14}, {"type": "equation", "text": "$$\nu_{i}(x)=\\frac{C_{2}}{\\sqrt{2^{i}i!}}e^{-y^{2}}H_{i}(y)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for some $y\\in\\mathbb R$ . On the other hand, we have the following inequality due to Indritz [1961]. For all $x\\in\\mathbb R$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\ne^{-x^{2}}H_{i}(x)\\leq1.09{\\sqrt{2^{i}i!}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore $u_{i}(x)\\leq1.09C_{2}$ for all $x$ . We can use the fact that $H_{i}(x)$ is either odd or even to obtain an analogous lower bound. Therefore $\\|u_{i}\\|_{\\infty}\\leq1.09C_{2}$ for all $i$ , so $s=0$ and (E) holds. ", "page_idx": 15}, {"type": "text", "text": "We will now show that $\\Gamma_{i}=0$ for all $i\\geq1$ so that (R) holds with $b=+\\infty$ and there is no requirement on the eigengaps $\\Delta_{i}$ . ", "page_idx": 15}, {"type": "text", "text": "Expanding $\\textstyle\\int u_{i}(x)\\mathrm{d}\\mu(x)$ , collecting exponential terms and applying a change-of-variables, one can calculate that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\int u_{i}(x)\\mathrm{d}\\mu(x)=C_{3}\\int_{-\\infty}^{+\\infty}e^{-y^{2}}H_{i}(y)\\mathrm{d}y.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "It is a standard result that $e^{-y^{2}}H_{i}(y)=0$ as long as $i\\neq0$ [Gradshteyn and Ryzhik, 2014], and therefore $\\begin{array}{r}{\\left|\\int u_{i}(x)\\mathrm{d}\\mu(x)\\right|=0}\\end{array}$ for all $i\\geq1$ , and by (11), we have that $\\Gamma_{i}=0$ for all $i\\geq1$ . ", "page_idx": 15}, {"type": "text", "text": "A.2 Proof of Proposition 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "For any $p\\geq3$ , dot-product kernels with respect to the uniform measure on the sphere exhibit the spectral decomposition ", "page_idx": 15}, {"type": "equation", "text": "$$\nk(x,y)=\\sum_{l=0}^{\\infty}\\lambda_{l}^{*}\\sum_{m=1}^{N_{l}}u_{l,m}^{*}(x)u_{l,m}^{*}(y)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the eigenfunctions $\\{u_{l,m}^{*}\\}$ are the $m$ th spherical harmonic of degree $l$ , $\\begin{array}{r}{N_{l}=\\frac{2l+p-2}{l}\\binom{l+p-3}{p-2}=}\\end{array}$ is the number of harmonics of each degree, and are the distinct eigenvalues [Smola et al., 2000]. ", "page_idx": 15}, {"type": "text", "text": "The first spherical harmonic is a constant function, and therefore by the orthogonality of the eigenfunctions in $L_{\\rho}^{2}$ , $\\begin{array}{r}{\\int u_{l,m}^{*}(x)\\mathrm{d}\\rho(x)=0}\\end{array}$ for all $l\\geq1$ , and therefore $\\Gamma_{i}=0$ for all $i\\geq1$ . Therefore (R) holds with $s=+\\infty$ , and there are no requirements on the eigengaps. ", "page_idx": 15}, {"type": "text", "text": "In addition, Lemma 3 of Minh et al. [2006] shows that the supremum-norm of a spherical harmonic is upper bounded by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|u_{l,m}^{*}\\|_{\\infty}\\leq\\sqrt{\\frac{N_{l}}{\\left|\\mathbb{S}^{p-1}\\right|}}=\\mathcal{O}\\left(i^{\\frac{p-2}{2}}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The eigenvalue decay rates are obtained from Propositions 2.3 and 2.4 of Scetbon and Harchaoui [2021], and given (12), the condition $a>(p^{2}-4p\\,\\dot{+}\\,5)/2$ ensures that the conditions for (P) are met. ", "page_idx": 15}, {"type": "text", "text": "B Proof of Equation (5) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We begin this section with two upper bounds on polynomial and exponential series, which we prove in Section B.1, and which we will use throughout this proof. ", "page_idx": 15}, {"type": "text", "text": "Lemma 2. Let $\\alpha>1$ , $\\beta>0$ and $0<\\gamma\\leq1$ for fixed constants. Then the following upper bounds hold: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{i=d+1}^{\\infty}i^{-\\alpha}=\\mathcal{O}\\left(d^{-\\alpha+1}\\right),\\qquad\\sum_{i=d+1}^{\\infty}e^{-\\beta i^{\\gamma}}=\\mathcal{O}\\left(e^{-\\beta d^{\\gamma}}d^{1-\\gamma}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Throughout this proof, $\\varepsilon>0$ will denote some constant which may change from line to line, and even within lines. ", "page_idx": 15}, {"type": "text", "text": "To show equation (5), we first note that under (P) with $d=\\Omega(n^{1/\\alpha})$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{l=d+1}^{n}\\lambda_{l}\\lesssim\\sum_{i=d+1}^{n}i^{-\\alpha}\\lesssim d^{-\\alpha+1}=\\mathcal{O}\\left(n^{\\frac{-\\alpha-1}{\\alpha}}\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where we have used Lemma 2. In addition, under $\\mathrm{(E)}$ with $d>\\log^{1/\\gamma}(n^{1/\\beta})$ , we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{l=d+1}^{n}\\lambda_{l}\\lesssim\\sum_{l=d+1}^{n}e^{-\\beta l^{\\gamma}}\\lesssim e^{-\\beta d^{\\gamma}}d^{1-\\gamma}=n^{-(1+\\varepsilon)}\\log^{(1-\\gamma)/\\gamma}(n^{1/\\beta})=\\mathcal{O}(n^{-1})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we have again used Lemma 2. Now, by the triangle inequality we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\sum_{l=d+1}^{n}\\left|\\widehat{\\lambda}\\iota\\right|\\leq\\sum_{l=d+1}^{n}\\lambda_{l}+\\sum_{l=d+1}^{n}\\left|\\frac{\\widehat{\\lambda}\\iota}{n}-\\lambda_{l}\\right|\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and therefore we are left to show that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{l=d+1}^{n}\\left|{\\frac{\\widehat{\\lambda}_{l}}{n}}-\\lambda_{l}\\right|=\\left\\{{\\mathcal{O}}\\left(n^{-(\\alpha-1)/\\alpha}\\log(n)\\right)\\quad{\\mathrm{~under~}}(\\mathbb{P}){\\mathrm{~with~}}d=\\Omega\\left(n^{1/\\alpha}\\right);\\quad\\right.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To bound (13), we employ a fine-grained concentration bound due to Valdivia [2018]. We begin with the polynomial hypothesis. The authors only consider the cases that $\\alpha,r$ are natural numbers, since they draw a comparison between between these values and a Sobolev-type notion of regularity. Inspecting their proofs, they treat the cases $r=0$ and $r\\geq1$ separately, however their proofs follow through in exactly the same way when the $r\\geq1$ case is replaced with $r>0$ , in order to cover all values of $\\alpha>2r+1$ , $r\\geq0$ . For the $r>0$ case, they derive the following result. ", "page_idx": 16}, {"type": "text", "text": "Lemma 3. Suppose that the hypothesis $(P)$ holds with $r>0$ . Then, with overwhelming probability ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left|\\frac{\\widehat{\\lambda}_{i}}{n}-\\lambda_{i}\\right|\\lesssim B(i,n)\\log(n)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where ", "page_idx": 16}, {"type": "equation", "text": "$$\nB(i,n)=\\left\\{\\begin{array}{l l}{i^{-\\alpha+\\frac{\\alpha}{\\alpha-1}(r+\\frac{1}{2})}n^{-1/2}}&{i f1\\leq i\\leq n^{\\frac{\\alpha-1}{\\alpha}\\frac{1}{2r+1}};}\\\\ {i^{-\\alpha+1+\\frac{\\alpha-1}{\\alpha}(r+\\frac{1}{2})}n^{-1/2}}&{i f n^{\\frac{\\alpha-1}{\\alpha}\\frac{1}{2r+1}}\\leq i\\leq n^{\\frac{1}{2r}};}\\\\ {i^{-\\alpha+r+1}n^{-1/2}}&{i f n^{\\frac{1}{2r}}\\leq i\\leq n;}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Via some rearrangement we can show that ", "page_idx": 16}, {"type": "equation", "text": "$$\nB(i,n)=\\mathcal{O}\\left(i^{-\\alpha}\\right),\\qquad\\mathrm{if}\\,1\\leq i\\leq n^{\\frac{1}{2r}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and by Lemma 2 we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{l=d+1}^{\\lfloor n^{1/2r}\\rfloor}\\left\\vert\\frac{\\widehat{\\lambda}_{l}}{n}-\\lambda_{l}\\right\\vert\\lesssim\\log(n)\\sum_{l=d+1}^{\\lfloor n^{1/2r}\\rfloor}i^{-\\alpha}\\lesssim n^{-\\frac{\\alpha-1}{\\alpha}}\\log(n).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In addition, we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{l=\\lceil n^{1/2r}\\rceil}^{n}\\left|\\frac{\\widehat{\\lambda}_{l}}{n}-\\lambda_{l}\\right|\\lesssim n^{-1/2}\\log(n)\\displaystyle\\sum_{l=\\lceil n^{1/2r}\\rceil}^{n}i^{-\\alpha+r+1}}&{}\\\\ &{\\qquad\\lesssim n^{-1/2}\\log(n)\\cdot\\left(n^{\\frac{1}{2r}}\\right)^{-\\alpha+r+2}}\\\\ &{\\qquad\\lesssim n^{-1}\\log(n)}\\\\ &{\\qquad\\lesssim n^{-\\frac{\\alpha-1}{\\alpha}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we have used that $0<r<(\\alpha-1)/2$ . Combining (14) with (15) establishes (13) under (P) assuming $r>0$ . The case with $r=0$ follows analogous fashion so we omit the details. ", "page_idx": 16}, {"type": "text", "text": "We now turn to the hypothesis (E). The authors only explicitly derive a result for the case that $\\gamma=1$ , however following through their proof with Lemma 2 to hand, we obtain the following for the general case that $0<\\gamma\\leq1$ . ", "page_idx": 16}, {"type": "text", "text": "Lemma 4. Suppose that the hypothesis $(E)$ holds with $s>0$ . Then, with overwhelming probability ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left|{\\frac{\\widehat{\\lambda}_{i}}{n}}-\\lambda_{i}\\right|\\lesssim e^{(-\\beta+\\delta)i^{\\gamma}}i^{1-\\gamma}n^{-1/2}\\log(n)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for all $1\\leq i\\leq n$ . ", "page_idx": 16}, {"type": "text", "text": "Therefore we have that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{i=d+1}^{\\infty}\\left|\\frac{\\widehat{\\lambda}_{i}}{n}-\\lambda_{i}\\right|\\lesssim n^{-1/2}\\log(n)\\sum_{i=d+1}^{n}e^{(-\\beta+s)i}i^{1-\\gamma}}}\\\\ &{\\leq n^{-1/2}\\log(n)\\sum_{i=d+1}^{n}e^{-(\\beta/2+\\varepsilon)i^{\\gamma}i^{1-\\gamma}}}\\\\ &{\\lesssim n^{-1/2}\\log(n)\\sum_{i=d+1}^{n}e^{-(\\beta i^{\\gamma}/2+\\varepsilon)}}\\\\ &{\\lesssim n^{-1/2}\\log(n)n^{-(1/2+\\varepsilon)}}\\\\ &{=\\mathcal{O}(n^{-1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where in the second inequality we have used the assumption that $s<\\beta/2$ and in the fourth we have used Lemma 2. The case for $s=0$ follows similarly, so we omit the details. Then Equation (5) is established. ", "page_idx": 17}, {"type": "text", "text": "B.1 Proof of Lemma 2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To bound the polynomial series, we upper bound it by an integral as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{i=d+1}^{\\infty}i^{-\\alpha}\\leq\\int_{d}^{\\infty}t^{-\\alpha}\\mathrm{d}t=\\frac{d^{-\\alpha+1}}{-\\alpha+1}=\\mathcal{O}\\left(d^{-\\alpha+1}\\right).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "To bound the exponential series, we again employ an integral approximation, and upper bound it as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{i=d+1}^{\\infty}e^{-\\beta i^{\\gamma}}\\leq\\int_{d}^{\\infty}e^{-\\beta t^{\\gamma}}\\mathrm{d}t.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We then apply the substitution $u=\\beta t^{\\gamma}$ to obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\int_{d}^{\\infty}e^{-\\beta t^{\\gamma}}\\mathrm{d}t=\\frac{1}{\\gamma}\\int_{\\beta d^{\\gamma}}^{\\infty}e^{-u}u^{(1-\\gamma)/\\gamma}\\mathrm{d}u=\\frac{1}{\\gamma}\\Gamma\\left(\\frac{1}{\\gamma},\\beta d^{\\gamma}\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\Gamma$ denotes the incomplete Gamma function. We can then use the fact that $\\Gamma(s,x)\\leq e^{-x}x^{s-1}$ for $s>0$ to obtain the upper bound ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\frac{1}{\\gamma}\\Gamma\\left(\\frac{1}{\\gamma},\\beta d^{\\gamma}\\right)\\leq\\frac{1}{\\gamma}e^{-\\beta d^{\\gamma}}\\left(\\beta d^{\\gamma}\\right)^{1/\\gamma-1}=\\frac{\\beta}{\\gamma}e^{\\beta d^{\\gamma}}d^{1-\\gamma},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "from which we can conclude that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{i=d+1}^{\\infty}e^{-\\beta i^{\\gamma}}=\\mathcal{O}\\left(e^{\\beta d^{\\gamma}}d^{1-\\gamma}\\right),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "as required. ", "page_idx": 17}, {"type": "text", "text": "C Proof of Lemma 1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The proof of Lemma 1 generalises the proof of Lemma 43 of Tao and $\\mathrm{Vu}$ [2011]. We will make use of the following theorem due to Ledoux [2001] which is a corollary of Talagrand\u2019s inequality [Talagrand, 1996]. ", "page_idx": 17}, {"type": "text", "text": "Theorem 4 (Talagrand\u2019s inequality). Let $y\\,=\\,(y_{1},\\ldots,y_{n})^{\\top}$ be a vector of independent random variables, and let $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ be a convex 1-Lipschitz function. Then, for all $t\\geq0$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|f(y)-M(f)|\\ge t\\right)\\le4\\exp\\left(-t^{2}/16\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $M(f)$ denotes the median of $f$ . ", "page_idx": 17}, {"type": "text", "text": "It is easy to verify that the map $y\\,\\rightarrow\\,\\Vert\\pi_{H}(y)\\Vert$ is convex and 1-Lipschitz, and so by Talagrand\u2019s inequality we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(|\\Vert\\pi_{H}(y)\\Vert-M(\\Vert\\pi_{H}(y)\\Vert)|\\ge t\\right)\\le4\\exp\\left(-t^{2}/16\\right).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For $t\\geq8$ , we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n4\\exp\\left(-(t-4)^{2}/16\\right)\\leq4\\exp\\left(-t^{2}/32\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "so to conclude the proof, it suffices to show that ", "page_idx": 18}, {"type": "equation", "text": "$$\n|M(\\|\\pi_{H}(x)\\|)-\\sigma{\\sqrt{q}}|\\leq4.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Le\u221at $\\mathcal{E}_{+}$ denote the event that $\\|\\pi_{H}(x)\\|\\,\\geq\\,\\sigma{\\sqrt{q}}+4$ and let $\\mathcal{E}_{-}$ denote the event that $\\left\\|\\pi_{H}(x)\\right\\|\\leq$ $\\sigma{\\sqrt{q}}-4$ . By the definition of a median, (17) is established if we can show that $\\mathbb{P}(\\mathcal{E}_{+})<1/2$ and $\\mathbb{P}(\\mathcal{E}_{-})<1/2$ . ", "page_idx": 18}, {"type": "text", "text": "Let $\\varepsilon$ be the mean-zero random vector such that $y=\\bar{y}+\\varepsilon$ , and let $P=(p_{i j})_{1\\leq i,j\\leq n}$ be the orthogonal projection matrix onto $H$ . We have that $\\begin{array}{r}{\\mathrm{tr}\\,P^{2}=\\mathrm{tr}\\,P=\\sum_{i}p_{i i}=q}\\end{array}$ and $|p_{i i}|\\leq1$ . Furthermore ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\|\\pi_{H}(\\varepsilon)\\right\\|^{2}-\\sigma^{2}q=\\sum_{1\\leq i,j\\leq n}p_{i j}\\varepsilon_{i}\\varepsilon_{j}-\\sigma^{2}q=S_{1}+S_{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\begin{array}{r}{S_{1}=\\sum_{i=1}^{n}p_{i i}(\\varepsilon_{i}^{2}-\\sigma^{2})}\\end{array}$ and $\\begin{array}{r}{S_{2}=\\sum_{1\\leq i\\neq j\\leq n}p_{i j}\\varepsilon_{i}\\varepsilon_{j}}\\end{array}$ . We now upper bound the expectations of $S_{1}^{2}$ and $S_{2}^{2}$ which we will use later on for bounding the probabilities of $\\mathcal{E}_{+}$ and $\\mathcal{E}_{-}$ using Markov\u2019s inequality. Before we do, note that since $\\varepsilon\\in[-\\bar{y},1-\\bar{y}]$ almost surely, Popoviciu\u2019s inequality implies that $\\sigma^{2}\\overset{\\cdot}{\\leq}1/4$ . Therefore, we also have that $\\mathbb{E}(\\varepsilon_{i}^{4})\\leq\\bar{\\sigma}^{2}$ , and so ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left(S_{1}^{2}\\right)=\\displaystyle\\sum_{i,j=1}^{n}p_{i i}p_{j j}\\mathbb{E}\\left\\{\\left(\\varepsilon_{i}^{2}-\\sigma^{2}\\right)\\left(\\varepsilon_{j}^{2}-\\sigma^{2}\\right)\\right\\}=\\displaystyle\\sum_{i=1}^{n}p_{i i}^{2}\\mathbb{E}\\left\\{\\left(\\varepsilon_{i}^{2}-\\sigma^{2}\\right)^{2}\\right\\}}\\\\ &{\\qquad\\le\\displaystyle\\sum_{i=1}^{n}p_{i i}^{2}\\left\\{\\mathbb{E}\\varepsilon_{i}^{4}-2\\sigma^{2}\\mathbb{E}\\left(\\varepsilon_{i}^{2}\\right)+(\\sigma^{2})^{2}\\right\\}\\le\\displaystyle\\sum_{i=1}^{n}p_{i i}^{2}\\left\\{\\sigma^{2}-2(\\sigma^{2})^{2}+(\\sigma^{2})^{2}\\right\\}\\le\\sigma^{2}q,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the second inequality follows from the independence of $\\left(\\varepsilon_{i}^{2}-\\sigma^{2}\\right)$ and $\\left(\\varepsilon_{j}^{2}-\\sigma^{2}\\right)$ for all $i\\neq j$ . In addition, we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left(S_{2}^{2}\\right)=\\mathbb{E}\\left(\\sum_{i\\neq j}p_{i j}\\varepsilon_{i}\\varepsilon_{j}\\right)^{2}=\\sum_{i\\neq j}p_{i j}^{2}\\mathbb{E}(\\varepsilon_{i}^{2})\\mathbb{E}(\\varepsilon_{j}^{2})\\leq(\\sigma^{2})^{2}q\\leq\\frac{\\sigma^{2}q}{4}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "To upper bound the probability of $\\mathcal{E}_{+}$ , we first observe that by assumption ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\Vert\\pi_{H}(y)\\right\\Vert^{2}=\\left\\Vert\\pi_{H}(\\bar{y})\\right\\Vert^{2}+\\left\\Vert\\pi_{H}(\\varepsilon)\\right\\Vert^{2}\\leq4\\sigma\\sqrt{q}+\\left\\Vert\\pi_{H}(\\varepsilon)\\right\\Vert^{2}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and therefore we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{^{\\!3}(\\mathcal{E}_{+})=\\mathbb{P}\\left(\\|\\pi_{H}(y)\\|\\geq\\sigma\\sqrt{q}+4\\right)\\leq\\mathbb{P}\\left(\\left\\|\\pi_{H}(y)\\right\\|^{2}\\geq\\sigma^{2}q+8\\sigma\\sqrt{q}\\right)\\leq\\mathbb{P}\\left(\\left\\|\\pi_{H}(\\varepsilon)\\right\\|^{2}\\geq\\sigma^{2}q+4\\sigma\\sqrt{\\sigma}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Using the definitions of $S_{1}$ and $S_{2}$ , it follows that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\mathcal{E}_{+})\\leq\\mathbb{P}\\left(S_{1}+S_{2}\\geq4\\sigma\\sqrt{q}\\right)\\leq\\mathbb{P}\\left(S_{1}\\geq2\\sigma\\sqrt{q}\\right)+\\mathbb{P}\\left(S_{2}\\geq2\\sigma\\sqrt{q}\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By Markov\u2019s inequality, we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(S_{1}\\geq2\\sigma\\sqrt{q}\\right)=\\mathbb{P}\\left(S_{1}^{2}\\geq4\\sigma^{2}q\\right)\\leq\\frac{\\mathbb{E}\\left(S_{1}^{2}\\right)}{4\\sigma^{2}q}\\leq\\frac{1}{4}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "and similarly that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(S_{2}\\geq2\\sigma\\sqrt{q}\\right)=\\mathbb{P}\\left(S_{2}^{2}\\geq4\\sigma^{2}q\\right)\\leq\\frac{\\mathbb{E}\\left(S_{2}^{2}\\right)}{4\\sigma^{2}q}\\leq\\frac{1}{16}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "It therefore follows that $\\mathbb{P}(\\mathcal{E}_{+})<1/2$ . We upper bound $\\mathbb{P}(\\mathcal{E}_{-})$ in a similar fashion. Since $\\|\\pi_{H}(x)\\|\\geq$ $\\left\\|\\pi_{H}(\\varepsilon)\\right\\|$ , we have that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{^{\\mathfrak{p}}(\\mathcal{E}_{-})=\\mathbb{P}(\\|\\pi_{H}(y)\\|\\le\\sigma\\sqrt{q}-4)\\le\\mathbb{P}(\\|\\pi_{H}(\\varepsilon)\\|\\le\\sigma\\sqrt{q}-4)=\\mathbb{P}(\\|\\pi_{H}(\\varepsilon)\\|^{2}\\le\\sigma^{2}q-8\\sigma\\sqrt{q}+16).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Again, recalling the definitions of $S_{1}$ and $S_{2}$ we have that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\mathcal{E}_{-}\\right)\\leq\\mathbb{P}\\left(S_{1}+S_{2}\\leq-8\\sigma\\sqrt{q}+16\\right)\\leq\\mathbb{P}(S_{1}\\leq8-4\\sigma\\sqrt{q})+\\mathbb{P}(S_{2}\\leq8-4\\sigma\\sqrt{q}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "As before, applying Markov\u2019s inequality we have that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}(S_{1}\\leq8-4\\sigma\\sqrt{q})\\leq\\mathbb{P}(S_{1}^{2}\\geq64-64\\sigma\\sqrt{q}+16\\sigma^{2}q)\\leq\\mathbb{P}\\left(S_{1}^{2}\\geq8\\sigma^{2}q\\right)\\leq\\frac{\\mathbb{E}\\left(S_{1}^{2}\\right)}{8\\sigma^{2}q}\\leq\\frac{1}{8}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we have twice used the assumption that $q\\geq64/\\sigma^{2}$ . Similarly ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{P}(S_{2}\\leq8-4\\sigma\\sqrt{q})\\leq\\mathbb{P}\\left(S_{2}^{2}\\geq8\\sigma^{2}q\\right)\\leq\\frac{\\mathbb{E}\\left(S_{2}^{2}\\right)}{8\\sigma^{2}q}\\leq\\frac{1}{32}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "It therefore follows that $\\mathbb{P}\\left(\\mathcal{E}_{-}\\right)<1/2$ , thereby establishing (17) and concluding the proof. ", "page_idx": 19}, {"type": "text", "text": "D Proof of the claim that $\\|\\pi_{J}(\\bar{y})\\|\\le2|J|^{1/4}$ ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we prove the claim made in the proof of Theorem 1 that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\|\\pi_{J}(\\bar{y})\\|\\leq2|J|^{1/4},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with overwhelming probability, which we require in order to invoke Lemma 1. For notational convenience, we shall assume we are working in an $n$ -dimensional space, rather than an $(n-1)$ - dimensional space as this is immaterial in our big- $\\scriptscriptstyle\\mathcal{O}$ bounds. ", "page_idx": 19}, {"type": "text", "text": "Recall that $\\bar{y}$ is a constant vector with entries in $[0,1]$ , and let 1 denote the all-ones vector. Let $\\xi$ be some value such that $8a<\\xi<b/2$ , which exists by assumption (R), and let $d^{\\prime}$ denote the smallest index such that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\lambda_{d^{\\prime}+1}={\\mathcal{O}}\\left(n^{-1/\\xi}\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "This implies that under $({\\bf P})$ , $d^{\\prime}=\\mathcal{O}\\left(n^{1/\\xi\\alpha}\\right)$ , and under (E), $d^{\\prime}<\\log^{1/\\gamma}(n^{1/\\xi\\beta})$ . Clearly $d^{\\prime}\\leq d$ and so $J\\subset\\{d^{\\prime}+1,\\ldots,n\\}$ , and therefore ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|\\pi_{J}(\\bar{y})\\right\\|\\leq\\left\\|\\pi_{\\{d^{\\prime}+1,\\ldots,n\\}}(\\mathbf{1})\\right\\|.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In addition, we observe that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left|\\left|\\pi_{\\{d^{\\prime}+1,\\dots,n\\}}(\\mathbf{1})\\right|\\right|^{2}=n-\\left|\\left|\\pi_{\\{1,\\dots,d^{\\prime}\\}}(\\mathbf{1})\\right|\\right|^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Since $|J|=\\Omega(n)$ , it will suffice to show that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\left\\|\\pi_{\\{1,\\ldots,d^{\\prime}\\}}(\\mathbf{1})\\right\\|^{2}=1-\\Omega\\left(n^{-1/2-\\Omega(1)}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with overwhelming probability. ", "page_idx": 19}, {"type": "text", "text": "Let $\\widehat{U}_{d^{\\prime}}$ and $U_{d^{\\prime}}$ denote the $n\\times d^{\\prime}$ matrices with entries ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\widehat{U}_{d^{\\prime}}(i,j)=\\widehat{u}_{j}(i),\\qquad U_{d^{\\prime}}(i,j)={\\frac{u_{j}(x_{i})}{n^{1/2}}}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "respectively. Then we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\left\\Vert\\pi_{\\{1,...,d^{\\prime}\\}}(\\mathbf{1})\\right\\Vert^{2}=\\frac{1}{n}\\left\\Vert\\widehat{U}_{d^{\\prime}}\\widehat{U}_{d^{\\prime}}^{\\top}\\mathbf{1}\\right\\Vert^{2}=\\frac{1}{n}\\left\\Vert\\widehat{U}_{d^{\\prime}}^{\\top}\\mathbf{1}\\right\\Vert^{2}=\\frac{1}{n}\\left\\Vert(\\widehat{U}_{d^{\\prime}}W)^{\\top}\\mathbf{1}\\right\\Vert^{2},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $W$ is an orthogonal matrix which we will define later on. By the triangle inequality, we have that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\left\\|(\\widehat{U}_{d^{\\prime}}W)^{\\top}\\mathbf{1}\\right\\|^{2}\\geq\\frac{1}{n}\\left\\|U_{d^{\\prime}}^{\\top}\\mathbf{1}\\right\\|^{2}-\\frac{1}{n}\\left\\|\\widehat{U}_{d^{\\prime}}W-U_{d^{\\prime}}^{\\top}\\right\\|^{2}\\left\\|\\mathbf{1}\\right\\|^{2}=\\frac{1}{n}\\left\\|U_{d^{\\prime}}^{\\top}\\mathbf{1}\\right\\|^{2}-\\left\\|\\widehat{U}_{d^{\\prime}}W-U_{d^{\\prime}}^{\\top}\\right\\|^{2}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and therefore to show (21), we need to show that ", "page_idx": 19}, {"type": "equation", "text": "$$\nn^{-1}\\left\\|U_{d^{\\prime}}^{\\top}\\mathbf{1}\\right\\|^{2}=1-\\mathcal{O}\\left(n^{-1/2-\\Omega(1)}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "and ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|\\widehat{U}_{d^{\\prime}}W-U_{d^{\\prime}}^{\\top}\\right\\|=O\\left(n^{-1/4-\\Omega(1)}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with overwhelming probability. ", "page_idx": 19}, {"type": "text", "text": "D.1 Bounding (22) ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "To bound (22), we begin by using the the inequality $(c_{1}+c_{2})^{2}\\leq2(c_{1}^{2}+c_{2}^{2})$ to write ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{n^{-1}\\left\\|U_{d^{\\prime}}^{\\top}\\mathbf{1}\\right\\|^{2}=n^{-1}\\displaystyle\\sum_{l=1}^{d^{\\prime}}\\left(\\displaystyle\\sum_{i=1}^{n}\\frac{u_{l}(x_{i})}{n^{1/2}}\\right)^{2}=\\displaystyle\\sum_{l=1}^{d^{\\prime}}\\left(\\displaystyle\\sum_{i=1}^{n}\\frac{u_{l}(x_{i})}{n}\\right)^{2}}\\\\ &{\\phantom{n^{-1}\\displaystyle\\sum_{l=1}^{d^{\\prime}}}\\geq\\displaystyle\\sum_{l=1}^{d^{\\prime}}\\left(\\displaystyle\\int u_{l}(x)\\mathrm{d}\\rho(x)\\right)^{2}-2\\displaystyle\\sum_{l=1}^{d^{\\prime}}\\left(\\displaystyle\\sum_{i=1}^{n}\\frac{u_{l}(x_{i})}{n}-\\displaystyle\\int u_{l}(x)\\mathrm{d}\\rho(x)\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "To bound the first term, we observe that since $\\{u_{i}\\}_{i=1}^{\\infty}$ forms an orthonormal basis for $L_{\\rho}^{2}(\\mathcal{X})$ , we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{l=1}^{\\infty}\\left(\\int u_{l}(x)\\mathrm{d}\\rho(x)\\right)^{2}=1\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and therefore ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{l=1}^{d^{\\prime}}\\left(\\int u_{l}(x)\\mathrm{d}\\rho(x)\\right)^{2}=1-\\sum_{l=d^{\\prime}+1}^{\\infty}\\left(\\int u_{l}(x)\\mathrm{d}\\rho(x)\\right)^{2}=:1-\\Gamma_{d^{\\prime}+1}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By assumption, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Gamma_{d^{\\prime}+1}=\\mathcal{O}\\left(\\lambda_{d^{\\prime}+1}^{b}\\right)=\\mathcal{O}\\left(n^{-b/\\xi}\\right)=\\mathcal{O}\\left(n^{-1/2-\\Omega(1)}\\right)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we have used the assumption that $b>\\xi/2$ , and therefore ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{l=1}^{d^{\\prime}}\\left(\\int u_{l}(x)\\mathrm{d}\\rho(x)\\right)^{2}=1-\\mathcal{O}\\left(n^{-b/\\xi}\\right)=1-\\mathcal{O}\\left(n^{-1/2-\\Omega(1)}\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "as required. ", "page_idx": 20}, {"type": "text", "text": "Now to bound the second term, we use Hoeffding\u2019s inequality to obtain that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left|\\sum_{i=1}^{n}\\frac{u_{l}(x_{i})}{n}-\\int u_{l}(x)\\mathrm{d}\\rho(x)\\right|=\\mathcal{O}\\left(\\|u_{l}\\|_{\\infty}\\frac{\\log^{1/2}(n)}{n^{1/2}}\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Under $({\\bf P})$ , we have that for all $l\\leq d^{\\prime}$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\lVert u_{l}\\right\\rVert_{\\infty}\\lesssim d^{\\prime^{r}}\\lesssim n^{r/\\xi\\alpha}=\\mathcal{O}\\left(n^{1/2\\xi}\\right)=\\mathcal{O}\\big(n^{1/16-\\Omega(1)}\\big)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we have used that $r<(\\alpha-1)/2\\leq\\alpha/2$ , and that $\\xi>8$ , and under (E) ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\|u_{l}\\right\\|_{\\infty}\\lesssim e^{s d^{\\prime\\gamma}}\\lesssim e^{s\\log(n)/\\xi\\beta}=\\mathcal{O}\\left(n^{1/2\\xi}\\right)=\\mathcal{O}(n^{1/16-\\Omega(1)})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where we have used that $s<\\beta/2$ . Therefore, since $d^{\\prime}=\\mathcal{O}\\left(n^{1/8}\\right)$ , we have that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{l=1}^{d^{\\prime}}\\left(\\sum_{i=1}^{n}\\frac{u_{l}(x_{i})}{n}-\\int u_{l}(x)\\mathrm{d}\\rho(x)\\right)^{2}\\lesssim d^{\\prime}\\left(n^{1/16-1/2}\\right)^{2}=\\mathcal{O}\\left(n^{-3/4}\\right)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which is ${\\mathcal{O}}\\left(n^{-1/2-\\Omega\\left(1\\right)}\\right)$ as required. ", "page_idx": 20}, {"type": "text", "text": "D.2 Bounding (23) ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "To bound (23), we begin by defining the $d^{\\prime}\\times d^{\\prime}$ diagonal matrices $\\widehat{\\Lambda}_{d^{\\prime}}$ and $\\Lambda_{d^{\\prime}}$ with diagonal entries ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\widehat{\\Lambda}_{d^{\\prime}}(i,i):=\\widehat{\\frac{\\lambda_{i}}{n}},\\qquad\\Lambda_{d^{\\prime}}(i,i):=\\lambda_{i},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "respectively, and the $n\\times d^{\\prime}$ matrices $\\widehat{\\Phi}_{d^{\\prime}}$ and $\\Phi_{d^{\\prime}}$ with entries ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\widehat{\\Phi}_{d^{\\prime}}(i,j)={\\widehat\\lambda}_{j}^{1/2}\\widehat{u}_{j}(i),\\qquad\\Phi_{d^{\\prime}}(i,j)=\\lambda_{j}^{1/2}u_{j}(i),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "respectively. Then in matrix notation we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\widehat{\\Phi}_{d^{\\prime}}=n^{1/2}\\widehat{U}_{d^{\\prime}}\\widehat{\\Lambda}_{d^{\\prime}}^{1/2},\\qquad\\Phi_{d^{\\prime}}=n^{1/2}U_{d^{\\prime}}\\Lambda_{d^{\\prime}}^{1/2}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now, we decompose $\\widehat{U}_{d^{\\prime}}W_{d^{\\prime}}-U_{d^{\\prime}}$ as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\widehat{U}_{d^{\\prime}}W_{d^{\\prime}}-n^{-1/2}U_{d^{\\prime}}=\\left\\{n^{-1/2}\\left(\\widehat{\\Phi}_{d^{\\prime}}W_{d^{\\prime}}-\\Phi_{d^{\\prime}}\\right)+\\widehat{U}_{d^{\\prime}}\\left(W_{d^{\\prime}}\\Lambda_{d^{\\prime}}^{1/2}-\\widehat{\\Lambda}_{d^{\\prime}}^{1/2}W_{d^{\\prime}}\\right)\\right\\}\\Lambda^{-1/2}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and so we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\widehat{U}_{d^{\\prime}}W_{d^{\\prime}}-U_{d^{\\prime}}\\right\\|\\leq\\lambda_{d^{\\prime}}^{-1/2}\\left\\{n^{-1/2}\\left\\|\\widehat{\\Phi}_{d^{\\prime}}W_{d^{\\prime}}-\\Phi_{d^{\\prime}}\\right\\|_{\\mathrm{F}}+\\left\\|W_{d^{\\prime}}\\Lambda_{d^{\\prime}}^{1/2}-\\widehat{\\Lambda}_{d^{\\prime}}^{1/2}W_{d^{\\prime}}\\right\\|\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By the construction of $d^{\\prime}$ we have that $\\lambda_{d^{\\prime}}\\,=\\,\\Omega(n^{-1/\\xi})\\,=\\,\\Omega(n^{-1/8})$ and therefore $\\lambda_{d^{\\prime}}^{-1/2}\\;=\\;$ $O\\left(n^{1/16}\\right)$ . Therefore to show (23), it will suffice to show that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\|\\widehat{\\Phi}_{d^{\\prime}}W_{d^{\\prime}}-\\Phi_{d^{\\prime}}\\right\\|_{\\mathrm{F}}=\\mathcal{O}\\left(n^{\\frac{3}{16}-\\Omega(1)}\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\|{\\cal W}_{d^{\\prime}}\\Lambda_{d^{\\prime}}^{1/2}-{\\widehat\\Lambda}_{d^{\\prime}}^{1/2}{\\cal W}_{d^{\\prime}}\\right\\|=\\mathcal{O}\\left(n^{-\\frac{5}{16}-\\Omega(1)}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "To obtain the bound (24), we make use of the following result which is Equation 3.8 of Tang et al.   \n[2013]. ", "page_idx": 21}, {"type": "text", "text": "Lemma 5. The exists an orthogonal matrix $W_{d}$ (constructed as in (29)) such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\|\\widehat{\\Phi}_{d}W_{d}-\\Phi_{d}\\right\\|_{\\mathrm{F}}=\\mathcal{O}\\left(\\frac{\\log^{1/2}(n)}{\\lambda_{d}-\\lambda_{d+1}}\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "with overwhelming probability. ", "page_idx": 21}, {"type": "text", "text": "Now, let ", "page_idx": 21}, {"type": "equation", "text": "$$\nd^{\\star}:=\\arg\\operatorname*{max}_{i\\geq d^{\\prime}}\\left\\{\\lambda_{i}-\\lambda_{i+1}\\right\\},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "then by the assumption (R), we have that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\lambda_{d^{\\star}}-\\lambda_{d^{\\star}+1}=\\Omega(\\lambda_{d^{\\prime}}^{a})=\\Omega(n^{-a/\\xi})=\\Omega\\left(n^{-1/8+\\Omega(1)}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Using Lemma 5, we obtain that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\|\\widehat{\\Phi}_{d^{\\prime}}W_{d^{\\prime}}-\\Phi_{d^{\\prime}}\\right\\|_{\\mathrm{F}}\\leq\\left\\|\\widehat{\\Phi}_{d^{\\star}}W_{d^{\\star}}-\\Phi_{d^{\\star}}\\right\\|_{\\mathrm{F}}=\\mathcal{O}\\left(\\frac{\\log^{1/2}(n)}{\\lambda_{d^{\\star}}-\\lambda_{d^{\\star}+1}}\\right)=\\mathcal{O}\\left(n^{1/8-\\Omega(1)}\\right),\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which establishes (24). Obtaining the bound (25) requires some new concepts, so we dedicate it its own section. ", "page_idx": 21}, {"type": "text", "text": "D.3 Bounding (25) ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We now turn our attention to the bound (25). We begin by defining $\\mathcal{H}$ , the reproducing kernel Hilbert space associated with the kernel $k$ , and define the operators $\\kappa_{\\mathcal{H}},\\widehat{\\mathcal{K}}_{\\mathcal{H}}:\\mathcal{H}\\overset{\\,\\cdot\\,}{\\to}\\mathcal{H}$ given by ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{\\mathcal K}_{\\mathcal H}f=\\int\\langle f,k_{x}\\rangle_{\\mathcal H}\\,k_{x}\\mathrm d\\rho(x),}\\\\ {\\displaystyle\\widehat{\\kappa}_{\\mathcal H}=\\frac{1}{n}\\sum_{i=1}^{n}\\langle f,k_{x_{i}}\\rangle_{\\mathcal H}\\,k_{x_{i}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $k_{x}(\\cdot)\\,=\\,k(\\cdot,x)$ and $\\left<\\cdot,\\cdot\\right>_{\\mathcal{H}}$ is the inner product in $\\mathcal{H}$ . These operators are known as the \u201cextension operators\u201d of $\\kappa$ and $\\scriptstyle{\\frac{1}{n}}K$ , respectively, and it may be shown that each has the same eigenvalues as its corresponding operator, possibly up to zeros (see e.g. Propositions 8 and 9 of Rosasco et al. [2010]). We will make use of the following concentration inequality for $\\kappa_{\\mathcal{H}}-\\widehat{\\kappa}_{\\mathcal{H}}$ which is due to De Vito et al. [2005] and appears as Theorem 7 of Rosasco et al. [2010]. ", "page_idx": 21}, {"type": "text", "text": "Lemma 6 (Theorem 7 of Rosasco et al. [2010]). The operators $\\kappa_{\\mathcal{H}}$ and $\\widehat{\\kappa}_{\\mathcal{H}}$ are Hilbert-Schmidt, and ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\left\\|\\widehat{\\mathcal{K}}_{\\mathcal{H}}-\\mathcal{K}_{\\mathcal{H}}\\right\\|_{H S}=\\mathcal{O}\\left(\\frac{\\log^{1/2}(n)}{n^{1/2}}\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "with overwhelming probability. ", "page_idx": 22}, {"type": "text", "text": "Let $\\left\\{u_{\\mathcal{H},i}\\right\\}_{i\\leq d}$ denote the eigenfunctions of $\\kappa_{\\mathcal{H}}$ corresponding to the eigenvalues $\\{\\lambda_{i}\\}_{i\\le d}$ , and let $\\{\\widehat{u}_{\\mathcal{H},i}\\}_{i\\leq d}$ denote the eigenfunctions of $\\widehat{\\kappa}$ corresponding to the eigenvalues $\\{\\widehat\\lambda_{i}/n\\}_{i\\leq d}$ . We define the (infinite-dimensional) \u201cmatrices\u201d $U_{\\mathcal{H},d}$ and $\\widehat{U}_{\\mathcal{H},d}$ whose columns contain the eigenfunctions $\\left\\{u_{\\mathcal{H},i}\\right\\}_{i\\leq d}$ and $\\left\\{\\widehat{u}_{\\mathcal{H},i}\\right\\}_{i\\leq d}$ , respectively. These \u201cm atrices\u201d are well-defined with matrix multiplication is defined as usual with inner products taken in $\\mathcal{H}$ . We will refer to $U_{\\mathcal{H},d},\\widehat{U}_{\\mathcal{H},d}$ and the subspaces spanned by their columns interchangably. ", "page_idx": 22}, {"type": "text", "text": "Let $H_{d}\\;=\\;U_{\\mathcal{H},d}^{\\top}\\widehat{U}_{\\mathcal{H},d}$ with entries $H_{d}(i,j)\\;=\\;\\langle u\\varkappa,i,\\widehat{u}\\varkappa,j\\rangle_{\\mathcal{H}}$ and denote its singular values by $\\xi_{1},\\allowbreak\\dots,\\allowbreak\\xi_{d}$ . Then the principal angles between the subspaces $U_{\\mathcal{H},d}$ and $\\widehat{U}_{\\mathcal{H},d}$ , which we will denote by $\\theta_{1},\\ldots,\\theta_{d}$ , are define as via $\\xi_{i}=\\cos(\\theta_{i})$ . Define the matrix ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sin\\Theta\\left(\\widehat{U}_{\\mathcal{H},d},U_{\\mathcal{H},d}\\right)=\\mathrm{diag}\\left(\\sin(\\theta_{1}),\\ldots,\\sin(\\theta_{d})\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Let $d^{\\star}$ be as in (26) so that $\\lambda_{d^{\\star}}\\,-\\lambda_{d^{\\star}+1}\\,=\\,\\Omega\\left(n^{-1/8+\\Omega\\left(1\\right)}\\right)$ . Since $d^{\\star}\\geq d^{\\prime}$ , by the Davis-Kahan theorem, we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\sin\\Theta\\left(\\widehat{U}_{\\mathcal{H},d^{\\prime}},U_{\\mathcal{H},d^{\\prime}}\\right)\\right\\|\\leq\\left\\|\\sin\\Theta\\left(\\widehat{U}_{\\mathcal{H},d^{\\star}},U_{\\mathcal{H},d^{\\star}}\\right)\\right\\|\\leq\\displaystyle\\frac{\\sqrt{2}\\left\\|\\widehat{K}_{\\mathcal{H}}-K_{\\mathcal{H}}\\right\\|}{\\lambda_{d^{\\star}}-\\lambda_{d^{\\star}+1}}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{\\sqrt{2}\\left\\|\\widehat{K}_{\\mathcal{H}}-K_{\\mathcal{H}}\\right\\|_{\\mathrm{HS}}}{\\lambda_{d^{\\star}}-\\lambda_{d^{\\star}+1}}=\\mathcal{O}\\left(n^{-3/8-\\Omega(1)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the second inequality comes from the relationship $\\lVert\\cdot\\rVert\\leq\\lVert\\cdot\\rVert_{\\mathrm{HS}}$ , and the final inequality follows from Lemma 6 and (27). ", "page_idx": 22}, {"type": "text", "text": "We now come to constructing the matrix $W_{d}$ . We denote the singular value decomposition of $H$ as $H=W_{d,1}\\Xi W_{d,2}^{\\top}$ and define the matrix $W_{d}$ by ", "page_idx": 22}, {"type": "equation", "text": "$$\nW=W_{d,1}W_{d,2}^{\\top},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which is known as the \u201cmatrix sign\u201d of $H$ . Let $\\widehat{\\mathcal{P}}_{d}$ and $\\mathcal{P}_{d}$ to be the projections onto the subspaces $\\widehat{U}_{\\mathcal{H},d}$ and $U_{\\mathcal{H},d}$ , respectively. Then we have the following decomposition. ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{d^{\\prime}}\\Lambda_{d^{\\prime}}^{1/2}-\\widehat\\Lambda_{d^{\\prime}}^{1/2}W_{d^{\\prime}}=(W_{d^{\\prime}}-H_{d^{\\prime}})\\widehat\\Lambda_{d^{\\prime}}+\\Lambda_{d^{\\prime}}(H_{d^{\\prime}}-W_{d^{\\prime}})}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\;U_{\\mathcal{H},d^{\\prime}}^{\\top}(\\widehat{K}_{\\mathcal{H}}-K_{\\mathcal{H}})(\\widehat{\\mathcal{P}}_{d^{\\prime}}-\\mathcal{P}_{d^{\\prime}})\\widehat{U}_{\\mathcal{H},d^{\\prime}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\;U_{\\mathcal{H},d^{\\prime}}^{\\top}(\\widehat{K}_{\\mathcal{H}}-K_{\\mathcal{H}})U_{\\mathcal{H},d^{\\prime}}H_{d^{\\prime}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We first observe that $\\|H_{d^{\\prime}}\\|\\leq\\|\\widehat{U}_{\\mathcal{H},d^{\\prime}}\\|\\|U_{\\mathcal{H},d^{\\prime}}\\|=1$ , and by (5), $\\left\\|\\widehat{\\Lambda}_{d^{\\prime}}\\right\\|,\\left\\|\\Lambda_{d^{\\prime}}\\right\\|=\\mathcal{O}(1)$ . In addition, following the same steps as in the proof Lemma 6.7 of Cape et al. [2019] (who prove similar (in)equalities for finite-dimensional matrices) we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\|\\widehat{\\mathcal{P}}_{d^{\\prime}}-\\mathcal{P}_{d^{\\prime}}\\right\\|=\\left\\|\\sin\\Theta\\left(\\widehat{U}_{\\mathcal{H},d^{\\prime}},U_{\\mathcal{H},d^{\\prime}}\\right)\\right\\|=\\mathcal{O}\\left(n^{-3/8-\\Omega(1)}\\right)}\\\\ &{\\left\\|H_{d^{\\prime}}-W_{d^{\\prime}}\\right\\|\\leq\\left\\|\\sin\\Theta\\left(\\widehat{U}_{\\mathcal{H},d^{\\prime}},U_{\\mathcal{H},d^{\\prime}}\\right)\\right\\|^{2}=\\mathcal{O}\\left(n^{-3/4-\\Omega(1)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We now turn to bounding $\\|U_{\\mathcal{H},d}^{\\top}(\\widehat{\\mathcal{K}}_{\\mathcal{H}}-K_{\\mathcal{H}})U_{\\mathcal{H},d}\\|$ . To condense notation, let $Q=U_{\\mathcal{H},d}^{\\top}(\\widehat{\\mathcal{K}}_{\\mathcal{H}}-$ $\\kappa_{\\mathcal{H}})U_{\\mathcal{H},d}$ . We will bound $\\lVert Q\\rVert$ using a classical $\\varepsilon$ -net argument. Let $\\mathbb{S}_{\\varepsilon}^{d-1}$ be an $\\varepsilon$ -net of the $(d-1)$ - dimensional unit sphere $\\mathbb{S}^{d-1}:=\\{v:\\|v\\|=1\\}$ , that is, a subset of $\\mathbb{S}^{d-1}$ such that for any $v\\in\\mathbb{S}^{d-1}$ , ", "page_idx": 22}, {"type": "text", "text": "there exists some $w_{v}\\in\\mathbb{S}_{\\varepsilon}^{d-1}$ such that $\\|v-w_{v}\\|<\\varepsilon$ . Then, we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|Q\\|=\\underset{v:\\|v\\|\\leq1}{\\operatorname*{max}}\\left|v^{\\top}Q v\\right|}\\\\ &{\\quad\\quad=\\underset{v:\\|v\\|\\leq1}{\\operatorname*{max}}\\left|\\left(v-w_{v}+w_{v}\\right)^{\\top}Q\\left(v-w_{v}+w_{v}\\right)\\right|}\\\\ &{\\quad\\quad\\leq\\left(\\varepsilon^{2}+2\\varepsilon\\right)\\|Q\\|+\\underset{w\\in\\mathbb S_{\\varepsilon}^{d-1}}{\\operatorname*{max}}\\left|w^{\\top}Q w\\right|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "With $\\varepsilon=1/3$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\|Q\\right\\|\\leq\\frac{9}{2}\\operatorname*{max}_{w\\in\\mathbb{S}_{\\varepsilon}^{d-1}}\\left|w^{\\top}Q w\\right|.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Using the definitions (28), its $(l,m)$ th entry can be calculated as ", "page_idx": 23}, {"type": "equation", "text": "$$\nQ(l,m)=\\frac{1}{n^{2}}\\sum_{i,j=1}^{n}k(x_{i},x_{j})u_{l}(x_{i})u_{m}(x_{j})-\\iint k(x,y)u_{l}(x)u_{m}(y)\\mathrm{d}\\rho(x)\\mathrm{d}\\rho(y),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and so for a given $w\\in\\mathbb{S}_{\\varepsilon}^{d-1}$ , we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle w^{\\top}Q w\\big|=\\left|\\displaystyle\\sum_{l,m=1}^{d}Q(l,m)w(l)w(m)\\right|}}\\\\ {{\\displaystyle~~~~~~~=\\left|\\displaystyle\\sum_{l,m=1}^{d}w(l)w(m)\\left(\\frac{1}{n^{2}}\\sum_{i,j=1}^{n}k(x_{i},x_{j})u_{l}(x_{i})u_{m}(x_{j})-\\iint k(x,y)u_{l}(x)u_{m}(y)\\mathrm{d}\\rho(x)\\mathrm{d}\\rho(y)\\mathrm{d}\\rho(y)\\mathrm{d}x\\right|}}\\\\ {{\\displaystyle~~~~~\\leq\\operatorname*{max}_{1\\leq l\\leq d}\\|u_{l}\\|_{\\infty}\\left|\\sum_{i=1}^{n}\\left\\{\\sum_{l,m=1}^{d}w(l)w(m)\\left(\\frac{u_{m}(x_{i})}{n}-\\int u_{m}(x)\\mathrm{d}\\rho(x)\\right)\\right\\}\\right|}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we have used that $k(x,y)\\leq1$ for all $x,y\\in\\mathcal{X}$ . This is a sum of independent random variables, so by Hoeffding\u2019s inequality, we that that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb P\\left(\\left|w^{\\top}Q w\\right|\\geq t\\right)\\leq2\\exp\\left(-\\frac{2n t^{2}}{\\operatorname*{max}_{1\\leq l\\leq d}\\left\\|u_{l}\\right\\|_{\\infty}^{4}}\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we have used that $\\begin{array}{r}{\\sum_{l,m=1}^{d}w(l)w(m)=1}\\end{array}$ . The set $\\mathbb{S}_{1/3}^{d-1}$ can be selected so that its cardinality is no greater than $18^{d}$ (see, for example, Pollard [1990]), so using a union bound we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(\\|Q\\|\\geq t)\\leq\\mathbb{P}\\left(\\underset{w\\in\\mathbb{S}_{1/3}^{d-1}}{\\operatorname*{max}}\\left|w^{\\top}Q w\\right|\\geq\\frac{2}{9}t\\right)}\\\\ &{\\qquad\\leq\\sum w\\in\\mathbb{S}_{1/3}^{d-1}\\mathbb{P}\\left(\\left|w^{\\top}Q w\\right|\\geq\\frac{2}{9}t\\right)}\\\\ &{\\qquad\\leq2\\cdot18^{d^{\\prime}}\\exp\\left(-\\frac{8n t^{2}}{81\\cdot\\operatorname*{max}_{1\\leq l\\leq\\ell^{\\prime}}\\left\\|u_{l}\\right\\|_{\\infty}^{4}}\\right)}\\\\ &{\\qquad\\leq2\\exp\\left(d^{\\prime}\\log(18)-\\frac{8n t^{2}}{81\\cdot\\operatorname*{max}_{1\\leq l\\leq\\ell^{\\prime}}\\left\\|u_{l}\\right\\|_{\\infty}^{4}}\\right)}\\\\ &{\\qquad\\leq2\\exp\\left(C\\left(n^{1/8-01}-n^{3/4}t^{2}\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we have used that $d^{\\prime}=O(n^{1/8-\\Omega(1)})$ and $\\mathrm{max}_{1\\le l\\le d^{\\prime}}\\left\\|u_{l}\\right\\|_{\\infty}=\\mathcal{O}(n^{1/16})$ . Choosing $t=$ we have that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\|Q\\|\\ge2n^{-5/16-\\Omega(1)}\\right)\\le2\\exp\\left(-n^{1/8}\\right)\\le n^{-c}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for any $c>0$ for large enough $n$ . Therefore ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\|Q\\right\\|=\\mathcal{O}\\left(n^{-5/16-\\Omega(1)}\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "image", "img_path": "ziYC4FHRNr/tmp/edf1c5f23d4346a3f35fbc1d3fa0bc6892e1a1492ed3e40f1e04b14f40eda57d.jpg", "img_caption": ["Figure 2: The Frobenius-norm error against rank for low-rank approximations of kernel matrices constructed from a collection of datasets. The kernel matrices are constructed using Mat\u00e9rn kernels with a range of smoothness parameters, each of which is represented by a line in each plot. Details of the experiment are provided in Section 5. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "with overwhelming probability. ", "page_idx": 24}, {"type": "text", "text": "Combining the above bounds with (30) we obtain that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left|\\left|W_{d^{\\prime}}\\Lambda_{d^{\\prime}}^{1/2}-\\widehat\\Lambda_{d^{\\prime}}^{1/2}W_{d^{\\prime}}\\right|\\right|=\\mathcal{O}\\left(n^{-5/16-\\Omega(1)}\\right)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "which establishes (25) and therefore completes the proof. ", "page_idx": 24}, {"type": "text", "text": "E Additional details about the experiments ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we provide some additional details and plots relating to the experiments in Section 5. ", "page_idx": 24}, {"type": "text", "text": "E.1 Details about the datasets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "GMM is a synthetic dataset of 1,000 simulated data points from a 10-component Gaussian mixture model with unit isotropic covariances and means of size ten on the axes. Abalone [Nash et al., 1995] and Wine Quality [Cortez et al., 2009] are popular benchmark datasets which we standarised in the usual way by centering and rescaling each feature to have unit variance. We drop the binary Sex feature from the Abalone dataset to retain only the continuous features. MNIST [Deng, 2012] is a dataset of handwritten digits, represented as $28\\mathrm{x}28$ gray-scale pixels which we concatenate into 784 dimensional vectors. These four datasets might be described as \u201clow-dimensional\u201d, and are thus representative of the theory we present in Section 3. ", "page_idx": 24}, {"type": "text", "text": "In addition, we consider two \u201chigh-dimensional\u201d datasets. 20 Newsgroups [Lang, 1995] is a popular natural language dataset of messages collected from twenty different \u201cnewnews\u201d newsgroups. We remove stop-word and words which appear in fewer than 5 documents or more than $80\\%$ of them, and convert each document into a vector using term frequency-inverse document frequency (tf-idf) features for each word. Zebrafish [Wagner et al., 2018] is a dataset of single-cell gene expression in a zebrafish embryo taken during their first day of development. We subsample $10\\%$ of the cells, and process the data following the steps in Wagner et al. [2018]. ", "page_idx": 24}, {"type": "text", "text": "E.2 Frobenius norm errors ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Figure 2 shows the Frobenius norm error of the low-rank approximations. For the low-dimensional datasets GMM, Abalone, Wine Quality and MNIST, the Frobenius norm error decays very quickly. However for the high-dimensional datasets 20 Newsgroups and Zebrafish, the Frobenius norm error decays much more slowly. As pointed out in the main text, the Frobenius norm error of the 20 Newsgroups dataset does not exhibit the sharp drop between $d\\,=\\,2500$ and $d\\,=\\,3000$ that the maximum entrywise error exhibits. ", "page_idx": 25}, {"type": "text", "text": "E.3 Implementation details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The experiments were performed on the HPC cluster at Imperial College London with 8 cores and 16GB of RAM. The GMM experiment took less than 1 minute; the Abalone, Wine Quality, MNIST and Zebrafish experiments took less than 8 hours; and the 20 Newsgroups experiment took less than 24 hours. ", "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 26}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 26}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 26}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 26}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 26}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 26}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: See Section 1.1. Contributions ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The limitations of our theory are discussed extensively in Section 6.1. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: The assumptions for the main theorem, Theorem 1, are provided in Section 3 and are summarised in Table 1. The full proof is provided in Section 4 and additonal technical details are provided in Sections B, C and D. Proofs of Propositions 1 and 2 are provided in Section A. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Code to reproduce the experiments is provided with the submission. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: All datasets used for the experiments are openly available and a link to the code to reproduce the experiments is provided in Section 5. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. ", "page_idx": 28}, {"type": "text", "text": "\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Preprocessing of data is explained and code provided. There are no hyperparameters to tune or data splits in our experiment. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [No] ", "page_idx": 29}, {"type": "text", "text": "Justification: Our experiments are deterministic, so there are no error bars to show. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Details are provided in Section E. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 29}, {"type": "text", "text": "\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: In the introduction, we discuss the importance of seeking methods with low entrywise error bounds for applications in which individual errors carry a high cost. Our work is foundational theory, and we don\u2019t foresee any negative societal impacts. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Licenses for the datasets used in the experiments are described with the provided code. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]