[{"figure_path": "Y2I0Fy4sm7/tables/tables_4_1.jpg", "caption": "Table 4: Communication during training, where P, A, N refer to parameter, activation size and number of sub-batches respectively.", "description": "This table compares the communication costs of SpeedLoader and ZeRO during the training process. It breaks down the communication into collective communication (parameter gathering, gradient reduce-scatter) and local communication (parameter loading, activation loading/offloading, gradient loading/offloading).  It highlights SpeedLoader's significant reduction in communication overhead, especially in collective communication, compared to ZeRO.", "section": "3.3 Hyperparameter Tuning Strategies and Communication Analysis"}, {"figure_path": "Y2I0Fy4sm7/tables/tables_5_1.jpg", "caption": "Table 1: Platform specifications for each experiment", "description": "This table lists the hardware and network configurations used in the experiments.  It shows the internode and intranode connections, along with the specific accelerators (GPUs) used for each type of experiment: functionality benchmark, profiling, scalability, and compatibility testing.  The different types of experiments used different hardware setups to explore the impact of various factors on SpeedLoader's performance.", "section": "4 Evaluation"}, {"figure_path": "Y2I0Fy4sm7/tables/tables_8_1.jpg", "caption": "Table 2: Pretraining results of DeepSpeed and SpeedLoader", "description": "This table presents the results of pretraining experiments conducted using both DeepSpeed and the proposed SpeedLoader approach.  It compares the loss and the number of tokens processed (in millions) for two different model sizes (7B and 13B parameters) across three datasets: Wikipedia, OpenWebText, and C4.  The comparison highlights SpeedLoader's improved performance in terms of tokens processed while achieving comparable or lower loss values.", "section": "5.4 Functionality Test and Scalability"}, {"figure_path": "Y2I0Fy4sm7/tables/tables_13_1.jpg", "caption": "Table 3: List of Abbreviations", "description": "This table lists the common abbreviations used throughout the paper, including their full forms and meanings.  The abbreviations cover key concepts in large language model (LLM) training and optimization, such as FLOPs (floating-point operations), MFU (Model FLOPs Utilization), HBM (High-Bandwidth Memory), and others.", "section": "A.1 List of Abbreviations"}, {"figure_path": "Y2I0Fy4sm7/tables/tables_14_1.jpg", "caption": "Table 4: Communication during training, where P, A, N refer to parameter, activation size and number of sub-batches respectively.", "description": "This table compares the communication overhead of the proposed SpeedLoader method with the ZeRO method during the training process.  It breaks down the communication into collective communication (operations involving all participating devices) and local communication (operations between a device and the host). For each category, it lists the amount of communication in terms of P (parameter size), A (activation size), and N (number of sub-batches). The total communication cost is also summarized for each method.", "section": "A.3 Communication Analysis"}]