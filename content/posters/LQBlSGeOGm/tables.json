[{"figure_path": "LQBlSGeOGm/tables/tables_5_1.jpg", "caption": "Table 1: Impact of pre-trained Phenom1 and MolGPS on CLOOME and MolPhenix for a matched number of seen samples (Top), where we observe an 8.1 \u00d7 improvement of MolPhenix over the CLOOME baseline for active unseen molecules. SOTA results trained with a higher number of steps by utilizing the best hyperparameters (Bottom *). We note that MolPhenix's main components such as S2L and embedding averaging relies on having a pre-trained uni-modal phenomics model.", "description": "This table compares the performance of CLOOME and MolPhenix models in a zero-shot setting.  The top part of the table shows results with a matched number of seen samples, highlighting that MolPhenix achieves an 8.1x improvement over CLOOME for identifying active unseen molecules. The bottom section displays results when using the best hyperparameters obtained after extensive training, demonstrating further performance gains and emphasizing the key role of the pre-trained uni-modal phenomics model in MolPhenix's success.", "section": "4 Experimental Setup"}, {"figure_path": "LQBlSGeOGm/tables/tables_6_1.jpg", "caption": "Table 1: Impact of pre-trained Phenom1 and MolGPS on CLOOME and MolPhenix for a matched number of seen samples (Top), where we observe an 8.1 \u00d7 improvement of MolPhenix over the CLOOME baseline for active unseen molecules. SOTA results trained with a higher number of steps by utilizing the best hyperparameters (Bottom *). We note that MolPhenix's main components such as S2L and embedding averaging relies on having a pre-trained uni-modal phenomics model.", "description": "This table compares the performance of CLOOME and MolPhenix models on the task of zero-shot molecular retrieval. The results are broken down into different categories: active molecules (molecules known to have a biological effect), all molecules, and for unseen images (images of cells treated with molecules not seen during training), unseen images + unseen molecules (zero-shot molecular retrieval), and unseen dataset (zero-shot molecular retrieval on an independent test dataset).  The table shows significant improvements in performance achieved by MolPhenix, particularly when using both pre-trained phenomics and molecular models and the novel S2L loss function. The bottom section shows results with optimal hyperparameters, revealing an 8.1x improvement over the state-of-the-art.  Note that the effectiveness of MolPhenix relies on using a pre-trained uni-modal phenomics model.", "section": "Results and Discussion"}, {"figure_path": "LQBlSGeOGm/tables/tables_7_1.jpg", "caption": "Table 1: Impact of pre-trained Phenom1 and MolGPS on CLOOME and MolPhenix for a matched number of seen samples (Top), where we observe an 8.1 \u00d7 improvement of MolPhenix over the CLOOME baseline for active unseen molecules. SOTA results trained with a higher number of steps by utilizing the best hyperparameters (Bottom *). We note that MolPhenix's main components such as S2L and embedding averaging relies on having a pre-trained uni-modal phenomics model.", "description": "This table compares the performance of CLOOME and MolPhenix models on several tasks, highlighting the significant improvement achieved by MolPhenix, particularly for active unseen molecules (8.1x better than CLOOME).  The impact of using pre-trained uni-modal models (Phenom1 and MolGPS) is also shown.  The table also presents state-of-the-art (SOTA) results achieved with additional training.", "section": "Results and Discussion"}, {"figure_path": "LQBlSGeOGm/tables/tables_7_2.jpg", "caption": "Table 1: Impact of pre-trained Phenom1 and MolGPS on CLOOME and MolPhenix for a matched number of seen samples (Top), where we observe an 8.1 \u00d7 improvement of MolPhenix over the CLOOME baseline for active unseen molecules. SOTA results trained with a higher number of steps by utilizing the best hyperparameters (Bottom *). We note that MolPhenix's main components such as S2L and embedding averaging relies on having a pre-trained uni-modal phenomics model.", "description": "This table compares the performance of CLOOME and MolPhenix models on three different datasets.  The top part shows results for the matched number of seen samples, highlighting MolPhenix's 8.1x improvement in top-1% retrieval accuracy over CLOOME for active unseen molecules. The bottom section presents state-of-the-art (SOTA) results achieved with more training steps and optimal hyperparameters, emphasizing MolPhenix's reliance on a pre-trained uni-modal phenomics model for its improved performance.", "section": "4 Experimental Setup"}, {"figure_path": "LQBlSGeOGm/tables/tables_8_1.jpg", "caption": "Table 2: Top-1% recall accuracy with use of the proposed MolPhenix guidelines, such as Phenom1 and embedding averaging. We omit explicit concentration from this experiment.", "description": "This table presents the top-1% recall accuracy results achieved by different methods on unseen images and unseen molecules, along with the overall performance on unseen datasets. The results highlight the improvement in accuracy obtained by incorporating the proposed MolPhenix guidelines, namely utilizing a pre-trained phenomics model (Phenom1) and averaging embeddings.  The experiment omits explicit concentration encoding.", "section": "5 Results and Discussion"}, {"figure_path": "LQBlSGeOGm/tables/tables_23_1.jpg", "caption": "Table 1: Impact of pre-trained Phenom1 and MolGPS on CLOOME and MolPhenix for a matched number of seen samples (Top), where we observe an 8.1 \u00d7 improvement of MolPhenix over the CLOOME baseline for active unseen molecules. SOTA results trained with a higher number of steps by utilizing the best hyperparameters (Bottom *). We note that MolPhenix's main components such as S2L and embedding averaging relies on having a pre-trained uni-modal phenomics model.", "description": "This table compares the performance of CLOOME and MolPhenix models on active and all molecules, with different modalities (images, Phenom1, and MolGPS), showing a significant improvement in MolPhenix's performance, especially for active unseen molecules. It highlights the impact of using pre-trained uni-modal encoders, undersampling inactive molecules, and encoding molecular concentrations.", "section": "4 Experimental Setup"}, {"figure_path": "LQBlSGeOGm/tables/tables_23_2.jpg", "caption": "Table 1: Impact of pre-trained Phenom1 and MolGPS on CLOOME and MolPhenix for a matched number of seen samples (Top), where we observe an 8.1 \u00d7 improvement of MolPhenix over the CLOOME baseline for active unseen molecules. SOTA results trained with a higher number of steps by utilizing the best hyperparameters (Bottom *). We note that MolPhenix's main components such as S2L and embedding averaging relies on having a pre-trained uni-modal phenomics model.", "description": "This table compares the performance of CLOOME and MolPhenix models on various tasks, demonstrating MolPhenix's significant improvements, especially regarding active molecule retrieval.  The top section shows results with a matched number of seen samples, highlighting MolPhenix's superior performance. The bottom section shows state-of-the-art results achieved by training with a higher number of steps.  It emphasizes the importance of MolPhenix's key components\u2014S2L loss and embedding averaging\u2014which rely on a pre-trained uni-modal phenomics model.", "section": "Experimental Setup"}, {"figure_path": "LQBlSGeOGm/tables/tables_30_1.jpg", "caption": "Table 1: Impact of pre-trained Phenom1 and MolGPS on CLOOME and MolPhenix for a matched number of seen samples (Top), where we observe an 8.1 \u00d7 improvement of MolPhenix over the CLOOME baseline for active unseen molecules. SOTA results trained with a higher number of steps by utilizing the best hyperparameters (Bottom *). We note that MolPhenix's main components such as S2L and embedding averaging relies on having a pre-trained uni-modal phenomics model.", "description": "This table compares the performance of CLOOME and MolPhenix models on active and all molecules under various conditions.  It highlights the significant performance gain achieved by MolPhenix (8.1x improvement over CLOOME) when using a pre-trained Phenom1 model and MolGPS embeddings. The table also shows results for both a matched number of seen samples and state-of-the-art (SOTA) results with more training steps, emphasizing the impact of pre-trained models and the proposed MolPhenix techniques.", "section": "4 Experimental Setup"}, {"figure_path": "LQBlSGeOGm/tables/tables_31_1.jpg", "caption": "Table 1: Impact of pre-trained Phenom1 and MolGPS on CLOOME and MolPhenix for a matched number of seen samples (Top), where we observe an 8.1 \u00d7 improvement of MolPhenix over the CLOOME baseline for active unseen molecules. SOTA results trained with a higher number of steps by utilizing the best hyperparameters (Bottom *). We note that MolPhenix's main components such as S2L and embedding averaging relies on having a pre-trained uni-modal phenomics model.", "description": "This table presents a comparison of the performance of CLOOME and MolPhenix models on unseen molecules.  The top section shows the results for a matched number of seen samples, highlighting the 8.1x improvement achieved by MolPhenix over CLOOME for active molecules. The bottom section displays the state-of-the-art results obtained by training with a larger number of steps and optimal hyperparameters, demonstrating MolPhenix's consistent superior performance.  The table also notes the reliance of MolPhenix's key components (S2L loss and embedding averaging) on a pre-trained uni-modal phenomics model.", "section": "Experimental Setup"}, {"figure_path": "LQBlSGeOGm/tables/tables_31_2.jpg", "caption": "Table 1: Impact of pre-trained Phenom1 and MolGPS on CLOOME and MolPhenix for a matched number of seen samples (Top), where we observe an 8.1 \u00d7 improvement of MolPhenix over the CLOOME baseline for active unseen molecules. SOTA results trained with a higher number of steps by utilizing the best hyperparameters (Bottom *). We note that MolPhenix's main components such as S2L and embedding averaging relies on having a pre-trained uni-modal phenomics model.", "description": "This table presents a comparison of the performance of CLOOME and MolPhenix models on unseen data, specifically focusing on the top 1% and top 5% retrieval accuracy for both active and all molecules. The results highlight the significant performance improvement achieved by MolPhenix (8.1x improvement over CLOOME for active molecules) through the utilization of a pre-trained uni-modal phenomics model and other methodological improvements.", "section": "4 Experimental Setup"}, {"figure_path": "LQBlSGeOGm/tables/tables_32_1.jpg", "caption": "Table 1: Impact of pre-trained Phenom1 and MolGPS on CLOOME and MolPhenix for a matched number of seen samples (Top), where we observe an 8.1 \u00d7 improvement of MolPhenix over the CLOOME baseline for active unseen molecules. SOTA results trained with a higher number of steps by utilizing the best hyperparameters (Bottom *). We note that MolPhenix's main components such as S2L and embedding averaging relies on having a pre-trained uni-modal phenomics model.", "description": "This table compares the performance of CLOOME and MolPhenix models on different tasks, using various modalities (images, Phenom1, and MolGPS). The top part shows the results for a matched number of seen samples, highlighting the 8.1x improvement of MolPhenix over CLOOME for active unseen molecules. The bottom part shows the state-of-the-art results, obtained by training with more steps and using optimal hyperparameters.  The table emphasizes that MolPhenix's performance gain relies on using a pre-trained uni-modal phenomics model (Phenom1) and incorporating techniques like S2L loss and embedding averaging.", "section": "Experimental Setup"}, {"figure_path": "LQBlSGeOGm/tables/tables_32_2.jpg", "caption": "Table 1: Impact of pre-trained Phenom1 and MolGPS on CLOOME and MolPhenix for a matched number of seen samples (Top), where we observe an 8.1 \u00d7 improvement of MolPhenix over the CLOOME baseline for active unseen molecules. SOTA results trained with a higher number of steps by utilizing the best hyperparameters (Bottom *). We note that MolPhenix's main components such as S2L and embedding averaging relies on having a pre-trained uni-modal phenomics model.", "description": "This table compares the performance of CLOOME and MolPhenix models on various tasks, using different modalities (images, Phenom1, and MolGPS). The top section shows results with a matched number of seen samples, highlighting the significant improvement of MolPhenix (8.1x) over CLOOME for active, unseen molecules. The bottom section presents state-of-the-art (SOTA) results achieved with more training steps and optimal hyperparameters.  It emphasizes that MolPhenix's key improvements (S2L loss and embedding averaging) rely on using a pre-trained uni-modal phenomics model.", "section": "Results and Discussion"}, {"figure_path": "LQBlSGeOGm/tables/tables_32_3.jpg", "caption": "Table 1: Impact of pre-trained Phenom1 and MolGPS on CLOOME and MolPhenix for a matched number of seen samples (Top), where we observe an 8.1 \u00d7 improvement of MolPhenix over the CLOOME baseline for active unseen molecules. SOTA results trained with a higher number of steps by utilizing the best hyperparameters (Bottom *). We note that MolPhenix's main components such as S2L and embedding averaging relies on having a pre-trained uni-modal phenomics model.", "description": "This table compares the performance of CLOOME and MolPhenix models on active and all molecules in different scenarios (unseen images, unseen images+molecules, unseen datasets).  The top part shows the results for the matched number of seen samples, highlighting an 8.1x improvement of MolPhenix over CLOOME on active, unseen molecules. The bottom part displays SOTA results using optimized hyperparameters and a higher number of training steps, emphasizing the importance of MolPhenix's components (S2L loss and embedding averaging) and the pretrained phenomics model.", "section": "4 Experimental Setup"}, {"figure_path": "LQBlSGeOGm/tables/tables_33_1.jpg", "caption": "Table 1: Impact of pre-trained Phenom1 and MolGPS on CLOOME and MolPhenix for a matched number of seen samples (Top), where we observe an 8.1 \u00d7 improvement of MolPhenix over the CLOOME baseline for active unseen molecules. SOTA results trained with a higher number of steps by utilizing the best hyperparameters (Bottom *). We note that MolPhenix's main components such as S2L and embedding averaging relies on having a pre-trained uni-modal phenomics model.", "description": "This table compares the performance of CLOOME and MolPhenix models on three different unseen datasets, showcasing MolPhenix's significant improvement in zero-shot molecular retrieval. The top section shows results for a matched number of seen samples, highlighting MolPhenix's 8.1x improvement over CLOOME for active unseen molecules. The bottom section presents state-of-the-art results achieved by training with a higher number of steps and optimal hyperparameters, further emphasizing MolPhenix's superior performance.", "section": "4 Experimental Setup"}, {"figure_path": "LQBlSGeOGm/tables/tables_34_1.jpg", "caption": "Table 17: Ablation across different p-value cutoff thresholds. p values < .1 benefit retrieval of active molecules.", "description": "This ablation study investigates the impact of different p-value cutoffs on the retrieval performance of active molecules. The results show that using a p-value cutoff of less than 0.1 improves the retrieval performance of active molecules across different datasets.", "section": "E.5 Ablation Studies"}, {"figure_path": "LQBlSGeOGm/tables/tables_34_2.jpg", "caption": "Table 1: Impact of pre-trained Phenom1 and MolGPS on CLOOME and MolPhenix for a matched number of seen samples (Top), where we observe an 8.1 \u00d7 improvement of MolPhenix over the CLOOME baseline for active unseen molecules. SOTA results trained with a higher number of steps by utilizing the best hyperparameters (Bottom *). We note that MolPhenix's main components such as S2L and embedding averaging relies on having a pre-trained uni-modal phenomics model.", "description": "This table compares the performance of CLOOME and MolPhenix models on different tasks, highlighting the impact of using pre-trained Phenom1 and MolGPS models. The top section shows results for a matched number of seen samples, demonstrating MolPhenix's significant improvement over CLOOME, especially for active unseen molecules.  The bottom section presents state-of-the-art (SOTA) results achieved with more training steps and optimal hyperparameters, further showcasing MolPhenix's superior performance. The table emphasizes the importance of using a pre-trained uni-modal phenomics model and components like S2L and embedding averaging for MolPhenix's success.", "section": "Experimental Setup"}, {"figure_path": "LQBlSGeOGm/tables/tables_34_3.jpg", "caption": "Table 1: Impact of pre-trained Phenom1 and MolGPS on CLOOME and MolPhenix for a matched number of seen samples (Top), where we observe an 8.1 \u00d7 improvement of MolPhenix over the CLOOME baseline for active unseen molecules. SOTA results trained with a higher number of steps by utilizing the best hyperparameters (Bottom *). We note that MolPhenix's main components such as S2L and embedding averaging relies on having a pre-trained uni-modal phenomics model.", "description": "This table compares the performance of CLOOME and MolPhenix models on active and all molecules using different modalities (images, Phenom1, MolGPS).  It highlights the significant improvement in zero-shot retrieval accuracy achieved by MolPhenix (8.1x improvement over CLOOME for active unseen molecules). It also shows that MolPhenix's performance is highly dependent on the use of a pre-trained phenomics model and the proposed guidelines (uni-modal pretrained models, S2L loss, undersampling of inactive molecules, and encoding concentration).", "section": "Experimental Setup"}, {"figure_path": "LQBlSGeOGm/tables/tables_34_4.jpg", "caption": "Table 1: Impact of pre-trained Phenom1 and MolGPS on CLOOME and MolPhenix for a matched number of seen samples (Top), where we observe an 8.1 \u00d7 improvement of MolPhenix over the CLOOME baseline for active unseen molecules. SOTA results trained with a higher number of steps by utilizing the best hyperparameters (Bottom *). We note that MolPhenix's main components such as S2L and embedding averaging relies on having a pre-trained uni-modal phenomics model.", "description": "This table compares the performance of different models (CLOOME and MolPhenix) with various configurations (using different modalities like images, Phenom1, and MolGPS). It demonstrates a significant improvement in zero-shot molecular retrieval accuracy achieved by MolPhenix, especially for active unseen molecules, compared to the previous state-of-the-art (CLOOME).  The table highlights the impact of using pre-trained uni-modal models and the novel techniques implemented in MolPhenix (like S2L loss and embedding averaging).  The results are presented for both active and all molecules, and show that MolPhenix's performance is consistent across various scenarios.", "section": "4 Experimental Setup"}, {"figure_path": "LQBlSGeOGm/tables/tables_34_5.jpg", "caption": "Table 1: Impact of pre-trained Phenom1 and MolGPS on CLOOME and MolPhenix for a matched number of seen samples (Top), where we observe an 8.1 \u00d7 improvement of MolPhenix over the CLOOME baseline for active unseen molecules. SOTA results trained with a higher number of steps by utilizing the best hyperparameters (Bottom *). We note that MolPhenix's main components such as S2L and embedding averaging relies on having a pre-trained uni-modal phenomics model.", "description": "This table compares the performance of different models (CLOOME and MolPhenix) in a molecular retrieval task, considering different modalities for input (images and fingerprints) and the usage of pre-trained models for phenomic and molecular information. The top section shows results for a matched number of samples, highlighting the 8.1x improvement of MolPhenix over CLOOME for active unseen molecules. The bottom section presents state-of-the-art (SOTA) results achieved with a higher number of training steps and optimal hyperparameters.  The table emphasizes that MolPhenix's superior performance depends on its use of a pre-trained uni-modal phenomics model.", "section": "4 Experimental Setup"}, {"figure_path": "LQBlSGeOGm/tables/tables_34_6.jpg", "caption": "Table 1: Impact of pre-trained Phenom1 and MolGPS on CLOOME and MolPhenix for a matched number of seen samples (Top), where we observe an 8.1 \u00d7 improvement of MolPhenix over the CLOOME baseline for active unseen molecules. SOTA results trained with a higher number of steps by utilizing the best hyperparameters (Bottom *). We note that MolPhenix's main components such as S2L and embedding averaging relies on having a pre-trained uni-modal phenomics model.", "description": "This table presents a comparison of the performance of CLOOME and MolPhenix models, with and without pre-trained models (Phenom1 and MolGPS), on various metrics (top-1% and top-5% recall accuracy for active and all molecules on unseen images, unseen images+molecules, and unseen datasets).  The top section shows results with a matched number of seen samples, highlighting the significant improvement achieved by MolPhenix. The bottom section shows the state-of-the-art (SOTA) results obtained by training with more steps using optimal hyperparameters. The table emphasizes the contribution of MolPhenix's key components, like S2L loss and embedding averaging, enabled by the pre-trained uni-modal phenomics model.", "section": "4 Experimental Setup"}]