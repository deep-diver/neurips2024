[{"figure_path": "ORQiboaRqY/figures/figures_5_1.jpg", "caption": "Figure 1: The architectures of a single layer in packing (left) and covering (right) GD-Nets. Learnable parameters are colored in red.", "description": "This figure shows the architecture of a single layer in both packing and covering GD-Nets. The left side illustrates the packing GD-Net, and the right side shows the covering GD-Net.  Both architectures consist of several blocks including matrix multiplication with A or A transpose, addition of bias terms, ELU activation functions, learnable functions (f\u03b8k and g\u03b8k) represented by neural networks, ReLU activation, and matrix multiplication with learnable weight matrices (Wk). The learnable parameters are highlighted in red, indicating the parts of the network that are trained during the learning process.  The diagrams depict the flow of information through the network, showing how the input features (Xk and Yk) are processed to generate the output features (Xk+1 and Yk+1) in a single layer. Each layer represents one iteration of the gradient descent algorithm that the GD-Net simulates.", "section": "3 Design of packing GD-Net"}]