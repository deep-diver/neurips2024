[{"figure_path": "XXVfj4P8nr/tables/tables_6_1.jpg", "caption": "Table 1: Comparison of object detection and segmentation results on LVIS minival. 'Open-Ended' denotes that we do not have exact object categories during inference [26]. We report fixed AP [9] for rare objects. * denotes using the external data.", "description": "The table compares the performance of various object detection and segmentation methods on the LVIS minival dataset.  It breaks down methods into three categories: Close-Set, Open-Set, and Open-Ended, highlighting the difference in their requirements for object category information during inference.  The results are presented in terms of box APrare and mask APrare (Average Precision for rare objects) to focus on the model's ability to handle less frequently seen classes.  The table also notes which methods used external data during training.", "section": "4.2 Main Results"}, {"figure_path": "XXVfj4P8nr/tables/tables_7_1.jpg", "caption": "Table 2: Comparison of object detection results on CODA. We chose the best performance for * results from CODA. \u2020 denotes few-shot object detectors in the one-shot setting. \u2018Oracle\u2019 represents utilizing ground-truth boxes as the box prompt for SAM.", "description": "This table compares the performance of various object detection methods on the CODA dataset, focusing on the mAR (mean Average Recall), AR50 (average recall at 50% IoU), and AR75 (average recall at 75% IoU) metrics.  It categorizes methods as Close-Set, Open-Set, and Open-Ended, highlighting the impact of utilizing VLMs and whether or not training is involved.  The 'Oracle' row provides an upper bound performance using ground truth information.", "section": "4.2 Main Results"}, {"figure_path": "XXVfj4P8nr/tables/tables_7_2.jpg", "caption": "Table 1: Comparison of object detection and segmentation results on LVIS minival. 'Open-Ended' denotes that we do not have exact object categories during inference [26]. We report fixed AP [9] for rare objects. * denotes using the external data.", "description": "This table compares the performance of various object detection and segmentation methods on the LVIS minival dataset.  It categorizes methods into three types: Close-Set, Open-Set, and Open-Ended, based on whether predefined object categories are needed during inference. The table shows the box APrare and mask APrare for each method, highlighting the performance on rare object categories. The use of external data in certain methods is also indicated.", "section": "4.2 Main Results"}, {"figure_path": "XXVfj4P8nr/tables/tables_8_1.jpg", "caption": "Table 4: Ablation of attention generation. We can obtain high-quality attention maps with the proposed modules.", "description": "This table presents the ablation study results for the attention generation module in the VL-SAM framework. It shows the impact of different components on the model's performance, measured by mean Average Recall (mAR) on the CODA dataset. The components include: Naive Attention Map, Attention Flow (with and without regularization), and Head Weight. The results demonstrate that using the proposed modules significantly improves the quality of generated attention maps.", "section": "4.3 Ablation Study"}, {"figure_path": "XXVfj4P8nr/tables/tables_8_2.jpg", "caption": "Table 5: Ablation of model generalization. VL-SAM can adopt various vision-language models and segmentation models.", "description": "This table shows the results of using different vision-language models (CogVLM, MiniGPT-4, LLaVA) and segmentation models (SAM, MobileSAM) in the VL-SAM framework.  It demonstrates the model's generalization ability by showcasing performance variations with different combinations of these models. The mAR (mean Average Recall) metric is used to evaluate the object detection performance. ", "section": "4.3 Ablation Study"}]