{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020", "reason": "This paper introduces the foundational concept of few-shot learning in large language models, which is highly relevant to the training-free framework proposed in the current paper."}, {"fullname_first_author": "Alexander Kirillov", "paper_title": "Segment anything", "publication_date": "2023", "reason": "This paper introduces the Segment Anything Model (SAM), a key component of the proposed framework, enabling generalized object localization."}, {"fullname_first_author": "Junnan Li", "paper_title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "publication_date": "2023", "reason": "This paper introduces the BLIP-2 model, a vision-language model that is incorporated into the framework for generalized object recognition."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021", "reason": "This paper introduces the CLIP model, which is referenced in the context of open-set methods and is related to the approach of combining vision and language models."}, {"fullname_first_author": "Chuang Lin", "paper_title": "Generative region-language pretraining for open-ended object detection", "publication_date": "2024", "reason": "This paper introduces the GenerateU method, which is compared to the proposed method in terms of performance and training requirements."}]}