[{"figure_path": "x2zY4hZcmg/figures/figures_3_1.jpg", "caption": "Figure 1: Overview of an execution cycle in MPS (0, 2) and DMPS (1, 2, 3, 4).", "description": "This figure shows the control flow of both MPS and DMPS.  In MPS, if the learned policy suggests an action that would lead to an unsafe state, the backup policy is used.  DMPS enhances this by using a local planner to find a better recovery action before resorting to the backup policy. The numbered steps highlight the differences: 1. Initial action from the learned policy; 2. Planner determines safe action; 3. The planner's suggested safe action is used instead of the backup policy; 4. The backup policy is only used if the planner fails to find a safe and optimal plan.", "section": "4 Model Predictive Shielding"}, {"figure_path": "x2zY4hZcmg/figures/figures_3_2.jpg", "caption": "Figure 2: (a) Unsafe trajectory leading to a collision. (b) Safe but sub-optimal trajectory. (c) Optimal and safe trajectory. (d) An instance of the planning phase.", "description": "This figure shows four different trajectories for an agent trying to reach a goal while avoiding static obstacles. (a) shows an unsafe trajectory where the agent collides with an obstacle. (b) shows a safe trajectory generated by MPS, but it is suboptimal because the agent halts instead of finding a better path around the obstacle. (c) shows an optimal and safe trajectory, which is what DMPS aims to achieve. (d) illustrates the planning phase of DMPS, where the planner searches for a safe and optimal path to reach the goal.", "section": "4 Model Predictive Shielding"}, {"figure_path": "x2zY4hZcmg/figures/figures_8_1.jpg", "caption": "Figure 5: Example trajectories in double-gate+.", "description": "This figure shows example trajectories of DMPS and MPS agents in the double-gate+ environment.  Panel (a) displays trajectories during early training, illustrating that DMPS (green) navigates more effectively through the gates than MPS (blue).  A failed DMPS attempt (red) is also shown. Panel (b) shows trajectories in a later stage of training, demonstrating that while DMPS can successfully navigate the obstacles, MPS still struggles to pass through even one gate.", "section": "6.3 Analysis"}, {"figure_path": "x2zY4hZcmg/figures/figures_19_1.jpg", "caption": "Figure 6: Visualization of the dynamic environments. The agent is depicted as a red circle. The direction of rotation of the walls is shown with red arrows. The goal position is shown with *.", "description": "This figure visualizes three dynamic environments used in the experiments: single-gate, double-gates, and double-gates+.  Each environment features a goal (black star) and an agent (red circle) that must navigate through one or more concentric rotating walls to reach the goal. The direction of rotation for the walls is indicated by red arrows.  The environments increase in complexity, starting with a single rotating wall (single-gate), then two concentric walls (double-gates), and finally two concentric walls with increased thickness (double-gates+), making navigation more challenging.", "section": "A.3.1 Benchmarks"}, {"figure_path": "x2zY4hZcmg/figures/figures_21_1.jpg", "caption": "Figure 7: Planner Computation Scaling", "description": "This figure shows how the computation time required for planning scales with the planning horizon.  It plots the number of node expansions performed by the MCTS planner (y-axis, log scale) against the planning horizon (x-axis).  The exponential relationship demonstrates the increased computational cost of planning as the lookahead increases.", "section": "A.4 Analysis of Computational Overhead"}, {"figure_path": "x2zY4hZcmg/figures/figures_21_2.jpg", "caption": "Figure 8: Multiple Horizons Experiments", "description": "The plots show the episodic return and number of shield invocations over time for the double-gates+ environment (double integrator dynamics) when using planning horizons of 1, 5, and 9.  The shaded regions represent the standard deviation over five random seeds. The results indicate that longer planning horizons lead to better performance, achieving higher rewards and fewer shield invocations. However, there is a diminishing return in performance as the horizon length increases, suggesting a trade-off between the benefits of long-horizon planning and the associated computational cost.", "section": "A.5 Effect of Planning Horizon"}]