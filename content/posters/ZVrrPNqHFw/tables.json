[{"figure_path": "ZVrrPNqHFw/tables/tables_7_1.jpg", "caption": "Table 1: The average and the standard error over three runs. Ours indicates our method applied to a model initially trained with the prefix method. The best accuracy is annotated in bold. \u2713 indicates that a given method uses bias information while X denotes that a given model does not use any bias information.", "description": "This table presents the average accuracy and standard error for various methods on three datasets (CMNIST, CIFAR10C, and BFFHQ) with different bias ratios.  The \"Bias Info\" column indicates whether the method utilizes bias information (\u2713) or not (X). The best accuracy for each setting is shown in bold.  The \"Ours\" rows show the results when the proposed method is applied on top of other methods.", "section": "5 Experiments"}, {"figure_path": "ZVrrPNqHFw/tables/tables_8_1.jpg", "caption": "Table 1: The average and the standard error over three runs. Ours indicates our method applied to a model initially trained with the prefix method. The best accuracy is annotated in bold. \u2713 indicates that a given method uses bias information while X denotes that a given model does not use any bias information.", "description": "This table presents the average accuracy and standard error over three runs for various debiasing methods on four different datasets (CMNIST, CIFAR10C, BFFHQ, Waterbird) with different bias ratios.  The \"Ours\" methods represent the authors' proposed method applied after each of the other methods (ERM, LfF, DFA, SelecMix). The table also indicates whether each method uses bias information or not.", "section": "5 Experiments"}, {"figure_path": "ZVrrPNqHFw/tables/tables_17_1.jpg", "caption": "Table 1: The average and the standard error over three runs. Ours indicates our method applied to a model initially trained with the prefix method. The best accuracy is annotated in bold. \u2713 indicates that a given method uses bias information while X denotes that a given model does not use any bias information.", "description": "This table presents the average accuracy and standard error achieved by various methods on different datasets (CMNIST, CIFAR10C, BFFHQ) with varying levels of bias (0.5%, 1%, 2%, 5%).  The methods compared include ERM (Empirical Risk Minimization), several debiasing methods (GroupDRO, LfF, DFA, SelecMix), and the proposed method ('Ours') applied to models pre-trained with different methods. The table shows how the proposed method improves the accuracy of other debiasing methods, especially at higher bias levels, and its competitiveness to ERM, a baseline approach that does not explicitly attempt to remove bias.", "section": "5 Experiments"}, {"figure_path": "ZVrrPNqHFw/tables/tables_17_2.jpg", "caption": "Table 4: Comparison of bias-conflicting sample detection precisions between self-influence (SI), and bias-conditioned self-influence (BCSI) across various datasets. The average and the standard error of precision over three runs are provided.", "description": "This table presents the performance comparison of two methods in detecting bias-conflicting samples across various datasets.  Self-Influence (SI) and Bias-Conditioned Self-Influence (BCSI) are compared. The precision, which is the ratio of correctly identified bias-conflicting samples to the total number of bias-conflicting samples, is calculated for each dataset and bias ratio. The average precision and its standard error across three runs are reported for a more robust assessment of each method's performance.", "section": "C Detection precision for other datasets"}, {"figure_path": "ZVrrPNqHFw/tables/tables_18_1.jpg", "caption": "Table 1: The average and the standard error over three runs. Ours indicates our method applied to a model initially trained with the prefix method. The best accuracy is annotated in bold. \u2713 indicates that a given method uses bias information while X denotes that a given model does not use any bias information.", "description": "This table shows the average accuracy and standard error for different bias-handling methods on four benchmark datasets (CMNIST, CIFAR10C, BFFHQ, Waterbird).  The results are shown for various bias ratios (0.5%, 1%, 2%, 5%).  The \"Ours\" methods represent the proposed method applied after other baseline methods. The presence or absence of bias information in each method's training is also indicated.  Bold values represent the highest achieved accuracy in each setting.", "section": "5 Experiments"}, {"figure_path": "ZVrrPNqHFw/tables/tables_18_2.jpg", "caption": "Table 2: The average and the standard error over three runs on low-bias scenarios.", "description": "This table presents the average accuracy and standard error, across three runs, for various methods on low-bias scenarios using different datasets.  The low-bias scenarios are defined by the bias-conflicting ratio in the datasets (ranging from 20% to 90% for CIFAR10C). The methods compared include ERM (Empirical Risk Minimization), LfF (Learning from Failure), DFA (Disentangled Feature Augmentation), SelecMix, and the proposed method combined with each of these baselines (Ours ERM, Ours LfF, Ours DFA, Ours SelecMix). The table aims to demonstrate the performance of the proposed method in low-bias settings, where existing debiasing methods may underperform.", "section": "5.3 Results on low-bias scenarios"}, {"figure_path": "ZVrrPNqHFw/tables/tables_19_1.jpg", "caption": "Table 7: The average and the worst-group accuracy on NLP datasets.", "description": "This table presents the average and worst-group accuracy results on two NLP datasets, MultiNLI and CivilComments, for different methods: ERM, JTT, and the proposed method combined with JTT.  It demonstrates the effectiveness of the proposed method in improving the worst-group accuracy, particularly when used in conjunction with the JTT method.  The \"Avg.\" column represents the average accuracy across all groups, while \"Worst-group\" represents the accuracy on the group with the lowest performance.", "section": "5 Experiments"}, {"figure_path": "ZVrrPNqHFw/tables/tables_19_2.jpg", "caption": "Table 1: The average and the standard error over three runs. Ours indicates our method applied to a model initially trained with the prefix method. The best accuracy is annotated in bold. \u2713 indicates that a given method uses bias information while X denotes that a given model does not use any bias information.", "description": "This table presents the average accuracy and standard error achieved by various methods on benchmark datasets with different bias levels.  The methods are categorized as either using bias information or not.  The 'Ours' method represents the proposed approach applied to models pre-trained with other methods. The best accuracy for each setting is highlighted in bold.", "section": "5 Experiments"}, {"figure_path": "ZVrrPNqHFw/tables/tables_19_3.jpg", "caption": "Table 1: The average and the standard error over three runs. Ours indicates our method applied to a model initially trained with the prefix method. The best accuracy is annotated in bold. \u2713 indicates that a given method uses bias information while X denotes that a given model does not use any bias information.", "description": "This table presents the average accuracy and standard error obtained by various methods (ERM, GroupDRO, LfF, DFA, BPA, DCWP, SelecMix, and Ours) on four benchmark datasets (CMNIST, CIFAR10C, BFFHQ, and Waterbirds) with different bias ratios.  The \"Ours\" column shows the results of the proposed method, both when applied to models trained with ERM and when applied to models already debiased by other methods. The best accuracy for each setting is highlighted in bold. A checkmark (\u2713) indicates that a method explicitly used bias information during training, while an \"X\" denotes that it did not.", "section": "5 Experiments"}, {"figure_path": "ZVrrPNqHFw/tables/tables_20_1.jpg", "caption": "Table 10: The average and the standard error of the number of pivotal sets over three runs considering numbers of intersections.", "description": "This table shows the average and standard error of the number of samples selected for the pivotal set across three different runs for varying numbers of intersections.  The data is broken down by the percentage of bias-conflicting samples in the CIFAR10C dataset (0.5%, 1%, 2%, 5%, 20%, 30%, 50%, 70%).  It demonstrates the effect of using multiple random model initializations on the size of the pivotal set.", "section": "5.4 Ablation study"}, {"figure_path": "ZVrrPNqHFw/tables/tables_20_2.jpg", "caption": "Table 1: The average and the standard error over three runs. Ours indicates our method applied to a model initially trained with the prefix method. The best accuracy is annotated in bold. \u2713 indicates that a given method uses bias information while X denotes that a given model does not use any bias information.", "description": "This table compares the performance of different debiasing methods on four benchmark datasets (CMNIST, CIFAR10C, BFFHQ, Waterbird) with varying bias levels (0.5%, 1%, 2%, 5%).  The methods include ERM (Empirical Risk Minimization), LfF (Learning from Failure), DFA (Disentangled Feature Augmentation), BPA (Bias-aware Pseudo-attribute), SelecMix, and the proposed method ('Ours'). The table shows the average accuracy and standard error for each method across three runs.  The 'Ours' results represent the proposed approach used alone and in conjunction with other methods. The 'Info' column indicates whether a method uses bias information (\u2713) or not (X).  The bold values highlight the best accuracy for each setting.", "section": "Experiments"}, {"figure_path": "ZVrrPNqHFw/tables/tables_20_3.jpg", "caption": "Table 1: The average and the standard error over three runs. Ours indicates our method applied to a model initially trained with the prefix method. The best accuracy is annotated in bold. \u2713 indicates that a given method uses bias information while X denotes that a given model does not use any bias information.", "description": "This table compares the performance of different bias mitigation methods across four datasets (CMNIST, CIFAR10C, BFFHQ, Waterbirds) with varying bias ratios (0.5%, 1%, 2%, 5%).  The methods include ERM (Empirical Risk Minimization), GroupDRO, LfF, DFA, BPA, DCWP, SelecMix, and the proposed method (Ours), applied both independently and in combination with other methods.  The table shows the average accuracy and standard error for each method and dataset.  The \"Info\" column indicates whether each method utilizes bias information (\u2713) or not (X). Bold values represent the best-performing method for each combination.", "section": "5 Experiments"}, {"figure_path": "ZVrrPNqHFw/tables/tables_21_1.jpg", "caption": "Table 1: The average and the standard error over three runs. Ours indicates our method applied to a model initially trained with the prefix method. The best accuracy is annotated in bold. \u2713 indicates that a given method uses bias information while X denotes that a given model does not use any bias information.", "description": "This table presents the average accuracy and standard error over three runs for different bias mitigation methods on four benchmark datasets (CMNIST, CIFAR10C, BFFHQ, Waterbird).  The results are categorized by bias ratio (0.5%, 1%, 2%, 5% for CMNIST and CIFAR10C; 0.5% for BFFHQ;  0.5%, 1%, 2%, 5% for Waterbird).  The 'Ours' methods indicate the results when the proposed method is applied to models already pre-trained with other methods.  The bold values represent the best performance for each setting.  The \u2713 and X symbols denote whether bias information was used by the method or not.", "section": "5 Experiments"}, {"figure_path": "ZVrrPNqHFw/tables/tables_22_1.jpg", "caption": "Table 1: The average and the standard error over three runs. Ours indicates our method applied to a model initially trained with the prefix method. The best accuracy is annotated in bold. \u2713 indicates that a given method uses bias information while X denotes that a given model does not use any bias information.", "description": "This table presents the average accuracy and standard error over three runs for different bias detection methods on four datasets: CMNIST, CIFAR10C, BFFHQ, and Waterbirds.  The methods compared include ERM, LfF, DFA, SelecMix, and the proposed method (Ours). The table shows the performance of the methods on datasets with varying levels of bias (0.5%, 1%, 2%, 5%). The best accuracy for each setting is highlighted in bold.  A checkmark indicates whether a method used bias information during training; an 'X' indicates it did not.", "section": "5 Experiments"}, {"figure_path": "ZVrrPNqHFw/tables/tables_22_2.jpg", "caption": "Table 1: The average and the standard error over three runs. Ours indicates our method applied to a model initially trained with the prefix method. The best accuracy is annotated in bold. \u2713 indicates that a given method uses bias information while X denotes that a given model does not use any bias information.", "description": "This table presents the average accuracy and standard error for different bias mitigation methods across various datasets (CMNIST, CIFAR10C, BFFHQ) and bias levels (0.5%, 1%, 2%, 5%).  It compares the performance of several existing methods (ERM, LfF, DFA, SelecMix) with and without the proposed method, denoted as \"Ours\". The 'Info' column indicates whether the method utilizes bias information or not. Bold values highlight the best-performing method for each dataset and bias level.", "section": "5 Experiments"}, {"figure_path": "ZVrrPNqHFw/tables/tables_23_1.jpg", "caption": "Table 1: The average and the standard error over three runs. Ours indicates our method applied to a model initially trained with the prefix method. The best accuracy is annotated in bold. \u2713 indicates that a given method uses bias information while X denotes that a given model does not use any bias information.", "description": "This table presents the average accuracy and standard error, over three runs, for different bias mitigation methods across four datasets (CMNIST, CIFAR10C, BFFHQ, Waterbirds) and varying bias ratios (0.5%, 1%, 2%, 5%).  The \"Ours\" methods show the performance of the proposed method when applied to models pre-trained with other methods (ERM, LfF, DFA, SelecMix). The best accuracy for each condition is highlighted in bold.  The table also indicates whether each method utilizes bias information (\u2713) or not (X).", "section": "5 Experiments"}]