{"references": [{"fullname_first_author": "Agarwal, A.", "paper_title": "On the theory of policy gradient methods: Optimality, approximation, and distribution shift", "publication_date": "2021-MM-DD", "reason": "This paper provides a comprehensive theoretical foundation for policy gradient methods, which is crucial for understanding and improving the proposed PD-ANPG algorithm."}, {"fullname_first_author": "Bai, Q.", "paper_title": "Achieving zero constraint violation for constrained reinforcement learning via conservative natural policy gradient primal-dual algorithm", "publication_date": "2023-MM-DD", "reason": "This paper is directly related to the work presented, focusing on constrained reinforcement learning with general parameterization, making it a key comparative reference."}, {"fullname_first_author": "Ding, D.", "paper_title": "Natural policy gradient primal-dual method for constrained Markov decision processes", "publication_date": "2020-MM-DD", "reason": "This work introduces a primal-dual natural policy gradient approach for constrained MDPs, which directly informs the algorithm design in this paper."}, {"fullname_first_author": "Liu, Y.", "paper_title": "An improved analysis of (variance-reduced) policy gradient and natural policy gradient methods", "publication_date": "2020-MM-DD", "reason": "This paper offers crucial insights into the convergence properties of policy gradient methods, directly relevant to analyzing the convergence of PD-ANPG."}, {"fullname_first_author": "Vaswani, S.", "paper_title": "Near-optimal sample complexity bounds for constrained mdps", "publication_date": "2022-MM-DD", "reason": "This paper establishes theoretical lower bounds for sample complexity in constrained MDPs, providing a benchmark against which to measure the efficiency of the PD-ANPG algorithm."}]}