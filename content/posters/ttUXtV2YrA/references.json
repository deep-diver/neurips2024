{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, which is foundational to many vision transformer models and is directly relevant to the core idea of using MHSA in the proposed GLMix model."}, {"fullname_first_author": "Ze Liu", "paper_title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows", "publication_date": "2021-10-27", "reason": "The Swin Transformer is a highly influential vision transformer model, and the GLMix model builds upon its architecture and design choices for improved efficiency and performance."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-01-01", "reason": "This work was among the first to demonstrate the effectiveness of transformers in image recognition tasks, inspiring the use of MHSA in vision backbones and influencing the GLMix model's design."}, {"fullname_first_author": "Tsung-Yi Lin", "paper_title": "Feature Pyramid Networks for Object Detection", "publication_date": "2017-07-01", "reason": "The Feature Pyramid Network (FPN) is a crucial component of many object detection models, and the GLMix model incorporates the idea of FPN for feature fusion, leveraging the strengths of both local and global features."}, {"fullname_first_author": "Kaiming He", "paper_title": "Mask R-CNN", "publication_date": "2017-10-01", "reason": "Mask R-CNN is a highly influential instance segmentation model.  The paper uses Mask R-CNN as a framework for evaluating the proposed GLNet model on the COCO dataset for instance segmentation."}]}