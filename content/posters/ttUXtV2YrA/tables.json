[{"figure_path": "ttUXtV2YrA/tables/tables_4_1.jpg", "caption": "Table 1: System-to-system comparison with existing works under the Swin-Tiny-Layout protocol [70]. \u2020: implemented by us with modules provided by timm [54].", "description": "This table compares the proposed GLNet-STL model with other state-of-the-art models using the Swin-Tiny-Layout protocol.  The metrics include the number of parameters (Params), floating point operations (FLOPs), throughput (Throu.), and top-1 accuracy on the ImageNet-1K dataset (IN1K Top-1).  It highlights that GLNet-STL achieves the best top-1 accuracy (82.5%) with a similar parameter count and FLOPs, but a significantly higher throughput.", "section": "3 Methodology"}, {"figure_path": "ttUXtV2YrA/tables/tables_4_2.jpg", "caption": "Table 2: Model configurations of the GLNet family. C: base channels (i.e., feature channels of the first stage). e: FFN expansion ratio. #Blocks: the 4-stage block numbers. FLOPs are measured at 224 \u00d7 224 resolution.", "description": "This table details the configurations of the GLNet model family, showing the base channels, FFN expansion ratio, number of blocks in each of the four stages, whether advanced designs were used, and the FLOPs (floating point operations) at a 224x224 resolution.  It provides a quantitative comparison of the different GLNet models.", "section": "3.3 GLNet"}, {"figure_path": "ttUXtV2YrA/tables/tables_5_1.jpg", "caption": "Table 3: Comparison with different state-of-the-art backbones on ImageNet-1K classification. All models are trained and evaluated on 224 \u00d7 224 input resolution. Top-1 refers to top-1 accuracy (%). We compare models trained with a standard supervised recipe and those trained with an advanced distillation recipe [26].", "description": "This table compares the performance of the proposed GLNet model against other state-of-the-art models on the ImageNet-1K classification task.  It shows the FLOPs, number of parameters, and top-1 accuracy for each model under both standard supervised training and advanced distillation training.", "section": "4 Experiments"}, {"figure_path": "ttUXtV2YrA/tables/tables_6_1.jpg", "caption": "Table 4: Results on the COCO 2017 dataset using the RetinaNet [33] framework for object detection, and Mask R-CNN [21] framework for instance segmentation. 1\u00d7 refers to 12 epochs, and 3\u00d7 refers to 36 epochs. MS means multi-scale training. mAPb and mAPm denote box mAP and mask mAP, respectively. FLOPs are measured at 800 \u00d7 1280 resolution.", "description": "This table presents the results of object detection and instance segmentation experiments on the COCO 2017 dataset.  Two models, RetinaNet and Mask R-CNN, were used with different training schedules (1x and 3x epochs with multi-scale training).  The table compares the performance of several backbones (including the proposed GLNet) across different metrics: mAP (mean Average Precision), AP50, AP75, APS (small object AP), APM (medium object AP), APL (large object AP), and FLOPs (floating point operations).  The FLOPs are calculated at a resolution of 800x1280 pixels.", "section": "4.2 Object Detection and Instance Segmentation"}, {"figure_path": "ttUXtV2YrA/tables/tables_7_1.jpg", "caption": "Table 5: Performance comparison of different backbones on the ADE20K segmentation task. We report mIoU with no test-time augmentation. FLOPs are computed at 512 \u00d7 2048 resolution.", "description": "This table compares the performance of various backbones on the ADE20K semantic segmentation dataset.  The metrics shown are the number of parameters, FLOPs (floating point operations), and mean Intersection over Union (mIoU).  The results are presented for two different semantic segmentation frameworks, Semantic FPN and UperNet, each trained for a different number of iterations (80k and 160k, respectively).  The table allows for a comparison of model efficiency and accuracy across different architectures.", "section": "4.3 Semantic Segmentation on ADE20K"}, {"figure_path": "ttUXtV2YrA/tables/tables_9_1.jpg", "caption": "Table 6: Ablation study on the GLMix design choices. We investigate the effect of (a) local-global collaboration, (b) the clustering strategy, (c) convolution kernel size of the local branch, and (d) number of slots in the global branch. \u2020: W-MHSA stands for window MHSA; we use a window size of 7 because size divisibility is required. \u2021: It is implemented with the official release of Clustered Attention [49], NaN loss occurred during training.", "description": "This table presents an ablation study on the GLMix integration scheme.  The study systematically investigates the impact of different design choices within the GLMix block on the model's performance, measured by IN1k Top-1 accuracy and throughput.  Specifically, it analyzes: 1) the use of both local and global branches, 2) different clustering strategies (soft clustering, k-means, static initialization), 3) various convolution kernel sizes in the local branch, and 4) different numbers of semantic slots in the global branch.  Results show the importance of both local and global branches working in parallel and the effectiveness of the proposed soft clustering method over alternatives.", "section": "4.5 Ablation Study"}, {"figure_path": "ttUXtV2YrA/tables/tables_14_1.jpg", "caption": "Table 7: The evolution path from GLNet-STL to GLNet-4G. Modifications are applied sequentially.", "description": "This table shows the impact of several advanced architectural designs on the performance of the GLNet model. Starting with the Swin-T layout (GLNet-STL), each row shows the effect of adding a new design, such as overlapped patch embedding or hybrid stage 3, on the number of parameters, FLOPs, throughput, and ImageNet-1k Top-1 accuracy. The final row shows the performance of GLNet-4G after all modifications have been applied.", "section": "3.3 GLNet"}, {"figure_path": "ttUXtV2YrA/tables/tables_15_1.jpg", "caption": "Table 3: Comparison with different state-of-the-art backbones on ImageNet-1K classification. All models are trained and evaluated on 224 \u00d7 224 input resolution. Top-1 refers to top-1 accuracy (%). We compare models trained with a standard supervised recipe and those trained with an advanced distillation recipe [26].", "description": "This table compares the performance of GLNet models with other state-of-the-art models on ImageNet-1k classification.  It shows the FLOPs, number of parameters, and top-1 accuracy for each model, broken down by whether they were trained using standard supervised training or an advanced distillation technique.  This allows for a comparison of both accuracy and efficiency across different model architectures.", "section": "4.1 Image Classification on ImageNet-1k"}]