[{"figure_path": "CyzZeND3LB/figures/figures_18_1.jpg", "caption": "Figure 1: Models with very different CGFs coexist within the same model class. On the left, we display several metrics for InceptionV3 models trained on CIFAR10 without regularization (Standard) and with L2 regularization (L2). Random refers to a model learned over randomly reshuffled labels and Zero refers to a model where all the weights are equal to zero. For each model, the metrics include train and test accuracy, test log-loss, l2-norm of the parameters of the model, the variance of the log-loss function, denoted V\u2081(l(x, \u03b8)), and the expected norm of the input-gradients, denoted E, [||\u2207xl(x, \u03b8)||2]. On the right, we display the estimated CGFs of each model, following Masegosa and Ortega (2023). Note how models with smaller variance V(l(x, \u03b8)), l2-norm or input-gradient norm E, [||\u2207xl(x, \u03b8)||2] have smaller CGFs. Bounds derived from Theorem 7 naturally exploit these differences. Experimental details in Appendix C.", "description": "This figure shows that models with different characteristics (different regularization, random labels, zero weights) have very different cumulant generating functions (CGFs).  The left panel displays several performance metrics for these models. The right panel displays the estimated CGFs, demonstrating a clear relationship between the CGFs and the other metrics, particularly the variance of the log-loss function and the L2 norm of the model parameters.", "section": "Overview and Contributions"}]