[{"type": "text", "text": "Offilne Oracle-Efficient Learning for Contextual MDPs via Layerwise Exploration-Exploitation Tradeoff ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Jian Qian Haichen Hu David Simchi-LeviMIT EECS MIT LIDS MIT IDSSjianqian@mit.edu huhc@mit.edu dslevi@mit.edu", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Motivated by the recent discovery of a statistical and computational reduction from contextual bandits to offline regression [36], we address the general (stochastic) Contextual Markov Decision Process (CMDP) problem with horizon $H$ (as known as CMDP with $H$ layers). In this paper, we introduce a reduction from CMDPs to offilne density estimation under the realizability assumption, i.e., a model class $\\mathcal{M}$ containing the true underlying CMDP is provided in advance. We develop an efficient, statistically near-optimal algorithm requiring only $O(H\\log T)$ calls to an offline density estimation algorithm (or oracle) across all $T$ rounds of interaction. This number can be further reduced to $O(H\\log\\log T)$ if $T$ is known in advance. Our results mark the first efficient and near-optimal reduction from CMDPs to offline density estimation without imposing any structural assumptions on the model class. A notable feature of our algorithm is the design of a layerwise exploration-exploitation tradeoff tailored to address the layerwise structure of CMDPs. Additionally, our algorithm is versatile and applicable to pure exploration tasks in reward-free reinforcement learning. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Markov Decision Processes (MDPs) model the long-term interaction between a learner and the environment and are used in diverse areas such as inventory management, recommendation systems, advertising, and healthcare [35, 37]. The Contextual MDP (CMDP) extends MDPs by incorporating external factors, known as contexts, such as gender, age, location, or device in customer interactions, or lab data and medical history in healthcare [19, 33]. In an $H$ -layer CMDP, the learner receives an instantaneous reward at each step over $H$ steps and aims to maximize the cumulative reward (return). For $T$ rounds of interaction, the learner\u2019s performance is measured by regret, which is the difference between the total return obtained and that of an optimal policy. ", "page_idx": 0}, {"type": "text", "text": "In the special case of contextual bandits (one-layer CMDPs), a decade of research has led to algorithms with optimal regret bounds and efficient implementations with access to an offilne regression algorithm (also termed as an offilne regression oracle) [13, 2, 3, 15, 14, 36, 40]. Most notably, Simchi-Levi and Xu [36] demonstrates an offline-oracle-based algorithm FALCON that achieves optimal regret for general (stochastic) contextual bandits with access to an offline regression oracle (e.g., the Empirical Risk Minimization (ERM) oracle). Moreover, the algorithm is efficient given the output of the offilne oracle (also referred to as offline oracle-efficient) and requires only $O(\\log\\log T)$ calls to the oracle across all $T$ rounds if $T$ is known. These properties are highly desirable in practice since they reduce the computational problem of contextual bandits to the classical problem of offline regression with little overhead. However, to the best of our knowledge, no algorithm with these properties is available in the literature for general (stochastic) CMDPs. So, in this paper, we study the following question: Is there an offilne-oracle-efficient algorithm that achieves the optimal regret for general (stochastic) CMDPs with only $O(H\\log\\log T)$ number of oracle calls? ", "page_idx": 0}, {"type": "table", "img_path": "848vuK2cKp/tmp/92ac04d9e01056359f2b1c47166f51616f576c2a5e17b1de8bcef01cf2739e38.jpg", "table_caption": ["Table 1: Algorithms\u2019 performance with general finite model class $\\mathcal{M}$ and i.i.d. contexts. The optimal rate here refers to $\\widetilde{O}(\\mathrm{poly}(H,S,A)\\sqrt{T\\log|\\mathcal{M}|})$ . All algorithms assume realizability, so it is omitted from the table. T he reachability assumption and the varying representation assumption are very stringent for tabular CMDP, for details we refer to Appendix B. "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "Several works have provided partial results for this question. Foster et al. [16] provides a general reduction from interactive decision making to online density estimation and has CMDP as an application. The proposed E2D algorithm achieves optimal regret but is online-oracle-efficient (as opposed to offilne-oracle-efficient) since it requires access to an online density estimation algorithm. Foster et al. [18] provides a further reduction from online density estimation to offline density estimation, with the caveat that the reduction itself is inefficient. A similar online-oracle-efficient algorithm is developed by Levy et al. [26]. A separate thread of optimism-based algorithms for CMDPs extending the UCCB algorithm for contextual bandits [40] is studied by Levy and Mansour [24], Deng et al. [11] with either assumption on the reachability of the CMDP or the representation structure of the CMDP (see Appendix B for more details). Last but not least, the algorithms proposed by Foster et al. [16, 18], Levy and Mansour [24], Deng et al. [11] all require ${\\cal O}(T)$ number of oracle calls to the online/offline oracle respectively. ", "page_idx": 1}, {"type": "text", "text": "In this work, we present an affirmative answer to the question by introducing the algorithm of LOLIPOP (Algorithm 1). For $S$ number of states, $A$ number of actions, and a given model class $\\mathcal{M}$ where the underlying true model lies, the algorithm achieves the regret guarantee of $\\widetilde{O}(\\mathrm{poly}(H,S,A)\\sqrt{T\\log|\\mathcal{M}|})$ . This regret guarantee is minimax optimal up to po $\\operatorname{ly}(H,S,A)$ factor [24]. The LOLIPOP algorithm assumes access to a Maximum Likelihood Estimation (MLE) oracle and is offline-oracle efficient. The results can be generalized to general offline density estimation oracles. The most notable technical features are: (1) It generalizes the FALCON algorithm by SimchiLevi and $\\mathrm{Xu}$ [36] to adapt to the multi-layer structure of a CMDP. More specifically, the FALCON algorithm is divided into ${\\bar{O}}(\\log\\log T)$ epochs, each corresponding to an oracle call, a fixed randomized policy. However, it is known for the MDPs that the learner has to switch its randomized policy at least $\\widetilde\\Omega(H)$ times to achieve sublinear regret [44]. Indeed, we further divide each epoch into $H$ segments, each with an oracle call, a new randomized policy for layerwise exploration-exploitation tradeoff. (2) In each segment, the exploration-exploitation tradeoff is done through Inverse Gap Weighting (IGW) on estimated regret for a set of explorative policies. The idea of running IGW on such a policy cover is proposed by Foster et al. [16]. However, their policy cover is designed for $H$ -layer exploration-exploration tradeoff and only works with strong online estimation oracles. In contrast, we refine the estimation of the occupancy measure layerwise by introducing the trusted occupancy measures. This refinement enables our algorithm to work with offilne oracles. (3) Many other policy cover-based methods [12, 29, 30, 5] are developed for exploration tasks. Most notably, Mhammedi et al. [29] clips the occupancy measures on states with low reachability. Our approach takes a step forward to clip all transitions with low reachability to compute the trusted occupancy measures. ", "page_idx": 1}, {"type": "text", "text": "Besides all the above novelties, the LOLIPOP algorithm is versatile and applicable to the pure exploration task of reward-free reinforcement learning for CMDPs. Concretely, it obtains nearoptimal sample complexity of $O\\big(H^{7}S^{4}A^{3}\\log(|\\mathcal{M}|/\\delta)\\big/\\bar{\\varepsilon}^{2}\\big)$ with only $O(H)$ number of oracle calls. Both the sample complexity bound and computational efficiency result for reward-free learning for stochastic CMDPs are new to the best of our knowledge. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We defer the standard notation and related works to Appendices A and B. ", "page_idx": 1}, {"type": "text", "text": "2.1 Problem Setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A Contextual Makovian Decision Process (CMDP) is defined by the tuple $\\begin{array}{r l}{(\\mathcal{C},\\mathcal{M}}&{{}=}\\end{array}$ $\\{M(c)\\}_{c\\in c},S,\\mathcal{A},s^{1})$ , where $\\mathcal{C}$ is the contextual space, $\\boldsymbol{S}$ is the state space, $\\boldsymbol{\\mathcal{A}}$ is the action space and $s^{1}\\in S$ is a fixed starting state independent of the context. We focus on tabular CMDPs which assumes $S=|S|,A=|A|<\\infty$ . For any context $c\\in{\\mathcal{C}}$ , $\\boldsymbol{M}(\\boldsymbol{c})=\\{P_{M}^{h}(\\boldsymbol{c}),R_{M}^{h}(\\boldsymbol{c})\\}_{h=1}^{H}$ consists of $H$ -layers of probability transition kernel $\\{P_{M}^{h}(c)\\}_{h=1}^{H}$ and reward distributions $\\{R_{M}^{h}(c)\\}_{h=1}^{H}$ , where $P_{M}^{h}(c)$ and $R_{M}^{h}(c)$ are specified by $P_{M}^{h}(\\cdot\\mid s,a;c)\\in\\Delta(S)$ and $R_{M}^{h}(s,a;c)\\in\\Delta([0,1])$ for all $h\\in[H]$ and $s,a\\in{\\mathcal{S}}\\times{\\mathcal{A}}$ . For simplicity, we also denote $M=\\{P_{M}^{h},R_{M}^{h}\\}_{h=1}^{H}$ , where $P_{M}^{h}=\\{P_{M}^{h}(c)\\}_{c\\in\\mathcal{C}}$ and $R_{M}^{h}=\\{R_{M}^{h}(c)\\}_{c\\in{\\cal C}}.$ . Let $\\Pi_{\\mathrm{RNS}}$ denote the set of all randomized, non-stationary policies, where any $\\pi=(\\pi^{1},\\cdot\\cdot\\cdot,\\pi^{H})\\in\\Pi_{\\mathrm{RNS}}$ has $\\pi^{h}:{\\mathcal{S}}\\to\\Delta(A)$ for any $h\\in[H]$ . We use $T$ to denote the total number of rounds, $H$ to denote the horizon (the total number of layers). Let $M_{\\star}=\\{P_{\\star}^{h},R_{\\star}^{h}\\}_{h\\in[H]}$ be the underlying true CMDP the learner interact with. The interactive protocal proceeds in $T$ rounds, where for each round $t$ , the $t$ -th trajectory is generated as: ", "page_idx": 2}, {"type": "text", "text": "\u2022 A context $c_{t}$ is draw i.i.d. from an unknown distribution $\\mathcal{D}$ and $s_{t}^{1}=s^{1}$ .   \n\u2022 The learner chooses the policy $\\pi_{t}$ based on the context.   \n\u2022 For $h=1,\\ldots,H$ : \u2022 The action is drawn from the randomized policy $a_{t}^{h}\\sim\\pi_{t}^{h}(s_{t}^{h})$ . \u2022 The reward and the next state is drawn respectively from the reward distribution and the transition kernel, i.e., $r_{t}^{h}\\sim R_{\\star}^{h}(s_{t}^{h},a_{t}^{h};c_{t})$ and sth+1\u223cP \u22c6h(\u00b7 | sth , ath ; ct). aWlimthosotu st ulroesley .o fF ogre annerya lmitoy,d etlh , gchoonutte xtth paanpd epr,o lwicey s, uwme eu tshe a tl or edwenarodt $\\begin{array}{r}{0\\leq\\sum_{h=1}^{H}r^{h}\\leq1}\\end{array}$ $M$ $c$ $\\pi$ $M(\\pi,c)$   \nthe trajectory $c_{1},\\pi_{1};s_{1}^{1},a_{1}^{1},r_{1}^{1},\\ldots,s_{1}^{H},a_{1}^{H},r_{1}^{\\hat{H}}$ given $M_{\\star}=M$ , $c_{1}=c$ , and $\\pi_{1}=\\pi$ . Also denote the probability and the expectation under $M(\\pi,c)$ to be $\\mathbb{P}^{M,\\pi,c}(\\cdot)$ and $\\mathbb{E}^{M,\\pi,c}[\\cdot]$ respectively. Given any policy $\\pi$ , state $s$ and action $a$ , we define the action value function $Q_{\\star}^{h}(s,a;\\pi,c)$ at layer $h$ and the value function $V_{\\star}^{h}(s;\\pi,c)$ at layer $h$ under context $c$ and policy $\\pi$ as ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{Q_{\\star}^{h}(s,a;\\pi,c)=\\sum_{j=h}^{H}\\mathbb{E}^{M_{\\star},\\pi,c}[r_{1}^{j}\\ |\\ s_{1}^{h},a_{1}^{h}=s,a]\\ \\ \\mathrm{and}\\ \\ V_{\\star}^{h}(s;\\pi,c)=\\operatorname*{max}_{a\\in A}Q_{\\star}^{h}(s,a;\\pi,c).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We denote the optimal policy under context $c$ as $\\pi_{\\star,c}$ and abbreviate its value function as $V_{\\star}^{h}(\\cdot;c)$ . For $h=1$ , we further simply the notation by denoting $V_{\\star}^{1}(c)=V_{\\star}^{1}(s^{1};c)$ and $V_{\\star}^{1}(\\pi,c)=V_{\\star}^{1}(s^{1};\\pi,c)$ . The regret of policy $\\pi$ under context $c$ and the total regret1 of the learner are defined as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{reg}(\\pi,c)=V_{\\star}^{1}(c)-V_{\\star}^{1}(\\pi,c)\\quad\\mathrm{and}\\quad\\mathrm{Reg}(T):=\\sum_{t=1}^{T}\\mathbb{E}_{t}[\\mathrm{reg}(\\pi_{t},c_{t})],}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbb{E}_{t}[\\cdot]$ is the conditional expectation given the interaction up to round $t$ . ", "page_idx": 2}, {"type": "text", "text": "Assumption 2.1 (Realizability). The learner is given a model class $\\mathcal{M}$ where the true underlying model $M_{\\star}$ lies, that is, $M_{\\star}\\in\\mathcal{M}$ . ", "page_idx": 2}, {"type": "text", "text": "2.2 Offline Densitiy Estimation Oracles ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "For any model class $\\mathcal{M}$ , a general offilne density estimation oracle associated with $\\mathcal{M}$ , denoted by $\\mathrm{OHDE}_{\\mathcal{M}}$ , is defined as an algorithm that generates a predictor $\\widehat{M}$ based on the input data and $\\mathcal{M}$ . In this paper, we measure the performance of the predictor in terms of the squared Hellinger distance, which is defined for any two distributions $\\mathbb{P}$ and $\\mathbb{Q}$ for any common dominating measure $\\nu^{2}$ by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{D_{\\mathrm{H}}^{2}(\\mathbb{P},\\mathbb{Q}):=\\frac{1}{2}\\int\\bigg(\\sqrt{\\frac{d\\mathbb{P}}{d\\nu}}-\\sqrt{\\frac{d\\mathbb{Q}}{d\\nu}}\\bigg)^{2}d\\nu,}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Using squared Hellinger distance for reinforcement learning is popularized by Foster et al. [16], and we adopt such a divergence for our purpose as well. Concretely, we are interested in the following statistical guarantee. ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1 (Offline density estimation oracle). Let $p$ be a map from a context to a distribution on the set of policies $\\Pi_{\\mathrm{RNS}}$ , that is, for any $c\\in\\mathcal{C}$ , $p(c)\\;\\in\\;\\Delta(\\Pi_{\\mathrm{RNS}})$ . Given n training trajectories $\\left(c_{i},\\bar{\\pi_{i}};s_{i}^{1},a_{i}^{1},r_{i}^{1},\\ldots,s_{i}^{H},a_{i}^{H},\\bar{r}_{i}^{H}\\right)$ i.i.d. drawn according to $c_{i}\\sim\\mathcal{D}$ , $\\pi_{i}\\sim p(c_{i})$ and ", "page_idx": 2}, {"type": "text", "text": "$s_{i}^{1},a_{i}^{1},r_{i}^{1},\\ldots,s_{i}^{H},a_{i}^{H},r_{i}^{H}$ be the trajectory sampled according to $M_{\\star}(\\pi_{i},c_{i})$ . The offline density estimation oracle $\\mathrm{OHDE}_{\\mathcal{M}}$ returns a predictorM. For any $\\delta\\in(0,1/2)$ , with probability at least $1-\\delta$ , we have ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{c\\sim\\mathcal{D},\\pi\\sim p(c)}\\Big[D_{\\mathrm{H}}^{2}\\Big(\\widehat{M}(\\pi,c),M_{\\star}(\\pi,c)\\Big)\\Big]\\leq\\mathcal{E}_{\\mathcal{M},\\delta}(n).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The Maximum Likelihood Estimation estimator $\\mathsf{M L E}_{\\mathcal{M}}$ is an example of an offline density estimation oracle that achieves $\\mathcal{E}_{\\mathcal{M},\\delta}(n)\\lesssim\\log(|\\mathcal{M}|/\\delta)/n$ (see more details in Appendix C) and can be implemented using ERM on the log loss. Moreover, this implementation can be efficient for cases like the multinomial logit model [32, Example 2.4] and [1]. ", "page_idx": 3}, {"type": "text", "text": "3 Main Results and Algorithm ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we present our main results and introduce the algorithm of LOLIPOP (Algorithm 1). First, we give an overview of the algorithm. Then, we discuss the theoretical guarantees obtained by this algorithm. Finally, we introduce the algorithm\u2019s different components with corresponding guarantees. All proofs are deferred to Appendix D. ", "page_idx": 3}, {"type": "text", "text": "3.1 Main Results ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Overview of Algorithm 1. The algorithm proceeds with epochs. The total number of $T$ rounds is divided into $N$ epochs. For an epoch schedule $0=\\tau_{0}<\\tau_{1}<\\cdot\\cdot<\\tau_{N}=T/H$ to be specified later, the $m$ -th epoch will last $H(\\tau_{m}-\\tau_{m-1})$ rounds. Furthermore, each epoch is evenly divided into $H$ segments, each consisting of $\\tau_{m}-\\tau_{m-1}$ rounds. During the $h$ -th segment in $m$ -th epoch, a kernel $p_{m}^{h^{-}}\\colon\\mathcal{C}\\to\\Delta(\\Pi)$ will be specified to determine the policy. More specifically, upon receiving the context $c_{t}$ , a policy $\\pi_{t}$ will be sampled from $p_{m}^{h}(c_{t})$ and executed. After collecting the trajectories $\\{c_{t},\\pi_{t}\\}\\cup\\{s_{t}^{j},a_{t}^{j},r_{t}^{j}\\}_{j\\in[H]}$ in the $h$ -th segment of the $m$ -th epoch for $\\tau_{m-1}H+(\\tau_{m}-\\tau_{m-1})(h-$ $1)+1\\leq t\\leq\\tau_{m-1}H+(\\tau_{m}-\\tau_{m-1})h.$ , the offilne density estimation oracle $\\mathrm{OHDE}_{\\mathcal{M}}$ is called with tohuetspeu ttr, ajwehcitcohri ews ea sd einnpotuet.  bDy .t pTuht $\\widehat{M}_{m}^{h}$ ,e  wceo llweilclt ioonnlsy  obfe  eisnttiemreasttoerds $h$ $\\{\\widehat{P}_{m}^{h},\\widehat{R}_{m}^{h}\\}$ $\\widehat{M}_{m}=\\{\\widehat{P}_{m}^{h},\\widehat{R}_{m}^{h}\\}_{h=1}^{H}$ ", "page_idx": 3}, {"type": "text", "text": "Throughout this paper, we will adopt the following convention for the free variables $m,\\pi,c,h,s,a$ . They will be used to denote an epoch index in $[N]$ , a policy in $\\Pi_{\\mathrm{RNS}}$ , a context in $\\mathcal{C}$ , a layer index in $[H]$ , a state in $\\boldsymbol{S}$ , and an action in $\\boldsymbol{\\mathcal{A}}$ respectively. ", "page_idx": 3}, {"type": "text", "text": "Before we dive into the details of the algorithm, we highlight first the theoretical guarantees obtained. ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.1. If $T$ is known, then by choosing the epoch schedule $\\tau_{m}=2(T/H)^{1-2^{-m}}$ for $m\\geq1$ and the offilne density estimation oracle $\\mathrm{OffDE}_{\\mathcal{M}}=\\mathsf{M L E}_{\\mathcal{M}}$ , the outputs $\\{\\pi_{t}\\}_{t\\in[T]}$ of Algorithm $^{\\,l}$ satisfies that with probability at least $1-\\delta$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{Reg}(T)\\lesssim\\sqrt{H^{7}S^{4}A^{3}T\\cdot\\log(|\\mathcal{M}|\\log\\log T/\\delta)\\log\\log T}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with only $O(H\\log\\log T)$ number of oracle calls to the $\\mathsf{M L E}_{\\mathcal{M}}$ oracle for $\\delta\\in(0,1/2)$ . ", "page_idx": 3}, {"type": "text", "text": "Theorem 3.2. If $T$ is not known, then by choosing the epoch schedule $\\tau_{m}=2^{m}$ for $m\\geq1$ and the offline density estimation oracle $\\mathrm{OffDE}_{\\mathcal{M}}=\\mathsf{M L E}_{\\mathcal{M}}.$ , the outputs $\\{\\pi_{t}\\}_{t\\in[T]}$ of Algorithm $^{\\,l}$ satisfies that with probability at least $1-\\delta$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{Reg}(T)\\lesssim\\sqrt{H^{7}S^{4}A^{3}T\\cdot\\log(|\\mathcal{M}|\\log T/\\delta)}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "with $O(H\\log T)$ number of oracle calls to the $\\mathsf{M L E}_{\\mathcal{M}}$ oracle for $\\delta\\in(0,1/2)$ . ", "page_idx": 3}, {"type": "text", "text": "The theorems above show that Algorithm 1 with both epoch schedules achieve near-optimal statistical complexity that a matches the lower bound of $\\Omega(\\sqrt{H S A T\\log|\\mathcal{M}|/\\log A})$ proven by Levy and Mansour [24] up to a poly $(H,S,A)$ factor. ", "page_idx": 3}, {"type": "text", "text": "Computational efficiency. Consider the epoch schedule $\\tau_{m}\\,=\\,2^{m}$ for $m\\in\\mathbb{N}$ as discussed in Theorem 3.2. For any unknown $T$ , our algorithm operates over ${\\cal O}(\\log T)$ epochs, making one oracle call per epoch. Thus, the computational complexity is ${\\cal O}(\\log T)$ oracle calls over $T$ rounds, with an additional per-round cost of $O(\\mathrm{poly}(H,S,A,\\log T))$ . This offers potential advantages over existing algorithms that achieve near-optimal rates without assumptions beyond realizability. The E2D ", "page_idx": 3}, {"type": "text", "text": "Require: epoch schedule $0\\;=\\;\\tau_{-1}\\;=\\;\\tau_{0}\\;<\\;\\tau_{1}\\;<\\;\\cdots\\;<\\;\\tau_{N}\\;=\\;T/H$ , confidence parameter   \n$\\dot{\\delta}\\in(0,\\bar{1}/2)$ , model class $\\mathcal{M}$ , offline oracle $\\mathrm{OHDE}_{\\mathcal{M}}$ .   \n1: Initialize: $\\widehat{M_{0}}=\\{\\widehat{P}_{0}^{h},\\widehat{R}_{0}\\}_{h=1}^{H}$ , where $\\widehat{P}_{0}^{h}$ is any transtion kernel and ${\\widehat{R}}_{0}$ is constantly 0.   \n2: for epoch $m=1,2,\\cdot\\cdot\\cdot,N\\,\\cdot$ do   \n3: Let $\\mathcal{E}_{m}=\\mathcal{E}_{\\mathcal{M},\\delta/2N^{2}}(\\tau_{m-1}-\\tau_{m-2})$ , $\\gamma_{m}=\\sqrt{H^{6}S^{4}A^{3}/\\varepsilon_{m}}$ and $\\eta_{m}=\\gamma_{m}/720e H^{5}S^{3}A^{2}$ .   \n4: for segment $h=1,\\hdots,H$ do   \n5: for round $t=\\tau_{m-1}H+(\\tau_{m}-\\tau_{m-1})(j-1)+1,\\cdot\\cdot\\cdot,\\tau_{m-1}H+(\\tau_{m}-\\tau_{m-1})h$ do   \n6: Observe context $c_{t}\\in\\mathcal{C}$ from the environment.   \n7: for $s,a\\in{\\mathcal{S}}\\times{\\mathcal{A}}$ do   \n8: Compute ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pi_{m,c_{t}}^{h,s,a}=\\arg\\operatorname*{max}_{\\pi}\\frac{\\widetilde{d}_{m}^{h}(s,a;\\pi,c_{t})}{S A+\\eta_{m}\\cdot\\widehat{\\mathrm{reg}}_{m-1}(\\pi,c_{t})},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\tilde{d}_{m}^{h}$ is the trusted occupancy measure defined as in Definition 3.1. ", "page_idx": 4}, {"type": "text", "text": "LDeetf tinhee $p_{m}^{h}(c_{t})$ to be the Inverse G ap Wetighting disttribution on the policy cover $\\Pi_{m}^{h}(c_{t})=\\{\\widehat{\\pi}_{m-1,c_{t}}\\}\\cup\\{\\pi_{m,c_{t}}^{h,s,a}\\}_{s,a\\in\\mathcal{S}\\times\\mathcal{A}}.$ $\\Pi_{m,c_{t}}^{h}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\np_{m}^{h}(c_{t},\\pi)=\\frac{1}{\\lambda_{m,c_{t}}^{h}+\\eta_{m}\\cdot\\widehat{\\mathrm{reg}}_{m-1}(\\pi,c_{t})},\\quad\\forall\\pi\\in\\Pi_{m}^{h}(c_{t}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lambda_{m,c_{t}}^{h}$ is the constant that normalize the distribution. ", "page_idx": 4}, {"type": "text", "text": "11: ", "page_idx": 4}, {"type": "text", "text": "12: ", "page_idx": 4}, {"type": "text", "text": "Sample and execute $\\pi_{t}\\sim p_{m}^{h}(c_{t})$ and observe the trajectory $c_{t},\\pi_{t},\\{s_{t}^{j},a_{t}^{j},r_{t}^{j}\\}_{j\\in[H]}$ . Run $\\mathrm{OHDE}_{\\mathcal{M}}$ with the input trajectories $\\left\\{c_{t},\\pi_{t},\\{s_{t}^{j},a_{t}^{j},r_{t}^{j}\\}_{j\\in[H]}\\right\\}_{t:m(t)=m}$ and obtain the $h$ -th layer estimator $\\widehat{P}_{m}^{h}$ and $\\widehat{R}_{m}^{h}$ . ", "page_idx": 4}, {"type": "text", "text": "algorithm [16], for instance, requires ${\\cal O}(T)$ calls to an online density estimation oracle, involving significantly more calls to a more complex oracle for a general model class $\\mathcal{M}$ . On the other hand, the Version Space Averaging $+\\,\\mathrm{E}2\\mathrm{D}$ algorithm [18] requires ${\\cal O}(T)$ calls to an offline density estimation oracle and incurs a computational cost scaling with $O(|\\mathcal{M}|)$ per round. Compared to our algorithm, this results in far more oracle calls and considerably higher computational costs per round. ", "page_idx": 4}, {"type": "text", "text": "If the total number of rounds $T$ is known to the learner, we can further reduce the computational cost of LOLIPOP. For any $T\\in\\mathbb N$ , consider the epoch schedule $\\tau_{m}=2(T/H)^{1-2^{-m}}$ as in Theorem 3.1, similar to Simchi-Levi and $\\mathrm{Xu}$ [36]. In this scenario, LOLIPOP will run in $O(\\log\\log T)$ epochs, making only $O(\\log\\log T)$ oracle calls over $T$ rounds while still maintaining a slightly worse regret guarantee. ", "page_idx": 4}, {"type": "text", "text": "Lower bound on switching cost. There is a lower bound on the switching cost of the scale $\\Omega(\\log\\log T)$ [44], where the switching cost is the number of switches in the learner\u2019s randomized policy. Thus, any learner that only switches its randomized policy after an oracle call will need more than $\\Omega(\\log\\log T)$ number of oracle calls. ", "page_idx": 4}, {"type": "text", "text": "Technical challenge. The main technical challenge is to accurately estimate the occupancy measures for all layers. Naively, the upper bound of divergence between the true occupancy measure and the estimated occupancy measure accumulates exponentially with respect to the number of layers because, naively, the divergence at each layer is upper bounded by the summation of divergences from previous steps. This phenomenon is unavoidable when estimating all the occupancy measures from one dataset generated from a single policy. To avoid this exponential divergence, we apply two methods. First, we turn to layerwise design. Specifically, we generate for occupancy measures at each layer a new dataset from a different policy. This alleviates the exponential accumulation of divergence. Second, we turn to multiplicative guarantees between true occupancy measures and the approximated occupancy measures, i.e., they are equivalent up to a small constant. To achieve this, we construct the trusted occupancy measure (see Definition 3.1), which discards the rarely visited state-action pairs. We then use the trusted occupancy measures to guide exploration. ", "page_idx": 4}, {"type": "image", "img_path": "848vuK2cKp/tmp/ec904cdc8f886916d969b6d9953e3c7dda0839f4530402ebd2f870ec5b3730c4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 1: The dependence graph of the construction. The estimation $\\widehat{M}_{m-1}=\\{\\widehat{P}_{m-1}^{h},\\widehat{R}_{m-1}^{h}\\}_{h\\in[H]}$ from the previous round provides the optimal policy $\\widehat{\\pi}_{m-1}$ (Line 9) a nd the regre t estim ation $\\widehat{\\mathrm{reg}}_{m-1}$ (Line 8) for the construction of $\\Pi_{m}^{h},p_{m}^{h}$ . The estim a tion $\\widehat{P}_{m}^{h},\\widehat{R}_{m}^{h}$ is generated by calling th e  oracle $\\mathrm{OHDE}_{\\mathcal{M}}$ on the trajectories collected with policy kernel $p_{m}^{h}$ (Line 12). The trusted transitions and trusted occupancy measures $\\widetilde{\\cal T}_{m}^{h},\\widetilde{d}_{m}^{h+1}$ are computed from ${\\widetilde{d}}_{m}^{h},{\\widehat{P}}_{m}^{h}$ (Eqs. (2) and (3)). The policy cover $\\Pi_{m}^{h}$ is the union of $\\widehat{\\pi}_{m-1,}$ \u00b7 an d the policies $\\{\\pi_{m,\\cdot}^{h,s,a}\\}_{s,a\\in S\\times A}$ calculated in Line 8 which requires $\\widetilde{T}_{m}^{h-1},\\widetilde{d}_{m}^{h}$ . The policy kernel $p_{m}^{h}$ is the inverse gap weighting on $\\Pi_{m}^{h}$ (Line 10). ", "page_idx": 5}, {"type": "text", "text": "3.2 Detailed Construction and Guarantees of Each Component ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we explain in detail our construction and the guarantees of each component along the dependence graph (Figure 1). We first introduce how the estimators $\\{\\widehat{P}_{m-1}^{h},\\widehat{R}_{m-1}^{h}\\}_{h\\in[H]}$ from the previous epoch are used in the new epoch. Then we proceed to introduce how to construct $p_{m}^{h}(c_{t})$ given $\\Pi_{m}^{h}\\big(c_{t}\\big)$ . Next, we introduce how are $\\widehat{P}_{m}^{h},\\widehat{R}_{m}^{h}$ obtained given $p_{m}^{h}$ . Subsequently, we present the definition of the set of trusted transition s ${\\widetilde{\\mathcal{T}}}_{m}^{h}$ and trusted occupancy measure $\\tilde{d}_{m}^{h}$ . Finally, we present how the policy cover $\\Pi_{m}^{h}(c_{t})$ is constructed. ", "page_idx": 5}, {"type": "text", "text": "During epoch $m$ , we will be using the estimators $\\{\\widehat{P}_{m-1}^{h},\\widehat{R}_{m-1}^{h}\\}_{h=1}^{H}$ from the previous epoch for regret estimation. More specifically, for $\\pi,c,h,s$ , we define the value functions with respect to the model $\\{\\widehat{P}_{m-1}^{h}(c),\\widehat{R}_{m-1}^{h}(c)\\}_{h=1}^{H}$ as $\\widehat{V}_{m-1}^{h}(s;\\pi,c)$ . The optimal value function is denoted by $\\begin{array}{r}{\\widehat V_{m-1}^{1}(s;c)\\,=\\,\\operatorname*{max}_{\\pi}\\widehat V_{m-1}^{1}(s;\\pi,c)}\\end{array}$ . For $h\\,=\\,1$ , we further simply the notation by denoting $\\widehat V_{m-1}^{1}(c)=\\widehat V_{m-1}^{1}(s^{1};c)$ and $\\widehat V_{m-1}^{1}(\\pi,c)=\\widehat V_{m-1}^{1}(s^{1};\\pi,c)$ . Also denote the optimal policy under context $c$ by $\\widehat{\\pi}_{m-1,c}=\\arg\\operatorname*{max}_{\\pi}\\widehat{V}_{m-1}^{1}(\\pi,c)$ . Thus, the regret is estimated to be ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\widehat{\\mathrm{reg}}_{m-1}(\\pi,c)=\\widehat{V}_{m-1}^{1}(c)-\\widehat{V}_{m-1}^{1}(\\pi,c).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "At round $t$ , let $m(t)$ and $h(t)$ be the epoch in which the segment round $t$ lies. We note that during each epoch $m$ and segment $h$ , all of the notions $\\Pi_{m}^{h}(c_{t}),\\widehat{P}_{m}^{h}(c_{t}),\\;\\widehat{R}_{m}^{h}(c_{t}),\\;\\widetilde{\\mathcal{T}}_{m}^{h}(c_{t}),\\;\\widetilde{d}_{m}^{h}(\\cdot,\\cdot;\\cdot,c_{t})$ , $p_{m}^{h}(c_{t})$ , and $\\pi_{m,c_{t}}^{h,s,a}$ will not depend on the specific time st e p $t$ , but  o nly the  c ontext $c_{t}$ . Thus, we will use $\\Pi_{m}^{h}(c)$ to denote the policy cover if $c_{t}=c$ when $m(t),h(t)=m,h$ . Similar conventions regarding the context $c$ apply to the notations $\\widehat{P}_{m}^{h},\\widehat{R}_{m}^{h},\\widetilde{T}_{m}^{h},\\widetilde{d}_{m}^{h},p_{m}^{h},\\pi_{m,\\cdot}^{h,s,a}$ . Under any context $c$ , the policy cover $\\Pi_{m}^{h}$ will include $\\widehat{\\pi}_{m-1,c}$ and has no more than $S A+1$ policies. These two properties together guarantee that the Inverse Gap Weighting [14] randomized policy $p_{m}^{h}(c)$ (Line 10) satisfies the following guarantee on the estimated regret. ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.1. For any $m$ , $h$ , c, the definition of the randomized policy $p_{m}^{h}(c)$ is well defined, i.e., there exist $\\lambda_{m,c}^{h}\\in[0,2S A]$ such that $\\begin{array}{r}{\\sum_{\\pi\\in\\Pi_{m}^{h}(c)}p_{m,c}^{h}(\\pi)=1.}\\end{array}$ . Furthermore, we have the estimated regret is bounded by $\\begin{array}{r}{\\mathbb{E}_{\\pi\\sim p_{m}^{h}(c)}\\left[\\widehat{\\mathrm{reg}}_{m-1}(\\pi,c)\\right]\\lesssim\\sqrt{H^{4}S^{4}A^{3}\\cdot\\mathcal{E}_{m}}.}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "The choice of $\\lambda_{m,c}^{h}$ here is for compactness of presentation. It can be chosen to be $2S A$ for suboptimal arms and collect the probability remained to the optimal arm [36], which is computationally efficient. The computation for the policy \u03c0hm((tt)),,sc,ta for any t, s, a can be computed in poly(H, S, A, log T) time by formulating it as a linear fractional programming problem. We defer the details to Appendix G. ", "page_idx": 5}, {"type": "text", "text": "Since $p_{m}^{h}$ maps $\\mathcal{C}$ to randomized policies, it is thus a policy kernel. This means the trajectories gention 2.1. By applying the guarantee from Definition 2.1, we have the following guarantee on erated in epoch $m$ and segment $h$ follow an i.i.d. distribution as described in the definition of Defini- $\\widehat{P}_{m}^{h},\\widehat{R}_{m}^{h}$ . ", "page_idx": 5}, {"type": "text", "text": "Lemma 3.2. For any $m,\\,h_{\\!}$ , and $c\\sim\\mathcal{D},\\pi\\sim p_{m}^{h}(c),$ , we have with probability at least $1-\\frac{\\delta}{2N^{2}}$ 2N 2 , that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Xi_{c,\\pi}\\left[\\mathbb{E}^{M_{\\star},\\pi,c}\\left[D_{\\mathrm{H}}^{2}\\Big(\\widehat{P}_{m}^{h}\\big(s_{1}^{h},a_{1}^{h};c\\big),P_{\\star}^{h}\\big(s_{1}^{h},a_{1}^{h};c\\big)\\Big)+D_{\\mathrm{H}}^{2}\\Big(\\widehat{R}_{m}^{h}\\big(s_{1}^{h},a_{1}^{h};c\\big),R_{\\star}^{h}\\big(s_{1}^{h},a_{1}^{h};c\\big)\\Big)\\right]\\right]\\leq\\mathcal{E}_{m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "If the offline density estimation oracle is chosen to be the Maximum Likelihood Estimation oracle $\\mathsf{M L E}_{\\mathcal{M}}$ , we will obtain $\\mathcal{E}_{m}\\lesssim\\log(T\\mathcal{M}/\\delta)/(\\tau_{m-1}-\\tau_{m-2})$ . ", "page_idx": 6}, {"type": "text", "text": "The most involved part of our construction concerns the idea of trusted transitions and trusted occupancy measures. This construction eliminates the parts of transitions that are too scarcely visited. The purpose will be clear in the guarantees (Lemmas 3.3 and 3.4) subsequent to the definitions. ", "page_idx": 6}, {"type": "text", "text": "Definition 3.1. For any $m,\\pi,c,h,s,a,$ , we define iteratively the trusted occupancy measures $\\widetilde{d}_{m}^{h}(s;\\pi,c),\\,\\widetilde{d}_{m}^{h}(s,a;\\pi,c)$ and the set of trusted transitions $\\widetilde{\\mathcal{T}}_{m}^{h}(c)$ at layer $h$ as the following: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\widetilde{d}_{m}^{1}(s;\\pi,c)=\\mathbb{1}(s=s^{1}),\\qquad\\widetilde{d}_{m}^{1}(s,a;\\pi,c)=\\mathbb{1}(s=s^{1})\\pi^{1}(s,a),}}\\\\ {{\\widetilde{d}_{m}^{h}(s;\\pi,c):=\\displaystyle\\sum_{s^{\\prime},a^{\\prime},s\\in\\widetilde{\\mathcal{T}}_{m}^{h-1}(c)}\\widetilde{d}_{m}^{h-1}(s^{\\prime},a^{\\prime};\\pi,c)\\widehat{P}_{m}^{h-1}(s|s^{\\prime},a^{\\prime};c),}}\\\\ {{\\widetilde{d}_{m}^{h}(s,a;\\pi,c):=\\widetilde{d}_{m}^{h}(s;\\pi,c)\\pi^{h}(s,a).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "For any $m,h,c,$ the set of trusted transitions are defined as the set of transtions ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{T}}_{m}^{h}(c)\\triangleq\\left\\{(s,a,s^{\\prime})\\Big|\\operatorname*{max}_{\\pi}\\frac{\\widetilde{d}_{m}^{h}(s,a;\\pi,c)}{S A+\\eta_{m}\\cdot\\widehat{\\mathrm{reg}}_{m-1}(\\pi,c)}\\cdot\\widehat{P}_{m}^{h}(s^{\\prime}|s,a;c)\\geq\\frac{1}{\\zeta_{m}}\\right\\},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where \u03b6m = 8eH(Hm+1)2 . Notice that to define $\\widetilde{d}_{m}^{h}(s;\\pi,c)$ and $\\widetilde{d}_{m}^{h}(s,a;\\pi,c)$ , we only need $\\{\\widetilde{T}_{m}^{j}(c),\\widehat{P}_{m}^{j}(c)\\}_{j\\in[h-1]}$ . Thus the two definitions are iteratively well-defined. Meanwhile, we also define th e observable occupancy measures as the occupancy measures of the true model going through only the trusted transitions, i.e., ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{c l l r}{{}}&{{}}&{{d_{m}^{1}(s;\\pi,c)=\\mathbb{1}(s=s^{1}),~~~~~~d_{m}^{1}(s,a;\\pi,c)=\\mathbb{1}(s=s^{1})\\pi^{1}(s,a),}}\\\\ {{}}&{{}}&{{d_{m}^{h}(s;\\pi,c):=\\sum_{s^{\\prime},a^{\\prime},s\\in\\widetilde{\\mathcal{T}}_{m}^{h-1}(c)}d_{m}^{h-1}(s^{\\prime},a^{\\prime};\\pi,c)P_{\\star}^{h-1}(s|s^{\\prime},a^{\\prime};c),}}\\\\ {{}}&{{}}&{{d_{m}^{h}(s,a;\\pi,c):=d_{m}^{h}(s;\\pi,c)\\pi_{h}(s,a).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The computation of the set of trusted transitions need not enumerate all policies. The trusted transition set can be computed in poly $(H,S,A,\\log T)$ time by formulating it as a linear fractional programming problem. We defer the details to Appendix G. ", "page_idx": 6}, {"type": "text", "text": "Define the estimated occupancy measures $\\widehat{d}_{m}^{h}(s;\\pi,c):=\\mathbb{E}^{\\widehat{M}_{m},\\pi,c}[\\mathbb{1}(s_{1}^{h}=s)]$ and $\\widehat{d}_{m}^{h}(s,a;\\pi,c):=$ $\\mathbb{E}^{\\widehat{M}_{m},\\pi,c}[\\mathbb{1}(s_{1}^{h},a_{1}^{h}\\,=\\,s,a)]$ . The trusted occupancy measure, though it eliminates rarely visited transitions, remains a valid estimate for all policies because the divergence between the estimated occupancy measure and itself is bounded. Specifically, we have the following lemma. ", "page_idx": 6}, {"type": "text", "text": "Lemma 3.3. For all $m,\\pi,h,s,a,$ , under any context c, we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\widehat{d}_{m}^{h}(s,a;\\pi,c)-\\widetilde{d}_{m}^{h}(s,a;\\pi,c)\\leq32e\\sqrt{H^{4}S^{2}A\\cdot\\mathcal{E}_{m}}+\\widehat{\\mathrm{reg}}_{m-1}(\\pi;c)/(90H S A).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The next guarantee is the key to our analysis and is the most non-trivial guarantee of our construction. The following lemma states that if, for a context $c$ , the Hellinger divergence between $\\widehat{P}$ and $P_{\\star}^{h}$ at layer $h$ is small for all $h\\in[H]$ , then the trusted occupancy measure is upper bounded by a scaling of the observable occupancy measure. ", "page_idx": 6}, {"type": "text", "text": "Lemma 3.4. For any m and $c,$ assume for all $h\\in[H]$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\pi\\sim p_{m}^{h}(c)}\\mathbb{E}^{M_{\\star},\\pi,c}\\left[D_{\\mathrm{H}}^{2}\\Big(\\widehat{P}_{m}^{h}\\big(s_{1}^{h},a_{1}^{h};c\\big),P_{\\star}^{h}\\big(s_{1}^{h},a_{1}^{h};c\\big)\\Big)\\right]\\leq H/\\gamma_{m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Then for the same $m,c$ and all $\\pi,h,s,a,$ , we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\widetilde{d}_{m}^{h}(s,a;\\pi,c)\\leq(1+1/H)^{2(h-1)}d_{m}^{h}(s,a;\\pi,c).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Since the trusted occupancy measure is upper bounded up to scaling by the observable occupancy measure, then the state-action pairs with large trusted occupancy measures are guaranteed to be visited often in the true dynamics as well. This enables more accurate planning and is thus crucial to our analysis. ", "page_idx": 7}, {"type": "text", "text": "Finally, we state the coverage guarantee achieved by the construction of $\\Pi_{m}^{h}$ and $p_{m}^{h}$ . Concretely, we upper bound the trusted occupancy measure $\\tilde{d}_{m}^{h}(\\cdot,\\cdot;\\pi,\\cdot)$ of any policy $\\pi$ by the trusted occupancy measure induced by policy kernel phm. ", "page_idx": 7}, {"type": "text", "text": "Lemma 3.5. For any $m,\\pi,c,h,s,a,$ , we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{d}_{m}^{h}(s,a;\\pi,c)\\cdot D_{\\mathrm{H}}(P_{\\star}^{h}(s,a;c),\\widehat{P}_{m}^{h}(s,a;c))}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{\\gamma_{m}}{e^{2}H}\\cdot p_{m}^{h}(c,\\pi_{m,c}^{h,s,a})\\widehat{d}_{m}^{h}(s,a;\\pi_{m,c}^{h,s,a},c)\\cdot D_{\\mathrm{H}}^{2}(P_{\\star}^{h}(s,a;c),\\widehat{P}_{m}^{h}(s,a;c))+\\mathcal{E}_{m}^{\\prime},}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\begin{array}{r l r}{\\mathcal{E}_{m}^{\\prime}}&{=}&{\\Bigg(2e^{2}\\sqrt{\\frac{\\mathcal{E}_{m}}{H^{4}S^{2}A}}+\\frac{1}{720H^{4}S^{3}A^{2}}\\widehat{\\mathrm{reg}}_{m-1}(\\pi,c)\\Bigg)\\widetilde{d}_{m}^{h}(s,a;\\pi,c)}\\end{array}$ ,and $p_{m}^{h}(c,\\pi_{m,c}^{h,s,a})$ is the probability of $p_{m}^{h}(c)$ on $\\pi_{m,c}^{h,s,a}$ . The guarantee Eq. (4) also holds replacing $D_{\\mathrm{H}}(P_{\\star}^{h}(s,a;c),\\widehat{P}_{m}^{h}(s,a;c))$ with $D_{\\mathrm{H}}(R_{\\star}^{h}(s,a;c),\\widehat{R}_{m}^{h}(s,a;c))$ on both sides of the inequality. ", "page_idx": 7}, {"type": "text", "text": "4 Regret Analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we prodive a proof sketch of the regret analysis. Detailed proofs are deferred to Appendix E. We first aggregate the component-wise guarantees (Lemmas 3.1 to 3.5) from Section 3 to present the following epoch-wise guarantee. ", "page_idx": 7}, {"type": "text", "text": "Lemma 4.1. For any $m,$ , any policies $\\{\\pi_{c}\\}_{c\\in c}$ , and $\\delta\\in(0,1/2)$ , with probability at least $1-\\delta/M$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}_{c\\sim\\mathcal{D}}\\left[\\left|\\widehat{V}_{m}^{1}(\\pi_{c},c)-V_{\\star}^{1}(\\pi_{c},c)\\right|\\right]\\leq\\frac{1}{20}\\mathbb{E}_{c\\sim\\mathcal{D}}\\left[\\widehat{\\mathrm{reg}}_{m-1}(\\pi_{c},c)\\right]+77e\\sqrt{H^{6}S^{4}A^{3}\\cdot{\\mathcal{E}}_{m}}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Proof sketch of Lemma 4.1. For simplicity, in this proof sketch, we assume the true reward distribution is known3. For this, we first apply the celebrated local simulation lemma (Lemma C.2) in reinforcement learning to relate the divergence of the value functions to the stepwise divergences as the following. Under any context $c$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\left|\\widehat{V}_{m}^{1}(\\pi_{c},c)-V_{\\star}^{1}(\\pi_{c},c)\\right|\\leq\\sum_{h,s,a}\\widehat{d}_{m}^{h}(s,a;\\pi_{c},c)D_{\\mathrm{H}}\\Bigl(\\widehat{P}_{m}^{h}(s,a;c),P_{\\star}^{h}(s,a;c)\\Bigr).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Then we can exchange the estimated occupancy measure $\\widehat{d}_{m}^{h}(s,a;\\pi_{c},c)$ by the trusted occupancy measure through Lemma 3.3, that is, ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{d}_{m}^{h}(s,a;\\pi_{c},c)D_{\\mathrm{H}}\\Big(\\widehat{P}_{m}^{h}(s,a;c),P_{\\star}^{h}(s,a;c)\\Big)\\leq\\widehat{d}_{m}^{h}(s,a;\\pi_{c},c)D_{\\mathrm{H}}\\Big(\\widehat{P}_{m}^{h}(s,a;c),P_{\\star}^{h}(s,a;c)\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\,32e\\sqrt{H^{4}S^{2}A\\cdot\\mathcal{E}_{m}}+\\widehat{\\mathrm{reg}}_{m-1}(\\pi;c)/(90H S A).}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Then by coverage guarantee Lemma 3.5, for any $h,s,a$ , we can bound ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{d}_{m}^{h}(s,a;\\pi_{c},c)D_{\\mathrm{H}}\\!\\left(\\widehat{P}_{m}^{h}(s,a;c),P_{\\star}^{h}(s,a;c)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\frac{\\gamma_{m}}{e^{2}H}\\cdot p_{m}^{h}(c,\\pi_{m,c}^{h,s,a})\\widetilde{d}_{m}^{h}(s,a;\\pi_{m,c}^{h,s,a},c)\\cdot D_{\\mathrm{H}}^{2}\\!\\left(P_{\\star}^{h}(s,a;c),\\widehat{P}_{m}^{h}(s,a;c)\\right)+\\mathcal{E}_{m}^{\\prime}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "If the assumption in Lemma 3.4 is satisfied, then by Lemma 3.4 and the definition of $p_{m}^{h}$ , we have ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{h,s,a}\\frac{\\gamma_{m}}{e^{2}H}\\cdot p_{m}^{h}(c,\\pi_{m,c}^{h,s,a})\\widehat{d}_{m}^{h}(s,a;\\pi_{m,c}^{h,s,a},c)\\cdot D_{\\mathrm{H}}^{2}(P_{\\star}^{h}(s,a;c),\\widehat{P}_{m}^{h}(s,a;c))}}\\\\ &{}&{\\leq\\sum_{h,s,a}\\frac{\\gamma_{m}}{H}\\cdot p_{m}^{h}(c,\\pi_{m,c}^{h,s,a})d_{m}^{h}(s,a;\\pi_{m,c}^{h,s,a},c)\\cdot D_{\\mathrm{H}}^{2}(P_{\\star}^{h}(s,a;c),\\widehat{P}_{m}^{h}(s,a;c))}\\\\ &{}&{\\leq\\frac{\\gamma_{m}}{H}\\cdot\\sum_{h}\\mathbb{E}_{\\pi\\sim p_{m}^{h}(c)}\\left[\\mathbb{E}^{M_{\\star},\\pi,c}\\left[D_{\\mathrm{H}}^{2}\\Big(\\widehat{P}_{m}^{h}(s_{1}^{h},a_{1}^{h};c),P_{\\star}^{h}(s_{1}^{h},a_{1}^{h};c)\\Big)\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "3The uncertainty in reward distribution is not the main hardness in this problem. Specifically, our proof extends to offilne oracles with squared loss guarantees between the mean rewards for divergence between the reward distributions as in [8]. ", "page_idx": 7}, {"type": "text", "text": "If the assumptions in Lemma 3.4 are not satisfied, we have similar control as well (see the full proof Appendix $\\boldsymbol{\\mathrm E}$ for details). Altogether with taking expectation on $c$ , by the offline density estimation bound from Lemma 3.2, we have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{c\\sim\\mathcal{D}}\\bigg[\\Big|\\widehat{V}_{m}^{1}(\\pi_{c},c)-V_{\\star}^{1}(\\pi_{c},c)\\Big|\\Big]\\leq\\mathbb{E}\\big[\\widehat{\\mathrm{reg}}_{m-1}(\\pi_{c},c)\\big]/40+39e\\sqrt{H^{6}S^{4}A^{3}\\cdot\\mathcal{E}_{m}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "A revised version of the regret analysis (Lemma E.1) in Simchi-Levi and $\\mathrm{\\DeltaXu}$ [36], which relates the epoch-wise guarantee to the regret estimation error, can be found in Appendix E. Combining Lemmas 4.1 and E.1, we arrive at the following general regret guarantee. ", "page_idx": 8}, {"type": "text", "text": "Theorem 4.1. The outputs $\\{\\pi_{t}\\}_{t\\in[T]}$ of Algorithm $^{\\,l}$ satisfies with probability at least $1-\\delta$ that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathrm{Reg}(T)\\lesssim\\sum_{m=1}^{N}(\\tau_{m}-\\tau_{m-1})\\cdot\\sqrt{H^{8}S^{4}A^{3}\\cdot\\mathcal{E}_{m}}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "for $\\delta\\in(0,1/2)$ . ", "page_idx": 8}, {"type": "text", "text": "5 Extension: Reward-free Reinforcement Learning for CMDPs ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we introduce the application of Algorithm 1 in the task of reward-free reinforcement learning in (stochastic) CMDPs. All proofs in this section will be deferred to Appendix F. ", "page_idx": 8}, {"type": "text", "text": "Reward-free reinforcement learning [22]. Reward-free reinforcement learning aims to efficiently explore the environment without relying on observed rewards. By doing so, it aims to enable the computation of a nearly optimal policy for any given reward function, utilizing only the trajectory data collected during exploration and without needing further interaction with the environment. This approach holds particular significance in scenarios where reward functions are refined over multiple iterations to encourage specific behaviors through trial and error, such as in constrained RL formulations. In such cases, repeatedly applying the same RL algorithm with varying reward functions can prove to be highly inefficient regarding sample usage, underscoring the efficiency of reward-free reinforcement learning. ", "page_idx": 8}, {"type": "text", "text": "Problem formulation. The major differences between the regret minimization setting (Section 2.1) and the reward-free reinforcement learning are that in the latter, no reward signals are observed during the interaction, and the goal of the latter is to output a CMDP predictionM whose value functions are close to the underlying true CMDP $M_{\\star}$ for any reward distributions. To  accommodate such a change, for any model $M\\doteq\\{\\bar{P}_{M}^{h},R_{M}^{h}\\}_{h\\in[H]}$ and reward distribution $R=\\{R^{h}\\}_{h\\in[H]}$ , we define $M(\\cdot;R)\\,{=}$ $\\{P_{M}^{h},R^{h}\\}_{h\\in[H]}\\;=\\;\\{P_{M}^{h}(c),R^{h}(\\dot{c})\\}_{h\\in[H],c\\in{\\mathcal{C}}}$ to be model $M$ with the reward distribution part replaced by $\\dot{R}$ . Thus in the reward-free reinforcement learning setting, the underlying true model satisfies $M_{\\star}=M_{\\star}(\\cdot;0)$ where 0 is used to denote the reward distribution that is constantly 0. ", "page_idx": 8}, {"type": "text", "text": "For any model $M$ reward distribution $R$ , context $c$ and policy $\\pi$ , we use $M(\\pi,c;R)$ to denote the distribution of the trajectory $c_{1},\\pi_{1},s_{1}^{1},a_{1}^{1},r_{1}^{1},\\ldots,s_{1}^{H},\\dot{a_{1}^{H}},r_{1}^{\\bar{H}}$ given $M_{\\star}\\,=\\,M(\\cdot;R)$ , $c_{1}=c$ , and $\\pi_{1}=\\pi$ . Also denote the probability and the expectation under $M(\\pi,c;R)$ to be $\\mathbb{P}^{M,\\pi,c,R}(\\cdot)$ and $\\mathbb{E}^{M,\\pi,c,R}[\\cdot]$ respectively. Given reward distribution $R$ , any policy $\\pi$ , state $s$ and action $a$ , we define the action value function $Q_{\\star}^{h}(s,a;\\pi,c,R)$ at layer $h$ and the value function $V_{\\star}^{1}(s;\\pi,c,R)$ at layer $h$ under context $c$ and policy $\\pi$ as ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathcal{I}_{\\star}^{h}(s,a;\\pi,c,R)=\\sum_{j=h}^{H}\\mathbb{E}^{M_{\\star},\\pi,c,R}[r_{1}^{j}\\ |\\ s_{1}^{h},a_{1}^{h}=s,a]\\ \\mathrm{~and~}\\ V_{\\star}^{h}(s;\\pi,c,R)=\\operatorname*{max}_{a\\in\\mathcal{A}}Q_{\\star}^{h}(s,a;\\pi,c,R).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We denote the optimal policy with reward distribution $R$ under context $c$ as $\\pi_{\\star,c,R}$ and abbreviate its value function as $V_{\\star}^{h}(\\cdot;c,R)$ . For $h=1$ , we denote $V_{\\star}^{1}(c,R)=V_{\\star}^{1}(s^{1};c,R)$ and $V_{\\star}^{1}(\\pi,c,R)=$ $V_{\\star}^{1}(s^{1};\\pi,c,R)$ . We also denote ${V}_{M}^{h}$ as the value functions when $M_{\\star}=M$ . ", "page_idx": 8}, {"type": "text", "text": "Assumption 5.1 (Realizability for reward-free RL). Suppose the learner is given a model class $\\mathcal{M}$ that contains the underlying true model $M_{\\star}$ . Assume all models $M\\in\\mathcal{M}$ have 0 reward. ", "page_idx": 8}, {"type": "text", "text": "For a given $\\varepsilon,\\delta>0$ and a model class $\\mathcal{M}$ , the goal of the learner is to output a modelM at the end of the interaction such that for any reward distribution $R$ and set of policies $\\bar{\\{\\pi_{c}\\}}_{c\\in c}$ , the  model satisfies ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{c\\sim\\mathcal{D}}\\left[\\left|V_{\\star}^{1}(\\pi_{c},c,R)-V_{\\widehat{M}}^{1}(\\pi_{c},c,R)\\right|\\right]\\leq\\varepsilon}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "with probability at least $1-\\delta$ . An algorithm that achieves this objective is called $(\\varepsilon,\\delta)$ -learns the model class $\\mathcal{M}$ . Then we have the following guarantee from Algorithm 1. ", "page_idx": 8}, {"type": "text", "text": "Theorem 5.1. If we choose $\\tau_{1}=T/(2H)$ and $\\tau_{2}=T/H$ , the outputs $\\widehat{M}_{2}$ of Algorithm 1 satisfies the reward-free objective Eq. (5) with probability at least $1-\\delta$ , with $T$ at most bounded by ", "page_idx": 9}, {"type": "equation", "text": "$$\nT\\le{\\cal O}\\big(H^{7}S^{4}A^{3}\\log(|\\mathcal{M}|/\\delta)/\\varepsilon^{2}\\big)\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "for $\\delta\\in(0,1/2)$ . Moreover, the algorithm requires $O(H)$ number of oracle calls to the $\\mathsf{M L E}_{\\mathcal{M}}$ oracle. The proof follows a similar argument of Lemma 4.1. In addition, we have a matching lower bound up to a poly $(H,S,A)$ factor adapted from the non-contextual lower bound from Jin et al. [22]. ", "page_idx": 9}, {"type": "text", "text": "Theorem 5.2. Fix $\\varepsilon\\leq1$ , $\\delta\\leq1/2$ , $H,A\\ge2$ . Suppose $S\\geq L\\log A$ for a large enough universal constant $L$ and $K\\ge0$ large enough. Then, there exists a CMDP class $\\mathcal{M}$ with $|S|=S$ , $|{\\mathcal{A}}|=A$ , $|{\\mathcal{M}}|=K$ , and horizon $H$ and a distribution $\\mu$ on $\\mathcal{M}$ such that any algorithm Alg that $(\\varepsilon/24,\\delta)$ - learns the class $\\mathcal{M}$ satisfies $\\mathbb{E}_{M\\sim\\mu}\\mathbb{E}_{M,\\mathsf{A l g}}[T]\\gtrsim\\log|\\mathcal{M}|/\\varepsilon^{2}$ , where $T$ is the number of trajectories required by the algorithm $\\mathsf{A l g}$ to achieve $(\\varepsilon/24,\\delta)$ accuracy and $\\mathbb{E}_{M,\\mathsf{A l g}}[\\cdot]$ is the expectation under the interaction between the algorithm $\\mathsf{A l g}$ and model $M$ . ", "page_idx": 9}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Extension to low rank CMDPs. Low rank MDPs represent a significant extension to tabular MDPs, as explored in various studies [6, 28, 21, 4]. Linear MDPs are typically the first step beyond tabular MDPs. Extending our approach to linear CMDPs would be a substantial achievement. The primary challenge lies in identifying the trusted transitions within linear CMDPs. The current construction for tabular CMDPs does not readily apply here because it does not utilize the low-rank structure. ", "page_idx": 9}, {"type": "text", "text": "Extension to model-free learning. Our approach is model-based. However, model-free methods are often more practical for real-world applications. The main challenge lies in effectively balancing exploration and exploitation using only the value functions, as opposed to our method which depends on the occupancy measure. ", "page_idx": 9}, {"type": "text", "text": "More efficient oracles. In this paper, we focus on offline density estimation oracles due to the necessity of a small Hellinger distance between the estimated model and the true model for our approach. An offline regression oracle would only provide a 2-norm distance guarantee, which is inadequate for our purposes. Nevertheless, it is interesting to explore whether a reduction from CMDPs to offline regression could be feasible. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "JQ acknowledges support from ARO through award W911NF-21-1-0328 and from the Simons Foundation and NSF through award DMS-2031883. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Alekh Agarwal. Selective sampling algorithms for cost-sensitive multiclass prediction. In International Conference on Machine Learning, pages 1220\u20131228. PMLR, 2013.   \n[2] Alekh Agarwal, Miroslav Dud\u00edk, Satyen Kale, John Langford, and Robert E Schapire. Contextual bandit learning with predictable rewards. In Artificial Intelligence and Statistics, 2012.   \n[3] Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming the monster: A fast and simple algorithm for contextual bandits. In International Conference on Machine Learning, pages 1638\u20131646, 2014.   \n[4] Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. FLAMBE: Structural complexity and representation learning of low rank MDPs. Neural Information Processing Systems (NeurIPS), 2020.   \n[5] Philip Amortila, Dylan J. Foster, and Akshay Krishnamurthy. Scalable online exploration via coverability, 2024.   \n[6] Steven J Bradtke and Andrew G Barto. Linear least-squares algorithms for temporal difference learning. Machine learning, 22(1):33\u201357, 1996.   \n[7] Abraham Charnes and William W Cooper. Programming with linear fractional functionals. Naval Research logistics quarterly, 9(3-4):181\u2013186, 1962. [8] Fan Chen, Song Mei, and Yu Bai. Unified algorithms for rl with decision-estimation coefficients: No-regret, pac, and reward-free learning. arXiv preprint arXiv:2209.11745, 2022. [9] Jinglin Chen, Aditya Modi, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal. On the statistical efficiency of reward-free exploration in non-linear rl. Advances in Neural Information Processing Systems, 35:20960\u201320973, 2022.   \n[10] Yuan Cheng, Ruiquan Huang, Jing Yang, and Yingbin Liang. Improved sample complexity for reward-free reinforcement learning under low-rank mdps. arXiv preprint arXiv:2303.10859, 2023.   \n[11] Junze Deng, Yuan Cheng, Shaofeng Zou, and Yingbin Liang. Sample complexity characterization for linear contextual mdps. arXiv preprint arXiv:2402.02700, 2024.   \n[12] Simon S Du, Yuping Luo, Ruosong Wang, and Hanrui Zhang. Provably efficient Q-learning with function approximation via distribution shift error checking oracle. In Advances in Neural Information Processing Systems, pages 8060\u20138070, 2019.   \n[13] Miroslav Dudik, Daniel Hsu, Satyen Kale, Nikos Karampatziakis, John Langford, Lev Reyzin, and Tong Zhang. Efficient optimal learning for contextual bandits. In Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence, pages 169\u2013178. AUAI Press, 2011.   \n[14] Dylan J Foster and Alexander Rakhlin. Beyond UCB: Optimal and efficient contextual bandits with regression oracles. International Conference on Machine Learning (ICML), 2020.   \n[15] Dylan J Foster, Alekh Agarwal, Miroslav Dud\u00edk, Haipeng Luo, and Robert E. Schapire. Practical contextual bandits with regression oracles. International Conference on Machine Learning, 2018.   \n[16] Dylan J Foster, Sham M Kakade, Jian Qian, and Alexander Rakhlin. The statistical complexity of interactive decision making. arXiv preprint arXiv:2112.13487, 2021.   \n[17] Dylan J Foster, Alexander Rakhlin, Ayush Sekhari, and Karthik Sridharan. On the complexity of adversarial decision making. Advances in Neural Information Processing Systems, 35: 35404\u201335417, 2022.   \n[18] Dylan J Foster, Yanjun Han, Jian Qian, and Alexander Rakhlin. Online estimation via offline estimation: An information-theoretic framework. arXiv preprint arXiv:2404.10122, 2024.   \n[19] Assaf Hallak, Dotan Di Castro, and Shie Mannor. Contextual markov decision processes, 2015.   \n[20] Pihe Hu, Yu Chen, and Longbo Huang. Towards minimax optimal reward-free reinforcement learning in linear mdps. In The Eleventh International Conference on Learning Representations, 2022.   \n[21] Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contextual decision processes with low Bellman rank are PAC-learnable. In International Conference on Machine Learning, pages 1704\u20131713, 2017.   \n[22] Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu. Reward-free exploration for reinforcement learning. In International Conference on Machine Learning, pages 4870\u20134879. PMLR, 2020.   \n[23] Yin Tat Lee and Aaron Sidford. Solving linear programs with sqrt(rank) linear system solves. arXiv preprint arXiv:1910.08033, 2019.   \n[24] Orin Levy and Yishay Mansour. Optimism in face of a context: Regret guarantees for stochastic contextual mdp, 2023.   \n[25] Orin Levy, Asaf Cassel, Alon Cohen, and Yishay Mansour. Eluder-based regret for stochastic contextual mdps. arXiv preprint arXiv:2211.14932, 2022.   \n[26] Orin Levy, Alon Cohen, Asaf Cassel, and Yishay Mansour. Efficient rate optimal regret for adversarial contextual mdps using online function approximation. In International Conference on Machine Learning, pages 19287\u201319314. PMLR, 2023.   \n[27] Gen Li, Yuling Yan, Yuxin Chen, and Jianqing Fan. Optimal reward-agnostic exploration in reinforcement learning. 2023.   \n[28] Francisco S Melo and M Isabel Ribeiro. Q-learning with linear function approximation. In International Conference on Computational Learning Theory, pages 308\u2013322. Springer, 2007.   \n[29] Zakaria Mhammedi, Dylan J Foster, and Alexander Rakhlin. Representation learning with multi-step inverse kinematics: An efficient and optimal approach to rich-observation rl. In International Conference on Machine Learning, pages 24659\u201324700. PMLR, 2023.   \n[30] Zakaria Mhammedi, Adam Block, Dylan J. Foster, and Alexander Rakhlin. Efficient model-free exploration in low-rank mdps, 2024.   \n[31] Sobhan Miryoosef iand Chi Jin. A simple reward-free approach to constrained reinforcement learning. In International Conference on Machine Learning, pages 15666\u201315698. PMLR, 2022.   \n[32] Aditya Modi and Ambuj Tewari. Contextual markov decision processes using generalized linear models. arXiv preprint arXiv:1903.06187, 2019.   \n[33] Aditya Modi, Nan Jiang, Satinder Singh, and Ambuj Tewari. Markov decision processes with continuous side information. In Algorithmic Learning Theory, pages 597\u2013618. PMLR, 2018.   \n[34] Yury Polyanskiy and Yihong Wu. Lecture notes on information theory. Lecture Notes for ECE563 (UIUC) and, 6(2012-2016):7, 2014.   \n[35] Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 2014.   \n[36] David Simchi-Levi and Yunzong Xu. Bypassing the monster: A faster and simpler optimal algorithm for contextual bandits under realizability. Mathematics of Operations Research, 2021.   \n[37] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.   \n[38] Andrew Wagenmaker, Max Simchowitz, and Kevin Jamieson. Beyond no regret: Instancedependent pac reinforcement learning, 2022.   \n[39] Ruosong Wang, Simon S Du, Lin Yang, and Russ R Salakhutdinov. On reward-free reinforcement learning with linear function approximation. Advances in neural information processing systems, 33:17816\u201317826, 2020.   \n[40] Yunbei Xu and Assaf Zeevi. Upper counterfactual confidence bounds: a new optimism principle for contextual bandits. arXiv preprint arXiv:2007.07876, 2020.   \n[41] Yunbei Xu and Assaf Zeevi. Upper counterfactual confidence bounds: a new optimism principle for contextual bandits, 2024.   \n[42] Xuezhou Zhang, Yuda Song, Masatoshi Uehara, Mengdi Wang, Alekh Agarwal, and Wen Sun. Efficient reinforcement learning in block mdps: A model-free representation learning approach. In International Conference on Machine Learning, pages 26517\u201326547. PMLR, 2022.   \n[43] Zihan Zhang, Simon S Du, and Xiangyang Ji. Nearly minimax optimal reward-free reinforcement learning. arXiv preprint arXiv:2010.05901, 2020.   \n[44] Zihan Zhang, Yuhang Jiang, Yuan Zhou, and Xiangyang Ji. Near-optimal regret bounds for multi-batch reinforcement learning. Advances in Neural Information Processing Systems, 35: 24586\u201324596, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Notation ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "For any integer $n$ , we use $[n]$ to denote the set $\\{1,\\ldots,n\\}$ . For any set $\\mathcal{X}$ , we use $\\Delta(\\mathcal{X})$ to denote the set of all distributions on the set $\\mathcal{X}$ . We define $O(\\cdot),\\Omega(\\cdot),\\Theta(\\cdot),\\widetilde{O}(\\cdot),\\widetilde{\\Omega}(\\cdot),\\widetilde{\\Theta}(\\cdot)$ following standard non-asymptotic big-oh notation. We use the binary relation $x\\lesssim y$ to indicate that $x\\leq O(y).\\ \\mathbb{1}(\\mathcal{E})$ is an indicator function of event $\\mathcal{E}$ . ", "page_idx": 12}, {"type": "text", "text": "B Related Works ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Contextual bandits and contextual MDPs. The SquareCB [14] obtains optimal regret for contextual bandits with access to an online regression oracle. This is extended to the CMDPs by Foster et al. [16] with the E2D algorithm. However, the algorithm requires ${\\cal O}(T)$ called to an online density estimation oracle. Compared to our algorithm, it necessitates significantly more calls to an oracle that is harder to implement for a general model class $\\mathcal{M}$ . After the FALCON algorithm [36] establishes the reduction from contextual bandits to offline regression, $\\mathrm{Xu}$ and Zeevi [41] proposed the UCCB algorithm, which is less oracle-efficient in terms of oracle calls but adopts the prevalent \"optimism in the face of uncertainty\" principle and is thus easier to generalize. More specifically, the UCCB algorithm is extended to CMDPs by Levy and Mansour [24], Deng et al. [11] with assumptions on the model class. The RM-UCDD algorithm proposed by Levy and Mansour [24] requires the model class to have a minimum reachability $p_{\\mathrm{{min}}}$ to all states under any policy and the regret guarantees scale with $O(\\mathrm{poly}(H,S,A)\\cdot(1/p_{\\mathrm{min}})\\sqrt{T\\log|\\mathcal{M}|})$ . This assumption precludes model classes with small reachability, which frequently happens in practice [4]. The CMDP-VR algorithm proposed by Deng et al. [11] assumes a varying representation assumption on the model class instead. The assumption asserts that any model $\\stackrel{\\smile}{M}=\\dot{\\mathrm{\\large\\{}}P^{h},R^{h}\\mathrm{\\large\\}}_{h\\in[H]}\\in\\dot{\\mathcal{M}}$ satisfies for any $c,h,s,a,s^{\\prime},P^{h}(s^{\\prime}|s,a;c)=\\langle\\phi^{h}(s,a;c),\\mu^{h}(s^{\\prime})\\rangle$ for the known feature vector $\\phi^{h}(s,a;c)\\in\\mathbb{R}^{d}$ and an unknown vector $\\mu^{h}(s^{\\prime})\\in\\mathbb{R}^{d}$ which does not depend on the context $c$ . This assumption is stringent because canonically, the feature vector for CMDPs will be chosen to be the unit vector indexed by $s,a$ in $\\mathbb{R}^{S A}$ , i.e., $\\phi^{h}(s,a;c)=e_{s,a}\\in\\mathbb{R}^{S A}$ . Then, the requirement of $\\mu^{h}(s^{\\prime})$ not depending on the context forces $P^{h}$ to not depend on the context as well. This reduces the CMDP to an MDP. While it is possible to complicate the feature vector to not reduce to an MDP, this would result in a higher dimension in the feature vector space, which will be reflected in the regret bounds obtained $(\\tilde{\\cal O}(\\mathrm{poly}(H,d)\\sqrt{T\\log|{\\mathcal M}|})$ . Another significant disadvantage compared with our algorithm is that t he RM-UCDD and CMDP-VR algorithm requires ${\\cal O}(T)$ number of oracle calls. Other structural assumptions are explored by Modi and Tewari [32], Levy et al. [25]. ", "page_idx": 12}, {"type": "text", "text": "Reward-free reinforcement learning. Reward-free reinforcement learning aims to efficiently explore the environment without relying on observed rewards. By doing so, it aims to enable the computation of a nearly optimal policy for any given reward function, utilizing only the trajectory data collected during exploration and without needing further interaction with the environment. This framework is proposed by [22] and has been extensively studied for MDPs with various assumptions [43, 4, 39, 42, 9, 31, 38, 20, 10, 27, 30, 5]. However, to the best of our knowledge, we are the first to study the reward-free reinforcement learning setting for stochastic CMDPs. We provide a near-optimal sample complexity upper bound and a matching lower bound up to a po $\\operatorname{\\bar{\\operatorname{Jy}}}(H,S,A)$ factor with only $O(H)$ number of oracle calls. Nevertheless, the upper bound is obtained by adjusting the exploration-exploitation, highlighting the flexibility of our algorithm. ", "page_idx": 12}, {"type": "text", "text": "C Technical Tools ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "C.1 Maximum Likelihood Estimation for Density Estimation ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Example C.1 (MLE for finite model class). Let $\\mathcal{M}$ be a finite model class and the MLE estimator $\\widehat{M}$ be defined by ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\widehat{M}=\\operatorname*{arg\\,max}_{M\\in\\mathcal{M}}\\prod_{i=1}^{n}\\mathbb{P}^{M,\\pi_{i},c_{i}}\\left(\\{s_{i}^{h},a_{i}^{h},r_{i}^{h}\\}_{h\\in[H]}\\right).\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "For any $\\delta\\in(0,1/2)$ , we have with probability at least $1-\\delta$ , ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathbb{E}_{c\\sim\\mathcal{D},\\pi\\sim p(c)}\\Big[D_{\\mathrm{H}}^{2}\\Big(\\widehat{M}(\\pi,c),M_{\\star}(\\pi,c)\\Big)\\Big]\\lesssim\\frac{\\log(|\\mathcal{M}|/\\delta)}{n}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof of Example C.1. For any $\\delta\\,\\in\\,(0,1/2)$ , by Lemma C.2 of Foster et al. [18], we have with probability at least $1-\\delta/2$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{n}D_{\\mathrm{H}}^{2}\\Big(\\widehat{M}(\\pi_{i},c_{i}),M_{\\star}(\\pi_{i},c_{i})\\Big)\\lesssim\\log(|\\mathcal{M}|/\\delta).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then by Lemma A.3 of Foster et al. [16], we have with probability at least $1-\\delta/2$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}_{c\\sim\\mathcal{D},\\pi\\sim p(c)}\\left[D_{\\mathrm{H}}^{2}\\Big(\\widehat{M}(\\pi,c),M_{\\star}(\\pi,c)\\Big)\\right]\\lesssim\\sum_{i=1}^{n}D_{\\mathrm{H}}^{2}\\Big(\\widehat{M}(\\pi_{i},c_{i}),M_{\\star}(\\pi_{i},c_{i})\\Big)+\\log(|\\mathcal{M}|/\\delta).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then by union bound, we obtain the desired result. ", "page_idx": 13}, {"type": "text", "text": "C.2 Information Theory ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Lemma C.1 (Lemma B.4 of Foster et al. [17]). Let $\\mathbb{P}$ and $\\mathbb{Q}$ be two distributions on space $\\chi$ . Let $h:\\chi\\to R$ be a function. Then we have: ", "page_idx": 13}, {"type": "equation", "text": "$$\n|\\mathbb{E}_{\\mathbb{P}}[h]-\\mathbb{E}_{\\mathbb{Q}}[h]|\\leq\\sqrt{2^{-1}(\\mathbb{E}_{\\mathbb{P}}[h^{2}]+\\mathbb{E}_{\\mathbb{Q}}[h^{2}])D_{H}^{2}(\\mathbb{P},\\mathbb{Q})}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "C.3 Reinforcement Learning ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Lemma C.2 (Lemma F.3 of [16]). Let $M=\\{P_{M}^{h},R_{M}^{h}\\}_{h\\in[H]}$ and $\\overline{{M}}=\\{P_{\\overline{{M}}}^{h},R_{\\overline{{M}}}^{h}\\}_{h\\in[H]}$ be two CMDPs. For any policy $\\pi$ and context $c$ , we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle V_{M}^{1}(\\pi,c)-V_{M}^{1}(\\pi,c)}\\\\ {\\displaystyle=\\sum_{h=1}^{H}\\mathbb{E}^{\\overline{{M}},\\pi,c}\\big[\\big(P_{M}^{h}(s_{1}^{h+1}|s_{1}^{h},a_{1}^{h};c)-P_{M}^{h}(s_{1}^{h+1}|s_{1}^{h},a_{1}^{h};c)\\big)V_{M}^{h+1}(s_{1}^{h+1};\\pi,c)\\big]}\\\\ {\\displaystyle\\ \\ \\ +\\sum_{h=1}^{H}\\mathbb{E}^{\\overline{{M}},\\pi,c}\\big[\\mathbb{E}_{r^{h}\\sim R_{M}(s_{1}^{h},a_{1}^{h};c)}[r^{h}]-\\mathbb{E}_{r^{h}\\sim R_{M}(s_{1}^{h},a_{1}^{h};c)}[r^{h}]\\big]}\\\\ {\\displaystyle\\leq\\sum_{h=1}^{H}\\sum_{s,a}\\mathbb{E}^{\\overline{{M}},\\pi,c}\\big[\\mathbb{I}(s_{1}^{h},a_{1}^{h}=s,a)\\big]\\big(D_{\\mathrm{H}}\\big(P_{M}^{h}(s,a;c),P_{\\overline{{M}}}^{h}(s,a;c)\\big)+D_{\\mathrm{H}}\\big(R_{M}^{h}(s,a;c),R_{\\overline{{M}}}^{h}(s,a)\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "D Proofs from Section 3 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we present the proofs for Lemmas 3.1 to 3.5. ", "page_idx": 13}, {"type": "text", "text": "Proof of Lemma 3.1. We fix an arbitrary context $c$ throughout the proof. Let $u(\\lambda)\\;:=\\;\\;$ $\\begin{array}{r}{\\sum_{\\pi\\in\\Pi_{m}^{h}}1/(\\lambda\\,+\\,\\eta_{m}\\,\\cdot\\,\\widehat{\\mathrm{reg}}_{m-1}(\\pi))}\\end{array}$ . Since for any $\\textit{b}\\in\\:[H]$ , $\\widehat{\\pi}_{m-1,c}\\;\\;\\in\\;\\;\\Pi_{m}^{h}$ , then $u(\\lambda)~\\geqq$ $1/(\\lambda\\,+\\,\\eta_{m}\\,\\cdot\\,\\widehat{\\mathrm{reg}}_{m-1}(\\widehat{\\pi}_{m-1,c}))\\;=\\;1/\\lambda$ . On the other hand, $u(\\lambda)\\;\\leq\\;(S A+1)/\\lambda$ . Moreover, $u(\\lambda)$ is clearl y  monoto nically decreasing with $u(0)=\\infty$ and $u(S A+1)\\leq1$ . Thus there exists $\\lambda_{m,c}^{h}\\in(0,S A+1]$ such that $u(\\lambda_{m,c}^{h})=1$ as we desired. ", "page_idx": 13}, {"type": "text", "text": "Now we have the regret is bounded by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\pi\\sim p_{m}^{h}(c)}\\big[\\widehat{\\mathrm{reg}}_{m-1}(\\pi;c)\\big]=\\sum_{\\pi\\in\\Pi_{m,c}^{h}}\\frac{\\widehat{\\mathrm{reg}}_{m-1}(\\pi;c)}{\\lambda_{m,c}^{h}+\\eta_{m}\\cdot\\widehat{\\mathrm{reg}}_{m-1}(\\pi;c)}\\leq\\sum_{\\pi\\in\\Pi_{m}^{h}(c)}\\frac{1}{\\eta_{m}}\\leq\\frac{2S A}{\\eta_{m}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Finally, we recall $\\eta_{m}=\\gamma_{m}/(720e^{3}H^{5}S^{3}A^{2})$ and $\\begin{array}{r}{\\gamma_{m}=\\sqrt{\\frac{H^{6}S^{4}A^{3}}{\\mathcal{E}_{m}}}}\\end{array}$ plug this bound in, then we have ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\pi\\sim p_{m}^{h}(c)}\\left[\\widehat{\\mathrm{reg}}_{m-1}(\\pi;c)\\right]\\leq1440e^{3}\\cdot\\sqrt{H^{4}S^{4}A^{3}\\cdot\\mathcal{E}_{m}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Proof of Lemma 3.2. By definition of Definition 2.1. ", "page_idx": 13}, {"type": "text", "text": "Proof of Lemma 3.3. For any $m,\\pi,c,h,s,a$ , by the definition of $\\widetilde{d}_{m}^{h}(s,a;\\pi,c)$ , we the difference between $\\widehat{d}_{m}^{h}(s,a;\\pi,c)$ and $\\widetilde{d}_{m}^{h}(s,a;\\pi,c)$ are the parts of occupancy measures that do not go through the trust ed transitions, i.e., ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\widehat{d}_{m}^{h}(s,a;\\pi,c)-\\widehat{d}_{m}^{h}(s,a;\\pi,c)}\\\\ {\\displaystyle\\qquad=\\sum_{j=1}^{h-1}\\sum_{(s^{j},a^{j},s^{j+1})\\notin\\widetilde{\\mathcal{T}}_{m}^{j}(c)}\\widetilde{d}_{m}^{j}(s^{j},a^{j};\\pi,c)\\widehat{P}_{m}^{j}(s^{j+1}|s^{j},a^{j};c)\\widehat{P}_{m+1}^{j+1;h}(s|s^{j+1};\\pi,c)\\pi^{h}(s,a),}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "whereP jm++11:h (s $\\widehat{P}_{m+1}^{j+1:h}(s|s^{j+1};\\pi,c)$ is the estimated transition probability from $s^{j+1}$ at step $j+1$ to $s$ at step $h$ under policy $\\pi$ and context $c$ . Then since $(s^{j},a^{j},s^{j+1})\\notin\\widetilde{\\mathcal{T}}_{m}^{j}(c)$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{j=1}{\\overset{h-1}{\\sum}}\\sum_{\\substack{(s^{j},a^{j},s^{j+1})\\notin\\tilde{T}_{m}^{i}(c)}}\\widehat{d}_{m}^{j}(s^{j},a^{j};\\pi,c)\\widehat{P}_{m}^{j}(s^{j+1}|s^{j},a^{j};c)\\widehat{P}_{m+1}^{j+1;h}(s|s^{j+1};\\pi,c)\\pi^{h}(s,a)}\\\\ &{\\overset{h-1}{\\leq}\\underset{j=1}{\\overset{h-1}{\\sum}}\\underset{(s^{j},a^{j},s^{j+1})\\notin\\tilde{T}_{m}^{j}(c)}{\\sum}\\widehat{d}_{m}^{j}(s^{j},a^{j};\\pi,c)\\widehat{P}_{m}^{j}(s^{j+1}|s^{j},a^{j};c)}\\\\ &{\\leq\\underset{j=1}{\\overset{h-1}{\\sum}}\\sum_{\\substack{(s^{j},a^{j},s^{j+1})\\notin\\tilde{T}_{m}^{j}(c)}}\\frac{S A+\\eta_{m}r\\widehat{\\Theta}_{m-1}(\\pi;c)}{\\zeta_{m}}}\\\\ &{\\leq\\underset{j=1}{\\overset{h-2}{\\sum}}(s^{j},a^{j},s^{j+1})\\notin\\tilde{T}_{m}^{j}(c)}\\\\ &{\\leq\\frac{h S^{2}A(S A+\\eta_{m}\\mathsf{r}\\widehat{\\Theta}_{m-1}(\\pi;c))}{\\zeta_{m}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Recall the choice of $\\begin{array}{r}{\\zeta_{m}=\\frac{\\gamma_{m}}{8e H(H+1)^{2}}}\\end{array}$ 8eH(Hm+1)2 , \u03b7m = $\\begin{array}{r}{\\eta_{m}=\\frac{\\gamma_{m}}{720e^{3}H^{5}S^{3}A^{2}}}\\end{array}$ 720e3Hm5S3A2 and \u03b3m = $\\begin{array}{r}{\\gamma_{m}=\\sqrt{\\frac{H^{6}S^{4}A^{3}}{\\mathcal{E}_{m}}}}\\end{array}$ we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\widehat{d}_{m}^{h}(s,a;\\pi,c)-\\widehat{d}_{m}^{h}(s,a;\\pi,c)\\leq32e\\sqrt{H^{4}S^{2}A\\cdot\\mathcal{E}_{m}}+\\frac{1}{90H S A}\\widehat{\\mathrm{reg}}_{m-1}(\\pi;c).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Proof of Lemma 3.4. We prove iteratively between the objective ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\tilde{d}_{m}^{h}(s,a;\\pi,c)\\leq\\bigg(1+{\\frac{1}{H}}\\bigg)^{2(h-1)}d_{m}^{h}(s,a;\\pi,c)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and the following claim: For any $h$ and any $(s,a,s^{\\prime})\\in\\widetilde{T}_{m}^{h}(c)$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\widehat{P}_{m}^{h}(s^{\\prime}|s,a;c)\\leq\\bigg(1+\\frac{1}{H}\\bigg)^{2}P_{\\star}^{h}(s^{\\prime}|s,a;c).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "First, we show that for any $h\\in[H]$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\forall\\pi,s,a,\\,\\widetilde{d}_{m}^{h}(s,a;\\pi,c)\\le\\bigg(1+\\frac{1}{H}\\bigg)^{2(h-1)}d_{m}^{h}(s,a;\\pi,c)}}\\\\ &{}&{\\qquad\\qquad\\Rightarrow\\ \\forall(s,a,s^{\\prime})\\in\\widetilde{\\mathcal{T}}_{m}^{h}(c),\\ \\widehat{\\mathcal{P}}_{m}^{h}(s^{\\prime}|s,a;c)\\le\\bigg(1+\\frac{1}{H}\\bigg)^{2}P_{\\star}^{h}(s^{\\prime}|s,a;c).}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For this, we note by Lemma C.1 that for any $h,s,a,s^{\\prime}$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\overset{\\hat{\\gamma}h}{\\leq}(s^{\\prime}\\vert s,a;c)}\\\\ &{\\leq P_{\\star}^{h}(s^{\\prime}\\vert s,a;c)+\\sqrt{2^{-1}(\\hat{P}_{m}^{h}(s^{\\prime}\\vert s,a;c)+P_{\\star}^{h}(s^{\\prime}\\vert s,a;c))D_{\\mathrm{H}}^{2}\\Big(\\mathrm{Ber}(\\hat{P}_{m}^{h}(s^{\\prime}\\vert s,a;c)),\\mathrm{Ber}(P_{\\star}^{h}(s^{\\prime}\\vert s,a;c))\\Big)^{2}}}\\\\ &{\\leq P_{\\star}^{h}(s^{\\prime}\\vert s,a;c)+\\sqrt{2^{-1}(\\hat{P}_{m}^{h}(s^{\\prime}\\vert s,a;c)+P_{\\star}^{h}(s^{\\prime}\\vert s,a;c))D_{\\mathrm{H}}^{2}\\Big(\\hat{P}_{m}^{h}(s,a;c),P_{\\star}^{h}(s,a;c)\\Big)},\\qquad\\mathrm{~(8)~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where the second inequality is by data-processing inequality [34]. Then by AM-GM, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sqrt{2^{-1}(\\hat{P}_{m}^{h}(s^{\\prime}|s,a;c)+P_{\\star}^{h}(s^{\\prime}|s,a;c))D_{\\mathbf{H}}^{2}\\Big(\\hat{P}_{m}^{h}(s,a;c),P_{\\star}^{h}(s,a;c)\\Big)}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\cfrac{1}{4H}(\\hat{P}_{m}^{h}(s^{\\prime}|s,a;c)+P_{\\star}^{h}(s^{\\prime}|s,a;c))+H\\cdot D_{\\mathbf{H}}^{2}\\Big(\\hat{P}_{m}^{h}(s,a;c),P_{\\star}^{h}(s,a;c)\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Plug the above back into Eq. (8) and reorganize, we obtain ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\widehat{P}_{m}^{h}(s^{\\prime}|s,a;c)\\leq\\bigg(1+\\frac{1}{H}\\bigg)\\cdot P_{\\star}^{h}(s^{\\prime}|s,a;c)+(H+1)D_{\\mathrm{H}}^{2}\\Big(\\widehat{P}_{m}^{h}(s,a;c),P_{\\star}^{h}(s,a;c)\\Big).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then multiply both sides by $\\tilde{d}_{m}^{h}(s,a;\\pi,c)$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{\\gamma}_{m}^{h}(s,a;\\pi,c)\\bigg(\\widehat{P}_{m}^{h}(s^{\\prime}|s,a;c)-\\bigg(1+\\frac{1}{H}\\bigg)P_{\\star}^{h}(s^{\\prime}|s,a;c)\\bigg)\\leq(H+1)\\widehat{d}_{m}^{h}(s,a;\\pi,c)D_{\\mathrm{H}}^{2}\\Big(\\widehat{P}_{m}^{h}(s,a;c),\\log(\\widehat{P}_{m}^{h}(s,a;c))\\Big)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Meanwhile, by the definition of $\\pi_{m,c}^{h,s,a}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\widetilde{d}_{m}^{h}(s,a;\\pi,c)\\leq\\frac{\\widetilde{d}_{m}^{h}(s,a;\\pi_{m,c}^{h,s,a},c)}{S A+\\eta_{m}\\cdot\\widehat{\\mathrm{reg}}_{m-1}(\\pi_{m,c}^{h,s,a},c)}\\cdot(S A+\\eta_{m}\\cdot\\widehat{\\mathrm{reg}}_{m-1}(\\pi,c)).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then by the induction hypothesis, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\widetilde{d}_{m}^{h}(s,a;\\pi_{m,c}^{h,s,a},c)}{S A+\\eta_{m}\\cdot\\widehat{\\mathrm{reg}}_{m-1}(\\pi_{m,c}^{h,s,a},c)}\\cdot\\left(S A+\\eta_{m}\\cdot\\widehat{\\mathrm{reg}}_{m-1}(\\pi,c)\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\frac{e^{2}d_{m}^{h}(s,a;\\pi_{m,c}^{h,s,a},c)}{S A+\\eta_{m}\\cdot\\widehat{\\mathrm{reg}}_{m-1}(\\pi_{m,c}^{h,s,a},c)}\\cdot\\left(S A+\\eta_{m}\\cdot\\widehat{\\mathrm{reg}}_{m-1}(\\pi,c)\\right)\\!.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus we have futher by the definition of $p_{m}^{h}(c)$ and the assumption that $\\begin{array}{r}{\\mathbb{E}_{\\pi\\sim p_{m}^{h}(c)}\\mathbb{E}^{M_{\\star},\\pi,c}\\left[D_{\\mathrm{H}}^{2}\\Big(\\widehat{P}_{m}^{h}\\big(s_{1}^{h},a_{1}^{h};c\\big),P_{\\star}^{h}\\big(s_{1}^{h},a_{1}^{h};c\\big)\\Big)\\right]\\leq H/\\gamma_{m},}\\end{array}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{H+1)\\widehat{d}_{m}^{h}(s,a;\\pi,c)D_{\\mathbf{t}}^{2}\\Big(\\widehat{P}_{m}^{h}(s,a;c),P_{\\mathbf{\\cdot}_{\\mathbf{\\pi}^{\\textit{h}}}}^{h}(s,a;c)\\Big)}\\\\ &{\\quad\\quad\\leq e^{2}(H+1)(S A+\\eta_{m}\\cdot\\widehat{\\mathrm{reg}}_{m-1}(\\pi,c))\\frac{d_{m}^{h}}{S A+\\eta_{m}\\cdot\\widehat{\\mathrm{reg}}_{m-1}(\\pi_{m}^{h,s,a},c)}\\cdot D_{\\mathbf{t}}^{2}\\Big(\\widehat{P}_{m}^{h}(s,a;c),P_{\\mathbf{\\cdot}_{\\mathbf{\\pi}^{\\textit{h}}}}^{h}(s,a;c)\\Big)}\\\\ &{\\quad\\quad\\leq e^{2}(H+1)(S A+\\eta_{m}\\cdot\\widehat{\\mathrm{reg}}_{m-1}(\\pi,c))p_{m}^{h}(c,\\pi_{m}^{h,s,a})d_{m}^{h}(s,a;\\pi_{m-1}^{h,s,a},c))\\mathcal{D}_{\\mathbf{t}}^{2}\\Big(P_{\\mathbf{\\cdot}_{\\mathbf{\\pi}^{\\textit{h}}}}^{h}(s,a;c),\\widehat{P}_{m}^{h}(s,a;c)\\Big)}\\\\ &{\\quad\\quad\\leq e^{2}(H+1)(S A+\\eta_{m}\\cdot\\widehat{\\mathrm{reg}}_{m-1}(\\pi,c))\\mathbb{E}_{\\pi\\sim\\gamma_{m}^{h}(c)}\\mathbb{E}^{M_{\\mathbf{\\cdot}_{\\mathbf{\\pi}^{\\textit{h}}}}(s,a;\\pi,c)}\\left[D_{\\mathbf{H}}^{2}\\Big(\\widehat{P}_{m}^{h}(s_{1}^{h},a_{1}^{h};c),P_{\\mathbf{\\cdot}_{\\mathbf{\\pi}^{\\textit{h}}}}^{h}(s_{1}^{h},a_{1}^{h};c)\\Big)\\right]}\\\\ &{\\quad\\quad\\leq\\frac{e^{2}H(H+1)}{\\gamma_{m}}(S A+\\eta_{m}\\cdot\\widehat{\\mathrm{reg}}_{m-1}(\\pi,c))}\\\\ &{\\quad\\quad=\\frac{1}{(H+1)\\zeta_{m}}(S A+\\eta_{m}\\cdot\\widehat{\\mathrm{reg}}_{m-1}(\\pi,c)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the last equality is by the definition of $\\zeta_{m}$ . In all, we have shown that ", "page_idx": 15}, {"type": "equation", "text": "$$\nr_{m}^{h}(s,a;\\pi,c)\\bigg(\\widehat{P}_{m}^{h}(s^{\\prime}|s,a;c)-\\bigg(1+\\frac{1}{H}\\bigg)P_{\\star}^{h}(s^{\\prime}|s,a;c)\\bigg)\\leq\\frac{1}{(H+1)\\zeta_{m}}(S A+\\eta_{m}\\cdot\\widehat{\\mathrm{reg}}_{m-1}(\\pi,c)).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Now we prove by contradiction, if for any $(s,a,s^{\\prime})\\,\\in\\,\\widetilde{\\mathcal{T}}_{m}^{h}(c)$ , the reverse inequality is true, i.e., $\\begin{array}{r}{\\widehat{P}_{m}^{h}(s^{\\prime}|s,a;c)>\\big(1+\\frac{1}{H}\\big)^{2}P_{\\star}^{h}(s^{\\prime}|s,a;c)}\\end{array}$ . Then for any $\\pi$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{1}{H+1}\\widetilde{d}_{m}^{h}(s,a;\\pi,c)\\widehat{P}_{m}^{h}(s^{\\prime}|s,a;c)<\\widetilde{d}_{m}^{h}(s,a;\\pi,c)\\bigg(\\widehat{P}_{m}^{h}(s^{\\prime}|s,a;c)-\\bigg(1+\\displaystyle\\frac{1}{H}\\bigg)P_{\\star}^{h}(s^{\\prime}|s,a;c)\\bigg)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\displaystyle\\frac{1}{(H+1)\\zeta_{m}}(S A+\\eta_{m}\\cdot\\widehat{\\mathrm{reg}}_{m-1}(\\pi,c)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This contradicts the definition of $\\widetilde{\\mathcal{T}}_{m}^{h}(c)$ . ", "page_idx": 16}, {"type": "text", "text": "For the other direction of Eq. (6), we prove for all $h\\in[H]$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\int\\widehat{P}_{m}^{h}\\big(s^{\\prime}|s,a;c\\big)\\leq\\big(1+\\frac{1}{H}\\big)^{2}P_{\\star}^{h}\\big(s^{\\prime}|s,a;c\\big)\\qquad\\forall(s,a,s^{\\prime})\\in\\widetilde{\\mathcal{T}}_{m}^{h},}\\\\ &{\\displaystyle\\Big\\{\\tilde{d}_{m}^{h}\\big(s,a;\\pi,c\\big)\\leq\\big(1+\\frac{1}{H}\\big)^{2(h-1)}d_{m}^{h}\\big(s,a;\\pi,c\\big)\\quad\\forall\\pi,s,a.}\\\\ &{\\displaystyle\\qquad\\qquad\\qquad\\Rightarrow\\quad\\tilde{d}_{m}^{h+1}(s,a;\\pi,c)\\leq\\bigg(1+\\frac{1}{H}\\bigg)^{2h}d_{m}^{h+1}(s,a;\\pi,c),\\ \\forall\\pi,s,a.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This direction is straightforward since ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l l l}{\\displaystyle\\hat{d}_{m}^{h+1}(s,a;\\pi,c)=\\sum_{(s^{\\prime},a^{\\prime},s)\\in\\tilde{\\mathcal{T}}_{m}^{h}}(\\hat{d}_{m}^{h}(s,a;\\pi,c)\\widehat{P}_{m}^{h}(s^{\\prime}|s,a;c)\\pi^{h+1}(s,a;c)}\\\\ {\\displaystyle}&{\\qquad\\qquad\\leq\\left(1+\\displaystyle\\frac{1}{H}\\right)^{2h}\\sum_{(s^{\\prime},a^{\\prime},s)\\in\\tilde{\\mathcal{T}}_{m}^{h}}d_{m}^{h}(s,a;\\pi,c)P_{\\star}^{h}(s^{\\prime}|s,a;c)\\pi_{h+1}(s,a;c)}\\\\ {\\displaystyle}&{=\\left(1+\\displaystyle\\frac{1}{H}\\right)^{2h}d_{m}^{h+1}(s,a;\\pi,c).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "With the two derivations Eq. (6) and Eq. (9), along with the fact that the initial argument of Eq. (6) holds by defintion for $h=1$ . Thus we conclude the proof. ", "page_idx": 16}, {"type": "text", "text": "Proof of Lemma 3.5. For any $m,\\pi,c,h,s,a,$ , by AM-GM, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{d}_{m}^{h}(s,a;\\pi,c)D_{\\mathrm{H}}\\Big(P_{\\star}^{h}(s,a;c),\\widehat{P}_{m}^{h}(s,a;c)\\Big)}\\\\ &{\\qquad\\qquad\\leq\\frac{2e^{2}H(S A+\\eta_{m}\\widehat{\\mathrm{reg}}_{m-1}(\\pi_{m,c}^{h,s,a};c))}{\\gamma_{m}\\widetilde{d}_{m}^{h}(s,a;\\pi_{m,c}^{h,s,a},c)}\\cdot(\\widetilde{d}_{m}^{h}(s,a;\\pi,c))^{2}}\\\\ &{\\qquad\\qquad+\\,\\frac{\\gamma_{m}\\widetilde{d}_{m}^{h}(s,a;\\pi_{m,c}^{h,s,a};c)}{2e^{2}H(S A+\\eta_{m}\\widehat{\\mathrm{reg}}_{m-1}(\\pi_{m,c}^{h,s,a};c))}\\cdot D_{\\mathrm{H}}^{2}\\Big(P_{\\star}^{h}(s,a;c),\\widehat{P}_{m}^{h}(s,a;c)\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By the definition of $\\pi_{m,c}^{h,s,a}$ , we have futher ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{2e^{2}H(S A+\\eta_{m}\\widehat{\\mathrm{reg}}_{m-1}(\\pi_{m,c}^{h,a};c))}{\\gamma_{m}\\widetilde{d}_{m}^{h}(s,a;\\pi_{m,c}^{h,s,a},c)}\\cdot(\\widehat{d}_{m}^{h}(s,a;\\pi,c))^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{2e^{2}H(S A+\\eta_{m}\\widehat{\\mathrm{reg}}_{m-1}(\\pi;c))}{\\gamma_{m}\\widetilde{d}_{m}^{h}(s,a;\\pi,c)}\\cdot(\\widehat{d}_{m}^{h}(s,a;\\pi,c))^{2}}\\\\ &{\\qquad\\qquad\\qquad=\\frac{2e^{2}H S A+2e^{2}H\\eta_{m}\\widehat{\\mathrm{reg}}_{m-1}(\\pi;c)}{\\gamma_{m}}\\cdot\\widehat{d}_{m}^{h}(s,a;\\pi,c).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Recall the choice of $\\eta_{m}=\\gamma_{m}/(720e^{3}H^{5}S^{3}A^{2})$ and $\\begin{array}{r}{\\gamma_{m}=\\sqrt{\\frac{H^{6}S^{4}A^{3}}{\\mathcal{E}_{m}}}}\\end{array}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{2e^{2}H S A+2e^{2}H\\eta_{m}\\widehat{\\mathrm{reg}}_{m-1}(\\pi;c)}{\\gamma_{m}}\\leq2e^{2}\\sqrt{\\frac{\\mathcal{E}_{m}}{H^{4}S^{2}A}}+\\frac{1}{720H^{4}S^{3}A^{2}}\\widehat{\\mathrm{reg}}_{m}(\\pi,c).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Also by the definition of $p_{m}^{h}(c)$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\gamma_{m}\\widetilde{d}_{m}^{h}(s,a;\\pi_{m,c}^{h,s,a};c)}{2e^{2}H(S A+\\eta_{m}\\mathrm{reg}_{m-1}(\\pi_{m,c}^{h,s,a};c))}\\cdot D_{\\mathrm{H}}^{2}\\Big(P_{\\star}^{h}(s,a;c),\\widehat{P}_{m}^{h}(s,a;c)\\Big)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\frac{\\gamma_{m}}{e^{2}H}\\cdot p_{m}^{h}(c,\\pi_{m,c}^{h,s,a})\\widehat{d}_{m}^{h}(s,a;\\pi_{m,c}^{h,s,a},c)D_{\\mathrm{H}}^{2}\\Big(P_{\\star}^{h}(s,a;c),\\widehat{P}_{m}^{h}(s,a;c)\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now we plug Eqs. (11) to (13) back into Eq. (10) to obtain that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{d}_{m}^{h}(s,a;\\pi,c)D_{\\mathrm{H}}\\Big(P_{\\star}^{h}(s,a;c),\\widehat{P}_{m}^{h}(s,a;c)\\Big)}\\\\ &{\\qquad\\qquad\\leq\\Bigg(2e^{2}\\sqrt{\\frac{\\mathcal{E}_{m}}{H^{4}S^{2}A}}+\\frac{1}{720H^{4}S^{3}A^{2}}\\widehat{\\mathrm{reg}}_{m-1}(\\pi,c)\\Bigg)\\widehat{d}_{m}^{h}(s,a;\\pi,c)}\\\\ &{\\qquad\\qquad+\\,\\frac{\\gamma_{m}}{e^{2}H}\\cdot p_{m}^{h}(c,\\pi_{m,c}^{h,s,a})\\widehat{d}_{m}^{h}(s,a;\\pi_{m,c}^{h,s,a},c)D_{\\mathrm{H}}^{2}\\Big(P_{\\star}^{h}(s,a;c),\\widehat{P}_{m}^{h}(s,a;c)\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Similar bounds can be obtained replacing DH P \u22c6h(s, a; c),P  mh(s, a; c) with $D_{\\mathrm{H}}\\Big(R_{\\star}^{h}(s,a;c),\\widehat{R}_{m}^{h}(s,a;c)\\Big).$ \u53e3 ", "page_idx": 17}, {"type": "text", "text": "E Proofs from Section 4 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof of Lemma 4.1. For this we first apply the local simulation lemma (Lemma C.2) in reinforcement learning to relate the divergence of the value functions to the stepwise divergences as the following. Under any context $c$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\widehat{V}_{m}^{1}(\\pi_{c},c)-V_{\\star}^{1}(\\pi_{c},c)\\Big|\\leq\\sum_{h,s,a}\\widehat{d}_{m}^{h}(s,a;\\pi_{c},c)\\Big(D_{\\mathrm{H}}\\Big(\\widehat{P}_{m}^{h}(s,a;c),P_{\\star}^{h}(s,a;c)\\Big)+D_{\\mathrm{H}}\\Big(\\widehat{R}_{m}^{h}(s,a;c),R_{\\star}^{h}(s,a;c)\\Big)\\Big)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then we can exchange the estimated occupancy measure $\\widehat{d}_{m}^{h}(s,a;\\pi_{c},c)$ by the trusted occupancy measure through Lemma 3.3, that is, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{h,s,a}\\widehat{d}_{m}^{h}\\big(s,a;\\pi_{c},c\\big)D_{\\mathbb{H}}\\Big(\\widehat{P}_{m}^{h}\\big(s,a;c\\big),P_{\\star}^{h}\\big(s,a;c\\big)\\Big)}\\quad}&{}\\\\ &{\\leq\\displaystyle\\sum_{h,s,a}\\widehat{d}_{m}^{h}\\big(s,a;\\pi_{c},c\\big)D_{\\mathbb{H}}\\Big(\\widehat{P}_{m}^{h}\\big(s,a;c\\big),P_{\\star}^{h}\\big(s,a;c\\big)\\Big)}\\\\ &{\\quad\\quad+\\displaystyle\\sum_{h,s,a}\\left(32e\\sqrt{H^{4}S^{2}A\\cdot\\mathcal{E}_{m}}+\\frac{1}{90H S A}\\widehat{\\Gamma\\mathrm{e}}_{\\underline{{\\xi}}}\\alpha_{m-1}(\\pi;c)\\right)}\\\\ &{\\leq\\displaystyle\\sum_{h,s,a}\\widehat{d}_{m}^{h}\\big(s,a;\\pi_{c},c)D_{\\mathbb{H}}\\Big(\\widehat{P}_{m}^{h}\\big(s,a;c\\big),P_{\\star}^{h}\\big(s,a;c\\big)\\Big)}\\\\ &{\\quad\\quad+32e\\sqrt{H^{6}S^{4}A^{3}\\cdot\\mathcal{E}_{m}}+\\frac{1}{90}\\widehat{\\Gamma\\mathrm{e}}_{\\underline{{\\xi}}}\\alpha_{m-1}\\big(\\pi_{c};c\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then by coverage guarantee Lemma 3.5, for any $h,s,a$ , we can bound ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{h,s,a}\\widehat{a}_{m}^{h}(s,a;\\pi_{c},c)D_{\\mathbb{H}}\\Big(\\widehat{P}_{m}^{h}(s,a;c),P_{*}^{h}(s,a;c)\\Big)}\\quad}&{}\\\\ &{\\leq\\displaystyle\\sum_{h,s,a}\\frac{\\gamma_{m}}{e^{2}H}\\cdot\\widehat{p}_{m}^{h}(c,\\pi_{m,c}^{h,s,a})\\widehat{a}_{m}^{h}(s,a;\\pi_{m,c}^{h,s,a},c)\\cdot D_{\\mathbb{H}}^{2}(P_{\\star}^{h}(s,a;c),\\widehat{P}_{m}^{h}(s,a;c))}\\\\ &{\\qquad\\qquad+\\displaystyle\\sum_{h,s,a}\\left(2e^{2}\\sqrt{\\frac{\\xi_{m}}{H^{4}\\xi^{2}A}}+\\frac{1}{720H^{4}S^{3}A^{2}}\\widehat{\\mathrm{reg}}_{m-1}(\\pi,c)\\right)\\widehat{a}_{m}^{h}(s,a;\\pi,c)}\\\\ &{\\leq\\displaystyle\\sum_{h,s,a}\\frac{\\gamma_{m}}{e^{2}H}\\cdot\\widehat{p}_{m}^{h}(c,\\pi_{m,c}^{h,s,a})\\widehat{a}_{m}^{h}(s,a;\\pi_{m,c}^{h,s,a},c)\\cdot D_{\\mathbb{H}}^{2}(P_{\\star}^{h}(s,a;c),\\widehat{P}_{m}^{h}(s,a;c))}\\\\ &{\\qquad+2e^{2}\\sqrt{\\frac{\\xi_{m}}{H^{2}A}}+\\frac{1}{720H^{3}S^{2}A^{2}}\\widehat{\\mathrm{reg}}_{m-1}(\\pi_{c},c).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Suppose the assumption in Lemma 3.4 is satisfied, then by Lemma 3.4, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{h,s,a}\\frac{\\gamma_{m}}{e^{2}H}\\cdot p_{m}^{h}(c,\\pi_{m,c}^{h,s,a})\\widehat{d}_{m}^{h}(s,a;\\pi_{m,c}^{h,s,a},c)\\cdot D_{\\mathrm{H}}^{2}(P_{\\star}^{h}(s,a;c),\\widehat{P}_{m}^{h}(s,a;c))}}\\\\ &{}&{\\leq\\sum_{h,s,a}\\frac{\\gamma_{m}}{H}\\cdot p_{m}^{h}(c,\\pi_{m,c}^{h,s,a})d_{m}^{h}(s,a;\\pi_{m,c}^{h,s,a},c)\\cdot D_{\\mathrm{H}}^{2}(P_{\\star}^{h}(s,a;c),\\widehat{P}_{m}^{h}(s,a;c))}\\\\ &{}&{\\leq\\frac{\\gamma_{m}}{H}\\cdot\\sum_{h}\\mathbb{E}_{\\pi\\sim p_{m}^{h}(c)}\\left[\\mathbb{E}^{M_{\\star},\\pi,c}\\left[D_{\\mathrm{H}}^{2}\\Big(\\widehat{P}_{m}^{h}(s_{1}^{h},a_{1}^{h};c),P_{\\star}^{h}(s_{1}^{h},a_{1}^{h};c)\\Big)\\right]\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The $D_{\\mathrm{H}}\\Big(\\widehat{P}_{m}^{h}(s_{1}^{h},a_{1}^{h};c),P_{\\star}^{h}(s_{1}^{h},a_{1}^{h};c)\\Big)$ in Eqs. (15), (16) and (26) can be replaced with $D_{\\mathrm{H}}\\Big(\\widehat{R}_{m}^{h}(s_{1}^{h},a_{1}^{h};c),R_{\\star}^{h}(s_{1}^{h},a_{1}^{h};c)\\Big)$ as well. If the assumption in Lemma 3.4 is not satisfied, then we there exists $j$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\pi\\sim p_{m}^{h}(c)}\\mathbb{E}^{M_{\\star},\\pi,c}\\left[D_{\\mathrm{H}}^{2}\\Big(\\widehat{P}_{m}^{h}\\big(s_{1}^{j},a_{1}^{j};c\\big),P_{\\star}^{h}\\big(s_{1}^{j},a_{1}^{j};c\\big)\\Big)\\right]>H/\\gamma_{m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This implies ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left|\\widehat{V}_{m}^{1}(\\pi_{c},c)-V_{\\star}^{1}(\\pi_{c},c)\\right|\\leq1\\leq\\frac{\\gamma_{m}}{H}\\cdot\\mathbb{E}_{\\pi\\sim p_{m}^{h}(c)}\\mathbb{E}^{M_{\\star},\\pi,c}\\left[D_{\\mathbf{H}}^{2}\\Big(\\widehat{P}_{m}^{h}(s_{1}^{j},a_{1}^{j};c),P_{\\star}^{h}(s_{1}^{j},a_{1}^{j};c)\\Big)\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, altogether, no matter the assumption in Lemma 3.4 is satisfied or not, we have shown ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\widehat{V}_{m}^{1}(\\pi_{c},c)-V_{\\star}^{1}(\\pi_{c},c)\\Big|\\le76e\\sqrt{H^{6}S^{4}A^{3}\\cdot\\mathcal{E}_{m}}+\\frac{1}{20}\\widehat{\\mathrm{reg}}_{m-1}(\\pi_{c};c)\\quad}}\\\\ &{}&{\\qquad+\\,\\frac{2\\gamma_{m}}{H}\\cdot\\sum_{h}\\mathbb{E}_{\\pi\\sim p_{m}^{h}(c)}\\left[\\mathbb{E}^{M_{\\star},\\pi,c}\\left[D_{\\mathrm{H}}^{2}\\Big(\\widehat{P}_{m}^{h}(s_{1}^{h},a_{1}^{h};c),P_{\\star}^{h}(s_{1}^{h},a_{1}^{h};c)\\Big)\\right]\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Taking expectation on $c$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{c\\sim\\mathcal{D}}\\bigg[\\bigg|\\widehat{V}_{n}^{1}(\\pi_{c},c)-V_{\\star}^{1}(\\pi_{c},c)\\bigg|\\bigg]}\\\\ &{\\qquad\\qquad\\le\\frac{1}{20}\\mathbb{E}_{c\\sim\\mathcal{D}}\\big[\\widehat{\\mathrm{reg}}_{m-1}(\\pi_{c},c)\\big]+76e\\sqrt{H^{6}S^{4}A^{3}\\cdot\\mathcal{E}_{m}}}\\\\ &{\\qquad\\qquad\\qquad+\\frac{2\\gamma_{m}}{H}\\cdot\\sum_{h}\\mathbb{E}_{c\\sim\\mathcal{D}}\\mathbb{E}_{\\pi\\sim p_{m}^{h}(c)}\\Big[\\mathbb{E}^{M_{\\star},\\pi,c}\\left[D_{\\mathbf{H}}^{2}\\Big(\\widehat{P}_{m}^{h}(s_{1}^{h},a_{1}^{h};c),P_{\\star}^{h}(s_{1}^{h},a_{1}^{h};c)\\Big)\\right]\\Big]}\\\\ &{\\qquad\\qquad\\le\\frac{1}{20}\\mathbb{E}\\big[\\widehat{\\mathrm{reg}}_{m-1}(\\pi_{c},c)\\big]+76e\\sqrt{H^{6}S^{4}A^{3}\\cdot\\mathcal{E}_{m}}+\\gamma_{m}\\cdot\\mathcal{E}_{m}}\\\\ &{\\qquad\\le\\frac{1}{20}\\mathbb{E}\\big[\\widehat{\\mathrm{reg}}_{m-1}(\\pi_{c},c)\\big]+77e\\sqrt{H^{6}S^{4}A^{3}\\cdot\\mathcal{E}_{m}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the second inequality is by the offline density estimation bound from Lemma 3.2 ", "page_idx": 18}, {"type": "text", "text": "Lemma E.1 ([36]). Let $\\varepsilon_{1},\\ldots,\\varepsilon_{N}$ be $N$ positive values. Suppose for any $m>0$ and an arbitrary policy set $\\{\\pi_{c}\\}_{c\\in c}$ , we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{c\\sim\\mathcal{D}}[|\\widehat{V}_{m}^{1}(\\pi_{c},c)-V_{\\star}^{1}(\\pi_{c},c)|]\\leq\\frac{1}{20}\\mathbb{E}_{c\\sim\\mathcal{D}}[\\widehat{\\mathrm{reg}}_{m-1}(\\pi_{c},c)]+\\varepsilon_{m}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Then for any $m>0$ , ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{c\\sim\\mathcal{D}}[\\mathrm{reg}(\\pi_{c},c)]\\leq\\displaystyle\\frac{10}{9}\\cdot\\mathbb{E}_{c\\sim\\mathcal{D}}[\\widehat{\\mathrm{reg}}_{m}(\\pi_{c},c)]+\\delta_{m},}\\\\ {\\mathbb{E}_{c\\sim\\mathcal{D}}[\\widehat{\\mathrm{reg}}_{m}(\\pi_{c},c)]\\leq\\displaystyle\\frac{9}{8}\\cdot\\mathbb{E}_{c\\sim\\mathcal{D}}[\\mathrm{reg}(\\pi_{c},c)]+\\delta_{m},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\begin{array}{r}{\\delta_{1}=2\\varepsilon_{1}+\\frac{1}{10}}\\end{array}$ and $\\begin{array}{r}{\\delta_{m}=\\frac{1}{9}\\delta_{m-1}+\\frac{20}{9}\\varepsilon_{m}}\\end{array}$ for any $m\\geq2$ . ", "page_idx": 18}, {"type": "text", "text": "Proof of Lemma E.1. We present the proof here for completeness. By Eq. (17), we have that for all $m\\geq0$ and $\\pi_{c}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{c\\sim\\mathcal{D}}[\\mathrm{reg}(\\pi_{c},c)]-\\mathbb{E}_{c\\sim\\mathcal{D}}[\\widehat{\\mathrm{reg}}_{m}(\\pi_{c},c)]}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{c\\sim\\mathcal{D}}[V_{\\star}^{1}(\\pi_{\\star,c},c)-\\widehat{V}_{m}^{1}(\\pi_{\\star,c},c)]+\\mathbb{E}_{c\\sim\\mathcal{D}}[\\widehat{V}_{m}^{1}(\\pi_{\\star,c},c)-\\widehat{V}_{m}^{1}(\\widehat{\\pi}_{m,c},c)]}\\\\ &{\\qquad\\qquad+\\mathbb{E}_{c\\sim\\mathcal{D}}[\\widehat{V}_{m}^{1}(\\pi_{c},c)-V_{\\star}^{1}(\\pi_{c},c)]}\\\\ &{\\qquad\\qquad\\leq\\frac{1}{20}\\mathbb{E}_{c\\sim\\mathcal{D}}[\\widehat{\\mathrm{reg}}_{m-1}(\\pi_{\\star,c},c)]+\\frac{1}{20}\\mathbb{E}_{c\\sim\\mathcal{D}}[\\widehat{\\mathrm{reg}}_{m-1}(\\pi_{c},c)]+2\\varepsilon_{m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Symmetrically, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{c\\sim\\mathcal{D}}[\\widehat{\\mathrm{reg}}_{m}(\\pi_{c},c)]-\\mathbb{E}_{c\\sim\\mathcal{D}}[\\mathrm{reg}(\\pi_{c},c)]}\\\\ &{\\qquad\\qquad\\le\\cfrac{1}{20}\\mathbb{E}_{c\\sim\\mathcal{D}}[\\widehat{\\mathrm{reg}}_{m-1}(\\widehat{\\pi}_{m,c},c)]+\\cfrac{1}{20}\\mathbb{E}_{c\\sim\\mathcal{D}}[\\widehat{\\mathrm{reg}}_{m-1}(\\pi_{c},c)]+2\\varepsilon_{m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Now we inductively show for $m\\,=\\,1,2,...$ that Eq. (18) and Eq. (19) hold. For $m\\,=\\,1$ , since $\\mathbb{E}_{c\\sim\\mathcal{D}}[\\mathrm{reg}(\\pi_{c},c)],\\mathbb{\\vec{E}}_{c\\sim\\mathcal{D}}[\\widehat{\\mathrm{reg}}_{1}(\\pi_{c},c)]\\leq1$ for all $\\pi$ , then we have from Eq. (20) and Eq. (21) ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{c\\sim\\mathcal{D}}[\\mathrm{reg}(\\pi_{c},c)]\\leq\\mathbb{E}_{c\\sim\\mathcal{D}}[\\widehat{\\mathrm{reg}}_{1}(\\pi_{c},c)]+2\\varepsilon_{1}+\\displaystyle\\frac{1}{10}\\!\\!\\!\\!\\!\\!\\mathrm{~and}}\\\\ &{\\mathbb{E}_{c\\sim\\mathcal{D}}[\\widehat{\\mathrm{reg}}_{1}(\\pi_{c},c)]\\leq\\mathbb{E}_{c\\sim\\mathcal{D}}[\\mathrm{reg}(\\pi_{c},c)]+2\\varepsilon_{1}+\\displaystyle\\frac{1}{10}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence we have shown Eq. (18) and Eq. (19) for $m=1$ and $\\delta_{1}=2\\varepsilon_{1}+\\frac{1}{10}$ . Now, suppose Eq. (18) and Eq. (19) holds for all $1,2,\\dots,m-1$ . Plugging Eq. (19) for $m-1$ into the right hand side of Eq. (20), we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{c\\sim\\mathcal{D}}[\\mathrm{reg}(\\pi_{c},c)]-\\mathbb{E}_{c\\sim\\mathcal{D}}[\\widehat{\\mathrm{reg}}_{m}(\\pi_{c},c)]}\\\\ &{\\qquad\\qquad\\leq\\frac{1}{20}\\mathbb{E}_{c\\sim\\mathcal{D}}[\\widehat{\\mathrm{reg}}_{m-1}(\\pi_{*,c},c)]+\\frac{1}{20}\\mathbb{E}_{c\\sim\\mathcal{D}}[\\widehat{\\mathrm{reg}}_{m-1}(\\pi_{c},c)]+2\\varepsilon_{m}}\\\\ &{\\qquad\\qquad\\leq\\frac{1}{20}\\bigg(\\frac{9}{8}\\mathbb{E}_{c\\sim\\mathcal{D}}[\\mathrm{reg}(\\pi_{*,c},c)]+\\delta_{m-1}\\bigg)+\\frac{1}{20}\\bigg(\\frac{9}{8}\\mathbb{E}_{c\\sim\\mathcal{D}}[\\mathrm{reg}(\\pi_{c},c)]+\\delta_{m-1}\\bigg)+2\\varepsilon_{m}}\\\\ &{\\qquad\\qquad=\\frac{1}{20}\\delta_{m-1}+\\frac{1}{20}\\bigg(\\frac{9}{8}\\mathbb{E}_{c\\sim\\mathcal{D}}[\\mathrm{reg}(\\pi_{c},c)]+\\delta_{m-1}\\bigg)+2\\varepsilon_{m},}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the last equality is by $\\mathbb{E}_{c\\sim\\mathcal{D}}[\\mathrm{reg}(\\pi_{\\star,c},c)]=0$ . Thus reorganizing the terms, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}_{c\\sim\\mathcal{D}}[\\mathrm{reg}(\\pi_{c},c)]\\leq\\frac{10}{9}\\mathbb{E}_{c\\sim\\mathcal{D}}[\\widehat{\\mathrm{reg}}_{m}(\\pi_{c},c)]+\\delta_{m},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\begin{array}{r}{\\delta_{m}=\\frac{1}{9}\\delta_{m-1}+\\frac{20}{9}\\varepsilon_{m}}\\end{array}$ . Thus we have shown Eq. (18) for $m$ . Then plugging Eq. (19) for $m-1$ into the right hand side of Eq. (21), we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{c\\sim\\mathcal{D}}[\\widehat{\\mathrm{reg}}_{m}(\\pi_{c},c)]-\\mathbb{E}_{c\\sim\\mathcal{D}}[\\mathrm{reg}(\\pi_{c},c)]}\\\\ &{\\qquad\\qquad\\leq\\cfrac{1}{20}\\mathbb{E}_{c\\sim\\mathcal{D}}[\\widehat{\\mathrm{reg}}_{m-1}(\\widehat{\\pi}_{m,c},c)]+\\cfrac{1}{20}\\mathbb{E}_{c\\sim\\mathcal{D}}[\\widehat{\\mathrm{reg}}_{m-1}(\\pi_{c},c)]+2\\varepsilon_{m}}\\\\ &{\\qquad\\qquad\\leq\\cfrac{1}{20}\\bigg(\\cfrac{9}{8}\\mathbb{E}_{c\\sim\\mathcal{D}}[\\mathrm{reg}(\\widehat{\\pi}_{m,c},c)]+\\delta_{m-1}\\bigg)+\\cfrac{1}{20}\\bigg(\\cfrac{9}{8}\\mathbb{E}_{c\\sim\\mathcal{D}}[\\mathrm{reg}(\\pi_{c},c)]+\\delta_{m-1}\\bigg)+2\\varepsilon_{m}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Furthermore, by Eq. (18) for $m$ we have $\\begin{array}{r}{\\mathbb{E}_{c\\sim\\mathcal{D}}[\\mathrm{reg}(\\widehat{\\pi}_{m,c},c)]\\leq\\frac{10}{9}\\mathbb{E}_{c\\sim\\mathcal{D}}[\\widehat{\\mathrm{reg}}(\\widehat{\\pi}_{m,c},c)]+\\delta_{m}=\\delta_{m}.}\\end{array}$ Plug this in the aforementioned inequality, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathfrak{L}_{c\\sim\\mathcal{D}}[\\widehat{\\mathrm{reg}}_{m}(\\pi_{c},c)]-\\mathbb{E}_{c\\sim\\mathcal{D}}[\\mathrm{reg}(\\pi_{c},c)]\\leq\\frac{1}{20}\\bigg(\\frac{9}{8}\\delta_{m}+\\delta_{m-1}\\bigg)+\\frac{1}{20}\\bigg(\\frac{9}{8}\\mathbb{E}_{c\\sim\\mathcal{D}}[\\mathrm{reg}(\\pi_{c},c)]+\\delta_{m-1}\\bigg)-\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Reorganizing the terms, this in turn gives ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{E}_{c\\sim\\mathcal{D}}[\\widehat{\\mathrm{reg}}_{m}(\\pi_{c},c)]\\leq\\frac{9}{8}\\mathbb{E}_{c\\sim\\mathcal{D}}[\\mathrm{reg}(\\pi_{c},c)]+\\delta_{m},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where recall $\\begin{array}{r}{\\delta_{m}=\\frac{1}{9}\\delta_{m-1}+\\frac{20}{9}\\varepsilon_{m}}\\end{array}$ . This proves Eq. (19) for $m$ which completes our induction. ", "page_idx": 19}, {"type": "text", "text": "Pr\u221aoof of Theorem 4.1. Let $\\mathcal{E}_{0}\\,=\\,1$ . Then we have by Lemma 4.1, Eq. (17) holds with $\\varepsilon_{m}=$ $L\\sqrt{H^{6}S^{4}A^{3}\\mathcal{E}_{m}}$ for $L>0$ large enough for all $m>0$ . Combining Lemmas 3.1 and E.1, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{c\\sim\\mathcal{D},\\pi\\sim p_{m}^{h}(c)}[\\mathrm{reg}(\\pi,c)]\\lesssim\\mathbb{E}_{c\\sim\\mathcal{D},\\pi\\sim p_{m}^{h}(c)}[\\widehat{\\mathrm{reg}}_{m-1}(\\pi_{c},c)]+\\sum_{i=0}^{m-1}\\frac{1}{9^{m-i}}\\sqrt{H^{6}S^{4}A^{3}\\cdot\\mathcal{E}_{i}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then by Lemma 3.1, we have further ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{c\\sim\\mathcal{D},\\pi\\sim p_{m}^{h}(c)}[\\widehat{\\mathrm{reg}}_{m-1}(\\pi_{c},c)]\\lesssim\\sqrt{H^{6}S^{4}A^{3}\\cdot\\mathcal{E}_{m}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbb{E}_{c\\sim\\mathcal{D},\\pi\\sim p_{m}^{h}(c)}[\\mathrm{reg}(\\pi,c)]\\lesssim\\sum_{i=0}^{m}\\frac{1}{9^{m-i}}\\sqrt{H^{6}S^{4}A^{3}\\cdot{\\mathcal{E}}_{i}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In all, we can obtain the following regret bound with probability at least $1-\\delta$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sum_{t=1}^{T}\\mathrm{Reg}(T)=\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{c_{t}\\sim\\mathcal{D},\\pi_{t}\\sim p_{m(t)}^{h(t)}(c_{t})}\\big[\\mathrm{reg}(\\pi_{t},c_{t})\\big]}\\\\ &{\\qquad\\qquad\\qquad=\\displaystyle\\sum_{h,m}\\mathbb{E}_{c\\sim\\mathcal{D},\\pi\\sim p_{m}^{h}(c)}\\big[\\mathrm{reg}(\\pi,c)\\big]\\cdot\\big(\\tau_{m}-\\tau_{m-1}\\big)}\\\\ &{\\qquad\\qquad\\qquad\\lesssim\\displaystyle\\sum_{m=1}^{N}\\big(\\tau_{m}-\\tau_{m-1}\\big)\\cdot\\sqrt{H^{8}S^{4}A^{3}\\cdot\\mathcal{E}_{m}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last step takes a union bound on the offline oracle guarantees. ", "page_idx": 20}, {"type": "text", "text": "Proof of Theorem 3.1. Without loss of generality, assume that $T/H>1000$ . Since we are choosing \u03c4m = 2(T/H)1\u22122 , we first note that since $\\tau_{m}\\leq T/H$ , we have $(T/H)^{2^{-m}}\\geq2$ . Then we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\tau_{m}-\\tau_{m-1}=2(T/H)^{1-2^{-m}}-2(T/H)^{1-2^{1-m}}}&{}\\\\ {=2(T/H)^{1-2^{1-m}}(T^{2^{-m}}-1)}&{}\\\\ {\\geq2(T/H)^{1-2^{1-m}}=\\tau_{m-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This implies $\\begin{array}{r}{\\tau_{m}-\\tau_{m-1}\\geq\\frac{1}{2}\\tau_{m}}\\end{array}$ for $m\\in[N]$ . Then by Theorem 4.1, we have with probability at least $1-\\delta$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{t=1}^{T}\\mathrm{Reg}(T)\\leq\\sum_{m=1}^{N}(\\tau_{m}-\\tau_{m-1})\\cdot\\sqrt{H^{8}S^{4}A^{3}\\cdot\\mathcal{E}_{m}}}}\\\\ &{\\lesssim\\sqrt{H^{8}S^{4}A^{3}\\log(|\\mathcal{M}|N/\\delta)}\\cdot\\sum_{m=2}^{N}\\frac{\\tau_{m}-\\tau_{m-1}}{\\sqrt{\\tau_{m-1}-\\tau_{m-2}}}+\\tau_{1}\\sqrt{H^{8}S^{4}A^{3}}}\\\\ &{\\lesssim\\sqrt{H^{8}S^{4}A^{3}\\log(|\\mathcal{M}|N/\\delta)}\\cdot\\sum_{m=2}^{N}\\frac{\\tau_{m}}{\\sqrt{\\tau_{m-1}}}+\\sqrt{H^{7}S^{4}A^{3}T}}\\\\ &{\\lesssim\\sqrt{H^{8}S^{4}A^{3}\\log(|\\mathcal{M}|N/\\delta)}N\\sqrt{T/H}=\\sqrt{H^{7}S^{4}A^{3}T\\cdot\\log(|\\mathcal{M}|\\log\\log T/\\delta)\\log\\log1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last inequality follows from ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\tau_{m}}{\\sqrt{\\tau_{m-1}}}\\leq\\frac{\\tau_{m}}{\\sqrt{(\\tau_{m-1}+1)/2}}\\leq\\frac{2(T/H)^{1-2^{-m}}}{(T/H)^{\\frac{1}{2}(1-2^{1-m})}}=2\\sqrt{T/H}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and $N=O(\\log\\log T)$ . So the number of oracle calls are $O(H\\log\\log T)$ . ", "page_idx": 20}, {"type": "text", "text": "Proof of Theorem 3.2. Since we are choosing $\\tau_{m}=2^{m}$ , we have $N=O(\\log T)$ . So the number of oracle calls are $O(H\\log T)$ . Meanwhile, by Theorem 4.1, we have with probability at least $1-\\delta$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{t=1}^{T}\\mathrm{Reg}(T)\\leq\\sum_{m=1}^{N}(\\tau_{m}-\\tau_{m-1})\\cdot\\sqrt{H^{8}S^{4}A^{3}\\cdot\\mathcal{E}_{m}}}}\\\\ &{\\lesssim\\sqrt{H^{8}S^{4}A^{3}\\log(|\\mathcal{M}|N/\\delta)}\\bigg(1+\\displaystyle\\sum_{m=3}^{N}\\frac{2^{m-1}}{\\sqrt{2^{m-2}}}\\bigg)}\\\\ &{\\lesssim\\sqrt{H^{8}S^{4}A^{3}\\log(|\\mathcal{M}|N/\\delta)}\\cdot2^{N/2}=\\sqrt{H^{7}S^{4}A^{3}T\\cdot\\log(|\\mathcal{M}|\\log T/\\delta)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "F Proofs from Section 5 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proof of Theorem 5.1. The reward distributions $\\widehat{R}_{m}$ are 0 for $m=0,1$ since ${\\widehat{R}}_{0}$ are set to be 0 and the model class $\\mathcal{M}$ consists of models with const antly 0 reward. The regret es timations $\\widehat{\\mathrm{reg}}_{m-1}$ are all 0 for $m=1,2$ . Thus we apply the component-wise guarantees (Lemmas 3.2 to 3.5) to $\\widehat{P}_{2},\\widehat{R}_{2}$ where $\\widehat{R}_{2}=0$ . Concretely, from Lemma 3.2 we have with probability at least $1-\\delta$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{c\\sim\\mathcal{D},\\pi\\sim p_{2}^{h}(c)}\\left[\\mathbb{E}^{M_{\\star},\\pi,c}\\left[D_{\\mathrm{H}}^{2}\\Big(\\widehat{P}_{2}^{h}\\big(s_{1}^{h},a_{1}^{h};c\\big),P_{\\star}^{h}\\big(s_{1}^{h},a_{1}^{h};c\\big)\\Big)\\right]\\right]\\lesssim\\mathcal{E}_{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "with the offline $\\mathsf{M L E}_{\\mathcal{M}}$ oracle. From Lemma 3.3, we have for any $\\pi,c,h,s,a.$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\widehat{d}_{2}^{h}(s,a;\\pi,c)-\\widetilde{d}_{2}^{h}(s,a;\\pi,c)\\leq32e\\sqrt{H^{4}S^{2}A\\cdot\\mathcal{E}_{2}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "From Lemma 3.4, we have if for a context $c$ and all $h\\in[H]$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\pi\\sim p_{2}^{h}(c)}\\mathbb{E}^{M_{\\star},\\pi,c}\\left[D_{\\mathrm{H}}^{2}\\Big(\\widehat{P}_{2}^{h}\\big(s_{1}^{h},a_{1}^{h};c\\big),P_{\\star}^{h}\\big(s_{1}^{h},a_{1}^{h};c\\big)\\Big)\\right]\\leq H/\\gamma_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then for the same $c$ and all $\\pi,h,s,a$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\widetilde{d}_{2}^{h}(s,a;\\pi,c)\\leq(1+1/H)^{2(h-1)}d_{2}^{h}(s,a;\\pi,c).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Finally, from Lemma 3.5, we have for any $\\pi,c,h,s,a$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{d}_{2}^{h}(s,a;\\pi,c)\\cdot D_{\\mathrm{H}}(P_{\\star}^{h}(s,a;c),\\widehat{P}_{2}^{h}(s,a;c))}\\\\ &{\\qquad\\qquad\\leq\\frac{\\gamma_{2}}{e^{2}H}\\cdot p_{2}^{h}(c,\\pi_{2,c}^{h,s,a})\\widetilde{d}_{2}^{h}(s,a;\\pi_{2,c}^{h,s,a},c)\\cdot D_{\\mathrm{H}}^{2}(P_{\\star}^{h}(s,a;c),\\widehat{P}_{2}^{h}(s,a;c))}\\\\ &{\\qquad\\qquad\\qquad+\\,2e^{2}\\sqrt{\\frac{\\xi_{2}}{H^{4}S^{2}A}}\\cdot\\widetilde{d}_{2}^{h}(s,a;\\pi,c).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now we are ready to prove our claim following similar derivations from the proof of Lemma 4.1. Concretely, we have for any set of policies $\\{\\pi_{c}\\}_{c\\in{\\mathcal{C}}}$ and any context $c$ , by local simulation lemma (Lemma C.2) ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Big|V_{\\star}^{1}(\\pi_{c},c,R)-\\widehat{V}_{2}^{1}(\\pi_{c},c,R)\\Big|\\leq\\sum_{h,s,a}\\widehat{d}_{2}^{h}(s,a;\\pi_{c},c)D_{\\mathrm{H}}\\Big(\\widehat{P}_{2}^{h}(s,a;c),P_{\\star}^{h}(s,a;c)\\Big).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then by Eq. (23), we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{h,s,a}\\widehat{d}_{2}^{h}(s,a;\\pi_{c},c)D_{\\mathrm{H}}\\Big(\\widehat{P}_{2}^{h}(s,a;c),P_{\\star}^{h}(s,a;c)\\Big)}}\\\\ &{}&{\\leq\\displaystyle\\sum_{h,s,a}\\widehat{d}_{2}^{h}(s,a;\\pi_{c},c)D_{\\mathrm{H}}\\Big(\\widehat{P}_{2}^{h}(s,a;c),P_{\\star}^{h}(s,a;c)\\Big)+32e\\sqrt{H^{6}S^{4}A^{3}\\cdot\\mathcal{E}_{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Case I: If for a context $c$ and all $h\\in[H]$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\pi\\sim p_{2}^{h}(c)}\\mathbb{E}^{M_{\\star},\\pi,c}\\left[D_{\\mathrm{H}}^{2}\\Big(\\widehat{P}_{2}^{h}\\big(s_{1}^{h},a_{1}^{h};c\\big),P_{\\star}^{h}\\big(s_{1}^{h},a_{1}^{h};c\\big)\\Big)\\right]\\leq H/\\gamma_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then by Eqs. (24) and (25), ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\sum_{h,s,a}\\widehat{d}_{2}^{h}(s,a;\\pi_{c},c)D_{\\mathrm{H}}\\Big(\\widehat{P}_{2}^{h}(s,a;c),P_{\\star}^{h}(s,a;c)\\Big)}}\\\\ &{\\leq\\frac{\\gamma_{2}}{e^{2}H}\\cdot p_{2}^{h}(c,\\pi_{2,c}^{h,s,a})\\widehat{d}_{2}^{h}(s,a;\\pi_{2,c}^{h,s,a},c)\\cdot D_{\\mathrm{H}}^{2}(P_{\\star}^{h}(s,a;c),\\widehat{P}_{2}^{h}(s,a;c))}\\\\ &{\\quad+2e^{2}\\sqrt{\\frac{\\xi_{2}}{H^{4}S^{2}A}}\\cdot\\widehat{d}_{2}^{h}(s,a;\\pi_{c},c)}\\\\ &{\\leq\\frac{\\gamma_{2}}{H}\\cdot p_{2}^{h}(c,\\pi_{2,c}^{h,s,a})d_{2}^{h}(s,a;\\pi_{2,c}^{h,s,a},c)\\cdot D_{\\mathrm{H}}^{2}(P_{\\star}^{h}(s,a;c),\\widehat{P}_{2}^{h}(s,a;c))}\\\\ &{\\quad+2e^{2}\\sqrt{\\frac{\\xi_{2}}{H^{4}S^{2}A}}\\cdot\\widehat{d}_{2}^{h}(s,a;\\pi_{c},c)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|V_{\\star}^{1}(\\pi_{c},c,R)-\\widehat{V}_{2}^{1}(\\pi_{c},c,R)\\right|}\\\\ &{\\qquad\\qquad\\lesssim\\sqrt{H^{6}S^{4}A^{3}\\cdot\\mathcal{E}_{2}}+\\frac{\\gamma_{2}}{H}\\cdot\\displaystyle\\sum_{h=1}^{H}\\mathbb{E}_{\\pi\\sim p_{2}^{h}(c)}\\mathbb{E}^{M_{\\star},\\pi,c}\\left[D_{\\mathrm{H}}^{2}\\Big(\\widehat{P}_{2}^{h}(s_{1}^{h},a_{1}^{h};c),P_{\\star}^{h}(s_{1}^{h},a_{1}^{h};c)\\Big)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Case $\\mathbf{II}$ : If for a context $c$ there exists $j\\in[H]$ such that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\pi\\sim p_{2}^{h}(c)}\\mathbb{E}^{M_{\\star},\\pi,c}\\left[D_{\\mathrm{H}}^{2}\\Big(\\widehat{P}_{2}^{j}\\big(s_{1}^{j},a_{1}^{j};c\\big),P_{\\star}^{h}\\big(s_{1}^{j},a_{1}^{j};c\\big)\\Big)\\right]>H/\\gamma_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|V_{\\star}^{1}(\\pi_{c},c,R)-\\widehat V_{2}^{1}(\\pi_{c},c,R)\\right|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq1\\leq\\displaystyle\\frac{\\gamma_{2}}{H}\\cdot\\sum_{h=1}^{H}\\mathbb{E}_{\\pi\\sim p_{2}^{h}(c)}\\mathbb{E}^{M_{\\star},\\pi,c}\\left[D_{\\mathrm{H}}^{2}\\Big(\\widehat P_{2}^{h}(s_{1}^{h},a_{1}^{h};c),P_{\\star}^{h}(s_{1}^{h},a_{1}^{h};c)\\Big)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Combine Case I and $\\mathrm{II}$ , we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|V_{\\star}^{1}(\\pi_{c},c,R)-\\widehat{V}_{2}^{1}(\\pi_{c},c,R)\\right|}\\\\ &{\\qquad\\qquad\\lesssim\\sqrt{H^{6}S^{4}A^{3}\\cdot\\mathcal{E}_{2}}+\\frac{\\gamma_{2}}{H}\\cdot\\displaystyle\\sum_{h=1}^{H}\\mathbb{E}_{\\pi\\sim p_{2}^{h}(c)}\\mathbb{E}^{M_{\\star},\\pi,c}\\left[D_{\\mathrm{H}}^{2}\\Big(\\widehat{P}_{2}^{h}(s_{1}^{h},a_{1}^{h};c),P_{\\star}^{h}(s_{1}^{h},a_{1}^{h};c)\\Big)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then take expectation with respect to $c$ together with Eq. (22) we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{c\\sim\\mathcal{D}}\\left[\\left|V_{\\star}^{1}(\\pi_{c},c,R)-\\widehat{V}_{2}^{1}(\\pi_{c},c,R)\\right|\\right]\\lesssim\\sqrt{H^{6}S^{4}A^{3}\\cdot\\mathcal{E}_{2}}+\\gamma_{2}\\mathcal{E}_{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\lesssim\\sqrt{\\frac{H^{7}S^{4}A^{3}\\log(|\\mathcal{M}|/\\delta)}{T}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\lesssim\\varepsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the second inequality is by $\\mathcal{E}_{2}=H\\log(|\\mathcal{M}|/\\delta)/T$ and the last inequality holds when ", "page_idx": 22}, {"type": "equation", "text": "$$\nT\\geq\\Omega\\bigg(\\frac{H^{7}S^{4}A^{3}\\log(|\\mathcal{M}|/\\delta)}{\\varepsilon^{2}}\\bigg).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus the reward-free objective Eq. (5) is satisfied with probability at least $1-\\delta$ with ", "page_idx": 22}, {"type": "equation", "text": "$$\nT\\leq O\\bigg(\\frac{H^{7}S^{4}A^{3}\\log(|\\mathcal{M}|/\\delta)}{\\varepsilon^{2}}\\bigg).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lemma F.1 (Lemma D.2 of Jin et al. [22]). Fix $\\varepsilon\\leq1$ , $\\delta\\,\\leq\\,1/2,\\;H,A\\,\\geq\\,2$ , and suppose that $S\\geq L\\log A$ for a universal constant $L$ . Then consider the trivial context space $\\mathcal{C}=\\{c_{0}\\}$ , there exists a model class $\\mathcal{M}=\\{M_{j}\\}_{j\\in\\mathcal{J}}$ , with $|S|=S$ , $|{\\mathcal{A}}|=A$ , $|{\\mathcal{I}}|\\leq e^{S A}$ , and horizon $H$ and $a$ distribution $\\mu$ on $\\mathcal{M}$ such that any algorithm $\\mathsf{A l g}$ that $(\\varepsilon/12,\\delta)$ -learns the class $\\mathcal{M}$ satisfies ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}_{M\\sim\\mu}\\mathbb{E}_{M,\\mathsf{A l g}}[T]\\gtrsim\\frac{S A}{\\varepsilon^{2}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $T$ is the number of trajectories required by the algorithm $\\mathsf{A l g}$ to achieve $(\\varepsilon/12,\\delta)$ accuracy and $\\mathbb{E}_{M,\\mathsf{A l g}}[\\cdot]$ is the expectation under the interaction between the algorithm $\\mathsf{A l g}$ and model $M$ . ", "page_idx": 23}, {"type": "text", "text": "Proof of Theorem 5.2. . Let $n=\\log K/(S A)$ , then we consider the context space $\\mathcal{C}=\\{c_{1},...,c_{n}\\}$ and the i.i.d. distribution $\\mathcal{D}$ on the context space being uniform. Denote the model class obtained from Lemma F.1 by $\\overline{{\\mathcal{M}}}=\\{\\overline{{M}}_{j}(c_{0})\\}_{j\\in\\mathcal{J}}$ . Let $J=\\{j_{1},...,j_{n}\\}\\in\\mathcal{J}^{n}$ be an index. Then we consider the model class $\\mathcal{M}=\\{M_{J}\\}_{J\\in\\mathcal{J}^{n}}$ , where $M_{J}(c_{i})=\\overline{{M}}_{j_{i}}(c_{0})$ , that is, the model class $\\mathcal{M}$ is on each context $c_{i}\\in{\\mathcal{C}}$ an independent $\\overline{{\\mathcal{M}}}$ . We first have that the size of the model class is bounded by $|{\\mathcal{M}}|=|{\\mathcal{J}}|^{n}\\leq e^{n S A}\\leq K$ . Then we have for any algorithm $\\mathsf{A l g}$ that $(\\varepsilon/24,\\delta)$ -learns the class $\\mathcal{M}$ , it must have $(\\varepsilon/12,\\delta)$ -learns the class $\\mathcal{M}(c_{i})=\\{M_{J}(c_{i})\\}_{J\\in\\mathcal{J}^{n}}=\\{\\overline{{M}}_{j}(c_{0})\\}_{j\\in\\mathcal{J}}$ for at least half of the contexts $c_{i}\\in\\mathcal{C}$ by Markov\u2019s inequality. This, in turn, combined with Lemma F.1 gives that there exists a distribution $\\mu$ on the model class $\\mathcal{M}$ such that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{E}_{M\\sim\\mu}\\mathbb{E}_{M,\\mathsf{A l g}}[T]\\gtrsim\\frac{n}{2}\\frac{S A}{\\varepsilon^{2}}\\gtrsim\\frac{\\log K}{\\varepsilon^{2}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $T$ is the number of trajectories required by the algorithm $\\mathsf{A l g}$ to achieve $(\\varepsilon/12,\\delta)$ accuracy and $\\mathbb{E}_{M,\\mathsf{A l g}}[\\cdot]$ is the expectation under the interaction between the algorithm $\\mathsf{A l g}$ and model $M$ . Thus concludes our proof. ", "page_idx": 23}, {"type": "text", "text": "G Computation ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we show that for any m, c, h, s, a, the policy \u03c0hm,,sc,a (Line 8) and the trusted transitions $\\widetilde{T}_{m}^{h}(c)$ (Definition 3.1) can be computed efficiently through linear programming. For simplicity, we fix a context $c$ throughout this section and omit its dependence. We first note that if $\\widetilde{\\mathcal{T}}_{m}^{j}$ for $j\\leq h-1$ and $\\pi_{m}^{h,s,a}$ are computed, then ${\\widetilde{\\mathcal{T}}}_{m}^{h}$ can be computed in poly $(H S A)$ time. To see t his, for any (s, a, s\u2032), we have by the definition o f \u03c0hm,s ,athat ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\pi}\\frac{\\widetilde{d}_{m}^{h}(s,a;\\pi)}{S A+\\eta_{m}\\cdot\\widehat{\\mathrm{reg}}_{m-1}(\\pi)}=\\frac{\\widetilde{d}_{m}^{h}(s,a;\\pi_{m}^{h,s,a})}{S A+\\eta_{m}\\cdot\\widehat{\\mathrm{reg}}_{m-1}(\\pi_{m}^{h,s,a})}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Then $(s,a,s^{\\prime})\\in\\widetilde{T}_{m}^{h}$ iff ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{\\widetilde{d}_{m}^{h}(s,a;\\pi_{m}^{h,s,a})}{S A+\\eta_{m}\\cdot\\widehat{\\mathrm{reg}}_{m-1}(\\pi_{m}^{h,s,a})}\\widehat{P}_{m}^{h}(s^{\\prime}|s,a)\\geq\\frac{1}{\\zeta_{m}},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the left-hand side can be computed given ${\\widetilde{\\mathcal{T}}}_{m}^{j}$ for $j\\leq h-1$ in $\\mathrm{poly}(H S A)$ time. Thus we only need to show how to compute $\\pi_{m}^{h,s,a}$ . Assume w e want to compute \u03c0\u00afhm,s\u00af,a\u00affor \u00afh \u2208[H], s\u00af \u2208S, \u00afa \u2208A, given ${\\widetilde{T}}_{m}^{h}$ for $h\\leq\\bar{h}-1$ . Let $\\bar{r}_{m}^{h}(s,a)=\\mathbb{E}_{r^{h}\\sim\\widehat{R}_{m-1}^{h}}[r^{h}]$ be the mean reward for model $\\widehat{M}_{m-1}$ for $h,s,a$ . Let $\\bar{\\mathbf{r}}_{m}=(\\bar{r}_{m}^{h}(s,a))_{h,s,a}$ be the vector of mean rewards for the model $\\widehat{M}_{m-1}$ . We consider the following linear fractional program with the following decision variables: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{d}_{m}^{\\bar{h}}=\\left(\\widehat{\\mathbf{d}}_{m-1}^{\\bar{h}}\\right),\\ \\mathrm{where}\\ \\widetilde{\\mathbf{d}}_{m}^{\\bar{h}}=(\\widetilde{d}_{h,s,a,m})_{h,s,a\\in[\\bar{h}]\\times S\\times A},\\ \\widehat{\\mathbf{d}}_{m-1}=(\\widehat{d}_{h,s,a,m-1})_{h,s,a\\in[H]\\times S\\times A}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We will use the variable $\\widetilde{\\mathbf{d}}_{m}^{\\bar{h}}$ to simulate the trusted occupancy measures and $\\widehat{\\mathbf{d}}_{m-1}$ to simulate the estimated occupancy measures from $\\widehat{M}_{m-1}$ by linear constraints. Concretely, consider the following ", "page_idx": 23}, {"type": "text", "text": "linear fractional program: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\operatorname*{max}}{\\mathbf{d}_{n}^{h}}\\ \\frac{\\widetilde{d}_{\\tilde{h},\\tilde{s},\\tilde{a},m}}{S A+\\eta_{m}(\\widetilde{V}_{m-1}^{1}-\\langle\\widehat{\\mathbf{d}}_{m-1},\\bar{\\mathbf{r}}_{m-1}\\rangle)},}\\\\ &{\\quad\\quad\\quad\\quad\\left(\\begin{array}{l l}{\\hat{d}_{h,s,a,m-1}\\geq0,}&{\\forall{h,s,a\\in[H]\\times S\\times A},}\\\\ {\\sum_{a}\\hat{d}_{1,s,a,m-1}=1(s=s^{1}),}&{\\forall s\\in S,}\\\\ {\\sum_{\\tilde{d}_{h,s,a},\\tilde{d}_{h,s,a,m-1}}\\widehat{P}_{m-1}^{h}(s^{\\prime}|s,a)=\\sum_{a}\\widehat{d}_{h+1,s^{\\prime},a,m-1},}&{\\forall{\\ h,s^{\\prime}\\in[H]\\times S},}\\\\ {\\vdots_{h,s,a,m}\\geq0,}&{\\forall h,s,a\\in[\\tilde{h}]\\times S\\times A}\\\\ {\\sum_{a}\\hat{d}_{1,s,a,m}=1(s=s^{1}),}&{\\forall s\\in S,}\\\\ {\\sum_{s,a,s^{\\prime}\\in\\tilde{Y}_{n}^{h-1}}\\widetilde{d}_{h-1,s,a,m}\\widehat{P}_{m}^{h-1}(s^{\\prime}|s,a)=\\sum_{a}\\widetilde{d}_{h,s^{\\prime},a,m}\\quad\\forall{\\ h\\leq\\tilde{h},s^{\\prime}\\in\\mathcal{S}}.}\\end{array}\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This is a linear fractional program of $H S A+\\bar{h}S A$ decision variables with $H S A+H S+\\bar{h}S A+\\bar{h}S$ constraints. It is clear from the linear constraints that this program simulates the MDPs $\\widehat{M}_{m-1}$ and $\\widehat{M}_{m}$ , and the program obtains the right objective. Then, for this linear fractional program, we apply the Charnes-Cooper transformation [7] to transform it into a linear program. After the transformation, we can apply existing tools for solving linear programs (e.g., Lee and Sidford [23]) to solve for $\\widetilde{\\mathbf{d}}_{m}^{\\bar{h}}$ which encodes the policy \u03c0\u00afhm,s\u00af in $\\operatorname{poly}(H S A)$ time. ", "page_idx": 24}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 25}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 25}, {"type": "text", "text": "\u2022 You should answer [Yes] , [No] , or [NA] .   \n\u2022 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u2022 Please provide a short (1\u20132 sentence) justification right after your answer (even for NA). ", "page_idx": 25}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 25}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 25}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 25}, {"type": "text", "text": "\u2022 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u2022 Keep the checklist subsection headings, questions/answers and guidelines below. \u2022 Do not modify the questions and only use the provided macros for your answers. 1. Claims ", "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] . ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. The claims are validated by detailed proofs. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper discusses the limitations of the work in the discussion section. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] . ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: The paper provides detailed assumptions and proofs. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 27}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. \u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 29}, {"type": "text", "text": "Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This is a theoretical work. There is no societal impact on the work performed. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 29}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 30}, {"type": "text", "text": "Justification: This paper does not use existing assets. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 31}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 31}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 31}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing or research with human subjects. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]