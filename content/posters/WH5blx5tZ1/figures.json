[{"figure_path": "WH5blx5tZ1/figures/figures_0_1.jpg", "caption": "Figure 1: TABULA-8B outperforms SOTA tabular baselines across 0 - 32-shot tasks from five tabular benchmarks.", "description": "The figure shows the zero-shot and few-shot performance of TABULA-8B compared to several state-of-the-art tabular prediction models (Llama 3 8B, XGBoost, TabPFN) across five benchmark datasets.  The x-axis represents the number of shots (training examples), and the y-axis represents the open-vocabulary accuracy. TABULA-8B consistently outperforms the baselines, demonstrating its effectiveness in transfer learning for tabular data. The improvement is particularly noticeable in the few-shot setting (1-32 shots), where TABULA-8B significantly surpasses models that have been explicitly trained on the target data.", "section": "Experimental Results"}, {"figure_path": "WH5blx5tZ1/figures/figures_3_1.jpg", "caption": "Figure 1: TABULA-8B outperforms SOTA tabular baselines across 0 - 32-shot tasks from five tabular benchmarks.", "description": "The figure displays a graph comparing the performance of TABULA-8B against other state-of-the-art tabular prediction models (Llama 3 8B, XGBoost, TabPFN) across different numbers of shots (from zero-shot to 32-shot).  The x-axis represents the number of shots, and the y-axis shows the open-vocabulary accuracy. The graph demonstrates that TABULA-8B consistently outperforms the other models, especially in the zero-shot and few-shot scenarios, highlighting its superior transfer learning capabilities.", "section": "1 Introduction"}, {"figure_path": "WH5blx5tZ1/figures/figures_7_1.jpg", "caption": "Figure 1: TABULA-8B outperforms SOTA tabular baselines across 0 - 32-shot tasks from five tabular benchmarks.", "description": "The figure shows a comparison of the performance of TABULA-8B against several state-of-the-art (SOTA) tabular prediction models across different numbers of shots (from zero-shot to 32-shot).  The x-axis represents the number of shots (training examples provided), and the y-axis represents the open-vocabulary accuracy.  TABULA-8B consistently outperforms XGBoost and TabPFN, demonstrating superior transfer learning capabilities, even with limited training data. The plot includes curves for Llama 3-8B (without fine-tuning), highlighting the effectiveness of the TABULA-8B fine-tuning process.", "section": "Abstract"}, {"figure_path": "WH5blx5tZ1/figures/figures_18_1.jpg", "caption": "Figure 1: TABULA-8B outperforms SOTA tabular baselines across 0 - 32-shot tasks from five tabular benchmarks.", "description": "The figure shows a comparison of the performance of TABULA-8B against other state-of-the-art (SOTA) tabular prediction models across different numbers of shots (from zero-shot to 32-shot).  The x-axis represents the number of shots, which indicates the amount of labeled training data provided to the models. The y-axis represents the open-vocabulary accuracy.  The plot demonstrates that TABULA-8B consistently outperforms the other models, achieving significantly higher accuracy with minimal labeled data, highlighting its effective transfer learning capabilities.", "section": "Abstract"}, {"figure_path": "WH5blx5tZ1/figures/figures_18_2.jpg", "caption": "Figure 1: TABULA-8B outperforms SOTA tabular baselines across 0-32-shot tasks from five tabular benchmarks.", "description": "The figure compares the performance of TABULA-8B against other state-of-the-art (SOTA) tabular prediction models on zero-shot and few-shot tasks.  It shows that TABULA-8B significantly outperforms the other methods, especially when the number of training examples is limited. The graph displays the open-vocabulary accuracy of each method across different numbers of shots (from 0 to 32). The five tabular benchmarks used are also indicated in the legend.", "section": "1 Introduction"}, {"figure_path": "WH5blx5tZ1/figures/figures_22_1.jpg", "caption": "Figure 1: TABULA-8B outperforms SOTA tabular baselines across 0 - 32-shot tasks from five tabular benchmarks.", "description": "The figure shows the performance comparison of TABULA-8B against state-of-the-art (SOTA) tabular prediction models (XGBoost and TabPFN) across different numbers of shots (0-32) on five tabular benchmarks.  The x-axis represents the number of shots (training examples provided to the model), and the y-axis represents the open-vocabulary accuracy.  The plot demonstrates that TABULA-8B consistently outperforms the SOTA baselines, especially in the zero-shot (no training examples) and few-shot scenarios.  This showcases TABULA-8B's ability to effectively transfer knowledge from a large-scale training dataset to new, unseen tabular prediction tasks.", "section": "Abstract"}, {"figure_path": "WH5blx5tZ1/figures/figures_22_2.jpg", "caption": "Figure 4: Zero- and few-shot accuracy across five tabular benchmarks. For each benchmark, we evaluate on all tasks, but in the figures above we only display the subset of tasks where k shots fit into the 8192-token context window of TABULA-8B. Complete results are in Supplementary Section H. The final plot (lower right) shows curves separately over decontaminated vs. potentially-contaminated evaluation tasks (see Section 5.6); we find no impact on our overall findings due to contamination (and performance on tasks which may be in our training set is lower on average, across all models).", "description": "This figure compares the zero-shot and few-shot performance of TABULA-8B against several baselines across five tabular benchmarks.  Each plot shows the accuracy for a given number of shots (examples provided to the model).  The plots display the model's ability to generalize to unseen data, highlighting its performance relative to other models trained explicitly on larger datasets.  The bottom-right plot specifically examines the impact of potential data contamination, concluding that it doesn't significantly affect the results.", "section": "5 Main Results: Assessing TABULA-8B's Transfer Learning"}, {"figure_path": "WH5blx5tZ1/figures/figures_23_1.jpg", "caption": "Figure 1: TABULA-8B outperforms SOTA tabular baselines across 0 - 32-shot tasks from five tabular benchmarks.", "description": "This figure compares the performance of TABULA-8B against other state-of-the-art tabular prediction models (XGBoost, TabPFN) across a range of different shots (0-32).  It shows that TABULA-8B consistently outperforms other models, particularly in the zero-shot and few-shot scenarios, demonstrating the model's ability to perform well with limited training data. The figure highlights TABULA-8B's superior transfer learning capabilities.", "section": "Experimental Results"}, {"figure_path": "WH5blx5tZ1/figures/figures_24_1.jpg", "caption": "Figure 1: TABULA-8B outperforms SOTA tabular baselines across 0 - 32-shot tasks from five tabular benchmarks.", "description": "This figure compares the performance of TABULA-8B against other state-of-the-art (SOTA) tabular prediction models across a range of shot settings (from zero-shot to 32-shot).  It shows that TABULA-8B consistently outperforms the other models, demonstrating its ability to achieve high accuracy with limited training data. The five tabular benchmarks used in the comparison represent diverse datasets and prediction tasks.", "section": "Abstract"}, {"figure_path": "WH5blx5tZ1/figures/figures_25_1.jpg", "caption": "Figure 4: Zero- and few-shot accuracy across five tabular benchmarks. For each benchmark, we evaluate on all tasks, but in the figures above we only display the subset of tasks where k shots fit into the 8192-token context window of TABULA-8B. Complete results are in Supplementary Section H. The final plot (lower right) shows curves separately over decontaminated vs. potentially-contaminated evaluation tasks (see Section 5.6); we find no impact on our overall findings due to contamination (and performance on tasks which may be in our training set is lower on average, across all models).", "description": "The figure displays the zero-shot and few-shot learning performance of the TABULA-8B model across five different tabular datasets.  It compares TABULA-8B's performance against several state-of-the-art baselines such as XGBoost and TabPFN.  The results are visualized using line graphs that show accuracy as a function of the number of shots (examples) provided to the model.  A key finding highlighted is TABULA-8B's superior performance, especially in low-shot scenarios, and its robustness to potential data contamination.", "section": "5 Main Results: Assessing TABULA-8B's Transfer Learning"}, {"figure_path": "WH5blx5tZ1/figures/figures_26_1.jpg", "caption": "Figure 1: TABULA-8B outperforms SOTA tabular baselines across 0 - 32-shot tasks from five tabular benchmarks.", "description": "The figure shows the performance of TABULA-8B compared to other state-of-the-art models across different shot settings (0-32 shots).  It demonstrates that TABULA-8B outperforms existing models even without fine-tuning on the specific tasks (zero-shot) and significantly improves with a small number of training examples (few-shot). The comparison includes XGBoost and TabPFN, which are commonly used in tabular prediction. The results are shown across five benchmark datasets, indicating consistent improvement across different tabular datasets.", "section": "1 Introduction"}, {"figure_path": "WH5blx5tZ1/figures/figures_27_1.jpg", "caption": "Figure 1: TABULA-8B outperforms SOTA tabular baselines across 0 - 32-shot tasks from five tabular benchmarks.", "description": "The figure shows the zero-shot and few-shot performance of TABULA-8B compared to other state-of-the-art tabular prediction models (Llama 3-8B, XGBoost, TabPFN) across five tabular benchmarks.  The x-axis represents the number of shots (examples provided to the model), and the y-axis represents the open-vocabulary accuracy.  TABULA-8B consistently outperforms the other models, particularly in the zero-shot and few-shot settings, demonstrating its superior transfer learning capabilities.", "section": "1 Introduction"}, {"figure_path": "WH5blx5tZ1/figures/figures_28_1.jpg", "caption": "Figure 1: TABULA-8B outperforms SOTA tabular baselines across 0 - 32-shot tasks from five tabular benchmarks.", "description": "The figure compares the performance of TABULA-8B against other state-of-the-art (SOTA) tabular prediction models across various shot settings (0-32 shots).  It shows that TABULA-8B consistently outperforms these baselines (XGBoost and TabPFN), achieving higher accuracy even without any fine-tuning on the target datasets (zero-shot). This demonstrates TABULA-8B's strong transfer learning capabilities on tabular data.", "section": "Abstract"}, {"figure_path": "WH5blx5tZ1/figures/figures_29_1.jpg", "caption": "Figure 4: Zero- and few-shot accuracy across five tabular benchmarks. For each benchmark, we evaluate on all tasks, but in the figures above we only display the subset of tasks where k shots fit into the 8192-token context window of TABULA-8B. Complete results are in Supplementary Section H. The final plot (lower right) shows curves separately over decontaminated vs. potentially-contaminated evaluation tasks (see Section 5.6); we find no impact on our overall findings due to contamination (and performance on tasks which may be in our training set is lower on average, across all models).", "description": "This figure compares the zero-shot and few-shot performance of TABULA-8B against several state-of-the-art baselines across five benchmark datasets.  Each plot shows accuracy across different numbers of training examples (shots).  The lower right plot specifically isolates results on tasks where contamination (overlapping datasets) may have occurred, showing this has minimal impact on performance.", "section": "5 Main Results: Assessing TABULA-8B's Transfer Learning"}, {"figure_path": "WH5blx5tZ1/figures/figures_29_2.jpg", "caption": "Figure 4: Zero- and few-shot accuracy across five tabular benchmarks. For each benchmark, we evaluate on all tasks, but in the figures above we only display the subset of tasks where k shots fit into the 8192-token context window of TABULA-8B. Complete results are in Supplementary Section H. The final plot (lower right) shows curves separately over decontaminated vs. potentially-contaminated evaluation tasks (see Section 5.6); we find no impact on our overall findings due to contamination (and performance on tasks which may be in our training set is lower on average, across all models).", "description": "This figure displays the performance of TABULA-8B and several baseline models on five different tabular datasets. It showcases the zero-shot (no training examples) and few-shot (small number of training examples) learning capabilities across various benchmark datasets. The results highlight TABULA-8B's superior performance compared to traditional methods, especially in zero-shot and few-shot scenarios.  The final subplot analyzes the model's robustness to potential data contamination.", "section": "5 Main Results: Assessing TABULA-8B's Transfer Learning"}, {"figure_path": "WH5blx5tZ1/figures/figures_31_1.jpg", "caption": "Figure 1: TABULA-8B outperforms SOTA tabular baselines across 0 - 32-shot tasks from five tabular benchmarks.", "description": "The figure shows a comparison of the zero-shot and few-shot performance of TABULA-8B against several state-of-the-art tabular prediction models (XGBoost, TabPFN) across five benchmark datasets.  The x-axis represents the number of shots (examples provided to the model), and the y-axis represents the open-vocabulary accuracy.  The figure demonstrates that TABULA-8B significantly outperforms the other models, especially in the zero-shot setting (0 shots) where it achieves an accuracy considerably higher than random guessing.  In the few-shot settings, TABULA-8B maintains a consistent lead over the baselines, showcasing its strong transfer learning capabilities.", "section": "1 Introduction"}, {"figure_path": "WH5blx5tZ1/figures/figures_31_2.jpg", "caption": "Figure 1: TABULA-8B outperforms SOTA tabular baselines across 0 - 32-shot tasks from five tabular benchmarks.", "description": "The figure displays a graph showing the zero-shot and few-shot performance of TABULA-8B against other state-of-the-art tabular prediction models (Llama 3-8B, XGBoost, and TabPFN).  The x-axis represents the number of shots (training examples), and the y-axis represents the open-vocabulary accuracy.  The graph demonstrates that TABULA-8B significantly outperforms the baselines across all shot settings, especially in the zero-shot scenario (0 shots). The superior performance highlights TABULA-8B's ability to effectively transfer learning to unseen tabular datasets.", "section": "1 Introduction"}]