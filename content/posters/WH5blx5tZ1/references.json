{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-XX-XX", "reason": "This paper is foundational to the field of large language models and demonstrates their ability to perform well on tasks they were not explicitly trained on, a core concept underlying transfer learning in this work."}, {"fullname_first_author": "Jacob Devlin", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2018-XX-XX", "reason": "BERT introduced the masked language modeling objective which has been highly influential in the development of transformer-based language models, a key architecture used in this work."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-XX-XX", "reason": "This paper demonstrates the effectiveness of transfer learning in the computer vision domain, providing further support for the generalizability of transfer learning across different modalities."}, {"fullname_first_author": "Leo Grinsztajn", "paper_title": "Why do tree-based models still outperform deep learning on tabular data?", "publication_date": "2022-XX-XX", "reason": "This paper provides a comprehensive analysis of the relative performance of tree-based models and deep learning models on tabular data, informing the design choices and baselines in this work."}, {"fullname_first_author": "Ruiyu Wang", "paper_title": "Unipredict: Large language models are universal tabular predictors", "publication_date": "2023-XX-XX", "reason": "This paper is the most closely related prior work and explores the use of LLMs for tabular prediction, although at a much smaller scale and without similar methods for scaling up the training data."}]}