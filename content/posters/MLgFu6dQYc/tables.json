[{"figure_path": "MLgFu6dQYc/tables/tables_14_1.jpg", "caption": "Table 1: Summary of formal assumptions about loss F used to prove algorithms' convergence in recent papers on zeroth order optimization, in different ML settings (see text for details). We use \"smoothness\" as a portmanteau for various conditions on the \u2265 1 order differentiability condition of F. \"conv.\" = convex, \"diff.\" = differentiable, \"Lip.\" = Lipschitz, \"Lb\" = lower-bounded, \"alt. GD\" = general alternative to gradient descent (stochastic or not), \"alt. FW\" = idem for Frank-Wolfe. Our paper relies on no such assumptions.", "description": "This table summarizes the assumptions made on the loss function (F) in various recent zeroth-order optimization algorithms.  It compares different properties of the loss functions, such as convexity, differentiability, Lipschitz continuity, and smoothness. The table also indicates whether the algorithms are focused on online ML, distributed ML, saddle point optimization, or use alternative methods such as Frank-Wolfe. The key takeaway is that the current paper's algorithm makes no assumptions about the loss function.", "section": "Related work"}]