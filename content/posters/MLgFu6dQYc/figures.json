[{"figure_path": "MLgFu6dQYc/figures/figures_2_1.jpg", "caption": "Figure 1: Left: value of SF|v(z'||z) for convex F, v = z4 \u2212 z and various z' (colors), for which the Bregman Secant distortion is positive (z' = z1, green), negative (z' = z2, red), minimal (z' = z3) or null (z' = z4, z). Right: depiction of QF(z, z + v, z') for non-convex F (Definition 4.6).", "description": "The left panel shows the Bregman Secant distortion for a convex function F.  The distortion is positive, negative, minimal or null depending on the choice of z'. The right panel shows the Optimal Bregman Information (OBI) for a non-convex function F, illustrating the concept of the maximal difference between the line passing through (a,F(a)) and (b, F(b)) and the function F in the interval [a,c].", "section": "4 v-derivatives and Bregman secant distortions"}, {"figure_path": "MLgFu6dQYc/figures/figures_7_1.jpg", "caption": "Figure 2: Simplified depiction of W2,t \"regimes\" (Assumption 5.5). We only plot the components of the v-derivative part in (8): removing index i for readability, we get d{et,vt\u22121}F(\u0113t\u22121) = (Bt \u2212 At)/(yHt(x) \u2212 yHt\u22121(x)) with At = \u03b4vt\u22121F(yHt\u22121(x)) = \u2212wt and Bt = \u03b4vt\u22121F(yHt(x)) (= \u2212wt+1 iff vt\u22121 = vt). If the loss is \"nice\" like the exponential or logistic losses, we always have a small W2,t (a). Place a bump in the loss (b-d) and the risk happens that W2,t is too large for the WRA to hold. Workarounds include two strategies: picking small enough offsets (b) or fit offsets large enough to pass the bump (c). The blue arrow in (d) is discussed in Section 6.", "description": "This figure illustrates different scenarios for the quantity W2,t in Assumption 5.5, which is related to the variation of weights in the boosting algorithm.  Panel (a) shows a \"nice\" loss function where W2,t remains small. Panels (b) through (d) depict scenarios where a \"bump\" in the loss function can cause W2,t to become large.  Panels (c) and (d) propose strategies to mitigate this issue either by selecting smaller offsets or using larger offsets to bypass the problematic area.", "section": "5.1 Algorithm: SECBOOST"}, {"figure_path": "MLgFu6dQYc/figures/figures_8_1.jpg", "caption": "Figure 3: A simple way to build Iti(z) for a discontinuous loss F (\u0113ti < \u0113(t\u22121)i and z are represented), O being the set of solutions as it is built. We rotate two half-lines, one passing through (\u0113ti, F(\u0113ti)) (thick line, (\u25b3)) and a parallel one translated by -z (dashed line) (a). As soon as (\u2206) crosses F on any point (z', F(z')) with z \u2260 \u011bti while the dashed line stays below F, we obtain a candidate offset v for OO, namely v = z' \u2013 \u011bti. In (b), we obtain an interval of values. We keep on rotating (\u0394), eventually making appear several intervals for the choice of v if F is not convex (c). Finally, when we reach an angle such that the maximal difference between (\u25b3) and F in [\u0113ti, \u0113(t\u22121)i] is z (z can be located at an intersection between F and the dashed line), we stop and obtain the full Iti(z) (d).", "description": "This figure illustrates a simple method to construct the set Iti(z) for a discontinuous loss function F.  The method involves rotating two half-lines and identifying points where one line intersects the function F while the other remains below F.  The process generates candidate offsets (v) that are collected to form the final set Iti(z).", "section": "Implementation of the offset oracle"}, {"figure_path": "MLgFu6dQYc/figures/figures_8_2.jpg", "caption": "Figure 3: A simple way to build Iti(z) for a discontinuous loss F (\u0113ti < \u0113(t\u22121)i and z are represented), O being the set of solutions as it is built. We rotate two half-lines, one passing through (\u0113ti, F(\u0113ti)) (thick line, (\u25b3)) and a parallel one translated by -z (dashed line) (a). As soon as (\u2206) crosses F on any point (z', F(z')) with z \u2260 \u011bti while the dashed line stays below F, we obtain a candidate offset v for OO, namely v = z' \u2013 \u011bti. In (b), we obtain an interval of values. We keep on rotating (\u0394), eventually making appear several intervals for the choice of v if F is not convex (c). Finally, when we reach an angle such that the maximal difference between (\u25b3) and F in [\u0113ti, \u0113(t\u22121)i] is z (z can be located at an intersection between F and the dashed line), we stop and obtain the full Iti(z) (d).", "description": "This figure illustrates a simple method for constructing the set Iti(z) for a discontinuous loss function F. The method involves rotating two half-lines and identifying points where one half-line intersects the function F while the other remains below F.  The process iteratively builds the set Iti(z) by identifying candidate offsets, and the complete set is obtained when the maximum difference between one half-line and F reaches the target value z. The figure showcases the construction process for both convex and non-convex functions, highlighting how the structure of Iti(z) can vary.", "section": "5.4 Implementation of the offset oracle"}, {"figure_path": "MLgFu6dQYc/figures/figures_16_1.jpg", "caption": "Figure 5: Left: representation of the difference of averages in (22). Each of the secants (\u25b3\u2081) and (A2) can take either the red or black segment. Which one is which depends on the signs of c and b, but the general configuration is always the same. Note that if F is convex, one necessarily sits above the other, which is the crux of the proof of Lemma 5.2. For the sake of illustration, suppose we can analytically have b, c \u2192 0. As c converges to 0 but b remains > 0, 8{b,c}F(a) becomes proportional to the variation of the average secant midpoint; the then-convergence of b to 0 makes d{b,c}F(a) converge to the second-order derivative of F at a. Right: in the special case where F is convex, one of the secants always sits above the other.", "description": "The figure demonstrates the relationship between the Bregman Secant distortions and Bregman divergences. The left panel shows how the difference of averages in equation (22) is related to the secants (\u25b3\u2081) and (\u25b32).  It illustrates that if the function F is convex, one secant will always sit above the other. The right panel focuses on the specific case where F is convex, emphasizing that one secant will always sit above the other.  This geometric illustration helps to explain why the Bregman Secant distortion is a useful generalization of the Bregman divergence.", "section": "4 v-derivatives and Bregman secant distortions"}, {"figure_path": "MLgFu6dQYc/figures/figures_22_1.jpg", "caption": "Figure 6: The spring loss in (53) is neither convex, nor Lipschitz or differentiable and has an infinite number of local minima. Yet, an implementation of the offset oracle is trivial as an output for OO can be obtained from the computation of a single tangent point (here, the orange v, see text; best viewed in color).", "description": "Figure 6 shows a spring loss function, which is neither convex, nor Lipschitz, nor differentiable and has infinitely many local minima.  Despite this, a trivial implementation of the offset oracle is possible.  The orange line segment illustrates how a single tangent point can be computed to obtain the offset, simplifying the offset oracle computation.", "section": "Implementation of the offset oracle"}, {"figure_path": "MLgFu6dQYc/figures/figures_22_2.jpg", "caption": "Figure 7: Computing the OBI QF(z, z + v, z + v) for F convex, (z, v) being given and v > 0. We compute the line (\u25b3t) crossing F at any point t, with slope equal to the secant [(z, F(z)), (z + v, F(z + v))] and then the difference between F at z + v and this line at z + v. We move t so as to maximize this difference. The optimal t (in green) gives the corresponding OBI. In (56) and (58), we are interested in finding v given this difference, r. We also need to replicate this computation for v < 0.", "description": "This figure illustrates how to compute the Optimal Bregman Information (OBI) for a convex function F.  It shows how to find the value of v that maximizes the difference between the secant connecting points (z, F(z)) and (z+v, F(z+v)) and the function F itself within the interval [z, z+v]. The optimal value of v is found when the maximal difference between the secant and the function is equal to r.", "section": "4 v-derivatives and Bregman secant distortions"}, {"figure_path": "MLgFu6dQYc/figures/figures_24_1.jpg", "caption": "Figure 8: Case F strictly convex, with two cases of limit OBI z and z' in Iti(.). Example i has eti > 0 and \u1ebd(t\u22121)i > 0 (??) large enough (hence, edges with respect to weak classifiers ht and ht\u22121 large enough) so that Iti(z) \u2229 I(t\u22121)i(z) = I(t\u22121)i(z) \u2229 I(t\u22122)i(z) = Iti(z) \u2229 \u2161(t\u22122)i(z) = \u00d8. In this case, regardless of the offsets chosen by 00, we are guaranteed that its weights satisfy W(t+1)i < Wti < W(t\u22121)i, which follows the boosting pattern that examples receiving the right classification by weak classifiers have their weights decreasing. If however the limit OBI changes from z to a larger z', this is not guaranteed anymore: in this case, it may be the case that w(t+1)i > Wti.", "description": "This figure shows two scenarios for a strictly convex function F.  The first scenario (smaller z) demonstrates that if the intervals generated by the algorithm (Iti(z)) are disjoint, then the weights will decrease monotonically with each iteration, following standard boosting behavior. The second scenario (larger z') illustrates that if the intervals are not disjoint, then this weight decrease is not guaranteed and the weights may increase in subsequent iterations.", "section": "B.10 A boosting pattern that can \"survive\" above differentiability"}, {"figure_path": "MLgFu6dQYc/figures/figures_25_1.jpg", "caption": "Figure 9: How our algorithm works with the 0/1 loss (in red): at the initialization stage, assuming we pick ho = 0 for simplicity and some vo < 0, all training examples get the same weight, given by negative the slope of the thick blue dashed line. All weights are thus > 0. At iteration t when we update the weights (Step 2.6), one of two cases can happen on some training example (x, y). In (A), the edge of the strong model remains the same: either both are positive (blue) or both negative (olive green) (the ordering of edges is not important). In this case, regardless of the offset, the new weight will be 0. In (B), both edges have different sign (again, the ordering of edges is not important). In this case, the examples will keep non-zero weight over the next iteration. See text below for details.", "description": "Figure 9 shows how the algorithm handles the 0/1 loss.  Initialization gives all examples positive weights. During weight updates, if the strong model's edge remains the same sign (positive or negative), the next weight will be 0. If the signs are different, the next weight will be non-zero.", "section": "B.11 The case of piecewise constant losses for SOLVE"}, {"figure_path": "MLgFu6dQYc/figures/figures_26_1.jpg", "caption": "Figure 3: A simple way to build Iti(z) for a discontinuous loss F (\u0113ti < \u0113(t\u22121)i and z are represented), O being the set of solutions as it is built. We rotate two half-lines, one passing through (\u0113ti, F(\u0113ti)) (thick line, (\u25b3)) and a parallel one translated by -z (dashed line) (a). As soon as (\u25b3) crosses F on any point (z', F(z')) with z' \u2260 \u011bti while the dashed line stays below F, we obtain a candidate offset v for OO, namely v = z' \u2013 \u011bti. In (b), we obtain an interval of values. We keep on rotating (\u0394), eventually making appear several intervals for the choice of v if F is not convex (c). Finally, when we reach an angle such that the maximal difference between (\u25b3) and F in [\u0113ti, \u0113(t\u22121)i] is z (z can be located at an intersection between F and the dashed line), we stop and obtain the full Iti(z) (d).", "description": "This figure illustrates a method for building the set Iti(z) for a discontinuous loss function F. The method involves rotating two half-lines, one passing through (\u0113ti, F(\u0113ti)) and the other translated by -z, and identifying points where the first half-line intersects F while the second remains below F.  Each intersection point provides a candidate offset v. The process continues until the maximal difference between the rotated half-line and F within the interval [\u0113ti, \u0113(t\u22121)i] equals z. The resulting set of offsets v forms Iti(z). Different panels show the process for various stages and conditions.", "section": "5.4 Implementation of the offset oracle"}, {"figure_path": "MLgFu6dQYc/figures/figures_27_1.jpg", "caption": "Figure 11: Crops of the two losses whose optimization has been experimentally tested with SEC-BOOST, in addition to the logistic loss. See text for details.", "description": "The figure shows plots of two loss functions used in the experiments: the clipped logistic loss and the spring loss.  These loss functions are compared to the standard logistic loss to highlight their non-standard properties (non-convexity, non-differentiability, etc.).  The figure provides visual context for the challenges presented by these loss functions in the optimization experiments described in the paper.", "section": "C.3 A toy experiments"}, {"figure_path": "MLgFu6dQYc/figures/figures_29_1.jpg", "caption": "Figure 12: Experiments on UCI tictactoe showing estimated test errors after minimizing each of the three losses we consider, with varying training noise level \u03b7, max tree size and initial hyperparameter \u03b4 value in (60). See text.", "description": "This figure displays the results of experiments conducted on the UCI tic-tac-toe dataset using the SECBOOST algorithm.  The experiments compare the performance of SECBOOST when minimizing three different loss functions: the logistic loss, the clipped logistic loss, and the spring loss.  The results are shown for different levels of training noise (\u03b7), maximum tree size, and initial hyperparameter (\u03b4). Each combination of parameters is represented by a different colored line and shaded region. The shaded regions represent the confidence intervals of the results. The x-axis represents the number of trees used in the boosting algorithm and y-axis represents the test error rate.", "section": "C.3 A toy experiments"}]