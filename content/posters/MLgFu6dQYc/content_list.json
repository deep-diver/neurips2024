[{"type": "text", "text": "How to Boost Any Loss Function ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Richard Nock Google Research richardnock@google.com ", "page_idx": 0}, {"type": "text", "text": "Yishay Mansour   \nTel Aviv University   \nGoogle Research   \nmansour@google.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Boosting is a highly successful ML-born optimization setting in which one is required to computationally efficiently learn arbitrarily good models based on the access to a weak learner oracle, providing classifiers performing at least slightly differently from random guessing. A key difference with gradient-based optimization is that boosting\u2019s original model does not requires access to first order information about a loss, yet the decades long history of boosting has quickly evolved it into a first order optimization setting \u2013 sometimes even wrongfully defining it as such. Owing to recent progress extending gradient-based optimization to use only a loss\u2019 zeroth $(0^{t h})$ order information to learn, this begs the question: what loss functions can be efficiently optimized with boosting and what is the information really needed for boosting to meet the original boosting blueprint\u2019s requirements? ", "page_idx": 0}, {"type": "text", "text": "We provide a constructive formal answer essentially showing that any loss function can be optimized with boosting and thus boosting can achieve a feat not yet known to be possible in the classical $0^{t h}$ order setting, since loss functions are not required to be be convex, nor differentiable or Lipschitz \u2013 and in fact not required to be continuous either. Some tools we use are rooted in quantum calculus, the mathematical field \u2013 not to be confounded with quantum computation \u2013 that studies calculus without passing to the limit, and thus without using first order information. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In ML, zeroth order optimization has been devised as an alternative to techniques that would otherwise require access to $\\geqslant1$ -order information about the loss to minimize, such as gradient descent (stochastic or not, constrained or not, etc., see Section 2). Such approaches replace the access to a so-called oracle providing derivatives for the loss at hand, operations that can be consuming or not available in exact form in the ML world, by the access to a cheaper function value oracle, providing loss values at queried points. ", "page_idx": 0}, {"type": "text", "text": "Zeroth order optimization has seen a considerable boost in ML over the past years, over many settings and algorithms, yet, there is one foundational ML setting and related algorithms that, to our knowledge, have not yet been the subject of investigations: boosting [32, 31]. Such a question is very relevant: boosting has quickly evolved as a technique requiring first-order information about the loss optimized [6, Section 10.3], [41, Section 7.2.2] [53]. It is also not uncommon to find boosting reduced to this first-order setting [9]. However, originally, the boosting model did not mandate the access to any first-order information about the loss, rather requiring access to a weak learner providing classifiers at least slightly different from random guessing [31]. In the context of zeroth-order optimization gaining traction in ML, it becomes crucial to understand not just whether differentiability is necessary for boosting, but more generally what are loss functions that can be boosted with a weak learner and in fine where boosting stands with respect to recent formal progress on lifting gradient descent to zeroth-order optimisation. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we settle the question: we design a formal boosting algorithm for any loss function whose set of discontinuities has zero Lebesgue measure. With traditional floating point encoding (e.g. float64), any stored loss function would de facto meet this condition; mathematically speaking, we encompass losses that are not necessarily convex, nor differentiable or Lipschitz. This is a key difference with classical zeroth-order optimization results where the algorithms are zeroth-order but their proof of convergence makes various assumptions about the loss at hand, such as convexity, differentiability (once or twice), Lipschitzness, etc. . Our proof technique builds on a simple boosting technique for convex functions that relies on an order-one Taylor expansion to bound the progress between iterations [45]. Using tools from quantum calculus\\*, we replace this progress using $v$ -derivatives and a quantity related to a generalisation of the Bregman information [7]. The boosting rate involves the classical weak learning assumption\u2019s advantage over random guessing and a new parameter bounding the ratio of the expected weights (squared) over a generalized notion of curvature involving $v$ -derivatives. Our algorithm, which learns a linear model, introduces notable generalisations compared to the AdaBoost / gradient boosting lineages, chief among which the computation of acceptable offsets for the $v$ -derivatives used to compute boosting weights, offsets being zero for classical gradient boosting. To preserve readability and save space, all proofs and additional information are postponed to an Appendix. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Over the past years, ML has seen a substantial push to get the cheapest optimisation routines, in general batch [14], online [27], distributed [3], adversarial [20, 18] or bandits settings [2] or more specific settings like projection-free [26, 28, 51] or saddle-point optimisation [25, 38]. We summarize several dozen recent references in Table 1 in terms of assumptions for the analysis about the loss optimized, provided in Appendix, Section A. Zeroth-order optimization reduces the information available to the learner to the \"cheapest\" one which consists in (loss) function values, usually via a so-called function value oracle. However, as Table 1 shows, the loss itself is always assumed to have some form of \"niceness\" to study the algorithms\u2019 convergence, such as differentiability, Lipschitzness, convexity, etc. . Another quite remarkable phenomenon is that throughout all their diverse settings and frameworks, not a single one of them addresses boosting. Boosting is however a natural candidate for such investigations, for two reasons. First, the most widely used boosting algorithms are first-order information hungry [6, 41, 53]: they require access to derivatives to compute examples\u2019 weights and classifiers\u2019 leveraging coefficients. Second and perhaps most importantly, unlike other optimization techniques like gradient descent, the original boosting model does not mandate the access to a first-order information oracle to learn, but rather to a weak learning oracle which supplies classifiers performing slightly differently from random guessing [32, 31]. Only few approaches exist to get to \"cheaper\u201d algorithms relying on less assumptions about the loss at hand, and to our knowledge do not have boosting-compliant convergence proofs, as for example when alleviating convexity [16, 46] or access to gradients of the loss [54]. Such questions are however important given the early negative results on boosting convex potentials with first-order information [37] and the role of the classifiers in the negative results [39]. ", "page_idx": 1}, {"type": "text", "text": "Finally, we note that a rich literature has developed in mathematics as well for derivative-free optimisation [34], yet methods would also often rely on assumptions included in the three above (e.g. [42]). It must be noted however that derivative-free optimisation has been implemented in computers for more than seven decades [24]. ", "page_idx": 1}, {"type": "text", "text": "3 Definitions and notations ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The following shorthands are used: $\\begin{array}{r l r}{[n]}&{{}\\doteq}&{\\{1,2,...,n\\}}\\end{array}$ for $\\begin{array}{r l r}{n}&{{}\\in}&{\\mathbb{N}_{*}}\\end{array}$ , $z\\ \\cdot\\ [a,b]\\ \\ \\doteq$ \u201c $[\\operatorname*{min}\\{z a,z b\\},\\operatorname*{max}\\{z a,z b\\}]$ for $z\\in\\mathbb{R},a\\leqslant b\\in\\mathbb{R}$ . In the batch supervised learning setting, one is given a training set of $m$ examples $S\\doteq\\{(\\mathbf{\\boldsymbol{x}}_{i},y_{i}),i\\in[m]\\}$ , where $\\mathbf{\\boldsymbol{x}}_{i}\\in\\mathcal{X}$ is an observation ( $\\mathcal{X}$ is called the domain: often, $\\mathcal{X}\\subseteq\\mathbb{R}^{d},$ and $y_{i}\\in\\mathfrak{Y}\\doteq\\{-1,1\\}$ is a label, or class. We study the empirical convergence of boosting, which requires fast convergence on training. Such a setting is standard in zeroth order optimization [42]. Also, investigating generalization would entail specific design choices about the loss at hand and thus would restrict the scope of our result (see e.g. [8]). The objective is to learn a classifier, i.e. a function $h:\\mathcal{X}\\to\\mathbb{R}$ which belongs to a given set $\\mathcal{H}$ . The goodness of fit of some $h$ on $S$ is evaluated from a given function $F:\\mathbb{R}\\rightarrow\\mathbb{R}$ called a loss function, whose expectation on training is sought to be minimized: ", "page_idx": 1}, {"type": "image", "img_path": "MLgFu6dQYc/tmp/b2af15f34ad5bf91dabf45a00f78442eaaf25d770288c6aa8ef9b2d8f9d9a363.jpg", "img_caption": ["Figure $1\\colon L e f t$ : value of $S_{F\\mid v}(z^{\\prime}\\|z)$ for convex $F$ , $v\\doteq z_{4}-z$ and various $z^{\\prime}$ (colors), for which the Bregman Secant distortion is positive ${\\bf\\nabla}z^{\\prime}=z_{1}$ , green), negative $\\langle z^{\\prime}=z_{2}$ , red), minimal $\\langle z^{\\prime}=z_{3}\\rangle$ ) or null $\\langle z^{\\prime}=z_{4},z\\rangle$ . Right: depiction of $Q_{F}(z,z+v,z^{\\prime})$ for non-convex $F$ (Definition 4.6). "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\nF(S,h)\\doteq\\mathsf{E}_{i\\sim[m]}[F(y_{i}h(\\pmb{x}_{i}))].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The set of most popular losses comprises convex functions: the exponential loss $(F_{\\mathrm{EXP}}(z)\\doteq\\exp(-z))$ , the logistic loss $(\\bar{F}^{\\mathrm{LoG}}(z)\\doteq\\log(1+\\exp(-z)))$ , the square loss $\\ \\bar{(F_{\\mathrm{sQ}}(z)=(1-z)^{2})}$ , the Hinge loss $(F_{\\mathrm{H}}(z)\\doteq\\operatorname*{max}\\{0,1-z\\})$ . These are surrogate losses because they all define upperbounds of the 0/1-loss $(F_{0/1}(z)\\doteq1_{z\\leqslant0}$ , \"1\u201d being the indicator variable). ", "page_idx": 2}, {"type": "text", "text": "Our ML setting is that of boosting [31]. It consists in having primary access to a weak learner WL that when called, provides so-called weak hypotheses, weak because barely anything is assumed in terms of classification performance relatively to the sample over which they were trained. Our goal is to devise a so-called \"boosting\" algorithm that can take any loss $F$ as input and training sample $S$ and a target loss value $F_{*}$ and after some $T$ calls to the weak learner crafts a classifier $H_{T}$ satisfying $F(S,\\bar{H}_{T})\\leqslant F_{*}$ , where $T$ depends on various parameters of the ML problem. Our boosting architecture is a linear model: $\\textstyle H_{T}\\doteq\\sum_{t}\\alpha_{t}h_{t}$ where each $h_{t}$ is an output from the weak learner and leveraging coefficients $\\alpha_{t}$ have to  b\u0159e computed during boosting. Notice that this is substantially more general than the classical boosting formulation where the loss would be fixed or belong to a restricted subset of functions. ", "page_idx": 2}, {"type": "text", "text": "4 $v$ -derivatives and Bregman secant distortions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Unless otherwise stated, in this Section, $F$ is a function defined over $\\mathbb{R}$ . ", "page_idx": 2}, {"type": "text", "text": "Definition 4.1. [30] For any $z,v\\in\\mathbb{R}$ , we let $\\delta_{v}F(z)\\doteq(F(z+v)\\!-\\!F(z))/v$ denote the v-derivative of $F$ in $z$ . ", "page_idx": 2}, {"type": "text", "text": "This expression, which gives the classical derivative when the offset $v\\rightarrow0$ , is called the $h$ -derivative in quantum calculus [30, Chapter 1]. We replaced the notation for the risk of confusion with classifiers. Notice that the $v$ -derivative is just the slope of the secant that passes through points $\\left(z,F(z)\\right)$ and $(z+v,F(z+v))$ (Figure 1). Higher order $v$ -derivatives can be defined with the same offset used several times [30]. Here, we shall need a more general definition that accommodates for variable offsets. ", "page_idx": 2}, {"type": "text", "text": "Definition 4.2. Let $v_{1},v_{2},...,v_{n}\\in\\mathbb{R}$ and $\\mathcal{V}\\doteq\\{v_{1},v_{2},...,v_{n}\\}$ and $z\\in\\mathbb{R}$ . The $\\boldsymbol{\\mathcal{V}}$ -derivative $\\delta_{\\ensuremath{\\stackrel{\\s}{\\v{v}}}}F$ is: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\delta_{\\mathcal{V}}F(z)\\doteq\\left\\{\\begin{array}{r l}{F(z)}&{\\ i f\\quad\\mathcal{V}=\\emptyset}\\\\ {\\delta_{v_{1}}F(z)}&{\\ i f\\quad\\mathcal{V}=\\{v_{1}\\}}\\\\ {\\delta_{\\{v_{n}\\}}(\\delta_{\\mathcal{V}\\backslash\\{v_{n}\\}}F)(z)}&{\\ o t h e r w i s e}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "If $v_{i}=v,\\forall i\\in[n]$ then we write $\\delta_{v}^{(n)}F(z)\\doteq\\delta_{\\nabla}F(z)$ . ", "page_idx": 2}, {"type": "text", "text": "In the Appendix, Lemma B.1 computes the unravelled expression of $\\delta\\gamma F(z)$ , showing that the order of the elements in $\\mathcal{V}$ does not matter; $n$ is called the order of the $\\mathcal{V}$ -derivative. ", "page_idx": 2}, {"type": "text", "text": "We can now define a generalization of Bregman divergences called Bregman Secant distortions. ", "page_idx": 2}, {"type": "text", "text": "Definition 4.3. For any $z,z^{\\prime},v\\in\\mathbb{R},$ , the Bregman Secant distortion $S_{F\\mid v}(z^{\\prime}\\|z)$ with generator $F$ and offset v is: ", "page_idx": 3}, {"type": "equation", "text": "$$\nS_{F|v}(z^{\\prime}\\|z)\\doteq F(z^{\\prime})-F(z)-(z^{\\prime}-z)\\delta_{v}F(z).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Even if $F$ is convex, the distortion is not necessarily positive, though it is lowerbounded (Figure 1).   \nThere is an intimate relationship between the Bregman Secant distortions and Bregman divergences.   \nWe shall use a definition slightly more general than the original one when $F$ is differentiable [11, eq.   \n(1.4)], introduced in information geometry [5, Section 3.4] and recently reintroduced in ML [10]. ", "page_idx": 3}, {"type": "text", "text": "Definition 4.4. The Bregman divergence with generator $F$ (scalar, convex) between $z^{\\prime}$ and $z$ is $D_{F}(z^{\\prime}\\|z)\\doteq F(z^{\\prime})+F^{\\star}(z)-z^{\\prime}z$ , where $\\begin{array}{r}{F^{\\star}(z)\\doteq\\operatorname*{sup}_{t}t z-F(t)}\\end{array}$ is the convex conjugate of $F$ . ", "page_idx": 3}, {"type": "text", "text": "We state the link between $S_{F\\mid v}$ and $D_{F}$ (proof omitted). ", "page_idx": 3}, {"type": "text", "text": "Lemma 4.5. Suppose $F$ strictly convex differentiable. Then $\\begin{array}{r}{\\operatorname*{lim}_{v\\to0}S_{F|v}(z^{\\prime}\\|z)=D_{F}(z^{\\prime}\\|F^{\\prime}(z)).}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "Relaxed forms of Bregman divergences have been introduced in information geometry [43]. ", "page_idx": 3}, {"type": "text", "text": "Definition 4.6. For any $a,b,\\alpha\\in\\mathbb{R},$ , denote for short $\\mathbb{I}_{a,b}\\,\\doteq\\,[\\operatorname*{min}\\{a,b\\},\\operatorname*{max}\\{a,b\\}]$ and $(u v)_{\\alpha}\\doteq$ $\\alpha u+(1-\\alpha)v$ . The Optimal Bregman Information $(O B I)$ of $F$ defined by triple $(a,b,c)\\in\\mathbb{R}^{3}$ is: ", "page_idx": 3}, {"type": "equation", "text": "$$\nQ_{F}(a,b,c)\\doteq\\operatorname*{max}_{\\alpha:(a b)_{\\alpha}\\in\\mathbb{I}_{a,c}}\\{(F(a)F(b))_{\\alpha}-F((a b)_{\\alpha})\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "As represented in Figure 1 (right), the OBI is obtained by drawing the line passing through $(a,F(a))$ and $\\bar{(b,F(b))}$ and then, in the interval $\\mathbb{I}_{a,c}$ , look for the maximal difference between the line and $F$ . We note that $Q_{F}$ is non negative because $a\\in\\mathbb{I}_{a,c}$ and for the choice $\\alpha=1$ , the RHS in (1) is 0. We also note that when $F$ is convex, the RHS is indeed the maximal Bregman information of two points in [7, Definition 2], where maximality is obtained over the probability measure. The following Lemma follows from the definition of the Bregman secant divergence and the OBI. An inspection of the functions in Figure 1 provides a graphical proof. ", "page_idx": 3}, {"type": "text", "text": "Lemma 4.7. For any $F$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\forall z,v,z^{\\prime}\\in\\mathbb{R},S_{F|v}(z^{\\prime}\\|z)\\geqslant-Q_{F}(z,z+v,z^{\\prime}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and if $F$ is convex, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\forall z,v\\in\\mathbb{R},\\forall z^{\\prime}\\notin\\mathbb{I}_{z,z+v},S_{F|v}(z^{\\prime}\\|z)\\geqslant0,}\\\\ {\\forall z,v,z^{\\prime}\\in\\mathbb{R},S_{F|v}(z^{\\prime}\\|z)\\geqslant-Q_{F}(z,z+v,z+v).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We shall abbreviate the two possible forms of OBI in the RHS of (2), (3) as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nQ_{F}^{*}(z,z^{\\prime},v)\\doteq\\left\\{\\begin{array}{c l}{{Q_{F}(z,z+v,z+v)}}&{{\\mathrm{~if~}F\\;\\mathrm{convex}}}\\\\ {{Q_{F}(z,z+v,z^{\\prime})}}&{{\\mathrm{~otherwise}}}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "5 Boosting using only queries on the loss ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We make the assumption that predictions of so-called \"weak classifiers\" are finite and non-zero on training without loss of generality (otherwise a simple tweak ensures it without breaking the weak learning framework, see Appendix, Section B.2). Excluding 0 ensures our algorithm does not make use of derivatives. ", "page_idx": 3}, {"type": "text", "text": "Assumption 5.1. $\\begin{array}{r}{\\mathrm{.~}\\forall t>0,\\forall i\\in[m],\\,|h_{t}(\\pmb{x_{i}})|\\in(0,+\\infty)\\,(w e\\,t h u s\\,l e t\\,M_{t}\\doteq\\mathrm{max}_{i}\\,|h_{t}(\\pmb{x_{i}})|).}\\end{array}$ ", "page_idx": 3}, {"type": "text", "text": "For short, we define two edge quantities for $i\\in[m]$ and $t=1,2,\\ldots$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\ne_{t i}\\doteq\\alpha_{t}\\cdot y_{i}h_{t}({\\pmb x}_{i}),\\quad\\tilde{e}_{t i}\\doteq y_{i}H_{t}({\\pmb x}_{i}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\alpha_{t}$ is a leveraging coefficient for the weak classifiers in an ensemble $\\begin{array}{r}{H_{T}(.)\\doteq\\sum_{t\\in[T]}\\alpha_{t}h_{t}(.)}\\end{array}$ . We observe ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\tilde{e}_{t i}=\\tilde{e}_{(t-1)i}+e_{t i}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Algorithm 1 SECBOOST(S, T) // red boxes pinpoint substantial differences with \"classical\" boosting ", "page_idx": 4}, {"type": "text", "text": "Input sample $S\\,=\\,\\{({\\pmb x}_{i},y_{i}),i\\,=\\,1,2,...,m\\}$ , number of iterations $T$ , initial $(h_{0},v_{0})$ (constant   \nclassification and offset).   \nStep $1:$ let $H_{0}\\leftarrow1\\cdot h_{0}$ and $\\begin{array}{r}{|{\\bf w}_{1}=-\\delta_{v_{0}}F(h_{0})\\cdot{\\bf1}|,}\\end{array}$ $\\eta\\,h_{0},v_{0}\\neq0$ chosen s. t. $\\updelta_{v_{0}}F(h_{0})\\neq0$   \nStep $2:$ for $t=1,2,...,T$ Step 2.1 : let $h_{t}\\gets\\mathrm{wL}(S_{t},|\\mathbf{w}_{t}|)$ //weak learner call, $\\boxed{S_{t}\\doteq\\{(\\pmb{x}_{i},y_{i}\\cdot\\mathrm{sign}(w_{t i}))\\}}$ Step 2.2 : let $\\begin{array}{r}{\\eta_{t}\\gets(1/m)\\cdot\\sum_{i}w_{t i}y_{i}h_{t}(\\pmb{x}_{i})}\\end{array}$ //unnormalized edge If bound on $\\overline{{W}}_{2,t}$ av ailable (Section 5.3) otherwise | general procedure Step 2.3 : ${\\overline{{\\operatorname{pick}\\varepsilon_{t}>0,\\pi_{t}\\in(0,1)}}}$ $\\alpha_{t}\\in\\frac{\\eta_{t}}{2(1+\\varepsilon_{t})M_{t}^{2}\\overline{{W}}_{2,t}}\\cdot\\left[1-\\pi_{t},1+\\pi_{t}\\right];$ and\u00a8 (6) $//\\,\\overline{{W}}_{2,t}>0,\\varepsilon_{t}>0,\\pi_{t}\\in(0,1)$ // Theorem 5.8 Step 2.4 : let $H_{t}\\leftarrow H_{t-1}+\\alpha_{t}\\cdot h_{t}$ //classifier update Step 2.5 : if $\\mathbb{I}_{t i}(\\varepsilon_{t}\\cdot\\alpha_{t}^{2}M_{t}^{2}\\overline{{W}}_{2,t})\\neq\\varnothing,\\forall i\\in[m]$ then //new offsets for $i=1,2,...,m$ , let $v_{t i}\\leftarrow\\mathrm{oo}(t,i,\\varepsilon_{t}\\cdot\\alpha_{t}^{2}M_{t}^{2}\\overline{{W}}_{2,t})\\,\\,\\,;$ else return $H_{t}$ ; Step $2.6:$ for $i=1,2,...,m$ , let //weight update $\\boxed{w_{(t+1)i}\\gets-\\delta_{v_{t i}}F(y_{i}H_{t}(\\pmb{x}_{i}))}\\;;$ (7) Step 2.7 : if $\\pmb{w}_{t+1}=\\mathbf{0}$ then break;   \nReturn $\\overline{{H_{T}}}$ . ", "page_idx": 4}, {"type": "text", "text": "5.1 Algorithm: SECBOOST ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "5.1.1 General steps ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Without further ado, Algorithm SECBOOST presents our approach to boosting without using derivatives information. The key differences with traditional boosting algorithms are red color framed. We summarize its key steps. ", "page_idx": 4}, {"type": "text", "text": "Step 1 This is the initialization step. Traditionally in boosting, one would pick $h_{0}=0$ . Note that $\\pmb{w}_{1}$ is not necessarily positive. $v_{0}$ is the initial offset (Section 4). ", "page_idx": 4}, {"type": "text", "text": "Step 2.1 This step calls the weak learner, as in traditional boosting, using variable \"weights\" on examples (the coordinate-wise absolute value of $\\pmb{w}_{t}$ , denoted $\\lvert w_{t}\\rvert,$ ). The key difference with traditional boosting is that examples labels can switch between iterations as well, which explains that the training sample, $S_{t}$ , is indexed by the iteration number. ", "page_idx": 4}, {"type": "text", "text": "Step 2.3 This step computes the leveraging coefficient $\\alpha_{t}$ of the weak classifier $h_{t}$ . It involves a quantity, $\\overline{{W}}_{2,t}$ , which we define as any strictly positive real satisfying ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}_{i\\sim[m]}\\left[\\delta_{\\{e_{t i},v_{(t-1)i}\\}}F(\\tilde{e}_{(t-1)i})\\cdot\\left(\\frac{h_{t}(x_{i})}{M_{t}}\\right)^{2}\\right]\\leqslant\\overline{{W}}_{2,t}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For boosting rate\u2019s sake, we should find $\\overline{{W}}_{2,t}$ as small as possible. We refer to (5) for the $e_{.},\\tilde{e}$ . notations; $v$ . is the current (set of) offset(s) (Section 4 for their definition). The second-order $\\boldsymbol{\\mathcal{V}}$ - derivative in the LHS plays the same role as the second-order derivative in classical boosting rates, see for example [45, Appendix, eq. 29]. As offsets $\\rightarrow0$ , it converges to a second-order derivative; otherwise, they still share some properties, such as the sign for convex functions. ", "page_idx": 4}, {"type": "text", "text": "Lemma 5.2. Suppose $F$ convex. For any $a\\in\\mathbb{R},b,c\\in\\mathbb{R}_{*}$ , $\\delta_{\\{b,c\\}}{\\cal F}(a)>0$ . ", "page_idx": 4}, {"type": "text", "text": "(Proof in Appendix, Section B.3) We can also see a link with weights variation since, modulo a slight abuse of notation, we have $\\delta_{\\{e_{t i},v_{(t-1)i}\\}}F(\\tilde{e}_{(t-1)i})=\\delta_{e_{t i}}w_{t i}$ . A substantial difference with traditional boosting algorithms is that we have two ways to pick the leveraging coefficient $\\alpha_{t}$ ; the first one can be used when a convenient $\\overline{{W}}_{2,t}$ is directly accessible from the loss. Otherwise, there is a simple algorithm that provides parameters (including $\\overline{{W}}_{2,t}$ ) such that (8) is satisfied. Section 5.3 details those two possibilities and their implementation. In the more favorable case (the former one), $\\alpha_{t}$ can be chosen in an interval, furthermore defined by flexible parameters $\\varepsilon_{t}>0,\\pi_{t}\\in(0,1)$ . Note that fixing beforehand these parameters is not mandatory: we can also pick any ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\alpha_{t}\\in\\eta_{t}\\cdot\\left(0,\\frac{1}{M_{t}^{2}\\overline{{W}}_{2,t}}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "and then compute choices for the corresponding $\\varepsilon_{t}$ and $\\pi_{t}$ . $\\varepsilon_{t}$ is important for the algorithm and both parameters are important for the analysis of the boosting rate. From the boosting standpoint, a smaller $\\varepsilon_{t}$ yields a larger $\\alpha_{t}$ and a smaller $\\pi_{t}$ reduces the interval of values in which we can pick $\\alpha_{t}$ ; both cases tend to favor better convergence rates as seen in Theorem 5.3. ", "page_idx": 5}, {"type": "text", "text": "Step 2.4 is just the crafting of the final model. ", "page_idx": 5}, {"type": "text", "text": "Step 2.5 is new to boosting, the use of a so-called offset oracle, detailed in Section 5.1.2. Step 2.6 The weight update does not rely on a first-order oracle as in traditional boosting, but uses only loss values through $v$ -derivatives. The finiteness of $F$ implies the finiteness of weights. Step 2.7 Early stopping happens if all weights are null. While this would never happen with traditional (e.g. strictly convex) losses, some losses that are unusual in the context of boosting can lead to early stopping. A discussion on early stopping and how to avoid it is in Section 6. ", "page_idx": 5}, {"type": "text", "text": "5.1.2 The offset oracle, OO ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Let us introduce notation ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{I}_{t i}(z)\\doteq\\bigl\\{v:Q_{F}^{*}(\\tilde{e}_{t i},\\tilde{e}_{(t-1)i},v)\\leqslant z\\bigr\\}\\,,\\forall i\\in[m],\\forall z>0.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "(see Figure 3 below to visualize $\\mathbb{I}_{t i}(z)$ for a non-convex $F$ ) The offset oracle is used in Step 2.5, which is new to boosting. It requests the offsets to carry out weight update in (7) to an offset oracle, which achieves the following, for iteration $\\#t$ , example $\\#i$ , limit OBI $z$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{oo}(t,i,z)\\mathrm{\\returns~some\\}v\\in\\mathbb{I}_{t i}(z)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that the offset oracle has the freedom to pick the offset in a whole set. Section 5.4 investigates implementations of the offset oracle, so let us make a few essentially graphical remarks here. OO does not need to build the whole $\\mathbb{I}_{t i}(z)$ to return some $v\\,\\in\\,\\mathbb{I}_{t i}(z)$ for Step 2.5 in SECBOOST. In the construction steps of Figure 3, as soon as $\\mathcal{O}\\neq\\mathcal{Q}$ , one element of O can be returned. Figure 4 presents more examples of $\\mathbb{I}_{t i}(z)$ . One can remark that the sign of the offset $v_{t i}$ in Step 2.5 of SECBOOST is the same as the sign of $\\tilde{e}_{(t-1)i}-\\tilde{e}_{t i}=-y_{i}\\alpha_{t}h_{t}(\\pmb{x}_{i})$ . Hence, unless $F$ is derivable or all edges $y_{i}h_{t}(x_{i})$ are of the same sign $(\\forall i)$ , the set of offsets returned in Step 2.5 always contain at least two different offsets, one non-negative and one non-positive (Figure 4, (a-b)). ", "page_idx": 5}, {"type": "text", "text": "5.2 Convergence of SECBOOST ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The offset oracle has a technical importance for boosting: $\\mathbb{I}_{t i}(z)$ is the set of offsets that limit an OBI for a training example (Definition 4.6). The importance for boosting comes from Lemma 4.7: upperbounding an OBI implies lowerbounding a Bregman Secant divergence, which will also guarantee a sufficient slack between two successive boosting iterations. This is embedded in a blueprint of a proof technique to show boosting-compliant convergence which is not new, see e.g. [45]. We now detail this convergence. ", "page_idx": 5}, {"type": "text", "text": "Remark that the expected edge $\\eta_{t}$ in Step 2.2 of SECBOOST is not normalized. We define a normalized version of this edge as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\left[-1,1\\right]\\ni\\tilde{\\eta}_{t}\\doteq\\sum_{i}\\frac{|w_{t i}|}{W_{t}}\\cdot\\tilde{y}_{t i}\\cdot\\frac{h_{t}(\\mathbf{x}_{i})}{M_{t}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with ${\\tilde{y}}_{t i}\\;\\doteq\\;y_{i}\\,\\cdot\\,\\mathrm{sign}(w_{t i})$ , $\\begin{array}{r}{W_{t}\\,\\doteq\\,\\sum_{i}|w_{t i}|\\,=\\,\\sum_{i}|\\delta_{v_{(t-1)i}}F(\\tilde{e}_{(t-1)i})|}\\end{array}$ . Remark that the labels are corrected by the weight sign and th u\u0159s may swit ch\u0159 between iterations. In the particular case where the loss is non-increasing (such as with traditional convex surrogates), the labels do not switch. We need also a quantity which is, in absolute value, the expected weight: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\overline{{W}}_{1,t}\\doteq\\left|\\mathbb{E}_{i\\sim[m]}\\left[\\delta_{v_{(t-1)i}}F(\\tilde{e}_{(t-1)i})\\right]\\right|\\quad}&{\\mathrm{(we~indeed~observe~}\\overline{{W}}_{1,t}=\\left|\\mathbb{E}_{i\\sim[m]}\\left[w_{t i}\\right]\\right|).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In classical boosting for convex decreasing losses\u2020, weights are non-negative and converge to a minimum (typically 0) as examples get the right class with increasing confidence. Thus, $\\overline{{W}}_{1,t}$ can be an indicator of when classification becomes \"good enough\" to stop boosting. In our more general setting, it shall be used in a similar indicator. We are now in a position to show a first result about SECBOOST. ", "page_idx": 6}, {"type": "text", "text": "Theorem 5.3. Suppose assumption 5.1 holds. Let ${\\cal F}_{0}\\doteq{\\cal F}({\\cal S},h_{0})$ in SECBOOST and $z^{*}$ any real such that $F(z^{*})\\leqslant F_{0}$ . Then we are guaranteed that classifier $H_{T}$ output by SECBOOST satisfies $F(S,H_{T})\\leqslant F(z^{*})$ when the number of boosting iterations $T$ yields: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\frac{\\overline{{W}}_{1,t}^{2}(1-\\pi_{t}^{2})}{\\overline{{W}}_{2,t}(1+\\varepsilon_{t})}\\cdot\\tilde{\\eta}_{t}^{2}\\geqslant4(F_{0}-F(z^{*})),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where parameters $\\varepsilon_{t},\\pi_{t}$ appear in Step 2.3 of SECBOOST. ", "page_idx": 6}, {"type": "text", "text": "(proof in Appendix, Section B.4) We observe the tradeoff between the freedom in picking parameters and convergence guarantee as exposed by (14): to get more freedom in picking the leveraging coefficient $\\alpha_{t}$ , we typically need $\\pi_{t}$ large (Step 2.3) and to get more freedom in picking the offset $v_{t}~\\neq~0$ , we typically need $\\varepsilon_{t}$ large (Step 2.5). However, allowing more freedom in such ways reduces the LHS and thus impairs the guarantee in (14). Therefore, there is a subtle balance between \"freedom\" of choice and convergence. This balance becomes more clear as boosting compliance formally enters convergence requirement. ", "page_idx": 6}, {"type": "text", "text": "Boosting-compliant convergence We characterize convergence in the boosting framework, which shall include the traditional weak learning assumption. ", "page_idx": 6}, {"type": "text", "text": "Assumption 5.4. $\\gamma$ -Weak Learning Assumption, $\\gamma{-}W L A$ ) We assume the following on the weak learner: $\\exists\\gamma>0$ such that $\\forall t>0,$ , $|\\tilde{\\eta}_{t}|\\geqslant\\gamma$ . ", "page_idx": 6}, {"type": "text", "text": "As is usually the case in boosting, the weights are normalized in the weak learning assumption (12). So the minimization \"potential\" of the loss does not depend on the absolute scale of weight. This is not surprising because the loss is \"nice\" in classical boosting: a large $\\gamma$ guarantees most examples\u2019 edges moving to the right of the $x$ -axis after the classifier update which, because the loss is strictly decreasing (exponential loss, logistic loss, etc.), is sufficient to yield a smaller expected loss. In our case it is not true anymore as for example there could be a local bump in the loss that would have it increase after the update. This is not even a pathological example: one may imagine that instead of a single bump the loss jiggles a lot locally. How can we keep boosting operating in such cases ? A sufficient condition takes the form of a second assumption that also integrates weights, ensuring that the variation of weights is locally not too large compared to (unnormalized) weights, which is akin to comparing local first- and second-order variations of the loss in the differentiable case. We encapsulate this notion in what we call a weight regularity assumption. ", "page_idx": 6}, {"type": "text", "text": "Assumption 5.5. $\\dot{\\rho}$ -Weight Regularity Assumption, $\\rho$ -WRA) Let $\\rho_{t}\\,\\dot{=}\\,\\overline{{W}}_{1,t}^{2}/\\overline{{W}}_{2,t}$ . We assume there exists $\\rho>0$ such that $\\forall t\\geqslant1$ , $\\rho_{t}>\\rho$ . ", "page_idx": 6}, {"type": "text", "text": "In Figure 2 we present a(n overly) simplified depiction of the cases where $\\overline{{W}}_{2,t}$ is large for \"not nice\" losses, and two workarounds on how to keep it small enough for the WRA to hold. Keep in mind that $\\overline{{W}}_{1,t}$ is an expected local variation of the loss (13), (5), so as it goes to zero, boosting converges to a local minimum and it is reasonable to expect that the WRA breaks. Otherwise, there are two strategies that keep $\\overline{{W}}_{2,t}$ relatively small enough for WRA to hold: either we pick small enough offsets, which essentially works for most losses but make us converge in general to a local minimum (this is in essence our experimental choice) or we optimize the offset oracle so that it sometimes \"passes\" local jiggling (Figure 2 (d)). While this eventually requires to tune the weak learner jointly with the offset oracle and fine-tune that latter algorithm on a loss-dependent basis, such a strategy can be used to eventually pass local minima of the loss. To do so, \"larger\" offsets directly translate into corresponding requests for larger magnitude classification for the next weak classifier, for the related examples. We are now in a position to state a simple corollary to Theorem 5.3. ", "page_idx": 6}, {"type": "image", "img_path": "MLgFu6dQYc/tmp/b4b014bdd4f46d6fce98d1a0515d4c1daebbc985d8ada56e395f872a3d923ea7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 2: Simplified depiction of $\\overline{{W}}_{2,t}$ \"regimes\" (Assumption 5.5). We only plot the components of the $v$ -derivative part in (8): removing index $i$ for readability, we get $\\delta_{\\{e_{t},{\\bar{v}}_{t-1}\\}}F({\\tilde{e}}_{t-1})=(B_{t}-$ $A_{t})/(y H_{t}(\\pmb{x})-y H_{t-1}(\\pmb{x}))$ with $A_{t}\\;\\doteq\\;\\delta_{v_{t-1}}F(y H_{t-1}({\\pmb x}))\\;=\\;-w_{t}$ and $B_{t}\\,\\doteq\\,\\delta_{v_{t-1}}F(y H_{t}(\\mathbf{x}))$ $(=-w_{t+1}$ iff $v_{t-1}=v_{t})$ ). If the loss is \"nice\" like the exponential or logistic losses, we always have a small $\\overline{{W}}_{2,t}$ (a). Place a bump in the loss (b-d) and the risk happens that $\\overline{{W}}_{2,t}$ is too large for the WRA to hold. Workarounds include two strategies: picking small enough offsets (b) or fit offsets large enough to pass the bump (c). The blue arrow in (d) is discussed in Section 6. ", "page_idx": 7}, {"type": "text", "text": "Algorithm $\\boldsymbol{2}\\,\\operatorname{SOLVE}_{\\alpha}(S,\\pmb{w},h)$ ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Input sample $S=\\{(\\mathbf{\\boldsymbol{x}}_{i},y_{i}),i=1,2,...,m\\}$ , ${\\pmb w}\\in\\mathbb{R}^{m}$ , $h:\\mathcal{X}\\to\\mathbb{R}$ . Step 1 : find any $a>0$ such that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\frac{|\\eta(\\pmb{w},h)-\\eta(\\tilde{\\pmb{w}}(\\mathrm{sign}(\\eta(\\pmb{w},h))\\cdot\\pmb{a}),h)|}{|\\eta(\\pmb{w},h)|}<1.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Return $\\mathrm{sign}(\\eta({\\pmb w},h))\\cdot a$ . ", "page_idx": 7}, {"type": "text", "text": "Corollary 5.6. Suppose assumptions 5.1, 5.5 and 5.4 hold. Let ${\\cal F}_{0}\\doteq{\\cal F}(S,h_{0})$ in SECBOOST and $z$ any real such that $F(z)\\leqslant F_{0}$ . If SECBOOST is run for a number $T$ of iterations satisfying ", "page_idx": 7}, {"type": "equation", "text": "$$\nT\\geqslant\\frac{4(F_{0}-F(z))}{\\gamma^{2}\\rho}\\cdot\\frac{1+\\operatorname*{max}_{t\\in[T]}\\varepsilon_{t}}{1-\\operatorname*{max}_{t\\in[T]}\\pi_{t}^{2}},\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "then $F(S,H_{T})\\leqslant F(z).$ . ", "page_idx": 7}, {"type": "text", "text": "We remark that the dependency in $\\gamma$ is optimal [4]. ", "page_idx": 7}, {"type": "text", "text": "5.3 Finding $\\overline{{W}}_{2,t}$ ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "There is lots of freedom in the choice of $\\alpha_{t}$ in Step 2.3 of SECBOOST, and even more if we look at (9). This, however, requires access to some bound $\\overline{{W}}_{2,t}$ . In the general case, the quantity it upperbounds in (8) also depends on $\\alpha_{t}$ because $e_{t i}\\doteq\\alpha_{t}\\cdot y_{i}h_{t}({\\pmb x}_{i})$ . So unless we can obtain such a \"simple\" $\\overline{{W}}_{2,t}$ that does not depend on $\\alpha_{t}$ , (6) \u2013 and (9) \u2013 provide a system to solve for $\\alpha_{t}$ . ", "page_idx": 7}, {"type": "text", "text": "$\\overline{{W}}_{2,t}$ via properties of $F$ Classical assumptions on loss functions for zeroth-order optimization can provide simple expressions for $\\overline{{W}}_{2,t}$ (Table 1). Consider smoothness: we say that $F$ is $\\beta.$ -smooth if it is derivable and its derivative satisfies the Lipschitz condition $|F^{\\prime}(z^{\\prime})-F^{\\prime}(z)|\\leqslant\\beta|z^{\\prime}-z|,\\forall z,z^{\\prime}$ [12]. Notice that this implies the condition on the $v$ -derivative of the derivative: $|\\delta_{v}F^{\\prime}(z)|\\leqslant\\dot{\\beta},\\forall z,v$ . This also provides a straightforward useful expression for $\\overline{{W}}_{2,t}$ . ", "page_idx": 7}, {"type": "text", "text": "Lemma 5.7. Suppose that the loss $F$ is $\\beta$ -smooth. Then we can fix $\\overline{{\\mathbf{\\nabla}}}\\cdot\\overline{{W}}_{2,t}=2\\beta$ . ", "page_idx": 7}, {"type": "text", "text": "(Proof in Appendix, Section B.5) What the Lemma shows is that a bound on the $v$ -derivative of the derivative implies a bound on order-2 $\\boldsymbol{\\mathcal{V}}$ -derivatives (in the quantity that $\\overline{{W}}_{2,t}$ bounds (8)). Such a condition on $v$ -derivatives is thus weaker than a condition on derivatives, and it is strictly weaker if we impose a strictly positive lowerbound on the offset\u2019s absolute value, which would be sufficient to characterize the boosting convergence of SECBOOST. ", "page_idx": 7}, {"type": "text", "text": "A general algorithm for $\\overline{{W}}_{2,t}$ If we cannot make any assumption on $F$ , there is a simple way to first obtain $\\alpha_{t}$ and then $\\overline{{W}}_{2,t}$ , from which all other parameters of Step 2.3 can be computed. We first need a few definitions. We first generalize the edge notation appearing in Step 2.2: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\eta(\\pmb{w},h)\\doteq\\mathbb{E}_{i\\sim[m]}\\left[w_{i}y_{i}h(\\pmb{x}_{i})\\right],\n$$", "text_format": "latex", "page_idx": 7}, {"type": "image", "img_path": "MLgFu6dQYc/tmp/f183b76a486136001836910a4525b4857bf2d2f26e43b08793fdde4b93d46e7d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 3: A simple way to build $\\mathbb{I}_{t i}(z)$ for a discontinuous loss $F$ $\\tilde{e}_{t i}<\\tilde{e}_{(t-1)i}$ and $z$ are represented), O being the set of solutions as it is built. We rotate two half-lines, one passing through $(\\tilde{e}_{t i},F(\\tilde{e}_{t i}))$ (thick line, $(\\Delta),$ ) and a parallel one translated by $-z$ (dashed line) (a). As soon as $(\\Delta)$ crosses $F$ on any point $\\left(z^{\\prime},\\bar{F}(z^{\\prime})\\right)$ with $z\\neq\\tilde{e}_{t i}$ while the dashed line stays below $F$ , we obtain a candidate offset $v$ for OO, namely $v\\,=\\,z^{\\prime}-\\tilde{e}_{t i}$ . In (b), we obtain an interval of values. We keep on rotating $(\\Delta)$ , eventually making appear several intervals for the choice of $v$ if $F$ is not convex (c). Finally, when we reach an angle such that the maximal difference between $(\\Delta)$ and $F$ in $[\\tilde{e}_{t i},\\tilde{e}_{(t-1)i}]$ is $z$ ( $\\bar{z}$ can be located at an intersection between $F$ and the dashed line), we stop and obtain the full $\\mathbb{I}_{t i}(z)$ (d). ", "page_idx": 8}, {"type": "image", "img_path": "MLgFu6dQYc/tmp/c4f470ec8e9142de618e454ee7528fc429345d007f061b70e25c5fe4666642a6.jpg", "img_caption": ["Figure 4: More examples of ensembles $\\mathbb{I}_{t i}(z)$ (in blue) for the $F$ in Figure 3. (a): $\\mathbb{I}_{t i}(z)$ is the union of two intervals with all candidate offsets non negative. (b): it is a single interval with non-positive offsets. (c): at a discontinuity, if $z$ is smaller than the discontinuity, we have no direct solution for $\\mathbb{I}_{t i}(z)$ for at least one positioning of the edges, but a simple trick bypasses the difficulty (see text). "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "so that $\\eta_{t}\\doteq\\eta(\\pmb{w}_{t},h_{t})$ . Remind the weight update, $w_{t i}\\,\\doteq\\,-\\delta_{v_{(t-1)i}}F\\bigl(y_{i}H_{t-1}({\\pmb x}_{i})\\bigr)$ . We define a \"partial\" weight update, ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\tilde{w}_{t i}(\\alpha)\\doteq-\\delta_{v_{(t-1)i}}F(\\alpha y_{i}h_{t}({\\pmb x}_{i})+y_{i}H_{t-1}({\\pmb x}_{i}))\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "(if we were to replace $v_{(t-1)i}$ by $v_{t i}$ and let $\\alpha\\doteq\\alpha_{t}$ , then $\\tilde{w}_{t i}(\\alpha)$ would be $w_{(t+1)i}$ , hence the partial weight update). Algorithm 2 presents the simple procedure to find $\\alpha_{t}$ . Notice that we use $\\tilde{w}$ with sole dependency on the prospective leveraging coefficient; we omit for clarity the dependences in the current ensemble $(H)$ , weak classifier $(h_{.})$ and offsets $(v_{.i})$ needed to compute (17). ", "page_idx": 8}, {"type": "text", "text": "Theorem 5.8. Suppose Assumptions 5.1 and 5.4 hold and $F$ is continuous at all abscissae $\\{\\tilde{e}_{(t-1)i}\\,\\doteq\\,y_{i}H_{t-\\bar{1}}({\\pmb x}_{i}),i\\,\\in\\,[m]\\}$ . Then there are always solutions to Step $^{\\,l}$ of $\\mathrm{{SOLVE}}_{\\alpha}$ and $i f$ we let $\\alpha_{t}\\gets\\mathrm{SOLVE}_{\\alpha}(S,{\\pmb w}_{t},h_{t})$ and then compute ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\overline{{{W}}}_{2,t}\\doteq\\left|\\mathbb{E}_{i\\sim[m]}\\left[\\frac{h_{t}^{2}({\\pmb x}_{i})}{M_{t}^{2}}\\cdot\\delta_{\\{\\alpha_{t}y_{i}h_{t}({\\pmb x})_{i}\\},v_{(t-1)i}}\\}F(\\tilde{e}_{(t-1)i})\\right]\\right|,\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "then $\\overline{{W}}_{2,t}$ satisfies (8) and $\\alpha_{t}$ satisfies (6) for some $\\varepsilon_{t}>0,\\pi_{t}\\in(0,1)$ . ", "page_idx": 8}, {"type": "text", "text": "The proof, in Section B.6, proceeds by reducing condition (9) to (16). The Weak Learning Assumption (5.4) is important for the denominator in the LHS of (16) to be non zero. The continuity assumption at all abscissae is important to have $\\begin{array}{r}{\\operatorname*{lim}_{a\\rightarrow0}\\eta(\\tilde{\\pmb{w}}_{t}(a),h_{t})\\;=\\;\\eta_{t}}\\end{array}$ , which ensures the existence of solutions to (16), also easy to find, e.g. by a simple dichotomic search starting from an initial guess for $a$ . Note the necessity of being continuous only at abscissae defined by the training sample, which is finite in size. Hence, if this condition is not satisfied but discontinuities of $F$ are of Lebesgue measure 0, it is easy to add an infinitesimal constant to the current weak classifier, ensuring the conditions of Theorem 5.8 and keeping the boosting rates. ", "page_idx": 8}, {"type": "text", "text": "5.4 Implementation of the offset oracle ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Figure 3 explains how to build graphically $\\mathbb{I}_{t i}(z)$ for a general $F$ . While it is not hard to implement a general procedure following the blueprint (i.e. accepting the loss function as input), it would be far ", "page_idx": 8}, {"type": "text", "text": "from achieving computational optimality: a much better choice consists in specializing it to the (set of) loss(es) at hand via hardcoding specific optimization features of the desired loss(es). This would not prevent \"loss oddities\" to get absolutely trivial oracles (see Appendix, Section B.7). ", "page_idx": 9}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "For an efficient implementation, boosting requires specific design choices to make sure the weak learning assumption stands for as long as necessary; experimentally, it is thus a good idea to adapt the weak learner to build more complex models as iterations increase (e.g. learning deeper trees), keeping Assumption 5.4 valid with its advantage over random guessing parameter $\\gamma>0$ . In our more general setting, our algorithm SECBOOST pinpoints two more locations that can make use of specific design choices to keep assumptions stand for a larger number of iterations. ", "page_idx": 9}, {"type": "text", "text": "The first is related to handling local minima. When Assumption 5.5 breaks, it means we are close to a local optimum of the loss. One possible way of escaping those local minima is to adapt the offset oracle to output larger offsets (Step 2.5) that get weights computed outside the domain of the local minimum. Such offsets can be used to inform the weak learner of the specific examples that then need to receive larger magnitude in classification, something we have already discussed in Section 5. There is also more: the sign of the weight indicates the polarity of the next edge $(e_{t}.$ , (5)) needed to decrease the loss in the interval spanned by the last offset. To simplify, suppose a substantial fraction of examples have an edge $\\tilde{e}_{t}$ . in the vicinity of the blue dotted line in Figure 2 (d) so that the loss value is indicated by the big arrow and suppose their current offset $=v_{t-1}$ so that their weight (positive) signals that to minimize further the loss, the weak learner\u2019s next weak classifier has to have a positive edge over these examples. Such is the polarity constraint which essentially comes to satisfy the WLA, but there is a magnitude constraint that comes from the WRA: indeed, if the positive edge is too small so that the loss ends up in the \"bump\" region, then there is a risk that the WRA breaks because the loss around the bump is quite flat, so the numerator of $\\rho_{t}$ in Assumption 5.5 can be small. Passing the bump implies escaping the local minimum at which the loss would otherwise be trapped. Section 5.4 has presented a general blueprint for the offset oracle but more specific implementation designs can be used; some are discussed in the Appendix, Section B.7. ", "page_idx": 9}, {"type": "text", "text": "The second is related to handling losses that take on constant values over parts of their domain. To prevent early stopping in Step 2.7 of SECBOOST, one needs $\\boldsymbol{w}_{t+1}\\neq\\mathbf{0}$ . The update rule of $\\pmb{w}_{t}$ imposes that the loss must then have non-zero variation for some examples between two successive edges (5). If the loss $F$ is constant, then clearly the algorithm obviously stops without learning anything. If $F$ is piecewise-constant, this constrain the design of the weak learner to make sure that some examples receive a different loss with the new model update $H_{\\mathrm{\\ell}}$ . As explained in Appendix, Section B.11, this can be efficiently addressed by specific designs on $\\mathrm{{SOLVE}}_{\\alpha}$ . ", "page_idx": 9}, {"type": "text", "text": "In the same way as there is no $\"1$ size fits all\" weak learner for all domains in traditional boosting, we expect specific design choices to be instrumental in better handling specific losses in our more general setting. Our theory points two locations further work can focus on. ", "page_idx": 9}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Boosting has rapidly moved to an optimization setting involving first-order information about the loss optimized, rejoining, in terms of information needed, that of the hugely popular (stochastic) gradient descent. But this was not a formal requirement of the initial setting and in this paper, we show that essentially any loss function can be boosted without this requirement. From this standpoint, our results put boosting in a slightly more favorable light than recent development on zeroth-order optimization since, to get boosting-compliant convergence, we do not need the loss to meet any of the assumptions that those analyses usually rely on. Of course, recent advances in zeroth-order optimization have also achieved substantial design tricks for the implementation of such algorithms, something that undoubtedly needs to be adressed in our case, such as for the efficient optimization of the offset oracle. We leave this as an open problem but provide in Appendix some toy experiments that a straightforward implementation achieves, hinting that SECBOOST can indeed optimize very \u201cexotic\u201d losses. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] A. Akhavan, E. Chzhen, M. Pontil, and A.-B. Tsybakov. A gradient estimator via l1- randomization for online zero-order optimization with two point feedback. In NeurIPS\\*35, 2022.   \n[2] A. Akhavan, M. Pontil, and A.-B. Tsybakov. Exploiting higher order smoothness in derivativefree optimization and continuous bandits. In NeurIPS\\*33, 2020.   \n[3] A. Akhavan, M. Pontil, and A.-B. Tsybakov. Distributed zero-order optimisation under adversarial noise. In NeurIPS\\*34, 2021.   \n[4] N. Alon, A. Gonen, E. Hazan, and S. Moran. Boosting simple learners. In STOC\u201921, 2021.   \n[5] S.-I. Amari and H. Nagaoka. Methods of Information Geometry. Oxford University Press, 2000.   \n[6] F. Bach. Learning Theory from First Principles. Course notes, MIT press (to appear), 2023.   \n[7] A. Banerjee, S. Merugu, I. Dhillon, and J. Ghosh. Clustering with bregman divergences. In Proc. of the $4^{t h}$ SIAM International Conference on Data Mining, pages 234\u2013245, 2004.   \n[8] P.-L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. JMLR, 3:463\u2013482, 2002.   \n[9] G. Biau, B. Cadre, and L. Rouvi\u00e8re. Accelerated gradient boosting. Mach. Learn., 108(6):971\u2013 992, 2019.   \n[10] M. Blondel, A.-F. T. Martins, and V. Niculae. Learning with Fenchel-Young losses. J. Mach. Learn. Res., 21:35:1\u201335:69, 2020.   \n[11] L. M. Bregman. The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming. USSR Comp. Math. and Math. Phys., 7:200\u2013217, 1967.   \n[12] S. Bubeck. Convex optimization: Algorithms and complexity. Found. Trends Mach. Learn., 8(3-4):231\u2013357, 2015.   \n[13] P.-S. Bullen. Handbook of means and their inequalities. Kluwer Academic Publishers, 2003.   \n[14] H. Cai, Y. Lou, D. McKenzie, and W. Yin. A zeroth-order block coordinate descent algorithm for huge-scale black-box optimization. In $38^{t h}$ ICML, pages 1193\u20131203, 2021.   \n[15] C. Cartis and L. Roberts. Scalable subspace methods for derivative-free nonlinear least-squares optimization. Math. Prog., 199:461\u2013524, 2023.   \n[16] S. Cheamanunkul, E. Ettinger, and Y. Freund. Non-convex boosting overcomes random label noise. CoRR, abs/1409.2905, 2014.   \n[17] L. Chen, J. Xu, and L. Luo. Faster gradient-free algorithms for nonsmooth nonconvex stochastic optimization. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 5219\u20135233. PMLR, 2023.   \n[18] X. Chen, S. Liu, K. Xu, X. Li, X. Lin, M. Hong, and D. Cox. ZO-AdaMM: Zeroth-order adaptive momentum method for black-box optimization. In NeurIPS\\*32, 2019.   \n[19] X. Chen, Y. Tang, and N. Li. Improve single-point zeroth-order optimization using high-pass and low-pass fliters. In $39^{t h}$ ICML, volume 162 of Proceedings of Machine Learning Research, pages 3603\u20133620. PMLR, 2022.   \n[20] S. Cheng, G. Wu, and J. Zhu. On the convergence of prior-guided zeroth-order optimisation algorithms. In NeurIPS\\*34, 2021.   \n[21] Z. Cranko and R. Nock. Boosted density estimation remastered. In $36^{t h}$ ICML, pages 1416\u2013 1425, 2019.   \n[22] W. de Vazelhes, H. Zhang, H. Wu, X. Yuan, and B. Gu. Zeroth-order hard-thresholding: Gradient error vs. expansivity. In NeurIPS\\*35, 2022.   \n[23] D. Dua and C. Graff. UCI machine learning repository, 2021.   \n[24] E. Fermi and N. Metropolis. Numerical solutions of a minimum problem. Technical Report TR LA-1492, Los Alamos Scientific Laboratory of the University of California, 1952.   \n[25] L. Flokas, E.-V. Vlatakis-Gkaragkounis, and G. Piliouras. Efficiently avoiding saddle points with zero order methods: No gradients required. In NeurIPS\\*32, 2019.   \n[26] H. Gao and H. Huang. Can stochastic zeroth-order frank-wolfe method converge faster for non-convex problems? In $37^{t h}$ ICML, pages 3377\u20133386, 2020.   \n[27] A. H\u00e9liou, M. Martin, P. Mertikopoulos, and T. Rahier. Zeroth-order non-convex learning via hierarchical dual averaging. In $3\\bar{\\delta}^{t h}$ ICML, pages 4192\u20134202, 2021.   \n[28] F. Huang, L. Tao, and S. Chen. Accelerated stochastic gradient-free and projection-free methods. In $37^{t h}$ ICML, pages 4519\u20134530, 2020.   \n[29] B. Irwin, E. Haber, R. Gal, and A. Ziv. Neural network accelerated implicit flitering: Integrating neural network surrogates with provably convergent derivative free optimization methods. In $40^{t h}$ ICML, volume 202 of Proceedings of Machine Learning Research, pages 14376\u201314389. PMLR, 2023.   \n[30] V. Kac and P. Cheung. Quantum calculus. Springer, 2002.   \n[31] M. J. Kearns and U. V. Vazirani. An Introduction to Computational Learning Theory. M.I.T. Press, 1994.   \n[32] M.J. Kearns. Thoughts on hypothesis boosting, 1988. ML class project.   \n[33] M.J. Kearns and Y. Mansour. On the boosting ability of top-down decision tree learning algorithms. J. Comp. Syst. Sc., 58:109\u2013128, 1999.   \n[34] J. Larson, M. Menickelly, and S.-M. Wild. Derivative-free optimization methods. Acta Numerica, pages 287\u2013404, 2019.   \n[35] Z. Li, P.-Y. Chen, S. Liu, S. Lu, and Y. Xu. Zeroth-order optimization for composite problems with functional constraints. In AAAI\u201922, pages 7453\u20137461. AAAI Press, 2022.   \n[36] T. Lin, Z. Zheng, and M.-I. Jordan. Gradient-free methods for deterministic and stochastic nonsmooth nonconvex optimization. In NeurIPS\\*35, 2022.   \n[37] P.-M. Long and R.-A. Servedio. Random classification noise defeats all convex potential boosters. MLJ, 78(3):287\u2013304, 2010.   \n[38] C. Maheshwari, C.-Y. Chiu, E. Mazumdar, S. Shankar Sastry, and L.-J. Ratliff. Zerothorder methods for convex-concave minmax problems: applications to decision-dependent risk minimization. In $25^{t h}$ AISTATS, 2022.   \n[39] Y. Mansour, R. Nock, and R.-C. Williamson. Random classification noise does not defeat all convex potential boosters irrespective of model choice. In $40^{t h}$ ICML, 2023.   \n[40] E. Mhanna and M. Assaad. Single point-based distributed zeroth-order optimization with a non-convex stochastic objective function. In $40^{t h}$ ICML, volume 202 of Proceedings of Machine Learning Research, pages 24701\u201324719. PMLR, 2023.   \n[41] M. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of Machine Learning. MIT Press, 2018.   \n[42] Y. Nesterov and V. Spokoiny. Random gradient-free optimization of convex functions. Foundations of Computational Mathematics, 17:527\u2013566, 2017.   \n[43] F. Nielsen and R. Nock. The Bregman chord divergence. In Geometric Science of Information - 4th International Conference, 2019, pages 299\u2013308, 2019.   \n[44] R. Nock and A. K. Menon. Supervised learning: No loss no cry. In $37^{t h}$ ICML, 2020.   \n[45] R. Nock and R.-C. Williamson. Lossless or quantized boosting with integer arithmetic. In $36^{t h}$ ICML, pages 4829\u20134838, 2019.   \n[46] N.-E. Pfetsch and Sebastian Pokutta. IPBoost - non-convex boosting via integer programming. In $37^{t h}$ ICML, volume 119, pages 7663\u20137672, 2020.   \n[47] Y. Qiu, U.-V. Shanbhag, and F. Yousefian. Zeroth-order methods for nondifferentiable, nonconvex and hierarchical federated optimization. In NeurIPS\\*36, 2023.   \n[48] M. Rando, C. Molinari, L. Rosasco, and S. Villa. Structured zeroth-order for non-smooth optimization. In NeurIPS\\*36, 2023.   \n[49] M.-D. Reid and R.-C. Williamson. Information, divergence and risk for binary experiments. JMLR, 12:731\u2013817, 2011.   \n[50] Z. Ren, Y. Tang, and N. Li. Escaping saddle points in zeroth-order optimization: the power of two-point estimators. In $40^{t h}$ ICML, volume 202 of Proceedings of Machine Learning Research, pages 28914\u201328975. PMLR, 2023.   \n[51] A.-K. Sahu, M. Zaheer, and S. Kar. Towards gradient free and projection free stochastic optimization. In $22^{n d}$ AISTATS, pages 3468\u20133477, 2019.   \n[52] W. Shi, H. Gao, and B. Gu. Gradient-free method for heavily constrained nonconvex optimization. In $39^{t h}$ ICML, volume 162 of Proceedings of Machine Learning Research, pages 19935\u201319955. PMLR, 2022.   \n[53] M.-K. Warmuth and S. V. N. Vishwanathan. Tutorial: Survey of boosting from an optimization perspective. In $2\\delta^{t h}$ ICML, 2009.   \n[54] T. Werner and P. Ruckdeschel. The column measure and gradient-free gradient boosting, 2019.   \n[55] H. Zhang and B. Gu. Faster gradient-free methods for escaping saddle points. In ICLR\u201923. OpenReview.net, 2023.   \n[56] H. Zhang, H. Xiong, and B. Gu. Zeroth-order negative curvature finding: Escaping saddle points without gradients. In NeurIPS\\*35, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To differentiate with the numberings in the main flie, the numbering of Theorems, etc. is letter-based (A, B, ...). ", "page_idx": 13}, {"type": "text", "text": "Table of contents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A quick summary of recent zeroth-order optimization approachess Pg 15 ", "page_idx": 13}, {"type": "text", "text": "Supplementary material on proofs Pg 15 ", "page_idx": 13}, {"type": "text", "text": "$\\hookrightarrow$ Helper results Pg 15   \n$\\hookrightarrow$ Removing the $\\neq0$ part in Assumption 5.1 Pg 16   \n$\\hookrightarrow$ Proof of Lemma 5.2 Pg 16   \n$\\hookrightarrow$ Proof of Theorem 5.3 Pg 17   \n$\\hookrightarrow$ Proof of Lemma 5.7 Pg 20   \n$\\hookrightarrow$ Proof of Theorem 5.8 Pg 20   \n$\\hookrightarrow$ Implementation of the offset oracle Pg 21   \n$\\hookrightarrow$ Proof of Lemma B.5 Pg 22   \n$\\hookrightarrow$ Handling discontinuities in the offset oracle to prevent stopping in Step 2.5 of SECBOOSTPg 24   \n$\\hookrightarrow\\mathbf{A}$ boosting pattern that can \"survive\" above differentiability Pg 24   \n$\\hookrightarrow$ The case of piecewise constant losses for $\\mathrm{{SOLVE}}_{\\alpha}$ Pg 26 ", "page_idx": 13}, {"type": "text", "text": "Supplementary material on algorithms, implementation tricks and a toy experiment Pg 26 ", "page_idx": 13}, {"type": "table", "img_path": "MLgFu6dQYc/tmp/372f069947423b098e09f4f97a35df1f8495989fdaca3422ce4a08321b0d56b2.jpg", "table_caption": [], "table_footnote": ["Table 1: Summary of formal assumptions about loss $F$ used to prove algorithms\u2019 convergence in recent papers on zeroth order optimization, in different ML settings (see text for details). We use \"smoothness\" as a portmanteau for various conditions on the $\\geqslant1$ order differentiability condition of $F$ . \"conv.\" $=$ convex, \"diff.\" $=$ differentiable, \"Lip.\" $=$ Lipschitz, \"Lb\" $=$ lower-bounded, \"alt. $\\mathrm{GD\"=}$ general alternative to gradient descent (stochastic or not), \"alt. FW\" $=$ idem for Frank-Wolfe. Our paper relies on no such assumptions. "], "page_idx": 14}, {"type": "text", "text": "A A quick summary of recent zeroth-order optimization approaches ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table 1 summarizes a few dozens of recent approaches that can be related to zeroth-order optimization in various topics of ML. Note that no such approaches focus on boosting. ", "page_idx": 14}, {"type": "text", "text": "B Supplementary material on proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Helper results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We now show that the order of the elements of $\\boldsymbol{\\mathcal{V}}$ does not matter to compute the $\\boldsymbol{\\mathcal{V}}$ -derivative as in Definition 4.2. For any $\\pmb{\\sigma}\\in\\{0,1\\}^{n}$ , we let $1_{\\sigma}\\doteq\\sum_{i}\\sigma_{i}$ . ", "page_idx": 14}, {"type": "text", "text": "Lemma B.1. For any $z\\in\\mathbb{R}$ , any $n\\in\\mathbb{N}_{*}$ and an y $\\mathcal{V}\\doteq\\{v_{1},v_{2},...,v_{n}\\}\\subset\\mathbb{R},$ ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\delta_{\\boldsymbol{\\mathcal{V}}}F(\\boldsymbol{z})=\\frac{\\sum_{\\boldsymbol{\\sigma}\\in\\{0,1\\}^{n}}(-1)^{n-1_{\\boldsymbol{\\mathcal{\\sigma}}}}F(\\boldsymbol{z}+\\sum_{i=1}^{n}\\sigma_{i}\\boldsymbol{v_{i}})}{\\prod_{i=1}^{n}v_{i}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Hence, $\\delta_{\\ensuremath{\\stackrel{\\s}{\\v{v}}}}F$ is invariant to permutations of the elements of $\\mathcal{V}$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. We show the result by induction on the size of $\\boldsymbol{\\mathcal{V}}$ , first noting that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\delta_{\\{v_{1}\\}}F(z)=\\delta_{v_{1}}F(z)\\doteq\\frac{F(z+v_{1})-F(z)}{v_{1}}=\\frac{1}{\\prod_{i=1}^{1}v_{i}}\\cdot\\sum_{\\sigma\\in\\{0,1\\}}(-1)^{1-1_{\\sigma}}F(z+\\sigma v_{1}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "We then assume that (18) holds for $\\mathcal{V}_{n}\\doteq\\{v_{1},v_{2},...,v_{n}\\}$ and show the result for $\\mathcal{V}_{n+1}\\doteq\\mathcal{V}_{n}\\cup\\bigl\\{v_{n+1}\\bigr\\}$ , writing (induction hypothesis used in the second identity): ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\delta_{\\nabla_{n+1}F}(\\zeta)}\\\\ &{=\\frac{\\delta_{\\nabla_{n}}F(z+v_{n+1})-\\delta_{\\nabla_{n}}F(z)}{v_{n+1}}}\\\\ &{=\\frac{\\sum_{\\sigma\\in\\{0,1\\}^{n}}(-1)^{n-1}e^{\\int(z+\\sum_{i=1}^{n}\\sigma_{i}v_{i}+v_{n+1})}-\\sum_{\\sigma\\in\\{0,1\\}^{n}}(-1)^{n-1}e^{\\int(z+\\sum_{i=1}^{n}\\sigma_{i}v_{i})}}{\\prod_{i=1}^{n-1}v_{i}}}\\\\ &{=\\frac{\\sum_{\\sigma\\in\\{0,1\\}^{n}}(-1)^{n-1}e^{\\int(z+\\sum_{i=1}^{n}\\sigma_{i}v_{i}+v_{n+1})}+\\sum_{\\sigma\\in\\{0,1\\}^{n}}(-1)^{n-1}e^{-\\sum_{i\\sigma}+1}F(z+\\sum_{i=1}^{n}\\sigma_{i}v_{i})}{\\prod_{i=1}^{n-1}v_{i}}}\\\\ &{=\\frac{\\displaystyle\\left\\{\\begin{array}{l l}{\\sum_{\\sigma^{\\prime}\\in\\{0,1\\}^{n}+1:\\sigma_{i+1}^{\\prime}=1}(-1)^{n-1}e^{-\\sum_{i=1}^{n}\\sigma_{i}^{\\prime}v_{i}}}{\\sum_{i=1}^{n-1}v_{i}}}\\\\ {-\\displaystyle\\sum_{\\sigma^{\\prime}\\in\\{0,1\\}^{n+1}}(-1)^{n-1}\\frac{(\\sigma^{\\prime}-1)^{n}\\int\\zeta+\\sum_{i=1}^{n+1}\\sigma_{i}^{\\prime}v_{i})}{\\sum_{i=1}^{n+1}}}\\end{array}\\right.}\\\\ &{=\\frac{\\displaystyle\\sum_{\\sigma^{\\prime}\\in\\{0,1\\}^{n+1}}(-1)^{n+1-1}e^{\\int(z+\\sum_{i=1}^{n}\\sigma_{i}^{\\prime}v_{i})}}{\\prod_{i=1}^{n+1}v_{i}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "as claimed. ", "page_idx": 15}, {"type": "text", "text": "We also have the following simple Lemma, which is a direct consequence of Lemma B.1. ", "page_idx": 15}, {"type": "text", "text": "Lemma B.2. For all $z,\\in\\mathbb{R},v,z^{\\prime}\\in\\mathbb{R}_{*}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\delta_{v}F(z+z^{\\prime})=\\delta_{v}F(z)+z^{\\prime}\\cdot\\delta_{\\{z^{\\prime},v\\}}F(z).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. It comes from Lemma B.1 that $\\delta_{\\{z^{\\prime},v\\}}F(z)\\,=\\,\\delta_{\\{v,z^{\\prime}\\}}F(z)\\,=\\,(\\delta_{v}F(z+z^{\\prime})-\\delta_{v}F(z))/z^{\\prime}$ (and we reorder terms). ", "page_idx": 15}, {"type": "text", "text": "B.2 Removing the $\\neq0$ part in Assumption 5.1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Because everything needs to be encoded, finiteness is not really an assumption. However, the non-zero assumption may be seen as limiting (unless we are happy to use first-order information about the loss (Section 5). There is a simple trick to remove it. Suppose $h_{t}$ zeroes on some training examples. The training sample being finite, there exists an open neighborhood $\\mathbb{I}$ in 0 such that $h_{t}^{\\bar{\\prime}}\\doteq h_{t}\\,\\bar{+}\\,\\delta$ does not zero anymore on training examples, for any $\\delta\\in\\mathbb{I}$ . This changes the advantage $\\gamma$ in the WLA (Definition 5.4) to some $\\gamma^{\\prime}$ satisfying (we assume $\\delta>0$ wlog) ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\gamma^{\\prime}\\geqslant\\frac{\\gamma M_{t}}{M_{t}+\\delta}-\\frac{\\delta}{M_{t}+\\delta}}\\\\ {\\geqslant\\gamma-\\frac{\\delta}{M_{t}}\\cdot(1+\\gamma),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "from which it is enough to pick $\\delta\\leqslant\\varepsilon\\gamma M_{t}/(1+\\gamma)$ to guarantee advantage $\\gamma^{\\prime}\\geqslant(1-\\varepsilon)\\gamma$ . If $\\varepsilon$ is a constant, this translates in a number of boosting iterations in Corollary 5.6 affected by a constant factor that we can choose as close to 1 as desired. ", "page_idx": 15}, {"type": "text", "text": "B.3 Proof of Lemma 5.2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We reformulate ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\delta_{\\{b,c\\}}F(a)=\\frac{2}{b}\\cdot\\frac{1}{c}\\cdot\\left(\\underbrace{\\frac{F(a+b+c)+F(a)}{\\sqrt[2]{a}\\sqrt{\\phantom{+\\beta}\\!\\!\\!-\\!\\!\\!-\\!\\!\\!-\\!\\!\\!\\!-\\!\\!\\!\\!-\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!+\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!+\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!+\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!+\\!\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!+\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\\!\\!\\!\\!\\!-\\!\n$$", "text_format": "latex", "page_idx": 15}, {"type": "image", "img_path": "MLgFu6dQYc/tmp/e259ebbe2d121c5bd5c33a0deb67071f4a4cb7856e8bd2fca7b7bc34f87c47b3.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 5: Left: representation of the difference of averages in (22). Each of the secants $(\\Delta_{1})$ and $\\bar{(\\Delta_{2})}$ can take either the red or black segment. Which one is which depends on the signs of $c$ and $b$ , but the general configuration is always the same. Note that if $F$ is convex, one necessarily sits above the other, which is the crux of the proof of Lemma 5.2. For the sake of illustration, suppose we can analytically have $b,c\\to0$ . As $c$ converges to 0 but $b$ remains $>0$ , $\\delta_{\\{b,c\\}}F(a)$ becomes proportional to the variation of the average secant midpoint; the then-convergence of $b$ to 0 makes $\\delta_{\\{b,c\\}}F(a)$ converge to the second-order derivative of $F$ at $a$ . Right: in the special case where $F$ is convex, one of the secants always sits above the other. ", "page_idx": 16}, {"type": "text", "text": "Both $\\mu_{1}$ and $\\mu_{2}$ are averages that can be computed from the midpoints of two secants (respectively): ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\Delta_{1})\\doteq[(a+c,F(a+c)),(a+b,F(a+b))],}\\\\ {(\\Delta_{2})\\doteq[(a,F(a)),(a+b+c,F(a+b+c))].}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Also, the midpoints of both secants have the same abscissa (and the ordinates are $\\mu_{1}$ and $\\mu_{2}$ ), so to study the sign of $\\delta_{\\{b,c\\}}F(a)$ , we can study the position of both secants with respect to each other. $F$ being convex, we show that the abscissae of one secant are included in the abscissae of the other, this being sufficient to give the position of both secants with respect to each other. We distinguish four cases. ", "page_idx": 16}, {"type": "text", "text": "Case 1: $c>0,b>0$ . We have $a+b+c>\\operatorname*{max}\\{a+b,a+c\\}$ and $a<\\operatorname*{min}\\{a+b,a+c\\}$ . $F$ being convex, $(\\Delta_{2})$ sits above $(\\Delta_{1})$ . So, $\\mu_{2}\\geqslant\\mu_{1}$ and finally $\\mathfrak{z}_{\\{b,c\\}}F(a)\\geqslant0$ . ", "page_idx": 16}, {"type": "text", "text": "Case 2: $c<0,b<0$ . We now have $a+b+c<\\operatorname*{min}\\{a+b,a+c\\}$ while $a>\\operatorname*{max}\\{a+b,a+c\\}$ , so $\\left(\\Delta_{2}\\right)$ sits above $(\\Delta_{1})$ . Again, $\\mu_{2}\\geqslant\\mu_{1}$ and finally $\\mathfrak{z}_{\\{b,c\\}}F(a)\\geqslant0$ . ", "page_idx": 16}, {"type": "text", "text": "Case 3: $c>0,b<0$ . We have $a+b<a$ and $a+b<a+b+c$ . Also $a+c>\\operatorname*{max}\\{a+b+c,a\\}$ , so this time $\\left(\\Delta_{2}\\right)$ sits below $(\\Delta_{1})$ but $c b<0$ , so $\\delta_{\\{b,c\\}}{\\cal F}(a)\\geqslant0$ again. ", "page_idx": 16}, {"type": "text", "text": "Case 4: $c<0,b>0$ . So $a+c<a<a+b$ and $a+c<a+b+c$ . So $a+c<\\operatorname*{min}\\{a,a+b+c\\}$ and $a+b>\\operatorname*{max}\\{a,a+c\\}$ , so $(\\Delta_{2})$ sits below $(\\Delta_{1})$ . Since $c b<0$ , so $\\delta_{\\{b,c\\}}{\\cal F}(a)\\geqslant\\dot{0}$ again. ", "page_idx": 16}, {"type": "text", "text": "B.4 Proof of Theorem 5.3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Let us remind key simplified notations about edges, $\\forall t\\geqslant0$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{e}_{t i}\\doteq y_{i}\\cdot H_{t}(\\pmb{x}_{i}),}\\\\ &{e_{t i}\\doteq y_{i}\\cdot\\alpha_{t}h_{t}(\\pmb{x}_{i})=\\tilde{e}_{t i}-\\tilde{e}_{(t-1)i}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For short, we also let: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{t i}^{*}\\doteq Q_{F}^{*}\\bigl(\\tilde{e}_{t i},\\tilde{e}_{(t-1)i},v_{i(t-1)}\\bigr),}\\\\ &{\\Delta_{t i}\\doteq\\delta_{v_{i(t-1)}}F\\bigl(\\tilde{e}_{t i}\\bigr)-\\delta_{v_{i(t-1)}}F\\bigl(\\tilde{e}_{(t-1)i}\\bigr),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $Q^{*}$ is defined in (4). We also split the computation of the leveraging coefficient $\\alpha_{t}$ in SECBOOSTin two parts, the first computing a real $a_{t}$ as: ", "page_idx": 16}, {"type": "equation", "text": "$$\na_{t}\\in\\frac{1}{2(1+\\varepsilon_{t})M_{t}^{2}\\overline{{W}}_{2,t}}\\cdot\\left[1-\\pi_{t},1+\\pi_{t}\\right],\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and then using $\\alpha_{t}\\gets a_{t}\\eta_{t}$ . We now use Lemma 4.7 (main file) and get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{i\\sim[m]}\\left[S_{F|v_{t i}}(\\tilde{e}_{t i}\\|\\tilde{e}_{(t+1)i})\\right]\\geqslant-\\mathbb{E}_{i\\sim D}\\left[Q_{(t+1)i}^{*}\\right],\\forall t\\geqslant0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "If we reorganise (28) using the definition of $S_{F|.}(.\\|.)$ , we get: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}_{t_{i}\\sim[m]}\\left[F(\\bar{\\epsilon}_{t+1/i})\\right]}\\\\ &{\\leqslant\\mathbb{E}_{t_{i}\\sim[m]}\\left[F(\\bar{\\epsilon}_{t+1/i})\\right]}\\\\ &{\\leqslant\\mathbb{E}_{t_{i}\\sim[m]}\\left[F(\\bar{\\epsilon}_{t+1/i})-\\mathbb{E}_{t_{i}\\sim[m]}\\left[\\left(\\bar{\\epsilon}_{t_{i}-t_{i}(1)}\\right)\\cdot\\delta_{\\nu_{t_{i}}}F(\\bar{\\epsilon}_{t+1/i})\\right]+\\mathbb{E}_{t_{i}\\sim[m]}\\left[Q_{\\bar{\\epsilon}(t+1)}^{*}\\right]\\right.}\\\\ &{\\qquad\\left.-\\mathbb{E}_{t_{i}\\sim[m]}\\left[F(\\bar{\\epsilon}_{t+1})\\right]-\\mathbb{E}_{t_{i}\\sim[m]}\\left[-\\epsilon_{\\epsilon(t+1)}\\cdot\\delta_{\\nu_{t_{i}}}F(\\bar{\\epsilon}_{t+1/i})\\right]+\\mathbb{E}_{t_{i}\\sim[m]}\\left[Q_{\\bar{\\epsilon}(t+1)}^{*}\\right]\\right]}\\\\ &{=\\mathbb{E}_{t_{i}\\sim[m]}\\left[F(\\bar{\\epsilon}_{t+1})\\right]+\\alpha_{t+1}\\cdot\\mathbb{E}_{t_{i}\\sim[m]}\\left[y_{t}h_{t+1}\\left(\\mathbf{x}_{t}\\right)\\cdot\\delta_{\\nu_{t}}F(\\bar{\\epsilon}_{t+1/i})\\right]+\\mathbb{E}_{t_{i}\\sim[m]}\\left[Q_{\\bar{\\epsilon}(t+1)}^{*}\\right]}\\\\ &{=\\mathbb{E}_{t_{i}\\sim[m]}\\left[F(\\bar{\\epsilon}_{t+1})\\right]+\\alpha_{t+1}\\cdot\\mathbb{H}_{t+[m]}+\\mathbb{E}_{t_{i}\\sim[m]}\\left[y_{t}h_{t+1}\\left(\\mathbf{x}_{t}\\right)\\cdot\\delta_{\\nu_{t}}F(\\bar{\\epsilon}_{t})\\right]}\\\\ &{\\quad+\\alpha_{t+1}y_{t+1}\\cdot\\mathbb{E}_{t_{i}\\sim[m]}\\left[y_{t}h_{t+1}\\left(\\mathbf{x}_{t}\\right)\\cdot\\Delta_{\\nu(t+1)}\\right]+\\mathbb{E}_{t_{i}\\sim[m]}\\left[Q_{\\bar{\\epsilon}(t+1)}^{*}\\right]}\\\\ &{=\\mathbb{E}_{t_{i}\\sim \n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "(29) \u2013 (31) make use of definitions (24) (twice) and (26) as well as the decomposition of the leveraging coefficient in (27). ", "page_idx": 17}, {"type": "text", "text": "Looking at (32), we see that we can have a boosting-compliant decrease of the loss if the two quantities depending on $\\Delta_{(t+1)}$ . and $Q_{(t+1)}^{*}$ . can be made small enough compared to $a_{t+1}\\eta_{t+1}^{2}$ . This is what we investigate. ", "page_idx": 17}, {"type": "text", "text": "Bounding the term depending on $\\Delta_{(t+1)}$ . \u2013 We use Lemma B.2 with $z\\doteq\\tilde{e}_{t i},z^{\\prime}\\doteq e_{(t+1)i},v\\doteq v_{t}$ , which yields (also using (24) and the assumption that $h_{t+1}(\\pmb{x}_{i})\\neq0)$ ): ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Delta_{(t+1)i}\\doteq\\delta_{v_{t i}}F\\bigl(\\tilde{e}_{(t+1)i}\\bigr)-\\delta_{v_{t i}}F\\bigl(\\tilde{e}_{t i}\\bigr)}&{}\\\\ {=\\delta_{v_{t i}}F\\bigl(\\tilde{e}_{t i}+e_{(t+1)i}\\bigr)-\\delta_{v_{t i}}F\\bigl(\\tilde{e}_{t i}\\bigr)}&{}\\\\ {=e_{(t+1)i}\\cdot\\delta_{\\{e_{(t+1)i},v_{t i}\\}}F\\bigl(\\tilde{e}_{t i}\\bigr)}&{}\\\\ {=y_{i}\\cdot\\alpha_{t+1}h_{t+1}(x_{i})\\cdot\\delta_{\\{e_{(t+1)i},v_{t i}\\}}F\\bigl(\\tilde{e}_{t i}\\bigr),}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and so we get: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{a_{t+1}\\eta_{t+1}\\cdot\\mathbb{E}_{i\\sim[m]}\\left[y_{i}h_{t+1}(\\pmb{x}_{i})\\cdot\\Delta_{(t+1)i}\\right]}\\\\ &{\\ =a_{t+1}\\eta_{t+1}\\cdot\\mathbb{E}_{i\\sim[m]}\\left[\\alpha_{t+1}(y_{i}h_{t+1}(\\pmb{x}_{i}))^{2}\\cdot\\delta_{\\{e_{(t+1)i},v_{t i}\\}}F(\\tilde{e}_{t i})\\right]}\\\\ &{\\ =a_{t+1}^{2}\\eta_{t+1}^{2}\\cdot\\mathbb{E}_{i\\sim[m]}\\left[(h_{t+1}(\\pmb{x}_{i}))^{2}\\cdot\\delta_{\\{e_{(t+1)i},v_{t i}\\}}F(\\tilde{e}_{t i})\\right]}\\\\ &{\\ \\leqslant a_{t+1}^{2}\\eta_{t+1}^{2}M_{t+1}^{2}\\cdot\\overline{{W}}_{2,t+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Bounding the term depending on $Q_{.(t+1)}^{*}-\\mathrm{We}$ immediately get from the value picked in argument of $\\mathbb{I}_{t+1}$ in step 2.5 of SECBOOST, the definition of $\\mathbb{I}_{t i}(.)$ in (10) and our decomposition $\\alpha_{t}\\gets a_{t}\\eta_{t}$ that $Q_{(t+1)i}^{*}\\leqslant\\varepsilon_{t+1}\\cdot a_{t+1}^{2}\\eta_{t+1}^{2}M_{t+1}^{2}\\cdot\\overline{{W}}_{2,t+1},\\forall i\\in[m],$ , so that: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{i\\sim[m]}\\left[Q_{(t+1)i}^{*}\\right]\\leqslant\\varepsilon_{t+1}\\cdot a_{t+1}^{2}\\eta_{t+1}^{2}M_{t+1}^{2}\\cdot\\overline{{W}}_{2,t+1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Finishing up with the proof \u2013 Suppose that we choose $\\varepsilon_{t+1}>0$ , $\\pi_{t+1}\\in(0,1)$ and $a_{t+1}$ as in (27). We then get from (32), (34), (35) that for any choice of $v_{t i}$ in Step 2.5 of SECBOOST, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbb{E}_{i\\sim[m]}\\left[F(\\widetilde{e}_{(t+1)i})\\right]}}\\\\ &{\\leqslant\\mathbb{E}_{i\\sim[m]}\\left[F(\\widetilde{e}_{t i})\\right]-a_{t+1}\\eta_{t+1}^{2}+a_{t+1}^{2}\\eta_{t+1}^{2}M_{t+1}^{2}\\cdot\\overline{{W}}_{2,t+1}+\\varepsilon_{t+1}\\cdot a_{t+1}^{2}\\eta_{t+1}^{2}M_{t+1}^{2}\\cdot\\overline{{W}}_{2,t+1}}\\\\ &{=\\mathbb{E}_{i\\sim[m]}\\left[F(\\widetilde{e}_{t i})\\right]-a_{t+1}\\eta_{t+1}^{2}\\cdot\\left(1-a_{t+1}\\left(1+\\varepsilon_{t+1}\\right)M_{t+1}^{2}\\cdot\\overline{{W}}_{2,t+1}\\right)}\\\\ &{\\leqslant\\mathbb{E}_{i\\sim[m]}\\left[F(\\widetilde{e}_{t i})\\right]-\\frac{\\eta_{t+1}^{2}\\left(1-\\pi_{t+1}^{2}\\right)}{4\\left(1+\\varepsilon_{t+1}\\right)M_{t+1}^{2}\\cdot\\overline{{W}}_{2,t+1}},}&{\\ \\left(3+\\varepsilon_{t+1}\\right)\\cdot\\overline{{W}}_{2,t+1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the last inequality is a consequence of (27). Suppose we pick $H_{0}\\doteq h_{0}\\in\\mathbb{R}$ a constant and $v_{0}>0$ such that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\delta_{v_{0}}F(h_{0})\\neq0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The final classifier $H_{T}$ of SECBOOST satisfies: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}_{i\\sim[m]}\\left[F(y_{i}H_{T}(\\pmb{x}_{i}))\\right]\\leqslant F_{0}-\\frac{1}{4}\\cdot\\sum_{t=1}^{T}\\frac{\\eta_{t}^{2}(1-\\pi_{t}^{2})}{(1+\\varepsilon_{t})M_{t}^{2}\\overline{{W}}_{2,t}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with $\\begin{array}{r c c c c c l}{F_{0}}&{\\doteq}&{\\mathbb{E}_{i\\sim[m]}\\left[F(\\tilde{e}_{i0})\\right]}&{\\doteq}&{\\mathbb{E}_{i\\sim[m]}\\left[F(y_{i}H_{0})\\right]}&{=}&{\\mathbb{E}_{i\\sim[m]}\\left[F(y_{i}h_{0})\\right]}\\end{array}$ . If we want $\\mathbb{E}_{i\\sim[m]}\\left[F(y_{i}H_{T}(\\pmb{x}_{i}^{\\cdot}))\\right]\\leqslant F(z^{*})$ , assuming wlog $F(z^{*})\\leqslant F_{0}$ , then it suffices to iterate until: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\frac{1-\\pi_{t}^{2}}{\\overline{{W}}_{2,t}(1+\\varepsilon_{t})}\\cdot\\frac{\\eta_{t}^{2}}{M_{t}^{2}}\\geqslant4(F_{0}-F(z^{*})).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Remind that the edge $\\eta_{t}$ is not normalized. We have defined a normalized edge, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left[-1,1\\right]\\ni\\tilde{\\eta}_{t}\\doteq\\sum_{i}\\frac{|w_{t i}|}{W_{t}}\\cdot\\tilde{y}_{t i}\\cdot\\frac{h_{t}(\\mathbf{x}_{i})}{M_{t}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "with ${\\tilde{y}}_{t i}\\;\\doteq\\;y_{i}\\,\\cdot\\,\\mathrm{sign}(w_{t i})$ and $\\begin{array}{r}{W_{t}\\ \\dot{=}\\ \\sum_{i}|w_{t i}|\\ =\\ \\sum_{i}|\\delta_{v_{(t-1)i}}F(\\tilde{e}_{(t-1)i})|}\\end{array}$ . We have the simple relationship between $\\eta_{t}$ and $\\tilde{\\eta}_{t}$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\tilde{\\eta}_{t}=\\displaystyle\\sum_{i}\\frac{\\lvert w_{t i}\\rvert}{W_{t}}\\cdot(y_{i}\\cdot\\mathrm{sign}(w_{t i}))\\cdot\\frac{h_{t}(\\pmb{x}_{i})}{M_{t}}}}\\\\ {{=\\displaystyle\\frac{1}{W_{t}M_{t}}\\cdot\\sum_{i}w_{t i}y_{i}h_{t}(\\pmb{x}_{i})}}\\\\ {{=\\displaystyle\\frac{m}{W_{t}M_{t}}\\cdot\\eta_{t},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "resulting in $(\\forall t\\geqslant1)$ ), ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\eta_{t}^{2}}{M_{t}^{2}}=\\tilde{\\eta}_{t}^{2}\\cdot\\left(\\frac{W_{t}}{m}\\right)^{2}}\\\\ &{\\qquad=\\tilde{\\eta}_{t}^{2}\\cdot\\left(\\mathbb{E}_{i\\sim[m]}\\left[|\\delta_{v_{(t-1)i}}F(\\tilde{e}_{(t-1)i})|\\right]\\right)^{2}}\\\\ &{\\qquad\\geqslant\\tilde{\\eta}_{t}^{2}\\cdot\\left(\\left|\\mathbb{E}_{i\\sim[m]}\\left[\\delta_{v_{(t-1)i}}F(\\tilde{e}_{(t-1)i})\\right]\\right|\\right)^{2}}\\\\ &{\\qquad=\\tilde{\\eta}_{t}^{2}\\cdot\\overline{{W}}_{1,t}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "recalling $\\overline{{W}}_{1,t}\\doteq\\left|\\mathbb{E}_{i\\sim D}\\left[\\delta_{v_{(t-1)i}}F(\\tilde{e}_{(t-1)i})\\right]\\right|$ . It comes from (42) that a sufficient condition for (39) to hold is: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\frac{\\overline{{W}}_{1,t}^{2}(1-\\pi_{t}^{2})}{\\overline{{W}}_{2,t}(1+\\varepsilon_{t})}\\cdot\\tilde{\\eta}_{t}^{2}\\geqslant4(F_{0}-F(z^{*})),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "which is the statement of Theorem 5.3. ", "page_idx": 18}, {"type": "text", "text": "B.5 Proof of Lemma 5.7 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We first observe that for any $a\\in\\mathbb{R},b,c\\in\\mathbb{R}_{*}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{|\\delta_{\\{b,c\\}}F(a)|=\\frac{1}{|b c|}\\cdot\\left|\\begin{array}{c}{F\\big(a+b+c\\big)-F\\big(a+c\\big)-b F^{\\prime}\\big(a+c\\big)}\\\\ {-(F(a+b)-F(a)-b F^{\\prime}(a)\\big)}\\\\ {+b(F^{\\prime}(a+c)-F^{\\prime}(a))}\\end{array}\\right|}&{}\\\\ {\\leqslant\\frac{1}{|b c|}\\cdot\\left(\\begin{array}{c}{|F\\big(a+b+c\\big)-F\\big(a+c)-b F^{\\prime}(a+c)|}\\\\ {+|(F\\big(a+b)-F\\big(a)-b F^{\\prime}(a)\\big)|}\\end{array}\\right)}&{}\\\\ {+|b(F^{\\prime}(a+c)-F^{\\prime}(a))|}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where we used the $\\beta$ -smoothness of $F$ and twice [12, Lemma 3.4]. We can also make a permutation in the expression of $\\delta_{\\{b,c\\}}F(a)$ and instead write ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left|\\delta_{\\{b,c\\}}F(a)\\right|=\\frac{1}{\\left|b c\\right|}\\cdot\\left|\\begin{array}{c}{F\\big(a+b+c\\big)-F\\big(a+b\\big)-c F^{\\prime}(a+b)}\\\\ {-(F(a+c)-F(a)-c F^{\\prime}(a))}\\\\ {+c(F^{\\prime}(a+b)-F^{\\prime}(a))}\\end{array}\\right|}&{}\\\\ {\\leqslant\\frac{1}{\\left|b c\\right|}\\cdot\\left(\\begin{array}{c}{|F\\big(a+b+c)-F\\big(a+b\\big)-c F^{\\prime}(a+b)|}\\\\ {+|(F(a+c)-F\\big(a)-c F^{\\prime}(a))|}\\end{array}\\right)}&{}\\\\ {\\leqslant\\frac{1}{\\left|b c\\right|}\\cdot\\left(\\frac{\\beta}{2}\\cdot c^{2}+\\frac{\\beta}{2}\\cdot c^{2}+\\beta|b c|\\right)=\\beta+\\beta\\cdot\\frac{c^{2}}{\\left|b c\\right|}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We thus have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\delta_{\\{b,c\\}}F(a)|\\leqslant\\beta+\\beta\\cdot\\left(\\frac{\\operatorname*{min}\\{|b|,|c|\\}}{\\sqrt{|b c|}}\\right)^{2}}\\\\ &{\\qquad\\qquad\\leqslant2\\beta,}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "by the power mean inequality [13, Chapter III, Theorem 2]. Since $|h_{t}(\\pmb{x}_{i})|\\leqslant M_{t}$ by definition, we thus have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}_{i\\sim[m]}\\left[\\delta_{\\{e_{t i},v_{(t-1)i}\\}}F(\\tilde{e}_{(t-1)i})\\cdot\\left(\\frac{h_{t}({\\pmb x}_{i})}{M_{t}}\\right)^{2}\\right]\\right|\\leqslant2\\beta,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which allows us to fix $\\overline{{W}}_{2,t}=2\\beta$ and completes the proof of Lemma 5.7. ", "page_idx": 19}, {"type": "text", "text": "Remark B.3. Our result is optimal in the sense that if we make one offset (say b) go to zero, then the ratio in (46) goes to zero and we recover the condition on the $v$ -derivative of the derivative, $|\\delta_{c}F^{\\prime}(z)|\\leqslant\\beta$ . ", "page_idx": 19}, {"type": "text", "text": "B.6 Proof of Theorem 5.8 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We consider the upperbound:: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{N}_{2,t}}\\\\ &{\\overset{,}{=}\\left|\\mathbb{E}_{i\\sim[m]}\\left[\\frac{h_{t}^{2}(x_{i})}{M_{t}^{2}}\\cdot\\delta_{\\{e_{t i},v_{(t-1)i}\\}}F(\\tilde{e}_{(t-1)i})\\right]\\right|}\\\\ &{=\\left|\\mathbb{E}_{i\\sim[m]}\\left[\\frac{h_{t}^{2}(x_{i})}{M_{t}^{2}}\\cdot\\frac{1}{\\tilde{e}_{t i}}\\cdot\\left(\\frac{F(\\tilde{e}_{t i}+v_{(t-1)i})-F(\\tilde{e}_{t i})}{v_{(t-1)i}}-\\frac{F(\\tilde{e}_{(t-1)i}+v_{(t-1)i})-F(\\tilde{e}_{(t-1)i})}{v_{(t-1)i}}\\right)\\right]\\right|}\\\\ &{=\\left|\\frac{1}{\\alpha_{t}}\\cdot\\mathbb{E}_{i\\sim[m]}\\left[\\frac{h_{t}(x_{i})}{y_{i}M_{t}^{2}}\\cdot\\left(\\frac{F(\\tilde{e}_{t i}+v_{(t-1)i})-F(\\tilde{e}_{t i})}{v_{(t-1)i}}-\\frac{F(\\tilde{e}_{(t-1)i}+v_{(t-1)i})-F(\\tilde{e}_{(t-1)i})}{v_{(t-1)i}}\\right)\\right]\\right|}\\\\ &{=\\left|\\frac{1}{\\alpha_{t}}\\cdot\\mathbb{E}_{i\\sim[m]}\\left[\\frac{y_{i}h_{t}(x_{i})}{M_{t}^{2}}\\cdot\\left(\\frac{\\delta_{v_{(t-1)i}}F(\\tilde{e}_{t i})-\\delta_{v_{(t-1)i}}F(\\tilde{e}_{(t-1)i})}{v_{(t-1)i}}\\right)\\right]\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "(The last identity uses the fact that $y_{i}\\;\\in\\;\\{-1,1\\})$ . Remark that we have extracted $\\alpha_{t}$ from the denominator but it is still present in the arguments $\\tilde{e}_{t i}$ . For any classifier $h$ , we introduce notation ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\eta(\\pmb{w},h)\\doteq\\mathbb{E}_{i\\sim[m]}\\left[w_{i}y_{i}h(\\pmb{x}_{i})\\right],\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and so $\\eta_{t}$ (Step 2.2 in SECBOOST) is also $\\eta(\\boldsymbol{w}_{t},h_{t})$ , which is guaranteed to be non-zero by the Weak Learning Assumption (5.4). We want, for some $\\varepsilon_{t}>0,\\pi_{t}\\in[0,1)$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\alpha_{t}\\in\\frac{\\eta_{t}}{2(1+\\varepsilon_{t})M_{t}^{2}\\overline{{W}}_{2,t}}\\cdot\\left[1-\\pi_{t},1+\\pi_{t}\\right].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This says that the sign of $\\alpha_{t}$ is the same as the sign of $\\eta(\\pmb{w}_{t},h_{t})=\\eta_{t}$ . Since we know its sign, let us look for its absolute value: ", "page_idx": 20}, {"type": "equation", "text": "$$\n|\\alpha_{t}|\\in\\frac{|\\eta_{t}|}{2(1+\\varepsilon_{t})M_{t}^{2}\\overline{{W}}_{2,t}}\\cdot\\left[1-\\pi_{t},1+\\pi_{t}\\right].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "From (9) (main flie), we can in fact search $\\alpha_{t}$ in the union of all such intervals for $\\varepsilon_{t}>0,\\pi_{t}\\in[0,1)$ , which amounts to find first: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left|\\alpha_{t}\\right|\\in\\left(0,\\frac{\\left|\\eta_{t}\\right|}{M_{t}^{2}\\overline{{W}}_{2,t}}\\right),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and then find any $\\varepsilon_{t}>0,\\pi_{t}\\in[0,1)$ such that (50) holds. Using (48) and simplifying the external dependency on $\\alpha_{t}$ , we then need ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{\\Omega}_{1}\\in\\left(\\begin{array}{l}{0,\\displaystyle\\frac{|\\eta_{t}|}{\\underbrace{|\\mathbb{E}_{i\\sim[m]}\\left[y_{i}h_{t}(\\mathbf{x}_{i})\\cdot\\left(\\delta_{v_{(t-1)i}}F(\\alpha_{t}y_{i}h_{t}(\\mathbf{x}_{i})+\\tilde{e}_{(t-1)i})-\\delta_{v_{(t-1)i}}F(\\tilde{e}_{(t-1)i})\\right)\\right]}_{=B(\\alpha_{t})}}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "under the constraint that the sign of $\\alpha_{t}$ be the same as that of $\\eta_{t}$ . But, using notation (17) (main flie), we have ", "page_idx": 20}, {"type": "equation", "text": "$$\nB(\\alpha_{t})=|\\eta(\\pmb{w}_{t},h_{t})-\\eta(\\tilde{\\pmb{w}}_{t}(\\alpha_{t}),h_{t})|,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "and so to get (51) satisfied, it is sufficient that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{|\\eta_{t}-\\eta(\\tilde{\\pmb{w}}_{t}(\\alpha_{t}),h_{t})|}{|\\eta_{t}|}<1,\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "which is Step 1 in $\\mathrm{{SOLVE}}_{\\alpha}$ . The Weak Learning Assumption (5.4) guarantees that the denominator is $\\neq~0$ so this can always be evaluated. The continuity of $F$ in all $\\tilde{e}_{(t-1)i}$ guarantees $\\begin{array}{r}{\\operatorname*{lim}_{\\alpha_{t}\\rightarrow0}\\eta(\\tilde{\\pmb{w}}_{t}(\\alpha_{t}),h_{t})=\\eta_{t}}\\end{array}$ , and thus guarantees the existence of solutions to (52) for some $|\\alpha_{t}|>0$ . ", "page_idx": 20}, {"type": "text", "text": "To summarize, finding $\\alpha_{t}$ can be done in two steps, (i) solve ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{|\\eta_{t}-\\eta(\\tilde{\\mathbf{w}}_{t}(\\mathrm{sign}(\\eta_{t})\\cdot a),h_{t})|}{|\\eta_{t}|}<1\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "for some $a>0$ and (ii) let $\\alpha_{t}\\doteq\\mathrm{sign}(\\eta_{t})\\cdot a$ . This is the output of ${\\mathrm{SOLVE}}_{\\alpha}(S,{\\pmb w}_{t},h_{t})$ , which ends the proof of Theorem 5.8. ", "page_idx": 20}, {"type": "text", "text": "B.7 Implementation of the offset oracle: particular cases ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Consider the \"spring loss\" that we define, for $[.]$ denoting the nearest integer, as: ", "page_idx": 20}, {"type": "equation", "text": "$$\nF_{\\mathrm{sL}}(z)\\doteq\\log(1+\\exp(-z))+1-\\sqrt{1-4\\left(z-[z]\\right)^{2}}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Figure 6 plots this loss, which composes the logistic loss with a \"U\"-shaped term. This loss would escape all optimization algorithms of Table 1 (Appendix), yet there is a trivial implementation of our offset oracle, as explained in Figure 6: ", "page_idx": 20}, {"type": "text", "text": "1. if the interval I defined by $\\tilde{e}_{(t-1)i}$ and $\\tilde{e}_{t i}$ contains at least one peak, compute the tangence point $(z_{t})$ at the closest local \"U\" that passes through $\\big(\\tilde{e}_{(t-1)i},F(\\tilde{e}_{(t-1)i})\\big)$ ; then if $z_{t}\\in\\mathbb{I}$ then $v_{t i}\\leftarrow z_{t}-\\tilde{e}_{(t-1)i}$ , else $v_{t i}\\leftarrow\\tilde{e}_{t i}-\\tilde{e}_{(t-1)i}$ ; ", "page_idx": 21}, {"type": "text", "text": "2. otherwise $F$ in $\\mathbb{I}$ is strictly convex and differentiable: a simple dichotomic search can retrieve a feasible $v_{t i}$ (see convex losses below); ", "page_idx": 21}, {"type": "text", "text": "Notice that one can alleviate the repetitive dichotomic search by pre-tabulating a feasible $v$ for a set of differences $|a-b|$ $[a,b$ belonging to the abscissae of the same $\"\\mathrm{U}\")$ decreasing by a fixed factor, choosing $v_{t i}\\leftarrow v$ of the largest tabulated $|a-b|$ no larger than $|\\tilde{e}_{t i}-\\tilde{e}_{(t-1)i}|$ . ", "page_idx": 21}, {"type": "text", "text": "Discontinuities discontinuities do not represent issues if the argument $z$ of $\\mathbb{I}_{t i}(z)$ is large enough, as shown from the following simple Lemma. ", "page_idx": 21}, {"type": "text", "text": "Lemma B.4. Define the discontinuity of $F$ as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname{disc}(F)\\doteq\\operatorname*{max}\\left\\{\\begin{array}{l}{\\operatorname*{sup}_{z}|F(z)-\\operatorname*{lim}_{z^{-}}F(z)|,}\\\\ {\\operatorname*{sup}_{z}|F(z)-\\operatorname*{lim}_{z^{+}}F(z)|}\\end{array}\\right\\}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Figure 4 (c) shows a case where the discontinuity is larger than $z$ . In this case, an issue eventually happens for computing the next weight happens, only when the current edge is at the discontinuity. We note that as iterations increase and the weak learner finds it eventually more difficult to return weak hypotheses with $\\eta$ . large enough, the discontinuities may become an issue for SECBOOST to not stop at Step 2.5. Or one can always use a simple trick to avoid stopping and which relies on the leveraging coefficient $\\alpha_{t}$ : this is described in the Appendix, Section B.9. ", "page_idx": 21}, {"type": "text", "text": "The case of convex losses If $F$ is convex (not necessarily differentiable nor strictly convex), there is a simple way to find a valid output for the offset oracle, which relies on the following Lemma. ", "page_idx": 21}, {"type": "text", "text": "Lemma B.5. Suppose $F$ convex. Then for any $z,z^{\\prime}\\in\\mathbb{R},v\\neq0,$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\{v>0:Q_{F}^{*}(z,z^{\\prime},v)=r\\}}\\\\ {\\displaystyle=\\left\\{v>0:D_{F}\\left(z\\left\\|\\frac{F(z+v)-F(z)}{v}\\right)=r\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "(proof in Appendix, Section B.8) By definition, $\\mathbb{I}_{t i}(z^{\\prime})\\subseteq\\mathbb{I}_{t i}(z)$ for any $z^{\\prime}\\leqslant z$ , so a simple way to implement the offset oracle\u2019s output $\\operatorname{oo}(t,i,z)$ is, for some $0<r<z$ , to solve the Bregman identity in the RHS of (55) and then return any relevant $v$ . If $F$ is strictly convex, there is just one choice. ", "page_idx": 21}, {"type": "text", "text": "If solving the Bregman identity is tedious but $F$ is strictly convex, there a simple dichotomic search that is guaranteed to find a feasible $v$ . It exploits the fact that the abscissa maximizing the difference between any secant of $F$ and $F$ has a simple closed form (see [21, Supplement, Figure 13]) and so the OBI in (1) (Definition 4.6) has a closed form as well. In this case, it is enough, after taking a first non-zero guess for $v$ (either positive or negative), to divide it by a constant $>\\,1$ until the corresponding OBI is no larger than the $z$ in the query $\\operatorname{oo}(t,i,z)$ . ", "page_idx": 21}, {"type": "text", "text": "B.8 Proof of Lemma B.5 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "$F$ being convex, we first want to compute the set ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbb{I}_{z,v,r}\\doteq\\{v>0:Q_{F}(z,z+v,z+v)=r\\},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $r$ is supposed small enough for $\\mathbb{I}_{z,v,r}$ to be non-empty. There is a simple graphical solution to this which, as Figure 7 explains, consists in finding $v$ solution of ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t}F(z+v)-\\left(F(t)+\\left({\\frac{F(z+v)-F(z)}{v}}\\right)\\cdot(z+v-t)\\right)=r.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "image", "img_path": "MLgFu6dQYc/tmp/e7c5e06e9d798981893c627c689109031e3aa013b475d722d60220af263a1044.jpg", "img_caption": ["Figure 6: The spring loss in (53) is neither convex, nor Lipschitz or differentiable and has an infinite number of local minima. Yet, an implementation of the offset oracle is trivial as an output for OO can be obtained from the computation of a single tangent point (here, the orange $v$ , see text; best viewed in color). "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "MLgFu6dQYc/tmp/16dbe5934b61e2cab33109e25a78f447d59052abaaab54978b45b07868c7fc10.jpg", "img_caption": ["Figure 7: Computing the OBI $Q_{F}(z,z+v,z+v)$ for $F$ convex, $(z,v)$ being given and $v>0$ . We compute the line $\\left(\\Delta_{t}\\right)$ crossing $F$ at any point $t$ , with slope equal to the secant $[(z,F(z)),(z+$ $v,F(z+v))]$ and then the difference between $F$ at $z+v$ and this line at $z+v$ . We move $t$ so as to maximize this difference. The optimal $t$ (in green) gives the corresponding OBI. In (56) and 58, we are interested in finding $v$ given this difference, $r$ . We also need to replicate this computation for $v<0$ . "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "The LHS simplifies: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{sup}_{t}F(z+v)-\\left(F(t)+\\left(\\frac{F(z+v)-F(z)}{v}\\right)\\cdot(z+v-t)\\right)}\\\\ &{=\\displaystyle\\frac{(z+v)F(z)-z F(z+v)}{v}+\\operatorname*{sup}_{t}\\left\\{t\\cdot\\frac{F(z+v)-F(z)}{v}-F(t)\\right\\}}\\\\ &{=\\displaystyle\\frac{(z+v)F(z)-z F(z+v)}{v}+F^{\\star}\\left(\\frac{F(z+v)-F(z)}{v}\\right)}\\\\ &{=F(z)+F^{\\star}\\left(\\frac{F(z+v)-F(z)}{v}\\right)-z\\cdot\\frac{F(z+v)-F(z)}{v}}\\\\ &{=D_{F}\\left(z\\left\\|\\frac{F(z+v)-F(z)}{v}\\right),2^{3}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "so we end up with an equivalent but more readable definition for $\\mathbb{I}_{z,v,r}$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{I}_{z,v,r}=\\left\\{v>0:D_{F}\\left(z\\left\\|{\\frac{F(z+v)-F(z)}{v}}\\right)=r\\right\\},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "which yields the statement of the Lemma. ", "page_idx": 23}, {"type": "text", "text": "B.9 Handling discontinuities in the offset oracle to prevent stopping in Step 2.5 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Theorem 5.3 and Lemma 5.6 require to run SECBOOST for as many iterations are required. This implies not early stopping in Step 2.5. Lemma B.4 shows that early stopping can only be triggered by too large local discontinuities at the edges. This is a weak requirement on running SECBOOST, but there exists a weak assumption on the discontinuities of the loss itself that simply prevent any early stopping and does not degrade the boosting rates. The result exploits the freedom in choosing $\\alpha_{t}$ in Step 2.3. ", "page_idx": 23}, {"type": "text", "text": "Lemma B.6. Suppose $F$ is any function defined over $\\mathbb{R}$ discontinuities of zero Lebesgue measure. Then Corollary 5.6 holds for boosting $F$ with its inequality strict while never triggering early stopping in Step 2.5 of SECBOOST. ", "page_idx": 23}, {"type": "text", "text": "Proof. To show that we never trigger stopping in Step 2.5, it is sufficient to show that we can run SECBOOSTwhile ensuring $F$ is continuous in an open neighborhood around all edges $y_{i}H_{t}(x_{i})$ , $\\forall i\\in$ $[m],\\forall t\\geqslant0$ (by letting $H_{0}\\doteq h_{0}.$ ). Remind that $\\tilde{e}_{t i}\\doteq\\tilde{e}_{(t-1)i}+\\alpha_{t}\\cdot y_{t}h_{t}({\\pmb x}_{i})$ , so changing $\\alpha_{t}$ changes all edges. We just have to show that either computing $\\alpha_{t}$ ensures such a continuity, or $\\alpha_{t}$ can be slightly modified to do so. We have two ways to compute $\\alpha_{t}$ : ", "page_idx": 23}, {"type": "text", "text": "1. using a value for $\\overline{{W}}_{2,t}$ that represents an \"absolute\" upperbound in the sense of (8) (e.g. Lemma 5.7) and then compute $\\alpha_{t}$ as in Step 2.3 of SECBOOST; ", "page_idx": 23}, {"type": "text", "text": "2. using algorithm $\\mathrm{{SOLVE}}_{\\alpha}$ . ", "page_idx": 23}, {"type": "text", "text": "Because of the assumption on $F$ , we can always ensure that $F$ is continuous in an open neighborhood of all edges (the basis of the induction amounts to a straightforward choice for $h_{0}$ ). This proves the Lemma for [2.]. ", "page_idx": 23}, {"type": "text", "text": "If we rely on [1.] and the $\\alpha_{t}$ computed leads to some discontinuities, then we have complete control to change $\\alpha_{t}$ : any continuous change of $\\varepsilon_{t}$ induces a continuous change in $\\alpha_{t}$ and thus a continuous change of all edges as well. So, starting from the initial $\\varepsilon_{t}$ chosen in Step 2.3, we increase it to a value $\\varepsilon_{t}^{\\ast}>\\varepsilon_{t}$ , which we want to keep as small as possible. We can define for each $i\\in[m]$ an open set $(a_{i},b_{i})$ which is the interval spanned by the new $\\tilde{e}_{t i}(\\varepsilon_{t}^{\\prime})$ using $\\boldsymbol{\\varepsilon}_{t}^{\\prime}\\in\\left(\\varepsilon_{t},\\varepsilon_{t}^{*}\\right)$ . Since there are only finitely many discontinuities on $F$ , there exists a small $\\varepsilon_{t}^{\\ast}>\\varepsilon_{t}$ such that ", "page_idx": 23}, {"type": "text", "text": "This means that $\\forall\\varepsilon_{t}^{\\prime}\\in\\left(\\varepsilon_{t},\\varepsilon_{t}^{*}\\right)$ , we end up with a loss without any discontinuities on the new edges. Now comes the reason why we want $\\varepsilon_{t}^{*}-\\varepsilon_{t}$ small: we can check that there always exist a small enough $\\varepsilon_{t}^{\\ast}>\\varepsilon_{t}$ such that for any $\\varepsilon_{t}^{\\prime}$ we choose, the boosting rate in Corollary 5.6 is affected by at most 1 additional iteration. Indeed, while we slightly change parameter $\\varepsilon_{t}$ to land all new edges outside of discontinuities of $F$ , we also increase the contribution of the boosting iteration in the RHS of (15) by a quantity $\\delta>0$ which can be made as small as required \u2014 hence we can just replace the inequality in (15) by a strict inequality. This proves the statement of the Lemma if we rely on [1.] above. ", "page_idx": 23}, {"type": "text", "text": "This completes the proof of Lemma B.6. ", "page_idx": 23}, {"type": "text", "text": "B.10 A boosting pattern that can \"survive\" above differentiability ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Suppose $F$ is strictly convex and strictly decreasing as for classical convex surrogates (e.g. logistic loss). Assuming wlog all $\\alpha_{\\cdot}>0$ and example $i$ has both $y_{i}h_{t}(\\pmb{x}_{i})>0$ and $y_{i}h_{t-1}(\\pmb{x}_{i})>0$ , as long as $z$ is small enough, we are guaranteed that any choice $v_{t-1}\\in\\mathbb{I}_{(t-1)i}(z)$ and $v_{t}\\in\\mathbb{I}_{t i}(z)$ results in $0<w_{(t+1)i}<w_{t i}$ , which follows the classical boosting pattern that examples receiving the right class by weak hypotheses have their weight decreased (See Figure 8). If $z=z^{\\prime}$ is large enough, then this does not hold anymore as seen from Figure 8. ", "page_idx": 23}, {"type": "image", "img_path": "MLgFu6dQYc/tmp/ab295617eefdcc01d9bc9a534884415bb811abd3d74b5a3111adddd4e4a06346.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 8: Case $F$ strictly convex, with two cases of limit OBI $z$ and $z^{\\prime}$ in $\\mathbb{I}_{.i}(.)$ . Example $i$ has $e_{t i}\\,>\\,0$ and $e_{(t-1)i}\\,>\\,0$ (??) large enough (hence, edges with respect to weak classifiers $h_{t}$ and $h_{t-1}$ large enough) so that $\\mathbb{I}_{t i}(z)\\cap\\mathbb{I}_{(t-1)i}(z)=\\mathbb{I}_{(t-1)i}(z)\\cap\\mathbb{I}_{(t-2)i}(z)=\\mathbb{I}_{t i}(z)\\cap\\mathbb{I}_{(t-2)i}(z)=\\emptyset$ . In this case, regardless of the offsets chosen by OO, we are guaranteed that its weights satisfy $w_{(t+1)i}\\,<\\,w_{t i}\\,<\\,w_{(t-1)i}$ , which follows the boosting pattern that examples receiving the right classification by weak classifiers have their weights decreasing. If however the limit OBI changes from $z$ to a larger $z^{\\prime}$ , this is not guaranteed anymore: in this case, it may be the case that $w_{(t+1)i}>w_{t i}$ . ", "page_idx": 24}, {"type": "image", "img_path": "MLgFu6dQYc/tmp/6f3141a22fa344ea530630d113d8cacf6441cce0b1a62cf0fca5fe056536d51a.jpg", "img_caption": ["Figure 9: How our algorithm works with the $0/1$ loss (in red): at the initialization stage, assuming we pick $h_{0}=0$ for simplicity and some $v_{0}<0$ , all training examples get the same weight, given by negative the slope of the thick blue dashed line. All weights are thus $>0$ . At iteration $t$ when we update the weights (Step 2.6), one of two cases can happen on some training example $(\\pmb{x},y)$ . In (A), the edge of the strong model remains the same: either both are positive (blue) or both negative (olive green) (the ordering of edges is not important). In this case, regardless of the offset, the new weight will be 0. In $\\mathbf{\\tau}(\\mathbf{B})$ , both edges have different sign (again, the ordering of edges is not important). In this case, the examples will keep non-zero weight over the next iteration. See text below for details. "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 9 schematizes a run of our algorithm when training $\\mathrm{loss}=0/1$ loss. At the initialization, it is easy to get all examples to have non-zero weight. The weight update for example $(\\boldsymbol{x},\\boldsymbol{y})$ of our algorithm in Step 2.3 is (negative) the slope of a secant that crosses the loss in two points, both being in between $y H_{t-1}(\\pmb{x})$ and $y H_{t}(\\pmb{x})$ . Hence, if the predicted label does not change $(\\operatorname{sign}(H_{t}(\\pmb{x}))=\\operatorname{sign}(H_{t-1}(\\pmb{x})))$ , then the next weight $(w_{t+1})$ of the example will be zero (Figure 9, case (A)). However, if the predicted label does change $(\\mathrm{sign}(H_{t}(\\pmb{x}))\\neq\\mathrm{sign}(H_{t-1}(\\pmb{x})))$ then the example may get a non-zero weight depending on the offset chosen. ", "page_idx": 25}, {"type": "text", "text": "Hence, our generic implementation of Algorithms 3 and 4 may completely fail at providing non-zero weights for the next iteration, which makes the algorithm stop in step 2.7. And even when not all weights are zero, there may be just a too small subset of those, that would break the Weak Learning Assumption for boosting compliance of the next iteration (Assumption 5.5). ", "page_idx": 25}, {"type": "text", "text": "C Supplementary material on algorithms, implementation tricks and a toy experiment ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "C.1 Algorithm and implementation of $\\mathbf{SoLVE}_{\\alpha}$ and how to find parameters from Theorem 5.8 ", "page_idx": 25}, {"type": "text", "text": "As Theorem 5.8 explains, $\\mathrm{SOLVE}_{\\alpha}$ can easily get to not just the leveraging coefficient $\\alpha_{t}$ , but also other parameters that are necessary to implement SECBOOST: $\\overline{{W}}_{2,t}$ and $\\varepsilon_{t}$ (both used in Step 2.5). We now provide a simple pseudo code on how to implement $\\mathrm{{SOLVE}}_{\\alpha}$ amnd get, on top of it, the two other parameters. We do not seek $\\pi_{t}$ since it is useful only in the convergence analysis. Also, our proposal implementation is optimized for complexity (because of the geometric updating of $\\delta,W$ in their respective loops) but much less so for for accuracy. Algorithm SOLVE_extended explains the overall procedure. ", "page_idx": 25}, {"type": "text", "text": "Input sample $S=\\{(\\pmb{x}_{i},y_{i}),i=1,2,...,m\\}$ , ${\\pmb w}\\in\\mathbb{R}^{m}$ , $h:\\mathcal{X}\\to\\mathbb{R}$ , $M\\ne0$ . ", "page_idx": 26}, {"type": "text", "text": "$//$ in our case, $\\pmb{w}\\leftarrow\\pmb{w}_{t};h\\leftarrow h_{t};M\\leftarrow M_{t}$ (current weights, weak hypothesis and max confidence, see Step 2.3 in SECBOOST and Assumption 5.1) Step 1 : // all initializations ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{r l}&{\\eta_{\\mathrm{init}}\\leftarrow\\eta(\\pmb{w},h);}\\\\ &{\\quad\\delta\\leftarrow1.0;}\\\\ &{W_{\\mathrm{init}}\\leftarrow1.0;}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Step 2 : do ", "page_idx": 26}, {"type": "text", "text": "// Step 2 computes the leveraging coefficient $\\alpha_{t}$ ", "page_idx": 26}, {"type": "text", "text": "$\\alpha\\leftarrow\\delta\\cdot\\mathrm{sign}(\\eta_{\\mathrm{init}});$   \n$\\eta_{\\mathrm{new}}\\gets\\eta(\\tilde{\\pmb{w}}(\\alpha),h);$ ;   \nif $|\\eta_{\\mathrm{new}}-\\eta_{\\mathrm{init}}|<|\\eta_{\\mathrm{init}}|$ then found_alpha $\\gets$ true else $\\delta\\gets\\delta/2$ ; hile found_alpha $=$ false; ", "page_idx": 26}, {"type": "text", "text": "Step $3:W\\gets1$ Left Hand Side of (8) (main file) // Step 3 computes $\\overline{{W}}_{2,t}$ // we can use (8) (main file) because we know $\\alpha$ ", "page_idx": 26}, {"type": "text", "text": "if $W=_{\\mathrm{machine}}0$ then $//$ the LHS of (8) is (machine) 0: just need to find $W$ such that (9) holds ! W \u00d0 Winit; while $|\\alpha|>|\\eta_{\\mathrm{init}}|/(W\\cdot M^{2})\\:\\mathbf{do}\\:W\\leftarrow W/2;$   \nendif ", "page_idx": 26}, {"type": "text", "text": "$\\varepsilon\\gets(b_{\\mathrm{sup}}/\\alpha)-1$ ; Return $(\\alpha,W,\\varepsilon)$ ; ", "page_idx": 26}, {"type": "image", "img_path": "MLgFu6dQYc/tmp/fdaea9b15484a1d958a12640dcaec49995139b074273571a301bcd8dd087b4a1.jpg", "img_caption": ["Figure 10: How to find some $v\\in\\mathbb{I}_{t i}(z)$ : parse the interval $[\\tilde{e}_{t i},\\tilde{e}_{(t-1)i}]$ with a regular step $\\delta$ , seek the secant with minimal slope (because $\\tilde{e}_{t i}<\\tilde{e}_{(t-1)i}$ ; otherwise, we would seek the secant with maximal slope). It is necessarily the one minimizing the OBI among all regularly spaced choices. If the OBI is still too large, decrease the step $\\delta$ and start the search again. "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "C.2 Algorithm and implementation of the offset oracle ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "There exists a very simple trick to get some adequate offset $v$ to satisfy (11) (main flie), explained in Figure 10. In short, we seek the optimally bended secant and check that the OBI is no more than a required $z$ . This can be done via parsing the interval $[\\tilde{e}_{t i},\\tilde{e}_{(t-1)i}]$ using regularly spaced values. If the OBI is too large, we can start again with a smaller step size. Algorithm OO_simple details the key part of the search. ", "page_idx": 26}, {"type": "text", "text": "Input loss $F$ , two last edges $\\tilde{e}_{t},\\tilde{e}_{t-1}$ , maximal OBI $z$ , precision $Z$ . $//$ in our case, $\\tilde{e}_{t}\\gets\\tilde{e}_{t i};\\tilde{e}_{t-1}\\gets\\tilde{e}_{(t-1)i}$ ; (for training example index $i\\in[m])$ Step 1 : $//$ all initializations ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\delta\\gets\\frac{\\tilde{e}_{t-1}-\\tilde{e}_{t}}{Z};}\\\\ {z_{c}\\gets\\tilde{e}_{t}+\\delta;}\\\\ {i\\gets0;}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Step 2 : do ", "page_idx": 27}, {"type": "text", "text": "Return $\\boldsymbol{z}_{\\ast}-\\tilde{\\boldsymbol{e}}_{t}$ ; ", "page_idx": 27}, {"type": "text", "text": "// checks that $z_{c}$ is still in the interval $//$ this is the offset $v$ ", "page_idx": 27}, {"type": "image", "img_path": "MLgFu6dQYc/tmp/c8feac174126b4d1c9d69ba8ef0e3b52df69d2254688e7c1f42c22e8eda5f70c.jpg", "img_caption": ["Figure 11: Crops of the two losses whose optimization has been experimentally tested with SECBOOST, in addition to the logistic loss. See text for details. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "C.3 A toy experiments ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We provide here a few toy experiments using SECBOOST. These are just meant to display that a simple implementation of the algorithm, following the blueprints given above, can indeed manage to optimize various losses. These are not meant to explain how to pick the best hyperparameters (e.g. (60)) nor how to choose the best loss given a domain, a problem that is far beyond the scope of our paper. ", "page_idx": 27}, {"type": "text", "text": "In this implementation, the weak learner learns decision trees and we minimize Matushita\u2019s loss at the leaves of decision trees to learn fixed size trees, see [33] for the criterion and induction scheme, which is standard for decision trees. SECBOOST is implemented as is given in the paper, and so are the implementation of $\\mathrm{{SOLVE}}_{\\alpha}$ and the offset oracle provided above. We have made no optimization whatsoever, with one exception: when numerical approximation errors lead to an offset that is machine 0, we replace it by a small random value to prevent the use of derivatives in SECBOOST. ", "page_idx": 27}, {"type": "text", "text": "We have investigated three losses. The first is the well known logistic loss: ", "page_idx": 27}, {"type": "equation", "text": "$$\nF_{\\mathrm{LoG}}(z)\\doteq\\log(1+\\exp(-z)).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "The other two are tweaks of the logistic loss. We have investigated a clipped version of the logistic loss, ", "page_idx": 27}, {"type": "equation", "text": "$$\nF_{\\mathrm{cL},q}(z)\\doteq\\operatorname*{min}\\{\\log(1+\\exp(-z)),\\log(1+\\exp(-q))\\},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "with $q\\in\\mathbb{R}$ , which clips the logistic loss above a certain value. This loss is non-convex and nondifferentiable, but it is Lipschitz. We have also investigated a generalization of the spring loss (main file): ", "page_idx": 28}, {"type": "equation", "text": "$$\nF_{\\mathrm{sL},Q}(z)\\doteq\\log(1+\\exp(-z))+\\frac{1-\\sqrt{1-4\\left(z_{Q}-[z_{Q}]\\right)^{2}}}{Q},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "with $z_{Q}\\doteq Q z-1/2$ (r.s is the closest integer), which adds to the logistic loss regularly spaced peaks of variable width. This loss is non-convex, non-differentiable, non-Lipschitz. Figure 11 provides a crop of the clipped logistic loss and spring loss we have used in our test. Notice the \u201chardness\u201d that the spring loss intuitively represents for ML. ", "page_idx": 28}, {"type": "text", "text": "We provide an experiment on public domain UCI tictactoe [23] (using a 10-fold stratified crossvalidation to estimate test errors). In addition to the three losses, we have crossed them with several other variables: the size of the trees (either they have a single internal node $=$ stumps or at most 20 nodes) and, to give one example of how changing a (key) hyperparameter can change the result, we have tested for a scale of changes on the initial value of $\\delta$ in (60). Finally, we have crossed all these variables with the existence of symmetric label noise in the training data, following the setup of [37, 39]. We flip each label in the training sample with probability $\\eta$ . Table 12 summarizes the results obtained. One can see that SECBOOST manages to optimize all losses in pretty much all settings, with an eventual early stopping required for the spring loss if $\\delta$ is too large. Note that the best initial value for $\\delta$ depends on the loss optimized in these experiments: for $\\delta=0.1$ , test error from the spring loss decreases much faster than for the other losses, yet we remind that the spring loss is just the logistic loss plus regularly spaced peaks. This could signal interesting avenues for the best possible implementation of SECBOOST, or a further understanding of the best formal ways to fix those paramaters, all of which are out of the scope of this paper. ", "page_idx": 28}, {"type": "image", "img_path": "MLgFu6dQYc/tmp/29aa898f08cbb7fc812b78124bb79839e37abc1de7bb809d19d7dd13330a6f04.jpg", "img_caption": ["Figure 12: Experiments on UCI tictactoe showing estimated test errors after minimizing each of the three losses we consider, with varying training noise level $\\eta$ , max tree size and initial hyperparameter $\\delta$ value in (60). See text. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our paper is a theory paper: all claims are properly formalized and used. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: The discussion section is devoted to limitations and improvement of our results Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 30}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Our paper is a theory paper: all assumptions, statements and proofs provided. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: Though our paper is a theory paper, we have included in the supplement a detailed statement of all related algorithms and a toy experiment of a simple implementation of these algorithms showcasing a simple run on a public UCI domain. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [No] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our paper is a theory paper. All algorithms we introduce are either in the main file or the appendix. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our paper is a theory paper. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our paper is a theory paper. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 32}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our paper is a theory paper. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: The research of the paper follows the code of ethics. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: Our paper is a theory paper. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: No release of data or models. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: no outside code, data or models used requiring licensing. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 34}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: No new assets provided. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: No crowdsourcing or research with human subjects. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: No research with human subjects. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 35}]