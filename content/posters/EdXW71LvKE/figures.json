[{"figure_path": "EdXW71LvKE/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison of temporal fusion methods: (a) Previous methods concatenate BEV feature maps without considering object motion. (b) CRT-Fusion estimates and compensates for object motion before concatenation. (c) Performance gain of CRT-Fusion over the direct concatenation method, showing CRT-Fusion's superior accuracy across different object velocity ranges.", "description": "This figure compares different approaches to temporal fusion in 3D object detection. (a) shows how previous methods simply concatenate BEV (bird's-eye view) feature maps from different time frames without accounting for object motion. (b) illustrates the CRT-Fusion approach, which estimates and compensates for object motion before concatenation, leading to a more accurate representation.  (c) presents a bar graph demonstrating the performance improvement of CRT-Fusion over the direct concatenation method, showing gains across various object velocity ranges.", "section": "1 Introduction"}, {"figure_path": "EdXW71LvKE/figures/figures_3_1.jpg", "caption": "Figure 2: Overall architecture of CRT-Fusion: Features are extracted from radar and camera data using backbone networks at each timestamp. The MVF module combines these features to generate fused BEV feature maps. The MFE module predicts the location and velocity of dynamic objects from these maps. The MGTF module then uses the predicted motion information to create the final feature map for the current timestamp, which is fed into the 3D detection head.", "description": "This figure illustrates the overall architecture of the CRT-Fusion framework for 3D object detection. It shows the flow of data from the input (camera and radar data at multiple timestamps) through different modules: Multi-View Fusion (MVF), Motion Feature Estimation (MFE), and Motion Guided Temporal Fusion (MGTF), finally leading to the 3D detection head that outputs the 3D object detection results. The MVF module fuses camera and radar features to create a unified BEV representation.  The MFE module predicts object locations and velocities. The MGTF module aligns and fuses feature maps across multiple timestamps using the motion information from MFE.  The detailed workings of MVF and MFE are shown as sub-figures.", "section": "3 CRT-Fusion"}, {"figure_path": "EdXW71LvKE/figures/figures_4_1.jpg", "caption": "Figure 3: Core components of CRT-Fusion: (a) RCA module enhances image features with radar features for accurate depth prediction. (b) MGTF module compensates for object motion across multiple frames, producing the final BEV feature map for 3D object detection.", "description": "This figure illustrates the core components of the CRT-Fusion framework. (a) shows the Radar-Camera Azimuth Attention (RCA) module, which enhances camera features with radar features to improve depth prediction accuracy.  (b) shows the Motion Guided Temporal Fusion (MGTF) module, which compensates for object motion across multiple frames to generate a robust and temporally consistent bird's-eye view (BEV) feature map. This refined BEV feature map is then used for 3D object detection.", "section": "3 CRT-Fusion"}, {"figure_path": "EdXW71LvKE/figures/figures_9_1.jpg", "caption": "Figure 4: Qualitative results comparing CRT-Fusion and CRN: Green boxes indicate CRT-Fusion prediction boxes, blue boxes denote CRN prediction boxes, and red boxes represent ground truth (GT) boxes.", "description": "This figure presents a qualitative comparison of the 3D object detection results obtained using CRT-Fusion and CRN on the nuScenes validation set.  Each image shows a BEV (bird's-eye view) representation of a scene.  Green boxes represent the bounding boxes predicted by CRT-Fusion, blue boxes show the bounding boxes predicted by CRN, and red boxes represent the ground truth annotations. The figure demonstrates that CRT-Fusion often produces more accurate and robust bounding boxes, particularly in challenging scenarios.", "section": "Qualitative results"}, {"figure_path": "EdXW71LvKE/figures/figures_14_1.jpg", "caption": "Figure 5: Comparison of velocity prediction using the MFE module in BEVDepth and CRT-Fusion. Red boxes are the Ground Truth (GT) boxes, red arrows show GT velocity, and white arrows represent predicted velocity. Yellow highlights indicate areas where CRT-Fusion predicts velocity more accurately, while orange highlights show static objects correctly identified by CRT-Fusion but misclassified by BEVDepth.", "description": "This figure compares velocity prediction results between BEVDepth and CRT-Fusion using the Motion Feature Estimation (MFE) module.  Ground truth boxes and velocities are shown in red.  CRT-Fusion's predictions are shown in white, with yellow highlighting areas of improved accuracy and orange highlighting areas where CRT-Fusion correctly identifies static objects that BEVDepth misclassifies as dynamic.", "section": "C.1 Analysis of the Motion Feature Estimation Module"}, {"figure_path": "EdXW71LvKE/figures/figures_17_1.jpg", "caption": "Figure 6: Qualitative results under different scenarios on the nuScenes validation set. Red boxes represent ground truth annotations, while blue and green boxes indicate the predicted bounding boxes from BEVDepth and CRT-Fusion, respectively. The white points represent the radar point cloud.", "description": "This figure compares the qualitative results of object detection between BEVDepth (baseline) and CRT-Fusion under various scenarios from the nuScenes validation set.  For each scenario, three images are shown, from left to right: the BEVDepth prediction, the CRT-Fusion prediction, and the ground truth.  Red boxes represent the ground truth bounding boxes, blue boxes indicate BEVDepth's predictions, and green boxes show CRT-Fusion's predictions. White points represent the radar point cloud data, which is used in the fusion process.", "section": "D Qualitative results of CRT-Fusion"}]