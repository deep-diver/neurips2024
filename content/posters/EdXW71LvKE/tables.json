[{"figure_path": "EdXW71LvKE/tables/tables_6_1.jpg", "caption": "Table 1: Performance comparisons with 3D object detector on the nuScenes val set. 'L', 'C', and 'R' represent LiDAR, camera, and radar, respectively. \u2020: trained with CBGS. \u2021: use TTA.", "description": "This table presents a comparison of the performance of different 3D object detection methods on the nuScenes validation set.  The methods are categorized by the sensor types they use (LiDAR only, camera only, camera-radar fusion).  Key performance metrics are reported, including NDS (NuScenes Detection Score), mAP (mean Average Precision), and various metrics related to accuracy of bounding box estimations (mATE, mASE, MAOE, mAVE, mAAE).  The table also notes whether certain methods were trained using CBGS (Class-Balanced Grouping and Sampling) or TTA (Test Time Augmentation).", "section": "4.2 Comparison to the state of the art"}, {"figure_path": "EdXW71LvKE/tables/tables_7_1.jpg", "caption": "Table 1: Performance comparisons with 3D object detector on the nuScenes val set. 'L', 'C', and 'R' represent LiDAR, camera, and radar, respectively. \u2020: trained with CBGS. \u2021: use TTA.", "description": "This table compares the performance of different methods for 3D object detection on the nuScenes validation set.  The methods use various combinations of LiDAR, camera, and radar data.  The table shows several metrics including NDS (NuScenes Detection Score), mAP (mean Average Precision), and other metrics related to the accuracy of bounding box estimation.  It also notes which methods were trained with CBGS (Class-Balanced Grouping and Sampling) and used TTA (Test Time Augmentation).", "section": "4.2 Comparison to the state of the art"}, {"figure_path": "EdXW71LvKE/tables/tables_8_1.jpg", "caption": "Table 3: Ablation study of the main components of CRT-Fusion.", "description": "This table presents the ablation study results of the CRT-Fusion model. It shows the impact of adding key components sequentially to a baseline model. The baseline is BEVDepth [17].  The components added are BEV fusion, RCA (Radar-Camera Azimuth Attention), and finally MFE (Motion Feature Estimator) & MGTF (Motion Guided Temporal Fusion). The table displays the performance metrics (NDS, mAP, mATE, mAOE) for each configuration, demonstrating the contribution of each component to the overall performance improvement.", "section": "4.3 Ablation studies"}, {"figure_path": "EdXW71LvKE/tables/tables_8_2.jpg", "caption": "Table 4: Performance comparison under different weather and lighting conditions.", "description": "This table compares the performance of several 3D object detection methods under various weather and lighting conditions.  The methods compared include BEVDepth, RCBEV, RCM-Fusion, CRN, and CRT-Fusion. The input sensor modalities are specified (C for camera only, C+R for camera and radar fusion). The mAP (mean Average Precision) results are shown for four different weather conditions: Sunny, Rainy, Day, and Night.  The table highlights how the performance of the methods varies depending on these conditions.", "section": "4.2 Comparison to the state of the art"}, {"figure_path": "EdXW71LvKE/tables/tables_8_3.jpg", "caption": "Table 3: Ablation study of the main components of CRT-Fusion.", "description": "This table presents the ablation study results of the CRT-Fusion model. It shows the impact of adding each key component (BEV fusion, RCA, MFE & MGTF) on the model's performance (NDS, mAP, mATE, MAOE).  The baseline is BEVDepth, and each row adds one component to the previous model to evaluate their contributions to improved accuracy.  'X' indicates the component is included, while 'O' indicates it is excluded.", "section": "4.3 Ablation studies"}, {"figure_path": "EdXW71LvKE/tables/tables_8_4.jpg", "caption": "Table 6: Comparison of radar-based view transformation methods. RGVT: Radar-Guided View Transformer. RVT: Radar-assisted View Transformation.", "description": "This table compares three different radar-based view transformation methods: RGVT, RVT, and RCA.  The methods are evaluated based on their performance in 3D object detection using the nuScenes dataset.  The metrics used for evaluation are NDS, mAP, mATE, mAOE, and mAVE.  RCA, the method proposed by the authors, outperforms the existing methods in terms of NDS, mAOE, and mAVE.", "section": "4.2 Comparison to the state of the art"}, {"figure_path": "EdXW71LvKE/tables/tables_14_1.jpg", "caption": "Table 1: Performance comparisons with 3D object detector on the nuScenes val set. 'L', 'C', and 'R' represent LiDAR, camera, and radar, respectively. \u2020: trained with CBGS. \u2021: use TTA.", "description": "This table compares the performance of different 3D object detection methods on the nuScenes validation dataset.  The methods use various sensor combinations (LiDAR only, camera only, camera and radar). The table shows the performance metrics: NDS (NuScenes Detection Score), mAP (mean Average Precision), and several other metrics related to the accuracy of 3D bounding box estimation.  It also indicates which methods used certain training techniques (CBGS and TTA).", "section": "4.2 Comparison to the state of the art"}, {"figure_path": "EdXW71LvKE/tables/tables_15_1.jpg", "caption": "Table 1: Performance comparisons with 3D object detector on the nuScenes val set. 'L', 'C', and 'R' represent LiDAR, camera, and radar, respectively. \u2020: trained with CBGS. \u2021: use TTA.", "description": "This table compares the performance of various 3D object detection methods on the nuScenes validation set.  It shows the NDS (NuScenes Detection Score), mAP (mean Average Precision), and other metrics (mATE, MASE, MAOE, MAVE, MAAE, FPS) for each method.  Different sensor combinations (LiDAR only, camera only, camera-radar fusion) and backbone networks are used.  The table also notes if a method used CBGS (class-balanced grouping and sampling) or TTA (test-time augmentation).  The metrics evaluate the accuracy of the 3D bounding boxes predicted by each method for various object classes.", "section": "4.2 Comparison to the state of the art"}, {"figure_path": "EdXW71LvKE/tables/tables_15_2.jpg", "caption": "Table 1: Performance comparisons with 3D object detector on the nuScenes val set. \u2018L\u2019, \u2018C\u2019, and \u2018R\u2019 represent LiDAR, camera, and radar, respectively. \u2020: trained with CBGS. \u2021: use TTA.", "description": "This table presents a comparison of the performance of different 3D object detection methods on the nuScenes validation set.  The methods use various sensor combinations (LiDAR only, camera only, camera and radar).  The table shows several metrics such as NDS (NuScenes Detection Score), mAP (mean Average Precision), and others to assess the accuracy of each method.  The table also indicates which methods were trained with CBGS (class-balanced grouping and sampling) and those that used TTA (test-time augmentation).", "section": "4.2 Comparison to the state of the art"}, {"figure_path": "EdXW71LvKE/tables/tables_16_1.jpg", "caption": "Table 1: Performance comparisons with 3D object detector on the nuScenes val set. 'L', 'C', and 'R' represent LiDAR, camera, and radar, respectively. \u2020: trained with CBGS. \u2021: use TTA.", "description": "This table compares the performance of different methods for 3D object detection on the nuScenes validation dataset.  The methods use various sensor combinations (LiDAR only, camera only, camera-radar fusion).  The table shows several metrics including NDS (nuScenes Detection Score), mAP (mean Average Precision), and several other metrics related to accuracy and precision.  It also notes the backbone network used and the input image size.  Abbreviations such as CBGS (Class Balanced Grouping and Sampling) and TTA (Test Time Augmentation) are explained in the paper.", "section": "4.2 Comparison to the state of the art"}, {"figure_path": "EdXW71LvKE/tables/tables_16_2.jpg", "caption": "Table 11: Ablation study of Inference Time.", "description": "This table presents a breakdown of the inference time for different components of the CRT-Fusion model.  It compares the original CRT-Fusion model with a lightweight version (CRT-Fusion-light).  The components are Camera Backbone (C.B.), Radar Backbone (R.B.), Multi-View Fusion (MVF), Motion Feature Estimation (MFE), Motion Guided Temporal Fusion (MGTF), and the detection Head.  The total inference time is also given for each model. The lightweight version shows significantly faster inference times, primarily due to improvements in the Radar Backbone and MGTF modules.", "section": "4.3 Ablation studies"}]