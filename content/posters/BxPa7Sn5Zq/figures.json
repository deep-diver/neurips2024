[{"figure_path": "BxPa7Sn5Zq/figures/figures_1_1.jpg", "caption": "Figure 1: We present a novel interaction-aware Gaussian splatting framework that creates animatable interacting hand avatars from a single image. These high-fidelity avatars support various applications, such as editing, animation, combination, duplication, re-scaling, and text-to-avatar conversion.", "description": "This figure demonstrates the capabilities of the proposed interaction-aware Gaussian splatting framework.  It showcases the creation of animatable interacting hand avatars from a single input image.  The top row illustrates the process: a single input image is used to generate a 3D animatable avatar, which can then be rendered in novel poses and views. The bottom row provides examples of various applications enabled by this method, including text-to-hand avatar generation (where text prompts generate corresponding hand avatars), the ability to create single or interacting hands, skin tone editing, hand combination, and size editing.", "section": "1 Introduction"}, {"figure_path": "BxPa7Sn5Zq/figures/figures_1_2.jpg", "caption": "Figure 2: Paradigm comparison between existing one-shot hand avatar methods (a-c) and the proposed method (d). By decoupling the learning and fitting stages, our method leverages the advantages of learning-based methods (a, b) in modeling cross-subject hand priors, and the advantages of inversion-based methods (c) in one-shot fitting without the extra cost of network fine-tuning.", "description": "This figure compares the proposed method with existing one-shot hand avatar methods. The existing methods are categorized into three types: (a) methods using conditional generators, (b) methods using image encoders and differentiable renderers, and (c) inversion-based methods. The proposed method combines the advantages of these methods by decoupling the learning and fitting stages. This allows for leveraging cross-subject hand priors while maintaining the efficiency of one-shot fitting.", "section": "1 Introduction"}, {"figure_path": "BxPa7Sn5Zq/figures/figures_3_1.jpg", "caption": "Figure 3: The architecture of the proposed interaction-aware Gaussian splatting network, of which the core components are the disentangled hand representation, the interaction detection module, the interaction-aware attention module, and the Gaussian refinement module labeled in green.", "description": "This figure presents a detailed architecture of the Interaction-Aware Gaussian Splatting Network.  The network takes a training image as input and processes it through several modules to generate a rendered image of interacting hands. Key modules include disentangled hand representation (separating identity maps, geometric features, and neural texture maps), interaction detection, interaction-aware attention, and a Gaussian refinement module. The interaction-aware attention module focuses on enhancing image rendering in areas with interactions, while the Gaussian refinement module optimizes the number and position of Gaussians for improved rendering quality.  The figure highlights the flow of information through texture and geometry decoders and encoders.  The final output is a rendered image of interacting hands.", "section": "3 Methodology"}, {"figure_path": "BxPa7Sn5Zq/figures/figures_7_1.jpg", "caption": "Figure 4: Qualitative comparisons with state-of-the-art methods. The input image is shown in the top-left grid labeled in red. The first row presents results without changing the pose from the input view (left) and an alternative view (right), while results in the remaining rows are with novel poses.", "description": "This figure presents a qualitative comparison of the proposed method against several state-of-the-art methods for novel view and novel pose synthesis of interacting hands.  The input image is displayed in the top-left corner. The first row shows results using the same pose as the input image, while subsequent rows show results with different poses. This visualization helps to assess the ability of each method to accurately reconstruct the geometry, texture, and interactions of the hands in various poses and viewpoints.", "section": "4 Experiments"}, {"figure_path": "BxPa7Sn5Zq/figures/figures_7_2.jpg", "caption": "Figure 5: Visual examples of the ablation study on the proposed components in the hand-prior learning stage (top) and the one-shot fitting stage (bottom).", "description": "This figure shows the results of ablation studies performed on the proposed method. The top row shows the ablation on the hand-prior learning stage, demonstrating the effects of removing the interaction-aware attention module (IAttn), the Gaussian refinement module (GRM), and the identity map (IMap). The bottom row shows the ablation on the one-shot fitting stage, comparing the proposed method with different combinations of the components in the hand-prior learning stage, and to the OHTA* baseline.", "section": "4.3 Ablation Study"}, {"figure_path": "BxPa7Sn5Zq/figures/figures_8_1.jpg", "caption": "Figure 4: Qualitative comparisons with state-of-the-art methods. The input image is shown in the top-left grid labeled in red. The first row presents results without changing the pose from the input view (left) and an alternative view (right), while results in the remaining rows are with novel poses.", "description": "This figure compares the results of the proposed method against several state-of-the-art techniques in terms of novel view and novel pose synthesis. The input image is displayed in the top-left corner.  The first row shows results using the same pose as the input image (left) and then a novel viewpoint (right). Subsequent rows show results with novel poses, demonstrating the method's ability to generate high-quality results under different pose and view conditions.", "section": "4 Experiments"}, {"figure_path": "BxPa7Sn5Zq/figures/figures_12_1.jpg", "caption": "Figure 7: Visual examples of the proposed method in various scenarios, including text-to-avatar, in-the-wild reconstruction from real images, and texture editing.", "description": "This figure shows examples of the results obtained by applying the proposed method to various tasks. The first row demonstrates the text-to-avatar capability of the method, where different hand avatars are generated based on textual descriptions such as \"hand, spider man\", \"hand, blue\", etc. The second row shows the in-the-wild performance of the method, where avatars are reconstructed from real images captured in various settings. The third row illustrates the flexibility and versatility of the method by showcasing texture editing capabilities, where the color and appearance of the hands are altered.", "section": "A Appendix"}, {"figure_path": "BxPa7Sn5Zq/figures/figures_13_1.jpg", "caption": "Figure 8: Qualitative comparisons between Gaussians by mesh upsampling (denoted as MeshUp) and the proposed GRM.", "description": "This figure compares the results of using mesh upsampling and the Gaussian refinement module (GRM) to generate Gaussian points for hand representation. The top row shows the input images with hand meshes, followed by results from MeshUp and GRM. The bottom row shows close-up views of the hand regions, highlighting the differences in detail and texture between the two methods. The ground truth (GT) images are shown for reference.", "section": "A.3 Visualization of GRM"}, {"figure_path": "BxPa7Sn5Zq/figures/figures_13_2.jpg", "caption": "Figure 9: Visual examples of shadow disentanglement (top) and four ablation studies (bottom).", "description": "This figure shows the results of shadow disentanglement and ablation studies. The top row demonstrates visual examples of shadow disentanglement, where for each pair of hands, the corresponding shaded image, albedo image, and shadow image are shown. The bottom four rows illustrate the ablation study results.  The ablation study on single-hand images examines performance differences when using only images of single hands instead of both hands, showing two-hand images provide more complementary information. The ablation study on the segmentation method compares the results of using SAM-predicted masks versus ground-truth meshes. The ablation study S1 examines several variants: the model without camera parameters, a model with shadow coefficients, and a model with a reduced number of Gaussian points. Finally, the ablation study on mesh quality investigates the impact of noisy mesh input on the model's performance.", "section": "A.5 More Ablations"}, {"figure_path": "BxPa7Sn5Zq/figures/figures_14_1.jpg", "caption": "Figure 4: Qualitative comparisons with state-of-the-art methods. The input image is shown in the top-left grid labeled in red. The first row presents results without changing the pose from the input view (left) and an alternative view (right), while results in the remaining rows are with novel poses.", "description": "This figure compares the results of the proposed method against several state-of-the-art methods for novel view and pose synthesis of interacting hands from a single image.  The first row shows results using the same pose and view as the input image, showcasing the ability to reconstruct high-fidelity images. Subsequent rows show results generated with novel poses, demonstrating the method's capability for animation.  The ground truth (GT) images are also provided for direct comparison.", "section": "4.2 Comparison with State-of-the-art Methods"}]