[{"type": "text", "text": "Kangaroo: Lossless Self-Speculative Decoding for Accelerating LLMs via Double Early Exiting ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Fangcheng Liu\u2020 Yehui Tang\u2020 Zhenhua Liu\u2020 Yunsheng Ni\u2020 Duyu Tang\u2021 Kai Han\u2020,\u2217 Yunhe Wang\u2020,\u2217 ", "page_idx": 0}, {"type": "text", "text": "\u2020Huawei Noah\u2019s Ark Lab \u2021Consumer Business Group, Huawei {liufangcheng3,yehui.tang,kai.han,yunhe.wang}@huawei.com ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Speculative decoding has demonstrated its effectiveness in accelerating the inference of large language models (LLMs) while maintaining an identical sampling distribution. However, the conventional approach of training separate draft model to achieve a satisfactory token acceptance rate can be costly and impractical. In this paper, we propose a novel self-speculative decoding framework Kangaroo with double early exiting strategy, which leverages the shallow sub-network and the LM Head of the well-trained target LLM to construct a self-drafting model. Then, the self-verification stage only requires computing the remaining layers over the early-exited hidden states in parallel. To bridge the representation gap between the sub-network and the full model, we train a lightweight and efficient adapter module on top of the sub-network. One significant challenge that comes with the proposed method is that the inference latency of the self-draft model may no longer be negligible compared to the big model. To boost the token acceptance rate while minimizing the latency of the self-drafting model, we introduce an additional early exiting mechanism for both single-sequence and the tree decoding scenarios. Specifically, we dynamically halt the small model\u2019s subsequent prediction during the drafting phase once the confidence level for the current step falls below a certain threshold. This approach reduces unnecessary computations and improves overall efficiency. Extensive experiments on multiple benchmarks demonstrate our effectiveness, where Kangaroo achieves walltime speedups up to $2.04\\times$ , outperforming Medusa-1 with $88.7\\%$ fewer additional parameters. The code for Kangaroo is available at https://github.com/Equationliu/Kangaroo. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large Language Models (LLMs) [1, 2, 3, 4, 5, 6] have demonstrated remarkable performance across various natural language processing tasks, such as chain-of-thought reasoning [7] and agents [8]. Beyond model performance, recent research has also focused on inference efficiency [9, 10], particularly in scenarios involving the deployment of LLMs for service applications. However, constrained by the bottleneck of memory bandwidth [11], the primary latency for autoregressive decoding of LLMs arises mainly from memory read/write operations rather than arithmetic computations, leading to inadequate parallelism. For instance, decoding Vicuna-33B [12] on four NVIDIA V100 GPUs yields a throughput of only seven new tokens per second. ", "page_idx": 0}, {"type": "text", "text": "To address this challenge, Speculative Decoding (SD) techniques [13, 14] have been developed, aiming to accelerate autoregressive decoding by verifying multiple tokens generated by a draft model ", "page_idx": 0}, {"type": "image", "img_path": "lT3oc04mDp/tmp/ea7484deae3cc12b1647fa6dffb2febe0e9bc0c6551cb0eeecfa831471811040.jpg", "img_caption": ["(a) Token acceptance rate on the mathematical reasoning subtask. Token position \u201c2\" denotes the task to predict the next-next-token. "], "img_footnote": [], "page_idx": 1}, {"type": "image", "img_path": "lT3oc04mDp/tmp/e3c9442af7fa35f9e6fdb93c6bbb0250b65e28a9d5a734121036fb0fb1345596.jpg", "img_caption": ["(b) End-to-end speedup ratio on four subtasks in SpecBench. \u201cMath\" and \u201cRAG\" denote mathematical reasoning and retrieval-augmented generation, respectively. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Comparison of various self-drafting speculative decoding methods without tree mask on Spec-Bench [22] for Vicuna-7B [12]. Kangaroo outperforms all other methods w.r.t. end-to-end speedup ratio across all the four subtasks. Specifically, Kangaroo (without tree) achieves speedups of $1.68\\times$ on MT-bench [23], outperforming Medusa with $88.7\\%$ fewer additional parameters. ", "page_idx": 1}, {"type": "text", "text": "in parallel. Given $\\gamma$ draft tokens, SD can generate 1 to $\\gamma+1$ new tokens within each forward pass of the big LLM. Most of existing SD methods typically train a tiny draft model from scratch on a large corpus to accelerate LLMs from the same series, e.g., LLaMA-68M [15] for LLaMA-7B [2]. Distillspec [16] uses knowledge distillation to better align the draft model with the target model. However, the training of such task-specific models can be costly [16, 17], limiting its application in real-world scenarios. ", "page_idx": 1}, {"type": "text", "text": "To mitigate these costs, several studies have proposed self-drafting methods that do not rely on external drafter models. LLMA [18] and REST [19] generate draft tokens by selecting text spans from references or retrieving relevant tokens from a database. Notably, Medusa [20] trains multiple time-independent Feed-Forward neural Network (FFN) heads on top of the last decoder layer. Although Medusa and REST could generate draft tokens efficiently, however, the token acceptance rate is not always satisfactory (see Figure 1(a)). On the other hand, focusing exclusively on the token acceptance rate without considering the latency of generating draft tokens can lead to suboptimal speedup ratio. For instance, as shown in Figure 1, Lookahead [21] achieves a significantly higher token acceptance rate than Medusa in the mathematical reasoning subtask. However, due to its lower efficiency in the drafting phase when compared to Medusa, its end-to-end speedup ratio is slightly lower than that of Medusa (see Figure 1(b)). ", "page_idx": 1}, {"type": "text", "text": "In this paper, we propose a novel self-speculative decoding framework based on a double early-exiting mechanism. Rather than training the draft model from scratch, we enhance efficiency by inheriting part of the target model\u2019s knowledge through parameter sharing. Specifically, Kangaroo utilizes both the shallow sub-network and the LM Head of the target LLM to construct a self-drafting model. However, there is a natural discrepancy in the representation capabilities between the shallow network and the final layer of the model. To bridge this gap, we incorporate a lightweight and efficient adapter module trained on top of the sub-network. Kangaroo\u2019s adapter network is streamlined yet effective, comprising only one multi-head attention layer [24] and two normalization layers [25], utilizing just $11.3\\%$ of the parameters used by Medusa\u2019s heads. This layer-level early-exiting strategy significantly reduces both the training and deployment costs typically associated with traditional self-speculative decoding methods that necessitate separate draft model. ", "page_idx": 1}, {"type": "text", "text": "Moreover, to achieve an optimal balance between token acceptance rate and drafting efficiency, Kangaroo introduces an additional token-level dynamic drafting strategy via early exiting. Unlike existing methods that rely on a fixed drafting step and a predefined static token tree, our approach dynamically adjusts the depth and width of the token tree based on the conditional probability distribution of the self-drafting model. In the case of single-sequence decoding, which is a specific scenario within the token tree, the drafting phase is immediately halted if the top-1 probability of the current sample token (in the self-drafting model) falls below a predefined threshold. Extensive experiments conducted on the Spec-Bench demonstrate the effectiveness of our approach. Kangaroo achieves speedups of up to $2.04\\times$ , significantly outperforming Medusa-1 while using $88.7\\%$ fewer additional parameters (67M compared to 591M.). These results highlight the efficiency and potential of Kangaroo in improving decoding processes without compromising performance. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Inference Acceleration of Large Language Models With the rapid development of large language models, significant research effort has been dedicated to accelerating their inference speed [26]. Techniques such as knowledge distillation [27], model compression [28], and quantization [29] have also been widely applied in this area. However, these approaches often require additional training of the backbone or substantial modifications to the model architecture. FlashAttention [9] and vLLM [10] frameworks accelerate the inference speed of large language models by optimizing memory management. Recent efforts have explored early exiting on models like the T5 series [30, 31, 32] and decoder-only architectures [33]. However, since early exiting accelerates inference by saving subsequent computations, it inevitably incurs the issue of performance degradation [30]. ", "page_idx": 2}, {"type": "text", "text": "Speculative Decoding Speculative decoding has gained significant attention due to its ability to accelerate the inference of LLMs while maintaining the same sampling distribution. Generally, speculative decoding [13, 14] involves finding or training [16, 34] a small draft model closely aligned with the target LLM. Consequently, recent research has focused on more convenient self-drafting methods. For instance, approaches like blockwise parallel decoding [35] and Medusa [20] expedite the generation of draft tokens by training multiple time-independent Feedforward Neural Networks at the second top layer. Several self-drafting acceleration techniques are inspired by early exiting. Draft & Verify [36], for example, generates draft tokens by skipping intermediate redundant layers of the target LLM. While this approach could achieve a high token acceptance rate, the inference latency of the \u201csmall model\u201d is exceptionally high, which can hinder end-to-end acceleration efficiency. SPEED [37] adapts early exiting to pipelined speculative execution for transformer decoders that employ parameter sharing. There are also several works [38, 39, 40] that make improvement on Medusa by introducing time dependency among the draft tokens. Unlike our approach, which utilizes early exiting from shallow layers, these methods typically extrapolate directly from the features of the penultimate layer. For more related works on speculative decoding, we refer readers to a recent survey [22] for a detailed summarization. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we introduce background and the formulations of standard speculative decoding under greedy decoding. We use $\\bar{x^{t}}$ to denote the discrete token sequence $(x_{1},\\cdot\\cdot\\cdot\\,,x_{t})$ and $x^{i:\\overline{{j}}}$ to represent sequence $(x_{i},\\cdot\\cdot\\cdot,x_{j})$ . Let $\\mathcal{X}$ be a discrete space over all possible tokens $x$ in the LLM\u2019s vocabulary, we model the autoregressive process of a language model $\\mathcal{M}$ by the conditional distributions $\\mathcal{M}(\\dot{\\cdot}\\mid x^{t})\\in\\mathbb{R}^{|\\mathcal{X}|}$ where $|{\\mathcal{X}}|$ is the vocabulary size. We denote the big target language model and the speculative small model as $\\mathcal{M}_{b}$ and $\\mathcal{M}_{s}$ , respectively. ", "page_idx": 2}, {"type": "text", "text": "Single-Sequence Decoding Due to the memory bandwidth [11] limitations of autoregressive decoding in large language models, the latency to generate one new token is approximately the same as the time required to parallelly infer $\\gamma$ tokens. Based on this characteristic, speculative decoding [13, 14] leverages a low-cost small model to quickly generate $\\gamma$ candidates ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{x}_{t+i}=\\arg\\operatorname*{max}_{x\\in\\mathcal{X}}\\;\\mathcal{M}_{s}(x\\mid\\hat{x}^{t+i-1}),\\quad i=1,2,\\cdots\\,,\\gamma,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\hat{x}^{t}\\ \\ =\\ \\ x^{t}$ . Subsequently, the big model $\\mathcal{M}_{b}$ only needs one forward pass over $(\\hat{x}_{t},\\hat{x}_{t+1},\\cdot\\cdot\\cdot\\,,\\hat{x}_{t+\\gamma})$ to generate $\\gamma+1$ tokens $(x_{t+1},\\cdot\\cdot\\cdot\\,,x_{t+\\gamma+1})$ . The accept length over this single-sequence verification is ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{i=0,\\cdots,\\gamma}\\;\\{i\\mid\\hat{x}_{t+i}=x_{t+i}\\}+1,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "ranging from 1 to $\\gamma+1$ . ", "page_idx": 2}, {"type": "image", "img_path": "lT3oc04mDp/tmp/6b20349c569b028cbd2df43825d6c4f64cacdc30f31752bddf306f2928f34b44.jpg", "img_caption": ["Figure 2: The framework of Kangaroo under single-sequence verification. The adapter network $\\boldsymbol{\\mathcal{A}}$ consists of only one multi-head attention and two normalization layers. The self-drafting model $\\begin{array}{r}{\\mathcal{M}_{s}=\\mathcal{A}\\circ\\mathcal{M}_{b}[:\\,l]}\\end{array}$ will reuse the LM Head of the target LLM $\\mathcal{M}_{b}$ for better alignment, where $l$ denotes the early exit layer. To avoid unnecessary costs on more difficult tokens, $\\mathcal{M}_{s}$ stops drafting once the top-1 probability of the current sampled token falls below a certain threshold, e.g., $\\mathcal{M}_{s}(\\hat{x}_{3}\\overset{\\sim}{\\vert}\\hat{x}^{0:2})\\leq\\eta$ . Note that we will concatenate the stopped token\u2019s next early feature $f_{3}$ with all previous exited features into a parallel compute unit $[f_{0},f_{1},\\cdot\\cdot\\cdot\\,,f_{3}]$ , which will be verified by the remaining layers $\\mathcal{M}_{b}[l:]$ in parallel. Once all drafted tokens are accepted $\\mathbf{\\hat{x}}_{i}=x_{i}$ for $i=1,2,3)$ ), we could start the next round with $x_{4}$ rather than $x_{3}$ if we have not calculated $f_{3}$ in advance. The decoding on parallel unit $[f_{3},f_{4}]$ could save the latency for a single forward pass of $\\boldsymbol{\\mathcal{A}}$ . "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "4 Kangaroo ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we present Kangaroo, a novel self-speculative decoding framework based on a double early-exiting mechanism. We leverage the shallow sub-network and the LM Head of the target LLM to construct a self-drafting model. Subsequently, the self-verification stage only requires computing the remaining layers over the early-exited hidden states in parallel. ", "page_idx": 3}, {"type": "text", "text": "4.1 Early Exiting as Self-Drafting Model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Training an additional small model from scratch is often costly and impractical, thus it is worth considering sharing a portion of the parameters from the target LLM. Drawing inspiration from early exiting, we directly extract hidden states from a fixed shallow sub-network of the target LLM ", "page_idx": 3}, {"type": "equation", "text": "$$\nf_{t}=\\mathcal{M}_{b}[:l](x_{t}),\\quad l\\in\\{1,2,\\cdots,L\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathcal{M}_{b}[:\\,l]$ denotes the first $l$ layers from the target model $\\mathcal{M}_{b}$ . Note that there is a natural representation gap between the shallow sub-network and the full model. Therefore, we train a lightweight and efficient adapter $\\boldsymbol{\\mathcal{A}}$ to bridge this gap. As shown in Figure 2, the architecture of the adapter $\\boldsymbol{\\mathcal{A}}$ consists of only one multi-head attention and two normalization layers. Given an exited hidden states $f_{t}$ , the forward pass of the adapter model $\\boldsymbol{\\mathcal{A}}$ can be represented as ", "page_idx": 3}, {"type": "equation", "text": "$$\nf_{t}^{\\prime}=f_{t}+\\tt M u1t\\tt\\dot{z}\\tt H e a d\\left(\\tt L a y e r N o r m(\\it f_{t})\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where we keep the residual connection and the multi-head attention module but remove the FFN in a standard transformer block. Besides the shallow sub-network, Kangaroo also reuses the LM Head of the target model to get the final conditional distribution, i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{M}_{s}(\\boldsymbol{x}\\mid\\boldsymbol{x}^{t})=\\mathsf{S o f t m a x}\\left(\\mathbf{W}^{\\top}\\mathtt{L a y e r N o r m}(f_{t}^{\\prime})\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Title: \"Hawaian Adventures: Discovering the Islands' Rich Culture and Natural Wonders\"\\n\\nlntroduction onders 0.98 0.82 Nn 0.99 Nn 0.99 Description 0.16 ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Figure 3: The difficulty of token acceptance varies across different contexts. The values on the right side of the rectangular blocks represent the top-1 probability of the tokens on the self-drafting model $\\mathcal{M}_{s}$ . Green boxes indicate accepted tokens, while red boxes represent rejected tokens. Blue tokens signify corrections made by the big target model $\\mathcal{M}_{b}$ . ", "page_idx": 4}, {"type": "image", "img_path": "lT3oc04mDp/tmp/2fa61c2281cfd19bd9ad795c91f12b6be2f2782481894b3333c7c063599667f7.jpg", "img_caption": ["Figure 4: The density of top-1 conditional probability on the mathematical reasoning subtask. \u201cAccept\u201d denotes the top-1 confidence of accepted draft tokens while \u201cReject\u201d denotes the corresponding confidence of rejected tokens. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{W}\\in\\mathbb{R}^{N\\times|\\mathcal{X}|}$ is the frozen LM Head and $N$ denotes the dimension of hidden states. The total parameters of $\\boldsymbol{\\mathcal{A}}$ comes from the four projection matrix in the MultiHead layer and two vectors in the LayerNorm layers, i.e., $4N^{2}+\\dot{2}N^{'}\\!\\!=67.1\\mathsf{M}$ when $N=4096$ . ", "page_idx": 4}, {"type": "text", "text": "Single-Sequence Self-Speculative Decoding In the drafting phase, the self-drafting model $\\mathcal{M}_{s}$ generates $\\gamma$ new tokens autoregressively. Similar to the standard speculative decoding in Equation (1), we have $\\hat{x}_{t+i}=\\arg\\operatorname*{max}_{x\\in\\mathcal{X}}$ $\\mathcal{M}_{s}(x\\mid\\hat{x}^{t+i-1})$ and the exited hidden states ", "page_idx": 4}, {"type": "equation", "text": "$$\nf_{t+i}=\\mathcal{M}_{b}[:l](\\hat{x}_{t+i}),\\quad i=1,2,\\cdot\\cdot\\cdot\\,,\\gamma,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Subsequently, the remaining layers $\\mathbb{M}_{b}[l:]$ will in charge of verifying the concatenated early-exited hidden features in parallel, i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\ \\ \\ \\ \\ \\ \\ f^{t:t+\\gamma}\\overset{\\mathrm{def}}{=}\\mathrm{Concat}\\left(\\left[f_{t},f_{t+1},\\cdots,f_{t+\\gamma}\\right]\\right)\\in\\mathbb{R}^{N\\times(\\gamma+1)},}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\mathcal{M}_{b}(x^{t+1:t+\\gamma+1}\\mid x^{t})=\\mathrm{Sof}\\tan\\!\\mathrm{ax}\\left(\\mathbf{W}^{\\top}\\!\\mathrm{LayerNorm}\\left(\\mathcal{M}_{b}[l:\\!](f^{t:t+\\gamma})\\right)\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where W is the frozen LM Head. In the verification procedure, we keep the same as the conventional speculative decoding by comparing the top-1 candidates of $\\mathcal{M}_{b}(x^{t+1:t+\\gamma+1}\\mid x^{t})$ and $\\hat{x}_{t+i}$ for $i=1,2,\\cdots,\\gamma$ , following Equation (2). ", "page_idx": 4}, {"type": "text", "text": "Training of the Adapter $\\boldsymbol{\\mathcal{A}}$ To bridge the representation gap between the self-drafting model $\\mathcal{M}_{s}$ and the full model $\\mathcal{M}_{b}$ , we train the adapter $\\boldsymbol{\\mathcal{A}}$ with the conventional cross-entropy loss, i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{A}^{\\ast}=\\underset{\\mathcal{A}}{\\arg\\operatorname*{min}}\\;\\sum_{\\boldsymbol{t}}\\sum_{\\boldsymbol{x}\\in\\mathcal{X}}-\\mathcal{M}_{b}(\\boldsymbol{x}\\mid\\boldsymbol{x}^{t})\\log\\mathcal{M}_{s}(\\boldsymbol{x}\\mid\\boldsymbol{x}^{t}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the token positions $t$ is averaged over the whole training set and the condition probability distribution of the target LLM $\\mathcal{M}_{b}$ serves as a distillation for the self-drafting model $\\mathcal{M}_{s}$ \u2019s. ", "page_idx": 4}, {"type": "text", "text": "4.2 Dynamic Drafting Steps via Token-Level Early-Exiting ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Speculative decoding typically employs a fixed drafting step $\\gamma$ during the drafting phase, but this often leads to local optima. Fixed-step drafting can result in unnecessary time spent on more challenging samples or missed opportunities to speculate on simpler tokens. As shown in Figure 3, the difficulty of predicting the next token varies across different contextual scenarios, necessitating the design of a token-wise dynamic drafting strategy. ", "page_idx": 4}, {"type": "text", "text": "Single-Sequence Decoding During drafting phase, we do not have access to the full target model $\\mathcal{M}_{b}$ and must rely solely on the information from the self-drafting model $\\mathcal{M}_{s}$ , to determine whether to continue or terminate the drafting process. Fortunately, we observe a strong correlation between the small model\u2019s confidence level for the current sampled token and the likelihood that the token will be accepted by the big target model. In Figure 4, all conditional probability values of the tokens are divided into two groups. The peak values for tokens accepted by the large model skew to the right, while the conditional probability values for tokens rejected by the large model cluster at lower confidence levels. This indicates that the top-1 probability on the Kangaroo\u2019s small model is a reliable metric for determining when to stop drafting. Therefore, we stop drafting once the top-1 probability on the self-drafting model falls below a predefined threshold $\\eta$ , i.e., ", "page_idx": 4}, {"type": "image", "img_path": "lT3oc04mDp/tmp/c24a3534ef5f8fbe2827aeda5f98a834a4e17b9d58834d8beb5d40c1b3758eb4.jpg", "img_caption": ["Figure 5: Comparison of various speculative decoding methods on Spec-Bench [22] for Vicuna-7B, where Kangaroo outperforms other approaches in most subtasks, especially in mathematical reasoning and retrieval-augmented generation. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{x\\in\\mathcal{X}}\\;\\mathcal{M}_{s}(x\\mid x^{t})\\leq\\eta.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Extension to Tree Decoding Based on the single-sequence verification procedure, the big model $\\mathcal{M}_{b}$ can only verify one feasible path, composed of the top-1 candidates generated by the small model $\\mathcal{M}_{s}$ . Tree verification technology [15, 20] enhances parallelism and improves GPU utilization by designing a sparse token tree to simultaneously verify multiple feasible paths. Most current works that adopt token tree verification rely on handcrafted or pre-searched [41] static tree. We generalize the early stopping mechanism of Kangaroo in single-sequence decoding to tree decoding, where both the tree depth and node selection at each level of the tree are token-wise dynamic. ", "page_idx": 5}, {"type": "text", "text": "We define the first draft token from $\\mathcal{M}_{s}$ as the root2 node. To increase the token acceptance rate, we feed the Top- $\\cdot\\mathrm{K}^{3}$ most probable tokens into the draft model $\\mathcal{M}_{s}$ in parallel at each drafting step. The confidence score of a child node is computed as the product of its conditional probability in $\\mathcal{M}_{s}$ and its parent\u2019s conditional probability, Formally, ", "page_idx": 5}, {"type": "equation", "text": "$$\nc(x_{i})=\\prod_{x_{j}\\in\\mathrm{Path}(\\mathrm{root},x_{i})}c(x_{j})\\;,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\operatorname{Path}(\\operatorname{root},x_{i})$ denotes the path from root to node $x_{i}$ , $c(x_{i})$ is the probability confidence on $\\mathcal{M}_{s}$ and $c(\\mathrm{root})=1$ . At the first level, we begin with Top- $\\cdot\\mathrm{K}_{x\\in{\\mathcal{X}}}{\\mathcal{M}}_{s}(x\\mid\\mathrm{root})$ . For level $\\geq2$ , we consider the Top-K child nodes for each token in the previous level\u2019s Top-K tokens. In this way, the tokens with the Top-K largest confidence score will be selected from Top- $\\mathrm{K}\\times\\mathrm{Top-K}$ candidates. For the selected Top-K tokens, their parent nodes will be added into the token tree $\\tau$ directly. For tokens in the previous level\u2019s Top-K set without any child node selected, half of those with the lowest confidence scores are pruned from $\\tau$ . This process continues until the tree size $|\\tau|$ surpasses a predefined maximum or the highest confidence score at the current level falls below a threshold $\\eta$ . ", "page_idx": 5}, {"type": "table", "img_path": "lT3oc04mDp/tmp/67988411845aaa1434003b38fbcb88b2e8679620d0f7ac86e291dc4b1fb77651.jpg", "table_caption": ["Table 1: Speedup comparison of various speculative decoding methods on Spec-Bench [22] for Vicuna [12]. Speedup is the walltime speedup ratio and CR denotes the compression rate. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Models and Benchmarks We conduct experiments on Vicuna [12] models with size of 7B and 13B. We select several speculative decoding approaches for comparison including Lookahead [21], Medusa [20], REST [19] and SpS [13] using Vicuna-68M [17] as the draft model. For fair and comprehensive comparison, we evaluate the acceleration performance with the recently proposed Spec-Bench [22], which consists of six subtasks including Multi-turn Conversation, Translation, Summarization, Question Answering, Mathematical Reasoning and Retrieval-augmented Generation. ", "page_idx": 6}, {"type": "text", "text": "Evaluation Metrics Speculative decoding is often evaluated using two primary metrics: walltime speedup ratio and compression rate. Given a speculative decoding algorithm, we execute it to generate $T$ new tokens and record the accepted tokens per forward pass of the big model as a list $S=[s_{1},s_{2},\\cdot\\cdot\\cdot\\cdot,s_{|S|}]$ where $\\textstyle\\sum_{k}s_{k}=T$ . The Compression Rate (CR) is defined as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{CR}=\\frac{1}{\\vert S\\vert}\\sum_{k}s_{k},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "which does not accurately reflect the acceptance levels of the drafting algorithm for tokens at varying distances. Thus, we propose a new metric consistent token acceptance rate $\\mathrm{CTAR}(w)$ , given a prefix and a following window with size $w$ , is the probability that the $w$ guessed tokens from the draft model are all accepted by the target model: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{CTAR}(w)=\\frac{1}{\\lvert S\\rvert}\\sum_{k}\\mathbb{I}(s_{k}-w>0),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "which is a decreasing function $w.r t$ . the window size $w$ . We plot the empirical CTARs of several self-drafting speculative decoding algorithms on the mathematical reasoning subtask of Spec-Bench in Figure 1, where we observe a trade-off between the token acceptance rate and drafting efficiency. ", "page_idx": 6}, {"type": "text", "text": "Training and Inference As shown in Figure 2, the big target model is frozen and the trainable parameters are the multi-head attention and two normalization layers. For Kangaroo, we train the adapter $\\boldsymbol{\\mathcal{A}}$ for 10 epochs with the AdamW [42] optimizer on the ShareGPT dataset following Medusa [20]. The training of the adapter $\\boldsymbol{\\mathcal{A}}$ for Vicuna-7B takes around 24 hours on 8 NVIDIA V100 GPUs. During the inference stage, we set $\\ell=2$ for Vicuna-7B and $\\ell=3$ for Vicuna-13B. For the single-sequence decoding in Kangaroo, we set $\\gamma=6$ and $\\eta=0.6$ . For the dynamic tree decoding scenario, we set Top-K as 10, and $\\eta=0.4$ . ", "page_idx": 6}, {"type": "text", "text": "5.1 Effectiveness: Comparison with other Speculative Decoding Methods ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The Spec-Bench results for Vicuna families4 are available at Figure 5 and Table 1, where we can conclude that: ", "page_idx": 6}, {"type": "text", "text": "\u2022 For both 7B and 13B model sizes, Kangaroo achieves the highest average speedup across all tasks, with $1.72\\times$ and $1.65\\times$ respectively. Furthermore, at both model sizes, Kangaroo with dynamic tree verification demonstrates approximately a $12\\%$ improvement in speedup compared to its single-sequence decoding version, indicating its effectiveness. \u2022 In both single-sequence and tree verification scenarios, Kangaroo outperforms Medusa across most datasets, despite Kangaroo\u2019s additional parameters comprising only $11.3\\%$ of Medusa\u2019s heads. Kangaroo shows consistent high performance across different tasks such as QA, Math, and MT Bench, indicating its robustness and versatility. \u2022 Under single-sequence decoding, Kangaroo (67M) achieves comparable results to the SpS method using Vicuna-68M, which is specifically pre-trained from scratch. Furthermore, with tree verification, Kangaroo significantly outperforms SpS across most datasets. ", "page_idx": 7}, {"type": "text", "text": "5.2 Ablation Studies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To fully explore the sensitivity of different hyperparameters in the Kangaroo framework, we conducted comprehensive ablation experiments focusing on aspects such as the depth of shallow sub-Network $\\ell$ , the early stopping threshold $\\eta$ , and the architectural choices of the adapter network $\\boldsymbol{\\mathcal{A}}$ . In the settings of the ablation experiments described below, we focus on single-sequence decoding scenario. ", "page_idx": 7}, {"type": "text", "text": "The Depth of Shallow Sub-Network. The capacity of the self-drafting model $\\mathcal{M}_{s}$ highly depends on the depth of the shared shallow subnetwork. However, selecting deeper early exiting layers, such as half layers of $\\mathcal{M}_{b}$ , would result in excessively high inference latency. As shown in Figure 7(a), the choice of the optimal early exitlayer $l$ significantly influences the trade-off between token acceptance rate and drafting efficiency. For Kangaroo, we set $l=2$ for Vicuna-7B and $l\\,=\\,3$ for Vicuna-13B. In general cases, the early exit layer $\\ell$ can be set based on an empirical ratio derived from the ", "page_idx": 7}, {"type": "image", "img_path": "lT3oc04mDp/tmp/ad08c0c75ddfbc54eed1082322d5b5452adca3b901b9b542b0df2f0231a04ddb.jpg", "img_caption": ["Figure 6: The optimal early-exit ratio $\\frac{\\ell}{N}$ "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "target model\u2019s depth $N$ . To determine the optimal depth for shallow sub-networks relative to the full model depth, we conducted multiple comparative experiments across models with different architectures and sizes. Figure 6 records the average speedup achieved on Spec-Bench with early exits at various depths. We recommend setting $\\frac{\\ell}{N}$ between $\\textstyle{\\frac{1}{16}}$ and $\\textstyle{\\frac{1}{10}}$ in practical applications. ", "page_idx": 7}, {"type": "text", "text": "The Architecture of the Adapter Module. In a transformer block, the Feed-Forward Network (FFN) component counts for $67\\%$ of the whole parameters. As shown in Table 2, removing the FFN component and sharing the LM Head from the target Large Language Model (LLM) has been proven to be highly effective and efficient. Specifically, Kangaroo\u2019s adapter module $\\boldsymbol{\\mathcal{A}}$ for Vicuna-7B achieves a higher \u201cSpeedup\u201d of $1.50\\;\\times$ , compared to $1.41~\\times$ for Medusa, while using only about one-tenth of the additional parameters as Medusa. ", "page_idx": 7}, {"type": "text", "text": "Table 2: Ablation studies on the architecture of the adapter module $\\boldsymbol{\\mathcal{A}}$ for Vicuna-7B. \u201cSpeedup\u201c denotes the average speedup ratio on Spec-Bench [22]. ", "page_idx": 7}, {"type": "table", "img_path": "lT3oc04mDp/tmp/10daefe2e5bbd6e97517f8c35f8bea6e34ea9a76d17695aca3f39786b5416c91.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Dynamic Exiting v.s. Fixed Step Drafting. Kangaroo uses a static threshold to determine the timing of the second early stopping, based on the observation that the confidence of draft tokens in the small model is strongly correlated with their acceptance by the large model. To validate the effectiveness of dynamic drafting steps with fixed threshold, we plot the comparison for various $\\eta$ in Figure 7(b). The fixed step strategy $(\\eta=0)$ ) achieves the maximum compression rate, however, leading to sub-optimal end-to-end walltime speedup. Overall, the optimal threshold $\\eta$ is consistent across different maximum steps. For Kangaroo, we set $\\gamma\\,=\\,6$ and $\\eta\\,=\\,0.6$ . Besides, the static threshold is also robust across different subtasks and architectures. We visualized the conditional distributions of the small model\u2019s Top-1 confidence across different model architectures and subtasks in Figures 8 and 9 in the appendix, where the thereshold is stable, ranging from 0.6 to 0.8. ", "page_idx": 7}, {"type": "image", "img_path": "lT3oc04mDp/tmp/0be1137fa0515fa617a3914ed32f746600013962b172a55bf262b3191c88dae2.jpg", "img_caption": ["Figure 7: Ablation studies on hyper-parameters. The compression rate and walltime speedup is averaged across all sub-benchmarks in Spec-Bench [22]. It can be seen that there is a trade-off between token acceptance rate and drafting efficiency. While deeper early-exit layer $\\ell$ can enhance the expressive power of the equivalent draft model in Kangaroo, the increased inference latency can actually hinder the overall acceleration performance of the system. ", "(a) Optimal exit-layer $l$ ", "(b) Optimal threshold $\\eta$ "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Temperature Sampling It\u2019s well known that speculative sampling suffers from decreased speedup ratio when increasing the sampling temperature [22]. We consider two sets of comparative experiments ion Kangaroo for the temperature sampling case. One strategy uses the original top-1 confidence as a stopping criterion, while the other uses an adjusted top-1 confidence which is computed by applying softmax to adjusted logits with a temperature scaling factor. As shown in Table 3, it can be seen that Kangaroo still achieves a speedup effect very similar to that of greedy decoding when $T=0.2$ . Moreover, the top-1 confidence used for the second early stopping mechanism should remain unadjusted because when $T<1$ , the adjusted top-1 confidence tends to be overestimated, making it more difficult to trigger the early stopping mechanism. ", "page_idx": 8}, {"type": "table", "img_path": "lT3oc04mDp/tmp/1e881e360f616be4a61938b73e78fc43c5cf8dfb16200f517135f740a75922c0.jpg", "table_caption": ["Table 3: Speedup comparison of various temperature $T$ on Spec-Bench [22] for Vicuna [12]. Speedup is the walltime speedup ratio and CR denotes the compression rate. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In conclusion, Kangaroo presents a novel self-speculative decoding framework that significantly enhances the efficiency of autoregressive decoding for large language models. By leveraging a double early-exit mechanism, Kangaroo achieves remarkable speedups compared to existing methods, even with significantly fewer additional parameters. Experimental results demonstrate the effectiveness of Kangaroo in balancing the token acceptance rate and drafting efficiency, making it a promising approach for accelerating inference of LLMs in real-world scenarios. ", "page_idx": 8}, {"type": "text", "text": "Limitations Although we introduce a lightweight adapter module to bridge the gap between the shallow network and the final layer of the model, the effectiveness of Kangaroo relies on the target model and the specific task. In certain complex tasks, this representation gap may still persist. ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1 [2] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 1, 2, 7, 13 [3] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. 1 [4] Yunhe Wang, Hanting Chen, Yehui Tang, Tianyu Guo, Kai Han, Ying Nie, Xutao Wang, Hailin Hu, Zheyuan Bai, Yun Wang, et al. Pangu- $\\pi$ : Enhancing language model architectures via nonlinearity compensation. arXiv preprint arXiv:2312.17276, 2023. 1 [5] Yehui Tang, Fangcheng Liu, Yunsheng Ni, Yuchuan Tian, Zheyuan Bai, Yi-Qi Hu, Sichao Liu, Shangling Jui, Kai Han, and Yunhe Wang. Rethinking optimization and architecture for tiny language models. arXiv preprint arXiv:2402.02791, 2024. 1 [6] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. 1 [7] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824\u201324837, 2022. 1 [8] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):1\u201326, 2024. 1 [9] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344\u201316359, 2022. 1, 3   \n[10] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611\u2013626, 2023. 1, 3   \n[11] Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150, 2019. 1, 3   \n[12] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with $90\\%*$ chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6, 2023. 1, 2, 7, 9, 13   \n[13] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023. 1, 3, 7, 13   \n[14] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pages 19274\u201319286. PMLR, 2023. 1, 3   \n[15] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. Specinfer: Accelerating generative llm serving with speculative inference and token tree verification. arXiv preprint arXiv:2305.09781, 2023. 2, 6   \n[16] Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv Kumar, Jean-Fran\u00e7ois Kagy, and Rishabh Agarwal. Distillspec: Improving speculative decoding via knowledge distillation. arXiv preprint arXiv:2310.08461, 2023. 2, 3   \n[17] Sen Yang, Shujian Huang, Xinyu Dai, and Jiajun Chen. Multi-candidate speculative decoding. arXiv preprint arXiv:2401.06706, 2024. 2, 7   \n[18] Nan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin Jiang, Linjun Yang, Rangan Majumder, and Furu Wei. Inference with reference: Lossless acceleration of large language models. arXiv preprint arXiv:2304.04487, 2023. 2   \n[19] Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D Lee, and Di He. Rest: Retrieval-based speculative decoding. arXiv preprint arXiv:2311.08252, 2023. 2, 7   \n[20] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv preprint arXiv:2401.10774, 2024. 2, 3, 6, 7   \n[21] Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. Break the sequential dependency of llm inference using lookahead decoding. arXiv preprint arXiv:2402.02057, 2024. 2, 7   \n[22] Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and Zhifang Sui. Unlocking efficiency in large language model inference: A comprehensive survey of speculative decoding. arXiv preprint arXiv:2401.07851, 2024. 2, 3, 6, 7, 8, 9, 13   \n[23] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36, 2024. 2   \n[24] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 2   \n[25] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. 2   \n[26] Zixuan Zhou, Xuefei Ning, Ke Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, et al. A survey on efficient inference for large language models. arXiv preprint arXiv:2404.14294, 2024. 3   \n[27] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Minillm: Knowledge distillation of large language models. In The Twelfth International Conference on Learning Representations, 2023. 3   \n[28] Yehui Tang, Yunhe Wang, Jianyuan Guo, Zhijun Tu, Kai Han, Hailin Hu, and Dacheng Tao. A survey on transformer compression. arXiv preprint arXiv:2402.05964, 2024. 3   \n[29] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pages 38087\u201338099. PMLR, 2023. 3   \n[30] Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Tran, Yi Tay, and Donald Metzler. Confident adaptive language modeling. Advances in Neural Information Processing Systems, 35:17456\u201317472, 2022. 3   \n[31] Sangmin Bae, Jongwoo Ko, Hwanjun Song, and Se-Young Yun. Fast and robust early-exiting framework for autoregressive language models with synchronized parallel decoding. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5910\u20135924, 2023. 3   \n[32] Shengkun Tang, Yaqing Wang, Zhenglun Kong, Tianchi Zhang, Yao Li, Caiwen Ding, Yanzhi Wang, Yi Liang, and Dongkuan Xu. You need multiple exiting: Dynamic early exiting for accelerating unified vision language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10781\u201310791, 2023. 3   \n[33] Neeraj Varshney, Agneet Chatterjee, Mihir Parmar, and Chitta Baral. Accelerating llama inference by enabling intermediate layer decoding via instruction tuning with lite. arXiv e-prints, pages arXiv\u20132310, 2023. 3   \n[34] Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ahmad Beirami, Himanshu Jain, and Felix Yu. Spectr: Fast speculative decoding via optimal transport. arXiv preprint arXiv:2310.15141, 2023. 3   \n[35] Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Blockwise parallel decoding for deep autoregressive models. Advances in Neural Information Processing Systems, 31, 2018. 3   \n[36] Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, and Sharad Mehrotra. Draft & verify: Lossless large language model acceleration via self-speculative decoding. arXiv preprint arXiv:2309.08168, 2023. 3, 13   \n[37] Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Hasan Genc, Kurt Keutzer, Amir Gholami, and Sophia Shao. Speed: Speculative pipelined execution for efficient decoding. arXiv preprint arXiv:2310.12072, 2023. 3   \n[38] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle: Speculative sampling requires rethinking feature uncertainty. arXiv preprint arXiv:2401.15077, 2024. 3   \n[39] Zachary Ankner, Rishab Parthasarathy, Aniruddha Nrusimha, Christopher Rinard, Jonathan Ragan-Kelley, and William Brandon. Hydra: Sequentially-dependent draft heads for medusa decoding. arXiv preprint arXiv:2402.05109, 2024. 3   \n[40] Aonan Zhang, Chong Wang, Yi Wang, Xuanyu Zhang, and Yunfei Cheng. Recurrent drafter for fast speculative decoding in large language models. arXiv preprint arXiv:2403.09919, 2024. 3   \n[41] Zhuoming Chen, Avner May, Ruslan Svirschevski, Yuhsun Huang, Max Ryabinin, Zhihao Jia, and Beidi Chen. Sequoia: Scalable, robust, and hardware-aware speculative decoding. arXiv preprint arXiv:2402.12374, 2024. 6   \n[42] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 7 ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Table 4: Speedup comparison of various speculative decoding methods on Spec-Bench [22] for Llama2-13B-Chat [2]. Speedup is the walltime speedup ratio and CR denotes the compression rate. ", "page_idx": 12}, {"type": "table", "img_path": "lT3oc04mDp/tmp/c407260715adbf381dd58e95c4dd8499d445ea4f2cb90e42bbfa24f11ae5634f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 12}, {"type": "image", "img_path": "lT3oc04mDp/tmp/20153820fe3933a341f19b40efe664c154e83f031d304adde34c86432db1d5f6.jpg", "img_caption": ["Figure 8: The density of top-1 conditional probability on various subtasks for Vicuna-7B [12]. "], "img_footnote": [], "page_idx": 12}, {"type": "image", "img_path": "lT3oc04mDp/tmp/79c8b74c6b97c98a9a5bc65e63ce3520de54365c22dcf1de6fccd97c521c508c.jpg", "img_caption": ["Figure 9: The density of top-1 conditional probability on various subtasks for Llama2-13B-Chat [2]. "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 13}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction reflect the paper\u2019s contributions and scope accurately. ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 13}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 13}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Justification: The limitations are discussed in the Conclusion. ", "page_idx": 13}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms ", "page_idx": 13}, {"type": "text", "text": "that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 14}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 14}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 14}, {"type": "text", "text": "Justification: The paper does not include theoretical results ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 14}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: All the information needed to reproduce the main experimental results are provided in the Section 5. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.   \n(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: The data is open-sourced and the code implementation will be released upon accepted. Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 15}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: The details are provided in the Section 5. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 15}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 16}, {"type": "text", "text": "Answer: [No] ", "page_idx": 16}, {"type": "text", "text": "Justification: The error bar is not provided. Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 16}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Justification: The details are provided in the Section 5. Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 16}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] Justification: It is conformed. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 17}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 17}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 17}, {"type": "text", "text": "Answer:[NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: There is no risk in the paper. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. \u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. ", "page_idx": 17}, {"type": "text", "text": "\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. \u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 18}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Justification: CC-BY 4.0 is included for each asset ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 18}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 18}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 19}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 19}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 19}, {"type": "text", "text": "Answer:[NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 19}]