[{"figure_path": "v8RRFNbJ43/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of our one-model d\u00e9j\u00e0 vu test for image representation learning. The task is to predict the foreground object given a background crop. The original d\u00e9j\u00e0 vu test [Meehan et al., 2023] trains two models SSLA and SSLB on disjoint splits of the training set, and uses SSLB to quantify the degree of dataset-level correlation between the foreground and background crop. Our one-model test replaces SSLB with a classifier that directly predicts the foreground given background crop, and we show that both ResNet50 network and Naive Bayes classifier work well for this purpose.", "description": "This figure illustrates the difference between the original two-model d\u00e9j\u00e0 vu test and the proposed one-model test. The original test trains two separate models on disjoint subsets of the training data to measure memorization by comparing their performance. In contrast, the one-model test uses a single classifier trained to predict the foreground object from only the background crop, thus making the memorization evaluation much more efficient and scalable.", "section": "3 Measuring Dataset-level Correlations"}, {"figure_path": "v8RRFNbJ43/figures/figures_4_1.jpg", "caption": "Figure 2: Left: Population-level correlation accuracy scores across different models. The accuracies for two model tests are based on KNNs computed on top of VICReg, Barlow Twins and DINO representations. ResNet50 and Naive Bayes classifier are used for one model tests. The results show that ResNet50 and NB Top-2 are similar to both VICReg and Barlow Twins. Right: Corresponding Top-5 predicted dataset-level correlation classes and the percentage of per class correlated examples.", "description": "This figure presents a comparison of population-level correlation accuracy across different models using two different test types: a two-model test and a one-model test.  The two-model test utilizes KNN on VICReg, Barlow Twins, and DINO representations, while the one-model test employs ResNet50 and a Naive Bayes classifier.  The left panel displays the accuracy scores, showing that ResNet50 and Naive Bayes with top-2 classifications perform similarly to VICReg and Barlow Twins. The right panel shows the top 5 predicted dataset-level correlation classes and their corresponding percentages.", "section": "4.1 Image Representation Learning"}, {"figure_path": "v8RRFNbJ43/figures/figures_4_2.jpg", "caption": "Figure 2: Left: Population-level correlation accuracy scores across different models. The accuracies for two model tests are based on KNNs computed on top of VICReg, Barlow Twins and DINO representations. ResNet50 and Naive Bayes classifier are used for one model tests. The results show that ResNet50 and NB Top-2 are similar to both VICReg and Barlow Twins. Right: Corresponding Top-5 predicted dataset-level correlation classes and the percentage of per class correlated examples.", "description": "This figure presents a comparison of population-level correlation accuracy across different models, using both two-model and one-model tests.  The left panel shows the accuracy scores, highlighting the similarity between ResNet50/Naive Bayes and VICReg/Barlow Twins. The right panel displays the top 5 predicted dataset-level correlation classes and the percentage of examples in each class that show correlation.", "section": "Measuring Dataset-level Correlations"}, {"figure_path": "v8RRFNbJ43/figures/figures_5_1.jpg", "caption": "Figure 3: Left: Pairwise sample-level agreement in measuring dataset-level correlations and Right: Examples demonstrating when one model tests (Resnet and Naive Bayes classifiers) succeed and two model tests (KNN) fail and vice versa. One model tests learn the correlations between foreground and background better since it is enforced by the classifier training, however, they are less accurate when the relationships between foreground and background are ambiguous. One model tests, in contrast, are better at disambiguating the foreground and background relationships. They, however, sometimes tend to predict what's on the background and not what foreground it is associated with.", "description": "This figure shows two subfigures. The left one shows a heatmap representing the pairwise sample-level correlation agreement among different models in predicting dataset-level correlations. The right one shows examples where one-model tests (ResNet and Naive Bayes) are successful in predicting the foreground object from the background crop, while two-model tests (KNN) fail and vice versa.  It highlights the different strengths and weaknesses of the two approaches in various scenarios, especially dealing with ambiguous relationships between foreground and background.", "section": "3 Measuring Dataset-level Correlations"}, {"figure_path": "v8RRFNbJ43/figures/figures_6_1.jpg", "caption": "Figure 4: Pairwise sample-level agreement (using Jaccard similarity for predicting correct objects) between the reference VLM fB in previous two-model test and the GTE language model g. The heatmap shows that the agreement fraction for one model and two model tests are comparable.", "description": "This figure compares the performance of a two-model test (using a reference VLM fB) and a one-model test (using a GTE language model g) for predicting objects in images based on their captions.  The heatmaps show pairwise sample-level agreement, indicating the level of consistency between the two methods in their predictions. The results suggest that the one-model test, which is computationally less expensive, can provide a reasonable approximation of the memorization measured by the more expensive two-model test.", "section": "3.3 Vision Language Models"}, {"figure_path": "v8RRFNbJ43/figures/figures_7_1.jpg", "caption": "Figure 5: Comparison of overall and Top 20% most confident D\u00e9j\u00e0 vu (DV) scores using one model (ResNet Classifier, Naive Bayes w/ Top-k Crop Annotations (CA)) and two model (KNN Classifier) tests for VICReg, Barlow Twins and DINO trained on a 300k subset of ImageNet.", "description": "This figure compares the overall and top 20% most confident DejaVu scores obtained using one-model and two-model tests on three different self-supervised learning (SSL) models (VICReg, Barlow Twins, and DINO) trained on a 300k subset of ImageNet. The one-model tests utilize a ResNet classifier and a Naive Bayes classifier with varying numbers of top-k crop annotations, while the two-model test employs a KNN classifier.  The comparison helps to assess the consistency of memorization measurement across different methods.", "section": "4.1.1 How close is the d\u00e9j\u00e0 vu memorization of one-model and the two-model tests?"}, {"figure_path": "v8RRFNbJ43/figures/figures_7_2.jpg", "caption": "Figure 5: Comparison of overall and Top 20% most confident D\u00e9j\u00e0 vu (DV) scores using one model (ResNet Classifier, Naive Bayes w/ Top-k Crop Annotations (CA)) and two model (KNN Classifier) tests for VICReg, Barlow Twins and DINO trained on a 300k subset of ImageNet.", "description": "This figure compares the overall and top 20% most confident DejaVu scores obtained using one-model tests (ResNet classifier and Naive Bayes with top k crop annotations) and two-model tests (KNN classifier) for three different self-supervised learning models: VICReg, Barlow Twins, and DINO.  The models were trained on a 300k subset of the ImageNet dataset. The DejaVu score reflects the degree of memorization, with higher scores indicating more memorization. The comparison helps assess the consistency and accuracy of different memorization measurement approaches.", "section": "4.1.1 How close is the d\u00e9j\u00e0 vu memorization of one-model and the two-model tests?"}, {"figure_path": "v8RRFNbJ43/figures/figures_8_1.jpg", "caption": "Figure 8: Data set level memorization of various VLMs. We use top-10 public set NNs to predict the top-k objects and report PPG and PRG as done in Jayaraman et al. [2024].", "description": "This figure compares the population-level memorization results for various Vision-Language Models (VLMs) using two different evaluation metrics: Population Precision Gap (PPG) and Population Recall Gap (PRG).  It shows the results for predicting the top-1, top-10, and all objects in a dataset. The results are broken down into two model tests and one model tests, providing a comparison of the two approaches.", "section": "4.2 Vision Language Models"}, {"figure_path": "v8RRFNbJ43/figures/figures_8_2.jpg", "caption": "Figure 5: Comparison of overall and Top 20% most confident D\u00e9j\u00e0 vu (DV) scores using one model (ResNet Classifier, Naive Bayes w/ Top-k Crop Annotations (CA)) and two model (KNN Classifier) tests for VICReg, Barlow Twins and DINO trained on a 300k subset of ImageNet.", "description": "This figure compares the overall and top 20% most confident d\u00e9j\u00e0 vu (memorization) scores obtained using one-model tests (ResNet and Naive Bayes classifiers with varying numbers of top-k crop annotations) and a two-model test (KNN classifier).  The comparison is performed for three different self-supervised learning models: VICReg, Barlow Twins, and DINO, all trained on a 300k subset of the ImageNet dataset.  The results illustrate how closely the one-model and two-model test results align and provide insights into memorization levels for different models. ", "section": "4.1.1 How close is the d\u00e9j\u00e0 vu memorization of one-model and the two-model tests?"}, {"figure_path": "v8RRFNbJ43/figures/figures_8_3.jpg", "caption": "Figure 7: A histogram of sample-based memorization confidence for VICReg OOB model. Given a background patch, VICReg predicts the correct class (green). ResNet (correlation classifier) predicts the incorrect (red) class.", "description": "This figure visualizes the distribution of memorization confidence scores for pre-trained VICReg OSS model with ResNet as correlation detector.  The memorization confidence for each example is computed as the difference between the entropy of the correlation classifier and the entropy of the KNN classifier. The histogram shows that memorized examples with high memorization confidence scores are rarer and more likely to be memorized. Examples in the middle of the distribution are easier to confuse with another class, while those with negative memorization confidence have higher memorization and slightly lower correlation entropy.", "section": "4.1.3 Sample-level memorization"}, {"figure_path": "v8RRFNbJ43/figures/figures_9_1.jpg", "caption": "Figure 9: Sample-level memorization in VLM trained on 40M Shutterstock images, quantified in terms of precision and recall gap between target VLM and off-the-shelf GTE LM.", "description": "This figure visualizes the distribution of memorization confidence scores for a pre-trained VICReg OSS model.  The x-axis represents the top-L records (samples sorted from high to low memorization confidence), and the y-axis shows precision, recall, and F-score gaps.  Positive gaps indicate that the target model memorizes the training sample; larger gaps suggest higher degrees of memorization.  Different lines represent different numbers of nearest neighbors (NNs) used in the similarity search during the evaluation process.", "section": "4.2 Vision Language Models"}, {"figure_path": "v8RRFNbJ43/figures/figures_13_1.jpg", "caption": "Figure 10: Two common dataset-level correlations: 1) \u2018stove, kitchen\u2019 and \u2018microwave\u2019, 2) \u2018sky, pole, water\u2019 and \u2018gondola\u2019. ResNet and Naive Bayes classifiers learn to associate the background crops with the image label.", "description": "This figure demonstrates two examples of common dataset-level correlations identified by the ResNet and Naive Bayes classifiers. The first correlation shows a strong association between images containing stoves or kitchens and the presence of microwaves. The second correlation shows that images with skies, poles, and water are frequently associated with gondolas.  These correlations, learned by the classifiers, highlight the ability of the models to predict foreground objects based solely on background information, even in the absence of explicit memorization.", "section": "4.1 Image Representation Learning"}, {"figure_path": "v8RRFNbJ43/figures/figures_13_2.jpg", "caption": "Figure 11: KNN predicts correct class \u2018shopping cart\u2019 given VICReg\u2019s representation of the background crop. Here the original image of the crop is part of the VICReg\u2019s training. ResNet and NB classifiers, however, fail to predict the correct class which concludes that the image is memorized. correlation between the background and foreground and some images could be unique than the rest of the ImageNet.", "description": "This figure illustrates a one-model d\u00e9j\u00e0 vu test. The task is to predict the foreground object given only the background crop of an image. The figure shows that while KNN correctly predicts the foreground object, ResNet and Naive Bayes classifiers fail, indicating memorization.", "section": "3.2 Image Representation Learning Models"}, {"figure_path": "v8RRFNbJ43/figures/figures_14_1.jpg", "caption": "Figure 12: Top-5 images memorized by VICReg OSS model. Both Naive Bayes and ResNet classifiers fail to predict the correct class based on the background crops.", "description": "This figure shows five examples of images that the VICReg OSS model memorized.  For each image, the original image is shown alongside its corresponding background crop. The labels of the objects are also shown.  The caption indicates that neither the Naive Bayes nor ResNet classifiers could predict the correct object label based on the background crop alone, suggesting that the model memorized the association between the background and foreground object in these cases.", "section": "4.1.2 Do pre-trained representation learning models in the wild exhibit d\u00e9j\u00e0 vu memorization?"}, {"figure_path": "v8RRFNbJ43/figures/figures_15_1.jpg", "caption": "Figure 1: Illustration of our one-model d\u00e9j\u00e0 vu test for image representation learning. The task is to predict the foreground object given a background crop. The original d\u00e9j\u00e0 vu test [Meehan et al., 2023] trains two models SSL\u0104 and SSLB on disjoint splits of the training set, and uses SSLB to quantify the degree of dataset-level correlation between the foreground and background crop. Our one-model test replaces SSLB with a classifier that directly predicts the foreground given background crop, and we show that both ResNet50 network and Naive Bayes classifier work well for this purpose.", "description": "This figure illustrates the proposed one-model approach for measuring d\u00e9j\u00e0 vu memorization in image representation learning.  It contrasts the original two-model method (which trains two separate models on disjoint datasets to assess dataset-level correlations) with the new one-model method. The one-model method uses a single classifier (either a ResNet50 or a Naive Bayes classifier) trained to directly predict the foreground object from the background crop, eliminating the need for training a second model for correlation estimation. This simplification allows efficient memorization measurement on large, pre-trained models.", "section": "3 Measuring Dataset-level Correlations"}, {"figure_path": "v8RRFNbJ43/figures/figures_17_1.jpg", "caption": "Figure 1: Illustration of our one-model d\u00e9j\u00e0 vu test for image representation learning. The task is to predict the foreground object given a background crop. The original d\u00e9j\u00e0 vu test [Meehan et al., 2023] trains two models SSL\u0104 and SSLB on disjoint splits of the training set, and uses SSLB to quantify the degree of dataset-level correlation between the foreground and background crop. Our one-model test replaces SSLB with a classifier that directly predicts the foreground given background crop, and we show that both ResNet50 network and Naive Bayes classifier work well for this purpose.", "description": "This figure illustrates the proposed one-model d\u00e9j\u00e0 vu test, which simplifies the original two-model approach by replacing the second model with a classifier directly predicting foreground objects from background crops.  It contrasts the two-model approach, which trains separate models on disjoint data splits to measure dataset-level correlations, with the proposed one-model test's more efficient single-classifier approach.", "section": "3 Measuring Dataset-level Correlations"}, {"figure_path": "v8RRFNbJ43/figures/figures_17_2.jpg", "caption": "Figure 14: Examples showing correlations captured by the reference VLM (fB) and the LLM (g).", "description": "This figure presents several examples to illustrate how well the Large Language Model (LLM) and the Vision-Language Model (VLM) capture correlations for predicting objects in images.  Each example shows a target image and its caption.  Then, the top 5 nearest neighbors (NNs) from a public dataset are displayed for both the VLM and LLM, along with the predicted labels and the number of objects recovered, precision, and recall.  Comparing the results of the VLM and LLM across different images highlights their strengths and weaknesses in capturing correlations and predicting objects based on textual descriptions.", "section": "4.2 Vision Language Models"}, {"figure_path": "v8RRFNbJ43/figures/figures_18_1.jpg", "caption": "Figure 1: Illustration of our one-model d\u00e9j\u00e0 vu test for image representation learning. The task is to predict the foreground object given a background crop. The original d\u00e9j\u00e0 vu test [Meehan et al., 2023] trains two models SSLA and SSLB on disjoint splits of the training set, and uses SSLB to quantify the degree of dataset-level correlation between the foreground and background crop. Our one-model test replaces SSLB with a classifier that directly predicts the foreground given background crop, and we show that both ResNet50 network and Naive Bayes classifier work well for this purpose.", "description": "This figure illustrates the proposed one-model d\u00e9j\u00e0 vu test, comparing it to the original two-model approach.  The task remains predicting the foreground object from only the background. The key difference is that the new method uses a single classifier (ResNet50 or Naive Bayes) trained to directly predict the foreground, eliminating the need for training a second model to estimate dataset-level correlations.", "section": "3 Measuring Dataset-level Correlations"}]