[{"figure_path": "v8RRFNbJ43/tables/tables_12_1.jpg", "caption": "Table 1: Example-level correlation accuracy between the intersection of two model tests and each one model test.", "description": "This table presents the accuracy of different classifiers (ResNet and Naive Bayes with varying numbers of top crop annotations) in predicting dataset-level correlations.  It compares these one-model test accuracies against the intersection of results from three two-model tests (VICReg, Barlow Twins, and DINO).  The goal is to evaluate how well the simpler one-model tests align with the more complex two-model approach in identifying these correlations.", "section": "4.1 Image Representation Learning"}, {"figure_path": "v8RRFNbJ43/tables/tables_14_1.jpg", "caption": "Table 2: Comparing the population-level memorization for predicting all objects for various settings where the 40M D set is used as the target set. For g, we use the GTE model where we match the target caption with the public set captions. For the VLMs, t2i is the cross-modal setting where target caption is matched with public set images for kNN search, t2t is the unimodal setting where only the text modality of the model is used for kNN search, i.e., target caption is matched with public set captions similar to the g case. We do not consider the image-to-image search as the target image is not known to the adversary.", "description": "This table compares population-level memorization results for predicting all objects using different methods. It uses a 40M subset (D) for the target set and compares a two-model test with two one-model tests (ft2i vs g and ft2t vs g).  The GTE model is used for g, matching target captions with public set captions.  The VLMs use cross-modal (ft2i) and unimodal (ft2t) settings. Image-to-image search is not used because the target image is unknown to the adversary.  The table shows PPG and PRG values for each method using Top-1, Top-10 and Top-100 nearest neighbors (NNs).", "section": "Additional results for Image Representation Learning"}, {"figure_path": "v8RRFNbJ43/tables/tables_15_1.jpg", "caption": "Table 3: Comparing the population-level memorization for various settings with top-10 public NNs where the 40M D set is used as the target set. For g, we use the GTE model where we match the target caption with the public set captions. For the VLMs, t2i is the cross-modal setting where target caption is matched with public set images for kNN search, t2t is the unimodal setting where only the text modality of the model is used for kNN search, i.e., target caption is matched with public set captions similar to the g case. We do not consider the image-to-image search as the target image is not known to the adversary.", "description": "This table compares the population-level memorization results (PPG and PRG) for different experimental settings in a vision-language model (VLM).  It contrasts a two-model approach with a one-model approach using a pre-trained GTE language model (g) as a reference. The comparison is made across three scenarios: predicting the top-1, top-10, and all objects.  The results highlight how the different methods and settings affect the quantification of memorization.", "section": "4.2.1 How close is the d\u00e9j\u00e0 vu memorization of one-model and the two-model tests?"}, {"figure_path": "v8RRFNbJ43/tables/tables_15_2.jpg", "caption": "Table 4: Population-level memorization for predicting all objects with the pre-trained YFCC15M OSS model. For g, we use the GTE model where we match the target caption with the public set captions. For the VLMs, t2i is the cross-modal setting where target caption is matched with public set images for kNN search, t2t is the unimodal setting where only the text modality of the model is used for kNN search, i.e., target caption is matched with public set captions similar to the g case. We do not consider the image-to-image search as the target image is not known to the adversary.", "description": "This table presents the population-level memorization results for predicting all objects using different numbers of nearest neighbors (NNs) from a public set.  It compares the performance of a two-model test (using a Vision Language Model, or VLM) against a one-model test (using a pre-trained Generalized Text Embedding, or GTE, language model). The results show the Population Precision Gap (PPG) and Population Recall Gap (PRG) for both methods under various settings. The \"t2i\" setting represents cross-modal retrieval, while \"t2t\" represents unimodal (text-only) retrieval.  The table highlights the differences between the two methods and explores their memorization capabilities in different retrieval scenarios.", "section": "4.2.2 Do pre-trained vision-language models in the wild exhibit d\u00e9j\u00e0 vu memorization?"}, {"figure_path": "v8RRFNbJ43/tables/tables_16_1.jpg", "caption": "Table 3: Comparing the population-level memorization for various settings with top-10 public NNs where the 40M D set is used as the target set. For g, we use the GTE model where we match the target caption with the public set captions. For the VLMs, t2i is the cross-modal setting where target caption is matched with public set images for kNN search, t2t is the unimodal setting where only the text modality of the model is used for kNN search, i.e., target caption is matched with public set captions similar to the g case. We do not consider the image-to-image search as the target image is not known to the adversary.", "description": "This table presents a comparison of population-level memorization results for different settings using top-10 nearest neighbors from a public set.  It contrasts a two-model approach with a one-model approach using a pre-trained language model (GTE) as a reference, showing results for predicting top-1, top-10, and all objects. The comparison considers cross-modal and unimodal settings for the VLMs, highlighting that the image-to-image search was excluded because the target image was unknown to the adversary.", "section": "4.2.1 How close is the d\u00e9j\u00e0 vu memorization of one-model and the two-model tests?"}]