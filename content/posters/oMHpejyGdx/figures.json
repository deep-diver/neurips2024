[{"figure_path": "oMHpejyGdx/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of a portrait with (a) no defense, (b) prompt-specific and (c) our PAP prompt-agnostic perturbation. In (a), the portrait is easily tampered with by the diffusion model. In (b), the prompt-specific methods only perform well on learned prompts (i.e., Prompt A) and are fruitless to unseen prompts (i.e., Prompts B and C). In (c), the proposed PAP is robust to both the seen and unseen prompts, and successfully protects the portrait from diffusion model tampering", "description": "This figure illustrates the effectiveness of the proposed PAP (Prompt-Agnostic Perturbation) method compared to methods without defense and prompt-specific methods.  It shows three scenarios: (a) a portrait without any defense, which is easily modified by the diffusion model; (b) a portrait protected using a prompt-specific perturbation, which is only effective against the specific prompt it was trained on (Prompt A); and (c) a portrait protected with the PAP method, which effectively protects the portrait from modification regardless of the prompt used (Prompts A, B, and C). The PAP method achieves this robustness by modeling the prompt distribution, instead of relying on pre-defined prompts.", "section": "1 Introduction"}, {"figure_path": "oMHpejyGdx/figures/figures_7_1.jpg", "caption": "Figure 2: Qualitative defense results of different methods in VGGFace2 (left) and Wikiart (right). Each row represents a method, and each column represents a different test prompt (shown at the bottom). The adversarial examples generated by our method effectively defend against all prompts in both datasets. In contrast, other baselines primarily focus on protecting the fixed prompt (the first column), resulting in compromised defense for other prompts.", "description": "This figure presents a qualitative comparison of different adversarial perturbation methods for protecting images from tampering by diffusion models.  The left side shows results on the VGGFace2 dataset (faces), while the right side shows results on the Wikiart dataset (paintings). Each row represents a different method: Clean (no defense), AdvDM, Anti-DreamBooth, IAdvDM, and PAP (the proposed method). Each column shows the results for a different test prompt. The figure demonstrates that the proposed PAP method is robust to unseen prompts, unlike the other methods which perform poorly on prompts different from the one they were trained on.", "section": "4.2 Comparison with State-of-the-Art Methods"}, {"figure_path": "oMHpejyGdx/figures/figures_8_1.jpg", "caption": "Figure 1: Illustration of a portrait with (a) no defense, (b) prompt-specific and (c) our PAP prompt-agnostic perturbation. In (a), the portrait is easily tampered with by the diffusion model. In (b), the prompt-specific methods only perform well on learned prompts (i.e., Prompt A) and are fruitless to unseen prompts (i.e., Prompts B and C). In (c), the proposed PAP is robust to both the seen and unseen prompts, and successfully protects the portrait from diffusion model tampering", "description": "This figure illustrates the effectiveness of the proposed Prompt-Agnostic Adversarial Perturbation (PAP) method in protecting images from diffusion model tampering.  It compares three scenarios: (a) no defense, where the portrait is easily modified; (b) prompt-specific perturbation, which only works well on the prompts it was trained on; and (c) PAP, which successfully defends against both seen and unseen prompts.", "section": "1 Introduction"}, {"figure_path": "oMHpejyGdx/figures/figures_21_1.jpg", "caption": "Figure 1: Illustration of a portrait with (a) no defense, (b) prompt-specific and (c) our PAP prompt-agnostic perturbation. In (a), the portrait is easily tampered with by the diffusion model. In (b), the prompt-specific methods only perform well on learned prompts (i.e., Prompt A) and are fruitless to unseen prompts (i.e., Prompts B and C). In (c), the proposed PAP is robust to both the seen and unseen prompts, and successfully protects the portrait from diffusion model tampering", "description": "This figure compares three different scenarios of protecting a portrait from being manipulated by a diffusion model. (a) shows a portrait without any defense, which is easily modified. (b) shows a portrait protected using a prompt-specific method. This method is effective only for prompts it was trained on, failing to protect against unseen prompts. (c) shows the proposed prompt-agnostic adversarial perturbation (PAP) method, which is robust to both seen and unseen prompts, successfully protecting the portrait.", "section": "1 Introduction"}, {"figure_path": "oMHpejyGdx/figures/figures_25_1.jpg", "caption": "Figure 1: Illustration of a portrait with (a) no defense, (b) prompt-specific and (c) our PAP prompt-agnostic perturbation. In (a), the portrait is easily tampered with by the diffusion model. In (b), the prompt-specific methods only perform well on learned prompts (i.e., Prompt A) and are fruitless to unseen prompts (i.e., Prompts B and C). In (c), the proposed PAP is robust to both the seen and unseen prompts, and successfully protects the portrait from diffusion model tampering", "description": "This figure illustrates the difference between no defense, prompt-specific perturbation, and the proposed prompt-agnostic adversarial perturbation (PAP) method for protecting images from tampering using diffusion models. It shows how easily a portrait can be modified without any defense (a), how prompt-specific methods fail to generalize to unseen prompts (b), and how PAP successfully protects the portrait against both seen and unseen prompts (c).", "section": "1 Introduction"}, {"figure_path": "oMHpejyGdx/figures/figures_29_1.jpg", "caption": "Figure 1: Illustration of a portrait with (a) no defense, (b) prompt-specific and (c) our PAP prompt-agnostic perturbation. In (a), the portrait is easily tampered with by the diffusion model. In (b), the prompt-specific methods only perform well on learned prompts (i.e., Prompt A) and are fruitless to unseen prompts (i.e., Prompts B and C). In (c), the proposed PAP is robust to both the seen and unseen prompts, and successfully protects the portrait from diffusion model tampering", "description": "This figure illustrates the effectiveness of the proposed Prompt-Agnostic Adversarial Perturbation (PAP) method compared to methods without defense and prompt-specific perturbation methods.  It shows how easily a portrait can be modified using a diffusion model without any defense (a). It demonstrates that prompt-specific methods only work well for prompts they were trained on (b), failing when encountering unseen prompts. Finally, it highlights that PAP successfully protects the portrait from tampering, even with unseen prompts (c), showcasing its robustness and generalizability.", "section": "1 Introduction"}, {"figure_path": "oMHpejyGdx/figures/figures_30_1.jpg", "caption": "Figure 1: Illustration of a portrait with (a) no defense, (b) prompt-specific and (c) our PAP prompt-agnostic perturbation. In (a), the portrait is easily tampered with by the diffusion model. In (b), the prompt-specific methods only perform well on learned prompts (i.e., Prompt A) and are fruitless to unseen prompts (i.e., Prompts B and C). In (c), the proposed PAP is robust to both the seen and unseen prompts, and successfully protects the portrait from diffusion model tampering", "description": "This figure illustrates the effectiveness of the proposed PAP method by comparing it against other methods. The leftmost image shows a portrait that can easily be manipulated without any defense. The middle image demonstrates that prompt-specific methods only work well for seen prompts but fail for unseen ones. Finally, the rightmost image shows that the proposed PAP is effective for both seen and unseen prompts, protecting the portrait from tampering.", "section": "1 Introduction"}, {"figure_path": "oMHpejyGdx/figures/figures_31_1.jpg", "caption": "Figure 1: Illustration of a portrait with (a) no defense, (b) prompt-specific and (c) our PAP prompt-agnostic perturbation. In (a), the portrait is easily tampered with by the diffusion model. In (b), the prompt-specific methods only perform well on learned prompts (i.e., Prompt A) and are fruitless to unseen prompts (i.e., Prompts B and C). In (c), the proposed PAP is robust to both the seen and unseen prompts, and successfully protects the portrait from diffusion model tampering", "description": "This figure illustrates the effectiveness of the proposed Prompt-Agnostic Adversarial Perturbation (PAP) method in protecting portraits from tampering by diffusion models.  It compares three scenarios:\n(a) **No defense:** A portrait is easily modified by the diffusion model using different prompts.\n(b) **Prompt-specific perturbation:** A perturbation is generated to protect against a specific prompt (Prompt A). However, it is ineffective against unseen prompts (Prompt B and C).\n(c) **PAP:** The proposed prompt-agnostic perturbation is effective against both seen and unseen prompts, demonstrating robustness and generalization.", "section": "1 Introduction"}, {"figure_path": "oMHpejyGdx/figures/figures_32_1.jpg", "caption": "Figure 9: Clean examples and corresponding adversarial examples generated by our method with different noise budgets (ranging from 0.01 to 0.15) on VGGFace2. The training prompt is \u201ca photo of sks person\u201d.", "description": "This figure shows the results of applying the proposed Prompt-Agnostic Adversarial Perturbation (PAP) method to protect images on VGGFace2 dataset.  The top row displays the original \"clean\" images. The subsequent rows demonstrate the adversarial examples generated by PAP with varying noise budgets (0.01, 0.03, 0.05, 0.10, 0.15).  The noise budget is a hyperparameter controlling the strength of the perturbation. A higher noise budget generally leads to stronger protection (making the image less recognizable) but also more visible distortion.", "section": "4.2.1 Face Privacy Protection"}, {"figure_path": "oMHpejyGdx/figures/figures_33_1.jpg", "caption": "Figure 1: Illustration of a portrait with (a) no defense, (b) prompt-specific and (c) our PAP prompt-agnostic perturbation. In (a), the portrait is easily tampered with by the diffusion model. In (b), the prompt-specific methods only perform well on learned prompts (i.e., Prompt A) and are fruitless to unseen prompts (i.e., Prompts B and C). In (c), the proposed PAP is robust to both the seen and unseen prompts, and successfully protects the portrait from diffusion model tampering", "description": "This figure illustrates the effectiveness of the proposed Prompt-Agnostic Adversarial Perturbation (PAP) method in protecting images from diffusion model tampering.  It compares three scenarios: (a) no defense, where the image is easily modified; (b) prompt-specific perturbation, which only works well for prompts seen during training; and (c) the proposed PAP method, which is robust to both seen and unseen prompts, demonstrating improved defense stability.", "section": "1 Introduction"}, {"figure_path": "oMHpejyGdx/figures/figures_34_1.jpg", "caption": "Figure 2: Qualitative defense results of different methods in VGGFace2 (left) and Wikiart (right). Each row represents a method, and each column represents a different test prompt (shown at the bottom). The adversarial examples generated by our method effectively defend against all prompts in both datasets. In contrast, other baselines primarily focus on protecting the fixed prompt (the first column), resulting in compromised defense for other prompts.", "description": "This figure compares the performance of different methods (Clean, AdvDM, Anti-DB, IAdvDM, and PAP) in protecting images from being manipulated by a diffusion model. The left side shows the results for face images (VGGFace2 dataset), while the right side shows the results for paintings (Wikiart dataset).  Each method is tested against various prompts, including unseen ones.  The results demonstrate that PAP is the most robust method, effectively protecting images against manipulation regardless of the prompt used. Other methods struggle when the prompt changes to one that wasn't used during training.", "section": "4.2 Comparison with State-of-the-Art Methods"}, {"figure_path": "oMHpejyGdx/figures/figures_34_2.jpg", "caption": "Figure 1: Illustration of a portrait with (a) no defense, (b) prompt-specific and (c) our PAP prompt-agnostic perturbation. In (a), the portrait is easily tampered with by the diffusion model. In (b), the prompt-specific methods only perform well on learned prompts (i.e., Prompt A) and are fruitless to unseen prompts (i.e., Prompts B and C). In (c), the proposed PAP is robust to both the seen and unseen prompts, and successfully protects the portrait from diffusion model tampering", "description": "This figure illustrates the effectiveness of the proposed Prompt-Agnostic Adversarial Perturbation (PAP) method in protecting images from being manipulated by diffusion models. It compares three scenarios: (a) no defense, where the image is easily tampered with; (b) prompt-specific perturbation, where the defense only works for specific prompts it has been trained on; and (c) PAP, which is robust to both seen and unseen prompts, effectively protecting the image from tampering.", "section": "1 Introduction"}, {"figure_path": "oMHpejyGdx/figures/figures_35_1.jpg", "caption": "Figure 1: Illustration of a portrait with (a) no defense, (b) prompt-specific and (c) our PAP prompt-agnostic perturbation. In (a), the portrait is easily tampered with by the diffusion model. In (b), the prompt-specific methods only perform well on learned prompts (i.e., Prompt A) and are fruitless to unseen prompts (i.e., Prompts B and C). In (c), the proposed PAP is robust to both the seen and unseen prompts, and successfully protects the portrait from diffusion model tampering", "description": "The figure illustrates the effectiveness of the proposed Prompt-Agnostic Adversarial Perturbation (PAP) method compared to methods without defense and prompt-specific methods.  It shows how easily a portrait can be manipulated by a diffusion model without any defense (a). It then demonstrates the limitations of prompt-specific methods which only work well for prompts seen during training (b). Finally, it showcases the robustness of PAP, which effectively protects the portrait even against unseen prompts (c).", "section": "1 Introduction"}, {"figure_path": "oMHpejyGdx/figures/figures_35_2.jpg", "caption": "Figure 5: Visualized results of different test prompts toward Anti-DB method on the CelebA-HQ dataset and Wikiart dataset, with training prompt: a photo of sks person (top) / a sks painting (bottom). The left column are adversarial examples (denoted as AE) by Anti-DB.", "description": "This figure shows a comparison of the results obtained using the Anti-DreamBooth (Anti-DB) method and the proposed Prompt-Agnostic Adversarial Perturbation (PAP) method.  The top row uses CelebA-HQ dataset with the training prompt \"a photo of sks person\", and the bottom row uses Wikiart dataset with the training prompt \"a sks painting\". For each dataset, the figure shows the original image, adversarial examples generated using the Anti-DB method, and adversarial examples generated by PAP under different test prompts.  The results demonstrate that PAP is more robust to unseen prompts than the Anti-DB method, which shows inferior performance for test prompts other than the training prompt.", "section": "4.2 Comparison with State-of-the-Art Methods"}, {"figure_path": "oMHpejyGdx/figures/figures_36_1.jpg", "caption": "Figure 1: Illustration of a portrait with (a) no defense, (b) prompt-specific and (c) our PAP prompt-agnostic perturbation. In (a), the portrait is easily tampered with by the diffusion model. In (b), the prompt-specific methods only perform well on learned prompts (i.e., Prompt A) and are fruitless to unseen prompts (i.e., Prompts B and C). In (c), the proposed PAP is robust to both the seen and unseen prompts, and successfully protects the portrait from diffusion model tampering", "description": "This figure compares three scenarios: (a) a portrait without any defense, easily modified by the diffusion model; (b) a portrait protected using a prompt-specific method that only works well on the training prompt (Prompt A), failing to protect against unseen prompts (Prompts B and C); (c) a portrait protected using the proposed PAP method, which remains robust against both seen and unseen prompts.", "section": "1 Introduction"}]