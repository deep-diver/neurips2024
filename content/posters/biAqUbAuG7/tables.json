[{"figure_path": "biAqUbAuG7/tables/tables_15_1.jpg", "caption": "Figure 7: Comparison of the regressed median on the Atari-10 benchmark of Adam and Adam-Rel for DQN with Polyak Averaging.", "description": "This figure compares the performance of Adam and Adam-Rel optimizers on the Atari-10 benchmark when using the Deep Q-Network (DQN) algorithm with Polyak averaging.  Polyak averaging is a technique used to smooth the updates to the target network in DQN. The regressed median is used as a performance metric. The plot shows that Adam-Rel significantly outperforms Adam in this setting.", "section": "5.3 On-policy RL"}, {"figure_path": "biAqUbAuG7/tables/tables_15_2.jpg", "caption": "Table 2: Atari Adam PPO hyperparameters", "description": "This table shows the hyperparameters used for the Proximal Policy Optimization (PPO) algorithm with the Adam optimizer on the Atari benchmark.  It lists the learning rate, number of epochs, minibatch size, discount factor (\u03b3), generalized advantage estimation (GAE) \u03bb parameter, whether advantages were normalized, the value function clipping parameter (\u03b5), whether value function clipping was used, max gradient norm, number of environments, and the number of rollout steps. These settings were used in the experiments evaluating the Adam-Rel algorithm's performance.", "section": "5.1 Experimental setup"}, {"figure_path": "biAqUbAuG7/tables/tables_15_3.jpg", "caption": "Table 3: Atari Adam-Rel and Adam-MR PPO hyperparameters", "description": "This table lists the hyperparameters used for both Adam-Rel and Adam-MR in the Proximal Policy Optimization (PPO) algorithm on the Atari benchmark.  It specifies values for learning rate, number of epochs, minibatches, gamma (\u03b3), Generalized Advantage Estimation (GAE) lambda (\u03bb), whether advantages are normalized, epsilon (\u03f5), whether value function clipping is used, the maximum gradient norm, the number of environments used for training, and the number of rollout steps.", "section": "5 Experiments"}, {"figure_path": "biAqUbAuG7/tables/tables_16_1.jpg", "caption": "Table 4: Atari-10 DQN hyperparameters", "description": "This table lists the hyperparameters used for the Deep Q-Network (DQN) algorithm in the Atari-10 experiments.  It includes the learning rate, replay buffer size, discount factor (\u03b3), generalized advantage estimation (GAE) \u03bb parameter, target network update frequency, batch size, exploration rate (start and end epsilon), exploration fraction, number of steps without training, and train frequency.", "section": "5.1 Experimental setup"}, {"figure_path": "biAqUbAuG7/tables/tables_16_2.jpg", "caption": "Table 5: Craftax Adam and Adam-MR PPO hyperparameters", "description": "This table lists the hyperparameters used for the Proximal Policy Optimization (PPO) algorithm with Adam and Adam-MR optimizers on the Craftax environment.  The hyperparameters include the learning rate, number of epochs, minibatches, gamma (\u03b3), Generalized Advantage Estimation lambda (GAE \u03bb), whether advantages were normalized, value function clipping epsilon (\u03f5), maximum gradient norm, number of environments, and number of rollout steps. Note that the hyperparameters were tuned separately for Adam and Adam-MR.", "section": "5.1 Experimental setup"}, {"figure_path": "biAqUbAuG7/tables/tables_16_3.jpg", "caption": "Table 6: Craftax Adam-Rel hyperparameters", "description": "This table presents the hyperparameters used for the Adam-Rel algorithm during the Craftax experiments.  It includes the learning rate, the number of epochs and minibatches used in training, the discount factor (\u03b3), the generalized advantage estimation lambda (GAE \u03bb), whether advantages were normalized, the value function clipping parameter (\u03b5), whether value function clipping was used, the maximum gradient norm, the number of environments used for training, and the number of rollout steps.", "section": "5.1 Experimental setup"}]