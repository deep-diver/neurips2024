{"references": [{"fullname_first_author": "Polyak", "paper_title": "Some methods of speeding up the convergence of iteration methods", "publication_date": "1964-01-01", "reason": "This paper introduced the concept of Polyak averaging, a crucial technique for stabilizing optimization in non-stationary environments like reinforcement learning."}, {"fullname_first_author": "Ilya Sutskever", "paper_title": "On the importance of initialization and momentum in deep learning", "publication_date": "2013-01-01", "reason": "This foundational paper analyzed the effects of initialization and momentum on deep learning, providing a basis for understanding the impact of non-stationarity on Adam and similar optimization algorithms."}, {"fullname_first_author": "Volodymyr Mnih", "paper_title": "Human-level control through deep reinforcement learning", "publication_date": "2015-01-01", "reason": "This groundbreaking paper demonstrated the effectiveness of deep reinforcement learning in achieving human-level performance on Atari games, highlighting the importance of addressing non-stationarity in RL optimization."}, {"fullname_first_author": "John Schulman", "paper_title": "Proximal policy optimization algorithms", "publication_date": "2017-01-01", "reason": "This paper introduced the Proximal Policy Optimization (PPO) algorithm, a widely used RL algorithm that effectively tackles non-stationarity through clipped policy updates."}, {"fullname_first_author": "Diederik P Kingma", "paper_title": "Adam: A method for stochastic optimization", "publication_date": "2014-01-01", "reason": "This highly influential paper introduced the Adam optimizer, a widely used algorithm that has shown substantial effectiveness in supervised learning, but faces challenges in non-stationary RL settings."}]}