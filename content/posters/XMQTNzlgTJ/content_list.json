[{"type": "text", "text": "High-probability complexity guarantees for nonconvex minimax problems ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yassine Laguel\u2217   \nLaboratoire Jean Alexandre Dieudonn\u00e9 Universit\u00e9 C\u00f4te d\u2019Azur Nice, France   \nyassine.laguel@univ-cotedazur.fr   \nYasa Syed   \nDepartment of Statistics   \nRutgers University   \nPiscataway, New Jersey, USA   \nyasa.syed@rutgers.edu ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "Necdet Serhat Aybat Department of Industrial Engineering Penn State University University Park, PA, USA nsa10@psu.edu ", "page_idx": 0}, {"type": "text", "text": "Mert G\u00fcrb\u00fczbalaban   \nRutgers Business School   \nRutgers University   \nPiscataway, New Jersey, USA   \nmg1366@rutgers.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Stochastic smooth nonconvex minimax problems are prevalent in machine learning, e.g., GAN training, fair classification, and distributionally robust learning. Stochastic gradient descent ascent (GDA)-type methods are popular in practice due to their simplicity and single-loop nature. However, there is a significant gap between the theory and practice regarding high-probability complexity guarantees for these methods on stochastic nonconvex minimax problems. Existing high-probability bounds for GDA-type single-loop methods only apply to convex/concave minimax problems and to particular non-monotone variational inequality problems under some restrictive assumptions. In this work, we address this gap by providing the first high-probability complexity guarantees for nonconvex/PL minimax problems corresponding to a smooth function that satisfies the PL-condition in the dual variable. Specifically, we show that when the stochastic gradients are light-tailed, the smoothed alternating GDA method can compute an $\\varepsilon$ -stationary point within $\\begin{array}{r}{\\mathcal{O}(\\frac{\\ell\\kappa^{2}\\delta^{2}}{\\varepsilon^{4}}\\,+\\,\\frac{\\kappa}{\\varepsilon^{2}}(\\ell\\,+\\,\\delta^{2}\\log(1/\\bar{q})))}\\end{array}$ stochastic gradient calls with probability at least $1-\\bar{q}$ for any $\\bar{q}~\\in~(0,1)$ , where $\\mu$ is the $\\mathrm{PL}$ constant, $\\ell$ is the Lipschitz constant of the gradient, $\\kappa\\,=\\,\\ell/\\mu$ is the condition number, and $\\delta^{2}$ denotes a bound on the variance of stochastic gradients. We also present numerical results on a nonconvex/PL problem with synthetic data and on distributionally robust optimization problems with real data, illustrating our theoretical findings. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Minimax optimization problems arise frequently in machine learning (ML) applications; indeed, constrained optimization problems such as deep learning with model constraints [24], dictionary learning [10, 57] or matrix completion [28] can be recast as a minimax optimization problem through Lagrangian duality. Other applications include but are not limited to the training of GANs [59], fair learning [72], supervised learning [61, 49, 51, 74], adversarial deep learning [75], game theory [54, 58], robust optimization [4, 3], distributionally robust learning [46, 75, 24], meta-learning [70] and multi-agent reinforcement learning [15]. Many of these applications can be reformulated in ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}^{d_{1}}}\\operatorname*{max}_{y\\in\\mathbb{R}^{d_{2}}}f(x,y),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $f:\\mathbb{R}^{d_{1}}\\times\\mathbb{R}^{d_{2}}\\rightarrow\\mathbb{R}$ is a smooth function, i.e., differentiable with a Lipschitz gradient; $f$ can possibly be nonconvex in $x$ and nonconcave in $y$ . First-order primal-dual (FOPD) methods have been the leading computational approach for computing low-to-medium-accuracy stationary points for these problems because of their cheap iterations and mild dependence of their overall complexities on the problem dimension and data size [9, 8, 36]. In the context of FOPD methods, there are two key settings for (1): ", "page_idx": 1}, {"type": "text", "text": "(i) the deterministic setting, where the partial gradients $\\nabla_{x}f$ and $\\nabla_{y}f$ are exactly available, (ii) the stochastic setting, where we have only access to (inexact) stochastic estimates of the partial gradients, in which case the problem in (1) is called a stochastic minimax problem. ", "page_idx": 1}, {"type": "text", "text": "It can be argued that the stochastic setting is more relevant to modern machine learning applications where gradients are typically estimated randomly from mini-batches of data, or sometimes intentionally perturbed with random noise to ensure data privacy [14, 34, 1]. ", "page_idx": 1}, {"type": "text", "text": "Convex and nonconvex minimax optimization. In the convex case (when $f$ is convex2 in $x$ and concave in $y$ ), several approaches have been considered including Variational Inequalities (VIs) and primal-dual algorithms, see. e.g. [29, 20, 5, 67, 12, 73, 60, 11] and the references therein. One disadvantage of using the VI approach for solving minimax problems (by identifying the signed gradient map $G(x,y){\\triangleq}\\,[\\nabla_{x}f(x,y)^{\\top},-\\nabla_{y}f(x,y)^{\\top}]^{\\top}$ as the corresponding operator in the VI) is that one needs to set the primal and dual stepsize to be the same. This can be restrictive in applications where $f$ exhibits different smoothness properties in the primal $(x)$ and dual $(y)$ block coordinates \u2013this is often the case in distributionally robust learning [73], adversarial learning [43] and in the Lagrangian reformulations of constrained optimization problems that involve many constraints [47]. The gap function $\\mathcal{G}(x_{k},y_{k})\\triangleq\\operatorname*{sup}_{x,y}f(x_{k},y)-f(x,y_{k})$ , and the squared distance to the set of saddle points $\\mathcal{D}(x_{k},y_{k})\\triangleq\\operatorname*{min}\\{\\|x_{k}-x^{\\star}\\|^{2}+\\|y_{k}-y^{\\star}\\|^{2}\\mid(x^{\\star},y^{\\star})$ is a saddle point $\\}$ are standard metrics for assessing the quality of the output ${\\boldsymbol{z}}_{k}=(x_{k},y_{k})$ generated by an FOPD algorithm after $k$ iterations among many others [35, 73, 20]. ", "page_idx": 1}, {"type": "text", "text": "In the nonconvex setting, i.e., when $f$ is nonconvex in $x$ , the aim is to compute a stationary point. Let $\\mathcal{M}(x_{k},y_{k})$ denote a measure for the stationarity of iterates $(x_{k},y_{k})$ ; a common metric is the norm of the gradient, i.e., $\\mathcal{M}(x_{k},y_{k})\\triangleq\\|\\nabla f(x_{k},y_{k})\\|$ and its variants such as $\\|\\nabla\\Phi(\\boldsymbol{x}_{k})\\|$ when $f$ is strongly concave in $y$ , where $\\Phi(\\cdot)=\\operatorname*{max}_{y}f(\\cdot,y)$ denotes the primal function \u2013for other metrics and relation between them, see [62]. There are several algorithms that admit (gradient) complexity guarantees for computing a stationary point of nonconvex minimax problems under various strong concavity, concavity or weak concavity-type assumptions in the $y$ variable $-\\mathbf{S}\\mathbf{e}\\mathbf{e}$ the references in [62]. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we consider smooth nonconvex-PL (NCPL) problems where $f$ is a smooth function such that it is possibly nonconvex in $x$ and it satisfies the Polyak-Lojasiewicz (PL) condition in $y$ . The PL condition is a weaker assumption (milder condition) than strong concavity in $y$ \u2013in fact, PL condition in the dual does not even require quasi-concavity. NCPL problems constitute a rich class of problems arising in many ML applications including but not limited to fair classification [48], robust neural network training with dual regularization [48, eqn. (14)], overparametrized systems and neural networks [42], linear quadratic regulators [17], smoothed Lasso problems [23] subject to constraints, distributionally robust learning with $\\ell_{2}$ regularization in the dual [72], deep AUC maximization [68] and covariance matrix learning with Wasserstein GANs [52]. For deterministic NCPL problems, the alternating gradient descent ascent (AGDA) method and its smoothed version (smoothed AGDA) have the complexity of $\\mathcal{O}(\\kappa^{2}/\\epsilon^{2})$ and ${\\mathcal{O}}(\\kappa/\\epsilon^{2})$ , respectively, for finding a point $(\\tilde{x},\\tilde{y})$ satisfying $\\|\\nabla f(\\tilde{x},\\tilde{y})\\|\\leq\\epsilon$ as shown in [70, 66]. Here $\\kappa\\triangleq\\ell/\\mu$ is the condition number, where $\\ell$ is the Lipschitz constant of the gradient, and $\\mu$ is the $\\mathrm{PL}$ constant. For Catalyst-AGDA, [66] shows also the rate ${\\mathcal{O}}(\\kappa/\\epsilon^{2})$ for deterministic NCPL problems. ", "page_idx": 1}, {"type": "text", "text": "In expectation and high-probability bounds. Most of the existing guarantees in the literature for stochastic FOPD algorithms are provided in expectation, i.e., a bound on the number of iterations $k$ (or the stochastic gradient evaluations) is provided for $\\mathbb{E}[\\mathcal{G}(x_{k},y_{k})]\\le\\varepsilon$ or $\\mathbb{E}[D(x_{k},y_{k})]\\le\\varepsilon$ to hold (see, e.g., [73, 72, 29] and the references therein). Yet having such guarantees on average does not allow to control tail events, i.e., even if $\\mathbb{E}[\\mathcal{G}(x_{k},y_{k})]$ is small, $\\mathcal{G}(x_{k},y_{k})$ can still be arbitrarily large with a non-zero probability. To this end, high-probability guarantees have been considered in the literature [35, 64, 20, 32, 22]. These results allow to control the risk associated with the worst-case tail events as they specify how many iterations would be sufficient to ensure $\\mathcal{G}(x_{k},y_{k})$ is sufficiently small for any given failure probability $\\bar{q}\\in(0,1)$ . To derive high-probability bounds, one common approach involves running the algorithm in parallel multiple times and strategically selecting an optimal output to convert in-expectation bounds into high-probability guarantees [62, 37]. Alternatively, advanced concentration inequalities can be employed under light-tail assumptions to control noise accumulation across iterates without requiring multiple runs [53, 27]. For saddle point problems, we note that existing high-probability bounds mostly apply to the monotone VI setting, or to strongly convex/strongly concave (SCSC) minimax problems. To our knowledge, high-probability guarantees for nonconvex minimax problems are non-existent in the literature, even for nonconvex/strongly concave (NCSC) problems with the exception of trivial loose bounds one can obtain by a standard application of Markov\u2019s inequality (see Remark 13 for details). In particular, we note that the existing VI literature with high-probability bounds on non-monotone operators such as star-co-coercive operators [20, 55], do not apply to NCSC problems.3 ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "New high-probability bounds for NCPL optimization. To address these shortcomings, we focus on developing high-probability guarantees for NCPL problems. Among the existing algorithms in the stochastic NCPL setting [64], stochastic gradient descent ascent (SGDA) methods and their variants are quite popular for ML applications, e.g., training GANs and adversarial learning, as SGDA is easy to implement due to its single-loop structure. Guarantees in expectation for stochastic NCSC problems are well supported by the literature - see [72, 63, 40, 6, 31, 30, 44, 66, 33, 39] and the references therein. To our knowledge, among single-loop methods for NCPL problems, the best guarantees in expectation are given by the smoothed alternating gradient descent ascent (sm-AGDA) method [66], which can compute an almost stationary point $(\\tilde{x},\\tilde{y})$ satisfying $\\mathbb{E}[\\|\\nabla f(\\tilde{x},\\tilde{y})\\|\\,\\leq\\,\\varepsilon$ in $\\mathcal{O}(\\ell\\kappa^{\\bar{2}}\\delta^{2}/\\varepsilon^{4}+\\ell\\kappa/\\varepsilon^{2})$ stochastic gradient calls, where $\\delta^{2}$ is an upper bound on the variance of the stochastic gradients. In this work, we consider the sm-AGDA algorithm, and to our knowledge we provide the first-time high-probability bounds (using a single-loop method that does not resort to restarts and parallel runs) for the minimax problem (1) in the NCSC and NCPL settings. More precisely, we focus on a purely stochastic regime in which data streams over time which renders the use of mini-batch schemes or running the method in parallel impractical; therefore, approaches based on Markov\u2019s inequality [62] are no longer applicable (see also Remark 13). ", "page_idx": 2}, {"type": "text", "text": "Contributions. Our contributions are threefold: ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "\u2022 We present the first high-probability complexity result for the sm-AGDA algorithm in the NCPL setting by building upon a Lyapunov function first introduced in [70] for nonconvex-concave problems. Later, for the same Lyapunov function, state-of-the-art complexity bounds in expectation are provided for the NCPL setting in [66]. In this paper, we derive a novel descent property for this Lyapunov function in the almost sure sense (Theorem 7 and Corollary 8), allowing us to develop useful concentration arguments for it to derive high-probability bounds. Our Lyapunov analysis not only sheds light on the convergence properties of sm-AGDA, but also guides the parameter selection for sm-AGDA. Specifically, we show that sm-AGDA can compute an almost stationary point $(\\tilde{x},\\tilde{y})$ satisfying $\\|\\nabla f(\\tilde{x},\\tilde{y})\\|\\leq\\varepsilon$ with probability $1-\\bar{q}\\in(0,1)$ within \u2113\u03ba\u03b524\u03b42+ \u03b5\u03ba2 \u2113+ \u03b42 log(1/q\u00af)  stochastic gradient calls. The lower complexity bound of $\\Omega\\big(\\frac{1}{\\varepsilon^{2}}+\\frac{1}{\\varepsilon^{4}}\\big)$ for NCSC problems [38, 71] in expectation (see also [63, 72]) suggests that our high-probability bound for sm-AGDA is tight in terms of its dependence on $\\varepsilon$ . Furthermore, to our knowledge, these are the first high-probability guarantees for any algorithm in the NCPL setting. \u2022 Under light-tail (sub-Gaussian) assumption on the gradient noise (Assumption 3), which is common in the literature [35, 32, 19], we develop a new concentration result (Theorem 9) that can be of independent interest. From this concentration inequality, we observe that the cost of strengthening the existing complexity result in expectation to a high-probability one is relatively low, i.e., in the final complexity, the probability parameter $\\bar{q}$ only appears in an additive term that scales with $\\varepsilon^{-2}$ . Consequently, this represents a non-dominant overhead compared to the $\\varepsilon^{-4}$ term already present in state-of-the-art expectation bounds [66]. ", "page_idx": 2}, {"type": "table", "img_path": "XMQTNzlgTJ/tmp/45f27c3411b90e029cbe771336d4a4dd37cd87605a28241efadae3a5e41fbef8.jpg", "table_caption": [], "table_footnote": ["Table 1: Summary of the high-probability bounds for minimax problem classes when the gradient of $f$ is Lipschitz (with parameter $\\ell$ ) and stochastic gradient variance is bounded by $\\delta^{2}$ . The second column reports the complexity (number of calls to stochastic gradient oracle) required to achieve the (stationarity) metric reported in the fourth column to be at most $\\varepsilon$ with probability $1-\\bar{q}\\in(0,1)$ ; $\\tilde{O}(\\cdot)$ ignores some logarithmic terms. Here, $\\mu_{s}$ is the strong convexity constant, $\\mu$ is the PL constant, and $\\kappa\\triangleq\\ell/\\mu$ . Let $G(z)\\triangleq[\\nabla_{x}f(z)^{\\top},-\\nabla_{y}f(z)^{\\top}]^{\\top}$ with $z=(x,y)$ and $\\begin{array}{r}{\\bar{z}_{k}=\\frac{1}{K+1}\\sum_{j=0}^{K}z_{j},\\mathcal{G}(z)\\triangleq\\operatorname*{max}_{\\{\\tilde{z}\\in\\mathcal{Z}\\}}\\langle G(z),\\tilde{z}-z_{*}\\rangle}\\end{array}$ , where $Z$ is the domain of the problem with diameter $D\\in(0,+\\infty]$ , and $\\mathcal{G}_{R}(z)\\triangleq\\operatorname*{max}_{\\{\\substack{z\\in Z:\\|z-z_{*}\\|\\leq R\\}}}\\left\\langle G(z),\\tilde{z}-z_{*}\\right\\rangle$ where $z_{*}=(x_{*}^{\\top},y_{*}^{\\top})^{\\top}$ is a stationary point. The third column reports the minimax problem class. The fifth column indicates whether the results supports nonconvexity, i.e., whether $f$ can be a smooth function nonconvex in $_x$ . \u2020 [64] is a two-loop method. \u2021 Applicable to quasi-strongly monotone $G$ that is star-co-coercive around $z_{*}$ and supports heavy-tailed gradients. \u266dApplicable to quasi-strongly monotone $G$ and supports heavy-tailed gradients. \u25b7Supports proximal steps to handle non-smooth convex penalty. \u266fApplies to monotone $G$ that is star-co-coercive around $z_{\\ast}$ . \u2217Makes a light-tail assumption (Ass. 3). "], "page_idx": 3}, {"type": "text", "text": "\u2022 Third, we provide experiments that illustrate our theoretical results. We first provide an example of an NCPL-game with synthetic data and then focus on distributionally robust optimization problems with real data, illustrating the performance of the sm-AGDA in terms of high-probability guarantees. ", "page_idx": 3}, {"type": "text", "text": "2 Preliminaries and Technical Background ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Stationarity metric. We consider the minimax problem in (1) for $f:\\mathbb{R}^{d_{1}}\\times\\mathbb{R}^{d_{2}}\\rightarrow\\mathbb{R}$ such that $f$ is smooth (Assumption 1) and $f(x,\\cdot)$ satisfies the PL property for all $x\\in R^{d_{1}}$ (Assumption 2); moreover, we also assume that we only have access to unbiased stochastic estimates of $\\nabla f$ such that the stochastic error $G(x,y,\\xi)-\\nabla f(x,y)$ has a light tail (Assumption 3) for any $(x,y)$ , where $G(x,y,\\xi)$ denote the stochastic estimate of $\\nabla f(x,y)$ and $\\xi$ denotes the randomness in the estimator. Our aim is to compute a $(\\varepsilon_{x},\\varepsilon_{y})$ -stationary point $(\\tilde{x},\\tilde{y})$ for (1) such that $\\|\\nabla_{x}f(\\tilde{x},\\tilde{y})\\|\\,\\leq\\,\\varepsilon_{x}$ and $\\|\\nabla_{y}f(\\tilde{x},\\tilde{y})\\|\\leq\\varepsilon_{y}$ . We also call $(\\tilde{x},\\tilde{y})$ an $\\varepsilon$ -stationary point if $\\|\\nabla f(\\tilde{x},\\tilde{y})\\|\\leq\\varepsilon$ . Clearly, whenever $(\\tilde{x},\\tilde{y})$ is $(\\varepsilon_{x},\\varepsilon_{y})$ -stationary, then it is also $\\varepsilon$ -stationary for $\\varepsilon=(\\varepsilon_{x}^{2}+\\varepsilon_{y}^{2})^{1/2}$ . ", "page_idx": 3}, {"type": "text", "text": "Smoothed alternating gradient descent ascent (sm-AGDA): The method can be considered as an inexact proximal point method and was introduced in [70]. More specifically, in each iteration of sm-AGDA, given a proximal center $z_{t}$ and the current iterate $\\left({{x}_{t}},{{y}_{t}}\\right)$ , the method computes the next iterate $(x_{t+1},y_{t+1})$ using a stochastic gradient descent ascent step on a regularized function $\\hat{f}$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{f}(x,y;z_{t})\\triangleq f(x,y)+\\frac{p}{2}\\|x-z_{t}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Following the stochastic alternating gradient descent ascent (stochastic AGDA) steps, the proximal center at iteration $t$ , i.e., $z_{t}$ , is updated as shown in Algorithm 1, where $G_{x}(x_{t},y_{t},\\xi_{t+1}^{x})$ and $G_{y}(x_{t+1},y_{t},\\xi_{t+1}^{y})$ denote conditionally unbiased stochastic estimators of the gradients $\\nabla_{x}f(x_{t},y_{t})$ and $\\nabla_{y}f(x_{t+1},y_{t})$ . Throughout the analysis we assume that $\\nabla f$ is Lipschitz, which is standard in the study of first-order optimization algorithms for smooth minimax problems; see, e.g., [72, 73, 75, 63]. ", "page_idx": 3}, {"type": "text", "text": "Assumption 1. (Lipschitz gradient) For all $(x_{1},y_{1}),(x_{2},y_{2})\\in\\mathbb{R}^{d_{1}}\\times\\mathbb{R}^{d_{2}}$ , there exists $\\ell>0$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{x}f(x_{1},y_{1})-\\nabla_{x}f(x_{2},y_{2})\\|\\leq\\ell(\\|x_{1}-x_{2}\\|+\\|y_{1}-y_{2}\\|)}\\\\ &{\\|\\nabla_{y}f(x_{1},y_{1})-\\nabla_{y}f(x_{2},y_{2})\\|\\leq\\ell(\\|x_{1}-x_{2}\\|+\\|y_{1}-y_{2}\\|).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The following condition, known as Polyak-\u0141ojaciewicz (PL) condition is weaker than assuming strong concavity in $y$ , and does not even necessitate $f$ to be even quasi-concave in the $y$ variable. It holds in many ML applications including those in [48, 48, 42, 17, 23, 72, 68, 52, 66]. ", "page_idx": 3}, {"type": "text", "text": "Assumption 2. $P L$ condition in $y.$ ) For every $\\boldsymbol{x}\\in\\mathbb{R}^{d_{1}}$ , $\\operatorname*{max}_{y\\in\\mathbb{R}^{d_{2}}}f(x,y)$ has a non-empty solution set and a finite optimal value. Moreover, there exists $\\mu>0$ such that: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\|\\nabla_{y}f(x,y)\\|^{2}\\geq2\\mu[\\operatorname*{max}_{y\\in\\mathbb{R}^{d_{2}}}f(x,y)-f(x,y)],\\quad\\forall\\,x\\in\\mathbb{R}^{d_{1}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We assume that we have only access to stochastic estimates $G_{x}(x_{t},\\dot{y}_{t},\\xi_{t+1}^{x})$ and $G_{y}(x_{t+1},y_{t},\\xi_{t+1}^{y})$ of the partial gradients $\\nabla_{y}f(x_{k},y_{k})$ and $\\nabla_{x}f(x_{t+1},y_{t})$ , where \u03bet+1 and \u03bety+1 are random variables defined on a probability space $(\\Omega,\\mathbb{P})$ , i.e., the source of randomness in the gradient estimates. Note that sm-AGDA has Gauss-Seidel updates, i.e., the stochastic estimate of the partial gra", "page_idx": 4}, {"type": "text", "text": "Input: $\\left(x_{0},y_{0},z_{0}\\right)$ , $\\tau_{1},\\tau_{2}>0$ , $\\beta\\in[0,1],p\\geq0$   \nfor $t=0,1,2,\\ldots,T-1$ do $x_{t+1}=x_{t}-\\tau_{1}[G_{x}(x_{t},y_{t},\\xi_{t+1}^{x})+p(x_{t}-z_{t})]$ yt+1 = yt + \u03c42Gy(xt+1, yt, \u03bety+1) zt+1 = zt + \u03b2(xt+1 \u2212zt)   \nend for ", "page_idx": 4}, {"type": "text", "text": "dient $G_{y}(x_{t+1},y_{t},\\xi_{t+1}^{y})$ is evaluated at the updated point $(x_{t+1},y_{t})$ instead of $\\left({{x}_{t}},{{y}_{t}}\\right)$ . To capture the sequential information flow, we next introduce the natural filtrations that represent all the information available before an update: Let $\\xi_{t}^{x}$ and $\\xi_{t}^{y}$ be revealed sequentially in the natural order of the sm-AGDA updates, i.e., $\\xi_{1}^{x}\\to\\xi_{1}^{y}\\to\\xi_{2}^{x}\\to\\bar{\\xi}_{2}^{y}\\to\\xi_{3}^{x}\\to\\cdot\\cdot\\cdot$ , and let $(\\mathcal{F}_{t}^{x})_{t\\geq1}$ and $(\\mathcal{F}_{t}^{y})_{t\\geq1}$ denote the associated filtration4, i.e., let $\\mathcal{F}_{0}^{y}\\triangleq\\{\\varnothing,\\Omega\\}$ , and ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{F}_{t+1}^{x}=\\sigma(\\mathcal{F}_{t}^{y},\\sigma(\\xi_{t+1}^{x})),\\quad\\mathcal{F}_{t+1}^{y}=\\sigma(\\mathcal{F}_{t+1}^{x},\\sigma(\\xi_{t+1}^{y})),\\quad\\forall\\,t\\geq0.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Introducing multiple flitrations to represent the sequential information flow is common in the study of stochastic algorithms with Gauss-Seidel updates \u2013see, e.g., papers on stochastic ADMM, and [7, 69]; and we follow the same approach. Consider the gradient noise (errors) at time $t\\in\\mathbb{N}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Delta_{t}^{x}\\triangleq G_{x}(x_{t},y_{t},\\xi_{t+1}^{x})-\\nabla_{x}f(x_{t},y_{t}),\\quad\\Delta_{t}^{y}\\triangleq G_{y}(x_{t+1},y_{t},\\xi_{t+1}^{y})-\\nabla_{y}f(x_{t+1},y_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Finally, we also assume that the gradient noise is unbiased conditionally on the past information and that it admits a light (sub-Gaussian) tail. ", "page_idx": 4}, {"type": "text", "text": "Assumption 3. (Light tail) For any $t\\geq0$ , there exists scalars $\\delta_{x},\\delta_{y}>0$ such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\Delta_{t}^{x}\\mid\\mathcal{F}_{t}^{y}\\right]=0,\\quad\\mathbb{P}\\left[\\left\\Vert\\Delta_{t}^{x}\\right\\Vert\\ge s\\mid\\mathcal{F}_{t}^{y}\\right]\\le2e^{\\frac{-s^{2}}{2\\delta_{y}^{2}}},}\\\\ &{\\mathbb{E}\\left[\\Delta_{t}^{y}\\mid\\mathcal{F}_{t+1}^{x}\\right]=0,\\quad\\mathbb{P}\\left[\\left\\Vert\\Delta_{t}^{y}\\right\\Vert\\ge s\\mid\\mathcal{F}_{t+1}^{x}\\right]\\le2e^{\\frac{-s^{2}}{2\\delta_{x}^{2}}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "For developing high-probability bounds in the learning context, it is common to assume that gradient estimates are sub-Gaussian [32, 35, 18]. While this assumption may not always hold (see e.g. [25, 56]), it often holds when gradients are estimated via mini-batching, as a consequence of the central limit theorem. It will also hold when the gradient noise is bounded. Additionally, adoption of differential privacy mechanisms within gradient-based schemes [14, 34, 1], to enhance data privacy, results frequently in sub-Gaussian gradient errors. ", "page_idx": 4}, {"type": "text", "text": "3 High-probability bounds for sm-AGDA ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For analyzing sm-AGDA, similar to [66, 70], we consider the following Lyapunov function: ", "page_idx": 4}, {"type": "equation", "text": "$$\nV_{t}\\triangleq V(x_{t},y_{t};z_{t})=\\hat{f}(x_{t},y_{t};z_{t})+2P(z_{t})-2\\Psi(y_{t};z_{t}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $P(z)$ and $\\Psi(\\cdot;z)$ denote the saddle point value and the dual function value, respectively, of the auxiliary problem $\\operatorname*{min}_{x}\\operatorname*{max}_{y}\\hat{f}(x,y;z)$ for any fixed $z$ and $\\hat{f}$ defined in (2), i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Psi(y;z)\\triangleq\\operatorname*{min}_{x\\in\\mathbb{R}^{d_{1}}}\\hat{f}(x,y;z)\\quad\\mathrm{and}\\quad P(z)\\triangleq\\operatorname*{min}_{x\\in\\mathbb{R}^{d_{1}}}\\operatorname*{max}_{y\\in\\mathbb{R}^{d_{2}}}\\hat{f}(x,y;z).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Next, we introduce a natural assumption, commonly made in the literature [66, 65]. Without this assumption, there are pathological cases where primal function $\\Phi(x)$ may be unbounded leading to divergence of gradient-based methods; an example would be $f(x,\\dot{y})^{'}{=}\\,\\dot{-}x^{2}-y^{2}$ in dimension one. ", "page_idx": 4}, {"type": "text", "text": "Assumption 4. Consider the primal function $\\Phi:\\mathbb{R}^{d_{1}}\\rightarrow\\mathbb{R}.$ , i.e., $\\Phi(x)=\\operatorname*{max}_{y\\in\\mathbb{R}^{d_{2}}}f(x,y)$ . There exists $x^{*}\\in\\mathbb{R}^{d_{1}}$ such that $\\begin{array}{r}{\\Phi^{*}\\triangleq\\Phi(x^{*})=\\operatorname*{min}_{x\\in\\mathbb{R}^{d_{1}}}\\Phi(x)}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "Under Assumption 4, it immediately follows that $V_{t}\\geq\\Phi^{*}$ for all $t\\in\\mathbb{N}$ \u2013since $P(z)-\\Psi(y,z)\\ge0$ , $\\hat{f}(x,y;z)-\\Psi(y;z)\\geq0$ and $P(z)\\geq\\Phi^{*}$ for all $x,y,z$ . We will next study the change $V_{t}-V_{t+1}$ in the Lyapunov function and show that an approximate descent property holds. First, we need two key lemmas that characterize the evolution of $\\hat{f}(x_{t},y_{t};z_{t})$ and $\\Psi(y_{t};z_{t})$ over the iterations. ", "page_idx": 5}, {"type": "text", "text": "Lemma 5. Suppose Assumptions 1, 2, 3 and $^{4}$ hold. Consider sm-AGDA given in Alg. 1 with $\\begin{array}{r}{\\tau_{1}\\in(0,\\frac{1}{p+\\ell}]}\\end{array}$ and $\\beta\\in(0,1]$ . For any $t\\in\\mathbb{N}$ , we have: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{f}(x_{t+1},y_{t+1};z_{t+1})-\\hat{f}(x_{t},y_{t};z_{t})\\leq-\\displaystyle\\frac{\\tau_{1}}{2}\\|\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})\\|^{2}+\\tau_{2}\\left(1+\\displaystyle\\frac{\\ell}{2}\\tau_{2}\\right)\\|\\nabla_{y}f(x_{t+1},y_{t})\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\tau_{1}((p+\\ell)\\tau_{1}-1)\\langle\\Delta_{t}^{x},\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})\\rangle+\\displaystyle\\frac{p+\\ell}{2}\\tau_{1}^{2}\\|\\Delta_{t}^{x}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\tau_{2}(1+\\ell\\tau_{2})\\langle\\nabla_{y}f(x_{t+1},y_{t}),\\Delta_{t}^{y}\\rangle-\\displaystyle\\frac{p}{2\\beta}\\|z_{t}-z_{t+1}\\|^{2}+\\displaystyle\\frac{\\ell\\tau_{2}^{2}}{2}\\|\\Delta_{t}^{y}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Proof. The proof is provided in Appendix B.1. ", "page_idx": 5}, {"type": "text", "text": "From Assumption 1, when $p>\\ell$ , the auxilliary function $\\hat{f}(\\cdot,y;z)$ is $(p-\\ell)$ -strongly convex for any fixed $y,z$ ; hence, there is a unique minimizer for every $y,z$ fixed, denoted by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{x^{*}(y,z)\\triangleq\\operatorname{argmin}_{x\\in\\mathbb R^{d_{1}}}\\,\\hat{f}(x,y;z),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "i.e., $\\Psi(y,z)=\\hat{f}(x^{*}(y,z),y;z)$ . In the rest of the paper, we will take $p>\\ell$ and exploit this property.   \nThe following lemma characterizes the change in the dual function $\\Psi$ . ", "page_idx": 5}, {"type": "text", "text": "Lemma 6. Suppose Assumptions 1, 2, 3 and $^{4}$ hold. Consider the sm-AGDA iterate sequence $\\{(x_{t},y_{t},z_{t})\\}_{t\\in\\mathbb{N}}f o r\\,p>\\ell$ . For any $t\\in\\mathbb{N}$ , it holds that ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\jmath(y_{t+1};z_{t+1})-\\Psi(y_{t};z_{t})\\geq\\!\\tau_{2}\\langle\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t}),\\nabla_{y}f(x_{t+1},y_{t})\\rangle+\\tau_{2}\\langle\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t}),\\Delta_{t}^{y}\\rangle}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad-\\cfrac{L\\Psi}{2}\\tau_{2}^{2}\\left(\\|\\nabla_{y}f(x_{t+1},y_{t})\\|^{2}+2\\langle\\nabla_{y}f(x_{t+1},y_{t}),\\Delta_{t}^{y}\\rangle+\\|\\Delta_{t}^{y}\\|^{2}\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad+\\cfrac{p}{2}\\langle z_{t+1}-z_{t},z_{t+1}+z_{t}-2x^{*}(y_{t+1},z_{t+1})\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\begin{array}{r}{L_{\\Psi}\\triangleq\\ell\\left(1+\\frac{p+\\ell}{p-\\ell}\\right)}\\end{array}$ and the map $x^{*}(\\cdot,\\cdot)$ is defined by (11). ", "page_idx": 5}, {"type": "text", "text": "Proof. The proof is provided in Appendix B.2. ", "page_idx": 5}, {"type": "text", "text": "The next result provides an approximate descent property on the Lyapunov function. Its proof builds on Lemmas 5 and 6 and a descent property on the function $P$ (given in Lemma 15 of the Appendix); and leverages smoothness properties of the functions $\\hat{f}$ and $\\Psi$ and the map $(y,z)\\mapsto x^{*}(y,z)$ as well as the strong convexity of $\\hat{f}$ with respect to $x$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 7. Suppose Assumptions 1, 2, 3 and $^{4}$ hold. Consider the sm-AGDA algorithm with parameters $p>\\ell$ , $\\beta\\in(0,1]$ , $\\begin{array}{r}{\\bar{\\tau}_{1}\\in(0,\\frac{1}{p+\\ell}]}\\end{array}$ and $\\tau_{2}>0$ chosen such that ", "page_idx": 5}, {"type": "equation", "text": "$$\nc_{0}\\triangleq-\\tau_{2}^{2}\\ell\\nu+\\tau_{2}\\left(1-\\frac{\\ell}{2}\\tau_{2}-L_{\\Psi}\\tau_{2}\\right)\\geq0,\\quad c_{0}^{\\prime}\\triangleq\\frac{p}{3\\beta}-\\left(\\frac{2p^{2}}{p-\\ell}+48\\beta\\frac{p^{3}}{(p-\\ell)^{2}}\\right)\\geq0,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for some constant $\\nu>0$ , where $\\begin{array}{r}{L_{\\Psi}=\\ell\\left(1+\\frac{p+\\ell}{p-\\ell}\\right)}\\end{array}$ . Then, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{t}-V_{t+1}\\geq c_{1}\\|\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})\\|^{2}+c_{2}\\|\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t})\\|^{2}+c_{3}\\|x_{t}-z_{t}\\|^{2}}\\\\ &{\\qquad\\qquad+c_{4}\\langle\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t}),\\Delta_{t}^{x}\\rangle+\\langle c_{5}\\nabla_{y}f(x_{t},y_{t})+c_{6}\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t}),\\Delta_{t}^{y}\\rangle}\\\\ &{\\qquad\\qquad+c_{7}\\|\\Delta_{t}^{x}\\|^{2}+c_{8}\\|\\Delta_{t}^{y}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "for some constants $\\{c_{i}\\}_{i=1}^{8}\\subset\\mathbb{R}$ that are explicitly given in Appendix $C_{s}$ , which may depend on $\\nu$ , as well as the problem and sm-AGDA parameters that can be chosen such that $c_{1},c_{2},c_{3}>0$ . ", "page_idx": 5}, {"type": "text", "text": "Proof. The proof is given in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "With some specific choice of parameters in sm-AGDA, we can obtain simplifications to the coefficients $\\{c_{i}\\}_{i=1}^{8}$ from Theorem 7 (explicitly given in Appendix C). As such, this yields the following corollary. ", "page_idx": 5}, {"type": "text", "text": "Corollary 8. Under the premise of Theorem 7, let $p=2\\ell$ , $\\begin{array}{r}{\\tau_{1}\\in(0,\\frac1{3\\ell}].}\\end{array}$ , $\\begin{array}{r}{\\tau_{2}\\,=\\,\\frac{\\tau_{1}}{48},\\beta\\,=\\,\\alpha\\mu\\tau_{2}}\\end{array}$ for $\\alpha\\in(0,\\frac{1}{406}]$ $\\begin{array}{r}{\\frac{\\tilde{A}_{t+1}-\\tilde{A}_{t}}{\\tau_{1}}\\leq-\\tilde{B}_{t}+\\tilde{C}_{t+1}+\\tilde{D}_{t+1}}\\end{array}$ $t\\in\\mathbb{N},$ $\\begin{array}{r}{\\nu=\\frac{12}{\\tau_{1}\\ell}}\\end{array}$ $\\tilde{A}_{t}\\triangleq\\tau_{1}V_{t}$ $\\tilde{B}_{t}\\triangleq\\frac{\\tau_{1}}{5}\\|\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})\\|^{2}+\\frac{\\tau_{2}}{8}\\|\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t})\\|^{2}+\\frac{\\beta p}{8}\\|x_{t}-^{*}\\!z_{t}\\|^{2},$ $\\begin{array}{r l}&{\\tilde{C}_{t+1}\\triangleq\\Big[\\Big(192\\beta p\\Big(\\frac{p+\\ell}{p-\\ell}\\Big)^{2}\\ell^{2}\\tau_{2}^{2}+\\frac{4\\ell}{\\nu}+4c_{0}\\ell^{2}+2c_{0}^{\\prime}\\beta^{2}\\Big)\\tau_{1}^{2}+\\Big((p+\\ell)\\tau_{1}-1\\Big)\\tau_{1}\\Big]\\langle\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t}),\\Delta_{t}^{x}\\rangle}\\\\ &{\\qquad\\qquad+\\tau_{2}\\langle(1+\\ell\\tau_{2}+2L_{\\Psi}\\tau_{2})\\nabla_{y}f(x_{t},y_{t})-2\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t}),\\Delta_{t}^{y}\\rangle,}\\\\ &{\\tilde{D}_{t+1}\\triangleq2\\ell\\tau_{1}^{2}\\|\\Delta_{t}^{x}\\|^{2}+8\\ell\\tau_{2}^{2}\\|\\Delta_{t}^{y}\\|^{2}.}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "Proof. The proof is given in Appendix D ", "page_idx": 6}, {"type": "text", "text": "Next, we provide a concentration inequality which will be key to obtain our high-probability bounds. Theorem 9. Let $\\{\\mathcal{F}_{t}\\}_{t\\in\\mathbb{N}}$ be a flitration on $\\textstyle(\\Omega,{\\mathcal{F}},\\mathbb{P})$ . Let $A_{t},B_{t},C_{t},D_{t}$ be four stochastic processes adapted to the filtration such that there exist $\\sigma_{C},\\sigma_{D}>0$ and $\\tau_{1}>0$ such that for all $t\\in\\mathbb{N}$ : (i) $B_{t}~\\geq~0,$ , (i $\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\$ for all $\\lambda\\:>\\:0,$ $0,\\ (i i i)\\ \\mathbb{E}[e^{\\lambda D_{t+1}}\\ \\ |\\ \\mathcal{F}_{t}]\\ \\leq\\ e^{\\lambda\\sigma_{D}^{2}}$ for all $\\begin{array}{r}{\\lambda\\in\\left[0,\\frac{1}{\\sigma_{D}^{2}}\\right]}\\end{array}$ and $\\begin{array}{r}{(i v)\\ \\frac{A_{t+1}-A_{t}}{\\tau_{1}}\\leq-B_{t}+C_{t+1}+D_{t+1}}\\end{array}$ . Then, for any $\\bar{q}\\in(0,1]$ , we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\frac{\\tau_{1}}{2}\\sum_{t=0}^{T-1}B_{t}\\leq(A_{0}-A_{T})+\\tau_{1}\\sigma_{D}^{2}T+2\\tau_{1}\\operatorname*{max}\\{2\\sigma_{C}^{2},\\sigma_{D}^{2}\\}\\log\\left(\\frac{1}{\\bar{q}}\\right)\\right)\\geq1-\\bar{q}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Proof. The proof is provided in Appendix E. ", "page_idx": 6}, {"type": "text", "text": "Remark 10. While the above concentration inequality seems tailored to the analysis of sm-AGDA, it can also aid in deriving high probability bounds for many other first-order methods for nonconvex minimax problems that outputs a randomized iterate; indeed, the majority of existing Lyapunov arguments in the nonconvex setting are built upon constructing telescoping sums in line with Theorem 9, e.g., stochastic alternating GDA [66] for NCPL minimax problems and optimistic GDA [45] for strongly convex-strongly concave problems. ", "page_idx": 6}, {"type": "text", "text": "We next present our main result which provides a high-probability bound on the sm-AGDA iterates.   \nThe main idea of the proof is to apply Theorem 9 to the processes introduced in Corollary 8. ", "page_idx": 6}, {"type": "text", "text": "Theorem 11. In the premise of Corollary 8, sm-AGDA iterates $\\left({{x}_{t}},{{y}_{t}}\\right)$ for $\\begin{array}{r}{\\tau_{1}\\leq\\frac{1}{3\\ell}}\\end{array}$ satisfy ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Bigg(\\frac{1}{T}\\sum_{t=0}^{T-1}\\big[\\|\\nabla_{x}f(x_{t},y_{t})\\|^{2}+\\kappa\\|\\nabla_{y}f(x_{t},y_{t})\\|^{2}\\big]\\leq\\mathcal{Q}_{\\bar{q},T,\\cdot}\\Bigg)\\geq1-\\bar{q},\\quad\\forall\\,T\\in\\mathbb{N},\\quad\\forall\\,\\bar{q}\\in(0,1],\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "$\\begin{array}{r}{Q_{\\bar{q},T}=\\mathcal{O}\\Big(\\frac{\\kappa(\\Delta_{0}+b_{0})}{\\tau_{1}T}+\\kappa(\\delta_{x}^{2}+\\delta_{y}^{2})\\Big(\\tau_{1}\\ell+\\frac{1}{T}\\log\\Big(\\frac{1}{\\bar{q}}\\Big)\\Big)\\Big)}\\end{array}$ explicitly stated in Appendix $F$ , where \u22060 \u225c\u03a6(z0) \u2212\u03a6\u2217, $b_{0}\\triangleq2\\operatorname*{sup}_{x,y}\\{\\hat{f}(x_{0},y;z_{0})-\\hat{f}(x,y_{0};z_{0})\\}.$ . ", "page_idx": 6}, {"type": "text", "text": "Proof Sketch. Let the stochastic processes $A_{t},B_{t},C_{t},D_{t}$ in Theorem 9 be chosen as $A_{t}\\;=\\;\\tilde{A}_{t}$ , $B_{t}=\\tilde{B}_{t}$ , $C_{t}=\\tilde{C}_{t}$ , $D_{t}=\\tilde{D}_{t}$ where $\\tilde{A}_{t},\\tilde{B}_{t},\\tilde{C}_{t},\\tilde{D}_{t}$ are defined in Corollary 8 and $\\tau_{1}>0$ be the primal stepsize in sm-AGDA; according to Corollary 8, we have At+\u03c411\u2212At \u2264\u2212Bt + Ct+1 + Dt+1 for $t\\,\\in\\,\\mathbb{N}$ . Since $\\Delta_{t}^{x}$ and $\\Delta_{t}^{y}$ admit sub-Gaussian tails, it can be shown that the conditions of Theorem 9 are satisfied for some appropriate constants $\\sigma_{C}^{2}$ and $\\sigma_{D}^{2}$ . Therefore, Theorem 9 implies a tail bound on $\\sum_{t=0}^{T-1}\\tilde{B}_{t}$ . Using the relation between $f$ and $\\hat{f}$ , one can also show that $\\|\\nabla_{x}f(x_{t},y_{t})\\|^{2}+$ $\\kappa\\|\\nabla_{y}f(x_{t},y_{t})\\|^{2}=\\mathcal{O}(\\tilde{B}_{t})$ , for all $t\\in\\mathbb{N}$ . This last inequality allows to translate the tail bound for $\\sum_{t=0}^{T-1}\\tilde{B}_{t}$ to a tail bound for $\\begin{array}{r}{\\sum_{t=0}^{T-1}\\|\\nabla_{x}f(x_{t},y_{t})\\|^{2}+\\kappa\\|\\nabla_{y}f(x_{t},y_{t})\\|^{2}}\\end{array}$ . The details of the proof is provided in Appendix of th e supplementary material. \u53e3 ", "page_idx": 6}, {"type": "text", "text": "Remark 12. Suppose sm-AGDA, given in Alg. 1, is run for $T$ iterations, and it outputs a randomly selected iterate $\\bar{(x_{U},y_{U})}$ , where the random iteration index $U$ is chosen uniformly at random from the set $\\{0,1,\\ldots,T-1\\}$ , i.e., $\\mathbb{P}(U=t)=1/T$ for $t=0,1,\\ldots,T-1$ . Theorem $^ \u1e0a I I \u1e0c$ implies that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\bigg(\\|\\nabla_{x}f(x_{U},y_{U})\\|^{2}+\\kappa\\|\\nabla_{y}f(x_{U},y_{U})\\|^{2}\\leq\\mathcal{Q}_{\\bar{q},T}\\bigg)\\geq1-\\bar{q}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Furthermore, in comparison with existing complexity bounds in expectation for sm-AGDA [66], our quantile bound requires only an overhead of order ${\\mathcal{O}}(\\varepsilon^{-2}\\log(1/{\\bar{q}}))$ . Unless $\\bar{q}$ is very small, this is typically negligible in comparison to the $O(\\varepsilon^{-4})$ already present in rates in expectation. ", "page_idx": 6}, {"type": "text", "text": "Remark 13. In contrast to high-probability bounds derived from standard Markov-type arguments, our approach achieves significantly better scaling with respect to both $\\bar{q}$ and $\\epsilon$ . Specifically, consider an oracle that can generate a sample $(\\hat{x},\\hat{y})$ with $\\mathbb{E}[\\|\\nabla f(\\bar{\\hat{x}},\\hat{y})\\|]\\leq\\epsilon$ after $\\mathcal{G}(\\epsilon)$ iterations/stochastic samples. In particular, [66] shows that one can take $\\begin{array}{r}{\\mathcal{G}(\\epsilon)\\,=\\,\\mathcal{O}\\left(\\frac{\\ell\\kappa^{2}\\delta^{2}}{\\epsilon^{4}}+\\frac{\\kappa\\ell}{\\epsilon^{2}}\\right)}\\end{array}$ for the sm-AGDA algorithm assuming the variance of the stochastic gradient is bounded by $\\delta^{2}$ . A naive high-probability bound could be constructed by ensuring $\\mathbb{E}[\\|\\nabla f({\\hat{x}},{\\hat{y}})\\|]\\leq{\\bar{q}}\\cdot\\epsilon$ and applying Markov\u2019s inequality to yield an $\\epsilon$ -stationary point with probability at least $1-\\bar{q}$ . However, this approach results in $a$ complexity bound of $\\begin{array}{r}{\\mathcal{G}(\\bar{q}\\epsilon)=O\\left(\\frac{\\ell\\kappa^{2}\\delta^{2}}{\\bar{q}^{4}\\epsilon^{4}}+\\frac{\\kappa\\ell}{\\bar{q}^{2}\\epsilon^{2}}\\right)}\\end{array}$ , leading to a significantly worse dependence on $\\bar{q}$ than ours. Alternatively, following the rationale in [19, 62], to generate a high-probability bound, one can run the sm-AGDA algorithm $m=\\Omega(\\log(1/\\bar{q}))$ times in parallel; where in each run we generate an $\\varepsilon/2$ -solution and among the solutions, we select the one with the smallest estimated gradient norm. This would require $\\begin{array}{r}{m\\mathcal{G}(\\epsilon)=\\mathcal{O}\\left(\\log(1/\\bar{q})\\frac{\\ell\\kappa^{2}\\delta^{2}}{\\epsilon^{4}}+\\log(1/\\bar{q})\\frac{\\kappa\\ell}{\\epsilon^{2}}\\right)}\\end{array}$ iterations/stochastic samples. In this approach, the logarithmic term $\\log\\left({\\frac{1}{\\bar{q}}}\\right)$ multiplies the high-order $\\mathcal{O}\\big(\\frac{1}{\\varepsilon^{4}}\\big)$ term, whereas in our approach it only affects the second-order $\\textstyle{\\mathcal{O}}({\\frac{1}{\\varepsilon^{2}}})$ term. Therefore, our results scale better with respect to $\\bar{q}$ and $\\varepsilon$ . In addition, such $a$ (multiple) parallel run approach, is often impractical in streaming/online settings, where data arrives sequentially, and real-time processing is essential. ", "page_idx": 7}, {"type": "text", "text": "Corollary 14. Under the premise of Theorem $_{l l}$ , consider running the sm-AGDA method for some fixed number of iterations $T\\;\\in\\;\\mathbb{N}$ with parameters chosen as $\\begin{array}{r}{\\tau_{1}=\\operatorname*{min}\\left(\\frac{1}{3\\ell},\\frac{48\\sqrt{\\Delta_{0}+b_{0}}}{\\sqrt{T\\ell\\delta^{2}}}\\right)}\\end{array}$ and $\\tau_{2}=\\tau_{1}/48$ where $\\delta^{2}\\triangleq\\delta_{x}^{2}+\\delta_{y}^{2}$ . Then, for any $\\bar{q}\\in(0,1)$ , sm-AGDA can compute an $(\\varepsilon,\\varepsilon/\\sqrt{\\kappa})$ stationarity point with probability at least $1\\,-\\,{\\bar{q}}$ when the number of iterations $T$ is fixed to $\\begin{array}{r}{T_{\\varepsilon,\\bar{q}}=\\mathcal{O}\\Big(\\frac{(\\bar{\\Delta}_{0}+b_{0})\\ell\\kappa}{\\varepsilon^{2}}+\\frac{\\bar{\\delta}^{2}\\log\\left(\\frac{1}{\\bar{q}}\\right)\\kappa^{'}}{\\varepsilon^{2}}+\\frac{\\delta^{2}(\\Delta_{0}+b_{0})\\ell\\kappa^{2}}{\\varepsilon^{4}}\\Big)}\\end{array}$ which requires $T_{\\varepsilon,\\bar{q}}$ stochastic gradient calls. Proof. This is a direct consequence of Theorem 11, a proof is provided in Appendix G. ", "page_idx": 7}, {"type": "text", "text": "4 Numerical Illustrations ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we illustrate the performance of sm-AGDA. We consider an NCPL problem with synthetic data, as well as a nonconvex DRO problem using real datasets. For synthetic experiments, we used an ASUS Laptop model Q540VJ with 13th Generation Intel Core i9-13900H using 16GB RAM and 1TB SSD hard drive. For the DRO experiments, we used a high-performance computing cluster with automatic GPU selection (NVIDIA RTX 3050, RTX 3090, A100, or Tesla P100) based on GPU availability, ensuring optimal use of computational resources. ", "page_idx": 7}, {"type": "text", "text": "Synthetic experiments on an NCPL game. We consider the following NCPL problem: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{x\\in\\mathbb{R}^{d_{1}}}\\operatorname*{min}_{y\\in\\mathbb{R}^{d_{2}}}m_{1}\\left[\\|x\\|^{2}+\\sin\\Bigl(3\\sqrt{\\|x\\|^{2}+1}\\Bigr)\\right]+x^{\\top}K y-m_{2}\\left[\\|y\\|^{2}+3\\sin^{2}(\\|y\\|)\\right],\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "which can be interpreted as a game between two players [48, 35] where $m_{1}$ $,m_{2}>0$ are constants and the symmetric matrix $K$ is set randomly, similar to the standard bilinear game setting considered in [35]. More specifically, we set $K=10\\tilde{K}/\\|\\tilde{K}\\|$ , $\\tilde{K}=(M\\!+M^{\\top})/2$ where $M$ is a $d\\times d$ matrix with entries being i.i.d centered Gaussian having variance $\\sigma^{2}$ . This problem is nonconvex in $x$ (without satisfying the PL condition in $x$ ). Though the exact gradient is known, we consider a stochastic gradient oracle, which returns noisy gradients similar to the setting of [35, 11, 16, 2], i.e., for each iteration $t\\;\\in\\;\\{0,...,T-1\\}$ , $G_{x}(\\overbar{x_{t}^{}},y_{t};\\xi_{t+1}^{x})\\,=\\,\\nabla_{x}f(x_{t},y_{t})\\,+\\,\\overbar{\\xi}_{t+1}^{x}$ and $G_{y}(x_{t+1},y_{t},\\xi_{t+1}^{y})\\;=$ $\\nabla_{y}f(x_{t+1},y_{t})+\\xi_{t+1}^{y}$ , with $(\\xi_{t+1}^{x})_{t\\geq0}\\overset{i i d}{\\sim}\\mathcal{N}(\\mathbf{0},\\delta^{2}I_{d_{1}})$ and $(\\xi_{t+1}^{y})_{t\\geq0}\\overset{i i d}{\\sim}\\mathcal{N}(\\mathbf{0},\\delta^{2}I_{d_{2}})$ where $I_{d}$ is the $d\\times d$ identity matrix and $\\delta^{2}$ is some constant variance. This setting satisfies all our assumptions, and our high-probability results (Theorem 11 and Coro. 14) are applicable. In this experiment, we fix $d_{1}=d_{2}=30$ , and ${m_{1}}^{\\cdot}\\mathrm{=}\\,m_{2}=\\delta^{2}=\\sigma^{2}=1$ . The solution to this problem is $(x^{*},\\Bar{y}^{*})=(\\mathbf{0},\\mathbf{0})$ . ", "page_idx": 7}, {"type": "text", "text": "Experimental results. The parameters of the problem are explicitly available as $\\mu\\,=\\,2m_{2}$ , and $\\ell\\;=\\;\\operatorname*{max}\\{12m_{1},8m_{2},\\lvert\\lvert K\\rvert\\rvert\\}$ . To illustrate Theorem 11, we set 1\u03c4620\u00b50, \u03c42 = 4\u03c418, p = 2\u2113 and we considered two cases: $\\begin{array}{r}{\\tau_{1}\\,=\\,\\frac{1}{3\\ell}}\\end{array}$ (long step) and $\\begin{array}{r}{\\tau_{1}\\ =\\ \\frac{1}{12\\ell}}\\end{array}$ (short step) to explore the behavior of sm-AGDA for different stepsizes. We generated $N=25$ sample paths for iterations, and on the left panel of Fig. 1, for each iteration $t$ , we report the average of $\\mathcal{M}_{\\kappa}(t)\\triangleq$ $\\|\\nabla_{x}f(x_{t},y_{t})\\|^{2}+\\kappa\\|\\nabla_{y}\\bar{f}(x_{t},y_{t})\\|^{2}$ over $N\\,=\\,25$ realizations corresponding to different sample paths, and the shaded region depicts the range statistic, i.e., for every fixed iteration $t$ , we shade the vertical line between the maximum and minimum values of $\\mathcal{M}_{\\kappa}(t)$ for the 25 paths. On the right panel of Fig. 1, we also report the squared distance, $\\mathcal{Z}(t)\\triangleq\\|x_{t}-x^{*}\\|^{2}+\\|y_{t}-y^{*}\\|^{2}$ , in a similar manner. We observe that the range statistic for $\\mathcal{M}_{\\kappa}(t)$ diminishes to a value inversely proportional to $T$ as $t\\rightarrow T$ ; this is inline with our theoretical results in Theorem 11. The existence of sinusoidal terms in the minimax objective is a source for oscillatory behavior in these figures. As $t\\rightarrow T$ , both $\\mathcal{M}_{\\kappa}(t)$ and $\\mathcal{T}(t)$ exhibit oscillations, of which amplitude are smaller for the small step size compared to the large step-size \u2013at the expense of a slower convergence; the iterate paths are not as oscillatory. ", "page_idx": 7}, {"type": "image", "img_path": "XMQTNzlgTJ/tmp/c37b349b86b107d1ef54c6bc479769896aa138938da8c795f1cb24d6c951a0ad.jpg", "img_caption": ["Figure 1: NCPL game with $\\begin{array}{r}{\\tau_{1}=\\frac{1}{3\\ell}}\\end{array}$ (long step) and $\\begin{array}{r}{\\tau_{1}=\\frac{1}{12\\ell}}\\end{array}$ (short step). (Left) Average of $\\mathcal{M}_{k}(t)$ over 25 sample paths vs. iterations $^{t}$ . (Right) Average of $\\mathcal{T}(t)$ over 25 sample paths vs. iterations $t$ . In both plots, shaded regions depict the range statistic over 25 sample paths. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "For $T\\ \\ =\\ \\ 10,000$ and $\\begin{array}{r l r}{\\tau_{1}}&{{}=}&{\\frac{1}{3\\ell}}\\end{array}$ fixed, we next consider the path averages $\\begin{array}{r}{X_{T}\\triangleq\\frac{1}{T}\\sum_{t=0}^{T-1}\\mathcal{M}_{\\kappa}(t)}\\end{array}$ . Indeed, based on 1000 sample paths, each for $T=10$ , 000 iterations, we compare the empirical quantiles of $X_{T}$ with the theoretical upper bound $\\mathcal{Q}_{q,T}$ on its quantiles (implied by Theorem 11). Figure 2 shows the cumulative distribution function (CDF) of the empirical distribution alongside the theoretical explicit upper bound $\\mathcal{Q}_{q,T}$ for the stationarity measure $\\|\\nabla_{x}f(x_{U},y_{U})\\,+\\,\\kappa\\nabla_{y}f(x_{U},y_{U})\\|$ with $U$ ", "page_idx": 8}, {"type": "image", "img_path": "XMQTNzlgTJ/tmp/cd5875fa288206feb0690089d7015ba3c6eeb98cf639ad1cebcf2f674b23ef74.jpg", "img_caption": ["Figure 2: Comparison of our theoretical upper bound in Theorem 11 and the empirical stationarity measure of sm-AGDA for Problem (13). Results are reported as cumulative distribution functions. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "uniform over $\\{1,\\ldots,T\\}$ . The details of the empirical quantile estimation are provided Appendix I due to space considerations. We observe that the empirical CDF has a sigmoid-like shape, while the theoretical quantiles that lie above display a concave form. This difference may arise because the theoretical quantile bounds $\\mathcal{Q}_{q,T}$ are designed to capture the worst-case behavior across the class of NCPL problems, whereas this specific NCPL example may not represent the worst-case scenario. ", "page_idx": 8}, {"type": "text", "text": "Distributionally Robust nonconvex Logistic Regression. We consider the DRO problem ", "text_level": 1, "page_idx": 8}, {"type": "image", "img_path": "XMQTNzlgTJ/tmp/15a7d5e1137e6d2f499311822d54dda29dc45aa7c385133b1f8ab40aed0fe47e.jpg", "img_caption": ["Figure 3: Histograms for the stationarity measure $\\log_{10}\\|\\nabla f(x_{t},y_{t})\\|^{2}$ for sm-AGDA and baseline algorithms $\\mathrm{(SAPD+}$ , SMDA, and SMDAVR) on a9a, gisette, and sido0 datasets. Each algorithm is run 200 times. First row reports histograms after 20 epochs, and second row reports histograms at the end of training. "], "img_footnote": [], "page_idx": 8}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{x\\in\\mathbb{R}^{d_{1}}}\\operatorname*{max}_{y\\in Y}\\Big(\\frac{1}{d_{2}}\\sum_{j=1}^{d_{2}}y_{j}\\ell_{j}(x;a_{j},b_{j})+r(x)-g(y)\\Big),\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where $\\ell_{j}(x;a_{j},b_{j})=\\log\\!\\left(1+\\exp\\!\\left(-b_{j}\\mathbf{a}_{j}^{\\top}\\mathbf{x}\\right)\\right)$ denotes the logistic loss tied to an input-output pair (aj, bj) \u2208Rd1 \u00d7 {\u22121, 1}, and r(x) = \u03bb1 id1=11+\u03c9x\u03c9ix2 a primal regularization for the learning model $\\boldsymbol{x}\\in\\mathbb{R}^{d_{1}}$ . We allow the distribution $\\boldsymbol{y}\\in Y\\triangleq\\{\\boldsymbol{y}\\in\\mathbb{R}^{d_{2}}:\\;\\boldsymbol{y}\\geq0,\\boldsymbol{\\mathbf{1}}^{\\top}\\boldsymbol{y}=1\\}$ to deviate from the uniform distribution $\\begin{array}{r}{u\\,\\triangleq\\,\\frac{1}{d_{2}}{\\bf1}}\\end{array}$ where 1 denotes the vector of ones, and we penalize the distance between $y$ and $u$ through the regularization map $\\begin{array}{r}{g:y\\mapsto\\frac{\\lambda_{2}d_{2}}{2}\\|y-u\\|^{2}}\\end{array}$ \u03bb22d 2\u2225y \u2212u\u22252. We set the regularization parameters as $\\omega=10$ , $\\lambda_{1}=10e^{-4}$ , and $\\lambda_{2}=1$ . Since $r$ is nonconvex with a Lipschitz gradient and $g$ is strongly convex, this is an NCSC problem. ", "page_idx": 9}, {"type": "text", "text": "Datasets, Algorithms and Hyperparameters. We consider three standard datasets for this problem, which are summarized as follows: The sido0 dataset [50] has $d_{1}=4932$ and $d_{2}=12678$ . The gisette dataset [26] has $d_{1}=5000$ and $d_{2}=6000$ . Finally, the $a9a$ dataset [13] has $d_{1}=123$ and $d_{2}=32561$ . We compare the performances of sm-AGDA against two other baselines that achieve state-of-the-art performance in expectation for these datasets [72]. Specifically, we evaluate $\\mathtt{S A P D+}$ , which is a two-loop method where the subproblems are solved by the SAPD algorithm [73], and SMDAVR, a variance reduced extension of SMDA algorithm [31]. Since (14) is constrained, we augment sm-AGDA with a projection step in the update of the y variable onto the $d_{2}$ -dimensional simplex and adopt the analogous stationarity metric $\\|\\nabla_{x}f(x_{t},y_{t})\\|^{2}+\\|P_{Y}\\nabla_{y}f(x_{t},y_{t})\\|^{2}$ for constrained problems where $P_{Y}$ is a projection to the dual domain $Y$ . For all datasets, the primal stepsize $\\tau_{1}$ of sm-AGDA is tuned via a grid-search over $\\{10^{-k},1\\leq k\\leq4\\}$ . The dual stepsize $\\tau_{2}$ is set as $\\begin{array}{r}{\\tau_{2}=\\frac{\\tau_{1}}{48}}\\end{array}$ . Similarly, $\\beta$ is estimated through a grid-search over $\\{10^{-k},3\\leq k\\leq5\\}$ . The parameter $p$ is also tuned similarly on a grid, our code is provided as a supplementary document for the details. For other methods, our hyperparameters are tuned in accordance with [72]. ", "page_idx": 9}, {"type": "text", "text": "Experimental results. In Figure 2, we plot histograms of our stationarity metric, across 200 runs in a logarithmic scale. We report the stationarity measure both in early phase of the training (i.e. $t=20$ epochs), and in later phases (i.e. $t\\,=\\,550$ epochs for gisette and $t\\,=\\,250$ epochs for a9a and sido0). Our theoretical results are presented for unconstrained problems in the dual, therefore they are not directly applicable to the DRO problem where the dual domain is constrained. That being said, we observe that they are still predictive of performance in the DRO setting. More specifically, Figure 2 is supportive of our high-probability complexity bounds for sm-AGDA, in the sense that the distribution of the stationarity metric for sm-AGDA tends to concentrate. Notably, it outperforms the concentration behaviour of the other baselines. Furthermore, we observe that histograms for all baselines hardly evolve after 20 epochs. This is consistent with previous experiments carried on these datasets [72] where performance was measured in terms of the decay of the average loss and its standard deviation. As such, we conclude that sm-AGDA performs better both in the early phase and the later stage. In our experience, we observed sm-AGDA could accomodate larger stepsizes compared to the other algorithms, which may have contributed to its good performance. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Existing high-probability bounds only apply to convex/concave minimax problems or non-monotone variational inequality problems under restrictive assumptions to our knowledge. We close this gap by providing the first high-probability complexity guarantees for nonconvex/PL minimax problems satisfying the PL-condition in the dual variable for the sm-AGDA method. We also provide numerical results for an NCPL example and for nonconvex distributionally robust logistic regression. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgements ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Yassine Laguel, Yasa Syed and Mert G\u00fcrb\u00fczbalaban\u2019s research are supported in part by the grants Office of Naval Research Award Numbers N00014-21-1-2244 and N00014-24-1-2628, National Science Foundation (NSF) CCF-1814888 and NSF DMS-2053485. Necdet Serhat Aybat\u2019s work was supported in part by the Office of Naval Research Awards N00014-21-1-2271 and N00014-24-1-2666. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Jason Altschuler and Kunal Talwar. Privacy of noisy stochastic gradient descent: More iterations without more privacy loss. Advances in Neural Information Processing Systems, 35:3788\u20133800, 2022.   \n[2] Necdet Serhat Aybat, Alireza Fallah, Mert Gurbuzbalaban, and Asuman Ozdaglar. Robust accelerated gradient methods for smooth strongly convex functions. SIAM Journal on Optimization, 30(1):717\u2013751, 2020.   \n[3] Amir Beck and Aharon Ben-Tal. Duality in robust optimization: primal worst equals dual best. Operations Research Letters, 37(1):1\u20136, 2009.   \n[4] Aharon Ben-Tal, Laurent El Ghaoui, and Arkadi Nemirovski. Robust Optimization, volume 28. Princeton University Press, 2009.   \n[5] Aleksandr Beznosikov, Boris Polyak, Eduard Gorbunov, Dmitry Kovalev, and Alexander Gasnikov. Smooth monotone stochastic variational inequalities and saddle point problems: A survey. European Mathematical Society Magazine, (127):15\u201328, 2023.   \n[6] Radu Ioan Bo\u00b8t and Axel B\u00f6hm. Alternating proximal-gradient steps for (stochastic) nonconvexconcave minimax problems. arXiv preprint arXiv:2007.13605, 2020.   \n[7] Radu Ioan Bot\u00b8, Panayotis Mertikopoulos, Mathias Staudigl, and Phan Tu Vuong. Minibatch forward-backward-forward methods for solving stochastic variational inequalities. Stochastic Systems, 11(2):112\u2013139, 2021.   \n[8] L\u00e9on Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMPSTAT\u20192010, pages 177\u2013186. Springer, 2010.   \n[9] L\u00e9on Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. SIAM Review, 60(2):223\u2013311, 2018.   \n[10] Nicolas Boumal and Pierre-antoine Absil. Rtrmc: A Riemannian trust-region method for low-rank matrix completion. Advances in Neural Information Processing Systems, 24, 2011.   \n[11] Bugra Can, Mert Gurbuzbalaban, and Lingjiong Zhu. Accelerated linear convergence of stochastic momentum methods in Wasserstein distances. In International Conference on Machine Learning, pages 891\u2013901. PMLR, 2019.   \n[12] Antonin Chambolle and Thomas Pock. A first-order primal-dual algorithm for convex problems with applications to imaging. Journal of mathematical imaging and vision, 40:120\u2013145, 2011.   \n[13] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a library for support vector machines. ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):1\u201327, 2011.   \n[14] Rishav Chourasia, Jiayuan Ye, and Reza Shokri. Differential privacy dynamics of Langevin diffusion and noisy gradient descent. Advances in Neural Information Processing Systems, 34:14771\u201314781, 2021.   \n[15] Constantinos Daskalakis, Dylan J Foster, and Noah Golowich. Independent policy gradient methods for competitive reinforcement learning. Advances in Neural Information Processing Systems, 33:5527\u20135540, 2020.   \n[16] Alireza Fallah, Asuman Ozdaglar, and Sarath Pattathil. An optimal multistage stochastic gradient method for minimax problems. In 2020 59th IEEE Conference on Decision and Control (CDC), pages 3573\u20133579. IEEE, 2020.   \n[17] Maryam Fazel, Rong Ge, Sham Kakade, and Mehran Mesbahi. Global convergence of policy gradient methods for the linear quadratic regulator. In International Conference on Machine Learning, pages 1467\u20131476. PMLR, 2018.   \n[18] Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization I: A generic algorithmic framework. SIAM Journal on Optimization, 22(4):1469\u20131492, 2012.   \n[19] Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341\u20132368, 2013.   \n[20] Eduard Gorbunov, Marina Danilova, David Dobre, Pavel Dvurechenskii, Alexander Gasnikov, and Gauthier Gidel. Clipped stochastic methods for variational inequalities with heavy-tailed noise. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 31319\u201331332, 2022.   \n[21] Eduard Gorbunov, Nicolas Loizou, and Gauthier Gidel. Extragradient method: $\\mathcal{O}(1/k)$ lastiterate convergence for monotone variational inequalities and connections with cocoercivity. In International Conference on Artificial Intelligence and Statistics, pages 366\u2013402, 2022.   \n[22] Eduard Gorbunov, Abdurakhmon Sadiev, Marina Danilova, Samuel Horv\u00e1th, Gauthier Gidel, Pavel Dvurechensky, Alexander Gasnikov, and Peter Richt\u00e1rik. High-probability convergence for composite and distributed stochastic minimization and variational inequalities with heavytailed noise. arXiv preprint arXiv:2310.01860, 2023.   \n[23] Mert G\u00fcrb\u00fczbalaban, Yuanhan Hu, Umut Simsekli, and Lingjiong Zhu. Cyclic and randomized stepsizes invoke heavier tails in SGD than constant stepsize. Transactions on Machine Learning Research, August 2023.   \n[24] Mert G\u00fcrb\u00fczbalaban, Andrzej Ruszczyn\u00b4ski, and Landi Zhu. A stochastic subgradient method for distributionally robust non-convex and non-smooth learning. Journal of Optimization Theory and Applications, 194(3):1014\u20131041, 2022.   \n[25] Mert G\u00fcrb\u00fczbalaban, Umut Simsekli, and Lingjiong Zhu. The heavy-tail phenomenon in SGD. In International Conference on Machine Learning, pages 3964\u20133975. PMLR, 2021.   \n[26] Isabelle Guyon, Steve Gunn, Asa Ben-Hur, and Gideon Dror. Result analysis of the NIPS2003 feature selection challenge. Advances in Neural Information Processing Systems, 17, 2004.   \n[27] Nicholas JA Harvey, Christopher Liaw, Yaniv Plan, and Sikander Randhawa. Tight analyses for non-smooth stochastic gradient descent. In Conference on Learning Theory, pages 1579\u20131613. PMLR, 2019.   \n[28] Mark Herbster, Stephen Pasteris, and Lisa Tse. Online matrix completion with side information. Advances in Neural Information Processing Systems, 33:20402\u201320414, 2020.   \n[29] Yu-Guan Hsieh, Franck Iutzeler, J\u00e9r\u00f4me Malick, and Panayotis Mertikopoulos. On the convergence of single-call stochastic extra-gradient methods. Advances in Neural Information Processing Systems, 32, 2019.   \n[30] Feihu Huang, Shangqian Gao, Jian Pei, and Heng Huang. Accelerated zeroth-order and firstorder momentum methods from mini to minimax optimization. Journal of Machine Learning Research, 23(36):1\u201370, 2022.   \n[31] Feihu Huang, Xidong Wu, and Heng Huang. Efficient mirror descent ascent methods for nonsmooth minimax problems. Advances in Neural Information Processing Systems, 34, 2021.   \n[32] Anatoli Juditsky, Arkadi Nemirovski, and Claire Tauvel. Solving variational inequalities with stochastic mirror-prox algorithm. Stochastic Systems, 1(1):17\u201358, 2011.   \n[33] Yang Junchi, Xiang Li, and Niao He. Nest your adaptive algorithm for parameter-agnostic nonconvex minimax optimization. In Advances in Neural Information Processing Systems, 2022.   \n[34] Nurdan Kuru, \u00b8S. \u02d9Ilker Birbil, Mert G\u00fcrb\u00fczbalaban, and Sinan Yildirim. Differentially private accelerated optimization algorithms. SIAM Journal on Optimization, 32(2):795\u2013821, 2022.   \n[35] Yassine Laguel, Necdet Serhat Aybat, and Mert G\u00fcrb\u00fczbalaban. High probability and risk-averse guarantees for stochastic saddle point problems. accepted to Journal of Machine Learning (JMLR), arXiv preprint arXiv:2304.00444, 2023.   \n[36] Guanghui Lan. First-order and stochastic optimization methods for machine learning, volume 1. Springer, 2020.   \n[37] Dongyang Li, Haobin Li, and Junyu Zhang. General procedure to provide high-probability guarantees for stochastic saddle point problems. arXiv preprint arXiv:2405.03219, 2024.   \n[38] Haochuan Li, Yi Tian, Jingzhao Zhang, and Ali Jadbabaie. Complexity lower bounds for nonconvex-strongly-concave min-max optimization. Advances in Neural Information Processing Systems, 34:1792\u20131804, 2021.   \n[39] Xiang Li, Junchi Yang, and Niao He. Tiada: A time-scale adaptive algorithm for nonconvex minimax optimization. In The Eleventh International Conference on Learning Representations, https://openreview.net/forum?id=zClyiZ5V6sL, 2023.   \n[40] Tianyi Lin, Chi Jin, and Michael Jordan. On gradient descent ascent for nonconvex-concave minimax problems. In International Conference on Machine Learning, pages 6083\u20136093. PMLR, 2020.   \n[41] Tianyi Lin, Chi Jin, and Michael I Jordan. Near-optimal algorithms for minimax optimization. In Conference on Learning Theory, pages 2738\u20132779. PMLR, 2020.   \n[42] Chaoyue Liu, Libin Zhu, and Mikhail Belkin. Loss landscapes and optimization in overparameterized non-linear systems and neural networks. Applied and Computational Harmonic Analysis, 59:85\u2013116, 2022. Special Issue on Harmonic Analysis and Machine Learning.   \n[43] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.   \n[44] Gabriel Mancino-Ball and Yangyang Xu. Variance-reduced accelerated methods for decentralized stochastic double-regularized nonconvex strongly-concave minimax problems. arXiv preprint arXiv:2307.07113, 2023.   \n[45] Aryan Mokhtari, Asuman E Ozdaglar, and Sarath Pattathil. Convergence rate of $0(1/\\mathrm{k})$ for optimistic gradient and extragradient methods in smooth convex-concave saddle point problems. SIAM Journal on Optimization, 30(4):3230\u20133251, 2020.   \n[46] Hongseok Namkoong and John C Duchi. Stochastic gradient methods for distributionally robust optimization with f-divergences. In Advances in Neural Information Processing Systems, pages 2208\u20132216, 2016.   \n[47] Angelia Nedich and Tatiana Tatarenko. Huber loss-based penalty approach to problems with linear constraints. arXiv preprint arXiv:2311.00874, 2023.   \n[48] Maher Nouiehed, Maziar Sanjabi, Tianjian Huang, Jason D Lee, and Meisam Razaviyayn. Solving a class of non-convex min-max games using iterative first order methods. Advances in Neural Information Processing Systems, 32, 2019.   \n[49] Balamurugan Palaniappan and Francis Bach. Stochastic variance reduction methods for saddlepoint problems. In Advances in Neural Information Processing Systems, pages 1416\u20131424, 2016.   \n[50] DTP AIDS Antiviral Screen program. Sido0: a phamacology dataset. https://www. causality.inf.ethz.ch/data/SIDO.html, 2008.   \n[51] H Rafique, M Liu, Q Lin, and T Yang. Non-convex min\u2013max optimization: provable algorithms and applications in machine learning (2018). arXiv preprint arXiv:1810.02060, 1810.   \n[52] Harish Rajagopal. Multistage step size scheduling for minimax problems. Master\u2019s thesis, ETH Zurich. URL: https://www.research-collection.ethz.ch/handle/20.500. 11850/572991, 2022.   \n[53] Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for strongly convex stochastic optimization. arXiv preprint arXiv:1109.5647, 2011.   \n[54] Meisam Razaviyayn, Tianjian Huang, Songtao Lu, Maher Nouiehed, Maziar Sanjabi, and Mingyi Hong. Nonconvex min-max optimization: Applications, challenges, and recent theoretical advances. IEEE Signal Processing Magazine, 37(5):55\u201366, 2020.   \n[55] Abdurakhmon Sadiev, Marina Danilova, Eduard Gorbunov, Samuel Horv\u00e1th, Gauthier Gidel, Pavel Dvurechensky, Alexander Gasnikov, and Peter Richt\u00e1rik. High-probability bounds for stochastic optimization and variational inequalities: the case of unbounded variance. In International Conference on Machine Learning, pages 29563\u201329648. PMLR, 2023.   \n[56] Umut Simsekli, Levent Sagun, and Mert G\u00fcrb\u00fczbalaban. A tail-index analysis of stochastic gradient noise in deep neural networks. In International Conference on Machine Learning, pages 5827\u20135837. PMLR, 2019.   \n[57] Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere ii: Recovery by riemannian trust-region method. IEEE Transactions on Information Theory, 63(2):885\u2013914, 2017.   \n[58] J v. Neumann. Zur theorie der gesellschaftsspiele. Mathematische Annalen, 100(1):295\u2013320, 1928.   \n[59] Emmanouil-Vasileios Vlatakis-Gkaragkounis, Lampros Flokas, and Georgios Piliouras. Solving min-max optimization with hidden structure via gradient descent ascent. Advances in Neural Information Processing Systems, 34:2373\u20132386, 2021.   \n[60] Yuanhao Wang and Jian Li. Improved algorithms for convex-concave minimax optimization. Advances in Neural Information Processing Systems, 33:4800\u20134810, 2020.   \n[61] Linli Xu, James Neufeld, Bryce Larson, and Dale Schuurmans. Maximum margin clustering. In Advances in Neural Information Processing systems, pages 1537\u20131544, 2005.   \n[62] Qiushui Xu, Xuan Zhang, Necdet Serhat Aybat, and Mert G\u00fcrb\u00fczbalaban. A stochastic gda method with backtracking for solving nonconvex (strongly) concave minimax problems. arXiv preprint arXiv:2403.07806, 2024.   \n[63] Qiushui Xu, Xuan Zhang, Necdet Serhat Aybat, and Mert G\u00fcrb\u00fczbalaban. A Stochastic GDA Method With Backtracking For Solving Nonconvex (Strongly) Concave Minimax Problems. arXiv e-prints, page arXiv:2403.07806, March 2024.   \n[64] Yan Yan, Yi Xu, Qihang Lin, Wei Liu, and Tianbao Yang. Optimal epoch stochastic gradient descent ascent methods for min-max optimization. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 5789\u20135800, 2020.   \n[65] Junchi Yang, Negar Kiyavash, and Niao He. Global convergence and variance reduction for a class of nonconvex-nonconcave minimax problems. Advances in Neural Information Processing Systems, 33:1153\u20131165, 2020.   \n[66] Junchi Yang, Antonio Orvieto, Aurelien Lucchi, and Niao He. Faster single-loop algorithms for minimax optimization without strong concavity. In International Conference on Artificial Intelligence and Statistics, pages 5485\u20135517. PMLR, 2022.   \n[67] TaeHo Yoon and Ernest K Ryu. Accelerated minimax algorithms flock together. arXiv preprint arXiv:2205.11093, 2022.   \n[68] Zhuoning Yuan, Yan Yan, Milan Sonka, and Tianbao Yang. Large-scale robust deep auc maximization: A new surrogate loss and empirical studies on medical image classification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3040\u20133049, 2021.   \n[69] Yuxuan Zeng, Zhiguo Wang, Jianchao Bai, and Xiaojing Shen. An accelerated stochastic ADMM for nonconvex and nonsmooth finite-sum optimization. arXiv preprint arXiv:2306.05899, 2023.   \n[70] Jiawei Zhang, Peijun Xiao, Ruoyu Sun, and Zhiquan Luo. A single-loop smoothed gradient descent-ascent algorithm for nonconvex-concave min-max problems. Advances in Neural Information Processing Systems, 33:7377\u20137389, 2020.   \n[71] Siqi Zhang, Junchi Yang, Crist\u00f3bal Guzm\u00e1n, Negar Kiyavash, and Niao He. The complexity of nonconvex-strongly-concave minimax optimization. In Uncertainty in Artificial Intelligence, pages 482\u2013492. PMLR, 2021.   \n[72] Xuan Zhang, Necdet Serhat Aybat, and Mert G\u00fcrb\u00fczbalaban. SAPD $^+$ : An accelerated stochastic method for nonconvex-concave minimax problems. Advances in Neural Information Processing Systems, 35:21668\u201321681, 2022.   \n[73] Xuan Zhang, Necdet Serhat Aybat, and Mert G\u00fcrb\u00fczbalaban. Robust accelerated primal-dual methods for computing saddle points. SIAM Journal on Optimization, 34(1):1097\u20131130, 2024.   \n[74] Yuchen Zhang and Lin Xiao. Stochastic primal-dual coordinate method for regularized empirical risk minimization. The Journal of Machine Learning Research, 18(1):2939\u20132980, 2017.   \n[75] Landi Zhu, Mert G\u00fcrb\u00fczbalaban, and Andrzej Ruszczy\u00b4nski. Distributionally robust learning with weakly convex losses: Convergence rates and finite-sample guarantees. arXiv preprint arXiv:2301.06619, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "High-probability complexity guarantees for nonconvex minimax problems APPENDIX ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The organization of the Appendix is as follows: ", "page_idx": 15}, {"type": "text", "text": "\u2022 In Section A, we summarize the key notations that are used throughout the main paper and the Appendix.   \n\u2022 In Section B, we provide the proofs of Lemmas from Section 3 that were used for deriving the approximate Lyapunov descent property.   \n\u2022 In Section C, we present the proof of Theorem 7 which provides an approximate descent property in terms of the Lyapunov function for the sm-AGDA iterates.   \n\u2022 In Section D, we provide the proof of Corollary 8, which studies particular choice of parameters within Theorem 7.   \n\u2022 In Section E we present a proof of Theorem 9 which provides a concentration inequality.   \n\u2022 In Section F, we provide the proof of Theorem 11 which yields high-probability results for the sm-AGDA algorithm.   \n\u2022 In Section G, we provide a proof of Corollary 14 that provides a high-probability (iteration) complexity bound for the sm-AGDA iterates.   \n\u2022 In Section H, we present supplementary lemmas that were used to derive the results.   \n\u2022 In Section I, we provide further details about numerical experiments. ", "page_idx": 15}, {"type": "text", "text": "A Notation ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The key notations that will be used throughout the Appendix is as follows: ", "page_idx": 15}, {"type": "text", "text": "\u2022 $\\begin{array}{r}{\\hat{f}(x,y;z)=f(x,y)+\\frac{p}{2}\\|x-z\\|^{2}}\\end{array}$ denotes the auxiliary problem.   \n\u2022 $\\Psi(y;z)=\\operatorname*{min}_{x}\\hat{f}(x,y;z)$ is the dual function of the auxiliary problem.   \n\u2022 $\\Phi(x;z)=\\operatorname*{max}_{y}\\hat{f}(x,y;z)$ is the primal function of the auxiliary problem.   \n\u2022 $\\begin{array}{r}{P(z)=\\operatorname*{min}_{x}\\operatorname*{max}_{y}\\hat{f}(x,y;z)}\\end{array}$ is the optimal value of the primal problem $\\operatorname*{min}_{x}\\Phi(x;z)$   \n\u2022 $x^{*}(y,z)=\\mathrm{argmin}_{x}\\;\\hat{f}(x,y;z)$ for given $y,z$ in the auxiliary function.   \n\u2022 $x^{*}(z)=\\operatorname{argmin}_{x}\\Phi(x;z)$ is the unique optimal solution to the auxiliary primal problem.   \n\u2022 $Y^{*}(z)=\\operatorname{argmax}_{y}\\Psi(y;z)$ is the set of optimal solutions to the auxiliary dual problem.   \n\u2022 $y^{+}(z)=y+\\tau_{2}\\nabla_{y}f(x^{*}(y,z),y)$ denotes an update in $y$ in the direction of the gradient of the dual function, i.e., along the direction $\\nabla\\Psi(y;z)=\\nabla_{y}f(x^{*}(y,z),y)$ .   \n\u2022 $\\hat{G}_{x}(x,y,\\xi;z)\\triangleq G_{x}(x,y,\\xi)+p(x-z)$   \n\u2022 $\\Delta_{t}^{x}=G_{x}(x_{t},y_{t},\\xi_{t+1}^{x})-\\nabla_{x}f(x_{t},y_{t})$ and $\\Delta_{t}^{y}=G_{y}(x_{t+1},y_{t},\\xi_{t+1}^{y})-\\nabla_{y}f(x_{t+1},y_{t})$ denote the gradient error, i.e., the difference between the stochastic estimates of the partial gradients and the exact partial gradients. ", "page_idx": 15}, {"type": "text", "text": "B Proofs of Lemmas from Section 3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Proof of Lemma 5 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Lemma 5. Suppose Assumptions 1, 2 and $^3$ hold. Consider sm-AGDA, stated in Algorithm 1, with $\\begin{array}{r}{\\tau_{1}\\in(0,\\frac{1}{p+\\ell}]}\\end{array}$ and $\\beta\\in(0,1]$ . For any $t\\in\\mathbb{N}$ , we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\hat{f}(x_{t+1},y_{t+1};z_{t+1})-\\hat{f}(x_{t},y_{t};z_{t})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq-\\;\\frac{\\tau_{1}}{2}\\|\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})\\|^{2}+\\tau_{2}\\left(1+\\displaystyle\\frac{\\ell}{2}\\tau_{2}\\right)\\|\\nabla_{y}f(x_{t+1},y_{t})\\|^{2}-\\frac{p}{2\\beta}\\|z_{t}-z_{t+1}\\|^{2}}\\\\ &{\\quad+\\,\\tau_{1}((p+\\ell)\\tau_{1}-1)\\langle\\Delta_{t}^{x},\\,\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})\\rangle+\\tau_{2}(1+\\ell\\tau_{2})\\langle\\Delta_{t}^{y},\\,\\nabla_{y}f(x_{t+1},y_{t})\\rangle}\\\\ &{\\quad+\\,\\displaystyle\\frac{p+\\ell}{2}\\tau_{1}^{2}\\|\\Delta_{t}^{x}\\|^{2}+\\frac{\\ell\\tau_{2}^{2}}{2}\\|\\Delta_{t}^{y}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Since, $\\hat{f}(\\cdot,y;z)$ is $(p+\\ell)$ -smooth, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{f}(x_{t+1},y_{t};z_{t})-\\hat{f}(x_{t},y_{t};z_{t})}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\langle x_{t+1}-x_{t},\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})\\rangle+\\displaystyle\\frac{p+\\ell}{2}\\|x_{t+1}-x_{t}\\|^{2}}\\\\ &{\\quad\\quad\\quad=-\\tau_{1}\\langle\\hat{G}_{x}(x_{t},y_{t};z_{t}),\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})\\rangle+\\displaystyle\\frac{p+\\ell}{2}\\tau_{1}^{2}\\|\\hat{G}_{x}(x_{t},y_{t};z_{t})\\|^{2}.}\\\\ &{\\quad\\quad\\quad=(\\displaystyle\\frac{p+\\ell}{2}\\tau_{1}^{2}-\\tau_{1})\\|\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})\\|^{2}-(\\tau_{1}+(p+\\ell)\\tau_{1}^{2})\\langle\\Delta_{t}^{x},\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})\\rangle}\\\\ &{\\quad\\quad\\quad\\quad+\\displaystyle\\frac{p+\\ell}{2}\\|\\Delta_{t}^{x}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where last equality follows from $\\hat{G}_{x}(x,y_{t};z_{t})=\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})+\\Delta_{t}^{x}$ . Hence, for $\\begin{array}{r}{\\tau_{1}\\leq\\frac{1}{p+\\ell}}\\end{array}$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{f}(x_{t+1},y_{t};z_{t})-\\hat{f}(x_{t},y_{t};z_{t})}\\\\ &{\\qquad\\leq-\\frac{71}{2}\\|\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})\\|^{2}+\\tau_{1}((p+\\ell)\\tau_{1}-1)\\langle\\Delta_{t}^{x},\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})\\rangle+\\frac{p+\\ell}{2}\\tau_{1}^{2}\\|\\Delta_{t}^{x}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Similarly, we observe that for all, $\\nabla_{y}\\hat{f}(x,y;z)=\\nabla_{y}f(x,y)$ , for all $x,y,z$ , which together with the smoothness of $f(x,\\cdot)$ implies ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{f}(x_{t+1},y_{t+1};z_{t})-\\hat{f}(x_{t+1},y_{t};z_{t})}\\\\ &{\\quad\\quad\\leq\\langle\\nabla_{y}\\hat{f}(x_{t+1},y_{t};z_{t}),\\ y_{t+1}-y_{t}\\rangle+\\displaystyle\\frac{\\ell}{2}\\|y_{t+1}-y_{t}\\|^{2}}\\\\ &{\\quad\\quad=\\tau_{2}\\langle\\nabla_{y}f(x_{t+1},y_{t}),G_{y}(x_{t+1},y_{t},\\xi_{t+1}^{y})\\rangle+\\displaystyle\\frac{\\ell}{2}\\tau_{2}^{2}\\|G_{y}(x_{t+1},y_{t},\\xi_{t+1}^{y})\\|^{2}}\\\\ &{\\quad\\quad=\\tau_{2}\\left(1+\\displaystyle\\frac{\\ell}{2}\\tau_{2}\\right)\\|\\nabla_{y}f(x_{t+1},y_{t})\\|^{2}+\\tau_{2}(1+\\ell\\tau_{2})\\langle\\nabla_{y}f(x_{t+1},y_{t}),\\Delta_{t}^{y}\\rangle+\\displaystyle\\frac{\\ell\\tau_{2}^{2}}{2}\\|\\Delta_{t}^{y}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we used again the identity $G_{y}(x_{t+1},y_{t},\\xi_{t+1}^{y})=\\nabla_{y}\\hat{f}(x_{t+1},y_{t};z_{t})+\\Delta_{t}^{y}.$ ", "page_idx": 16}, {"type": "text", "text": "Finally, we observe from the sm-AGDA update rule $z_{t+1}-z_{t}=\\beta(x_{t+1}-z_{t})$ that $\\begin{array}{r l}{\\frac{1}{\\beta}\\big(z_{t+1}-z_{t}\\big)=}&{{}}\\end{array}$ $x_{t+1}-z_{t}$ and $(1-\\beta)(x_{t+1}-z_{t})=(x_{t+1}-z_{t})-(z_{t+1}-z_{t})=x_{t+1}-z_{t+1}$ . This gives ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{f}(x_{t+1},y_{t+1};z_{t+1})-\\hat{f}(x_{t+1},y_{t+1};z_{t})=\\displaystyle\\frac{p}{2}\\left[\\|x_{t+1}-z_{t+1}\\|^{2}-\\|x_{t+1}-z_{t}\\|^{2}\\right]}\\\\ &{\\phantom{\\hat{f}(x_{t+1},y_{t+1};z_{t+1})}=\\displaystyle\\frac{p}{2}\\left[\\|(1-\\beta)(x_{t+1}-z_{t})\\|^{2}-\\frac{1}{\\beta^{2}}\\|z_{t+1}-z_{t}\\|^{2}\\right]}\\\\ &{\\phantom{\\hat{f}(x_{t+1},y_{t+1};z_{t+1})}=\\displaystyle\\frac{p}{2}\\left[\\frac{(1-\\beta)^{2}}{\\beta^{2}}\\|z_{t+1}-z_{t}\\|^{2}-\\frac{1}{\\beta^{2}}\\|z_{t+1}-z_{t}\\|^{2}\\right]}\\\\ &{\\phantom{\\hat{f}(x_{t+1},y_{t+1};z_{t+1})}\\leq\\displaystyle\\frac{-p}{2\\beta}\\|z_{t}-z_{t+1}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where we used $0<\\beta\\leq1$ . Therefore, summing up (15), (16), (17) yields the claim. ", "page_idx": 16}, {"type": "text", "text": "B.2 Proof of Lemma 6 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma 6. Suppose Assumptions 1, 2 and $^3$ hold, and $p\\geq\\ell.$ . Then, for any $t\\in\\mathbb{N}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{I}(y_{t+1};z_{t+1})-\\Psi(y_{t};z_{t})\\geq\\tau_{2}\\langle\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t}),\\,\\nabla_{y}f(x_{t+1},y_{t})\\rangle+\\tau_{2}\\langle\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t}),\\Delta_{t}^{y}\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{-\\;\\frac{L_{\\Psi}}{2}\\tau_{2}^{2}\\left(\\|\\nabla_{y}f(x_{t+1},y_{t})\\|^{2}+2\\langle\\nabla_{y}f(x_{t+1},y_{t}),\\Delta_{t}^{y}\\rangle+\\|\\Delta_{t}^{y}\\|^{2}\\right)}\\\\ &{+\\;\\frac{p}{2}\\langle z_{t+1}-z_{t},z_{t+1}+z_{t}-2x^{*}(y_{t+1},z_{t+1})\\rangle,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\begin{array}{r}{L_{\\Psi}\\triangleq\\ell\\left(1+\\frac{p+\\ell}{p-\\ell}\\right)}\\end{array}$ and the map $x^{*}(\\cdot,\\cdot)$ is defined in (11). ", "page_idx": 17}, {"type": "text", "text": "Proof. By Lemma 19, $\\Psi$ is $L_{\\Psi}$ -smooth in $y$ for any given $z\\,\\in\\,\\mathbb{R}^{d_{1}}$ . Then, using $\\nabla_{y}\\Psi(y_{t};z_{t})=$ $\\nabla_{y}f(x^{*}(\\dot{y}_{t},z_{t}),y_{t})$ , we obtain ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{F}(y_{t+1},z_{t})-\\Psi(y_{t},z_{t})\\geq\\langle\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t};z_{t}),\\ y_{t+1}-y_{t}\\rangle-\\frac{L_{\\Psi}}{2}\\|y_{t+1}-y_{t}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\geq\\tau_{2}\\langle\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t}),\\ \\nabla_{y}f(x_{t+1},y_{t})\\rangle+\\tau_{2}\\langle\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t}),\\ \\Delta_{t}^{y}\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\qquad-\\,\\frac{L_{\\Psi}}{2}\\tau_{2}^{2}\\left(\\|\\nabla_{y}f(x_{t+1},y_{t})\\|^{2}+2\\langle\\nabla_{y}f(x_{t+1},y_{t}),\\Delta_{t}^{y}\\rangle+\\|\\Delta_{t}^{y}\\|^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Furthermore, by definition of $\\Psi$ , we also have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Psi\\big(y_{t+1};z_{t+1}\\big)-\\Psi\\big(y_{t+1};z_{t}\\big)=\\hat{f}\\big(x^{*}(y_{t+1},z_{t+1}),y_{t+1};z_{t+1}\\big)-\\hat{f}\\big(x^{*}(y_{t+1},z_{t}),y_{t+1};z_{t}\\big)}&{}\\\\ {\\geq\\hat{f}\\big(x^{*}(y_{t+1},z_{t+1}),y_{t+1};z_{t+1}\\big)-\\hat{f}\\big(x^{*}(y_{t+1},z_{t+1}),y_{t+1};z_{t}\\big)}&{}\\\\ {=\\frac{p}{2}\\left[\\|z_{t+1}-x^{*}(y_{t+1},z_{t+1})\\|^{2}-\\|z_{t}-x^{*}(y_{t+1},z_{t+1})\\|^{2}\\right]}&{}\\\\ {=\\frac{p}{2}\\big(z_{t+1}-z_{t}\\big)^{\\top}\\big[z_{t+1}+z_{t}-2x^{*}(y_{t+1},z_{t+1})\\big].}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Summing (18) and (19), we conclude. ", "page_idx": 17}, {"type": "text", "text": "C Proof of Theorem 7 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Before we move on to the proof of Theorem 7, we first provide a result from [70, 64] that quantifies the change in the function $P$ over the iterations. We also provide its proof for the sake of completeness. Lemma 15 (Lemma B.7 in [70]). Suppose Assumptions 1, 2 and $^3$ hold. Consider the sm-AGDA iterate sequence $(x_{t},y_{t},z_{t})_{t\\in\\mathbb{N}}$ for $p\\;>\\;\\ell,$ and let $Y^{*}(z)\\,\\triangleq\\,\\operatorname{argmax}_{y}\\,\\Psi(y;z)$ denote the set of maximizers of $\\Psi(\\cdot,z)$ for given $z$ . For any $t\\in\\mathbb{N}$ and $y^{*}(z_{t+1})\\in Y^{*}(z_{t+1})$ , it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\nP(z_{t+1})-P(z_{t})\\leq\\frac{p}{2}\\langle z_{t+1}-z_{t},\\ z_{t+1}+z_{t}-2x^{\\ast}(y^{\\ast}(z_{t+1}),z_{t})\\rangle.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Proof. Let $y^{*}(z_{t+1})\\in Y^{*}(z_{t+1})$ and $y^{\\ast}(z_{t})\\in Y^{\\ast}(z_{t})$ be two arbitrary maximizers. We have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{P(z_{t+1})-P(z_{t})=\\operatorname*{min}_{x\\atop x\\to y}\\hat{f}(x,y;z_{t+1})-\\operatorname*{min}_{x\\atop x\\to y}\\operatorname*{max}_{y}\\hat{f}(x,y;z_{t})}}\\\\ &{=\\underset{y\\to z}{\\operatorname*{max}}\\,\\hat{f}(x,y;z_{t+1})-\\underset{y\\to z}{\\operatorname*{max}}\\,\\hat{f}(x,y;z_{t})}\\\\ &{=\\Psi(y^{*}(z_{t+1});z_{t+1})-\\Psi(y^{*}(z_{t});z_{t})}\\\\ &{\\leq\\Psi(y^{*}(z_{t+1});z_{t+1})-\\Psi(y^{*}(z_{t+1});z_{t})}\\\\ &{=\\hat{f}(x^{*}(y^{*}(z_{t+1}),z_{t+1}),y^{*}(z_{t+1});z_{t+1})-\\hat{f}(x^{*}(y^{*}(z_{t+1});z_{t}),y^{*}(z_{t+1});z_{t})}\\\\ &{\\leq\\hat{f}(x^{*}(y^{*}(z_{t+1}),z_{t}),y^{*}(z_{t+1});z_{t+1})-\\hat{f}(x^{*}(y^{*}(z_{t+1});z_{t}),y^{*}(z_{t+1});z_{t})}\\\\ &{=\\frac{p}{2}(z_{t+1}-z_{t})^{\\top}[z_{t+1}+z_{t}-2x^{*}(y^{*}(z_{t+1}),z_{t})].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The first and the third equality above hold by the definition of $P(z)$ and $\\Psi(y;z)$ functions; on the other hand, for the second equality, one needs strong duality to hold. This interchange is valid and can be justified by the simple fact that $\\hat{f}$ is strongly convex in $x$ (therefore, it satisfies the $\\mathrm{PL}$ condition in $x^{\\dagger}$ ), and it also satisfies the PL condition in $y$ according to our assumption 2. It has been established in [65, Lemma 2.1] that the double-sided PL property allows one for the min-max switch. \u53e3 ", "page_idx": 17}, {"type": "text", "text": "Equipped with this lemma, the stage is set to prove Theorem 7 from Section 3. We first restate an extended version of this theorem, where the constants $\\{c_{i}\\}_{i=1}^{8}$ are provided explicitly. ", "page_idx": 18}, {"type": "text", "text": "Theorem 7. Suppose Assumptions 1, 2, 3 and $^{4}$ hold. Consider the sm-AGDA algorithm with parameters $\\begin{array}{r}{p>\\dot{\\ell},\\,\\beta\\in(0,1],\\,\\dot{\\tau}_{1}\\in(0,\\frac{1}{p+\\ell}]}\\end{array}$ and $\\tau_{2}>0$ chosen such that ", "page_idx": 18}, {"type": "equation", "text": "$$\nc_{0}\\triangleq-\\tau_{2}^{2}\\ell\\nu+\\tau_{2}\\left(1-\\frac{\\ell}{2}\\tau_{2}-L_{\\Psi}\\tau_{2}\\right)\\geq0,\\quad c_{0}^{\\prime}\\triangleq\\frac{p}{3\\beta}-\\left(\\frac{2p^{2}}{p-\\ell}+48\\beta\\frac{p^{3}}{(p-\\ell)^{2}}\\right)\\geq0\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for some constant $\\nu>0$ , where $\\begin{array}{r}{L_{\\Psi}=\\ell\\left(1+\\frac{p+\\ell}{p-\\ell}\\right)}\\end{array}$ . Then, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{t}-V_{t+1}\\geq c_{1}\\|\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})\\|^{2}+c_{2}\\|\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t})\\|^{2}+c_{3}\\|x_{t}-z_{t}\\|^{2}}\\\\ &{\\qquad\\qquad+c_{4}\\langle\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t}),\\Delta_{t}^{x}\\rangle+\\langle c_{5}\\nabla_{y}f(x_{t},y_{t})+c_{6}\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t}),\\Delta_{t}^{y}\\rangle}\\\\ &{\\qquad\\qquad+c_{7}\\|\\Delta_{t}^{x}\\|^{2}+c_{8}\\|\\Delta_{t}^{y}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where the coefficients $c_{1}$ to $c_{8}$ have the following forms: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{c_{1}=\\frac{\\tau_{1}}{2}-2\\left(\\frac{1}{(p-\\ell)^{2}}+\\tau_{1}^{2}\\right)\\left(c_{0}\\ell^{2}+\\frac{\\ell}{\\nu}+48\\beta p\\Big(\\frac{p+\\ell}{p-\\ell}\\Big)^{2}\\ell^{2}\\tau_{2}^{2}\\right)-\\left(c_{0}^{\\prime}\\beta^{2}+\\frac{\\ell}{6}(1+\\ell\\tau_{2}+2L_{\\Psi}\\tau_{2})\\right)\\tau_{1}^{2}}\\\\ &{c_{2}=\\frac{c_{0}}{2}-\\frac{24\\beta p}{(p-\\ell)p}\\left(1+\\tau_{2}\\frac{2p\\ell}{p-\\ell}\\right)^{2},\\quad c_{3}=c_{0}^{\\prime}\\beta^{2}/2,}\\\\ &{c_{4}=-\\left(192\\beta p\\Big(\\frac{p+\\ell}{p-\\ell}\\Big)^{2}\\ell^{2}\\tau_{2}^{2}+\\frac{4\\ell}{\\nu}+4c_{0}\\ell^{2}+2c_{0}^{\\prime}\\beta^{2}\\right)\\tau_{1}^{2}-\\Big((p+\\ell)\\tau_{1}-1\\Big)\\tau_{1},}\\\\ &{c_{5}=-\\tau_{2}(1+\\ell\\tau_{2}+2L_{\\Psi}\\tau_{2}),\\quad c_{6}=2\\tau_{2},}\\\\ &{c_{7}=-\\left(96\\beta p\\Big(\\frac{p+\\ell}{p-\\ell}\\Big)^{2}\\ell^{2}\\tau_{2}^{2}+\\frac{2\\ell}{\\nu}+\\frac{p+\\ell}{2}+2c_{0}\\ell^{2}+\\frac{\\ell}{6}(1+\\ell\\tau_{2}+2L_{\\Psi}\\tau_{2})+c_{0}^{\\prime}\\beta^{2}\\right)\\tau_{1}^{2},}\\\\ &{c_{8}=-\\left(48\\beta p\\Big(\\frac{p+\\ell}{p-\\ell}\\Big)^{2}+\\frac{\\ell}{2}+L_{\\Psi}+3\\ell(1+\\ell\\tau_{2}+2L_{\\Psi}\\tau_{2})\\right)\\tau_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Furthermore, the sm-AGDA parameters can be chosen such that $c_{1},c_{2},c_{3}>0$ . ", "page_idx": 18}, {"type": "text", "text": "Proof. Combining the inequalities in Lemmas 5, 6 and 15 gives us a lower bound of the form: ", "page_idx": 18}, {"type": "equation", "text": "$$\nV_{t}-V_{t+1}\\geq A_{1}+A_{2}+A_{3}+A_{4}+A_{5}+A_{6},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "for any $y^{*}(z_{t+1})\\in Y^{*}(z_{t+1})$ appearing in $A_{3}$ , where ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{1}=\\frac{\\tau_{1}}{2}\\|\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})\\|^{2}+\\tau_{2}\\left(1-\\frac{\\ell}{2}\\tau_{2}-L_{\\Psi}\\tau_{2}\\right)\\|\\nabla_{y}f(x_{t+1},y_{t})\\|^{2}+\\frac{p}{2\\beta}\\|z_{t}-z_{t+1}\\|^{2},}\\\\ &{A_{2}=2\\tau_{2}\\langle\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t})-\\nabla_{y}f(x_{t+1},y_{t}),\\nabla_{y}f(x_{t+1},y_{t})\\rangle,}\\\\ &{A_{3}=2p\\langle z_{t+1}-z_{t},\\ x^{*}(y^{*}(z_{t+1}),z_{t})-x^{*}(y_{t+1},z_{t+1})\\rangle}\\\\ &{A_{4}=-\\tau_{1}((p+\\ell)\\tau_{1}-1)\\langle\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t}),\\Delta_{t}^{x}\\rangle,}\\\\ &{A_{5}=\\langle\\,2\\tau_{2}\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t})-\\tau_{2}(1+\\ell\\tau_{2}+2L_{\\Psi}\\tau_{2})\\nabla_{y}f(x_{t+1},y_{t}),\\,\\Delta_{t}^{y}\\rangle,}\\\\ &{A_{6}=-\\frac{p+\\ell}{2}\\tau_{1}^{2}\\|\\Delta_{t}^{x}\\|^{2}-\\frac{\\ell\\tau_{2}^{2}}{2}\\|\\Delta_{t}^{y}\\|^{2}-L_{\\Psi}\\tau_{2}^{2}\\|\\Delta_{t}^{y}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Next, we provide lower bounds for several terms in the above inequality, including $A_{2},\\,A_{3},\\,A_{5}$ , $\\|\\nabla_{y}f(x_{t+1},y_{t})\\|^{2}$ and $\\|z_{t+1}-z_{t}\\|^{2}$ . At the end, using these bounds within (21), we will be able to establish a descent property for $\\{V_{t}\\}$ , which will allow to apply our concentration result (Theorem 9) for deriving the desired high probability bounds. ", "page_idx": 18}, {"type": "text", "text": "Lower bound for $A_{2}$ . Using Cauchy-Schwarz inequality and $\\ell$ -smoothness of $f$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{2}\\geq-2\\tau_{2}\\|\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t})-\\nabla_{y}f(x_{t+1},y_{t})\\|\\|\\nabla_{y}f(x_{t+1},y_{t})\\|}\\\\ &{\\quad\\geq-2\\tau_{2}\\ell\\|x_{t+1}-x^{*}(y_{t},z_{t})\\|\\|\\nabla_{y}f(x_{t+1},y_{t})\\|}\\\\ &{\\quad\\geq-\\tau_{2}^{2}\\ell\\nu\\|\\nabla_{y}f(x_{t+1},y_{t})\\|^{2}-\\ell\\nu^{-1}\\|x_{t+1}-x^{*}(y_{t},z_{t})\\|^{2},\\quad\\forall\\nu>0,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where last line follows from Young\u2019s inequality. Thus, by Lemma 20, we obtain for any $\\nu>0$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{2}\\geq-\\tau_{2}^{2}\\ell\\nu\\|\\nabla_{y}f(x_{t+1},y_{t})\\|^{2}-\\displaystyle\\frac{2\\ell}{\\nu}\\left(1+\\frac{1}{\\tau_{1}^{2}(p-\\ell)^{2}}\\right)\\tau_{1}^{2}\\|\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})\\|^{2}}\\\\ &{\\qquad-\\displaystyle\\frac{4\\ell}{\\nu}\\tau_{1}^{2}\\langle\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t}),\\Delta_{t}^{x}\\rangle-\\frac{2\\ell}{\\nu}\\tau_{1}^{2}\\|\\Delta_{t}^{x}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Lower bound for $A_{3}$ . By Cauchy-Schwarz inequality and Lemma 21, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{3}=2p\\langle z_{t+1}-z_{t},\\ x^{*}(y^{*}(z_{t+1}),z_{t})-x^{*}(y^{*}(z_{t+1}),z_{t+1})\\rangle}\\\\ &{\\quad\\quad+2p\\langle z_{t+1}-z_{t},\\ x^{*}(y^{*}(z_{t+1}),z_{t+1})-x^{*}(y_{t+1},z_{t+1})\\rangle}\\\\ &{\\quad\\geq-2p\\|z_{t+1}-z_{t}\\|\\|x^{*}(y^{*}(z_{t+1}),z_{t})-x^{*}(y^{*}(z_{t+1}),z_{t+1})\\|}\\\\ &{\\quad\\quad+2p\\langle z_{t+1}-z_{t},\\ x^{*}(y^{*}(z_{t+1}),z_{t+1})-x^{*}(y_{t+1},z_{t+1})\\rangle}\\\\ &{\\quad\\geq-\\frac{2p^{2}}{p-\\ell}\\|z_{t+1}-z_{t}\\|^{2}+2p\\langle z_{t+1}-z_{t},\\ x^{*}(y^{*}(z_{t+1}),z_{t+1})-x^{*}(y_{t+1},z_{t+1})\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence, using Young\u2019s inequality, for all $\\beta>0$ , we obtain ", "page_idx": 19}, {"type": "equation", "text": "$$\nA_{3}\\geq-\\left(\\frac{p}{6\\beta}+\\frac{2p^{2}}{p-\\ell}\\right)\\|z_{t+1}-z_{t}\\|^{2}-6\\beta p\\|x^{*}(y^{*}(z_{t+1}),z_{t+1})-x^{*}(y_{t+1},z_{t+1})\\|^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We now lower bound the second term on the right-hand side of (23). First, note that we have $x^{*}(y^{*}(z_{t+1}),z_{t+1})\\,=\\,x^{*}(z_{t+1})$ , which follows from the fact that $\\begin{array}{r l}{\\operatorname*{min}_{x}\\operatorname*{max}_{y}\\hat{f}(x,y;z)=}\\end{array}$ $\\operatorname*{max}_{y}\\operatorname*{min}_{x}\\hat{f}(x,y;z)$ for all $z$ since $\\hat{f}$ is strongly convex in $x$ and satisfies the PL condition in Assumption 2. Hence, using the inequality $\\begin{array}{r}{\\left\\|\\sum_{i=1}^{N}w_{i}\\right\\|^{2}\\;\\leq\\;N\\sum_{i=1}^{N}\\left\\|w_{i}\\right\\|^{2}}\\end{array}$ , which holds for any $\\{w_{i}\\}\\in\\mathbb{R}^{d_{1}}$ and $N\\geq1$ , we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|x^{*}(y^{*}(z_{t+1}),z_{t+1})-x^{*}(y_{t+1},z_{t+1})\\|^{2}}\\\\ &{\\qquad\\qquad\\leq\\ \\ 4\\|x^{*}(z_{t+1})-x^{*}(z_{t})\\|^{2}+4\\|x^{*}(z_{t})-x^{*}(y_{t}^{+}(z_{t}),z_{t})\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad+4\\|x^{*}(y_{t}^{+}(z_{t}),z_{t})-x^{*}(y_{t+1},z_{t})\\|^{2}+4\\|x^{*}(y_{t+1},z_{t})-x^{*}(y_{t+1},z_{t+1})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By Lemma 21 and Lemma 23, we observe that ", "page_idx": 19}, {"type": "equation", "text": "$$\n4\\|x^{*}(z_{t+1})-x^{*}(z_{t})\\|^{2}+4\\|x^{*}(y_{t+1},z_{t})-x^{*}(y_{t+1},z_{t+1})\\|^{2}\\leq\\frac{8p^{2}}{(p-\\ell)^{2}}\\|z_{t+1}-z_{t}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Using Lemma 24, we also have ", "page_idx": 19}, {"type": "equation", "text": "$$\n4\\|x^{*}(z_{t})-x^{*}(y_{t}^{+}(z_{t}),z_{t})\\|^{2}\\leq\\frac{4}{(p-\\ell)\\mu}\\left(1+\\tau_{2}\\frac{2p\\ell}{p-\\ell}\\right)^{2}\\|\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t})\\|^{2}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Finally, using Lemma 18, we get ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{4\\|x^{*}(y_{t}^{+}(z_{t}),z_{t})-x^{*}(y_{t+1},z_{t})\\|^{2}}\\\\ &{\\qquad\\leq4\\Big(\\displaystyle\\frac{p+\\ell}{p-\\ell}\\Big)^{2}\\|y_{t}^{+}(z_{t})-y_{t+1}\\|^{2}}\\\\ &{\\qquad=4\\Big(\\displaystyle\\frac{p+\\ell}{p-\\ell}\\Big)^{2}\\tau_{2}^{2}\\|\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t})-G_{y}(x_{t+1},y_{t},\\xi_{t+1}^{y})\\|^{2}}\\\\ &{\\qquad=4\\Big(\\displaystyle\\frac{p+\\ell}{p-\\ell}\\Big)^{2}\\tau_{2}^{2}\\|\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t})-\\big(\\nabla_{y}f(x_{t+1},y_{t})+\\Delta_{t}^{y}\\big)\\|^{2},}\\\\ &{\\qquad\\leq4\\Big(\\displaystyle\\frac{p+\\ell}{p-\\ell}\\Big)^{2}\\tau_{2}^{2}\\left(2\\ell^{2}\\|x^{*}(y_{t},z_{t})-x_{t+1}\\|^{2}+2\\|\\Delta_{t}^{y}\\|^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the last inequality stems from the $\\ell$ -smoothness of $f$ . In view of Lemma 20, we may further upper bound the above quantity as follows: ", "page_idx": 19}, {"type": "equation", "text": "$$\n4\\|x^{*}(y_{t}^{+}(z_{t}),z_{t})-x^{*}(y_{t+1},z_{t})\\|^{2}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq16\\Big(\\displaystyle\\frac{p+\\ell}{p-\\ell}\\Big)^{2}\\ell^{2}\\tau_{1}^{2}\\tau_{2}^{2}\\Big[\\Big(\\displaystyle\\frac{1}{\\tau_{1}^{2}(p-\\ell)^{2}}+1\\Big)\\|\\nabla_{x}\\widehat f(x_{t},y_{t};z_{t})\\|^{2}+2\\langle\\nabla_{x}\\widehat f(x_{t},y_{t};z_{t}),\\Delta_{t}^{x}\\rangle+\\|\\Delta_{t}^{x}\\|^{2}\\Big]}\\\\ &{\\quad+8\\Big(\\displaystyle\\frac{p+\\ell}{p-\\ell}\\Big)^{2}\\tau_{2}^{2}\\|\\Delta_{t}^{y}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Summing up the three intermediate inequalities we established above, we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|x^{*}(y^{*}(z_{t+1}),z_{t+1})-x^{*}(y_{t+1},z_{t+1})\\|^{2}}\\\\ &{\\quad\\leq\\frac{8p^{2}}{(p-\\ell)^{2}}\\|z_{t+1}-z_{t}\\|^{2}+\\frac{4}{(p-\\ell)\\mu}\\left(1+\\tau_{2}\\ell+\\frac{\\tau_{2}\\ell(p+\\ell)}{p-\\ell}\\right)^{2}\\|\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t})\\|^{2}}\\\\ &{\\qquad+16\\Big(\\frac{p+\\ell}{p-\\ell}\\Big)^{2}\\ell^{2}\\left(\\frac{1}{\\tau_{1}^{2}(p-\\ell)^{2}}+1\\right)\\tau_{1}^{2}\\tau_{2}^{2}\\|\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})\\|^{2}}\\\\ &{\\qquad+32\\Big(\\frac{p+\\ell}{p-\\ell}\\Big)^{2}\\ell^{2}\\tau_{1}^{2}\\tau_{2}^{2}\\langle\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t}),\\Delta_{t}^{x}\\rangle}\\\\ &{\\qquad+16\\Big(\\frac{p+\\ell}{p-\\ell}\\Big)^{2}\\ell^{2}\\tau_{1}^{2}\\tau_{2}^{2}\\|\\Delta_{t}^{x}\\|^{2}+8\\frac{(p+\\ell)^{2}}{(p-\\ell)^{2}}\\tau_{2}^{2}\\|\\Delta_{t}^{y}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In conclusion for $A_{3}$ , using (23) and the above inequality, we obtain after some rearrangement ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{A_{3}\\geq-\\left(\\frac{p}{6\\beta}+\\frac{2p^{2}}{p-\\ell}+48\\beta\\frac{p^{3}}{\\left(p-\\ell\\right)^{2}}\\right)\\|z_{t+1}-z_{t}\\|^{2}-\\left(96\\beta p\\Big(\\frac{p+\\ell}{p-\\ell}\\Big)^{2}\\ell^{2}\\tau_{1}^{2}\\tau_{2}^{2}\\right)\\|\\Delta_{t}^{x}\\|^{2}}}\\\\ {{\\qquad-\\left(\\frac{24\\beta p}{\\left(p-\\ell\\right)\\mu}\\left(1+\\tau_{2}\\frac{2p\\ell}{p-\\ell}\\right)^{2}\\right)\\|\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t})\\|^{2}}}\\\\ {{\\qquad-\\left[96\\beta p\\Big(\\frac{p+\\ell}{p-\\ell}\\Big)^{2}\\ell^{2}\\left(\\frac{1}{\\tau_{1}^{2}(p-\\ell)^{2}}+1\\right)\\tau_{1}^{2}\\tau_{2}^{2}\\right]\\|\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})\\|^{2}}}\\\\ {{\\qquad-\\left(192\\beta p\\Big(\\frac{p+\\ell}{p-\\ell}\\Big)^{2}\\ell^{2}\\tau_{1}^{2}\\tau_{2}^{2}\\right)\\langle\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t}),\\Delta_{t}^{x}\\rangle-\\left(48\\beta p\\Big(\\frac{p+\\ell}{p-\\ell}\\Big)^{2}\\tau_{2}^{2}\\right)\\|\\Delta_{t}^{y}\\|^{2}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Lower bound for $A_{5}$ . Below we first bound $\\langle\\nabla_{y}f(x_{t+1},y_{t}),\\Delta_{t}^{y}\\rangle$ , i.e., ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\nabla_{y}f(x_{t+1},y_{t}),\\Delta_{t}^{y}\\rangle=\\langle\\nabla_{y}f(x_{t},y_{t}),\\Delta_{t}^{y}\\rangle+\\langle\\nabla_{y}f(x_{t+1},y_{t})-\\nabla_{y}f(x_{t},y_{t}),\\Delta_{t}^{y}\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\leq\\langle\\nabla_{y}f(x_{t},y_{t}),\\Delta_{t}^{y}\\rangle+\\frac{1}{12\\tau_{2}\\ell}\\Vert\\nabla_{y}f(x_{t+1},y_{t})-\\nabla_{y}f(x_{t},y_{t})\\Vert^{2}+3\\tau_{2}\\ell\\Vert\\Delta_{t}^{y}\\Vert^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\langle\\nabla_{y}f(x_{t},y_{t}),\\Delta_{t}^{y}\\rangle+\\frac{\\ell\\tau_{1}^{2}}{12\\tau_{2}}\\Vert\\hat{G}_{x}(x_{t},y_{t},\\xi_{t+1}^{x};z_{t})\\Vert^{2}+3\\tau_{2}\\ell\\Vert\\Delta_{t}^{y}\\Vert^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\langle\\nabla_{y}f(x_{t},y_{t}),\\Delta_{t}^{y}\\rangle+\\frac{\\ell\\tau_{1}^{2}}{6\\tau_{2}}\\Vert\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})\\Vert^{2}+\\frac{\\ell\\tau_{1}^{2}}{6\\tau_{2}}\\Vert\\Delta_{t}^{x}\\Vert^{2}+3\\tau_{2}\\ell\\Vert\\Delta_{t}^{y}\\Vert^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the first inequality follows from Young\u2019s inequality, the second inequality follows from $\\nabla_{y}f$ being $\\ell$ -smooth and the update rule $x_{t+1}=x_{t}-\\tau_{1}\\hat{G}_{x}(x_{t},y_{t},\\xi_{t+1}^{x};z_{t})$ , and in the third inequality we use $\\Delta_{t}^{x}=$ $\\hat{G}_{x}(x,y_{t},\\xi_{t+1}^{x};z_{t})-\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})$ . Thus, since $-\\tau_{2}\\bigl(1+\\ell\\tau_{2}+2L\\uppsi\\tau_{2}\\bigr)<0$ , for any $\\tau_{2}>0$ , we obtain ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{A_{5}\\geq\\,-\\,\\tau_{2}(1+\\ell\\tau_{2}+2L_{\\Psi}\\tau_{2})\\Big(\\frac{\\ell\\tau_{1}^{2}}{6\\tau_{2}}\\|\\nabla_{x}\\widehat f(x_{t},y_{t};z_{t})\\|^{2}+\\frac{\\ell\\tau_{1}^{2}}{6\\tau_{2}}\\|\\Delta_{t}^{x}\\|^{2}+3\\tau_{2}\\ell\\|\\Delta_{t}^{y}\\|^{2}\\Big)}\\\\ &{\\qquad+\\,\\langle\\,2\\tau_{2}\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t})-\\tau_{2}(1+\\ell\\tau_{2}+2L_{\\Psi}\\tau_{2})\\nabla_{y}f(x_{t},y_{t}),\\,\\Delta_{t}^{y}\\rangle.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Lower bound for $\\|\\nabla_{y}f(x_{t+1},y_{t})\\|^{2}$ . Using Lemma 20 we can lower bound $\\|\\nabla_{y}f(x_{t+1},y_{t})\\|^{2}$ as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{y}f(x_{t+1},y_{t})\\|^{2}\\geq\\frac{1}{2}\\|\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t})\\|^{2}-\\|\\nabla_{y}f(x_{t+1},y_{t})-\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t})\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\geq\\frac{1}{2}\\|\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t})\\|^{2}-\\ell^{2}\\|x_{t+1}-x^{*}(y_{t},z_{t})\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\geq\\frac{1}{2}\\|\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t})\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad-2\\ell^{2}\\tau_{1}^{2}\\left[\\left(\\frac{1}{\\tau_{1}^{2}(p-\\ell)^{2}}+1\\right)\\|\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})\\|^{2}+2\\langle\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t}),\\Delta_{t}^{x}\\rangle+\\|\\Delta_{t}^{x}\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We observe that $\\|\\nabla_{y}f(x_{t+1},y_{t})\\|^{2}$ appears in both $A_{1}$ and the lower bound given in (22) for $A_{2}$ ; hence, grouping the two terms together and using the definition of $c_{0}\\geq0$ , we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\Bigg(\\tau_{2}\\left(1-\\frac{\\ell}{2}\\tau_{2}-L_{\\Psi}\\tau_{2}\\right)-\\tau_{2}^{2}\\ell\\nu\\Bigg)\\left\\|\\nabla_{y}f(x_{t+1},y_{t})\\right\\|^{2}=c_{0}\\|\\nabla_{y}f(x_{t+1},y_{t})\\|^{2}}\\\\ {\\displaystyle\\qquad\\ge\\frac{c_{0}}{2}\\|\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t})\\|^{2}-2c_{0}\\ell^{2}\\left(\\frac{1}{(p-\\ell)^{2}}+\\tau_{1}^{2}\\right)\\left\\|\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})\\right\\|^{2}}\\\\ {\\displaystyle\\qquad-\\,2c_{0}\\ell^{2}\\tau_{1}^{2}\\Big(2\\langle\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t}),\\Delta_{t}^{x}\\rangle+\\|\\Delta_{t}^{x}\\|^{2}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Lower bound for $\\|z_{t+1}-z_{t}\\|^{2}$ . Since $z_{t+1}=z_{t}+\\beta(x_{t+1}-z_{t})$ for some $\\beta>0$ , we observe that: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|z_{t+1}-z_{t}\\|^{2}=\\beta^{2}\\|x_{t+1}-z_{t}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad=\\beta^{2}\\|x_{t}-z_{t}-\\tau_{1}\\hat{G}_{x}(x_{t},y_{t},\\xi_{t+1}^{x};z_{t})\\|^{2}}\\\\ &{\\qquad\\qquad\\quad\\geq\\beta^{2}\\left(\\frac{1}{2}\\|x_{t}-z_{t}\\|^{2}-\\tau_{1}^{2}\\|\\hat{G}_{x}(x_{t},y_{t},\\xi_{t+1}^{x};z_{t})\\|^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "and since $\\hat{G}_{x}(x_{t},y_{t},\\xi_{t+1}^{x};z_{t})=\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})+\\Delta_{t}^{x}$ , we obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|z_{t}-z_{t+1}\\|^{2}\\geq\\frac{\\beta^{2}}{2}\\|x_{t}-z_{t}\\|^{2}-\\beta^{2}\\tau_{1}^{2}\\|\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})\\|^{2}-2\\beta^{2}\\tau_{1}^{2}\\langle\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t}),\\Delta_{t}^{x}\\rangle-\\beta^{2}\\tau_{1}^{2}\\|\\Delta_{t}^{x}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that $\\|z_{t+1}-z_{t}\\|^{2}$ appears in both $A_{1}$ and the lower bound given in (24) for $A_{3}$ ; hence, grouping the two terms together and using the definition of $c_{0}^{\\prime}\\geq0$ Using (27), we obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(\\frac{p}{2\\beta}-\\frac{p}{6\\beta}-\\frac{2p^{2}}{p-\\ell}-48\\beta\\frac{p^{3}}{(p-\\ell)^{2}}\\right)\\|z_{t+1}-z_{t}\\|^{2}=c_{0}^{\\prime}\\|z_{t+1}-z_{t}\\|^{2}}\\\\ &{\\qquad\\geq-c_{0}^{\\prime}\\beta^{2}\\tau_{1}^{2}\\|\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})\\|^{2}+c_{0}^{\\prime}\\frac{\\beta^{2}}{2}\\|x_{t}-z_{t}\\|^{2}-2c_{0}^{\\prime}\\beta^{2}\\tau_{1}^{2}\\langle\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t}),\\Delta_{t}^{x}\\rangle-c_{0}^{\\prime}\\beta^{2}\\tau_{1}^{2}\\|\\Delta_{t}^{x}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We conclude by combining all these lower bounds, i.e., the claimed Lyapunov descent inequality follows directly from summing (21), (22), (24), (25), (26) and (27). Finally, it follows after straightforward computations that there exist choice of sm-AGDA parameters which yield $c_{1},c_{2},c_{3}>0$ while satisfying the conditions $c_{0}\\geq0$ and $c_{0}^{\\prime}\\geq0$ ; in fact Corollary 8 provides such sm-AGDA parameters explicitly. This completes the proof. \u53e3 ", "page_idx": 21}, {"type": "text", "text": "D Proof of Corollary 8 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The lower bound provided in Theorem 7 resembles the descent property we require for our concentration result in Theorem 9. To allow for its proper application, we develop a stepsize policy inspired by [66]. ", "page_idx": 21}, {"type": "text", "text": "Corollary 8. Under the premise of Theorem 7, consider the parameters $p\\,=\\,2\\ell$ , $\\scriptstyle\\tau_{1}\\ \\in\\ (0,\\,{\\frac{1}{3\\ell}}]$ , $\\begin{array}{r}{\\tau_{2}\\,=\\,\\frac{\\tau_{1}}{48},\\beta\\,=\\,\\alpha\\mu\\tau_{2}}\\end{array}$ for any $\\alpha\\in(0,\\frac{1}{406}]$ . Then, A\u02dct+\u03c411\u2212A\u02dct \u2264\u2212B\u02dct + C\u02dct+1 + D\u02dct+1 for all t \u2208N , where $\\begin{array}{r}{\\nu=\\frac{12}{\\tau_{1}\\ell}}\\end{array}$ and $\\begin{array}{r l}&{\\quad\\tilde{A}_{t}\\triangleq\\tau_{1}V_{t},\\quad\\tilde{B}_{t}\\triangleq\\frac{\\tau_{1}}{5}\\|\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})\\|^{2}+\\frac{\\tau_{2}}{8}\\|\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t})\\|^{2}+\\frac{\\beta p}{8}\\|x_{t}-z_{t}\\|^{2}}\\\\ &{\\quad\\tilde{C}_{t+1}\\triangleq\\Big[\\Big(192\\beta p\\Big(\\frac{p+\\ell}{p-\\ell}\\Big)^{2}\\ell^{2}\\tau_{2}^{2}+\\frac{4\\ell}{\\nu}+4c_{0}\\ell^{2}+2c_{0}^{\\prime}\\beta^{2}\\Big)\\tau_{1}^{2}+\\Big((p+\\ell)\\tau_{1}-1\\Big)\\tau_{1}\\Big]\\langle\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t}),\\Delta_{t}^{x}\\rangle}\\\\ &{\\qquad\\qquad+\\tau_{2}\\langle(1+\\ell\\tau_{2}+2L\\psi\\tau_{2})\\nabla_{y}f(x_{t},y_{t})-2\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t}),\\Delta_{t}^{y}\\rangle,}\\\\ &{\\tilde{D}_{t+1}\\triangleq2\\ell\\tau_{1}^{2}\\|\\Delta_{t}^{x}\\|^{2}+8\\ell\\tau_{2}^{2}\\|\\Delta_{t}^{y}\\|^{2}.}\\end{array}$ ", "page_idx": 21}, {"type": "text", "text": "Proof. In view of Theorem 7, it suffices to prove that setting $p=2\\ell$ $\\begin{array}{r}{\\ ',\\tau_{1}\\in(0,\\frac{1}{3\\ell}],\\tau_{2}=\\frac{\\tau_{1}}{48},\\beta=\\alpha\\mu\\tau_{2}}\\end{array}$ for for some positive \u03b1 \u2264 4106 and $\\begin{array}{r}{\\nu=\\frac{12}{\\tau_{1}\\ell}}\\end{array}$ leads to both $c_{0}\\geq0$ and $c_{0}^{\\prime}\\geq0$ ; furthermore, we also need to show that this choice of parameters implies the following lower bounds: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(i\\right)c_{1}\\geq\\frac{\\tau_{1}}{5},\\quad\\left(i i\\right)c_{2}\\geq\\frac{\\tau_{2}}{8},\\quad\\left(i i i\\right)c_{3}\\geq\\frac{p\\beta}{8},\\quad\\left(i v\\right)c_{7}\\geq-2\\ell\\tau_{1}^{2},\\quad\\left(v\\right)c_{8}\\geq-8\\ell\\tau_{2}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "First, we show that our parameter choice implies that $c_{0},c_{0}^{\\prime}\\geq0$ . Noting that $L_{\\Psi}=4\\ell$ , using $\\begin{array}{r}{\\tau_{1}\\leq\\frac{1}{3\\ell}}\\end{array}$ , we may bound $c_{\\mathrm{0}}$ from above and below as follows: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\tau_{2}\\geq c_{0}\\triangleq-\\tau_{2}^{2}\\ell\\nu+\\tau_{2}\\left(1-\\frac{\\ell}{2}\\tau_{2}-L_{\\Psi}\\tau_{2}\\right)}}\\\\ {{\\qquad=\\tau_{2}\\left(1-\\left(\\frac{\\ell}{2}+L_{\\Psi}+\\ell\\nu\\right)\\frac{\\tau_{1}}{48}\\right)\\geq\\tau_{2}\\left(1-\\frac{9}{2\\cdot144}-\\frac{1}{4}\\right)\\geq\\frac{1}{2}\\tau_{2}\\geq0.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Moreover, since $\\beta=\\alpha\\mu\\tau_{2}$ and $\\tau_{2}\\leq\\frac{1}{144\\ell}$ , we get $\\beta\\leq\\frac{\\alpha}{144}$ using $\\kappa\\geq1$ . Hence, for $\\begin{array}{r}{\\alpha\\leq\\frac{1}{406}}\\end{array}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n0\\leq\\frac{2p^{2}}{p-\\ell}+48\\beta\\frac{p^{3}}{(p-\\ell)^{2}}=8\\ell(1+48\\beta)=\\left(\\frac{\\alpha}{36}(1+\\alpha/3)\\right)\\frac{p}{\\beta}\\leq\\frac{p}{12\\beta},\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the last inequality follows from $\\begin{array}{r}{\\alpha\\leq\\frac{1}{406}\\leq\\frac{3}{2}(\\sqrt{5}-1)}\\end{array}$ ; therefore, $c_{0}^{\\prime}\\in[\\frac{p}{4\\beta},\\frac{p}{3\\beta}]$ . ", "page_idx": 22}, {"type": "text", "text": "Next, we prove bounds on $c_{1},c_{2},c_{3},c_{7},c_{8}$ separately. ", "page_idx": 22}, {"type": "text", "text": "Proof of part $(i)$ . Since $p=2\\ell$ , $\\begin{array}{r}{\\nu=\\frac{12}{\\tau_{1}\\ell}}\\end{array}$ and $\\tau_{1}{\\leq}\\frac{1}{3\\ell}$ , we first observe that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{2\\ell}{\\nu}\\left(\\frac{1}{(p-\\ell)^{2}}+\\tau_{1}^{2}\\right)=\\frac{\\tau_{1}}{6}(\\tau_{1}^{2}\\ell^{2}+1)\\leq\\frac{\\tau_{1}}{6}\\left(\\frac{1}{9}+1\\right)=\\frac{5}{27}\\tau_{1}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Furthermore, using $\\begin{array}{r}{\\tau_{2}=\\frac{\\tau_{1}}{48}}\\end{array}$ , and $\\beta=\\alpha\\mu\\tau_{2}$ , we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{96\\beta p\\bigg(\\frac{p+\\ell}{p-\\ell}\\bigg)^{2}\\ell^{2}\\tau_{2}^{2}\\left(\\frac{1}{(p-\\ell)^{2}}+\\tau_{1}^{2}\\right)=\\bigg[96\\beta\\ell^{2}p\\frac{(p+\\ell)^{2}}{(p-\\ell)^{4}}\\Big(1+\\tau_{1}^{2}(p-\\ell)^{2}\\Big)\\frac{\\tau_{2}^{2}}{\\tau_{1}}\\bigg]\\tau_{1}}&{}\\\\ {\\leq\\left[96\\cdot\\alpha\\tau_{2}\\mu\\cdot18\\ell\\left(1+\\frac{1}{9}\\right)\\frac{\\tau_{2}^{2}}{\\tau_{1}}\\right]\\tau_{1}}&{}\\\\ {\\leq1920\\cdot\\alpha\\cdot\\frac{\\tau_{2}^{3}\\ell^{2}}{\\tau_{1}}\\cdot\\frac{\\mu}{\\ell}\\cdot\\tau_{1}}&{}\\\\ {\\leq\\frac{5\\alpha}{2592}\\tau_{1},}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where last line follows from the fact that \u00b5/\u2113\u22641 and \u03c4 2\u03c43 \u21132 = \u03c44 12 8\u211332 $\\begin{array}{r}{\\frac{\\tau_{2}^{3}\\ell^{2}}{\\tau_{1}}=\\frac{\\tau_{1}^{2}\\ell^{2}}{48^{3}}\\leq\\frac{1}{144^{2}\\cdot48}}\\end{array}$ , in which we used $\\begin{array}{r}{\\tau_{1}\\leq\\frac{1}{3\\ell}}\\end{array}$ . ", "page_idx": 22}, {"type": "text", "text": "Since $\\tau_{2}\\geq c_{0}\\geq0$ and $\\begin{array}{r}{-2\\ell^{2}\\left(\\frac{1}{(p-\\ell)^{2}}+\\tau_{1}^{2}\\right)\\ge-\\frac{20}{9}}\\end{array}$ , using $\\begin{array}{r}{\\tau_{2}=\\frac{\\tau_{1}}{48}}\\end{array}$ , we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n-2c_{0}\\ell^{2}\\left(\\frac{1}{(p-\\ell)^{2}}+\\tau_{1}^{2}\\right)\\geq-\\frac{20\\tau_{2}}{9}=-\\frac{5}{108}\\tau_{1}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Using $\\begin{array}{r}{L_{\\Psi}=4\\ell,\\tau_{1}\\leq\\frac{1}{3\\ell}}\\end{array}$ and $\\tau_{2}\\leq\\frac{1}{144\\ell}$ , we get ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{\\ell}{6}(1+\\ell{\\tau}_{2}+2L_{\\Psi}{\\tau}_{2}){\\tau}_{1}^{2}\\leq\\frac{\\ell}{6}\\left(1+\\frac{1}{144}+\\frac{8}{144}\\right){\\tau}_{1}^{2}\\leq\\frac{1}{18}\\left(1+\\frac{1}{144}+\\frac{8}{144}\\right){\\tau}_{1}=\\frac{17}{288}{\\tau}_{1}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Using $\\begin{array}{r}{\\tau_{1}\\leq\\frac{1}{3\\ell},\\tau_{2}\\leq\\frac{1}{144\\ell}}\\end{array}$ , this implies ", "page_idx": 22}, {"type": "equation", "text": "$$\n-c_{0}^{\\prime}\\tau_{1}^{2}\\beta^{2}\\ge-\\frac{p}{3}\\beta\\tau_{1}^{2}\\ge-\\frac{2\\ell}{3}\\cdot\\frac{\\alpha\\mu}{144\\ell}\\frac{1}{3\\ell}\\tau_{1}\\ge-\\frac{\\alpha}{648}\\tau_{1}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Combining equations (29), (30), (31), (32) and (33), we obtain ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{c_{1}\\geq\\left(\\frac{1}{2}-\\frac{5}{27}-\\frac{5\\alpha}{2592}-\\frac{5}{108}-\\frac{17}{288}-\\frac{\\alpha}{648}\\right)\\tau_{1}=\\left(\\frac{1}{2}-\\frac{25}{108}-\\frac{17+\\alpha}{288}\\right)\\tau_{1}}}\\\\ {{\\phantom{\\frac{1}{2}}=\\frac{181-3\\alpha}{864}\\tau_{1}\\geq\\frac{60-\\alpha}{288}\\geq\\frac{\\tau_{1}}{5},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "for $\\begin{array}{r}{\\alpha\\leq\\frac{1}{406}<2.4}\\end{array}$ , which completes the proof of part $(i)$ . ", "page_idx": 23}, {"type": "text", "text": "Proof of part $(i i)$ . Due to our parameter choice, we first note that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{24\\beta p}{(p-\\ell)\\mu}\\left(1+\\tau_{2}\\frac{2p\\ell}{p-\\ell}\\right)^{2}=48\\alpha\\left(1+\\tau_{2}\\frac{2p\\ell}{p-\\ell}\\right)^{2}\\tau_{2}\\leq96\\alpha\\tau_{2}\\leq\\frac{\\tau_{2}}{8},\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where last line follows from $\\begin{array}{r}{\\tau_{2}\\leq\\frac{1}{144\\ell}}\\end{array}$ and $\\begin{array}{r}{\\alpha\\leq\\frac{1}{406}}\\end{array}$ . Finally, (27) implies that $\\begin{array}{r}{\\frac{c_{0}}{2}\\geq\\frac{\\tau_{2}}{4}}\\end{array}$ ; hence, $\\begin{array}{r}{c_{2}\\geq\\left(\\frac{1}{4}-\\frac{1}{8}\\right)\\tau_{2}=\\frac{\\tau_{2}}{8}}\\end{array}$ . ", "page_idx": 23}, {"type": "text", "text": "Proof of part $(i i i)$ . We have shown that $c_{0}^{\\prime}\\geq\\frac{p}{4\\beta}$ ; hence, $\\begin{array}{r}{c_{3}\\geq\\frac{p\\beta}{8}}\\end{array}$ . ", "page_idx": 23}, {"type": "text", "text": "Proof of part $(i v)$ . According to (27), $\\tau_{2}\\geq c_{0}$ ; hence, we deduce that ", "page_idx": 23}, {"type": "equation", "text": "$$\n-2c_{0}\\ell^{2}\\geq-2\\tau_{2}\\ell^{2}\\geq-\\frac{1}{72}\\ell,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where the last inequality follows from $\\begin{array}{r}{\\tau_{2}\\leq\\frac{1}{144\\ell}}\\end{array}$ . Furthermore, we note that ", "page_idx": 23}, {"type": "equation", "text": "$$\n-96\\beta p\\Bigl({\\frac{p+\\ell}{p-\\ell}}\\Bigr)^{2}\\ell^{2}\\tau_{2}^{2}=-1728\\alpha\\tau_{2}^{3}\\ell^{3}\\mu\\geq-{\\frac{\\alpha}{12^{3}}}\\ell,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "with the last inequality following also from $\\textstyle{\\frac{\\mu}{\\ell}}\\leq1$ and $\\tau_{2}\\leq\\frac{1}{144\\ell}$ . We also note that ", "page_idx": 23}, {"type": "equation", "text": "$$\n-\\frac{2\\ell}{\\nu}-\\frac{p+\\ell}{2}=-\\left(\\frac{\\ell\\tau_{1}}{6}+\\frac{3}{2}\\right)\\ell\\geq-\\frac{14}{9}\\ell.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Finally, since $c_{0}^{\\prime}\\leq\\frac{p}{3\\beta}$ according to (28), we get ", "page_idx": 23}, {"type": "equation", "text": "$$\n-c_{0}^{\\prime}\\beta^{2}\\geq-\\beta^{2}{\\frac{p}{3\\beta}}=-{\\frac{2}{3}}\\alpha\\mu\\tau_{2}\\ell\\geq-{\\frac{\\alpha}{216}}\\ell,\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we used $\\begin{array}{r}{\\tau_{2}\\leq\\frac{1}{144\\ell}}\\end{array}$ . Thus, since $\\alpha\\in(0,1)$ , it follows from (36), (37), (38) and (32) that ", "page_idx": 23}, {"type": "equation", "text": "$$\nc_{7}\\geq-\\ell\\tau_{1}^{2}\\left(\\frac{1}{72}+\\frac{\\alpha}{12^{3}}+\\frac{14}{9}+\\frac{\\alpha}{216}+\\frac{17}{96}\\right)\\geq-2\\ell\\tau_{1}^{2}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof of part $(v)$ . For $c_{8}$ , we may simply observe that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{c_{8}=-\\left(48\\beta p\\Big(\\frac{p+\\ell}{p-\\ell}\\Big)^{2}+\\frac{\\ell}{2}+L_{\\Psi}+3\\ell(1+\\ell\\tau_{2}+2L_{\\Psi}\\tau_{2})\\right)\\tau_{2}^{2}}\\\\ &{\\quad=-\\left(864\\alpha\\mu\\tau_{2}+\\frac{1}{2}+4+3\\left(1+\\ell\\tau_{2}+2L_{\\psi}\\tau_{2}\\right)\\right)\\ell\\tau_{2}^{2}}\\\\ &{\\quad\\geq-\\left(6\\alpha\\frac{\\mu}{\\ell}+\\frac{1}{2}+4+3\\left(1+\\frac{1}{144}+\\frac{8}{144}\\right)\\right)\\ell\\tau_{2}^{2}\\geq-8\\ell\\tau_{2}^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where we used $\\begin{array}{r}{\\mu/\\ell\\leq1,L_{\\Psi}=4\\ell,\\tau_{2}\\leq\\frac{1}{144\\ell}}\\end{array}$ , and $\\begin{array}{r}{\\alpha<\\frac{1}{20}}\\end{array}$ . ", "page_idx": 23}, {"type": "text", "text": "E Proof of Theorem 9 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Theorem 9. Let $\\{\\mathcal{F}_{t}\\}_{t\\in\\mathbb{N}}$ be a flitration on $\\textstyle(\\Omega,{\\mathcal{F}},\\mathbb{P})$ . Let $A_{t},B_{t},C_{t},D_{t}$ be four stochastic processes adapted to the filtration such that there exist $\\sigma_{C},\\sigma_{D}>0$ and $\\tau_{1}>0$ such that for all $t\\in\\mathbb{N}$ : (i) $B_{t}~\\geq~0,$ $\\geq\\;0,\\;(i i)\\;\\mathbb{E}[e^{\\lambda C_{t+1}}\\;\\;|\\;\\;\\mathcal{F}_{t}]\\;\\leq\\;e^{\\lambda^{2}\\sigma_{C}^{2}B_{t}}$ for all $\\lambda\\;>\\;0,\\;(i i i)\\;\\mathbb{E}[e^{\\lambda D_{t+1}}\\;\\mid\\;{\\mathcal F}_{t}]\\;\\le\\;e^{\\lambda\\sigma_{D}^{2}}$ for all $\\begin{array}{r}{\\lambda\\in\\left[0,\\frac{1}{\\sigma_{D}^{2}}\\right]}\\end{array}$ and $\\begin{array}{r}{(i v)\\ \\frac{A_{t+1}-A_{t}}{\\tau_{1}}\\leq-B_{t}+C_{t+1}+D_{t+1}}\\end{array}$ . Then, for any $\\bar{q}\\in(0,1]$ , we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\frac{\\tau_{1}}{2}\\sum_{t=0}^{T-1}B_{t}\\leq(A_{0}-A_{T})+\\tau_{1}\\sigma_{D}^{2}T+2\\tau_{1}\\operatorname*{max}\\{2\\sigma_{C}^{2},\\sigma_{D}^{2}\\}\\log\\left(\\frac{1}{\\bar{q}}\\right)\\right)\\geq1-\\bar{q}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. For some fixed $\\gamma>0$ , let ", "page_idx": 24}, {"type": "equation", "text": "$$\nS_{T}\\triangleq\\gamma\\sum_{t=0}^{T-1}B_{t}-(A_{0}-A_{T})\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "with the convention that $S_{0}\\triangleq0$ . For any $T\\in\\mathbb N$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{S_{T+1}=\\gamma\\displaystyle\\sum_{t=0}^{T}B_{t}-(A_{0}-A_{T+1})}\\\\ {\\qquad=\\gamma\\displaystyle\\sum_{t=0}^{T-1}B_{t}-(A_{0}-A_{T})+\\gamma B_{T}-(A_{T}-A_{T+1})}\\\\ {\\qquad=S_{T}+\\gamma B_{T}-(A_{T}-A_{T+1})}\\\\ {\\qquad=S_{T}+(\\gamma-\\tau_{1})B_{T}+\\tau_{1}B_{T}-(A_{T}-A_{T+1})}\\\\ {\\qquad\\leq S_{T}+(\\gamma-\\tau_{1})B_{T}+\\tau_{1}C_{T+1}+\\tau_{1}D_{T+1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the inequality follows from condition (iv) of the hypothesis. Hence, for any 0 < \u03bb \u22642\u03c411\u03c32D and any $T\\in\\mathbb N$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[e^{\\lambda S_{T+1}}\\mid\\mathcal{F}_{T}]\\le\\mathbb{E}[e^{\\lambda S_{T}}e^{\\lambda(\\gamma-\\tau_{1})B_{T}}e^{\\lambda\\tau_{1}C_{T+1}}e^{\\lambda\\tau_{1}D_{T+1}}\\mid\\mathcal{F}_{T}]}\\\\ &{\\qquad\\qquad\\qquad\\le e^{\\lambda S_{T}}e^{\\lambda(\\gamma-\\tau_{1})B_{T}}\\mathbb{E}[e^{2\\lambda\\tau_{1}C_{T+1}}\\mid\\mathcal{F}_{T}]^{\\frac{1}{2}}\\mathbb{E}[e^{2\\lambda\\tau_{1}D_{T+1}}\\mid\\mathcal{F}_{T}]^{\\frac{1}{2}}}\\\\ &{\\qquad\\qquad\\le e^{\\lambda S_{T}}e^{\\lambda(\\gamma-\\tau_{1})B_{T}}\\left(e^{4\\lambda^{2}\\tau_{1}^{2}\\sigma_{C}^{2}B_{T}}\\right)^{\\frac{1}{2}}\\left(e^{2\\lambda\\tau_{1}\\sigma_{D}^{2}}\\right)^{\\frac{1}{2}}}\\\\ &{\\qquad\\qquad=e^{\\lambda S_{T}}e^{\\lambda(\\gamma-\\tau_{1}+2\\tau_{1}^{2}\\lambda\\sigma_{C}^{2})B_{T}}e^{\\lambda\\tau_{1}\\sigma_{D}^{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the first inequality follows from Cauchy-Schwarz and in the second one we use $(i i)$ and $(i i i)$ of the hypothesis. Fixing \u03b3 = \u03c41/2 yields for all 0 < \u03bb \u22644\u03c411\u03c32 : ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\gamma-\\tau_{1}+2{\\tau_{1}}^{2}\\lambda\\sigma_{C}^{2}=\\tau_{1}\\left(-\\frac{1}{2}+2\\lambda\\sigma_{C}^{2}\\tau_{1}\\right)\\le0.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, for $\\begin{array}{r}{0<\\lambda\\leq\\operatorname*{min}\\left\\{\\frac{1}{4\\tau_{1}\\sigma_{C}^{2}},\\frac{1}{2\\tau_{1}\\sigma_{D}^{2}}\\right\\}}\\end{array}$ , using $B_{T}\\geq0$ by $(i)$ of the hypothesis, we get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}[e^{\\lambda S_{T+1}}\\mid\\mathcal{F}_{T}]\\le e^{\\lambda S_{T}}e^{\\lambda\\tau_{1}\\sigma_{D}^{2}},\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and rolling this recursion backwards and noting $S_{0}=0$ yields: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}[e^{\\lambda S_{T}}]\\leq e^{\\lambda\\tau_{1}\\sigma_{D}^{2}T};\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "thus, using a Chernoff bound, we get ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{P}(S_{T}>t)\\le\\mathbb{E}[e^{\\lambda S_{T}}]e^{-\\lambda t}\\le e^{\\lambda(\\tau_{1}\\sigma_{D}^{2}T-t)}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Since for $\\bar{q}\\in(0,1]$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\ne^{\\lambda(\\tau_{1}\\sigma_{D}^{2}T-t)}\\leq\\bar{q}\\iff t\\geq\\tau_{1}\\sigma_{D}^{2}T-\\frac{1}{\\lambda}\\log(\\bar{q}),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{P}\\left(\\frac{\\tau_{1}}{2}\\sum_{t=0}^{T-1}B_{t}\\le(A_{0}-A_{T})+\\tau_{1}\\sigma_{D}^{2}T-\\frac{1}{\\lambda}\\log(\\bar{q})\\right)\\ge1-\\bar{q}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The claim follows by taking $\\begin{array}{r}{\\lambda=\\frac{1}{2\\tau_{1}}\\operatorname*{min}\\left\\{\\frac{1}{2\\sigma_{C}^{2}},\\frac{1}{\\sigma_{D}^{2}}\\right\\}}\\end{array}$ ", "page_idx": 24}, {"type": "text", "text": "F Proof of Theorem 11 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Theorem 11. In the premise of Corollary 8, sm-AGDA iterates $\\left({{x}_{t}},{{y}_{t}}\\right)$ for $\\begin{array}{r}{\\tau_{1}\\leq\\frac{1}{3\\ell}}\\end{array}$ satisfy ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Bigg(\\frac{1}{T}\\sum_{t=0}^{T-1}\\big[\\|\\nabla_{x}f(x_{t},y_{t})\\|^{2}+\\kappa\\|\\nabla_{y}f(x_{t},y_{t})\\|^{2}\\big]\\leq\\mathcal{Q}_{\\bar{q},T,\\cdot}\\Bigg)\\geq1-\\bar{q},\\quad\\forall\\,T\\in\\mathbb{N},\\quad\\forall\\,\\bar{q}\\in(0,1],\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for some $\\begin{array}{r}{\\mathcal{Q}_{\\bar{q},T}=\\mathcal{O}\\Big(\\frac{\\kappa(\\Delta_{0}+b_{0})}{\\tau_{1}T}+\\kappa(\\delta_{x}^{2}+\\delta_{y}^{2})\\Big(\\tau_{1}\\ell+\\frac{1}{T}\\log\\Big(\\frac{1}{\\bar{q}}\\Big)\\Big)\\Big)}\\end{array}$ explicitly stated in Appendix $F_{;}$ where $\\begin{array}{r}{\\lambda_{0}\\triangleq\\Phi(z_{0})-\\Phi^{*},\\,b_{0}\\triangleq2\\operatorname*{sup}_{x,y}\\{\\hat{f}(x_{0},y;z_{0})-\\hat{f}(x,y_{0};z_{0})\\}}\\end{array}$ . ", "page_idx": 25}, {"type": "text", "text": "As a first step, we provide a helper lemma that shows that our concentration result (Theorem 9) is applicable to the sm-AGDA-related processes $\\tilde{A}_{t},\\tilde{B}_{t},\\tilde{C}_{t},\\tilde{D}_{t}$ introduced in Corollary 8. ", "page_idx": 25}, {"type": "text", "text": "Lemma 16. Let $A_{t}\\ =\\ \\tilde{A}_{t}$ , $B_{t}\\;=\\;\\tilde{B}_{t}$ , $C_{t}\\;=\\;\\tilde{C}_{t}$ , $D_{t}\\;=\\;\\tilde{D}_{t}$ , where $\\tilde{A}_{t},\\tilde{B}_{t},\\tilde{C}_{t},\\tilde{D}_{t}$ are defined in Corollary 8; moreover, let $\\tau_{1}~>~0$ be the primal stepsize in sm-AGDA. Then, the processes $A_{t},B_{t},C_{t},D_{t}$ are adapted to the flitration $\\mathcal{F}_{t}\\triangleq\\mathcal{F}_{t}^{y}$ , where $\\mathcal{F}_{t}^{y}$ is defined in (6), and they satisfy the conditions of Theorem $^{9}$ with the following constants: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(i)\\;\\sigma_{C}^{2}=\\tau_{1}(240\\delta_{x}^{2}+32\\delta_{y}^{2}),\\quad(i i)\\quad\\sigma_{D}^{2}=16\\ell\\tau_{1}^{2}\\delta_{x}^{2}+64\\ell\\tau_{2}^{2}\\delta_{y}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. The fact that $A_{t},B_{t},C_{t},D_{t}$ are measurable with respect to $\\mathcal{F}_{t}=\\mathcal{F}_{t}^{y}$ for any $t\\in\\mathbb{N}$ follows directly from the definition of $\\mathcal{F}_{t}$ . Note that $x_{t}$ and $y_{t}$ are also $\\mathcal{F}_{t}$ -measurable for all $t\\in\\mathbb{N}$ . We prove part $(i)$ and part $(i i)$ separately. ", "page_idx": 25}, {"type": "text", "text": "Proof of part $(i)$ . First, recall that $C_{t+1}=\\tilde{C}_{t+1}=-c_{4}\\langle\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t}),\\Delta_{t}^{x}\\rangle-\\langle c_{5}\\nabla_{y}f(x_{t},y_{t})+$ $c_{6}\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t}),\\Delta_{t}^{y}\\rangle$ . We would like to show that $\\mathbb{E}[e^{\\lambda C_{t+1}}\\mid\\mathcal{F}_{t}]\\le e^{\\lambda^{2}\\sigma_{C}^{2}B_{t}}$ for all $\\lambda>0$ . From Assumption 3, we note that for any $\\lambda\\geq0$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\!\\left[\\exp\\left(\\lambda\\langle-c_{4}\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t}),\\Delta_{t}^{x}\\rangle\\right)\\mid\\mathcal{F}_{t}\\right]\\leq\\exp\\left(8\\lambda^{2}c_{4}^{2}\\|\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})\\|^{2}\\delta_{x}^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where we used [35, Lemma 3]. Now, given the value of $c_{4}$ in Theorem 7 and the convexity of $t\\mapsto t^{2}$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{c_{4}^{2}\\leq5\\left(192\\beta p\\Big(\\frac{p+\\ell}{p-\\ell}\\Big)^{2}\\ell^{2}\\tau_{2}^{2}\\tau_{1}^{2}\\right)^{2}+5\\tau_{1}^{2}\\Big((p+\\ell)\\tau_{1}-1\\Big)^{2}+5\\left(\\frac{4\\ell}{\\nu}\\tau_{1}^{2}\\right)^{2}+5\\left(4c_{0}\\ell^{2}\\tau_{1}^{2}\\right)^{2}+5\\left(2c_{0}^{\\prime}\\beta^{2}\\tau_{1}^{2}\\right)^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Now, leveraging the stepsize policy specified in Corollary 8, since $\\alpha\\in(0,1)$ , using (37) we get ", "page_idx": 25}, {"type": "equation", "text": "$$\n5\\left(192\\beta p\\Big(\\frac{p+\\ell}{p-\\ell}\\Big)^{2}\\ell^{2}\\tau_{2}^{2}\\tau_{1}^{2}\\right)^{2}\\le20\\cdot\\left(\\frac{\\alpha}{12^{3}}\\ell\\tau_{1}^{2}\\right)^{2}\\le20\\cdot\\left(\\frac{\\alpha}{3\\cdot12^{3}}\\right)^{2}\\tau_{1}^{2}\\le\\frac{\\tau_{1}^{2}}{4}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Similarly, as $\\begin{array}{r}{\\tau_{1}\\leq\\frac{1}{3\\ell}}\\end{array}$ , have $|(p+\\ell)\\tau_{1}-1)|\\in[0,1]$ , which implies ", "page_idx": 25}, {"type": "equation", "text": "$$\n5\\tau_{1}^{2}\\Big((p+\\ell)\\tau_{1}-1\\Big)^{2}\\leq5\\tau_{1}^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since $\\begin{array}{r}{\\nu=\\frac{12}{\\tau_{1}\\ell}}\\end{array}$ , we have $\\begin{array}{r}{\\left.\\gamma\\left(\\frac{4\\ell}{\\nu}\\tau_{1}^{2}\\right)^{2}\\leq\\frac{5}{9}\\ell^{4}\\tau_{1}^{6}\\leq\\frac{\\tau_{1}^{2}}{4}}\\end{array}$ . Furthermore, using (27), we have 5 $\\left(4c_{0}\\ell^{2}\\tau_{1}^{2}\\right)^{2}\\leq$ $\\begin{array}{r}{80\\tau_{1}^{4}\\tau_{2}^{2}\\ell^{4}\\leq\\frac{\\tau_{1}^{2}}{4}}\\end{array}$ . Finally, (38) and $\\alpha\\in(0,1)$ imply that ", "page_idx": 25}, {"type": "equation", "text": "$$\n5\\left(2c_{0}^{\\prime}\\beta^{2}\\tau_{1}^{2}\\right)^{2}\\leq20\\left(\\frac{\\alpha}{216}\\ell\\tau_{1}\\right)^{2}\\tau_{1}^{2}\\leq\\frac{\\tau_{1}^{2}}{4}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thus, $c_{4}^{2}\\leq6\\tau_{1}^{2}$ , which implies that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}\\Bigg[\\exp\\left(-\\lambda\\langle c_{4}\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t}),\\Delta_{t}^{x}\\rangle\\right)\\mid\\mathcal{F}_{t}\\Bigg]\\leq\\exp\\left(48\\lambda^{2}\\tau_{1}^{2}\\|\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})\\|^{2}\\delta_{x}^{2}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Moreover, noting $\\Delta_{t}^{y}$ is revealed after $\\Delta_{t}^{x}$ , using [35, Lemma 3] again along with the inequality $\\|u+v\\|\\leq2\\|u\\|^{\\frac{\\gamma}{2}}+^{2}\\|v\\|^{2}$ , we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbb{E}\\Bigg[\\exp\\left(-\\lambda\\langle c_{5}\\nabla_{y}f(x_{t},y_{t})+c_{6}\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t}),\\Delta_{t}^{y}\\rangle\\mid\\mathcal{F}_{t},\\Delta_{t}^{x}\\right)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\mathbb{E}\\Bigg[\\exp\\left(\\lambda\\tau_{2}\\langle(1+\\ell\\tau_{2}+2L_{\\Psi}\\tau_{2})\\,\\nabla_{y}f(x_{t},y_{t})-2\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t}),\\,\\Delta_{t}^{y}\\rangle\\mid\\mathcal{F}_{t},\\Delta_{t}^{x}\\right)\\Bigg]}\\\\ &{\\leq\\exp\\left(8\\lambda^{2}\\tau_{2}^{2}\\left\\|(1+\\ell\\tau_{2}+2L_{\\Psi}\\tau_{2}\\right)\\nabla_{y}f(x_{t},y_{t})-2\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t})\\right\\|^{2}\\delta_{y}^{2}\\right)}\\\\ &{\\leq\\exp\\left(64\\lambda^{2}\\tau_{2}^{2}\\left\\|\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t})\\right\\|^{2}\\delta_{y}^{2}+16\\lambda^{2}\\tau_{2}^{2}\\left(1+\\ell\\tau_{2}+2L_{\\Psi}\\tau_{2}\\right)^{2}\\left\\|\\nabla_{y}f(x_{t},y_{t})\\right\\|^{2}\\delta_{y}^{2}\\right)}\\\\ &{\\leq\\exp\\left(64\\lambda^{2}\\tau_{2}^{2}\\left(\\left\\|\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t})\\right\\|^{2}+\\left\\|\\nabla_{y}f(x_{t},y_{t})\\right\\|^{2}\\right)\\delta_{y}^{2}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the last line follows from $\\begin{array}{r}{0<1+\\ell\\tau_{2}+2L_{\\Psi}\\tau_{2}\\leq1+\\frac{1}{144}+\\frac{8}{144}\\leq2.}\\end{array}$ . ", "page_idx": 26}, {"type": "text", "text": "Thus, since $\\Delta_{t}^{y}$ is revealed after $\\Delta_{t}^{x}$ , using (39) and (41) together with the tower property of the conditional expectations, we get ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\Bigg[\\exp\\Bigg(-\\lambda\\left(c_{4}\\langle\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t}),\\Delta_{t}^{x}\\rangle+\\langle c_{5}\\nabla_{y}f(x_{t},y_{t})+c_{6}\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t}),\\Delta_{t}^{y}\\rangle\\right)\\Bigg]}\\\\ &{\\qquad\\leq\\exp\\Bigg(48\\lambda^{2}\\tau_{1}^{2}\\|\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})\\|^{2}\\delta_{x}^{2}+64\\lambda^{2}\\tau_{2}^{2}\\left(\\|\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t})\\|^{2}+\\|\\nabla_{y}f(x_{t},y_{t})\\|^{2}\\right)\\delta_{y}^{2}\\Bigg).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Finally, observe that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Vert\\nabla_{y}f(x_{t},y_{t})\\Vert-\\Vert\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t})\\Vert\\leq\\Vert\\nabla_{y}f(x_{t},y_{t})-\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t})\\Vert}&{}\\\\ {\\leq\\ell\\Vert x_{t}-x^{*}(y_{t},z_{t})\\Vert}&{}\\\\ {\\leq\\displaystyle\\frac{\\ell}{p-\\ell}\\Vert\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})\\Vert,}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where in the last inequality we used the $(p-\\ell)$ -strong convexity of $\\hat{f}(\\cdot,y_{t};z_{t})$ and the fact that we have $\\nabla\\hat{f}_{x}(x^{*}(y_{t},z_{t}),y_{t};z_{t})=0$ . Therefore, since $p=2\\ell$ , we obtain ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\|\\nabla_{y}f(x_{t},y_{t})\\|^{2}\\leq2\\|\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t})\\|^{2}+2\\|\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})\\|^{2}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Plugging this inequality into (42) yields ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\Bigg[\\exp\\Bigg(-\\lambda\\left(c_{4}\\langle\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t}),\\Delta_{t}^{x}\\rangle+\\langle c_{5}\\nabla_{y}f(x_{t},y_{t})+c_{6}\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t}),\\Delta_{t}^{y}\\rangle\\right)\\Bigg]}\\\\ &{\\qquad\\leq\\exp\\Bigg(\\lambda^{2}\\left(48\\tau_{1}^{2}\\delta_{x}^{2}+128\\tau_{2}^{2}\\delta_{y}^{2}\\right)\\|\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})\\|^{2}+192\\lambda^{2}\\tau_{2}^{2}\\delta_{y}^{2}\\|\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t})\\|^{2}\\Bigg)}\\\\ &{\\qquad\\leq\\exp\\Bigg(\\lambda^{2}\\operatorname*{max}\\bigg\\{\\frac{5}{\\tau_{1}}\\left(48\\tau_{1}^{2}\\delta_{x}^{2}+128\\tau_{2}^{2}\\delta_{y}^{2}\\right),~1536\\lambda^{2}\\tau_{2}\\delta_{y}^{2}\\bigg\\}B_{t}\\Bigg)}\\\\ &{\\qquad\\leq\\exp\\Bigg(\\lambda^{2}\\tau_{1}\\left(240\\delta_{x}^{2}+32\\delta_{y}^{2}\\right)B_{t}\\Bigg),}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the last inequality follows from $\\begin{array}{r}{\\tau_{2}=\\frac{\\tau_{1}}{48}}\\end{array}$ . ", "page_idx": 26}, {"type": "text", "text": "Proof of part $(i i)$ . Recall that $D_{t+1}\\triangleq\\tilde{D}_{t+1}=2\\ell\\tau_{1}^{2}\\|\\Delta_{t}^{x}\\|^{2}+8\\ell\\tau_{2}^{2}\\|\\Delta_{t}^{y}\\|^{2}$ . We would like to show that $\\mathbb{E}[e^{\\lambda D_{t+1}}\\mid\\mathcal{F}_{t}]\\le e^{\\lambda\\sigma_{D}^{2}}$ for all $\\begin{array}{r}{\\lambda\\in\\left[0,\\frac{1}{\\sigma_{D}^{2}}\\right]}\\end{array}$ for some $\\sigma_{D}>0$ . First, observe that for any $\\lambda>0$ such that $\\begin{array}{r}{\\lambda\\leq\\operatorname*{min}\\Bigl\\{\\frac{1}{16\\ell\\tau_{1}^{2}\\delta_{x}^{2}}}\\end{array}$ , $\\frac{1}{64\\ell\\tau_{2}^{2}\\delta_{y}^{2}}\\}$ , we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[e^{\\lambda D_{t+1}}\\mid\\mathcal{F}_{t}]=\\mathbb{E}\\left[e^{2\\lambda\\ell\\tau_{1}^{2}\\|\\Delta_{t}^{x}\\|^{2}+8\\lambda\\ell\\tau_{2}^{2}\\|\\Delta_{t}^{y}\\|^{2}}\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\mathbb{E}\\left[e^{4\\lambda\\ell\\tau_{1}^{2}\\|\\Delta_{t}^{x}\\|^{2}}\\right]^{\\frac{1}{2}}\\mathbb{E}\\left[e^{16\\lambda\\ell\\tau_{2}^{2}\\|\\Delta_{t}^{y}\\|^{2}}\\right]^{\\frac{1}{2}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leq\\left(e^{32\\lambda\\ell\\tau_{1}^{2}\\delta_{x}^{2}}\\right)^{\\frac{1}{2}}\\left(e^{128\\lambda\\ell\\tau_{2}^{2}\\delta_{y}^{2}}\\right)^{\\frac{1}{2}}}\\\\ &{\\quad\\quad\\quad\\quad\\leq e^{\\lambda(16\\ell\\tau_{1}^{2}\\delta_{x}^{2}+64\\ell\\tau_{2}^{2}\\delta_{y}^{2})},}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where we used [35, Lemma 2] in the second inequality. ", "page_idx": 26}, {"type": "text", "text": "Before completing the proof of Theorem 11, we provide another lemma that gives a lower bound on $B_{t}$ in terms of the squared norm of the partial gradients, based on our choice of sm-AGDA parameters. Lemma 17. For any $t\\in\\mathbb{N}$ , we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\|\\nabla_{x}f(x_{t},y_{t})\\|^{2}+\\kappa\\|\\nabla_{y}f(x_{t},y_{t})\\|^{2}\\leq\\operatorname*{max}\\left\\{\\frac{20\\kappa}{\\tau_{1}},\\frac{16\\kappa}{\\tau_{2}},\\frac{32\\ell}{\\beta}\\right\\}B_{t}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. Since $\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})=\\nabla_{x}f(x_{t},y_{t})+p(x_{t}-z_{t})$ , using $\\|a+b\\|^{2}\\leq2\\|a\\|^{2}+2\\|b\\|^{2}$ , we get ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla_{x}f(x_{t},y_{t})\\|^{2}\\leq2\\|\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})\\|^{2}+2p^{2}\\|x_{t}-z_{t}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Hence, together with (43) and the definition of $B_{t}$ , we obtain ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{x}f(x_{t},y_{t})\\|^{2}+\\kappa\\|\\nabla_{y}f(x_{t},y_{t})\\|^{2}}\\\\ &{\\qquad\\leq(2+2\\kappa)\\|\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})\\|^{2}+2\\kappa\\|\\nabla_{y}f(x^{*}(y_{t},z_{t}),y_{t})\\|^{2}+2p^{2}\\|x_{t}-z_{t}\\|^{2}}\\\\ &{\\qquad\\leq\\operatorname*{max}\\left\\{\\displaystyle\\frac{(2+2\\kappa)\\cdot5}{\\tau_{1}},\\displaystyle\\frac{2\\kappa\\cdot8}{\\tau_{2}},\\displaystyle\\frac{2p^{2}\\cdot8}{\\beta p}\\right\\}B_{t}}\\\\ &{\\qquad\\leq\\operatorname*{max}\\left\\{\\displaystyle\\frac{20\\kappa}{\\tau_{1}},\\displaystyle\\frac{16\\kappa}{\\tau_{2}},\\displaystyle\\frac{32\\ell}{\\beta}\\right\\}B_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "with the last inequality following from $\\kappa\\geq1$ . ", "page_idx": 27}, {"type": "text", "text": "We may now provide our proof for Theorem 11. ", "page_idx": 27}, {"type": "text", "text": "Proof of Theorem $_{l l}$ . According to Lemma 16, the processes $A_{t},B_{t},C_{t}$ , and $D_{t}$ defined in the statement of Lemma 16 satisfy the conditions of Theorem 9 if we set $\\tau_{1}$ as the primal stepsize of sm-AGDA. Therefore, for any $\\bar{q}\\in[0,1)$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbb{P}\\Bigg(\\frac{\\tau_{1}}{2}\\sum_{t=0}^{T-1}B_{t}\\leq\\tau_{1}(V_{0}-V_{T})+\\tau_{1}\\sigma_{D}^{2}T+2\\tau_{1}\\operatorname*{max}\\left\\{2\\sigma_{C}^{2},\\sigma_{D}^{2}\\right\\}\\log\\Bigg(\\frac{1}{\\bar{q}}\\Bigg)\\Bigg)\\geq1-\\bar{q}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Thus, dividing by $\\begin{array}{r}{\\frac{\\tau_{1}}{2}T}\\end{array}$ and using Lemma 17, we can conclude that with the probability at least $1-\\bar{q}$ , the following event holds: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac1T\\sum_{t=0}^{T-1}\\|\\nabla_{x}f(x_{t},y_{t})\\|^{2}+\\kappa\\|\\nabla_{y}f(x_{t},y_{t})\\|^{2}}\\\\ {\\displaystyle\\qquad\\le2\\operatorname*{max}\\left\\{\\frac{20\\kappa}{\\tau_{1}},\\frac{16\\kappa}{\\tau_{2}},\\frac{32\\ell}{\\beta}\\right\\}\\left(\\frac1T\\left(V_{0}-V_{T}\\right)+\\sigma_{D}^{2}+\\frac1T\\operatorname*{max}\\left\\{4\\sigma_{C}^{2},2\\sigma_{D}^{2}\\right\\}\\log\\left(\\frac1{\\bar{q}}\\right)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Finally, we can relate the potential gap $V_{0}\\mathrm{~-~}V_{T}$ to the primal suboptimality, i.e., $\\Delta_{0}$ , and to the duality gap, i.e., $b_{0}/2=\\operatorname*{sup}_{x,y}\\{\\hat{f}(x_{0},y;z_{0})-\\hat{f}(x,y_{0};z_{0})$ , at the initialization, following the same arguments provided in [66]. More precisely, for $V(x,y,z)\\triangleq\\hat{f}(x,y;z)-2\\Psi(y,z)+2P(z)$ , we first observe that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{0}-V_{T}\\leq V_{0}-\\underset{x,y,z}{\\operatorname*{min}}V(x,y,z)=V_{0}-\\underset{x,y,z}{\\operatorname*{min}}\\,\\hat{f}(x,y;z)-2\\Psi(y,z)+2P(z)}\\\\ &{\\qquad\\qquad\\leq V_{0}-\\underset{z}{\\operatorname*{min}}P(z).}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where last line follows from $\\hat{f}(x,y;z)-\\Psi(y,z)\\geq0$ for all $y,z$ and $P(z)-\\Psi(y,z)\\ge0$ for all $y,z$ . Since $p=2\\ell$ , we also note that ", "page_idx": 27}, {"type": "equation", "text": "$$\nP(z_{0})=\\operatorname*{min}_{x}\\operatorname*{max}_{y}f(x,y)+\\ell\\|x-z_{0}\\|^{2}\\leq\\operatorname*{max}_{y}f(z_{0},y)=\\Phi(z_{0}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Finally, since $P$ is the Moreau envelope of $\\Phi$ , we have ${\\mathrm{min}}_{z}\\,P(z)={\\mathrm{min}}_{z}\\,\\Phi(z)$ ; therefore, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{0}-\\underset{z}{\\operatorname*{min}}P(z)=\\hat{f}(x_{0},y_{0};z_{0})-2\\Psi(y_{0},z_{0})+2P(z_{0})-\\underset{z}{\\operatorname*{min}}\\,\\Phi(z)}\\\\ &{\\qquad\\qquad\\qquad\\leq\\Phi(z_{0})-\\underset{z}{\\operatorname*{min}}\\,\\Phi(z)+\\hat{f}(x_{0},y_{0};z_{0})-\\Psi(y_{0},z_{0})+P(z_{0})-\\Psi(y_{0},z_{0})}\\\\ &{\\qquad\\qquad\\qquad\\leq\\Phi(z_{0})-\\underset{z}{\\operatorname*{min}}\\,\\Phi(z)+\\frac{1}{2}b_{0}+\\frac{1}{2}b_{0}=\\Delta_{0}+b_{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Note that $\\begin{array}{r}{\\tau_{2}=\\frac{\\tau_{1}}{48}}\\end{array}$ implies $16\\kappa/\\tau_{2}\\geq20\\kappa/\\tau_{1}$ , and $32\\ell/\\beta=\\textstyle{\\frac{32}{\\alpha}}\\cdot\\kappa/\\tau_{2}>16\\kappa/\\tau_{2}$ since $\\alpha\\in(0,1)$ . Therefore, ", "page_idx": 28}, {"type": "equation", "text": "$$\n2\\operatorname*{max}\\left\\{\\frac{20\\kappa}{\\tau_{1}},\\frac{16\\kappa}{\\tau_{2}},\\frac{32\\ell}{\\beta}\\right\\}=\\frac{64}{\\alpha}\\cdot\\kappa/\\tau_{2}.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Therefore, combining (45), (46), (47) and (48), we conclude that $\\mathcal{Q}_{q,T}$ has the following explicit form: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{Q}_{\\bar{q},T}=r_{1}\\Big\\{\\frac{\\Delta_{0}+b_{0}}{T}+r_{2}+\\frac{r_{3}}{T}\\log\\Big(\\frac{1}{\\bar{q}}\\Big)\\Big\\},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the constants $r_{1},r_{2}$ and $r_{3}$ are defined as ", "page_idx": 28}, {"type": "equation", "text": "$$\nr_{1}=\\frac{64}{\\alpha}\\frac{\\kappa}{\\tau_{2}},\\;r_{2}=\\sigma_{D}^{2}=16\\ell\\tau_{1}\\Big(\\tau_{1}\\delta_{x}^{2}+\\frac{1}{12}\\tau_{2}\\delta_{y}^{2}\\Big),\\;r_{3}=\\operatorname*{max}\\{4\\sigma_{C}^{2},2\\sigma_{D}^{2}\\}=4\\sigma_{C}^{2}=4\\tau_{1}\\big(240\\delta_{x}^{2}+32\\delta_{y}^{2}\\big),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the equalities follow from the expressions of $\\sigma_{C}^{2}$ and $\\sigma_{D}^{2}$ provided in Lemma 16. ", "page_idx": 28}, {"type": "text", "text": "G Proof of Corollary 14 ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Setting $\\tau_{2}~~=~~\\tau_{1}/48$ for any $\\tau_{1}~>~0$ implies that $\\mathcal{Q}_{\\bar{q},T}$ defined in (49) satisfies $\\begin{array}{r l}{\\mathcal{Q}_{\\bar{q},T}}&{{}=}\\end{array}$ O \u03ba(\u03c4\u22060T+b)+ \u03c41\u2113\u03ba\u03b42 + \u03b42T\u03ba log q1\u00af  . Hence, setting \u03c41 = min $\\begin{array}{r}{\\tau_{1}=\\operatorname*{min}\\left(\\frac{1}{3\\ell},\\frac{48\\sqrt{\\Delta_{0}+b_{0}}}{\\sqrt{T\\ell\\delta^{2}}}\\right)}\\end{array}$ ensures that ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\mathcal{Q}_{q,T}=\\mathcal{O}\\left(\\frac{(\\Delta_{0}+b_{0})\\ell\\kappa}{T}+\\sqrt{\\frac{(\\Delta_{0}+b_{0})\\ell}{T}}\\delta\\kappa+\\frac{\\delta^{2}\\kappa}{T}\\log{\\left(\\frac{1}{\\bar{q}}\\right)}\\right).\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Finally, to obtain an $O(\\varepsilon,\\varepsilon/\\sqrt{\\kappa})$ -stationary point, it suffices to have the above bound smaller than $O(\\varepsilon^{2})$ , which is guaranteed when the following three conditions are met up to a constant factor: $(i)$ (\u22060+Tb0)\u2113\u03ba \u2264 \u03b532 , (ii) (\u22060T+b0)\u2113\u03b4\u03ba \u2264\u03b532 , and (iii) \u03b42T\u03ba log q1\u00af \u2264\u03b532 . This is directly implied by the value $T_{\\varepsilon,\\bar{q}}$ given in our corollary statement. ", "page_idx": 28}, {"type": "text", "text": "H Supplementary Lemmas ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In this section, we present a sequence of supplementary lemmas essential for deriving our main result, Theorem (11). Some of these results are well-known and are directly referenced. For others, we have improved specific algebraic constants. Additionally, some lemmas are extensions of existing bounds provided in expectation. ", "page_idx": 28}, {"type": "text", "text": "Lemma 18. For any $z\\in\\mathbb{R}^{d_{1}}$ and $y_{1},y_{2}\\in\\mathbb{R}^{d_{2}}$ , \u2225x\u2217(y1, z) \u2212x\u2217(y2, z)\u2225\u2264 pp\u2212+\u2113\u2113 \u2225y1 \u2212y2\u2225. ", "page_idx": 28}, {"type": "text", "text": "Proof. This result is provided in [66, Lemma C.1], which immediately follows from [41, Lemma B.2, part (c)]. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "Lemma 19. For any $z\\in\\mathbb{R}^{d_{1}}$ , the map $y\\mapsto\\Psi(y,z)=\\operatorname*{min}_{x}\\hat{f}(x,y,z)$ is $\\begin{array}{r}{\\ell\\left(1+\\frac{p+\\ell}{p-\\ell}\\right)}\\end{array}$ -smooth. ", "page_idx": 28}, {"type": "text", "text": "Proof. This result immediately follows from [41, Lemma B.2, part (d)]. Indeed, for any $y_{1},y_{2}\\in\\mathbb{R}^{d_{2}}$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\nabla_{y}\\Psi(y_{1},z)-\\nabla_{y}\\Psi(y_{2},z)\\|}\\\\ &{\\qquad=\\|\\nabla_{y}f(x^{*}(y_{1},z),y_{1})-\\nabla_{y}f(x^{*}(y_{2},z),y_{2})\\|}\\\\ &{\\qquad\\leq\\ell\\|x^{*}(y_{1},z)-x^{*}(y_{2},z)\\|+\\ell\\|y_{1}-y_{2}\\|\\leq\\left(1+\\displaystyle\\frac{p+\\ell}{p-\\ell}\\right)\\ell\\|y_{1}-y_{2}\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where the first equality follows from Danskin\u2019s theorem, the first inequality follos from $\\ell$ -smoothness of $f$ , and the last inequality follows from Lemma 18. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "Lemma 20. Consider the iterates $(x_{t},y_{t},z_{t})$ of the sm-AGDA algorithm. For any $t\\in\\mathbb{N}$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r c l}{\\|x_{t+1}-x^{*}(y_{t},z_{t})\\|^{2}}&{\\leq}&{2\\left(1+\\displaystyle\\frac{1}{\\tau_{1}^{2}(p-\\ell)^{2}}\\right)\\tau_{1}^{2}\\|\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})\\|^{2}}\\\\ &&{\\quad+4\\tau_{1}^{2}\\langle\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t}),\\Delta_{t}^{x}\\rangle+2\\tau_{1}^{2}\\|\\Delta_{t}^{x}\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Proof. For any $t\\in\\mathbb N$ , using the fact that $\\hat{f}$ is strongly convex in $x$ with modulus $(p-\\ell)$ together with $x^{*}(y_{t},z_{t})=\\operatorname{argmin}_{x}\\hat{f}(x,y_{t};z_{t})$ , and $x_{t+1}=x_{t}-\\tau_{1}\\hat{G}_{x}\\big(x_{t},y_{t},\\xi_{t+1}^{x};z_{t}\\big)$ , we get ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|x_{t+1}-x^{*}(y_{t},z_{t})\\|^{2}\\leq2\\|x_{t}-x^{*}(y_{t},z_{t})\\|^{2}+2\\|x_{t+1}-x_{t}\\|^{2}}\\\\ {\\displaystyle\\leq\\frac{2}{(p-\\ell)^{2}}\\|\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})\\|^{2}+2\\tau_{1}^{2}\\|\\hat{G}_{x}(x_{t},y_{t},\\xi_{t+1}^{x};z_{t})\\|^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Using the identity $\\begin{array}{r}{\\hat{G}_{x}(x_{t},y_{t},\\xi_{t+1}^{x};z_{t})=\\Delta_{t}^{x}+\\nabla_{x}\\hat{f}(x_{t},y_{t};z_{t})}\\end{array}$ within the above inequality yields the desired result. \u53e3 ", "page_idx": 29}, {"type": "text", "text": "Lemma 21. For any $\\boldsymbol{y}\\in\\mathbb{R}^{d_{2}},\\,z_{1},\\boldsymbol{z}_{2}\\in\\mathbb{R}^{d_{1}},$ , we have: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|x^{*}(y,z_{1})-x^{*}(y,z_{2})\\|\\leq\\frac{p}{p-\\ell}\\|z_{1}-z_{2}\\|.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. This result follows from [70, Lemma B.2]. Indeed, since $\\hat{f}$ is strongly convex in $x$ with modulus $p-\\ell$ , we get ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\hat{f}(x^{*}(y,z_{1}),y;z_{2})-\\hat{f}(x^{*}(y,z_{2}),y;z_{2})\\geq\\frac{p-\\ell}{2}\\|x^{*}(y,z_{1})-x^{*}(y,z_{2})\\|^{2}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Then swapping $z_{1},z_{2}$ leads to ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\hat{f}(x^{*}(y,z_{2}),y;z_{1})-\\hat{f}(x^{*}(y,z_{1}),y;z_{1})\\geq\\frac{p-\\ell}{2}\\|x^{*}(y,z_{1})-x^{*}(y,z_{2})\\|^{2}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Furthermore, from the definition $\\hat{f}$ , it follows that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\hat{f}(x^{*}(y,z_{2}),y;z_{1})-\\hat{f}(x^{*}(y,z_{2}),y;z_{2})=\\frac{p}{2}\\left(\\|x^{*}(y,z_{2})-z_{1}\\|^{2}-\\|x^{*}(y,z_{2})-z_{2}\\|^{2}\\right)}\\\\ &{}&{=\\frac{p}{2}\\left(\\|z_{1}\\|^{2}-\\|z_{2}\\|^{2}+2\\langle x^{*}(y,z_{2}),z_{2}-z_{1}\\rangle\\right);}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "similarly, swapping $z_{1},z_{2}$ in the above inequality leads to ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\hat{f}(x^{*}(y,z_{1}),y;z_{2})-\\hat{f}(x^{*}(y,z_{1}),y;z_{1})=\\frac{p}{2}\\left(\\Vert x^{*}(y,z_{1})-z_{2}\\Vert^{2}-\\Vert x^{*}(y,z_{1})-z_{1}\\Vert^{2}\\right)}\\\\ &{}&{=\\frac{p}{2}\\left(\\Vert z_{2}\\Vert^{2}-\\Vert z_{1}\\Vert^{2}-2\\langle x^{*}(y,z_{1}),z_{2}-z_{1}\\rangle\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Thus, summing the above two identities and applying Cauchy-Schwartz gives us ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\hat{f}(x^{*}(y,z_{2}),y;z_{1})-\\hat{f}(x^{*}(y,z_{2}),y;z_{2})+\\hat{f}(x^{*}(y,z_{1}),y;z_{2})-\\hat{f}(x^{*}(y,z_{1}),y;z_{1})}\\\\ &{\\quad\\quad\\leq p\\|x^{*}(y,z_{1})-x^{*}(y,z_{2})\\|\\|z_{1}-z_{2}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We can lower bound the left hand side of the above inequality using (50) and (51), which leads to ", "page_idx": 29}, {"type": "equation", "text": "$$\n(p-\\ell)\\|x^{*}(y,z_{1})-x^{*}(y,z_{2})\\|^{2}\\leq p\\|x^{*}(y,z_{1})-x^{*}(y,z_{2})\\|\\|z_{1}-z_{2}\\|.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Rearranging this inequality yields the desired result. ", "page_idx": 29}, {"type": "text", "text": "Lemma 22. For any $\\boldsymbol{x}\\in\\mathbb{R}^{d_{1}}$ , $x\\mapsto\\Phi(x;z)$ is $(p-\\ell)$ -strongly convex. ", "page_idx": 29}, {"type": "text", "text": "Proof. For any $\\boldsymbol{y}\\,\\in\\,\\mathbb{R}^{d_{2}},\\boldsymbol{z}\\,\\in\\,\\mathbb{R}^{d_{1}}$ , $x\\mapsto{\\hat{f}}(x,y;z)$ is strongly convex with modulus $p-\\ell$ , i.e., $\\begin{array}{r}{x\\mapsto\\hat{f}(x,y;z)-\\frac{p-\\ell}{2}\\|x\\|^{2}}\\end{array}$ is convex. Then, $\\begin{array}{r}{x\\mapsto\\operatorname*{sup}_{y\\in\\mathbb{R}^{d_{2}}}\\hat{f}(x,y;z)-\\frac{p-\\ell}{2}\\|x\\|^{2}}\\end{array}$ is convex as it is the pointwise supremum of convex functions. Therefore, $\\begin{array}{r}{x\\mapsto\\Phi(x;z)-\\frac{p-\\ell}{2}\\|x\\|^{2}}\\end{array}$ is convex, and this implies $x\\mapsto\\Phi(x,z)$ is strongly convex with modulus $p-\\ell$ . \u53e3 ", "page_idx": 29}, {"type": "text", "text": "Lemma 23. For all $z_{1},z_{2}\\in\\mathbb{R}^{d_{1}}$ , we have ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|x^{*}(z_{1})-x^{*}(z_{2})\\|\\leq\\frac{p}{p-\\ell}\\|z_{1}-z_{2}\\|.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. Using the result of Lemma 22, one can show this result following exactly the same arguments in the proof of Lemma 21. \u53e3 ", "page_idx": 29}, {"type": "text", "text": "Lemma 24. For any $\\boldsymbol{y}\\in\\mathbb{R}^{d_{2}}$ , $z\\in\\mathbb{R}^{d_{1}}$ , it holds that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\Vert x^{*}(z)-x^{*}(y^{+}(z),z)\\Vert^{2}\\leq\\frac{1}{(p-\\ell)\\mu}\\left(1+\\tau_{2}\\ell\\frac{2p}{p-\\ell}\\right)^{2}\\Vert\\nabla_{y}f(x^{*}(y,z),y)\\Vert^{2}.\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. The proof is the same as [66, Lemma C.2]. ", "page_idx": 29}, {"type": "text", "text": "I Further Details about Figure 2 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In this section, we provide further details about how the empirical and theoretical quantiles are estimated in Figure 2 and are compared. ", "page_idx": 30}, {"type": "text", "text": "Generation of the theoretical quantiles $\\mathcal{Q}_{q,T}$ . For any given $q\\,\\in\\,(0,1)$ , estimating an upper bound on $\\mathcal{Q}_{q,T}$ requires estimating an upper bound on the quantity $\\Delta_{0}+b_{0}$ based on Theorem 11. Other constants such as $r_{1},r_{2},r_{3}.$ , $\\ell$ , and $\\mu$ are explicitly known in the setting of this experiment based on the NCPL game where $T=10{,}000$ is fixed. ", "page_idx": 30}, {"type": "text", "text": "First, we set the initial point $(x_{0},y_{0})$ randomly, where each component of $x_{0}$ and $y_{0}$ is sampled uniformly from the interval $[-20,20]$ , and we set $z_{0}=x_{0}$ . We then estimate an upper bound on the quantity $\\Delta_{0}+b_{0}$ numerically based on a grid search, resulting in $\\Delta_{0}+b_{0}=12$ . Second, we generate a linear mesh $I_{m}$ with a grid size $m=0.0002$ over the interval $[0,1]$ . For $q\\in I_{m}$ , we calculate $\\mathcal{Q}_{q,T}$ based on Theorem 11. Third, we generate a sequence of quantiles $\\mathcal{Q}_{I_{m},T}$ . These quantiles are used to create a CDF via linear interpolation using the scipy.interpolate package\u2019s interp1d function in Python. Note that this quantile sequence generates a CDF over the values $\\mathcal{Q}_{I_{m},T}$ . ", "page_idx": 30}, {"type": "text", "text": "Generation of the empirical quantiles of the random variable $X_{T}$ . We generate 1,000 samples $\\{X_{T}^{(i)}\\}_{i=1}^{1000}$ from the sample paths corresponding to the NCPL game with $T\\;=\\;10{,}000$ , where $\\begin{array}{r}{X_{T}=\\frac{1}{T}\\sum_{t=1}^{T}\\|\\nabla_{x}f(x_{t},y_{t})\\|^{2}+\\kappa\\|\\nabla_{y}f(x_{t},y_{t})\\|^{2}}\\end{array}$ represents the path averages of the gradients. Quantiles for this sequence were generated over $I_{m}$ using NumPy\u2019s quantile generator in Python, ensuring alignment with the mesh over which the theoretical quantiles were generated. Evidently, our theoretical quantiles dominate the empirical quantiles pointwise, demonstrating that in the challenging NCPL regime, our theory provides empirically verifiable guarantees on the tail behavior of the random variable $X_{T}$ . ", "page_idx": 30}, {"type": "text", "text": "Comparison of quantiles. We plotted the CDF corresponding to the theoretical quantiles $\\mathcal{Q}_{q,T}$ over the values of the empirical quantiles using a common mesh grid over the range of the empirical averages of the sample paths. In other words, we scaled the quantiles $\\mathcal{Q}_{q,T}$ with an affine transformation so that their range matches the range of the empirical quantiles. This affine scaling preserves the shape of the distribution corresponding to the theoretical quantiles and allows for better visualization. ", "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] The abstract and the introduction reflect the paper\u2019s contributions and scope accurately. In fact, we provide adequate references to the results from our paper for the claims in the introduction so that the paper\u2019s contributions and scope are easier to understand. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] Although we provide the first high-probability bounds for nonconvex/PL minimax problems to the best of our knowledge, there may still be some room to improve the condition number $\\kappa$ dependency based on the existing lower complexity bounds [38, 71] in expectation, unless the lower complexity bounds are loose. Also, we acknowledged that for the distributionally robust optimization experiment our assumptions do not all hold (which is a limitation); but that our results are still predictive of the practical performance. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] All assumptions are clearly stated or referenced in the statement of any theorem, we provide a sketch of the proof of our main theorem. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] We uploaded our code as a supplementary material, and explained in detail how our experiments are performed. All the datasets we use are well-known benchmark datasets and we provided the adequate references to them. ", "page_idx": 31}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] We explain the experimental setup in detail and provide the code, the datasets are well-known benchmark sets that are publicly available and we provided the adequate references. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] Our numerical experiments section has all the details. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] In the experiments, we compare the histogram of the solutions found by multiple algorithms. This has more resolution than standard approaches that has the average performance (as an average over the algorithm runs) and the standard deviation over the runs; because one can visualize the behavior (histogram) of all runs. ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] We explained the details of the operating system/computing resources. ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] We preserve anonymity and we absolutely confirm with the code of Ethics. ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Our paper is mainly a theoretical paper with convergence guarantees for stochastic mini-max algorithms, so we are not aware of any direct impacts to the society. ", "page_idx": 32}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] Our paper has theoretical nature an we are not aware of any potential misuse risk for our results. ", "page_idx": 32}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] We use public datasets, and implemented the algorithms on our own in Python; no licensed material is used. We use public well-known benchmark datasets which are properly referenced. ", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?   \nAnswer: [NA] We do not release new assets. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] No human subjects or crowdsourcing were involved. ", "page_idx": 32}]