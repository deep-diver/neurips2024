[{"figure_path": "sy7eSEXdPC/tables/tables_8_1.jpg", "caption": "Table 1: Accuracy of a solo model, debate, and our debate interventions: 10 rounds, 6 models.", "description": "This table presents the accuracy results of four different settings: using a single model to answer the questions, the original multi-agent debate method, and the proposed method with three different model combinations.  The accuracy is measured across four different benchmark datasets (BoolQ, MMLU, TruthfulQA, MathQ) and with different numbers of models (6 models of a single type and 3 models of two different types). The results demonstrate the improvement in accuracy achieved by the proposed approach.", "section": "7 Experiments"}, {"figure_path": "sy7eSEXdPC/tables/tables_17_1.jpg", "caption": "Table 1: Accuracy of a solo model, debate, and our debate interventions: 10 rounds, 6 models.", "description": "This table presents the accuracy results of four different methods for solving tasks using large language models (LLMs): a single LLM, the standard multi-LLM debate method, and the proposed method with three interventions (diversity pruning, quality pruning, and misconception refutation).  It shows the accuracy for each method across four different benchmark datasets (BoolQ, MMLU, TruthfulQA, and MathQ) and for various combinations of six LLMs. The results demonstrate the improvement in accuracy achieved by using the proposed multi-LLM debate method with the three interventions compared to using a single LLM or the standard multi-LLM debate approach.", "section": "7 Experiments"}, {"figure_path": "sy7eSEXdPC/tables/tables_18_1.jpg", "caption": "Table 1: Accuracy of a solo model, debate, and our debate interventions: 10 rounds, 6 models.", "description": "This table shows the accuracy results of four different settings: using a single model to answer questions, using the original multi-agent debate method, and two versions of the proposed method with and without the interventions.  Each setting was tested using 6 models (either all the same type or a mix of different types) and 10 rounds of debate for the BoolQ, MMLU, TruthfulQA, and MathQ datasets.  The table allows a direct comparison of accuracy across the different approaches.", "section": "7 Experiments"}, {"figure_path": "sy7eSEXdPC/tables/tables_18_2.jpg", "caption": "Table 1: Accuracy of a solo model, debate, and our debate interventions: 10 rounds, 6 models.", "description": "This table presents the accuracy results of four different experimental setups: using a single model, using the standard multi-agent debate method, using the proposed method with the three interventions combined, and using each intervention individually.  The table shows the accuracy for each setup across four different benchmark datasets (BoolQ, MMLU, TruthfulQA, MathQ) and four different language models (GPT-3.5, Llama-2, Llama-3, Mistral). The results demonstrate the improvement in accuracy achieved by the proposed method compared to the baseline methods.", "section": "7 Experiments"}]