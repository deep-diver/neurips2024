[{"type": "text", "text": "Multi-LLM Debate: Framework, Principals, and Interventions ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Andrew Estornell Yang Liu ByteDance Research University of California, Santa Cruz andrew.estornell@bytedance.com yangliu@ucsc.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The flexible and generalized nature of large language models has allowed for their application in a wide array of language-based domains. Much like their human contemporaries, these models are capable of engaging in discussions and debates as a means of improving answer quality. We first take a theoretical approach to analyzing debate and provide a framework through which debate can be mathematically examined. Building on this framework, we provide several theoretical results for multi-agent debate. In particular, we demonstrate that similar model capabilities, or similar model responses, can result in static debate dynamics where the debate procedure simply converges to the majority opinion. When this majority opinion is the result of a common misconception (possibly ingrained in the models through shared training data) debate is likely to converge to answers associated with that common misconception. Using insights from our theoretical results, we then propose three interventions that improve the efficacy of debate. For each intervention, we provide theoretical results demonstrating how debate is improved. We also demonstrate that these interventions result in better performance on four common benchmark tasks. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large language models (LLMs) have demonstrated a remarkable ability to perform unseen tasks with high efficacy. This behavior, often referred to as emergent, allows LLMs to serve as general-purpose tools for a wide array of language-based functions. One such behavior of particular interest is the ability of LLMs to intake and process opinions from other models (or humans). As shown in several previous works, this ability allows LLMs to collaboratively solve tasks by engaging in debate Chan et al. [2023], Liang et al. [2023], Du et al. [2023]. For a given task, multi-agent debate operates by eliciting responses from each model, distributing those responses among the models, and then eliciting updated responses from each model. ", "page_idx": 0}, {"type": "text", "text": "In this work, we aim to explore the debate procedure, by first providing a theoretical framework through which debate can be better understood. This framework draws inspiration from Bayesian inference and in-context learning, showing that debate can be partially viewed as a special type of in-context learning. Through this framework, we then provide several theoretical insights into the debate procedure. In particular, we demonstrate the susceptibility of multi-agent debate to echochamber effects. These echo-chamber effects are especially consequential when they stem from a shared misconception between a majority of models, which can arise from circumstances such as highly correlated training data between each model. ", "page_idx": 0}, {"type": "text", "text": "We then leverage results from our theoretical framework to improve the efficacy of the debate procedure. In particular, we propose three interventions (modifications to the debate procedure). First, diversity-pruning which aims to maximize the information entropy in model responses at each round of debate; this intervention has the added benefti of reducing the severity of the echo chamber effect. Second, quality-pruning which aims to maximize the relevance of each model\u2019s response. We demonstrate that this intervention improves the likelihood that the debate procedure converges to correct answers. Lastly, misconception-refutation which directly identifies, and attempts to refute misconceptions in model responses. This intervention takes inspiration from works such as Robinson et al. [2022] which demonstrate that LLMs are often more skilled at evaluating answers, compared with directly providing answers. For each of our interventions, we provide theoretical results outlining the way in which each improves debate. We also conduct experiments on four common benchmarks demonstrating that these interventions improve debate efficacy in practice. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Our contributions 1) We propose a theoretical framework for multi-LLM debate that draws on connections from in-context learning and Bayesian inference. 2) We provide theoretical insights on several key principles of multi-LLM debate. 3) Using these insights, we design three debate interventions which result in consistent improvement to debate, across four language benchmarks (BoolQ, MMLU, MathQ, TruthfuQA) and three families of models (GPT, Lama, and Mistral). ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Our work is closely related to multi-agent debate, which focuses on iterative collaboration between agents to make a decision Chan et al. [2023], Liang et al. [2023], Du et al. [2023], Khan et al. [2024], Irving et al. [2018], Michael et al. [2023], Rasal [2024], Pham et al. [2023], Chang [2024b]. These works often focus on multi-agent debate in the context of question-answering tasks and aim to provide higher quality answers (compared to those of a single model) by engaging multiple models in discussion. The preliminary debate framework, proposed by Du et al. [2023], facilitates debate by first asking each model a question, and then iteratively re-asking agents that same question contextualized by the responses of all models in the previous round. Different variants of this procedure have been proposed: debate where models have different functionality Liang et al. [2023], round-robin style debate Chan et al. [2023], dynamically controlling the level of disagreement between agents in debate Chang [2024a], or using judges to assess the correctness of debaters Khan et al. [2024]. Other techniques for iteratively improving the quality of answers have also been proposed, e.g., chain-of-thought Wei et al. [2022], Kojima et al. [2022], self-consistency Wang et al. [2022], Singhal et al. [2023], and self-reflection Ren et al. [2023]. ", "page_idx": 1}, {"type": "text", "text": "Similar to debate, there have been investigations into the use of different LLMs to engage with one-another Liu et al. [2023], Abdelnabi et al. [2023], Zhang et al. [2023], Li et al. [2023c], Park et al. [2023a], explain their rational to others Wang et al. [2023a], or collaboratively engage in general tasks Li et al. [2023a], Wang et al. [2023b], Park et al. [2023b], Wu et al. [2023], Hong et al. [2023], Li et al. [2023d,b], Tsao and AILAB [2023]. While debate has shown promise in a wide range of domains, several works have also demonstrated that the debate process can be unstable and can lead to worse performance than using just a single model Wang et al. [2024], Smit et al. [2023]. ", "page_idx": 1}, {"type": "text", "text": "Our work is also related to in-context learning and Bayesian inference. The former, Brown et al. [2020], Min et al. [2022], Lampinen et al. [2022] demonstrates that LLMs can perform unseen tasks when provided only a few examples of that task. Other works Xie et al. [2021], Jiang [2023] have shown a connection between in-context learning and Bayesian inference; the additional examples provided to the model can be viewed as updates to the model\u2019s posterior distribution over tokens. ", "page_idx": 1}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Debate Let $\\mathbf{x}$ be a given question, with associated answer y, for example $\\mathbf{x}=$ \u201cWhat color is the $s k y?^{,*}$ and ${\\bf y}=^{\\ast}B l u e^{\\ast}$ . Following the debate procedure proposed by Du et al. [2023], a collection of $n$ LLMs (also referred to as agents) collaborate to infer the correct answer y by iteratively engaging in discussion over $T$ rounds, as described next: ", "page_idx": 1}, {"type": "text", "text": "\u2022 At round $t=0$ each agent $i$ observes task $\\mathbf{x}$ , then provides response $\\mathbf{z}_{i}^{(0)}$ . \u2022 At rounds $t>0$ each agent $i$ observes task $\\mathbf{x}$ and the outputs of the $n$ agents at the previous timesetep $Z^{(t-1)}=(\\mathbf{z}_{1}^{(t-1)},\\ldots,\\mathbf{z}_{n}^{(t-1)})$ , then provides response ${\\bf z}_{i}^{(t)}$ . \u2022 The debate process ends if $t=T$ or if agents reach a consensus. ", "page_idx": 1}, {"type": "text", "text": "To measure if consensus is reached a function $a$ extracts an answer1 from a given response ${\\bf z}$ . Suppose $\\mathbf{z}={}^{\\ast}D$ uring the day, the sky is blue\u201d, then $a(\\mathbf{z})={}^{*}\\!B l u e^{*}$ . At round $t$ , the probability that agent $i$ provides response $\\mathbf{z}_{i}^{(t+1)}$ , is given by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathrm{model}}\\left(\\underbrace{\\mathbf{z}_{i}^{(t+1)}}_{\\mathrm{updated~model~response}}\\Bigm|\\underbrace{\\mathbf{x}}_{\\mathrm{task}},\\underbrace{Z^{(t)}=(\\mathbf{z}_{1}^{(t)},\\underbrace{\\dots,\\mathbf{z}_{n}^{(t)}}_{\\mathrm{all~model~responses~from~previous~round}},\\underbrace{\\phi_{i}}_{\\mathrm{model~parameters}})}_{\\mathrm{tangle~vare~}}\\right)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\phi_{i}$ captures model hyperparameters (such as training data, architecture, etc.). Both the input $(Z^{(t)},\\mathbf{x})$ as well as the hyperparameters $\\phi_{i}$ , ultimately influence the output zi(t+1). Note that on each round, all agents observe the same input, namely $(Z^{(t)},\\mathbf{x})$ . Thus, differences in agent output $\\mathbf{z}_{i}^{(t+1)}$ are determined by both the stochastic nature of output generations, and the unique parameters $\\phi_{i}$ of each model. For notational convenience, we drop the subscript in $\\mathbb{P}_{\\mathrm{model}}$ when the parameters $\\phi_{i}$ are given, and simply write $\\mathbb{P}(\\cdot|\\phi_{i})$ . ", "page_idx": 2}, {"type": "text", "text": "The key distinction between our approach and \u201cvanilla\" debate, is that we will leverage latent concepts (discussed next) to modify the responses in $Z^{(t)}$ in between each round of debate. ", "page_idx": 2}, {"type": "text", "text": "Latent Concepts Central to our investigation is the idea of latent concepts in language generation. As outlined by Xie et al. [2021], Jiang [2023] latent concepts capture the idea that language is not generated at random (either by humans or by models). Rather, when generating language, we first have an idea or an intention form in our minds; we then select words that we believe will convey that underlying idea or intention. More formally, let $\\Theta$ be a latent concept space and let $\\pmb\\theta\\in\\Theta$ be a concept. Following Xie et al. [2021], tasks $\\mathbf{x}$ , and their associated answer y are generated by first selecting a vector of latent concept $\\pmb\\theta\\in\\Theta$ and then sampling $(\\mathbf{x},\\mathbf{y})\\sim D(\\pmb{\\theta})$ , where $D$ is some distribution mapping concepts to task-answer pairs. Similarly, when providing responses, models will observe $\\mathbf{x}$ , and infer the latent concept $\\pmb{\\theta}$ , or more generally a distribution over the latent concept space, and then generate a response according to those inferred concepts, i.e., the model\u2019s generation probability in Equation 1 can be expressed as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(\\mathbf{z}_{i}^{(t+1)}\\big|\\;\\mathbf{x},Z^{(t)},\\phi_{i}\\big)=\\prod_{\\theta\\in\\Theta}\\mathbb{P}\\big(\\mathbf{z}_{i}^{(t+1)}\\big|\\;\\theta,\\mathbf{x},Z^{(t)},\\phi_{i}\\big)\\mathbb{P}\\big(\\theta\\big|\\;\\mathbf{x},Z^{(t)},\\phi_{i}\\big)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Note that the above holds by the law of total probability for any choice of latent concept space. ", "page_idx": 2}, {"type": "text", "text": "To provide a more concrete example of latent concepts, consider the question-answering task in the BoolQ dataset. One of the questions in this dataset is \"Did Abraham Lincoln write the letter in the film Saving Private Ryan?\" to which the correct answer is \"Yes\". The latent concept, in this case, corresponds to the actual scene in the movie where the Bixby letter (written by Lincoln) is read to a group of soldiers. Just as in our case, first a concept $\\theta$ is drawn, e.g., the flim is made; then from the film, a string $\\mathbf{x}$ is sampled, i.e., the previous question about the film. ", "page_idx": 2}, {"type": "text", "text": "For another example of latent concepts, we can think of arithmetic calculations such as multiplication. When we wish to express multiplication through language, we may write something like $\"4*4\"$ . The latent concepts behind this string are the mechanisms of multiplication (e.g., multiplication is just iterative addition, and addition itself is simply increasing the value of a number by one iteratively). These examples are intended to be easily digestible. However, latent concepts can also be significantly more abstract, such as a vector in some unknown embedding space. ", "page_idx": 2}, {"type": "text", "text": "4 A Theoretical Formulation of Multi-Agent Debate ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We begin by providing a theoretical formulation of multi-agent debate. This formulation will provide key insights into the inner workings of the debate procedure, which we will use to improve debate. ", "page_idx": 2}, {"type": "text", "text": "The key behind our framework is to use the idea of latent concepts and expansion of each model\u2019s generation probability (Equation 2) in order to better understand debate. Prior to presenting our theoretical framework, we first state an important assumption. ", "page_idx": 2}, {"type": "text", "text": "Assumption 4.1. For a given latent concept space $\\Theta$ , the probability of generating response $\\mathbf{z}_{i}^{(t+1)}$ i conditionally independent of both the responses $Z^{(t)}$ and the task $\\mathbf{x}$ , given concept $\\pmb\\theta\\in\\Theta$ and model parameters $\\phi_{i}$ , i.e., $\\mathbb{P}\\!\\left(\\mathbf{z}_{i}^{(t+1)}\\!\\mid\\!\\theta,\\mathbf{x},Z^{(t)},\\phi_{i}\\right)=\\mathbb{P}\\!\\left(\\mathbf{z}_{i}^{(t+1)}\\!\\mid\\!\\theta,\\phi_{i}\\right)$ . ", "page_idx": 2}, {"type": "text", "text": "Assumption 4.1 can be interpreted as saying that a model\u2019s generation $\\mathbf{z}_{i}$ , is uniquely determined by the model\u2019s parameters $\\phi_{i}$ and the concepts identified by a model, namely $\\pmb{\\theta}$ . In the case of encoder-decoder-based models, one can conceptualize the joint between $\\phi$ and $\\pmb{\\theta}$ as corresponding to the embedding produced by the encoder. With this embedding in hand, the original input $(\\mathbf{x},Z^{(t)})$ no longer influences the model\u2019s output, rather the embedding and model parameters will uniquely determine the model\u2019s output. ", "page_idx": 3}, {"type": "text", "text": "Next, we derive the following lemma which will be useful in examining the way that model responses evolve debate rounds. ", "page_idx": 3}, {"type": "text", "text": "Lemma 4.2. The generation of model i at time $t+1$ can be expressed as, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(\\mathbf{z}_{i}^{(t+1)}|\\,Z^{(t)},\\mathbf{x},\\phi_{i}\\big)\\propto\\sum_{\\theta\\in\\Theta}\\frac{\\mathbb{P}\\big(\\mathbf{z}_{i}^{(t+1)}|\\theta,\\phi_{i}\\big)\\mathbb{P}\\big(\\mathbf{x}|\\theta,\\phi_{i}\\big)\\mathbb{P}\\big(\\theta|\\phi_{i}\\big)}{g e n e r a t i o n\\,w i t h o u t\\,o t h e r\\,a g e n s}\\,\\underbrace{\\prod_{j=1}^{n}\\mathbb{P}\\big(\\mathbf{z}_{j}^{(t)}|\\,\\theta,\\phi_{i}\\big)}_{\\theta\\in\\mathbf{1}}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The significance of this lemma is that we are able to express the probability of generating a given response $\\mathbf{z}_{i}^{(t+1)}$ with the other model responses $Z^{(t)}$ in terms of the probability of generating $\\mathbf{z}_{i}^{(t+1)}$ without the other model responses and a skew term caused by those model responses. Note that, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(\\mathbf{z}_{i}^{(t+1)}|\\mathbf{x},\\boldsymbol{\\phi}_{i}\\big)\\propto\\sum_{\\boldsymbol{\\theta}\\in\\Theta}\\mathbb{P}\\big(\\mathbf{z}_{i}^{(t+1)}|\\boldsymbol{\\theta},\\boldsymbol{\\phi}_{i}\\big)\\mathbb{P}\\big(\\mathbf{x}|\\boldsymbol{\\theta},\\boldsymbol{\\phi}_{i}\\big)\\mathbb{P}\\big(\\boldsymbol{\\theta}|\\boldsymbol{\\phi}_{i}\\big)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Thus, we can think of $\\mathbb{P}\\big(\\mathbf{z}_{i}^{(t+1)}|\\;Z^{(t)},\\mathbf{x},\\phi_{i}\\big)$ as a weighted version of $\\mathbb{P}\\big(\\mathbf{z}_{i}^{(t+1)}|\\;\\mathbf{x},\\phi_{i}\\big)$ , where the weights are given by the skew term $\\begin{array}{r}{\\prod_{j=1}^{n}\\mathbb{P}\\big({\\mathbf z}_{j}^{(t)}|\\,\\pmb{\\theta},\\phi_{i}\\big)}\\end{array}$ . ", "page_idx": 3}, {"type": "text", "text": "Debate and In-Context Learning To help conceptualize the role of latent concepts in debate, we discuss the work of Xie et al. [2021], which uses Bayesian inference over latent concepts to understand in-context learning. There are natural connections between in-context learning and multi-agent debate. In-context learning works as follows: given a task $\\mathbf{x}$ and a model $f$ , select several examples of taskanswers pairs $\\bar{(\\mathbf{x}_{1},y_{1})}\\cdot\\cdot\\cdot\\left(\\mathbf{x}_{m},y_{m}\\right)$ which are similar to $\\mathbf{x}$ . Then prompt the model $f$ for an answer to task $\\mathbf{x}$ , given examples $(\\mathbf{x}_{j},y_{j})$ , i.e. $\\mathbf{z}=f\\left(\\mathbf{x}|\\ (\\mathbf{x}_{1},y_{1})\\dots(\\mathbf{x}_{m},y_{m})\\right)$ . A key result of Xie et al. [2021] is that latent concepts in the examples $(\\mathbf{x}_{j},y_{j})$ , particularly concepts shared between many examples, influence the model\u2019s answer ${\\bf z}$ . For any concept where $\\mathbb{P}\\big(\\pmb{\\theta}|\\ (\\mathbf{x}_{1},y_{1})\\,.\\,.\\,.\\,(\\mathbf{x}_{m},y_{m})\\big)$ is large relative to other concepts (i.e., there is a shared concept $\\pmb{\\theta}$ between the examples), the model is more likely to give response $\\mathbf{z}$ which also share that concept. Model responses at the previous round $Z^{(t)}$ serve a similar function to the examples of in-context learning. The model\u2019s updated response at time $t+1$ , namely $\\mathbf{z}_{i}^{(t)}$ , is influenced by concepts shared between the responses in $Z^{(t)}$ . The skew term in Lemma 4.2 provides a glimpse of how latent concepts conveyed by $Z^{(t)}$ will influence the generation of ${\\bf z}_{i}^{(t)}$ , namely that $\\begin{array}{r}{\\overline{{\\prod_{j=1}^{n}\\mathbb{P}\\big(\\mathbf{z}_{j}^{(t)}|\\,\\theta,\\phi_{i}\\big)}}}\\end{array}$ reweighs the model\u2019s generation. ", "page_idx": 3}, {"type": "text", "text": "4.1 Debate Objective ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Through this perspective of debate we can more effectively design debate procedures by leveraging the concept space $\\Theta$ . To do this, we will first formulate debate as an optimization problem where the skew term, described in Lemma 4.2, corresponds to the optimization variables. For a given task $\\mathbf{x}$ and answer $y$ , each round of debate can be formulated as the following optimization problem. ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{max}_{Z^{(t)}}\\sum_{i=0}^{n}\\mathbb{P}\\big(a(\\mathbf{z}_{i}^{(t+1)})=y|\\;Z^{(t)},\\phi_{i}\\big)\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "At time $t$ we aim to craft responses $Z^{(t)}$ such that they maximize the probability of providing the correct answer at the next time step. Expanding this objective over the latent concept space $\\Theta$ , yields ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\arg\\operatorname*{max}_{Z^{(t)}}\\;\\sum_{i=1}^{n}\\sum_{\\theta\\in\\Theta}\\left(\\mathbb{P}\\big(a(\\mathbf{z}^{(t+1)})=y|\\;\\theta,\\phi_{i}\\big)\\mathbb{P}\\big(\\theta|\\;\\phi_{i}\\big)\\mathbb{P}\\big(\\mathbf{x}|\\;\\theta,\\phi_{i}\\big)\\prod_{j=1}^{n}\\mathbb{P}\\big(\\mathbf{z}_{j}^{(t)}|\\;\\theta,\\phi_{i}\\big)\\right).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The key challenges with directly optimizing this objective are: 1) the true concept $\\pmb{\\theta}^{*}$ from which $\\mathbf{x}$ and $y$ originate, as well as the relationship between $\\mathbf{z}_{j}^{(t)}$ and the underlying concepts, is unknown, 2) the responses in $Z^{(t)}$ ) are natural language. However, will allow us to design several approaches within the concept space to better optimize debate. To motivate these approaches, we first need to make several observations about the debate procedure as a whole. ", "page_idx": 4}, {"type": "text", "text": "5 Debate Principals ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we discuss factors that affect the efficacy of LLMs debate. In particular, we look at the role of information diversity, both in terms of the diversity of responses in $Z^{\\bar{(}t)}$ as well as diversity in model capabilities. We see that a lack of diversity in either aspect is detrimental to the debate process. Lastly, we study a particular type of homogeneity in debate, namely shared misconceptions in which a large portion of models all share a similar erroneous belief about the task $x$ . ", "page_idx": 4}, {"type": "text", "text": "5.1 Information Diversity ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We begin by examining how the debate procedure is affected by both the diversity of model abilities and the diversity of model responses. Homogeneity in either ability or responses will bias the debate procedure towards certain latent concepts. ", "page_idx": 4}, {"type": "text", "text": "Similar Model Capabilities Suppose the debate process is conducted with only one type of model (in effect $n$ copies of the same model). That is, $\\phi_{i}\\equiv\\phi$ for all $i\\,\\in\\,[n]$ . Then, as the number of agents increases, the debate procedure is more greatly impacted by the echo chamber effect, i.e., the probability that a round of debate results in a change to the most likely concept, perceived by agents, approaches 0. That is, a greater number of similar agents results in static debate dynamics, in essence defeating the purpose of debate. ", "page_idx": 4}, {"type": "text", "text": "Theorem 5.1. Suppose all n agents have identical configurations $(\\phi_{i}\\equiv\\phi$ for all $i_{.}$ ). For round $t>0$ let $\\pmb{\\theta}^{\\prime(t)}=\\big(\\arg\\operatorname*{max}_{\\pmb{\\theta}\\in\\Theta}\\mathbb{P}\\big(\\pmb{\\theta}|\\;\\mathbf{x},Z^{(t)},\\pmb{\\phi}\\big)\\big)$ and $\\pmb{\\theta}^{\\prime(t+1)}=\\big(\\arg\\operatorname*{max}_{\\pmb{\\theta}\\in\\Theta}\\mathbb{P}\\big(\\pmb{\\theta}|\\;\\mathbf{x},Z^{(t+1)},\\phi\\big)\\big)$ , i.e., $\\pmb{\\theta}^{\\prime(t)}$ and $\\pmb{\\theta}^{\\prime(t+1)}$ are the concepts most likely to be inferred b\u2032y a model with parameters $\\phi$ when given task $\\mathbf{x}$ and responses $Z^{(t)}$ , $Z^{(t+1)}$ respectively. Then $\\mathbb{P}\\big(\\pmb\\theta^{'(t)}=\\pmb\\theta^{\\prime(t+1))}\\big)\\rightarrow1$ as $n\\to\\infty$ . ", "page_idx": 4}, {"type": "text", "text": "We defer a full proof to the Supplement, Section A. Theorem 5.1 implies that when debate is conducted with multiple copies of the same model (or very similar models), increasing the number of models results in debate centering on a single (unchanging) concept, rather than a balanced distribution over multiple concepts. ", "page_idx": 4}, {"type": "text", "text": "Similar Model Opinions Next, we examine how similar responses impact the collaboration process. At time $t$ suppose that there are $n$ responses $Z^{(t)}$ and at least $m$ of those responses are similar, i.e., there exists some concept $\\pmb{\\theta}^{\\prime}$ such that $\\pmb{\\theta}^{\\prime}=\\arg\\operatorname*{max}_{\\pmb{\\theta}\\in\\Theta}\\mathbb{P}\\big(\\pmb{\\theta}\\vert\\ \\mathbf{z}_{j}^{(t)},\\pmb{\\phi}_{i}\\big)$ for all $j\\leq m$ . That is, each of the $m$ responses has a shared \u201cmost likely\" concept when viewed by a model with parameters $\\phi_{i}$ . ", "page_idx": 4}, {"type": "text", "text": "Theorem 5.2. Suppose that $Z^{(t)}$ contains at least $m$ responses with the property that $\\pmb{\\theta}^{\\prime}=\\arg\\operatorname*{max}_{\\pmb{\\theta}\\in\\Theta}\\mathbb{P}\\!\\left(\\pmb{\\mathrm{z}}_{i}^{(t)}|\\ \\pmb{\\theta},\\pmb{\\phi}_{i}\\right)$ . Then, as $m~\\rightarrow~\\infty$ the model\u2019s generation at the next round $(t+1)$ becomes uniquely determined by a single concept $\\pmb{\\theta}^{\\prime}$ i.e. $\\begin{array}{r}{\\frac{\\mathbb{P}\\Big(\\mathbf{z}_{(i,1)}^{(t+1)}\\mid Z^{(t)},\\mathbf{x},\\phi_{i}\\Big)}{\\mathbb{P}\\Big(\\mathbf{z}_{(i,2)}^{(t+1)}\\mid Z^{(t)},\\mathbf{x},\\phi_{i}\\Big)}\\to\\frac{\\mathbb{P}\\Big(\\mathbf{z}_{(i,1)}^{(t+1)}\\mid\\theta^{\\prime},\\phi_{i}\\Big)}{\\mathbb{P}\\Big(\\mathbf{z}_{(i,2)}^{(t+1)}\\mid\\theta^{\\prime},\\phi_{i}\\Big)}}\\end{array}$ for all response pairs z((it,+1)1 ), z((it,+2)1 ). ", "page_idx": 4}, {"type": "text", "text": "We defer a full proof to the Supplement Section A. Theorem 5.2 indicates the susceptibility that LLM debate has towards tyranny of the majority. If a large number of models provide similar responses to a task $\\mathbf{x}$ , then those repeated answers will drown out the single provided by the other models\u2019 responses, as well as the task $\\mathbf{x}$ itself. In Section 7 we demonstrate that this occurs in practice. ", "page_idx": 4}, {"type": "text", "text": "5.2 Shared Misconceptions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Next, we study a particular type of homogeneity in model capabilities and responses, namely shared misconceptions. When a common misconception is shared among the models, debate is less effective ", "page_idx": 4}, {"type": "text", "text": "and is likely to converge to erroneous concepts associated with the shared misconception. We now formalize the notion of misconceptions. ", "page_idx": 5}, {"type": "text", "text": "Definition 5.3. (Misconception): For a given concept $\\pmb{\\theta}^{*}$ , a model with parameters $i$ is said to have a misconception regarding $\\pmb{\\theta}^{*}$ if there exists another concept $\\theta^{\\prime}$ s.t., ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbb{P}_{\\mathbf{x}\\sim D(\\pmb{\\theta}^{*})}\\left[\\mathbb{P}(\\mathbf{x}|\\,\\pmb{\\theta}^{\\prime},\\phi_{i})>\\mathbb{P}(\\mathbf{x}|\\,\\pmb{\\theta}^{*},\\phi_{i})\\right]>1/2\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "That is, for tasks generated from the concept $\\pmb{\\theta}^{*}$ , the model believes that the erroneous concept $\\pmb{\\theta}^{\\prime}$ explains more than half of the tasks better than the true concept $\\pmb{\\theta}^{*}$ . ", "page_idx": 5}, {"type": "text", "text": "There is said to be a shared misconception if $m$ of agents have a misconception and share the same erroneous concept $\\pmb{\\theta}^{\\prime}$ . When the models share a common misconception the responses produced by those models are biased towards the erroneous concept $\\theta^{\\prime}$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 5.4. Let the true concept be $\\pmb{\\theta}^{*}$ and suppose that $m$ of the $n$ agents have a shared misconception for erroneous concept $\\theta^{\\prime}$ . Then, task and answer $(\\mathbf{x},y)\\sim D(\\pmb{\\theta}^{*})$ expected average correctness of the debate procedure at the final round $T$ is monotonically decreasing with m, i.e., $\\begin{array}{r}{\\frac{1}{n}\\sum_{i}^{n}\\mathbb{P}\\big(a(\\mathbf{z}_{i}^{(T)})=y\\big)}\\end{array}$ is decreasing with $m$ . ", "page_idx": 5}, {"type": "text", "text": "We defer the full proof to the supplement Section A. It should be noted that the phenomenon of converging to erroneous concepts is unlikely to be mitigated by adding more models. When the misconceptions of one model are formed through training data, it is likely that other models will possess the same misconception unless specifically trained to avoid such errors due to the high correlation in training data between models. ", "page_idx": 5}, {"type": "text", "text": "6 Interventions ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we discuss several modifications to the debate procedure, referred to as interventions. We break our interventions into two categories: Pruning which focuses on choosing which responses to keep in $Z^{(t)}$ , and Modifying which focuses on changing or editing the responses $Z^{(t)}$ . ", "page_idx": 5}, {"type": "text", "text": "6.1 Pruning Interventions ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "At round $t$ of debate, running interventions work by selecting only a subset of responses $Z^{\\prime(t)}$ from $Z^{(t)}$ before starting the next round $t+1$ . When using a pruning intervention, the models at round $t+1$ will see only the pruned response set $Z^{\\prime(t)}$ , rather than the full response set $Z^{(t)}$ . ", "page_idx": 5}, {"type": "text", "text": "Diversity Pruning Let KL represent Kullback\u2013Leibler divergence. The diversity pruning intervention selects $k$ of the $n$ responses in $Z^{(t)}$ which maximizes information entropy, i.e., ", "page_idx": 5}, {"type": "equation", "text": "$$\nZ^{\\prime(t)}=\\underset{Y\\subset Y^{(t)}\\mathbf{z}_{i},\\mathbf{z}_{j}\\in Z}{\\mathrm{argmax}}\\,\\sum_{\\mathbf{z}_{i},\\mathbf{z}_{j}\\in Z}\\mathrm{KL}\\big(D(\\pmb{\\theta}|\\mathbf{\\z}_{i}),\\:D(\\pmb{\\theta}|\\mathbf{\\z}_{j})\\big)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{s.t.}\\ |Z|=k\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Quality Pruning Quality pruning aims to select the $k$ responses in $Z^{(t)}$ with the highest similarity to the task $x$ . Similar to diversity pruning, quality pruning selects $k$ of the $n$ responses at time $t$ Rather than selecting for diversity, quality pruning aims to select the $k$ highest question responses. This is done by selecting the $k$ responses which maximize ", "page_idx": 5}, {"type": "equation", "text": "$$\nZ^{\\prime(t)}=\\underset{Z\\subset Z^{(t)}}{\\mathrm{argmin}}\\sum_{\\mathbf{z}_{i},\\in Z}\\mathrm{KL}\\big(D(\\pmb{\\theta}|\\mathbf{x}),\\:D(\\pmb{\\theta}|\\mathbf{z}_{i})\\big)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{s.t.}\\ |Z|=k\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "In practice computing $\\mathrm{KL}\\big(D(\\pmb\\theta|\\mathbf x),D(\\pmb\\theta|\\mathbf z_{i})\\big)$ or $\\mathrm{KL}\\big(D(\\pmb\\theta|\\mathbf z_{i}),D(\\pmb\\theta|\\mathbf z_{j})\\big)$ is intractable. However, sentence embedding can be used as a proxy for these values. Section C discusses this in further detail. ", "page_idx": 5}, {"type": "text", "text": "Next, we show that when models have a shared misconception, diversity pruning decreases the likelihood that the debate procedure will converge to the erroneous concept corresponding to the shared misconception. ", "page_idx": 5}, {"type": "text", "text": "Theorem 6.1. Let the true concept be $\\pmb{\\theta}^{*}$ and suppose that at least $n/2$ agents have a shared misconception for erroneous concept $\\theta^{\\prime}$ . Then diversity pruning decreases the probability that debate converges to an answer $y^{\\prime}$ which is sourced from the erroneous concept $\\pmb{\\theta}^{\\prime}$ , i.e. $y^{\\prime}\\sim D(\\pmb\\theta^{\\prime})$ . ", "page_idx": 5}, {"type": "text", "text": "We defer the full proof to the Supplement, Section A. ", "page_idx": 6}, {"type": "text", "text": "Theorem 6.2. For a given task-answer pair $(\\mathbf{x},y)$ quality pruning increases the probability that debate converges to the correct answer, i.e. let $Z^{(t)}$ be the set of all responses at time $t$ and $Z^{\\prime(t)}$ be the result of quality pruning, then $\\begin{array}{r}{\\sum_{i=1}^{n}\\mathbb{P}(a({\\mathbf z}_{i}^{(t+1)})=y|{\\mathbf x},{Z}^{\\prime(t)},\\phi_{i})~>}\\end{array}$ $\\begin{array}{r}{\\sum_{i=1}^{n}\\mathbb{P}(a(\\mathbf{z}_{i}^{(t+1)}=y|\\mathbf{x},Z^{(t)},\\boldsymbol{\\phi}_{i})}\\end{array}$ . ", "page_idx": 6}, {"type": "text", "text": "We defer the full proof to the Supplement, Section A. ", "page_idx": 6}, {"type": "text", "text": "Remark 6.3. As shown by Theorems 6.1 and 6.2, diversity pruning decreases the probability that debate converges to incorrect answers sourced from a particular concept, while quality pruning increases the probability that debate converges to a correct answer sourced from the true concept. Both interventions can be used simultaneously to guide the debate procedure more effectively away from wrong answers and towards correct answers. ", "page_idx": 6}, {"type": "text", "text": "6.2 Modification Interventions ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Misconception Refutation In addition to selecting which responses in $Z^{(t)}$ will be used in the next round of debate, we can also modify the responses in $Z^{(t)}$ . Misconception refutation aims to do precisely this by updating response $\\mathbf{z}_{j}^{(t)}$ to be more relevant to the task $\\mathbf{x}$ . ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathbf{z}_{j}^{\\ast}=\\arg\\operatorname*{min}_{\\mathbf{z}}\\mathrm{KL}\\big(D(\\pmb{\\theta}|\\mathbf{x}),\\:D(\\pmb{\\theta}|\\:\\mathbf{z})\\big)-\\mathrm{KL}\\big(D(\\pmb{\\theta}|\\mathbf{z}_{j}^{(t)}),\\:D(\\pmb{\\theta}|\\:\\mathbf{z})\\big)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Similar to Diversity Pruning and Quality Pruning, the distributions in the above objective are intractable in practice. As such, we use a proxy to update each response $\\mathbf{z}_{j}^{(t)}$ , specifically produce $\\mathbf{z}_{j}^{\\ast}$ by having an LLM minimally modify the given response $\\mathbf{z}_{j}^{(t)}$ . The model is first prompted for a list of misconceptions and errors identified in the response. Given the list of misconceptions, the model is asked for both a refutation of the misconception and a corrected version of the response. For more details, see Section C of the Supplement. ", "page_idx": 6}, {"type": "text", "text": "Theorem 6.4. For task-answer pair $(\\mathbf{x},y)$ , misconception refutation increases the probability of debate converging to the correct answer, i.e. let $Z^{(t)},Z^{*(t)}$ be the responses before and after misconception refutation, then $\\begin{array}{r}{\\sum_{i=1}^{n}\\mathbb{P}(a({\\mathbf z}_{i}^{(t+1)})=y|{\\mathbf x},Z^{*(t)},\\phi_{i})>\\sum_{i=1}^{n}\\mathbb{P}(a({\\mathbf z}_{i}^{(t+1)}=y|{\\mathbf x},Z^{(t)},\\phi_{i})}\\end{array}$ ", "page_idx": 6}, {"type": "text", "text": "7 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Experimental Design We conduct experiments on four common language model benchmarks. BoolQ Clark et al. [2019], which consists of 3, 270 yes-no questions, MMLU Hendrycks et al. [2020] which consists of 13, 869 multiple-choice questions (we use the 3, 406 high-school-level questions), TruthfulQA Lin et al. [2021] which consists of 817 open-ended questions, and MathQ which consists of 3, 000 arithmetic questions of the from $\\boldsymbol{a}{\\cdot}\\boldsymbol{b}{\\cdot}\\boldsymbol{c}+d{\\cdot}\\boldsymbol{e}{\\cdot}\\boldsymbol{f}$ . In the BoolQ, MMLU, MathQ, datasets model correctness is measured through regular expression matching. In the TruthfulQA dataset, model correctness is measured via an LLM judge (we use GPT-4 as the judge in all experiments) ", "page_idx": 6}, {"type": "text", "text": "We use four LLMs of increasing capability, GPT-3.5 (GPT-3.5 Turbo) OpenAI [2022], Llama-2 (Llama-2 7B Chat) Touvron et al. [2023], Llama-3 (Llama-3 8B Instruct) Meta AI [2024], and Mistral (Mistral 7B Instruct v0.2) Jiang et al. [2023]. For sentence embeddings (which serve as a proxy of the latent concepts $\\Theta$ ), we use sentence embeddings from ADA-2 OpenAI [2022]. We compare a combination of our three interventions Ours (see Algorithm 1 full details) with the debate paradigm of Du et al. [2023] (Society of Minds) SoM. ", "page_idx": 6}, {"type": "text", "text": "We begin by making several empirical observations about the multi-agent debate process. ", "page_idx": 6}, {"type": "text", "text": "Tyranny of the Majority First, we examine the susceptibility of models towards agreement with the majority opinion. That is, how likely are models to give a specific answer at round $t+1$ when $m$ of the models provided that specific answer at round the previous round (round $t$ )? For example, in BoolQ suppose the specific answer is \u201cYes\", then we want to know: how likely is a model to give a \"Yes\"-answer at round $t+1$ if that model observes m \"Yes\" answers at round $t$ . ", "page_idx": 6}, {"type": "image", "img_path": "sy7eSEXdPC/tmp/51b56fb212bf47687df25a708e3999574eb06d34220e6cf0c32e327377e051dd.jpg", "img_caption": ["Figure 1: Probability that each model echoes the majority answer at round $t=11$ , as the number of responses at time $t=0$ gives that majority answer (debate between 12 models are used). "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "sy7eSEXdPC/tmp/1e6e461e470c6de101d467b11bf435fb237582d19e704a5cf23dc24b8c05a1e1.jpg", "img_caption": ["Figure 2: Average accuracy improvement as a function of response diversity at round 0 of debate. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "To measure this, we first select a random target answer, e.g., \u201cYes\", and then prompt $m$ of the models (out of 11) to provide responses \u201cYes\"-answers2 (while the other 11- $^m$ models are prompted to provide a different randomly selected answer). These 11 responses make up $Z^{(t)}$ , we then test each model\u2019s likelihood of providing the target at round $t\\!+\\!1$ when observing $Z^{(t)}$ before diversity pruning (solid) and after diversity running (hatched). ", "page_idx": 7}, {"type": "text", "text": "In Figure 1, we see models are susceptible to echo chamber effects (this phenomenon is predicted by Theorem 5.1). The likelihood of providing the majority answer increases when $Z^{(t)}$ contains more instances of the majority answer (i.e., as $m$ increases). Figure 1 also demonstrates that diversity pruning (with $k=5$ ) reduces this echo chamber effect. See the Supplement for details. ", "page_idx": 7}, {"type": "text", "text": "Diversity of Opinions Next, we examine the effectiveness of SoM and our method as a function of opinion diversity. Figure 2 shows the average accuracy improvement of SoM (dashed) and our method (solid) over single model performance (i.e., average performance at round $t\\,=\\,0$ ), as a function of the similarity between all responses at round $t=0$ of debate (measured via pairwise cosine similarity). We see that for BoolQ, MMLU, and TruthfulQA, SoM is less effective when the similarity between responses increases. This observation is predicted by Theorems 5.1 and 5.2, which show that debate, without interventions, is less effective when model responses are too similar. ", "page_idx": 7}, {"type": "text", "text": "We see that our method\u2019s improvement compared to SoM is greatest when model opinions are more similar (cosine similarity close to 1). Note that the MathQ benchmark, where responses consist primarily of arithmetic, serves as a counter-example to these observations. This is due to the fact that sentence embedding of any two arithmetic expansions will be similar, regardless of their true similarity; as such, the cosine similarity between embedding is less meaningful on this benchmark. ", "page_idx": 7}, {"type": "text", "text": "Debate Interventions Now, we examine the effectiveness of a combination of our three debate interventions (see Algorithm 1 for full details of how the interventions are combined). We begin with a per-round performance of our method and SoM, as shown in Figure 3. We see that typically, the advantage of our method over debate arises in the later rounds of debate. Next, in Table 1, we present a full set of results for single models, SoM, and a combination of our three interventions. In all cases, our method is either competitive with, or superior to, SoM. ", "page_idx": 7}, {"type": "table", "img_path": "sy7eSEXdPC/tmp/5e3fe2a267e3c7b6e1559065d56d569aa4a14ad9945d7dc580a7da5518a31df1.jpg", "table_caption": [], "table_footnote": ["Table 1: Accuracy of a solo model, debate, and our debate interventions: 10 rounds, 6 models. "], "page_idx": 8}, {"type": "image", "img_path": "sy7eSEXdPC/tmp/ad876ce77d445ee02452fcfdb09987c09c5caf79fa7fdfefa790d37a783a202b.jpg", "img_caption": ["Figure 3: Accuracy per round, our method and SoM when combing GPT-3.5 with Llama-3 or Mistral. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "In addition to providing results for the combination of our interventions, we also investigate the effectiveness of each intervention applied individually (see Table 3 of the Supplement). These results indicate that our method is most successful when applying all three interventions simultaneously. In fact, some interventions can be detrimental to the debate process when applied in isolation. This is expected as each intervention is inherently designed to be complementary. ", "page_idx": 8}, {"type": "text", "text": "8 Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "While we aim to address some of the fundamental issues of multi-LLM debate, such as tyranny of the majority, there are several factors that need to be considered when adopting our framework. Firstly, our theoretical results leverage a latent concept space, which may not be accessible in practice, necessitating the use of proxies such as sentence embeddings. Reliance on proxies is particularly consequential for quality and diversity pruning; these interventions are less effective in domains where sentence embeddings are less meaningful, e.g., arithmetic questions. Additionally, our interventions can increase the inference time of the debate procedure. Increased inference time stems primarily from misconception refutation, as this intervention requires re-prompting each debater multiple times. ", "page_idx": 8}, {"type": "text", "text": "9 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Multi-agent debate is an effective tool for improving the efficacy of LLM responses. However, debate is naturally susceptible to issues such as tyranny of the majority and shared misconceptions between models. By making use of our theoretical framework for debate, we are able to establish interventions for the debate procedure which help to alleviate these issues and improve the general performance of multi-agent debate. We saw that diversity pruning reduces the influence of similar responses. This is especially helpful in settings where the majority of agents provide incorrect responses that share a common error. A combination of all three interventions consistently leads to better debate. ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Sahar Abdelnabi, Amr Gomaa, Sarath Sivaprasad, Lea Sch\u00f6nherr, and Mario Fritz. Llm-deliberation: Evaluating llms with interactive multi-agent negotiation games. arXiv preprint arXiv:2309.17234, 2023.   \nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \nChi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. Chateval: Towards better llm-based evaluators through multi-agent debate. arXiv preprint arXiv:2308.07201, 2023.   \nEdward Y Chang. Evince: Optimizing adversarial llm dialogues via conditional statistics and information theory. arXiv preprint arXiv:2408.14575, 2024a.   \nEdward Y. Chang. Llm collaborative intelligence: The path to artificial general intelligence. SocraSynth.com, 2024b.   \nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019.   \nYilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325, 2023.   \nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.   \nSirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352, 2023.   \nGeoffrey Irving, Paul Christiano, and Dario Amodei. Ai safety via debate. arXiv preprint arXiv:1805.00899, 2018.   \nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.   \nHui Jiang. A latent space theory for emergent abilities in large language models. arXiv preprint arXiv:2304.09960, 2023.   \nAkbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, Edward Grefenstette, Samuel R Bowman, Tim Rockt\u00e4schel, and Ethan Perez. Debating with more persuasive llms leads to more truthful answers. arXiv preprint arXiv:2402.06782, 2024.   \nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35: 22199\u201322213, 2022.   \nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611\u2013626, 2023.   \nAndrew K Lampinen, Ishita Dasgupta, Stephanie CY Chan, Kory Matthewson, Michael Henry Tessler, Antonia Creswell, James L McClelland, Jane X Wang, and Felix Hill. Can language models learn from explanations in context? arXiv preprint arXiv:2204.02329, 2022. ", "page_idx": 9}, {"type": "text", "text": "Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for\" mind\" exploration of large scale language model society. arXiv preprint arXiv:2303.17760, 2023a. ", "page_idx": 10}, {"type": "text", "text": "Huao Li, Yu Quan Chong, Simon Stepputtis, Joseph Campbell, Dana Hughes, Michael Lewis, and Katia Sycara. Theory of mind for multi-agent collaboration via large language models. arXiv preprint arXiv:2310.10701, 2023b. ", "page_idx": 10}, {"type": "text", "text": "Yang Li, Yangyang Yu, Haohang Li, Zhi Chen, and Khaldoun Khashanah. Tradinggpt: Multi-agent system with layered memory and distinct characters for enhanced financial trading performance. arXiv preprint arXiv:2309.03736, 2023c. ", "page_idx": 10}, {"type": "text", "text": "Yuan Li, Yixuan Zhang, and Lichao Sun. Metaagents: Simulating interactions of human behaviors for llm-based task-oriented coordination via collaborative generative agents. arXiv preprint arXiv:2310.06500, 2023d. ", "page_idx": 10}, {"type": "text", "text": "Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. Encouraging divergent thinking in large language models through multi-agent debate. arXiv preprint arXiv:2305.19118, 2023. ", "page_idx": 10}, {"type": "text", "text": "Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021. ", "page_idx": 10}, {"type": "text", "text": "Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang. Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization. arXiv preprint arXiv:2310.02170, 2023. ", "page_idx": 10}, {"type": "text", "text": "Meta AI. Meta llama 3. https://ai.meta.com/blog/meta-llama-3/, 2024. ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Julian Michael, Salsabila Mahdi, David Rein, Jackson Petty, Julien Dirani, Vishakh Padmakumar, and Samuel R Bowman. Debate helps supervise unreliable experts. arXiv preprint arXiv:2311.08702, 2023. ", "page_idx": 10}, {"type": "text", "text": "Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022. ", "page_idx": 10}, {"type": "text", "text": "OpenAI, 2022. URL https://openai.com/blog/chatgpt/. ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Jeongeon Park, Bryan Min, Xiaojuan Ma, and Juho Kim. Choicemates: Supporting unfamiliar online decision-making with multi-agent conversational interactions. arXiv preprint arXiv:2310.01331, 2023a. ", "page_idx": 10}, {"type": "text", "text": "Joon Sung Park, Joseph O\u2019Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, pages 1\u201322, 2023b. ", "page_idx": 10}, {"type": "text", "text": "Chau Pham, Boyi Liu, Yingxiang Yang, Zhengyu Chen, Tianyi Liu, Jianbo Yuan, Bryan A Plummer, Zhaoran Wang, and Hongxia Yang. Let models speak ciphers: Multiagent debate through embeddings. arXiv preprint arXiv:2310.06272, 2023. ", "page_idx": 10}, {"type": "text", "text": "Sumedh Rasal. Llm harmony: Multi-agent communication for problem solving. arXiv preprint arXiv:2401.01312, 2024. ", "page_idx": 10}, {"type": "text", "text": "Jie Ren, Yao Zhao, Tu Vu, Peter J Liu, and Balaji Lakshminarayanan. Self-evaluation improves selective generation in large language models. arXiv preprint arXiv:2312.09300, 2023. ", "page_idx": 10}, {"type": "text", "text": "Joshua Robinson, Christopher Michael Rytting, and David Wingate. Leveraging large language models for multiple choice question answering. arXiv preprint arXiv:2210.12353, 2022. ", "page_idx": 10}, {"type": "text", "text": "Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, et al. Towards expert-level medical question answering with large language models. arXiv preprint arXiv:2305.09617, 2023. ", "page_idx": 10}, {"type": "text", "text": "Andries Smit, Paul Duckworth, Nathan Grinsztajn, Kale-ab Tessera, Thomas D Barrett, and Arnu Pretorius. Are we going mad? benchmarking multi-agent debate between language models for medical q&a. arXiv preprint arXiv:2311.17371, 2023.   \nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \nWen-Kwang Tsao and TrendMicro AILAB. Multi-agent reasoning with large language models for effective corporate planning. In The 10th International Conf. on Computational Science and Computational Intelligence, 2023.   \nBoshi Wang, Xiang Yue, and Huan Sun. Can chatgpt defend its belief in truth? evaluating llm reasoning via debate. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 11865\u201311881, 2023a.   \nQineng Wang, Zihao Wang, Ying Su, Hanghang Tong, and Yangqiu Song. Rethinking the bounds of llm reasoning: Are multi-agent discussions the key? arXiv preprint arXiv:2402.18272, 2024.   \nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.   \nZhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. Unleashing the emergent cognitive synergy in large language models: A task-solving agent through multi-persona self-collaboration. arXiv preprint arXiv:2307.05300, 2023b.   \nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824\u201324837, 2022.   \nQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155, 2023.   \nSang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021.   \nJintian Zhang, Xin Xu, and Shumin Deng. Exploring collaboration mechanisms for llm agents: A social psychology view. arXiv preprint arXiv:2310.02124, 2023. ", "page_idx": 11}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A Theoretical Results ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Proof of Lemma 4.2. This result holds via marginalization of the posterior predictive distribution over the latent concepts $\\Theta$ , namely ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}\\big(\\mathbf{z}_{i}^{(t+1)}\\vert\\,Z^{(t)},\\mathbf{x},\\phi_{i}\\big)=\\displaystyle\\sum_{\\theta\\in\\Theta}\\mathbb{P}\\big(\\mathbf{z}_{i}^{(t+1)}\\vert\\,\\theta,Z^{(t)},\\mathbf{x},\\phi_{i}\\big)\\mathbb{P}\\big(\\theta\\big\\vert\\,Z^{(t)},\\mathbf{x},\\phi_{i}\\big)}\\\\ &{\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\sum_{\\theta\\in\\Theta}\\mathbb{P}\\big(\\mathbf{z}_{i}^{(t+1)}\\vert\\,\\theta,\\phi_{i}\\big)\\mathbb{P}\\big(Z^{(t)},\\mathbf{x}\\vert\\theta,\\phi_{i}\\big)\\frac{\\mathbb{P}(\\theta\\vert\\,\\phi_{i})}{\\mathbb{P}(Z^{(t)},\\mathbf{x})\\vert\\phi_{i})}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\times\\displaystyle\\sum_{\\theta\\in\\Theta}\\mathbb{P}\\big(\\mathbf{z}_{i}^{(t+1)}\\vert\\,\\theta,\\phi_{i}\\big)\\mathbb{P}\\big(\\mathbf{x}\\vert\\,\\theta,\\phi_{i}\\big)\\mathbb{P}\\big(\\theta\\big\\vert\\,\\phi_{i}\\big)\\prod_{j=1}^{n}\\mathbb{P}\\big(\\mathbf{z}_{j}^{(t)}\\,\\theta,\\phi_{i}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Proof of Theorem 5.1. Since each model has an identical configuration, i.e. $\\phi_{i}=\\phi_{j}$ for all $i,j\\in[n]$ , we simply refer to the configuration as $\\phi$ . For the given task $\\mathbf{x}$ let $\\pmb{\\theta}^{*(t)}$ be the realization of $\\pmb{\\theta}$ at time step $t$ . Let $Z^{(t)}=\\big(\\mathbf{z}_{1}^{(t)},\\cdot\\cdot\\cdot,\\mathbf{z}_{n}^{(t)}\\big)$ z(nt) , where each z(jt $\\mathbf{z}_{j}^{(t)}\\sim q(\\mathbf{z}|\\,\\mathbf{x},Z^{(t)},\\phi)$ . Then the conditional density for each concept $\\pmb{\\theta}$ can be written as, ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~\\mathbb{P}\\big(\\pmb\\theta|~Z^{(t)},\\mathbf x,\\pmb\\phi\\big)}\\\\ &{\\propto\\!\\!\\mathbb{P}\\big(Z^{(t)},\\mathbf x|~\\pmb\\theta,\\phi\\big)\\mathbb{P}\\big(\\pmb\\theta|~\\phi\\big)}\\\\ &{=\\!\\!\\mathbb{P}\\big(\\mathbf x|~\\pmb\\theta,\\phi\\big)\\mathbb{P}\\big(\\pmb\\theta|~\\phi\\big)\\displaystyle\\prod_{j=1}^{n}\\mathbb{P}\\big(\\mathbf z_{j}^{(t)}|~\\pmb\\theta,\\phi\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where the term $\\mathbb{P}\\big(\\mathbf{x}|\\ \\pmb{\\theta},\\phi\\big)\\big(\\theta|\\ \\phi\\big)$ is a constant with respect to the number of agents $n$ . Since $\\mathbb{P}\\big(\\pmb{\\theta}|\\phi\\big)>0$ for all $\\pmb\\theta\\in\\theta$ , each $\\mathbf{z}_{j}^{(t)}$ is an i.i.d. draw from $q(\\mathbf{z}|\\;\\mathbf{x},Z^{(t-1)},\\phi)$ , then ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\pmb{\\theta}^{*(t)}=\\operatorname*{lim}_{n\\to\\infty}\\Bigg(\\arg\\operatorname*{max}_{\\pmb{\\theta}\\in\\Theta}\\mathbb{P}\\big(\\pmb{\\theta}|\\,Z^{(t)},\\mathbf{x},\\pmb{\\phi}\\big)\\Bigg)\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Thus, as $n\\to\\infty$ , all models predict the same concept, namely $\\pmb{\\theta}^{*(t)}$ , at timestep $t$ with probability 1. ", "page_idx": 12}, {"type": "text", "text": "Proof of Theorem 5.2. Consider any two responses from agent i at time t + 1, namely z((it,+1)1 ), z((it,+2)1 ). When there are $n$ duplicate messages, the ratio between the conditional generation probabilities of both responses can be written as ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathbb{P}\\big(\\mathbf{z}_{(i+1)}^{(t+1)}\\mid Z^{(t)},\\mathbf{x},\\phi_{i}\\big)}{\\mathbb{P}\\big(\\mathbf{z}_{(i,2)}^{(t+1)}\\mid Z^{(t)},\\mathbf{x},\\phi_{i}\\big)}}\\\\ &{=\\!\\frac{\\sum_{\\theta\\in\\Theta}\\mathbb{P}\\big(\\mathbf{z}_{(i+1)}^{(t+1)}\\mid\\theta,\\phi_{i}\\big)\\mathbb{P}\\big(\\theta\\big|Z^{(t)},\\mathbf{x},\\phi_{i}\\big)}{\\sum_{\\theta\\in\\Theta}\\mathbb{P}\\big(\\mathbf{z}_{(i,2)}^{(t+1)}\\mid\\theta,\\phi_{i}\\big)\\mathbb{P}\\big(\\theta\\big|Z^{(t)},\\mathbf{x},\\phi_{i}\\big)}}\\\\ &{=\\!\\frac{\\sum_{\\theta\\in\\Theta}\\mathbb{P}\\big(\\mathbf{z}_{(i,1)}^{(t+1)}\\mid\\theta,\\phi_{i}\\big)\\mathbb{P}\\big(Z^{(t)},\\mathbf{x}\\mid\\theta,\\phi_{i}\\big)\\mathbb{P}\\big(\\theta\\big|\\phi_{i}\\big)}{\\sum_{\\theta\\in\\Theta}\\mathbb{P}\\big(\\mathbf{z}_{(i,2)}^{(t+1)}\\mid\\theta,\\phi_{i}\\big)\\mathbb{P}\\big(Z^{(t)},\\mathbf{x}\\mid\\theta,\\phi_{i}\\big)\\mathbb{P}\\big(\\theta\\big|\\phi_{i}\\big)}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}\\\\ &{=\\!\\frac{\\sum_{\\theta\\in\\Theta}\\mathbb{P}\\big(\\mathbf{z}_{(i,1)}^{(t+1)}\\mid\\theta,\\phi_{i}\\big)\\mathbb{P}\\big(Z^{(t)},\\mathbf{x}\\mid\\theta,\\phi_{i}\\big)\\mathbb{P}\\big(\\theta\\big|\\phi_{i}\\big)}{\\sum_{\\theta\\in\\Theta}\\mathbb{P}\\big(\\mathbf{z}_{(i,1)}^{(t+1)}\\mid\\theta,\\phi_{i}\\big)\\mathbb{P}\\big(\\mathbf{z}\\mid\\theta,\\phi_{i}\\big)\\mathbb{P}\\big(\\theta\\big|\\phi_{i}\\big)}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}\\\\ &{=\\!\\frac{\\sum_{\\theta\\in\\Theta}\\mathbb{P}\\big(\\mathbf{z}_{(i,1)}^{(t+1)}\\mid\\theta,\\phi_{i}\\big)\\mathbb{P}\\big(\\mathbf\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "$\\mathbb{P}\\big(Z^{(t)},\\mathbf{x}|\\;\\phi_{i}\\big)$ ", "page_idx": 12}, {"type": "text", "text": "Let $\\pmb{\\theta}^{\\prime}$ be the concept which model $i$ , with configuration $\\phi_{i}$ , believes is most likely to have produced response $\\mathbf{z}^{\\prime}$ , i.e., ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\pmb{\\theta}^{\\prime}=\\arg\\operatorname*{max}_{\\pmb{\\theta}}\\mathbb{P}\\big(\\pmb{z}^{\\prime}|\\ \\pmb{\\theta},\\pmb{\\phi}_{i}\\big)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Then the above ratio can be rewritten as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\varphi\\in\\Theta}{\\sum}\\mathbb{P}(\\mathbf{z}_{(i,1)}^{(t+1)}|\\,\\theta,\\phi_{i})\\mathbb{P}(\\mathbf{x}|\\,\\theta,\\phi_{i})\\mathbb{P}(\\theta|\\,\\phi_{i})\\left(\\prod_{j=1}^{k}\\mathbb{P}(\\mathbf{z}_{j}^{(t)}\\,\\theta,\\phi_{i})\\right)\\left(\\mathbb{P}(\\mathbf{z}^{(t)}\\,|\\,\\theta,\\phi_{i})\\right)^{m}\\frac{1}{\\left(\\mathbb{P}(\\mathbf{z}^{(t)}\\,|\\,\\theta,\\phi_{i})\\right)^{m}}\\frac{\\prod_{j=1}^{k}\\theta,\\phi_{i}}{\\left(\\mathbb{P}(\\mathbf{z}_{i,2}^{(t+1)}|\\,\\theta,\\phi_{i})\\right)^{k}}}\\\\ &{\\frac{\\theta\\exp{\\theta}}{\\exp{\\theta}}\\mathbb(\\mathbf{z}_{(i,2)}^{(t+1)}|\\,\\theta,\\phi_{i})\\mathbb{P}(\\mathbf{x}|\\,\\theta,\\phi_{i})\\mathbb{P}(\\theta|\\,\\phi_{i})\\left(\\prod_{j=1}^{k}\\mathbb{P}(\\mathbf{z}_{j}^{(t)}\\,|\\,\\theta,\\phi_{i})\\right)\\left(\\mathbb{P}(\\mathbf{z}^{(t)}\\,|\\,\\theta,\\phi_{i})\\right)^{m}\\frac{1}{\\left(\\mathbb{P}(\\mathbf{z}^{(t)}\\,|\\,\\theta,\\phi_{i})\\right)^{m}}}\\\\ &{\\qquad\\underset{\\theta\\in\\Theta}{\\sum}\\mathbb{P}(\\mathbf{z}_{(i,1)}^{(t+1)}\\,|\\,\\theta,\\phi_{i})\\mathbb{P}(\\mathbf{z}|\\,\\phi_{i})\\mathbb{P}(\\theta|\\,\\phi_{i})\\left(\\prod_{j=1}^{k}\\mathbb{P}(\\mathbf{z}_{j}^{(t)}\\,|\\,\\theta,\\phi_{i})\\right)\\left(\\frac{\\mathbb{P}(\\mathbf{z}^{(t)}\\,|\\,\\theta,\\phi_{i})}{\\mathbb{P}(\\mathbf{z}^{(t)}\\,|\\,\\theta,\\phi_{i})}\\right)^{m}}\\\\ &{\\frac{\\theta\\exp(\\theta,\\phi_{i})\\theta^{\\prime}}{\\sum}\\mathbb{P}(\\mathbf{z}_{(i,2)}^{(t+1)}\\,|\\,\\theta,\\phi_{i})\\mathbb{P}(\\mathbf{z}\\,|\\,\\theta,\\phi_{i})\\mathbb\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "For each $\\theta\\neq\\theta^{\\prime}$ we have $\\frac{\\mathbb{P}\\!\\left(\\mathbf{z}^{\\prime}|\\ \\theta,\\boldsymbol{\\phi}_{i}\\right)}{\\mathbb{P}\\!\\left(\\mathbf{z}^{\\prime}|\\ \\theta^{\\prime},\\boldsymbol{\\phi}_{i}\\right)}<1$ . Therefore ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\operatorname*{lim}_{m\\rightarrow\\infty}\\left(\\frac{\\mathbb{P}\\!\\left(\\mathbf{z}^{\\prime}\\!\\mid\\!\\theta,\\phi_{i}\\right)}{\\mathbb{P}\\!\\left(\\mathbf{z}^{\\prime}\\!\\mid\\!\\theta^{\\prime},\\phi_{i}\\right)}\\right)^{m}=0\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Note that the only summands which do not have an exponential dependency on $m$ are those associated with concept $\\pmb{\\theta}^{\\prime}$ . Therefore, when examining the limit of the above ratio with respect to the number of repeated signals $m$ , we get ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{m\\rightarrow\\infty}{\\operatorname*{lim}}\\frac{\\sum_{\\theta\\in\\Theta}\\mathbb{P}\\left(\\mathbf{z}_{(i,1)}^{(t+1)}\\mid\\theta,\\phi_{i}\\right)\\mathbb{P}\\left(\\mathbf{x}\\mid\\theta,\\phi_{i}\\right)\\mathbb{P}\\left(\\theta\\mid\\phi_{i}\\right)\\left(\\prod_{j=1}^{k}\\mathbb{P}\\left(\\mathbf{z}_{j}^{(t)}\\mid\\theta,\\phi_{i}\\right)\\right)\\left(\\mathbb{P}\\left(\\mathbf{z}^{\\prime(t)}\\mid\\theta,\\phi_{i}\\right)\\right)^{m}}{\\sum_{\\theta\\in\\Theta}\\mathbb{P}\\left(\\mathbf{z}_{(i,2)}^{(t+1)}\\mid\\theta,\\phi_{i}\\right)\\mathbb{P}\\left(\\mathbf{x}\\mid\\theta,\\phi_{i}\\right)\\mathbb{P}\\left(\\theta\\mid\\phi_{i}\\right)\\left(\\prod_{j=1}^{k}\\mathbb{P}\\left(\\mathbf{z}_{j}^{(t)}\\mid\\theta,\\phi_{i}\\right)\\right)\\left(\\mathbb{P}\\left(\\mathbf{z}^{\\prime(t)}\\mid\\theta,\\phi_{i}\\right)\\right)^{m}}}\\\\ &{=\\frac{\\mathbb{P}\\left(\\mathbf{z}_{(i,1)}^{(t+1)}\\mid\\theta^{\\prime},\\phi_{i}\\right)\\mathbb{P}\\left(\\mathbf{x}\\mid\\theta,\\phi_{i}\\right)\\mathbb{P}\\left(\\theta^{\\prime}\\mid\\phi_{i}\\right)}{\\mathbb{P}\\left(\\mathbf{z}_{(i,2)}^{(t+1)}\\mid\\theta^{\\prime},\\phi_{i}\\right)\\mathbb{P}\\left(\\mathbf{x}\\mid\\theta,\\phi_{i}\\right)\\mathbb{P}\\left(\\theta^{\\prime}\\mid\\phi_{i}\\right)}}\\\\ &{=\\frac{\\mathbb{P}\\left(\\mathbf{z}_{(i,1)}^{(t+1)}\\mid\\theta^{\\prime},\\phi_{i}\\right)}{\\mathbb{P}\\left(\\mathbf{z}_{(i,2)}^{(t+1)}\\mid\\theta^{\\prime},\\phi_{i}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Thus the relationship between any two conditional generation probabilities can be uniquely defined by $\\pmb{\\theta}^{\\prime}$ . \u53e3 ", "page_idx": 13}, {"type": "text", "text": "Proof of Theorem 5.4. First, we examine the models\u2019 outputs at the first rounds of debate. For a model $i$ which possess the shared misconception, their conditional generation probability on the first round of debate can be expressed as, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(\\mathbf{z}_{i}^{(t+1)}|\\mathbf{x},\\boldsymbol{\\phi}_{i}\\big)\\propto\\sum_{\\pmb{\\theta}}\\mathbb{P}\\big(\\mathbf{z}_{i}^{(t+1)}|\\ \\pmb{\\theta},\\boldsymbol{\\phi}_{i}\\big)\\mathbb{P}\\big(\\mathbf{x}|\\ \\pmb{\\theta},\\boldsymbol{\\phi}_{i}\\big)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Next, consider the term $\\mathbb{P}\\big(\\mathbf{x}|\\ \\pmb{\\theta},\\phi_{i}\\big)$ . Let $\\phi_{i}^{\\prime}$ be a set of model parameters which does posses the common misconception. Then, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathbb{P}\\!\\left(\\mathbf{x}|\\,\\pmb{\\theta},\\pmb{\\phi}_{i}\\right)=\\mathbb{P}\\!\\left(\\mathbf{x},\\mathbf{x}^{\\prime}|\\,\\pmb{\\theta},\\pmb{\\phi}_{i}^{\\prime}\\right)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\mathbf{x}^{\\prime}$ is a message which conveys the erroneous concept $\\theta^{\\prime}$ . Using this change of model parameters, we can express the condition generation probability as, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{P}\\big(\\mathbf{z}_{i}^{(t+1)}|\\mathbf{x},\\phi_{i}\\big)}\\\\ {\\displaystyle\\propto\\sum_{\\theta}\\mathbb{P}\\big(\\mathbf{z}_{i}^{(t+1)}|\\,\\theta,\\phi_{i}\\big)\\mathbb{P}\\big(\\mathbf{x},\\mathbf{x}^{\\prime}|\\,\\theta,\\phi_{i}^{\\prime}\\big)}\\\\ {\\displaystyle=\\sum_{\\theta}\\mathbb{P}\\big(\\mathbf{z}_{i}^{(t+1)}|\\,\\theta,\\phi_{i}\\big)\\mathbb{P}\\big(\\mathbf{x}|\\,\\theta,\\phi_{i}^{\\prime}\\big)\\mathbb{P}\\big(\\mathbf{x}^{\\prime}|\\,\\theta,\\phi_{i}^{\\prime}\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "With this formulation of the conditional generation probability, we can the ratio between the true concept $\\pmb{\\theta}^{*}$ and the erroneous concept $\\pmb{\\theta}^{\\prime}$ . ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\mathbb{P}\\left(\\mathbf{z}_{i}^{(t+1)}|\\,\\theta^{*},\\phi_{i}\\right)\\mathbb{P}\\left(\\mathbf{x}|\\,\\theta^{*},\\phi_{i}^{\\prime}\\right)\\mathbb{P}\\left(\\mathbf{x}^{\\prime}|\\,\\theta^{*},\\phi_{i}^{\\prime}\\right)}{\\mathbb{P}\\left(\\mathbf{z}_{i}^{(t+1)}|\\,\\theta^{\\prime},\\phi_{i}\\right)\\mathbb{P}\\left(\\mathbf{x}|\\,\\theta^{\\prime},\\phi_{i}^{\\prime}\\right)\\mathbb{P}\\left(\\mathbf{x}^{\\prime}|\\,\\theta^{\\prime},\\phi_{i}^{\\prime}\\right)}}\\\\ &{<\\frac{\\mathbb{P}\\left(\\mathbf{z}_{i}^{(t+1)}|\\,\\theta^{*},\\phi_{i}\\right)\\mathbb{P}\\left(\\mathbf{x}|\\,\\theta^{*},\\phi_{i}^{\\prime}\\right)}{\\mathbb{P}\\left(\\mathbf{z}_{i}^{(t+1)}|\\,\\theta^{\\prime},\\phi_{i}\\right)\\mathbb{P}\\left(\\mathbf{x}|\\,\\theta^{\\prime},\\phi_{i}^{\\prime}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which follows directly from $\\mathbb{P}\\big(\\mathbf{x}^{\\prime}|\\,\\pmb{\\theta}^{\\prime},\\phi_{i}^{\\prime}\\big)>\\mathbb{P}\\big(\\mathbf{x}^{\\prime}|\\,\\pmb{\\theta}^{*},\\phi_{i}^{\\prime}\\big)$ . When models have a shared misconception (left side of Equation 4), ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}\\!\\left(\\mathbf{z}_{i}^{(t+1)}|\\mathbf{x},\\boldsymbol{\\phi}_{i}\\right)<\\mathbb{P}\\!\\left(\\mathbf{z}_{i}^{(t+1)}|\\mathbf{x},\\boldsymbol{\\phi}_{i}^{\\prime}\\right)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "for any $\\mathbf{z}_{i}$ with ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbb{P}\\!\\left(\\mathbf{z}_{i}^{(t+1)}|\\,\\theta^{*},\\phi_{i}\\right)<\\mathbb{P}\\!\\left(\\mathbf{z}_{i}^{(t+1)}|\\,\\theta^{\\prime},\\phi_{i}\\right)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Therefore, at timestep $t=0$ , models with the shared misconception are more likely to yield responses which correlate with $\\pmb{\\theta}^{\\prime}$ . ", "page_idx": 14}, {"type": "text", "text": "For rounds $t>0$ , we can express the conditional probability as, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{P}\\big(\\mathbf{z}_{i}^{(t+1)}|\\,\\mathbf{x},Z^{(t)},\\phi_{i}\\big)}\\\\ {\\displaystyle\\propto\\sum_{\\theta}\\mathbb{P}\\big(\\mathbf{z}_{i}^{(t+1)}|\\,\\theta,\\phi_{i}\\big)\\prod_{j=1}^{n}\\mathbb{P}\\big(\\mathbf{z}_{j}^{(t)}|\\,\\theta,\\phi_{i}\\big)\\mathbb{P}\\big(\\mathbf{x},\\mathbf{x}^{\\prime}|\\,\\theta,\\phi_{i}^{\\prime}\\big)}\\\\ {\\displaystyle=\\sum_{\\theta}\\left(\\mathbb{P}\\big(\\mathbf{z}_{i}^{(t+1)}|\\,\\theta,\\phi_{i}\\big)\\prod_{j\\le m}^{n}\\mathbb{P}\\big(\\mathbf{z}_{j}^{(t)}|\\,\\theta,\\phi_{i}\\big)\\right.}\\\\ {\\displaystyle\\left.\\prod_{j>m}^{n}\\mathbb{P}\\big(\\mathbf{z}_{j}^{(t)}|\\,\\theta,\\phi_{i}\\big)\\mathbb{P}\\big(\\mathbf{x}|\\,\\theta,\\phi_{i}^{\\prime}\\big)\\mathbb{P}\\big(\\mathbf{x}^{\\prime}|\\,\\theta,\\phi_{i}^{\\prime}\\big)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For a given $\\mathbf{x}$ , the terms ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\prod_{j\\leq m}^{n}\\mathbb{P}\\big(\\pmb{\\mathscr{Q}}_{j}^{(t)}|\\,\\pmb{\\theta},\\phi_{i}\\big)\\big(\\mathbf{x}^{\\prime}|\\,\\pmb{\\theta},\\phi_{i}^{\\prime}\\big)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "are maximized, in expectation, for ${\\boldsymbol{\\theta}}={\\boldsymbol{\\theta}}^{\\prime}$ . Moreover, for the any concept $\\theta\\neq\\theta^{\\prime}$ , the ratio ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\prod_{j\\leq m}^{n}\\mathbb{P}\\big(\\mathbf{z}_{j}^{(t)}|\\,\\theta^{\\prime},\\phi_{i}\\big)\\big(\\mathbf{x}^{\\prime}|\\,\\theta^{\\prime},\\phi_{i}^{\\prime}\\big)}{\\prod_{j\\leq m}^{n}\\mathbb{P}\\big(\\mathbf{z}_{j}^{(t)}|\\,\\theta,\\phi_{i}\\big)\\big(\\mathbf{x}^{\\prime}|\\,\\theta,\\phi_{i}^{\\prime}\\big)}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "is monotonically increasing as $m$ increases. Therefore, for any round, a model with the shared misconception is more likely to generate answers correlating with $\\theta^{\\prime}$ , than those without the shared misconception. Further, the likelihood of generating such answers increases with $m$ . \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Proof of Theorem 6.1. As shown in the proof of Theorem 5.4, we can express each model\u2019s conditional generation probability as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbb{P}\\big(\\mathbf{z}_{i}^{(t+1)}|\\,\\mathbf{x},Z^{(t)},\\phi_{i}\\big)}\\\\ {\\displaystyle\\propto\\sum_{\\theta}\\mathbb{P}\\big(\\mathbf{z}_{i}^{(t+1)}|\\,\\theta,\\phi_{i}\\big)\\prod_{j=1}^{n}\\mathbb{P}\\big(\\mathbf{z}_{j}^{(t)}|\\,\\theta,\\phi_{i}\\big)\\mathbb{P}\\big(\\mathbf{x},\\mathbf{x}^{\\prime}|\\,\\theta,\\phi_{i}^{\\prime}\\big)}\\\\ {\\displaystyle=\\sum_{\\theta}\\left(\\mathbb{P}\\big(\\mathbf{z}_{i}^{(t+1)}|\\,\\theta,\\phi_{i}\\big)\\prod_{j\\le m}^{n}\\mathbb{P}\\big(\\mathbf{z}_{j}^{(t)}|\\,\\theta,\\phi_{i}\\big)\\right.}\\\\ {\\displaystyle\\left.\\prod_{j>m}^{n}\\mathbb{P}\\big(\\mathbf{z}_{j}^{(t)}|\\,\\theta,\\phi_{i}\\big)\\mathbb{P}\\big(\\mathbf{x}|\\,\\theta,\\phi_{i}^{\\prime}\\big)\\mathbb{P}\\big(\\mathbf{x}^{\\prime}|\\,\\theta,\\phi_{i}^{\\prime}\\big)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\mathbf{x}^{\\prime}$ is a response which conveys the erroneous concept $\\pmb{\\theta}^{\\prime}$ . Under this expression, each response $\\mathbf{z}_{i}^{(t)}$ is generated according to $D(\\mathbf{x},Z^{(t)},\\phi_{i})$ where each distribution differs only by the model parameters $\\phi_{i}$ . For $i\\leq m$ , the model parameters $\\phi_{i}$ posses the common misconception, i.e., each distribution $D(\\mathbf{z}_{j}^{(t)}|\\propto,Z^{(t)},\\phi_{i})$ has a common scaling factor $\\mathbb{P}\\big(\\mathbf{x}^{\\prime}|\\ \\pmb{\\theta},\\phi_{i}^{\\prime}\\big)$ which is maximized at ${\\boldsymbol{\\theta}}={\\boldsymbol{\\theta}}^{\\prime}$ . Since only these models share this scaling factor, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[\\mathrm{KL}\\big(D(\\pmb{\\theta}^{\\prime}|\\ \\mathbf{z}_{i_{1}}),\\ D(\\pmb{\\theta}^{\\prime}|\\ \\mathbf{z}_{i_{2}})\\big)\\big]\\ge\\ \\mathbb{E}\\big[\\mathrm{KL}\\big(D(\\pmb{\\theta}^{\\prime}|\\ \\mathbf{z}_{i_{1}}),\\ D(\\pmb{\\theta}^{\\prime}|\\ \\mathbf{z}_{j})\\big)\\big]\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "whenever $i_{1},i_{2}\\leq m<j$ . Hence, diversity pruning is more likely to select terms from agents with $m<j$ (i.e., those without the shared misconception), then agents with $j\\leq m$ . Since this holds true on every round of debate, all responses in the debate process place a lower weight on responses associated with $\\pmb{\\theta}^{\\prime}$ , i.e., a lower weight is placed on responses which have a higher chance of being incorrect. \u53e3 ", "page_idx": 15}, {"type": "text", "text": "Proof of Theorem 6.2. At each round $t$ , quality pruning selection a set of $k$ responses, ", "page_idx": 15}, {"type": "equation", "text": "$$\nZ^{\\prime(t)}=\\!\\underset{Y\\subset Z^{(t)}}{\\mathrm{argmin}}\\sum_{\\mathbf{z}_{i},\\in Y}\\mathrm{KL}\\big(D(\\pmb{\\theta}|\\mathbf{x}),\\:D(\\pmb{\\theta}|\\mathbf{z}_{i})\\big)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n|Y|=k\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "As such, for any set of responses $Z^{(t)}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{\\mathbf{z}_{i}^{\\prime}\\in Z^{\\prime}^{(t)}}\\mathrm{KL}\\big(D(\\pmb{\\theta}|\\,\\mathbf{x}),\\,D(\\pmb{\\theta}|\\,\\mathbf{z}_{i}^{\\prime})\\big)\\leq\\sum_{\\mathbf{z}_{i}\\in Z^{(t)}}\\mathrm{KL}\\big(D(\\pmb{\\theta}|\\,\\mathbf{x}),\\,D(\\pmb{\\theta}|\\,\\mathbf{z}_{i})\\big)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Using the fact that $Z^{\\prime(t)}\\subset Z^{(t)}$ , we can write, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{{\\boldsymbol x}_{i}^{\\prime}\\in{\\boldsymbol Z}^{\\prime\\prime}^{(t)}}{\\sum}\\mathrm{KL}\\big(D({\\boldsymbol\\theta}|{\\boldsymbol\\mathbf x}),\\,D({\\boldsymbol\\theta}|\\,\\mathbf z_{i}^{\\prime})\\big)}\\\\ &{\\quad\\le\\underset{{\\boldsymbol x}_{i}^{\\prime}\\in{\\boldsymbol Z}^{\\prime\\prime}^{(t)}}{\\sum}\\mathrm{KL}\\big(D({\\boldsymbol\\theta}|{\\boldsymbol\\mathbf x}),\\,D({\\boldsymbol\\theta}|\\,\\mathbf z_{i}^{\\prime})\\big)+\\underset{\\mathbf z_{i}\\in{\\boldsymbol Z}^{(t)}\\backslash{\\boldsymbol{Z}}^{\\prime\\prime}^{(t)}}{\\sum}\\mathrm{KL}\\big(D({\\boldsymbol\\theta}|{\\boldsymbol\\mathbf x}),\\,D({\\boldsymbol\\theta}|\\,\\mathbf z_{i})\\big)}\\\\ &{\\implies}\\\\ &{\\quad0\\le\\underset{\\mathbf z_{i}\\in{\\boldsymbol Z}^{(t)}\\backslash{\\boldsymbol{Z}}^{\\prime\\prime}^{(t)}}{\\sum}\\mathrm{KL}\\big(D({\\boldsymbol\\theta}|\\,\\mathbf x),\\,D({\\boldsymbol\\theta}|\\,\\mathbf z_{i})\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, for a random task and answer pair $(\\mathbf{x},y)\\sim D(\\pmb{\\theta}^{*})$ , the relationship between the correctness of answers in $Z^{(t)}$ and $Z^{\\prime(t)}$ is ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathfrak{L}_{(\\mathbf{x},y)\\sim D(\\theta^{*})}\\left[\\frac{1}{|Z^{\\prime(t)}|}\\sum_{\\mathbf{z}_{i}\\in Z^{\\prime}(t)}\\mathbb{P}\\big(a(\\mathbf{z}_{i})=y\\big)\\right]\\ge\\mathbb{E}_{(\\mathbf{x},y)\\sim D(\\theta^{*})}\\left[\\frac{1}{|Z^{(t)}\\setminus Z^{\\prime(t)}|}\\sum_{\\mathbf{z}_{i}\\in Z^{\\prime(t)}\\backslash Z^{\\prime(t)}}\\mathbb{P}\\big(a(\\mathbf{z}_{i})=y\\big)\\right],\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "That is, in expectation, the responses which are selected for quality pruning are at least as correct as those which are removed by quality pruning. Therefore, in expectation across all possible generations ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathbb{E}_{\\mathbf{z}_{i}^{(t+1)}}\\left[\\sum_{\\theta\\in\\Theta}\\mathbb{P}\\big(\\mathbf{z}_{i}^{(t+1)}\\mid\\theta,\\phi_{i}\\big)\\mathbb{P}\\big(\\mathbf{x}\\mid\\theta,\\phi_{i}\\big)\\mathbb{P}\\big(\\theta\\mid\\phi_{i}\\big)\\right.\\quad\\prod_{z_{j}^{\\prime}\\in Z^{\\prime}(t)}^{n}\\mathbb{P}\\big(\\mathbf{z}_{j}^{\\prime(t)}\\mid\\theta,\\phi_{i}\\big)\\right]}\\\\ &{\\displaystyle\\ge\\mathbb{E}_{\\mathbf{z}_{i}^{(t+1)}}\\left[\\sum_{\\theta\\in\\Theta}\\mathbb{P}\\big(\\mathbf{z}_{i}^{(t+1)}\\mid\\theta,\\phi_{i}\\big)\\mathbb{P}\\big(\\mathbf{x}\\mid\\theta,\\phi_{i}\\big)\\mathbb{P}\\big(\\theta\\mid\\phi_{i}\\big)\\prod_{z_{j}\\in Z^{(t)}}^{n}\\mathbb{P}\\big(\\mathbf{z}_{j}^{(t)}\\mid\\theta,\\phi_{i}\\big)\\right]}\\\\ &{\\displaystyle\\implies\\mathbb{E}_{\\mathbf{z}_{i}^{(t+1)}}\\left[\\mathbb{P}\\big(\\mathbf{z}_{i}^{(t+1)}\\mid\\mathbf{x},Z^{(t)},\\phi_{i}\\big)\\right]\\ge\\mathbb{E}_{\\mathbf{z}_{i}^{(t+1)}}\\left[\\mathbb{P}\\big(\\mathbf{z}_{i}^{(t+1)}\\mid\\mathbf{x},Z^{\\prime(t)},\\phi_{i}\\big)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, the probability of model $i$ generating a response $\\mathbf{z}_{i}^{(t)}$ at time $t$ , which has $a(\\mathbf{z}_{i}^{(t)})=y$ is greater when conditioning only on the responses selected by quality pruning, i.e., $Z^{\\prime(t)}$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "Proof of Theorem 6.4. Let $\\mathbf{z}$ be a given response and $\\mathbf{z}^{\\prime}$ be a corrected version of that response after misconception refutation, then ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{KL}\\big(D(\\pmb\\theta|\\;\\mathbf x,y),D(\\pmb\\theta|\\;\\mathbf z^{\\prime})\\big)\\leq\\mathrm{KL}\\big(D(\\pmb\\theta|\\;\\mathbf x,y),D(\\pmb\\theta|\\;\\mathbf z)\\big)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "That is, the distribution over concepts given the corrected response $\\mathbf{z}^{\\prime}$ is more similar to the true distribution over concepts given the task $\\mathbf{x}$ and answer $y$ compared to the original answer $\\mathbf{z}$ . ", "page_idx": 16}, {"type": "text", "text": "Similar to the case of quality pruning, we can then express the KL divergence of all responses before refutation $Z^{(t)}$ , and after refutation $Z^{\\prime(t)}$ , as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\mathbf{z}_{i}^{\\prime}\\in Z^{\\prime\\prime}^{(t)}}\\mathrm{KL}\\big(D(\\pmb{\\theta}|\\,\\mathbf{x},y),D(\\pmb{\\theta}|\\,\\mathbf{z}_{i}^{\\prime})\\big)\\leq\\displaystyle\\sum_{\\mathbf{z}_{i}\\in Z^{(t)}}\\mathrm{KL}\\big(D(\\pmb{\\theta}|\\,\\mathbf{x},y),D(\\pmb{\\theta}|\\,\\mathbf{z}_{i})\\big)}\\\\ &{\\displaystyle\\implies\\sum_{i=1}^{n}\\mathrm{KL}\\big(D(\\pmb{\\theta}|\\,\\mathbf{x},y),D(\\pmb{\\theta}|\\,\\mathbf{z}_{i}^{\\prime})\\big)-\\mathrm{KL}\\big(D(\\pmb{\\theta}|\\,\\mathbf{x},y),D(\\pmb{\\theta}|\\,\\mathbf{z}_{i})\\big)\\leq0}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, in aggregate, misconception refutation results in all responses in $Y^{\\left(\\right)}t_{.}$ ) inducing a distribution over concepts which is more similar to the distribution over concepts given the task $\\mathbf{x}$ and answer $y$ . As shown in the case of quality pruning, this relationship implies that at the next step of generation, model $i$ is more likely to generate $\\mathbf{z}_{i}^{(t+\\bar{1})}$ with $a(\\mathbf{z}_{i}^{(t+1)})\\bar{\\mathbf{\\alpha}}=y$ when conditioning on $Z^{\\prime(t)}$ compared with $Z^{(t)}$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "B Shared Misconceptions ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Latent concepts, and by extension, shared misconceptions, are quite general and may not always be human-interpretable. To better elucidate what is meant by a shared misconception, we provide an example of one possible type of misconception. In Figure 4 we see model responses to a question regarding the song \u201cTake Me Home, Country Roads\u201d, which is a well-known song about the state West Virginia3. While the models identify the connection between the song and West Virginia, they each erroneously equate West Virginia and Virginia, ultimately leading to each model providing the wrong answer. In situations such as this, debate will converge to an incorrect answer due to each model sharing the same false belief. ", "page_idx": 16}, {"type": "text", "text": "Misconceptions can also be viewed through the lens of hallucinations. As a byproduct of erroneous training, the models in the above example have learned a false connection between two topics (Virginia and West Virginia). ", "page_idx": 16}, {"type": "text", "text": "C Experiments ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Interventions When combining our three interventions together, we find that first applying quality pruning, then diversity pruning, then misconception refutation, results in the best performance. In practice, we consider all previous responses up to the current round $t$ when applying pruning, i.e., ", "page_idx": 16}, {"type": "text", "text": "Figure 4: Example of a common misconception between models and a refutation of that misconception. ", "page_idx": 17}, {"type": "table", "img_path": "sy7eSEXdPC/tmp/69cd6f1d8c189c3a4d9c68cd8b687631216991e8a64e6f6f729643eb41263be0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "$Z^{(0)}\\cup...\\cup Z^{(t)}$ results in better performance. When considering all past responses during pruning, there is a potentential misconception refutation, then al that the same set of responses is picked at each round. We simply prevent the methods from selecting the same response during two consecutive rounds to avoid this issue. ", "page_idx": 17}, {"type": "text", "text": "Application of Interventions in Practice For approximate the KL-divergence between distributions of concepts needed for our running interventions, i.e. ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{KL}\\big(D(\\pmb{\\theta}|\\mathbf{x}),D(\\pmb{\\theta}|\\mathbf{z}_{i})\\big)}&{{}\\,\\mathrm{or}\\quad\\mathrm{KL}\\big(D(\\pmb{\\theta}|\\mathbf{z}_{i}),D(\\pmb{\\theta}|\\mathbf{z}_{j})\\big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "we use the distance between sentence embeddings of each string, $\\mathbf{x},\\,\\mathbf{z}_{i}$ , and $\\mathbf{z}_{j}$ , i.e., given sentence embedding model $g$ , we to approximate the above in practice via, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{KL}\\lVert g(\\mathbf{x})-g(\\mathbf{z}_{i})\\rVert\\quad\\mathrm{~or~}\\quad\\mathrm{KL}\\lVert g(\\mathbf{z}_{i})-g(\\mathbf{z}_{j})\\rVert}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Algorithm 1 Application of Combined Interventions ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "1: Input: task $\\mathbf{x}$ ,   \n2: $Z_{\\mathrm{all}}=\\{\\}$ // Set of all responses to consider when applying interventions   \n3: for $j=1\\ldots n$ do   \n4: get response $\\mathbf{z}_{j}^{(0)}$ by prompting $\\mathrm{LLM}_{j}$ , i.e. sample $\\mathbf{z}_{j}^{(0)}$ according to $\\mathbb{P}\\big(\\mathbf{z}|\\,\\mathbf{x},\\boldsymbol{\\phi}_{j}\\big)$   \n5: $Z_{\\mathrm{all}}.\\mathrm{add}(\\mathbf{z}_{j}^{(0)})$ // Get an initial set of responses from each LLM   \n6: end for   \n7: for $t=1\\ldots T$ do   \n8: $Z^{\\prime(t)}={\\mathrm{QualityPrune}}\\!\\left(Z_{\\mathrm{all}},k=1/2|Z_{\\mathrm{all}}|\\right)\\,,$ // Prune half the current responses   \n9: $Z^{\\prime(t)}={\\mathrm{DiversityPrune}}\\left(Z^{\\prime(t)},k=n\\right)$   \n10: $Z^{\\prime(t)}=$ MisconceptionRefutation $\\left(Z^{\\prime(t)}\\right)$ // Set of responses to use agents to consider   \n11: for $j=1\\ldots n$ do   \n12: get $\\mathbf{z}_{j}^{(t)}$ by sampling according to $\\mathbb{P}\\big(\\mathbf{z}|\\;\\mathbf{x},Z^{\\prime(t)},\\phi_{j}\\big)$ , for each $j$ // Updated responses   \n13: $Z_{\\mathrm{all}}.\\mathrm{add}\\big(\\mathbf{z}_{j}^{(t)}\\big)$   \n15: end for   \n16: Return $Z_{\\mathrm{all}}[-n:]\\,//$ Each model\u2019s response on the last round of debate ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "Models For our experiments we make use of four models: GPT-3.5, Llama-2, Llama-3, and Mistral. ", "page_idx": 17}, {"type": "text", "text": "For certain tasks such as BoolQ or MMLU, such as only providing \u201cYes\u201d or \u201cNo\u201d to BoolQ questions. Justifications for answers are important for both our method and regular debate, as such we set the minimum token for Flan-T5 to be 10, and we set the repetition penalty to be 1.5. ", "page_idx": 17}, {"type": "text", "text": "Measuring Accuracy In the BoolQ, MMLU, and Math datasets, we extract the model\u2019s answers through regular expression checking and compare these extracted answers to the true answer; models are prompted to provide their final answer in the form \u201cFinal Answer: $X\"$ . In the TrutfhulQA dataset model answers are taken to be their entire response, which is then judged as being correct or incorrect by a GPT-4 judge. This judge is prompted to provide a yes-no answer to the question \u201cDose the answer {_answer_} accurately answer the question {_question_}?\". We allow for models to provide answers of abstention, e.g., responding \u201cI do not know\u201d. Abstentions correspond to an accuracy of .5 in BoolQ, .25 in MMLU, 0 in Math, and are directly scored by the LLM judge in TruthfulQA. ", "page_idx": 17}, {"type": "table", "img_path": "sy7eSEXdPC/tmp/c706d24db7d8176d0db5ecf66aa477361f077cf1472d68a2d7c5da4ae2b09503.jpg", "table_caption": ["Table 2: List of specific types of models used in experiments "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "Target Answers In Figure 1 we measure the likelihood that a given model will echo a target answer as a function of how many other model select that target answer. Target answers are Yes in BoolQ, option A in MMLU, correctAnswer \u221230 in Math, and a false answer in TruthfulQA. In our experiment, we elicit 20 answers from each model (GPT-3.5, Llama-2, Flan-T5) and then downsample these answers to ensure that each model receives a specific number of target answers. ", "page_idx": 18}, {"type": "text", "text": "Abletion of Interventions Here we provide an ablation of each of our three interventions: Misconception Refutation, Diversity Pruning, and Quality Pruning. Results are shown in Table 3. From this table, we see two key takeaways. First, a combination of all three interventions achieves the highest performance in almost all cases. Second, applying intervention individually can result in worse performance (even when compared with a single model). This is expected as our interventions are designed to work together, rather than separately. Recall that when combining the interventions we first do Quality Pruning, then Diversity Pruning, and then Misconception Refutation. This ordering of interventions ensures that we first select sufficiently relevant responses (Quality Pruning), among those relevant responses we then ensure that the distribution of opinions within these responses is well balanced (Diversity Pruning), and then we lastly ensure that none of the responses contain errors or misconceptions (Misconception Refutation). ", "page_idx": 18}, {"type": "text", "text": "Table 3: Average accuracy for each intervention: Misconception Refutation (MR). Diversity Pruning (DP), Quality Pruning (QP), a combination of all three (Ours), and vanilla debate (Debate), for 10 rounds and 6 models. Note that the \u201cSingle\u201d, \u201cDebate\u201d, and \u201cOurs\u201d columns correspond to the same columns in Table 3 ", "page_idx": 18}, {"type": "table", "img_path": "sy7eSEXdPC/tmp/24f26388192e0eb547f5133e29639cb70be5f7b3eee63a705876e8991eb28de5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Resource Details For all experiments, we use one Nvdida Tesla V100 GPU and one Intel 32-core CPU. For inference with non-API models (i.e., Llama-2, Llama-3, and Mistral, we use the VLLM library Kwon et al. [2023]). Ten rounds of debate with six models and 3,000 questions has a mean completion time of 4 hours for Llama-2, Llama-3, and Mistral. For GPT-3.5, we use the open library, ten rounds of debate with six models and 3,000 questions, has a mean completion time of 12 hours. ", "page_idx": 19}, {"type": "text", "text": "C.1 Prompt Examples ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We provide example prompt templates for the BoolQ dataset. Prompt templates for other datasets are similar but have changes to reflect the different task types (e.g., multiple-choice answers in MMLU compared to yes-no answers in BoolQ). ", "page_idx": 19}, {"type": "text", "text": "Round 0, or No Debate: ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "prompt $=$ You will be given a yes-no question which is based on a passage. You should use the passage to help you answer the question. You should give a brief justification for your answer, and you must provide a final answer of either Yes or No. \\n Question: {_QUESTION_} \\n Passage: {_PASSAGE_} ", "page_idx": 19}, {"type": "text", "text": "Debate with Round $>0$ ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "prompt $=$ Several other models have provided responses to a yes-no question, below are their responses: \\n Model 1: {_RESPONSE[1]_} ", "page_idx": 19}, {"type": "text", "text": "\\n Model n: {_RESPONSE[n]_}   \n\\n You should consider these responses when answering the following yes-no   \nquestion which is based on a passage.   \nYou should use the given responses and the passage to help you answer the question. You should give a brief justification for your answer, and you must provide a final answer of either Yes or No.   \n\\n Question: {_QUESTION_}   \n\\n Passage: {_PASSAGE_} ", "page_idx": 19}, {"type": "text", "text": "Misconception Refutation (Identifying Misconceptions) ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "prompt $\\mathbf{\\mu}=\\mathbf{\\mu}\\tau$ would like you to evaluate an answer to a question based on a passage. Please evaluate this answer and identify any errors, misconceptions, or inconsistencies with the passage. If you identify any such errors, please provide a short list of specific details and briefly discuss how the misconceptions can be fixed. \\n Question: {_QUESTION_} \\n Passage: {_PASSAGE_} \\n Answer to Evaluate Answer: {_GIVEN_ANSWER_} ", "page_idx": 19}, {"type": "text", "text": "Misconception Refutation (Fixing Misconceptions) ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "prompt $\\mathbf{\\mu}=\\mathbf{\\mu}\\tau$ would like you to make corrections to a response. You will be given a yes-no question based on a passage, a response to that question, and a list of possible issues with the response. I want you to provide a corrected version of the response based on the list of possible issues. You should make as few changes as possible. \\n Question: {_QUESTION_} ", "page_idx": 19}, {"type": "text", "text": "\\n Passage: {_PASSAGE_} \\n Response to Correct: {_RESPONSE_} \\n Possible Issues: {_LIST_OF_ISSUES_} ", "page_idx": 20}, {"type": "text", "text": "Targeted Answer (Advocating for a Specific Answer) ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "prompt $=$ You will be given a yes-no question which is based on a passage. You should use the passage to provide an answer of {_TARGET_ANSWER_}. You should give a brief justification for that answer, and you must provide a final answer {_TARGET_ANSWER_}. \\n Question: {_QUESTION_} \\n Passage: {_PASSAGE_} ", "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The claims made in the abstract match theoretical and experimental results. Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We mention the limitation of our approach in the conclusion section and experiment section. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "All of our theorems state the assumption, and we provide complete proofs in the appendix. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: We include the details of the experimental results and setup. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We cite all data used in the paper and we will release our code publicly upon publication. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We provide full information about all expeirments. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We provide confidence intervals for experimental results. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: We report the GPU and CPU types and mounts. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "We have read the code of ethics and are adhering to it. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We discuss societal impacts. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: All data that we use is publicly available and properly cited. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 25}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] Justification: [NA] Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]