{"references": [{"fullname_first_author": "Geoffrey Hinton", "paper_title": "Distilling the knowledge in a neural network", "publication_date": "2015-00-00", "reason": "This paper introduced the concept of knowledge distillation, a core technique that the current paper builds upon and analyzes in the context of self-distillation."}, {"fullname_first_author": "Rushiv Das", "paper_title": "Understanding self-distillation in the presence of label noise", "publication_date": "2023-00-00", "reason": "This paper provides a theoretical analysis of one-step self-distillation in linear regression, which the current paper extends to analyze multi-step self-distillation."}, {"fullname_first_author": "Tommaso Furlanello", "paper_title": "Born again neural networks", "publication_date": "2018-00-00", "reason": "This paper empirically demonstrated the benefits of repeated self-distillation, a key phenomenon investigated in the current paper."}, {"fullname_first_author": "Zhilin Li", "paper_title": "Learning from noisy labels with distillation", "publication_date": "2017-00-00", "reason": "This paper showed performance gains from self-distillation, motivating further research into understanding the phenomenon explored in the current work."}, {"fullname_first_author": "Zihang Zhang", "paper_title": "Self-distillation as instance-specific label smoothing", "publication_date": "2020-00-00", "reason": "This paper offers an empirical explanation for the gains of self-distillation, providing a different perspective to that presented in the current work."}]}