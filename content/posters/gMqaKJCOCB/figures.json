[{"figure_path": "gMqaKJCOCB/figures/figures_3_1.jpg", "caption": "Figure 1: The standard 1-step self-distillation defined in Eq. (1) with parameter \u03be and k-step self-distillation that repeatedly applies Eq. (1) with parameter \u03be(k) = [\u03be(k), \u03be(k),..., \u03be(k)] \u2208 Rk.", "description": "This figure illustrates the difference between 1-step and k-step self-distillation. In 1-step self-distillation, a teacher model (T) is used to train a student model (S) using a combined loss function that considers both the teacher's prediction and the ground truth label. In k-step self-distillation, this process is repeated k times, where the student model from the previous step acts as the teacher for the next step. The figure shows this process using boxes to represent the models and arrows to represent the training process with parameters \u03be.", "section": "Repeated self-distillation"}, {"figure_path": "gMqaKJCOCB/figures/figures_5_1.jpg", "caption": "Figure 2: On a synthetic problem family with dimension d = 100, noise variance \u03b3 = 0.1, and \u03b8* = u1 (agreement with Asmp. 2.2); we set the singular values of X with a power law from s1 = 1 to sr = {0.8,0.5} (left and right panels) and vary r = rank(X). Both plots show a linear increase of the relative gain of r-step self-distillation in excess risk, i.e. the ratio A/B where A := min\u03bb>0 ExcessRisk(\u03b8(\u03bb)) and B := min\u03bb>0,\u03be(r)\u2208Rr ExcessRisk(\u03b8(\u03bb,\u03be(r))); demonstrating that r-step SD outperforms ridge by a factor of \u03a9(r), with the constant inside the \u03a9 (i.e. slope of the line) changing with the effective condition number, s1/sr.", "description": "This figure empirically demonstrates the multiplicative separation between the excess risk achieved by r-step self-distillation and the excess risk achieved by 1-step self-distillation and ridge regression, in the context of a synthetic problem family.  The plots show that the relative gain in excess risk increases linearly with the rank (r) of the input data matrix (X). This empirically supports Theorem 1 which theoretically proves this multiplicative separation under specific assumptions.", "section": "4.1 The r-step self-distillation significantly improves upon the 1-step self-distillation"}, {"figure_path": "gMqaKJCOCB/figures/figures_6_1.jpg", "caption": "Figure 3: On a synthetic task (explained in Section 5.1), X has rank 4 with (a) \u03b8* = u\u2081 and distinct sj's; (b) s = [1, 1, 1/2, 1/3]; (c) \u03b8* = 0.5(u\u2081 + u\u2082 + u\u2083 + u\u2084). Each additional step of SD with optimal choice of \u03be(k) reduces ExcessRisk(\u03b8(\u03bb, (\u03be(k))*)) for any choice of \u03bb on the x-axis. Panel (a) satisfies Asmp. 2 and hence 4-step SD is necessary to achieve the optimal excess risk. This is no longer true when Asmp. 2.1 is violated (b) or Asmp. 2.2 is violated (c). Excess risk achieved by 4-step SD (i.e. the green lines) in panels (a) and (c) exactly match the numerical value given by RHS of eq. (14), i.e. the fundamental lower bound for any SD estimator. But this is not the case in panel (b) [which has the same lower bound from eq. (14) as panel (a)], because Asmp. 2.1 is violated.", "description": "The figure shows the necessity of the assumptions made in Theorem 1. It shows the excess risk of different estimators (ridge, 1-step SD, 2-step SD, 3-step SD, and 4-step SD) for different values of \u03bb and 3 different synthetic datasets. The first dataset satisfies the assumptions, the second violates assumption 2.1, and the third violates assumption 2.2. The results show that the assumptions are necessary for multi-step SD to significantly outperform 1-step SD and ridge.", "section": "5 Synthetic Experiments"}, {"figure_path": "gMqaKJCOCB/figures/figures_6_2.jpg", "caption": "Figure 4: On the synthetic problem from Figure 3a, we fix X = 0.125 and set the singular values of X as s; = {1 \u2013 (j \u2013 1)\u20ac}, i.e. consecutive values are separated by e. For k-step SD with k \u2208 {1,2,3}, we plot (g(k))*(x) (i.e. optimal values of the \u00a7 parameters) by varying \u20ac \u2208 {0.2, 0.1, 0.05, 0.02, 0.01}. The magnitude of (k) values increases as the singular gap e decreases, verifying Remark 4.2.", "description": "This figure shows the impact of the singular value gap on the optimal hyperparameter values for different numbers of self-distillation steps. As the gap decreases (i.e., singular values become closer), the magnitude of the optimal hyperparameters increases. This behavior is consistent with Remark 4.2 in the paper, which discusses the necessity of a significant gap between singular values for achieving large performance gains with multi-step self-distillation.", "section": "4.2 Necessity of Assumption 2"}, {"figure_path": "gMqaKJCOCB/figures/figures_13_1.jpg", "caption": "Figure 1: The standard 1-step self-distillation defined in Eq. (1) with parameter \u03be and k-step self-distillation that repeatedly applies Eq. (1) with parameter \u03be(k) = [\u03be(k), \u03be(k),..., \u03be(k)] \u2208 Rk.", "description": "This figure illustrates the difference between 1-step self-distillation and k-step self-distillation.  In 1-step self-distillation, a teacher model (T) trains a student model (S) using a combined loss function of teacher predictions and ground truth labels. In k-step self-distillation, this process is repeated k times, where the student model from the previous step becomes the teacher for the next step. Each step introduces an additional hyperparameter \u03be(k) which influences the weighting of teacher predictions and ground truth labels in the loss function.", "section": "Repeated self-distillation"}, {"figure_path": "gMqaKJCOCB/figures/figures_14_1.jpg", "caption": "Figure 1: The standard 1-step self-distillation defined in Eq. (1) with parameter \u03be and k-step self-distillation that repeatedly applies Eq. (1) with parameter \u03be(k) = [\u03be(k), \u03be(k), ..., \u03be(k)] \u2208 Rk.", "description": "This figure illustrates the difference between 1-step self-distillation and k-step self-distillation.  In 1-step self-distillation, a single student model (S) is trained using predictions from a teacher model (T) and ground truth labels.  In k-step self-distillation, the process is repeated k times; each time, the student model from the previous step becomes the teacher for the next student model.  The parameters \u03be and \u03be(k) represent the imitation parameter that balances the contributions of teacher predictions and ground truth labels.", "section": "3 Repeated self-distillation"}, {"figure_path": "gMqaKJCOCB/figures/figures_27_1.jpg", "caption": "Figure 8: Plot of excess risk of k-step SD with optimal \u03be(k) for each \u03bb, for k \u2208 {0, 1, 2, 3, 4} on a synthetic problem (Section 5.1).", "description": "This figure displays the excess risk curves for ridge regression and 1-4 steps of self-distillation for a range of \u03bb values. The curves show how the excess risk changes as the number of steps and the regularization parameter \u03bb are varied. It demonstrates how multi-step self-distillation can reduce the excess risk in linear regression. The figure also validates the necessity of assumptions made earlier in the paper. The two different panels represent different noise levels and different relationships between \u03b8* and the eigenvectors of XX\u1d40.  The results demonstrate how multi-step SD significantly improves upon the 1-step SD and Ridge regression.", "section": "5.1 Synthetic Experiments"}, {"figure_path": "gMqaKJCOCB/figures/figures_27_2.jpg", "caption": "Figure 1: The standard 1-step self-distillation defined in Eq. (1) with parameter \u03be and k-step self-distillation that repeatedly applies Eq. (1) with parameter \u03be(k) = [\u03be(k), \u03be(k), ..., \u03be(k)] \u2208 Rk.", "description": "This figure illustrates the difference between one-step self-distillation and k-step self-distillation.  In one-step self-distillation, a teacher model (T) trains a student model (S) using a combined loss function of the teacher's prediction and ground truth. In k-step self-distillation, the process is repeated k times, with the student model from the previous step becoming the teacher for the next step. Each step uses a parameter \u03be to weight the influence of the teacher's prediction and ground truth labels.", "section": "Repeated self-distillation"}, {"figure_path": "gMqaKJCOCB/figures/figures_30_1.jpg", "caption": "Figure 10: The alignment of \u03b8* to the eigenbasis directions {uj}j=1 for the three datasets used in the experiments. The low-alignment of \u03b8* to any of the eigenbasis directions for the AEP dataset explains why SD provides no gain over ridge in the test set MSE values observed in Table 1. Details of the methodology used to compute the alignment are provided in Appendix L.2.", "description": "This figure shows the alignment between the true parameter vector \u03b8* and the eigenvectors of the data covariance matrix for three real-world datasets.  The height of each bar represents the squared cosine similarity between \u03b8* (approximated by the ridge regression solution) and each eigenvector. For the Air Quality and Airfoil datasets, \u03b8* aligns strongly with a few eigenvectors, indicating that the data contains strong linear relationships that self-distillation can exploit to reduce error. In contrast, for the AEP dataset, \u03b8* does not align strongly with any eigenvector, which explains why self-distillation does not improve the model's performance.", "section": "L.2 Measuring the alignment between \u03b8* and the eigenbasis directions for the datasets"}, {"figure_path": "gMqaKJCOCB/figures/figures_30_2.jpg", "caption": "Figure 5: Validation set MSE vs \u03bb for three estimators: Ridge, 1-step SD and 2-step SD.", "description": "This figure shows the validation set MSE (mean squared error) plotted against the regularization parameter \u03bb (lambda) for three different estimators: Ridge regression, 1-step self-distillation (SD), and 2-step SD.  Each curve represents the performance of a specific estimator across different values of \u03bb. The purpose is to compare the performance of the three estimators on a validation set and to show the effect of repeated self-distillation on model performance, particularly on the test error. By tuning \u03bb and the self-distillation parameters, the 2-step SD aims to achieve lower MSE on the validation set compared to the other estimators, indicating its potential for better generalization.", "section": "5.3 Real-world regression experiments"}]