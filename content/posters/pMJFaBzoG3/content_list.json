[{"type": "text", "text": "OT4P: Unlocking Effective Orthogonal Group Path for Permutation Relaxation ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yaming Guo1,4, Chen $\\mathbf{Z}\\mathbf{h}\\mathbf{u}^{2}$ ,\u2217 Hengshu Zhu3,4,\u2217 Tieru Wu1\u2217 ", "page_idx": 0}, {"type": "text", "text": "1School of Artificial Intelligence, Jilin University ", "page_idx": 0}, {"type": "text", "text": "2School of Management, University of Science and Technology of China 3Computer Network Information Center, Chinese Academy of Sciences 4The Hong Kong University of Science and Technology (Guangzhou) {yamingguo98,zc3930155,zhuhengshu}@gmail.com,wutr@jlu.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Optimization over permutations is typically an NP-hard problem that arises extensively in ranking, matching, tracking, etc. Birkhoff polytope-based relaxation methods have made significant advancements, particularly in penalty-free optimization and probabilistic inference. Relaxation onto the orthogonal group offers unique potential advantages such as a lower representation dimension and preservation of inner products; however, equally effective approaches remain unexplored. To bridge the gap, we present a temperature-controlled differentiable transformation that maps unconstrained vector space to the orthogonal group, where the temperature, in the limit, concentrates orthogonal matrices near permutation matrices. This transformation naturally implements a parameterization for the relaxation of permutation matrices, allowing for gradient-based optimization of problems involving permutations. Additionally, by deriving a re-parameterized gradient estimator, this transformation also provides efficient stochastic optimization over the latent permutations. Extensive experiments involving the optimization over permutation matrices validate the effectiveness of the proposed method. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Permutation refers to the reordering of elements within a finite set, commonly encountered in problems involving bijections between two equally sized sets [17, 42, 15, 12]. A permutation of $n$ elements can be denoted by an $n\\times n$ permutation matrix, which is a square binary matrix that has exactly one entry of 1 in each row and each column, with all other entries being 0. We denote the set of all $n$ -order permutation matrices as $\\begin{array}{r}{\\mathcal{P}_{n}:=\\{P\\in\\{0,1\\}^{n\\times n}\\mid\\sum_{i}P_{i,j}=1,\\stackrel{.}{\\sum}_{j}P_{i,j}=1\\,(\\forall i,j)\\}}\\end{array}$ This work considers optimization over permutation matrices: ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{P\\in\\mathcal{P}_{n}}f(P).\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "Due to the combinatorial nature of permutation matrices, the cardinality of the set ${\\mathcal{P}}_{n}$ grows factorially with the dimension $n$ , typically rendering the problem NP-hard [26]. From a theoretical perspective, one of the most renowned special cases of Equation (1) is the quadratic assignment problem, which has attracted extensive research [37, 44]. In practical terms, Equation (1) also arises extensively in various machine learning tasks, including ranking [20, 75, 67, 68], matching [2], tracking [43], etc. ", "page_idx": 0}, {"type": "text", "text": "Previous studies have proposed relaxing permutation matrices into continuous spaces, including the convex hull of permutation matrices\u2014the Birkhoff polytope [15, 42]\u2014and their embeddings in a differentiable manifold\u2014the orthogonal group [72, 27]. Recently, relaxation methods involving the Birkhoff polytope have made significant advancements, particularly in penalty-free optimization ", "page_idx": 0}, {"type": "image", "img_path": "pMJFaBzoG3/tmp/207bf8f0d1785fe5e8317642bd4b2a5aa568243746a7c019fd698121f906f229.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Illustration of OT4P with colored dots to help visualize the transformation. In the limit of temperature, the orthogonal matrices obtained from OT4P converge near the permutation matrices. ", "page_idx": 1}, {"type": "text", "text": "and probabilistic inference [59, 43, 49]. As a notable example, Mena et al. [49] utilize the Sinkhorn operator [61] to transform matrices into the Birkhoff polytope, bringing them closer to permutation matrices under temperature control. This approach avoids introducing penalty terms and supports variational inference. ", "page_idx": 1}, {"type": "text", "text": "However, providing equally good relaxation methods within the orthogonal group remains an unexplored area. Indeed, relaxation onto the orthogonal group offers several unique potential advantages, lseuacdhi nasg:  tio)  aa  lsomwaelrl erre psreeasrecnht astpioanc ed;i imi)e tnhsieo onr t( $\\!\\left\\langle{\\frac{n(n-1)}{2}}\\right\\rangle$ l  cmoamtrpiaxr epdr etsoe trhvee  tBhirek ihnonfef rp porlyotdoupcet $((n-1)^{2})$ which is useful for tasks requiring the maintenance of geometric structures. In light of the above advantages, this work aims to develop an effective method for relaxing the permutation matrices onto the orthogonal group, with a particular focus on: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Flexibility: can control the degree of approximation to permutation matrices.   \n\u2022 Simplicity: does not rely on additional penalty terms.   \n\u2022 Scalability: enables learning the latent variable model with permutations. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we present Orthogonal Group-based Transformation for Permutation Relaxation (OT4P), a temperature-controlled differentiable transformation. OT4P maps unconstrained vector space to the orthogonal group, where the temperature, in the limit, concentrates orthogonal matrices near permutation matrices. As illustrated in Figure 1, OT4P involves two steps: I) map a vector ( ) to an orthogonal matrix ( ) utilizing the Lie exponential; II) move the orthogonal matrix ( ) along the geodesic, controlled by temperature, to another orthogonal matrix ( ), making it nearer to the closest permutation matrix $\\Join$ or $\\ast$ ). OT4P naturally implements a parameterization for the relaxation of permutation matrices, allowing for gradient-based optimization of problems involving permutations. In addition, OT4P, combined with the re-parameterization trick, provides stochastic optimization over the latent permutations. ", "page_idx": 1}, {"type": "text", "text": "In summary, our main contributions are as follows: ", "page_idx": 1}, {"type": "text", "text": "1. We present OT4P, a differentiable transformation for relaxing permutation matrices onto the orthogonal group, characterized by its flexibility, simplicity, and scalability (Section 3.1).   \n2. We use OT4P to implement a parameterization for the relaxation of permutation matrices, which has the advantages of not altering the original problem, not complicating the original problem, and an efficient optimization process (Section 3.2).   \n3. We derive a gradient estimator using OT4P and the re-parameterization trick, providing an efficient tool for stochastic optimization over latent permutations (Section 3.3).   \n4. We validate the effectiveness of the proposed method through extensive experiments involving the optimization of permutation matrices, including finding mode connectivity, inferring neuron identities, and solving permutation synchronization (Section 4). ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we give a brief overview of the Riemannian geometry [38] and the Lie group theory [21] involved, with a more comprehensive version available in Appendix B. ", "page_idx": 1}, {"type": "text", "text": "An $n$ -dimensional manifold $\\mathcal{M}$ is a space that can be locally approximated by a Euclidean space $\\mathbb{R}^{n}$ , where each point $x\\in\\mathcal{M}$ possesses a tangent space $T_{x}\\mathcal{M}$ as a first-order local approximation of $\\mathcal{M}$ around $x$ . The Riemannian metric is a collection $m:=\\{m_{x}\\mid x\\in\\mathcal{M}\\}$ of inner products $m_{x}(\\cdot,\\cdot):T_{x}{\\mathcal{M}}\\times T_{x}{\\mathcal{M}}\\to\\mathbb{R}$ , which may define distances on the manifold. A geodesic is a smooth curve that the tangent vector is parallel transported along the curve w.r.t. the Levi-Civita connection. ", "page_idx": 2}, {"type": "text", "text": "A Lie group $G$ is a differentiable manifold equipped with differentiable group operations, whose tangent space at the identity $e$ is the Lie algebra g. For each $g\\in G$ , there exist diffeomorphisms (Definition 2) given by the left translation $L_{g}(x):=g x$ $(\\forall x\\in G)$ , which lead to a vector space isomorphism (Definition 1) that relates the tangent space $T_{g}G$ to the Lie algebra $\\mathfrak{g}$ , i.e., $(\\mathrm{d}\\bar{L_{g}})_{e}:$ ${\\mathfrak{g}}\\to T_{g}G$ . Analogously, one can introduce the right translation: $R_{g}(x):=x g\\ (\\forall g,x\\in G)$ . A Riemannian metric $m$ on Lie group $G$ is called left-invariant (right-invariant) if it renders each left (right) translation an isometry (Definition 3), allowing us to associate neighborhoods of the identity $e$ with any point $g\\in G$ using left (right) translation. If a metric on the Lie group is both left and right invariant, it is termed the $b i$ -invariant metric. ", "page_idx": 2}, {"type": "text", "text": "The Lie group we are interested in is the orthogonal group $\\mathrm{O}(n)$ , which consists of all $n\\,\\times\\,n$ orthogonal matrices $O$ satisfying $O^{\\top}O=O O^{\\top}=I,$ . We equip $\\mathrm{O}(n)$ with the canonical metric, a biinvariant metric defined as $\\langle A,B\\rangle_{\\mathrm{F}}:=\\operatorname{trace}(A^{\\top}B)$ , where $\\langle\\cdot,\\cdot\\rangle_{\\mathrm{F}}$ is the Frobenius inner product and $\\operatorname{trace}(\\cdot)$ is the trace of a matrix. The subset of $\\mathrm{O}(n)$ with determinant $+1$ forms a subgroup known as the special orthogonal group, denoted by $\\operatorname{SO}(n):=\\{O\\in\\mathbb{R}^{n\\times n}\\mid O^{\\top}O=I,\\det O=+1\\}$ . The Lie algebra ${\\mathfrak{s o}}(n)$ of the Lie group $\\mathrm{SO}(n)$ comprises $n\\times n$ skew-symmetric matrices, expressed as ${\\mathfrak{s o}}(n):=\\{A\\in\\mathbb{R}^{n\\times n}\\mid A^{\\top}=-A\\}$ . The Lie exponential $\\mathrm{{expm}(\\cdot)}$ , coinciding with the matrix exponential in the context of the matrix Lie group, maps elements in ${\\mathfrak{s o}}(n)$ to $\\mathrm{SO}({\\bar{n}})$ , defined by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname{expm}(A):=I+\\sum_{k=1}^{\\infty}{\\frac{A^{k}}{k!}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The series in Equation (2) converges for all matrices $A$ . The local inverse function of the matrix exponential is supposed to be the matrix logarithm, which is defined as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname{logm}(A):=\\sum_{k=1}^{\\infty}(-1)^{k+1}{\\frac{(A-I)^{k}}{k}}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The series in Equation (3) converges whenever $\\|A-I\\|_{\\mathrm{F}}<1$ , where $\\|A\\|_{\\mathrm{F}}:=\\sqrt{\\langle A,A\\rangle_{\\mathrm{F}}}$ represents the Frobenius norm induced by the Frobenius inner product. ", "page_idx": 2}, {"type": "text", "text": "3 Relaxing permutation on orthogonal group ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In Section 3.1, we introduce the two steps of the proposed OT4P and analyze its key properties. Then, in Section 3.2, we demonstrate how to use OT4P to implement a parameterization for the relaxation of permutation matrices, emphasizing the advantages of such a parameterization. Finally, in Section 3.3, we provide efficient stochastic optimization over the latent permutations using OT4P and the re-parameterization trick. ", "page_idx": 2}, {"type": "text", "text": "3.1 The proposed OT4P transformation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The proposed OT4P comprises two steps: I) map a point in the vector space to an orthogonal matrix; II) move the orthogonal matrix along the geodesic under temperature control, bringing it nearer to the closest permutation matrix. We summarize the pseudo-code of OT4P in Algorithm 1. ", "page_idx": 2}, {"type": "text", "text": "Step I ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Consider an unconstrained vector space $\\mathbb{R}^{\\frac{n(n-1)}{2}}$ . For a vector $a\\in\\mathbb{R}^{\\frac{n(n-1)}{2}}$ , we can fill it into an upper triangular $n\\times n$ matrix with zero in the diagonal. For example, in the case of $n=3$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n[a_{1},a_{2},a_{3}]=a\\Longleftrightarrow A=\\left(\\!\\!{\\begin{array}{c c c}{0}&{a_{1}}&{a_{2}}\\\\ {0}&{0}&{a_{3}}\\\\ {0}&{0}&{0}\\end{array}}\\!\\!\\right).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Input: Input matrix $A\\in\\mathbb{R}^{n\\times n}$ , hyperparameters $\\tau\\in(0,1]$ , $B\\in\\mathrm{SO}(n)$   \n1: Map $A$ to an orthogonal matrix: $O=\\phi(A)$ \u25b7Defined in Equation (6)   \n2: Shift $O$ to handle boundary issues: $O=B O$ \u25b7Details in Appendix C   \n3: Find the closest permutation matrix: $P=\\rho(O)$ \u25b7Defined in Equation (7)   \n4: if $\\operatorname*{det}(P)=-1$ then   \n5: Set $\\dot{\\cal D}=\\mathrm{diag}(\\{1,\\ldots,1,-1\\})$ \u25b7Extension to odd permutations, see Appendix D   \n6: else ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "8: end if ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "9: Move $O$ toward $P$ along the geodesic controlled by $\\tau$ $:\\widetilde{O}=\\psi_{\\tau}(O)$ \u25b7Defined in Equation (11) Output: The resulting orthogonal matrixO  converge near the permutation matrix $P$ ", "page_idx": 3}, {"type": "text", "text": "In the following, we employ matrices $A\\in\\mathbb{R}^{n\\times n}$ rather than vectors $a$ to represent the elements in $\\mathbb{R}^{\\frac{n(n-1)}{2}}$ . A skew-symmetric matrix is uniquely determined by $\\textstyle{\\frac{n(n-1)}{2}}$ scalars, i.e., the entries above the main diagonal. Therefore, there exists an isomorphism between the vector space $\\mathbb{R}^{\\frac{n(n-1)}{2}}$ and the Lie algebra ${\\mathfrak{s o}}(n)$ formed by skew-symmetric matrices, given by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\alpha:\\mathbb{R}^{\\frac{n(n-1)}{2}}\\rightarrow\\mathfrak{s o}(n)}}\\\\ {{A\\mapsto A-A^{\\top}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "As mentioned in Section 2, we can use the matrix exponential (Lie exponential) $\\mathrm{{expm}(\\cdot)}$ to map the Lie algebra ${\\mathfrak{s o}}(n)$ to the Lie group, i.e., special orthogonal group $\\mathrm{SO}(n)$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\beta:{\\mathfrak{s o}}(n)\\to{\\mathrm{SO}}(n)}\\\\ {A\\mapsto\\operatorname{expm}(A).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Combining Equation (4) and Equation (5), we can map the unconstrained vector space R 2 to the special orthogonal group $\\mathrm{SO}(n)$ , denoted as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\phi:\\mathbb{R}^{\\frac{n(n-1)}{2}}\\rightarrow\\operatorname{SO}(n)}\\\\ {A\\mapsto\\mathrm{expm}(A-A^{\\top}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This mapping belongs to the classical category in Lie group theory and serves as an efficient solution for addressing orthogonal constraints in the field of machine learning [40, 53, 48]. Similarly to Lezcano Casado [39], we present the important properties of the mapping $\\phi(\\cdot)$ below. ", "page_idx": 3}, {"type": "text", "text": "Theorem 1. The mapping $\\phi(\\cdot)$ is differentiable, surjective, and it is injective on the domain $\\mathcal{U}:=$ $\\{A\\in\\mathbb{R}^{\\frac{n(n-1)}{2}}\\mid\\operatorname{Im}\\lambda_{k}(A-A^{\\top})\\in(-\\pi,\\pi),\\forall k\\}$ with $\\lambda_{k}(\\cdot)$ the eigenvalues. Additionally, the set $\\operatorname{SO}(n)\\setminus\\phi(\\mathcal{U})$ has a zero Lebesgue measure in $\\mathrm{SO}(n)$ . ", "page_idx": 3}, {"type": "text", "text": "The theorem indicates that each orthogonal matrix in $\\mathrm{SO}(n)$ can be represented by a vector in $\\mathbb{R}^{\\frac{n(n-1)}{2}}$ , with each representation being uniquely defined within set $\\boldsymbol{\\mathcal{U}}$ , provided it exists there. However, permutation matrices may include $-1$ as one of their eigenvalues (see Figure 4), with their corresponding representations precisely lying on the boundary of $\\boldsymbol{\\mathcal{U}}$ . If the optimal solution to Equation (1) is a permutation matrix with an eigenvalue of $-1$ , it may lead the optimization path to deviate from $\\boldsymbol{\\mathcal{U}}$ . To counter this, we propose shifting the boundary of $\\boldsymbol{\\mathcal{U}}$ to other eigenvalues by left-multiplying the result of Equation (6) with an orthogonal matrix $B\\in\\mathrm{SO}(n)$ . Theoretically, the left translation $L_{B}(O)\\,:=\\,B O$ $\\forall O\\,\\in\\,\\mathrm{SO}(n)\\,,$ ) creates a diffeomorphism (Definition 2) on $\\mathrm{SO}(n)$ , where the representation of the permutation matrix $P$ in $\\boldsymbol{\\mathcal{U}}$ is changed from $\\mathrm{logm}(P)$ to $\\log\\!\\mathrm{m}(B^{\\top}P)^{2}$ . We have empirically observed that the left translation $L_{B}$ effectively relocates the majority of permutation matrices\u2019 representations into the interior of $\\boldsymbol{\\mathcal{U}}$ . More discussion can be found in Appendix C. ", "page_idx": 3}, {"type": "text", "text": "Step II ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Given an orthogonal matrix $O\\in\\mathrm{SO}(n)$ , we would like to move it toward the closest permutation matrix along the geodesic. To achieve this, we first need to find the permutation matrix $P\\in\\mathcal{P}_{n}$ closest to $O$ , which can be expressed as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\rho(O):=\\underset{P\\in\\mathcal P_{n}}{\\arg\\operatorname*{max}}\\langle P,O\\rangle_{\\mathrm F},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\langle A,B\\rangle_{\\mathrm{F}}\\;=\\;\\mathrm{trace}(A^{\\top}B)$ denotes the Frobenius inner product. Equation (7) is a linear assignment problem that can be solved in cubic time using the Hungarian algorithm [36], with further details available in the Appendix E. ", "page_idx": 4}, {"type": "text", "text": "Once $P$ is found, we can move $O$ towards $P$ along the geodesic $O P$ . In the Lie group, utilizing the mapping $\\mathrm{logm}(\\cdot)$ , movement along the geodesic can be transformed into a more manageable movement on the Lie algebra. However, since $O$ or $P$ may be far from the identity matrix $I$ , the convergence speed of the series expansion of $\\bar{\\log\\!\\mathrm{m}}(\\cdot)$ may be slow or even fail to converge. ", "page_idx": 4}, {"type": "text", "text": "We propose to carry out the above process in the tangent space $T_{P}\\mathrm{SO}(n)$ rather than in the Lie algebra ${\\mathfrak{s o}}(n)$ . Due to the biinvariant metric $\\langle\\cdot,\\cdot\\rangle_{\\mathrm{F}}$ equipped on $\\mathrm{SO}(n)$ , the left translation $L_{P}(O)=P O$ ( $\\forall O\\in$ ${\\mathrm{SO}}(n))$ establishes an isometry (Definition 3) between the neighborhoods of $I$ and $P$ , and its derivative $(\\mathrm{d}L_{P})_{e}:{\\mathfrak{s o}}\\rightarrow$ $T_{P}\\mathrm{SO}(n)$ provides an isomorphism (Definition 1) between the Lie algebra ${\\mathfrak{s o}}(n)$ and the tangent space $T_{P}\\mathrm{SO}(n)$ . Hence, we first push $O\\in\\mathrm{SO}(n)$ into the neighbor", "page_idx": 4}, {"type": "image", "img_path": "pMJFaBzoG3/tmp/bd02291d976337e161baf3839e7f1fa01763933ed44d987174b4b8b0b7fbebbb.jpg", "img_caption": ["Figure 2: Illustration of the mappings $\\mathrm{logm}_{P}$ and $\\mathrm{expm}_{P}$ . The left translation $L_{P}$ establishes an isometry between the neighborhoods of $I$ and $P$ , and its derivative $(\\mathrm{d}L_{P})_{e}$ provides an isomorphism between ${\\mathfrak{s o}}(n)$ and $T_{P}\\mathrm{SO}(n)$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "hood of $I$ , then map it to the Lie algebra ${\\mathfrak{s o}}(n)$ using $\\operatorname{logm}({\\mathord{\\cdot}})$ , and finally pull the result into the tangent space $T_{P}\\mathrm{SO}(n)$ . In this way, we define the logarithm map at $P$ as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{logm}_{\\mathrm{P}}:\\mathrm{SO}(n)\\rightarrow T_{P}\\mathrm{SO}(n)}\\\\ &{\\quad\\quad\\quad\\quad O\\mapsto P\\log\\mathrm{m}(P^{\\top}O).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Equation (8) maps $O\\in\\mathrm{SO}(n)$ near $P$ to the tangent space $T_{P}\\mathrm{SO}(n)$ , and its convergence domain also concentrates near $P$ . Similarly, we can define its local inverse mapping: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\mathrm{expm}_{\\mathrm{P}}:T_{P}\\mathrm{SO}(n)\\rightarrow\\mathrm{SO}(n)}}\\\\ {{{}}}\\\\ {{A\\mapsto P\\,\\mathrm{expm}(P^{\\top}A).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Equation (9) maps $A\\in T_{P}\\mathrm{SO}(n)$ onto the special orthogonal group $\\mathrm{SO}(n)$ located near $P$ . With the aforementioned tools, we can easily move orthogonal matrix $O$ toward its closest permutation matrix $P$ by interpolation. Specifically, we map $P$ and $O$ to the tangent space $T_{P}\\mathrm{SO}(\\bar{n})$ for linear interpolation, and then map the interpolation result back to $\\mathrm{SO}(n)$ , given as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{O}=P\\exp(P^{\\top}\\left[\\tau P\\log\\mathfrak{m}(P^{\\top}O)+(1-\\tau)P\\log\\mathfrak{m}(P^{\\top}P)\\right])}\\\\ &{\\quad=P\\exp\\mathfrak{m}(P^{\\top}\\left[\\tau P\\log\\mathfrak{m}(P^{\\top}O)\\right])}\\\\ &{\\quad=P\\exp\\mathfrak{m}(\\tau\\log\\mathfrak{m}(P^{\\top}O))}\\\\ &{\\quad=P(P^{\\top}O)^{\\tau}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The second equation stems from the fact that $\\mathrm{logm}(I)\\,=\\,{\\bf0}$ , and the last equation follows from $A^{\\tau}\\,=\\,\\mathrm{expm}(\\tau\\log\\!\\mathrm{m}(A))$ when $\\|A-I\\|_{\\mathrm{F}}\\,<\\,1$ . The temperature parameter $\\tau\\,\\in\\,(0,1]$ is used to control the degree to which the resulting orthogonal matrix $\\widetilde O$ approaches $P$ . It is clear that $\\begin{array}{r}{\\operatorname*{lim}_{\\tau\\rightarrow0^{+}}\\lVert\\widetilde{O}-P\\rVert_{\\mathrm{F}}=0}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "Remark on odd permutations. The attentive reader may notice that Equation (10) presupposes $P\\in\\mathrm{SO}(n)$ , which works only for $P$ corresponds to even permutations. However, we can readily extend it to cases where $P$ corresponds to odd permutations. Firstly, we solve $\\arg\\operatorname*{min}_{\\widehat{P}\\in\\mathrm{SO}(n)}\\|\\widehat{P}-$ ", "page_idx": 5}, {"type": "text", "text": "$P\\mathrm{\\|_{F}^{2}}$ to identify an agent of $P$ within $\\mathrm{SO}(n)$ , which admits an analytical solution $\\widehat{\\cal P}={\\cal P}{\\cal D}$ with $D=\\operatorname{diag}(\\{1,\\ldots,1,-1\\})$ . Then, by substituting $P$ withP  in Equation (10), $O$ is moved toward $\\widehat{P}$ to obtain $\\widehat{O}$ . Finally, we right-multiply $\\widehat{O}$ by $D^{\\top}$ to map it to the neighborhood of $P$ , resulting in $\\widetilde{O}=\\widehat{O}D^{\\top}$ . Essentially, the neighborhoods of $P$ and its agent $\\widehat{P}$ are linked through an isometry, specifically the right translation $R_{D}(O):=O D$ $\\left\\langle\\forall O\\in\\mathrm{O}(n)\\right\\rangle$ ). Consequently, when $\\widehat{O}$ is moved close enough to $\\widehat{P}$ , the resulting orthogonal matrix $\\widetilde O$ will also be sufficiently close to $P$ . Please refer Appendix D for more theoretical details. ", "page_idx": 5}, {"type": "text", "text": "Let $\\displaystyle{S_{P}}$ denote the set of orthogonal matrices in $\\mathrm{SO}(n)$ whose closest permutation matrix is $P$ , and let ${\\cal S}_{P}^{\\prime}$ represent its image obtained through Equation (10). Equation (10) actually transforms the submanifold $S_{P}$ into a new submanifold ${\\cal S}_{P}^{\\prime}$ that is closer to the permutation matrix $P$ . We define the manifold consisting of all images as $\\mathcal{M}_{\\mathcal{P}}^{\\,^{\\star}}:=\\{S_{P}^{\\prime}\\subset\\mathrm{O}(n)\\;|\\;\\bar{P}\\in\\mathcal{P}_{n}\\}$ . Consequently, the special orthogonal group $\\mathrm{SO}(n)$ is mapped to a manifold $\\mathcal{M}_{\\mathcal{P}}$ that tightly wraps around the permutation matrices, which can be more formally expressed as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\psi_{\\tau}:\\operatorname{SO}(n)\\rightarrow\\mathcal{M}_{\\mathcal{P}}}\\\\ {O\\mapsto\\rho(O)D\\left(\\left[\\rho(O)D\\right]^{\\top}O\\right)^{\\tau}D^{\\top}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The aforementioned mapping covers all cases, where $D=\\mathrm{diag}(\\{1,\\dots,1,-1\\})$ for odd permutations and $D=I$ for even permutations. In Figure 5, we provide a visualization of the results of $\\psi_{\\tau}(\\cdot)$ as the temperature parameter $\\tau$ varies. The mapping $\\psi_{\\tau}(\\cdot)$ is meaningless at points where $\\rho(\\cdot)$ w.r.t. Equation (7) is discontinuous. It is important to note that $\\rho(\\cdot)$ is a piecewise constant function, changing only at points where multiple permutation matrices are equidistant. The following theorem presents key properties of the mapping $\\bar{\\psi}_{\\tau}(\\cdot)$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 2. The mapping $\\psi_{\\tau}(\\cdot)$ is differentiable, surjective, and injective on each submanifold $\\displaystyle{S_{P}}$ .   \nAdditionally, the set of meaningless points for $\\psi_{\\tau}(\\cdot)$ has a zero Lebesgue measure in $\\mathrm{SO}(n)$ . ", "page_idx": 5}, {"type": "text", "text": "The theorem shows that any point in the relaxation manifold $\\mathcal{M}_{\\mathcal{P}}$ of permutation matrices can be uniquely identified by an orthogonal matrix in the special orthogonal group $\\mathrm{SO}(n)$ , where the set of meaningless elements (i.e., not mapped any point in $\\mathcal{M}_{\\mathcal{P}}$ ) can be disregarded. Using the composite mapping $\\psi_{\\tau}\\circ\\phi$ , we create a one-to-one correspondence between $\\bar{\\mathcal{U}}:=\\{A\\in\\mathbb{R}^{\\frac{n(\\bar{n-1})}{2}}~|$ $\\operatorname{Im}\\lambda_{k}(A-A^{\\top})\\in(-\\pi,\\pi),\\forall k\\}$ and $\\mathcal{M}_{\\mathcal{P}}$ , except for points associated with zero measure sets in $\\mathrm{SO}(n)$ that are either not representable by $\\phi(\\cdot)$ or are meaningless for $\\psi_{\\tau}(\\cdot)$ . In other words, points in the manifold $\\mathcal{M}_{\\mathcal{P}}$ , which tightly wraps around the permutation matrices, can almost be represented one-to-one by points in $\\boldsymbol{\\mathcal{U}}$ that lie in the unconstrained vector space. ", "page_idx": 5}, {"type": "text", "text": "3.2 Parameterization for gradient-based optimization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "This section demonstrates how to use OT4P to implement a parameterization for the relaxation of permutation matrices, thereby allowing gradient-based optimization for Equation (1). More importantly, we present three advantages of this parameterization, making it a reasonable solution. ", "page_idx": 5}, {"type": "text", "text": "Recalling the manifold $\\mathcal{M}_{\\mathcal{P}}$ obtained from Equation (11), which converges around the permutation matrices controlled by the temperature parameter $\\tau$ . We first relax Equation (1) into an optimization problem on the manifold $\\mathcal{M}_{\\mathcal{P}}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{O\\in{\\mathcal{M}}_{\\mathcal{P}}}f(O).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Using the composite mapping $\\psi_{\\tau}\\circ\\phi$ , we transform the constrained optimization problem on the manifold $\\mathcal{M}_{\\mathcal{P}}$ into an unconstrained optimization problem in the vector space R 2 : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{A\\in\\mathbb{R}^{\\frac{n(n-1)}{2}}}f(\\psi_{\\tau}\\circ\\phi(A)).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "For the aforementioned optimization problem, we can employ standard optimization techniques, such as SGD and Adam algorithms [58], to approximate the solution. Below, we thoroughly discuss the three advantages brought about by the parameterization of OT4P. ", "page_idx": 5}, {"type": "text", "text": "The surjectivity does not alter the original problem. The surjectivity of the mapping $\\psi_{\\tau}\\circ\\phi$ implies that every point in the manifold $\\mathcal{M}_{\\mathcal{P}}$ has at least one corresponding pre-image in the vector space Rn(n2\u22121). This guarantees that, during the optimization process, any point within the manifold $\\mathcal{M}_{\\mathcal{P}}$ can be reached, thereby preventing the overlooking of any potential solutions to Equation (12). In particular, if we find a solution $A$ while dealing with Equation (13), we can solve Equation (12) by mapping $O=\\psi_{\\tau}\\circ\\phi(A)$ . ", "page_idx": 6}, {"type": "text", "text": "The injectivity does not complicate the original problem. If the optimization stays within $\\mathcal{U}:=\\{A\\in\\mathbb{R}^{\\frac{n(n-1)}{2}}\\mid\\operatorname{Im}\\lambda_{k}(A-A^{\\top})\\in(-\\pi,\\pi),\\,\\forall k\\}$ , the mapping $\\psi_{\\tau}\\circ\\phi$ map different elements in $\\boldsymbol{\\mathcal{U}}$ to different elements on $\\mathcal{M}_{\\mathcal{P}}$ . This means that each update in $\\boldsymbol{\\mathcal{U}}$ results in a unique outcome in $\\mathcal{M}_{\\mathcal{P}}$ , thereby reducing unnecessary redundant searches. Furthermore, the mapping $\\psi_{\\tau}\\circ\\phi$ does not introduce spurious local minima, as each local minima in $\\mathcal{M}_{\\mathcal{P}}$ creates a single local minima in $\\boldsymbol{\\mathcal{U}}$ . ", "page_idx": 6}, {"type": "text", "text": "The efficient optimization process. At first glance, the mapping $\\psi_{\\tau}\\circ\\phi$ involves matrix exponential and matrix power, which might demand substantial computational resources during the optimization process. Lezcano-Casado and Mart\u0131nez-Rubio [40] has proposed a cheap method for computing matrix exponential $\\mathrm{{expm}(\\cdot)}$ and its gradient, thanks to the efficient utilization of the scaling-squaring technique and Pad\u00e9 approximation [3]. Therefore, we will focus on how to handle the matrix power function efficiently. ", "page_idx": 6}, {"type": "text", "text": "\u2022 Forward process. The orthogonal matrix $O$ can be factorized, utilizing eigendecomposition, as $\\mathbf{\\bar{\\rho}}=\\,Q X Q^{-1}$ , where $Q~\\in~\\mathbb{R}^{n\\times n}$ with each column representing an eigenvector of $O$ , and $X=\\operatorname{diag}(\\{\\lambda_{1},\\ldots,\\lambda_{n}\\})$ is a diagonal matrix whose elements are the eigenvalues of $O$ . In this case, the matrix power $O^{\\tau}$ can be computed by applying the power function to the eigenvalues while keeping the eigenvectors unchanged [24], yielding $\\mathbf{\\dot{\\}}^{\\mathcal{O}^{\\tau}}=Q\\mathrm{diag}(\\{\\lambda_{1}^{\\tau},\\dots,\\bar{\\lambda}_{n}^{\\tau}\\})Q^{-1}$ . It is evident that calculating $O^{\\tau}$ is not significantly more complex than computing $n$ scalar powers. ", "page_idx": 6}, {"type": "text", "text": "\u2022 Backward process. Given an orthogonal matrix $O$ , we assume that $\\tilde{O}\\;=\\;\\psi_{\\tau}(O)$ has been obtained through Equation (11). Then, there exists a unique ortho gonal matrix $W_{\\tau}$ such that $\\tilde{O}\\,=\\,W_{\\tau}\\bar{O}$ due to the closure property of the Lie group. Therefore, in the forward pass, one can initially acquire $\\widetilde O$ using Equation (11), followed by computing the equivalent transformation of the mapping $\\psi_{\\tau}$ as $W_{\\tau}=\\widetilde{O}O^{\\top}$ . In this way, the forward pass is streamlined into $\\widetilde{O}=W_{\\tau}O$ , thereby rendering the backward pass highly efficient, as it only involves one li near transformation. ", "page_idx": 6}, {"type": "text", "text": "3.3 Re-parameterization provides stochastic optimization ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In the previous section, we considered deterministic optimization over permutation matrices. However, in many scenarios, we commonly build a probabilistic model to express the uncertainty inherent in the problem [7]. For such a task, it is crucial to have the ability to learn latent variable models associated with the latent nodes corresponding to permutations [20]. This section demonstrates how to perform stochastic optimization over the latent permutations using OT4P and the re-parameterization trick. ", "page_idx": 6}, {"type": "text", "text": "We restrict our attention to the scenario where the latent variable is a permutation matrix, $z=P$ , without loss of generality. Therefore, consider the probabilistic form of Equation (1) as follows ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\mathbb{E}_{P\\sim q(P;\\theta)}f(P).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The above equation deals with a distribution over permutation matrices rather than a single permutation matrix as in Equation (1). Evaluating and differentiating Equation (14) is challenging due to the expectation involving a sum of $n!$ terms. To remedy this, we employ the re-parameterization trick [32, 56, 14]. In particular, we simulate $q(P;\\theta)$ using the mappings $\\rho(\\cdot)$ w.r.t. Equation (7) and $\\phi(\\cdot)$ w.r.t. Equation (11), expressed as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P\\sim q(P;\\theta)\\Longleftrightarrow P=\\rho(\\phi(A+B\\epsilon))\\mathrm{~with~}\\theta:=\\{A,B\\in\\mathbb{R}^{\\frac{n(n-1)}{2}}\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\epsilon\\sim q(\\epsilon)$ is a random noise distribution. Equation (15) cleverly decouples the stochastic nature of the distribution $q(P;\\theta)$ , thereby obviating the dependence of the expectation on the parameters $\\theta$ . This enables us to draw multiple samples for the evaluation of Equation (14) with lower variance. ", "page_idx": 6}, {"type": "text", "text": "However, there exists a not differentiable mapping $\\rho(\\cdot)$ , which hinders gradient-based optimization for $\\theta$ . Recalling OT4P, we can approximate Equation (15) by relaxing the mapping $\\rho(\\cdot)$ to $\\psi_{\\tau}(\\cdot)$ . It is evident that samples drawn from distribution $\\psi_{\\tau}(\\phi(A+B\\epsilon))$ converge almost surely to those from distribution $\\rho(\\phi(A+B\\epsilon))$ owing to $\\ensuremath{\\operatorname*{lim}}_{\\tau\\to0}\\psi_{\\tau}=\\rho$ . To summarize, we can bring the gradient inside the expectation, as depicted below: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\nabla\\mathbb{E}_{\\epsilon\\sim q(\\epsilon)}f\\left(\\psi_{\\tau}(\\phi(A+B\\epsilon))\\right)=\\mathbb{E}_{\\epsilon\\sim q(\\epsilon)}\\nabla f\\left(\\psi_{\\tau}(\\phi(A+B\\epsilon))\\right),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "which can now be computed using Monte Carlo [50]. ", "page_idx": 7}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "This section conducts experiments to evaluate the performance of OT4P in optimization problems and probabilistic tasks. All experimental details not stated here, along with additional results, can be found in Appendix F. The core code for OT4P is available at https://github.com/YamingGuo98/OT4P. ", "page_idx": 7}, {"type": "text", "text": "4.1 Finding mode connectivity ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In the first experiment, we consider an optimization problem inspired by the concept of linear mode connectivity. Recent studies have shown that neural networks trained with SGD belong to a set whose weights can be permuted so that we linearly connect those weights with no detriment to the loss [13, 57]. For demonstration purposes, we examine a multi-layer perceptron (MLP) with $L$ layers and denote its weights as $\\theta=\\{\\bar{W_{l}}|l^{'}\\!\\in[L]\\}$ . To find the optimal permutation between models $\\theta_{A}$ and $\\theta_{B}$ , Ainsworth et al. [2] propose the following data-free optimization problem: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\pi=\\{P_{i}\\in\\mathcal{P}_{n_{i}}\\}}\\|W_{1}^{(A)}-P_{1}W_{1}^{(B)}\\|_{\\mathrm{F}}^{2}+\\|W_{2}^{(A)}-P_{2}W_{2}^{(B)}P_{1}^{\\top}\\|_{\\mathrm{F}}^{2}+\\cdot\\cdot+\\|W_{L}^{(A)}-P_{L}W_{L}^{(B)}\\|_{\\mathrm{F}}^{2}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "The above problem is challenging because it does not admit a polynomial-time constant-factor approximation scheme [2]. We can use OT4P to relax permutation matrices, as demonstrated in Section 3.2, enabling a gradient-based solution to Equation (17). ", "page_idx": 7}, {"type": "text", "text": "We explore a variety of network architectures, including MLP5 (5-layer MLP) [54], VGG11 [60], and ResNet18 [22]. The weights for these networks are derived from official pre-trained models in PyTorch [52], with the exception of the MLP5, which is initialized randomly. For model $\\theta_{B}$ , we randomly sample permutation matrices from a uniform distribution and apply them to permute the weights, yielding model $\\theta_{A}=\\pi(\\theta_{B})$ . The AdamW [45] with an initial learning rate of 0.1 is employed to minimize the loss w.r.t. Equation (17), with a maximum of 500 iterations. To evaluate the results, we use the $\\ell_{1}$ -Distance, $\\lVert\\theta_{A}-\\pi(\\theta_{B})\\rVert_{1}$ , to measure the difference from the target weights. Additionally, we flatten the permutation matrices and evaluate their alignment with the ground truth using Precision, Recall, and Hamming Distance. ", "page_idx": 7}, {"type": "text", "text": "We compare: 1) Weight Matching [2], which goes through each layer and greedily selects its best permutation matrix $P_{i}$ ; 2) Sinkhorn [54], relaxing the permutation matrices to the vicinity of the Birkhoff polytope utilizing the Sinkhorn operator [61, 49]; and 3) OT4P, our proposed method, which is evaluated with various temperature parameters. The results are reported in Tables 1 and 4, with each experiment conducted five times. The findings indicate that Weight Matching occasionally fails to reach ground truth due to its sensitivity to random initialization. Additionally, Sinkhorn yields poor results in the VGG11 network architecture, which we attribute to the relaxation on the Birkhoff polytope producing unreliable local minima. In contrast, OT4P finds the optimal permutation matrix in most cases. Indeed, a neural network can be conceptualized as a geometric object whose vertices correspond to the rows of the weight matrices [35]. A reasonable relaxation of the matching task in Equation (17) involves rigid transformations represented by orthogonal matrices. The proposed OT4P relaxes the permutation matrices into the orthogonal group, which is likely the primary reason for its powerful performance. ", "page_idx": 7}, {"type": "text", "text": "4.2 Inferring neuron identities ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In the second experiment, we tackle a probabilistic task motivated by the study of the neural dynamics in C. elegans [66]. This worm serves as a model organism in neuroscience, with its complete neuronal connectivity known and represented by the adjacency matrix $A\\in\\{0,1\\}^{n\\times n}$ . However, matching traces from the observed neural dynamics $Y\\in\\mathbb{R}^{n\\times1}$ to the neurons in the reference connectome $A$ poses a challenging task. Linderman et al. [43] propose simulating neural activity using a linear dynamical system $\\begin{array}{r}{\\dot{Y}_{t}=P(A\\odot W)P^{\\top}Y_{t-1}+\\epsilon}\\end{array}$ , where $\\epsilon$ is Gaussian noise, $W\\in\\mathbb{R}^{n\\times n}$ , $P\\in\\mathcal{P}_{n}$ are latent variables, and $\\odot$ is element-wise product. Our goal is to infer the latent permutation $P$ to align the observed $Y$ with the shared dynamics matrix $W$ . We address this task by maximizing the marginal log-likelihood, i.e., max $\\mathbb{E}_{P\\sim q(P;\\theta)}\\log p(Y|P)$ , using the techniques outlined in Section 3.3. ", "page_idx": 7}, {"type": "table", "img_path": "pMJFaBzoG3/tmp/7eb5c1aa147803312b453e4a471062efc73cdc9c38c3731eddce233f69a2b1b2.jpg", "table_caption": ["Table 1: $\\ell_{1}$ -Distance (converted by $\\log(1+x))$ and Precision $(\\%)$ of algorithms for finding mode connectivity across different network architectures. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Taking the methodology in Linderman et al. [43], we generate parameters $A,W$ , and $P$ with $n=250$ and randomly generate 1000 samples, where the noise follows a Gaussian distribution $\\mathcal{N}(0,0.01)$ . We formulate tasks of varying difficulty depending on the different proportions of known neurons [43]. Conceiving a constraint matrix $C\\in\\dot{\\mathbb{R}^{n\\times n}}$ where all elements are initialized to 1, if we ascertain that the reference neuron $i$ corresponds to the observed neuron $j$ , then set all elements to 0 in the $i$ -th row and $j$ -th column except for $C_{i,j}$ (see Equation (22) for an example). This constraint is enforced by zeroing corresponding entries before solving Equation (7). We conduct 500 iterations using the Adam optimizer [31] with an initial learning rate of 0.01. We report the marginal log-likelihood of the best model throughout the training, ranked first by Hamming Distance and then by the marginal log-likelihood (estimated with 5 repeats). As done in Section 4.1, Precision, Recall, and Hamming Distance are utilized to evaluate the permutation matrices obtained from the best model. ", "page_idx": 8}, {"type": "text", "text": "For comparison, we include: 1) Naive [43], which does not enforce that $P$ is a permutation matrix and instead normalizes each row using the softmax function; 2) Gumbel-Sinkhorn [49], introducing Gumbel noise for re-parameterization before the Sinkhorn operator; and 3) OT4P, our proposed method using the re-parameterization trick, with different temperature parameters. Each experiment is conducted five times, and the results are presented in Tables 2 and 5. We observe that Naive fails to produce any meaningful solutions, and Gumbel-Sinkhorn performs poorly in the more challenging scenario (Known $5\\bar{\\%}$ ). In contrast, OT4P consistently identifies the optimal permutation, except for OT4P $\\tau=0.7$ ), which achieves suboptimal results in the Known $5\\%$ setting. One possible reason why OT4P performs better is that the orthogonal group $\\textstyle{\\big(}{\\frac{n(n-1)}{2}}{\\big)}$ has a lower dimension than the Birkhoff polytope $((n-1)^{2})$ . This lower dimensionality makes the randomness simulated by the noise more effective in exploring latent permutations. ", "page_idx": 8}, {"type": "table", "img_path": "pMJFaBzoG3/tmp/a011b9a3eac72c7834e327c87b4d6e52db38748ebbc0fb0ffc5379453b48e680.jpg", "table_caption": ["Table 2: Marginal log-likelihood and Precision $(\\%)$ of algorithms for inferring neuron identities across different proportions of known neurons. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3 Solving permutation synchronization ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In the third experiment, we aim to explore the effectiveness of our proposed OT4P on large-scale problems. We specifically focus on the permutation synchronization problem [51, 47, 5], which tries to improve matching across multiple objects. Consider $k$ objects with $n$ points each, and let the permutation matrix $\\mathbf{\\bar{\\boldsymbol{P}}}(i,j)\\in\\mathcal{P}_{n}$ to represent the correspondence between points in objects $i$ and ", "page_idx": 8}, {"type": "image", "img_path": "pMJFaBzoG3/tmp/4492b95ce63b14910baffb1d91983e2b30137d4fa15457c97978bb0d75d36c18.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 3: F-scores $(\\%)$ for different algorithms on the WILLOW-ObjectClass dataset, where the size of permutation synchronization problem instances varies along the horizontal axis. ", "page_idx": 9}, {"type": "text", "text": "$j$ . Permutation synchronization seeks to identify the underlying permutations $P_{i}(i\\in[k])$ such that $P(i,j)=P_{i}P_{j}^{\\top}$ for all $i,j\\in[k]$ , which can be expressed as the following optimization problem: ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\{P_{i}\\in\\mathcal{P}_{n}\\}}\\sum_{i,j}^{k}\\lVert P(i,j)-P_{i}P_{j}^{\\top}\\rVert_{\\mathrm{F}}^{2}.\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "We select baselines: 1) Reg optimizes in Euclidean space with a regularization term $\\textstyle\\sum_{j}(\\sum_{j}P_{i,j}\\!-\\!1)^{2}$ that encourages each column to sum to 1. 2) OrthReg [72] optimizes over the special orthogonal group, using a regularization term $\\begin{array}{r}{\\frac{2}{3}\\operatorname{trace}(\\dot{P}^{T}(P-\\check{P}\\odot\\bar{P^{\\prime}}))}\\end{array}$ ( $\\odot$ is element-wise product) to force orthogonal matrices to converge to permutation matrices. 3) RiemanBirk [6] optimizes on Birkhoff polytope utilizing Riemannian gradient descent. 4) Sinkhorn [49] optimizes in the vicinity of the Birkhoff polytope, using the Sinkhorn operator to adjust positive matrices into approximate doubly stochastic matrices. All algorithms employ the Adam optimizer for 100 iterations, with RiemanBirk utilizing Riemannian Adam [4, 34]. The initial learning rates are tuned within the set $\\{0.1,0.01,0.001,0.0\\bar{0}01\\}$ . ", "page_idx": 9}, {"type": "text", "text": "We use the WILLOW-ObjectClass dataset [9] to generate problem instances (see Appendix F.4 for more details) and utilize the $\\boldsymbol{\\mathrm{F}}.$ -score to evaluate the alignment between the flattened permutation matrices and the ground truth. Each experiment is conducted five times, and the results are shown in Figure 3. RiemanBirk and Sinkhorn demonstrate poorer performance. A primary reason is that both methods are based on Birkhoff polytope to relax permutations, leading to unreliable local minima and preventing optimal solutions. Beneftiing from the potential advantages offered by the orthogonal group, OrthReg generally produces competitive results. However, due to the instability of its regularization term, OrthReg sometimes underperforms, which may necessitate careful adjustment of the regularization coefficient for each class. In contrast, our proposed OT4P consistently outperforms other methods and demonstrates robustness to variations in the hyperparameter $\\tau$ . ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we present a novel differentiable transformation, OT4P, designed for relaxing permutation matrices over the orthogonal group. This method is characterized by its flexibility, simplicity, and scalability. OT4P is utilized to parametrize the relaxation of permutation matrices, with advantages of not altering the original problem, not complicating the original problem, and an efficient optimization process. By deriving a gradient estimator, OT4P further provides an efficient tool for stochastic optimization over latent permutations. Extensive experiments show that OT4P achieves competitive results in optimization problems and probabilistic tasks compared to relaxation methods on the Birkhoff polytope. We believe our elementary work is a significant step toward relaxing permutations onto the orthogonal group. Please see Appendix A for further discussion, including related work, limitations, and broader impacts. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work is supported by the National Key Research and Development Program of China (No.   \n2020YFA0714103). ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Ryan Prescott Adams and Richard S Zemel. Ranking via sinkhorn propagation. arXiv preprint arXiv:1106.1925, 2011.   \n[2] Samuel Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. Git re-basin: Merging models modulo permutation symmetries. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=CQsmMYmlP5T.   \n[3] Awad H Al-Mohy and Nicholas J Higham. A new scaling and squaring algorithm for the matrix exponential. SIAM Journal on Matrix Analysis and Applications, 31(3):970\u2013989, 2010.   \n[4] Gary Becigneul and Octavian-Eugen Ganea. Riemannian adaptive optimization methods. In International Conference on Learning Representations, 2019. URL https://openreview. net/forum?id $=$ r1eiqi09K7.   \n[5] Florian Bernard, Daniel Cremers, and Johan Thunberg. Sparse quadratic optimisation over the stiefel manifold with application to permutation synchronisation. Advances in Neural Information Processing Systems, 34:25256\u201325266, 2021.   \n[6] Tolga Birdal and Umut Simsekli. Probabilistic permutation synchronization using the riemannian structure of the birkhoff polytope. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11105\u201311116, 2019.   \n[7] David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. Journal of the American statistical Association, 112(518):859\u2013877, 2017.   \n[8] Rainer E Burkard, Stefan E Karisch, and Franz Rendl. Qaplib\u2013a quadratic assignment problem library. Journal of Global optimization, 10:391\u2013403, 1997.   \n[9] Minsu Cho, Karteek Alahari, and Jean Ponce. Learning graphs to match. In Proceedings of the IEEE International Conference on Computer Vision, pages 25\u201332, 2013.   \n[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[11] Zvi Drezner. The quadratic assignment problem. Location science, pages 345\u2013363, 2015.   \n[12] Hannah Dr\u00f6ge, Zorah L\u00e4hner, Yuval Bahat, Onofre Martorell Nadal, Felix Heide, and Michael M\u00f6ller. Kissing to find a match: Efficient low-rank permutation representation. Advances in Neural Information Processing Systems, 36, 2024.   \n[13] Rahim Entezari, Hanie Sedghi, Olga Saukh, and Behnam Neyshabur. The role of permutation invariance in linear mode connectivity of neural networks. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=dNigytemkL.   \n[14] Luca Falorsi, Pim de Haan, Tim R Davidson, and Patrick Forr\u00e9. Reparameterizing distributions on lie groups. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 3244\u20133253. PMLR, 2019.   \n[15] Marcelo Fiori, Pablo Sprechmann, Joshua Vogelstein, Pablo Mus\u00e9, and Guillermo Sapiro. Robust multimodal graph matching: Sparse coding meets graph matching. Advances in neural information processing systems, 26, 2013.   \n[16] Matteo Fischetti, Michele Monaci, and Domenico Salvagnin. Three ideas for the quadratic assignment problem. Operations research, 60(4):954\u2013964, 2012.   \n[17] Fajwel Fogel, Rodolphe Jenatton, Francis Bach, and Alexandre d\u2019Aspremont. Convex relaxations for permutation problems. Advances in neural information processing systems, 26, 2013.   \n[18] Jean Gallier. Logarithms and square roots of real matrices existence, uniqueness and applications in medical imaging. arXiv preprint arXiv:0805.0245, 2011.   \n[19] Zheng Gong and Ying Sun. An energy-centric framework for category-free out-of-distribution node detection in graphs. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 908\u2013919, 2024.   \n[20] Aditya Grover, Eric Wang, Aaron Zweig, and Stefano Ermon. Stochastic optimization of sorting networks via continuous relaxations. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id $\\equiv$ H1eSS3CcKX.   \n[21] Brian C Hall. Lie groups, Lie algebras, and representations. Springer, 2013.   \n[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[23] Nicholas J Higham. Functions of matrices: theory and computation. SIAM, 2008.   \n[24] Nicholas J Higham and Lijing Lin. A schur\u2013pad\u00e9 algorithm for fractional powers of a matrix. SIAM Journal on Matrix Analysis and Applications, 32(3):1056\u20131078, 2011.   \n[25] Einar Hille. On roots and logarithms of elements of a complex banach algebra. Mathematische Annalen, 136(1):46\u201357, 1958.   \n[26] Bo Jiang, Ya-Feng Liu, and Zaiwen Wen. L_p-norm regularization algorithms for optimization over permutation matrices. SIAM Journal on Optimization, 26(4):2284\u20132313, 2016.   \n[27] Bo Jiang, Jin Tang, Chris Ding, and Bin Luo. Nonnegative orthogonal graph matching. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 31, 2017.   \n[28] Ivan Karpukhin and Andrey Savchenko. Detpp: Leveraging object detection for robust longhorizon event prediction. arXiv preprint arXiv:2408.13131, 2024.   \n[29] Ivan Karpukhin, Foma Shipilov, and Andrey Savchenko. Hotpp benchmark: Are we good at the long horizon events forecasting? arXiv preprint arXiv:2406.14341, 2024.   \n[30] Sunyoung Kim, Masakazu Kojima, and Kim-Chuan Toh. A lagrangian\u2013dnn relaxation: a fast method for computing tight lower bounds for a class of quadratic optimization problems. Mathematical Programming, 156(1):161\u2013187, 2016.   \n[31] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[32] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.   \n[33] Virginia Klema and Alan Laub. The singular value decomposition: Its computation and some applications. IEEE Transactions on automatic control, 25(2):164\u2013176, 1980.   \n[34] Max Kochurov, Rasul Karimov, and Serge Kozlukov. Geoopt: Riemannian optimization in pytorch, 2020.   \n[35] Vignesh Kothapalli. Neural collapse: A review on modelling principles and generalization. arXiv preprint arXiv:2206.04041, 2022.   \n[36] Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):83\u201397, 1955.   \n[37] Eugene L Lawler. The quadratic assignment problem. Management science, 9(4):586\u2013599, 1963. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[38] John M Lee. Introduction to Riemannian manifolds, volume 2. Springer, 2018. ", "page_idx": 12}, {"type": "text", "text": "[39] Mario Lezcano Casado. Trivializations for gradient-based optimization on manifolds. Advances in Neural Information Processing Systems, 32, 2019.   \n[40] Mario Lezcano-Casado and David Mart\u0131nez-Rubio. Cheap orthogonal constraints in neural networks: A simple parametrization of the orthogonal and unitary group. In International Conference on Machine Learning, pages 3794\u20133803. PMLR, 2019.   \n[41] Innar Liiv. Seriation and matrix reordering methods: An historical overview. Statistical Analysis and Data Mining: The ASA Data Science Journal, 3(2):70\u201391, 2010.   \n[42] Cong Han Lim and Stephen Wright. Beyond the birkhoff polytope: Convex relaxations for vector permutation problems. Advances in neural information processing systems, 27, 2014.   \n[43] Scott Linderman, Gonzalo Mena, Hal Cooper, Liam Paninski, and John Cunningham. Reparameterizing the birkhoff polytope for variational permutation inference. In International Conference on Artificial Intelligence and Statistics, pages 1618\u20131627. PMLR, 2018.   \n[44] Eliane Maria Loiola, Nair Maria Maia De Abreu, Paulo Oswaldo Boaventura-Netto, Peter Hahn, and Tania Querido. A survey for the quadratic assignment problem. European journal of operational research, 176(2):657\u2013690, 2007.   \n[45] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.   \n[46] Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.   \n[47] Eleonora Maset, Federica Arrigoni, and Andrea Fusiello. Practical and efficient multi-view matching. In Proceedings of the IEEE International Conference on Computer Vision, pages 4568\u20134576, 2017.   \n[48] Estelle Massart and Vinayak Abrol. Coordinate descent on the orthogonal group for recurrent neural network training. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 7744\u20137751, 2022.   \n[49] Gonzalo Mena, David Belanger, Scott Linderman, and Jasper Snoek. Learning latent permutations with gumbel-sinkhorn networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id $\\equiv$ Byt3oJ-0W.   \n[50] Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte carlo gradient estimation in machine learning. Journal of Machine Learning Research, 21(132):1\u201362, 2020.   \n[51] Deepti Pachauri, Risi Kondor, and Vikas Singh. Solving the multi-way matching problem by permutation synchronization. Advances in neural information processing systems, 26, 2013.   \n[52] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.   \n[53] Gao Peifeng, Qianqian Xu, Peisong Wen, Zhiyong Yang, Huiyang Shao, and Qingming Huang. Feature directions matter: Long-tailed learning via rotated balanced representation. In International Conference on Machine Learning, pages 27542\u201327563. PMLR, 2023.   \n[54] Fidel A Guerrero Pe\u00f1a, Heitor Rapela Medeiros, Thomas Dubail, Masih Aminbeidokhti, Eric Granger, and Marco Pedersoli. Re-basin via implicit sinkhorn differentiation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20237\u201320246, 2023.   \n[55] Janez Povh. Semidefinite approximations for quadratic programs over orthogonal matrices. Journal of Global Optimization, 48(3):447\u2013463, 2010.   \n[56] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In International conference on machine learning, pages 1278\u20131286. PMLR, 2014.   \n[57] Simone Rossi, Ankit Singh, and Thomas Hannagan. On permutation symmetries in bayesian neural network posteriors: a variational perspective. Advances in Neural Information Processing Systems, 36, 2024.   \n[58] Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747, 2016.   \n[59] Rodrigo Santa Cruz, Basura Fernando, Anoop Cherian, and Stephen Gould. Deeppermnet: Visual permutation learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3949\u20133957, 2017.   \n[60] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.   \n[61] Richard Sinkhorn. A relationship between arbitrary positive matrices and doubly stochastic matrices. The annals of mathematical statistics, 35(2):876\u2013879, 1964.   \n[62] Ying Sun, Hengshu Zhu, Chuan Qin, Fuzhen Zhuang, Qing He, and Hui Xiong. Discerning decision-making process of deep neural networks with hierarchical voting transformation. Advances in Neural Information Processing Systems, 34:17221\u201317234, 2021.   \n[63] Ying Sun, Fuzhen Zhuang, Hengshu Zhu, Qing He, and Hui Xiong. Cost-effective and interpretable job skill recommendation with deep reinforcement learning. In Proceedings of the Web Conference 2021, pages 3827\u20133838, 2021.   \n[64] Ying Sun, Hengshu Zhu, Lu Wang, Le Zhang, and Hui Xiong. Large-scale online job search behaviors reveal labor market shifts amid covid-19. Nature Cities, 1(2):150\u2013163, 2024.   \n[65] David M Tate and Alice E Smith. A genetic approach to the quadratic assignment problem. Computers & Operations Research, 22(1):73\u201383, 1995.   \n[66] Lav R Varshney, Beth L Chen, Eric Paniagua, David H Hall, and Dmitri B Chklovskii. Structural properties of the caenorhabditis elegans neuronal network. PLoS computational biology, 7(2): e1001066, 2011.   \n[67] Chao Wang, Hengshu Zhu, Chen Zhu, Chuan Qin, and Hui Xiong. Setrank: A setwise bayesian approach for collaborative ranking from implicit feedback. In Proceedings of the aaai conference on artificial intelligence, volume 34, pages 6127\u20136136, 2020.   \n[68] Chao Wang, Hengshu Zhu, Chen Zhu, Chuan Qin, Enhong Chen, and Hui Xiong. Setrank: A setwise bayesian approach for collaborative ranking in recommender system. ACM Transactions on Information Systems, 42(2):1\u201332, 2023.   \n[69] Runzhong Wang, Junchi Yan, and Xiaokang Yang. Neural graph matching network: Learning lawler\u2019s quadratic assignment problem with extension to hypergraph and multiple-graph matching. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(9):5261\u20135279, 2021.   \n[70] Tianxin Wang, Fuzhen Zhuang, Ying Sun, Xiangliang Zhang, Leyu Lin, Feng Xia, Lei He, and Qing He. Adaptively sharing multi-levels of distributed representations in multi-task learning. Information Sciences, 591:226\u2013234, 2022.   \n[71] Junchi Yan, Xu-Cheng Yin, Weiyao Lin, Cheng Deng, Hongyuan Zha, and Xiaokang Yang. A short survey of recent advances in graph matching. In Proceedings of the 2016 ACM on international conference on multimedia retrieval, pages 167\u2013174, 2016.   \n[72] Michael M Zavlanos and George J Pappas. A dynamical systems approach to weighted graph matching. Automatica, 44(11):2817\u20132824, 2008.   \n[73] He Zhang, Ying Sun, Weiyu Guo, Yafei Liu, Haonan Lu, Xiaodong Lin, and Hui Xiong. Interactive interior design recommendation via coarse-to-fine multimodal reinforcement learning. In Proceedings of the 31st ACM International Conference on Multimedia, pages 6472\u20136480, 2023.   \n[74] Yuting Zhang, Ying Sun, Fuzhen Zhuang, Yongchun Zhu, Zhulin An, and Yongjun Xu. Triple dual learning for opinion-based explainable recommendation. ACM Transactions on Information Systems, 42(3):1\u201327, 2023.   \n[75] Hengshu Zhu, Hui Xiong, Yong Ge, and Enhong Chen. Discovery of ranking fraud for mobile apps. IEEE Transactions on knowledge and data engineering, 27(1):74\u201387, 2014. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Additional discussions ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "A.1 Related work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Optimization over permutations is commonly encountered in problems involving bijections between two sets of equal size [17, 42, 15, 12]. From a theoretical perspective, a well-known special case is the quadratic assignment problem, which has been drawing researchers\u2019 attention since its first formulation [37, 8, 44]. In practical terms, optimization over permutations is also arises extensively in various machine learning tasks, including ranking [20, 75, 67, 68], matching [2], tracking [43], recommendation [63, 74, 73], etc. Since our interest lies in addressing generic problems defined on permutation matrices, we will not discuss theoretical approaches for specific issues [11], such as exact methods [16], lower bound methods [30], and vertex-based methods [65]. ", "page_idx": 15}, {"type": "text", "text": "Previous studies have proposed relaxing the permutation matrices to continuous spaces, including the Birkhoff polytope [17, 15, 42] and orthogonal group [72, 55, 27]. The Birkhoff polytope is the convex hull of all permutation matrices, while the orthogonal group serves as a natural embedding of permutation matrices in a differentiable manifold. Such relaxation techniques have proven to be very powerful, with successful applications to various problems such as seriation [1, 17, 41] and graph matching [15, 69, 71]. Most of the methods mentioned above rely on the explicit penalty term and are challenging to extend to probabilistic scenarios. ", "page_idx": 15}, {"type": "text", "text": "Recently, relaxation methods involving the Birkhoff polytope have made significant advancements, particularly in penalty-free optimization and probabilistic inference. For instance, Linderman et al. [43] propose a rounding transformation regulated by a temperature parameter, which rounds matrices towards the vertices of the Birkhoff polytope, i.e., permutation matrices. Similarly, Mena et al. [49] utilize the Sinkhorn operator [61] to transform matrices into the Birkhoff polytope, bringing them closer to permutation matrices under temperature control. Additionally, Grover et al. [20] suggest mapping vectors to unimodal row stochastic matrices, a subset of the Birkhoff polytope that removes the requirement of every column sum being equal to 1. These works not only avoid introducing penalty terms but also provide probabilistic inference. However, providing equally effective relaxation methods within orthogonal groups remains an unexplored area. As mentioned in the main text, relaxation onto the orthogonal group possesses unique potential advantages. Therefore, our elementary work aims to address this gap. ", "page_idx": 15}, {"type": "text", "text": "A.2 Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The first concerns the computational efficiency of OT4P when dealing with very large matrices $(n\\,>\\,1000)$ , as the cost of eigendecomposition becomes prohibitive. This could benefit from efficient implementations of eigendecomposition on GPUs. The second limitation involves the noise distribution used for re-parameterization in OT4P, which may not equally capture the latent permutation matrices. This can be improved by more carefully designing the noise distribution under the characteristics of the orthogonal group. The third challenge relates to the boundary issues in representing permutation matrices, hindering OT4P as a building block of deep neural networks. Indeed, such integration assumes all permutation matrices lie within $\\boldsymbol{\\mathcal{U}}$ . We believe that an ideal (analytical) solution can be devised to relocate all permutation matrices back into $\\boldsymbol{\\mathcal{U}}$ , given that they constitute a finite discrete set within the orthogonal group of order $n$ . ", "page_idx": 15}, {"type": "text", "text": "A.3 Broader impacts ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "This work presents a novel differentiable transformation for relaxing permutation matrices onto the orthogonal group, which enables gradient-based (stochastic) optimization of problems involving permutation matrices. Given the theoretical nature of our work, we have not identified any direct ethical concerns or negative societal impacts related to our research. Our study may have the broader impacts for a variety of areas of machine learning, such as deep learning [62], data mining [64]. Similar impacts were observed in multi-task learning [70] and graph learning [19]. ", "page_idx": 15}, {"type": "text", "text": "B Riemannian Geometry and Lie Group ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "This section briefly summarizes the key concepts of Riemannian geometry and the Lie group theory involved. For a comprehensive understanding, we recommend the standard textbooks Introduction to ", "page_idx": 15}, {"type": "text", "text": "Riemannian manifolds [38] and Lie groups, Lie algebras, and representations [21]. Additionally, the analysis of matrix functions can be found in Functions of matrices: theory and computation [23]. ", "page_idx": 16}, {"type": "text", "text": "An $n$ -dimensional manifold $\\mathcal{M}$ is a topological space that can be locally approximated by the Euclidean space $\\mathbb{R}^{n}$ . At each point $x\\in{\\mathcal{M}}$ , there exists a tangent space $T_{x}\\mathcal{M}$ , an $n$ -dimensional vector space, serving as a first-order local approximation of $\\mathcal{M}$ around $x$ . The Riemannian metric is a collection $m:=\\{m_{x}\\mid x\\in\\mathcal{M}\\}$ of inner products $m_{x}(\\cdot,\\cdot):T_{x}\\mathcal{M}\\times T_{x}\\mathcal{M}\\to\\mathbb{R}$ . It induces a norm $\\|\\cdot\\|_{x}\\,:\\,T_{x}{\\mathcal{M}}\\,\\rightarrow\\,\\mathbb{R}$ defined by $\\|y\\|_{x}:=\\sqrt{m_{x}(y,y)}$ . The length $L(\\gamma)$ of a smooth curve $\\gamma:[a,b]\\rightarrow{\\mathcal{M}}$ is defined as $\\begin{array}{r}{L(\\gamma):=\\int_{a}^{b}\\lvert\\rvert\\gamma^{\\prime}(t)\\rvert\\rvert_{\\gamma(t)}\\,\\mathrm{d}t}\\end{array}$ . The distance $d(x,y)$ is set as the infimum of the lengths of all smooth curves between $x$ and $y$ in $\\mathcal{M}$ . A geodesic is a smooth curve $\\gamma:[a,b]\\rightarrow{\\mathcal{M}}$ that the tangent vector $\\gamma^{\\prime}(t)$ is parallel transported along the curve $\\gamma$ w.r.t. the Levi-Civita connection, i.e., $\\nabla_{\\gamma^{\\prime}(t)}\\bar{\\gamma}^{\\prime}(t)=0$ for all $t\\in[a,b]$ . With the basic concept of manifolds established, we can present the definitions of isomorphism, diffeomorphism, and isometry. ", "page_idx": 16}, {"type": "text", "text": "Definition 1 (Vector space isomorphism). The vector spaces $\\mathcal{X}$ and $\\boldsymbol{\\wp}$ are called to be isomorphic if there exists a bijection $\\alpha:\\mathcal{X}\\rightarrow\\mathcal{Y}$ that preserves addition and scalar multiplication. ", "page_idx": 16}, {"type": "text", "text": "Definition 2 (Diffeomorphism). Given two differentiable manifolds $\\mathcal{M}$ and $\\mathcal{N}$ , a differentiable map $\\alpha:\\mathcal{M}\\to\\mathcal{N}$ is a diffeomorphism if it is a bijection and its inverse $\\alpha^{-1}$ is differentiable as well. ", "page_idx": 16}, {"type": "text", "text": "Definition 3 (Isometry). Given two metric spaces $(\\mathcal{X},m_{\\mathcal{X}})$ and $(y,m_{\\mathcal{Y}})$ , a map $\\alpha:\\mathcal{X}\\rightarrow\\mathcal{Y}$ is called an isometry if $m_{\\mathcal{X}}(x_{1},x_{2})=m_{\\mathcal{Y}}(\\alpha(x_{1}),\\alpha(x_{2}))$ for all points $x_{1},x_{2}\\in\\mathcal{X}$ . ", "page_idx": 16}, {"type": "text", "text": "A Lie group $G$ is a differentiable manifold equipped with differentiable group multiplication and inverse operations. The tangent space at the identity $e$ is known as the Lie algebra of $G$ , denoted as ${\\mathfrak{g}}:=T_{e}G$ . For all $g\\in G$ , there exist diffeomorphisms given by the left translation $L_{g}(x):=$ $g x$ $\\forall x\\in G)$ and the right translation $R_{g}(x):=x\\bar{g^{\\prime}}(\\forall x\\in\\bar{G})$ . The left and right translations lead to a vector space isomorphism that relates the tangent space to the Lie algebra. For left translation $L_{g}$ , the mapping $(\\mathrm{d}L_{g})_{e}^{-}:=\\mathfrak{g}\\to T_{g}G$ maps elements of the Lie algebra into the tangent space $T_{g}G$ , with its inverse mapping $(\\mathrm{d}L_{g^{-1}})_{g}:=T_{g}G\\to\\mathfrak{g}$ . Similarly, for right translation $R_{g}$ , there is mapping $(\\mathrm{d}R_{g})_{e}:={\\mathfrak{g}}\\to T_{g}G$ and its inverse $\\bar{(\\mathrm{d}R_{g^{-1}})}_{g}:=T_{g}G\\to\\mathfrak{g}$ . A Riemannian metric $m$ on the Lie group $G$ is said to be left-invariant (right-invariant) if it renders each left (right) translation an isometry. When adopting a left-invariant (right-invariant) metric on the Lie group $G$ , we can associate neighborhoods of the identity $e$ with neighborhoods of any point $g\\in G$ using left (right) translation, and vice versa. A metric on the Lie group that is both left and right invariant is termed a $b i$ -invariant metric. It is worth noting that compact Lie groups always possess the bi-invariant metric. ", "page_idx": 16}, {"type": "text", "text": "The Lie group we are interested in is the orthogonal group $\\mathrm{O}(n)$ , consisting of all $n\\times n$ orthogonal matrices $O$ satisfying $O^{\\top}O=O O^{\\top}=I$ . We equip $\\mathrm{O}(n)$ with the canonical metric, which is a bi-invariant metric inherited from $\\mathbb{R}^{n\\times n}$ and defined as $\\langle A,B\\rangle_{\\mathrm{F}}:=\\operatorname{trace}(A^{\\top}B)$ , where $\\langle\\cdot,\\cdot\\rangle_{\\mathrm{F}}$ is the Frobenius inner product and $\\operatorname{trace}(\\cdot)$ is the trace of a matrix. The orthogonal group $\\mathrm{O}(n)$ is divided into two connected components depending on the value of the determinant, $+1$ or $-1$ . The connected component with determinant $+1$ forms a subgroup known as the special orthogonal group $\\operatorname{SO}(n):=\\{O\\stackrel{\\cdot}{\\in}\\mathbb{R}^{n\\times n}\\mid O^{\\top}O=I,\\det O=+1\\}$ . The Lie algebra ${\\mathfrak{s o}}(n)$ of the Lie group SO(n) comprises $n\\times n$ skew-symmetric matrices, expressed as ${\\mathfrak{s o}}(n):=\\{A\\in\\mathbb{R}^{n\\times n}\\mid A^{\\top}=-A\\}$ . It icso innoctiediwnogr twhiyt ht htahte ${\\mathfrak{s o}}(n)$ xi se xap ovencetnotir asl pian cteh ew citohn tdeixtm eofn stihoe n $\\frac{n(n-1)}{2}$ .i e Tghreo uLpi,e  eexxapctolny enmtaiapls $\\mathrm{{expm}(\\cdot)}$ s, in ${\\mathfrak{s o}}(n)$ to elements in $\\mathrm{SO}(n)$ , defined by ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname{expm}(A):=I+\\sum_{k=1}^{\\infty}{\\frac{A^{k}}{k!}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The series in the above equation converges for all matrices $A$ . ", "page_idx": 16}, {"type": "text", "text": "In general, the matrix logarithm is expected to be the inverse function of the matrix exponential, meaning the logarithm of $A$ is the solution $B$ to the matrix equation $\\mathrm{expm}(B)\\,=\\,A$ . Since the complex logarithm is a multi-valued function, there may be an infinite number of matrices $B$ that satisfy $\\mathrm{expm}(B)=A$ . If $A$ has no eigenvalues on $\\mathbb{R}_{0}^{-}:=\\{x\\in\\mathbb{R}\\mid x\\leq0\\}$ , then there exists a unique logarithm called the principal logarithm, denoted as $\\operatorname{logm}(A)$ . All eigenvalues $\\lambda$ of $\\mathrm{logm}(A)$ satisfy $-\\pi<\\operatorname{Im}\\lambda<\\pi$ . Assuming convergence of the series, the matrix logarithm $\\log\\!\\mathrm{m}(\\cdot)$ can be ", "page_idx": 16}, {"type": "text", "text": "defined by Taylor series expansion as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname{logm}(A):=\\sum_{k=1}^{\\infty}(-1)^{k+1}{\\frac{(A-I)^{k}}{k}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The series in the above equation converges whenever $\\|A-I\\|_{\\mathrm{F}}<1$ , where $\\|A\\|_{\\mathrm{F}}\\,:=\\,\\sqrt{\\langle A,A\\rangle_{\\mathrm{F}}}$ represents the Frobenius norm induced by the Frobenius inner product. ", "page_idx": 17}, {"type": "text", "text": "Define the $\\mathcal{V}:=\\{A\\in\\mathbb{R}^{n\\times n}\\mid\\operatorname{Im}\\lambda_{k}(A)\\in(-\\pi,\\pi),\\,\\forall k\\}$ as the set of all matrices $A\\in\\mathbb{R}^{n\\times n}$ for which all eigenvalues $\\lambda_{k}$ satisfies $-\\pi<\\operatorname{Im}\\lambda_{k}<\\pi$ . Let $\\mathcal{W}:=\\{A\\in\\mathbb{R}^{n\\times n}\\mid\\lambda_{k}(A)\\not\\in\\mathbb{R}_{0}^{-},\\,\\forall k\\}$ denote the set of all matrices $A\\in\\mathbb{R}^{n\\times n}$ without non-positive real eigenvalues. Then, we can verify that (prove in Appendix G.1): ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bullet\\;\\log\\mathrm{m}(\\exp(A))=A,\\,\\forall A\\in\\mathcal{V};}\\\\ &{\\bullet\\;\\exp\\mathrm{m}(\\log\\mathrm{m}(A))=A,\\,\\forall A\\in\\mathcal{W}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This indicates that the matrix logarithm is a locally inverse function of the matrix exponential. ", "page_idx": 17}, {"type": "text", "text": "C Boundary issues in representation of permutation matrix ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we provide an in-depth analysis of the potential adverse effects on optimization stemming from the direct application of mapping $\\phi(\\cdot)$ w.r.t. Equation (6) . Subsequently, we show that left-multiplying the result of Equation (6) by an orthogonal matrix, as described in Step I, effectively alleviates this problem. ", "page_idx": 17}, {"type": "text", "text": "Since a permutation matrix must be an orthogonal matrix, its eigenvalues $\\lambda_{k}$ lie on the unit circle in the complex plane, i.e., $|\\lambda_{k}|=1$ . Additionally, any permutation can be expressed as the product of cycles with disjoint supports. For a cycle of length $m$ , its corresponding $m\\times m$ submatrix satisfies $P^{m}=I$ , which implies that the eigenvalues of $P$ admit the following form: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\lambda_{k}=\\mathrm{e}^{2\\pi i\\frac{k}{m}},\\quad k=0,1,\\dots,m-1.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In Figure 4, we visualize the eigenvalues corresponding to cycles of lengths 2, 3, 4, and 5. It is clear that cycles of even length consistently possess an eigenvalue of $-1$ . Notably, the eigenvalues of a permutation matrix are composed of those from the submatrices corresponding to its contained cycles. Therefore, many permutation matrices have $-1$ as one of their eigenvalues. ", "page_idx": 17}, {"type": "text", "text": "As elucidated Lezcano-Casado and Mart\u0131nezRubio [40], the representation in $\\mathcal{U}:=\\{A\\in\\mathcal{$ $\\mathbb{R}^{\\frac{n(n-1)}{2}}\\;\\left|\\;\\operatorname{Im}\\lambda_{k}(A-A^{\\top})\\in(-\\pi,\\pi),\\forall k\\right\\}$ of permutation matrices with an eigenvalue of $-1$ precisely lies on the boundary of $\\boldsymbol{\\mathcal{U}}$ . For problems where these permutation matrices serve as the optimal solution, it may cause the optimization path to deviate from $\\boldsymbol{\\mathcal{U}}$ . A straightforward method is to left-multiply the result of Equation (6) by a random orthogonal matrix $B\\in\\mathrm{SO}(n)$ , thereby shifting the boundary of $\\boldsymbol{\\mathcal{U}}$ to other eigenvalues. Theoretically, this left translation $\\bar{L_{B}}(O):=B O$ $\\langle\\forall O\\in\\operatorname{SO}(n)$ ) creates a diffeomorphism on $\\mathrm{SO}(n)$ that transforms the representation of the permutation matrix $P$ in $\\boldsymbol{\\mathcal{U}}$ from $\\mathrm{logm}(P)$ to $\\operatorname{logm}(B^{\\top}P)$ . ", "page_idx": 17}, {"type": "image", "img_path": "pMJFaBzoG3/tmp/d8a7c52ede89798e26acc847401cb425e368652f43e17dadd8e005560549cd9f.jpg", "img_caption": ["Figure 4: Eigenvalues corresponding to cycles of different lengths, where eigenvalues from the same cycle are connected to illustrate repeated values at $-1$ . "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "To validate the efficacy of the aforementioned approach, we conducted a simple empirical study. Given a random orthogonal matrix $B$ with a determinant of 1, we randomly generate 1000 permutation matrices $P$ , where odd permutations are projected to $\\mathrm{SO}(n)$ as done in Appendix D. We then estimate the probability of the eigenvalue $-1$ occurring in $P$ and $B^{\\top}{\\boldsymbol{P}}$ , respectively. We examine permutation matrices ranging in dimension $n$ from 3 to 50. Each experiment is replicated five times and the results are presented in Table 3. The findings show that the left translation $L_{B}$ effectively relocates the majority of permutation matrices\u2019 representations into the interior of $\\boldsymbol{\\mathcal{U}}$ . ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "table", "img_path": "pMJFaBzoG3/tmp/712778d0ede3e052a22d7e58e6fec8465dbfcbeb7a89c04bb2aa89bf55d4f2cf.jpg", "table_caption": ["Table 3: Probability $(\\%)$ of eigenvalue $-1$ occurring in matrices. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Remark C.1. A high-level idea involves dynamically adjusting $B$ to ensure that the permutation matrix near the current iteration point consistently remains within the interior of $\\boldsymbol{\\mathcal{U}}$ . This is a special case of dynamic trivialization proposed in Lezcano Casado [39]. ", "page_idx": 18}, {"type": "text", "text": "D Theoretical details of the odd permutation ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section, we provide the details of handling odd permutations and focus on elucidating the underlying theory. ", "page_idx": 18}, {"type": "text", "text": "To deal with the case where the permutation matrix $P$ corresponds to an odd permutation, our core idea is to find an agent, $\\widehat{P}$ , of $P$ within $\\mathrm{SO}(n)$ . In this way, we may move the orthogonal matrix $O\\in\\mathrm{SO}(n)$ to the vicinity of $\\widehat{P}$ , and then restore the result to the neighborhood of $P$ . This idea is formalized by the following m ethod: ", "page_idx": 18}, {"type": "text", "text": ". We seek an agent of $P$ within $\\mathrm{SO}(n)$ , which is determined by solving the optimization problem: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\underset{\\widehat{P}\\in\\mathrm{SO}(n)}{\\arg\\operatorname*{min}}\\|\\widehat{P}-P\\|_{\\mathrm{F}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The analytical solution to Equation (20) is given by $\\begin{array}{r l r}{\\widehat{P}}&{{}=}&{P D}\\end{array}$ , where $\\textit{D}=$ $\\mathrm{diag}(\\{1,\\bar{.}..,1,-1\\})$ is the identity matrix with the last column multiplied by $-1$ . ", "page_idx": 18}, {"type": "text", "text": "2. Given that $\\widehat{P}\\,\\in\\,\\mathrm{SO}(n)$ , we can, by substituting $P$ with $\\widehat{P}$ in Equation (10), move the orthogonal matrix $O\\in\\mathrm{SO}(n)$ to the vicinity of $\\widehat{P}$ , yielding $\\widehat{O}$ . ", "page_idx": 18}, {"type": "text", "text": "We now derive the analytical solution to Equation (20). By the definition of the Frobenius norm, we can express: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\widehat{P}-P\\|_{\\mathrm{F}}^{2}=\\operatorname{trace}((\\widehat{P}-P)^{\\top}(\\widehat{P}-P))}\\\\ &{\\qquad\\qquad=\\operatorname{trace}(\\widehat{P}^{\\top}\\widehat{P})+\\operatorname{trace}(P^{\\top}P)-2\\operatorname{trace}(\\widehat{P}^{\\top}P)}\\\\ &{\\qquad\\qquad=2n-2\\operatorname{trace}(\\widehat{P}^{\\top}P)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The last equation uses the fact that both $\\widehat{P}$ and $P$ are orthogonal matrices, i.e., $\\mathrm{trace}(\\widehat{P}^{\\intercal}\\widehat{P})=$ $\\operatorname{trace}(P^{\\top}P)=n$ . Considering the singular value decomposition (SVD) [33], we have $P=U\\Sigma V^{\\top}$ , where $U$ and $V$ are orthogonal matrices, and $\\Sigma$ is a diagonal matrix with non-negative real singular values on the diagonal. Since the orthogonal matrix singular value is 1, there exists $\\Sigma=I$ . By leveraging the cyclic property of the trace, we have: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{trace}(\\widehat{P}^{\\top}P)=\\mathrm{trace}(\\widehat{P}^{\\top}U\\Sigma V^{\\top})}\\\\ &{\\qquad\\qquad\\qquad=\\mathrm{trace}(V^{\\top}\\widehat{P}^{\\top}U)=\\mathrm{trace}(Z),}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $Z\\,:=\\,V^{\\top}\\widehat{P}^{\\top}U$ is an orthogonal matrix with $\\operatorname*{let}Z\\;=\\;-1$ . Hence, all elements $z_{i,j}$ of $Z$ satisfy $|z_{i,j}|\\,\\leq\\,1$ and $Z$ must have an odd number of $-1$ eigenvalues. Notice that the trace of a matrix is the sum of its elements on the main diagonal, Equation (21) is maximized when ", "page_idx": 18}, {"type": "text", "text": "$Z=\\mathrm{diag}(\\{1,1,\\dots,1,-1\\})$ . Thus, the analytical solution to Equation (20) is $\\widehat{P}=(D P^{\\top})^{\\top}=P D$ , where $D=\\mathrm{diag}(\\{1,\\dots,1,-1\\})$ is the identity matrix with the last column  multiplied by $-1$ . ", "page_idx": 19}, {"type": "text", "text": "Since $D$ is an orthogonal matrix, right-multiplying by $D$ becomes a right translation $R_{D}(O):=$ $O D$ $\\forall O\\,\\in\\,\\mathrm{O}(n)]$ ). Notably, $\\mathrm{O}(n)$ is equipped with a bi-invariant metric $\\langle\\cdot,\\cdot\\rangle_{\\mathrm{F}}$ , which implies that $R_{D}$ is an isometry, linking the the neighborhoods of $P$ and its agent $\\widehat{P}$ . Thus, we have $\\langle A D,B D\\rangle_{\\mathrm{F}}=\\langle A,B\\rangle_{\\mathrm{F}}$ for any $A,B\\in\\mathrm{O}(n)$ . In other words, when $\\widehat{O}$ is moved close enough to $\\widehat{P}$ , $\\widetilde O$ will also be sufficiently close to $P$ . ", "page_idx": 19}, {"type": "text", "text": "E More discussion on Step II ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Visualization. In Figure 5, we visualize the results of Equation (11) as the temperature parameter $\\tau$ varies. The first row corresponds to the even permutation, while the second row corresponds to the odd permutation. The leftmost image $\\tau=1.0)$ ) represents the original orthogonal matrices $O$ from Step I, and the rightmost image $\\tau=0.0)$ ) is the permutation matrix $P$ that is closest to $O$ . The temperature parameter $\\tau$ controls how closely the resulting orthogonal matrices $\\widetilde O$ , obtained in Step II, approach $P$ . As $\\tau\\rightarrow0$ , the resulting orthogonal matricesO increasingly converge to $P$ . ", "page_idx": 19}, {"type": "image", "img_path": "pMJFaBzoG3/tmp/b1bb1447f0251ddaa4983e59a8edb506d336c08062f1a09ba0d8a8399d32840d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 5: Visualization of the results of Equation (11) as the parameter $\\tau$ varies. At $\\tau=1.0$ , the matrices are original orthogonal matrices; at $\\tau=0.0$ , they are the permutation matrices closest to original orthogonal matrices. ", "page_idx": 19}, {"type": "text", "text": "Rounding to permutation matrix. Generally, the Hungarian algorithm requires a cost matrix without negative values, which is unsuitable for orthogonal matrices that may contain negative elements. To implement the rounding w.r.t. Equation (7) from the orthogonal matrix $O$ , we first eliminate negative elements by subtracting the minimum element found within $O$ , i.e., $O-\\operatorname*{min}(O)$ . This approach is justified by the fact that ar $;\\mathrm{max}_{P}\\langle P,O\\rangle_{\\mathrm{F}}=\\arg\\operatorname*{max}_{P}\\langle P,O-\\mathrm{min}(O)\\rangle_{\\mathrm{F}}$ . Subsequent to this adjustment, we employ the Hungarian algorithm, available in existing libraries, to round $O-\\operatorname*{min}(O)$ to the closest permutation matrix. ", "page_idx": 19}, {"type": "text", "text": "Computational costs. The primary computational costs of the proposed OT4P arise from solving the linear assignment problem and performing eigendecomposition, both of which typically scale with ${\\mathcal{O}}(n^{3})$ . Numerous efforts have been made to accelerate these computations through parallel implementations on GPUs. We have employed existing implementations, specifically torch-linearassignment [28, 29] and torch.linalg.eig [52]. ", "page_idx": 19}, {"type": "text", "text": "F Experimental details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "F.1 Reproducibility and compute resources ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "To enhance the reproducibility, we provide a comprehensive overview of the experiments in Appendices F.2 to F.4, including but not limited to data generation, hyperparameters, evaluation procedures. For all experiments, we repeat the results using five different random seeds: 2021, 2022, 2023, 2024, and 2025. The core code has been released at https://github.com/YamingGuo98/OT4P. ", "page_idx": 20}, {"type": "text", "text": "We conducted the experiments (Appendices F.2 and F.3) using a single NVIDIA A800, where the runtime environment was Python ${\\bf10}$ , $\\mathbf{CUDA=11.7}$ , and PyTorch $=\\!2.01$ . The experiments in Section 4.1 took approximately 15 hours, while those in Section 4.2 consumed another 15 hours. This estimate does not include the time spent on hyperparameter search or other experiments conducted during the research process. ", "page_idx": 20}, {"type": "text", "text": "F.2 Finding mode connectivity ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Steup. We investigate various network architectures for Equation (17), including 5-layer MLP [54], VGG11 [60], and ResNet18 [22] For MLP5, both input and output are set to 1, with each of the four hidden layers having a dimension of 10, and the hyperbolic tangent serves as the activation function. The model weights of MLP5 are initialized randomly, while VGG11 and ResNet18 are derived from the official pre-trained models in PyTorch [52]. We minimize the loss corresponding to Equation (17) using AdamW [45] with an initial learning rate of 0.1 and a maximum of 500 iterations. Following Pe\u00f1a et al. [54], we adopt an early stopping strategy upon finding the optimal permutation matrices. ", "page_idx": 20}, {"type": "text", "text": "Baseline. We take Weight Matching [2] and Sinkhorn [54] as baselines. Weight Matching is a method specifically designed for addressing Equation (17), which goes through each layer and greedily selects its best permutation matrix $P_{i}$ . We limit the maximum number of traversal rounds to 10. Sinkhorn relaxes permutation matrices to the vicinity of the Birkhoff polytope utilizing the Sinkhorn operator [61, 49], solving Equation (17) in a differentiable fashion. The Sinkhorn operator undergoes 20 iterations, with the temperature parameter set to 0.5. ", "page_idx": 20}, {"type": "text", "text": "Evaluation. During the evaluation phase, we round the matrices obtained from Sinkhorn and OT4P to permutation matrices using Equation (7). We employ the $\\ell_{1}$ -Distance, $\\lVert{\\boldsymbol{\\theta}}_{A}-\\pi({\\boldsymbol{\\theta}}_{B})\\rVert_{1}$ , to measure the difference in weights with the target network. In addition, we flatten the permutation matrices and evaluate their alignment with the ground truth using Precision, Recall, and Hamming Distance. ", "page_idx": 20}, {"type": "table", "img_path": "pMJFaBzoG3/tmp/26b46cbcb1c9d3e3850ca5e6fcae9bfe7e23b750d83f06ef47abe13916247ce6.jpg", "table_caption": ["Table 4: Recall $(\\%)$ and Hamming Distance of algorithms for finding mode connectivity across different network architectures. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "F.3 Inferring neuron identities ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Setup. Following the methodology in Linderman et al. [43], we generate parameters $A,W$ , and $P$ with $n=250$ . Specifically, we randomly generate a binary upper triangular matrix $A\\in\\{0,1\\}^{n\\times n}$ and its symmetric version $A=A+A^{\\top}$ as the adjacency matrix. Parameter $W$ is sampled from a Gaussian distribution $\\mathcal{N}(0,1)$ according to the sparse sparsity defined by $A$ . The permutation matrix $P$ is randomly sampled from a uniform distribution. We generate 1000 samples according to $Y_{t}=P(A\\odot W)P^{\\top}Y_{t-1}\\,\\mathbf{\\dot{+}}\\,\\epsilon$ , where the noise $\\epsilon$ follows a Gaussian distribution $\\bar{\\mathcal{N}}(0,0.01)$ . ", "page_idx": 20}, {"type": "text", "text": "We formulate tasks of varying difficulty depending on the different proportions of known neurons [43]. Conceiving a constraint matrix $C\\in\\mathbb{R}^{n\\times n}$ where all elements are initialized to 1. If we ascertain that the reference neuron $i$ corresponds to the observed neuron $j$ , then set all elements to 0 in the $i$ -th row and $j$ -th except for $C_{i,j}$ . Equation (22) provides a simple example, where we know that the observed neuron 1 (or 3) corresponds to the reference neuron 2 (or 5). ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left(\\begin{array}{l l l l l}{1}&{1}&{1}&{1}&{1}\\\\ {1}&{1}&{1}&{1}&{1}\\\\ {1}&{1}&{1}&{1}&{1}\\\\ {1}&{1}&{1}&{1}&{1}\\\\ {1}&{1}&{1}&{1}&{1}\\end{array}\\right)\\xrightarrow{\\mathrm{know}\\ (1,2)}\\ \\left(\\begin{array}{l l l l l}{0}&{1}&{0}&{0}&{0}\\\\ {1}&{0}&{1}&{1}&{1}\\\\ {1}&{0}&{1}&{1}&{1}\\\\ {1}&{0}&{1}&{1}&{1}\\\\ {1}&{0}&{1}&{1}&{1}\\end{array}\\right)\\xrightarrow{\\mathrm{know}\\ (3,5)}\\ \\left(\\begin{array}{l l l l l}{0}&{1}&{0}&{0}&{0}\\\\ {1}&{0}&{1}&{1}&{0}\\\\ {0}&{0}&{0}&{0}&{1}\\\\ {1}&{0}&{1}&{1}&{0}\\\\ {1}&{0}&{1}&{1}&{0}\\end{array}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This modeling decision significantly reduces the number of latent permutations that need to be inferred. The constraint matrix $C$ is enforced by zeroing corresponding entries, where for Naive it is before row normalization, for Gumbel-Sinkhorn it is before the Sinkhorn operator, and for OT4P it is before solving Equation (7). ", "page_idx": 21}, {"type": "text", "text": "Baseline. We compare Naive [43] and Gumbel-Sinkhorn [49]. Naive does not enforce $P$ to be a permutation matrix but instead normalizes each row using the softmax function. Gumbel-Sinkhorn, an extension of the Gumbel-Softmax [26, 46] method for permutations, introduces Gumbel noise for re-parameterization before the Sinkhorn operator. Both Naive and our OT4P use the stand Gaussian noise for re-parameterization. All methods estimate the gradient of the marginal log-likelihood $\\mathbb{E}_{P\\sim q(P;\\theta)}\\log p(Y|P)$ with 5 repeats. ", "page_idx": 21}, {"type": "text", "text": "Evaluation. We retain the best model throughout training, ranked first by Hamming Distance and then by the marginal log-likelihood (estimated with 5 repeats). We report the marginal log-likelihood of the best model. As done in Section 4.1, Precision, Recall, and Hamming Distance are utilized to evaluate the permutation matrices obtained from the best model without adding noise. ", "page_idx": 21}, {"type": "table", "img_path": "pMJFaBzoG3/tmp/71c5436826c8edfa7fec0ca163930a24cc075dfddb1332b27cf1d39ada112287.jpg", "table_caption": ["Table 5: Recall $(\\%)$ and Hamming Distance of algorithms for inferring neuron identities across different proportions of known neurons. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "F.4 Solving permutation synchronization ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Setup. We use the WILLOW-ObjectClass dataset [9] to generate problem instances. The WILLOWObjectClass dataset comprises images of five object classes, each containing 10 equal key points of at least 40 images. For each image, we extract interpolated features from the relu4_2 and relu5_1 layers through a pre-trained VGG16 [60] model on ImageNet [10]. The initial pairwise correspondences are established by applying the Hungarian algorithm [36] to the distance matrices of features. We increase the number of objectives, $k$ , from 20 to the its largest value (multiples of 5) for each object class. ", "page_idx": 21}, {"type": "text", "text": "Permutationness. We also take the $\\ell_{1}$ -Distance to assess the \u2018permutationness\u2019 of the final matrix. Specifically, we round the matrix $\\widetilde{P}$ , returned by the algorithms, to its closest permutation matrix $P$ , and then calculate the $\\ell_{1}$ -Distance between $\\widetilde{P}$ and $P$ . Table 6 lists the results for the problem instances corresponding to the largest size (multiples of 5) in each object class. We observe that the relaxation extent of Sinkhorn is unstable. Unlike them, OT4P consistently maintains smaller distances in almost all cases and exhibits a positive correlation with changes in the hyperparameter $\\tau$ . ", "page_idx": 21}, {"type": "table", "img_path": "pMJFaBzoG3/tmp/22ae8c0dea4abaa568895c6bb282056a55419b423ad66789dfc500129b21471d.jpg", "table_caption": ["Table 6: $\\ell_{1}$ -Distance between the matrix returned by the algorithms and its closest permutation matrix. In each object class, we select the largest problem instance size that is a multiple of five. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Impact of Optimizers. We compare the results of different optimizers in Table 7, selecting the largest instances (multiples of 5) for each object class. Methods based on the Birkhoff polytope show notable performance improvements on most datasets when using (Riemannian) SGD. For our proposed OT4P, the choice of optimizer appears to be less critical, as it consistently outperforms other methods regardless. ", "page_idx": 22}, {"type": "table", "img_path": "pMJFaBzoG3/tmp/79ed1863fa026502275da79df797bd9194dfae3e3e1cb97ebe8621bffae05db7.jpg", "table_caption": ["Table 7: F-scores $(\\%)$ for different algorithms with various optimizers on the WILLOW-ObjectClass dataset. In each object class, we select the largest problem instance size that is a multiple of five. "], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "Case study. This case study aims to explore the runtime and memory efficiency of our proposed OT4P on large-scale problems. We use the CMU House [5] image sequence to generate problem instances, comprising 111 frames of a video of a toy house. A total of $111\\times111$ pairwise matching, with each image having 30 hand labeled landmark points, are provided in Bernard et al. [5]. We increase the number of objectives, $k$ , from 20 to 110 and utilize the proposed OT4P to approximately solve Equation (18) on the NVIDIA GeForce RTX 3090. For all instances, we conduct 100 iterations using AdamW [45] with an initial learning rate of 0.1. ", "page_idx": 22}, {"type": "image", "img_path": "pMJFaBzoG3/tmp/95738d6287847eb5f821b3763d14a7e06005c7b0ed5e8bc51395a8fa074723c8.jpg", "img_caption": ["Figure 6: F-score, runtime, and memory usage of OT4P on the CMU House, where the size of permutation synchronization problem instances varies along the horizontal axis. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "We flatten the permutation matrices and evaluate their alignment with the ground truth using F-score, while also assessing the runtime and the maximum GPU memory consumption during training. Each experiment is conducted five times, and the results are depicted in Figure 6, where the size of problem instances varies along the horizontal axis. The findings indicate that OT4P can find satisfactory solutions (with F-scores exceeding $97\\%$ ) in less than 4 seconds, and the memory usage is manageable. Additionally, in Figure 7, we present the matching between the first and last images for $k=110$ , where the obtained matchings are connected (correct: , incorrect: ). Overall, the proposed OT4P demonstrates high efficiency in both runtime and memory usage when dealing with large-scale tasks amenable to parallel processing. ", "page_idx": 22}, {"type": "image", "img_path": "pMJFaBzoG3/tmp/d12f2043c5219957af7fa63558ce6a643b311a8505bbc7c49f3b1004b29f9e55.jpg", "img_caption": ["Figure 7: Matching between the first and last images of the CMU House for $k=110$ , where the obtained matchings are connected (green: correct, red: incorrect). "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "F.5 Licenses ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We will publicly release our code under the MIT license, in accordance with community standards. The licenses for the code, data, and models used in this study are provided below. Please refer to individual links for more details. ", "page_idx": 23}, {"type": "text", "text": "\u2022 PyTorch: BSD-style   \n\u2022 Torchvision: BSD 3-Clause   \n\u2022 torch-linear-assignment: Apache   \n\u2022 sinkhorn-rebasin: MIT   \n\u2022 SparseStiefelOpt: GNU Affero General Public   \n\u2022 geoopt: Apache ", "page_idx": 23}, {"type": "text", "text": "G Proof ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we first elucidate some key properties of the matrix exponential and the matrix logarithm. Based on these, we prove Theorems 1 and 2 in Appendix G.2. Finally, we list some essential lemmas used in the proofs. ", "page_idx": 23}, {"type": "text", "text": "G.1 Properties of matrix exponential and logarithm ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Theorem G.1. The matrix exponential map $\\mathrm{expm}\\left(\\cdot\\right)$ is injective over $\\mathcal{V}\\;:=\\;\\{A\\;\\in\\;\\mathbb{R}^{n\\times n}$ $\\operatorname{Im}\\lambda_{k}(A)\\in(-\\pi,\\pi),\\,\\forall k\\}$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. For all $A,B\\in\\mathcal{V}$ , our goal is to prove that $A=B$ if $\\mathrm{expm}\\left(A\\right)=\\mathrm{expm}\\left(B\\right)$ . ", "page_idx": 23}, {"type": "text", "text": "According to Lemma G.1, if $\\mathrm{expm}\\left(A\\right)=\\mathrm{expm}\\left(B\\right)$ , then $A$ and $B$ commute, i.e., $A B=B A$ . In fact, this commutativity implies that the equation $\\operatorname{expm}\\left(A+B\\right)=\\operatorname{expm}\\left(A\\right)\\exp\\left(B\\right)$ holds [23]. ", "page_idx": 23}, {"type": "text", "text": "Thus, we can express ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\operatorname{expm}\\left(A\\right)=\\operatorname{expm}\\left(B\\right)}\\\\ {\\operatorname{expm}\\left(A\\right)\\operatorname{expm}\\left(-B\\right)=\\operatorname{expm}\\left(B\\right)\\operatorname{expm}\\left(-B\\right)=I}\\\\ {\\operatorname{expm}\\left(A-B\\right)=I}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The above equation shows that all eigenvalues of expm $(A-B)$ are one. Since the eigenvalues of the exponential of a matrix are the exponential of its eigenvalues, the eigenvalues of $A-B$ satisfy $2m\\pi i$ for integers $m\\in\\mathbb{Z}$ . However, given that $A,B\\in\\mathcal{V}$ , the range of $\\operatorname{Im}\\lambda_{k}(A-B)$ is restricted to $(-2\\pi,2\\pi)$ for all $k$ , yielding $m=0$ . Thus, all eigenvalues of $A-B$ are zero, which means $A-B$ is a nilpotent matrix. ", "page_idx": 24}, {"type": "text", "text": "Recalling Lemma G.2, we know that under the exponential map, the only nilpotent matrix mapped to the identity matrix $I$ is the null matrix. Therefore, we conclude that $A-B=\\mathbf{0}$ , which implies $A=B$ . ", "page_idx": 24}, {"type": "text", "text": "This confirms that the matrix exponential map is injective within the set $\\nu$ , thereby completing the proof. ", "page_idx": 24}, {"type": "text", "text": "Theorem G.2. The matrix logarithm map logm (\u00b7) is injective over $\\mathcal{W}:=\\{A\\in\\mathbb{R}^{n\\times n}\\mid\\lambda_{k}(A)\\notin\\mathcal{$ $\\mathbb{R}_{0}^{-},\\,\\forall k\\}$ , and $\\mathrm{logm}(\\mathcal{W})\\subseteq\\mathcal{V}$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. We first prove that if $A\\in\\mathcal{W}$ , then $B=\\log\\!\\mathbf{m}\\left(A\\right)\\in\\mathcal{V}$ . ", "page_idx": 24}, {"type": "text", "text": "Assume $A\\in\\mathcal{W}$ with eigenvalues $\\lambda_{k}=\\rho_{k}\\mathrm{e}^{i\\theta_{k}}$ , where $\\rho_{k}>0$ and $\\theta_{k}$ are real number. As shown in Gallier [18], the complex eigenvalues of $B=\\log\\!\\operatorname{m}\\left(A\\right)$ appear only for real Jordan blocks: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left({\\stackrel{\\log(\\rho_{k})}{\\theta}}\\begin{array}{c}{{-\\theta}}\\\\ {{\\log(\\rho_{k})}}\\end{array}\\right),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the eigenvalues are given by $\\log(\\rho_{k})\\pm i\\theta_{k}$ . Since $A$ has no eigenvalues on $\\mathbb{R}_{0}^{-}$ , it follows that $\\theta_{k}\\in(-\\pi,\\pi)$ for all $k$ . Thus, for any $A\\in\\mathcal{W}$ , we have $\\operatorname{logm}(A)\\in\\mathcal{V}$ , that is, $\\mathrm{logm}(\\mathcal{W})\\subseteq\\mathcal{V}$ . ", "page_idx": 24}, {"type": "text", "text": "Recall that the matrix logarithm is defined as a solution to the equation $\\mathrm{expm}\\left(B\\right)=A$ . Coupled with Theorem G.1 and noting that $\\operatorname{logm}(A)\\in\\mathcal{V}$ , then such a logarithm is unique. This proves that the matrix logarithm map is injective over $\\mathcal{V}$ and completes the proof. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "Corollary G.3. The image expm $(\\mathcal{V})$ of $\\mathcal{V}$ by the matrix exponential expm (\u00b7) is the set $\\mathcal{W}$ and the mapping expm : $\\mathcal{V}\\rightarrow\\mathcal{W}$ is a diffeomorphism. ", "page_idx": 24}, {"type": "text", "text": "Proof. By Theorem G.2, it is clear that $\\mathcal{W}\\subseteq\\mathrm{expm}\\left(\\mathcal{V}\\right)$ . We now prove $\\mathrm{expm}\\left(\\mathcal{V}\\right)\\subseteq\\mathcal{W}$ . ", "page_idx": 24}, {"type": "text", "text": "For any matrix $A\\in\\mathcal V$ , the eigenvalues have the form $a+i b$ , where $a$ and $b$ are real number and $-\\pi<b<\\pi$ . Then, the eigenvalues of $\\operatorname{expm}\\left(A\\right)$ take the form $\\mathrm{e}^{a+i b}=\\mathrm{e}^{a}\\mathrm{e}^{i b}$ . Since $\\mathrm{e}^{i b}$ never lies on $\\mathbb{R}_{0}^{-}$ , $\\operatorname{expm}\\left(A\\right)$ has no non-positive real eigenvalues, i.e., $\\mathrm{expm}\\left(A\\right)\\in\\mathcal{W}$ . The arbitrariness of $A$ leads to the conclusion of $\\mathrm{expm}\\left(\\mathcal{V}\\right)\\subseteq\\mathcal{W}$ . ", "page_idx": 24}, {"type": "text", "text": "The diffeomorphism results directly from Theorems G.1 and G.2. ", "page_idx": 24}, {"type": "text", "text": "G.2 Proof of Theorems 1 and 2 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Theorem $^{1\\ast}$ . The mapping $\\phi(\\cdot)$ is differentiable, surjective, and it is injective on the domain $\\mathcal{U}:=\\{A\\in\\mathbb{R}^{\\frac{n(n-1)}{2}}\\mid\\operatorname{Im}\\lambda_{k}(A-A^{\\top})\\in(-\\pi,\\pi),\\,\\forall k\\}$ with $\\lambda_{k}(\\cdot)$ the eigenvalues. Additionally, the set $\\operatorname{SO}(n)\\setminus\\phi(\\mathcal{U})$ has a zero Lebesgue measure in $\\mathrm{SO}(n)$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. It is trivial that $\\phi(\\cdot)$ is differentiable. ", "page_idx": 24}, {"type": "text", "text": "We start by establishing that $\\phi(\\cdot)$ is surjective. It is worth mentioning that the conclusion of Lemma G.3 is applicable to $\\mathrm{SO}(n)$ due to it being connected and compact. However, we provide a straightforward proof based on Theorem G.1. Indeed, any orthogonal matrix $O\\in\\mathrm{SO}(n)$ can be diagonalized into a block-diagonal matrix with diagonal blocks consisting of $2\\times2$ rotation matrices: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left(\\begin{array}{l l}{\\cos\\theta}&{\\sin\\theta}\\\\ {-\\sin\\theta}&{\\cos\\theta}\\end{array}\\right),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\theta\\in(-\\pi,\\pi]$ represents the rotation angle. For $\\mathrm{SO}(2n+1)$ , an extra block containing a single 1 exists. Correspondingly, we have a skew-symmetric matrix $B\\in{\\mathfrak{s o}}(n)$ satisfying exp $\\mathrm{m}\\left\\{B\\right\\}={\\bar{O}}$ , which is also a block-diagonal matrix with diagonal blocks of the form: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left(\\begin{array}{l l}{0}&{\\theta}\\\\ {-\\theta}&{0}\\end{array}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "There is an additional block with a single 0 on ${\\mathfrak{s o}}(2n+1)$ . As a result, for all orthogonal matrices $O\\,\\in\\,\\mathrm{SO}(n)$ , we can surely find a skew-symmetric matrix $B\\,\\in\\,{\\mathfrak{s o}}(n)$ such that ${\\dot{\\mathrm{expm}}}\\left(B\\right)={\\cal O}$ . Given that ${\\mathfrak{s o}}(n)$ is isomorphism to the vector space $\\mathbb{R}^{\\frac{n(n-1)}{2}}$ , this confirms that $\\phi(\\cdot)$ is surjective. Regarding $\\phi(\\cdot)$ is injective on the domain $\\boldsymbol{\\mathcal{U}}$ , it can be derived from Theorem G.1 because $\\mathcal{U}\\subseteq\\mathcal{V}$ . ", "page_idx": 25}, {"type": "text", "text": "The above reasoning also indicates that $\\phi(\\mathcal{U})\\subseteq\\mathrm{SO}(n)\\subseteq\\phi(\\bar{\\mathcal{U}})$ holds and that the boundary of $\\boldsymbol{\\mathcal{U}}$ has zero Lebesgue measure. Since the matrix exponential map expm $(\\cdot)$ is injective within the interior of $\\boldsymbol{\\mathcal{U}}$ and the complex exponential function is single-valued, the set $\\dot{\\mathrm{SO}}(n)\\,\\dot{\\setminus}\\,\\phi(\\mathcal{U})$ has a zero Lebesgue measure in $\\mathrm{SO}(n)$ . \u53e3 ", "page_idx": 25}, {"type": "text", "text": "Theorem $^{2*}$ . The mapping $\\psi_{\\tau}(\\cdot)$ is differentiable, surjective, and injective on each submanifold $\\displaystyle{S_{P}}$ .   \nAdditionally, the set of meaningless points for $\\psi_{\\tau}(\\cdot)$ has a zero Lebesgue measure in $\\mathrm{SO}(n)$ . ", "page_idx": 25}, {"type": "text", "text": "Proof. Since the right translation $R_{D}$ establishes an isometry between the neighborhoods of $P$ and its agent $\\widehat{P}$ , we can, without loss of generality, restrict proof to the case where $P\\,=\\,\\rho(O)$ is an even permutation. In this case, we rewrite $\\psi_{\\tau}$ as $\\phi_{\\tau}(O)=P\\exp\\left(\\tau\\log\\mathbf{m}\\left(P^{\\top}O\\right)\\right)$ . According to Corollary G.3, if $P^{\\top}O$ is within $\\mathcal{W}$ , then we can assert that $\\psi_{\\tau}(\\cdot)$ is surjective and injective on each submanifold $S_{P}$ . ", "page_idx": 25}, {"type": "text", "text": "Assuming the eigenvalues of $P^{\\top}O$ are $\\lambda_{1},\\ldots,\\lambda_{n}$ , the eigenvalues of $P^{\\top}O{-}I$ are $\\lambda_{1}\\!-\\!1,\\ldots,\\lambda_{n}\\!-\\!1$ . Since the inverse mapping of the left translation, $L_{P^{\\top}}(O):=P^{\\top}O$ ( $\\forall O\\in\\mathrm{SO}(n))$ ), maps $O$ to a neighborhood around $I$ , we may assume $\\|P^{\\top}O-I\\|_{\\mathrm{F}}<1$ . A well-known result is $r(A)\\leq\\|A\\|_{\\mathrm{F}}$ for any $A\\in\\mathbb{R}^{n\\times n}$ , where $r(A):=\\operatorname*{max}\\{|\\lambda_{k}(A)|\\mid\\forall k\\}$ is the spectral radius. As a result, we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\nr(P^{\\top}O-I)=\\operatorname*{max}\\{|\\lambda_{k}-1|\\mid\\forall k\\}\\leq\\|P^{\\top}O-I\\|_{\\mathrm{F}}<1.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Equation (23) shows that the eigenvalues of $P^{\\top}O$ are within a unit circle centered at 1 in the complex plane. Therefore, $P^{\\top}O$ has no eigenvalues on $\\mathbb{R}_{0}^{-}$ , meaning $P^{\\top}O\\in\\mathcal{W}$ . ", "page_idx": 25}, {"type": "text", "text": "Since $\\rho(\\cdot)$ w.r.t. Equation (7) is a piecewise constant function, it follows that the set of points where $\\psi_{\\tau}(\\cdot)$ is meaningless has a zero Lebesgue measure. This completes the proof. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "G.3 Lemmas used ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Lemma G.1 (Theorem 4. in Hille [25]). Let $A,B\\in\\mathbb{C}^{n\\times n}$ . If there are no two eigenvalues in $A$ such that their difference is of the form $2k\\pi i$ for $k>0$ and, $i f\\exp({A})=\\exp\\!{\\left(B\\right)}.$ , we have that $A B=B A$ . ", "page_idx": 25}, {"type": "text", "text": "Let $\\operatorname{Nil}(k)$ denote the set of (real or complex) nilpotent matrices, $A$ , of any dimension $n\\geq1$ such that $A^{r}=\\mathbf{0}$ and $\\operatorname{Uni}(k)$ denote the set of unipotent matrices, $B=I+A$ , where $A\\in\\mathrm{Nil}(k)$ . ", "page_idx": 25}, {"type": "text", "text": "Lemma G.2 (Proposition 3.2. in Gallier [18]). The map $\\exp\\operatorname{m}:\\mathrm{Nil}(k)\\to\\mathrm{Uni}(k)$ is a homeomorphism whose inverse is the matrix logarithm. ", "page_idx": 25}, {"type": "text", "text": "Lemma G.3 (Corollary 11.10. in Hall [21]). If $G$ is a connected, compact matrix Lie group, the exponential map for $G$ is surjective. ", "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The main claims accurately reflect the contribution and scope of the paper. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 26}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: We discuss the limitations in Appendix A ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: In the Appendix G, we provide complete proofs for all theoretical results and list some essential lemmas used in the proofs. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: In Appendix F.1, we provide all the experimental details, the random seeds used and the runtime environment. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The core code has been released at https://github.com/YamingGuo98/ OT4P. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We provide a comprehensive overview of the experiments in Appendix F. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: We report the experimental results using five different random seeds. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Appendix F.1 provides information on compute resources. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We have reviewed the NeurIPS Code of Ethics. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: We discuss the potential social impacts of this work in Appendix A.3 Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: Appendix F.5 have included the licenses for the assets used in the paper. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 30}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This does not apply on our research work. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]