[{"type": "text", "text": "Capturing the Denoising Effect of PCA via Compression Ratio ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Chandra Sekhar Mukherjee \u2217 chandrasekhar.mukherjee@usc.edu ", "page_idx": 0}, {"type": "text", "text": "Nikhil Deorkar \u2217 deorkar@usc.edu ", "page_idx": 0}, {"type": "text", "text": "Jiapeng Zhang \u2217 jiapengz@usc.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Principal component analysis (PCA) is one of the most fundamental tools in machine learning with broad use as a dimensionality reduction and denoising tool. In the later setting, while PCA is known to be effective at subspace recovery and is proven to aid clustering algorithms in some specific settings, its improvement of noisy data is still not well quantified in general. ", "page_idx": 0}, {"type": "text", "text": "In this paper, we propose a novel metric called compression ratio to capture the effect of PCA on high-dimensional noisy data. We show that, for data with underlying community structure, PCA significantly reduces the distance of data points belonging to the same community while reducing inter-community distance relatively mildly. We explain this phenomenon through both theoretical proofs and experiments on real-world data. ", "page_idx": 0}, {"type": "text", "text": "Building on this new metric, we design a straightforward algorithm that could be used to detect outliers. Roughly speaking, we argue that points that have a lower variance of compression ratio do not share a common signal with others (hence could be considered outliers). ", "page_idx": 0}, {"type": "text", "text": "We provide theoretical justification for this simple outlier detection algorithm and use simulations to demonstrate that our method is competitive with popular outlier detection tools. Finally, we run experiments on real-world high-dimension noisy data (single-cell RNA-seq) to show that removing points from these datasets via our outlier detection method improves the accuracy of clustering algorithms. Our method is very competitive with popular outlier detection tools in this task. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Principal component analysis, commonly known as PCA, is one of the most fundamental tools in machine learning. PCA is primarily used as a dimensionality reduction tool that transforms highdimensional data to lower dimensions for better visualization as well as a heuristic that reduces the complexity of the algorithms that are to be run on the data. On the other hand, it is also known to have certain denoising effects on high-dimensional data. This denoising phenomenon has been observed in different domains, including biological data [KAH19, VKS17], speech data [Li18], signal measurement data $[\\mathrm{ARS^{+}04}$ , KHK19], image data [MBSP12] among others. The denoising effect of PCA has been extensively studied over the last decades [And58, HR03, Jac05, RVdBB96, Nad08, Nad14, VN17, MZ23, MZ24]. ", "page_idx": 0}, {"type": "text", "text": "One of the most fundamental problems in unsupervised learning is the analysis of data in the presence of community structures [CA16]. This includes clustering of such data [XT15], visualization [TWT21], outlier detection [AM13], and others. The primary progress in understanding the denoising effect of PCA has been solely in clustering, particularly in connection to the K-Means algorithm [DH04, KK10, AS12], where PCA in combination with a K-Means based iterative algorithm is shown to provide a good clustering of that dataset with mild assumptions with the underlying community structure. ", "page_idx": 1}, {"type": "text", "text": "However, PCA seems to have a more \u201cgeneral\u201d denoising effect in data, as it improves the performance of various downstream algorithms, including clustering [VKS17] as well community structure preserving graph embedding $[\\mathrm{HHAN}^{+}21]$ and this denoising effect is evident in many real-world datasets. ", "page_idx": 1}, {"type": "text", "text": "1.1 Contributions ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "To this end, we propose a metric, called compression ratio, that quantifies PCA\u2019s improvement of high dimensional noisy data with underlying community structure in a geometric, and thus algorithm-independent manner 2. ", "page_idx": 1}, {"type": "text", "text": "Compression ratio. Let $\\textbf{\\em u}$ and $\\pmb{v}$ be two data points from a dataset and let $\\Pi_{t}$ be the $t^{\\th}$ -dimensional PCA projection operator. Then the compression ratio between the two points is defined as the ratio between their pre-PCA and post-PCA distance, which is ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\frac{\\lVert\\boldsymbol{u}-\\boldsymbol{v}\\rVert}{\\lVert\\Pi_{t}(\\boldsymbol{u})-\\Pi_{t}(\\boldsymbol{v})\\rVert}.\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "In a dataset with a community structure, we show the compression ratio for intra-community pairs is higher than that of inter-community pairs even in settings where the pre-PCA inter-community and intra-community distances are very similar. We demonstrate (through a random vector mixture model) that this ratio gap reflects the denoising effect of PCA. As a consequence, PCA brings points from the same community much closer, improving the performance of downstream algorithms such as K-Means. ", "page_idx": 1}, {"type": "text", "text": "As a motivating byproduct, we show that this metric can be used to design an outlier detection method that can detect points deviating from a community structure. Furthermore, we show that this method can improve the accuracy of clustering algorithms in real-world high-dimensional datasets. ", "page_idx": 1}, {"type": "text", "text": "Outlier detection method. Our outlier detection is a simple process inspired by compression ratio. Intuitively, any data point that belongs to an underlying community should have large compression ratios with many points from the same community, whereas it will have a lower compression ratio w.r.t inter-community points. On the other hand, outliers will have more similar compression ratios with all the other points. This difference can be captured by the variance of the list of compression ratios between one point and all of the other points, with outliers having a lower variance of compression. Thus our algorithm simply removes points with low variance of compression. ", "page_idx": 1}, {"type": "text", "text": "We analyze this simple algorithm in an extension of the standard random vector mixture model. We also compare our algorithm with popular algorithms such as the Local Outlier Factor (LOF) method [BKNS00] and KNN-dist [RRS00] as well as more recent methods such as Isolation forest [LTZ08] and ECOD $[\\mathrm{L}Z\\mathrm{H}^{+}22]$ through both simulations and experiments on real-world data. We show that this simple algorithm is very competitive with those popular outlier detection tools. ", "page_idx": 1}, {"type": "text", "text": "Overall, we believe the effect of PCA on denoising becomes more significant if for each datapoint, there are many data points with large compression ratio variance. ", "page_idx": 1}, {"type": "text", "text": "Real world experiments Finally, we test the relevance of compression ratio as a metric and the outlier detection method in real-world data. We focus on single-cell data, as it is both high dimensional $(20,000\\mathrm{~-~}40,000$ dimension) and noisy [KAH19], using datasets from a popular benchmark database [DRS20] with ground truth community labels. We first show that the average intra-community compression ratio is higher than the average inter-community compression ratio in all of the datasets. We then show that removing outliers in these datasets via our variance of compression technique improves the performance of clustering algorithms, such as $\\mathrm{PCA+K}.$ -Means, where we again outperform standard outlier detection methods. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Organization of the paper In Section 2, we discuss our theoretical analysis. Concretely, we define the random vector mixture model and provide bounds on the intra-community and inter-community compression ratios. Next, we define our outlier detection metric and justify it in an extension of our generative model. Section 3 contains the simulation results validating the compression ratio metric and we also compare the performance of our outlier detection method with other methods. Finally in Section 4 we demonstrate that PCA exhibits an average version of compression ratio in real-world biological datasets [DRS20] and then test our outlier detection-based clustering accuracy improvement idea discussed above. ", "page_idx": 2}, {"type": "text", "text": "1.2 Related Works ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "PCA and its effect on noisy data has been subject to a lot of investigation in the last 50 years. Before 2008, most of the work focused on the asymptotic setting, where the number of points $(n)$ and/or the dimension $(d)$ are infinite (see [And58, RVdBB96, HR03, Jac05] and the references therein). In the last two decades, several works have also considered the finite sample setting [Nad08, Nad14]. These works have primarily focused on the denoising aspect of PCA in different variants of Gaussian noise. In a recent line of work [VN17] has studied the subspace recovery problem in the presence of bounded and (nice) sub-Gaussian noise. However, there seems to be no direct way to convert these results into a clustering setting. In comparison, we study PCA\u2019s denoising effect on data in random vector mixture model via the compression ratio metric, where the noise can be heavy sub-Gaussian. ", "page_idx": 2}, {"type": "text", "text": "PCA in clustering tasks With regards to PCA\u2019s impact on data with community structure, the primary work has been in connection to K-Means. Here, one of the first works [DH04] showed that the outcome of PCA can be viewed as an approximation result to the K-Means outcome in clustering data. In this direction, a lot of progress has been made in the last two decades. ", "page_idx": 2}, {"type": "text", "text": "A beautiful recent work [KK10] has shown that PCA followed by several iterations of K-Means along with modifications can cluster data with reasonable parameters in the random vector mixture model that we discuss here, which was then improved in [AS12]. Both of the works focused on the setting of $n\\gg d$ (for example, [KK10] worked with $n\\geq d^{8}$ ). More recently, tighter results have been obtained in the context of the Gaussian-mixture model in [LZZ21] (still on the setting of $n\\gg d^{2}$ ). ", "page_idx": 2}, {"type": "text", "text": "In comparison, we study PCA\u2019s relative compression in an algorithm-independent fashion, focusing on its effect on the geometry of the data in the high-dimensional setting of $\\bar{n}=\\Omega(d)$ with sub-Gaussian noise. We are motivated to analyze this setting as single-cell datasets often have $n<d$ . ", "page_idx": 2}, {"type": "text", "text": "2 Random vector model and relative compression of PCA ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To theoretically study the relative compression of PCA, we use a high-dimensional mixture model, similar to one in [KK10, AS12]. We call this a random vector mixture model. This can also be interpreted as a signal-plus-noise model where the signal imposes a community structure on the data. The dataset of interest is a set of $n$ many $d$ dimensional real vectors $\\mathbf{x_{i}}\\,\\in\\,\\mathbb{R}^{d},1\\,\\leq\\,i\\,\\leq\\,n,$ which is together represented as the dataset $X$ . We express $X$ as a $d\\times n$ matrix, with each column representing a data point. The data points have an underlying hidden community structure that is expressed as a partition of $[n]:=\\{1,\\overline{{\\ldots}},n\\}$ into $k$ many sets $V_{1},\\ldots,V_{k}$ such that each $i\\in[n]$ lies in any one $V_{j}$ . We then have the following problem structure. ", "page_idx": 2}, {"type": "text", "text": "1. Each cluster $V_{j},1\\leq j\\leq k$ is associated with a ground truth center $\\mathbf{c_{j}}\\in\\mathbb{R}^{d}$ . ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2. Additionally, each cluster $V_{j}$ is associated with a distribution $\\mathcal{D}^{(j)}$ such that $\\mathcal{D}^{(j)}$ is a coordinate wise independent zero mean distribution. For ease of exposition, we define the support of $\\mathcal{D}^{(j)}$ to be $[-\\alpha,\\alpha]^{d}$ for some $\\alpha$ (which can also depend on $n,d)$ , but our methods also directly apply to sub-Gaussian \u221adistributions where each coordinate has a constant sub-Gaussian norm (resulting in $\\mathcal{O}(\\sqrt{d})$ norm of any column vector). Then $\\alpha$ would be replaced with $C^{\\prime}\\log{n}$ for some constant $C^{\\prime}$ in our bounds. ", "page_idx": 2}, {"type": "text", "text": "Then, the dataset $X$ is set up as follows. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.1 (Random vector mixture model). For each $i\\in[n]$ , if $i\\in V_{j}$ , then $\\mathbf{\\boldsymbol{x}}_{i}=\\mathbf{c_{j}}+e_{i}$ where $e_{i}\\sim\\mathcal{D}^{(j)}$ , i.e. $e_{i}$ is independently sampled from $\\mathcal{D}^{(j)}$ . Here we abuse notation and denote both $i\\in V_{j}$ as well as $x_{i}\\in V_{j}$ . ", "page_idx": 3}, {"type": "text", "text": "With this setup, now we define the PCA projection operator and the compression ratio metric formally. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.2 (The PCA operator $\\Pi_{X}^{k^{\\prime}}$ ). Let $X$ be a $d\\times n$ matrix. Then the $k^{\\prime}$ dimensional PCA projection operator is simply the projection operator onto the first $k^{\\prime}$ principal components of $X$ . ", "page_idx": 3}, {"type": "text", "text": "Next we formally define the compression ratio metric. ", "page_idx": 3}, {"type": "text", "text": "Definition 2.3. For any pair $(i,i^{\\prime})$ we define the $k^{\\prime}$ -PC compression of the pair of vectors in $X$ as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Delta_{X,k^{\\prime}}(i,i^{\\prime})=\\frac{\\lVert\\pmb{x}_{i}-\\pmb{x}_{i^{\\prime}}\\rVert}{\\lVert\\Pi_{X}^{k^{\\prime}}(\\pmb{x}_{i})-\\Pi_{X}^{k^{\\prime}}(\\pmb{x}_{i^{\\prime}})\\rVert}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Before describing our results, we define certain parameters of the model. ", "page_idx": 3}, {"type": "text", "text": "1. The maximum variance of the entries, $\\sigma$ is defined as $\\begin{array}{r}{\\sigma^{2}=\\operatorname*{max}_{1\\leq j\\leq j,1\\leq\\ell\\leq d}\\operatorname{Var}[\\mathcal{D}_{\\ell}^{(j)}]}\\end{array}$ 2. The average variance of a column in a distribution $\\mathcal{D}^{(j)}$ , noted as $\\sigma_{j}$ is defined as $\\sigma_{j}^{2}=$ $\\begin{array}{r l}{\\frac{1}{d}\\sum_{\\ell}V a r\\big([\\mathcal{D}_{\\ell}^{(j)}]\\big)}\\end{array}$ . Here, $\\sigma_{j}{\\sqrt{d}}$ is the perturbation on the data points of $V_{j}$ due to the noise. ", "page_idx": 3}, {"type": "text", "text": "In this direction, we first lower, and upper bound the $(k\\!-\\!1)$ -PC intra-community and inter-community compression ratios respectively, as a function of the maximum variance, average variances, spectral structure of the noise and signal, and distance between the centers of the model, which can be found in Theorem B.1. ", "page_idx": 3}, {"type": "text", "text": "Although our result applies to any set of centers, the spectral properties of the resultant matrix, and their interactions make the result hard to interpret. To give more insight into our bounds, we instead define a restricted (still natural) structure on the centers, which allows us to giv\u221ae a more interpretable result in this case. For simplicity, we also work in the setting where $d\\geq10\\bar{\\alpha}\\sqrt{n}\\log n$ . ", "page_idx": 3}, {"type": "text", "text": "Definition 2.4 (Spatially unique centers). We say a set of vectors $\\mathbf{C}=\\{\\mathbf{c_{1}},\\dots,\\mathbf{c_{k}}\\}$ are $\\gamma$ -spatially unique, if we have that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{1\\leq j\\leq k}\\;\\operatorname*{min}_{\\mathbf{v}\\in{\\mathsf{S p a n}}(\\mathbf{C}\\setminus\\mathbf{c_{j}})}\\lVert\\mathbf{c_{j}}-\\mathbf{v}\\rVert\\geq\\gamma\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "This implies that each center has a unique pattern that cannot be approximated by a combination of the other centers. Here note that $\\begin{array}{r}{\\gamma\\geq\\operatorname*{min}_{j\\neq j^{\\prime}}\\|\\mathbf{c_{j}}-\\mathbf{c_{j^{\\prime}}}\\|}\\end{array}$ . For example, such a property is expected if the centers are mutually orthogonal. One can also think of them as vertices in a high-dimensional regular polygon. Then, we give some sufficient conditions for the separation of intra-community and inter-community compression ratios of PCA. ", "page_idx": 3}, {"type": "text", "text": "Theorem 2.5 (Separation of compression ratio). Let $\\gamma\\geq C\\operatorname*{max}\\{\\sigma\\sqrt{k}d^{1/4},\\sigma\\sqrt{k}+\\alpha\\log n\\}$ for some constant $C$ . Furthermore, let $i\\sim i^{\\prime}$ denote that $\\textit{\\textbf{y}}_{i}$ and $\\mathbf{\\mathit{y}}_{i^{\\prime}}$ belong to the same underlying community. Then, the following holds. ", "page_idx": 3}, {"type": "text", "text": "1. The perturbation of the points due to noise can be much larger than the distance between the community centers, i.e., the noise dominates the distance between the centers. 2. With probability $\\begin{array}{r}{1-\\mathcal{O}(1/n),\\operatorname*{min}_{(i,i^{\\prime}):i\\sim i^{\\prime}}\\Delta_{X,k-1}(i,i^{\\prime})\\geq4\\cdot\\operatorname*{max}_{(i,i^{\\prime}):i\\sim i^{\\prime}}\\Delta_{X,k-1}(i,i^{\\prime})}\\end{array}$ ", "page_idx": 3}, {"type": "text", "text": "This shows that the compression ratio of PCA provides a separation between intra-community and inter-community pairs even in a setting where the noise highly dominates the distance between the centers. One can find a more general theorem w.r.t spatially unique centers in Theorem C.4. ", "page_idx": 3}, {"type": "text", "text": "A natural question is whether post-PCA distance is a good metric for denoising due to PCA. In this regard, we point out that the compression ratio has an added normalization property. For example, consider the case where all pair-wise center distances are the same. In such a case, the post-PCA distances are dependent on $\\sigma_{j}$ , so communities with larger variances have larger intracommunity distances. However, this gets normalized in the compression factor as per Equation (9) of Theorem C.4, as the numerator also has a dependency on $\\sigma_{j}$ . ", "page_idx": 3}, {"type": "table", "img_path": "a4J7nDLXEM/tmp/d8bc29b0429341bcbc82e1f26304e15d5e7a45fe4b8aa607dbe515133387a37b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "2.1 Outlier detection with compression ratio ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Now, we discuss the usefulness of compression ratio on outlier detection. We first describe the notion of variance of compression ratio. ", "page_idx": 4}, {"type": "text", "text": "Definition 2.6 (Variance of compression ratio). Given a dataset $X$ and a PCA dimension $k^{\\prime}$ , variance of compression ratio of a point $\\pmb{u}\\in\\cal X$ is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathsf{V A R}\\Delta_{X,k^{\\prime}}(\\pmb{x}_{i})=\\mathrm{Var}(\\{\\Delta_{X,k^{\\prime}}(i,i^{\\prime})\\}_{i^{\\prime}\\neq i})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\begin{array}{r}{\\Delta_{X,k^{\\prime}}(i,i^{\\prime})=\\frac{\\Vert x_{i}-x_{i^{\\prime}}\\Vert}{\\Vert\\Pi_{X}^{k^{\\prime}}({\\pmb x}_{i})-\\Pi_{X}^{k^{\\prime}}({\\pmb x}_{i^{\\prime}})\\Vert}}\\end{array}$ is the compression ratio between points $i$ and $i^{\\prime}$ . ", "page_idx": 4}, {"type": "text", "text": "That is, it is simply the variance of the list of compression ratios of $\\mathbf{\\Delta}x_{i}$ with all the other points $\\pmb{x}_{i^{\\prime}}$ . ", "page_idx": 4}, {"type": "text", "text": "Then, our intuition is that if data consists of many points from the high dimensional mixture model, as well as several outlier points that don\u2019t share a common signal (center), they have a lower variance of compression ratio. We concretize this notion with the following simple detection algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "Mixture-model with outliers Now let us consider an extension of the mixture model in Definition 2.1 to incorporate outliers. ", "page_idx": 4}, {"type": "text", "text": "Definition 2.7 (Mixture model with outliers). Let $X$ be a $d\\,\\times\\,n$ dataset with the partition $V_{1},\\ldots,V_{k},{\\hat{V}}$ , a set of $k$ centers $\\{\\mathbf{c}_{\\mathbf{j}}\\}_{j=1}^{k}$ and distributions $\\{\\mathcal{D}^{(j)}\\}_{j=1}^{k}+1$ with the following generation method. ", "page_idx": 4}, {"type": "text", "text": "Let $|\\hat{V}|=n_{o}$ and $n=n_{o}+n_{c}$ . To keep the results simple, we make the average variance of each distribution $\\mathcal{D}^{(j)}$ same, which is $\\sigma^{\\prime}$ . ", "page_idx": 4}, {"type": "text", "text": "Such a scenario can occur in many different settings. For example, consider single-cell datasets which is a popular biological data type. Here each data point is a cell and the features (which are high, such as 20, 000) are specific gene expressions, A primary task here is to obtain cell sub-populations $\\mathrm{\\dot{[THL^{+}19}}$ , VKS17, KAH19]. Although the gene expressions within sub-populations should have similarities, they are perturbed by biological and technical noise, making high-dimensional mixture models a good setup to study them. However, some cells may not belong to any particular sub-populations, but rather be intermediate cells. Additionally, sometimes cells get merged during the biological experiment that records the gene expressions, generating data points that behave like a random mixture of two or more data points. Our model aims to model such scenarios. ", "page_idx": 4}, {"type": "text", "text": "In this setting, we get the following outlier detection result where the centers have spatially unique centers. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2.8 (Outlier detection via Alg\u221aorithm 1). Let $X$ be a $d\\times n$ dataset with $\\gamma$ -spatially unique $k$ many centers where $\\log n\\,\\leq\\,k\\,\\leq\\,\\sqrt{d}$ and $n_{0}$ outliers in the setting of Definition 2.7. Let the following conditions hold ", "page_idx": 4}, {"type": "equation", "text": "$\\|\\mathbf{c_{j}}-\\mathbf{c_{j^{\\prime}}}\\|=\\mathcal{O}(\\sigma^{\\prime}\\sqrt{d})$ ", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Then, for any $n_{0}\\le n/2$ , the first $n_{0}$ points ranked by Algorithm 1 all belong to the outlier group $(\\hat{V}_{\\ast}$ ) with probability $1-o(1)$ . ", "page_idx": 5}, {"type": "text", "text": "We discuss the connection between our results and the role of spatially unique centers in Appendix C.   \nThe proof of Theorems 2.5 and 2.8 can be found in Appendix C and C.2 respectively. ", "page_idx": 5}, {"type": "text", "text": "This gives us initial theoretical evidence that in the random-mixture model with outliers, our simple outlier detection method can detect outliers when a non-negligible fraction of the points are outliers. Next, we use simulations of our model to test the efficacy of our outlier detection method and its impact on the community structure of data and compare them with some popular outlier detection methods. ", "page_idx": 5}, {"type": "text", "text": "3 Simulations for outlier detection ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we first describe different instantiations of the random-vector mixture model, observe the intra-community and inter-community compression ratios in them, and then run simulations in the outlier mode. All simulations and experiments were run on a 2020 M1 MacBook Pro with 16 GB of memory within 1.5 hours of total running time. ", "page_idx": 5}, {"type": "text", "text": "Simulation setup For this setup, we set $n=3000,d=1000$ , and $k=3$ , with each community having the same number of points. For simplicity, we choose 3 equidistant centers, with $\\|\\mathbf{c_{i}}-\\mathbf{c_{j}}\\|=\\mathbf{c_{\\alpha}}$ . We set the noise distributions to be Bernoulli distributions with variance $\\sigma_{1},\\sigma_{2},\\sigma_{3}$ respectively. We work in two primary settings, of equal and unequal noise. ", "page_idx": 5}, {"type": "text", "text": "i) Equal noise. Here we have $\\sigma_{j}=\\sigma$ for all $i$ . ii)Unequal noise. Here one of the communities has variance $2\\sigma$ , whereas all the other communities have variance $\\sigma$ . ", "page_idx": 5}, {"type": "text", "text": "Then, we test the algorithms for three values of $\\sigma$ in the following manner. ", "page_idx": 5}, {"type": "text", "text": "\u2022 Low noise: We choose $\\sigma:\\|\\mathbf{c_{j}}-\\mathbf{c_{j^{\\prime}}}\\|\\approx3\\sigma\\sqrt{d}$ . This implies distance between the centers dominates the perturbation due to noise.   \n\u2022 Significant noise: Here $\\sigma:\\|\\mathbf{c_{j}}-\\mathbf{c_{j^{\\prime}}}\\|\\approx\\sigma\\sqrt{d}$ . Here the noise norm and distance between centers are of the same order.   \n\u2022 High noise: We have $\\sigma:\\|\\mathbf{c_{j}}-\\mathbf{c_{j^{\\prime}}}\\|\\approx0.3\\sigma\\sqrt{d}$ . Here the noise heavily dominates the center distances. ", "page_idx": 5}, {"type": "text", "text": "Let us look at the equal noise setting, i.e. the case where the variance of noise distributions for all communities are the same. We observe that in the low-noise setting, all intra-community compression ratios are higher than all inter-community compression ratios. As the noise increases, the gap between them decreases, so that in the high-noise setting, there is now an overlap between intra-community and inter-community compression ratios. We demonstrate this in Figure 1a. This further indicates that compression ratio is indeed a useful metric even when the noise has a strong perturbation effect on the data (even though there will be no clean separation between intra-community and inter-community compression ratios once the noise is very high). ", "page_idx": 5}, {"type": "image", "img_path": "a4J7nDLXEM/tmp/dd414e4e13cb92bc56a83e4d0725841a3a5c2c3e82b2a427db4f030fa9987b90.jpg", "img_caption": ["(a) Comparing intra and inter community compression ratios (b) AUROC of variance-based outlier removal in simulation ", "Figure 1: Simulation results for compression ratio and outlier detection "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "3.1 Outlier detection ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Now, we discuss the outlier detection, starting with the simulation setup in this case. We follow the random-mixture-outlier model and add outlier points along with the clean points as follows. ", "page_idx": 6}, {"type": "text", "text": "We add $n_{o}=n_{c}/10$ outliers following definition 2.7. That is, we randomly choose $\\alpha_{1},\\ldots,\\alpha_{3}$ and then a random mixture-center is chosen as $\\sum_{j}\\alpha_{j}\\mathbf{c}_{\\mathbf{j}}$ , and then we add a random noise vector from the Bernoulli distribution of variance $\\sigma_{4}$ . ", "page_idx": 6}, {"type": "text", "text": "Outlier detection algorithms for comparisons Outlier detection has been an active area of study in unsupervised learning, providing several influential algorithms. In a recent, comprehensive benchmarking of outlier detection algorithms, $[\\mathrm{HHH}^{+}22]$ compared the performance of several unsupervised learning algorithms on different datasets. They found that for unsupervised outlier detection methods, success was related to whether the underlying model of the data assumed by the outlier detection method followed the dataset at hand. They found that for local outliers, the popular Local Outlier Factor (LOF) method [BKNS00] performed the best statistically, whereas for global outliers, KNN-dist (where the outlier score is simply the distance to the $K$ -th nearest neighbors) [RRS00] performed the best. Owing to their generally impressive performance, we use them for comparison with our variance of compression method. Furthermore, we select a popular method called Isolation forest [LTZ08] and also a very recent and popular outlier detection method ECOD $[\\mathrm{LZH}^{+}22]$ . We also use $\\mathrm{PCA+}$ method for each of the methods as benchmarks, as both outliers and clean points are perturbed by zero-mean noise, and we now understand PCA can help mitigate the effect of said noise, as discussed in Section 2. ", "page_idx": 6}, {"type": "text", "text": "Outlier detection results We compare the AUROC values of the 5 outlier methods of interest in these settings. We record the results in Figure 2a and 2b for the equal and unequal noise settings respectively. ", "page_idx": 6}, {"type": "image", "img_path": "a4J7nDLXEM/tmp/23c2c582cf9b217ba022b993e7ca981ae036bc03b1606f264c1a1ec7867a4044.jpg", "img_caption": ["", "Figure 2: Comparison of outlier removal in different noise settings "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "As we can see, our method results in the highest AUROC value, followed by PCA $^{+}$ KNN-dist. we make two observations. ", "page_idx": 6}, {"type": "text", "text": "i) The performance gap between variance-of-compression and the next best method is higher in the unequal noise setting. ", "page_idx": 6}, {"type": "text", "text": "ii)As the noise level increases, the gap between our method and P $\\chi_{\\mathrm{A+K}}$ -NN dist increases. ", "page_idx": 6}, {"type": "text", "text": "These two points further highlight the compression ratio\u2019s normalizing effect as well as effectiveness in high noise settings. ", "page_idx": 6}, {"type": "text", "text": "Here we remark that in real-world data, while some points may indeed behave like outliers, they need not all be the same kind of outlier. Thus, we would like to verify our method\u2019s performance in the presence of a different kind of outlier, which we concretize below. ", "page_idx": 6}, {"type": "text", "text": "Higher variance-based outliers We consider the case that some points may have significantly higher noise perturbations than others. In this setting, we randomly select some points, and we generate some points with $c\\cdot\\sigma$ coordinate-wise variance, where $c=8$ for our experiments (recall that the noise in the other points has a coordinate-wise $\\sigma$ variance). It is well known that if noise is low-dimensional, then such outliers are well captured by LOF. We observe that while in the low noise setting our performance is worse than the other methods, as the overall noise increases, the performance of our method is more comparable to the other methods. We record the results in Figure 1b. ", "page_idx": 6}, {"type": "table", "img_path": "a4J7nDLXEM/tmp/95c313c32873966c19d34017c9e284d2cbe260125eed7892187db0ee794f62fc.jpg", "table_caption": [], "table_footnote": ["Table 1: Relative compression on RNA-seq datasets "], "page_idx": 7}, {"type": "table", "img_path": "a4J7nDLXEM/tmp/dc8747177ebdc6146bc8668e807b2dc2f696551a47d6e32c5d36cbcdcb7bf784.jpg", "table_caption": [], "table_footnote": ["Table 2: Average PCA+K-Means outcome before data removal "], "page_idx": 7}, {"type": "text", "text": "This shows that our outlier detection method is adept at detecting different kinds of outliers, outperforming popular outlier detection tools in some settings, and being competitive to them in others. We also observe that as the overall noise in the dataset increases, the performance of our method compared to the other outlier detection tools improves. This further highlights the power of compression ratio when especially dealing with noisy data. Having demonstrated the validity of our outlier detection method in two different settings, across different noise levels, we now focus on real-world datasets. ", "page_idx": 7}, {"type": "text", "text": "4 Real world experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "4.1 Datasets of interest ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we provide experimental results to exhibit the validity of compression ratio as a metric and the usefulness of our outlier detection method in improving the community structure of datasets. We focus on single-cell RNA sequencing datasets. The dataset consists of $n$ many data points, each corresponding to a cell. The features are gene expressions, and for the cell, the expression of some $d\\,\\geq\\,10,000$ genes are recorded. A fundamental problem here is to identify sub-populations of interest. However, the problem is challenging as the biological process of recording gene expressions is error-prone $[\\mathrm{THL^{+}19}]$ , and gene expressions within the same population may also vary due to internal randomness. Furthermore, experiments can cause cells to get merged during gene-expression recording [XL21]. This makes single-cell RNA sequencing data a good testing ground for high dimensional noisy data with outliers and underlying community structure. ", "page_idx": 7}, {"type": "text", "text": "In this direction, we consider the single-cell RNA sequencing datasets from a benchmark paper [DRS20]. These datasets also have moderate to highly reliable ground truth labels, that help us understand the usefulness of our metrics and our algorithm. These datasets vary in the number of cells, genes, clusters, cells per cluster, and the \"difficulty\" of clustering. A summary of the datasets is provided in Table 4 in Appendix E.1. ", "page_idx": 7}, {"type": "text", "text": "4.2 Average compression in the datasets ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "As discussed in Section 2 and described in Theorem 2.5, our primary result showed that the intra-community compression ratios are higher than inter-community compression ratios in a large range of parameters. Here we look at average statistics of compression ratio to provide a first layer of evidence supporting this phenomenon in real-world data. We define the following metric. For any community $V_{j}$ , we define the average intra-community compression ratio as intra $\\begin{array}{r}{\\mathbf{\\rho}_{X,k^{\\prime}}(V_{j})=\\underset{i,i^{\\prime}\\in V_{j}}{\\mathbb{E}}\\left[\\frac{\\lVert\\mathbf{x}_{i}-\\mathbf{x}_{i^{\\prime}}\\rVert}{\\lVert\\Pi_{X}^{k^{\\prime}}(\\mathbf{x}_{i}-\\mathbf{x}_{i^{\\prime}})\\rVert}\\right]}\\end{array}$ Similarly, the average inter-community compression ratio is defined as inter $\\begin{array}{r}{\\mathbf{\\rho}_{X,k^{\\prime}}(V_{j})=\\underset{i\\in V_{j},i^{\\prime}\\in[n]\\backslash V_{j}}{\\mathbb{E}}\\left[\\frac{\\lVert\\mathbf{x}_{i}-\\mathbf{x}_{i^{\\prime}}\\rVert}{\\lVert\\Pi_{X}^{k^{\\prime}}(\\mathbf{x}_{i}-\\mathbf{x}_{i^{\\prime}})\\rVert}\\right]}\\end{array}$ . This gives an average measurement of the compression ratio in the dataset. In this regard, we find that for each of the 9 datasets and each of the communities in the dataset, the intra-community compression ratio is higher than the inter-community compression ratio. We provide the results in the Appendix E.2. Here, for brevity we present the average of intra $X,k\\!-\\!1\\big(V_{j}\\big)$ and inter $X,k\\!-\\!1\\big(V_{j}\\big)$ for each dataset in Table 1. ", "page_idx": 7}, {"type": "text", "text": "4.3 Improvement of clustering results via outlier detection ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Next, we study the usefulness of our outlier detection method in these datasets. Unlike our simulations, there is no ground-truth labeling for outliers. Rather, we assume that in each community (as defined by the labels provided with the dataset), some points may behave more like an outlier, in that they may be a mixture of different signals. These can also be points that have uncharacteristically high noise compared to the rest of the data points. In such a case, these data points may muddle the community structure in the datasets, and thus, removing them may improve the community structure of the datasets. We capture this improvement by observing the change in the accuracy of clustering algorithms when some outlier-like points are removed from the dataset. For our experiments, we choose $\\mathrm{PCA+K}$ -Means as our clustering algorithm, as it is known to be effective in single-cell datasets [VKS17, KAH19]. ", "page_idx": 8}, {"type": "text", "text": "Experimental setup For each of the datasets, we do the following. Let $k$ be the number of communities. We apply some $c$ -dimensional PCA and then run a standard implementation of KMeans with $k$ -centers on the post-PCA data and record the NMI and purity score, which are popular clustering accuracy metrics. This gives us a starting point. Then, for each dataset, we apply 9 outlier detection methods. The algorithms are our variance-of-compression-ratio method, and the original and PCA-added versions of LOF, KNN-dist, Isolation forest, and ECOD. We have two settings. ", "page_idx": 8}, {"type": "text", "text": "First, we set $c=k-1$ (following our theory), and obtain the initial $\\mathrm{PCA+K}$ -means results in Table 2. Then, we remove $5\\%$ of the points according to the outlier score and then run $\\mathrm{PCA+K}.$ -Means on the rest of the dataset and obtain the new NMI values. Next, we repeat the same experiments by removing $10\\%$ of the points. Additionally, we also use $c=2k$ , and there, calculate the outcome only for $10\\%$ points removal, primarily to reduce redundancy. This is to test the sensitivity of the methods to the choice of PCA dimension. ", "page_idx": 8}, {"type": "text", "text": "Results As a comprehensive summary, we calculate the performance rank of the methods on all the datasets in each of the settings. We record the results in Table 3. As can be observed, we obtained the best rank in 5 out of 6 settings. The performance of each method for each dataset in the settings can be found in Appendix E. ", "page_idx": 8}, {"type": "table", "img_path": "a4J7nDLXEM/tmp/9f1bb3e07491ae82f1b9cb5a1b1c28c3fb901d283ce7fbb6f8486223b5d028cb.jpg", "table_caption": [], "table_footnote": ["Table 3: Average rank of improvement across all algorithms and experimental settings "], "page_idx": 8}, {"type": "text", "text": "Robustness to choice of dimension Finally, we note that the compression ratio is not overly sensitive to the choice of PCA dimension, and if we use more dimensions than the number of communities, we still get favorable results. For theoretical support, we show in Section E.4 of the appendix that the compression ratios of most points change only mildly when $k^{\\prime}>k$ . In terms of experiments, we verify it as follows. For $k^{\\prime}=2k$ , we calculate the average intra-community and inter-community compression ratios in Appendix E.4 and find them to be consistent with Table 1. As in the case with PCA dimension= $:k-1$ , our methods have the best performance in terms of improving clustering performance. ", "page_idx": 8}, {"type": "text", "text": "Limitations Finally, we note a few limitations with our outlier removal algorithm. First, the algorithm is dependent on selecting a reasonable removal percentage. While we observed greater NMI improvement with greater removal rates, it is important to understand what is a suitable choice for different datasets. Another concern is that our outlier detection tool may not be optimal for handling highly unbalanced communities, as a very small community will show a lower variance of compression ratio. These remain interesting research directions. We note more future directions in the Appendix F. ", "page_idx": 8}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[AM13] Mohiuddin Ahmed and Abdun Naser Mahmood. A novel approach for outlier detection and clustering improvement. In 2013 IEEE 8th Conference on Industrial Electronics and Applications (iciea), pages 577\u2013582. IEEE, 2013. [And58] TW Anderson. An introduction to multivariate statistical analysis. Wiley google schola, 2:289\u2013300, 1958. $\\mathrm{ARS^{+}04]}$ P. Antonelli, H. E. Revercomb, L. A. Sromovsky, W. L. Smith, R. O. Knuteson, D. C. Tobin, R. K. Garcia, H. B. Howell, H.-L. Huang, and F. A. Best. A principal component noise fliter for high spectral resolution infrared measurements. Journal of Geophysical Research: Atmospheres, 109(D23), 2004. [AS12] Pranjal Awasthi and Or Sheffet. Improved spectral-norm bounds for clustering. In International Workshop on Approximation Algorithms for Combinatorial Optimization, pages 37\u201349. Springer, 2012. [BKNS00] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander. Lof: identifying density-based local outliers. In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000. [CA16] M Emre Celebi and Kemal Aydin. Unsupervised learning algorithms, volume 9. Springer, 2016. [CTP19] Joshua Cape, Minh Tang, and Carey E. Priebe. The two-to-infinity norm and singular subspace geometry with applications to high-dimensional statistics. The Annals of Statistics, 2019. [DH04] Chris Ding and Xiaofeng He. K-means clustering via principal component analysis. In Proceedings of the Twenty-First International Conference on Machine Learning, ICML \u201904, page 29, New York, NY, USA, 2004. Association for Computing Machinery. [DK70] Chandler Davis and W. M. Kahan. The rotation of eigenvectors by a perturbation. iii. SIAM Journal on Numerical Analysis, 7(1):1\u201346, 1970. [DRS20] Angelo Du\u00f2, Mark Robinson, and Charlotte Soneson. A systematic performance evaluation of clustering methods for single-cell rna-seq data. F1000Research, 7:1141, 11 2020. [Han14] Paul Hanoine. An eigenanalysis of data centering in machine learning. Preprint, 2014. $[\\mathrm{HHAN}^{+}21]$ Yuhan Hao, Stephanie Hao, Erica Andersen-Nissen, William M. Mauck III, Shiwei Zheng, Andrew Butler, Maddie J. Lee, Aaron J. Wilk, Charlotte Darby, Michael Zagar, Paul Hoffman, Marlon Stoeckius, Efthymia Papalexi, Eleni P. Mimitou, Jaison Jain, Avi Srivastava, Tim Stuart, Lamar B. Fleming, Bertrand Yeung, Angela J. Rogers, Juliana M. McElrath, Catherine A. Blish, Raphael Gottardo, Peter Smibert, and Rahul Satija. Integrated analysis of multimodal single-cell data. Cell, 2021. $[\\mathrm{HHH}^{+}22]$ Songqiao Han, Xiyang Hu, Hailiang Huang, Minqi Jiang, and Yue Zhao. Adbench: Anomaly detection benchmark. Advances in Neural Information Processing Systems, 35:32142\u201332159, 2022. [HR03] DC Hoyle and M Rattray. Pca learning for sparse high-dimensional data. Europhysics Letters, 62(1):117, 2003. [Jac05] J Edward Jackson. A user\u2019s guide to principal components. John Wiley & Sons, 2005. [KAH19] Vladimir Yu Kiselev, Tallulah S Andrews, and Martin Hemberg. Challenges in unsupervised clustering of single-cell rna-seq data. Nature Reviews Genetics, 20(5):273\u2013282, 2019. [KHK19] Yasunari Kusaka, Takeshi Hasegawa, and Hironori Kaji. Noise reduction in solid-state nmr spectra using principal component analysis. The Journal of Physical Chemistry A, 123(47):10333\u201310338, 2019. PMID: 31682439. ", "page_idx": 9}, {"type": "text", "text": "[KK10] Amit Kumar and Ravindran Kannan. Clustering with spectral norm and the k-means algorithm. In 2010 IEEE 51st Annual Symposium on Foundations of Computer Science, pages 299\u2013308. IEEE, 2010. [Li18] Bingbing Li. A principal component analysis approach to noise removal for speech denoising. In 2018 International Conference on Virtual Reality and Intelligent Systems (ICVRIS), pages 429\u2013432, 2018. [LTZ08] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation forest. In 2008 eighth ieee international conference on data mining, pages 413\u2013422. IEEE, 2008.   \n$[\\mathrm{L}Z\\mathrm{H}^{+}22]$ ] Zheng Li, Yue Zhao, Xiyang Hu, Nicola Botta, Cezar Ionescu, and George H Chen. Ecod: Unsupervised outlier detection using empirical cumulative distribution functions. IEEE Transactions on Knowledge and Data Engineering, 35(12):12181\u201312193, 2022.   \n[LZZ21] Matthias L\u00f6ffler, Anderson Y Zhang, and Harrison H Zhou. Optimality of spectral clustering in the gaussian mixture model. The Annals of Statistics, 49(5):2506\u20132530, 2021.   \n[MBSP12] Y Murali, Murali Babu, Dr Subramanyam, and Dr Prasad. Pca based image denoising. Signal & Image Processing, 3, 04 2012. [MZ23] Xinyu Mao and Jiapeng Zhang. On the power of svd in the stochastic block model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [MZ24] Chandra Sekhar Mukherjee and Jiapeng Zhang. Detecting hidden communities by power iterations with connections to vanilla spectral algorithms. In Proceedings of the 2024 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 846\u2013879. SIAM, 2024. [Nad08] Boaz Nadler. Finite sample approximation results for principal component analysis: A matrix perturbation approach. The Annals of Statistics, 36(6):2791 \u2013 2817, 2008.   \n[Nad14] Raj Rao Nadakuditi. Optshrink: An algorithm for improved low-rank signal matrix denoising by optimal, data-driven singular value shrinkage. IEEE Transactions on Information Theory, 60(5):3002\u20133018, 2014.   \n[RRS00] Sridhar Ramaswamy, Rajeev Rastogi, and Kyuseok Shim. Efficient algorithms for mining outliers from large data sets. In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 427\u2013438, 2000. [RV08] Mark Rudelson and Roman Vershynin. The littlewood\u2013offord problem and invertibility of random matrices. Advances in Mathematics, 218(2):600\u2013633, 2008.   \nRVdBB96] Peter Reimann, Chris Van den Broeck, and Geert J Bex. A gaussian scenario for unsupervised learning. Journal of Physics A: Mathematical and General, 29(13):3521, 1996.   \n$[\\mathrm{THL}^{+}19]$ Xiaoning Tang, Yongmei Huang, Jinli Lei, Hui Luo, and Xiao Zhu. The single-cell sequencing: new developments and medical applications. Cell & Bioscience, 9(1):53, 2019.   \n[TWT21] Francesco Trozzi, Xinlei Wang, and Peng Tao. Umap as a dimensionality reduction tool for molecular dynamics simulations of biomacromolecules: a comparison study. The Journal of Physical Chemistry B, 125(19):5022\u20135034, 2021.   \n[VKS17] K Kirschner V Kiselev and M Schaub. SC3: consensus clustering of single-cell RNA-seq data. Nature Methods, 14:483\u2013486, 2017. [VN17] Namrata Vaswani and Praneeth Narayanamurthy. Finite sample guarantees for pca in non-isotropic and data-dependent noise. In 2017 55th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 783\u2013789. IEEE, 2017. [Vu18] Van Vu. A simple svd algorithm for finding hidden partitions. Combinatorics, Probability and Computing, 27(1):124\u2013140, 2018.   \n[VW15] Van Vu and Ke Wang. Random weighted projections, random quadratic forms and random eigenvectors. Random Structures & Algorithms, 47(4):792\u2013821, 2015.   \n[XL21] Nan Miles Xi and Jingyi Jessica Li. Benchmarking computational doublet-detection methods for single-cell rna sequencing data. Cell systems, 12(2):176\u2013194, 2021.   \n[XT15] Dongkuan Xu and Yingjie Tian. A comprehensive survey of clustering algorithms. Annals of Data Science, 2:165\u2013193, 2015. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "A Organization of the appendix ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In Sectiion B we obtain our first, generic proofs for compression ratio. Next, in Section C we interpret our results through the lens of spatially unique centers, and also prove our variance of compression result on outlier detection in this setting. Next in Section D we extend the results of Section B when number of components is more than $k+1$ . ", "page_idx": 12}, {"type": "text", "text": "Section E contains continuation of experimental results from the main paper. We conclude with some future directions in Section F. ", "page_idx": 12}, {"type": "text", "text": "B Primary theorem and proof ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "In this section, we describe our primary compression ratio related result in the random vector mixture model. We first describe our result when the projection dimension is $k-1$ . We first define some notations and useful results that we will use. ", "page_idx": 12}, {"type": "text", "text": "B.1 Preliminaries ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We first define the SVD projection operator for a matrix $X$ . Let the $k^{\\prime}$ -dimensional SVD projection operator for a matrix $X$ be $P_{X}^{k^{\\prime}}$ . ", "page_idx": 12}, {"type": "text", "text": "Next, for the dataset matrix $X$ , we denote by $Y$ its centered version. Then we have $\\Pi_{X}^{k^{\\prime}}=P_{Y}^{k^{\\prime}}$ .   \nThen the compression ratio of the data pair $(i,i^{\\prime})$ , defined as $\\frac{\\lVert\\mathbf{\\nabla}x_{i}\\!-\\!\\mathbf{x}_{i^{\\prime}}\\rVert}{\\lVert\\Pi^{k^{\\prime}}(\\mathbf{x}_{i}\\!-\\!\\mathbf{x}_{i^{\\prime}})\\rVert}$ is in fact $\\frac{\\lVert\\boldsymbol{y}_{i}-\\boldsymbol{y}_{i^{\\prime}}\\rVert}{\\lVert\\boldsymbol{P}_{Y}^{k^{\\prime}}(\\boldsymbol{y}_{i}-\\boldsymbol{y}_{i^{\\prime}})\\rVert}$ .   \nThen we have the following bound on the compression ratios in the random vector mixture model. ", "page_idx": 12}, {"type": "text", "text": "Theorem B.1 (Main result). Let $X$ be a $d\\times n$ dataset setup in the random vector mixture model with $k$ underlying communities, so that all centers $\\mathbf{c_{j}}$ and all column vectors $\\pmb{x}_{i}\\in X$ are in $[-\\alpha,\\alpha]^{d}$ . Let $Y$ be the corresponding centered dataset. Considering the following notations, ", "page_idx": 12}, {"type": "text", "text": "1 $\\delta_{k^{\\prime}}(M):=s_{k^{\\prime}}(M)-s_{k^{\\prime}+1}(M)\\,f o r\\,a n y\\;M,$   \n2. $\\sigma^{2}$ be the maximum variance of the random variables,   \n3. $\\mathcal{N}:=C_{0}\\sigma\\sqrt{d+n}$ for some constant $C_{0}$ ", "page_idx": 12}, {"type": "text", "text": "$I\\!f\\sigma^{2}\\geq C_{1}\\frac{\\log n}{n}$ for some constant $C_{1}$ then with probability $1-{\\mathcal{O}}(1/n)$ we have that the $(k-1)$ -PC compression ratio, $\\Delta_{X,k-1}(i,i^{\\prime})$ of all intra-cluster pairs $(i,i^{\\prime})$ is lower bounded as ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{X,k-1}(i,i^{\\prime})\\geq}\\\\ &{\\frac{\\sqrt{2d\\sigma_{j}^{2}-12\\alpha\\sqrt{d}\\log n}}{2\\sqrt{2}\\,\\Bigg(\\sigma\\sqrt{k}+C_{1}\\cdot\\alpha\\cdot\\sqrt{\\log n}+\\frac{2\\mathcal{N}\\cdot\\sqrt{\\sigma_{j}^{2}d+12\\sqrt{d}\\log n}}{\\delta_{k-1}(Y)}\\Bigg)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Similarly, the compression ratio of all inter-cluster pairs $(i,i^{\\,\\prime})$ is upper bounded by ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{X,k-1}(i,i^{\\prime})\\leq}\\\\ &{\\frac{\\sqrt{d(\\sigma_{j}^{2}+\\sigma_{j^{\\prime}}^{2})+\\|c_{j}-c_{j^{\\prime}}\\|^{2}+12\\alpha\\sqrt{d}\\log n}}{\\sqrt{2}\\,\\bigg(\\|c_{j}-c_{j^{\\prime}}\\|-2\\,\\bigg(\\sigma\\sqrt{k}+C_{1}\\cdot\\alpha\\cdot\\sqrt{\\log n}+\\frac{2\\sqrt{\\cdot\\sqrt{\\|c_{j}-c_{j^{\\prime}}\\|^{2}+2d\\sigma^{2}+12\\sqrt{d}\\log n}}}{\\delta_{k-1}(Y)}\\bigg)\\bigg)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "with probability $1-{\\mathcal{O}}(1/n)$ . ", "page_idx": 12}, {"type": "text", "text": "Here we make the following remark about the range of the datapoints. ", "page_idx": 12}, {"type": "text", "text": "B.2 Definitions and notations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "We start with the definition of the norm operator $\\|\\cdot\\|$ , which we use in the following two contexts. ", "page_idx": 12}, {"type": "text", "text": "1. If $\\textbf{\\em u}$ is a $d$ dimensional vector, then $\\lVert u\\rVert$ denotes the $\\ell_{2}$ norm of $\\textbf{\\em u}$ , which is $\\sqrt{\\textstyle\\sum_{i=1}^{d}(u_{i})^{2}}$ Then $\\lVert\\boldsymbol{u}-\\boldsymbol{v}\\rVert$ is the $\\ell_{2}$ distance between the two vectors. $M$ ", "page_idx": 13}, {"type": "text", "text": "2. If is a $d\\times n$ matrix $M$ , $\\lVert M\\rVert$ denotes the spectral norm of the matrix. That is, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|M\\|=\\operatorname*{max}_{\\mathbf{u},\\|\\mathbf{u}\\|\\leq1}\\{\\|M\\mathbf{u}\\|\\}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "We follow this by defining some more structures related to random matrices. ", "page_idx": 13}, {"type": "text", "text": "1. For any matrix $M$ , we denote $\\bar{M}:=\\mathbb{E}[M]$ . Then by definition, $\\bar{X}$ is a $d\\times n$ matrix such that if $i\\in V_{j}$ , the $i$ -th column of $\\bar{X}$ is $c_{j}$ (as $\\mathcal{D}^{(j)}$ is a coordinate wise zero mean distribution), the ground truth center of $V_{j}$ . Thus, we denote by $\\bar{X}$ as the ground-truth or expectation matrix of $X$ . Similarly $\\bar{Y}$ is the center matrix of $Y$ (recall that $Y$ is the column centered matrix of $X$ ). Furthermore let ${\\bar{M}}_{i}$ be the $i$ -th column of $\\bar{M}$ . Then $\\|\\bar{y}_{i}-\\bar{y}_{i^{\\prime}}\\|=\\|\\bar{x}_{i}-\\bar{x}_{i^{\\prime}}\\|$ for any $(i,i^{\\prime})$ pair. Thus we can call $\\bar{Y}$ as the ground truth matrix of $Y$ .   \n2. Corresponding to any matrix $M$ , we denote $E_{M}:=M-\\mathbb{E}[M]$ .   \n3. Choice of subscripts: From hereon we use the subscript $i$ to denote the columns of $X$ and $Y$ . We use the subscript $j$ for cluster identities and $\\ell$ for rows of the matrices or the column vectors. ", "page_idx": 13}, {"type": "text", "text": "With this a background, we give a short sketch of the proof. ", "page_idx": 13}, {"type": "text", "text": "Looking at the numerator and denominator separately: Proving the relative compressibility result requires the following results in turn. Recall that the compression ratio is the ratio between pre PCA and post PCA distances between pair of datapoints and we want to lower bound \u201cintracommunity\u201d compression ratio and upper bound \u201cinter-community\u201d compression ratio. This means we need the following bounds to prove Theorem B.1. ", "page_idx": 13}, {"type": "text", "text": "1. For the intra-community pairs of vectors, prove a lower bound on the pre PCA distance and upper bound on the post PCA distances.   \n2. For the inter-community pairs of vectors, prove an upper bound on the pre PCA distance and a lower bound on the post PCA distance. ", "page_idx": 13}, {"type": "text", "text": "We first obtain the pre PCA distance bounds, which are straightforward to obtain using the fact that the randomness in the vectors of $X$ are coordinate wise independent, and that $\\|y_{i}-y_{i^{\\prime}}\\|=\\|x_{i}-x_{i^{\\prime}}\\|$ for any $(i,i^{\\prime})$ pair. ", "page_idx": 13}, {"type": "text", "text": "B.3 Pre PCA distances ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Lemma B.2. Let $\\textit{\\textbf{y}}_{i}$ and $\\pmb{y}_{i^{\\prime}}$ be two vectors (datapoints) of $Y$ with ground truth communities $V_{j}$ and $V_{j^{\\prime}}$ respectively. If $j\\ =\\ j^{\\prime}$ then we have $\\|y_{i}\\,-\\,y_{i^{\\prime}}\\|\\;\\geq\\;\\sqrt{2d\\sigma_{j}^{2}-12\\alpha\\sqrt{d}\\log n}$ with probability $1\\ -\\ {\\cal O}(n^{-3})$ , otherwise if $\\begin{array}{r l r}{j}&{{}\\neq}&{j^{\\prime}}\\end{array}$ then we have $\\begin{array}{r l r}{\\|y_{i}\\ -\\ y_{i^{\\prime}}\\|}&{{}\\leq}&{}\\end{array}$ $\\sqrt{d(\\sigma_{j}^{2}+\\sigma_{j^{\\prime}}^{2})+\\|c_{j}-c_{j^{\\prime}}\\|^{2}+12\\alpha\\sqrt{d}\\log n}$ with probability $1-\\mathcal{O}(n^{-3})$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. We know that for any $(i,i^{\\prime})$ pair $\\|y_{i}-y_{i^{\\prime}}\\|=\\|x_{i}-x_{i^{\\prime}}\\|$ . Using this fact we prove the bounds on the datapoints of $X$ . ", "page_idx": 13}, {"type": "text", "text": "First we consider the case when $\\mathbf{\\Delta}x_{i}$ and $\\pmb{x}_{i^{\\prime}}$ belong to the same community. Then $\\|\\pmb{x}_{i}-\\pmb{x}_{i^{\\prime}}\\|^{2}=$ $\\begin{array}{r}{\\sum_{\\ell=1}^{d}((\\mathbf{\\boldsymbol{x}}_{i})_{\\ell}-(\\mathbf{\\boldsymbol{x}}_{i^{\\prime}})_{\\ell})^{2}}\\end{array}$ . Here for each $\\ell$ we define $\\begin{array}{r}{\\pmb{w}_{\\ell}=(\\pmb{x}_{i})_{\\ell}-(\\pmb{x}_{i^{\\prime}})_{\\ell}=(c_{j})_{\\ell}+(\\pmb{e}_{i})_{\\ell}-(c_{j})_{\\ell}-}\\end{array}$ $(e_{i^{\\prime}})_{\\ell}=(e_{i})_{\\ell}-(e_{i^{\\prime}})_{\\ell}$ . Then $\\begin{array}{r}{\\mathrm{E}\\left[w_{\\ell}^{2}\\right]=\\mathrm{E}[((e_{i})_{\\ell})^{2}]+\\mathrm{E}[((e_{i^{\\prime}})_{\\ell})^{2}]=V a r((e_{i})_{\\ell})+V a r((e_{i^{\\prime}})_{\\ell}).}\\end{array}$ . ", "page_idx": 13}, {"type": "text", "text": "We define $\\sigma_{l,i}^{2}=V a r\\left((e_{i})_{\\ell}\\right)$ (to use the more familiar row major representation). Recall that both $e_{i}$ and $e_{i^{\\prime}}$ are sampled from $\\mathcal{D}^{(j)}$ and $\\sigma_{j}^{2}$ is the average of variance of the coordinates of the distribution $\\mathcal{D}^{(j)}$ . i.e., E $\\begin{array}{r}{\\left[\\sum_{\\ell}\\pmb{w}_{\\ell}^{2}\\right]=\\sum_{\\ell=1}^{d}{\\sigma_{l,i}^{2}}+\\sigma_{l,i^{\\prime}}^{2}=2d\\sigma_{j}^{2}}\\end{array}$ . Now recall that the random variable $\\mathbf{\\nabla}w_{\\ell}$ is in the range $[-2,2]$ for any $\\ell$ . Then applying Hoeffding bound on this setup we get ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\left[\\sum_{\\ell=1}^{d}{\\pmb w}_{\\ell}^{2}\\leq2d\\sigma_{j}^{2}-12\\alpha\\sqrt{d}\\log{n}\\right]\\leq n^{-3}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Thus, if $\\pmb{x}_{i}$ and $\\pmb{x}_{i^{\\prime}}$ belong to the same community then with probability $1-{\\mathcal{O}}(n^{-3})$ we have $\\begin{array}{r}{\\|x_{i}-x_{i^{\\prime}}\\|\\geq\\sqrt{2d\\sigma_{j}^{2}-12\\alpha^{2}\\sqrt{d}\\log n}.}\\end{array}$ ", "page_idx": 14}, {"type": "text", "text": "Similarly, if $\\mathbf{\\Delta}x_{i}$ and $\\mathbf{\\nabla}x_{i^{\\prime}}$ belong to different communities $V_{j}$ and $V_{j^{\\prime}}$ , we have the random variable ${\\pmb w}_{\\ell}=({\\pmb x}_{i})_{\\ell}-({\\pmb x}_{i^{\\prime}})_{\\ell}$ with mean ${(c_{j})_{\\ell}-(c_{j^{\\prime}})_{\\ell}}$ (due to the difference in the centers ) and variance $\\sigma_{l,i}^{2}+\\sigma_{l,j}^{2}$ , where we define $c_{\\ell,j}=(c_{j})_{\\ell}$ . Then $\\operatorname{E}\\left[\\pmb{w}_{\\ell}^{2}\\right]=\\sigma_{l,i}^{2}+\\sigma_{l,j}^{2}+\\left(c_{\\ell,j}-c_{\\ell,j^{\\prime}}\\right)^{2}$ and ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname{E}\\left[\\sum_{\\ell=1}^{d}w_{\\ell}^{2}\\right]=\\sum_{\\ell=1}^{d}\\sigma_{\\ell,i}^{2}+\\sigma_{\\ell,j}^{2}+(c_{\\ell,j}-c_{\\ell,j^{\\prime}})^{2}=d(\\sigma_{j}^{2}+\\sigma_{j^{\\prime}}^{2})+\\lVert\\mathbf{c_{j}}-\\mathbf{c_{j^{\\prime}}}\\rVert^{2}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Applying Hoeffding bound we get ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{Pr}\\left[\\sum_{\\ell=1}^{d}w_{\\ell}^{2}\\geq d(\\sigma_{j}^{2}+\\sigma_{j^{\\prime}}^{2})+\\|c_{j}-c_{j^{\\prime}}\\|^{2}+12\\alpha\\sqrt{d}\\log n\\right]\\leq n^{-3}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus if $\\scriptstyle{\\mathbf{{x}}_{i}}$ and $\\pmb{x}_{i^{\\prime}}$ belong to different communities then with probability $1-n^{-3}$ we have $\\|x_{i}-$ $\\displaystyle x_{i^{\\prime}}\\|\\leq\\sqrt{d(\\sigma_{j}^{2}+\\sigma_{j^{\\prime}}^{2})+\\|c_{j}-c_{j^{\\prime}}\\|^{2}+12\\alpha\\sqrt{d}\\log n}.$ ", "page_idx": 14}, {"type": "text", "text": "Now, we move into the analysis of post-PCA distances, which is the more technical part of the proof. ", "page_idx": 14}, {"type": "text", "text": "B.4 Post PCA distance ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "High-level idea. The idea behind the proof is simple. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In our setup, $\\bar{X}=\\mathbb{E}[X]$ is the ground truth matrix, such that if the $i$ -th column of $X$ belongs to $V_{j}$ , then the $i$ -th column of $\\bar{X}$ is $\\mathbf{c_{j}}$ . Thus, $\\bar{X}$ is rank $k$ and thus $\\|P_{\\bar{X}}^{k}(\\mathbf{c_{j}}-\\mathbf{c_{j^{\\prime}}})\\|=\\|\\mathbf{c_{j}}-\\mathbf{c_{j^{\\prime}}}\\|$ . This implies $\\bar{Y}$ has rank $k-1$ and $\\|P_{\\bar{Y}}^{k-1}(\\mathbf{c_{j}}-\\mathbf{c_{j^{\\prime}}})\\|=\\|\\mathbf{c_{j}}-\\mathbf{c_{j^{\\prime}}}\\|$ . The crux of the proof is to show tnhoaits $P_{\\bar{Y}}^{k-1}$ can be well approximated with $P_{Y}^{k-1}$ , even when $Y$ and $\\bar{Y}$ differ significantly (due to the ", "page_idx": 14}, {"type": "text", "text": "To achieve this result we use tools from spectral analysis of random matrices, i.e. tools that study the behavior of eigenvalue and eigenvectors of random matrices. Here we face two hurdles. ", "page_idx": 14}, {"type": "text", "text": "1. First we note that the matrix $Y$ is rectangular and unsymmetric. The vast majority of tools in spectral analysis of random matrix theory are focused on symmetric matrices. To use this to our advantage we focus on a closely related symmetric matrix through the following symmetrization trick, which is essential to the proof. We define the matrix $Z$ as $Z:={\\binom{\\bar{0}}{Y^{T}}}\\quad Y\\quad$ . This is a $d+n\\times d+n$ symmetric matrix. We show that $P_{Y}^{k-1}$ can be analyzed through $P_{Z}^{k-1}$ and then approximate the second projection operator using $P_{\\mathbb{E}[Z]}^{k-1}$ , borrowing tools from classical random matrix theory. This gives us preliminary post PCA distance bounds expressed using $\\|Y-\\bar{Y}\\|$ .   \n2. Then obtaining the exact bounds of Theorem B.1 require bounds on the spectral norm of $Y-{\\bar{Y}}$ . There exists a rich literature on spectral norm of random symmetric matrices with independent entries, but $Y-{\\bar{Y}}$ does not satisfy this either. This is because, since $Y$ is obtained by subtracting the column mean from each vector of $X$ , the entries of $Y$ , and thus $Y-{\\bar{Y}}$ are not independent either. To this end, we first obtain the said properties for $X-\\mathbb{E}[X]$ borrowing tools from $[\\mathrm{Vu}18]$ on our symmetrization trick, and then accommodate for the effect of centering using results from [Han14] to complete our proof. ", "page_idx": 14}, {"type": "text", "text": "We now describe the symmetrization trick and its implications in detail. ", "page_idx": 14}, {"type": "text", "text": "B.4.1 A comparable symmetric case ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We start by recalling the symmetric matrix corresponding to $Y$ , $Z={\\left[\\begin{array}{l l}{0}&{Y}\\\\ {Y^{T}}&{0}\\end{array}\\right]}$ As per our notations we denote $\\bar{Z}=\\mathbb{E}[Z]$ and then we have $\\bar{Z}=\\left[\\!\\!\\begin{array}{l l}{0}&{\\bar{Y}}\\\\ {\\bar{Y}^{T}}&{0}\\end{array}\\!\\!\\right]$ Furthermore we have $E_{Z}=Z-\\bar{Z}$ . ", "page_idx": 14}, {"type": "text", "text": "Then the eigenvectors of $Z$ and singular vectors of $Y$ (and similarly $\\bar{Z}$ and $\\bar{Y}$ ) are related as follows. Fact B.3. Let the left and right singular vectors of $Y$ be $\\hat{l}_{t},1\\leq t\\leq d$ and $\\hat{\\pmb{r}}_{t}$ $_{!},1\\leq t\\leq n$ respectively. Then the eigenvectors of Z are\u221a12 lr\u02c6\u02c6tt with eigenvalue \u03bb\u02c6t = st and\u221a12 \u2212ltr\u02c6 with eigenvalue $\\hat{\\lambda}_{t}=-s_{t}$ where $1\\leq t\\leq\\operatorname*{min}(d,n)$ , The same follows for $\\bar{Y}$ and $\\bar{Z}$ . ", "page_idx": 15}, {"type": "text", "text": "Here we also formally define $P_{M}^{k}$ for symmetric matrices $M$ as in this case we work with eigenvectors corresponding to top eigenvalues, instead of top singular values (as in case of $Y$ ), for clarity. ", "page_idx": 15}, {"type": "text", "text": "Remark B.4. For any matrix $M$ , we have defined $P_{X}^{k^{\\prime}}$ as the matrix whose rows are the top $k^{\\prime}$ singular vectors of $M$ . ", "page_idx": 15}, {"type": "text", "text": "However, when we discuss a symmetric matrix $M^{\\prime}$ , $P_{M^{\\prime}}^{k^{\\prime}}$ is a matrix whose rows are the eigenvectors corresponding to the top $k^{\\prime}$ eigenvalues of $M^{\\prime}$ . ", "page_idx": 15}, {"type": "text", "text": "This in turn gives us the following results connecting $P_{Y}^{k^{\\prime}}$ and $P_{Z}^{k^{\\prime}}$ . Fact B.5. Let $0^{n}$ be the $n$ dimensional zero vector. Furthermore let $v|0^{n}:={\\binom{v}{0^{n}}}$ for any vector $\\pmb{v}$ . Then for any $d$ -dimensional vector $\\pmb{v}$ we have $\\|P_{Y}^{k^{\\prime}}\\pmb{v}\\|=\\sqrt{2}\\left\\|P_{Z}^{k}(\\pmb{v}|0^{n})\\right\\|$ ", "page_idx": 15}, {"type": "text", "text": "This result allows us to work with the symmetric matrices $Z$ and $\\bar{Z}$ instead of $Y$ . Now we obtain the   \nresults needed to approximate $P_{Z}^{k^{\\prime}}$ with P k\u00af. Z ", "page_idx": 15}, {"type": "text", "text": "Difference in spectral projections of $\\bar{Z}$ and $Z$ : Here we use the Davis-Kahan Theorem [DK70] along with a result by Cape et. al. [CTP19] to obtain an upper bound between the norm of difference of the leading eigenspaces of $Z$ and $\\bar{Z}$ under some appropriate orthonormal rotation that we shall use to obtain our results. The main reason behind using these tools is that the SVD projection matrix due to $\\bar{Z}$ is well behaved. ", "page_idx": 15}, {"type": "text", "text": "Theorem B.6 (Davis-Kahan Theorem: [DK70]). Let $D$ and $\\hat{D}$ be $p\\times p$ symmetric matrices, with eigenvalues $\\lambda_{1},\\ldots,\\lambda_{p}$ and $\\hat{\\lambda}_{1},...\\,\\hat{\\lambda}_{p}$ respectively. Define $E_{D}=\\hat{D}-D$ and $\\delta_{k^{\\prime}}=\\lambda_{k^{\\prime}}-\\lambda_{k^{\\prime}+1},1\\le$ $k<p$ . Let $U=[\\pmb{u}_{1}\\cdot\\cdot\\cdot,\\pmb{u}_{k^{\\prime}}]$ and $\\hat{U}=[\\hat{u}_{1}\\,.\\,.\\,.\\,,\\hat{u}_{k^{\\prime}}]$ are matrices in $\\mathbb{R}^{p\\times k^{\\prime}}$ where $\\mathbf{\\nabla}u_{i}$ and $\\hat{u_{i}}$ are eigenvectors of $D$ and $\\hat{D}\\ w.r.t$ to the $i$ -th top eigenvalue. Then ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\|\\sin\\Theta\\left(U,\\hat{U}\\right)\\right\\|\\leq\\frac{2\\|E_{D}\\|}{\\delta_{k^{\\prime}}}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Theorem B.7 (Perturbation under Procrustes Transformation: [CTP19]). Let $U$ and $\\hat{U}$ be two $p\\times k^{\\prime}$ matrices such that the columns of $U$ (and similarly $\\hat{U}$ ) comprise of $k^{\\prime}$ many unit vectors that are mutually orthogonal. ", "page_idx": 15}, {"type": "text", "text": "Then there exists a $k^{\\prime}\\times k^{\\prime}$ orthonormal matrix $W_{U}$ such that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\|\\sin\\Theta\\left(U,\\hat{U}\\right)\\right\\|\\leq\\left\\|U-\\hat{U}W_{U}\\right\\|\\leq\\sqrt{2}\\left\\|\\sin\\Theta\\left(U,\\hat{U}\\right)\\right\\|\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Combining them we get the following result. ", "page_idx": 15}, {"type": "text", "text": "Theorem B.8. Given the matrices $Y$ and $\\bar{Y}$ and $Z$ and $\\bar{Z}$ defined as described above, there exists an orthonormal matrix $W_{Z}$ such that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\|(P_{Z}^{k^{\\prime}})^{T}-(P_{\\bar{Z}}^{k^{\\prime}})^{T}(W_{Z})^{T}\\right\\|\\leq\\frac{2\\|E_{Z}\\|}{\\delta_{k^{\\prime}}(Y)}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This in turn implies ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left\\|P_{Z}^{k^{\\prime}}-W_{Z}P_{\\bar{Z}}^{k^{\\prime}}\\right\\|\\leq\\frac{2\\|E_{Z}\\|}{\\delta_{k^{\\prime}}(Y)}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Next, we obtain a result on the projection of a random vector on a $k^{\\prime}$ dimensional subspace. ", "page_idx": 15}, {"type": "text", "text": "B.4.2 Random Projection ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Now, we derive a strong bound for $\\|P_{M}^{k}e\\|$ where $e\\in\\mathbb{R}^{d}$ is a coordinate wise independent random vector with me\u221aan $0^{d}$ and $M$ is any $d\\times n$ a non-negative matrix. We essentially show that for any $\\|P_{M}^{k}e\\|=\\mathcal{O}(\\sqrt{k})$ even though $e=\\Omega({\\sqrt{d}})$ with high probability. Here an important condition to be satisfied is that $M$ and $^e$ are independent. ", "page_idx": 16}, {"type": "text", "text": "Then the projection matrix $P_{M}^{k}$ is a set of $k$ -orthonormal unit vectors $p_{t},1\\leq t\\leq k$ . Then the length of a projected vector $\\|P_{M}^{k}e\\|$ can be written down as ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\left\\|P_{M}^{k}e\\right\\|^{2}=\\sum_{t=1}^{k}\\left((p_{t})^{T}e\\right)^{2}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Here one can do an entry-wise analysis of the terms $\\left((p_{t})^{T}e\\right)^{2}$ but that forces a bound of the form that $\\left\\|P_{M}^{k}e\\right\\|\\leq\\sqrt{k}(\\sigma+\\sqrt{16\\log(n k)})$ with probability $1-{\\mathcal{O}}(n^{-3})$ . Instead, we recall a result from the Vu [VW15]. ", "page_idx": 16}, {"type": "text", "text": "Lemma B.9 ([VW15]). There are constants $C_{0},C_{1}$ such that the following happens. Let e be a random vector in $\\mathbb{R}^{d}$ such that its coordinates are independent random variables with 0 mean and variance $\\sigma^{2}$ . Assume furthermore that the coordinates are bounded by $\\alpha$ in their absolute value. Let $H$ be a subspace of dimension $k$ and let $\\Pi_{H}(e)$ be the length of the orthogonal projection of e onto $H$ . Then for any $n$ we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(\\Pi_{H}(e)\\geq\\sigma{\\sqrt{k}}+C_{1}\\alpha{\\sqrt{\\log n}}\\right)\\leq n^{-3}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Now, let us consider the case where $H$ is the subspace covered by the top $k$ many orthonormal eigenvectors of $M$ , denoted as $p_{t},1\\leq t\\leq k$ . Then the projection of $^e$ onto $H$ can be written as $\\textstyle\\sum_{t=1}^{k}\\langle p_{t},e\\rangle p_{t}$ . Then we have $\\begin{array}{r}{\\left(\\Pi_{H}(e)\\right)^{2}\\,=\\,\\sum_{t=1}^{k}\\langle p_{t},e\\rangle^{2}\\,=\\,\\sum_{t=1}^{k}\\left((p_{t})^{T}e\\right)^{2}\\,}\\end{array}$ . This is exactly $\\big|\\big|P_{M}^{k}(e)\\big|\\big|^{2}$ . Summarizing, we get the following result with respect to the matrix $\\bar{Z}$ . ", "page_idx": 16}, {"type": "text", "text": "Corollary B.10. Let $P_{\\bar{Z}}^{k^{\\prime}}$ be as defined above. Let e be a $d$ -dimensional random vector with each entry having zero mean and variance at most $\\sigma^{2}$ . Then with probability $1-n^{-3}$ we have, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|P_{\\bar{Z}}^{k}(e)\\|\\leq\\sigma\\sqrt{k}+C_{1}\\cdot\\alpha\\cdot\\sqrt{\\log n}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "We are now in a position to obtain our preliminary pots PCA distance bounds when the projection dimension is $k-1$ . ", "page_idx": 16}, {"type": "text", "text": "B.4.3 Preliminary post PCA bounds ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Preliminary intra-community bounds: We start by obtaining the post PCA distance $\\textstyle\\|\\Pi_{Y}^{k-1}(y_{i}-$ $\\left.y_{i^{\\prime}}\\right)\\right|$ where both $\\scriptstyle\\pmb{y}_{i}$ and $\\mathbf{\\mathit{y}}_{i^{\\prime}}$ belong to the same community $V_{j}$ . ", "page_idx": 16}, {"type": "text", "text": "Lemma B.11. Let $\\textit{\\textbf{y}}_{i}$ and $\\pmb{y}_{i^{\\prime}}$ be two columns of the data matrix $Y$ belonging to the same community $V_{j}$ . Then for some constants $C_{1}$ we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|P_{Y}^{k-1}(y_{i}-y_{i^{\\prime}})\\|\\leq2\\sqrt{2}\\frac{\\|Z-\\bar{Z}\\|\\cdot\\|y_{i}-y_{i^{\\prime}}\\|}{\\delta_{k-1}(Y)}+2\\sqrt{2}\\left(\\sigma\\sqrt{k}+C_{1}\\cdot\\alpha\\cdot\\sqrt{\\log n}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "with probability $1-\\mathcal{O}(n^{-3})$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. Initially we have $\\|P_{Y}^{k-1}({\\pmb y}_{i}-{\\pmb y}_{i^{\\prime}})\\|=\\sqrt{2}\\|P_{Z}^{k-1}({\\pmb y}_{i}|0^{n}-{\\pmb y}_{i^{\\prime}}|0^{n})\\|$ . Here we use the facts that the spectral projection operators due to $Z$ and $\\bar{Z}$ are close up to some orthonormal rotation and that $\\bar{Z}$ and ${\\pmb y}_{i}-{\\pmb y}_{i^{\\prime}}$ are independent. Furthermore $y_{i}-y_{i^{\\prime}}=\\mathbf{c_{j}}+e_{i}-c_{X}-\\mathbf{c_{j^{\\prime}}}-e_{i^{\\prime}}+c_{X}=e_{i}-e_{i^{\\prime}}$ , where $c_{X}$ is the centering vector which is a zero mean random vector. ", "page_idx": 16}, {"type": "text", "text": "Then we have for any $k-1$ dimensional orthonormal matrix $W$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P_{Z}^{k-1}(y_{i}|0^{n}-y_{i^{\\prime}}|0^{n})\\|\\leq\\|(P_{Z}^{k-1}-W P_{Z}^{k-1})(y_{i}|0^{n}-y_{i^{\\prime}}|0^{n})\\|+\\|W P_{\\bar{Z}}^{k-1}(y_{i}|0^{n}-y_{i^{\\prime}}|0^{n})\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\|P_{Z}^{k-1}-W P_{\\bar{Z}}^{k-1}\\|\\cdot\\|y_{i}|0^{n}-y_{i^{\\prime}}|0^{n}\\|+\\|W P_{\\bar{Z}}^{k-1}(e_{i}|0^{n}-e_{i^{\\prime}}|0^{n})\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\|P_{Z}^{k-1}-W P_{\\bar{Z}}^{k-1}\\|\\cdot\\|y_{i}|0^{n}-y_{i^{\\prime}}|0^{n}\\|+\\|W P_{\\bar{Z}}^{k-1}e_{i}|0^{n}\\|+\\|W P_{\\bar{Z}}^{k-1}e_{i}\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "From Theorem B.8 we have that for a choice of $\\begin{array}{r}{W\\;\\|P_{Z}^{k-1}-W P_{\\bar{Z}}^{k-1}\\|\\le\\frac{2\\|Z-\\bar{Z}\\|}{\\delta_{k-1}(Z)}=\\frac{2\\|Z-\\bar{Z}\\|}{\\delta_{k-1}(Y)}.}\\end{array}$ ", "page_idx": 17}, {"type": "text", "text": "Next, we can analyze $\\lVert W P_{\\bar{Z}}^{k-1}e_{i}|0^{n}\\rVert+\\lVert W P_{\\bar{Z}}^{k-1}e_{i^{\\prime}}|0^{n}\\rVert$ as $\\bar{Z}$ and the vectors are independent of each other. Then applyin\u221ag Corollary B.\u221a10 with probability $1-\\mathcal{O}(n^{-3})$ we have $\\|W P_{\\bar{Z}}^{k-1}e_{i}|0^{n}\\|+$ $\\|W P_{\\bar{Z}}^{k-1}e_{i^{\\prime}}|0^{n}\\|\\leq2\\sigma\\sqrt{k-1}+2C_{1}\\sqrt{\\log n}$ . This completes the proof. ", "page_idx": 17}, {"type": "text", "text": "Preliminary inter-community bounds. Now, we move to the inter-community results. In this part $\\underline{{P}}_{\\bar{Z}}^{k-1}$ plays an important role. This is because as per our discussion $\\|P_{\\bar{Y}}^{k-1}(\\mathbf{c_{j}}-\\mathbf{c_{j^{\\prime}}})\\|=\\|\\mathbf{c_{j}}-\\bar{\\mathbf{c}}_{\\mathbf{j^{\\prime}}}\\|$ . This implies ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|P_{\\bar{Z}}^{k-1}(\\bar{Y}_{i}|0^{d}-\\bar{Y}_{i^{\\prime}}|0^{d})\\|=\\|{\\bf c_{j}}-{\\bf c_{j^{\\prime}}}\\|\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Using this result we then prove the following inter-community post PCA bound. ", "page_idx": 17}, {"type": "text", "text": "Lemma B.12. Let $\\pmb{y}_{i},\\pmb{y}_{i^{\\prime}}$ be two columns of the data matrix $Y$ so that $i\\in V_{j}$ and $i^{\\prime}\\in V_{j^{\\prime}}$ , where $j\\neq j^{\\prime}$ . Then for the constant $C_{1}$ we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n|P_{Y}^{k-1}(y_{i}-y_{i^{\\prime}})||\\geq\\sqrt{2}\\left(\\|\\mathbf{c_{j}}-\\mathbf{c_{j^{\\prime}}}\\|-2\\left(\\sigma\\sqrt{k}+C_{1}\\cdot\\alpha\\cdot\\sqrt{\\log n}\\right)-\\frac{2\\|Z-\\bar{Z}\\|\\cdot\\|y_{i}-y_{i^{\\prime}}\\|}{\\delta_{k-1}(Y)}\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with probability $1-\\mathcal{O}(n^{-3})$ . ", "page_idx": 17}, {"type": "text", "text": "Proof. As before we have $\\|P_{Y}^{k-1}(\\pmb{y}_{i}-\\pmb{y}_{i^{\\prime}})\\|=\\sqrt{2}\\|P_{Z}^{k-1}(\\pmb{y}_{i}|0^{n}-\\pmb{y}_{i^{\\prime}}|0^{n})\\|.$ . Then we proceed with a basic decomposition. We have for any $k-1$ dimensional matrix $W$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\|P_{Z}^{k-1}(y_{i}|0^{n}-y_{i^{\\prime}}|0^{n})\\|}\\\\ &{\\geq\\!\\|W P_{Z}^{k-1}(\\mathbf{c_{j}}|0^{n}-\\mathbf{c_{j^{\\prime}}}|0^{n})\\|-\\|W P_{Z}^{k-1}(e_{i}|0^{n}-e_{i^{\\prime}}|0^{n})\\|-\\|(P_{Z}^{k-1}-W P_{Z}^{k-1})(y_{i}|0^{n}-y_{i^{\\prime}}|0^{n})\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$\\|W P_{\\bar{Z}}^{k-1}(\\mathbf{c_{j}}|0^{n}-\\mathbf{c_{j^{\\prime}}}|0^{n})\\|=\\|P_{\\bar{Y}}^{k-1}(\\mathbf{c_{j}}-\\mathbf{c_{j^{\\prime}}})\\|=\\|c_{j}-c_{j^{\\prime}}\\|.$ ", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Next from Lemma B.11 we have $\\|W P_{\\bar{Z}}^{k-1}(e_{i}|0^{n}-e_{i^{\\prime}}|0^{n})\\|\\leq2\\left(\\sigma\\sqrt{k}+C_{1}\\sqrt{\\log n}\\right)$ with probability $1-\\mathcal{O}(n^{-3})$ . ", "page_idx": 17}, {"type": "text", "text": "Finally from Lemma B.11 we know we can upper bound $\\|(P_{Z}^{k-1}-W P_{\\bar{Z}}^{k-1})(y_{i}|0^{n}-y_{i^{\\prime}}|0^{n})\\|$ with $\\begin{array}{r}{\\frac{2\\|Z-\\bar{Z}\\|}{\\delta_{k-1}(Y)}\\cdot\\|y_{i}-y_{i^{\\prime}}\\|}\\end{array}$ , which completes the proof. ", "page_idx": 17}, {"type": "text", "text": "At this point, we have obtained the pairwise post-PCA intra-community and inter-community distance bounds in terms of $\\lVert y_{i}-y_{i^{\\prime}}\\rVert,\\lVert\\dot{Z}-\\bar{Z}\\rVert,\\dot{k},\\sigma$ and $\\delta_{k-1}(Y)$ . Here $\\bar{\\delta}_{k-1}(Y)$ is the spectral gap of $Y$ and we already have bounds on $\\left||y_{i}-y_{i^{\\prime}}|\\right|$ . Next, we obtain bounds on $\\lVert Z-\\bar{Z}\\rVert$ and then put together the results obtained so far to prove Theorem B.1. ", "page_idx": 17}, {"type": "text", "text": "B.4.4 Spectral norm of the square marrix ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "First, we note down a result by $\\mathrm{Vu}\\ [\\mathrm{Vu}18]$ for upper bounds on the spectral norm of random matrices with independent entries. ", "page_idx": 17}, {"type": "text", "text": "Theorem B.13 (Norm of random symmetric matrix $\\mathrm{[Vul}8]$ ). Let $E$ be a $n\\times n$ random symmetric matrix where each entry in the upper diagonal is an independent random variable with 0 mean and $\\sigma$ variance, then there is a constant $C_{0}$ such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\|E\\|\\geq C_{0}\\sigma{\\sqrt{n}}\\right]\\leq n^{-3}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\sigma^{2}\\geq C_{1}\\frac{\\log n}{n}$ . ", "page_idx": 17}, {"type": "text", "text": "However, since the entries of $Y$ are not independent, the same follows with $E_{Z}$ . To bypass this issue we define the matrix $B:=\\left[\\!\\!\\begin{array}{l l}{0}&{X}\\\\ {X^{T}}&{0}\\end{array}\\!\\!\\right]$ and then ${\\bar{B}}:=\\mathbb{E}[B]={\\biggl[}{\\begin{array}{l l}{0}&{\\bar{X}}\\\\ {\\bar{X}^{T}}&{0}\\end{array}}{\\biggr]}$ Furthermore recall that $E_{M}=M-\\mathbb{E}[M]$ . Then we have the following results. ", "page_idx": 17}, {"type": "text", "text": "1. $\\|E_{Z}\\|$ is the largest eigenvalue of $E_{Z}$ , which is same as the largest singular value of $E_{Y}$ , that we denote as $s_{1}(\\bar{E_{Y}})$ . 2. $\\|E_{B}\\|$ is same as the largest singular value of $E_{X}$ , that we denote as $s_{1}(E_{X})$ . ", "page_idx": 18}, {"type": "text", "text": "Furthermore we have from Theorem B.13 that $\\|E_{B}\\|\\,\\leq\\,C_{0}\\sigma{\\sqrt{n}}$ with probability $1-{\\mathcal{O}}(n^{-3})$ . Finally we connect $s_{1}(E_{Y})$ with $s_{1}(E_{X})$ . To do so, note that $E_{Y}$ is the centered matrix of $E_{X}$ . This follows from the fact that $E_{Y}=Y-{\\bar{Y}}$ and $E_{X}=X-{\\bar{X}}$ . Then we use the following result by Hanoine [Han14]. ", "page_idx": 18}, {"type": "text", "text": "Theorem B.14 ([Han14]). Let $M$ be a rank m matrix and $\\bar{M}$ be the matrix obtained upon centering, with singular values (in descending order) $s_{1},\\ldots,s_{m}$ and $\\bar{s}_{1},\\dotsc,\\bar{s}_{m-1}^{\\prime}$ respectively. Then for any $1\\leq i<m$ we have $s_{i}\\geq s_{i}^{\\prime}\\geq s_{i+1}$ . ", "page_idx": 18}, {"type": "text", "text": "Using this result we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|E_{Z}\\|=s_{1}(E_{Y})\\leq s_{1}(E_{X})\\leq\\|E_{B}\\|\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Now, we bound $\\|E_{B}\\|$ , i.e. $\\left\\|\\left[{0\\atop(E_{X})^{T}}\\right.\\right.{\\cal E}_{X}\\right]\\right\\|$ . This is a $(d\\!+\\!n)\\times(d\\!+\\!n)$ random symmetric matrix with zero mean and maximum variance $\\sigma^{2}$ . Then applying Theorem B.13 we get the following bound. ", "page_idx": 18}, {"type": "text", "text": "Lemma B.15. Recall that we define $\\mathcal{N}=C_{0}\\sigma\\sqrt{d+n}$ . Then in the setting of Lemma B.11 we have $\\lVert Z-\\bar{Z}\\rVert\\leq\\mathcal{N}$ with probability $1-\\mathcal{O}(n^{-3})$ ", "page_idx": 18}, {"type": "text", "text": "Against this backdrop we summarize our bounds to prove Theorem B.1. ", "page_idx": 18}, {"type": "text", "text": "B.5 Proof of Theorem B.1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "From Lemma B.2 we have the lower bound on the intra-community distances and upper bound on the inter-community distances. Similarly, we can also use the results to obtain lower bound for the intra-community case. It is easy to see that if (i,i\u2019) belong to the same community $V_{j}$ then with probability $1-{\\mathcal O}(n^{-3}),\\|y_{i}-y_{i}\\|\\leq\\sqrt{2d\\sigma_{j}^{2}+12\\alpha\\sqrt{d}\\log n}.$ ", "page_idx": 18}, {"type": "text", "text": "Substituting this and the bound on $\\lVert Z-\\bar{Z}\\rVert$ to Lemma B.11 we have with probability $1-\\mathcal{O}(n^{-3})$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|P_{Y}^{k-1}(y_{i}-y_{i^{\\prime}})\\|\\leq2\\sqrt{2}\\left(\\sigma\\sqrt{k}+C_{1}\\cdot\\alpha\\cdot\\sqrt{\\log n}+\\frac{\\mathcal{N}\\cdot\\sqrt{2d\\sigma_{j}^{2}+12\\alpha\\sqrt{d}\\log n}}{\\delta_{k-1}(Y)}\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Similarly for the inter-community with $i\\in V_{j},i^{\\prime}\\in V_{j^{\\prime}}$ from Lemma B.12 we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nP_{Y}^{k-1}(y_{i}-y_{i^{\\prime}})\\|\\geq\\sqrt{2}\\left(\\|\\mathbf{c_{j}}-\\mathbf{c_{j^{\\prime}}}\\|-2\\left(\\sigma\\sqrt{k}+C_{1}\\cdot\\alpha\\cdot\\sqrt{\\log n}\\right)-\\frac{\\mathcal{N}\\cdot\\sqrt{\\|\\mathbf{c_{j}}-\\mathbf{c_{j^{\\prime}}}\\|^{2}+d(\\sigma_{j}^{2}+\\sigma_{i^{\\prime}}^{2})}}{\\delta_{k-1}(Y)}\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Finally, using the bounds of Lemma B.2 and applying a union bound on the total n2pairs of datapoints completes the proof of the theorem. ", "page_idx": 18}, {"type": "text", "text": "C Spatially unique centers and proof for the outlier detection theorem ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The primary quantity that is hard to interpret in a dataset with an underlying community structure is $\\delta_{k-1}(Y)$ . Here we make some observations. First note that $\\bar{\\delta}_{k-1}\\bar{(Y)}\\ {\\overset{*}{=}}\\ s_{k-1}(Y{\\overset{.}{)}}-s_{k}(Y)$ . Now, $s_{k-1}(Y)\\ \\geq\\ s_{k}(X)$ and $s_{k}(Y)\\;\\leq\\;C\\sigma{\\sqrt{d+n}}$ where the latter term comes from the fact that $s_{k}(Y)\\,\\leq\\,\\|E_{Y}\\|\\,\\leq\\,\\|E_{X}\\|\\,\\leq\\,C\\sigma{\\sqrt{d+n}}$ . This follows from a simple application of Weyl\u2019s inequality and \u221athe effect of centering on eigenvalues. For simplicity, we consider the case when $s_{k}(X)\\geq4C\\sigma{\\sqrt{d+n}}$ . We will come back to this and show that this assumption does make sense. Then, we have $\\delta_{k-1}(Y)\\geq0.66s_{k}(X)$ . Next note that $s_{k}(X)\\geq s_{k}(\\mathbb{E}[X])-{\\big\\|}E\\|$ . ", "page_idx": 18}, {"type": "text", "text": "This then implies that given the aforementioned conditions, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\delta_{k-1}\\geq0.25s_{k}(\\mathbb{E}[X])\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\mathbb{E}[X]$ is the center matrix, where each column is the center of the community the corresponding point belongs to. ", "page_idx": 18}, {"type": "text", "text": "Bounds on the singular values of the center matrix for $\\gamma$ -spatially unique centers Here, we make a connection between $s_{k}(\\mathbb{E}[X])$ and the notion of spatially unique centers. ", "page_idx": 19}, {"type": "text", "text": "Given a $n_{1}\\times n_{2}$ matrix $M$ , we define the minimum hyperplane distance, $\\mathsf{d i s t}_{\\mathsf{M}}$ as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathsf{d i s t}_{\\mathsf{M}}=\\operatorname*{min}_{j}\\operatorname*{min}_{\\mathbf{v}\\in\\mathsf{S p a n}(M_{-j})}\\|M_{j}-\\mathbf{v}\\|\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $M_{j}$ represents the $j$ -th column of $M$ and $M_{-j}$ denotes the set of all columns of $M$ except the $j$ -th one.That is, it denotes the minimum distance between a data point and the span of the rest of the data points. ", "page_idx": 19}, {"type": "text", "text": "We have the following classic result of matrix theory. ", "page_idx": 19}, {"type": "text", "text": "Lemma C.1 ([RV08]). For any $n_{1}\\times n_{2}$ matrix $M$ , the smallest singular value $s_{\\mathrm{min}}(M)$ is lower bounded by ${\\frac{1}{\\sqrt{n_{2}}}}\\cdot{\\mathsf{d i s t}}_{\\mathsf{M}}$ . ", "page_idx": 19}, {"type": "text", "text": "Now, this result does not directly help us as $\\mathbb{E}[X]$ has multiple identical columns (it is after all a $d\\times n$ rank $k$ matrix) and we only get a lower bound of 0. However, we can do a simple two-step analysis to get something nicer. ", "page_idx": 19}, {"type": "text", "text": "Consider the matrix $\\hat{C}$ which contains $k$ columns that are each copy of one of the centers of $X$ . Then from the definition of $\\gamma$ -spatially unique centers, we immediately have the following. ", "page_idx": 19}, {"type": "text", "text": "Fact C.2. If $X$ comes from a setup with $\\gamma.$ -spatially unique centers then $\\begin{array}{r}{s_{k}(\\hat{C})=s_{\\mathrm{min}}(\\hat{C})\\geq\\frac{\\gamma}{\\sqrt{k}}}\\end{array}$ . ", "page_idx": 19}, {"type": "text", "text": "Next, let the size of the underlying communities in $X$ . Then we know that $\\mathbb{E}[C]$ has at least $\\operatorname*{min}_{j}|V_{j}|$ many copies of $\\hat{C}$ in it (along with other columns corresponding to the larger communities). That means that the singular values in $\\mathbb{E}[X]$ is at least $\\sqrt{\\operatorname*{min}_{j}|V_{j}|}$ times the singular values in $\\hat{C}$ . This gives us the following result. ", "page_idx": 19}, {"type": "text", "text": "Lemma C.3. Let $X$ be generated from $\\gamma$ -spatially unique centers and let the minimum size of the underlying communities be $\\Omega(n/k)$ . Furthermore, assume $s_{k}(X)\\,\\gg\\,\\|E_{X}\\|$ . Then we get $\\begin{array}{r}{\\delta_{k-1}(Y)\\ge\\frac{C\\cdot\\gamma\\sqrt{n}}{k}}\\end{array}$ C\u00b7\u03b3k nfor some constant C. ", "page_idx": 19}, {"type": "text", "text": "Proof. This simply comes from putting the bounds on $\\hat{C}$ and multiplying them with $\\sqrt{\\operatorname*{min}_{j}|V_{j}|}$ and then connecting it with Equation 8. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Now, to go back to the assumption of $s_{k}(X)\\geq4C\\sigma{\\sqrt{d+n}}$ , consider that $n=\\Omega(d)$ (this is where will work from hereon), then the assumption holds as long as $\\gamma\\ge\\sigma k$ . Now, once we have this result, we can then obtain our main Theorem C.4 in the setting of Spatially unique centers. ", "page_idx": 19}, {"type": "text", "text": "Theorem C.4 (Relative compression with spatially unique centers). Let $X$ be a $d\\times n$ dataset $k$ many $\\gamma$ -spatially unique centers where the size of the smallest community is $\\Omega(n/k)$ . Then there is $a$ constant $C_{1}$ such that for all intra-community pairs in $V_{j}$ , the compression ratio is upper-bounded as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Delta_{X,k-1}(i,i^{\\prime})\\geq\\frac{\\sigma_{j}\\sqrt{d}}{C_{1}\\left(\\sigma\\sqrt{k}+\\alpha\\sqrt{\\log n}+\\frac{2\\sigma\\cdot\\sigma_{j}\\cdot k\\sqrt{d}}{\\gamma}\\right)}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Similarly for any $i\\in V_{j}$ and $i^{\\prime}\\in V_{j^{\\prime}}$ , the inter-community compression ratio is upper-bounded as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta_{X,k-1}(i,i^{\\prime})\\leq}\\\\ &{\\frac{\\sqrt{(\\sigma_{j}^{2}+\\sigma_{j^{\\prime}}^{2})d^{2}+\\|\\mathbf{c_{j}}-\\mathbf{c_{j^{\\prime}}}\\|^{2}}}{C_{1}\\left(\\|\\mathbf{c_{j}}-\\mathbf{c_{j^{\\prime}}}\\|-2\\sigma\\sqrt{k}-\\alpha\\sqrt{\\log n}-\\frac{\\sigma\\cdot\\sqrt{\\sigma_{j}^{2}+\\sigma_{j^{\\prime}}^{2}}\\cdot k\\cdot\\sqrt{d}}{\\gamma}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "with probability $1-{\\mathcal{O}}(1/n)$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. For simplicity of the statements we have made several assumptions, most of them to consider the harder setting (heavy noise). That is, $\\begin{array}{r}{\\sigma_{j}^{2}d=\\Omega(\\operatorname*{max}_{j^{\\prime}}\\|\\mathbf{c_{j}}-\\dot{\\mathbf{c}}_{\\mathbf{j^{\\prime}}}\\|)}\\end{array}$ . Furthermore, we assume $\\sigma$ is sufficiently large so that $\\sigma^{2}d\\geq100\\alpha\\sqrt{d}\\log n$ (this happens as long as $\\alpha\\,=\\,o(d^{1/4}))$ . This implies that the pre-PCA intra and inter-community distances are $\\Theta(2\\sigma_{j}{\\sqrt{d}})$ and $\\Theta\\left({\\sqrt{\\sigma_{j}^{2}+\\sigma_{j^{\\prime}}^{2}}}{\\sqrt{d}}\\right)$ respectively. ", "page_idx": 20}, {"type": "text", "text": "Next, in the intra-community compression ratio bound we have the term ", "page_idx": 20}, {"type": "equation", "text": "$$\n{\\frac{2{\\mathcal{N}}\\cdot{\\sqrt{\\sigma_{j}^{2}d+12{\\sqrt{d}}\\log n}}}{\\delta_{k-1}(Y)}}={\\frac{2\\sigma{\\sqrt{d+n}}\\cdot{\\sqrt{\\sigma_{j}^{2}d+12\\alpha{\\sqrt{d}}\\log n}}}{0.25\\gamma{\\sqrt{n}}/k}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Here recall that w\u221ae assume $n\\,=\\,\\Omega(d)$ which implies $2\\sigma{\\sqrt{d+n}}\\,\\leq\\,C\\sigma{\\sqrt{n}}$ for large enough $n$ . Furthermore, $12\\alpha\\sqrt{d}\\log n$ is dominated by $\\sigma_{j}^{2}d$ . Combining we get ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{2\\sigma\\sqrt{d+n}\\cdot\\sqrt{\\sigma_{j}^{2}d+12\\alpha\\sqrt{d}\\log n}}{0.25\\gamma\\sqrt{n}/k}=\\frac{C\\sigma\\sqrt{n}\\sigma_{j}\\sqrt{d}\\cdot k}{0.25\\gamma\\sqrt{n}}=\\frac{8C\\sigma\\sigma_{j}\\cdot k\\cdot\\sqrt{d}}{\\gamma}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Similarly the bound ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{2\\sigma\\sqrt{d+n}\\cdot\\sqrt{\\|\\mathbf{c_{j}}-\\mathbf{c_{j^{\\prime}}}\\|^{2}+2(\\sigma_{j}^{2}+\\sigma_{j^{\\prime}}^{2})d+12\\alpha\\sqrt{d}\\log n}}{\\delta_{k-1}(Y)}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "can be simplified to $\\begin{array}{r}{\\frac{C\\sqrt{\\sigma_{j}^{2}+\\sigma_{j^{\\prime}}^{2}}k\\sqrt{d}}{\\gamma}}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "Combining these bounds directly gets us result. ", "page_idx": 20}, {"type": "text", "text": "Then, Theorem 2.5 is immediately implied, as follows. ", "page_idx": 20}, {"type": "text", "text": "C.1 Proof of Theorem 2.5 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We\u221a know that $\\gamma\\geq C\\operatorname*{max}\\{\\sigma\\sqrt{k}d^{1/4},\\sigma\\sqrt{k}+\\alpha\\log n\\}$ . Furthermore, we assume $\\mathrm{max}_{i,j}\\,\\|\\mathbf{c_{i}}-\\mathbf{c_{j}}\\|\\ll$ $\\sigma{\\sqrt{\\mathbf{d}}}$ , which is the heavy noise setting. The other case follows the same way. Let $C>100C_{1}$ . ", "page_idx": 20}, {"type": "text", "text": "Furthermore, note that $\\|\\mathbf{c_{i}}-\\mathbf{c_{j}}\\|\\geq\\gamma$ . ", "page_idx": 20}, {"type": "text", "text": "Then we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{2\\sigma\\cdot\\sigma_{j}\\cdot k{\\sqrt{d}}}{\\gamma}\\leq0.01\\sigma_{j}{\\sqrt{k}}d^{1/4}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Similarly, we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{\\sqrt{\\sigma_{j}^{2}+\\sigma_{j^{\\prime}}^{2}}\\cdot k\\cdot\\sqrt{d}}{\\gamma}\\leq0.01\\sigma\\sqrt{k}d^{1/4}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Then, the \u221adenominator of the lower bound on the intra-community compression ratio is upper bounded by $0.02\\sigma\\sqrt{k}d^{1/4}$ , and the denominator of \u221athe lower bound o\u221an the inter-community compression ratio is lower bounded by $\\|\\mathbf{c_{i}}-\\|\\mathbf{c_{j}}\\|-0.02\\sigma\\sqrt{k}d^{1/4}\\geq0.98\\sigma\\sqrt{k}d^{1/4}$ . ", "page_idx": 20}, {"type": "text", "text": "Then, the intra-community compression ratio is low\u221aer b\u221aounded by $10\\sqrt{k}\\sqrt{d^{1/4}}$ and the intercommunity compression ratio is upper bounded by $0.05\\sqrt{k}\\sqrt{d^{1/4}}$ , obtaining the separation described in the Theorem 2.5. ", "page_idx": 20}, {"type": "text", "text": "C.2 Proofs for variance of compression ratios ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Having discussed the compression ratio bounds in the context of $\\gamma$ -spatially unique centers, we continue with the theoretical support for our outlier detection method in the random-mixture-outlier model. We recall the definition of this model for ease of exposition. ", "page_idx": 20}, {"type": "text", "text": "Definition C.5 (Mixture model with outliers (revisited)). Let $X$ be a $d\\times n$ dataset with the partition $V_{1},\\dots V_{k},{\\hat{V}}$ , a set of $k$ centers $\\{\\mathbf{c_{j}}\\}_{j=1}^{k}$ and distributions $\\{\\mathcal{D}^{(j)}\\}_{j=1}^{k}{+1}$ with the following generation method. ", "page_idx": 21}, {"type": "text", "text": "2. outliers: If $i\\in\\hat{V}$ , then we sample $p_{i,1},\\ldots p_{i,k}\\in[0.5,1]$ . Then $\\begin{array}{r}{u_{i}=\\sum_{j}\\alpha_{i,j}\\mathbf{c_{j}}\\!+\\!e_{i}}\\end{array}$ where $\\begin{array}{r}{\\alpha_{i,j}=\\frac{p_{i,j}}{\\sum_{j}p_{i,j}}}\\end{array}$ and $e_{i}$ is sampled from $\\mathcal{D}^{(k+1)}$ . ", "page_idx": 21}, {"type": "text", "text": "Let $|\\hat{V}|=n_{o}$ and $n=n_{o}+n_{c}$ . To keep the results simple, we make the average variance of each distribution $\\mathcal{D}^{(j)}$ same, which is $\\sigma^{\\prime}$ . ", "page_idx": 21}, {"type": "text", "text": "The concept of Algorithm 1 is simple. If each cluster has a large number of points, then even if there are a large number of outliers generated from the random-mixture-outlier model, the outliers will have a lower variance of compression than all the clean points. ", "page_idx": 21}, {"type": "text", "text": "First, let us obtain a lower bound on the variance of the compression ratio of clean points under the conditions of Theorem 2.8. We know that any clean point has high intra-community compression ratios. This implies that the expectation of the compression ratio for this point is high. On the other hand, the inter-compression ratio values are low. So just calculating the variance on the inter-community points yields a large value. ", "page_idx": 21}, {"type": "text", "text": "For the sake of simplicity, we will define $\\gamma\\geq2\\beta\\sqrt{\\sigma}k d^{1/4}$ . Then if we can show that under the other settings of Theorem 2.8, there is a separation in the variance of the compression ratios of the clean points and the outliers whenever \u03b2 \u2265C\u2032 \u03c3 \u03c3lo\u2032g n, we prove the theorem. ", "page_idx": 21}, {"type": "text", "text": "Lemma C.6. Let there\u221a be $n_{c}$ clean points in the random-mixture-outlier setting where $\\operatorname*{min}_{j}|V_{j}|=$ $\\Omega(n/k)$ and $\\gamma\\,\\geq\\,2\\beta\\sqrt{\\sigma}k d^{1/4}$ . Then the variance of all such points are lower bounded as $C_{4}$ \u00b7 $\\begin{array}{r}{\\frac{n_{c}-|V_{j}|}{n}\\cdot\\frac{d^{1/4}}{k}\\cdot\\left(\\frac{\\beta\\sigma^{\\prime}}{\\sigma}-\\frac{\\sigma}{\\beta\\cdot\\sigma^{\\prime}}\\right)}\\end{array}$ with probability $1-{\\mathcal{O}}(1/n)$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. Consider any point $x_{i}\\in V_{j}$ . Then t\u221ahe inter-compression ratio of $\\mathbf{\\boldsymbol{x}}_{i}$ with any intra-community point is lower bounded by\u03c3\u221ak+\u03b1\u221alo0g. 2n5+\u03c32\u221ad\u03c3\u03c3\u2032d1/4/(\u03b2) $\\begin{array}{r}{\\frac{0.25\\sigma^{\\prime}\\sqrt{d}}{\\sigma\\sqrt{k}+\\alpha\\sqrt{\\log n}+2\\sqrt{\\sigma\\sigma^{\\prime}}d^{1/4}/(\\beta)}\\geq\\frac{0.25\\beta\\sigma^{\\prime}}{\\sigma}d^{1/4}}\\end{array}$ 0.25\u03c3\u03b2\u03c3d1/4 with probability 1 \u2212O(1/n). Then the average of the compression ratios for $\\mathbf{\\Delta}x_{i}$ is lower bounded as $\\begin{array}{r}{\\frac{0.25\\sigma^{\\prime}/\\sigma d^{1/4}|V_{j}|}{n}\\geq\\frac{C_{3}\\beta\\sigma^{\\prime}/\\sigma d^{1/4}}{k}}\\end{array}$ On the other hand, probability $1-{\\mathcal{O}}(1/n)$ we have that for any inter-community point, the compression ratio is upper-bounded with $\\begin{array}{r}{\\frac{2\\sigma\\sqrt{d}}{(\\gamma-(2\\sigma\\sqrt{k}-\\alpha\\sqrt{\\log n}-C_{2}\\sigma^{\\prime}d^{1/4})}\\,\\leq\\,\\frac{2\\sigma\\sqrt{d}}{\\beta k\\sigma^{\\prime}d^{1/4}/C_{2}}\\,\\leq\\,\\frac{2C_{2}\\sigma/\\sigma^{\\prime}d^{1/4}}{\\beta\\cdot k}}\\end{array}$ ", "page_idx": 21}, {"type": "text", "text": "Then, the variance of compression of $\\mathbf{\\Delta}x_{i}$ is lower bounded by ", "page_idx": 21}, {"type": "equation", "text": "$$\nC_{4}\\cdot{\\frac{n-|V_{j}|}{n}}\\cdot\\left({\\frac{d^{1/4}}{k}}\\cdot\\left({\\frac{\\beta\\sigma^{\\prime}}{\\sigma}}-{\\frac{\\sigma}{\\beta\\sigma^{\\prime}}}\\right)\\right)^{2}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now, we aim to upper-bound the variance of compression for outliers. Here we want to show that since the underlying signal in any outlier is apart from the signal of any other point, they generally have a lower compression ratio with any other point, which then implies a lower variance of compression ratio. ", "page_idx": 21}, {"type": "text", "text": "First, we show that as long as there are not too many outliers, their underlying centers (which are random mixtures of the community centers) will not be too close (which implies they will not have a high compression ratio). ", "page_idx": 21}, {"type": "text", "text": "Lemma C.7 (Distance between signals of the outliers). Let there be $n_{o}$ many outliers in the dataset generated via the random-mixture model where $k\\geq\\log n$ . Let the set of outliers be $\\hat{V}$ . Then, for with probability 1 \u2212O(n), minu,v\u2208V\u02c6 \u2225u \u2212v\u2225\u2265lo\u03b3g n. ", "page_idx": 21}, {"type": "text", "text": "Proof. Let $|{\\hat{V}}|\\,=\\,n_{o}$ . Then, for the underlying mixture-center any two points, denoted as $\\textbf{\\em u}=$ $\\sum_{j}\\overset{\\cdot}{\\alpha}_{1,j}{\\bf c}{\\bf j}$ and $\\sum_{j}\\alpha_{2,j}\\mathbf{c}_{\\mathbf{j}}$ we say they are $\\epsilon$ -far if $\\mathrm{min}_{j}\\left|\\alpha_{1,j}-\\alpha_{2,j}\\right|\\geq\\epsilon$ . ", "page_idx": 22}, {"type": "text", "text": "Now, note that for any $\\epsilon$ -far mixture-centers, we have $\\lVert\\boldsymbol{u}-\\boldsymbol{v}\\rVert\\geq0.5\\epsilon\\gamma$ ", "page_idx": 22}, {"type": "text", "text": "Now, it is easy to see that the probability that there is a pair of mixed centers that is not $\\epsilon$ -far is $n_{0}^{2}\\cdot(\\epsilon)^{k}$ . Then, setting $\\epsilon={1}/{\\log n}$ and applying $k\\geq\\log n$ gives that even for $n_{0}=n/2$ , there all pairs of mixture centers are $1/\\log n$ -far with probability $1-{\\mathcal{O}}(1/n)$ . \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Then, we show that in such a case, the variance of compression for any outlier point is quite low even when measured crudely. ", "page_idx": 22}, {"type": "text", "text": "Lemma C.8 (Variance of compression of outliers). Let there be a set of $n_{o}$ many outliers so that the underlying mixture-centers are pairwise $1/\\log n$ -far Then under the condition of Lemma C.6, we have that the variance of compression for any outlier is upper bounded by $\\left(\\frac{4C_{2}\\log n(\\sigma/\\sigma^{\\prime})d^{1/4}}{\\beta\\cdot k}\\right)^{2}$ with probability $1-{\\mathcal{O}}(1/n)$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. Consider any outlier $u_{i}\\in V_{0}$ . First, consider the compression ratio between $\\mathbf{\\nabla}u_{i}$ and any $\\pmb{v}$ that is clean. Where $\\begin{array}{r}{{\\pmb u}_{i}=\\sum_{j}\\alpha_{j}{\\bf c_{j}}+{\\pmb e}_{i^{\\prime}}}\\end{array}$ and $\\pmb{v}=\\mathbf{c_{j^{\\prime}}}+e_{i}$ . ", "page_idx": 22}, {"type": "text", "text": "Next, remember that as every $\\alpha_{j}\\geq1/2k$ , we have $\\operatorname*{max}_{j}\\alpha_{j}\\le0.5$ . ", "page_idx": 22}, {"type": "text", "text": "Then, from the definition of $\\gamma$ -spatially unique centers we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n||\\sum_{j}\\alpha_{j}\\mathbf{c_{j}}-\\mathbf{c_{j^{\\prime}}}||\\geq||\\sum_{j\\neq j^{\\prime}}\\alpha_{j}\\mathbf{c_{j}}-(1-\\alpha_{j^{\\prime}})\\mathbf{c_{j^{\\prime}}}||\\geq0.5||\\sum_{j\\neq j^{\\prime}}\\alpha_{j}\\mathbf{c_{j}}-\\mathbf{c_{j^{\\prime}}}||\\geq0.5\\gamma\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then, following the analysis of Lemma C.6, we can show that in all such cases, the compression ratio is upper bounded by $\\frac{4C_{2}\\overset{-}{\\sigma/}\\sigma^{\\prime}d^{1/4}}{\\beta\\cdot k}$ ", "page_idx": 22}, {"type": "text", "text": "On the other hand, consider any two outliers. Then their compression ratios are upper bounded by $\\frac{4C_{2}\\log n\\sigma/\\sigma^{\\prime}d^{1/4}}{\\beta\\cdot k}$ (essentially replacing $\\gamma$ by $\\gamma/\\log n$ in the center-distance calculation). ", "page_idx": 22}, {"type": "text", "text": "Then, we can upper bound the variance of compression for an outlier as ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\frac{1}{n}\\left(|\\hat{V}|(\\frac{4C_{2}\\log n\\sigma/\\sigma^{\\prime}d^{1/4}}{\\beta\\cdot k})^{2}+(n-|\\hat{V}|)(\\frac{4C_{2}\\sigma/\\sigma^{\\prime}d^{1/4}}{\\beta\\cdot k})^{2}\\right)\\leq\\biggl(\\frac{4C_{2}\\log n(\\sigma/\\sigma^{\\prime})d^{1/4}}{\\beta\\cdot k}\\biggr)^{2}[\\alpha\\cdot\\sigma^{\\prime}\\cdot\\sigma^{\\prime}\\cdot\\sigma^{\\prime}].\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Proof of Theorem 2.8 Lemma C.6 shows that in the setting of Theorem 2.8, the variance of compression ratios for a clean point is lower bounded by $\\begin{array}{r}{C_{4}\\cdot\\frac{n-|V_{j}|}{n}\\cdot\\left(\\frac{d^{1/4}}{k}\\cdot\\left(\\frac{\\beta\\sigma^{\\prime}}{\\sigma}-\\frac{\\sigma}{\\beta\\sigma^{\\prime}}\\right)\\right)^{2}}\\end{array}$ . ", "page_idx": 22}, {"type": "text", "text": "Next, Lemma C.8 shows that the variance of compression ratios for an outlier is upper-bounded as $\\left(\\frac{4C_{2}\\log n(\\sigma/\\sigma^{\\prime})d^{1/4}}{\\beta\\cdot k}\\right)^{2}$ Both the aforementioned happen for all outlier and clean points with probability $1-{\\mathcal{O}}(1/n)$ . ", "page_idx": 22}, {"type": "text", "text": "Then, to show that with high probability, the variance of compression ratios of any clean point is higher than the variance of compression ratios of any outlier is ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{C_{4}\\cdot\\frac{n-|V_{j}|}{n}\\cdot\\left(\\frac{d^{1/4}}{k}\\cdot\\left(\\frac{\\beta\\sigma^{\\prime}}{\\sigma}-\\frac{\\sigma}{\\beta\\sigma^{\\prime}}\\right)\\right)^{2}>\\left(\\frac{4C_{2}\\log n(\\sigma/\\sigma^{\\prime})d^{1/4}}{\\beta\\cdot k}\\right)^{2}}\\\\ &{\\implies\\frac{\\sqrt{d}}{k^{2}}\\cdot\\left(\\frac{n-|\\hat{V}|}{n}\\cdot\\frac{\\beta\\sigma^{\\prime}}{\\sigma}-\\frac{C_{5}\\log n\\sigma}{\\sigma^{\\prime}\\beta}\\right)>0}\\\\ &{\\implies\\frac{\\sqrt{d}}{k^{2}}\\cdot\\left(\\frac{0.5\\beta\\sigma^{\\prime}}{\\sigma}-\\frac{C_{5}\\log n\\sigma}{\\sigma^{\\prime}\\beta}\\right)>0}\\end{array}\\quad\\mathrm{[Fr~]}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "or some constant $C_{5}]$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n[\\mathsf{A s}\\;n-|\\hat{V}|\\geq0.5n]\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Then, as long as $\\beta\\geq10C_{5}\\sigma/\\sigma^{\\prime}\\sqrt{\\log n}$ , this equation is satisfied. ", "page_idx": 22}, {"type": "text", "text": "D Projection with more principal components ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Here we show some results in the case of $k^{\\prime}=k-1+c.$ . The main challenge in theoretically proving our bounds for $k^{\\prime}\\neq k-1$ comes from Theorem B.8. A key ingredient towards proving Theorem B.1 is the following spectral gap. ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\left\\|(P_{Z}^{k^{\\prime}})^{T}-(P_{\\bar{Z}}^{k^{\\prime}})^{T}W\\right\\|\\leq\\frac{2\\|E_{Z}\\|}{\\delta_{k^{\\prime}}(Y)}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In general we work with the natural assumption $\\delta_{k-1}(Y)>>\\|E_{Z}\\|$ . However, in our model we have $s_{k}(Y)=\\mathcal{O}(\\|E_{Z}\\|)$ . This follows from Weyl\u2019s inequality, which states that if $Z={\\bar{Z}}+E_{Z}$ and $(k^{\\prime})$ -th singular value of $B$ is $0$ , then $(k-1+c)$ -th singular value of $Z$ is upper bounded by $O(\\|E_{Z}\\|)$ for any $c>0$ . ", "page_idx": 23}, {"type": "text", "text": "Thus $\\delta_{k^{\\prime}}(Y)\\,=\\,\\mathcal{O}(\\|E_{Z}\\|)$ for any $k^{\\prime}\\ge k$ , and our previous results alone cannot prove relative compressibility. ", "page_idx": 23}, {"type": "text", "text": "Here we bypass this issue to a loose but non-trivial extent. First we note that the inter-community compression can only decrease if the the projection dimension increases. Thus we have that for any $k^{\\prime}\\stackrel{<}{\\geq}k,\\Delta_{X,k^{\\prime}}(i,i^{\\prime})\\stackrel{<}{\\leq}\\Delta_{X,k-1}(i,i^{\\prime})$ . ", "page_idx": 23}, {"type": "text", "text": "Theorem D.1. Let us consider the random vector model as in Theorem B.1. Let Let $k^{\\prime}=k-1+c$ and any $0<f<1$ . Then we have that with probability $1-{\\mathcal{O}}(1/n)$ , ", "page_idx": 23}, {"type": "text", "text": "1. If $(i,i^{\\prime})$ is an inter-community pair, then $\\Delta_{k^{\\prime},X}(i,i^{\\prime})\\leq\\Delta_{k-1,X}(i,i^{\\prime})$ ", "page_idx": 23}, {"type": "text", "text": "2. If $(i,i^{\\prime})$ is an intra-community pair, then ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\Delta_{k-1,Y}(i,i^{\\prime})\\geq\\frac{\\sqrt{\\|\\mathbf{c_{j}}-\\mathbf{c_{j^{\\prime}}}\\|^{2}+d(\\sigma_{j}^{2}+\\sigma_{j^{\\prime}}^{2})+12\\sqrt{d}\\log n}}{\\sqrt{\\left\\|P_{Y}^{k-1}(y_{i}-y_{i^{\\prime}})\\right\\|^{2}+4C_{0}^{2}\\sigma^{2}(d+n)c^{2}f^{2}}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "for all but $c^{2}/f^{4}$ pairs of points with probability $1-{\\mathcal{O}}(1/n)$ . ", "page_idx": 23}, {"type": "text", "text": "Proof. The inter-community bound follows from definition and the numerator of the intra-community bound follows from Lemma B.2. We now prove the denominator (post PCA distance bounds) for the intra-community case. ", "page_idx": 23}, {"type": "text", "text": "Let us denote with $P_{Y}^{k_{1},k_{2}}$ the projection operator due to the $k_{1}$ -th to $k_{2}$ -th top singular vectors of $Y$ Then for any vector $\\textbf{\\em u}$ we have $\\|\\Pi_{X}^{k^{\\prime}}(u)\\|=\\sqrt{\\|\\Pi_{X}^{k-1}(u)\\|^{2}+\\|P_{Y}^{k,k^{\\prime}}(u)\\|^{2}}.$ ", "page_idx": 23}, {"type": "text", "text": "Then we are left with bounding $\\|P_{Y}^{k,k^{\\prime}}(u)\\|^{2}$ where ${\\pmb u}={\\pmb y}_{i}-{\\pmb y}_{i^{\\prime}}$ so that $i\\in V_{j},i^{\\prime}\\in V_{j^{\\prime}}$ . We aim to show that if $k^{\\prime}-k$ is small then this value is small as well. ", "page_idx": 23}, {"type": "text", "text": "We first represent $Y$ with its SVD decomposition. $l_{\\ell}$ and $\\boldsymbol{r}_{\\ell}$ represent the $\\ell$ -th left singular vector and right singular vector of $Y$ respectively. Then we have $\\begin{array}{r}{Y=\\sum_{\\ell=1}^{t}s_{i}(Y)l_{\\ell}(r_{\\ell})^{T}}\\end{array}$ where $t=r a n k(Y)$ . Then the projection of $\\textit{\\textbf{y}}_{i}$ due to the $\\ell$ -th principal compo nent of $X$ is $\\langle l_{\\ell},y_{i}\\rangle=s_{\\ell}(Y)r_{\\ell},i$ where $r_{\\ell,i}$ is the $i$ -th entry of the $\\ell$ -th right singular vector. Then we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\leq s_{k}(Y)\\sqrt{\\sum_{\\ell=k}^{k^{\\prime}}(r_{\\ell,i})^{2}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Here recall that each $\\boldsymbol{r}_{\\ell}$ is a $n$ -dimensional vector with unit norm, i.e. $\\|r_{\\ell}\\|\\,=\\,1$ . Then for any $f<1$ , the number of coordinates of $_{r_{\\ell}}$ that are larger than $f$ is less than $1/\\dot{f}^{2}$ . Thus considering all the $k\\,\\leq\\,\\ell\\,\\leq\\,k\\,-\\,1\\,+\\,c.$ , the total number of entries that are larger than $f$ is less than $c/f^{\\breve{2}}$ . Then, for all but c/f 2 m\u221aany points yi we have \u2225P Yk,k\u22121(yi)\u2225\u2264sk(Y ) \u00b7 f \u00b7 c. Here we substitute $s_{k}(Y)\\leq\\|E_{Y}\\|\\leq C_{0}\\sigma\\sqrt{d+n}$ with probability $1-\\mathcal{O}(n^{-3})$ . ", "page_idx": 23}, {"type": "text", "text": "This implies that with probability $1-\\mathcal{O}(1/n)\\,\\,\\|P_{Y}^{k,k-1}(\\pmb{y}_{i}-\\pmb{y}_{i^{\\prime}})\\|\\,\\leq\\,2C_{0}\\sigma\\sqrt{d+n}c f$ for all but $c^{2}/f^{4}$ pairs of points. This concludes our proof. ", "page_idx": 23}, {"type": "text", "text": "It should be noted that this is a much looser bound as compared to our $(k-1)$ -PC compression metric, especially in the paradigm where noise dominates the ground truth distances. As we discussed, the main reason that Theorem B.1 does not directly work for $k^{\\prime}>k-1$ can be pinned down to the following technical challenge. ", "page_idx": 24}, {"type": "text", "text": "D.0.1 Technical challenges in understanding PCA ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "afl ocr haa lrlaenndgoem i sv egcettotir .  Ia n bfeatctte,r $\\|(P_{Z}^{k-1}-P_{\\bar{Z}}^{k-1})(e)\\|$ $\\|P_{Z}^{k-1}-$ $P_{\\bar{Z}}^{k-1}\\|\\cdot\\|e\\|$ $^e$ $\\|(P_{Z}^{k-1}-P_{\\bar{Z}}^{k-1})e\\|$ P Zk\u22121\u2212P Zk\u00af\u22121)\u2225\u00b7 \u2225e\u2225only if $_{e/\\parallel e\\parallel}$ is a unit vector along which $(P_{Z}^{k-1}-P_{\\bar{Z}}^{k-1})$ realizes its spectral norm, which is unlikely to be the case for most noise $^e$ vectors, due to the inherent randomness in them. A better analysis of this term will allow us to extend the result of Theorem B.1 beyond $k^{\\prime}=k-1$ , which is what we observe in reality. For real datasets, the compression factor does not change much if the PCA dimension is changed by a small value. Furthermore, a tighter understanding of $\\|(P_{\\!_{Z}}^{k-1}-P_{\\bar{Z}}^{k-1})e\\|$ will also enable to us make progress towards proving optimality of perhaps the simplest spectral clustering algorithm for the SBM problem, as conjectured by $[\\mathrm{Vul}8]$ . There has indeed been some progress very recently [MZ24, MZ23] in some very specific settings, i.e. SBM (stochastic block model). Generalizing these results to the random vector mixture model is an outstanding open question. ", "page_idx": 24}, {"type": "text", "text": "E Experiments ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "E.1 Summary of datasets ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "First, we present a summary of the datasets. ", "page_idx": 24}, {"type": "table", "img_path": "a4J7nDLXEM/tmp/f75487e8554afae900b776815fe50fe9d4103fbe1f5030afedba119de7411dea.jpg", "table_caption": ["Table 4: Summary of data "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "E.2 Community-wise average compression ratios ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In Section 4 we showed the average of community-wise average of intra and inter-community compression ratios for the datasets in [DRS20] for PCA dimension $=k-1$ . Here we present the results for each community of the datasets. We observe that even in the community-level metric, the intra-community compression ratio is higher than the inter-community compression ratio for all datasets. ", "page_idx": 24}, {"type": "text", "text": "E.3 NMI and purity index improvement for PCA-dim=k \u22121 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Now, we continue with providing more experimental results. First, we note down the NMI improvement when $5\\%$ and $10\\bar{\\%}$ of the points are removed in the setting of PCA dimension $=k-1$ . ", "page_idx": 24}, {"type": "text", "text": "Next, we add the initial purity scores when running PCA( dimension= $\\!\\cdot\\!k-1)\\!+\\!\\mathrm{K}$ -means on the datasets in Table 6. ", "page_idx": 24}, {"type": "text", "text": "Then the improvement in purity index due to $5\\%$ and $10\\%$ points removal are recorded in Figures 5 and 6. ", "page_idx": 24}, {"type": "image", "img_path": "a4J7nDLXEM/tmp/bd590066bbb5a7ebe9281794797c560e08928bd8eea710c0942c63cd68a8c8ac.jpg", "img_caption": ["Figure 3: NMI improvement via removing $5\\%$ points "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "a4J7nDLXEM/tmp/034ed5718666f3c3934d57cc94f94c65dd692097278fc7d225afa7111f54186c.jpg", "img_caption": ["Figure 4: NMI improvement via removing $10\\%$ points "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "a4J7nDLXEM/tmp/aa80828fbfa45029dd878946c5dedd66f754ee4b7ee39654141327cd399db39e.jpg", "img_caption": ["Figure 5: Purity score improvement via $5\\%$ outlier removal "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "a4J7nDLXEM/tmp/e522df78f04096a13ec314e4ac9dbfe4523609d2439e9fff8a5d2cb3d7faa969.jpg", "img_caption": ["Figure 6: Purity score improvement via $10\\%$ outlier removal "], "img_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "a4J7nDLXEM/tmp/de592f67ad3a16bcb4d7e9e405f102dd9b8e15ac3fb10cd24ba15087b0728326.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "a4J7nDLXEM/tmp/694d1b7d9eee88d4c3382587f6bc6ff48ae976194974722ffb472c13a0e11d6c.jpg", "table_caption": ["Table 5: Community-wise Inter and Intra-Community Compression Ratios ", "Table 6: Purity index before data removal $\\operatorname{PCA}\\dim=k-1)$ "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "E.4 Different PCA dimension choice ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Finally, we show that our experiments on real-world data, both for average compression as well as clustering accuracy improvement through outlier detection, are fairly stable to a change in the PCA dimension. The average compression ratios can be found in Table 7. The NMI and purity index baselines can be found in Tables 8 and 9 respectively. ", "page_idx": 26}, {"type": "table", "img_path": "a4J7nDLXEM/tmp/8c1eca8b082c25642ae2533d59a62df8aa9c924c2d8d6c8dbf3f60b1bbc738ae.jpg", "table_caption": [], "table_footnote": ["Table 7: Relative compression on RNA-seq datasets when PCA dimension is $2k$ "], "page_idx": 26}, {"type": "table", "img_path": "a4J7nDLXEM/tmp/b0f8a561e985f7b0554fba5e15780e9b9b2a868bd576e6bdc7b7fe307d262675.jpg", "table_caption": [], "table_footnote": ["Table 8: NMI before data removal (PCA dim $=2k_{\\mathrm{\\ell}}$ ) "], "page_idx": 27}, {"type": "table", "img_path": "a4J7nDLXEM/tmp/7d2c8f76aadb31e5543b332e40a519e3645d616a3fe4a2741da00826abdc53b0.jpg", "table_caption": [], "table_footnote": ["Table 9: Purity score before data removal (PCA dim $=2k$ ) "], "page_idx": 27}, {"type": "image", "img_path": "a4J7nDLXEM/tmp/b46e4182201104890f88dadda1eea0274c3b7ffd4cfb22521e914a9d3c1f54d7.jpg", "img_caption": ["Figure 7: NMI improvement via removing $10\\%$ points when PCA dimension is $2k$ "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "For brevity, we show the improvement in NMI and purity index for $10\\%$ point removal in Figures 7 and 8 respectively. As one can observe, our method continues to be the most consistent, being the best method in most datasets. Indeed, in this case our performance is even comparatively better than in the case of PCA-dimension $\\scriptstyle=k\\;-\\;1$ . ", "page_idx": 27}, {"type": "text", "text": "F Future directions ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this paper, we have quantified PCA\u2019s denoising effect in high dimensional noisy data with underlying community structure via the metric of compression ratio. As an application, we have designed an outlier detection method that improves the community structure of datasets. We note two interesting theoretical and algorithmic questions. ", "page_idx": 27}, {"type": "text", "text": "i) Providing a more tight bound on the compression ratio seems an exciting and hard direction. ", "page_idx": 27}, {"type": "image", "img_path": "a4J7nDLXEM/tmp/ffaeb9f0ca504cbf2a8dc7fa6aef4ef9d5ae7a2e434afdb104980d3e541cafaf.jpg", "img_caption": ["Figure 8: Purity score improvement via removing $10\\%$ points when PCA dimension is $2k$ "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "ii) Using compression ratio as a metric for clustering algorithms also seems an interesting direction, especially for single-cell-RNA-seq datasets. ", "page_idx": 28}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provide a novel quantification of PCA\u2019s denoising effect in high dimensional data with heavy noise. Then, we use this quantification to develop an outlier detection method in this setting. We provide comprehensive theoretical, simulation, and real-world experiment results in the paper. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 29}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We discuss the limitations of our work in the last paragraph of the main paper. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 29}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: We provide the proof of our theorems in the Appendix B. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 30}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: We describe our experiments clearly in Section 4 and provide the full source code used to generate the results in the supplementary material. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 30}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The data is publicly available and we include its source. The supplementary material includes our simulation code, algorithms, and experiments. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 31}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We provide all experimental details within Section 4 of the paper and additional results within Appendix E for different experimental settings. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We provide error bars for all the applicable experiments, mainly in Appendix E. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We describe the computational environment and running time used to generate the results in the first paragraph of Section 3. ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 32}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We have read and understood the guidelines ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 32}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 32}, {"type": "text", "text": "Answer:[NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: ", "page_idx": 32}, {"type": "text", "text": "Our work focuses on understanding structures of graphs that appear in real-world data, and our application is focused on clustering of single-cell RNA sequencing datasets. As such, we do not see any immediate negative societal impact of our work. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 32}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the ", "page_idx": 33}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 33}, {"type": "text", "text": "Justification: We do not see any immediate risk of misuse of our work. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 33}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We use publicly available datasets and cite them. ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We provide our codes in the supplementary material. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 34}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 34}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]