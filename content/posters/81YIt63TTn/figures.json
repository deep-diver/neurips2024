[{"figure_path": "81YIt63TTn/figures/figures_2_1.jpg", "caption": "Figure 2: The effectiveness of Twin-Merging in terms of performance and parameter-efficiency.", "description": "This figure demonstrates the effectiveness of Twin-Merging in terms of both performance and parameter efficiency. Subfigure (a) shows the average performance on generative tasks plotted against the number of parameters.  Twin-Merging is compared to several other model merging baselines, with the size of the circles indicating different storage sizes. Subfigure (b) presents a comparison of the absolute accuracy across various individual tasks for both NLP benchmarks (using RoBERTa and Qwen models) covering discriminative and generative tasks. The figure visually highlights that Twin-Merging achieves better performance with improved parameter efficiency compared to other baselines.", "section": "2 Related Work"}, {"figure_path": "81YIt63TTn/figures/figures_3_1.jpg", "caption": "Figure 3: The impact of different ratios of shared knowledge and exclusive knowledge.", "description": "This figure shows two graphs illustrating the effects of different ratios of shared and exclusive knowledge on model merging performance. The top graph demonstrates the impact of shared knowledge by varying fine-tuned epochs. As the number of epochs increases, shared knowledge decreases, leading to a drop in merging performance despite good performance on individual tasks. This highlights the importance of shared knowledge in successful model merging. The bottom graph explores the impact of exclusive knowledge by varying the sparsity of task-specific weights when merging a single task-specific model. Even with high sparsity (99%), the single-merged model outperforms multi-model merging, showcasing the contribution of exclusive knowledge. These findings suggest that both shared and exclusive knowledge are crucial, but excessive exclusive knowledge can hinder merging performance.", "section": "3.2 Interpreting Interference From the Perspective of Knowledge"}, {"figure_path": "81YIt63TTn/figures/figures_8_1.jpg", "caption": "Figure 4: Averaged normalized accuracy vs. the number of tasks for various benchmarks. Twin-Merging maintains performance regardless of task number and compresses the fine-tuned checkpoints.", "description": "This figure shows two graphs. The left graph displays the average normalized accuracy against the number of tasks for different model merging methods.  It demonstrates that Twin-Merging significantly outperforms other methods, maintaining high accuracy even as the number of tasks increases, while other methods' accuracy significantly decreases. The right graph illustrates the trade-off between storage size and performance.  It shows that while maintaining individual task-specific models offers high performance but demands excessive storage, Twin-Merging effectively balances performance and storage efficiency by leveraging shared experts and compression techniques. ", "section": "4.4 Scale to More Tasks"}, {"figure_path": "81YIt63TTn/figures/figures_8_2.jpg", "caption": "Figure 5: Twin-Merging routing decisions of the experts for various tasks.", "description": "This figure visualizes the routing decisions made by the Twin-Merging model's router for different tasks.  The left panel shows the weight assigned to each expert (represented by different colors) for the QNLI task, demonstrating how the router dynamically combines knowledge from different experts based on the input. The right panel shows the weights for four generative tasks (MMLU, TruthfulQA, BBQ, CNN-DailyMail), highlighting how the router adapts its weighting scheme according to the task.  The weights are normalized using softmax.", "section": "Router Analysis"}, {"figure_path": "81YIt63TTn/figures/figures_9_1.jpg", "caption": "Figure 6: Twin-Merging performance vs. different sparsity levels and techniques for GLUE", "description": "This figure shows the results of experiments on the impact of different sparsity rates and techniques on the performance of Twin-Merging on the GLUE benchmark. The left panel shows a line graph plotting the average score against the sparsity rate for Twin-Merging with a shared expert (red dashed line) and a fine-tuned model (black dashed line). It shows that performance is relatively stable until around 90% sparsity, after which performance drops significantly.  The right panel shows a bar graph comparing the average scores for different sparsity rates across three different sparsification techniques: Magnitude, Bernoulli, and SVD. It shows that SVD generally outperforms the other two techniques.", "section": "4.6 Compression and Speed Analysis"}, {"figure_path": "81YIt63TTn/figures/figures_17_1.jpg", "caption": "Figure 7: The visualizations show normalized performance across eight GLUE tasks, highlighting the impact of combining expertise from the COLA and SST-2 domains (expert indicated by red vectors) through Task Arithmetic. Performance scores are normalized, with the unmerged pretrained model set to zero and other results scaled to the [-1,1] range. The x-axis (COLA) and y-axis (ysst-2) represent the merging weights for COLA and SST-2 expertise. Blue regions indicate improved performance over the pretrained model, while red regions indicate deterioration.", "description": "This figure visualizes the performance of Task Arithmetic on eight GLUE tasks by varying the weights of COLA and SST-2 experts.  It shows how different combinations of these two experts impact performance on each task, highlighting areas where combining expertise is beneficial (blue) and detrimental (red). The visualization helps illustrate the complex interactions between different models and the challenges of finding optimal weights for model merging.", "section": "3.1 Analysis of the Performance Gap in Model Merging"}]