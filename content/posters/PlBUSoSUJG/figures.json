[{"figure_path": "PlBUSoSUJG/figures/figures_6_1.jpg", "caption": "Figure 1: A comparison of the empirical PG variance and our bound for E-SoftTreeMax on randomly drawn MDPs. We present three cases for P\u03c0b: (i) close to uniform, (ii) drawn randomly, and (iii) close to a permutation matrix. This experiment verifies the optimal and worse-case rate decay cases. The variance bounds here are taken from Theorem 4.7 where we substitute a = |\u03bb2(P\u03c0b)|. To account for the constants, we match the values for the first point in d = 1.", "description": "This figure compares the empirical and theoretical policy gradient variance of E-SoftTreeMax for different behavior policies (\u03c0b). Three scenarios are considered: (i) transition probabilities close to uniform, (ii) randomly drawn transition probabilities, and (iii) transition probabilities close to a permutation matrix. The results demonstrate that the variance decays exponentially with the planning horizon (d), and that the decay rate is determined by the second eigenvalue of the transition kernel induced by \u03c0b. The theoretical bounds match well with the empirical results for all three scenarios.", "section": "4 Theoretical Analysis"}, {"figure_path": "PlBUSoSUJG/figures/figures_8_1.jpg", "caption": "Figure 3: Reward and Gradient variance: GPU SoftTreeMax (single worker) vs PPO (256 GPU workers). The blue reward plots show the average of 50 evaluation episodes. The red variance plots show the average gradient variance of the corresponding training runs, averaged over five seeds. The dashed lines represent the same for PPO. Note that the variance y-axis is in log-scale.", "description": "This figure compares the performance of SoftTreeMax and PPO across eight Atari games.  For each game, it shows the average reward obtained and the gradient variance during training for different depths (d) of the SoftTreeMax tree expansion.  The results are averaged over five separate training runs. Dashed lines indicate corresponding values for distributed PPO (using 256 GPU workers). The y-axis for variance is logarithmic, highlighting the significant differences in variance reduction between the methods.", "section": "6 Experiments"}, {"figure_path": "PlBUSoSUJG/figures/figures_23_1.jpg", "caption": "Figure 3: Reward and Gradient variance: GPU SoftTreeMax (single worker) vs PPO (256 GPU workers). The blue reward plots show the average of 50 evaluation episodes. The red variance plots show the average gradient variance of the corresponding training runs, averaged over five seeds. The dashed lines represent the same for PPO. Note that the variance y-axis is in log-scale.", "description": "This figure compares the performance of SoftTreeMax with different tree depths against a distributed PPO baseline on various Atari games.  The plots illustrate the average reward obtained (blue lines) and the gradient variance (red lines) during training.  SoftTreeMax consistently outperforms PPO in terms of rewards across all games while maintaining significantly lower gradient variance. The y-axis for variance is displayed in a log scale to highlight the large difference in magnitude.", "section": "6 Experiments"}, {"figure_path": "PlBUSoSUJG/figures/figures_24_1.jpg", "caption": "Figure 5: Training curves: GPU SoftTreeMax (single worker) vs PPO (256 GPU workers). The plots show average reward and standard deviation over 5 seeds. The x-axis is the number of online interactions with the environment. The runs ended after one week with varying number of time-steps. The training curves correspond to the evaluation runs in Figure 3.", "description": "This figure compares the training performance of SoftTreeMax and PPO across several Atari games.  The x-axis represents the number of online interactions, showing the cumulative reward obtained over time.  SoftTreeMax, while using a single worker, achieves comparable or even better performance than PPO which utilizes 256 workers. The plots also show the standard deviation to illustrate the variance. It demonstrates that SoftTreeMax is more sample efficient than PPO.", "section": "Experiments"}]