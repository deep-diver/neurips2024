[{"type": "text", "text": "Policy Gradient with Tree Expansion ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Policy gradient methods are notorious for having a large variance and high sample   \n2 complexity. To mitigate this, we introduce SoftTreeMax\u2014a generalization of   \n3 softmax that employs planning. In SoftTreeMax, we extend the traditional logits   \n4 with the multi-step discounted cumulative reward, topped with the logits of future   \n5 states. We analyze SoftTreeMax and explain how tree expansion helps to reduce   \n6 its gradient variance. We prove that the variance decays exponentially with the   \n7 planning horizon as a function of the chosen tree-expansion policy. Specifically,   \n8 we show that the closer the induced transitions are to being state-independent,   \n9 the stronger the decay. With approximate forward models, we prove that the   \n10 resulting gradient bias diminishes with the approximation error while retaining   \n11 the same variance reduction. Ours is the first result to bound the gradient bias for   \n12 an approximate model. In a practical implementation of SoftTreeMax, we utilize   \n13 a parallel GPU-based simulator for fast and efficient tree expansion. Using this   \n14 implementation in Atari, we show that SoftTreeMax reduces the gradient variance   \n15 by three orders of magnitude. This leads to better sample complexity and improved   \n16 performance compared to distributed PPO. ", "page_idx": 0}, {"type": "text", "text": "17 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "18 Policy Gradient (PG) methods [Sutton et al., 1999] for Reinforcement Learning (RL) are often the   \n19 first choice for environments that allow numerous interactions at a fast pace [Schulman et al., 2017].   \n20 Their success is attributed to several factors: they are easy to distribute to multiple workers, require   \n21 no assumptions on the underlying value function, and have both on-policy and off-policy variants.   \n22 Despite these positive features, PG algorithms are also notoriously unstable due to the high variance   \n23 of the gradients computed over entire trajectories [Liu et al., 2020, Xu et al., 2020]. As a result, PG   \n24 algorithms tend to be highly inefficient in terms of sample complexity. Several solutions have been   \n25 proposed to mitigate the high variance issue, including baseline subtraction [Greensmith et al., 2004,   \n26 Thomas and Brunskill, 2017, Wu et al., 2018], anchor-point averaging [Papini et al., 2018], and other   \n27 variance reduction techniques [Zhang et al., 2021, Shen et al., 2019, Pham et al., 2020].   \n28 A second family of algorithms that achieved state-of-the-art results in several domains is based on   \n29 planning. Planning is exercised primarily in the context of value-based RL and is usually implemented   \n30 using a Tree Search (TS) [Silver et al., 2016, Schrittwieser et al., 2020]. In this work, we combine   \n31 PG with TS by introducing a parameterized differentiable policy that incorporates tree expansion.   \n32 Namely, our SoftTreeMax policy replaces the standard policy logits of a state and action, with the   \n33 expected value of trajectories that originate from these state and action. We consider two variants of   \n34 SoftTreeMax, one for cumulative reward and one for exponentiated reward.   \n35 Combining TS and PG should be done with care given the biggest downside of PG\u2014its high gradient   \n36 variance. This raises questions that were ignored until this work: (i) How to design a PG method based   \n37 on tree-expansion that is stable and performs well in practice? and (ii) How does the tree-expansion   \n38 policy affect the PG variance? Here, we analyze SoftTreeMax, and provide a practical methodology   \n39 to choose the expansion policy to minimize the resulting variance. Our main result shows that a   \n40 desirable expansion policy is one, under which the induced transition probabilities are similar for   \n41 each starting state. More generally, we show that the gradient variance of SoftTreeMax decays at   \n42 a rate of $|\\bar{\\lambda_{2}}|^{d}$ , where $d$ is the depth of the tree and $\\lambda_{2}$ is the second eigenvalue of the transition   \n43 matrix induced by the tree expansion policy. This work is the first to prove such a relation between   \n44 PG variance and tree expansion policy. In addition, we prove that the with an approximate forward   \n45 model, the bias of the gradient is bounded proportionally to the approximation error of the model.   \n46 To verify our results, we implemented a practical version of SoftTreeMax that exhaustively searches   \n47 the entire tree and applies a neural network on its leaves. We test our algorithm on a parallelized   \n48 Atari GPU simulator [Dalton et al., 2020]. To enable a tractable deep search, up to depth eight, we   \n49 also introduce a pruning technique that limits the width of the tree. We do so by sampling only the   \n50 most promising nodes at each level. We integrate our SoftTreeMax GPU implementation into the   \n51 popular PPO [Schulman et al., 2017] and compare it to the flat distributed variant of PPO. This allows   \n52 us to demonstrate the potential benefit of utilizing learned models while isolating the fundamental   \n53 properties of TS without added noise. In all tested Atari games, our results outperform the baseline   \n54 and obtain up to $5\\mathrm{x}$ more reward. We further show in Section 6 that the associated gradient variance   \n55 is smaller by three orders of magnitude in all games, demonstrating the relation between low gradient   \n56 variance and high reward.   \n57 We summarize our key contributions. (i) We show how to combine two families of SoTA approaches:   \n58 PG and TS by introducing SoftTreeMax: a novel parametric policy that generalizes softmax to   \n59 planning. Specifically, we propose two variants based on cumulative and exponentiated rewards. (ii)   \n60 We prove that the gradient variance of SoftTreeMax in its two variants decays exponentially   \n61 with its tree depth. Our analysis sheds new light on the choice of tree expansion policy. It raises   \n62 the question of optimality in terms of variance versus the traditional regret; e.g., in UCT [Kocsis   \n63 and Szepesv\u00e1ri, 2006]. (iii) We prove that with an approximate forward model, the gradient bias is   \n64 proportional to the approximation error, while retaining the variance decay. This quantifies the   \n65 accuracy required from a learned forward model. (iv) We implement a differentiable deep version   \n66 of SoftTreeMax that employs a parallelized GPU tree expansion. We demonstrate how its gradient   \n67 variance is reduced by three orders of magnitude over PPO while obtaining up to 5x reward. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "68 2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "69 Let $\\Delta_{U}$ denote simplex over the set $U$ . Throughout, we consider a discounted Markov Decision   \n70 Process (MDP) $\\mathcal{M}\\overset{=}{=}(S,\\mathcal{A},P,r,\\gamma,\\nu)$ , where $\\boldsymbol{S}$ is a finite state space of size $S$ , $\\boldsymbol{\\mathcal{A}}$ is a finite action   \n71 space of size $A$ , $r:S\\times A\\to[0,1]$ is the reward function, $P:{\\mathcal{S}}\\times{\\mathcal{A}}\\rightarrow\\Delta_{S}$ is the transition   \n72 function, $\\gamma\\in(0,1)$ is the discount factor, and $\\nu\\in\\mathbb{R}^{S}$ is the initial state distribution. We denote   \n73 the transition matrix starting from state $s$ by $P_{s}\\in[0,1]^{A\\times S}$ , i.e., $[P_{s}]_{a,s^{\\prime}}=P(s^{\\prime}|a,s)$ . Similarly,   \n74 let $R_{s}\\,=\\,r(s,\\cdot)\\,\\in\\,\\mathbb{R}^{A}$ denote the corresponding reward vector. Separately, let $\\pi:S\\to\\Delta_{\\mathcal{A}}$ be a   \n75 stationary policy. Let $P^{\\pi}$ and $R_{\\pi}$ be the induced transition matrix and reward function, respectively,   \n76 i.e., $\\begin{array}{r}{P^{\\pi}(s^{\\prime}|s)\\;=\\;\\sum_{a}\\pi(a|s)\\operatorname*{Pr}(s^{\\prime}|s,a)}\\end{array}$ and $\\begin{array}{r}{R_{\\pi}(s)\\:=\\:\\sum_{a}\\pi(a|s)r(s,a)}\\end{array}$ . Denote the stationary   \n77 distribution of $P^{\\pi}$ by $\\boldsymbol{\\mu}_{\\pi}\\,\\in\\,\\mathbb{R}^{S}$ s.t. $\\mu_{\\pi}^{\\top}P^{\\pi}\\,=\\,P^{\\pi}$ , and  the discounted state visitation frequency   \n78 by $d_{\\pi}$ so that $\\begin{array}{r}{d_{\\pi}^{\\top}\\,=\\,(1\\,-\\,\\gamma)\\sum_{t=0}^{\\infty}\\gamma^{t}\\dot{\\nu}^{\\top}(P^{\\pi})^{t}}\\end{array}$ . Also, let $V^{\\pi}\\,\\in\\,\\mathbb{R}^{S}$ be the value function of $\\pi$   \n79 defined by $\\begin{array}{r}{\\mathrm{~}\\mathrm{~}V^{\\pi}(s)\\,=\\,\\mathbb{E}^{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}r\\left(s_{t},\\pi(s_{t})\\right)\\,|\\,s_{0}=s\\right]}\\end{array}$ , and let $Q^{\\pi}\\,\\in\\,\\mathbb{R}^{S\\times A}$ be the \u22c6Q-function   \n80 such that $Q^{\\pi}(s,a)\\,=\\,\\mathbb{E}^{\\pi}\\left[r(s,\\bar{a})+\\gamma V^{\\pi}(s^{\\prime})\\right]$ . Our goal is to find an optimal policy such that   \n81 $V^{\\star}(s)\\equiv V^{\\pi^{\\star}}(s)=\\operatorname*{max}_{\\pi}V^{\\pi}(s)$ , $\\forall s\\in S$ .   \n82 For the analysis in Section 4, we introduce the following notation. Denote by $\\Theta\\in\\mathbb{R}^{S}$ the vector   \n83 representation of $\\theta(s)\\;\\forall s\\in S$ . For a vector $u$ , denote by $\\exp(u)$ the coordinate-wise exponent of   \n84 $u$ and by $D(u)$ the diagonal square matrix with $u$ in its diagonal. For a matrix $A$ , denote its $i$ -th   \n85 eigenvalue by $\\lambda_{i}(A)$ . Denote the $k$ -dimensional identity matrix and all-ones vector by $I_{k}$ and ${\\bf1}_{k}$ ,   \n86 respectively. Also, denote the trace operator by $\\mathrm{Tr}$ . Finally, we treat all vectors as column vectors. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "87 2.1 Policy Gradient ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "88 PG schemes seek to maximize the cumulative reward as a function of the policy $\\pi_{\\theta}(a|s)$ by performing   \n89 gradient steps on $\\theta$ . The celebrated Policy Gradient Theorem [Sutton et al., 1999] states that ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\frac{\\partial}{\\partial\\theta}\\nu^{\\top}V^{\\pi_{\\theta}}=\\mathbb{E}_{s\\sim d_{\\pi_{\\theta}},a\\sim\\pi_{\\theta}(\\cdot|s)}\\left[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{\\pi_{\\theta}}(s,a)\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "90 The variance of the gradient is thus ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname{Var}_{s\\sim d_{\\pi_{\\theta}},a\\sim\\pi_{\\theta}(\\cdot|s)}\\left(\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{\\pi_{\\theta}}(s,a)\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "91 In the notation above, we denote the variance of a vector random variable $X$ by ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname{Var}_{x}\\left(X\\right)=\\operatorname{Tr}\\left[\\mathbb{E}_{x}\\left[\\left(X-\\mathbb{E}_{x}X\\right)^{\\top}\\left(X-\\mathbb{E}_{x}X\\right)\\right]\\right],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "92 similarly as in [Greensmith et al., 2004]. From now on, we drop the subscript from Var in (1)   \n93 for brevity. When the action space is discrete, a commonly used parameterized policy is softmax:   \n94 $\\pi_{\\theta}(a|s)\\stackrel{.}{\\propto}\\exp\\left(\\theta(s,a)\\right)$ , where $\\theta:S\\times A\\rightarrow\\mathbb{R}$ is a state-action parameterization. ", "page_idx": 2}, {"type": "text", "text": "95 3 SoftTreeMax: Exponent of trajectories ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "96 We introduce a new family of policies called SoftTreeMax, which are a model-based generalization   \n97 of the popular softmax. We propose two variants: Cumulative (C-SoftTreeMax) and Exponenti  \n98 ated (E-SoftTreeMax). In both variants, we replace the generic softmax logits $\\theta(s,a)$ with the   \n99 score of a trajectory of horizon $d$ starting from $(s,a)$ , generated by applying a behavior policy   \n100 $\\pi_{b}$ . In C-SoftTreeMax, we exponentiate the expectation of the logits. In E-SoftTreeMax, we first   \n101 exponentiate the logits and then only compute their expectation.   \n102 Logits. We define the SoftTreeMax logit $\\ell_{s,a}(d;\\theta)$ to be the random variable depicting the score of a   \n103 trajectory of horizon $d$ starting from $(s,a)$ and following the policy $\\pi_{b}$ : ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\ell_{s,a}(d;\\theta)=\\gamma^{-d}\\left[\\sum_{t=0}^{d-1}\\gamma^{t}r_{t}+\\gamma^{d}\\theta(s_{d})\\right].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "104 In the above expression, note that $s_{0}~=~s$ , $a_{0}\\,=\\,a$ , $a_{t}\\,\\sim\\,\\pi_{b}(\\cdot|s_{t})\\,\\,\\forall t\\,\\geq\\,1$ , and $r_{t}\\,\\equiv\\,r\\left(s_{t},a_{t}\\right)$ .   \n105 For brevity of the analysis, we let the parametric score $\\theta$ in (2) be state-based, similarly to a value   \n106 function. Instead, one could use a state-action input analogous to a Q-function. Thus, SoftTreeMax   \n107 can be integrated into the two types of implementation of RL algorithms in standard packages. Lastly,   \n108 the preceding $\\gamma^{-d}$ scales the $\\theta$ parametrization to correspond to its softmax counerpart. ", "page_idx": 2}, {"type": "text", "text": "109 C-SoftTreeMax. Given an inverse temperature parameter $\\beta$ , we let C-SoftTreeMax be ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pi_{d,\\theta}^{\\mathtt{C}}(a|s)\\propto\\exp\\left[\\beta\\mathbb{E}^{\\pi_{b}}\\ell_{s,a}(d;\\theta)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "110 C-SoftTreeMax gives higher weight to actions that result in higher expected returns. While standard   \n111 softmax relies entirely on parametrization $\\theta$ , C-SoftTreeMax also interpolates a Monte-Carlo portion   \n112 of the reward. ", "page_idx": 2}, {"type": "text", "text": "113 E-SoftTreeMax. The second operator we propose is E-SoftTreeMax: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pi_{d,\\theta}^{\\mathrm{E}}(a|s)\\propto\\mathbb{E}^{\\pi_{b}}\\exp\\left[(\\beta\\ell_{s,a}(d;\\theta))\\right];\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "114 here, the expectation is taken outside the exponent. This objective corresponds to the exponentiated   \n115 reward objective which is often used for risk-sensitive RL [Howard and Matheson, 1972, Fei et al.,   \n116 2021, Noorani and Baras, 2021]. The common risk-sensitive objective is of the form $\\log\\mathbb{E}[\\exp(\\delta R)]$ ,   \n117 where $\\delta$ is the risk parameter and $R$ is the cumulative reward. Similarly to that literature, the exponent   \n118 in (4) emphasizes the most promising trajectories.   \n119 SoftTreeMax properties. SoftTreeMax is a natural model-based generalization of softmax. For   \n120 $d=0$ , both variants above coincide since (2) becomes deterministic. In that case, for a state-action   \n121 parametrization, they reduce to standard softmax. When $\\beta\\rightarrow0$ , both variants again coincide and   \n122 sample actions uniformly (exploration). When $\\beta\\,\\rightarrow\\,\\infty$ , the policies become deterministic and   \n123 greedily optimize for the best trajectory (exploitation). For C-SoftTreeMax, the best trajectory is   \n124 defined in expectation, while for E-SoftTreeMax it is defined in terms of the best sample path.   \n125 SoftTreeMax convergence. Under regularity conditions, for any parametric policy, PG converges   \n126 to local optima [Bhatnagar et al., 2009], and thus also SoftTreeMax. For softmax PG, asymptotic   \n127 [Agarwal et al., 2021] and rate results [Mei et al., 2020b] were recently obtained, by showing that   \n128 the gradient is strictly positive everywhere [Mei et al., 2020b, Lemmas 8-9]. We conjecture that   \n129 SoftTreeMax satisfies the same property, being a generalization of softmax, but formally proving it is   \n130 subject to future work.   \n131 SoftTreeMax gradient. The two variants of SoftTreeMax involve an expectation taken over $S^{d}$   \n132 many trajectories from the root state $s$ and weighted according to their probability. Thus, during   \n133 the PG training process, the gradient $\\nabla_{\\theta}\\log\\pi_{\\theta}$ is calculated using a weighted sum of gradients over   \n134 all reachable states starting from $s$ . Our method exploits the exponential number of trajectories to   \n135 reduce the variance while improving performance. Indeed, in the next section we prove that the   \n136 gradient variance of SoftTreeMax decays exponentially fast as a function of the behavior policy $\\pi_{b}$   \n137 and trajectory length $d$ . In the experiments in Section 6, we also show how the practical version   \n138 of SoftTreeMax achieves a significant reduction in the noise of the PG process and leads to faster   \n139 convergence and higher reward. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "140 4 Theoretical Analysis ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "141 In this section, we first bound the variance of PG when using the SoftTreeMax policy. Later, we   \n142 discuss how the gradient bias resulting due to approximate forward models diminishes as a function   \n143 of the approximation error, while retaining the same variance decay.   \n144 We show that the variance decreases exponentially with the tree depth, and the rate is determined   \n145 by the second eigenvalue of the transition kernel induced by $\\pi_{b}$ . Specifically, we bound the same   \n146 expression for variance as appears in [Greensmith et al., 2004, Sec. 3.5] and [Wu et al., 2018, Sec. A,   \n147 Eq. (21)]. Other types of analysis could instead have focused on the estimation aspect in the context   \n148 of sampling [Zhang et al., 2021, Shen et al., 2019, Pham et al., 2020]. Indeed, in our implementation   \n149 in Section 5, we manage to avoid sampling and directly compute the expectations in Eqs. (3) and   \n150 (4). As we show later, we do so by leveraging efficient parallel simulation on the GPU in feasible   \n151 run-time. In our application, due to the nature of the finite action space and quasi-deterministic Atari   \n152 dynamics [Bellemare et al., 2013], our expectation estimator is noiseless. We encourage future work   \n153 to account for the finite-sample variance component. We defer all the proofs to Appendix A. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "154 We begin with a general variance bound that holds for any parametric policy. ", "page_idx": 3}, {"type": "text", "text": "155 Lemma 4.1 (Bound on the policy gradient variance). Let $\\nabla_{\\theta}\\log\\pi_{\\theta}({\\cdot}|s)\\in\\mathbb{R}^{A\\times\\dim(\\theta)}$ be a matrix   \n156 whose $a$ -th row is $\\nabla_{\\boldsymbol{\\theta}}\\log\\pi_{\\boldsymbol{\\theta}}(a|s)^{\\top}$ . For any parametric policy $\\pi_{\\theta}$ and function $Q^{\\pi_{\\theta}}:S\\times\\mathcal{A}\\rightarrow\\mathbb{R}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathrm{Var}\\left(\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{\\pi_{\\theta}}(s,a)\\right)\\leq\\operatorname*{max}_{s,a}\\big[Q^{\\pi_{\\theta}}(s,a)\\big]^{2}\\operatorname*{max}_{s}\\bigl\\|\\nabla_{\\theta}\\log\\pi_{\\theta}(\\cdot|s)\\bigr\\|_{F}^{2}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "157 Hence, to bound (1), it is sufficient to bound the Frobenius norm $\\|\\nabla_{\\theta}\\log\\pi_{\\theta}(\\cdot|s)\\|_{F}$ for any $s$ . ", "page_idx": 3}, {"type": "text", "text": "158 Note that SoftTreeMax does not reduce the gradient uniformly, which would have been equivalent   \n159 to a trivial change in the learning rate. While the gradient norm shrinks, the gradient itself scales   \n160 differently along the different coordinates. This scaling occurs along different eigenvectors, as a   \n161 function of problem parameters $(P,\\,\\theta)$ and our choice of behavior policy $(\\pi_{b})$ , as can be seen in   \n162 the proof of the upcoming Theorem 4.4. This allows SoftTreeMax to learn a good \u201cshrinkage\u201d that,   \n163 while reducing the overall gradient, still updates the policy quickly enough. This reduction in norm   \n164 and variance resembles the idea of gradient clipping Zhang et al. [2019], where the gradient is scaled   \n165 to reduce its variance, thus increasing stability and improving overall performance.   \n166 A common assumption in the RL literature [Szepesv\u00e1ri, 2010] that we adopt for the remainder of   \n167 the section is that the transition matrix $P^{\\pi_{b}}$ , induced by the behavior policy $\\pi_{b}$ , is irreducible and   \n168 aperiodic. Consequently, its second highest eigenvalue satisfies $|\\lambda_{2}(P^{\\pi_{b}})|<1$ .   \n169 From now on, we divide the variance results for the two variants of SoftTreeMax into two subsec  \n170 tions. For C-SoftTreeMax, the analysis is simpler and we provide an exact bound. The case of   \n171 E-SoftTreeMax is more involved and we provide for it a more general result. In both cases, we show   \n172 that the variance decays exponentially with the planning horizon. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "173 4.1 Variance of C-SoftTreeMax ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "174 We express C-SoftTreeMax in vector form as follows. ", "page_idx": 4}, {"type": "text", "text": "175 Lemma 4.2 (Vector form of C-SoftTreeMax). For $d\\geq1$ , (3) is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pi_{d,\\theta}^{C}(\\cdot|s)=\\frac{\\exp\\left[\\beta\\left(C_{s,d}+P_{s}\\left(P^{\\pi_{b}}\\right)^{d-1}\\Theta\\right)\\right]}{I_{A}^{\\top}\\exp\\left[\\beta\\left(C_{s,d}+P_{s}\\left(P^{\\pi_{b}}\\right)^{d-1}\\Theta\\right)\\right]},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "176 where ", "page_idx": 4}, {"type": "equation", "text": "$$\nC_{s,d}=\\gamma^{-d}R_{s}+P_{s}\\left[\\sum_{h=1}^{d-1}\\gamma^{h-d}\\left(P^{\\pi_{b}}\\right)^{h-1}\\right]R_{\\pi_{b}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "177 The vector $C_{s,d}\\in\\mathbb{R}^{A}$ represents the cumulative discounted reward in expectation along the trajectory   \n178 of horizon $d$ . This trajectory starts at state $s$ , involves an initial reward dictated by $R_{s}$ and an   \n179 initial transition as per $P_{s}$ . Thereafter, it involves rewards and transitions specified by ${\\cal R}_{\\pi_{b}}$ and $P^{\\pi_{b}}$ ,   \n180 respectively. Once the trajectory reaches depth $d$ , the score function $\\theta(s_{d})$ is applied,. ", "page_idx": 4}, {"type": "text", "text": "181 Lemma 4.3 (Gradient of C-SoftTreeMax). The $C_{}$ -SoftTreeMax gradient is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{\\theta}\\log\\pi_{d,\\theta}^{C}=\\beta\\left[I_{A}-I_{A}(\\pi_{d,\\theta}^{C})^{\\top}\\right]P_{s}\\left(P^{\\pi_{b}}\\right)^{d-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "182 in $\\mathbb{R}^{A\\times S}$ , where for brevity, we drop the s index in the policy above, i.e., $\\pi_{d,\\theta}^{C}\\equiv\\pi_{d,\\theta}^{C}(\\cdot|s)$ ", "page_idx": 4}, {"type": "text", "text": "183 We are now ready to present our first main result: ", "page_idx": 4}, {"type": "text", "text": "184 Theorem 4.4 (Variance decay of C-SoftTreeMax). For every $Q:S\\times A\\rightarrow\\mathbb{R}$ , the $C$ -SoftTreeMax   \n185 policy gradient variance is bounded by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{Var}\\left(\\nabla_{\\theta}\\log\\pi_{d,\\theta}^{C}(a|s)Q(s,a)\\right)\\leq2\\frac{A^{2}S^{2}\\beta^{2}}{(1-\\gamma)^{2}}|\\lambda_{2}(P^{\\pi_{b}})|^{2(d-1)}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "186 We provide the full proof in Appendix A.4, and briefly outline its essence here. ", "page_idx": 4}, {"type": "text", "text": "187 Proof outline. Lemma 4.1 allows us to bound the variance using a direct bound on the gradient   \n188 norm. The gradient is given in Lemma 4.3 as a product of three matrices, which we now study from   \n189 right to left. The matrix $P^{\\pi_{b}}$ is a row-stochastic matrix. Because the associated Markov chain is   \n190 irreducible and aperiodic, it has a unique stationary distribution. This implies that $P^{\\pi_{b}}$ has one and   \n191 only one eigenvalue equal to 1; all others have magnitude strictly less than 1. Let us suppose that   \n192 all these other eigenvalues have multiplicity 1 (the general case with repeated eigenvalues can be   \n193 handled via Jordan decompositions as in [Pelletier, 1998, Lemma1]). Then, $P^{\\pi_{b}}$ has the spectral   \n194 decomposition $\\begin{array}{r}{P^{\\pi_{b}}=\\mathbf{1}_{S}\\mu_{\\pi_{b}}^{\\top}+\\sum_{i=2}^{S}\\lambda_{i}v_{i}u_{i}^{\\top}}\\end{array}$ , where $\\lambda_{i}$ is the $i$ -th eigenvalue of $P^{\\pi_{b}}$ (ordered in   \n119965 ediegsecnevnedcitnogr so, rrdeesrp eacctcivoerldyi,n ag ntdo  tthheerierf omrae $\\begin{array}{r}{(P^{\\pi_{b}})^{d-1}=\\mathbf{1}_{S}\\mu_{\\pi_{b}}^{\\top}+\\sum_{i=2}^{S}\\lambda_{i}^{d-1}v_{i}u_{i}^{\\top}}\\end{array}$ $u_{i}$ $v_{i}$ n.ding left and right   \n197 The second matrix in the gradient relation in Lemma 4.3, $P_{s}$ , is a rectangular transition ma  \n198 trix that translates the vector of all ones from dimension $S$ to $A\\ :\\ P_{s}\\mathbf{1}_{S}\\ =\\ \\mathbf{1}_{A}$ . Lastly, the   \n199 first matrix $\\left[I_{A}-\\mathbf{1}_{A}\\big(\\\\pi_{d,\\theta}^{\\mathrm{C}}\\big)^{\\top}\\right]$ is a projection whose null-space includes the vector ${\\mathbf{1}}_{A}$ , i.e.,   \n200 $\\left[I_{A}-\\mathbf{1}_{A}\\big(\\boldsymbol{\\pi}_{d,\\theta}^{\\mathrm{C}}\\big)^{\\top}\\right]\\mathbf{1}_{A}=0.$ Combining the three properties above when multiplying the three matri  \n201 ces of the gradient, it is easy to see that the first term in the expression for $(P^{\\pi_{b}})^{d-1}$ gets canceled,   \n202 and we are left with bounded summands scaled by $\\lambda_{i}(P^{\\pi_{b}})^{d-1}$ . Recalling that $|\\lambda_{i}(P^{\\pi_{b}})|<1$ and   \n203 that $|\\lambda_{2}|\\geq|\\lambda_{3}|\\geq...$ for $i=2,\\dots,S.$ , we obtain the desired result. \u53e3   \n204 Theorem 4.4 guarantees that the variance of the gradient decays exponentially with $d$ . It also provides   \n205 a novel insight for choosing the behavior policy $\\pi_{b}$ as the policy that minimizes the absolute second   \n206 eigenvalue of the $P^{\\pi_{b}}$ . Indeed, the second eigenvalue of a Markov chain relates to its connectivity   \n207 and its rate of convergence to the stationary distribution [Levin and Peres, 2017].   \n208 Optimal variance decay. For the strongest reduction in variance, the behavior policy $\\pi_{b}$ should be   \n209 chosen to achieve an induced Markov chain whose transitions are state-independent. In that case, $P^{\\pi_{b}}$   \n210 is a rank one matrix of the form $\\mathbf{1}_{S}\\mu_{\\pi_{b}}^{\\top}$ , and $\\lambda_{2}(P^{\\pi_{b}})=0$ . Then, $\\operatorname{Var}\\left(\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q(s,a)\\right)=0$ .   \n211 Naturally, this can only be done for pathological MDPs; see Appendix C.1 for a more detailed   \n212 discussion. Nevertheless, as we show in Section 5, we choose our tree expansion policy to reduce the   \n213 variance as best as possible.   \n214 Worst-case variance decay. In contrast, and somewhat surprisingly, when $\\pi_{b}$ is chosen so that the   \n215 dynamics is deterministic, there is no guarantee that it will decay exponentially fast. For example, if   \n216 $P^{\\pi_{b}}$ is a permutation matrix, then $\\lambda_{2}(P^{\\pi_{b}})=1$ , and advancing the tree amounts to only updating the   \n217 gradient of one state for every action, as in the basic softmax. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "218 4.2 Variance of E-SoftTreeMax ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "219 The proof of the variance bound for E-SoftTreeMax is similar to that of C-SoftTreeMax, but more   \n220 involved. It also requires the assumption that the reward depends only on the state, i.e. $r(s,a)\\equiv r(s)$ .   \n221 This is indeed the case in most standard RL environments such as Atari and Mujoco. ", "page_idx": 5}, {"type": "text", "text": "222 Lemma 4.5 (Vector form of E-SoftTreeMax). For $d\\geq1$ , (4) is given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pi_{d,\\theta}^{E}(\\cdot|s)=\\frac{E_{s,d}\\exp(\\beta\\Theta)}{1_{A}^{\\top}E_{s,d}\\exp(\\beta\\Theta)},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "223 where ", "page_idx": 5}, {"type": "equation", "text": "$$\nE_{s,d}=P_{s}\\prod_{h=1}^{d-1}\\left(D\\left(\\exp(\\beta\\gamma^{h-d}R)\\right)P^{\\pi_{b}}\\right).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "224 The vector R above is the $S$ -dimensional vector whose s-th coordinate is $r(s)$ . ", "page_idx": 5}, {"type": "text", "text": "225 The matrix $E_{s,d}\\in\\mathbb{R}^{A\\times S}$ has a similar role to $C_{s,d}$ from (5), but it represents the exponentiated   \n226 cumulative discounted reward. Accordingly, it is a product of $d$ matrices as opposed to a sum. It   \n227 captures the expected reward sequence starting from $s$ and then iteratively following $P^{\\pi_{b}}$ . After $d$   \n228 steps, we apply the score function on the last state as in (6). ", "page_idx": 5}, {"type": "text", "text": "229 Lemma 4.6 (Gradient of E-SoftTreeMax). The $E$ -SoftTreeMax gradient is given by ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\log\\pi_{d,\\theta}^{E}=\\beta\\left[I_{A}-I_{A}(\\pi_{d,\\theta}^{E})^{\\top}\\right]\\times\\frac{D\\left(\\pi_{d,\\theta}^{E}\\right)^{-1}E_{s,d}D(\\exp(\\beta\\Theta))}{\\mathbf{1}_{A}^{\\top}E_{s,d}\\exp(\\beta\\Theta)}\\quad\\in\\quad\\mathbb{R}^{A\\times S},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "230 where for brevity, we drop the s index in the policy above, i.e., $\\pi_{d,\\theta}^{E}\\equiv\\pi_{d,\\theta}^{E}(\\cdot|s)$ . ", "page_idx": 5}, {"type": "text", "text": "231 This gradient structure is harder to handle than that of C-SoftTreeMax in Lemma 4.3, but here we   \n232 also can bound the decay of the variance nonetheless. ", "page_idx": 5}, {"type": "text", "text": "233 Theorem 4.7 (Variance decay of E-SoftTreeMax). There exists $\\alpha\\in(0,1)$ such that, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{Var}\\left(\\nabla_{\\theta}\\log\\pi_{d,\\theta}^{E}(a|s)Q(s,a)\\right)\\in\\mathcal{O}\\left(\\beta^{2}\\alpha^{2d}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "234 for every $Q$ . Further, if $P^{\\pi_{b}}$ is reversible or if the reward is constant, then $\\alpha=|\\lambda_{2}(P^{\\pi_{b}})|$ . ", "page_idx": 5}, {"type": "text", "text": "235 Theory versus Practice. We demonstrate the above result in simulation. We draw a random finite   \n236 MDP, parameter vector $\\Theta\\in\\mathbb{R}_{+}^{S}$ , and behavior policy $\\pi_{b}$ . We then empirically compute the PG   \n237 variance of E-SoftTreeMax as given in (1) and compare it to $|\\lambda_{2}(P^{\\pi_{b}})|^{d}$ . We repeat this experiment   \n238 three times for different $P^{\\pi_{b}}:(\\mathrm{i})$ close to uniform, (ii) drawn randomly, and (iii) close to a permutation   \n239 matrix. As seen in Figure 1, the empirical variance and our bound match almost identically. This   \n240 also suggests that $\\alpha=|\\lambda_{2}(P^{\\pi_{b}})|$ in the general case and not only when $P^{\\pi_{b}}$ is reversible or when   \n241 the reward is constant. ", "page_idx": 5}, {"type": "text", "text": "242 4.3 Bias with an Approximate Forward Model ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "243 The definition of the two SoftTreeMax variants involves the knowledge of the underlying environment,   \n244 in particular the value of $P$ and $r$ . However, in practice, we often can only learn approximations of   \n245 the dynamics from interactions, e.g., using NNs [Ha and Schmidhuber, 2018, Schrittwieser et al.,   \n246 2020]. Let $\\hat{P}$ and $\\hat{r}$ denote the approximate kernel and reward functions, respectively. In this section,   \n247 we study the consequences of the approximation error on the C-SoftTreeMax gradient. ", "page_idx": 5}, {"type": "image", "img_path": "PlBUSoSUJG/tmp/7e51ec018fdd04e38e973a49fa85eab9a9294252276f645f6bbf85cd2a1983ee.jpg", "img_caption": ["Figure 2: SoftTreeMax policy. Our exhaustive parallel tree expansion iterates on all actions at each state up to depth $d$ $\\mathit{\\Theta}(=2$ here). The leaf state of every trajectory is used as input to the policy network. The output is then added to the trajectory\u2019s cumulative reward as described in (2). I.e., instead of the standard softmax logits, we add the cumulative discounted reward to the policy network output. This policy is differentiable and can be easily integrated into any PG algorithm. In this work, we build on PPO and use its loss function to train the policy network. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 1: A comparison of the empirical PG variance and our bound for E-SoftTreeMax on randomly drawn MDPs. We present three cases for $P^{\\pi_{b}}:(\\mathrm{i})$ close to uniform, (ii) drawn randomly, and (iii) close to a permutation matrix. This experiment verifies the optimal and worse-case rate decay cases. The variance bounds here are taken from Theorem 4.7 where we substitute $\\alpha\\,=\\,|\\lambda_{2}(P^{\\pi_{b}})|$ . To account for the constants, we match the values for the first point in $d=1$ . ", "page_idx": 6}, {"type": "text", "text": "248 Let $\\hat{\\pi}_{d,\\theta}^{\\mathrm{C}}$ be the C-SoftTreeMax policy defined given the approximate forward model introduced   \n249 above. That is, let $\\hat{\\pi}_{d,\\theta}^{\\mathrm{C}}$ be defined exactly as in (5), but using $\\hat{R}_{s},\\hat{P}_{s},\\hat{R}_{\\pi_{b}}$ and $\\hat{P}^{\\pi_{b}}$ , instead of their   \n250 unperturbed counterparts from Section 2. Then, the variance of the corresponding gradient again   \n251 decays exponentially with a decay rate of $\\lambda_{2}(\\hat{P}^{\\pi_{b}})$ . However, a gradient bias is introduced. In the   \n252 following, we bound this bias in terms of the approximation error and other problem parameters. The   \n253 proof is provided in Appendix A.9.   \n254 Theorem 4.8. Let \u03f5 be the maximal model mis-specification, i.e., let $\\operatorname*{max}\\{\\|P-\\hat{P}\\|,\\|r-\\hat{r}\\|\\}=\\epsilon$ .   \n255 Then the policy gradient bias due to $\\hat{\\pi}_{d,\\theta}^{C}$ satisfies ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\n\\left\\lVert\\frac{\\partial}{\\partial\\theta}\\left(\\nu^{\\top}V^{\\pi_{d,\\theta}^{c}}\\right)-\\frac{\\partial}{\\partial\\theta}\\left(\\nu^{\\top}V^{\\hat{\\pi}_{d,\\theta}^{c}}\\right)\\right\\rVert=\\quad\\mathcal{O}\\left(\\frac{1}{(1-\\gamma)^{2}}S\\beta^{2}d\\epsilon\\right).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "256 To the best of our knowledge, Theorem 4.8 is the first result that bounds the bias of the gradient   \n257 of a parametric policy due to an approximate model. It states that if the learned model is accurate   \n258 enough, we expect similar convergence properties for C-SoftTreeMax as we would have obtained   \n259 with the true dynamics. It also suggests that higher temperature (lower $\\beta$ ) reduces the bias. In this   \n260 case, the logits get less weight, with the extreme of $\\beta=0$ corresponding to a uniform policy that has   \n261 no bias. Lastly, the error scales linearly with $d$ : the policy suffers from cumulative error as it relies   \n262 on further-looking states in the approximate model. ", "page_idx": 6}, {"type": "text", "text": "263 5 SoftTreeMax: Deep Parallel Implementation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "264 Following impressive successes of deep RL [Mnih et al., 2015, Silver et al., 2016], using deep NNs   \n265 in RL is standard practice. Depending on the RL algorithm, a loss function is defined and gradients   \n266 on the network weights can be calculated. In PG methods, the scoring function used in the softmax is   \n267 commonly replaced by a neural network $W_{\\theta}$ : $\\pi_{\\theta}(a|s)\\propto\\exp\\left(W_{\\theta}(s,a)\\right)$ . Similarly, we implement   \n268 SoftTreeMax by replacing $\\theta(s)$ in (2) with a neural network $W_{\\theta}(s)$ . Although both variants of   \n269 SoftTreeMax from Section 3 involve computing an expectation, this can be hard in general. One   \n270 approach to handle it is with sampling, though these introduce estimation variance into the process.   \n271 We leave the question of sample-based theory and algorithmic implementations for future work.   \n272 Instead, in finite action space environments such as Atari, we compute the exact expectation in   \n273 SoftTreeMax with an exhaustive TS of depth $d$ . Despite the exponential computational cost of   \n274 spanning the entire tree, recent advancements in parallel GPU-based simulation allow efficient   \n275 expansion of all nodes at the same depth simultaneously [Dalal et al., 2021, Rosenberg et al., 2022].   \n276 This is possible when a simulator is implemented on GPU [Dalton et al., 2020, Makoviychuk   \n277 et al., 2021, Freeman et al., 2021], or when a forward model is learned [Kim et al., 2020, Ha and   \n278 Schmidhuber, 2018]. To reduce the complexity to be linear in depth, we apply tree pruning to a   \n279 limited width in all levels. We do so by sub-sampling only the most promising branches at each level.   \n280 Limiting the width drastically improves runtime, and enables respecting GPU memory limits, with   \n281 only a small sacrifice in performance.   \n282 To summarize, in the practical SoftTreeMax algorithm we perform an exhaustive tree expansion with   \n283 pruning to obtain trajectories up to depth $d$ . We expand the tree with equal weight to all actions, which   \n284 corresponds to a uniform tree expansion policy $\\pi_{b}$ . We apply a neural network on the leaf states, and   \n285 accumulate the result with the rewards along each trajectory to obtain the logits in (2). Finally, we   \n286 aggregate the results using C-SoftTreeMax. We leave experiments E-SoftTreeMax for future work   \n287 on risk-averse RL. During training, the gradient propagates to the NN weights of $W_{\\theta}$ . When the   \n288 gradient $\\nabla_{\\theta}\\log\\pi_{d,\\theta}$ is calculated at each time step, it updates $W_{\\theta}$ for all leaf states, similarly to   \n289 Siamese networks [Bertinetto et al., 2016]. An illustration of the policy is given in Figure 2. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "290 6 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "291 We conduct our experiments on multiple games from the Atari simulation suite [Bellemare et al.,   \n292 2013]. As a baseline, we train a PPO [Schulman et al., 2017] agent with 256 GPU workers in parallel   \n293 [Dalton et al., 2020]. For the tree expansion, we employ a GPU breadth-first as in [Dalal et al., 2021].   \n294 We then train C-SoftTreeMax 1 for depths $d\\,=\\,1\\ldots8$ , with a single worker. For depths $d\\geq3$ ,   \n295 we limited the tree to a maximum width of 1024 nodes and pruned trajectories with low estimated   \n296 weights. Since the distributed PPO baseline advances significantly faster in terms of environment   \n297 steps, for a fair comparison, we ran all experiments for one week on the same machine. For more   \n298 details see Appendix B.   \n299 In Figure 3, we plot the reward and variance of SoftTreeMax for each game, as a function of depth.   \n300 The dashed lines are the results for PPO. Each value is taken after convergence, i.e., the average   \n301 over the last $20\\%$ of the run. The numbers represent the average over five seeds per game. The plot   \n302 conveys three intriguing conclusions. First, in all games, SoftTreeMax achieves significantly higher   \n303 reward than PPO. Its gradient variance is also orders of magnitude lower than that of PPO. Second,   \n304 the reward and variance are negatively correlated and mirror each other in almost all games. This   \n305 phenomenon demonstrates the necessity of reducing the variance of PG for improving performance.   \n306 Lastly, each game has a different sweet spot in terms of optimal tree depth. Recall that we limit the   \n307 run-time in all experiments to one week The deeper the tree, the slower each step and the run consists   \n308 of less steps. This explains the non-monotone behavior as a function of depth. For a more thorough   \n309 discussion on the sweet spot of different games, see Appendix B.3. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "310 7 Related Work ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "311 Softmax Operator. The softmax policy became a canonical part of PG to the point where theoretical   \n312 results of PG focus specifically on it [Zhang et al., 2021, Mei et al., 2020b, Li et al., 2021, Ding et al.,   \n313 2022]. Even though we focus on a tree extension to the softmax policy, our methodology is general   \n314 and can be easily applied to other discrete or continuous parameterized policies as in [Mei et al.,   \n315 2020a, Miahi et al., 2021, Silva et al., 2019]. Tree Search. One famous TS algorithm is Monte-Carlo   \n316 TS (MCTS; [Browne et al., 2012]) used in AlphaGo [Silver et al., 2016] and MuZero [Schrittwieser   \n317 et al., 2020]. Other algorithms such as Value Iteration, Policy Iteration and DQN were also shown to   \n318 give an improved performance with a tree search extensions [Efroni et al., 2019, Dalal et al., 2021].   \n319 Parallel Environments. In this work we used accurate parallel models that are becoming more   \n320 common with the increasing popularity of GPU-based simulation [Makoviychuk et al., 2021, Dalton   \n321 et al., 2020, Freeman et al., 2021]. Alternatively, in relation to Theorem 4.8, one can rely on recent   \n322 works that learn the underlying model [Ha and Schmidhuber, 2018, Schrittwieser et al., 2020] and   \n323 use an approximation of the true dynamics. Risk Aversion. Previous work considered exponential   \n324 utility functions for risk aversion [Chen et al., 2007, Garc\u0131a and Fern\u00e1ndez, 2015, Fei et al., 2021].   \n325 This utility function is the same as E-SoftTreeMax formulation from (4), but we have it directly   \n326 in the policy instead of the objective. Reward-free RL. We showed that the gradient variance is   \n327 minimized when the transitions induced by the behavior policy $\\pi_{b}$ are uniform. This is expressed by   \n328 the second eigenvalue of the transition matrix $P^{\\pi_{b}}$ . This notion of uniform exploration is common to   \n329 the reward-free RL setup [Jin et al., 2020]. Several such works also considered the second eigenvalue   \n330 in their analysis [Liu and Brunskill, 2018, Tarbouriech and Lazaric, 2019]. ", "page_idx": 7}, {"type": "image", "img_path": "PlBUSoSUJG/tmp/03c62efd0ae004504e0015a6daa82dcb9fc1ff3b724b9d0b713650eeb3c47b8d.jpg", "img_caption": ["Figure 3: Reward and Gradient variance: GPU SoftTreeMax (single worker) vs PPO (256 GPU workers). The blue reward plots show the average of 50 evaluation episodes. The red variance plots show the average gradient variance of the corresponding training runs, averaged over five seeds. The dashed lines represent the same for PPO. Note that the variance y-axis is in log-scale. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "331 8 Discussion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "332 In this work, we introduced for the first time a differentiable parametric policy that combines TS with   \n333 PG. We proved that SoftTreeMax is essentially a variance reduction technique and explained how to   \n334 choose the expansion policy to minimize the gradient variance. It is an open question whether optimal   \n335 variance reduction corresponds to the appealing regret properties the were put forward by UCT   \n336 [Kocsis and Szepesv\u00e1ri, 2006]. We believe that this can be answered by analyzing the convergence   \n337 rate of SoftTreeMax, relying on the bias and variance results we obtained here.   \n338 As the learning process continues, the norm of the gradient and the variance both become smaller.   \n339 On the face of it, one can ask if the gradient becomes small as fast as the variance or even faster can   \n340 there be any meaningful learning? As we showed in the experiments, learning happens because the   \n341 variance reduces fast enough (a variance of 0 represents deterministic learning, which is fastest).   \n342 Finally, our work can be extended to infinite action spaces. The analysis can be extended to infinite  \n343 dimension kernels that retain the same key properties used in our proofs. In the implementation, the   \n344 tree of continuous actions can be expanded by maintaining a parametric distribution over actions that   \n345 depend on $\\theta$ . This approach can be seen as a tree adaptation of MPPI [Williams et al., 2017]. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "346 9 Reproducibility and Limitations ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "347 In this submission, we include the code as part of the supplementary material. We also include a   \n348 docker file for setting up the environment and a README file with instructions on how to run both   \n349 training and evaluation. The environment engine is an extension of Atari-CuLE [Dalton et al., 2020],   \n350 a CUDA-based Atari emulator that runs on GPU. Our usage of a GPU environment is both a novelty   \n351 and a current limitation of our work. ", "page_idx": 8}, {"type": "text", "text": "352 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "353 A. Agarwal, S. M. Kakade, J. D. Lee, and G. Mahajan. On the theory of policy gradient methods:   \n354 Optimality, approximation, and distribution shift. J. Mach. Learn. Res., 22(98):1\u201376, 2021.   \n355 M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An   \n356 evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253\u2013279,   \n357 2013.   \n358 L. Bertinetto, J. Valmadre, J. F. Henriques, A. Vedaldi, and P. H. Torr. Fully-convolutional siamese   \n359 networks for object tracking. In European conference on computer vision, pages 850\u2013865. Springer,   \n360 2016.   \n361 S. Bhatnagar, R. S. Sutton, M. Ghavamzadeh, and M. Lee. Natural actor\u2013critic algorithms. Automatica,   \n362 45(11):2471\u20132482, 2009.   \n363 C. B. Browne, E. Powley, D. Whitehouse, S. M. Lucas, P. I. Cowling, P. Rohlfshagen, S. Tavener,   \n364 D. Perez, S. Samothrakis, and S. Colton. A survey of monte carlo tree search methods. IEEE   \n365 Transactions on Computational Intelligence and AI in games, 4(1):1\u201343, 2012.   \n366 S. Chatterjee and E. Seneta. Towards consensus: Some convergence theorems on repeated averaging.   \n367 Journal of Applied Probability, 14(1):89\u201397, 1977.   \n368 X. Chen, M. Sim, D. Simchi-Levi, and P. Sun. Risk aversion in inventory management. Operations   \n369 Research, 55(5):828\u2013842, 2007.   \n370 G. Dalal, A. Hallak, S. Dalton, S. Mannor, G. Chechik, et al. Improve agents without retraining:   \n371 Parallel tree search with off-policy correction. Advances in Neural Information Processing Systems,   \n372 34:5518\u20135530, 2021.   \n373 S. Dalton et al. Accelerating reinforcement learning through gpu atari emulation. Advances in Neural   \n374 Information Processing Systems, 33:19773\u201319782, 2020.   \n375 Y. Ding, J. Zhang, and J. Lavaei. On the global optimum convergence of momentum-based policy   \n376 gradient. In International Conference on Artificial Intelligence and Statistics, pages 1910\u20131934.   \n377 PMLR, 2022.   \n378 Y. Efroni, G. Dalal, B. Scherrer, and S. Mannor. How to combine tree-search methods in reinforcement   \n379 learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages   \n380 3494\u20133501, 2019.   \n381 Y. Fei, Z. Yang, Y. Chen, and Z. Wang. Exponential bellman equation and improved regret bounds   \n382 for risk-sensitive reinforcement learning. Advances in Neural Information Processing Systems, 34:   \n383 20436\u201320446, 2021.   \n384 C. D. Freeman, E. Frey, A. Raichuk, S. Girgin, I. Mordatch, and O. Bachem. Brax-a differen  \n385 tiable physics engine for large scale rigid body simulation. In Thirty-fifth Conference on Neural   \n386 Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021.   \n387 J. Garc\u0131a and F. Fern\u00e1ndez. A comprehensive survey on safe reinforcement learning. Journal of   \n388 Machine Learning Research, 16(1):1437\u20131480, 2015.   \n389 E. Greensmith, P. L. Bartlett, and J. Baxter. Variance reduction techniques for gradient estimates in   \n390 reinforcement learning. Journal of Machine Learning Research, 5(9), 2004.   \n391 D. Ha and J. Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.   \n392 R. A. Howard and J. E. Matheson. Risk-sensitive markov decision processes. Management science,   \n393 18(7):356\u2013369, 1972.   \n394 C. Jin, A. Krishnamurthy, M. Simchowitz, and T. Yu. Reward-free exploration for reinforcement   \n395 learning. In International Conference on Machine Learning, pages 4870\u20134879. PMLR, 2020.   \n396 S. W. Kim, Y. Zhou, J. Philion, A. Torralba, and S. Fidler. Learning to simulate dynamic environments   \n397 with gamegan. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern   \n398 Recognition, pages 1231\u20131240, 2020.   \n399 L. Kocsis and C. Szepesv\u00e1ri. Bandit based monte-carlo planning. In European conference on machine   \n400 learning, pages 282\u2013293. Springer, 2006.   \n401 D. A. Levin and Y. Peres. Markov chains and mixing times, volume 107. American Mathematical   \n402 Soc., 2017.   \n403 G. Li, Y. Wei, Y. Chi, Y. Gu, and Y. Chen. Softmax policy gradient methods can take exponential   \n404 time to converge. In Conference on Learning Theory, pages 3107\u20133110. PMLR, 2021.   \n405 Y. Liu and E. Brunskill. When simple exploration is sample efficient: Identifying sufficient conditions   \n406 for random exploration to yield pac rl algorithms. arXiv preprint arXiv:1805.09045, 2018.   \n407 Y. Liu, K. Zhang, T. Basar, and W. Yin. An improved analysis of (variance-reduced) policy gradient   \n408 and natural policy gradient methods. Advances in Neural Information Processing Systems, 33:   \n409 7624\u20137636, 2020.   \n410 V. Makoviychuk, L. Wawrzyniak, Y. Guo, M. Lu, K. Storey, M. Macklin, D. Hoeller, N. Rudin,   \n411 A. Allshire, A. Handa, et al. Isaac gym: High performance gpu-based physics simulation for robot   \n412 learning. arXiv preprint arXiv:2108.10470, 2021.   \n413 A. S. Mathkar and V. S. Borkar. Nonlinear gossip. SIAM Journal on Control and Optimization, 54   \n414 (3):1535\u20131557, 2016.   \n415 J. Mei, C. Xiao, B. Dai, L. Li, C. Szepesv\u00e1ri, and D. Schuurmans. Escaping the gravitational pull of   \n416 softmax. Advances in Neural Information Processing Systems, 33:21130\u201321140, 2020a.   \n417 J. Mei, C. Xiao, C. Szepesvari, and D. Schuurmans. On the global convergence rates of softmax   \n418 policy gradient methods. In International Conference on Machine Learning, pages 6820\u20136829.   \n419 PMLR, 2020b.   \n420 E. Miahi, R. MacQueen, A. Ayoub, A. Masoumzadeh, and M. White. Resmax: An alternative   \n421 soft-greedy operator for reinforcement learning. 2021.   \n422 V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Ried  \n423 miller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement   \n424 learning. nature, 518(7540):529\u2013533, 2015.   \n425 E. Noorani and J. S. Baras. Risk-sensitive reinforce: A monte carlo policy gradient algorithm for   \n426 exponential performance criteria. In 2021 60th IEEE Conference on Decision and Control (CDC),   \n427 pages 1522\u20131527. IEEE, 2021.   \n428 M. Papini, D. Binaghi, G. Canonaco, M. Pirotta, and M. Restelli. Stochastic variance-reduced policy   \n429 gradient. In International conference on machine learning, pages 4026\u20134035. PMLR, 2018.   \n430 M. Pelletier. On the almost sure asymptotic behaviour of stochastic algorithms. Stochastic processes   \n431 and their applications, 78(2):217\u2013244, 1998.   \n432 N. Pham, L. Nguyen, D. Phan, P. H. Nguyen, M. Dijk, and Q. Tran-Dinh. A hybrid stochastic   \n433 policy gradient algorithm for reinforcement learning. In International Conference on Artificial   \n434 Intelligence and Statistics, pages 374\u2013385. PMLR, 2020.   \n435 A. Raffin, A. Hill, M. Ernestus, A. Gleave, A. Kanervisto, and N. Dormann. Stable baselines3, 2019.   \n436 A. Rosenberg, A. Hallak, S. Mannor, G. Chechik, and G. Dalal. Planning and learning with adaptive   \n437 lookahead. arXiv preprint arXiv:2201.12403, 2022.   \n438 J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lockhart,   \n439 D. Hassabis, T. Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned   \n440 model. Nature, 588(7839):604\u2013609, 2020.   \n441 J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization   \n442 algorithms. arXiv preprint arXiv:1707.06347, 2017.   \n443 Z. Shen, A. Ribeiro, H. Hassani, H. Qian, and C. Mi. Hessian aided policy gradient. In International   \n444 conference on machine learning, pages 5729\u20135738. PMLR, 2019.   \n445 A. Silva, T. Killian, I. D. J. Rodriguez, S.-H. Son, and M. Gombolay. Optimization methods for inter  \n446 pretable differentiable decision trees in reinforcement learning. arXiv preprint arXiv:1903.09338,   \n447 2019.   \n448 D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser,   \n449 I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural   \n450 networks and tree search. nature, 529(7587):484\u2013489, 2016.   \n451 R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement   \n452 learning with function approximation. Advances in neural information processing systems, 12,   \n453 1999.   \n454 C. Szepesv\u00e1ri. Algorithms for reinforcement learning. Synthesis lectures on artificial intelligence   \n455 and machine learning, 4(1):1\u2013103, 2010.   \n456 J. Tarbouriech and A. Lazaric. Active exploration in markov decision processes. In The 22nd   \n457 International Conference on Artificial Intelligence and Statistics, pages 974\u2013982. PMLR, 2019.   \n458 P. S. Thomas and E. Brunskill. Policy gradient methods for reinforcement learning with function   \n459 approximation and action-dependent baselines. arXiv preprint arXiv:1706.06643, 2017.   \n460 G. Williams, N. Wagener, B. Goldfain, P. Drews, J. M. Rehg, B. Boots, and E. A. Theodorou.   \n461 Information theoretic mpc for model-based reinforcement learning. In 2017 IEEE International   \n462 Conference on Robotics and Automation (ICRA), pages 1714\u20131721. IEEE, 2017.   \n463 C. Wu, A. Rajeswaran, Y. Duan, V. Kumar, A. M. Bayen, S. Kakade, I. Mordatch, and P. Abbeel.   \n464 Variance reduction for policy gradient with action-dependent factorized baselines. In International   \n465 Conference on Learning Representations, 2018.   \n466 P. Xu, F. Gao, and Q. Gu. An improved convergence analysis of stochastic variance-reduced policy   \n467 gradient. In Uncertainty in Artificial Intelligence, pages 541\u2013551. PMLR, 2020.   \n468 J. Zhang, T. He, S. Sra, and A. Jadbabaie. Why gradient clipping accelerates training: A theoretical   \n469 justification for adaptivity. arXiv preprint arXiv:1905.11881, 2019.   \n470 J. Zhang, C. Ni, C. Szepesvari, M. Wang, et al. On the convergence and sample efficiency of   \n471 variance-reduced policy gradient method. Advances in Neural Information Processing Systems, 34:   \n472 2228\u20132240, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "473 Appendix ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "474 A Proofs ", "page_idx": 11}, {"type": "text", "text": "475 A.1 Proof of Lemma 4.1 \u2013 Bound on the policy gradient variance ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "476 For any parametric policy $\\pi_{\\theta}$ and function $Q:S\\times A\\rightarrow\\mathbb{R}$ , ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\operatorname{Var}\\left(\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q(s,a)\\right)\\leq\\operatorname*{max}_{s,a}\\left[Q(s,a)\\right]^{2}\\operatorname*{max}_{s}\\|\\nabla_{\\theta}\\log\\pi_{\\theta}(\\cdot|s)\\|_{F}^{2},\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "477 where $\\nabla_{\\theta}\\log\\pi_{\\theta}({\\cdot}|s)\\in\\mathbb{R}^{A\\times\\dim(\\theta)}$ is a matrix whose $a$ -th row is $\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)^{\\top}$ . ", "page_idx": 11}, {"type": "text", "text": "478 Proof. The variance for a parametric policy $\\pi_{\\theta}$ is given as follows: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle\\mathrm{T}_{\\theta}\\log\\pi_{\\theta}(a|s)Q(a,s)\\right\\rangle=\\!\\!\\mathbb{E}_{s\\sim d_{\\pi_{\\theta}},a\\sim\\pi_{\\theta}(\\cdot|s)}\\left[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)^{\\top}\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q(s,a)^{2}\\right]-}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\mathbb{E}_{s\\sim d_{\\pi_{\\theta}},a\\sim\\pi_{\\theta}(\\cdot|s)}\\left[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q(s,a)\\right]^{\\top}\\mathbb{E}_{s\\sim d_{\\pi_{\\theta}},a\\sim\\pi_{\\theta}(\\cdot|s)}\\left[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q(s,a)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "479 where $Q(s,a)$ is the currently estimated Q-function and $d_{\\pi_{\\theta}}$ is the discounted state visitation frequency   \n480 induced by the policy $\\pi_{\\theta}$ . Since the second term we subtract is always positive (it is of quadratic form   \n481 $v^{\\top}v)$ we can bound the variance by the first term: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\or\\,}\\big(\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q(a,s)\\big)\\leq\\!\\!\\mathbb{E}_{s\\sim d_{\\pi_{\\theta}},a\\sim\\pi_{\\theta}(\\cdot|s)}\\left[\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)^{\\top}\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q(s,a)^{2}\\right]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\sum_{s}d_{\\pi_{\\theta}}(s)\\sum_{a}\\pi_{\\theta}(a|s)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)^{\\top}\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q(s,a)^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\displaystyle\\operatorname*{max}_{s,a}\\Big[\\big[Q(s,a)\\big]^{2}\\pi_{\\theta}(a|s)\\Big]\\sum_{s}d_{\\pi_{\\theta}}(s)\\sum_{a}\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)^{\\top}\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\displaystyle\\operatorname*{max}_{s,a}[Q(s,a)]^{2}\\operatorname*{max}_{s}\\sum_{a}\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)^{\\top}\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\operatorname*{max}_{s,a}[Q(s,a)]^{2}\\operatorname*{max}_{s}\\|\\nabla_{\\theta}\\log\\pi_{\\theta}(\\cdot|s)\\|_{F}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "482 ", "page_idx": 12}, {"type": "text", "text": "483 A.2 Proof of Lemma 4.2 \u2013 Vector form of C-SoftTreeMax ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "484 In vector form, (3) is given by ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\pi_{d,\\theta}^{\\mathrm{C}}(\\cdot|s)=\\frac{\\exp\\left[\\beta\\left(C_{s,d}+P_{s}\\left(P^{\\pi_{b}}\\right)^{d-1}\\Theta\\right)\\right]}{\\mathbf{1}_{A}^{\\top}\\exp\\left[\\beta\\left(C_{s,d}+P_{s}\\left(P^{\\pi_{b}}\\right)^{d-1}\\Theta\\right)\\right]},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "485 where ", "page_idx": 12}, {"type": "equation", "text": "$$\nC_{s,d}=\\gamma^{-d}R_{s}+P_{s}\\left[\\sum_{h=1}^{d-1}\\gamma^{h-d}\\left(P^{\\pi_{b}}\\right)^{h-1}\\right]R_{\\pi_{b}}.\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "486 Proof. Consider the vector $\\ell_{s,\\cdot}\\in\\mathbb{R}^{|A|}$ . Its expectation satisfies ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}^{\\pi_{b}}\\ell_{s,\\cdot}(d;\\theta)=\\mathbb{E}^{\\pi_{b}}\\left[\\displaystyle\\sum_{t=0}^{d-1}\\gamma^{t-d}r_{t}+\\theta(s_{d})\\right]}\\\\ &{\\qquad\\qquad=\\gamma^{-d}R_{s}+\\displaystyle\\sum_{t=1}^{d-1}\\gamma^{t-d}P_{s}(P^{\\pi_{b}})^{t-1}R_{\\pi_{b}}+P_{s}(P^{\\pi_{b}})^{d-1}\\Theta.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "487 As required. ", "page_idx": 12}, {"type": "text", "text": "488 A.3 Proof of Lemma 4.3 \u2013 Gradient of C-SoftTreeMax ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "489 The C-SoftTreeMax gradient of dimension $A\\times S$ is given by ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\nabla_{\\theta}\\log\\pi_{d,\\theta}^{\\mathrm{C}}=\\beta\\left[I_{A}-\\mathbf{1}_{A}(\\pi_{d,\\theta}^{\\mathrm{C}})^{\\top}\\right]P_{s}\\left(P^{\\pi_{b}}\\right)^{d-1},}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "490 where for brevity, we drop the $s$ index in the policy above, i.e., $\\pi_{d,\\theta}^{\\mathrm{{C}}}\\equiv\\pi_{d,\\theta}^{\\mathrm{{C}}}(\\cdot|s)$ . ", "page_idx": 12}, {"type": "text", "text": "491 Proof. The $(j,k)$ -th entry of $\\nabla_{\\boldsymbol{\\theta}}\\log\\pi_{d,\\boldsymbol{\\theta}}^{\\mathbf{C}}$ satisifes ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\theta}\\log\\pi_{d,\\theta}^{c}\\vert_{j,k}=\\frac{\\partial\\log(\\pi_{d,\\theta}^{c}(a^{j}|s))}{\\partial\\theta(s^{k})}}\\\\ &{\\qquad\\qquad\\qquad=\\beta[P_{s}(P^{\\pi_{b}})^{d-1}]_{j,k}-\\frac{\\sum_{a}\\left[\\exp\\left[\\beta\\left(C_{s,d}+P_{s}\\left(P^{\\pi_{b}}\\right)^{d-1}\\Theta\\right)\\right]\\right]_{a}\\beta\\left[P_{s}\\left(P^{\\pi_{b}}\\right)^{d-1}\\right]_{a,b}}{\\mathbf{1}_{A}^{\\top}\\exp\\left[\\beta\\left(C_{s,d}+P_{s}\\left(P^{\\pi_{b}}\\right)^{d-1}\\Theta\\right)\\right]}}\\\\ &{\\qquad\\qquad\\qquad=\\beta[P_{s}(P^{\\pi_{b}})^{d-1}]_{j,k}-\\beta\\sum_{a}\\pi_{d,\\theta}^{c}(a|s)\\left[P_{s}(P^{\\pi_{b}})^{d-1}\\right]_{a,k}}\\\\ &{\\qquad\\qquad\\qquad=\\beta[P_{s}(P^{\\pi_{b}})^{d-1}]_{j,k}-\\beta\\left[(\\pi_{d,\\theta}^{c})^{\\top}P_{s}(P^{\\pi_{b}})^{d-1}\\right]_{k}}\\\\ &{\\qquad\\qquad\\qquad=\\beta[P_{s}(P^{\\pi_{b}})^{d-1}]_{j,k}-\\beta\\left[\\mathbf{1}_{A}(\\pi_{d,\\theta}^{c})^{\\top}P_{s}(P^{\\pi_{b}})^{d-1}\\right]_{j,k}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "492 Moving back to matrix form, we obtain the stated result. ", "page_idx": 12}, {"type": "text", "text": "493 A.4 Proof of Theorem 4.4 \u2013 Exponential variance decay of C-SoftTreeMax ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "494 The C-SoftTreeMax policy gradient is bounded by ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{Var}\\left(\\nabla_{\\theta}\\log\\pi_{d,\\theta}^{\\mathrm{C}}(a|s)Q(s,a)\\right)\\leq2\\frac{A^{2}S^{2}\\beta^{2}}{(1-\\gamma)^{2}}|\\lambda_{2}(P^{\\pi_{b}})|^{2(d-1)}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "449956 tPhreo omf.a xWime auls ev aLlueem omf at h4.e1  Qd-ifreucntcltyi.o Fni risst $\\textstyle{\\frac{1}{1-\\gamma}}$ lla, si tt hise  ksnuomw  atsh aitn fwinhiteen  tdhisec roeuwnatredd  irse bwoaurndsd.e dN ienx $[0,1]$   \n497 bound the Frobenius norm of the term achieved in Lemma 4.3, by applying the eigen-decomposition   \n498 on $P^{\\pi_{b}}$ : ", "page_idx": 13}, {"type": "equation", "text": "$$\nP^{\\pi_{b}}=\\mathbf{1}_{S}\\mu^{\\top}+\\sum_{i=2}^{S}\\lambda_{i}u_{i}v_{i}^{\\top},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "499 where $\\mu$ is the stationary distribution of $P^{\\pi_{b}}$ , and $u_{i}$ and $v_{i}$ are left and right eigenvectors correspond  \n500 ingly. ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\beta\\left(I_{A,A}-\\mathbf{1}_{A}\\pi^{\\top}\\right)P_{s}(P^{n})\\|^{d-1}\\|_{F}=\\beta\\left\\|\\left(I_{A,A}-\\mathbf{1}_{A}\\pi^{\\top}\\right)P_{s}\\left(\\mathbf{1}_{S,\\beta}^{\\top}+\\displaystyle\\sum_{i=1}^{S}\\lambda_{i}^{-1}u_{i}v_{i}^{\\top}\\right)\\|_{F}}&{}\\\\ {(P_{s}~i s~s t o c h a s t i c)}&{=\\beta\\left\\|\\left(I_{A,A}-\\mathbf{1}_{A}\\pi^{\\top}\\right)\\left(\\mathbf{1}_{A}\\mu^{\\top}+\\displaystyle\\sum_{i=2}^{S}\\lambda_{i}^{-1}P_{s}u_{i}v_{i}^{\\top}\\right)\\|_{F}}\\\\ {(p m)f e c t i o n~m u l l(f6\\thinspace\\mathbf{1}_{A}\\mu^{\\top}~\\mathbf{1}_{A}\\pi^{\\top})}&{=\\beta\\left\\|\\left(I_{A,A}-\\mathbf{1}_{A}\\pi^{\\top}\\right)\\left(\\displaystyle\\sum_{i=2}^{S}\\lambda_{i}^{-1}P_{s}u_{i}v_{i}^{\\top}\\right)\\right\\|_{F}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\ddots\\displaystyle\\sum_{i=2}^{S}\\|\\left(I_{A,A}-\\mathbf{1}_{A}\\pi^{\\top}\\right)\\left(\\lambda_{i}^{-1}P_{s}u_{i}v_{i}^{\\top}\\right)\\|_{F}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\vdots\\displaystyle\\sum_{i=2}^{S}\\|\\left(I_{A,A}-\\mathbf{1}_{A}\\pi^{\\top}\\right)\\left(\\lambda_{i}^{-1}P_{s}u_{i}v_{i}^{\\top}\\right)\\|_{F}}\\\\ {(m a t r i x~n o m~s u b.m a t i n g h t c a t i v i t y)}&{\\leq\\beta\\|\\lambda_{i}^{-1}\\displaystyle\\sum_{i=2}^{S}\\|\\left[\\lambda_{i}A_{i}-\\mathbf{1}_{A}\\pi^{\\top}\\right]\\|_{F}\\|\\nu\\|_{H}\\|_{H}\\nu_{i}^{\\top}\\|_{F}}\\\\ &{=\\beta\\|\\lambda_{i}^{-1}\\|(\\lambda-1)\\|L_{A}+1_{A}\\pi^{\\top}\\|_{F}\\|P_{s}\\|_{F}\\|_{F}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "501 Now, we can bound the norm $\\|I_{A,A}-\\mathbf{1}_{A}\\boldsymbol\\pi^{\\top}\\|_{F}$ by direct calculation: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|I_{A,A}-\\mathbf{1}_{A}\\pi^{\\top}\\|_{F}^{2}=\\mathrm{Tr}\\left[\\left(I_{A,A}-\\mathbf{1}_{A}\\pi^{\\top}\\right)\\left(I_{A,A}-\\mathbf{1}_{A}\\pi^{\\top}\\right)^{\\top}\\right]}\\\\ &{\\qquad\\qquad\\qquad=\\mathrm{Tr}\\left[I_{A,A}-\\mathbf{1}_{A}\\pi^{\\top}-\\pi\\mathbf{1}_{A}^{\\top}+\\pi^{\\top}\\pi\\mathbf{1}_{A}\\mathbf{1}_{A}^{\\top}\\right]}\\\\ &{\\qquad\\qquad\\qquad=A-1-1+A\\pi^{\\top}\\pi}\\\\ &{\\qquad\\qquad\\leq2A.}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "502 From the Cauchy-Schwartz inequality, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\|P_{s}\\|_{F}^{2}=\\sum_{a}\\sum_{s}\\left[[P_{s}]_{a,s}\\right]^{2}=\\sum_{a}\\|[P_{s}]_{a,\\cdot}\\|_{2}^{2}\\leq\\sum_{a}\\|[P_{s}]_{a,\\cdot}\\|_{1}\\|[P_{s}]_{a,\\cdot}\\|_{\\infty}\\leq A.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "503 So, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Var}\\left(\\nabla_{\\theta}\\log\\pi_{d,\\theta}^{\\mathrm{C}}(a|s)Q(s,a)\\right)\\leq\\underset{s,a}{\\operatorname*{max}}\\left[Q(s,a)\\right]^{2}\\underset{s}{\\operatorname*{max}}\\;\\|\\nabla_{\\theta}\\log\\pi_{d,\\theta}^{\\mathrm{C}}(\\cdot|s)\\|_{F}^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\frac{1}{(1-\\gamma)^{2}}\\|\\beta\\left(I_{A,A}-\\mathbf{1}_{A}\\pi^{\\top}\\right)P_{s}(P^{\\pi_{b}})^{d-1}\\|_{F}^{2}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\frac{1}{(1-\\gamma)^{2}}\\beta^{2}|\\lambda_{2}(P^{\\pi_{b}})|^{2(d-1)}S^{2}(2A^{2}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "504 which obtains the desired bound. ", "page_idx": 13}, {"type": "text", "text": "505 A.5 A lower bound on C-SoftTreeMax gradient (result not in the paper) ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "506 For completeness we also supply a lower bound on the Frobenius norm of the gradient. Note that   \n507 this result does not translate to the a lower bound on the variance since we have no lower bound   \n508 equivalence of Lemma 4.1. ", "page_idx": 14}, {"type": "text", "text": "509 Lemma A.1. The Frobenius norm on the gradient of the policy is lower-bounded by: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|\\nabla_{\\theta}\\log\\pi_{d,\\theta}^{C}(\\cdot|s)\\|_{F}\\geq C\\cdot\\beta|\\lambda_{2}(P^{\\pi_{b}})|^{(d-1)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "510 Proof. We begin by moving to the induced $l_{2}$ norm by norm-equivalence: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\|\\beta\\left(I_{A,A}-\\mathbf{1}_{A}\\boldsymbol{\\pi}^{\\top}\\right)P_{s}(P^{\\pi_{b}})^{d-1}\\|_{F}\\geq\\|\\beta\\left(I_{A,A}-\\mathbf{1}_{A}\\boldsymbol{\\pi}^{\\top}\\right)P_{s}(P^{\\pi_{b}})^{d-1}\\|_{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "511 Now, taking the vector $u$ to be the eigenvector of the second eigenvalue of $P^{\\pi_{b}}$ : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\beta\\left(I_{A,A}-\\mathbf{1}_{A}\\boldsymbol{\\pi}^{\\top}\\right)P_{s}(P^{\\pi_{b}})^{d-1}\\|_{2}\\geq\\|\\beta\\left(I_{A,A}-\\mathbf{1}_{A}\\boldsymbol{\\pi}^{\\top}\\right)P_{s}(P^{\\pi_{b}})^{d-1}\\boldsymbol{u}\\|_{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\beta\\|\\left(I_{A,A}-\\mathbf{1}_{A}\\boldsymbol{\\pi}^{\\top}\\right)P_{s}\\boldsymbol{u}\\|_{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\beta|\\lambda_{2}(P^{\\pi_{b}})|^{(d-1)}\\|\\left(I_{A,A}-\\mathbf{1}_{A}\\boldsymbol{\\pi}^{\\top}\\right)P_{s}\\boldsymbol{u}\\|_{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "512 Note that even though $P_{s}u$ can be $0$ , that is not the common case since we can freely change $\\pi_{b}$ (and   \n513 therefore the eigenvectors of $P^{\\pi_{b}}$ ). \u53e3 ", "page_idx": 14}, {"type": "text", "text": "514 A.6 Proof of Lemma 4.5 \u2013 Vector form of E-SoftTreeMax ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "515 For $d\\geq1$ , (4) is given by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\pi_{d,\\theta}^{\\mathrm{E}}(\\cdot|s)=\\frac{E_{s,d}\\exp(\\beta\\Theta)}{1_{A}^{\\top}E_{s,d}\\exp(\\beta\\Theta)},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "516 where ", "page_idx": 14}, {"type": "equation", "text": "$$\nE_{s,d}=P_{s}\\prod_{h=1}^{d-1}\\left(D\\left(\\exp[\\beta\\gamma^{h-d}R]\\right)P^{\\pi_{b}}\\right)\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "517 with $R$ being the $|S|$ -dimensional vector whose $s$ -th coordinate is $r(s)$ . ", "page_idx": 14}, {"type": "text", "text": "518 Proof. Recall that ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\ell_{s,a}(d;\\theta)=\\gamma^{-d}\\left[r(s)+\\sum_{t=1}^{d-1}\\gamma^{t}r(s_{t})+\\gamma^{d}\\theta(s_{d})\\right].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "519 and, hence, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\exp[\\beta\\ell_{s,a}(d;\\theta)]=\\exp\\left[\\beta\\gamma^{-d}\\left(r(s)+\\sum_{t=1}^{d-1}\\gamma^{t}r(s_{t})+\\gamma^{d}\\theta(s_{d})\\right)\\right].\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "520 Therefore, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\exp\\beta\\ell_{s,a}(d;\\theta)]=\\mathbb{E}\\left[\\exp\\left[\\beta\\gamma^{-d}\\left(r(s)+\\displaystyle\\sum_{t=1}^{d-1}\\gamma^{t}r(s_{t})\\right)\\right]\\mathbb{E}\\left[\\exp\\left[\\beta\\left(\\theta(s_{d})\\right)\\right]\\right|s_{1},\\dots,s_{d-1}\\right]\\right]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}\\left[\\exp\\left[\\beta\\gamma^{-d}\\left(r(s)+\\displaystyle\\sum_{t=1}^{d-1}\\gamma^{t}r(s_{t})\\right)\\right]P^{\\pi_{b}}(\\cdot|s_{d-1})\\right]\\exp(\\beta\\Theta)\\qquad\\qquad(21)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=\\mathbb{E}\\left[\\exp\\left[\\beta\\gamma^{-d}\\left(r(s)+\\displaystyle\\sum_{t=1}^{d-2}\\gamma^{t}r(s_{t})\\right)\\right]\\exp[\\beta\\gamma^{-1}r(s_{d-1})]P^{\\pi_{b}}(\\cdot|s_{d-1})\\right]\\exp(\\beta\\Theta)}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "521 By repeatedly using iterative conditioning as above, the desired result follows. Note that   \n522 $\\dot{\\exp}(\\dot{\\beta}\\gamma^{-d}r(\\dot{s}))$ does not depend on the action and is therefore cancelled out with the denomi  \n523 nator. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "524 A.7 Proof of Lemma 4.6 \u2013 Gradient of E-SoftTreeMax ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "525 The E-SoftTreeMax gradient of dimension $A\\times S$ is given by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\log\\pi_{d,\\theta}^{\\mathrm{E}}=\\beta\\left[I_{A}-\\mathbf{1}_{A}(\\pi_{d,\\theta}^{\\mathrm{E}})^{\\top}\\right]\\frac{D\\left(\\pi_{d,\\theta}^{\\mathrm{E}}\\right)^{-1}E_{s,d}D(\\exp(\\beta\\Theta))}{\\mathbf{1}_{A}^{\\top}E_{s,d}\\exp(\\beta\\Theta)},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "526 where for brevity, we drop the $s$ index in the policy above, i.e., $\\pi_{d,\\theta}^{\\mathrm{{E}}}\\equiv\\pi_{d,\\theta}^{\\mathrm{{E}}}(\\cdot|s)$ . ", "page_idx": 15}, {"type": "text", "text": "527 Proof. The $(j,k)$ -th entry of $\\nabla_{\\boldsymbol{\\theta}}\\log\\pi_{d,\\boldsymbol{\\theta}}^{\\mathrm{E}}$ satisfies ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\big[\\nabla_{\\theta}\\log\\pi_{d,\\theta}^{\\mathbb{E}}\\big]_{j,k}=\\frac{\\partial\\log\\big(\\overline{{\\pi}}_{d,\\theta}^{k}(a^{j}|s)\\big)}{\\partial\\theta(s^{k})}}\\\\ &{=\\frac{\\partial}{\\partial\\theta(s^{k})}\\left(\\log\\big([G_{s,d}]_{j}^{\\top}\\exp(\\beta\\Theta)\\big]-\\log[\\mathsf{1}_{A}^{\\top}E_{s,d}\\exp(\\beta\\Theta)\\big]\\right)}\\\\ &{=\\frac{\\beta(E_{s,d})_{j,k}\\exp(\\beta\\theta(s^{k}))}{(E_{s,d})_{j}^{\\top}\\exp(\\beta\\Theta)}-\\frac{\\beta\\ \\mathsf{1}_{A}^{\\top}E_{s,d}e_{k}\\exp(\\beta\\theta(s^{k}))}{\\mathsf{1}_{A}^{\\top}E_{s,d}\\exp(\\beta\\Theta)}}\\\\ &{=\\frac{\\beta(E_{s,d}\\exp(\\beta\\theta(s^{k})))_{j}}{(E_{s,d})_{j}^{\\top}\\exp(\\beta\\Theta)}-\\frac{\\beta\\mathsf{1}_{A}^{\\top}E_{s,d}e_{k}\\exp(\\beta\\theta(s^{k}))}{\\mathsf{1}_{A}^{\\top}E_{s,d}\\exp(\\beta\\Theta)}}\\\\ &{=\\beta\\left[\\frac{e_{j}^{\\top}}{e_{j}^{\\top}E_{s,d}\\exp(\\beta\\Theta)}-\\frac{\\mathsf{1}_{A}^{\\top}}{\\mathsf{1}_{A}^{\\top}E_{s,d}\\exp(\\beta\\Theta)}\\right]E_{s,d}e_{k}\\exp(\\beta\\theta(s^{k})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "528 Hence, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{[\\nabla_{\\theta}\\log\\pi_{d,\\theta}^{\\mathrm{E}}]_{\\cdot,k}=\\beta\\left[D(E_{s,d}\\exp(\\beta\\Theta))^{-1}-(\\mathbf{1}_{A}^{\\top}E_{s,d}\\exp(\\beta\\Theta))^{-1}\\mathbf{1}_{A}\\mathbf{1}_{A}^{\\top}\\right]E_{s,d}e_{k}\\exp(\\beta\\theta(s^{k}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "529 From this, it follows that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\log\\pi_{d,\\theta}^{\\mathrm{E}}=\\beta\\left[D\\left(\\pi_{d,\\theta}^{\\mathrm{E}}\\right)^{-1}-\\mathbf{1}_{A}\\mathbf{1}_{A}^{\\top}\\right]\\frac{E_{s,d}D(\\exp(\\beta\\Theta))}{\\mathbf{1}_{A}^{\\top}E_{s,d}\\exp(\\beta\\Theta)}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "530 The desired result is now easy to see. ", "page_idx": 15}, {"type": "text", "text": "531 A.8 Proof of Theorem 4.7 \u2014 Exponential variance decay of E-SoftTreeMax ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "532 There exists $\\alpha\\in(0,1)$ such that, for any function $Q:S\\times A\\rightarrow\\mathbb{R}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{Var}\\left(\\nabla_{\\theta}\\log\\pi_{d,\\theta}^{\\mathrm{E}}(a|s)Q(s,a)\\right)\\in\\mathcal{O}\\left(\\beta^{2}\\alpha^{2d}\\right).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "533 If all rewards are equal $r\\equiv\\mathsf{c o n s t})$ ), then $\\alpha=|\\lambda_{2}(P^{\\pi_{b}})|$ . ", "page_idx": 15}, {"type": "text", "text": "534 Proof outline. Recall that thanks to Lemma 4.1, we can bound the PG variance using a direct bound   \n535 on the gradient norm. The definition of the induced norm is ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\theta}\\log\\pi_{d,\\theta}^{\\mathrm{E}}\\|=\\operatorname*{max}_{z:\\|z\\|=1}\\|\\nabla_{\\theta}\\log\\pi_{d,\\theta}^{\\mathrm{E}}z\\|,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "536 with $\\nabla_{\\theta}\\log\\pi_{d,\\theta}^{\\mathrm{{E}}}$ given in Lemma 4.6. Let $z\\in\\mathbb{R}^{S}$ be an arbitrary vector such that $\\|z\\|=1$ . Then,   \n537 $\\textstyle z=\\sum_{i=1}^{S}c_{i}z_{i}$ , where $c_{i}$ are scalar coefficients and $z_{i}$ are vectors spanning the $S$ -dimensional space.   \n538 In the full proof, we show our specific choice of and prove they are linearly independent given that   \n539 choice. We do note that $z_{1}=1_{S}$ . ", "page_idx": 15}, {"type": "text", "text": "The first part of the proof relies on the fact that $(\\nabla_{\\theta}\\log\\pi_{d,\\theta}^{\\mathrm{E}})z_{1}=0$ . This is easy to verify using Lemma 4.6 together with (6), and because $\\left[I_{A}-\\mathbf{1}_{A}\\big(\\\\pi_{d,\\theta}^{\\mathrm{E}}\\big)^{\\top}\\right]$ is a projection matrix whose null-space is spanned by ${\\mathbf{}1}_{S}$ . Thus, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\nabla_{\\theta}\\log\\pi_{d,\\theta}^{\\mathrm{E}}z=\\nabla_{\\theta}\\log\\pi_{d,\\theta}^{\\mathrm{E}}\\sum_{i=2}^{S}c_{i}z_{i}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "540 In the second part of the proof, we focus on $E_{s,d}$ from (6), which appears within $\\nabla_{\\boldsymbol{\\theta}}\\log\\pi_{d,\\boldsymbol{\\theta}}^{\\mathrm{E}}$ . Notice   \n541 that $E_{s,d}$ consists of the product $\\left.\\prod_{h=1}^{d-1}\\left(D\\left(\\exp(\\beta\\gamma^{h-d}R\\right)P^{\\pi_{b}}\\right)\\right.$ . Even though the elements in this   \n542 product are not stochastic matrices, in the full proof we show how to normalize each of them to a   \n543 stochastic matrix $B_{h}$ . We thus obtain that ", "page_idx": 16}, {"type": "equation", "text": "$$\nE_{s,d}=P_{s}D(M_{1})\\prod_{h=1}^{d-1}B_{h},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "544 where $M_{1}\\in\\mathbb{R}^{S}$ is some strictly positive vector. Then, we can apply a result by Mathkar and Borkar   \n546 stochastic matrices 545 [2016], which itself builds on [Chatterjee and Seneta, 1977]. The result states that the product of $\\prod_{h=1}^{d-1}B_{h}$ of our particular form converges exponentially fast to a matrix of the   \n547 form $\\mathbf{1}_{S}\\mu^{\\top}$ s.t. $\\|\\mathbf{1}_{S}\\mu^{\\top}-\\prod_{h=1}^{d-1}B_{h}\\|\\leq C\\alpha^{d}$ for some constant .   \n548 Lastly, $\\mathbf{1}_{S}\\mu_{\\pi_{b}}^{\\top}$ gets canceled due to our choice of $z_{i}$ ${\\mathrm{\\it~i}},{\\mathrm{\\it~i}}=2,...,S.$ . This observation along with the   \n549 above fact that the remainder decays then shows that $\\begin{array}{r}{\\nabla_{\\theta}\\log\\pi_{d,\\theta}^{\\mathrm{E}}\\sum_{i=2}^{S}z_{i}=\\mathcal{O}(\\alpha^{d})}\\end{array}$ , which gives the   \n550 desired result. \u53e3 ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "551 Full technical proof. Let $d\\geq2$ . Recall that ", "page_idx": 16}, {"type": "equation", "text": "$$\nE_{s,d}=P_{s}\\prod_{h=1}^{d-1}\\left(D\\left(\\exp[\\beta\\gamma^{h-d}R]\\right)P^{\\pi_{b}}\\right),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "552 and that $R$ refers to the $S$ -dimensional vector whose $s$ -th coordinate is $r(s)$ . Define ", "page_idx": 16}, {"type": "equation", "text": "$$\nB_{i}={\\binom{P^{\\pi_{b}}}{D^{-1}(P^{\\pi_{b}}M_{i+1})P^{\\pi_{b}}D(M_{i+1})}}\\quad{\\mathrm{if~}}i=d-1,\\quad\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "553 and the vector ", "page_idx": 16}, {"type": "equation", "text": "$$\nM_{i}={\\left\\{\\begin{array}{l l}{\\exp(\\beta\\gamma^{-1}R)}&{{\\mathrm{if~}}i=d-1,}\\\\ {\\exp(\\beta\\gamma^{i-d}R)\\circ P^{\\pi_{b}}M_{i+1}}&{{\\mathrm{if~}}i=1,\\ldots,d-2,}\\end{array}\\right.}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "554 where $\\circ$ denotes the element-wise product. Then, ", "page_idx": 16}, {"type": "equation", "text": "$$\nE_{s,d}=P_{s}D(M_{1})\\prod_{i=1}^{d-1}B_{i}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "555 It is easy to see that each $B_{i}$ is a row-stochastic matrix, i.e., all entries are non-negative and   \n556 $B_{i}\\mathbf{1}_{S}=\\mathbf{1}_{S}$ .   \n557 Next, we prove that all non-zeros entries of $B_{i}$ are bounded away from 0 by a constant. This is   \n558 necessary to apply the next result from Chatterjee and Seneta [1977]. The $j$ -th coordinate of $M_{i}$   \n559 satisfies ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "equation", "text": "$$\n({\\cal M}_{i})_{j}=\\exp[\\beta\\gamma^{i-d}R_{j}]\\sum_{k}[P^{\\pi_{b}}]_{j,k}({\\cal M}_{i+1})_{k}\\le\\|\\exp[\\beta\\gamma^{i-d}R]\\|_{\\infty}\\|{\\cal M}_{i+1}\\|_{\\infty}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "560 Separately, observe that $\\|M_{d-1}\\|_{\\infty}\\leq\\|\\exp(\\beta\\gamma^{-1}R)\\|_{\\infty}$ . Plugging these relations in (26) gives ", "page_idx": 16}, {"type": "equation", "text": "$$\nM_{1}\\|_{\\infty}\\le\\prod_{h=1}^{d-1}\\|\\exp[\\beta\\gamma^{h-d}R]\\|_{\\infty}=\\prod_{h=1}^{d-1}\\|\\exp[\\beta\\gamma^{-d}R]\\|_{\\infty}^{\\gamma^{h}}=\\|\\exp[\\beta\\gamma^{-d}R]\\|_{\\infty}^{\\sum_{h=1}^{d-1}\\gamma^{h}}\\le\\|\\exp[\\beta\\gamma^{-d}]\\|_{\\infty}^{\\sum_{h=1}^{d-1}\\gamma^{h}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "561 Similarly, for every $1\\leq i\\leq d-1$ , we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\|M_{i}\\|_{\\infty}\\leq\\prod_{h=i}^{d-1}\\|\\exp[\\beta\\gamma^{-d}R]\\|_{\\infty}^{\\gamma^{h}}\\leq\\|\\exp[\\beta\\gamma^{-d}R]\\|_{\\infty}^{\\frac{1}{1-\\gamma}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "562 The $j k$ -th entry of $B_{i}=D^{-1}(P^{\\pi_{b}}M_{i+1})P^{\\pi_{b}}D(M_{i+1})$ is ", "page_idx": 16}, {"type": "equation", "text": "$$\n(B_{i})_{j k}=\\frac{P_{j k}^{\\pi_{b}}[M_{i+1}]_{k}}{\\sum_{\\ell=1}^{|S|}P_{j\\ell}^{\\pi_{b}}[M_{i+1}]_{\\ell}}\\geq\\frac{P_{j k}^{\\pi_{b}}}{\\sum_{\\ell=1}^{|S|}P_{j\\ell}^{\\pi_{b}}[M_{i+1}]_{\\ell}}\\geq\\frac{P_{j k}^{\\pi_{b}}}{\\|\\exp[\\beta\\gamma^{-d}R]\\|_{\\infty}^{\\frac{1}{1-\\gamma}}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "563 Hence, for non-zero $P_{j k}^{\\pi_{b}}$ , the entries are bounded away from zero by the same. We can now proceed   \n564 with applying the following result.   \n565 Now, by [Chatterjee and Seneta, 1977, Theorem 5] (see also (14) in [Mathkar and Borkar, 2016]),   \n566 $\\begin{array}{r}{\\operatorname*{lim}_{d\\to\\infty}\\prod_{i=1}^{d-1}B_{i}}\\end{array}$ exists and is of the form $\\mathbf{1}_{S}\\mu^{\\top}$ for some probability vector $\\mu$ . Furthermore, there   \n567 is some $\\alpha\\in(0,1)$ such that $\\begin{array}{r}{\\varepsilon(d):=\\left(\\prod_{i=1}^{d-1}B_{i}\\right)-\\mathbf{1}_{S}\\,\\mu^{\\top}}\\end{array}$ satisfies ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|\\varepsilon(d)\\|=O(\\alpha^{d}).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "568 Pick linearly independent vectors $w_{2},\\dots,w_{S}$ such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mu^{\\top}w_{i}=0\\;{\\mathrm{for}}\\;i=2,\\ldots,d.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "569 Since $\\textstyle\\sum_{i=2}^{S}\\alpha_{i}w_{i}$ is perpendicular to $\\mu$ for any $\\alpha_{2},\\ldots\\alpha_{S}$ and because $\\mu^{\\top}\\exp(\\beta\\Theta)\\,>\\,0$ , there   \n570 exists no choice of $\\alpha_{2},\\ldots,\\alpha_{S}$ such that $\\begin{array}{r}{\\sum_{i=2}^{S}\\alpha_{i}w_{i}\\,=\\,\\exp(\\beta\\Theta)}\\end{array}$ . Hence, if we let $z_{1}=1_{S}$ and   \n571 $z_{i}=D(\\exp(\\beta\\Theta))^{-1}w_{i}$ for $i=2,\\dots,S$ ,  then it follows that $\\{z_{1},\\ldots,z_{S}\\}$ is linearly independent.   \n572 In particular, it implies that $\\{z_{1},\\ldots,z_{S}\\}$ spans $\\mathbb{R}^{S}$ . ", "page_idx": 17}, {"type": "text", "text": "573 Now consider an arbitrary unit norm vector $\\textstyle z:=\\sum_{i=1}^{S}c_{i}z_{i}\\in\\mathbb{R}^{S}$ s.t. $\\|z\\|_{2}=1$ . Then, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\theta}\\log\\pi_{d,\\theta}^{\\theta}=\\nabla_{\\theta}\\log\\pi_{d,\\theta}^{\\theta}\\displaystyle\\sum_{s=0}^{\\infty}c_{s,\\theta}}\\\\ &{\\quad=\\beta\\left[L_{a}-L_{a}(\\pi_{a}^{\\theta})\\right]^{\\frac{1}{\\alpha}}\\frac{\\left(D\\left(\\pi_{a}^{\\theta}\\right)\\right)^{-1}E_{a}\\Lambda J^{(\\theta)}\\exp\\left(\\beta\\Theta\\right)}{1\\sum_{l=0}^{K}E_{a}\\Lambda J^{(\\theta)}(\\beta\\Theta)}\\displaystyle\\sum_{s=0}^{s}c_{s,\\theta}}\\\\ &{\\quad=\\beta\\left[L_{a}-L_{a}(\\pi_{a}^{\\theta})\\right]^{\\frac{1}{\\alpha}}\\frac{\\left(D\\left(\\pi_{a}^{\\theta}\\right)\\right)^{-1}E_{a}\\Lambda J^{\\theta}}{1\\sum_{l=K}L_{a}\\exp(\\beta\\Theta)}\\displaystyle\\sum_{s=0}^{s}c_{s,\\theta}}\\\\ &{\\quad=\\beta\\left[L_{a}-L_{a}(\\pi_{a}^{\\theta})\\right]^{\\frac{1}{\\alpha}}\\frac{\\left(D\\left(\\pi_{a}^{\\theta}\\right)\\right)^{-1}\\big[\\log^{\\theta}+c_{s}(\\pi)\\big]}{1\\sum_{l=K}L_{a}\\exp(\\beta\\Theta)}\\displaystyle\\sum_{s=0}^{s}c_{s,\\theta}}\\\\ &{\\quad=\\beta\\left[L_{a}-L_{a}(\\pi_{a}^{\\theta})\\right]^{\\frac{1}{\\alpha}}\\frac{\\left(D\\left(\\pi_{a}^{\\theta}\\right)\\right)^{-1}\\big[\\log^{\\theta}+c_{s}(\\pi)\\big]}{1\\sum_{l=K}L_{a}\\exp(\\beta\\Theta)}\\displaystyle\\sum_{s=0}^{s}c_{s,\\theta}}\\\\ &{\\quad=\\beta\\left[L_{a}-L_{a}(\\pi_{a}^{\\theta})\\right]^{\\frac{1}{\\alpha}}\\frac{\\left(\\pi_{a}^{\\theta}\\right)^{-1}\\varepsilon^{\\frac{\\alpha}{\\beta}}\\big[d\\beta\\Theta\\right]}{1\\sum_{l=K}L_{a}\\exp(\\beta\\Theta)}\\displaystyle\\sum_{s=0}^{s}c_{s,\\theta}}\\\\ &{\\quad=\\beta\\left[L_{a}-L_{a}(\\pi_{a}^{\\theta})\\right]^{\\frac{1}{\\alpha}}\\frac{\\left(D\\left(\\pi_{a}^{\\theta \n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "574 where (34) follows from the fact that $\\nabla_{\\theta}\\log\\pi_{d,\\theta}^{\\mathrm{E}}z_{1}\\;=\\;\\nabla_{\\theta}\\log\\pi_{d,\\theta}^{\\mathrm{E}}\\mathbf{1}_{S}\\;=\\;0$ , (35) follows from   \n575 Lemma 4.6, (36) holds since $z_{i}=D(\\exp(\\beta\\Theta))^{-1}w_{i}$ , (38) because $\\mu$ is perpendicular $w_{i}$ for each $i$ ,   \n576 while (39) follows by reusing $z_{i}=D(\\exp(\\beta\\Theta))^{-1}w_{i}$ relation along with the fact that $z_{1}=1_{S}$ . ", "page_idx": 17}, {"type": "text", "text": "577 From (39), it follows that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|\\nabla_{\\theta}\\log\\pi_{d,\\theta}^{\\mathrm{E}}z\\|\\leq\\beta\\|\\varepsilon(d)\\|\\left\\|\\left[I_{A}-\\mathbf{1}_{A}(\\pi_{d,\\theta}^{\\mathrm{E}})^{\\top}\\right]\\frac{D\\left(\\pi_{d,\\theta}^{\\mathrm{E}}\\right)^{-1}}{\\mathbf{1}_{A}^{\\top}E_{s,d}\\exp(\\beta\\Theta)}\\right\\|\\|D(\\exp(\\beta\\Theta))\\|\\;\\|z-c_{1}\\mathbf{1}_{S}\\|\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\leq\\beta\\alpha^{d}(\\|I_{A}\\|+\\|\\mathbf{1}_{A}(\\pi_{d,\\theta}^{\\mathrm{E}})^{\\top}\\|)\\left\\|\\frac{D\\left(\\pi_{d,\\theta}^{\\mathrm{E}}\\right)^{-1}}{\\mathbf{1}_{A}^{\\top}E_{s,d}\\exp(\\beta\\Theta)}\\right\\|\\exp(\\beta\\operatorname*{max}_{s}\\theta(s))\\|z-c_{1}\\mathbf{1}_{S}\\|\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\beta\\alpha^{d}(1+\\sqrt{\\lambda})\\left\\|\\frac{D\\big(\\frac{\\pi}{\\alpha_{d,\\delta}}\\big)^{-1}}{\\|\\frac{D}{\\lambda_{c}^{2}}E_{s,d}\\exp(\\beta\\Theta)\\|}\\right\\|\\exp(\\beta\\operatorname*{max}\\theta(s))\\|z-c_{1}\\mathbf{1}_{s}\\|}\\\\ &{\\leq\\beta\\alpha^{d}(1+\\sqrt{\\lambda})\\left\\|D^{-1}(E_{s,d}\\exp(\\beta\\Theta))\\right\\|\\exp(\\beta\\operatorname*{max}\\theta(s))\\|z-c_{1}\\mathbf{1}_{s}\\|}\\\\ &{\\leq\\beta\\alpha^{d}(1+\\sqrt{\\lambda})\\frac{1}{\\operatorname*{min}_{\\Im\\{{\\cal S}_{\\delta},t\\in\\mathrm{Sp}(\\beta\\Theta)\\}_{s}}}\\exp(\\beta\\operatorname*{max}\\theta(s))\\|z-c_{1}\\mathbf{1}_{s}\\|}\\\\ &{\\leq\\beta\\alpha^{d}(1+\\sqrt{\\lambda})\\frac{\\exp(\\beta\\operatorname*{max}_{\\theta}\\theta(s))}{\\exp(\\beta\\operatorname*{min}_{\\Im\\{{\\cal S}_{\\delta}}}(s))\\operatorname*{min}_{\\Im\\{{\\cal S}_{\\delta}\\}}|||\\boldsymbol{z}-c_{1}\\mathbf{1}_{s}|}}\\\\ &{\\leq\\beta\\alpha^{d}(1+\\sqrt{\\lambda})\\frac{\\exp(\\beta\\operatorname*{max}_{\\theta}\\theta(s))}{\\exp(\\beta\\operatorname*{min}_{\\Im\\{{\\cal S}_{\\delta}}}(s))\\exp(\\beta\\operatorname*{min}_{\\Im\\{{\\cal S}_{\\delta}}}(s))}\\|z-c_{1}\\mathbf{1}_{s}\\|}\\\\ &{\\leq\\beta\\alpha^{d}(1+\\sqrt{\\lambda})\\exp(\\beta\\operatorname*{min}_{\\Im\\{{\\cal S}_{\\delta}}}\\theta(s))\\exp(\\beta\\operatorname*{min}_{\\Im\\{{\\cal S}_{\\delta}}}r(s))\\|z-c_{1}\\mathbf{1}_{s}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "578 Lastly, we prove that $\\|z-c_{1}\\mathbf{1}_{S}\\|$ is bounded independently of $d$ . First, denote by $c=(c_{1},\\ldots,c_{S})^{\\top}$   \n579 and $\\tilde{\\boldsymbol{c}}=(0,c_{2},\\ldots,c_{S})^{\\top}$ . Also, denote by $Z$ the matrix with $z_{i}$ as its $i$ -th column. Now, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\|z-c_{1}\\mathbf{1}_{S}\\|=\\|\\sum_{i=2}^{S}c_{i}z_{i}\\|}}\\\\ &{}&{\\quad=\\|Z\\widetilde{c}\\|}\\\\ &{}&{\\quad\\le\\|Z\\|\\|\\widetilde{c}\\|}\\\\ &{}&{\\quad\\le\\|Z\\|\\|c\\|}\\\\ &{}&{\\quad=\\|Z\\|\\|Z^{-1}z\\|}\\\\ &{}&{\\quad\\le\\|Z\\|\\|Z^{-1}\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "580 where the last relation is due to $z$ being a unit vector. All matrix norms here are $l_{2}$ -induced norms. ", "page_idx": 18}, {"type": "text", "text": "581 Next, denote by $W$ the matrix with $w_{i}$ in its $i$ -th column. Recall that in (33) we only defined   \n582 $w_{2},\\dots,w_{S}$ . We now set $w_{1}\\,=\\,\\exp(\\beta\\Theta)$ . Note that $w_{1}$ is linearly independent of $\\{w_{2},\\dots,w_{S}\\}$   \n583 because of (33) together with the fact that $\\mu^{\\top}w_{1}>0$ . We can now express the relation between $Z$   \n584 and $W$ by $\\bar{Z}=D^{\\frac{\\epsilon}{-1}}(\\exp(\\beta\\Theta))W$ . Substituting this in (53), we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|z-c_{1}\\mathbf{1}_{S}\\|\\leq\\|D^{-1}(\\exp(\\beta\\Theta))W\\|\\|W^{-1}D(\\exp(\\beta\\Theta))\\|}\\\\ &{\\qquad\\qquad\\leq\\|W\\|\\|W^{-1}\\|\\|D(\\exp(\\beta\\Theta))\\|\\|D^{-1}(\\exp(\\beta\\Theta))\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "585 It further holds that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|D(\\exp(\\beta\\Theta))\\|\\leq\\operatorname*{max}_{s}\\exp\\big(\\beta\\theta(s)\\big)\\leq\\operatorname*{max}\\{1,\\exp[\\beta\\operatorname*{max}_{s}\\theta(s)]\\big)\\},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "586 where the last relation equals $1$ if $\\theta(s)<0$ for all $s$ . Similarly, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|D^{-1}(\\exp(\\beta\\Theta))\\|\\le\\frac{1}{\\operatorname*{min}_{s}\\exp\\left(\\beta\\theta(s)\\right)}\\le\\frac{1}{\\operatorname*{min}\\{1,\\exp[\\beta\\operatorname*{min}_{s}\\theta(s)])\\}}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "587 Furthermore, by the properties of the $l_{2}$ -induced norm, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|W\\|_{2}\\leq\\sqrt{S}\\|W\\|_{1}}\\\\ &{\\qquad\\qquad=\\sqrt{S}\\underset{1\\leq i\\leq S}{\\operatorname*{max}}\\,\\|w_{i}\\|_{1}}\\\\ &{\\qquad\\qquad=\\sqrt{S}\\operatorname*{max}\\{\\exp(\\beta\\Theta),\\underset{2\\leq i\\leq S}{\\operatorname*{max}}\\,\\|w_{i}\\|_{1}\\}}\\\\ &{\\qquad\\qquad\\leq\\sqrt{S}\\operatorname*{max}\\{1,\\exp[\\beta\\underset{s}{\\operatorname*{max}}\\,\\theta(s)],\\underset{2\\leq i\\leq S}{\\operatorname*{max}}\\,\\|w_{i}\\|_{1})\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "588 Lastly, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|W^{-1}\\|=\\frac{1}{\\sigma_{\\operatorname*{min}}(W)}}\\\\ &{\\qquad\\le\\left(\\displaystyle\\prod_{i=1}^{s-1}\\frac{\\sigma_{\\operatorname*{max}}(W)}{\\sigma_{i}(W)}\\right)\\frac{1}{\\sigma_{\\operatorname*{min}}(W)}}\\\\ &{\\qquad=\\frac{(\\sigma_{\\operatorname*{max}}(W))^{s-1}}{\\displaystyle\\prod_{i=1}^{s}\\sigma_{i}(W)}}\\\\ &{\\qquad=\\frac{\\|W\\|^{s-1}}{\\displaystyle\\|\\operatorname*{det}(W)\\|}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "589 The determinant of $W$ is a sum of products involving its entries. To upper bound (65) independently   \n590 of $d$ , we lower bound its denominator by upper and lower bounds on the entries $[W]_{i,1}$ that are   \n591 independent of $d$ , depending on their sign: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{min}\\{1,\\exp[\\beta\\operatorname*{min}_{s}\\theta(s)])\\}\\leq[W]_{i,1}\\leq\\operatorname*{max}\\{1,\\exp[\\beta\\operatorname*{max}_{s}\\theta(s)])\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "592 Using this, together with (53), (55), (56), (57), and (61), we showed that $\\|z-c_{1}\\mathbf{1}_{S}\\|$ is upper bounded   \n593 by a constant independent of $d$ . This concludes the proof. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "594 A.9 Bias Estimates ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Lemma A.2. For any matrix $A$ and $\\hat{A}$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{A}^{k}-A^{k}=\\sum_{h=1}^{k}\\hat{A}^{h-1}(\\hat{A}-A)A^{k-h}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "595 Proof. The proof follows from first principles: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\sum_{h=1}^{k}\\hat{A}^{h-1}(\\hat{A}-A)A^{k-h}=\\sum_{h=1}^{k}\\hat{A}^{h-1}\\hat{A}A^{k-h}-\\sum_{h=1}^{k}\\hat{A}^{h-1}A A^{k-h}}}\\\\ {{\\displaystyle=\\sum_{h=1}^{k}\\hat{A}^{h}A^{k-h}-\\sum_{h=1}^{k}\\hat{A}^{h-1}A^{k-h+1}}}\\\\ {{\\displaystyle=\\hat{A}^{k}-A^{k}+\\sum_{h=1}^{k-1}\\hat{A}^{h}A^{k-h}-\\sum_{h=2}^{k}\\hat{A}^{h-1}A^{k-h+1}}}\\\\ {{\\displaystyle=\\hat{A}^{k}-A^{k}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "596 ", "page_idx": 19}, {"type": "text", "text": "597 Henceforth, $\\Vert\\cdot\\Vert$ will refer to $\\|\\cdot\\|_{\\infty}$ , i.e. the induced infinity norm. Also, for brevity, we denote $\\pi_{d,\\theta}^{\\mathrm{C}}$   \n598 and $\\hat{\\pi}_{d,\\theta}^{\\mathrm{C}}$ by $\\pi_{\\theta}$ and $\\hat{\\pi}_{\\boldsymbol{\\theta}}$ , respectively. Similarly, we use $d_{\\pi_{\\theta}}$ and $d_{\\hat{\\pi}_{\\boldsymbol{\\theta}}}$ to denote $d_{\\pi_{d,\\theta}^{\\mathrm{C}}}$ and $d_{\\hat{\\pi}_{d,\\theta}^{\\mathrm{C}}}$ . As for   \n599 the induced norm of the matrix $P$ and its perturbed counterpart P\u02c6, which are of size $S\\times A\\times S$ ,   \n600 we slightly abuse notation and denote $\\|P-\\hat{P}\\|=\\operatorname*{max}_{s}\\{\\|P_{s}-\\hat{P}_{s}\\|\\}$ , where $P_{s}$ is as defined in   \n601 Section 2. ", "page_idx": 19}, {"type": "text", "text": "602 Definition A.3. Let $\\epsilon$ be the maximal model mis-specification, i.e., $\\operatorname*{max}\\{\\|P-\\hat{P}\\|,\\|r-\\hat{r}\\|\\}=\\epsilon$ . ", "page_idx": 20}, {"type": "text", "text": "603 Lemma A.4. Recall the definitions of $R_{s},P_{s},R_{\\pi_{b}}$ and $P^{\\pi_{b}}$ from Section 2, and respectively denote   \n604 their perturbed counterparts by $\\hat{R}_{s},\\hat{P}_{s},\\hat{R}_{\\pi_{b}}$ and $\\hat{P}^{\\pi_{b}}$ . Then, for $\\epsilon$ defined in Definition A.3, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{max}\\{\\|R_{s}-\\hat{R}_{s}\\|,\\|P_{s}-\\hat{P}_{s}\\|,\\|R_{\\pi_{b}}-\\hat{R}_{\\pi_{b}}\\|,\\|P^{\\pi_{b}}-\\hat{P}^{\\pi_{b}}\\|\\}=O(\\epsilon).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "605 Proof. The proof follows easily from the fact that the differences above are convex combinations of   \n606 $P-{\\hat{P}}$ and $r-\\hat{r}$ . \u53e3   \n607 Lemma A.5. Let $\\pi_{\\theta}$ be as in (5), and let $\\scriptstyle{\\hat{\\pi}}_{\\theta}$ also be defined as in (5), but with $R_{s},P_{s},P^{\\pi_{b}}$ replaced   \n608 by their perturbed counterparts $\\hat{R}_{s},\\hat{P}_{s},\\hat{P}^{\\pi_{b}}$ throughout. Then, ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "equation", "text": "$$\n\\lVert\\pi_{d,\\theta}^{C}-\\hat{\\pi}_{d,\\theta}^{C}\\rVert=O(\\beta d\\epsilon).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "609 Proof. To prove the desired result, we work with (5) to bound the error between $R_{s},P_{s},P^{\\pi_{b}},R_{\\pi_{b}}$   \n610 and their perturbed versions. ", "page_idx": 20}, {"type": "text", "text": "First, we apply Lemma A.2 together with Lemma A.4 to obtain that $\\|(P^{\\pi_{b}})^{k}-(\\hat{P}^{\\pi_{b}})^{k}\\|=O(k\\epsilon)$ .   \nNext, denote by $M$ the argument in the exponent in (5), i.e. ", "page_idx": 20}, {"type": "equation", "text": "$$\nM:=\\beta[C_{s,d}+P_{s}(P^{\\pi_{b}})^{d-1}\\Theta].\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "611 Similarly, let $\\hat{M}$ be the corresponding perturbed sum that relies on $\\hat{P}$ and $\\hat{r}$ . Combining the bounds   \n612 from Lemma A.4, and using the triangle inequality, we have that $\\lVert\\hat{M}-M\\rVert=O(\\beta d\\epsilon)$ . ", "page_idx": 20}, {"type": "text", "text": "Eq. (5) states that the C-SoftTreeMax policy in the true environment is $\\pi_{\\theta}=\\exp(M)/(1^{\\top}\\exp(M))$ . Similarly define $\\scriptstyle{\\hat{\\pi}}_{\\theta}$ using M\u02c6 for the approximate model. Then, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\hat{\\boldsymbol{\\pi}}_{\\boldsymbol{\\theta}}=(\\pi_{\\boldsymbol{\\theta}}\\circ\\exp(M-\\hat{M}))\\boldsymbol{1}^{\\intercal}\\exp(M)/(\\boldsymbol{1}^{\\intercal}\\exp(\\hat{M})),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "613 where $\\circ$ denotes element-wise multiplication. Using the above relation, we have that $\\lVert\\widehat{\\pi}_{\\theta}-\\pi_{\\theta}\\rVert=$   \n614 $\\begin{array}{r}{\\|\\pi_{\\theta}\\|\\|\\frac{\\exp(M-\\hat{M})\\boldsymbol{1}^{\\top}\\exp(M)}{\\boldsymbol{1}^{\\top}\\exp(\\hat{M})}-1\\|}\\end{array}$ . Using the relation $|e^{x}-1|=O(x)$ as $x\\to0$ , the desired result   \n615 follows. ", "page_idx": 20}, {"type": "text", "text": "616 ", "page_idx": 20}, {"type": "text", "text": "617 Theorem A.6. Let \u03f5 be as in Definition A.3. Further let $\\hat{\\pi}_{d,\\theta}^{C}$ being the corresponding approximate   \n618 policy as given in Lemma 4.2. Then, the policy gradient bias is bounded by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\|\\frac{\\partial}{\\partial\\theta}\\left(\\nu^{\\top}V^{\\pi_{\\theta}}\\right)-\\frac{\\partial}{\\partial\\theta}\\left(\\nu^{\\top}V^{\\hat{\\pi}_{\\theta}}\\right)\\right\\|=\\mathcal{O}\\left(\\frac{1}{(1-\\gamma)^{2}}S\\beta^{2}d\\epsilon\\right).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "619 We first provide a proof outline for conciseness, and only after it the complete proof. ", "page_idx": 20}, {"type": "text", "text": "Proof outline. First, we prove tha $\\operatorname*{max}\\{\\|R_{s}\\!-\\!\\hat{R}_{s}\\|,\\|P_{s}\\!-\\!\\hat{P}_{s}\\|,\\|R_{\\pi_{b}}\\!-\\!\\hat{R}_{\\pi_{b}}\\|,\\|P^{\\pi_{b}}\\!-\\!\\hat{P}^{\\pi_{b}}\\|\\}=\\mathcal{O}(\\epsilon).$ rTohwiss  foofl $P-{\\hat{P}}$ o omr $r-\\hat{r}$ .c t Wthe atu steh et hdei fafebroevnec eosb saebrovvaet iaorne  asluoitnagb lwe itcho nthvee xd ceofinmitbiionnatsi oofn $\\pi_{d,\\theta}^{\\mathrm{C}}$ eiatnhde $\\hat{\\pi}_{d,\\theta}^{\\mathrm{C}}$ given in (5) to show that $\\|\\pi_{d,\\theta}^{\\mathbf{C}}-\\hat{\\pi}_{d,\\theta}^{\\mathbf{C}}\\|=O(\\beta d\\epsilon)$ . The proof for the latter builds upon two key facts: $\\begin{array}{r}{\\|(P^{\\pi_{b}})^{k}-(\\hat{P}^{\\pi_{b}})^{k}\\|\\leq\\sum_{h=1}^{k}\\|\\hat{P}^{\\pi_{b}}\\|^{h-1}\\|\\hat{P}^{\\pi_{b}}-P^{\\pi_{b}}\\|\\|p^{\\pi_{b}}\\|^{k-h}=O(k\\epsilon)}\\end{array}$ for any $k\\geq0$ , and (b) $|e^{x}-1|=O(x)$ $x\\to0$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{s}\\left(\\prod_{i=1}^{4}X_{i}(s)-\\prod_{i=1}^{4}\\hat{X}_{i}(s)\\right)=\\sum_{s}\\sum_{i=1}^{4}\\hat{X}_{1}(s)\\cdot\\cdot\\cdot\\hat{X}_{i-1}(s)\\left(X_{i}(s)-\\hat{X}_{i}(s)\\right)\\times X_{i+1}(s)\\cdot\\cdot\\cdot X_{4}(s)\\,.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "620 where $X_{1}(s)\\,=\\,d_{\\pi_{d,\\theta}^{\\mathrm{c}}}(s)\\,\\in\\,\\mathbb{R}$ , $X_{2}(s)\\,=\\,(\\nabla_{\\theta}\\log\\pi_{d,\\theta}^{\\mathrm{{C}}}(\\cdot|s))^{\\intercal}\\,\\in\\,\\mathbb{R}^{S\\times A}$ , $X_{3}(s)\\,=\\,{\\cal D}(\\pi_{d,\\theta}^{\\mathrm{C}}(\\cdot|s))\\,\\in$   \n621 $\\mathbb{R}^{A\\times A}$ , $X_{4}(s)\\,=\\,Q^{\\pi_{d,\\theta}^{\\mathrm{C}}}(s,\\cdot)\\,\\in\\,\\mathbb{R}^{A\\times A}$ , and $\\hat{X}_{1}(s),\\ldots,\\hat{X}_{4}(s)$ are similarly defined with $\\pi_{d,\\theta}^{\\mathrm{C}}$ re  \n622 placed by $\\hat{\\pi}_{d,\\theta}^{\\mathrm{C}}$ . Then, we show that, for $i\\;=\\;1,...,4$ , (i) $\\|X_{i}(s)\\,-\\,\\hat{X}_{i}(s)\\|\\;=\\;O(\\epsilon)$ and (ii)   \n623 $\\operatorname*{max}\\{\\|X_{i}\\|,\\|\\hat{X}_{i}\\|\\}$ is bounded by problem parameters. From this, the desired result follows. \u518f\u53e3 ", "page_idx": 20}, {"type": "text", "text": "624 Proof. We have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial}{\\partial\\tilde{\\theta}}\\left(t^{\\gamma}V^{\\pi*}\\right)-\\frac{\\partial}{\\partial\\tilde{\\theta}}\\left(t^{\\gamma}V^{\\pi*}\\right)}\\\\ &{=\\mathbb{E}_{\\nu\\sim d_{\\theta},a^{\\gamma}\\pi*\\mathcal{U}(\\cdot|\\tilde{\\theta})}\\mathbb{C}_{\\theta}(a|s)Q^{\\pi*}(s,a)]-\\mathbb{E}_{\\nu\\sim d_{\\theta},a^{\\gamma}\\pi*(\\cdot|s|)}\\left[\\nabla_{\\theta}\\log\\bar{\\pi}_{\\theta}(a|s)Q^{\\theta*}(s,a)\\right.}\\\\ &{\\displaystyle=\\sum_{s,a}(d_{\\pi_{\\theta}}(s)\\pi\\varphi(a|s)\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)Q^{\\pi*}(s,a)-d_{\\theta}(s))\\bar{\\pi}_{\\theta}(a|s)\\nabla_{\\theta}\\log\\bar{\\pi}_{\\theta}(a|s)Q^{\\pi*}(s,a)\\right]}\\\\ &{\\displaystyle=\\sum_{s}\\left(d_{\\pi_{\\theta}}(s)(\\nabla_{\\theta}\\log\\pi_{\\theta}(\\cdot|s))^{\\top}D(\\pi_{\\theta}(\\cdot|s))Q^{\\pi*}(s,\\cdot)\\right.}\\\\ &{\\displaystyle\\left.\\quad-d_{\\theta_{\\theta}}(s)(\\nabla_{\\theta}\\log\\bar{\\pi}_{\\theta}(\\cdot|s))^{\\top}D(\\pi_{\\theta}(\\cdot|s))Q^{\\pi*}(s,\\cdot)\\right)}\\\\ &{=\\displaystyle\\sum_{s}\\left(\\prod_{i=1}^{4}X_{i}(s)-\\prod_{i=1}^{4}\\hat{X}_{i}(s)\\right)}\\\\ &{=\\displaystyle\\sum_{s}\\sum_{i=1}^{\\infty}\\hat{X}_{i}(s)\\cdots\\hat{X}_{i-1}(s)\\left(X_{i}(s)-\\hat{X}_{i}(s)\\right)X_{i+1}(s)\\cdots X_{4}(s),}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "625 where $X_{1}(s)=d_{\\pi_{\\theta}}(s)\\in\\mathbb{R}$ , $X_{2}(s)=(\\nabla_{\\theta}\\log\\pi_{\\theta}(\\cdot|s))^{\\top}\\in\\mathbb{R}^{S\\times A}$ , $X_{3}(s)=D(\\pi_{\\theta}(\\cdot|s))\\in\\mathbb{R}^{A\\times A}$ ,   \n626 $X_{4}(s)=Q^{\\pi_{\\theta}}(s,\\cdot)\\in\\mathbb{R}^{A\\times A}$ , and $\\hat{X}_{1}(s),\\ldots,\\hat{X}_{4}(s)$ are similarly defined with $\\pi_{\\theta}$ replaced by $\\hat{\\pi}_{\\boldsymbol{\\theta}}$ . ", "page_idx": 21}, {"type": "text", "text": "627 Therefore, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\|{\\frac{\\partial}{\\partial\\theta}}\\left(\\nu^{\\top}V^{\\pi_{\\theta}}\\right)-{\\frac{\\partial}{\\partial\\theta}}\\left(\\nu^{\\top}V^{\\pi_{\\theta}^{\\prime}}\\right)\\right\\|\\leq\\left(\\operatorname*{max}_{s}\\Gamma(s)\\right)S,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "628 where ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Gamma(s)=\\|\\sum_{s}\\sum_{i=1}^{4}\\hat{X}_{1}(s)\\cdot\\cdot\\cdot\\hat{X}_{i-1}(s)\\left(X_{i}(s)-\\hat{X}_{i}(s)\\right)X_{i+1}(s)\\cdot\\cdot\\cdot X_{4}(s)\\|.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "629 Next, since $d_{\\pi_{\\theta}},d_{\\hat{\\pi}_{\\theta}},\\pi_{\\theta}$ , and $\\scriptstyle{\\hat{\\pi}}_{\\theta}$ are all distributions, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{max}\\{|X_{1}(s)|,|\\hat{X}_{1}(s)|,|X_{3}(s,a)|,|\\hat{X}_{3}(s,a)|\\}\\leq1.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "630 Separately, using Lemma 4.3, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|X_{2}\\|=\\|\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)\\|\\le\\beta(\\|I_{A}\\|+\\|\\mathbf{1}_{A}\\pi_{\\theta}^{\\top}\\|)\\|P_{s}\\|\\|(P^{\\pi_{b}})^{d-1}\\|.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "631 Since all rows of the above matrices have non-negative entries that add up to $1$ , we get ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|Y\\|\\leq2\\beta.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "632 In the rest of the proof, we bound each of $\\|X_{1}-{\\hat{X}}_{1}\\|,\\ldots,\\|X_{4}-{\\hat{X}}_{4}\\|$ . ", "page_idx": 21}, {"type": "text", "text": "633 Finally, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\|X_{4}\\right\\|\\leq{\\frac{1}{1-\\gamma}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "634 Similarly, the same bounds hold for $\\hat{X_{1}},\\hat{X_{2}},\\hat{X_{3}}$ and $\\hat{X_{4}}$ . ", "page_idx": 21}, {"type": "text", "text": "635 From, we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|X_{1}-\\hat{X_{1}}\\|\\leq(1-\\gamma)\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}\\|\\nu^{\\top}(P^{\\pi_{\\theta}})^{t}-\\nu^{\\top}(P^{\\hat{\\pi}_{\\theta}})^{t}\\|}\\\\ &{\\qquad\\qquad\\leq(1-\\gamma)\\|\\nu\\|\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}t d\\epsilon}\\\\ &{\\qquad\\qquad\\leq(1-\\gamma)d\\epsilon\\displaystyle\\sum_{t=0}^{\\infty}\\gamma^{t}t}\\\\ &{\\qquad=\\frac{\\gamma d\\epsilon}{1-\\gamma}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "636 The last relation follows from the fact that $\\textstyle(1-\\gamma)^{-1}=\\sum_{t=0}^{\\infty}\\gamma^{t}$ , which in turn implies ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\gamma\\frac{\\partial}{\\partial\\gamma}\\left(\\frac{1}{1-\\gamma}\\right)=\\sum_{t=0}^{\\infty}t\\gamma^{t}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "637 From Lemma A.5, it follows that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|X_{3}-\\hat{X_{3}}\\|=O(\\beta d\\epsilon).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "638 Next, recall that from Lemma 4.3 that ", "page_idx": 22}, {"type": "equation", "text": "$$\nX_{2}(s,\\cdot)=\\beta\\left[I_{A}-\\mathbf{1}_{A}(\\pi_{\\theta})^{\\top}\\right]P_{s}\\left(P^{\\pi_{b}}\\right)^{d-1}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "639 Then, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|X_{2}(s,\\cdot)-\\hat{X}_{2}(s,\\cdot)\\|\\leq\\|\\beta\\left[I_{A}-\\mathbf{1}_{A}(\\pi_{\\theta})^{\\top}\\right]P_{s}\\|\\|\\left(P^{\\pi_{b}}\\right)^{d-1}-\\left(\\hat{P}^{\\pi_{b}}\\right)^{d-1}\\|}\\\\ &{\\qquad\\qquad\\qquad+\\left\\|\\beta\\left[I_{A}-\\mathbf{1}_{A}(\\pi_{\\theta})^{\\top}\\right]\\|\\|P_{s}-\\hat{P}_{s}\\|\\|\\left(\\hat{P}^{\\pi_{b}}\\right)^{d-1}\\|}\\\\ &{\\qquad\\qquad\\qquad\\qquad+\\beta\\|\\mathbf{1}_{A}(\\pi_{\\theta})^{\\top}-\\mathbf{1}_{A}(\\hat{\\pi}_{\\theta})^{\\top}\\|\\|\\hat{P}_{s}\\left(\\hat{P}^{\\pi_{b}}\\right)^{d-1}\\|.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "640 Following the same argument as in (85) and applying Lemma A.2, we have that (93) is $O(\\beta d\\epsilon)$ .   \n641 Similarly, from the argument of (85), Eq. (94) is $O(\\beta\\epsilon)$ . Lastly, (95) is $O(\\beta d\\epsilon)$ due to Lemma A.5.   \n642 Putting the above three terms together, we have that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|X_{2}(s,\\cdot)-\\hat{X}_{2}(s,\\cdot)\\|=O(\\beta d\\epsilon).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "643 Since the state-action value function satisfies the Bellman equation, we have ", "page_idx": 22}, {"type": "equation", "text": "$$\nQ^{\\pi_{\\theta}}=r+\\gamma P Q^{\\pi_{\\theta}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "644 and ", "page_idx": 22}, {"type": "equation", "text": "$$\nQ^{\\hat{\\pi}_{\\theta}}=\\hat{r}+\\gamma\\hat{P}Q^{\\hat{\\pi}_{\\theta}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "645 Consequently, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|Q^{\\pi_{\\theta}}-Q^{\\hat{\\pi}_{\\theta}}\\|\\leq\\|r-\\hat{r}\\|+\\gamma\\|P Q^{\\pi_{\\theta}}-P Q^{\\hat{\\pi}_{\\theta}}\\|+\\gamma\\|P Q^{\\hat{\\pi}_{\\theta}}-\\hat{P}Q^{\\hat{\\pi}_{\\theta}}\\|}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leq\\epsilon+\\gamma\\|P\\|\\|Q^{\\pi_{\\theta}}-Q^{\\hat{\\pi}_{\\theta}}\\|+\\gamma\\|P-\\hat{P}\\|\\|Q^{\\hat{\\pi}_{\\theta}}\\|}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\epsilon+\\gamma\\|Q^{\\pi_{\\theta}}-Q^{\\hat{\\pi}_{\\theta}}\\|+\\displaystyle\\frac{\\gamma}{1-\\gamma}\\epsilon,}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "646 which finally shows that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|X_{4}-\\hat{X}_{4}\\|=\\|Q^{\\pi_{\\theta}}-Q^{\\hat{\\pi}_{\\theta}}\\|\\leq{\\frac{\\epsilon}{(1-\\gamma)^{2}}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "647 ", "page_idx": 22}, {"type": "text", "text": "648 B Experiments ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "649 B.1 Implementation Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "650 The environment engine is the highly efficient Atari-CuLE [Dalton et al., 2020], a CUDA-based   \n651 version of Atari that runs on GPU. Similarly, we use Atari-CuLE for the GPU-based breadth-first TS   \n652 as done in Dalal et al. [2021]: In every tree expansion, the state $S_{t}$ is duplicated and concatenated   \n665534 taegnasion r woift hn ealxlt  pstoastseisb $(S_{t+1}^{0},\\ldots,S_{t+1}^{A-1})$ .t hTeh feo rnwexatr-ds tamtoe dteeln, seotrc .i sT thhise np rdoucpeldicuartee ids  arenpde catoendc autnetnila ttehde   \n656 final depth is reached, for which $W_{\\theta}(s)$ is applied per state.   \n657 We train SoftTreeMax for depths $d\\,=\\,1\\ldots8$ , with a single worker. We use five seeds for each   \n658 experiment.   \n659 For the implementation, we extend Stable-Baselines3 [Raffin et al., 2019] with all parameters taken   \n660 as default from the original PPO paper [Schulman et al., 2017]. For depths $d\\geq3$ , we limited the   \n661 tree to a maximum width of 1024 nodes and pruned non-promising trajectories in terms of estimated   \n662 weights. Since the distributed PPO baseline advances significantly faster in terms of environment   \n663 steps, for a fair comparison, we ran all experiments for one week on the same machine and use the   \n664 wall-clock time as the $\\mathbf{X}$ -axis. We use Intel(R) Xeon(R) CPU E5-2698 v4 $@$ 2.20GHz equipped with   \n665 one NVIDIA Tesla V100 32GB. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "666 B.2 Time-Based Training Curves ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "667 We provide the training curves in Figure 4. For brevity, we exclude a few of the depths from the plots.   \n668 As seen, there is a clear benefit for SoftTreeMax over distributed PPO with the standard softmax   \n669 policy. In most games, PPO with the SoftTreeMax policy shows very high sample efficiency: it   \n670 achieves higher episodic reward although it observes much less episodes, for the same running time. ", "page_idx": 23}, {"type": "image", "img_path": "PlBUSoSUJG/tmp/e7d139051ee7953d098df352e09da3249be40edf9fe408e21d662bd0228f765f.jpg", "img_caption": ["Figure 4: Training curves: GPU SoftTreeMax (single worker) vs PPO (256 GPU workers). The plots show average reward and standard deviation over 5 seeds. The $\\mathbf{X}$ -axis is the wall-clock time. The runs ended after one week with varying number of time-steps. The training curves correspond to the evaluation runs in Figure 3. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "671 B.3 Step-Based Training Curves ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "672 In Figure 5 we also provide the same convergence plots where the $\\mathbf{X}$ -axis is now the number of online   \n673 interactions with the environment, thus excluding the tree expansion complexity. As seen, due to the   \n674 complexity of the tree expansion, less steps are conducted during training (limited to one week) as   \n675 the depth increases. In this plot, the monotone improvement of the reward with increasing tree depth   \n676 is noticeable in most games.   \n677 We note that not for all games we see monotonicity. Our explanation for this phenomenon relates to   \n678 how immediate reward contributes to performance compared to the value. Different games benefit   \n679 differently from long-term as opposed to short-term planning. Games that require longer-term   \n680 planning need a better value estimate. A good value estimate takes longer to obtain with larger depths,   \n681 in which we apply the network to states that are very different from the ones observed so far in the   \n682 buffer (recall that as in any deep RL algorithm, we train the model only on states in the buffer). If   \n683 the model hasn\u2019t learned a good enough value function yet, and there is no guiding dense reward   \n684 along the trajectory, the policy becomes noisier, and can take more steps to converge \u2013 even more   \n685 than those we run in our week-long experiment.   \n686 For a concrete example, let us compare Breakout to Gopher. Inspecting Fig. 5, we observe that   \n687 Breakout quickly (and monotonically) gains from large depths since it relies on the short term goal   \n688 of simply keeping the paddle below the moving ball. In Gopher, however, for large depths $(>=5)$ ),   \n689 learning barely started even by the end of the training run. Presumably, this is because the task in   \n690 Gopher involves multiple considerations and steps: the agent needs to move to the right spot and   \n691 then hit the mallet the right amount of times, while balancing different locations. This task requires   \n692 long-term planning and thus depends more strongly on the accuracy of the value function estimate.   \n693 In that case, for depth 5 or more, we would require more train steps for the value to \u201ckick in\u201d and   \n694 become beneficial beyond the gain from the reward in the tree.   \n695 The figures above convey two key observations that occur for at least some non-zero depth: (1) The   \n696 final performance with the tree is better than PPO (Fig. 3); and (2) the intermediate step-based results   \n697 with the tree are better than PPO (Fig. 5). This leads to our main takeaway from this work \u2014 there   \n698 is no reason to believe that the vanilla policy gradient algorithm should be better than a multi-step   \n699 variant. Indeed, we show that this is not the case. ", "page_idx": 23}, {"type": "image", "img_path": "PlBUSoSUJG/tmp/f72b82b00c7040438869e6234b9d0e1812ec7b734263e014f40a99daad3eef24.jpg", "img_caption": ["Figure 5: Training curves: GPU SoftTreeMax (single worker) vs PPO (256 GPU workers). The plots show average reward and standard deviation over 5 seeds. The ${\\bf X}$ -axis is the number of online interactions with the environment. The runs ended after one week with varying number of time-steps. The training curves correspond to the evaluation runs in Figure 3. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "700 C Further discussion ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "701 C.1 The case of $\\lambda_{2}(P^{\\pi_{b}})=0$ ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "702 When $P^{\\pi_{b}}$ is rank one, it is not only its variance that becomes 0, but also the norm of the gradient   \n703 itself (similarly to the case of $d\\to\\infty$ ). Note that such a situation will happen rarely, in degenerate   \n704 MDPs. This is a local minimum for SoftTreeMax and it would cause the PG iteration to get stuck,   \n705 and to the optimum in the (desired but impractical) case where $\\pi_{b}$ is the optimal policy. However,   \n706 a similar phenomenon was also discovered in the standard softmax with deterministic policies:   \n707 $\\theta(s,a)\\to\\infty$ for one $a$ per $s$ . PG with softmax would suffer very slow convergence near these   \n708 local equilibria, as observed in Mei et al. [2020a]. To see this, note that the softmax gradient is   \n709 $\\nabla_{\\theta}\\log\\bar{\\pi}_{\\theta}(a|s)=e_{a}-\\pi_{\\theta}(\\cdot|s)$ , where $e_{a}\\in[0,1]^{A}$ is the vector with 0 everywhere except for the   \n710 $\\footnote{C h a n n e l a g i n g i s C S I i n a c c u r a c y d u e t o t i m e v a r i a t i o n o f w i r e l e s s c h a n n e l s a n d d e l a y s i n t h e c o m p u t a t i o n~.I n t h i s w o r k,w e a c t i v e l y i n t r o d u c e C S I i n a c c u r a c y b y u s i n g a n I-I R S.T o d i f f e r e n t i a t e,w e c a l l i t a c t i v e c h a n n e l a g i n g.}$ -th coordinate. I.e., it will be zero for a deterministic policy. SoftTreeMax avoids these local optima   \n711 by integrating the reward into the policy itself (but may get stuck in another, as discussed above). ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "712 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "713 1. Claims   \n714 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n715 paper\u2019s contributions and scope?   \n716 Answer: [Yes]   \n717 Justification: [NA]   \n718 Guidelines:   \n719 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n720 made in the paper.   \n721 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n722 contributions made in the paper and important assumptions and limitations. A No or   \n723 NA answer to this question will not be perceived well by the reviewers.   \n724 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n725 much the results can be expected to generalize to other settings.   \n726 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n727 are not attained by the paper.   \n728 2. Limitations   \n729 Question: Does the paper discuss the limitations of the work performed by the authors?   \n730 Answer: [Yes]   \n731 Justification: We included a relevant section at the end of the paper.   \n732 Guidelines:   \n733 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n734 the paper has limitations, but those are not discussed in the paper.   \n735 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n736 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n737 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n738 model well-specification, asymptotic approximations only holding locally). The authors   \n739 should reflect on how these assumptions might be violated in practice and what the   \n740 implications would be.   \n741 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n742 only tested on a few datasets or with a few runs. In general, empirical results often   \n743 depend on implicit assumptions, which should be articulated.   \n744 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n745 For example, a facial recognition algorithm may perform poorly when image resolution   \n746 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n747 used reliably to provide closed captions for online lectures because it fails to handle   \n748 technical jargon.   \n749 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n750 and how they scale with dataset size.   \n751 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n752 address problems of privacy and fairness.   \n753 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n754 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n755 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n756 judgment and recognize that individual actions in favor of transparency play an impor  \n757 tant role in developing norms that preserve the integrity of the community. Reviewers   \n758 will be specifically instructed to not penalize honesty concerning limitations.   \n759 3. Theory Assumptions and Proofs   \n760 Question: For each theoretical result, does the paper provide the full set of assumptions and ", "page_idx": 26}, {"type": "text", "text": "761 a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "63 Justification: All proofs can be found in the appendix.   \n64 Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 27}, {"type": "text", "text": "775 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "76 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n77 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n78 of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: Yes. We also attached the repository, together with a docker environment, as supplementary material. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instruc6 tions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "818 Answer: [Yes]   \n819 Justification: We attached the repository, together with a docker environment, as supplemen  \n820 tary material.   \n821 Guidelines:   \n822 \u2022 The answer NA means that paper does not include experiments requiring code.   \n823 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n824 public/guides/CodeSubmissionPolicy) for more details.   \n825 \u2022 While we encourage the release of code and data, we understand that this might not be   \n826 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n827 including code, unless this is central to the contribution (e.g., for a new open-source   \n828 benchmark).   \n829 \u2022 The instructions should contain the exact command and environment needed to run to   \n830 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n831 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n832 \u2022 The authors should provide instructions on data access and preparation, including how   \n833 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n834 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n835 proposed method and baselines. If only a subset of experiments are reproducible, they   \n836 should state which ones are omitted from the script and why.   \n837 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n838 versions (if applicable).   \n839 \u2022 Providing as much information as possible in supplemental material (appended to the   \n840 paper) is recommended, but including URLs to data and code is permitted.   \n841 6. Experimental Setting/Details   \n842 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n843 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n844 results?   \n845 Answer: [Yes]   \n846 Justification: [NA]   \n847 Guidelines:   \n848 \u2022 The answer NA means that the paper does not include experiments.   \n849 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n850 that is necessary to appreciate the results and make sense of them.   \n851 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n852 material.   \n853 7. Experiment Statistical Significance   \n854 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n855 information about the statistical significance of the experiments?   \n856 Answer: [Yes]   \n857 Justification: [NA]   \n858 Guidelines:   \n859 \u2022 The answer NA means that the paper does not include experiments.   \n860 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n861 dence intervals, or statistical significance tests, at least for the experiments that support   \n862 the main claims of the paper.   \n863 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n864 example, train/test split, initialization, random drawing of some parameter, or overall   \n865 run with given experimental conditions).   \n866 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n867 call to a library function, bootstrap, etc.)   \n868 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n869 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n870 of the mean.   \n871 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n872 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n873 of Normality of errors is not verified.   \n874 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n875 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n876 error rates).   \n877 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n878 they were calculated and reference the corresponding figures or tables in the text.   \n879 8. Experiments Compute Resources   \n880 Question: For each experiment, does the paper provide sufficient information on the com  \n881 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n882 the experiments?   \n883 Answer: [Yes]   \n884 Justification: All relevant information can be found in the paper and the appendix.   \n885 Guidelines:   \n886 \u2022 The answer NA means that the paper does not include experiments.   \n887 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n888 or cloud provider, including relevant memory and storage.   \n889 \u2022 The paper should provide the amount of compute required for each of the individual   \n890 experimental runs as well as estimate the total compute.   \n891 \u2022 The paper should disclose whether the full research project required more compute   \n892 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n893 didn\u2019t make it into the paper).   \n894 9. Code Of Ethics   \n895 Question: Does the research conducted in the paper conform, in every respect, with the   \n896 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n897 Answer: [Yes]   \n898 Justification: [NA]   \n899 Guidelines:   \n900 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n901 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n902 deviation from the Code of Ethics.   \n903 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n904 eration due to laws or regulations in their jurisdiction).   \n905 10. Broader Impacts   \n906 Question: Does the paper discuss both potential positive societal impacts and negative   \n907 societal impacts of the work performed?   \n908 Answer: [NA]   \n909 Justification: The paper has no scietal impact.   \n910 Guidelines:   \n911 \u2022 The answer NA means that there is no societal impact of the work performed.   \n912 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n913 impact or why the paper does not address societal impact.   \n914 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n915 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n916 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n917 groups), privacy considerations, and security considerations.   \n918 \u2022 The conference expects that many papers will be foundational research and not tied   \n919 to particular applications, let alone deployments. However, if there is a direct path to   \n920 any negative applications, the authors should point it out. For example, it is legitimate   \n921 to point out that an improvement in the quality of generative models could be used to   \n922 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n923 that a generic algorithm for optimizing neural networks could enable people to train   \n924 models that generate Deepfakes faster.   \n925 \u2022 The authors should consider possible harms that could arise when the technology is   \n926 being used as intended and functioning correctly, harms that could arise when the   \n927 technology is being used as intended but gives incorrect results, and harms following   \n928 from (intentional or unintentional) misuse of the technology.   \n929 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n930 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n931 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n932 feedback over time, improving the efficiency and accessibility of ML).   \n933 11. Safeguards   \n934 Question: Does the paper describe safeguards that have been put in place for responsible   \n935 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n936 image generators, or scraped datasets)?   \n937 Answer: [NA]   \n938 Justification: The paper poses no such risks.   \n939 Guidelines:   \n940 \u2022 The answer NA means that the paper poses no such risks.   \n941 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n942 necessary safeguards to allow for controlled use of the model, for example by requiring   \n943 that users adhere to usage guidelines or restrictions to access the model or implementing   \n944 safety filters.   \n945 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n946 should describe how they avoided releasing unsafe images.   \n947 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n948 not require this, but we encourage authors to take this into account and make a best   \n949 faith effort.   \n950 12. Licenses for existing assets   \n951 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n952 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n953 properly respected?   \n954 Answer: [Yes]   \n955 Justification: [NA]   \n956 Guidelines:   \n957 \u2022 The answer NA means that the paper does not use existing assets.   \n958 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n959 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n960 URL.   \n961 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n962 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n963 service of that source should be provided.   \n964 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n965 package should be provided. For popular datasets, paperswithcode.com/datasets   \n966 has curated licenses for some datasets. Their licensing guide can help determine the   \n967 license of a dataset.   \n968 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n969 the derived asset (if it has changed) should be provided.   \n970 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n971 the asset\u2019s creators.   \n972 13. New Assets   \n973 Question: Are new assets introduced in the paper well documented and is the documentation   \n974 provided alongside the assets?   \n975 Answer: [NA]   \n976 Justification: The paper does not release new assets.   \n977 Guidelines:   \n978 \u2022 The answer NA means that the paper does not release new assets.   \n979 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n980 submissions via structured templates. This includes details about training, license,   \n981 limitations, etc.   \n982 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n983 asset is used.   \n984 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n985 create an anonymized URL or include an anonymized zip file.   \n986 14. Crowdsourcing and Research with Human Subjects   \n987 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n988 include the full text of instructions given to participants and screenshots, if applicable, as   \n989 well as details about compensation (if any)?   \n990 Answer: [NA]   \n991 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n992 Guidelines:   \n993 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n994 human subjects.   \n995 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n996 tion of the paper involves human subjects, then as much detail as possible should be   \n997 included in the main paper.   \n998 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n999 or other labor should be paid at least the minimum wage in the country of the data   \n000 collector.   \n001 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n002 Subjects   \n003 Question: Does the paper describe potential risks incurred by study participants, whether   \n004 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n005 approvals (or an equivalent approval/review based on the requirements of your country or   \n006 institution) were obtained?   \n007 Answer: [NA]   \n008 Justification: The paper does not involve crowdsourcing nor research with human subjects.   \n009 Guidelines:   \n010 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n1011 human subjects.   \n012 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n013 may be required for any human subjects research. If you obtained IRB approval, you   \n014 should clearly state this in the paper.   \n1015 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n016 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n017 guidelines for their institution.   \n018 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n019 applicable), such as the institution conducting the review. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}]