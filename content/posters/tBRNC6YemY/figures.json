[{"figure_path": "tBRNC6YemY/figures/figures_1_1.jpg", "caption": "Figure 2: Accuracy (vs) hallucination in four settings, that is, zero-shot (i.e., without any retriever), and with retrievers. Commonly used BM25 and GPT retrievers, and the oracle - returns relevant documents with perfect recall, indicating an upper bound. Higher in the graph (higher accuracy) and to the left (lower hallucination) is better. Across settings, our model, Gorilla, improves accuracy while reducing hallucination.", "description": "This figure compares the performance of Gorilla and other LLMs (GPT-3.5, GPT-4, LLaMA, Claude) across four different retrieval settings: zero-shot (no retriever), BM25 retriever, GPT retriever, and oracle retriever.  The x-axis represents the hallucination rate, and the y-axis represents the accuracy.  Gorilla consistently outperforms other models in terms of both accuracy and reduced hallucination, demonstrating its superior ability to utilize tools accurately.", "section": "4 Evaluation"}, {"figure_path": "tBRNC6YemY/figures/figures_3_1.jpg", "caption": "Figure 3: Gorilla: A system for enabling LLMs to interact with APIs. The upper half represents the training procedure as described in Sec 3. This is the most exhaustive API data-set for ML to the best of our knowledge. During inference (lower half), Gorilla supports two modes - with retrieval, and zero-shot. In this example, it is able to suggest the right API call for generating the image from the user's natural language query.", "description": "This figure illustrates the Gorilla system architecture.  The upper half shows the training process, highlighting the creation of a comprehensive dataset of API calls from various sources (Torch Hub, TensorFlow Hub, HuggingFace),  the self-instruct method used to generate training examples, and the training of the Gorilla-7B LLaMA model. The lower half shows the inference process, illustrating how Gorilla takes a user's natural language query, uses an optional information retriever to find relevant APIs, and then generates an appropriate API call. The example showcases how a request to generate an image of dancing cats is correctly translated into an API call using the StableDiffusion pipeline.", "section": "3. Methodology"}, {"figure_path": "tBRNC6YemY/figures/figures_5_1.jpg", "caption": "Figure 4: AST Sub-Tree Matching to evaluate API calls. On the left is an API call returned by Gorilla. We first build the associated API tree. We then compare this to our dataset, to see if the API dataset has a subtree match. In the above example, the matching subtree is highlighted in green, signifying that the API call is indeed correct. Pretrained=True is an optional argument.", "description": "This figure illustrates the Abstract Syntax Tree (AST) based sub-tree matching technique used to evaluate the accuracy of API calls generated by the Gorilla model.  The left side shows a sample API call and its corresponding tree representation. The right side shows how this tree is compared against the API calls in the dataset. A match (highlighted in green) indicates a correct API call; a mismatch implies an error or hallucination. The example highlights how optional arguments are handled in the matching process.", "section": "3 Methodology"}, {"figure_path": "tBRNC6YemY/figures/figures_6_1.jpg", "caption": "Figure 5: Accuracy with GPT-retriever. Methods to the left of the dotted line are closed source. Gorilla outperforms on Torch Hub and Hugging-Face while matching performance on Tensorflow Hub for all existing state-of-the-art LLMs - closed source, and open source.", "description": "This figure shows the accuracy of different LLMs (GPT-3.5, GPT-4, Claude, LLaMA, and Gorilla) on three different API hubs (Torch Hub, HuggingFace, and TensorFlow Hub) when using a GPT retriever.  The results demonstrate that Gorilla significantly outperforms other LLMs on Torch Hub and HuggingFace, achieving comparable performance to the best models on TensorFlow Hub.  The dotted line in the chart separates closed-source models (to the left) from open-source models (to the right).", "section": "4.1 AST Accuracy on API call"}, {"figure_path": "tBRNC6YemY/figures/figures_17_1.jpg", "caption": "Figure 5: Accuracy with GPT-retriever. Methods to the left of the dotted line are closed source. Gorilla outperforms on Torch Hub and Hugging-Face while matching performance on Tensorflow Hub for all existing state-of-the-art LLMs - closed source, and open source.", "description": "The figure compares the accuracy of different LLMs (including Gorilla) on three different API hubs (Torch Hub, HuggingFace, and TensorFlow Hub) when using a GPT retriever.  The left side shows closed-source LLMs, and the right open-source LLMs.  Gorilla demonstrates superior or comparable performance across all three hubs compared to other LLMs.", "section": "4.1 AST Accuracy on API call"}, {"figure_path": "tBRNC6YemY/figures/figures_18_1.jpg", "caption": "Figure 5: Accuracy with GPT-retriever. Methods to the left of the dotted line are closed source. Gorilla outperforms on Torch Hub and Hugging-Face while matching performance on Tensorflow Hub for all existing state-of-the-art LLMs - closed source, and open source.", "description": "The figure compares the accuracy of several LLMs (including Gorilla) when using a GPT retriever across three different API hubs (Torch Hub, HuggingFace, and TensorFlow Hub).  It shows Gorilla achieving either comparable or superior performance to other models, particularly surpassing closed-source models on Torch Hub and HuggingFace.", "section": "4.1 AST Accuracy on API call"}, {"figure_path": "tBRNC6YemY/figures/figures_18_2.jpg", "caption": "Figure 1: Examples of API calls. Example API calls generated by GPT-4 [27], Claude [2], and Gorilla for the given prompt. In this example, GPT-4 presents a model that doesn't exist, and Claude picks an incorrect library. In contrast, our Gorilla model can identify the task correctly and suggest a fully-qualified API call.", "description": "This figure shows examples of API calls generated by three different large language models (LLMs): GPT-4, Claude, and the authors' proposed model, Gorilla.  The prompt asks each model to provide an API call to convert spoken language in a recorded audio file to text using Torch Hub.  The figure highlights that while GPT-4 hallucinates a non-existent model and Claude selects an incorrect library, Gorilla correctly identifies the task and generates a fully qualified API call.  This demonstrates Gorilla's superior ability to accurately and effectively use tools via API calls.", "section": "1 Introduction"}, {"figure_path": "tBRNC6YemY/figures/figures_19_1.jpg", "caption": "Figure 13: For the same train-eval dataset, our fine-tuning recipe, RAT, is robust to the underlying base model.", "description": "The bar chart displays the accuracy of Gorilla models (Gorilla-LLaMA, Gorilla-MPT, Gorilla-Falcon) trained on the HuggingFace dataset, showcasing the robustness of the RAT fine-tuning method across different base models.  Despite using different pre-trained models as the foundation, the accuracy remains relatively consistent, highlighting the effectiveness and generalizability of RAT. The figure supports the paper's claim that RAT is not highly sensitive to the choice of pre-trained model.", "section": "A.3.5 Sensitivity to pre-training"}]