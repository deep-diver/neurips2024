[{"figure_path": "O7IN4nsaIO/figures/figures_3_1.jpg", "caption": "Figure 1: Comparison among D-SGDA, D-TiAda and D-AdaST for NC-SC quadratic objective function (6) with n = 2 nodes and x = y. In (a), it shows the trajectories of primal and dual variables of the algorithms, the points on the black dash line are stationary points of f. In (b), it shows the convergence of ||\u2207x f (xk, Yk)||\u00b2 over the iterations. In (c), it shows the convergence of the inconsistency of stepsizes, \u03be2 defined in (8), over the iterations. Notably, \u03be2 fails to converge for D-TiAda and \u03be2 = 0 for non-adaptive D-SGDA.", "description": "This figure compares three distributed adaptive minimax optimization algorithms: D-SGDA, D-TiAda, and D-AdaST.  Subfigure (a) shows the trajectories of primal and dual variables for each algorithm on a simple NC-SC quadratic objective function with only 2 nodes. The black dashed line indicates the stationary points of the objective function. Subfigure (b) plots the convergence of ||\u2207x f (xk, Yk)||\u00b2, which measures the norm of the gradient of the objective function with respect to the primal variable x. Subfigure (c) displays the convergence of the stepsize inconsistency (\u03be2). This inconsistency is a key issue with vanilla distributed adaptive methods, and this figure highlights how D-AdaST effectively addresses it.", "section": "Non-Convergence of Direct Extensions"}, {"figure_path": "O7IN4nsaIO/figures/figures_8_1.jpg", "caption": "Figure 2: Performance comparison of algorithms on quadratic functions over exponential graphs with node counts n = {50, 100} and different initial stepsizes (yy = 0.1).", "description": "The figure compares the performance of four distributed adaptive minimax optimization algorithms (D-AdaGrad, D-NeAda, D-TiAda, and D-AdaST) on solving non-convex strongly concave quadratic objective functions across different network sizes (n=50 and n=100) and initial stepsizes. The plots illustrate the convergence of the algorithms, measured by the squared norm of the gradient of the primal variable (||\u2207xf(x,y)||\u00b2), over the number of gradient calls. The results demonstrate that D-AdaST consistently outperforms other algorithms across various settings, highlighting its effectiveness and robustness in distributed environments. The plots demonstrate the impact of network size and initial stepsizes on algorithm performance.", "section": "4 Experiments"}, {"figure_path": "O7IN4nsaIO/figures/figures_8_2.jpg", "caption": "Figure 1: Comparison among D-SGDA, D-TiAda and D-AdaST for NC-SC quadratic objective function (6) with n = 2 nodes and x = y. In (a), it shows the trajectories of primal and dual variables of the algorithms, the points on the black dash line are stationary points of f. In (b), it shows the convergence of ||\u2207x f (xk, Yk)||\u00b2 over the iterations. In (c), it shows the convergence of the inconsistency of stepsizes, 2 defined in (8), over the iterations. Notably, 2 fails to converge for D-TiAda and 2 = 0 for non-adaptive D-SGDA.", "description": "This figure compares the performance of three distributed minimax optimization algorithms (D-SGDA, D-TiAda, and D-AdaST) on a simple nonconvex-strongly-concave quadratic objective function.  It illustrates the trajectories of primal and dual variables, the convergence of the gradient norm, and importantly, the convergence of the inconsistency in stepsizes. The results highlight that D-AdaST is the only algorithm that converges to a stationary point and resolves the issue of inconsistent stepsizes.", "section": "Non-Convergence of Direct Extensions"}, {"figure_path": "O7IN4nsaIO/figures/figures_9_1.jpg", "caption": "Figure 4: Training GANs on CIFAR-10 dataset over exponential graphs with n = 10 nodes.", "description": "The figure compares the performance of three different algorithms (D-Adam, D-TiAda-Adam, and D-AdaST-Adam) on training GANs using the CIFAR-10 dataset.  The experiment is performed on an exponential graph with 10 nodes.  The x-axis represents the number of gradient calls, and the y-axis represents the inception score, a measure of the quality of the generated images.  Three subplots show results for different initial stepsizes (\u03b3x = \u03b3y = 0.001, \u03b3x = \u03b3y = 0.01, \u03b3x = \u03b3y = 0.05).  The results demonstrate that D-AdaST-Adam consistently achieves a higher inception score, indicating superior performance in generating high-quality images.", "section": "4 Experiments"}, {"figure_path": "O7IN4nsaIO/figures/figures_14_1.jpg", "caption": "Figure 5: Performance comparison of training CNN on MNIST with n = 20 nodes over directed ring and fully connected graphs.", "description": "The figure compares the performance of four different distributed adaptive minimax optimization algorithms (D-AdaGrad, D-NeAda, D-TiAda, and D-AdaST) on training a Convolutional Neural Network (CNN) for image classification using the MNIST dataset.  The algorithms are evaluated across two different graph topologies: directed ring and fully connected. The plot shows the convergence of the algorithms measured by the squared norm of the gradient of the loss function, \\(||\\nabla_x f(x, y)\\|^2\\), against the number of gradient calls.  The results demonstrate how D-AdaST outperforms other algorithms, especially in a dense graph topology, indicating the proposed algorithm's robustness and effectiveness.", "section": "4 Experiments"}, {"figure_path": "O7IN4nsaIO/figures/figures_14_2.jpg", "caption": "Figure 2: Performance comparison of algorithms on quadratic functions over exponential graphs with node counts n = {50, 100} and different initial stepsizes (yy = 0.1).", "description": "The figure compares the performance of D-AdaGrad, D-NeAda, D-TiAda, and D-AdaST algorithms on quadratic functions.  It shows the convergence of ||\u2207xf(x, y)||\u00b2 (the squared norm of the gradient of f with respect to x) over the number of gradient calls.  The comparison is made for two different network sizes (n=50 and n=100 nodes) and two different initial stepsize settings (x=0.1 and x=0.02, with y=0.1 in both cases).  The plots illustrate how D-AdaST consistently achieves faster convergence compared to other algorithms across all settings.", "section": "4 Experiments"}, {"figure_path": "O7IN4nsaIO/figures/figures_14_3.jpg", "caption": "Figure 2: Performance comparison of algorithms on quadratic functions over exponential graphs with node counts n = {50, 100} and different initial stepsizes (yy = 0.1).", "description": "This figure compares the performance of four distributed adaptive minimax optimization algorithms (D-AdaGrad, D-NeAda, D-TiAda, and D-AdaST) on a quadratic objective function using two different graph topologies (exponential and dense graphs) and two different numbers of nodes (50 and 100). The algorithms differ in how they handle step sizes: D-AdaGrad is a basic adaptive method; D-NeAda is a nested adaptive method; D-TiAda is a single-loop adaptive method with time-scale separation; and D-AdaST is a newly proposed method that includes a stepsize tracking mechanism to improve consistency of step sizes across nodes. The plots show the convergence of the gradient norm ||\u2207xf(x,y)||\u00b2 as a function of the number of gradient calls. The results demonstrate that D-AdaST achieves faster convergence compared to the other algorithms, especially when the initial step size is favorable. This highlights the effectiveness of the step size tracking mechanism in D-AdaST for ensuring the convergence of distributed adaptive methods.", "section": "4 Experiments"}, {"figure_path": "O7IN4nsaIO/figures/figures_14_4.jpg", "caption": "Figure 2: Performance comparison of algorithms on quadratic functions over exponential graphs with node counts n = {50, 100} and different initial stepsizes (yy = 0.1).", "description": "This figure compares the performance of four different distributed adaptive minimax optimization algorithms (D-AdaGrad, D-NeAda, D-TiAda, and D-AdaST) on two different graph topologies (exponential and dense graphs) with varying node counts (n = 50 and n = 100).  The algorithms are tested using two sets of initial stepsizes (x=0.1, y=0.1 and x=0.02, y=0.1) to find an e-stationary point of a nonconvex-strongly concave quadratic function.  The plots show the convergence of ||\u2207x f(x,y)||\u00b2 over gradient calls. The results demonstrate that D-AdaST consistently outperforms other algorithms across various settings.", "section": "4 Experiments"}, {"figure_path": "O7IN4nsaIO/figures/figures_15_1.jpg", "caption": "Figure 4: Training GANs on CIFAR-10 dataset over exponential graphs with n = 10 nodes.", "description": "This figure compares the performance of three distributed adaptive minimax optimization algorithms, namely D-Adam, D-TiAda-Adam, and D-AdaST-Adam, on the task of training Generative Adversarial Networks (GANs) using the CIFAR-10 dataset.  The x-axis represents the number of gradient calls, and the y-axis represents the inception score, a metric used to evaluate the quality of generated images. The experiment was conducted on an exponential graph with 10 nodes.  The figure demonstrates that D-AdaST-Adam achieves a higher inception score compared to the other two algorithms, indicating its superior performance in this task.", "section": "4 Experiments"}, {"figure_path": "O7IN4nsaIO/figures/figures_15_2.jpg", "caption": "Figure 10: Performance comparison of D-AdaST on quadratic functions over an exponential graph of n = 50 nodes with different choices of \u03b1 and \u03b2.", "description": "This figure shows the performance of the D-AdaST algorithm on quadratic functions with different choices of \u03b1 and \u03b2 parameters over an exponential graph with 50 nodes.  It illustrates how the convergence rate is affected by the selection of \u03b1 and \u03b2. The plot shows that smaller values of \u03b1 - \u03b2 lead to faster initial convergence but possibly a higher steady-state error.", "section": "4 Experiments"}]