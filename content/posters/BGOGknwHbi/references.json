{"references": [{"fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-06", "reason": "This paper is foundational in demonstrating the few-shot learning capabilities of large language models, which is a key aspect of the current paper's methodology."}, {"fullname_first_author": "Aman Madaan", "paper_title": "Self-refine: Iterative refinement with self-feedback", "publication_date": "2023-03-17", "reason": "This paper introduces the self-refine prompting technique, which is directly relevant to the self-guiding exploration strategy in the current paper."}, {"fullname_first_author": "Takeshi Kojima", "paper_title": "Large language models are zero-shot reasoners", "publication_date": "2022-12-01", "reason": "This paper establishes the zero-shot reasoning capabilities of LLMs, a crucial foundation for the current paper's approach of using LLMs for CPs without extensive task-specific training."}, {"fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-12-01", "reason": "The chain-of-thought prompting technique, introduced in this paper, is a core element of the SGE prompting strategy used in this work."}, {"fullname_first_author": "Shunyu Yao", "paper_title": "Tree of thoughts: Deliberate problem solving with large language models", "publication_date": "2023-05-10", "reason": "The tree-of-thoughts prompting method, presented in this paper, inspired the exploration of multiple solution trajectories in the current paper's SGE strategy."}]}