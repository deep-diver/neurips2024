{"references": [{"fullname_first_author": "Mohammad Gheshlaghi Azar", "paper_title": "Minimax regret bounds for reinforcement learning", "publication_date": "2017-00-00", "reason": "This paper provides foundational minimax regret bounds for reinforcement learning, which are crucial for analyzing the efficiency of the proposed algorithms in this paper."}, {"fullname_first_author": "Dimitri Bertsekas", "paper_title": "A course in reinforcement learning", "publication_date": "2023-00-00", "reason": "This book is a comprehensive overview of reinforcement learning, providing essential background on fundamental concepts used in the paper, such as dynamic programming and Bellman equations."}, {"fullname_first_author": "Omar Darwiche Domingues", "paper_title": "Episodic reinforcement learning in finite mdps: Minimax lower bounds revisited", "publication_date": "2021-00-00", "reason": "This paper establishes refined minimax lower bounds for episodic reinforcement learning, providing a benchmark against which the algorithm's performance in this paper can be measured."}, {"fullname_first_author": "Yonathan Efroni", "paper_title": "Tight regret bounds for model-based reinforcement learning with greedy policies", "publication_date": "2019-00-00", "reason": "This paper provides state-of-the-art regret bounds for model-based reinforcement learning, offering insights into the design and analysis of the efficient algorithms in this paper."}, {"fullname_first_author": "Zihan Zhang", "paper_title": "Is q-learning provably efficient?", "publication_date": "2018-00-00", "reason": "This paper addresses the fundamental question of Q-learning's efficiency, which is highly relevant to the analysis of the proposed algorithms that are inspired by Q-learning update rules."}]}