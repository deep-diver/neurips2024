[{"type": "text", "text": "Reprogramming Pretrained Target-Specific Diffusion Models for Dual-Target Drug Design ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xiangxin Zhou1,2 Jiaqi Guan3 Yijia Zhang4 ", "page_idx": 0}, {"type": "text", "text": "Xingang Peng5 Liang Wang1,2 Jianzhu Ma4,6,\u2217 ", "page_idx": 0}, {"type": "text", "text": "1School of Artificial Intelligence, University of Chinese Academy of Sciences 2New Laboratory of Pattern Recognition (NLPR),   \nState Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA)   \n3Department of Computer Science, University of Illinois Urbana-Champaign 4Department of Electronic Engineering, Tsinghua University 5Institute for Artificial Intelligence, Peking University 6Institute for AI Industry Research, Tsinghua University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Dual-target therapeutic strategies have become a compelling approach and attracted significant attention due to various benefits, such as their potential in overcoming drug resistance in cancer therapy. Considering the tremendous success that deep generative models have achieved in structure-based drug design in recent years, we formulate dual-target drug design as a generative task and curate a novel dataset of potential target pairs based on synergistic drug combinations. We propose to design dual-target drugs with diffusion models that are trained on single-target protein-ligand complex pairs. Specifically, we align two pockets in 3D space with protein-ligand binding priors and build two complex graphs with shared ligand nodes for SE(3)-equivariant composed message passing, based on which we derive a composed drift in both 3D and categorical probability space in the generative process. Our algorithm can well transfer the knowledge gained in single-target pretraining to dual-target scenarios in a zero-shot manner. We also repurpose linker design methods as strong baselines for this task. Extensive experiments demonstrate the effectiveness of our method compared with various baselines. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A promising paradigm of rational drug design is structure-based drug design (SBDD) [1], which uses computational chemistry tools in which the 3D structure of a protein target is used as the basis to identify or design new chemical entities. The foundation of structure-based drug design has been grounded in the lock-and-key hypothesis, positing that an optimal ligand molecule should possess a structure that is complementary to the target site. Recently, dual-target drug design, which aims to design \u201cone key\u201d for \u201ctwo locks\u201d, has attracted significant attention. Precisely, dual-target drug design [4] is a strategy in pharmaceutical research that aims to design single ligand molecules capable of interacting with two different biological targets simultaneously. A dual-target drug can potentially lower the odds of resistance developing [51] and effectively manage the disease which involve complex biological pathways with multiple proteins [40]. Recent years have witnessed a noticeable increase in the FDA\u2019s approval of dual-target drugs [27, 29]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Deep learning, particularly deep generative models [52] and geometric deep learning [38], has been introduced to SBDD and achieved promising results. Peng et al. [37], Zhang et al. [54] proposed to sequentially generate atoms or fragments using auto-regressive generative models conditioned on a specific protein binding site. Guan et al. [14], Lin et al. [28], Schneuing et al. [44] proposed to generate ligand molecules with diffusion models and achieved high binding affinity. However, due to the scarcity of data resources and high computational complexity, there is limited progress on introducing powerful generative models into dual-target drug design. Besides, there also lacks a comprehensive benchmark and dataset for evaluating the dual-target drug design, which also hinders the community from developing AI-powered computational tools for dual-target drug design. ", "page_idx": 1}, {"type": "text", "text": "To overcome the aforementioned challenges, we first propose a dataset for dual-target drug design. The design of dual-target drugs for arbitrary target pairs lacks substantive purpose. Inspired by the concept of drug synergism [48], where the combined effect of two drugs surpasses the effects of each drug when used individually, we carefully select pairs of targets from combinations of drugs that demonstrate significant synergistic interactions. The effectiveness of such combination therapy [34, 41, 36] has demonstrate significant efficacy in tumor eradication at both cellular level and in vivo study. Designing dual-target drugs for the paired targets may further improve the efficacy and reduce side effects. Additionally, we also provide a reference ligand for each target and the 3D structure of each protein-ligand complex in our dataset. Besides, we formulate the dual-target drug design as a generative task, based on which we further propose to reprogram pretrained target-specific diffusion models as introduced by Guan et al. [14] for the dual-target setting in zero-shot manner. More specifically, we first align dual pockets in 3D space with protein-ligand interaction priors that encapsulate the intricate features of the pockets. We compose the predicted drift terms in both 3D and categorical probability space in the reverse generative process of the diffusion model to generate dual-target drugs. We name this method as COMPDIFF. We further improve this method by building two complex graphs with shared ligand nodes for SE(3)-equivariant composed message passing. In this method, we compose the SE(3)-equivariant message at each layer of the equivariant neural network instead of only on the output level. We name this method as DUALDIFF. Our approach effectively transfers the knowledge acquired from pretraining on single-target datasets, circumventing the challenging demand for extensive training data required for dual-target drug design. We also repurpose linker design methods [17, 12] as a strong baseline for this task. We outline strategies to identify potential fragments from the synergistic drug combinations, serving as input for these linker design methods. We highlight our main contributions as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We present a meticulously curated dataset derived from synergistic drug combinations for dualtarget drug design, offering new opportunities for AI-driven drug discovery.   \n\u2022 We propose SE(3)-equivariant composed message for compositional generative sampling to reprogram pretrained single-target diffusion models for dual-target drug design in a zero-shot way.   \n\u2022 We propose fragment selection methods from synergistic drug combinations for repurposing linker design methods as strong baselines for dual-target drug design.   \n\u2022 Our method can be viewed as a general framework where any pretrained generative models for SBDD can be applied to dual-target drug design without any fine-tuning. We select TargetDiff as a demonstrative demo in our work. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Structure-based Drug Design Structure-based drug design (SBDD) aims to design ligand molecules that can bind to specific protein targets. The introduction of deep generative models has marked a paradigm shift, yielding noteworthy outcomes. Ragoza et al. [39] utilized a variational autoencoder to generate 3D molecules within atomic density grids. Luo et al. [35], Peng et al. [37], Liu et al. [31] employed an autoregressive model to sequentially construct 3D molecules atom by atom, while Zhang et al. [54] introduced a method for generating 3D molecules by successively predicting molecular fragments in an auto-regressive way. Guan et al. [14], Schneuing et al. [44], Lin et al. [28] introduced diffusion models [16] to SBDD, which first generate the types and positions of atoms by iteratively denoising with an SE(3)-equivariant neural network [43, 13] and then determine bond types by post-processing. Some recent studies have endeavored to further improve the aforementioned methods through the integration of biochemical prior knowledge. Guan et al. ", "page_idx": 1}, {"type": "text", "text": "[15] proposed decomposed priors, bond diffusion and validity guidance to improve the quality of ligand molecules generated by diffusion models. Zhang and Liu [53] augmented molecule generation through global interaction between subpocket prototypes and molecular motifs. Huang et al. [18] incorporated protein-ligand interaction prior into both forward and reverse processes to improve the diffusion models. Zhou et al. [55] integrated conditional diffusion models with iterative optimization to optimize properties of generated molecules. The above works focus on structure-based single-target drug design, while our work aims at dual-target drug design. ", "page_idx": 2}, {"type": "text", "text": "Molecular Linker Design Molecular linker design, which enables the connection of molecular fragments to form potent compounds, is an effective approach in rational drug discovery. Approaches like DeLinker [21] and Develop [22] design linkers by utilizing molecular graphs with distance and angle information between anchor atoms, but they lack 3D structural information of molecules. More recent techniques, such as 3DLinker [17] and DiffLinker [20], generate linkers directly in 3D space using conditional VAEs and diffusion models, respectively, but they assume known fragment poses. LinkerNet [12] relaxes this assumption by co-designing molecular fragment poses and the linker, making it applicable in cases where fragment poses are unknown, such as in the linker design of PROTACs (PROteolysis TArgeting Chimeras). Since pharmacophore combination is a traditional strategy to design dual-target drugs, we repurpose linker design methods as strong baselines for dual-target drug design. ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we will present the pipeline of our work, from dataset curation to method. In Section 3.1, we will introduce how we curate the dual-target dataset based on synergistic drug combinations and how we derived the protein-ligand complex structures. In Section 3.2, we will show how we reprogram the pretrained target-specific diffusion models for dual-target drug design and introduce two methods, COMPDIFF and DUALDIFF. In Section 3.3, we will show how we repurpose linker design methods for dual-target drug design. ", "page_idx": 2}, {"type": "image", "img_path": "Y79L45D5ts/tmp/4766ef44a6616494de78d1580a727c2ad0ed78c87fac183692af597c37b2b51f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: Overview of our method for dual-target drug design. (a) Illustration of COMPDIFF and DUALDIFF. We first align two pockets in 3D space with protein-ligand binding prior and build two complex graph with shared ligand nodes. We then compose the SE(3)-equivariant message to derive the drift on output level (COMPDIFF) or at each layer of the equivariant neural network (DUALDIFF). Based on the composed drift, we can generate dual-target ligand molecules by compositional reverse sampling. (b) Illustration of repurposing linker design methods for dual-target drug design. We first identity binding-related fragments from the reference molecules for each of the dual targets and then apply linker design methods to link the fragments and derive a complete molecule that can bind to the dual targets separately. ", "page_idx": 2}, {"type": "text", "text": "3.1 Data Curation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Designing dual-target drugs for random pairs of targets lacks significant intent. However, by taking cues from drug synergy, where two drugs together deliver an impact greater than the sum of their separate effects [48], we carefully select target pairs to ensure the dataset holds practical significance for drug discovery. ", "page_idx": 3}, {"type": "text", "text": "Drug Synergy To collect drug combination pairs, we start from DrugCombDB2 [30]. DrugCombDB is a comprehensive database devoted to the curation of drug combinations from various data sources including high-throughput screening (HTS) assays, manual curations from the literature, FDA Orange Book and external databases. DrugCombDB comprises a total of 448,555 combinations of drugs, encompassing 2,887 unique drugs and 124 human cancer cell lines. Particularly, DrugCombDB has more than 6,000,000 quantitative dose responses, from which we determine whether a drug combination is synergistic or not. Specifically, a drug combination with positive zero interaction potency (ZIP), Bliss, Loewe and the highest single agent (HSA) scores simultaneously in at least one cell line is supposed to be a synergistic one. Please refer to Appendix A for a comprehensive understanding of these scores. ", "page_idx": 3}, {"type": "text", "text": "Drug Information After collecting synergistic drug combinations, we need to collect other necessary information (e.g., SMILES and targets) according to their drug names provided by DrugCombDB. Before this procedure, we collect synonyms and cross-matching ID (e.g., CAS Number and ChEBI ID) mainly from DrugBank [25] and Therapeutic Target Database (TTD) [56]. This step facilitates comprehensive literature reviews, ensuring that all relevant data sources that may use alternate names for a drug is considered. We then collect SMILES or structures (if possible) also mainly from DrugBank and TTD. To identify drug targets, we also utilize DrugBank and TTD as the primary data sources, and supplement these with manual curation from the literature (e.g., [23]). For drugs for which we cannot find either SMILES or targets, we exclude them from our previously collected dataset of positive drug combinations. ", "page_idx": 3}, {"type": "text", "text": "Complex Structures For certain drug-target pairs, we incorporate their complex structures directly into our dataset if they are available in PDBBind [33], a repository of protein-ligand binding structures sourced from the Protein Data Bank (PDB) [2]. For drug-target pairs not present in PDBBind, we initially attempt to retrieve the target structures from PDB; if the structures are unavailable, we then source them from the AlphaFold Protein Structure Database (AlphaFold DB) [49] and exclude those whose confidence scores, referred to as pLDDT which provide an assessment of the structures predicted by AlphaFold 2 [24], are less than 70. For these protein targets with structures from PDB or AlphaFold DB, we first utilize P2Rank [26], a program that precisely predicts ligand-binding pockets from a protein structure, to find the most possible pocket given the target structure, and use AutoDock Vina [10] to obtain the protein-ligand complex structures. For each drug, there may exist more than one targets, in which case we use AutoDock Vina to measure the binding affinity and selected the target with the best binding affinity. Finally, we obtain 12,917 postive drug combinations with protein-ligand complex structures, among which there are 438 unique drugs. The 12,917 pairs of targets can be used to evaluate the ability of methods for dual-target drug design. And the binding ligands can be used for reference molecules. ", "page_idx": 3}, {"type": "text", "text": "3.2 Reprogramming Target-Specific Diffusion Models for Dual-Target Drug Design ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Diffusion models [16, 6, 47, 45] have been introduced to structure-based drug design and achieved promising results [14, 44, 28, 15]. We will first revisit the background of diffusion models for SBDD [14] and then introduce how we apply diffusion models trained on single-target protein-ligand datasets to dual-target drug design in a zero-shot manner. Our method is illustrated in Figure 1 (a). ", "page_idx": 3}, {"type": "text", "text": "Diffusion Models for single-target SBDD In this following, we denote the type of an atom as $\\pmb{v}\\ \\in\\ \\mathbb{R}^{K}$ and the coordinate of an atom as $\\textbf{\\em x}\\in\\mathbb{R}^{3}$ , where $K$ is the number of atom types of our interest. For single-target drug design, given a protein binding site denoted as a set of atoms ", "page_idx": 3}, {"type": "text", "text": "$\\mathcal{P}=\\{(\\pmb{x}_{P}^{(i)},\\pmb{v}_{P}^{(i)})\\}_{i=1}^{N_{P}}$ , where $N_{P}$ is the number of protein atoms, our goal is to generate binding   \ndmeonloetceusl etsh $\\mathcal{M}=\\{(\\pmb{x}_{L}^{(i)},\\pmb{v}_{L}^{(i)})\\}_{i=1}^{N_{M}}$ .r  Faonrd b udleen aost $M=[\\mathbf{x},\\mathbf{v}]$ ,n awthese rien $[\\cdot,\\cdot]$ $\\mathbf{x}\\in\\mathbb{R}^{N_{M}\\times3},\\mathbf{x}\\in\\mathbb{R}^{N_{M}\\times K}$   \nspace and one-hot atom types, respectively. So we can use generative models to model the conditional   \ndistribution $p(M|\\mathcal{P})$ . ", "page_idx": 4}, {"type": "text", "text": "In the forward diffusion process of the diffusion model, noises are gradually injected into the data sample (i.e., small molecule $M_{0}\\sim p(M|\\mathcal{P}))$ and lead to a sequence of latent variable $M_{1},M_{2},\\ldots,M_{T}$ . The final distribution $p(M_{T}|\\mathcal{P})$ , also known as prior distribution, is approximately standard normal distribution for atom positions and uniform distribution for atom types. The reverse generative process learns to recover data distribution from the noise distribution with a neural network parameterized by $\\pmb{\\theta}$ . The forward and reverse processes are both Markov chains defined as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nq(M_{1:T}|M_{0},\\mathcal{P})=\\prod_{t=1}^{T}q(M_{t}|M_{t-1},\\mathcal{P})\\quad\\mathrm{and}\\quad p_{\\theta}(M_{0:T-1}|M_{T},\\mathcal{P})=\\prod_{t=1}^{T}p_{\\theta}(M_{t-1}|M_{t},\\mathcal{P}).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "More specifically, the forward transition kernel in Guan et al. [14] are defined as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nq(M_{t}|M_{t-1},\\mathcal{P})=\\mathcal{N}(\\mathbf{x}_{t};\\sqrt{1-\\beta_{t}}\\mathbf{x}_{t-1},\\beta_{t}I)\\cdot\\mathcal{C}(\\mathbf{v}_{t}|(1-\\beta_{t})\\mathbf{v}_{t-1}+\\beta_{t}/K),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\{\\beta_{t}\\}_{t=1}^{T}$ are fixed noise schedule. The above diffusion process can be efficiently sampled directly from time step 0 to $t$ as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nq(\\mathbf{x}_{t}|\\mathbf{x}_{0})=\\mathcal{N}(\\mathbf{x}_{t};\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{x}_{0},(1-\\bar{\\alpha}_{t})I)\\quad\\mathrm{and}\\quad q(\\mathbf{v}_{t}|\\mathbf{v}_{0})=\\mathcal{C}(\\mathbf{v}_{t}|\\bar{\\alpha}_{t}\\mathbf{v}_{0}+(1-\\bar{\\alpha}_{t}/K),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\alpha_{t}:=1-\\beta_{t}$ and $\\begin{array}{r}{\\bar{\\alpha}_{t}:=\\prod_{s=1}^{t}\\alpha_{s}}\\end{array}$ . The posterior can be easily computed via Bayes theorem as: $q(\\mathbf{x}_{t-1}|\\mathbf{x}_{t},\\mathbf{x}_{0})=\\mathcal{N}(\\mathbf{x}_{t-1};\\tilde{\\mu}_{t}(\\mathbf{x}_{t},\\mathbf{x}_{0}),\\tilde{\\beta}_{t}I)\\quad\\mathrm{and}\\quad q(\\mathbf{v}_{t}|\\mathbf{v}_{0})=\\mathcal{C}(\\mathbf{v}_{t}|\\bar{\\alpha}_{t}\\mathbf{v}_{0}+(1-\\bar{\\alpha}_{t})/K)$ , (4) whe $\\begin{array}{r}{\\mathrm{re}\\;\\tilde{\\beta}_{t}=\\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_{t}}\\beta_{t},\\tilde{\\mu}_{t}({\\bf x}_{t},{\\bf x}_{0})=\\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_{t}}{1-\\bar{\\alpha}_{t}},\\tilde{c}_{t}({\\bf v}_{t},{\\bf v}_{0})=c^{*}/\\sum_{k=1}^{K}c_{k}^{*}\\;\\mathrm{and}\\;c^{*}({\\bf v}_{t},{\\bf v}_{0})=c^{*}/\\sum_{k=1}^{K}c_{k}^{*}\\;\\mathrm{and}\\;c^{*}({\\bf v}_{t},{\\bf v}_{0})=c^{*}/\\sum_{k=1}^{K}c_{k}^{*}\\;\\mathrm{and}\\;c_{k}^{*}({\\bf v}_{t},{\\bf v}_{0}),}\\end{array}$ $c^{*}(\\mathbf{v}_{t},\\mathbf{v}_{0})=[\\alpha_{t}\\mathbf{v}_{t}+$ $(1-\\alpha_{t})/K]\\odot[\\bar{\\alpha}_{t-1}\\mathbf{v}_{0}+(1-\\bar{\\alpha}_{t-1})/K].$ ", "page_idx": 4}, {"type": "text", "text": "Accordingly, the reverse transition kernel are defined as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p_{\\theta}(M_{t-1}|M_{t},\\mathcal{P})=\\mathcal{N}(\\mathbf{x}_{t-1};\\mu_{\\theta}([\\mathbf{x}_{t},\\mathbf{v}_{t}],t,\\mathcal{P}),\\sigma_{t}^{2}I)\\cdot\\mathcal{C}(\\mathbf{v}_{t-1}|\\mathbf{c}_{\\theta}([\\mathbf{x}_{t},\\mathbf{v}_{t}],t,\\mathcal{P})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Guan et al. [14] use SE(3)-equivariant neural networks [43, 13] to parameterize $\\mu_{\\theta}([\\mathbf{x}_{t},\\mathbf{v}_{t}],t,\\mathcal{P})$ and ${\\bf c}_{\\theta}([{\\bf x}_{t},{\\bf v}_{t}],t,\\mathcal{P})$ . More specifically, the $[\\mathbf{x}_{0},\\mathbf{v}_{0}]$ are first predicted using neural network $f_{\\theta}$ , i.e., $[\\hat{\\mathbf{x}}_{0},\\hat{\\mathbf{v}}_{0}]=f_{\\theta}([\\mathbf{x}_{t},\\mathbf{v}_{t}],t,\\mathcal{P})$ and then substitute in the posterior as in Equation (4). At the $l$ -th layer of $f_{\\theta}$ , the hidden embedding $\\mathbf{h}$ and coordinates $\\mathbf{x}$ of each atom are updated alternately as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{h}_{i}^{l+1}=\\mathbf{h}_{i}^{l}+\\displaystyle\\sum_{j\\in\\mathcal{V},i\\neq j}f_{\\pmb{\\theta}_{h}}(\\mathbf{h}_{i}^{l},\\mathbf{h}_{j}^{l},d_{i j}^{l},\\mathbf{e}_{i j}),}\\\\ &{\\mathbf{x}_{i}^{l+1}=\\mathbf{x}_{i}^{l}+\\displaystyle\\sum_{j\\in\\mathcal{V},i\\neq j}(\\mathbf{x}_{i}^{l}-\\mathbf{x}_{j}^{l})f_{\\pmb{\\theta}_{x}}(\\mathbf{h}_{i}^{l+1},\\mathbf{h}_{j}^{l+1},d_{i j}^{l},\\mathbf{e}_{i j})\\cdot\\mathbf{1}_{\\mathrm{ligand}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\nu$ is a $\\mathbf{k}$ -nearest neighbors (knn) graph, $d_{i j}=\\lVert{\\bf x}_{i}-{\\bf x}_{j}\\rVert$ is the Euclidean distance between two atoms $i$ and $j,\\mathbf{e}_{i j}$ is an additional feature that indicates the connection is between protein atoms, ligand atoms or protein atom and ligand atom, and $\\mathbf{1}_{\\mathrm{ligand}}$ is a mask for ligand nodes since only coordinates of ligand atoms are supposed to be updated. ", "page_idx": 4}, {"type": "text", "text": "The diffusion model is trained to minimize the KL-divergence between the ground-truth posterior $q(M_{t-1}|M_{0},M_{t},\\mathcal{P})$ and the estimated posterior $p_{\\pmb{\\theta}}(M_{t-1}|M_{t},\\mathscr{P})$ . After being trained, given a specific pocket, the ligand molecule can be generated by first sampling from prior distribution and sequentially applying the reverse generative process defined above. ", "page_idx": 4}, {"type": "text", "text": "Problem Definition of Dual-Target Drug Design The goal of dual-target drug design is to design a ligand molecule $M$ that can bind to both given pocket $\\mathcal{P}_{1}$ and pocket $\\mathcal{P}_{2}$ . The problem can be also formulated as a generative task which models the conditional distribution $p(M|\\mathcal{P}_{1},\\mathcal{T}\\mathcal{P}_{2})$ . Notebly, we introduce a transformation operator $\\tau$ here. This is because protein pockets exhibit a wide variety of shapes and chemical characteristics and it is necessary to achieve spatial alignment of the dual pockets when modeling the conditional distribution with both of them as conditions. To maintain a neat but siginificant setting, we restrict the transformation $\\tau$ to encompass solely translations $\\mathcal{T}_{T}$ and rotations $\\mathcal{T}_{R}$ , i.e., $\\tau=\\tau_{T}\\circ\\mathcal{T}_{R}$ . To be more precise, $(\\mathcal{T}_{T}\\circ\\mathcal{T}_{R})\\mathcal{P}_{2}=\\{(R\\pmb{x}_{P_{2}}^{(i)}+\\pmb{t}),\\pmb{v}_{P_{2}}^{(i)}\\}_{i=1}^{N_{P_{2}}}$ , where $R\\in\\mathrm{SO}(3)$ represents the rotation and $t\\in\\mathbb{R}^{3}$ represents the translation. ", "page_idx": 4}, {"type": "text", "text": "Aligning Dual Targets with Protein-Ligand Binding Priors Protein pockets can have intricate 3D structures with varying depths, widths, and surface contours, and distinct chemical properties, including differences in surface electron potentials, hydrophobicity, and the distribution of functional groups. The complex nature of pocket characteristics hinder us from directly aligning two pockets. Nevertheless, the binding mode is determined by the complex nature of pocket information, thus the protein-binding priors can effectively summarize the essential information needed for aligning the two pockets. This motivate us to propose to use a ligand molecule as a prober to implicitly reflect the spatial arrangement of the two pockets and then align them with the protein-ligand binding priors. More specifically, we first dock a ligand molecule to pocket $\\mathcal{P}1$ and pocket $\\mathcal{P}2$ separately. We can then compute $\\boldsymbol{R}$ and $\\pmb{t}$ by aligning the two docked poses of the ligand molecule. Experiments have demonstrated that even ligand molecules capable of approximate binding to the two pockets can effectively indicate a specific spatial alignment between them. Further details are provided in Section 4. ", "page_idx": 5}, {"type": "text", "text": "SE(3)-Equivariant Composed Message and Compositional Generative Sampling Inspired by compositional visual generation [7, 32], we can model the dual-target drug design with the following composed distribution: ", "text_level": 1, "page_idx": 5}, {"type": "equation", "text": "$$\np(M|\\mathcal{P}_{1},\\mathcal{T}\\mathcal{P}_{2})\\propto p_{\\pmb\\theta}(M|\\mathcal{P}_{1})p_{\\pmb\\theta}(M|\\mathcal{T}\\mathcal{P}_{2}).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Following Liu et al. [32], for the atom position prediction, we can reparameterize $\\mu_{\\theta}([\\mathbf{x}_{t},\\mathbf{v}_{t}],t,\\mathcal{P})$ with ${\\bf x}_{t}-\\epsilon_{\\theta}([{\\bf x}_{t},{\\bf v}_{t}],t,\\mathcal{P})$ , so that we can rewrite the transition kernel of the reverse generative process as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\np_{\\theta}({\\bf x}_{t-1}|x_{t},{\\mathcal P})=\\mathcal N({\\bf x}_{t-1};\\mu_{\\theta}([{\\bf x}_{t},{\\bf v}_{t}],t,{\\mathcal P}),\\sigma_{t}^{2}I)=\\mathcal N({\\bf x}_{t-1};{\\bf x}_{t}-\\epsilon_{\\theta}([{\\bf x}_{t},{\\bf v}_{t}],t,{\\mathcal P}),\\sigma_{t}^{2}I).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The reversed transition kernel corresponds to a step as follow: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{x}_{t-1}=\\mathbf{x}_{t}-\\epsilon_{\\theta}([\\mathbf{x}_{t},\\mathbf{v}_{t}],t,\\mathcal{P})+\\mathcal{N}(\\mathbf{0},\\sigma_{t}^{2}I).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\epsilon_{\\theta}([\\mathbf{x}_{t},\\mathbf{v}_{t}],t,\\mathcal{P})$ can be viewed as a drift term and $\\mathcal{N}(\\mathbf{0},\\sigma_{t}^{2}I)$ can be viewed as a diffusion term. As Liu et al. [32] points out, this is analogous to the Langevin dynamics [9] that used to sample from Energy-Based Models (EBMs) [8, 46], which can be formulated as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{x}_{t-1}=\\mathbf{x}_{t}-\\frac{\\lambda}{2}\\nabla_{\\mathbf{x}}E_{\\theta}([\\mathbf{x}_{t},\\mathbf{v}_{t}],t,\\mathcal{P})+\\mathcal{N}(\\mathbf{0},\\sigma_{t}^{2}I).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "The sampling procedure produces samples from the probability density $p_{\\pmb{\\theta}}(\\mathbf{x}|\\mathcal{P})\\propto\\exp\\left(-E_{\\pmb{\\theta}}(\\mathbf{x}|\\mathcal{P})\\right)$ where $E_{\\theta}(\\mathbf{x}|\\mathcal{P})$ is a energy function parameterized by model $\\pmb{\\theta}$ . Thus, accordingly, the composed distribution of atom positions can be written as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p(\\mathbf{x}|\\mathcal{P}_{1},\\mathcal{T}\\mathcal{P}_{2})\\propto p_{\\theta}(\\mathbf{x}|\\mathcal{P}_{1})p_{\\theta}(\\mathbf{x}|\\mathcal{T}\\mathcal{P}_{2})\\propto\\exp\\Big(-\\left(E_{\\theta}([\\mathbf{x}_{t},\\mathbf{v}_{t}],t,\\mathcal{P}_{1})+E_{\\theta}([\\mathbf{x}_{t},\\mathbf{v}_{t}],t,\\mathcal{T}\\mathcal{P}_{2})\\right)\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "This composed distribution corresponds to Langevin dynanmics as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathbf{x}_{t-1}=\\mathbf{x}_{t}-\\frac{\\lambda}{2}\\nabla_{\\mathbf{x}}\\Big(-\\big(E_{\\theta}([\\mathbf{x}_{t},\\mathbf{v}_{t}],t,\\mathcal{P}_{1})+E_{\\theta}([\\mathbf{x}_{t},\\mathbf{v}_{t}],t,\\mathcal{T}\\mathcal{P}_{2})\\big)\\Big).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Accordingly, each step in the compositional reverse generative sampling process can be defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{x}_{t-1}=\\mathbf{x}_{t}-\\eta\\Big(\\big(\\epsilon_{\\theta}([\\mathbf{x}_{t},\\mathbf{v}_{t}],t,\\mathcal{P}_{1})+\\epsilon_{\\theta}([\\mathbf{x}_{t},\\mathbf{v}_{t}],t,\\mathcal{P}_{2})\\Big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where we additionally introduce a hyperparameter $\\eta$ here to control the strength of the drift. In theory, this is equivalent to making a more flexible assumption that $p(\\mathbf{x}|\\mathcal{P}_{1},\\mathcal{T}\\mathcal{P}_{2})\\quad\\propto$ $[p_{\\pmb\\theta}(\\mathbf{x}|\\mathcal{P}_{1})p_{\\pmb\\theta}(\\mathbf{x}|\\mathcal{T}\\mathcal{P}_{2})]^{\\eta}$ . In this case the transition kernel is defined as $\\begin{array}{r l}{\\quad p_{\\pmb{\\theta}}(\\mathbf{x}_{t-1}|\\mathbf{x}_{t},\\mathcal{P}_{1},\\mathcal{T}\\mathcal{P}_{2})\\,=}&{{}}\\end{array}$ $[p_{\\pmb\\theta}(\\mathbf x_{t-1}|\\mathbf x_{t},\\mathcal P_{1})p_{\\pmb\\theta}(\\mathbf x_{t-1}|\\mathbf x_{t},\\mathcal T\\mathcal P_{2})]^{\\eta}$ . In practice, we set $\\eta=1/2$ by default. This composition operation is equivalent to averaging two $\\hat{\\mathbf{x}}_{\\mathrm{0}}$ predicted on two complex graphs, i.e. $\\protect\\nu_{1}$ and $\\protect\\nu_{2}$ . Similarly, for the atom types which are discrete variables, we can also compose the transition kernel as follows: p\u03b8(vt\u22121|v $\\bar{\\mathbf{\\xi}}_{t},\\mathcal{P}_{1},\\mathcal{T}\\mathcal{P}_{2})\\propto p_{\\theta}(\\mathbf{v}_{t-1}|\\mathbf{v}_{t},\\mathcal{P}_{1})p_{\\theta}(\\mathbf{v}_{t-1}|\\mathbf{v}_{t},\\mathcal{T}\\mathcal{P}_{2})$ . Note that $p_{\\theta}(\\mathbf{v}_{t-1}|\\mathbf{v}_{t},\\mathcal{P}_{1})$ is categorical distribution and its dimension (i.e., the number of atom types of our interest) is $K$ , which is small in practice. So $p_{\\pmb{\\theta}}(\\mathbf{v}_{t-1}|\\mathbf{v}_{t},\\mathcal{P}_{1},T\\mathcal{P}_{2})$ can be computed analytically. We name the compositional reverse sampling with composed transition kernel (i.e., composed drift) as COMPDIFF. ", "page_idx": 5}, {"type": "text", "text": "We further improve the compositional reverse sampling by introducing the composition into each layer of the equivariant neural network in the pretrained diffusion model. For brevity, we denote the SE(3)-equivariant message at the $l$ -th layer for the $i$ -th atom of the complex graph $\\nu_{v}$ $\\boldsymbol{v}=1,2$ ) as introduced in Equation (7) as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\mathbf{h}_{i}^{l}(\\mathcal{V}_{v}):=\\displaystyle\\sum_{j\\in\\mathcal{V}_{v},i\\neq j}f_{\\pmb{\\theta}_{h}}(\\mathbf{h}_{i}^{l},\\mathbf{h}_{j}^{l},d_{i j}^{l},\\mathbf{e}_{i j}),}\\\\ &{\\Delta\\mathbf{x}_{i}^{l}(\\mathcal{V}_{v}):=\\displaystyle\\sum_{j\\in\\mathcal{V}_{v},i\\neq j}(\\mathbf{x}_{i}^{l}-\\mathbf{x}_{j}^{l})f_{\\pmb{\\theta}_{x}}(\\mathbf{h}_{i}^{l+1},\\mathbf{h}_{j}^{l+1},d_{i j}^{l},\\mathbf{e}_{i j})\\cdot\\mathbf{1}_{\\mathrm{ligand}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The above SE(3)-equivariant message can also be interpreted as drift in 3D and latent space. Thus we can also compose them as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{h}_{i}^{l+1}=\\mathbf{h}_{i}^{l}+\\left(\\Delta\\mathbf{h}_{i}^{l}(\\mathcal{V}_{1})+\\Delta\\mathbf{h}_{i}^{l}(\\mathcal{V}_{2})\\right)/2\\quad\\mathrm{and}\\quad\\mathbf{x}_{i}^{l+1}=\\mathbf{x}_{i}^{l}+(\\Delta\\mathbf{x}_{i}^{l}(\\mathcal{V}_{1})+\\Delta\\mathbf{x}_{i}^{l}(\\mathcal{V}_{2}))/2.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We name the compositional reverse sampling with the above SE(3)-equivariant message at each layer as DUALDIFF. The proof of SE(3)-equivariance can be found in Appendix D. This more meticulous composition is supposed to lead to higher-quality samples. ", "page_idx": 6}, {"type": "text", "text": "3.3 Repurposing Linker Design Methods for Dual-Target Drug Design ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Pharmacophore combination is a prevalent strategy in traditional dual-target drug design, requiring the specialized knowledge of chemists. To automate this procedure, We design a strategy to identity crucial fragments from reference molecules of dual targets in our dataset and apply linker design methods [20, 12] to link the fragments and obtain complete molecules. ", "page_idx": 6}, {"type": "text", "text": "Specifically, we break all rotatable bonds of reference molecule $\\mathcal{M}_{1}$ (resp. $\\mathcal{M}_{2}$ ) of target $\\mathcal{P}_{1}$ (resp. $\\mathcal{P}_{2}$ ) to obtain fragments. Since DiffLinker [20] requires relative positions of fragments as input, we dock all fragments derived from $\\mathcal{M}_{1}$ and $\\mathcal{M}_{2}$ to $\\mathcal{P}_{2}$ (or $\\mathcal{P}_{1}$ ) and select the pair of fragments which has the best sum of binding affinity and no physical conflicts (i.e., the minimum between atoms from the two fragments is large than $1.4\\mathring\\mathrm{A}\\rangle$ . We then apply DiffLinker to link the two fragments, considering the existence of the pocket $\\mathcal{P}_{2}$ (or $\\mathcal{P}_{1}$ ). Since LinkerNet [12] models the translation and rotation of fragments by neural networks and does not require relative position of fragments as input, we directly dock all fragments derived from $\\mathcal{M}_{1}$ (resp. $\\mathcal{M}_{2}$ ) to $\\mathcal{P}_{1}$ (resp. $\\mathcal{P}_{2}$ ) and select their respective fragment with the best binding affinity. And we then link them using LinkerNet to obtain complete molecules. ", "page_idx": 6}, {"type": "text", "text": "4 Experiment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Dataset We use our dataset introduced in Section 3.1. All 12,917 pairs of targets (including 438 unique targets) are used for evaluation. For each target, there is an associated reference molecule that can be considered a benchmark for high-quality ligand molecules and utilized in linker design methods. ", "page_idx": 6}, {"type": "text", "text": "Baselines We compare method with various baselines: Pocket2Mol [37] generates 3D molecules atom by atom in an autoregressive manner given a specific protein binding site. TargetDiff [14] is a diffusion-based method which generates atom coordinates and atom types in a non-autoregressive way. Note that Pocket2Mol and TargetDiff are both proposed for structure-based single-target drug design. DiffLinker [20] is a diffusion-based model for linker design with given fragment poses. LinkerNet [12] is a diffusion-based model for co-designing molecular fragment poses and the linker. DiffLinker and LinkerNet are repurposed for dual-target drug design as we introduced in Section 3.3. ", "page_idx": 6}, {"type": "text", "text": "Evaluation We evaluate generated ligand molecules from the perspectives of target binding affinity and molecular properties. We employ AutoDock Vina [10] to estimate the target binding affinity, following Peng et al. [37], Guan et al. [14]. We first evaluate Pocket2Mol and TargetDiff under the single-target setting as a preliminary verification (see Appendix B). For dual-target drug design, we use each method to design 10 molecules for each pair of targets, denoted as $\\mathcal{P}_{1}$ and $\\mathcal{P}_{2}$ . (For reference ", "page_idx": 6}, {"type": "text", "text": "Table 1: Summary of different properties of reference molecules and molecules generated by baselines and our methods under the dual-target setting. (\u2191) / (\u2193) denotes a larger / smaller number is better. Top 2 results are highlighted with bold text and underlined text, respectively. ", "page_idx": 7}, {"type": "table", "img_path": "Y79L45D5ts/tmp/dfb9fdf3adb79af6da28708c31b26009143105b52b963a8553179a02a71d187b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "molecule, Pocet2Mol and TargetDiff, the ligand molecule is generated for $\\mathcal{P}_{1}$ but the target binding affinity is evaluated on both $\\mathcal{P}_{1}$ and $\\mathcal{P}_{2}$ .) We then collect all generated molecules across 12,917 pairs of targets and report the mean and median (denoted as \u201cAvg.\u201d and \u201cMed.\u201d respectively) of affinity-related metrics (P-1 Vina Dock, P-2 Vina Dock, Max Vina Dock, and Dual High Affinity) and property-related metrics (drug-likeness QED [3], synthesizability SA [11], and diversity). Vina Dock incorporates a re-docking step to assess the highest binding affinity achievable. Here we introduce P-1 Vina Dock and P-2 Vina Dock to represent the Vina Dock score evaluated on $\\mathcal{P}_{1}$ and $\\mathcal{P}_{2}$ , respectively. Besides, we introduce Max Vina Dock, which represents the maximum Vina Dock of a given molecule towards $\\mathcal{P}_{1}$ and $\\mathcal{P}_{2}$ . The Vina Dock will be low if and only if the molecule can bind to both targets simultaneously, which is the goal of dual-target drug design. Additionally, we report Dual High Affinity (abbreviated as Dual High Aff.) which represents the proportion of generated molecules that exhibit binding affinity that exceeds that of the reference molecules on both the respective targets. This reflects the success rate in achieving higher binding affinities simultaneously on both targets in dual-target drug design. We also evaluate the RMSD between docked poses towards dual targets. ", "page_idx": 7}, {"type": "text", "text": "4.2 Main Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We compare all methods under the dual-target setting. The results are reported in Table 1. Our methods, COMPDIFF and DUALDIFF, significantly outperforms other methods, especially in terms of binding affinity. Notably, DUALDIFF achieves the highest Dual High Affinity among all methods. In line with our expectations, for the single-target drug design methods (e.g., Pocket2Mol and TargetDiff), we observe a significant decline in performance according to P-2 Vina Dock compared to P-1 Vina Dock, which shows their inability in dual-target drug design. LinkerNet also achieves promising results except diversity. Note that DiffLinker and LinkerNet are provided with reference molecules while COMPDIFF and DUALDIFF are not. This indicates the strong generative abilities of our methods. Finally, DUALDIFF outperforms COMPDIFF, which shows that composition of SE(3)-equivariant message at each layer is more effective than only at the output level. ", "page_idx": 7}, {"type": "image", "img_path": "Y79L45D5ts/tmp/126be22f6b68338cc40b5ef11600765df6e512e80543cdafa2d743acb7d7c6e5.jpg", "img_caption": ["Figure 2: RMSD between docked poses towards dual targets of different methods. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "As shown in Figure 2, DUALDIFF performs better than LinkerNet the RMSD between docked poses towards dual targets. This indicates that the molecules generated by DUALDIFF can bind to dual targets with smaller conformation change. Additionally, we provide visualization of examples of generated molecules in Figure 3. See more examples in Appendix C. ", "page_idx": 7}, {"type": "text", "text": "4.3 Ablation Studies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Effects of Alignment of Dual Targets We perform different methods to align the pockets of dual targets for COMPDIFF and DUALDIFF. See the results in Table 2. Naively, we can align two pockets by their geometric centers (denoted as \u201c-Center\u201d). In our method, we propose to align pockets with protein-ligand binding priors. We select the ligand with minimum RMSD between docked poses (resp. minimum sum of Vina Dock scores) towards dual targets as the anchor to align the dual targets, which is denoted as \u201c-RMSD\u201d (resp. \u201c-Score\u201d). DUALDIFF-Score achieved the best performance among all variants, demonstrating the effectiveness of the alignment method. ", "page_idx": 7}, {"type": "image", "img_path": "Y79L45D5ts/tmp/ac3100a8aac4956b4aab640f63ee1696c148bd2e4cd245f92f85ed7698f1fd19.jpg", "img_caption": ["Figure 3: Reference molecules and examples of ligand molecules by different methods generated for the dual targets (UniProt ID: P18507 (top) and Q9UBS5 (bottom)). "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "Y79L45D5ts/tmp/2e27e4b2eb500b2afa1ffb11e4442700f9ae2fb416d9b1dbadb22fdc87141a6d.jpg", "table_caption": ["Table 2: Ablation on different ways of aligning dual targets. "], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "Y79L45D5ts/tmp/509ac715bc7aa61dba6bbe5abf4302e415880c21551642165f1be573f6c2bf26.jpg", "table_caption": ["Table 3: Ablation on different strategies of identifying fragments for linker design methods. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Different Strategies of Identifying Fragments for Linker Design Methods We conduct ablation on different strategies of identifying fragments for DiffLinker and LinkerNet for dual-target drug design. Since we use Vina Dock to select fragments, we have tried different box sizes for docking, i.e., $\\bar{5}\\mathring{\\mathrm{A}}$ and $8\\mathring\\mathrm{A}$ . For DiffLinker, since the relative poses of fragments are required as input, we dock fragments derived from $\\mathcal{M}_{1}$ and $\\mathcal{M}_{2}$ to target $\\mathcal{P}_{1}$ (or $\\mathcal{P}_{2}$ ). We then apply DiffLinker both with and without considering the pocket. The corresponding methods are denoted as \u201cDiffLinker- $5/-8/$ -pocket$5/.$ -pocket- $.8^{\\circ}$ . For LinkerNet, the relative poses of fragments are not required. So we try two setting: dock fragments from $\\mathcal{M}_{1}$ and $\\mathcal{M}_{2}$ to target $\\mathcal{P}_{1}$ (or $\\mathcal{P}_{2}$ ); dock fragments from $\\mathcal{M}_{1}$ (resp. $\\mathcal{M}_{2.}$ ) to target $\\mathcal{P}_{1}$ (resp. $\\mathcal{P}_{2}$ ). The difference is that all fragments are docked to the same pocket in the former setting while the fragments from two ligand molecules are docked to their respective pockets. The corresponding methods are denoted as \u201cLinkerNet-5/-8/-self-5/-self- $.8^{\\circ}$ . The results are reported in Table 3. The results show that a larger docking box size allows for better selection of fragments. As we expected, DiffLinker with consideration of pockets achieves better performance. Among all variants, \u201cLinkerNet-self-8\u201d achieves best performance, which implies that adjusting relative poses of fragments may play a crutial role in linker deisgn. This feature of LinkerNet allows for selecting the most important fragments for each pockets of the dual targets. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this work, we introduced a novel dataset for dual-target drug design. We formulate this problem as a generative task and propose compositional reverse sampling to reprogram pretrained target-specific model for dual-target drug design, which can successfully generate dual-target ligands and outperform all baselines, including repurposed linker design methods. Our research lays the groundwork for dual-target drug design using generative methods, with future progress expected. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported by the National Science and Technology Major Project (2023ZD0120901) and the National Natural Science Foundation of China No. 62377030. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Amy C Anderson. 2003. The process of structure-based drug design. Chemistry & biology, 10(9):787\u2013797.   \n[2] Helen Berman, Kim Henrick, and Haruki Nakamura. 2003. Announcing the worldwide protein data bank. Nature structural & molecular biology, 10(12):980\u2013980.   \n[3] G Richard Bickerton, Gaia V Paolini, J\u00e9r\u00e9my Besnard, Sorel Muresan, and Andrew L Hopkins. 2012. Quantifying the chemical beauty of drugs. Nature chemistry, 4(2):90\u201398.   \n[4] Maria Laura Bolognesi and Andrea Cavalli. 2016. Multitarget drug discovery and polypharmacology.   \n[5] Di Chen, Xi Liu, Yiping Yang, Hongjun Yang, and Peng Lu. 2015. Systematic synergy modeling: understanding drug synergy from a systems biology perspective. BMC Systems Biology.   \n[6] Prafulla Dhariwal and Alexander Nichol. 2021. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780\u20138794.   \n[7] Yilun Du, Shuang Li, and Igor Mordatch. 2020. Compositional visual generation with energy based models. Advances in Neural Information Processing Systems, 33:6637\u20136647.   \n[8] Yilun Du, Shuang Li, Joshua Tenenbaum, and Igor Mordatch. 2020. Improved contrastive divergence training of energy based models. arXiv preprint arXiv:2012.01316.   \n[9] Yilun Du and Igor Mordatch. 2019. Implicit generation and modeling with energy based models. Advances in Neural Information Processing Systems, 32.   \n[10] Jerome Eberhardt, Diogo Santos-Martins, Andreas F Tillack, and Stefano Forli. 2021. AutoDock Vina 1.2. 0: New docking methods, expanded force field, and python bindings. Journal of chemical information and modeling, 61(8):3891\u20133898.   \n[11] Peter Ertl and Ansgar Schuffenhauer. 2009. Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. Journal of cheminformatics, 1(1):1\u201311.   \n[12] Jiaqi Guan, Xingang Peng, PeiQi Jiang, Yunan Luo, Jian Peng, and Jianzhu Ma. 2023. LinkerNet: Fragment Poses and Linker Co-Design with 3D Equivariant Diffusion. In Thirty-seventh Conference on Neural Information Processing Systems.   \n[13] Jiaqi Guan, Wesley Wei Qian, Wei-Ying Ma, Jianzhu Ma, and Jian Peng. 2021. Energy-inspired molecular conformation optimization. In international conference on learning representations.   \n[14] Jiaqi Guan, Wesley Wei Qian, Xingang Peng, Yufeng Su, Jian Peng, and Jianzhu Ma. 2023. 3D Equivariant Diffusion for Target-Aware Molecule Generation and Affinity Prediction. In International Conference on Learning Representations.   \n[15] Jiaqi Guan, Xiangxin Zhou, Yuwei Yang, Yu Bao, Jian Peng, Jianzhu Ma, Qiang Liu, Liang Wang, and Quanquan Gu. 2023. DecompDiff: Diffusion Models with Decomposed Priors for Structure-Based Drug Design. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 11827\u201311846. PMLR.   \n[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851.   \n[17] Yinan Huang, Xingang Peng, Jianzhu Ma, and Muhan Zhang. 2022. 3DLinker: an E (3) equivariant variational autoencoder for molecular linker design. arXiv preprint arXiv:2205.07309.   \n[18] Zhilin Huang, Ling Yang, Xiangxin Zhou, Zhilong Zhang, Wentao Zhang, Xiawu Zheng, Jie Chen, Yu Wang, CUI Bin, and Wenming Yang. 2023. Protein-ligand interaction prior for binding-aware 3d molecule diffusion models. In The Twelfth International Conference on Learning Representations.   \n[19] Bliss Chester I. 1939. The toxicity of poisons applied jointly 1. Annals of applied biology, 26(3):585\u2013615.   \n[20] Ilia Igashov, Hannes St\u00e4rk, Cl\u00e9ment Vignac, Victor Garcia Satorras, Pascal Frossard, Max Welling, Michael Bronstein, and Bruno Correia. 2022. Equivariant 3d-conditional diffusion models for molecular linker design. arXiv preprint arXiv:2210.05274.   \n[21] Fergus Imrie, Anthony R Bradley, Mihaela van der Schaar, and Charlotte M Deane. 2020. Deep generative models for 3D linker design. Journal of chemical information and modeling, 60(4):1983\u20131995.   \n[22] Fergus Imrie, Thomas E Hadfield, Anthony R Bradley, and Charlotte M Deane. 2021. Deep generative design with 3D pharmacophoric constraints. Chemical science, 12(43):14577\u201314589.   \n[23] Francesco Iorio, Theo A Knijnenburg, Daniel J Vis, Graham R Bignell, Michael P Menden, Michael Schubert, Nanne Aben, Emanuel Gon\u00e7alves, Syd Barthorpe, Howard Lightfoot, et al. 2016. A landscape of pharmacogenomic interactions in cancer. Cell, 166(3):740\u2013754.   \n[24] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin \u017d\u00eddek, Anna Potapenko, et al. 2021. Highly accurate protein structure prediction with AlphaFold. Nature, 596(7873):583\u2013589.   \n[25] Craig Knox, Mike Wilson, Christen M Klinger, Mark Franklin, Eponine Oler, Alex Wilson, Allison Pon, Jordan Cox, Na Eun Chin, Seth A Strawbridge, et al. 2024. Drugbank 6.0: the drugbank knowledgebase for 2024. Nucleic Acids Research, 52(D1):D1265\u2013D1275.   \n[26] Radoslav Kriv\u00e1k and David Hoksza. 2018. P2Rank: machine learning based tool for rapid and accurate prediction of ligand binding sites from protein structure. Journal of cheminformatics, 10:1\u201312.   \n[27] Ying Hong Li, Pan Pan Wang, Xiao Xu Li, Chun Yan Yu, Hong Yang, Jin Zhou, Wei Wei Xue, Jun Tan, and Feng Zhu. 2016. The human kinome targeted by FDA approved multi-target drugs and combination products: A comparative study from the drug-target interaction network perspective. PloS one, 11(11):e0165737.   \n[28] Haitao Lin, Yufei Huang, Meng Liu, Xuanjing Li, Shuiwang Ji, and Stan Z Li. 2022. Diffbp: Generative diffusion of 3d molecules for target protein binding. arXiv preprint arXiv:2211.11214.   \n[29] Hui-Heng Lin, Le-Le Zhang, Ru Yan, Jin-Jian Lu, and Yuanjia Hu. 2017. Network analysis of drug\u2013target interactions: a study on FDA-approved new molecular entities between 2000 to 2015. Scientific reports, 7(1):12230.   \n[30] Hui Liu, Wenhao Zhang, Bo Zou, Jinxian Wang, Yuanyuan Deng, and Lei Deng. 2020. DrugCombDB: a comprehensive database of drug combinations toward the discovery of combinatorial therapy. Nucleic acids research, 48(D1):D871\u2013D881.   \n[31] Meng Liu, Youzhi Luo, Kanji Uchino, Koji Maruhashi, and Shuiwang Ji. 2022. Generating 3d molecules for target protein binding. arXiv preprint arXiv:2204.09410.   \n[32] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. 2022. Compositional visual generation with composable diffusion models. In European Conference on Computer Vision, pages 423\u2013439. Springer.   \n[33] Zhihai Liu, Minyi Su, Li Han, Jie Liu, Qifan Yang, Yan Li, and Renxiao Wang. 2017. Forging the basis for developing protein\u2013ligand interaction scoring functions. Accounts of chemical research, 50(2):302\u2013309.   \n[34] Georgina V Long, Daniil Stroyakovskiy, Helen Gogas, Evgeny Levchenko, Filippo de Braud, James Larkin, Claus Garbe, Thomas Jouary, Axel Hauschild, Jean Jacques Grob, et al. 2014. Combined BRAF and MEK inhibition versus BRAF inhibition alone in melanoma. New England Journal of Medicine, 371(20):1877\u20131888.   \n[35] Shitong Luo, Jiaqi Guan, Jianzhu Ma, and Jian Peng. 2021. A 3D generative model for structurebased drug design. Advances in Neural Information Processing Systems, 34:6229\u20136239.   \n[36] Reza Bayat Mokhtari, Tina S Homayouni, Narges Baluch, Evgeniya Morgatskaya, Sushil Kumar, Bikul Das, and Herman Yeger. 2017. Combination therapy in combating cancer. Oncotarget, 8(23):38022.   \n[37] Xingang Peng, Shitong Luo, Jiaqi Guan, Qi Xie, Jian Peng, and Jianzhu Ma. 2022. Pocket2mol: Efficient molecular sampling based on 3d protein pockets. In International Conference on Machine Learning, pages 17644\u201317655. PMLR.   \n[38] Alexander S Powers, Helen H Yu, Patricia Suriana, Rohan V Koodli, Tianyu Lu, Joseph M Paggi, and Ron O Dror. 2023. Geometric deep learning for structure-based ligand design. ACS Central Science, 9(12):2257\u20132267.   \n[39] Matthew Ragoza, Tomohide Masuda, and David Ryan Koes. 2022. Generating 3D molecules conditional on receptor binding sites with deep generative models. Chemical science, 13(9):2701\u20132713.   \n[40] Rona R Ramsay, Marija R Popovic-Nikolic, Katarina Nikolic, Elisa Uliassi, and Maria Laura Bolognesi. 2018. A perspective on multi-target drug discovery and design for complex diseases. Clinical and translational medicine, 7:1\u201314.   \n[41] Caroline Robert, Boguslawa Karaszewska, Jacob Schachter, Piotr Rutkowski, Andrzej Mackiewicz, Daniil Stroiakovski, Michael Lichinitser, Reinhard Dummer, Florent Grange, Laurent Mortier, et al. 2015. Improved overall survival in melanoma with combined dabrafenib and trametinib. New England Journal of Medicine, 372(1):30\u201339.   \n[42] Loewe S. 1953. The problem of synergism and antagonism of combined drugs. Arzneimittelforschung, 3(6):285\u201390.   \n[43] V\u0131ctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. 2021. E (n) equivariant graph neural networks. In International conference on machine learning, pages 9323\u20139332. PMLR.   \n[44] Arne Schneuing, Yuanqi Du, Charles Harris, Arian Jamasb, Ilia Igashov, Weitao Du, Tom Blundell, Pietro Li\u00f3, Carla Gomes, Max Welling, Michael Bronstein, and Bruno Correia. 2022. Structure-based Drug Design with Equivariant Diffusion Models. arXiv preprint arXiv:2210.13695.   \n[45] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. 2015. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 2256\u20132265. PMLR.   \n[46] Yang Song and Diederik P Kingma. 2021. How to train your energy-based models. arXiv preprint arXiv:2101.03288.   \n[47] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. 2020. Score-Based Generative Modeling through Stochastic Differential Equations. In International Conference on Learning Representations.   \n[48] Ronald J Tallarida. 2001. Drug synergism: its detection and applications. Journal of Pharmacology and Experimental Therapeutics, 298(3):865\u2013872.   \n[49] Mihaly Varadi, Stephen Anyango, Mandar Deshpande, Sreenath Nair, Cindy Natassia, Galabina Yordanova, David Yuan, Oana Stroe, Gemma Wood, Agata Laydon, et al. 2022. AlphaFold Protein Structure Database: massively expanding the structural coverage of protein-sequence space with high-accuracy models. Nucleic acids research, 50(D1):D439\u2013D444.   \n[50] Bhagwan Yadav, Krister Wennerberg, Tero Aittokallio, and Jing Tang. 2015. Searching for Drug Synergy in Complex Dose\u2013Response Landscapes Using an Interaction Potency Model. BMC Systems Biology.   \n[51] Jing Ye, Junhao Wu, and Bo Liu. 2023. Therapeutic strategies of dual-target small molecules to overcome drug resistance in cancer therapy. Biochimica et Biophysica Acta (BBA)-Reviews on Cancer, 1878(3):188866.   \n[52] Xiangxiang Zeng, Fei Wang, Yuan Luo, Seung-gu Kang, Jian Tang, Felice C Lightstone, Evandro F Fang, Wendy Cornell, Ruth Nussinov, and Feixiong Cheng. 2022. Deep generative molecular design reshapes drug discovery. Cell Reports Medicine, 3(12).   \n[53] Zaixi Zhang and Qi Liu. 2023. Learning Subpocket Prototypes for Generalizable Structure-based Drug Design. arXiv preprint arXiv:2305.13997.   \n[54] Zaixi Zhang, Yaosen Min, Shuxin Zheng, and Qi Liu. 2022. Molecule generation for target protein binding with structural motifs. In The Eleventh International Conference on Learning Representations.   \n[55] Xiangxin Zhou, Xiwei Cheng, Yuwei Yang, Yu Bao, Liang Wang, and Quanquan Gu. 2024. DecompOpt: Controllable and Decomposed Diffusion Models for Structure-based Molecular Optimization. In The Twelfth International Conference on Learning Representations.   \n[56] Ying Zhou, Yintao Zhang, Donghai Zhao, Xinyuan Yu, Xinyi Shen, Yuan Zhou, Shanshan Wang, Yunqing Qiu, Yuzong Chen, and Feng Zhu. 2024. TTD: Therapeutic Target Database describing target druggability information. Nucleic acids research, 52(D1):D1465\u2013D1477. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Term Definitions in Drug Synergy ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Drug Synergy Drug synergy refers to the phenomenon where two or more drugs used in combination, produce a more therapeutic effect than the sum of their individual effects. When drugs with distinct binding targets are strategically paired, they can leverage each other\u2019s strengths and compensate for respective weaknesses, which enables lower individual drug doses, thereby reducing the risk of adverse effects. This characteristic is usually used in cancer and HIV treatments, and has been proven to be a new but promising way to combat complex diseases [5]. ", "page_idx": 14}, {"type": "text", "text": "Zero Interaction Potency (ZIP) Yadav et al. [50] proposed ZIP score to describe the drug interaction by comparing the alteration in the potency of dose-response curves in the context of single drug administration versus their concurrent use in combinations. In the two-drug combination scenario, we will refer to drug A and drug B, respectively. The effects of these drugs are defined as $E_{A B}$ for combination, $E_{A}$ and $E_{B}$ for individual situations $[0\\leq E\\leq1]$ ). The ZIP score can be defined as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\nS_{\\mathrm{ZIP}}=\\bar{E}_{\\mathrm{AB}}-(\\bar{E}_{\\mathrm{A}}+\\bar{E}_{\\mathrm{B}}-\\bar{E}_{\\mathrm{A}}\\cdot\\bar{E}_{\\mathrm{B}}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The $\\bar{E}_{\\mathrm{AB}}$ in Equation (17) is the average response values obtained by fitting dose-response curves independently in each dimension of the measured combinatorial data sub-tensor, the explanation for the other variables are the same. ", "page_idx": 14}, {"type": "text", "text": "Bliss The Bliss independence model [19] assumes a stochastic process where the two drugs elicit their effects independently. In this model, the expected combination effect can be calculated based on the probability of the independent events occurring: ", "page_idx": 14}, {"type": "equation", "text": "$$\nS_{\\mathrm{Bliss}}=E_{A B}-(1-(1-E_{A})(1-E_{B})).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Equation (18) is similar to Equation (17), the difference between which is that Equation (17) employs fitted drug responses instead of observed ones. ", "page_idx": 14}, {"type": "text", "text": "Loewe The Loewe score [42] forecasts the dose combination that will produce a specific effect, it calculates the expected response as if both drugs are the same. Assume drug A can produce effect $E_{\\mathrm{A}}$ at dose $x_{\\mathrm{A}}$ , and drug B can produce effect $E_{\\mathrm{B}}$ at dose $x_{\\mathrm{B}}$ , then the loewe affinity states that the expected effect $E_{\\mathrm{Loewe}}$ can be determined by: ", "page_idx": 14}, {"type": "equation", "text": "$$\n{\\frac{x_{\\mathrm{A}}}{X_{\\mathrm{A}}}}+{\\frac{x_{\\mathrm{B}}}{X_{\\mathrm{B}}}}=1,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $X_{\\mathrm{A}}$ and $X_{\\mathrm{B}}$ are the doses drug A or $\\mathbf{B}$ alone that produces effect $E_{\\mathrm{Loewe}}$ . The Loewe score is then defined as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\nS_{\\mathrm{Loewe}}=E_{\\mathrm{AB}}-E_{\\mathrm{Loewe}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Highest Single Agent (HSA) The HSA is a straightforward scoring system for estimating drug synergy, which determines the incremental effect of combining drugs by comparing the enhanced combined effect to their individual effects [50]. The HSA score can be calculated as follows: ", "page_idx": 14}, {"type": "equation", "text": "$$\nS_{\\mathrm{HSA}}=E_{\\mathrm{AB}}-\\operatorname*{max}(E_{\\mathrm{A}},E_{\\mathrm{B}})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The ZIP score is the most prevalently utilized metric in the assessment of drug synergy. Furthermore, these scores can be collectively analyzed to identify optimal drug combinations for targeted therapy. ", "page_idx": 14}, {"type": "text", "text": "B Preliminary Verification on Single-Target Setting ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We evaluate Pocket2Mol and TargetDiff under the setting of single-target drug design on 438 unique targets in our dataset as a preliminary verification. We use each method generates 10 molecules for each target and collect all generated molecules across 438 proteins and report the mean, trimmed mean (i.e., averaging that removes $10\\%$ of the largest and smallest values before calculating the mean) and median (denoted as \u201cAvg.\u201d, \u201cT-Avg.\u201d and \u201cMed.\u201d respectively) of affinity-related metrics (Vina Score, Vina Min, Vina Dock, and High Affinity) and property-related metrics (drug-likeness QED [3], synthesizability SA [11], and diversity). Vina Score assesses binding affinity directly based on the generated 3D molecules. Vina Min carries out a energy minimization over the local structure before estimation. Vina Dock incorporates a re-docking step to assess the highest binding affinity achievable. Meanwhile, High Affinity evaluates the proportion of generated molecules that have stronger binding affinity than the reference molecule for each protein tested. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "The results are shown in Table 4, where molecules generated by TargetDiff exhibits binding affinities that are either comparable to or slightly greater than those of the reference molecules. Pocket2Mol achieves strong performance in terms of QED, SA, and diversity but fails in Vina-related metrics. Therefore, TargetDiff can be regarded as an effective molecular generative model on our dataset. ", "page_idx": 15}, {"type": "text", "text": "Table 4: Summary of different properties of reference molecules and molecules generated by baselines under the single-target setting. (\u2191) / (\u2193) denotes a larger / smaller number is better. Considering outliers significantly affect the mean, we ignore some abnormal values of Vina Score and Vina Min. ", "page_idx": 15}, {"type": "table", "img_path": "Y79L45D5ts/tmp/f6dafe25b1156fab99e9b006b4c59a07c63c3fef6365affbcdc849a576b45302.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "C Visualization of More Examples ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Here we provide visualization of more examples as shown in Figure 4. ", "page_idx": 15}, {"type": "image", "img_path": "Y79L45D5ts/tmp/604c0cf7fb55ce49e997c80173c77e5a5b897142e690e2178b9e7a59712ba54b.jpg", "img_caption": ["Figure 4: Visualization of more reference molecules and examples designed by TargetDiff, LinkerNet and DUALDIFF. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "D Proof of SE(3)-Equivariance ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We denote the global SE(3) transformation as $T_{g}$ , and which means the transformation as $T_{g}(\\mathbf{x}_{i})=$ $R_{g}{\\bf x}_{i}+b$ , where $\\pmb{R}_{g}\\in\\mathbb{R}^{3\\times3}$ is the rotation matrix and $b\\in\\mathbb{R}^{3}$ is the translation vector. In line with Section 3.2, we define the composed message passing as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Delta\\mathbf{h}_{i}^{l}(\\mathcal{V}_{v}):=\\displaystyle\\sum_{j\\in\\mathcal{V}_{v},\\,i\\neq j}f_{\\pmb{\\theta}_{h}}(\\mathbf{h}_{i}^{l},\\mathbf{h}_{j}^{l},d_{i j}^{l},\\mathbf{e}_{i j})}\\\\ &{\\Delta\\mathbf{x}_{i}^{l}(\\mathcal{V}_{v}):=\\displaystyle\\sum_{j\\in\\mathcal{V}_{v},\\,i\\neq j}(\\mathbf{x}_{i}^{l}-\\mathbf{x}_{j}^{l})f_{\\pmb{\\theta}_{x}}(\\mathbf{h}_{i}^{l+1},\\mathbf{h}_{j}^{l+1},d_{i j}^{l},\\mathbf{e}_{i j})\\cdot\\mathbf{1}_{\\mathrm{ligand}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{h}_{i}^{l+1}=\\mathbf{h}_{i}^{l}+\\frac{1}{2}\\big(\\Delta\\mathbf{h}_{i}^{l}(\\mathcal{V}_{1})+\\Delta\\mathbf{h}_{i}^{l}(\\mathcal{V}_{2})\\big)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{x}_{i}^{l+1}:=\\phi(\\mathbf{x}_{i}^{l})=\\mathbf{x}_{i}^{l}+\\frac{1}{2}(\\Delta\\mathbf{x}_{i}^{l}(\\mathcal{V}_{1})+\\Delta\\mathbf{x}_{i}^{l}(\\mathcal{V}_{2}))\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "It is easy to see that the atomic distance $d_{i j}^{l}\\,=\\,\\lVert{\\bf x}_{i}\\,-\\,{\\bf x}_{j}\\rVert$ and ${\\bf{e}}_{i j}$ feature are both invariant to SE(3) transformation The hidden embedding $\\mathbf{h}_{i}^{l}$ is also invariant since the its updates (as shown in Equation (22) and Equation (24)) are only related to invariant features. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\Delta T_{g}\\big(\\mathbf x_{i}^{l}(\\mathcal V_{v})\\big):=}&{\\displaystyle\\sum_{j\\in\\mathcal V_{v,i}\\neq j}\\big(T_{g}(\\mathbf x_{i}^{l})-T_{g}(\\mathbf x_{i}^{l})\\big)f_{\\theta_{x}}\\big(\\mathbf h_{i}^{l+1},\\mathbf h_{j}^{l+1},d_{i j}^{l},\\mathbf e_{i j}\\big)\\cdot\\mathbf1_{\\mathrm{ligand}}}\\\\ &{=\\displaystyle\\sum_{j\\in\\mathcal V_{v,i}\\neq j}\\big(R_{g}\\mathbf x_{i}^{l}+b-R_{g}\\mathbf x_{j}^{l}-b\\big)f_{\\theta_{x}}\\big(\\mathbf h_{i}^{l+1},\\mathbf h_{j}^{l+1},d_{i j}^{l},\\mathbf e_{i j}\\big)\\cdot\\mathbf1_{\\mathrm{ligand}}}\\\\ &{=\\displaystyle\\sum_{j\\in\\mathcal V_{v,i}\\neq j}R_{g}\\big(\\mathbf x_{i}^{l}-\\mathbf x_{j}^{l}\\big)f_{\\theta_{x}}\\big(\\mathbf h_{i}^{l+1},\\mathbf h_{j}^{l+1},d_{i j}^{l},\\mathbf e_{i j}\\big)\\cdot\\mathbf1_{\\mathrm{ligand}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "After applying $T_{g}$ to $\\mathbf{x}_{i}^{l}$ , the updated position $\\mathbf{x}_{i}^{l+1}=\\phi(\\mathbf{x}_{i}^{l})$ can be written as (using the above results): ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phi(T_{g}(\\mathbf{x}_{i}^{\\parallel}))=T_{g}(\\mathbf{x}_{i}^{\\parallel})+\\frac{1}{2}(\\Delta T_{g}(\\mathbf{x}_{i}^{\\parallel}(\\mathbf{\\hat{y}}_{1}))+\\Delta T_{g}(\\mathbf{x}_{i}^{\\parallel}(\\mathbf{\\hat{y}}_{2}))}\\\\ &{=R_{g}\\mathbf{x}_{i}^{\\parallel}+b+\\frac{1}{2}\\Bigg(\\sum_{v_{1}\\in\\Omega_{i}}\\sum_{j\\in\\mathcal{V}_{i}\\ne j}R_{g}(\\mathbf{x}_{i}^{\\parallel}-\\mathbf{x}_{j}^{\\parallel})f_{g,\\alpha}(\\mathbf{\\hat{h}}_{i}^{\\perp+1},\\mathbf{h}_{j}^{L+1},d_{i j}^{L},\\mathbf{e}_{i j})\\cdot\\mathbf{1}_{\\mathbf{\\hat{y}}_{2\\perp}}\\Bigg)}\\\\ &{=R_{g}\\mathbf{x}_{i}^{\\parallel}+\\frac{1}{2}R_{g}\\Bigg(\\sum_{v_{1}\\in\\Omega_{i}}\\sum_{j\\in\\mathcal{V}_{i}\\ne j}\\left(\\mathbf{x}_{i}^{l}-\\mathbf{x}_{j}^{l}\\right)f_{\\alpha}(\\mathbf{h}_{i}^{\\perp+1},\\mathbf{h}_{j}^{L+1},d_{i j}^{L},\\mathbf{e}_{i j})\\cdot\\mathbf{1}_{\\mathbf{\\hat{y}}_{2\\perp}}\\Bigg)+b}\\\\ &{=R_{g}\\mathbf{\\hat{x}}_{i}^{l}+\\frac{1}{2}R_{g}(\\Delta\\mathbf{x}_{i}^{l}(\\mathbf{\\hat{y}}_{1})+\\Delta\\mathbf{x}_{i}^{l}(\\mathbf{\\hat{y}}_{2}))+b}\\\\ &{=R_{g}\\left(\\mathbf{x}_{i}^{l}+\\frac{1}{2}\\left(\\Delta\\mathbf{x}_{i}^{l}(\\mathbf{\\hat{y}}_{1})+\\Delta\\mathbf{x}_{i}^{l}(\\mathbf{\\hat{y}}_{2})\\right)\\right)+b}\\\\ &{=T_{g}\\left(\\mathbf{x}_{i}^{l}+\\frac{1}{2}\\left(\\Delta\\mathbf{x}_{i}^{l}(\\mathbf{\\hat{y}}_{1})+\\Delta\\mathbf{x}_{i}^{l}(\\mathbf{\\hat{y}}_{2})\\right)\\right)}\\\\ &{=T_{g}\\left(\\mathbf{\\hat{x}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The above equation shows the SE(3)-equivariance of the atom position update formula Equation (25). Based on the fact that Equation (24) is SE(3)-invariant and Equation (25) is SE(3)-equivariant, we can say that the composition operation of our method is SE(3)-equivariant. ", "page_idx": 16}, {"type": "text", "text": "E Discussion, Limitation, and Future Work ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Our work provides a novel dataset and a general framework for dual-target drug design to the community. And our method can be easily adapted to the multi-target scenario. Our work is the first step towards generative dual-target drug design. There are still limitations in our work. For example, we do not consider flexibility of proteins in our work, which is a more practical setting, though this is also a issue for most works in SBDD. We will leave it as a future work. ", "page_idx": 16}, {"type": "text", "text": "F Societal Impacts ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Our research holds the promise of significantly advancing the pharmaceutical industry by aiding in the development of potent dual-target drugs. This could potentially streamline the path to new treatments, making efficient drug discovery a more attainable goal. Moreover, emphasizing the ethical implementation of our methods is of paramount importance. It is also needed to ensure that these scientific achievements are utilized for social good, safeguarding against any misuse that could lead to negative consequences for society. ", "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction precisely reflect the paper\u2019s contributions and scope. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: Limitations are discussed in Appendix E. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The paper provide the full set of assumptions and a complete and correct proof. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper fully discloses all the information needed to reproduce the main experimental results of the paper. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [No] ", "page_idx": 20}, {"type": "text", "text": "Justification: We require authorization to release the code. Once we obtain the necessary approval, we will proceed with the code release. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper has provided the procedures of processing the dataset and the chosen hyperparameters. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: For RMSD between docked poses towards dual targets, the paper has reported error bars. The main results are reported in mean and average over a large test dataset. The experimental results are stable and significant. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [No] ", "page_idx": 21}, {"type": "text", "text": "Justification: The experiments are done on several clusters. The overall accurate computer resources are not well recorded. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 21}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The broader impacts are discussed in Appendix F. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 21}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The creators or original owners of assets (e.g., code, data, models), used in the paper, are properly credited. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}]