[{"figure_path": "wcX04Wn34u/tables/tables_2_1.jpg", "caption": "Table 1: Vanilla zero-shot capacity. Without LiDAR translation, the performance of the 3D object detection model drops dramatically when applied to different domains. LiT is capable of translating LiDAR data across domains and can significantly improve the performance of the 3D object detection model. APBEV and AP3D of the car category at IoU = 0.7 of the SECOND-IoU [50] model are shown.", "description": "This table presents the results of a zero-shot experiment, comparing the performance of a 3D object detection model on three different LiDAR datasets (Waymo, nuScenes, and KITTI) with and without the LiDAR Translator (LiT).  The results demonstrate a significant performance drop when using a model trained on one dataset and directly applied to another (without translation).  In contrast, the LiT method substantially improves the performance across domains.  The metrics used are Average Precision for Bird's Eye View (APBEV) and Average Precision for 3D (AP3D), both at an Intersection over Union (IoU) threshold of 0.7.  The SECOND-IoU model is used for the experiment.", "section": "Pilot study"}, {"figure_path": "wcX04Wn34u/tables/tables_7_1.jpg", "caption": "Table 2: Statistical alignment with target domain. We report the distributional differences between translated and ground-truth target domains with Maximum-Mean Discrepancy (MMD) and Jensen-Shannon divergence (JSD). Baseline represents LiDAR data from the source domain.", "description": "This table presents a quantitative evaluation of how well LiT aligns the statistical distribution of translated LiDAR data with the ground truth target domain.  It compares the Maximum Mean Discrepancy (MMD) and Jensen-Shannon Divergence (JSD) for three different LiDAR data translation tasks. The baseline represents the distributional differences using the original source domain LiDAR data without any translation.  The lower values for LiT in both MMD and JSD demonstrate that LiT effectively translates the LiDAR data to match the target domain characteristics.", "section": "5.2 Experimental results"}, {"figure_path": "wcX04Wn34u/tables/tables_7_2.jpg", "caption": "Table 3: Single source domain unification. We compare the APBEV and AP3D of the car category at IoU = 0.7 as well as the domain gap closed by different methods. Source only denotes that the pre-trained detector is directly evaluated on the target domain, and Oracle represents the detection results trained on the fully annotated target domain. We highlight the best results in bold.", "description": "This table presents a comparison of the performance of different methods for single-source domain adaptation in 3D object detection.  It compares the Average Precision (AP) in Bird's Eye View (BEV) and 3D for the \"car\" category, using an Intersection over Union (IoU) threshold of 0.7. The methods compared include a baseline of using the source-only trained model on the target domain, SN [36], ST3D [7], ReSimAD [51], and the proposed LiDAR Translator (LiT).  An \"Oracle\" result is also given, representing the performance achievable with a model fully trained on the target domain's annotated data. The table highlights the best performing methods for each task, indicating the effectiveness of each approach in bridging domain gaps and improving object detection performance. The \"Closed gap\" column shows the percentage increase in performance compared to the source only method.", "section": "5.2 Experimental results"}, {"figure_path": "wcX04Wn34u/tables/tables_8_1.jpg", "caption": "Table 3: Single source domain unification. We compare the APBEV and AP3D of the car category at IoU = 0.7 as well as the domain gap closed by different methods. Source only denotes that the pre-trained detector is directly evaluated on the target domain, and Oracle represents the detection results trained on the fully annotated target domain. We highlight the best results in bold.", "description": "This table compares the performance of different methods for single-source domain adaptation in 3D object detection.  It shows the Average Precision (AP) for Bird's Eye View (BEV) and 3D object detection, along with the percentage improvement achieved by each method compared to a baseline (Source Only) and the performance of a model trained on fully annotated data (Oracle).  The results demonstrate the effectiveness of the LiDAR Translator (LiT) method in bridging domain gaps across different LiDAR datasets.", "section": "5.2 Experimental results"}, {"figure_path": "wcX04Wn34u/tables/tables_8_2.jpg", "caption": "Table 5: Ablation studies. We report the performance of LiT with nuScenes \u2192 KITTI translation tasks with SECOND-IoU [50] model to study the effects of different configurations in LiT.", "description": "This table presents the ablation study of LiDAR translator (LiT) on the nuScenes to KITTI translation task using the SECOND-IoU model.  It systematically investigates the impact of different components and choices within the LiT framework on the final performance. The rows show various configurations, including: using only source data without adaptation, variations in foreground modeling (diversity and noise levels), background noise, and the full LiT model.  The columns indicate the Average Precision (AP) for Bird's Eye View (BEV) and 3D object detection.", "section": "5.5 Ablation studies"}, {"figure_path": "wcX04Wn34u/tables/tables_9_1.jpg", "caption": "Table 6: Full-scene LiDAR translation in under 1 minute. We present averaged runtime and key statistics for the LiT LiDAR translation pipeline. Here, a \u201cscene\u201d is a LiDAR sequence containing multiple LiDAR point cloud \u201cframes\u201d. Runtime is measured on a single NVIDIA RTX 4090 GPU.", "description": "This table presents the runtime statistics and key metrics of the LiT LiDAR translation pipeline for three different domain translation tasks (Waymo \u2192 KITTI, Waymo \u2192 nuScenes, and nuScenes \u2192 KITTI).  It breaks down the time spent on different stages of the pipeline (background modeling, foreground modeling, and ray casting) and provides metrics like the number of frames, points, vertices, and rays involved in each process. The table shows LiT's efficiency in translating a full multi-frame LiDAR scene (around 200 frames) in under a minute.", "section": "5 Experiment"}, {"figure_path": "wcX04Wn34u/tables/tables_15_1.jpg", "caption": "Table 7: Parameters for scene modeling. The left table shows the parameters for background modeling. The right table shows the parameters for foreground modeling.", "description": "This table lists the hyperparameters used in the scene modeling part of the LiDAR translator.  It shows separate parameters for the background and foreground modeling components, including settings relevant to voxel size, normal estimation, solver iterations, and convergence tolerance, for both Waymo and nuScenes datasets. These parameters control how accurately the scene is reconstructed.  The difference in parameters between Waymo and nuScenes reflects the different characteristics of the datasets.", "section": "4 LiDAR translator"}, {"figure_path": "wcX04Wn34u/tables/tables_15_2.jpg", "caption": "Table 2: Statistical alignment with target domain. We report the distributional differences between translated and ground-truth target domains with Maximum-Mean Discrepancy (MMD) and Jensen-Shannon divergence (JSD). Baseline represents LiDAR data from the source domain.", "description": "This table presents a quantitative evaluation of LiDAR Translator's (LiT) ability to statistically align translated LiDAR data with the ground truth target domain. It uses two metrics: Maximum Mean Discrepancy (MMD) and Jensen-Shannon Divergence (JSD), computed in the Bird's Eye View (BEV) voxel occupancy. The comparison is made between LiT's translated data and the actual target domain LiDAR data for three different scenarios: Waymo to KITTI, Waymo to nuScenes, and nuScenes to KITTI.  Lower MMD and JSD values indicate better alignment.", "section": "5.2 Experimental results"}, {"figure_path": "wcX04Wn34u/tables/tables_16_1.jpg", "caption": "Table 8: Training hyperparameters for domain adaptation tasks. This summary presents the hyperparameters for training detection models under single-source domain unification settings. The table includes configurations for Second-IOU and PV-RCNN models.", "description": "This table lists the hyperparameters used for training object detection models in single-source domain adaptation experiments using the LiDAR Translator (LiT).  It covers two different models (Second-IOU and PV-RCNN) across three different domain adaptation scenarios: Waymo to KITTI, Waymo to nuScenes, and nuScenes to KITTI.  For each scenario and model, the table specifies the optimizer, scheduler, learning rate, momentum, weight decay, batch size, and number of training epochs.", "section": "5.2 Experimental results"}]