[{"figure_path": "KqbLzSIXkm/figures/figures_4_1.jpg", "caption": "Figure 1: Overview of DiMSUM architecture.", "description": "The figure provides a detailed overview of the DiMSUM architecture, illustrating the dynamic interactions between different sequential states. It showcases the input tokens, conditioning, and the core components of the DiMSUM block, including the Spatial-Frequency Mamba, Cross-Attention fusion layer, and the globally shared transformer blocks. The figure visually represents the flow of information through the network, highlighting the integration of spatial and frequency features, as well as the use of a selective mechanism to enhance the dynamic interactions of different sequential states.", "section": "3 Method"}, {"figure_path": "KqbLzSIXkm/figures/figures_5_1.jpg", "caption": "Figure 2: Illustration of Wavelet Mamba (Best view in color). For illustration purpose, we plot wavelet representations of an input image but our real process is performed on encoded features of the input. Giving an image of size (8, 8), for example, it is first decomposed to four wavelet subbands of size (4, 4) where each is further transformed to 2nd-level subbands of size (2, 2). Green dots indicate pixel points within each wavelet subband and a window of size 2 \u00d7 2 is used to perform scanning across multiple wavelet subbands like the CNN kernel.", "description": "This figure illustrates the Wavelet Mamba architecture, a key component of the DiMSUM model.  It shows how an input image is processed through multiple levels of wavelet decomposition, resulting in wavelet subbands representing different frequency components. These subbands are then processed using a windowed scanning approach within each subband, which combines spatial and frequency information. The figure visually demonstrates the process, highlighting the decomposition into wavelet subbands and the subsequent window-based scanning.", "section": "3.3 DiM block"}, {"figure_path": "KqbLzSIXkm/figures/figures_5_2.jpg", "caption": "Figure 2: Illustration of Wavelet Mamba (Best view in color). For illustration purpose, we plot wavelet representations of an input image but our real process is performed on encoded features of the input. Giving an image of size (8, 8), for example, it is first decomposed to four wavelet subbands of size (4, 4) where each is further transformed to 2nd-level subbands of size (2, 2). Green dots indicate pixel points within each wavelet subband and a window of size 2 \u00d7 2 is used to perform scanning across multiple wavelet subbands like the CNN kernel.", "description": "This figure illustrates the Wavelet Mamba, a key component of the DiMSUM architecture. It shows how an input image is decomposed into wavelet subbands using a two-level Haar wavelet transform.  The image is then processed by the Wavelet Mamba module, which uses a sliding window across wavelet subbands to capture spatial and frequency information. The green dots highlight the scanning path, showing how the window moves across the different wavelet levels and subbands to capture local and long-range dependencies.", "section": "3.3 DiM block"}, {"figure_path": "KqbLzSIXkm/figures/figures_7_1.jpg", "caption": "Figure 4: Result of our model versus others upon CelebA-HQ. \u2020 is our reproduced result based on Zigma's official code, and \u2021 is an adopted result from LFM paper.", "description": "This figure presents a comparison of the proposed DiMSUM model's performance against other state-of-the-art models on the CelebA-HQ dataset.  The comparison includes quantitative metrics (NFE, FID, Recall, Epochs) and qualitative results (sample images).  The results demonstrate DiMSUM's superior performance in terms of both FID and recall, while also showcasing faster training convergence compared to the other models.", "section": "4 Experiments"}, {"figure_path": "KqbLzSIXkm/figures/figures_7_2.jpg", "caption": "Figure 6: Training curve of DiT after training longer on CelebA, supporting Fig. 4d in the manuscript.", "description": "This figure shows the training curve of the DiT model on the CelebA dataset when trained for a longer duration, providing further support to the findings presented in Figure 4d.  It specifically illustrates the model's convergence speed and stability over a larger number of training epochs, highlighting the comparison between DiT and the proposed DiMSUM model in terms of training efficiency.", "section": "4 Experiments"}, {"figure_path": "KqbLzSIXkm/figures/figures_7_3.jpg", "caption": "Figure 4: Result of our model versus others upon CelebA-HQ. \u2020 is our reproduced result based on Zigma's official code, and \u2021 is an adopted result from LFM paper.", "description": "This figure presents a comparison of the proposed DiMSUM model with other state-of-the-art models on the CelebA-HQ dataset.  It shows quantitative results (FID and Recall scores) and qualitative results (sample images generated by each model). The quantitative results demonstrate the superior performance of DiMSUM compared to the other methods. The qualitative results provide a visual comparison of image quality and diversity.", "section": "4 Experiments"}, {"figure_path": "KqbLzSIXkm/figures/figures_17_1.jpg", "caption": "Figure 6: Training curve of DiT after training longer on CelebA, supporting Fig. 4d in the manuscript.", "description": "This figure shows the training curve of the DiT model trained on the CelebA dataset for a longer duration than shown in Figure 4d.  It demonstrates that DiT's performance initially improves and then plateaus or even slightly degrades after a certain point, highlighting the faster convergence achieved by the DiMSUM model proposed in the paper.", "section": "4 Experiments"}, {"figure_path": "KqbLzSIXkm/figures/figures_18_1.jpg", "caption": "Figure 7: FID-10K of varying NFE on CelebA 256.", "description": "This figure shows the FID-10K scores obtained using different numbers of function evaluations (NFEs) on the CelebA-HQ 256 dataset.  Two different sampling methods, Heun and Euler, were used.  The plot illustrates that increasing NFEs beyond a certain point (around 250 in this case) yields minimal improvement in FID-10K scores, demonstrating the efficiency of the flow-matching approach used in the DiMSUM model.", "section": "E More qualitative examples"}, {"figure_path": "KqbLzSIXkm/figures/figures_19_1.jpg", "caption": "Figure 2: Illustration of Wavelet Mamba (Best view in color). For illustration purpose, we plot wavelet representations of an input image but our real process is performed on encoded features of the input. Giving an image of size (8, 8), for example, it is first decomposed to four wavelet subbands of size (4, 4) where each is further transformed to 2nd-level subbands of size (2, 2). Green dots indicate pixel points within each wavelet subband and a window of size 2 \u00d7 2 is used to perform scanning across multiple wavelet subbands like the CNN kernel.", "description": "This figure illustrates the Wavelet Mamba module, a key component of the DiMSUM architecture.  It shows how an input image is decomposed into wavelet subbands at multiple levels (here, two levels are shown). Each subband is then processed using a windowed scanning method, similar to a convolutional kernel, to capture both local and global features from the frequency domain. This process is different from the traditional spatial scanning methods used in other Mamba models and it is designed to better capture both local and global information within the image.", "section": "3.3 DiM block"}, {"figure_path": "KqbLzSIXkm/figures/figures_19_2.jpg", "caption": "Figure 9: Illustration of Sweep scanning orders.", "description": "This figure illustrates different scanning orders used in the Wavelet Mamba component of the DiMSUM model.  It shows how the input image's features are processed in various patterns, including bidirectional, Sweep-4, and Sweep-8 methods. Each method involves scanning the image horizontally and vertically, and in reverse directions.  The different scanning orders aim to capture diverse local and global relationships between features.  Understanding these different scanning methods is crucial to grasp how DiMSUM leverages spatial information at multiple scales and how effectively it fuses this with the frequency data obtained from the wavelet transform.", "section": "3.3 DiM block"}, {"figure_path": "KqbLzSIXkm/figures/figures_20_1.jpg", "caption": "Figure 10: Uncurated generated samples of CelebA-HQ 256.", "description": "This figure displays 48 samples of images generated by the DiMSUM model. The images are 256x256 pixels and depict faces.  The samples are uncurated, meaning they were not selected or filtered in any way to show only the best results but rather to give a representative sample of the model's output.  This helps to demonstrate the model's ability to generate diverse and realistic-looking faces.", "section": "More qualitative examples"}, {"figure_path": "KqbLzSIXkm/figures/figures_21_1.jpg", "caption": "Figure 1: Overview of DiMSUM architecture.", "description": "The figure provides a comprehensive overview of the DiMSUM architecture, illustrating the interconnected components and their workflow. It details how spatial and frequency features from the input image are processed through wavelet transforms and multiple Mamba blocks. These features are then fused using a cross-attention fusion layer, combining spatial and frequency information. A globally-shared transformer block is incorporated to capture global relationships, enhancing the overall image generation quality. The figure clearly shows the sequence of steps, from input processing to output generation, showcasing the innovative integration of Mamba, wavelet transforms, and transformers.", "section": "3 Method"}, {"figure_path": "KqbLzSIXkm/figures/figures_22_1.jpg", "caption": "Figure 12: Uncurated generated samples of CelebA-HQ 512.", "description": "This figure displays twelve examples of high-resolution (512x512 pixels) facial images generated by the DiMSUM model.  These images are presented as a demonstration of the model's ability to generate high-quality and diverse facial images. The diversity is apparent in the different hairstyles, ages, ethnicities, and expressions among the generated faces. The high quality is evident in the fine details, such as individual strands of hair and subtle textural variations in skin tone.", "section": "More qualitative examples"}, {"figure_path": "KqbLzSIXkm/figures/figures_23_1.jpg", "caption": "Figure 2: Illustration of Wavelet Mamba (Best view in color). For illustration purpose, we plot wavelet representations of an input image but our real process is performed on encoded features of the input. Giving an image of size (8, 8), for example, it is first decomposed to four wavelet subbands of size (4, 4) where each is further transformed to 2nd-level subbands of size (2, 2). Green dots indicate pixel points within each wavelet subband and a window of size 2 \u00d7 2 is used to perform scanning across multiple wavelet subbands like the CNN kernel.", "description": "This figure illustrates the Wavelet Mamba module, a key component of the DiMSUM architecture. It shows how an input image is decomposed into wavelet subbands at multiple levels (in this example, 2 levels are shown).  Each subband is then processed using a windowed scanning approach, which is analogous to using a convolutional kernel. The green dots represent pixels, and the 2x2 windows show the way the network extracts features from the wavelet subbands by scanning.", "section": "3.3 DiM block"}, {"figure_path": "KqbLzSIXkm/figures/figures_23_2.jpg", "caption": "Figure 1: Overview of DiMSUM architecture.", "description": "This figure provides a detailed overview of the DiMSUM architecture. It illustrates the different components of the model, including the state space model (SSM), wavelet transform, cross-attention fusion layer, and globally shared transformer blocks. The figure also shows how these components interact with each other to generate high-quality images.  The flow of image data through the network is clearly depicted, showing how spatial and frequency information are integrated to improve image generation. It provides a comprehensive visual summary of the method's key aspects.", "section": "3 Method"}, {"figure_path": "KqbLzSIXkm/figures/figures_24_1.jpg", "caption": "Figure 2: Illustration of Wavelet Mamba (Best view in color). For illustration purpose, we plot wavelet representations of an input image but our real process is performed on encoded features of the input. Giving an image of size (8, 8), for example, it is first decomposed to four wavelet subbands of size (4, 4) where each is further transformed to 2nd-level subbands of size (2, 2). Green dots indicate pixel points within each wavelet subband and a window of size 2 \u00d7 2 is used to perform scanning across multiple wavelet subbands like the CNN kernel.", "description": "This figure illustrates the Wavelet Mamba, a core component of the DiMSUM architecture. It shows how an input image is decomposed into wavelet subbands at multiple levels, representing different frequency components. The process mimics a CNN kernel, scanning across subbands to extract features.", "section": "3.3 DiM block"}, {"figure_path": "KqbLzSIXkm/figures/figures_24_2.jpg", "caption": "Figure 1: Overview of DiMSUM architecture.", "description": "The figure shows the architecture of DiMSUM, a novel architecture for diffusion models that integrates spatial and frequency information using wavelet transforms and a cross-attention fusion layer. The architecture consists of multiple DiMSUM blocks, each containing DiM blocks that employ a novel Spatial-Frequency Mamba fusion technique, and globally shared transformer blocks for global context integration. The DiMSUM block receives input tokens and conditions, and processes them using the wavelet transform and Mamba blocks to produce spatial and frequency features. These features are fused using a cross-attention fusion layer, and the resulting features are passed through a globally shared transformer block before being decoded to output tokens.", "section": "3 Method"}, {"figure_path": "KqbLzSIXkm/figures/figures_25_1.jpg", "caption": "Figure 1: Overview of DiMSUM architecture.", "description": "This figure provides a high-level overview of the DiMSUM architecture, showing the main components and their interactions.  It illustrates the flow of data through the model, highlighting the use of Spatial-frequency Mamba, cross-attention fusion layer, and globally shared transformer blocks. The diagram visually represents the integration of spatial and frequency information to enhance image generation.", "section": "3 Method"}, {"figure_path": "KqbLzSIXkm/figures/figures_25_2.jpg", "caption": "Figure 1: Overview of DiMSUM architecture.", "description": "The figure provides a detailed overview of the DiMSUM architecture, illustrating the dynamic interactions between different sequential states and the fusion of spatial and frequency features through cross-attention mechanisms. It shows the input processing, wavelet transformation, spatial and frequency Mamba blocks, cross-attention fusion, and global transformer integration.  The figure highlights the key components of the DiMSUM model, including the state-space model, wavelet transform, and cross-attention, and how they work together to generate high-quality images.", "section": "3 Method"}, {"figure_path": "KqbLzSIXkm/figures/figures_26_1.jpg", "caption": "Figure 1: Overview of DiMSUM architecture.", "description": "The figure shows the overall architecture of DiMSUM, a novel architecture for diffusion models that integrates spatial and frequency information. It highlights the key components, including the Spatial-Frequency Mamba, Cross-Attention fusion layer, and globally shared transformer block.  The diagram illustrates the flow of information through the network, from the input image to the final generated image, showcasing the interplay between spatial and frequency features. ", "section": "3 Method"}, {"figure_path": "KqbLzSIXkm/figures/figures_26_2.jpg", "caption": "Figure 2: Illustration of Wavelet Mamba (Best view in color). For illustration purpose, we plot wavelet representations of an input image but our real process is performed on encoded features of the input. Giving an image of size (8, 8), for example, it is first decomposed to four wavelet subbands of size (4, 4) where each is further transformed to 2nd-level subbands of size (2, 2). Green dots indicate pixel points within each wavelet subband and a window of size 2 \u00d7 2 is used to perform scanning across multiple wavelet subbands like the CNN kernel.", "description": "This figure illustrates the Wavelet Mamba method used in the DiMSUM model.  It demonstrates how an input image (8x8 pixels) is decomposed into wavelet subbands (4x4, then 2x2). A scanning window moves across these subbands, processing the wavelet features.  This contrasts with the standard spatial Mamba method, enhancing local structure awareness and capturing frequency information.", "section": "3.3 DiM block"}]