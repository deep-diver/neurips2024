[{"figure_path": "KqbLzSIXkm/tables/tables_7_1.jpg", "caption": "Table 4: Result of our model versus others upon CelebA-HQ. \u2020 is our reproduced result based on Zigma's official code, and \u2021 is an adopted result from LFM paper.", "description": "This table compares the performance of the proposed DiMSUM model against other state-of-the-art image generation models on the CelebA-HQ dataset.  The metrics used for comparison are FID (Fr\u00e9chet Inception Distance), Recall, and the number of forward diffusion steps (NFE). The table also includes the number of training epochs required for each model.  The results demonstrate the superior performance of DiMSUM, achieving lower FID scores and higher recall rates with fewer NFEs and training epochs.  Note that some results for baseline models are reproduced from the original papers.", "section": "4 Experiments"}, {"figure_path": "KqbLzSIXkm/tables/tables_7_2.jpg", "caption": "Table 4: Result of our model versus others upon CelebA-HQ. \u2020 is our reproduced result based on Zigma's official code, and \u2021 is an adopted result from LFM paper.", "description": "This table compares the performance of the proposed DiMSUM model against several other state-of-the-art models on the CelebA-HQ dataset.  The metrics used for comparison are FID (Fr\u00e9chet Inception Distance), Recall, and the number of function evaluations (NFEs). The table shows that DiMSUM achieves superior performance compared to the other methods in terms of FID and Recall, while also requiring fewer function evaluations. The table also includes epochs used for training to show the comparison in training efficiency.", "section": "4 Experiments"}, {"figure_path": "KqbLzSIXkm/tables/tables_7_3.jpg", "caption": "Figure 5: Result of our model versus others upon LSUN Church 256 \u00d7 256 dataset.", "description": "This table presents a comparison of the proposed DiMSUM model's performance against several other image generation models on the LSUN Church dataset, specifically focusing on the Fr\u00e9chet Inception Distance (FID) and Recall metrics.  The number of network function evaluations (NFE) required and the training epochs are also included for a complete comparison.", "section": "4.1 Image Generation"}, {"figure_path": "KqbLzSIXkm/tables/tables_7_4.jpg", "caption": "Table 1: Class-conditional image generation on ImageNet 256 \u00d7 256 dataset.", "description": "This table presents a comparison of different models' performance on class-conditional image generation on the ImageNet 256x256 dataset.  Metrics include FID (Fr\u00e9chet Inception Distance), Recall, the number of parameters, the number of iterations multiplied by batch size, and the number of epochs.  Models are categorized into SSM-based, UNet-based, Transformer-based, and GAN models to facilitate comparison of different architectural approaches.", "section": "4.1 Image Generation"}, {"figure_path": "KqbLzSIXkm/tables/tables_8_1.jpg", "caption": "Table 2: Ablation studies on CelebA-HQ 256 \u00d7 256 dataset at epoch 250.", "description": "This table presents the ablation study results performed on the CelebA-HQ dataset at epoch 250 with image size of 256 x 256.  It shows the impact of different components on the FID and Recall metrics, allowing for a detailed analysis of the contribution of each component (Conditional Mamba, Wavelet Mamba, Cross-Attention Fusion Layer, Shared Transformer Block) to the overall performance.  The table also compares different scanning orders (Bi, Sweep-4, Sweep-8, Zigzag-8, Jpeg-8) and fusion layer types (Linear, Attention, CAFL (swap q), CAFL (swap k)) to determine their effectiveness. The results highlight the importance of the proposed Spatial-Frequency Mamba architecture and the cross-attention fusion layer.", "section": "4 Experiments"}, {"figure_path": "KqbLzSIXkm/tables/tables_8_2.jpg", "caption": "Table 2: Ablation studies on CelebA-HQ 256 \u00d7 256 dataset at epoch 250.", "description": "This ablation study analyzes the impact of different scanning orders (Bi, Sweep-4, Sweep-8, Zigzag-8, Jpeg-8, Window) on the performance of the Conditional Mamba model and the Spatial-frequency Mamba model.  The table compares the FID score (Fr\u00e9chet Inception Distance, lower is better), Recall (higher is better), and iterations per second (iters/s, higher is better) for each scanning order. The results show that Sweep-4 is the best performing order for Conditional Mamba, while Sweep-4 combined with Window scanning provides the best performance for the Spatial-frequency Mamba architecture.", "section": "4 Experiments"}, {"figure_path": "KqbLzSIXkm/tables/tables_8_3.jpg", "caption": "Table 2: Ablation studies on CelebA-HQ 256 \u00d7 256 dataset at epoch 250.", "description": "This ablation study analyzes the impact of different components of the DiMSUM model on its performance, specifically focusing on the CelebA-HQ dataset.  The results are evaluated at epoch 250, showing the FID and Recall scores for different model configurations.  The configurations tested include using only Conditional Mamba, adding Wavelet Mamba with different fusion methods (concatenation versus cross-attention), incorporating a shared transformer block, and using different scanning orders in spatial and frequency domains. The analysis aims to determine the relative contributions of each component and to identify the optimal architecture.", "section": "4.3 Ablation of network design"}, {"figure_path": "KqbLzSIXkm/tables/tables_8_4.jpg", "caption": "Table 2: Ablation studies on CelebA-HQ 256 \u00d7 256 dataset at epoch 250.", "description": "This ablation study on the CelebA-HQ dataset compares the performance of different frequency transformation methods (DCT, EinFFT, and Wavelet) used within the DiMSUM model.  The results are evaluated based on FID (Fr\u00e9chet Inception Distance), Recall, the number of parameters used, and GFLOPs (billion floating-point operations per second).  The table helps to determine the optimal frequency transformation technique for DiMSUM.", "section": "4 Experiments"}, {"figure_path": "KqbLzSIXkm/tables/tables_8_5.jpg", "caption": "Table 2: Ablation studies on CelebA-HQ 256 \u00d7 256 dataset at epoch 250.", "description": "This ablation study on the CelebA-HQ dataset (256x256 resolution) at epoch 250 analyzes the impact of different components on model performance.  It compares the FID and Recall scores for different model variations:  'Conditional Mamba Only' (with and without a globally shared transformer layer) and 'Spatial-Frequency Mamba' (with independent and globally shared transformer layers).  The results showcase the improvement achieved by incorporating the Spatial-Frequency Mamba and the globally shared transformer block.", "section": "4 Experiments"}, {"figure_path": "KqbLzSIXkm/tables/tables_15_1.jpg", "caption": "Table 3: Scaling DiMSUM's parameters on LSUN Church.", "description": "This table demonstrates the scalability of the DiMSUM model by showing the FID scores, training epochs, and model parameters for different model sizes (DIMSUM-L/2 and DIMSUM-XL/2) on the LSUN Church dataset.  It also includes results from baseline models (DIFFUSSM, StyleGAN, and StyleGAN2) for comparison. The results highlight the model's ability to achieve state-of-the-art performance with a reasonable number of parameters and training epochs.", "section": "4 Experiments"}, {"figure_path": "KqbLzSIXkm/tables/tables_16_1.jpg", "caption": "Table 4: Speed and GFLOPs comparison. Single-sample generation was performed, and all tests were conducted on an NVIDIA A100 40GB GPU.", "description": "This table compares the speed (time) and computational cost (GFLOPs) of the proposed DiMSUM model and the baseline DiT model for image generation tasks.  The comparison is done for two different image resolutions (256x256 and 512x512 pixels), showing the model's performance and scalability at different scales. Memory usage (MEM) and model parameters (Params) are also listed. The results reveal the efficiency of DiMSUM, particularly its ability to maintain reasonable speed even when generating higher-resolution images.", "section": "C Speed Analysis"}, {"figure_path": "KqbLzSIXkm/tables/tables_17_1.jpg", "caption": "Table 5: Additional ablation of scanning orders involves using window scanning for wavelet blocks and examining the sensitivity of DiMSUM to different scanning orders for spatial blocks. For simplicity, no shared-transformer block is used.", "description": "This table presents an ablation study on the effect of different scanning orders on the DiMSUM model's performance. It compares the model's FID scores when using various scanning orders (Bi, Jpeg, sweep-8, zigma-8, and sweep-4) for both spatial and frequency components.  The results show that the combination of window scanning for wavelet blocks and sweep-4 for spatial blocks (freq + spatial) achieves the best FID score (4.92), indicating the effectiveness of integrating frequency information with a specific spatial scanning order for improved model performance. The experiment was conducted without using the shared transformer block to better isolate the effect of scanning strategies.", "section": "4.3 Ablation of network design"}, {"figure_path": "KqbLzSIXkm/tables/tables_18_1.jpg", "caption": "Table 6: Hyper-parameters and network config of our DiMSUM network.", "description": "This table shows the hyperparameters and network configuration details used for training the DiMSUM model on three different datasets: CelebA-HQ 256 & 512, LSUN Church, and ImageNet.  The hyperparameters include learning rate, beta1 and beta2 parameters for Adam optimizer, batch size, droppath rate, maximum gradient norm, label dropout rate, and the number of epochs trained for each dataset. The network configuration details include the depth of the network, hidden size, patch size, whether learnable absolute positional embedding is used, the attention layer's interval (every k layers), the number of GPUs used for training, and the total training time in days. The different configurations for each dataset are specified to account for the varying dataset sizes and complexities.", "section": "A Training details"}]