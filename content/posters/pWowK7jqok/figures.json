[{"figure_path": "pWowK7jqok/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration that the exceptional temporal resolution afforded by event cameras, alongside their distinctive event-driven sensing paradigm, presents a significant opportunity for advancing the precision in predicting future motion trajectories.", "description": "This figure shows a comparison of motion prediction using event cameras and traditional RGB cameras.  The x-axis represents time, and the y-axis represents the state of motion.  The blue line represents the ground truth motion, the brown dashed line shows the motion prediction from RGB data, and the dark blue dashed line shows the motion prediction from event data. The circles along the lines represent the sampling points in time.  Event cameras capture motion with exceptional temporal granularity, which is highlighted by the figure, demonstrating the potential of event-based vision for accurate prediction of future motion.", "section": "1 Introduction"}, {"figure_path": "pWowK7jqok/figures/figures_3_1.jpg", "caption": "Figure 2: Inference workflow of the proposed method, where the left upper one indicates the random Gaussian noise, left lower one represents the prompted event sequence. We perform 7 steps forward diffusion processing on the event prompt and substitute a portion of the diffusion input noise, followed by T \u2212 \u03c4 Steps of conventional denoising.", "description": "This figure illustrates the inference workflow of the proposed Event-Sequence Diffusion Network.  It shows how the model processes an event sequence (high temporal resolution event prompt) and combines it with a pre-trained video diffusion model to predict future motion. The process involves adding Gaussian noise to the input, then using a U-Net to perform denoising steps, incorporating the event sequence information at a specific step (\u03c4). The final output is a prediction of future motion, which can then be used for downstream tasks such as segmentation, flow estimation, and tracking.", "section": "4 Methods"}, {"figure_path": "pWowK7jqok/figures/figures_5_1.jpg", "caption": "Figure 3: Qualitative comparison between SOTA methods. The first row of each sequence represents the ground truth of the event sequence. The second and third rows respectively depict the results of future event estimation by SimVP [14] and TAU [50]. The final row represents the results obtained by our method. The complete sequence is shown in Fig.9.", "description": "This figure compares the proposed E-Motion method against state-of-the-art (SOTA) methods, SimVP and TAU, for future event estimation.  Each column shows a different sequence.  The top row displays the ground truth event sequence, while the second, third, and bottom rows show the results generated by SimVP, TAU, and the proposed E-Motion method, respectively. The figure highlights the qualitative differences in the accuracy and detail of future motion prediction across the three methods.  A more complete set of sequences is available in Figure 9.", "section": "5 Experiments"}, {"figure_path": "pWowK7jqok/figures/figures_6_1.jpg", "caption": "Figure 4: More visualization of our method's prediction in various scenarios. The results of the complete sequence along with other methods are presented in Fig. 12 and Fig. 13.", "description": "This figure visualizes the results of the proposed method's motion prediction in four different scenarios: rotating umbrella, falling balloon, a car driving out of a tunnel (poor exposure), and a person walking into a shadow (occlusion).  For each scenario, the ground truth, the model's predictions, and the predictions of two other methods (SimVP and TAU) are shown. The visualization shows that the proposed method better handles scenarios with low lighting, occlusion, and complex motion.", "section": "5 Experiments"}, {"figure_path": "pWowK7jqok/figures/figures_7_1.jpg", "caption": "Figure 5: Visualization results on downstream tasks, where we show the tasks of tracking, segmentation, and flow estimation. (a) denotes the ceiling performance of settings (b) and (c).", "description": "This figure compares the results of three downstream tasks (tracking, segmentation, and optical flow estimation) using ground truth event and RGB frames (a), estimated events from the proposed method (b), and estimated frames from a standard stable video diffusion model (c).  Subfigure (a) provides the ceiling performance achievable with perfect input data.  The other subfigures demonstrate how well the proposed method and the baseline model perform on these tasks using their respective estimations of event and video data, respectively. ", "section": "5 Experiments"}, {"figure_path": "pWowK7jqok/figures/figures_14_1.jpg", "caption": "Figure 6: Training workflow of the proposed method, where the left upper one indicates the target noised latent and the left lower one represents the prompted event sequence. We concatenate the prompt information with denoising latent for noise learning. Moreover, the feature from the CLIP model is also injected into the diffusion U-Net.", "description": "This figure illustrates the training workflow of the proposed Event-Sequence Diffusion Network. The process begins with a sequence of event data, which is preprocessed and used as a conditional input. This input is combined with noised latent data from the video diffusion model. The model's U-Net architecture processes this combined input using a cross-attention mechanism, integrating the event information with the diffusion model\u2019s learned knowledge. This architecture comprises both trained and frozen parameters. The CLIP image embeddings are also injected into the U-Net, improving the overall performance. The output is a sequence of generated events, reflecting the model's learned capacity to simulate object motion from event data.", "section": "A Training and Inference Details"}, {"figure_path": "pWowK7jqok/figures/figures_17_1.jpg", "caption": "Figure 5: Visualization results on downstream tasks, where we show the tasks of tracking, segmentation, and flow estimation. (a) denotes the ceiling performance of settings (b) and (c).", "description": "This figure shows the results of three downstream tasks (tracking, segmentation, and optical flow estimation) using events estimated by the proposed method and compares them to using ground truth events and events estimated by a standard stable video diffusion model.  The top row shows the results using ground truth events and RGB frames as input, demonstrating the ceiling performance achievable with perfect event and frame information. The middle row displays the results using events generated by the proposed method, showcasing the method's ability to effectively improve downstream tasks. The bottom row presents the results using a standard stable video diffusion model which serves as a baseline for comparison and highlights the improvement offered by the approach.", "section": "5 Experiments"}, {"figure_path": "pWowK7jqok/figures/figures_18_1.jpg", "caption": "Figure 8: Analysis of the Reinforcement Learning Process. Fig. 8a illustrates the reward curves during training, with the purple curve representing the scenario with standard deviation normalization applied, and the green curve representing the scenario without it.Fig. 8b displays partial visualization results during the training process, where the first row represents the ground truth, and the second row depicts the results estimated by the pre-trained model. Fig. 8c illustrates the distribution of reward scores with respect to standard deviation normalization for all training samples.", "description": "This figure analyzes the reinforcement learning process. It shows the reward curves with and without standard deviation normalization during training, partial visualization results comparing ground truth and pre-trained model estimations, and the distribution of reward scores against standard deviation. The standard deviation normalization is demonstrated to improve the training process and result quality.", "section": "C Reinforcement learning for SVD"}, {"figure_path": "pWowK7jqok/figures/figures_19_1.jpg", "caption": "Figure 9: Qualitative comparison between SOTA methods. The first row of each sequence represents the ground truth of the event sequence. The second and third rows respectively depict the results of future event estimation by SimVP [14] and TAU [50]. The final row represents the results obtained by our method.", "description": "This figure compares the performance of the proposed E-Motion method against two state-of-the-art (SOTA) methods, SimVP and TAU, for future event estimation.  Each sequence shows the ground truth events alongside the predictions of SimVP, TAU and the proposed method. The figure demonstrates a qualitative comparison across different complex scenarios.  The results show that the proposed method generally achieves a more accurate and realistic prediction of future events compared to the other two methods.", "section": "5 Experiments"}, {"figure_path": "pWowK7jqok/figures/figures_19_2.jpg", "caption": "Figure 9: Qualitative comparison between SOTA methods. The first row of each sequence represents the ground truth of the event sequence. The second and third rows respectively depict the results of future event estimation by SimVP [14] and TAU [50]. The final row represents the results obtained by our method.", "description": "This figure compares the results of future event estimation from three state-of-the-art (SOTA) methods (SimVP, TAU) and the proposed method.  For each of three example scenarios, the ground truth event sequence is shown, along with the predictions from the three methods. This visualization allows for a qualitative comparison of the accuracy and robustness of each method in predicting future events.", "section": "5 Experiments"}, {"figure_path": "pWowK7jqok/figures/figures_19_3.jpg", "caption": "Figure 9: Qualitative comparison between SOTA methods. The first row of each sequence represents the ground truth of the event sequence. The second and third rows respectively depict the results of future event estimation by SimVP [14] and TAU [50]. The final row represents the results obtained by our method.", "description": "This figure compares the performance of the proposed E-Motion model against two state-of-the-art (SOTA) methods, SimVP and TAU, in predicting future events.  The top row shows the ground truth event sequences for three different scenarios. Subsequent rows show the predictions made by SimVP, TAU and the proposed E-Motion model respectively for each scenario. This provides a visual comparison of the accuracy and detail in the prediction of future events by the different methods.", "section": "5 Experiments"}, {"figure_path": "pWowK7jqok/figures/figures_20_1.jpg", "caption": "Figure 10: Additional results of Segmentation. The first row of each sequence represents the ground truth of the events, and the second row shows the segmentation results. The last two rows respectively display the results estimated by our method and the corresponding segmentation results.", "description": "This figure shows a qualitative comparison of segmentation results. The top row displays the ground truth segmentation for multiple sequences. The second row shows the segmentation results obtained using the proposed method. The third and fourth rows show the segmentation results using two state-of-the-art methods (TAU and SimVP), for comparison.  The figure highlights the effectiveness of the proposed method in accurately segmenting objects in complex scenes, showcasing its superior performance compared to existing methods.", "section": "5 Experiments"}, {"figure_path": "pWowK7jqok/figures/figures_21_1.jpg", "caption": "Figure 11: Visualization results of Motion Alignment. Fig. 11a shows the ground truth of the car sequence; Fig. 11b presents the estimated results using a pre-trained model with 9 different random seeds, where several instances resulted in failures; Fig. 11c illustrates the motion alignment results generated by applying reinforcement learning, yielding more stable estimation.", "description": "This figure compares the motion estimation results with and without motion alignment.  The top row (11a) displays the ground truth. The middle row (11b) shows the results of a pre-trained model without motion alignment, revealing several instances of failure due to the inherent randomness of the model. The bottom row (11c) presents the results *with* motion alignment, highlighting the improved stability and accuracy achieved by applying reinforcement learning.", "section": "5 Experiments"}, {"figure_path": "pWowK7jqok/figures/figures_21_2.jpg", "caption": "Figure 11: Visualization results of Motion Alignment. Fig. 11a shows the ground truth of the car sequence; Fig. 11b presents the estimated results using a pre-trained model with 9 different random seeds, where several instances resulted in failures; Fig. 11c illustrates the motion alignment results generated by applying reinforcement learning, yielding more stable estimation.", "description": "This figure compares the results of motion estimation using a pre-trained model without and with motion alignment. The ground truth of a car sequence is displayed in (a). (b) Shows the results of the pre-trained model using 9 different random seeds, illustrating that the model sometimes fails. (c) Shows the results of the model after reinforcement learning motion alignment, indicating more stable results.", "section": "5 Experiments"}, {"figure_path": "pWowK7jqok/figures/figures_21_3.jpg", "caption": "Figure 3: Qualitative comparison between SOTA methods. The first row of each sequence represents the ground truth of the event sequence. The second and third rows respectively depict the results of future event estimation by SimVP [14] and TAU [50]. The final row represents the results obtained by our method. The complete sequence is shown in Fig.9.", "description": "This figure compares the proposed method's performance against two state-of-the-art methods (SimVP and TAU) for predicting future events. It visually shows the ground truth event sequence and the predictions made by each method at different time steps (t=2, t=10, t=20). The results demonstrate the superior performance of the proposed method compared to SimVP and TAU in predicting the future event sequences accurately.", "section": "5 Experiments"}, {"figure_path": "pWowK7jqok/figures/figures_22_1.jpg", "caption": "Figure 12: Visualization of our method\u2019s prediction in hs-ergb dataset.", "description": "This figure shows a qualitative comparison of the proposed method's performance on the hs-ergb dataset for two different scenarios: rotating umbrella and falling balloon.  For each scenario, it displays the ground truth event sequence, the ground truth RGB sequence, predictions from SimVP and TAU methods, and finally the predictions generated by the proposed method. The visualizations allow for a comparison of the accuracy and detail in predicting future motion across the different methods.", "section": "5 Experiments"}, {"figure_path": "pWowK7jqok/figures/figures_23_1.jpg", "caption": "Figure 9: Qualitative comparison between SOTA methods. The first row of each sequence represents the ground truth of the event sequence. The second and third rows respectively depict the results of future event estimation by SimVP [14] and TAU [50]. The final row represents the results obtained by our method.", "description": "This figure compares the proposed E-Motion model's performance against two state-of-the-art (SOTA) methods, SimVP and TAU, for future event estimation.  The top row displays the ground truth event sequences for three different scenarios.  Each subsequent row shows the predictions made by SimVP, TAU, and the proposed E-Motion model, respectively. The visualization allows for a qualitative assessment of the models\u2019 accuracy in predicting future object motion based on event sequences.", "section": "5 Experiments"}, {"figure_path": "pWowK7jqok/figures/figures_23_2.jpg", "caption": "Figure 9: Qualitative comparison between SOTA methods. The first row of each sequence represents the ground truth of the event sequence. The second and third rows respectively depict the results of future event estimation by SimVP [14] and TAU [50]. The final row represents the results obtained by our method.", "description": "This figure shows a qualitative comparison of the proposed method's performance against two state-of-the-art (SOTA) methods, SimVP and TAU, in predicting future events.  Each column displays a sequence of events; the top row is the ground truth. The middle two rows show the results from SimVP and TAU, respectively. The bottom row displays the predictions produced by the proposed method.  The figure demonstrates the relative accuracy and fidelity of the different approaches in simulating future object motion using event data.", "section": "5 Experiments"}, {"figure_path": "pWowK7jqok/figures/figures_24_1.jpg", "caption": "Figure 9: Qualitative comparison between SOTA methods. The first row of each sequence represents the ground truth of the event sequence. The second and third rows respectively depict the results of future event estimation by SimVP [14] and TAU [50]. The final row represents the results obtained by our method.", "description": "This figure shows a qualitative comparison of the proposed E-Motion method against state-of-the-art (SOTA) methods, SimVP and TAU, for future event estimation.  The top row displays the ground truth event sequences for three different scenarios (a bicycle approaching an obstacle, a car moving left with a rotating camera, and two bicycles intersecting). Subsequent rows illustrate the predictions generated by SimVP, TAU, and the proposed E-Motion method for each scenario. This allows for a visual assessment of the relative performance of each model in predicting future events, with a focus on accuracy and capturing the subtleties of motion.", "section": "5 Experiments"}, {"figure_path": "pWowK7jqok/figures/figures_24_2.jpg", "caption": "Figure 1: Illustration that the exceptional temporal resolution afforded by event cameras, alongside their distinctive event-driven sensing paradigm, presents a significant opportunity for advancing the precision in predicting future motion trajectories.", "description": "The figure illustrates how event cameras' high temporal resolution and event-driven sensing offer advantages for predicting future motion compared to traditional RGB cameras.  It shows a comparison of the temporal sampling and prediction capabilities of event cameras versus RGB cameras, highlighting the superior temporal granularity of event data. This granularity allows for more precise motion prediction. The figure shows ground truth motion alongside predicted motion from both event data and RGB camera data.", "section": "1 Introduction"}]