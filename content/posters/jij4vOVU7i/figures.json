[{"figure_path": "jij4vOVU7i/figures/figures_1_1.jpg", "caption": "Figure 1: Accuray vs. inference time on 50Salads. The bubble size represents the FLOPs in inference. Under different backbones, BaFormer enjoys the benefit of boundary-aware query voting with less running time and improved accuracy.", "description": "This figure compares the accuracy and inference time of different temporal action segmentation methods on the 50Salads dataset.  The bubble size for each method visually represents its computational cost (FLOPs). BaFormer consistently demonstrates superior performance (higher accuracy) with significantly reduced inference time compared to other methods.  It shows that using different backbones (ASFormer and SSTCN) for BaFormer, the boundary-aware query voting mechanism is effective and maintains performance advantages.", "section": "1 Introduction"}, {"figure_path": "jij4vOVU7i/figures/figures_3_1.jpg", "caption": "Figure 3: Overview of BaFormer architecture. It predicts query classes and masks, along with boundaries from output heads. Although each layer in the Transformer decoder holds three heads, we illustrate the three heads in the last layer for simplicity.", "description": "This figure shows the overall architecture of the BaFormer model.  It starts with a frame-wise encoder-decoder which processes the video frames to extract features. These features, along with instance and global queries, are fed into a transformer decoder. The decoder then uses three output heads (classification, mask prediction, and boundary prediction) to generate predictions for each query. Finally, an inference step uses a voting mechanism to combine these predictions into the final segment results. The figure highlights the parallel processing of instance queries and a global query for boundary prediction. ", "section": "3 BaFormer"}, {"figure_path": "jij4vOVU7i/figures/figures_4_1.jpg", "caption": "Figure 4: Details of Transformer decoder. (a) Transformer decoder stacks L Transformer layers. (b) Each Transformer layer consists of a masked attention, self-attention, and a feed-forward network with residual connections and normalization.", "description": "This figure shows the detailed architecture of the Transformer decoder used in BaFormer. (a) illustrates the overall structure of the decoder, which consists of L stacked Transformer layers.  Each layer takes the previous layer's output and current frame features as input and produces updated query embeddings. (b) zooms in on a single Transformer layer, showing its three sub-layers: masked attention, self-attention, and a feed-forward network.  These layers process the information in parallel and use residual connections and normalization to improve the model's performance.", "section": "3.2 Transformer Decoder"}, {"figure_path": "jij4vOVU7i/figures/figures_4_2.jpg", "caption": "Figure 4: Details of Transformer decoder. (a) Transformer decoder stacks L Transformer layers. (b) Each Transformer layer consists of a masked attention, self-attention, and a feed-forward network with residual connections and normalization.", "description": "This figure provides a detailed illustration of the Transformer decoder used in the BaFormer architecture. Panel (a) shows the overall structure of the decoder, which consists of L stacked Transformer layers. Panel (b) zooms into a single Transformer layer, revealing its internal components: masked attention, self-attention, and a feed-forward network. Each component has residual connections and normalization for improved performance.  This design is crucial for the model's ability to process temporal data efficiently and generate sparse representations.", "section": "3 BaFormer"}, {"figure_path": "jij4vOVU7i/figures/figures_5_1.jpg", "caption": "Figure 5: Different matching strategies. Given an example video including ordered action [a3, a5, a1] from a dataset with all action classes {a}i=1, (a) and (b) are fixed matching, while (c) is dynamic matching.", "description": "This figure illustrates three different strategies for matching predicted query results to ground truth action segments. (a) Ordered Class Matching aligns queries sequentially to action classes. (b) Transcript Matching aligns queries to actions based on the video's transcript order. (c) Instance Matching dynamically matches queries to action instances using the Hungarian algorithm, allowing for flexible alignment and handling of varying numbers of queries and actions.", "section": "3.4 Matching Strategies and Loss Function"}, {"figure_path": "jij4vOVU7i/figures/figures_7_1.jpg", "caption": "Figure 6: Query predictions and frame-wise results on 50Salads.", "description": "This figure visualizes the query predictions and frame-wise results obtained using the BaFormer model on the 50Salads dataset. The upper part shows the query predictions, where each color represents a different action class. The lower part compares the frame-wise results obtained using frame-based voting (FV), query-based voting (QV), and the ground truth (GT). The red arrow highlights a specific segment where the query-based voting method correctly identifies an action segment that is missed by the frame-based voting method.", "section": "3.5 Inference"}, {"figure_path": "jij4vOVU7i/figures/figures_16_1.jpg", "caption": "Figure 7: (a) and (b) illustrate the single-level and multi-level feature connection strategies, respectively. In (a), a single-level feature from the frame decoder is shared with the transformer decoder layers. While (b) involves the integration of multi-level features from various layers of the frame decoder. (Note: Mask inputs have been omitted for simplicity.)", "description": "This figure compares two different ways of connecting the frame decoder and transformer decoder in the BaFormer architecture.  (a) shows a single-level connection, where only one layer's output from the frame decoder is used. (b) demonstrates a multi-level connection that uses outputs from multiple layers, enriching the information available to the transformer decoder. The image omits mask inputs for simplicity.", "section": "E Structure and loss"}, {"figure_path": "jij4vOVU7i/figures/figures_18_1.jpg", "caption": "Figure 8: Visualization of the 50Salads dataset. Each subfigure presents a comparison of instance segmentation and frame-wise results. \u201cF\u201d indicates the absence of boundary utilization. \u201cS\u201d signifies its inclusion. \"gt\" represents the ground truth.", "description": "This figure visualizes instance segmentation results and compares them with frame-wise results obtained with and without boundary utilization. The results are shown for four different videos from the 50Salads dataset.  Each video is shown in a separate subfigure.  The top section of each subfigure shows the instance segmentation results, with different colors representing different action classes. The bottom section shows a comparison of frame-wise results: one without considering boundary information (F), one using boundary information from the proposed BaFormer model (S), and the ground truth (gt).  The figure demonstrates that incorporating boundary information leads to significantly improved results, reducing over-segmentation and improving the accuracy of action segmentation.", "section": "G More visualization"}]