[{"figure_path": "bZpZMdY1sj/tables/tables_5_1.jpg", "caption": "Table 1: Summary of the evaluation datasets.", "description": "This table summarizes the four datasets used for evaluating the visual place recognition (VPR) methods.  It lists each dataset's name, a brief description of the types of scenes it contains (urban, suburban, natural, or a mix), and the number of database images and query images used in the experiments.  The datasets vary in size and the types of challenges they present (viewpoint changes, seasonal variations, etc.).", "section": "4.1 Datasets and Performance Evaluation"}, {"figure_path": "bZpZMdY1sj/tables/tables_6_1.jpg", "caption": "Table 2: Comparison to state-of-the-art methods on four VPR benchmark datasets. The best results are highlighted in bold and the second are underlined. The descriptor dimensionalities of two-stage methods are not displayed.", "description": "This table compares the proposed SuperVLAD method with several other state-of-the-art visual place recognition (VPR) methods across four benchmark datasets: Pitts30k, MSLS-val, Nordland, and SPED.  The table presents the recall@k (R@k) for k=1, 5, and 10, showcasing the retrieval performance of each method.  The backbone network used and the dimensionality of the descriptor are also shown.  The best performing method for each dataset and metric is shown in bold, with the second-best result underlined.  Note that two-stage methods are excluded from the dimensionality comparison due to their multi-stage nature.", "section": "4.3 Comparisons with State-of-the-Art Methods"}, {"figure_path": "bZpZMdY1sj/tables/tables_7_1.jpg", "caption": "Table 3: The number of parameters of SALAD and SuperVLAD that both use the DINOv2-base backbone. The value in parentheses is the number of parameters in the optional cross-image encoder.", "description": "This table compares the number of parameters in the SALAD and SuperVLAD models, both using the DINOv2-base backbone. It breaks down the total number of parameters, the number of trainable parameters, and the number of parameters specifically in the aggregator part of the model.  The values in parentheses for SuperVLAD represent the change in parameters compared to SALAD, showing a significant reduction in the number of parameters in SuperVLAD, especially in the aggregator. The addition of a cross-image encoder is optional and increases the parameters count for both models.", "section": "4.4 Ablation Study"}, {"figure_path": "bZpZMdY1sj/tables/tables_8_1.jpg", "caption": "Table 2: Comparison to state-of-the-art methods on four VPR benchmark datasets. The best results are highlighted in bold and the second are underlined. The descriptor dimensionalities of two-stage methods are not displayed.", "description": "This table compares the proposed SuperVLAD method against several state-of-the-art visual place recognition (VPR) methods across four benchmark datasets (Pitts30k, SPED, MSLS-val, and Nordland).  It shows the Recall@1, Recall@5, and Recall@10 for each method on each dataset, highlighting the best and second-best results. The table also specifies the backbone network (e.g., VGG16, ResNet50, DINOv2) and the descriptor dimensionality used by each method.  Two-stage methods are excluded from the dimensionality comparison, as their descriptor dimensions are not consistently defined in their respective papers.", "section": "4.3 Comparisons with State-of-the-Art Methods"}, {"figure_path": "bZpZMdY1sj/tables/tables_8_2.jpg", "caption": "Table 5: Comparison of SuperVLAD with and without the ghost cluster. \"SV\" is short for SuperVLAD. The methods with the \"-ng\" suffix are those without the ghost cluster. Specifically, DINOv2-SV is the model based on DINOv2 and trained on GSV-Cities as detailed in Table 4.", "description": "This table presents the ablation study on the effect of using ghost clusters in the SuperVLAD model.  It compares the performance (Recall@1, Recall@5, and Recall@10) of SuperVLAD models trained on GSV-Cities dataset, both with and without the inclusion of a ghost cluster. The comparison is done for both the Pitts30k and MSLS-val datasets, using two different backbones: CCT and DINOv2.  The results show that the inclusion of ghost clusters generally does not significantly impact performance, and the difference is minimal in most cases.", "section": "4.4 Ablation Study"}, {"figure_path": "bZpZMdY1sj/tables/tables_8_3.jpg", "caption": "Table 6: Comparison of the very low-dimensional global descriptors with the same dimensions as the local descriptors. That is, all methods produce 768-dim global descriptors (using DINOv2-base backbone). All models are trained on GSV-Cities.", "description": "This table compares three methods for generating 768-dimensional global descriptors from local descriptors using the DINOv2-base backbone pre-trained on the GSV-Cities dataset.  The methods compared are GeM pooling, using the class token, and the proposed 1-Cluster VLAD method. The evaluation metrics used are Recall@1, Recall@5, and Recall@10, on the Pitts30k and MSLS-val datasets.  The results show that 1-Cluster VLAD outperforms both GeM pooling and using only the class token.", "section": "4.4 Ablation Study"}, {"figure_path": "bZpZMdY1sj/tables/tables_9_1.jpg", "caption": "Table 2: Comparison to state-of-the-art methods on four VPR benchmark datasets. The best results are highlighted in bold and the second are underlined. The descriptor dimensionalities of two-stage methods are not displayed.", "description": "This table compares the proposed SuperVLAD method against seven other state-of-the-art visual place recognition (VPR) methods across four benchmark datasets (Pitts30k, SPED, MSLS-val, and Nordland).  Each method's performance is evaluated using Recall@1, Recall@5, and Recall@10 metrics at different descriptor dimensions.  The table highlights the best and second-best performing methods for each dataset and metric, showcasing SuperVLAD's superior performance in many cases.  Two-stage methods are excluded because their descriptor dimensionality is not consistently reported.", "section": "4.3 Comparisons with State-of-the-Art Methods"}, {"figure_path": "bZpZMdY1sj/tables/tables_15_1.jpg", "caption": "Table 2: Comparison to state-of-the-art methods on four VPR benchmark datasets. The best results are highlighted in bold and the second are underlined. The descriptor dimensionalities of two-stage methods are not displayed.", "description": "This table compares the proposed SuperVLAD method with several state-of-the-art visual place recognition (VPR) methods across four benchmark datasets: Pitts30k, SPED, MSLS-val, and Nordland.  It shows the Recall@1, Recall@5, and Recall@10 for each method, highlighting the best and second-best performance in bold and underlined, respectively.  The table also indicates the backbone network and descriptor dimensionality used by each method, excluding two-stage methods for which dimensionality is not provided.", "section": "4.3 Comparisons with State-of-the-Art Methods"}, {"figure_path": "bZpZMdY1sj/tables/tables_15_2.jpg", "caption": "Table 2: Comparison to state-of-the-art methods on four VPR benchmark datasets. The best results are highlighted in bold and the second are underlined. The descriptor dimensionalities of two-stage methods are not displayed.", "description": "This table compares the performance of SuperVLAD against other state-of-the-art visual place recognition (VPR) methods across four benchmark datasets (Pitts30k, SPED, MSLS-val, and Nordland).  It shows the recall@k (R@k) for k=1, 5, and 10, indicating the percentage of queries where at least one of the top k retrieved images is within a predefined threshold of the ground truth.  The table highlights the best and second-best performing methods for each dataset and metric, allowing for easy comparison of SuperVLAD's performance against various backbones and descriptor dimensions.  Two-stage methods are excluded from the descriptor dimensionality comparison. ", "section": "4.3 Comparisons with State-of-the-Art Methods"}, {"figure_path": "bZpZMdY1sj/tables/tables_16_1.jpg", "caption": "Table 2: Comparison to state-of-the-art methods on four VPR benchmark datasets. The best results are highlighted in bold and the second are underlined. The descriptor dimensionalities of two-stage methods are not displayed.", "description": "This table compares the proposed SuperVLAD method with several state-of-the-art visual place recognition (VPR) methods across four benchmark datasets: Pitts30k, SPED, MSLS-val, and Nordland.  Each dataset presents unique challenges in terms of viewpoint changes, environmental conditions, and perceptual aliasing. The table shows the recall@1, recall@5, and recall@10 for each method, indicating the percentage of queries where at least one of the top N retrieved images is within a predefined distance threshold of the ground truth. The descriptor dimensionality of each method is also included, highlighting SuperVLAD's compactness. Two-stage methods, which typically re-rank initial candidates using local features, are excluded due to their different descriptor dimensions.", "section": "4.3 Comparisons with State-of-the-Art Methods"}]