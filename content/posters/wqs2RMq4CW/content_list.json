[{"type": "text", "text": "Corruption-Robust Linear Bandits: Minimax Optimality and Gap-Dependent Misspecification ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Haolin Liu\\* University of Virginia srs8rh@virginia.edu ", "page_idx": 0}, {"type": "text", "text": "Artin Tajdini University of Washington artin@cs.washington.edu ", "page_idx": 0}, {"type": "text", "text": "Andrew Wagenmaker University of California, Berkeley ajwagen@berkeley.edu ", "page_idx": 0}, {"type": "text", "text": "Chen-Yu Wei University of Virginia chenyu.wei@virginia.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In linear bandits, how can a learner effectively learn when facing corrupted rewards? While significant work has explored this question, a holistic understanding across different adversarial models and corruption measures is lacking, as is a full characterization of the minimax regret bounds. In this work, we compare two types of corruptions commonly considered: strong corruption, where the corruption level depends on the learner's chosen action, and weak corruption, where the corruption level does not depend on the learner's chosen action. We provide a unified framework to analyze these corruptions. For stochastic linear bandits, we fully characterize the gap between the minimax regret under strong and weak corruptions. We also initiate the study of corrupted adversarial linear bandits, obtaining upper and lower bounds with matching dependencies on the corruption level. ", "page_idx": 0}, {"type": "text", "text": "Next, we reveal a connection between corruption-robust learning and learning with gap-dependent misspecification\u2014a setting first studied by Liu et al. (2023a), where the misspecification level of an action or policy is proportional to its suboptimality. We present a general reduction that enables any corruption-robust algorithm to handle gap-dependent misspecification. This allows us to recover the results of Liu et al. (2023a) in a black-box manner and significantly generalize them to settings like linear MDPs, yielding the first results for gap-dependent misspecification in reinforcement learning. However, this general reduction does not attain the optimal rate for gap-dependent misspecification. Motivated by this, we develop a specialized algorithm that achieves optimal bounds for gap-dependent misspecification in linear bandits, thus answering an open question posed by Liu et al. (2023a). ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The real world is rarely truly stochastic\u2014-in practice, our observations are often corrupted\u2014and furthermore, rarely are the modeling assumption typically made in theory\u2014that the true datagenerating process lives in our model class\u2014met in reality. Therefore, robustly handling these deviations from idealized assumptions is crucial. These challenges are particularly pronounced in interactive decision-making settings, where deviations from idealized assumptions could lead an algorithm to take unsafe or severely suboptimal actions. In this work, we seek to address these challenges, and develop a unified understanding for robust learning in corruption-robust and misspecified settings. ", "page_idx": 0}, {"type": "text", "text": "We first consider the corruption-robust learning setting. Robust learning in the presence of corruptions requires designing algorithms whose guarantee have a tight scaling in the corruption level. That is, although some amount of suboptimality is inevitable if our observations are corrupted, we would hope to obtain the minimum amount of suboptimality possible at a given corruption level. While much work has been done on learning with corrupted observations, existing work has failed to yield a tight characterization of this scaling in the corruption level, even in simple settings such as linear bandits. We address this shortcoming, and develop an algorithm which achieves the optimal scaling in the corruption level, and further extend this to a novel corrupted adversarial linear bandit setting, where in addition to corrupted observations, the rewards themselves may be adversarially chosen from round to round. We obtain the first provably effcient bounds in this setting. ", "page_idx": 1}, {"type": "text", "text": "Model misspecification, another extensively studied problem in the literature, can be thought of as a form of corruption, where the corruption level is the amount of misspecification between the \u201cclosest\" model in the model class and the true environment. Standard discussions on misspecification usually assume that the misspecification for every action has a uniform upper bound, and the final regret guarantee scales linearly with the amount of misspecification. The work of Liu et al. (2023a) initiated the study on the gap-dependent misspecification setting, where the misspecification level for a given action scales with the suboptimality of that action. They demonstrated that the linear scaling in regret is not necessary in this case. We revisit this problem, and show a general reduction from the gap-dependent misspecified setting to the corruption setting. We utilize this reduction to show that settings previously not known to be learnable\u2014-for example, linear MDPs with policy gap-dependent misspecification\u2014are in fact efficiently learnable with existing corruption robust algorithms. ", "page_idx": 1}, {"type": "text", "text": "Together, our results present a unified picture of optimally learning in the presence of observation corruption, and (certain types of) model misspecification. We summarize our contributions as follows (see Section 2 and Section 3 for formal definitions of the mentioned quantities): ", "page_idx": 1}, {"type": "text", "text": "1. In Section 4, we develop a stochastic linear bandit algorithm with $\\tilde{\\mathcal{O}}(d\\sqrt{T}+\\operatorname*{min}\\{d C,\\sqrt{d}C_{\\infty}\\})$ regret, where $d$ is the feature dimension, $T$ is the number of rounds, $C$ is the strong corruption measure, and $C_{\\infty}$ is the weak corruption measure. These bounds are unimprovable. 2. In Section 5, we initiate the study of adversarial linear bandits with corruptions. We obtain $\\tilde{\\mathcal{O}}(d\\sqrt{T}+\\sqrt{d}C_{\\infty})$ and $\\widetilde{\\mathcal{O}}(\\sqrt{d^{3}T}+d C)$ regret for weak and strong corruptions, respectively. 3. We prove a general reduction that efficiently handles gap-dependent misspecification with corruption-robust algorithms. We apply our reduction to show that linear MDPs with gapdependent misspecification are efficiently learnable (Section 6). 4. Finally, while the reduction in item 3 is general, it is unable to obtain the tightest possible rate for gap-dependent misspecification. We thus develop a specialized algorithm which, in the linear bandit setting, obtains the optimal rate. This resolves the open problem of Liu et al. (2023a). ", "page_idx": 1}, {"type": "text", "text": "In Section 2 we present our problem setting, and in Section 3, compare the corruption notions in previous and our work. More related works are discussed in Appendix A. In Section 4-Section 6, we present our main results as outlined above. ", "page_idx": 1}, {"type": "text", "text": "2   Problem Setting and Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We consider the corrupted linear bandit problem. The learner interacts with the environment for $T$ rounds. The learner is given an action set $\\mathcal{A}\\subset\\mathbb{R}^{d}$ . At the beginning of round $t$ , the environment determines a reward vector $\\theta_{t}\\,\\in\\,\\mathbb{R}^{d}$ and a corruption function $\\epsilon_{t}(\\cdot):\\mathcal{A}\\to[-1,1]$ , which are both hidden from the learner. The learner then selects an action $a_{t}\\ \\in\\ A$ .Then a reward value $r_{t}\\,=\\,a_{t}^{\\top}\\theta_{t}+\\epsilon_{t}(a_{t})+\\zeta_{t}$ is revealed to the learner, for some zero-mean noise $\\zeta_{t}\\,\\in\\,[-1,1]^{2}$ .We assume that $\\|a\\|_{2}\\leq1$ $\\|\\theta_{t}\\|_{2}\\leq{\\sqrt{d}}$ , and $a^{\\top}\\theta_{t}\\in[-1,1]$ for any $a\\in{\\mathcal{A}}$ and any $t=1,2,\\dots,T$ We define $\\epsilon_{t}=\\operatorname*{max}_{a\\in A}|\\epsilon_{t}(a)|$ ", "page_idx": 1}, {"type": "text", "text": "In the stochastic setting, the environment is restricted to choose $\\theta_{t}\\,=\\,\\theta^{\\star}$ for all $t$ , while in the adversarial setting, $\\theta_{t}$ can arbitrarily depend on the history up to round $t-1$ . The regret of the learner is defined as ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{Reg}_{T}=\\operatorname*{max}_{u\\in\\mathcal{A}}\\sum_{t=1}^{T}u^{\\top}\\boldsymbol{\\theta}_{t}-\\sum_{t=1}^{T}a_{t}^{\\top}\\boldsymbol{\\theta}_{t}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "Note that although the non-stationarity of $\\theta_{t}$ in the adversarial setting captures a certain degree of corruption, this form of corruption is limited to a linear form $a^{\\top}({\\theta}_{t}-\\overline{\\theta}^{\\star})$ , which is not as general as $\\epsilon_{t}(a)$ that could be an arbitrarily function. Therefore, the corrupted linear bandit problem cannot be reduced to an adversarial linear bandit problem. ", "page_idx": 2}, {"type": "text", "text": "Notation. We denote $[n]=\\{1,2,\\ldots,n\\}$ . Let $\\Delta(A)$ be the set of distribution over $\\boldsymbol{\\mathcal{A}}$ . For any $p\\,\\in\\,\\Delta(A)$ dn the levariae max $\\begin{array}{r}{\\widehat{\\mathbf{Cov}}(p)\\,=\\,\\mathbb{E}_{a\\sim p}\\left[\\!\\!\\begin{array}{l l}{a a^{\\top}}&{a}\\\\ {a^{\\top}}&{1\\!\\!\\right]\\,\\in\\,\\mathbb{R}^{(d+1)\\times(d+1)}}\\end{array}$ . For $A,B\\in\\mathbb{R}^{d\\times d}$ , define $\\langle A,B\\rangle=\\operatorname{Tr}(A B^{\\top}).\\,\\mathbb{E}_{t}[\\cdot]$ is the expectation conditioned on history up to $t-1$ ", "page_idx": 2}, {"type": "text", "text": "G-Optimal Design.  A G-optimal design over $\\boldsymbol{\\mathcal{A}}$ is a distribution $\\rho\\in\\Delta(A)$ such that $\\|a\\|_{G^{-1}}^{2}\\leq d$ for all $a\\in A$ where $\\begin{array}{r}{G=\\sum_{a\\in\\mathcal{A}}\\rho(a)a a^{\\top}}\\end{array}$ . Note that such a distribution is guaranteed to exist, and can be effciently computed (Pukelsheim, 2006; Lattimore and Szepesvari, 2020). ", "page_idx": 2}, {"type": "text", "text": "3  Two Equivalent Views: On Adversary Adaptivity and Corruption Measure ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Previous works have studied corruption with various assumptions on the adaptivity of the adversary and different measures for the corruption level. In this work, we consider both the strong and weak guarantees, which can cover different notions of corruptions studied in previous works. We provide two different viewpoints to understand them. In the first viewpoint, the weak and strong guarantee differ by the adaptivity of the adversary, while in the second viewpoint, the two guarantees differ in the measure of corruption. Then we argue that the two viewpoints are equivalent. ", "page_idx": 2}, {"type": "text", "text": "Adversary Adaptivity (AA) Viewpoint. In this viewpoint, the corruption is specified only for the chosen action. That is, in each round $t$ , the adversary only decides a single corruption level $\\epsilon_{t}\\in\\mathbb{R}_{\\geq0}$ and ensures $\\left|\\mathbb{E}[r_{t}]\\,-\\,\\langle a_{t},\\theta^{\\star}\\rangle\\,\\right|\\,\\leq\\,\\epsilon_{t}$ . We consider two kinds of adversary: strong adversary who decides $\\epsilon_{t}$ after seeing the chosen action $a_{t}$ , and weak adversary who decides $\\epsilon_{t}$ before seeing $a_{t}$ The robustnesso the algorithm is measured by how the regret depends on $\\textstyle\\sum_{t=1}^{T}\\epsilon_{t}$ ", "page_idx": 2}, {"type": "text", "text": "Corruption Measure (CM) Viewpoint. In this viewpoint, the corruption is individually specified for every action. That is, at each round $t$ , the adversary decides $\\epsilon_{t}(a)$ for all action $a\\in A$ and ensures $\\mathbb{E}[r_{t}|a_{t}=a]-\\langle a,\\theta^{\\star}\\rangle=\\epsilon_{t}(a)$ for all $a$ . The adversary always decides $\\epsilon_{t}(\\cdot)$ beforeseeing $a_{t}$ . To evaluate the performance, we consider two different measures of the total corruption: the strong measure $\\textstyle\\sum_{t=1}^{T}\\left|\\epsilon_{t}(a_{t})\\right|$ and the weak measure $\\begin{array}{r}{\\sum_{t=1}^{T}\\operatorname*{max}_{a\\in\\mathcal{A}}|\\epsilon_{t}(a)|}\\end{array}$ ", "page_idx": 2}, {"type": "text", "text": "We argue that the two viewpoints are equivalent in the sense that the performance guarantee of an algorithm under strong/weak adversary in the AA viewpoint are the same as those under strong/weak measure in the CM viewpoint, respectively. This is by the following observation. A strong adversary in the AA viewpoint who decides the corruption level $\\epsilon_{t}$ after seeing $a_{t}$ can be viewed as deciding the corruption $\\epsilon_{t}(a)$ for all action a before seeing $a_{t}$ , and set $\\epsilon_{t}=|\\epsilon_{t}(a_{t})|$ after seeing $a_{t}$ . In other words, $\\epsilon_{t}\\bar{(}a)$ is the corruption planned (before seeing $a_{t}$ ) by a strong adversary assuming $a_{t}=a$ , and the adversary simply carries out its plan after seeing $a_{t}$ . It is clear that this is equivalent to the CM viewpoint with $\\textstyle\\sum_{t=1}^{T}\\left|\\epsilon_{t}(a_{t})\\right|$ as the corruption measure. See Appendix B for more details. On the other hand, a weak adversary in the AA viewpoint has to decide an upper bound of the corruption level $\\epsilon_{t}$ no matter which action $a_{t}$ is chosen by the learner. This can be viewed as deciding the corruption $\\epsilon_{t}(a)$ for every action $a$ before seeing $a_{t}$ with the restriction $|\\epsilon_{t}(a)|\\leq\\epsilon_{t}$ for all $a$ . Therefore, this is equivalent to using $\\begin{array}{r}{\\sum_{t=1}^{T}\\operatorname*{max}_{a}|\\epsilon_{t}(a)|}\\end{array}$ to measure total corruption in the CM viewpoint. ", "page_idx": 2}, {"type": "text", "text": "In this work, we adopt the CM viewpoint as described in Section 2. With the CM viewpoint, for both strong and weak settings, the power of the adversary remains the same as the standard \u201cadaptive adversary\u201d (i.e., deciding the corruption function $\\epsilon_{t}(\\cdot)$ based on the history up to time $t-1)$ ),and we only need to derive regret bounds with different corruption measures. All our results can also be interpreted in the AA viewpoint, as the above argument suggests. ", "page_idx": 2}, {"type": "text", "text": "With this unified viewpoint, we categorize in Table 1 previous works on linear (contextual) bandits based on the corruption measure, all under the same type of adversary. According to the definitions in Table 1, $C$ and $C_{\\infty}$ correspond to the strong measure and weak measure mentioned above, respectively. It is easy to see that $C\\le\\{C_{\\infty},C_{\\mathsf{s q}}\\}\\le\\bar{C}_{\\mathsf{s q},\\infty}\\le C_{\\mathsf{m s}}$ , where $C_{\\infty}$ and $C_{\\mathsf{s q}}$ are incomparable. ", "page_idx": 2}, {"type": "text", "text": "Table 1: Classification of previous works based on the corruption measure. Foster et al. (2020), Takemura et al. (2021), and He et al. (2022) studied the more general linear contextual bandit setting where the action set can be chosen by an adaptive adversary in every round. Foster et al. (2020) and Takemura et al. (2021) reported their bounds in $C_{\\mathsf{s q},\\infty}$ andi $C_{\\sf m s}$ , respectively, though one canmake minor modifications to their analysis and show that their algorithms actually ensure the $C_{\\mathsf{s q}}$ bound. ", "page_idx": 3}, {"type": "table", "img_path": "wqs2RMq4CW/tmp/6d169a2c75936a8cd6a0c2d72251c7313ac8c5d21c8cdc40a6a3be0120829cb6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "table", "img_path": "wqs2RMq4CW/tmp/7014f041c674426252a8fcd16f984e9eca7076244455f8dcc0bb19de7de6a45e.jpg", "table_caption": ["Table 2: Regret bounds under corruption measure $C$ and $C_{\\infty}$ . See Table 1 for their definitions. He et al. (2022) studied the more general linear contextual bandits setting, though it also gives the state-of-the-art $C$ bound for linear bandits. "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "For stochastic linear bandits, considering the relations among different corruption measures, the Pareto frontiers of the existing upper bounds are $\\tilde{\\mathcal{O}}(d\\sqrt{T}+\\sqrt{d}C_{\\mathsf{s q}})$ by Foster et al. (2020) and Takemura et al. (2021), and $\\widetilde{\\mathcal{O}}(d\\sqrt{T}+d C)$ by He et al. (2022). The lower bound frontiers are $\\Omega(d\\sqrt{T}+\\sqrt{d}C_{\\mathrm{ms}})$ by Lattimore et al. (2020) and $\\Omega(d\\sqrt{T}+d C)$ by Bogunovic et al. (2020). These results imply an $\\tilde{\\mathcal{O}}(d\\sqrt{T}+d C_{\\infty})$ upper bound and an $\\Omega(d\\sqrt{T}+\\sqrt{d}C_{\\infty})$ lower bound, which still have a gap. In this work, we close the gap by showing an $\\tilde{\\mathcal{O}}(d\\sqrt{T}+\\sqrt{d}C_{\\infty})$ upper bound. ", "page_idx": 3}, {"type": "text", "text": "For adversarial linear bandits, we are only aware of upper bound $\\tilde{\\mathcal{O}}(d\\sqrt{T}+\\sqrt{d}C_{\\mathsf{s q},\\infty})$ by Liu et al. (2024), and not aware of any upper bounds related to $C_{\\infty}$ or $C$ .In this work, we show $\\tilde{\\mathcal{O}}(d\\sqrt{T}+\\sqrt{d}C_{\\infty})$ and $\\widetilde{\\mathcal{O}}(\\sqrt{d^{3}T}+\\dot{d C})$ upper bounds. The results are summarized in Table 2. As in most previous work, we assume that $C_{\\infty}$ and $C$ (or their upper bounds) are known by the learner when developing the algorithms. The case of unknown $C_{\\infty}$ or $C$ is discussed in Appendix C. ", "page_idx": 3}, {"type": "text", "text": "We emphasize that before our work, for both stochastic and adversarial linear bandits, it was unknown how to achieve $\\tilde{\\mathcal{O}}(d\\sqrt{T}+\\sqrt{d}C_{\\infty})$ regret. To see how $C_{\\infty}$ is different from other notions such as $C_{\\sf m s}$ and $C_{\\mathsf{s q}}$ , we observe that for stochastic linear bandits, while $\\tilde{\\mathcal{O}}(d\\sqrt{T}+\\sqrt{d}C_{\\mathsf{s q}})$ can be achieved via deterministic algorithms, it is not the case for $\\tilde{\\mathcal{O}}(d\\sqrt{T}+\\sqrt{d}C_{\\infty})$ . The reason is that for deterministic algorithms, the adversary can control $C_{\\infty}$ to be the same as $C$ , for which $\\Omega(d\\sqrt{T}+d C)$ is unavoidable. We formalize this in Proposition 1, with the proof given in Appendix D. _This precludes the possibility of many previous algorithms to actually achieve the $\\tilde{\\mathcal{O}}(d\\sqrt{T}+\\sqrt{d}C_{\\infty})$ upper bound, e.g., Lattimore et al. (2020), Takemura et al. (2021), Bogunovic et al. (2020, 2021), He et al. (2022). ", "page_idx": 3}, {"type": "text", "text": "PropositionFstochasticlneabandits,threexistsadeteministicalgoritachevng ${\\bf R e g}_{T}=$ $\\widetilde{\\mathcal{O}}(d\\sqrt{T}+\\sqrt{d}C_{s q})$ while any deterministic algorithm must suffer $\\mathrm{Reg}_{T}=\\Omega(d\\sqrt{T}+d C_{\\infty})$ ", "page_idx": 3}, {"type": "text", "text": "1 Input: $Z=\\sqrt{d}C_{\\infty}$ or $d C$ , action space $\\mathcal{A}\\subset\\mathbb{R}^{d}$ , confidence level $\\delta$   \n2 Let $A_{1}=A$ and $L=d\\log(|A|T/\\bar{\\delta})$   \n3 for $k=1,2,\\hdots{\\bf d}$ 0   \n4 Compute a G-optimal design (defined in Section 2) $p_{k}$ over $\\mathcal{A}_{k}$ , and let $\\begin{array}{r}{G_{k}=\\sum_{a}p_{k}(a)a a^{\\top}}\\end{array}$ Define $\\mathcal{Z}_{k}=[(2^{k-1}-1)L+1,(2^{k}-1)L]$ and $m_{k}=|{\\mathcal{I}}_{k}|=2^{k-1}L$   \n5 for $t\\in\\mathcal{T}_{k}$ do Draw $a_{t}\\sim p_{k}$ and receive $r_{t}$ where $\\mathbb{E}[r_{t}]=a_{t}^{\\top}\\theta^{\\star}+\\epsilon_{t}(a_{t})$   \n6 Define reward vecor estimator $\\begin{array}{r}{\\widehat{\\theta}_{k}=(m_{k}G_{k})^{-1}\\sum_{t\\in\\mathbb{Z}_{k}}a_{t}r_{t}}\\end{array}$ and active action set: $\\begin{array}{r}{\\mathcal{A}_{k+1}=\\Big\\{a\\in\\mathcal{A}_{k}:\\ \\underset{b\\in\\mathcal{A}_{k}}{\\operatorname*{max}}b^{\\top}\\widehat{\\theta}_{k}-a^{\\top}\\widehat{\\theta}_{k}\\leq8\\sqrt{\\frac{d\\log(|A|T/\\delta)}{m_{k}}}+\\frac{2Z}{m_{k}}\\Big\\}.}\\end{array}$ (1) ", "page_idx": 4}, {"type": "text", "text": "4  Stochastic Linear Bandits ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we introduce Algorithm 1, which achieves optimal regret for both $C$ and $C_{\\infty}$ ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 is an elimination-based algorithm. At each epoch $k$ , it samples actions from a fixed distribution $p_{k}\\in\\Delta(\\mathcal{A}_{k})$ , which is a G-optimal design over the active action set $\\mathcal{A}_{k}$ (Line 4). At the end of epoch $k$ , only actions that are within the error threshold will be kept in the active action set of the next epoch (Eq. (1)). While previous works by Lattimore et al. (2020) and Bogunovic et al. (2021) have used a similar elimination framework to obtain $\\tilde{\\mathcal{O}}(d\\sqrt{T}+\\sqrt{d}C_{\\sf m s})$ and $\\tilde{\\mathcal{O}}(\\bar{d}\\sqrt{T}+d^{\\frac{3}{2}}C)$ bounds, respectively, we note that their algorithms only specify the number of times the learner should sample for each action in each epoch. This is different from our algorithm that requires the learner to exactly use the distribution $p_{k}$ to sample actions in every round in epoch $k$ . As argued in Proposition 1, if their algorithms are instantiated as a deterministic algorithm, then the regret will be at least $\\Omega(d\\sqrt{T}+d C_{\\infty})$ . Thus, this subtle difference is important. ", "page_idx": 4}, {"type": "text", "text": "Note that to achieve the tight $C_{\\infty}$ (or $C$ )bound, $Z=\\sqrt{d}C_{\\infty}$ (or $Z=d C,$ has to be input to the algorithm to decide the error threshold. The guarantee of Algorithm 1 is stated in Theorem 4.1. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4.1. With input $Z=\\sqrt{d}C_{\\infty}$ or $Z=d C$ Algorithm 1 ensures with probability at least $1-\\delta$ that $\\mathrm{Reg}_{T}\\leq\\mathcal{O}(d\\sqrt{T\\log(T/\\delta)}+Z\\log T)$ ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 can also be shown to ensure that ${\\mathrm{Reg}}_{T}\\leq{\\mathcal{O}}(\\sqrt{d T\\log(|A|T/\\delta)}\\!+\\!Z\\log T)$ ,which could be smaller than the bound given in Theorem 4.1 when $|{\\mathcal{A}}|$ is small. ", "page_idx": 4}, {"type": "text", "text": "5  Adversarial Linear Bandits ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we consider corrupted adversarial linear bandits. Although adversarial linear bandits have been widely studied, robustness under corruption is an under-explored topic: there is no prior work obtaining regret bounds that linearly depends on either $C_{\\infty}$ Or $C$ ", "page_idx": 4}, {"type": "text", "text": "5.1 $C_{\\infty}$ bound in Adversarial Linear Bandits ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our algorithm (Algorithm 2) is based on follow-the-regularized-leader (FTRL) with logdet regularizer. Similar to previous works (Foster et al., 2020; Zimmert and Lattimore, 2022; Liu et al., 2024, 2023b) that utilize logdet regularizer, the feasible set $\\mathcal{H}$ is in $\\mathbb{R}^{(d+1)\\times(d+1)}$ , which is the space of the covariance matrix for distributions over the lifted action space (Line 2-Line 3). At round $t$ , the algorithm obtains a covariance matrix $\\pmb{H}_{t}$ by solving the FTRL objective (Eq. (2). The action distribution $p_{t}$ is such that the induced covariance matrix is equal to $\\pmb{H}_{t}$ (Eq. (3). After sampling $a_{t}\\sim p_{t}$ and obtaining the reward $r_{t}$ , the algorithm constructs reward vector estimator ${\\widehat{\\theta}}_{t}$ (Line 8) and feeds it to FTRL. The reader may refer to Zimmert and Lattimore (2022) for more details. ", "page_idx": 4}, {"type": "text", "text": "In typical corruption-free adversarial linear bandits, the learner would construct an unbiased reward vector estimator. However, in the presence of corruption, the learner can no longer construct an unbiased estimator. To compensate the bias, we adopt the idea of \u201cadding exploration bonus\" inspired ", "page_idx": 4}, {"type": "text", "text": "Parameters: $\\begin{array}{r}{\\alpha=\\operatorname*{max}\\bigg\\{\\frac{C_{\\infty}}{\\sqrt{d\\log(T)}},\\sqrt{T}\\bigg\\},\\eta=\\sqrt{\\frac{\\log(T)}{T}}}\\end{array}$ and=% ", "page_idx": 5}, {"type": "text", "text": "2 Let $\\rho\\in\\Delta(A)$ be a $\\mathrm{G}$ -optimal design over $\\boldsymbol{\\mathcal{A}}$ , and let $\\Delta_{\\gamma}(A)=\\{p:p=(1-\\gamma)p^{\\prime}+\\gamma\\rho,\\;\\;p^{\\prime}\\in\\Delta(A)\\}.$ ", "page_idx": 5}, {"type": "text", "text": "3 Define feasible set $\\mathcal{H}=\\left\\{\\widehat{\\mathbf{Cov}}(p):p\\in\\Delta_{\\gamma}(A)\\right\\}$ $\\widehat{(\\mathbf{Cov}(p)}$ is defined in Section 2) ", "page_idx": 5}, {"type": "equation", "text": "$$\nH_{t}=\\underset{H\\in\\mathcal{H}}{\\operatorname{argmax}}\\left\\{\\eta\\left\\langle H,\\Lambda_{t-1}\\right\\rangle-G(H)\\right\\}\\ \\mathrm{~where~}\\Lambda_{t-1}=\\left[\\!\\!\\begin{array}{c c}{\\alpha B_{t}}&{\\frac{1}{2}\\sum_{s=1}^{t-1}\\widehat{\\theta}_{s}}\\\\ {\\frac{1}{2}\\sum_{s=1}^{t-1}\\widehat{\\theta}_{s}^{\\top}}&{0}\\end{array}\\!\\!\\right]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\Sigma_{t}=\\sum_{a\\in A}p_{t}(a)a a^{\\top},}\\\\ {B_{t}=\\operatorname{BoNUS}(B_{t-1},\\Sigma_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Function $B^{\\prime}=80\\mathrm{NUS}(B,\\Sigma)$ if $B\\preceq\\Sigma^{-1}$ then return $B^{\\prime}=\\Sigma^{-1}$ else Perform eigen-decomposition: $\\begin{array}{r}{B^{-\\frac{1}{2}}\\Sigma^{-1}\\bar{B^{-\\frac{1}{2}}}=\\sum_{i=1}^{d}\\lambda_{i}v_{i}v_{i}^{\\top}}\\end{array}$ Where $\\{v_{i}\\}_{i=1}^{d}$ are unit eigenvectors. return $\\begin{array}{r}{B^{\\prime}=B^{\\frac{1}{2}}\\left(\\sum_{i=1}^{d}\\operatorname*{max}\\{\\lambda_{i},1\\}v_{i}v_{i}^{\\top}\\right)B^{\\frac{1}{2}}.}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "(a) The bonus function ", "page_idx": 5}, {"type": "image", "img_path": "wqs2RMq4CW/tmp/5577425c582d9d133050fff9bf0d917eb18292139e348094ab38f6ddddfaac50.jpg", "img_caption": ["Figure 1: The bonus function and its illustration "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "by previous work on high-probability adversarial linear bandits (Lee et al., 2020; Zimmert and Lattimore, 2022). In the regret analysis, the exploration bonus creates a negative term that cancels the bias of the loss estimator. The bonus is represented by the $B_{t}$ in Eq. (5). ", "page_idx": 5}, {"type": "text", "text": "To decide the form of $B_{t}$ , we first analyze the bias. With the standard construction of the reward estimator, the bias on the benchmark action $u$ can be calculated as (with $\\epsilon_{t}:=\\operatorname*{max}_{a}|\\epsilon_{t}(a)|)$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\bigl|u^{\\top}\\left(\\mathbb{E}_{t}[\\Sigma_{t}^{-1}a_{t}r_{t}]-\\theta_{t}\\right)\\bigr|=\\bigl|u^{\\top}\\mathbb{E}_{t}[\\Sigma_{t}^{-1}a_{t}\\epsilon_{t}(a_{t})]\\bigr|\\leq\\epsilon_{t}\\sqrt{u^{\\top}\\Sigma_{t}^{-1}\\mathbb{E}_{t}[a_{t}a_{t}^{\\top}]\\Sigma_{t}^{-1}u}=\\epsilon_{t}\\|u\\|_{\\Sigma_{t}^{-1}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\Sigma_{t}$ is the feature covariance matrix induced by $p_{t}$ (defined in Eq. (4)). Below, we compare different bonus designs in previous and our work. ", "page_idx": 5}, {"type": "text", "text": "Bonus design in previous work. In Zimmert and Lattimore (2022), which is also based on logdetFTRL but where the goal is only to get a high-probability bound, the bonus introduces an additional regret the form $\\begin{array}{r}{-\\alpha\\|\\bar{u}\\|_{\\Sigma_{t}^{-1}}^{2}+\\dot{\\alpha}\\sum_{a}\\^{\\bullet}p_{t}(a)\\|\\bar{a}\\|_{\\Sigma_{t}^{-1}}^{2}}\\end{array}$ . This can be used to cancel off the bias in Eq. (6): ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sum_{t=1}^{T}\\epsilon_{t}\\|u\\|_{\\Sigma_{t}^{-1}}-\\alpha\\sum_{t=1}^{T}\\|u\\|_{\\Sigma_{t}^{-1}}^{2}+\\alpha\\sum_{t=1}^{T}\\sum_{a\\in\\mathcal{A}}p_{t}(a)\\|a\\|_{\\Sigma_{t}^{-1}}^{2}\\leq\\sum_{t=1}^{T}\\frac{\\epsilon_{t}^{2}}{\\alpha}+\\alpha d T,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where we use AM-GM. Unfortunately, with the optimal $\\alpha$ , this only leads to an additive regret $\\begin{array}{r}{\\sqrt{d T\\sum_{t}\\epsilon_{t}^{2}}=\\sqrt{d}C_{\\mathsf{s q,\\infty}}>\\sqrt{d}C_{\\infty}}\\end{array}$ , which does not meet our goal. ", "page_idx": 5}, {"type": "text", "text": "Bonus design in our work. To obtain the tighter $\\sqrt{d}C_{\\infty}=\\sqrt{d}\\sum_{t}\\epsilon_{t}$ bound, our idea is to construct a positive-definite matrix $B_{t}$ such that $B_{t}\\succeq\\Sigma_{\\tau}^{-1}$ for all $\\tau\\in[t]$ , and add bonus $B_{t}-B_{t-1}$ at round $t$ This way, the total negative regret on $u$ becomes $-\\alpha\\Vert u\\Vert_{B_{T}}^{2}$ and the cancellation becomes ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\sum_{\\iota=1}^{T}\\epsilon_{t}\\|\\boldsymbol{u}\\|_{\\Sigma_{t}^{-1}}-\\alpha\\|\\boldsymbol{u}\\|_{B_{T}}^{2}+\\alpha\\sum_{\\iota=1}^{T}\\sum_{a\\in\\mathcal{A}}p_{t}(a)\\|\\boldsymbol{a}\\|_{B_{t}-B_{t-1}}^{2}\\leq\\frac{\\left(\\sum_{t=1}^{T}\\epsilon_{t}\\right)^{2}}{\\alpha}+\\alpha\\sum_{t=1}^{T}\\left\\langle\\Sigma_{t},B_{t}-B_{t-1}\\right\\rangle,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where we use $B_{T}\\,\\succeq\\,\\Sigma_{t}^{-1}$ for all $t$ and AM-GM. With this, it suffices to find $B_{t}$ satisfying our condition $B_{t}\\succeq\\Sigma_{\\tau}^{-1}$ for $\\tau\\leq t$ and bound the overhead $\\begin{array}{r}{\\sum_{t=1}^{T}\\left\\langle\\Sigma_{t},B_{t}-B_{t-1}\\right\\rangle}\\end{array}$ by $\\widetilde O(d)$ ", "page_idx": 6}, {"type": "text", "text": "It turns out that there exists a way to inductively construct $B_{t}$ so that $B_{t}\\succeq\\Sigma_{\\tau}^{-1}$ for all $\\tau\\leq t$ and $\\begin{array}{r}{\\sum_{t=1}^{T}\\left\\langle\\Sigma_{t},B_{t}-B_{t-1}\\right\\rangle\\lesssim\\log\\operatorname*{det}(B_{T})=\\tilde{\\mathcal{O}}(d)}\\end{array}$ This is by letting $B_{t}$ to be a minimal matrix such that $B_{t}\\succeq B_{t-1}$ and $B_{t}\\succeq\\Sigma_{t}^{-1}$ . By induction, this ensures $B_{t}\\succeq\\Sigma_{\\tau}^{-1}$ for all $\\tau\\leq t$ . The function $B_{t}=\\mathrm{BONUS}(B_{t-1},\\Sigma_{t})$ is formally defined in Figure 1a. The geometric interpretation is finding the minimal ellipsoid that contains both ellipsoids induced by $B_{t-1}$ and $\\Sigma_{t}^{-1}$ . An illustration figure is given in Figure 1b. ", "page_idx": 6}, {"type": "text", "text": "We adopt the fixed-point formulation in Zimmert and Lattimore (2022) (see their FTRL-FB) that includes the bonus for round $t$ (i.e., $B_{t}$ ) in the FTRL objective when calculating the policy at round $t$ (Eq. (2)). Notice that $B_{t}$ , in turn, depends on the policy at round $t$ (Eq. (5), where $\\Sigma_{t}$ depends on $p_{t})$ , and thus this forms a fixed-point problem. In the regret analysis, this avoids the \u201cstability term' of the bonus to appear in the regret bound. While the fixed-point solution always exists, it may not be computationally efficient to find. For completeness, in Algorithm 4 (Appendix F), we present a version that does not require solving fixed point but has a suboptimal $d{\\sqrt{\\log{T}}}C_{\\infty}$ additive regret. The guarantee of Algorithm 2 is stated in Theorem 5.1, with its proof deferred to Appendix F. ", "page_idx": 6}, {"type": "text", "text": "Theorem 5.1. Algorithm 2 ensures with probability of $1-\\delta$ $\\mathrm{Reg}_{T}=\\widetilde{\\mathcal{O}}(d\\sqrt{T}+\\sqrt{d}C_{\\infty}),$ where $\\widetilde O(\\cdot)$ hides $\\log(T/\\delta)$ factors. ", "page_idx": 6}, {"type": "text", "text": "5.2 $C$ bound in Adversarial Linear Bandits ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To see how to obtain a $C$ bound, we perform the bias analysis again. Similar but slightly different from Eq. (6), with the standard loss estimator, the bias on action $u$ 's reward is bounded by ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\left|u^{\\top}\\left(\\mathbb{E}_{t}[\\Sigma_{t}^{-1}a_{t}r_{t}]-\\theta_{t}\\right)\\right|=\\left|u^{\\top}\\mathbb{E}_{t}[\\Sigma_{t}^{-1}a_{t}\\epsilon_{t}(a_{t})]\\right|\\leq\\|u\\|_{\\Sigma_{t}^{-1}}\\mathbb{E}_{t}\\left[\\|a_{t}\\|_{\\Sigma_{t}^{-1}}|\\epsilon_{t}(a_{t})|\\right].\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Unlike in Eq. (6), we do not relax $\\left|\\epsilon_{t}(a_{t})\\right|$ to $\\epsilon_{t}=\\operatorname*{max}_{a}|\\epsilon_{t}(a)|$ because we want the final bound to depend on $\\begin{array}{r}{C=\\sum_{t}|\\epsilon_{t}(a_{t})|}\\end{array}$ . The idea to ensure that the sum of Eq. (9) over $t$ can be related to $C$ is to make $\\|a_{t}\\|_{\\Sigma_{t}^{-1}}$ bounded by a constant poly $(d)$ , which allows us to further bound Eq. (9) by $\\mathrm{poly}(d)\\|u\\|_{\\Sigma_{t}^{-1}}|\\epsilon_{t}(\\mathring{a}_{t})|$ . Such a property holds in standard linear bandit algorithms that operate in the continuous action space where $a_{t}$ is a point in the convex hull of $\\boldsymbol{\\mathcal{A}}$ , and utilize a more concentrated action sampling scheme. Algorithms that are of this type include SCRiBLe (Abernethy et al., 2008) and continuous exponential weights (CEW) (Ito et al., 2020). ", "page_idx": 6}, {"type": "text", "text": "For SCRiBLe and CEW, the work by Lee et al. (2020) and Zimmert and Lattimore (2022) developed techniques that incorporate bonus terms to get high probability regret bounds. The bonus terms introduced by Zimmert and Lattimore (2022) is similar to that discussed in Eq. (7), which only allows us to get a $C_{\\mathsf{s q}}$ bound. The bonus terms introduced by Lee et al. (2020) allows us to obtain a $C$ bound, but the overhead introduced by the bonus terms is much larger, resulting in a highly sub-optimal regret bound. Indeed, as shown in Appendix J, adopting their bonus construction results in an additional regret of $d^{\\frac{5}{2}}C$ With several attempts, we are only able to obtain the tight corruption dependency $d C$ using the bonus in Section 5.1. To use that bonus, however, it is necessary to lift the problem to $(d+\\bar{1)^{2}}$ -dimensional space. Unfortunately, existing SCRiBLe and CEW algorithms only operate in the original $d$ -dimensional space, and as discussed above, we need them to ensure $\\|a_{t}\\|_{\\Sigma_{t}^{-1}}\\leq\\mathrm{poly}(d)$ ", "page_idx": 6}, {"type": "text", "text": "In order to combine these two useful ideas (i.e., our bonus design in Section 5.1, and the concentrated sampling scheme by SCRiBLe or CEW), we end up with the algorithm that runs CEW over the lifted action space (Algorithm 3). In order to simplify the exposition, we assume without loss of generality ", "page_idx": 6}, {"type": "equation", "text": "$$\nq_{t}^{\\prime}(h)=\\frac{\\exp\\big(\\eta\\langle h,\\phi(\\Lambda_{t-1})\\rangle\\big)}{\\int_{h^{\\prime}\\in\\phi(\\varkappa)}\\exp\\big(\\eta\\langle h^{\\prime},\\phi(\\Lambda_{t-1})\\rangle\\big)\\mathrm{d}h^{\\prime}}\\quad\\mathrm{where}\\quad\\Lambda_{t-1}=\\left[\\underbrace{\\alpha B_{t}}_{\\textnormal{\\scriptsize{j}}\\sum_{s=1}^{t-1}\\widehat\\theta_{s}^{\\top}}\\quad\\frac{1}{2}\\sum_{s=1}^{t-1}\\widehat\\theta_{s}\\right].\n$$", "text_format": "latex", "page_idx": 7}, {"type": "equation", "text": "$$\nh\\sim q_{t}^{\\prime},\\quad\\pmb{H}=\\phi^{-1}(h),\\quad a=\\left[\\begin{array}{l l l l l}{1}&{0}&{\\cdots}&{0}&{0}\\\\ {0}&{1}&{\\cdots}&{0}&{0}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}&{\\vdots}\\\\ {0}&{0}&{\\cdots}&{1}&{0}\\end{array}\\right]\\pmb{H}e_{d+1}:=\\pmb{Z}\\pmb{H}e_{d+1}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{p}_{t}(a)=\\frac{p_{t}(a)\\mathbb{1}\\{\\|a\\|_{\\Sigma_{t}^{-1}}\\leq\\sqrt{d}\\beta\\}}{\\int_{a^{\\prime}\\in A}p_{t}(a^{\\prime})\\mathbb{1}\\{\\|a^{\\prime}\\|_{\\Sigma_{t}^{-1}}\\leq\\sqrt{d}\\beta\\}\\mathrm{d}a^{\\prime}},\\;\\;\\mathrm{where~}\\Sigma_{t}=\\mathbb{E}_{a\\sim p_{t}}[a a^{\\top}].}\\\\ &{B_{t}=\\mathrm{BONUS}(B_{t-1},\\tilde{\\Sigma}_{t}),\\;\\;\\mathrm{where~}\\tilde{\\Sigma}_{t}=\\gamma I+\\mathbb{E}_{a\\sim\\tilde{p}_{t}}[a a^{\\top}].}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "that $A=\\operatorname{conv}(A)$ . The lifted action space is $\\mathcal{H}=\\{\\widehat{\\mathrm{Cov}}(p):p\\in\\Delta(A)\\}\\,\\subset\\mathbb{R}^{(d+1)\\times(d+1)}$ . The price of the lifting is that the \u201cregularization penalty term\" in the regret analysis now grows from $\\widetilde{\\mathcal{O}}(d/\\eta)$ to $\\widetilde{\\mathcal{O}}(d^{2}/\\eta)$ , which gives us the $\\sqrt{d^{3}T}$ sub-optimal regret. ", "page_idx": 7}, {"type": "text", "text": "Note that CEW requires the assumption that the feasible set is a convex body with non-zero volume, but the effective dimension of $\\mathcal{H}$ is strictly smaller than $(d\\!+\\!1)^{2}$ and thus have zero volume in $\\mathbb{R}^{(d+1)^{2}}$ To correctly write the algorithm, we introduce an invertible linear transformation $\\phi:\\mathbb{R}^{(d+1)^{2}}\\rightarrow\\mathbb{R}^{m}$ that maps an $(d+1)^{2}$ -dimensional action set $\\mathcal{H}$ to an $m$ -dimensional one, where $m$ is the effective dimension of $\\mathcal{H}$ . In Appendix I, we formally define this $\\phi$ . The algorithm uses $\\phi$ to map all lifted actions and reward estimators from $\\mathbb{R}^{(d+1)\\times\\bar{(d+1)}}$ 0 $\\mathbb{R}^{m}$ ", "page_idx": 7}, {"type": "text", "text": "The exponential weights runs over the space of $\\phi(\\mathcal{H})$ (see Eq. (10). A point $h\\in\\phi(\\mathcal{H})$ sampled from the exponential weights can be linearly mapped to an action $a\\in A$ according to Eq. (11). We use $q_{t}^{\\prime}$ to denote the exponential weight distribution in $\\phi(\\mathcal{H})$ , and use $p_{t}$ to denote the corresponding distribution in $\\boldsymbol{\\mathcal{A}}$ Instead of sampling $a_{t}$ from $p_{t}$ , we sample it through rejection sampling that rejects samples with $\\|a_{t}\\|_{\\Sigma_{t}^{-1}}>\\tilde{\\Theta}(\\sqrt{d})$ (Eq. (12)). This technique was developed by Ito et al. (2020), and this guarantees $\\|a_{t}\\|_{\\Sigma_{t}^{-1}}\\leq\\widetilde O(\\sqrt{d})$ -which is our goal as discussed in Eq. (9)\u2014-while keeping the clipped distribution $\\tilde{p}_{t}$ close enough to the original distribution $p_{t}$ . This last property heavily relies on the log-concavity of the exponential weight distribution (Ito et al., 2020). The definition of the bonus term is similar to that in Algorithm 2 (Eq. (13)). The construction of the reward estimator (Line 7) and the way of lifting (Eq. (10) are also similar to those in Algorithm 2. Again, we adopt the fixed-point formulation where the calculation of the policy at time $t$ involves the bonus at time $t$ which, in turn, depends on the policy at time $t$ . It is unlikely that this algorithm can be polynomial time. As a remedy, we provide a polynomial time algorithm (Algorithm 6) in Appendix J with a much worse regret bound of $\\tilde{\\mathcal{O}}(d^{3}\\bar{\\sqrt{T}}+d^{5/2}C)$ . The regret guarantee of Algorithm 3 is given in the following theorem. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Theorem 5.2. Algorithm 3 ensures with probability at least $1-\\delta$ $\\mathrm{Reg}_{T}=\\widetilde{\\mathcal{O}}\\big(\\sqrt{d^{3}T}+d C\\big)$ where $\\widetilde O(\\cdot)$ hides polylog $(T/\\delta)$ factors. ", "page_idx": 7}, {"type": "text", "text": "6  Gap-Dependent Misspecification ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Intimately related to corrupted settings are misspecified settings, settings where our model class is unable to capture the true environment we are working with. For example, we might consider a stochastic linear bandit problem where the underlying reward function $f(\\cdot)$ is nearly linear, i.e., there exists some $\\theta$ and $\\epsilon^{\\mathrm{mis}}(\\cdot)$ such that $|f(a)-a^{\\top}\\theta|\\,\\leq\\,\\epsilon^{\\mathrm{mis}}(a)$ for each $a$ . Indeed, in such settings, playing on our true (nearly linear) environment is equivalent to playing on the environment with reward mean $a^{\\top}\\theta$ and with corruption $\\epsilon^{\\mathrm{mis}}(a)$ at each step. Thus, if we can solve corruption settings, it stands to reason that we can solve misspecified settings. ", "page_idx": 8}, {"type": "text", "text": "Here we are particularly interested in obtaining bounds on misspecified decision-making that scale precisely with action-dependent misspecification, $\\epsilon^{\\mathrm{mis}}(a)$ .While it is relatively straightforward to obtain bounds on learning in misspecified settings for a uniform level of misspecification $\\epsilon\\geq$ $\\operatorname*{max}_{a\\in\\mathcal{A}}\\epsilon^{\\mathrm{mis}}(a)$ , obtaining bounds on learning with action-dependent misspecification have proved more elusive. To formalize this, we consider, in particular, the following gap-dependent notion of misspecification defined in Liu et al. (2023a). ", "page_idx": 8}, {"type": "text", "text": "Assumption 1 (Gap-Dependent Misspecification (Liu et al., 2023a)). There exists some $\\theta\\in\\ensuremath{\\mathbb{R}}^{d}$ Such thatsome $\\rho>0$ denoting $\\Delta(a)=\\operatorname*{max}_{a^{\\prime}}\\,f(a^{\\prime})-f(a)$ wehavefor any $a\\in A$ ", "page_idx": 8}, {"type": "equation", "text": "$$\n|f(a)-a^{\\top}\\theta|\\leq\\rho\\cdot\\Delta(a).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We let $\\mathcal{M}^{\\star}$ dentethriginlronentwihrwadftn $f(a)$ (with ${\\mathrm{Reg}}_{T}^{\\mathcal{M}^{\\star}}$ the coresponding regret), and $\\mathcal{M}_{0}$ the evironment with linear reward, $a^{\\top}\\theta$ (with $\\mathrm{Reg}_{T}^{\\mathcal{M}_{0}}$ the corresponding regret). ", "page_idx": 8}, {"type": "text", "text": "Assumption 1 allows the reward to be misspecified, but the misspecification level for an action scales with how suboptimal that action is. This could correspond to real-world settings where, for example, significant attention has been given to modeling near-optimal behavior, such that it is accurately represented within our model class, but much less attention has been given to modeling suboptimal behavior. We assume access to a generic corruption-robust algorithm. ", "page_idx": 8}, {"type": "text", "text": "Assumption 2. We have access to a regret minimization algorithm which takes as input some $C^{\\prime}$ and withprobabilityat least $1-\\delta$ hasregretboundedon $\\mathbf{\\mathcal{M}}_{0}$ as ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathrm{Reg}_{T}^{\\mathcal{M}_{0}}\\le\\mathcal{C}_{1}(\\delta,T)\\sqrt{T}+\\mathcal{C}_{2}(\\delta,T)C^{\\prime}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "$\\begin{array}{r}{i f C^{\\prime}\\ge C\\triangleq\\sum_{t=1}^{T}\\epsilon^{\\mathrm{mis}}(a_{t})}\\end{array}$ and by $T$ otherwise,for $C$ asdefindabveandforproblem-dpenent) constants $\\mathcal{C}_{1}(\\delta,T),\\mathcal{C}_{2}(\\delta,T)$ which may scale at most logarithmically with $T$ and $\\frac{1}{\\delta}$ ", "page_idx": 8}, {"type": "text", "text": "Assumption 2 is essentially the guarantee of a corruption-robust algorithm in terms of strong corruption measure (defined in Section 3). Note, in particular, that Assumption 2 only needs to obtain a sub-linear regret guarantee in the known-corruption setting, and can have linear regret in the setting where the corruption level is unknown. We then have the following result. ", "page_idx": 8}, {"type": "text", "text": "Theorem 6.1. Assume our environment satisfies Assumption 1 and that we have access to a corruptionrobust algorithm satisfying Assumption 2. Then as long as $\\begin{array}{r}{\\rho\\le\\operatorname*{min}\\{\\frac{1}{2},\\frac{1}{4}\\mathcal{C}_{2}(\\frac{\\delta}{T},T)^{-1}\\}}\\end{array}$ withprobabilityatleast $1-2\\delta\\,w e$ canachieveregretboundedas: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{Reg}_{T}^{\\mathcal{M}^{\\star}}\\leq6\\mathcal{C}_{1}(\\frac{\\delta}{T},T)\\sqrt{T}+4\\sqrt{2T\\log(1/\\delta)}+4.}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Theorem 6.1 states that, assuming our environment exhibits gap-dependent misspecification with tolerance $\\rho\\le\\operatorname*{min}\\{\\frac{1}{2},\\frac{1}{4}{\\mathcal C}_{2}(\\frac{\\delta}{T},T)^{\\bar{-}1}\\}$ , then we can achieve regret on the true environment bounded as the leading-order term of our corruption-robust oracle, $\\mathcal{C}_{1}\\big(\\frac{\\delta}{T},T\\big)\\sqrt{T}$ , with additional overhead of only $\\tilde{O}(\\sqrt{T})$ . This reduction is almost entirely black-box: it requires knowledge of $\\mathcal{C}_{1}(\\delta,T)$ and $\\mathcal{C}_{2}(\\delta,T)$ but does not require knowledge of $\\rho$ Or any other facts about the corruption-robust algorithm. ", "page_idx": 8}, {"type": "text", "text": "Remark 1 (Anytime Algorithm). The oracle of Assumption $^{5}$ must be anytime, achieving the above regretguaranteefor any $T$ not given as an input. Though many existing corruption-robust algorithms take $T$ as input, the standard doubling trick can convert them into an anytime algorithm. ", "page_idx": 8}, {"type": "text", "text": "6.1  Optimal Misspecification Rate for Linear Bandits ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We are particularly interested in how stringent a condition on the misspecification level\u2014how small avalue of $\\rho$ Theorem 6.1 requires. As we have shown, Theorem 4.1 obtains the optimal misspecification level of $d C$ . We then have the following corollary. ", "page_idx": 8}, {"type": "text", "text": "Corollary 6.1.1. Assume our environment is a misspecified linear bandit satisfying Assumption 1 With $\\rho\\leq{\\dot{O}}({\\frac{1}{d\\log T}})$ Then instantiating Assumption 2 with the algorithm of Theorem 4.1, we can achieve regret bounded with probability $1-\\delta$ $\\mathrm{Reg}_{T}^{\\mathcal{M}^{\\star}}\\leq\\mathcal{O}(d\\sqrt{T\\log(T/\\delta)})$ ", "page_idx": 9}, {"type": "text", "text": "While the regret bound of Corollary 6.1.1 achieves a scaling of $\\widetilde{\\mathcal{O}}(d\\sqrt{T})$ , which is tight for linear bandits (Lattimore and Szepesvari, 2020), it is unclear its requirement on $\\rho$ of $\\rho\\leq\\widetilde O(\\frac{1}{d})$ is optimal. The result below shows that it is not optimal because $\\rho\\leq{\\mathcal{O}}({\\frac{1}{\\sqrt{d}}})$ suffices for $\\widetilde{\\mathcal{O}}(d\\sqrt{T})$ regret. ", "page_idx": 9}, {"type": "text", "text": "Theorem 6.2. Assume our environment is a misspecified linear bandit satisfying Assumption $^{\\,l}$ With $\\rho\\leq{\\mathcal{O}}({\\frac{1}{\\sqrt{d}}})$ . Then there exists an algorithm that achieves, w.p. $1-\\delta$ $\\mathrm{Reg}_{T}^{\\mathcal{M}^{\\star}}\\leq\\mathcal{O}(d\\sqrt{T\\log(T/\\delta)})$ ", "page_idx": 9}, {"type": "text", "text": "Theorem 6.2 relies on a specialized algorithm for the gap-dependent misspecification setting, and improves on the best-known bound for gap-dependent misspecification in linear bandits, which requires p \u2264 (l) (Liu et al., 2023a). Moreover, for p > cr Va 1 for some logarithmic term $c_{T}$ adapting the lower-bound instance from Lattimore et al. (2020), we show that achieving sub-linear regretis nthrs $\\textstyle\\rho\\approx{\\frac{1}{\\sqrt{d}}}$ is the best $\\rho$ we can hope for. This disproves the conjecture of Liu et al. (2023a) that $\\rho=\\Theta(1)$ is possible. ", "page_idx": 9}, {"type": "text", "text": "Note that the reduction in Theorem 6.1 is not able to achieve a tight $\\rho$ while reducing from gapdependent misspecification to corruption allows for black-box usage of existing algorithms, it requires more stringent conditions on the misspecification level than specialized algorithms for this setting. ", "page_idx": 9}, {"type": "text", "text": "6.2  Gap-Dependent Misspecification in Reinforcement Learning ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Theorem 6.1 is a corollary of a more general result, Theorem L.1, which applies to misspecified reinforcement learning, where we there assume a generalized notion of gap-dependent misspecification: foreach poliy $\\pi$ $\\begin{array}{r}{\\mathrm{,}\\mathbb{E}^{\\mathcal{M}^{\\star},\\pi}[\\sum_{h=1}^{H}\\epsilon_{h}^{\\mathrm{mis}}(s_{h},a_{h})]\\leq\\rho\\!\\cdot\\!(V_{0}^{\\star}-V_{0}^{\\pi})}\\end{array}$ for $V_{0}^{\\pi}$ thexpectedreward ofpoliey $\\pi$ and $\\epsilon_{h}^{\\mathrm{mis}}(s,a)$ a measure of the misspecification at step $h$ , state $s$ , and action $a$ To illustrate this general reduction, we consider the following setting, a generalization of linear MDPs (Jin et al., 2020). ", "page_idx": 9}, {"type": "text", "text": "Assumption 3 (Gap-Dependent Misspecified Linear MDPs). Let $\\phi(s,a):S\\times A\\rightarrow\\mathbb{R}^{d}$ denote some feature map and $\\pmb{\\mu}_{h}\\cdot\\pmb{S}\\,\\rightarrow\\,\\mathbb{R}^{d}$ some measure which satisfy $\\|\\phi(s,a)\\|_{2}\\;\\leq\\;1,\\forall s,a,$ and $\\begin{array}{r}{\\|\\int_{s}|\\mathrm{d}\\mu_{h}(s)|\\|_{2}\\leq\\sqrt{d}.}\\end{array}$ Assume that the transitions $P_{h}(\\cdot\\mid s,a)$ on our true environment satisfy: ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\Vert P_{h}(\\cdot\\mid s,a)-\\langle\\phi(s,a),\\mu_{h}(\\cdot)\\rangle\\Vert_{\\mathrm{TV}}\\leq\\epsilon_{h}^{\\mathrm{mis}}(s,a)\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "for some $\\epsilon_{h}^{\\mathrm{mis}}(s,a)\\geq0$ and $\\|P-Q\\|_{\\mathrm{TV}}$ thetotalvariationdistancebetween $P$ and $Q$ .Furthermore, assume that for any policy $\\pi$ we have $\\begin{array}{r}{\\mathbb{E}^{\\pi}[\\sum_{h=1}^{H}\\epsilon_{h}^{\\mathrm{mis}}(s_{h},a_{h})]\\leq\\rho\\cdot(V_{0}^{\\star}-V_{0}^{\\pi}).}\\end{array}$ ", "page_idx": 9}, {"type": "text", "text": "We then have the following result. ", "page_idx": 9}, {"type": "text", "text": "Corollary 6.2.1. Assume our environment satisfies Assumption 3 with $\\rho\\leq{\\widetilde{\\mathcal{O}}}({\\frac{1}{d H}})$ Then there exists an algorithm that achieves regret bounded with probability $1-\\delta$ as $\\mathrm{Reg}_{T}^{\\mathcal{M}^{\\star}}\\le\\widetilde{\\mathcal{O}}(\\sqrt{d^{3}H^{2}T})$ ", "page_idx": 9}, {"type": "text", "text": "To the best of our knowledge, Corollary 6.2.1 is the first result showing that it is possible to efficiently learn in linear MDPs with gap-dependent misspecification. Note that under Assumption 3, our MDP could be far from a linear MDP--we simply assume that if we play a \u201cgood\" policy, it appears as approximately linear. This result is almost immediate by instantiating our reduction with a known corruption-robust algorithm for linear MDPs (Ye et al., 2023). ", "page_idx": 9}, {"type": "text", "text": "7 Open Problems ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "It remains open how to achieve $d{\\sqrt{T}}+d C$ regret in corrupted adversarial linear bandits. The tight $C_{\\infty}$ bound for corrupted linear contextual bandits, where the action set can be chosen by an adaptive adversary in every round, also remains open. The best known upper and lower bounds for this setting are $\\tilde{\\mathcal{O}}(d\\sqrt{T}+d C_{\\infty})$ by He et al. (2022) and $\\Omega(d\\sqrt{T}+\\sqrt{d}C_{\\infty})$ by Lattimore and Szepesvari (2020). ", "page_idx": 9}, {"type": "text", "text": "With the AA viewpoint in Section 3, our work first shows the separation between the achievable regret under weak adversary and strong adversary in corrupted linear bandits. An interesting future direction is to investigate similar separation in general decision making (Foster et al., 2021). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Abernethy, J. D., Hazan, E., and Rakhlin, A. (2008). Competing in the dark: An efficient algorithm for bandit linear optimization. In COLT, pages 263-274. Citeseer.   \nAuer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R. E. (2002). The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48-77.   \nAuer, P. and Chiang, C.-K. (2016). An algorithm with nearly optimal pseudo-regret for both stochastic and adversarial bandits. In Conference on Learning Theory.   \nBogunovic, I, Krause, A., and Scarlett, J. (2020). Corruption-tolerant gaussian process bandit optimization. In International Conference on Artificial Intelligence and Statistics, pages 1071- 1081. PMLR.   \nBogunovic, I., Li, Z., Krause, A., and Scarlett, J. (2022). A robust phased elimination algorithm for corruption-tolerant gaussian process bandits. Advances in Neural Information Processing Systems, 35:23951-23964.   \nBogunovic, I, Losalka, A., Krause, A., and Scarlett, J. (2021). Stochastic linear bandits robust to adversarial attacks. In International Conference on Artificial Inteligence and Statistics, pages 991-999. PMLR.   \nBubeck, S., Cesa-Bianchi, N., and Kakade, S. M. (2012). Towards minimax policies for online linear optimization with bandit feedback. In Conference on Learning Theory, pages 41-1. JMLR Workshop and Conference Proceedings.   \nBubeck, S. and Slivkins, A. (2012). The best of both worlds: Stochastic and adversarial bandits. In Conference on Learning Theory.   \nChewi, S. (2023). The entropic barrier is n-self-concordant. In Geometric Aspects of Functional Analysis: Israel Seminar (GAFA) 2020-2022, pages 209-22. Springer.   \nDann, C., Latimore, T., and Brunskill, E. (2017). Unifying pac and regret: Uniform pac bounds for episodic reinforcement learning. Advances in Neural Information Processing Systems, 30.   \nDann, C., Wei, C.-Y., and Zimmert, J. (2023). A blackbox approach to best of both worlds in bandits and beyond. In The Thirty Sixth Annual Conference on Learning Theory, pages 503-5570. PMLR.   \nDu, S. S., Kakade, S. M., Wang, R., and Yang, L. F. (2019). Is a good representation sufficient for sample efficient reinforcement learning? arXiv preprint arXiv: 1910.03016.   \nFoster, D. J, Gentile, C., Mohri, M., and Zimmert, J. (2020). Adapting to misspecification in contextual bandits. Advances in Neural Information Processing Systems, 33:11478-11489.   \nFoster, D. J., Kakade, S. M., Qian, J, and Rakhlin, A. (2021). The statistical complexity of interactive decision making. arXiv preprint arXiv:2112.13487.   \nHajiesmaili, M., Talbi, M. S., Lui, J., Wong, W. S., et al. (2020). Adversarial bandits with corrutions: Regret lower bound and no-regret algorithm. Advances in Neural Information Processing Systems, 33:19943-19952.   \nHe, J., Zhou, D., Zhang, T., and Gu, Q. (2022). Nearly optimal algorithms for linear contextual bandits with adversarial corruptions. Advances in Neural Information Processing Systems, 35:34614- 34625.   \nIto, S. (2021). Parameter-free multi-armed bandit algorithms with hybrid data-dependent regret bounds. In Conference on Learning Theory.   \nIto, S., Hirahara, S., Soma, T., and Yoshida, Y (2020). Tight frst-and second-rder regret bounds for adversarial linear bandits. Advances in Neural Information Processing Systems, 33:2028-2038.   \nIto, S. and Takemura, K. (2023). Best-of-three-worlds linear bandit algorithm with variance-adaptive regret bounds. In The Thirty Sixth Annual Conference on Learning Theory, pages 2653-2677. PMLR.   \nIto, S. and Takemura, K. (2024). An exploration-by-optimization approach to best of both worlds in linear bandits. Advances in Neural Information Processing Systems, 36.   \nJiang, N., Krishnamurthy, A., Agarwal, A., Langford, J., and Schapire, R. E. (2017). Contextual decision processes with low bellman rank are pac-learnable. In International Conference on Machine Learning, pages 1704-1713. PMLR.   \nJin, C., Yang, Z, Wang, Z., and Jordan, M. 1. (2020). Provably effcient reinforcement learning with linear function approximation. In Conference on learning theory, pages 2137-2143. PMLR.   \nJin, T., Liu, J., Rouyer, C., Chang, W., Wei, C.-Y., and Luo, H. (2024). No-regret online reinforcement learning with adversarial losses and transitions. Advances in Neural Information Processing Systems, 36.   \nKong, F., Zhao, C., and Li, S. (2023). Best-of-three-worlds analysis for linear bandits with follow-theregularized-leader algorithm. In The Thirty Sixth Annual Conference on Learning Theory, pages 657-673. PMLR.   \nLattimore, T. and Szepesvari, C. (2020). Bandit algorithms. Cambridge University Press.   \nLatimore, T., Szepesvari, C., and Weisz, G. (2020). Learning with good feature representations in bandits and inri with a generative model. In International conference on machine learning, pages 5662-5670. PMLR.   \nLee, C.-W., Luo, H., Wei, C.-Y., and Zhang, M. (2020). Bias no more: high-probability datadependent regret bounds for adversarial bandits and mdps. Advances in neural information processing systems, 33:15522-15533.   \nLee, C.-W., Luo, H., Wei, C.-Y., Zhang, M., and Zhang, X. (2021). Achieving near instanceoptimality and minimax-optimality in stochastic and adversarial linear bandits simultaneously. In International Conference on Machine Learning, pages 6142-6151. PMLR.   \nLi, Y, Lou, E. Y., and Shan, L. (2019). Stochastic linear optimization with adversarial corruption. arXiv preprint arXiv: 1909.02109.   \nLi, Y. and Yang, L. (2024). On the model-misspecification in reinforcement learning. In International Conference on Artificial Intelligence and Statistics, pages 2764-2772. PMLR.   \nLiu, C., Yin, M., and Wang, Y-X. (2023a). No-regret linear bandits beyond realizability. arXiv preprint arXiv:2302.13252.   \nLiu, H., ei, CY., and Zimmert, J. (2023b). Towards optimal regret in adversarial linear mdps with bandit feedback. arXiv preprint arXiv:2310.11550.   \nLiu, H., Wei, C.-Y., and Zimmert, J. (2024). Bypassing the simulator: Near-optimal adversarial linear contextual bandits. Advances in Neural Information Processing Systems, 36.   \nNesterov, Y. and Nemirovski, A. (1994). Interior-point polynomial algorithms in convex programming. SIAM.   \nNeu, G. and Olkhovskaya, J. (2020). Efficient and robust algorithms for adversarial linear contextual bandits. In Conference on Learning Theory, pages 3049-3068. PMLR.   \nPukelsheim, F. (2006). Optimal design of experiments. SIAM.   \nSeldin, Y. and Lugosi, G. (2017). An improved parametrization and analysis of the $\\exp3{+}{+}$ algorithm for stochastic and adversarial bandits. In Conference on Learning Theory.   \nSeldin, Y. and Slivkins, A. (2014). One practical algorithm for both stochastic and adversarial bandits. In International Conference on Machine Learning.   \nTakemura, K., Ito, S., Hatano, D., Sumita, H, Fukunaga, T., Kakimura, N., and Kawarabayashi, K.-i. (2021). A parameter-free algorithm for misspecified linear contextual bandits. In International Conference on Artificial Intelligence and Statistics, pages 3367-3375. PMLR.   \nWang, R., Salakhutdinov, R. R., and Yang, L. (2020). Reinforcement learning with general value function approximation: Provably efficient approach via bounded eluder dimension. Advances in Neural Information Processing Systems, 33:6123-6135.   \nWei, C.-Y., Dann, C., and Zimmert, J. (2022). A model selection approach for corruption robust reinforcement learning. In International Conference on Algorithmic Learning Theory, pages 1043-1096. PMLR.   \nWei, C.-Y. and Luo, H. (2018). More adaptive algorithms for adversarial bandits. In Conference On LearningTheory.   \nYe, C., Xiong, W., Gu, Q., and Zhang, T. (2023). Corruption-robust algorithms with uncertainty weighting for nonlinear contextual bandits and markov decision processes. In International Conference on Machine Learning, pages 39834-39863. PMLR.   \nZanette, A., Lazaric, A., Kochenderfer, M., and Brunskill, E. (2020). Learning near optimal policies with low inherent bellman error. In International Conference on Machine Learning, pages 10978- 10989.PMLR.   \nZhang, W., He, J., Fan, Z., and Gu, Q. (2023). On the interplay between misspecification and sub-optimality gap in linear contextual bandits. In International Conference on Machine Learning, pages 41111-41132. PMLR.   \nZimmert, J. and Lattimore, T. (2022). Return of the bias: Almost minimax optimal high probability bounds for adversarial linear bandits. In Conference on Learning Theory, pages 3285-3312. PMLR.   \nZimmert, J., Luo, H., and Wei, C.-Y. (2019). Beating stochastic and adversarial semi-bandits optimally and simultaneously. In International Conference on Machine Learning.   \nZimmert, J. and Seldin, Y. (2019). An optimal algorithm for stochastic and adversarial bandits. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 467-475. PMLR. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Appendices ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "ARelated Work 15 ", "page_idx": 13}, {"type": "text", "text": "B  Equivalence Between AA and CM Viewpoints for Strong Corruption 16 ", "page_idx": 13}, {"type": "text", "text": "CThe Case of Unknown $C_{\\infty}$ or $C$ 16 ", "page_idx": 13}, {"type": "text", "text": "D Proof of Proposition 1 16 ", "page_idx": 13}, {"type": "text", "text": "E Proof of Theorem 4.1 17 ", "page_idx": 13}, {"type": "text", "text": "F Proof of Theorem 5.1 20 ", "page_idx": 13}, {"type": "text", "text": "G  Computationally Efficient Algorithm for Adversarial $C_{\\infty}$ Bound 25 ", "page_idx": 13}, {"type": "text", "text": "H Proof of Theorem 5.2 26 ", "page_idx": 13}, {"type": "text", "text": "1  Dimension Reduction for Continuous Exponential Weights 29 ", "page_idx": 13}, {"type": "text", "text": "J  Computationally Efficient Algorithm for Adversarial $C$ Bound 30 ", "page_idx": 13}, {"type": "text", "text": "J.1 Preliminaries for Entropic Barrier 31   \nJ.2 Auxiliary Lemmas. 31   \nJ.3 Regret Analysis . . 32   \nK  Gap-dependent Misspecification 35   \nL  General Reduction from Corruption-Robust Algorithms to Misspecification 38 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "M Auxiliary Lemmas 42 ", "page_idx": 13}, {"type": "text", "text": "A Related Work ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Model Misspecification  Theoretical works on bandits or RL often assume that the underlying world is well-specified by a particular model. Algorithms that are purely built on this assumption are vulnerable to potential misspecifications. Therefore, some works, besides proposing the main results, also discuss the case where the model is misspecified, such as Jiang et al. (2017); Jin et al. (2020); Zanette et al. (2020); Wang et al. (2020); Li and Yang (2024). These discussions, however, usually assume that the amount of misspecification has a uniform upper bound for all actions / states / policies, and the performance degradation is proportional to this uniform upper bound. ", "page_idx": 14}, {"type": "text", "text": "For settings like stochastic linear bandits and stochastic linear contextual bandits, it was also found that some widely used algorithm such as LinUCB cannot achieve the tightest guarantee under misspecification (Du et al., 2019). Therefore, a line of work developed better algorithms that have optimal robustness against misspecification, such as Lattimore et al. (2020); Foster et al. (2020); Takemura et al. (2021). ", "page_idx": 14}, {"type": "text", "text": "While most work focus on the stochastic setting, Neu and Olkhovskaya (2020) took a first step in studying misspecification in linear contextual bandits with stochastic contexts and adversarial rewards. They established near-optimal regret dependencies on the amount of misspecification. ", "page_idx": 14}, {"type": "text", "text": "Gap-dependent Misspecification  Gap-dependent misspecification is a setting where the amount of misspecification for an action is bounded by a constant times that action's sub-optimality gap. To our knowledge, this setting is first studied by Liu et al. (2023a) for linear bandits. Another related work is Zhang et al. (2023), which assumes that the misspecification is bounded by a constant times the minimal sub-optimality gap among all actions. Although this assumption is more restrictive, they handle the more general linear contextual bandit setting, and derive instance-dependent logarithmic regretbounds. ", "page_idx": 14}, {"type": "text", "text": "Corruption-robust Bandits  The guarantees on model misspecification is rather pessimistic in the sense that if the misspecification is time-varying, and large misspecification only appears in a few rounds, then the existing guarantees for misspeicifcation still scale with the largest misspeicification. To refine such guarantee, previous works have consider different notions of time-varying corruption, and established more fine-grained regret guarantees. These include $C_{\\mathsf{s q},\\infty},C_{\\mathsf{s q}},C_{\\infty}$ and $C$ discussed in Section 3. Among them, $C_{\\mathsf{s q},\\infty}$ and $C_{\\infty}$ are usually studied under the \u201cweak adversary\u201d framework where the adversary decides the corruption before seeing the action chosen by the learner. On the other hand, $C_{\\mathsf{s q}}$ and $C$ are usually studied under the \u201cstrong adversary\u201d framework where the adversary decides the corruption after seeing the action chosen the learner. In Section 3, we provide a unified view for them so that they can both be regarded as weak adversarial setting but with different corruption measure. ", "page_idx": 14}, {"type": "text", "text": "The algorithms of Foster et al. (2020) and Takemura et al. (2021) achieved the optimal bound with respect to $C_{\\mathsf{s q}}$ for stochastic linear contextual bandits (i.e., $d\\sqrt{T}+\\sqrt{d}C_{\\mathsf{s q}})$ , and He et al. (2022) showed the optimal bound with respect to $C$ (i.e., $d\\sqrt{T}+d C)$ . However, it is still unclear whether the tight dependency on $C_{\\infty}$ is $\\sqrt{d}C_{\\infty}$ or $d C_{\\infty}$ . In this paper, we answer it for the context-free linear bandit setting, showing that $d\\sqrt{T}+\\sqrt{d}C_{\\infty}$ is achievable. However, the question remains open for linear contextual bandits. ", "page_idx": 14}, {"type": "text", "text": "For the adversarial setting, Liu et al. (2024) showed $d^{2}\\sqrt{T}+\\sqrt{d}C_{\\mathsf{s q},\\infty}$ bound for linear contextual bandits with stochastic contexts and adversarial rewards, which can be improved to $d\\sqrt{T}+\\sqrt{d}C_{\\mathsf{s q},\\infty}$ when specialized to adversarial linear bandits. To our knowledge, no $C_{\\infty}$ Or $C$ bound has been shown for adversarial linear bandits, and our work make the first attempts on them. ", "page_idx": 14}, {"type": "text", "text": "We remark that for $A$ -armed adversarial bandits, it is easy to see that $\\sqrt{A T}+C_{\\infty}$ bound is achievable simply by running standard adversarial multi-armed bandit algorithm that handles adaptive adversary (e.g., EXP3.P by Auer et al. (2002)). The work of Hajiesmaili et al. (2020) is the only one that we know to obtain $C$ bound for adversarial bandits. They showed a ${\\sqrt{A T}}+A C$ bound for $A$ -armed bandits, which is tight. ", "page_idx": 14}, {"type": "text", "text": "Best-of-both-worlds Bounds  The study of the best-of-both-world problem was initiated by Bubeck and Slivkins (2012) and extended by Seldin and Slivkins (2014); Auer and Chiang (2016); Seldin and Lug0si (2017); Wei and Luo (2018); Zimmert and Seldin (2019); Zimmert et al. (2019); Ito (2021); Ito and Takemura (2023, 2024); Dann et al. (2023); Kong et al. (2023). The g0al of this line of work is to have a single algorithm that achieves a $\\widetilde{\\mathcal{O}}(\\sqrt{T})$ regret when the reward is adversarial and ${\\mathcal{O}}(\\log T)$ when the reward is stochastic, without knowing the type of reward in advance. These results should be viewed as refinements of the standard adversarial setting but not the corruption setting considered in our work, though they also used the term \u201ccorruption\" in their work. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "For example, Lee et al. (2021); Ito and Takemura (2024, 2023, 2024); Dann et al. (2023); Kong et al. (2023) studied the best-of-both-world linear bandits problem. The underlying world could be stochastic $\\begin{array}{r}{\\theta_{t}=\\theta^{\\star}}\\end{array}$ for all $t$ ) or adversarial ( $\\theta_{t}$ 's are arbitrary). Their algorithm achieves a bound of $\\mathcal{O}(d^{2}\\log(\\dot{T})/\\Delta)$ in the former case, where $\\Delta$ is the reward gap between the best and the second-best arm, and $\\widetilde{\\mathcal{O}}(d\\sqrt{T})$ in the latter phase. They also define the corruption $\\begin{array}{r}{C^{\\prime}=\\sum_{t}\\operatorname*{max}_{a}|a^{\\top}(\\theta_{t}-\\theta^{\\star})|}\\end{array}$ and show that their algorithm achieves a regret of $\\mathcal{O}(d^{2}\\log(T)/\\Delta+\\sqrt{d^{2}\\log(T)C^{\\prime}/\\Delta})$ . Compared to our setting, their corruption is in a more limited form, but their target regret bound in the stochastic setting is tighter than ours. ", "page_idx": 15}, {"type": "text", "text": "B   Equivalence Between AA and CM Viewpoints for Strong Corruption ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We show that strong corruption in both definitions is equivalent, that is, for any adversary having strong corruption $\\bar{C}=\\sum_{t}\\bar{|}\\epsilon_{t}|$ from AA viewpoint, there exists an adversary using the equal amount of strong corruption $\\textstyle\\sum_{t}|\\bar{\\epsilon}_{t}^{\\prime}(a_{t})|$ from CM viewpoint, where $|\\epsilon_{t}|=|\\epsilon_{t}^{\\prime}(a_{t})|$ for ali $t$ , and vice versa. ", "page_idx": 15}, {"type": "text", "text": "Assume that $\\epsilon(H_{t-1},a_{t})$ is the function used by an AA strong adversary to decide the corruption at time $t$ ,where $H_{t-1}$ is the history up to time $t-1$ and $a_{t}$ is the chosen action at time $t$ . Then we define $\\epsilon_{t}^{\\prime}(a)\\triangleq\\epsilon(H_{t-1},a),\\forall a$ for the CM viewpoint, thus $|\\epsilon_{t}|=|\\epsilon(H_{t-1},a_{t})|=|\\epsilon_{t}^{\\prime}(a_{t})|$ Note that the function $\\epsilon_{t}^{\\prime}(\\cdot)$ only depends on the history up to time $t-1$ , so the definition of $\\epsilon_{t}^{\\prime}$ is known to adversary before observing $a_{t}$ . The other direction of this equivalence is achieved by setting the corruption in AA viewpoint as $\\epsilon_{t}=\\epsilon_{t}^{\\prime}(a_{t})$ . Note that since $a_{t}$ is known to a strong adversary in AA viewpoint, $\\epsilon_{t}$ is also known. ", "page_idx": 15}, {"type": "text", "text": "C The Case of Unknown $C_{\\infty}$ or $C$ ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In the corrupted stochastic setting, Wei et al. (2022) developed a black-box reduction that can turn any algorithm achieving $\\beta_{1}\\sqrt{T}+\\beta_{2}+\\beta_{3}C_{\\infty}$ regret with the knowledge of $C_{\\infty}$ into an algorithm achieving $\\log(T)\\times(\\beta_{1}\\sqrt{T}+\\beta_{2}+\\beta_{3}C_{\\infty})$ regret without knowledge of $C_{\\infty}$ . This reduction can be directly applied to our stochastic $C_{\\infty}$ bound result (Theorem 4.1), which allows us to achieve almost the same regret bound without knowledge of $C_{\\infty}$ . The idea of Wei et al. (2022) has been extended to the adversarial setting by Jin et al. (2024) (see their Section 4). Similarly, for the adversarial setting, one can turn any algorithm achieving $\\beta_{1}\\sqrt{T}+\\beta_{2}+\\beta_{3}C_{\\infty}$ regret with known $C_{\\infty}$ into one achieving $\\log(T)\\times(\\beta_{1}\\sqrt{T}+\\beta_{2}+\\beta_{3}C_{\\infty})$ regret without knowing $C_{\\infty}$ . This can be directly applied to our adversarial $C_{\\infty}$ result (Theorem 5.1). ", "page_idx": 15}, {"type": "text", "text": "The case of unknown $C$ is quite different. It has been proven by Bogunovic et al. (2021) that it is impossible to achieve a bound that has linear scaling in $C$ (e.g., $\\bar{\\beta_{1}\\sqrt{T}}+\\beta_{2}+\\beta_{3}C)$ for all $C$ simultaneously if $C$ is not known by the learner. This is also mentioned in He et al. (2022) again. Hence, almost all previous work studying $C$ bound assumes knowledge on $C$ . If $C$ is unknown, simply setting $\\bar{C}=\\sqrt{T}$ as an upper bound of $C$ yields a bound of $O(\\sqrt{T}+C^{2})$ \u2014if $C\\leq{\\sqrt{T}}$ indeed holds, then $\\bar{C}$ is a correct upper bound, so the regret can be bounded by $\\mathcal{O}({\\sqrt{T}}+{\\bar{C}})=\\mathcal{O}({\\sqrt{T}})$ if $C>\\sqrt{T}$ , then simply bound the regret by $T\\leq\\mathcal{O}(C^{2})$ ", "page_idx": 15}, {"type": "text", "text": "D Proof of Proposition 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "First, we argue that there exists a deterministic algorithm achieving $\\tilde{\\mathcal{O}}(d\\sqrt{T}+\\sqrt{d}C_{\\mathsf{s q}})$ upperbound. The algorithm of Takemura et al. (2021) is such an algorithm, although they only showed an upper boundof $\\tilde{\\mathcal{O}}(d\\sqrt{T}+\\sqrt{d}C_{\\sf m s})$ .To argue the stronger $\\tilde{\\mathcal{O}}(d\\sqrt{T}+\\sqrt{d}C_{\\mathsf{s q}})$ bound, we only need to slightly modify their analysis: In their proof of Lemma 2 (in their Page 6), the original proof bound the per-step regret due to the misspecification as the following (the calculation below uses their original notation): ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{\\tau\\in\\Psi_{t,s}}\\epsilon_{\\tau}(i_{\\tau})x_{\\tau}(i_{\\tau})^{\\top}V_{t-1,s}^{-1}x_{t}(i)\\Bigg|\\leq\\epsilon\\sqrt{|\\Psi_{t,s}|\\sum_{\\tau\\in\\Psi_{t,s}}\\left(x_{\\tau}(i_{\\tau})^{\\top}V_{t-1,s}^{-1}x_{t}(i)\\right)^{2}},}\\\\ &{}&{\\leq\\epsilon\\sqrt{|\\Psi_{t,s}|x_{t}(i)^{\\top}V_{t-1,s}^{-1}x_{t}(i)}}\\\\ &{}&{\\leq\\epsilon\\sqrt{|\\Psi_{t,s}|c^{-2s}}}\\\\ &{}&{\\leq\\tilde{\\mathcal{O}}(\\epsilon\\sqrt{d}).}\\end{array}\\qquad\\mathrm{(by~their)}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Lemma 1) ", "page_idx": 16}, {"type": "text", "text": "We can tighten their analysis by doing the following: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{\\tau\\in\\Psi_{t,\\tau}}\\epsilon_{\\tau}(i_{\\tau})\\boldsymbol{x}_{\\tau}(i_{\\tau})^{\\top}V_{t-1,s}^{-1}\\boldsymbol{x}_{t}(i)\\Bigg|\\leq\\sqrt{\\left(\\displaystyle\\sum_{\\tau\\in\\Psi_{t,s}}\\epsilon_{\\tau}(i_{\\tau})^{2}\\right)\\left(\\displaystyle\\sum_{\\tau\\in\\Psi_{t,s}}\\left(x_{\\tau}(i_{\\tau})^{\\top}V_{t-1,s}^{-1}x_{t}(i)\\right)^{2}\\right)},}&{}\\\\ {\\leq\\sqrt{\\left(\\displaystyle\\sum_{\\tau\\in\\Psi_{t,s}}\\epsilon_{\\tau}(i_{\\tau})^{2}\\right)x_{t}(i)^{\\top}V_{t-1,s}^{-1}x_{t}(i)}}&{}\\\\ {\\leq\\sqrt{\\left(\\displaystyle\\sum_{\\tau\\in\\Psi_{t,s}}\\epsilon_{\\tau}(i_{\\tau})^{2}\\right)^{c-2}}}&{}\\\\ {\\leq\\tilde{\\sigma}\\left(\\sqrt{\\displaystyle\\prod_{\\tau\\in\\Psi_{t,s}}\\sum_{\\tau\\in\\Psi_{t,s}}\\epsilon_{\\tau}(i_{\\tau})^{2}}\\right).}&{\\qquad\\mathrm{(by~their~Lemma~1~}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Since the regret for every step in $\\Psi_{t,s}$ can be bounded by this value, when summing the regret over $\\Psi_{T+1,s}$ , one can get a regret of order ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\tilde{\\mathcal{O}}\\left(\\sqrt{d\\vert\\Psi_{T+1,s}\\vert\\sum_{\\tau\\in\\Psi_{T+1,s}}\\epsilon_{\\tau}(i_{\\tau})^{2}}\\right).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Further summing this over $s$ (there are logarithmically many different $s$ )and using that $\\left[T\\right]\\;=$ $\\bigcup_{s}\\Psi_{T+1,s}$ and using Cauchy-Schwarz, we get a $\\begin{array}{r}{\\sqrt{d T\\sum_{t=1}^{T}\\epsilon_{t}(i_{t})^{2}}=\\sqrt{d}C_{\\mathsf{s q}}}\\end{array}$ bound. ", "page_idx": 16}, {"type": "text", "text": "To argue that any deterministic algorithm must suffer at least $\\Omega(d\\sqrt{T}+d C_{\\infty})$ regret, we only need to use the lower bound instance of $\\Omega(d{\\sqrt{T}}+d C)$ .At the beginning of round $t$ , the adversary simply change the corruptions $\\epsilon_{t}(a)$ to be zero for all $a\\neq a_{t}$ (the adversary knows what $a_{t}$ since the algorithm is deterministic). This makes $C=C_{\\infty}$ , and thus the lower bound $\\Omega(d\\sqrt{T}+d C_{\\infty})$ holds. ", "page_idx": 16}, {"type": "text", "text": "E Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Lemma E.1. With probability at least $1-2\\delta$ forall $k$ and for all $b\\in\\mathcal{A}_{k}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n|\\langle b,\\widehat{\\theta}_{k}-\\theta^{\\star}\\rangle|\\leq4\\sqrt{\\frac{d\\log(|A|T/\\delta)}{m_{k}}}+\\frac{\\operatorname*{min}\\{\\sqrt{d}C^{\\prime},d C\\}}{m_{k}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. Let $\\mathbb{E}_{t}[\\cdot]$ be the expectation conditioned on the history up to round $t-1$ .Wefix $k$ and $b$ and consider ", "page_idx": 16}, {"type": "equation", "text": "$$\nX_{t}=b^{\\top}(m_{k}G_{k})^{-1}a_{t}r_{t}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "for $t\\in\\mathcal{T}_{k}$ . Notice that $\\begin{array}{r}{\\sum_{t\\in\\mathbb{Z}_{k}}X_{t}=b^{\\top}\\widehat{\\theta}_{k}}\\end{array}$ and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\displaystyle\\sum_{t\\in\\mathbb{Z}_{k}}\\mathbb{E}_{t}\\left[X_{t}\\right]=\\sum_{t\\in\\mathbb{Z}_{k}}b^{\\top}(m_{k}G_{k})^{-1}\\mathbb{E}_{t}\\left[a_{t}\\big(a_{t}^{\\top}\\theta^{\\star}+\\epsilon_{t}(a_{t})\\big)\\right]}&\\\\ &{\\displaystyle~~}&{\\displaystyle~~=b^{\\top}\\theta^{\\star}+b^{\\top}(m_{k}G_{k})^{-1}\\sum_{t\\in\\mathbb{Z}_{k}}\\mathbb{E}_{t}\\left[a_{t}\\epsilon_{t}(a_{t})\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Also, by the defnition of $G_{k}$ we have $\\begin{array}{r}{|X_{t}|=\\left|b^{\\top}(m_{k}G_{k})^{-1}a_{t}r_{t}\\right|\\leq\\frac{1}{m_{k}}\\left\\|b\\right\\|_{G_{k}^{-1}}\\left\\|a_{t}\\right\\|_{G_{k}^{-1}}\\leq\\frac{d}{m_{k}}.}\\end{array}$ Thus, by Freedman's inequality, with probability at least TAilr , the following holds: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left\\langle b,\\widehat{\\theta}_{k}-\\theta^{\\star}\\right\\rangle\\Big|=\\displaystyle\\left|\\sum_{t\\in\\mathbb{Z}_{k}}X_{t}-\\sum_{t\\in\\mathbb{Z}_{k}}\\mathbb{E}_{t}\\left[X_{t}\\right]\\right|+\\left|b^{\\top}(m_{k}G_{k})^{-1}\\sum_{t\\in\\mathbb{Z}_{k}}\\mathbb{E}_{t}\\left[a_{t}\\epsilon_{t}(a_{t})\\right]\\right|}\\\\ &{\\qquad\\qquad\\leq\\underbrace{\\sqrt{\\log\\left(\\frac{|A|T}{\\delta}\\right)_{t\\in\\mathbb{Z}_{k}}\\mathbb{E}_{t}\\left[X_{t}^{2}\\right]}}_{\\mathrm{term}_{1}}+\\frac{d}{m_{k}}\\log\\left(\\frac{|A|T}{\\delta}\\right)+\\underbrace{\\left|b^{\\top}(m_{k}G_{k})^{-1}\\sum_{t\\in\\mathbb{Z}_{k}}\\mathbb{E}_{t}\\left[a_{t}\\epsilon_{t}(a_{t})\\right]\\right|}_{\\mathrm{term}_{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We bound term $^1$ by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle{\\bf t e r m}_{1}=\\sqrt{\\log(|{\\cal A}|T/\\delta)\\sum_{t\\in\\mathcal{Z}_{k}}\\mathbb{E}_{t}\\left[b^{\\top}(m_{k}G_{k})^{-1}a_{t}a_{t}^{\\top}(m_{k}G_{k})^{-1}b\\right]}}\\ ~}\\\\ {{=\\sqrt{\\log(|{\\cal A}|T/\\delta)\\frac{1}{m_{k}^{2}}\\sum_{t\\in\\mathcal{Z}_{k}}b^{\\top}G_{k}^{-1}b}\\leq\\sqrt{\\frac{d\\log(|{\\cal A}|T/\\delta)}{m_{k}}},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and term $^2$ by ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbf{term}_{2}=\\frac{1}{m_{k}}\\left\\vert\\sum_{t\\in\\mathcal{T}_{k}}\\sum_{a\\in\\mathcal{A}_{k}}p_{k}(a)\\epsilon_{t}(a)b^{\\top}G_{k}^{-1}a\\right\\vert}\\\\ {\\displaystyle\\leq\\frac{1}{m_{k}}\\sum_{t\\in\\mathcal{T}_{k}}\\sqrt{\\sum_{a\\in\\mathcal{A}_{k}}p_{k}(a)\\epsilon_{t}(a)^{2}}\\sqrt{\\sum_{a\\in\\mathcal{A}_{k}}p_{k}(a)\\left(b^{\\top}G_{k}^{-1}a\\right)^{2}}}\\\\ {\\displaystyle\\leq\\frac{1}{m_{k}}\\sum_{t\\in\\mathcal{T}_{k}}\\operatorname*{max}_{a^{\\prime}}\\epsilon_{t}(a^{\\prime})\\sqrt{d}\\leq\\frac{\\sqrt{d}C^{\\prime}}{m_{k}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "or ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{term}_{2}\\leq\\frac{1}{m_{k}}\\sum_{t\\in\\mathcal{T}_{k}}\\mathbb{E}_{t}\\left[\\epsilon_{t}(a_{t})\\left|b^{\\top}G_{k}^{-1}a_{t}\\right|\\right]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{m_{k}\\ \\overset{\\mathrm{in}}{\\leq}\\ \\alpha_{k}}\\\\ &{\\leq\\frac{1}{m_{k}}\\sum_{\\ell\\in\\{a_{k}\\}}\\vert\\psi(G_{k}^{-1}u_{\\ell})+\\frac{1}{m_{k}}\\sqrt{\\log\\left(\\frac{\\left|A\\right|T}{\\delta}\\right)}\\sum_{\\ell\\in\\{a_{k}\\}}\\left[e(\\alpha_{\\ell})^{2}\\vert\\sqrt{\\alpha_{\\ell}^{-1}a_{\\ell}}\\vert\\right]^{2}}\\\\ &{\\qquad\\qquad+\\frac{\\vert e(\\alpha_{\\ell})\\vert\\sqrt{\\alpha_{k}^{-1}a_{\\ell}}\\vert}{m_{k}}\\log\\left(\\frac{\\left|A\\right|T}{\\delta}\\right)}\\\\ &{\\leq\\frac{d}{m_{k}}\\sum_{\\ell\\in\\{a_{k}\\}}\\frac{1}{m_{k}}\\sqrt{\\log\\left(\\frac{\\left|A\\right|T}{\\delta}\\right)}\\sum_{\\ell\\in\\ {a_{k}\\}}\\vert\\sqrt{\\mathcal{C}_{\\ell}^{-1}a_{\\ell}\\alpha_{k}^{-1}a_{\\ell}^{2}\\alpha_{k}^{-1}b_{\\ell}}+\\frac{d}{m_{k}}\\log\\left(\\frac{\\left|A\\right|T}{\\delta}\\right)}\\\\ &{\\leq\\frac{d\\mathcal{C}}{m_{k}}+\\frac{1}{m_{k}}\\sqrt{\\log\\left(\\frac{\\left|A\\right|T}{\\delta}\\right)}\\sum_{\\ell\\in\\ {a_{k}\\}}\\eta_{\\ell}\\ G_{k}^{-1}b_{\\ell}\\ \\frac{d}{m_{k}}\\log\\left(\\frac{\\left|A\\right|T}{\\delta}\\right)}\\\\ &{\\leq\\frac{d\\mathcal{C}}{m_{k}}+\\sqrt{\\frac{\\log\\left(\\vert A\\vert T/\\delta\\right)}{m_{k}}}+\\frac{d}{m_{k}}\\log\\left(\\frac{\\left|A\\right|T}{\\delta}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus, for any b  Ak, with probabilityat least 1 - T ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left\\langle b,\\widehat{\\theta}_{k}-\\theta^{\\star}\\right\\rangle\\right|\\leq2\\sqrt{\\frac{d\\log(|\\mathcal{A}|T/\\delta)}{m_{k}}}+\\frac{2d}{m_{k}}\\log(|\\mathcal{A}|T/\\delta)+\\frac{1}{m_{k}}\\operatorname*{min}\\left\\{\\sqrt{d}C^{\\prime},d C\\right\\}}\\\\ &{\\qquad\\qquad\\qquad\\leq4\\sqrt{\\frac{d\\log(|\\mathcal{A}|T/\\delta)}{m_{k}}}+\\frac{1}{m_{k}}\\operatorname*{min}\\left\\{\\sqrt{d}C^{\\prime},d C\\right\\}.\\qquad(m_{k}\\geq d\\log(|\\mathcal{A}|/\\delta))}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Taking a union bound over $k$ and $b\\in\\mathcal{A}_{k}$ finishes the proof. ", "page_idx": 18}, {"type": "text", "text": "Lemma E.2. Let $a^{\\star}=\\mathrm{argmax}_{a\\in A}\\,a^{\\top}\\theta^{\\star}$ . Then with probability at least 1 - 28, $a^{\\star}\\in\\mathcal{A}_{k}$ for all $k$ ", "page_idx": 18}, {"type": "text", "text": "Proof. Suppose that the high-probability event in Lemma E.1 holds. For any $k$ if $a^{\\star}\\in\\mathcal{A}_{k}$ , then for any $b\\in\\mathcal{A}_{k}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{b^{\\top}\\widehat{\\theta}_{k}-a^{\\star^{\\top}}\\widehat{\\theta}_{k}\\leq b^{\\top}\\theta^{\\star}-a^{\\star^{\\top}}\\theta^{\\star}+\\left|b^{\\top}(\\widehat{\\theta}_{k}-\\theta^{\\star})\\right|+\\left|a^{\\star^{\\top}}(\\widehat{\\theta}_{k}-\\theta^{\\star})\\right|}\\\\ {\\leq0+2\\left(4\\sqrt{\\frac{d\\log\\left(|A|T/\\delta\\right)}{m_{k}}}+\\frac{\\operatorname*{min}\\left\\{\\sqrt{d}C^{\\prime},d C\\right\\}}{m_{k}}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "By the definition of $\\boldsymbol{A}_{k+1}$ in Eq. (1), we have $a^{\\star}\\in\\mathcal{A}_{k+1}$ . The lemma is then proven by an induction argument. \u53e3 ", "page_idx": 18}, {"type": "text", "text": "Proof of Theorem 4.1. We first calculate the regret in epoch $k\\ >\\ 1$ assuming that the event in Lemma E.2 holds. ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t\\in\\mathbb{Z}_{k}}\\left(\\operatorname*{max}_{a\\in A}a^{\\top}\\theta^{\\star}-a_{t}^{\\top}\\theta^{\\star}\\right)}\\\\ &{\\leq\\displaystyle\\sum_{t\\in\\mathbb{Z}_{k}}\\left(\\operatorname*{max}_{a\\in A}a^{\\top}\\widehat\\theta_{k-1}-a_{t}^{\\top}\\widehat\\theta_{k-1}\\right)+2m_{k}\\operatorname*{max}_{a\\in A_{k}}\\left\\vert a^{\\top}\\left(\\widehat\\theta_{k-1}-\\theta^{\\star}\\right)\\right\\vert}\\\\ &{\\leq m_{k}\\cdot\\mathcal{O}\\left(\\sqrt{\\frac{d\\log(|A|T/\\delta)}{m_{k-1}}}+\\frac{\\operatorname*{min}\\{\\sqrt{d}C^{\\prime},d C\\}}{m_{k-1}}\\right)}\\\\ &{=\\mathcal{O}\\left(\\sqrt{d m_{k}\\log(|A|T/\\delta)}+\\operatorname*{min}\\{\\sqrt{d}C^{\\prime},d C\\}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Summing this over $k$ and using that $m_{1}=d\\log(|A|T/\\delta)$ ,we get ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\left(\\operatorname*{max}_{a\\in A}a^{\\top}\\theta^{\\star}-a_{t}^{\\top}\\theta^{\\star}\\right)\\leq\\mathcal{O}\\left(\\sqrt{d T\\log(|A|T/\\delta)}+d\\log(|A|T/\\delta)+\\operatorname*{min}\\{\\sqrt{d}C^{\\prime},d C\\}\\log T\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Notice that without loss of generality we can assume $d\\log(|A|T/\\delta)\\leq T$ (otherwise the right-hand side is vacuous). Using this fact gives the desired bound. ", "page_idx": 18}, {"type": "text", "text": "From Exercise 27.6 in Lattimore and Szepesvari (2020), the $\\epsilon_{}$ -covering number of $\\boldsymbol{\\mathcal{A}}$ is bounded by $\\left(\\frac{6d}{\\epsilon}\\right)^{d}$ Let $\\mathcal{C}(\\mathcal{A},\\epsilon)$ be the $\\epsilon$ net of $\\boldsymbol{\\mathcal{A}}$ wethen have $\\begin{array}{r}{\\left|\\mathcal{C}(\\mathcal{A},\\frac{6d}{T})\\right|\\leq T^{d}}\\end{array}$ Thus, when $|{\\mathcal{A}}|\\geq T^{d}$ , we can use $\\begin{array}{r}{\\mathcal{C}(\\mathcal{A},\\frac{6d}{T})}\\end{array}$ as $\\mathcal{A}_{1}$ in Algorithm 1 to conduct phase elimination. In that case, following above proof, wehave ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{=1}^{T}\\left(\\operatorname*{max}_{a\\in\\mathcal{C}(A,\\frac{\\epsilon^{d}}{T})}a^{\\top}\\theta^{\\star}-a_{t}^{\\top}\\theta^{\\star}\\right)\\leq\\mathcal{O}\\left(\\sqrt{d T\\log\\left(\\left|\\mathcal{C}\\left(A,\\frac{6d}{T}\\right)\\right|T/\\delta\\right)}+\\operatorname*{min}\\{\\sqrt{d}C^{\\prime},d C\\}\\log T\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\leq\\mathcal{O}\\left(d\\sqrt{T\\log(T/\\delta)}+\\operatorname*{min}\\{\\sqrt{d}C^{\\prime},d C\\}\\log T\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "From the definition of covering number, there exists a $\\begin{array}{r}{a_{c}^{\\star}\\in\\mathcal{C}(\\mathcal{A},\\frac{6d}{T})}\\end{array}$ such that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{a\\in A}a^{\\top}\\theta^{\\star}-(a_{c}^{\\star})^{\\top}\\theta^{\\star}\\leq\\frac{6d}{T}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{=1}^{T}\\left(\\operatorname*{max}a^{\\top}\\theta^{\\star}-\\operatorname*{max}_{a\\in\\mathcal{C}(4,\\frac{\\theta\\xi}{T})}a^{\\top}\\theta^{\\star}\\right)\\leq\\displaystyle\\sum_{t=1}^{T}\\left(\\operatorname*{max}a^{\\top}\\theta^{\\star}-(a_{c}^{\\star})^{\\top}\\theta^{\\star}\\right)+\\displaystyle\\sum_{t=1}^{T}\\left((a_{c}^{\\star})^{\\top}\\theta^{\\star}-\\operatorname*{max}_{a\\in\\mathcal{C}(4,\\frac{\\theta\\xi}{T})}a^{\\top}\\theta^{\\star}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq6d.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Thus, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\left(\\operatorname*{max}_{a\\in A}a^{\\top}\\theta^{\\star}-a_{t}^{\\top}\\theta^{\\star}\\right)\\leq\\mathcal{O}\\left(d\\sqrt{T\\log(T/\\delta)}+\\operatorname*{min}\\{\\sqrt{d}C^{\\prime},d C\\}\\log T\\right).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "F Proof of Theorem 5.1 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In this section, we use the following notation: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\widehat{\\gamma}_{t}=\\left[\\!\\!\\begin{array}{c c}{0}&{\\frac{1}{2}\\widehat{\\theta}_{t}}\\\\ {\\frac{1}{2}\\widehat{\\theta}_{t}^{\\top}}&{0}\\end{array}\\!\\!\\right],\\qquad D_{t}=\\left[\\!\\!\\begin{array}{c c}{\\alpha B_{t}-\\alpha B_{t-1}}&{0}\\\\ {0}&{0}\\end{array}\\!\\!\\right].\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Algorithm 2 is equivalent to the FTRL update: ", "page_idx": 19}, {"type": "equation", "text": "$$\nH_{t}=\\operatorname*{argmax}_{H\\in\\mathcal{H}}\\left\\{\\left\\langle H,\\sum_{s=1}^{t-1}\\widehat{\\gamma}_{s}+\\sum_{s=1}^{t}D_{s}\\right\\rangle-\\frac{G(H)}{\\eta}\\right\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Algorithm 4 is equivalent to ", "page_idx": 19}, {"type": "equation", "text": "$$\nH_{t}=\\underset{H\\in\\mathcal{H}}{\\mathrm{argmax}}\\left\\lbrace\\sum_{s=1}^{t-1}\\left<H,\\widehat{\\gamma}_{s}+D_{s}\\right\\rbrace-\\frac{G(H)}{\\eta}\\right\\rbrace.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "By the standard analysis for FTRL algorithms (e.g., Theorem 2 in Zimmert and Lattimore (2022)), the regret bounds of Eq. (14) and Eq. (15) are given by the following lemmas, respectively. ", "page_idx": 19}, {"type": "text", "text": "Lemma F.1. The update rule Eq. (14) (Algorithm 2) ensures for any $U\\in{\\mathcal{H}}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\langle U-H_{t},\\widehat\\gamma_{t}\\rangle}\\\\ &{\\displaystyle\\leq\\frac{G(U)-\\operatorname*{min}_{H\\in\\mathcal{H}}G(H)}{\\eta}-\\sum_{t=1}^{T}\\langle U-H_{t},D_{t}\\rangle+\\sum_{t=1}^{T}\\operatorname*{max}_{H\\in\\mathcal{H}}\\left\\{\\langle H-H_{t},\\widehat\\gamma_{t}\\rangle-\\frac{D_{G}(H,H_{t})}{\\eta}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Lemma F.2. The update rule Eq. (15) (Algorithm 4) ensures for any $U\\in{\\mathcal{H}}$ ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\i=1}^{T}\\langle U-H_{t},\\widehat{\\gamma}_{t}\\rangle}\\\\ &{\\leq\\displaystyle\\frac{G(U)-\\operatorname*{min}_{H\\in\\mathcal{H}}G(H)}{\\eta}-\\sum_{t=1}^{T}\\langle U-H_{t},D_{t}\\rangle+\\displaystyle\\sum_{t=1}^{T}\\operatorname*{max}_{H\\in\\mathcal{H}}\\left\\{\\langle H-H_{t},\\widehat{\\gamma}_{t}+D_{t}\\rangle-\\frac{D_{G}(H,H_{t})}{\\eta}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We consider an arbitrary comparator $p_{\\star}\\in\\Delta(A)$ with $u_{\\star}=\\mathbb{E}_{a\\sim p_{\\star}}[a]$ . Define $p=(1-\\gamma)p_{\\star}+\\gamma\\rho$ We have $p\\in\\Delta_{\\gamma}(A)$ , and define $u=\\mathbb{E}_{a\\sim p}[a]$ and $\\widehat{\\boldsymbol{U}}=\\widehat{\\mathbf{Cov}}(\\boldsymbol{p})$ . The regret with respect to $p_{\\star}$ can be ", "page_idx": 19}, {"type": "text", "text": "decomposed as the following: With probability at least $1-\\delta$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{\\mathbb{R}\\mathbf{e}_{\\mathbf{g}_{T}}(p_{*})}&{\\quad\\mathrm{(160~}\\mathrm{)}}\\\\ &{=\\displaystyle\\sum_{t=1}^{T}\\langle u_{*}-a_{t},\\theta_{t}\\rangle}&\\\\ &{=\\displaystyle\\sum_{t=1}^{T}\\langle u-a_{t},\\theta_{t}\\rangle+2\\gamma T}&\\\\ &{\\le\\displaystyle\\sum_{t=1}^{T}\\langle u-a_{t},\\theta_{t}\\rangle+\\mathcal{O}\\Big(\\sqrt{T\\log(1/\\delta)}\\Big)+2\\gamma T}&{\\quad\\mathrm{(Azuma's~inequality)}}\\\\ &{=\\displaystyle\\sum_{t=1}^{T}\\Big\\langle u-x_{t},\\theta_{t}-\\mathbb{E}_{t}[\\widehat{\\theta}_{t}]\\Big\\rangle+\\displaystyle\\sum_{t=1}^{T}\\Big\\langle u-x_{t},\\mathbb{E}_{t}[\\widehat{\\theta}_{t}]-\\widehat{\\theta}_{t}\\Big\\rangle+\\displaystyle\\sum_{t=1}^{T}\\langle U-H_{t},\\widehat{\\gamma}_{t}\\rangle+\\mathcal{O}\\Big(\\sqrt{T\\log(1/\\delta)}}&\\\\ &{\\quad\\quad\\quad\\quad\\quad\\times\\mathrm{Indue}\\quad\\displaystyle\\sum_{t=1}^{T}\\Big\\langle u,\\theta_{t}\\Big\\rangle+\\displaystyle\\sum_{t=1}^{T}\\Big\\langle u-x_{t},\\mathbb{E}_{t}[\\widehat{\\theta}_{t}]-\\widehat{\\theta}_{t}\\Big\\rangle+\\displaystyle\\sum_{t=1}^{T}\\Big\\langle U-H_{t},\\widehat{\\gamma}_{t}\\Big\\rangle+\\mathcal{O}\\Big(\\sqrt{T\\log(1/\\delta)}}&\\\\ &{=\\displaystyle\\sum_{t=1}^{T}\\Big\\langle u-x_{t},\\theta_{t}\\Big\\rangle+\\displaystyle\\sum_{t=1}^{T}\\Big\\langle u-x_{t},\\mathbb{E}_{t}[\\widehat{\\theta}_{t}]-\\widehat{\\theta}_{t}\\Big\\rangle+2\\gamma T}&{\\displaystyle\\{\\sum_{t=1}^{T}\\theta_{t}-\\theta_{t},\\widehat{\\gamma}_{t}\\}+\\mathcal{O}\\Big(\\sqrt{T\\log(1/\\delta)}}&\\\\ &{=\\displaystyle\\sum_{t=1}^{T}\\Big\\langle u-x_{t},\\theta_{t}\\Big\\rangle+\\displaystyle\\sum_{t=1}^{T}\\Big\\langle u-x_{t},\\theta_{t}\\Big\\rangle+\\displaystyle\\sum_{t=1}^{T}\\Big\\langle u-x_{t},\\widehat{\\gamma}_{t}\\Big\\rangle}&{\\quad\\mathrm{(A z \n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "By Lemma F.1, the FTRL term can further be bounded by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "In the following five lemmas, we bound the five terms Bias, Deviation, Penalty, Bonus, and Stability.   \nLemma F.3. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathrm{Bias}\\leq C_{\\infty}\\operatorname*{max}_{t}\\|u\\|_{\\Sigma_{t}^{-1}}+\\sqrt{d}C_{\\infty}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathbb{E}_{t}\\left[\\left\\langle u-x_{t},-\\Sigma_{t}^{-1}a_{t}\\epsilon_{t}(a_{t})\\right\\rangle\\right]\\leq\\mathbb{E}_{t}\\left[\\sqrt{(u-x_{t})^{\\top}\\Sigma_{t}^{-1}a_{t}a_{t}^{\\top}\\epsilon_{t}^{2}(a_{t})\\Sigma_{t}^{-1}(u-x_{t})}\\right]}&{}&\\\\ {\\leq\\sqrt{(u-x_{t})^{\\top}\\Sigma_{t}^{-1}\\mathbb{E}_{t}\\left[a_{t}a_{t}^{\\top}\\epsilon_{t}^{2}(a_{t})\\right]\\Sigma_{t}^{-1}(u-x_{t})}}&{}&\\\\ {\\leq\\epsilon_{t}\\left\\lVert u-x_{t}\\right\\rVert_{\\Sigma_{t}^{-1}}}&{}&\\\\ {\\leq\\epsilon_{t}\\left\\lVert x_{t}\\right\\rVert_{\\Sigma_{t}^{-1}}+\\epsilon_{t}\\left\\lVert u\\right\\rVert_{\\Sigma_{t}^{-1}}}&{}&\\\\ {\\leq\\sqrt{d}\\epsilon_{t}+\\epsilon_{t}\\left\\lVert u\\right\\rVert_{\\Sigma_{t}^{-1}}.}&{}&{\\left(\\Sigma_{t}\\succeq x_{t}x_{t}^{\\top}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Thus, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{Bias}=\\underbrace{\\sum_{t=1}^{T}\\left\\langle u-x_{t},\\theta_{t}-\\mathbb{E}_{t}\\left[\\Sigma_{t}^{-1}a_{t}a_{t}^{\\top}\\theta_{t}\\right]\\right\\rangle}_{=0}+\\mathbb{E}_{t}\\left[\\displaystyle\\sum_{t=1}^{T}\\left\\langle u-x_{t},-\\Sigma_{t}^{-1}a_{t}\\epsilon_{t}(a_{t})\\right\\rangle\\right]}\\\\ &{\\qquad\\le C_{\\infty}\\displaystyle\\operatorname*{max}_{t}\\|u\\|_{\\Sigma_{t}^{-1}}+\\sqrt{d}C_{\\infty}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Lemma F.4. With probability of at least $1-\\delta$ wehave ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathbf{iation}\\leq\\operatorname*{max}_{t}\\|u\\|_{\\Sigma_{t}^{-1}}\\left(12\\sqrt{T\\log(T/\\delta)}+\\frac{12\\sqrt{d}\\log(T/\\delta)}{\\sqrt{\\gamma}}\\right)+12\\sqrt{d T\\log(T/\\delta)}+\\frac{12d\\log(T/\\delta)}{\\sqrt{\\gamma}}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. Notice that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\left\\langle u-x_{t},\\widehat{\\theta}_{t}\\right\\rangle\\right|\\leq\\left|\\left(u-x_{t}\\right)^{\\top}\\Sigma_{t}^{-1}a_{t}\\right|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\left\\Vert u-x_{t}\\right\\Vert_{\\Sigma_{t}^{-1}}\\left\\Vert a_{t}\\right\\Vert_{\\Sigma_{t}^{-1}}}\\\\ &{\\qquad\\qquad\\leq\\displaystyle\\frac{\\sqrt{d}}{\\sqrt{\\gamma}}\\|u-x_{t}\\|_{\\Sigma_{t}^{-1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By the strengthened Freedman's inequality (Lemma M.3), with probability at least $1-\\delta$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathrm{\\mathop{periation}}=\\displaystyle\\sum_{t=1}^{T}\\left\\langle u-x_{t},\\mathbb{E}_{t}[\\widehat{\\theta}_{t}]-\\widehat{\\theta}_{t}\\right\\rangle}\\\\ &{\\leq3\\sqrt{\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{t}\\left[\\left\\langle u-x_{t},\\widehat{\\theta}_{t}\\right\\rangle^{2}\\right]\\log(d^{4}T^{4}/\\delta)}+2\\cdot\\displaystyle\\frac{\\sqrt{d}}{\\sqrt{\\gamma}}\\operatorname*{max}_{t}\\left\\|u-x_{t}\\right\\|_{\\Sigma_{t}^{-1}}\\log(d^{4}T^{4}/\\delta)}\\\\ &{\\leq\\operatorname*{max}_{t}\\left\\|u-x_{t}\\right\\|_{\\Sigma_{t}^{-1}}\\left(12\\sqrt{T\\log(T/\\delta)}+\\displaystyle\\frac{12\\sqrt{d}\\log(T/\\delta)}{\\sqrt{\\gamma}}\\right)}\\\\ &{\\leq\\operatorname*{max}_{t}\\left\\|u\\right\\|_{\\Sigma_{t}^{-1}}\\left(12\\sqrt{T\\log(T/\\delta)}+\\displaystyle\\frac{12\\sqrt{d}\\log(T/\\delta)}{\\sqrt{\\gamma}}\\right)+12\\sqrt{d T\\log(T/\\delta)}+\\displaystyle\\frac{12d\\log(T/\\delta)}{\\sqrt{\\gamma}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Lemma F.5. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{Penalty}\\leq{\\frac{(d+1)\\log(T)}{\\eta}}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof Define Ho = Ea\\~ aT i] By the definition of the feasible set $\\mathcal{H}$ , for any $H\\in{\\mathcal{H}}$ $\\begin{array}{r}{H\\succeq\\gamma H_{0}=\\frac{d+1}{T}H_{0}}\\end{array}$ and $\\pmb{H}\\preceq(d+1)\\pmb{H}_{0}$ . Thus, Penalty can be upper bounded by G(U) - $\\frac{-\\operatorname*{min}_{H\\in\\mathcal{H}}G(H)}{\\eta}\\leq\\frac{G\\left(\\frac{d+1}{T}H_{0}\\right)-G\\left((d+1)H_{0}\\right)}{\\eta}=\\frac{1}{\\eta}\\log\\left(\\frac{\\operatorname*{det}\\left((d+1)H_{0}\\right)}{\\operatorname*{det}\\left(\\frac{d+1}{T}H_{0}\\right)}\\right)=\\frac{(d+1)\\log(T)}{\\eta}$ ", "page_idx": 21}, {"type": "text", "text": "Lemma F.6. ", "text_level": 1, "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathbf{Bonus}\\leq3\\alpha d\\log(T)-\\alpha\\operatorname*{max}_{t}\\|u\\|_{\\Sigma_{t}^{-1}}^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Given $\\mathbb{E}_{a\\sim p_{0}}[a a^{\\top}]\\succeq\\mathbb{E}_{a\\sim p_{0}}[a]\\mathbb{E}_{a\\sim p_{0}}[a]^{\\top}=u u^{\\top}$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\langle U,D_{t}\\rangle=\\left\\langle\\mathbb{E}_{a\\sim p_{0}}[a a^{\\top}],\\alpha B_{T}\\right\\rangle\\geq\\left\\langle u u^{\\top},\\alpha B_{T}\\right\\rangle=\\alpha\\|u\\|_{B_{T}}^{2}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Recall that $B_{1}=\\Sigma_{1}^{-1}$ and for $t\\geq2$ \uff0c ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\Sigma_{t}^{-1}=B_{t-1}^{\\frac{1}{2}}\\left(\\sum_{i=1}^{d}\\lambda_{t i}v_{t i}v_{t i}^{\\top}\\right)B_{t-1}^{\\frac{1}{2}},}\\\\ {\\displaystyle B_{t-1}=B_{t-1}^{\\frac{1}{2}}\\left(\\sum_{i=1}^{d}v_{t i}v_{t i}^{\\top}\\right)B_{t-1}^{\\frac{1}{2}},}\\\\ {\\displaystyle B_{t}=B_{t-1}^{\\frac{1}{2}}\\left(\\sum_{i=1}^{d}\\operatorname*{max}\\{\\lambda_{t i},1\\}v_{t i}v_{t i}^{\\top}\\right)B_{t-1}^{\\frac{1}{2}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(\\sum_{i=1}^{d}v_{t i}v_{t i}^{\\top}=I)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "which ensures $B_{t}\\,\\succeq\\,B_{t-1}$ and $B_{t}\\,\\succeq\\,\\Sigma_{t}^{-1}$ . By induction, it leads to $B_{T}\\succeq\\Sigma_{t}^{-1}$ for any $t$ . This implies ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|u\\|_{B_{T}}^{2}\\geq\\operatorname*{max}_{t}\\|u\\|_{\\Sigma_{t}^{-1}}^{2}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus, $\\begin{array}{r}{\\sum_{t=1}^{T}\\left\\langle U,D_{t}\\right\\rangle\\geq\\alpha\\operatorname*{max}_{t}\\|u\\|_{\\Sigma_{t}^{-1}}^{2}.}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "Next, we upper bound $\\textstyle\\sum_{t=1}^{T}\\left\\langle\\pmb{H}_{t},\\pmb{D}_{t}\\right\\rangle$ . First, notice that $\\langle H_{1},D_{1}\\rangle=\\alpha\\mathrm{Tr}(\\Sigma_{t}B_{t})=\\mathrm{Tr}(I)=\\alpha d$ For $t\\geq2$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\langle H_{t},D_{t}\\rangle=\\alpha\\mathrm{Tr}(\\Sigma_{t}(B_{t}-B_{t-1}))}\\\\ &{=\\alpha\\mathrm{Tr}\\left(B_{t-1}^{-\\frac{1}{1}}\\left(\\displaystyle\\sum_{i=1}^{d}\\lambda_{t i}\\nu_{i t}v_{i i}^{\\top}\\right)^{-1}B_{t-1}^{-\\frac{1}{1}}B_{t-1}^{\\frac{1}{1}}\\left(\\displaystyle\\sum_{i=1}^{d}\\operatorname*{max}\\{\\lambda_{t i}-1,0\\}\\nu_{t i}v_{i i}^{\\top}\\right)B_{t-1}^{\\frac{1}{1}}\\right)}\\\\ &{\\qquad=\\alpha\\mathrm{Tr}\\left(\\left(\\displaystyle\\sum_{i=1}^{d}\\lambda_{t i}\\nu_{t i}v_{i i}^{\\top}\\right)^{-1}\\left(\\displaystyle\\sum_{i=1}^{d}\\operatorname*{max}\\{\\lambda_{t i}-1,0\\}\\nu_{t i}v_{t i}^{\\top}\\right)\\right)}\\\\ &{=\\alpha\\displaystyle\\sum_{i=1}^{d}\\operatorname*{max}\\left\\{1-\\frac{1}{\\lambda_{t i}},0\\right\\}}\\\\ &{\\leq\\alpha\\displaystyle\\sum_{i=1}^{d}\\operatorname*{max}\\left\\{\\log\\lambda_{t i},0\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We also have ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log\\operatorname*{det}\\left(B_{t}\\right)-\\log\\operatorname*{det}\\left(B_{t-1}\\right)}\\\\ &{=\\log\\left(\\frac{\\operatorname*{det}\\Big(B_{t-1}^{\\frac{1}{2}}\\Big)\\operatorname*{det}\\Big(\\sum_{i=1}^{d}\\operatorname*{max}\\{\\lambda_{t i},1\\}v_{t i}v_{t i}^{\\top}\\Big)\\operatorname*{det}\\Big(B_{t-1}^{\\frac{1}{2}}\\Big)}{\\operatorname*{det}\\Big(B_{t-1}^{\\frac{1}{2}}\\Big)\\operatorname*{det}\\Big(\\sum_{i=1}^{d}v_{t i}v_{t i}^{\\top}\\Big)\\operatorname*{det}\\Big(B_{t-1}^{\\frac{1}{2}}\\Big)}\\right)}\\\\ &{=\\log\\left(\\frac{\\operatorname*{det}\\Big(\\sum_{i=1}^{d}\\operatorname*{max}\\{\\lambda_{t i},1\\}v_{t i}v_{t i}^{\\top}\\Big)}{\\operatorname*{det}\\Big(\\sum_{i=1}^{d}v_{t i}v_{t i}^{\\top}\\Big)}\\right)}\\\\ &{=\\sum_{i=1}^{d}\\operatorname*{max}\\{\\log\\lambda_{t i},0\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\left<H_{t},D_{t}\\right>\\leq\\alpha d+\\alpha\\log\\operatorname*{det}\\left(B_{T}\\right)-\\alpha\\log\\operatorname*{det}\\left(B_{1}\\right)\\leq\\alpha d+\\alpha\\log\\operatorname*{det}\\left(B_{T}\\right).\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Finally, we bound $\\log\\operatorname*{det}\\left(B_{T}\\right)$ . Since $\\begin{array}{r}{\\Sigma_{t}=\\sum_{a}p_{t}(a)a a^{\\top}\\succeq\\gamma\\sum_{a}\\rho(a)a a^{\\top}}\\end{array}$ , by Theorem 3 of Bubeck et al. (2012), we have $\\Sigma_{t}\\succeq\\frac{\\gamma}{d}I$ and $\\begin{array}{r}{\\Sigma_{t}^{-1}\\preceq\\frac{d}{\\gamma}I}\\end{array}$ for all $t$ Thus, $\\begin{array}{r}{\\dot{B}_{1}=\\Sigma_{1}^{-1}\\preceq\\frac{d}{\\gamma}I}\\end{array}$ Below, we use induction to show that $\\begin{array}{r}{B_{t}\\preceq\\frac{t d}{\\gamma}I}\\end{array}$ Assume $\\begin{array}{r}{B_{t-1}\\preceq\\frac{(t-1)d}{\\gamma}I}\\end{array}$ Then, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{B_{t}}={B_{t-1}^{\\frac{1}{2}}}\\left(\\displaystyle\\sum_{i=1}^{d}\\operatorname*{max}\\{{\\lambda_{t i}},1\\}{v_{t i}}v_{t i}^{\\top}\\right){B_{t-1}^{\\frac{1}{2}}}}\\\\ &{\\quad\\le{B_{t-1}^{\\frac{1}{2}}}\\left(\\displaystyle\\sum_{i=1}^{d}({\\lambda_{t i}}+1){v_{t i}}v_{t i}^{\\top}\\right){B_{t-1}^{\\frac{1}{2}}}}\\\\ &{\\quad={\\Sigma_{t}^{-1}}+{B_{t-1}}\\preceq\\frac{d}{\\gamma}I+\\displaystyle\\frac{(t-1)d}{\\gamma}I=\\frac{t d}{\\gamma}I.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By induction, we get $\\begin{array}{r}{B_{T}\\preceq\\frac{T d}{\\gamma}I}\\end{array}$ and $\\log\\operatorname*{det}\\left(B_{T}\\right)\\leq2d\\log(T)$ by seting $\\textstyle\\gamma={\\frac{d}{\\sqrt{T}}}$ . Overall, by Eq. (20), we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\left\\langle H_{t},D_{t}\\right\\rangle\\leq3\\alpha d\\log(T).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Combining the upper bound for $\\textstyle\\sum_{t=1}^{T}\\left\\langle\\pmb{H}_{t},\\pmb{D}_{t}\\right\\rangle$ and the lower bound for $\\textstyle\\sum_{t=1}^{T}\\langle\\pmb{U},\\pmb{D}_{t}\\rangle$ finishes the proof. \u53e3 ", "page_idx": 23}, {"type": "text", "text": "Lemma F.7. With probability at least $1-\\delta$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\mathrm{Stability}}\\leq{\\mathcal{O}}\\left(d\\eta T+{\\frac{\\eta d\\log(1/\\delta)}{\\gamma}}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. For any $p$ define $\\mu(p)=\\mathbb{E}_{a\\sim p}[a]$ and ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{Cov}(p)=\\mathbb{E}_{a\\sim p}[(a-\\mu(p))(a-\\mu(p))^{\\top}],\\quad\\widehat{\\mathbf{Cov}}(p)=\\mathbb{E}_{a\\sim p}\\left[\\!\\begin{array}{c c}{\\mathbf{Cov}(p)+\\mu(p)\\mu(p)^{\\top}}&{\\mu(p)}\\\\ {\\mu(p)^{\\top}}&{1}\\end{array}\\!\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For any $H=\\left[\\!\\!{\\begin{array}{c c}{H+h h^{\\top}}&{h}\\\\ {h^{\\top}}&{1}\\end{array}}\\!\\!\\right]$ 1],given Ht = $H_{t}=\\left[\\!\\!\\!\\begin{array}{c c}{\\operatorname{Cov}(p_{t})+x_{t}x_{t}^{\\top}}&{x_{t}}\\\\ {x_{t}^{\\top}}&{1}\\end{array}\\!\\!\\!\\right]$ ,wehave ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\langle H-H_{t},\\widehat{\\gamma}_{t}\\rangle-\\frac{D_{G}(H,H_{t})}{2\\eta}\\leq\\langle H-H_{t},\\widehat{\\gamma}_{t}\\rangle-\\frac{\\|x_{t}-h\\|_{\\mathrm{Cov}(p_{t})}^{2}-1}{2\\eta}}&{\\quad{\\mathrm{(Lemma~M.1)}}}\\\\ &{}&{=\\Big\\langle h-x_{t},\\widehat{\\theta}_{t}\\Big\\rangle-\\frac{\\|x_{t}-h\\|_{\\mathrm{Cov}(p_{t})}^{2}-1}{2\\eta}}\\\\ &{}&{\\leq\\eta\\|\\widehat{\\theta}_{t}\\|_{\\mathrm{Cov}(p_{t})}^{2}}&{\\quad{\\mathrm{(AM.GM)}}}\\\\ &{}&{=\\eta r_{t}^{2}a_{t}^{\\top}\\Sigma_{t}^{-1}\\mathrm{Cov}(p_{t})\\Sigma_{t}^{-1}a_{t}}\\\\ &{}&{\\leq\\eta\\|a_{t}\\|_{\\Sigma_{t}^{-1}}^{2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By Freedman's inequality, since $\\mathbb{E}_{t}\\left[\\Vert a_{t}\\Vert_{\\Sigma_{t}^{-1}}^{2}\\right]=d,$ and $\\eta\\|a_{t}\\|_{\\Sigma_{t}^{-1}}^{2}\\leq\\frac{\\eta d}{\\gamma}$ , with probability at least $1-\\delta$ ,we have ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\eta\\sum_{t=1}^{T}\\|a_{t}\\|_{\\Sigma_{t}^{-1}}^{2}\\leq\\mathcal{O}\\left(d\\eta T+\\frac{\\eta d\\log(1/\\delta)}{\\gamma}\\right).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof of Theorem 5.1. Using Lemma F.3-Lemma F.7 in Eq. (17) and Eq. (18), we get ", "page_idx": 23}, {"type": "text", "text": "RegT ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\leq\\mathcal{O}\\left(\\frac{d\\log(T)}{\\eta}+\\eta d T+\\alpha d\\log(T)+\\sqrt{d T\\log(T/\\delta)}+\\frac{d\\log(T/\\delta)}{\\sqrt{\\gamma}}+\\frac{\\eta d\\log(1/\\delta)}{\\gamma}+\\sqrt{d}C_{\\infty}+\\gamma\\mathcal{L}_{\\infty}\\right)}\\\\ &{\\qquad+\\operatorname*{max}_{t}\\|u\\|_{\\Sigma_{t}^{-1}}\\left(12\\sqrt{T\\log(T/\\delta)}+\\frac{12\\sqrt{d}\\log(T/\\delta)}{\\sqrt{\\gamma}}+C_{\\infty}\\right)-\\alpha\\operatorname*{max}_{t}\\|u\\|_{\\Sigma_{t}^{-1}}^{2}}\\\\ &{\\leq\\mathcal{O}\\left(\\frac{d\\log(T)}{\\eta}+\\eta d T+\\alpha d\\log(T)+\\sqrt{d T\\log(T/\\delta)}+\\frac{d\\log(T/\\delta)}{\\sqrt{\\gamma}}+\\frac{\\eta d\\log(1/\\delta)}{\\gamma}\\right.}\\\\ &{\\qquad\\qquad+\\frac{(C_{\\infty})^{2}}{\\alpha}+\\frac{T\\log(T/\\delta)}{\\alpha}+\\frac{d\\log^{2}(T/\\delta)}{\\gamma\\alpha}+\\sqrt{d}C_{\\infty}+\\gamma T\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, the choice $\\begin{array}{r}{\\gamma=\\frac{d}{\\sqrt{T}},\\alpha=\\operatorname*{max}\\left\\{\\frac{C_{\\infty}}{\\sqrt{d\\log(T)}},\\sqrt{T}\\right\\}}\\end{array}$ and $\\eta=\\sqrt{\\frac{\\log(T)}{T}}$ gives ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{Reg}_{T}\\leq\\mathcal{O}\\left(d\\sqrt{T}\\log(T/\\delta)+C_{\\infty}\\sqrt{d\\log(T)}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "1 Parameters: $\\begin{array}{r}{\\alpha=\\operatorname*{max}\\bigg\\{\\frac{C_{\\infty}}{\\sqrt{d\\log(T)}},\\sqrt{T}\\bigg\\},\\eta=\\operatorname*{min}\\bigg\\{\\frac{\\sqrt{\\log(T)}}{16C_{\\infty}},\\sqrt{\\frac{\\log(T)}{T}}\\bigg\\}}\\end{array}$ and $\\begin{array}{r}{\\gamma=\\frac{d}{\\sqrt{T}}}\\end{array}$   \n2 Let $\\rho\\in\\Delta(A)$ be John\u2019s exploration over $\\boldsymbol{\\mathcal{A}}$ , and let $\\Delta_{\\gamma}(A)=\\Big\\{p:p=(1-\\gamma)p^{\\prime}+\\gamma\\rho,\\;\\;p^{\\prime}\\in\\Delta(A)\\Big\\}.$   \n3 Define feasible set $\\mathcal{H}=\\left\\{\\widehat{\\mathbf{Cov}}(p):p\\in\\Delta_{\\gamma}(A)\\right\\}$   \n4 Define $G(\\pmb{H})=-\\log\\operatorname*{det}(\\pmb{H})$ and $B_{0}=0$   \n5 for $t=1,2,\\dots\\epsilon$ do   \n6 Compute $\\pmb{H}_{t}=\\underset{\\pmb{H}\\in\\mathcal{H}}{\\mathrm{argmax}}\\left\\{\\eta\\,\\langle\\pmb{H},\\pmb{\\Theta}_{t-1}\\rangle-G(\\pmb{H})\\right\\}\\ \\mathrm{where}\\,\\,\\pmb{\\Theta}_{t-1}=\\left[\\begin{array}{c c}{\\alpha B_{t-1}}&{\\frac{1}{2}\\sum_{s=1}^{t-1}\\widehat{\\theta}_{s}}\\\\ {\\frac{1}{2}\\sum_{s=1}^{t-1}\\widehat{\\theta}_{s}^{\\top}}&{0}\\end{array}\\right],$ $p_{t}\\in\\Delta_{\\gamma}(A)$ be such that $\\widehat{H_{t}}=\\widehat{\\mathbf{Cov}}(p_{t})$ \uff0c $\\Sigma_{t}=\\sum_{a\\in\\mathcal{A}}p_{t}(a)a a^{\\top},$ $B_{t}=\\mathrm{BONUS}(B_{t-1},\\Sigma_{t}).$ (defined in Figure 1a)   \n7 Sample $a_{t}\\sim p_{t}$ . Observe reward $r_{t}$ with $\\mathbb{E}[r_{t}]=a_{t}^{\\top}\\theta_{t}+\\epsilon_{t}(a_{t})$   \n8 Construct reward estimator $\\widehat{\\theta}_{t}=\\Sigma_{t}^{-1}a_{t}r_{t}$   \n9 end ", "page_idx": 24}, {"type": "text", "text": "G  Computationally Efficient Algorithm for Adversarial $C_{\\infty}$ Bound ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Most proof is the same as Appendix F. Namely, we follow Eq. (17) in Appendix F together with a different decomposition ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\log_{T}(p_{\\star})\\leq\\sum_{t=1}^{T}\\left\\langle u-x_{t},\\theta_{t}-\\mathbb{E}_{t}[\\widehat\\theta_{t}]\\right\\rangle+\\sum_{t=1}^{T}\\left\\langle u-x_{t},\\mathbb{E}_{t}[\\widehat\\theta_{t}]-\\widehat\\theta_{t}\\right\\rangle+\\sum_{t=1}^{T}\\langle U-H_{t},\\widehat\\gamma_{t}\\rangle+\\mathcal{O}\\left(\\sqrt{T\\log_{T}(t)}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By Lemma F.2, we can further bound FTRL by ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bf T R L}\\le\\underbrace{\\frac{G(U)-\\operatorname*{min}_{H\\in\\mathcal{H}}G(H)}{\\eta}}_{{\\bf p e a l t y}}+\\sum_{\\underbrace{\\scriptstyle{\\overbrace{\\scriptstyle{\\bf\\delta}_{\\mathrm{pans}}}}_{{\\bf p a n s}}}_{{\\bf p}_{\\mathrm{the}}}}\\!\\!\\!\\!\\!\\!}}\\\\ {{\\displaystyle{\\quad+\\sum_{\\\\ell=1}^{T}\\!\\operatorname*{max}_{H\\in\\mathcal{H}}\\left\\{\\langle H-H_{t},\\widehat{\\gamma}_{t}\\rangle-\\frac{D_{G}(H,H_{t})}{2\\eta}\\right\\}+\\sum_{\\ell=1}^{T}\\!\\operatorname*{max}_{H\\in\\mathcal{H}}\\left\\{\\langle H_{t}-H,-{\\pmb D}_{t}\\rangle-\\frac{D_{G}(H,H_{t})}{2\\eta}\\right\\}}_{{\\bf p}_{\\ell}}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Among the terms above, Bias, Penalty, Deviation, Bonus, and Stability-1 follow the same bounds as Lemma F.3, Lemma F.5, Lemma F.6, and Lemma F.7, respectively. It remains to bound Stability-2. LemmaG.1. If $\\begin{array}{r}{\\eta\\leq\\frac{1}{16\\sqrt{d}\\alpha}}\\end{array}$ then ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbf{Stability-2}\\leq8\\eta\\alpha^{2}d.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Proof. From the analysis of bias term and $\\pmb{H}_{t}$ and $D_{t}$ are both positive semi-definite, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sqrt{\\mathrm{Tr}\\left(H_{t}D_{t}H_{t}D_{t}\\right)}=\\alpha\\sqrt{\\mathrm{Tr}\\left(\\Sigma_{t}\\left(B_{t}-B_{t-1}\\right)\\Sigma_{t}\\left(B_{t}-B_{t-1}\\right)\\right)}\\le\\alpha\\sqrt{d}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last inequalityis due to - \u2265 Bt - Bt-1. Since n \u2264 16Vaa , by Lemma M.2, with probability of at least $1-\\delta$ ,wehave ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathrm{Stability-2}\\le8\\eta\\sum_{t=1}^{T}\\mathrm{Tr}\\left(H_{t}D_{t}H_{t}D_{t}\\right)}}\\\\ {{\\displaystyle\\qquad\\qquad\\le8\\eta\\alpha^{2}\\sum_{t=1}^{T}\\mathrm{Tr}\\left(\\Sigma_{t}\\left(B_{t}-B_{t-1}\\right)\\Sigma_{t}\\left(B_{t}-B_{t-1}\\right)\\right)}}\\\\ {{\\displaystyle\\qquad\\le8\\eta\\alpha^{2}d}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the last step follows the similar analysis in Lemma F.6. ", "page_idx": 25}, {"type": "text", "text": "Proof of Theorem 5.1(Option $I I$ ). Using Lemma F.3-Lemma G.1 in Eq. (21) and Eq. (22), we get ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\log_{T}=\\mathcal{O}\\left(\\frac{d\\log(T)}{\\eta}+\\eta d T+d\\alpha\\log(T)+\\sqrt{d T\\log(T/\\delta)}+\\frac{d\\log(T/\\delta)}{\\sqrt{\\gamma}}+\\frac{\\eta d\\log(1/\\delta)}{\\gamma}+\\eta\\alpha^{2}d\\right.}\\\\ &{\\left.\\qquad\\qquad\\qquad+\\frac{(C_{\\infty})^{2}}{\\alpha}+\\frac{T\\log(T/\\delta)}{\\alpha}+\\frac{d\\log^{2}(T/\\delta)}{\\gamma\\alpha}+\\sqrt{d}C_{\\infty}+\\gamma T\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "By choosing $\\begin{array}{r}{\\alpha\\,=\\,\\operatorname*{max}\\left\\{\\frac{C_{\\infty}}{\\sqrt{d\\log(T)}},\\sqrt{T}\\right\\}}\\end{array}$ and $\\begin{array}{r}{\\eta\\,=\\,\\operatorname*{min}\\left\\{\\frac{\\sqrt{\\log(T)}}{16C_{\\infty}},\\sqrt{\\frac{\\log(T)}{T}}\\right\\}}\\end{array}$ and $\\textstyle\\gamma\\,=\\,{\\frac{d}{\\sqrt{T}}}$ we could ensure $\\begin{array}{r}{\\eta\\,\\le\\,\\frac{1}{16\\sqrt{d}\\alpha}}\\end{array}$ . This gives the final regret $\\mathcal{O}\\left(d\\sqrt{T}\\log(T/\\delta)+d C_{\\infty}\\sqrt{\\log T}\\right)$ . The additional $\\sqrt{d}$ factor comes from the additional condition for the Stability-2 term. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "H Proof of Theorem 5.2 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Similar to before, we define ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\widehat{\\gamma}_{t}=\\left[\\!\\!\\begin{array}{c c}{0}&{\\frac{1}{2}\\widehat{\\theta}_{t}}\\\\ {\\frac{1}{2}\\widehat{\\theta}_{t}^{\\top}}&{0}\\end{array}\\!\\!\\right],\\qquad D_{t}=\\left[\\!\\!\\begin{array}{c c}{\\alpha B_{t}-\\alpha B_{t-1}}&{0}\\\\ {0}&{0}\\end{array}\\!\\!\\right],\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "and $x_{t}=\\mathbb{E}_{a\\sim p_{t}}[a]$ \uff0c $\\tilde{x}_{t}=\\mathbb{E}_{a\\sim\\tilde{p}_{t}}[a]$ . We perform the regret decomposition as the following. ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Reg}_{T}=\\displaystyle\\sum_{t=1}^{T}\\langle u-a_{t},\\theta_{t}\\rangle}\\\\ &{=\\displaystyle\\sum_{t=1}^{T}\\langle u-x_{t},\\theta_{t}\\rangle+\\sum_{t=1}^{T}\\langle x_{t}-\\tilde{x}_{t},\\theta_{t}\\rangle+\\sum_{t=1}^{T}\\langle\\tilde{x}_{t}-a_{t},\\theta_{t}\\rangle}\\\\ &{=\\displaystyle\\sum_{t=1}^{T}\\langle u-x_{t},\\theta_{t}\\rangle+\\mathcal{O}(\\gamma T+\\sqrt{T\\log(1/\\delta)})}\\\\ &{=\\displaystyle\\sum_{t=1}^{T}\\Big\\langle u-x_{t},\\theta_{t}-\\mathbb{E}_{t}[\\widehat{\\theta}_{t}]\\Big\\rangle+\\sum_{t=1}^{T}\\Big\\langle u-x_{t},\\mathbb{E}_{t}[\\widehat{\\theta}_{t}]-\\widehat{\\theta}_{t}\\Big\\rangle+\\sum_{t=1}^{T}\\Big\\langle u-x_{t},\\widehat{\\theta}_{t}\\Big\\rangle+\\mathcal{O}\\big(\\gamma T+\\sqrt{T\\log(1/\\delta)}\\Big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The FTRL term can be further bounded as the following. ", "page_idx": 26}, {"type": "text", "text": "FTRL ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{=\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{a\\sim p_{t}}\\left[\\left\\langle u-a,\\widehat{\\theta}_{t}\\right\\rangle\\right]}\\\\ &{=\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{H\\sim q_{t}}\\left[\\left\\langle U-H,\\widehat{\\gamma}_{t}\\right\\rangle\\right]}\\\\ &{\\leq\\displaystyle\\frac{d^{2}\\log T}{\\eta}+\\frac{1}{\\eta}\\sum_{t=1}^{T}\\mathbb{E}_{H\\sim q_{t}}\\left[\\exp\\left(\\eta\\left\\langle H,\\widehat{\\gamma}_{t}\\right\\rangle\\right)-\\eta\\left\\langle H,\\widehat{\\gamma}_{t}\\right\\rangle-1\\right]+\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{H\\sim q_{t}}\\left[\\left\\langle H-U,D_{t}\\right\\rangle\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n=\\frac{d^{2}\\log T}{\\eta}+\\frac{1}{\\eta}\\sum_{t=1}^{T}\\mathbb{E}_{a\\sim p_{t}}\\left[\\exp\\left(\\eta\\left\\langle a,\\widehat{\\theta}_{t}\\right\\rangle\\right)-\\eta\\left\\langle a,\\widehat{\\theta}_{t}\\right\\rangle-1\\right]+\\alpha\\sum_{t=1}^{T}\\mathbb{E}_{a\\sim p_{t}}\\left[\\Vert a\\Vert_{B_{t}-B_{t-1}}^{2}\\right]-\\alpha\\sum_{t=1}^{T}\\mathbb{E}_{a\\sim p_{t}}\\left[\\Vert a\\Vert_{B_{t}-B_{t-1}}^{2}\\right].\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "In the following four lemmas, we bound the four terms Bias, Deviation, Bonus, Stability. ", "page_idx": 26}, {"type": "text", "text": "Lemma H.1. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbf{Bias}\\leq\\left(\\operatorname*{max}_{t}\\Vert x_{t}-u\\Vert_{\\tilde{\\Sigma}_{t}^{-1}}\\right)\\left(\\sqrt{d\\gamma}T+2\\sqrt{T\\log(1/\\delta)}+\\sqrt{d}\\beta C\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\operatorname{Bias}=\\displaystyle\\sum_{t=1}^{T}\\left\\langle u-x_{t},\\theta_{t}-\\mathbb{E}_{t}|\\hat{\\theta}_{t}|\\right\\rangle}\\\\ &{\\quad=\\displaystyle\\sum_{t=1}^{T}\\left\\langle u-x_{t},\\theta_{t}-\\widetilde{\\Sigma}_{t}^{-1}\\mathbb{E}_{t}\\left[a_{t}a_{t}^{\\top}\\right]\\theta_{t}+\\widetilde{\\Sigma}_{t}^{-1}\\mathbb{E}_{t}\\left[a_{t}\\epsilon_{t}(a_{t})\\right]\\right\\rangle}\\\\ &{\\quad=\\displaystyle\\gamma\\sum_{t=1}^{T}\\left\\langle u-x_{t},\\widetilde{\\Sigma}_{t}^{-1}\\theta_{t}\\right\\rangle+\\displaystyle\\sum_{t=1}^{T}\\left\\langle u-x_{t},\\widetilde{\\Sigma}_{t}^{-1}\\mathbb{E}_{t}\\left[a_{t}\\epsilon_{t}(a_{t})\\right]\\right\\rangle\\quad\\quad\\left.(\\widetilde{\\Sigma}_{t}=\\gamma I+\\mathbb{E}_{t}[a_{t}a_{t}^{\\top}])\\right.}\\\\ &{\\quad\\le\\displaystyle\\gamma\\sum_{t=1}^{T}\\lVert x_{t}-u\\rVert_{\\widetilde{\\Sigma}_{t}^{-1}}\\lVert\\theta_{t}\\rVert_{\\widetilde{\\Sigma}_{t}^{-1}}+\\displaystyle\\sum_{t=1}^{T}\\lVert x_{t}-u\\rVert_{\\widetilde{\\Sigma}_{t}^{-1}}\\mathbb{E}_{t}\\left[\\left\\lVert u_{t}\\right\\rVert_{\\widetilde{\\Sigma}_{t}^{-1}}\\left\\lvert\\epsilon_{t}(a_{t})\\right\\rvert\\right]}\\\\ &{\\quad\\le\\left(\\operatorname*{max}\\left\\lVert x_{t}-u\\right\\rVert_{\\widetilde{\\Sigma}_{t}^{-1}}\\right)\\left(\\gamma\\sqrt{\\frac{d}{2}}T+\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{t}\\left[\\left\\lVert u_{t}\\right\\rVert_{\\widetilde{\\Sigma}_{t}^{-1}}\\left\\lvert\\epsilon_{t}(a_{t})\\right\\rvert\\right]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\leq\\left(\\underset{t}{\\operatorname*{max}}\\,\\|x_{t}-u\\|_{\\widetilde{\\Sigma}_{t}^{-1}}\\right)\\left(\\sqrt{d\\gamma}T+\\sqrt{d}\\beta\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{t}\\left[\\vert\\epsilon_{t}(a_{t})\\vert\\right]\\right)}\\\\ {\\leq\\left(\\underset{t}{\\operatorname*{max}}\\,\\|x_{t}-u\\|_{\\widetilde{\\Sigma}_{t}^{-1}}\\right)\\left(\\sqrt{d\\gamma}T+2\\sqrt{T\\log(1/\\delta)}+\\sqrt{d}\\beta\\displaystyle\\sum_{t=1}^{T}\\vert\\epsilon_{t}(a_{t})\\vert\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "(Azuma's inequality) ", "page_idx": 26}, {"type": "text", "text": "Lemma H.2. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathbf{Deviation}\\leq\\mathcal{O}\\left(\\operatorname*{max}_{t}\\|u-x_{t}\\|_{\\widetilde{\\Sigma}_{t}^{-1}}d\\beta\\sqrt{T}\\log(T/\\delta)\\right).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Proof. Notice that ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\langle u-x_{t},\\widehat{\\theta}_{t}\\rangle\\right|\\leq\\left|(u-x_{t})^{\\top}\\widetilde{\\Sigma}_{t}^{-1}a_{t}\\right|}\\\\ &{\\qquad\\qquad\\leq\\|u-x_{t}\\|_{\\widetilde{\\Sigma}_{t}^{-1}}\\|a_{t}\\|_{\\widetilde{\\Sigma}_{t}^{-1}}}\\\\ &{\\qquad\\qquad\\leq\\sqrt{d}\\beta\\|u-x_{t}\\|_{\\widetilde{\\Sigma}_{t}^{-1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By the strengthened Freedman's inequality (Lemma M.3), with probability at least $1-\\delta$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbf{Deviation}=\\sum_{t=1}^{T}\\Big\\langle u-x_{t},\\mathbb{E}_{t}[\\widehat{\\theta}_{t}]-\\widehat{\\theta}_{t}\\Big\\rangle}}\\\\ &{}&{\\leq\\mathcal{O}\\left(3\\sqrt{\\sum_{t=1}^{T}\\mathbb{E}_{t}\\left[\\left\\langle u-x_{t},\\widehat{\\theta}_{t}\\right\\rangle^{2}\\right]\\log(T^{d}/\\delta)}+2\\sqrt{d}\\beta\\operatorname*{max}_{t}\\|u-x_{t}\\|_{\\widetilde{\\Sigma}_{t}^{-1}}\\log(T^{d}/\\delta)\\right)}\\\\ &{}&{\\leq\\mathcal{O}\\left(\\operatorname*{max}\\|u-x_{t}\\|_{\\widetilde{\\Sigma}_{t}^{-1}}d\\beta\\sqrt{T}\\log(T/\\delta)\\right).\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathrm{(using~the~assumption~}d\\le T)}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Lemma H.3. ", "text_level": 1, "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbf{Bonus}\\leq3\\alpha d\\log(T)-\\alpha\\operatorname*{max}_{t}\\|u\\|_{\\widetilde{\\Sigma}_{t}^{-1}}^{2}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. The proof the same as in the logdet case. See the proof of Lemma F.6. ", "page_idx": 27}, {"type": "text", "text": "Lemma H.4. ", "text_level": 1, "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbf{Stability}\\le{\\mathcal{O}}(\\eta d T\\log^{2}T).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof. ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\mathbf{Stability}=\\frac{1}{\\eta}\\sum_{t=1}^{T}\\mathbb{E}_{a\\sim p_{t}}\\left[\\exp\\left(\\eta\\left\\langle a,\\widehat{\\theta}_{t}\\right\\rangle\\right)-\\eta\\left\\langle a,\\widehat{\\theta}_{t}\\right\\rangle-1\\right]\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since $q_{t}^{\\prime}$ is log-concave distribution, so are $q_{t}$ and $p_{t}$ , which further implies that $\\eta\\left\\langle a,\\widehat{\\theta_{t}}\\right\\rangle$ follows a log-concave distribution. Furthermore, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{a\\sim p_{t}}\\left[\\eta^{2}\\left\\langle a,\\widehat{\\theta_{t}}\\right\\rangle^{2}\\right]\\leq\\mathbb{E}_{a\\sim p_{t}}\\left[\\eta^{2}a_{t}^{\\top}\\widetilde\\Sigma_{t}^{-1}a a^{\\top}\\widetilde\\Sigma_{t}^{-1}a_{t}\\right]\\leq2\\eta^{2}\\|a_{t}\\|_{\\widetilde\\Sigma_{t}^{-1}}^{2}\\leq2\\eta^{2}d\\beta^{2}\\leq\\frac{1}{100},}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where we use Lemma J.2 in the second-last inequality . By Lemma 6 of Ito et al. (2020), we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\overset{!}{\\underset{/}{\\sum}}\\sum_{t=1}^{T}\\mathbb{E}_{a\\sim p_{t}}\\left[\\exp\\left(\\eta\\left\\langle a,\\widehat{\\theta_{t}}\\right\\rangle\\right)-\\eta\\left\\langle a,\\widehat{\\theta_{t}}\\right\\rangle-1\\right]\\leq\\eta\\sum_{t=1}^{T}\\mathbb{E}_{a\\sim p_{t}}\\left[\\left\\langle a,\\widehat{\\theta_{t}}\\right\\rangle^{2}\\right]\\leq2\\eta\\sum_{t=1}^{T}\\|a_{t}\\|_{\\tilde{\\Sigma}_{t}^{-1}}^{2}\\leq2\\eta\\beta\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Proof of Theorem 5.2. Combining Eq. (23), Eq. (24), and Lemma H.1, Lemma H.2, Lemma H.3, Lemma H.4, we see that the regret is bounded by ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\widetilde{\\mathcal{O}}\\left(\\frac{d^{2}}{\\eta}+\\eta d T+\\operatorname*{max}_{t}\\|u-x_{t}\\|_{\\widetilde{\\Sigma}_{t}^{-1}}(d\\sqrt{T}+\\sqrt{d}C)+d C+\\alpha d\\right)-\\alpha\\|u\\|_{B_{T}}^{2}}\\\\ {\\displaystyle\\leq\\widetilde{\\mathcal{O}}\\left(\\frac{d^{2}}{\\eta}+\\eta d T+\\alpha d\\right)+\\frac{d^{2}T+d C^{2}}{\\alpha}\\qquad\\qquad\\qquad\\ (\\mathrm{AM-GM}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Choosing optimal $\\alpha$ and $\\eta$ leads to $\\widetilde{\\mathcal{O}}(\\sqrt{d^{3}T}+d C)$ ", "page_idx": 27}, {"type": "text", "text": "1  Dimension Reduction for Continuous Exponential Weights ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "First, the intrinsic dimension of $\\mathcal{X}$ can be defined as the following: ", "page_idx": 28}, {"type": "text", "text": "Definition 1. The intrinsic dimension of $\\mathcal{X}$ is defined as ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\dim(\\mathcal{X})=\\dim\\left(\\operatorname{span}\\left(\\mathcal{X}-\\mathcal{X}\\right)\\right),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\mathcal{X}-\\mathcal{X}\\triangleq\\{x-x^{\\prime}:x,x^{\\prime}\\in\\mathcal{X}\\}.$ ", "page_idx": 28}, {"type": "text", "text": "A convex region $\\mathcal{X}\\subset\\mathbb{R}^{n}$ can be translated and rotated so that it entirely lies in $\\mathbb{R}^{m}$ where $m=$ $\\dim({\\mathcal{X}})$ and has non-zero volume in $\\mathbb{R}^{m}$ . We more precisely define this transformation below. ", "page_idx": 28}, {"type": "text", "text": "Definition 2. Let $\\mathcal{X}\\subset\\mathbb{R}^{n}$ be a convex region with $\\dim(X)=m$ We define $\\phi:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{m}$ as the following linear transformation: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\phi(x)\\triangleq Z M x,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $M\\in\\mathbb{R}^{n\\times n}$ is a rotation matrix (i.e., orthogonal matrix) such that for any $v\\in\\mathcal{X}-\\mathcal{X}$ Mu has non-zero elements only in the first m coordinates (this is always possible by the definition of $\\dim({\\mathcal{X}})$ inDefinition1),and ", "page_idx": 28}, {"type": "equation", "text": "$$\nZ={\\left[\\begin{array}{l l l l l l l}{\\textstyle1}&{0}&{\\cdots}&{0}&{0}&{\\cdots}&{0}\\\\ {0}&{1}&{\\cdots}&{0}&{0}&{\\cdots}&{0}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {0}&{0}&{\\cdots}&{1}&{0}&{\\cdots}&{0}\\end{array}\\right]}\\in\\mathbb{R}^{m\\times n}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "extracts the first m coordinates of a given $n$ -dimensional vector. ", "page_idx": 28}, {"type": "text", "text": "Lemma I.1. For any $x\\in\\mathscr{X}$ and any $\\theta\\in\\mathbb{R}^{n}$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\langle x,\\theta\\rangle=\\langle\\phi(x),\\phi(\\theta)\\rangle+f(\\phi,\\theta),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $f(\\phi,\\theta)\\in\\mathbb{R}$ is some quantity that only depends on $\\phi$ and $\\theta$ but not $x$ ", "page_idx": 28}, {"type": "text", "text": "Proof. Let $x,x^{\\prime}\\in\\mathcal{X}$ . By the definition of $\\phi$ , we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\langle\\phi(x)-\\phi(x^{\\prime}),\\phi(\\theta)\\right\\rangle=\\left\\langle Z M(x-x^{\\prime}),Z M\\theta\\right\\rangle.\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "By the choice of $M$ in Definition 2, $M(x-x^{\\prime})$ only has non-zero elements in the first $m$ coordinates. Furthermore, since $Z$ extracts the first $m$ coordinates, we have ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\langle Z M(x-x^{\\prime}),Z M\\theta\\rangle=\\displaystyle\\sum_{i=1}^{m}(M(x-x^{\\prime}))_{i}(M\\theta)_{i}}\\\\ {=\\displaystyle\\sum_{i=1}^{n}(M(x-x^{\\prime}))_{i}(M\\theta)_{i}}\\\\ {=\\langle M(x-x^{\\prime}),M\\theta\\rangle}\\\\ {=\\langle x-x^{\\prime},\\theta\\rangle\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Thus, ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\left\\langle x,\\theta\\right\\rangle-\\left\\langle\\phi(x),\\phi(\\theta)\\right\\rangle=\\left\\langle x^{\\prime},\\theta\\right\\rangle-\\left\\langle\\phi(x^{\\prime}),\\phi(\\theta)\\right\\rangle,\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "meaning that the value of $\\langle x,\\theta\\rangle-\\langle\\phi(x),\\phi(\\theta)\\rangle$ is shared by all $x\\in\\mathscr{X}$ . Defining this value as $f(\\phi,\\theta)$ finishes the proof. \u53e3 ", "page_idx": 28}, {"type": "text", "text": "We consider the continuous exponential weight algorithm (Algorithm 5) running on $\\phi(\\mathcal{X})\\subset\\mathbb{R}^{m}$ ", "page_idx": 28}, {"type": "text", "text": "1 Let $\\mathcal{X}\\subset\\mathbb{R}^{n}$ , and let $\\phi({\\mathcal{X}})\\triangleq\\{\\phi(x):x\\in{\\mathcal{X}}\\}$   \n2 for $t=1,2,\\ldots$ do ", "page_idx": 29}, {"type": "equation", "text": "$$\nw_{t}(y)=\\exp\\left(\\eta\\sum_{s=1}^{t-1}\\left\\langle y,\\phi(\\theta_{s})\\right\\rangle+\\eta\\sum_{s=1}^{t}\\left\\langle y,\\phi(b_{s})\\right\\rangle\\right)\\ \\mathrm{~and~}\\,p_{t}(y)=\\frac{w_{t}(y)}{\\int_{y^{\\prime}\\in\\phi(X)}w_{t}(y^{\\prime})\\mathrm{d}y^{\\prime}}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Sample $y_{t}\\sim p_{t}$ , and play $x_{t}=\\phi^{-1}(y_{t})$ , where $\\phi^{-1}$ is the inverse mapping of $\\phi$ Receive $\\theta_{t}\\in\\mathbb{R}^{n}$ ", "page_idx": 29}, {"type": "text", "text": "In Algorithm 5, we require that the inverse mapping of $\\phi$ exists. This is true because for any $x,x^{\\prime}\\in\\mathcal{X}$ we have $\\|\\phi(x)-\\phi(x^{\\prime})\\|=\\|Z M(x-x^{\\prime})\\|=\\|M(x-x^{\\prime})\\|=\\|x-x^{\\prime}\\|$ and thus $\\phi$ cannot map $x,x^{\\prime}\\in\\dot{x}$ with $x\\neq x^{\\prime}$ to the same point. ", "page_idx": 29}, {"type": "text", "text": "Theorem I.2. Let $q_{t}\\,\\in\\,\\Delta(\\mathcal{X})$ be the distribution such that $x\\,\\sim\\,q_{t}$ isequivalent tofirst drawing $y\\sim p_{t}$ and then taking $x=\\phi^{-1}(y)$ Algorithm $^{5}$ ensures for any $x\\in\\mathscr{X}$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\left\\langle x,\\theta_{t}+b_{t}\\right\\rangle-\\sum_{t=1}^{T}\\mathbb{E}_{x\\sim q_{t}}\\left[\\left\\langle x,\\theta_{t}+b_{t}\\right\\rangle\\right]\\leq\\frac{m\\log T}{\\eta}+\\frac{1}{\\eta}\\sum_{t=1}^{T}\\mathbb{E}_{x\\sim q_{t}}\\left[\\exp\\left(\\eta\\left\\langle x,\\theta_{t}\\right\\rangle\\right)-\\eta\\left\\langle x,\\theta_{t}\\right\\rangle-1\\right]\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Proof. Note that Algorithm 5 is a standard continuous exponential weight algorithm over reward vectors $\\phi(\\theta_{t})$ and in the space of $\\phi(\\mathcal{X})\\subset\\mathbb{R}^{m}$ . By the standard analysis (see, e.g., Ito et al. (2020); Zimmert and Lattimore (2022)), we have for any sequence $\\lambda_{1},\\dots,\\lambda_{T}\\in\\mathbb{R}$ andany $y\\in\\phi(\\mathcal{X})$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}\\left<y,\\phi(\\theta_{t})+\\phi(b_{t})\\right>-\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{y\\sim p_{t}}\\left[\\langle y,\\phi(\\theta_{t})+\\phi(b_{t})\\rangle\\right]}\\\\ &{\\leq\\displaystyle\\frac{m\\log T}{\\eta}+\\frac{1}{\\eta}\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{y\\sim p_{t}}\\left[\\exp\\left(\\eta\\left<y,\\phi(\\theta_{t})\\right>+\\lambda_{t}\\right)-\\left(\\eta\\left<y,\\phi(\\theta_{t})\\right>+\\lambda_{t}\\right)-1\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "By Lemma I.1, the above implies ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{\\nmid=1}^{T}\\left\\langle\\phi^{-1}(y),\\theta_{t}+b_{t}\\right\\rangle-\\displaystyle\\sum_{t=1}^{T}\\mathbb{E}_{y\\sim p_{t}}\\left[\\left\\langle\\phi^{-1}(y),\\theta_{t}+b_{t}\\right\\rangle\\right]}\\\\ &{\\displaystyle\\leq\\frac{m\\log T}{\\eta}+\\frac{1}{\\eta}\\sum_{t=1}^{T}\\mathbb{E}_{y\\sim p_{t}}\\left[\\exp\\left(\\eta\\left\\langle\\phi^{-1}(y),\\theta_{t}\\right\\rangle-\\eta f(\\phi,\\theta_{t})+\\lambda_{t}\\right)-\\left(\\eta\\left\\langle\\phi^{-1}(y),\\theta_{t}\\right\\rangle-\\eta f(\\phi,\\theta_{t})+\\lambda_{t}\\right)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "which further implies that for any $x\\in\\mathscr{X}$ ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{T}\\left\\langle x,\\theta_{t}+b_{t}\\right\\rangle-\\sum_{t=1}^{T}\\mathbb{E}_{x\\sim q_{t}}\\left[\\left\\langle x,\\theta_{t}+b_{t}\\right\\rangle\\right]\\leq\\frac{m\\log T}{\\eta}+\\frac{1}{\\eta}\\sum_{t=1}^{T}\\mathbb{E}_{x\\sim q_{t}}\\left[\\exp\\left(\\eta\\left\\langle x,\\theta_{t}\\right\\rangle\\right)-\\eta\\left\\langle x,\\theta_{t}\\right\\rangle-1\\right]\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "by the definition of $q_{t}$ and by leting $\\lambda_{t}=\\eta f(\\phi,\\theta_{t})$ ", "page_idx": 29}, {"type": "text", "text": "J  Computationally Efficient Algorithm for Adversarial $C$ Bound ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "In this section, we present Algorithm 6, a polynomial-time algorithm that ensures $\\widetilde{\\mathcal{O}}(d^{3}\\sqrt{T}+d^{\\frac{5}{2}}C)$ regret. The algorithm is based on the continuous exponential weight algorithm in the original feature space (Ito et al., 2020; Zimmert and Lattimore, 2022), with the bonus construction similar to Lee et al. (2020). ", "page_idx": 29}, {"type": "text", "text": "J.1  Preliminaries for Entropic Barrier ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Entropic barrier  For any convex body $\\boldsymbol{\\mathcal{A}}$ , the family of exponential distribution is ", "page_idx": 30}, {"type": "equation", "text": "$$\np_{w}(x)=\\frac{\\exp(w^{\\top}x)\\mathbb{1}\\{y\\in A\\}}{\\int_{A}\\exp(w^{\\top}y)\\mathrm{d}y}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "For any $x\\in A$ there i unique $w(x)$ such that $\\mathbb{E}_{y\\sim p_{w(x)}}[y]=x$ . The entropic barrier $F(x)$ is the negative entropy of $p_{w(x)}$ . Namely ", "page_idx": 30}, {"type": "equation", "text": "$$\nF(x)=\\int p_{w(x)}(y)\\log{\\big(}p_{w(x)}(y){\\big)}\\;\\mathrm{d}y\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We have $\\nabla F(x)\\,=\\,w(x)$ and $\\nabla^{2}F(x)=\\mathbb{E}_{y\\sim p_{w(x)}}\\left[(y-x)(y-x)^{\\top}\\right]$ . We know that $F(x)$ is a $d$ -self-concordant barrier on $\\boldsymbol{\\mathcal{A}}$ ", "page_idx": 30}, {"type": "text", "text": "The equivalence of mean-oriented FTRL and continuous exponential weights  Consider FTRL with entropic barrier as the regularizer that solves $x_{t}$ for round $t\\in[T]$ following ", "page_idx": 30}, {"type": "equation", "text": "$$\nx_{t+1}=\\operatorname*{argmax}_{x\\in A}\\left\\{\\left\\langle x,\\sum_{s=1}^{t}\\theta_{s}\\right\\rangle-\\frac{F(x)}{\\eta_{t}}\\right\\}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "This is equivalent to ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\nabla F(x_{t+1})=\\eta_{t}\\sum_{s=1}^{t}\\theta_{s}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Given that $\\mathbb{E}_{y\\sim p_{w(x_{t+1})}}\\left[y\\right]\\;=\\;x_{t+1}$ and $\\nabla F(x_{t+1})\\;=\\;w(x_{t+1})$ playing $x_{t+1}$ yields the same expected reward as playing according to distribution $p_{w(x_{t+1})}$ where $\\begin{array}{r}{w(x_{t+1})=\\eta_{t}\\sum_{s=1}^{t}\\theta_{s}}\\end{array}$ . Thus, we have $\\begin{array}{r}{p_{w(x_{t+1})}(x)\\propto\\exp\\left(\\eta_{t}\\left\\langle x,\\sum_{s=1}^{t}\\theta_{s}\\right\\rangle\\right)}\\end{array}$ for $x\\in A$ ", "page_idx": 30}, {"type": "text", "text": "J.2Auxiliary Lemmas ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Lemma J.1 (Lemma 1 of Ito et al. (2020). If $x$ follows a log-concave distribution $p$ over $\\mathbb{R}^{d}$ and $\\mathbb{E}_{x\\sim p}[x x^{\\top}]\\preceq I$ wehave ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\|x\\|_{2}^{2}\\geq d\\beta^{2}\\right]\\leq d\\exp(1-\\beta).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "for arbitrary $\\beta>0$ ", "page_idx": 30}, {"type": "text", "text": "Lemma J.2. With the choice of $\\beta\\geq4\\log(10d T),$ we have ", "page_idx": 30}, {"type": "equation", "text": "$$\n|\\mathbb{E}_{a\\sim p_{t}}[f(a)]-\\mathbb{E}_{a\\sim\\tilde{p}_{t}}[f(a)]|\\leq10d\\exp(-\\beta)\\leq\\frac{1}{2T^{2}}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "for any $f:{\\mathcal{A}}\\rightarrow[-1,1]$ and ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{3}{4}\\mathbb{E}_{a\\sim p_{t}}[a a^{\\top}]\\preceq\\mathbb{E}_{a\\sim\\tilde{p}_{t}}[a a^{\\top}]\\preceq\\frac{4}{3}\\mathbb{E}_{a\\sim p_{t}}[a a^{\\top}].\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Proof. The proof follows that of Lemma 4 of Ito et al. (2020), with the observation that $p_{t}$ isa log-concave distribution. \u53e3 ", "page_idx": 30}, {"type": "text", "text": "Lemma J.3 (Lemma 14 of Zimmert and Lattimore (2022)). Let $f$ bea $\\nu$ -self-concordantbarrierfor $A\\subset\\ensuremath{\\mathbb{R}}^{d}$ .Thenforany $u,x\\in A$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\|u-x\\|_{\\nabla^{2}f(x)}\\le-\\gamma^{\\prime}\\left<u-x,\\nabla f(x)\\right>+4\\gamma^{\\prime}\\nu+2\\sqrt{\\nu}}}\\\\ {{\\frac{8}{\\sqrt{3}}+\\frac{7^{\\frac{3}{2}}}{6\\sqrt{3\\nu}}\\left(\\gamma^{\\prime}\\in[1,4]\\,f o r\\,\\nu\\ge1\\right)\\!.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where ", "page_idx": 30}, {"type": "text", "text": "Minkowsky Functions. The Minkowsky function of a convex boday $\\boldsymbol{\\mathcal{A}}$ with the pole at $w\\in\\operatorname{int}(A)$ is a function $\\pi_{w}:A\\to\\mathbb{R}$ defined as ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\pi_{w}(u)=\\operatorname*{inf}\\left\\{t>0\\;\\middle|\\;w+\\frac{u-w}{t}\\in A\\right\\}.\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Algorithm 6: Continuous exponential weights (for adversarial $C$ bound) ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1 Lel De a convex pouy anu De its entropic parrier.   \n2 Parameters: $\\begin{array}{r}{\\gamma=\\frac{\\log(T/\\delta)}{T}}\\end{array}$ $\\begin{array}{r}{\\alpha=\\tilde{\\Theta}(\\sqrt{d}C+d\\sqrt{T}),\\eta=\\operatorname*{min}\\Big\\{\\frac{1}{160\\sqrt{d^{3}T}},\\frac{1}{32\\sqrt{d}\\alpha}\\Big\\}.}\\end{array}$   \n3 for $t=1,2,\\ldots,T$ do   \n4 Define $\\begin{array}{r}{w_{t}(a)=\\exp\\left(\\eta\\sum_{s=1}^{t-1}\\langle a,\\widehat{\\theta}_{s}-b_{s}\\rangle\\right)}\\end{array}$ and   \n$p_{t}(a)=w_{t}(a)/\\big(\\int_{y\\in A}w_{t}(y)\\mathrm{d}y\\big),\\quad\\tilde{p}_{t}(a)=\\frac{p_{t}(a)\\mathbb{1}\\{\\|a\\|_{\\Sigma_{t}^{-1}}\\leq\\sqrt{d}\\beta\\}}{\\int_{a^{\\prime}\\in A}p_{t}(a^{\\prime})\\mathbb{1}\\{\\|a^{\\prime}\\|_{\\Sigma_{t}^{-1}}\\leq\\sqrt{d}\\beta\\}\\mathrm{d}a^{\\prime}},$   \nwhere $\\Sigma_{t}=\\mathbb{E}_{a\\sim p_{t}}[a a^{\\top}]$   \n5 Play $a_{t}\\sim\\tilde{p}_{t}$ , and observe reward $r_{t}$ with $\\mathbb{E}[r_{t}]=a_{t}^{\\top}\\theta_{t}+\\epsilon_{t}(a_{t})$   \n6 Construct reward estimator $\\widehat{\\theta}_{t}=\\widetilde\\Sigma_{t}^{-1}a_{t}r_{t}$ ,where $\\widetilde{\\Sigma}_{t}=\\gamma I+\\mathbb{E}_{a\\sim\\tilde{p}_{t}}[a a^{\\top}]$   \n7 Define $B_{t}=I+\\left[\\begin{array}{c c}{\\widetilde{\\Sigma}_{t}^{-1}}&{-\\widetilde{\\Sigma}_{t}^{-1}x_{t}}\\\\ {-x_{t}^{\\top}\\widetilde{\\Sigma}_{t}^{-1}}&{x_{t}^{\\top}\\widetilde{\\Sigma}_{t}^{-1}x_{t}}\\end{array}\\right]$ where $x_{t}=\\mathbb{E}_{a\\sim p_{t}}[a]$   \n8 if $\\begin{array}{r}{\\lambda_{\\operatorname*{max}}(B_{t}-\\sum_{\\tau\\in\\mathcal{T}}B_{s})>0}\\end{array}$ then   \n9 $\\mathcal{T}\\leftarrow\\mathcal{T}\\cup\\{t\\}$   \n10 $b_{t}=-\\alpha\\nabla F(x_{t})$ where $x_{t}=\\mathbb{E}_{a\\sim p_{t}}[a]$ and $\\begin{array}{r}{\\nabla F(x_{t})=\\eta\\sum_{s=1}^{t-1}(\\widehat{\\theta}_{s}-b_{s})}\\end{array}$   \n11 else $b_{t}=0$ ", "page_idx": 31}, {"type": "text", "text": "Lemma J.4 (Proposition 2.3.2 in Nesterov and Nemirovski (1994)). Let $f$ bea $\\nu$ -self-concordant barrieron $\\mathcal{A}\\subseteq\\dot{\\mathbb{R}}^{d}$ and $u,w\\in\\mathrm{int}(A)$ .Then ", "page_idx": 31}, {"type": "equation", "text": "$$\nf(u)-f(w)\\leq\\nu\\log\\left(\\frac{1}{1-\\pi_{w}(u)}\\right).\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "J.3 Regret Analysis ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We perform regret decomposition. For regret comparator $u^{\\star}\\in A$ ,define $x^{\\star}=\\operatorname*{min}_{x\\in A}F(x)$ and $\\begin{array}{r}{u=(1-\\frac{1}{T})u^{\\breve{\\star}}+\\frac{1}{T}x^{\\star}}\\end{array}$ With probability at least $1-\\delta$ ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbf{Reg}_{T}=\\sum_{i=1}^{T}(u^{*}-\\alpha_{i}\\theta_{i})}\\\\ {=\\ }&{\\frac{1}{m_{1}}\\left(u-\\alpha_{i}\\theta_{i}\\right)+\\frac{1}{T}\\frac{T}{\\alpha_{1}\\alpha_{1}}\\left(u^{*}-z^{*},\\theta_{i}\\right)}\\\\ {=\\ }&{\\frac{\\sum_{i=1}^{T}\\left(u-\\alpha_{i}\\theta_{i}\\right)+T}{t}+\\frac{1}{T}\\frac{T}{\\alpha_{1}\\alpha_{1}}\\left(u^{*}-z^{*},\\theta_{i}\\right)}\\\\ {=\\ }&{\\sum_{i=1}^{T}\\left(u-\\bar{x}_{i},\\theta_{i}\\right)+\\mathcal{O}\\left(\\sqrt{T\\log(1/\\beta)}\\right)+2}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{~(defing~\\alpha_{i}=g_{T\\bullet i-\\beta}/(\\theta_{i})~a n d b y~A n a m i s~i n e q u a l~i n e q u a l~i n})}\\\\ {=\\ }&{\\frac{\\sum_{i=1}^{T}\\left(u-\\alpha_{i},\\theta_{i}\\right)+\\frac{1}{T}\\sum_{i=1}^{T}\\left(x_{i}-\\bar{x}_{i},\\theta_{i}\\right)+\\mathcal{O}\\left(\\sqrt{T\\log(1/\\beta)}\\right)}{t\\alpha_{1}}}\\\\ &{=\\underbrace{\\sum_{i=1}^{T}\\left(-\\alpha_{i}\\theta_{i}-\\mathbb{E}_{i}[\\bar{\\theta}_{i}]\\right)+\\sum_{i=1}^{T}\\left(-\\alpha_{i},\\mathbb{E}_{i}[\\bar{\\theta}_{i}]-\\bar{\\theta}_{i}\\right)+\\sum_{i=1}^{T}\\left(u-x_{i},\\bar{\\theta}_{i}+\\bar{h}_{i}\\right)}_{t\\alpha_{1}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{~(beusion~)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "By standard FTRL analysis, we have ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbf{FTRL}\\leq\\underbrace{\\frac{F(u)-\\operatorname*{min}_{x\\in\\mathcal{A}}F(x)}{\\eta}}_{\\substack{\\forall}}+\\mathbb{E}\\left[\\sum_{t=1}^{T}\\operatorname*{max}_{x\\in\\mathcal{A}}\\left\\{\\left\\langle x-x_{t},\\widehat{\\theta}_{t}+b_{t}\\right\\rangle-\\frac{1}{\\eta}D_{F}(x,x_{t})\\right\\}\\right].\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The individual terms Bias, Deviation, Bonus, Penalty, Stability terms are bounded in Lemma J.6, Lemma J.7, Lemma J.9, Lemma J.10, Lemma J.12. ", "page_idx": 32}, {"type": "text", "text": "Lemma J.5. For any $t\\in[T],$ f $a\\sim p_{t}$ , then with probability of at least $1-\\delta$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|a\\|_{\\Sigma_{t}^{-1}}\\leq\\sqrt{d}\\log\\left(\\frac{3d}{\\delta}\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. Define $\\boldsymbol{y}~=~\\boldsymbol{\\Sigma}_{t}^{-\\frac{1}{2}}\\boldsymbol{a}$ .Then $\\mathbb{E}_{y}\\left[y y^{\\top}\\right]~=~\\Sigma_{t}^{-\\frac{1}{2}}\\mathbb{E}_{a\\sim p_{t}}[a a^{\\top}]\\Sigma_{t}^{-\\frac{1}{2}}~=~I.$ Since $p_{t}$ is a logconcave distribution, and log-concavity is preserved under liner transformation, $y$ is also log-concave. Applying Lemma J.1 on it leads to ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left[\\|a\\|_{\\Sigma_{t}^{-1}}^{2}\\geq d\\beta^{2}\\right]=\\operatorname*{Pr}\\left[\\|y\\|_{2}^{2}\\geq d\\beta^{2}\\right]\\leq d\\exp(1-\\beta)\\leq3d\\exp(-\\beta).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Setting $\\delta=3d\\exp(-\\beta)$ , we conclude that with probability at least $\\begin{array}{r}{.-\\delta,\\|a\\|_{\\Sigma_{t}^{-1}}^{2}\\leq d\\log\\left(\\frac{3d}{\\delta}\\right)^{2}}\\end{array}$ \uff1a\u53e3 ", "page_idx": 32}, {"type": "text", "text": "Lemma J.6. With probability at least $1-{\\mathcal{O}}(\\delta)$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbf{Bias}\\leq\\left(\\operatorname*{max}_{t}\\Vert x_{t}-u\\Vert_{\\tilde{\\Sigma}_{t}^{-1}}\\right)\\left(\\sqrt{d\\gamma}T+2\\sqrt{T\\log(1/\\delta)}+\\sqrt{d}\\beta C\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. The proof is the same as that of Lemma H.1. ", "page_idx": 32}, {"type": "text", "text": "Lemma J.7. ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbf{Deviation}\\leq\\mathcal{O}\\left(\\operatorname*{max}_{t}\\|u-x_{t}\\|_{\\widetilde{\\Sigma}_{t}^{-1}}d\\beta\\sqrt{T}\\log(T/\\delta)\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. The proof is the same as that of Lemma H.2. ", "page_idx": 32}, {"type": "text", "text": "Lemma J.8. ", "text_level": 1, "page_idx": 32}, {"type": "equation", "text": "$$\n\\left|{\\mathcal{Z}}\\right|\\leq d\\log_{2}\\left({\\frac{4T}{\\gamma}}\\right).\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Proof. Our proof is similar to Lemma B.12 in Lee et al. (2020). Let $\\{t_{1},\\cdot\\cdot\\cdot,t_{n+1}\\}$ be the rounds such that $b_{t}\\neq0$ . Define $\\begin{array}{r}{\\pmb{A}_{i}=\\sum_{j=1}^{i}{\\pmb{B}_{t_{j}}}}\\end{array}$ . For any $i>1$ since $\\lambda_{\\operatorname*{max}}\\left(B_{t_{i}}-A_{i-1}\\right)>0$ , there exists a vector $y\\in\\mathbb{R}^{d+1}$ such that $y^{\\top}B_{t_{i}}y>y^{\\top}A_{i-1}y$ Thus, $y^{\\top}A_{i}y\\geq2y^{\\top}A_{i-1}y$ Let $z=A_{i-1}^{\\frac{1}{2}}y$ we have $z^{\\top}A_{i-1}^{-\\frac{1}{2}}A_{i}A_{i-1}^{-\\frac{1}{2}}z\\geq2\\|z\\|_{2}^{2}$ . This implies $\\lambda_{\\operatorname*{max}}\\left(A_{i-1}^{-\\frac{1}{2}}A_{i}A_{i-1}^{-\\frac{1}{2}}\\right)\\geq2.$ Moreover, we have $\\lambda_{\\operatorname*{min}}\\left(A_{i-1}^{-\\frac{1}{2}}A_{i}A_{i-1}^{-\\frac{1}{2}}\\right)\\geq1$ because ", "page_idx": 32}, {"type": "equation", "text": "$$\nA_{i-1}^{-\\frac{1}{2}}A_{i}A_{i-1}^{-\\frac{1}{2}}=A_{i-1}^{-\\frac{1}{2}}\\left(A_{i-1}+B_{t_{i}}\\right)A_{i-1}^{-\\frac{1}{2}}\\succeq I.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Thus, det(Ai-1) $\\begin{array}{r}{\\frac{\\operatorname*{det}({\\cal A}_{i})}{\\operatorname*{det}({\\cal A}_{i-1})}=\\operatorname*{det}\\left({\\cal A}_{i-1}^{-\\frac{1}{2}}{\\cal A}_{i}{\\cal A}_{i-1}^{-\\frac{1}{2}}\\right)\\geq2.}\\end{array}$ By induction, we have det $(A_{n+1})\\geq2^{n}\\operatorname*{det}\\left(A_{1}\\right)$ We now give a upper bound for det(An+1) . Define a = $\\pmb{a}=\\left[\\!\\!{\\begin{array}{l}{a}\\\\ {1}\\end{array}}\\!\\!\\right]$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\operatorname*{det}\\left(A_{n+1}A_{1}^{-1}\\right)=\\operatorname*{det}\\left(\\sum_{j=1}^{n+1}B_{t_{j}}B_{t_{1}}^{-1}\\right)\\leq\\left(\\frac{1}{d}\\mathrm{Tr}\\left(\\sum_{j=1}^{n+1}B_{t_{j}}B_{t_{1}}^{-1}\\right)\\right)^{d}.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Notice that for any $t$ $B_{t}\\succeq I$ and $\\begin{array}{r}{\\mathrm{Tr}(B_{t})=\\mathrm{Tr}(I)+\\mathrm{Tr}(\\widetilde{\\Sigma}_{t}^{-1})+\\|x_{t}\\|_{\\widetilde{\\Sigma}_{t}^{-1}}^{2}\\leq\\frac{2(d+1)}{\\gamma}}\\end{array}$ . Thus, we can upper bound the last expression further by ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\left(\\frac1d\\mathrm{Tr}\\left(\\sum_{j=1}^{n+1}B_{t_{j}}\\right)\\right)^{d}\\le\\left(\\frac{2(d+1)(n+1)}{d\\gamma}\\right)^{d}\\le\\left(\\frac{4T}{\\gamma}\\right)^{d}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Overall we have $\\begin{array}{r}{2^{n}\\leq\\frac{\\operatorname*{det}({\\pmb A}_{n+1})}{\\operatorname*{det}({\\pmb A}_{1})}\\leq(4T/\\gamma)^{d}}\\end{array}$ , and thus $n\\leq d\\log_{2}(4T/\\gamma)$ ", "page_idx": 33}, {"type": "text", "text": "Lemma J.9. ", "text_level": 1, "page_idx": 33}, {"type": "equation", "text": "$$\n{\\bf B o n u s}\\leq-\\frac{\\alpha}{8}\\operatorname*{max}_{t}\\|u-x_{t}\\|_{\\widetilde{\\Sigma}_{t}^{-1}}+\\mathcal{O}\\left(\\alpha d^{2}\\log T\\right).\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. Let $\\rho=\\operatorname*{max}_{t}\\|u-x_{t}\\|_{{\\widetilde{\\Sigma}}_{t}^{-1}}$ and $t^{*}=\\mathrm{argmax}_{t}\\left\\lVert u-x_{t}\\right\\rVert_{\\widetilde{\\Sigma}_{t}^{-1}}$ , We discuss two conditions: ", "page_idx": 33}, {"type": "text", "text": "\u00b7If $t^{*}\\in\\mathcal{T}$ , then $\\begin{array}{r}{\\rho^{2}\\leq\\sum_{\\tau\\in\\mathcal{T}}\\|u-x_{t}\\|_{\\widetilde{\\Sigma}_{\\tau}^{-1}}^{2}}\\end{array}$ \u00b7If $t^{*}\\notin\\mathcal{T}$ , then $\\begin{array}{r}{B_{t^{\\star}}\\preceq\\sum_{\\tau\\in\\mathcal{T}}B_{\\tau}}\\end{array}$ . Let $\\pmb{u}\\triangleq\\left[\\!\\!\\begin{array}{l}{u}\\\\ {1}\\end{array}\\!\\!\\right]$ This implies $\\rho^{2}=\\|u-x_{t^{*}}\\|_{\\widetilde{\\Sigma}_{t^{*}}^{-1}}^{2}=\\|u\\|_{B_{t^{*}}}^{2}\\leq\\sum_{\\tau\\in\\mathbb{Z}}\\|u\\|_{B_{\\tau}}^{2}=\\sum_{\\tau\\in\\mathbb{Z}}\\|u-x_{\\tau}\\|_{\\widetilde{\\Sigma}_{\\tau}^{-1}}^{2},$ ", "page_idx": 33}, {"type": "text", "text": "where we use the definitions of $\\pmb{B}_{t}$ and $\\pmb{u}$ in the second and the last equality. ", "page_idx": 33}, {"type": "text", "text": "Thus, $\\begin{array}{r}{\\operatorname*{max}_{t}\\|u-x_{t}\\|_{\\widetilde{\\Sigma}_{t}^{-1}}\\leq\\sum_{\\tau\\in\\mathbb{Z}}\\|u-x_{\\tau}\\|_{\\widetilde{\\Sigma}_{\\tau}^{-1}}.}\\end{array}$ ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{T}{\\gamma\\epsilon}\\left[\\alpha_{t}-u,b_{t}\\right>}\\\\ &{=\\displaystyle\\sum_{\\tau\\in\\mathcal{X}}\\left(x_{\\tau}-u,b_{\\tau}\\right)}\\\\ &{=\\alpha\\displaystyle\\sum_{\\tau\\in\\mathcal{X}}\\left<x_{\\tau}-u,-\\nabla F(x_{\\tau})\\right>}\\\\ &{\\phantom{\\frac{1}{\\gamma}}\\displaystyle\\sum_{\\tau\\in\\mathcal{X}}\\left<x_{\\tau}-u,-\\nabla F(x_{\\tau})\\right>}\\\\ &{\\leq-\\frac{\\alpha}{\\gamma^{\\prime}}\\displaystyle\\sum_{\\tau\\in\\mathcal{X}}\\|u-x_{\\tau}\\|_{\\nabla^{2}F(x_{\\tau})}+4\\alpha d\\|Z\\|+\\frac{2\\alpha\\sqrt{d}|Z|}{\\gamma^{\\prime}}}\\\\ &{(\\displaystyle\\mathrm{Lemma})\\,\\lambda\\,\\mathrm{and}\\,F\\mathrm{~is~}d\\times\\mathrm{elf\\co}}\\\\ &{\\leq-\\displaystyle\\frac{\\alpha}{2\\gamma^{\\prime}}\\displaystyle\\sum_{\\tau\\in\\mathcal{X}}\\|u-x_{\\tau}\\|_{\\widetilde{\\Sigma}^{-1}}+\\mathcal{O}(\\alpha d^{2}\\log T)\\quad\\left(\\nabla^{2}F(x_{\\tau})-\\Sigma_{\\tau}^{-1}\\geq\\frac{1}{4}\\widetilde{\\Sigma}_{\\tau}^{-1}\\right.}\\\\ &{\\leq-\\displaystyle\\frac{\\alpha}{2\\gamma^{\\prime}}\\displaystyle\\operatorname*{max}_{\\tau}\\|u-x_{\\tau}\\|_{\\widetilde{\\Sigma}^{-1}}+\\mathcal{O}(\\alpha d^{2}\\log T).}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Lemma J.10. ", "text_level": 1, "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbf{Penalty}\\leq{\\frac{d\\log(T)}{\\eta}}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Proof. Since $x^{\\star}=\\operatorname*{min}_{x\\in A}F(x)$ and $\\begin{array}{r}{\\pi_{x^{\\star}}(u)\\leq1-\\frac{1}{T}}\\end{array}$ from Eq. (25). We have from Lemma J.4 ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbf{Penalty}=\\frac{{F(u)}-F(x^{\\star})}{\\eta}\\leq\\frac{d\\log(T)}{\\eta}.\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Lemma J.11 (Lemma 17 in Zimmert and Lattimore (2022). Let $F$ be the entropic barrier and $\\begin{array}{r}{\\|w\\|_{\\nabla^{2}F(x_{t})^{-1}}\\leq\\frac{1}{16\\eta}.}\\end{array}$ then ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{x\\in\\mathcal{A}}\\left\\{\\langle x-x_{t},w\\rangle-\\frac{1}{\\eta}D_{F}(x,x_{t})\\right\\}\\leq2\\eta\\|w\\|_{\\nabla^{2}F(x_{t})^{-1}}^{2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Lemma J.12. With probability at least $1-\\delta$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbf{Stability}\\leq\\mathcal{O}\\left(\\eta\\beta^{2}d T+\\eta\\alpha^{2}d^{2}\\log T\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Proof.Since $F$ is a $d$ -self-concordant barrier (Chewi, 2023), we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\|b_{t}\\|_{\\nabla^{2}F(x_{t})^{-1}}=\\alpha\\|\\nabla F(x_{t})\\|_{\\nabla^{2}F(x_{t})^{-1}}\\leq\\alpha\\sqrt{d}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "By Lemma J.2, we have $\\widetilde{\\Sigma}_{t}^{-1}\\preceq\\left(\\mathbb{E}_{a\\sim\\tilde{p}_{t}}[a a^{\\top}]\\right)^{-1}\\preceq2\\Sigma_{t}^{-1}$ , and thus ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\|\\widehat{\\theta}_{t}\\|_{\\nabla^{2}F(x_{t})^{-1}}^{2}=\\|\\widetilde{\\Sigma}_{t}^{-1}a_{t}r_{t}\\|_{\\Sigma_{t}}^{2}\\leq2a_{t}^{\\top}\\widetilde{\\Sigma}_{t}^{-1}a_{t}\\leq2d\\beta^{2}.\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Thus, $\\|\\widehat{\\theta}_{t}+b_{t}\\|_{\\nabla^{2}F(x_{t})^{-1}}\\le\\beta\\sqrt{2d}+\\alpha\\sqrt{d}$ $\\begin{array}{r}{\\eta\\le\\frac{1}{16(\\beta\\sqrt{2d}+\\alpha\\sqrt{d})}}\\end{array}$ byLemma J11, we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\mathrm{Stability}\\le2\\eta\\sum_{t=1}^{T}\\|\\widehat{\\theta}_{t}+b_{t}\\|_{\\nabla^{2}F(x_{t})^{-1}}^{2}}\\\\ &{\\qquad\\qquad\\le4\\eta\\displaystyle\\sum_{t=1}^{T}\\|\\widehat{\\theta_{t}}\\|_{\\nabla^{2}F(x_{t})^{-1}}^{2}+4\\eta\\displaystyle\\sum_{\\tau\\in\\mathbb{Z}}\\|b_{\\tau}\\|_{\\nabla^{2}F(x_{\\tau})^{-1}}^{2}}\\\\ &{\\qquad\\le\\mathcal{O}\\left(\\eta\\beta^{2}d T+\\eta\\alpha^{2}d\\vert\\mathcal{Z}\\vert\\right)}\\\\ &{\\qquad\\le\\mathcal{O}\\left(\\eta\\beta^{2}d T+\\eta\\alpha^{2}d^{2}\\log T\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Theorem J.13. Algorithm $^{\\sc6}$ ensures with probability at least $1-\\delta$ $\\mathrm{Reg}_{T}\\,=\\,\\widetilde{\\mathcal{O}}\\big(d^{3}\\sqrt{T}+d^{\\frac{5}{2}}C\\big)$ where $\\widetilde O(\\cdot)$ hides polylog $(T/\\delta)$ factors. ", "page_idx": 34}, {"type": "text", "text": "Proof. Putting Lemma J.6, Lemma J.7, Lemma J.9, Lemma J.10, Lemma J.12 into Eq. (26) and Eq. (27), with 7 \u2264 16(\u03b2/2d+aVa) and $\\begin{array}{r}{\\gamma=\\frac{1}{T}}\\end{array}$ , we have with probability at least $1-{\\mathcal{O}}(\\delta)$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathrm{Reg}\\le\\operatorname*{max}_{t}\\|u-x_{t}\\|_{\\tilde{\\Sigma}_{t}^{-1}}\\left(\\widetilde{\\mathcal{O}}(d\\sqrt{T}+\\sqrt{d}C)-\\frac{\\alpha}{8}\\right)+\\widetilde{\\mathcal{O}}\\left(\\alpha d^{2}+\\frac{d}{\\eta}+\\eta\\alpha^{2}d^{2}+\\eta d T+\\sqrt{T}\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "By setting $\\begin{array}{r}{\\frac{\\alpha}{8}=\\tilde{\\Theta}(d\\sqrt{T}+\\sqrt{d}C)}\\end{array}$ , we have ", "page_idx": 34}, {"type": "equation", "text": "$$\n{\\mathrm{Reg}}\\leq{\\widetilde{\\mathcal{O}}}\\left(d^{3}{\\sqrt{T}}+d^{\\frac{5}{2}}C+{\\frac{d}{\\eta}}+\\eta d^{4}T+\\eta d^{3}C^{2}\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Setting = $\\begin{array}{r}{\\eta=\\frac{1}{160d\\sqrt{T}+32\\alpha\\sqrt{d}}}\\end{array}$ 160dVT+32aVd, we get ", "page_idx": 34}, {"type": "equation", "text": "$$\n{\\mathrm{Reg}}\\leq{\\widetilde{\\mathcal{O}}}\\left(d^{3}{\\sqrt{T}}+d^{\\frac{5}{2}}C\\right).\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "K  Gap-dependent Misspecification ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "We consider the same setting as Liu et al. (2023a), but remove an assumption for it. Consider bandit learning with general reward function $f_{0}$ wherefor any action $\\boldsymbol{x}_{t}\\in\\mathcal{X}\\dot{\\subset}\\mathbb{R}^{d}$ at round $t$ , the learner get reward $y_{t}\\bar{=}\\;f_{0}(x_{t})+\\eta_{t}$ where $\\eta_{t}\\mathbf{s}$ are zero mean, $\\sigma$ -sub-Gaussian noise. We assume there exists a linear function $\\theta^{\\top}x$ that could approximate $f_{0}(x)$ in the following manner. ", "page_idx": 34}, {"type": "text", "text": "Definition 3. ", "text_level": 1, "page_idx": 35}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{x\\in\\mathcal{X}}\\left|\\frac{\\theta^{\\top}x-f_{0}(x)}{f_{0}^{\\star}-f_{0}(x)}\\right|\\leq\\rho\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $f_{0}^{\\star}=\\operatorname*{max}_{x\\in\\mathcal{X}}f_{0}(x)$ and $0\\leq\\rho<1$ ", "page_idx": 35}, {"type": "text", "text": "Th algorithm in Liu e al. 2023a only gets  (\u221aT) regret when p \u2264 avgT a and we improve it to $\\textstyle\\rho\\leq{\\frac{1}{\\sqrt{d}}}$ by using elimination-based methods in Algorithm 7. For any design $\\pi$ on action set $\\boldsymbol{\\mathcal{A}}$ define ", "page_idx": 35}, {"type": "equation", "text": "$$\nG(\\pi)=\\sum_{a\\in{\\mathcal{A}}}\\pi(a)a a^{\\top}\\qquad g(\\pi)=\\operatorname*{max}_{a\\in{\\mathcal{A}}}\\|a\\|_{G(\\pi)^{-1}}^{2}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Algorithm 7: Phased Elimination for Misspecification ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "1 Input: Action set $A_{1}=A$ Initialize $\\begin{array}{r}{m_{1}=\\left\\lceil64d\\log\\log d\\log\\left(\\frac{|\\mathcal{A}|}{\\delta}\\right)\\right\\rceil+16.}\\end{array}$   \nfor $\\ell=1,2,\\cdots\\,,L$ do   \n3 Find the approximate G-optimal design $\\pi_{\\ell}$ on $A_{\\ell}$ with $g(\\pi)\\leq2d$ and $|\\operatorname{Supp}(\\pi)|\\leq4d\\log\\log d+16$   \n4 Compute $\\dot{u}_{\\ell}(a)=\\lceil m_{\\ell}\\pi_{\\ell}(a)\\rceil$ and $\\begin{array}{r}{u_{\\ell}=\\sum_{a\\in A_{\\ell}}u_{\\ell}(a)}\\end{array}$   \n5 Take each action $a\\in\\mathcal{A}_{\\ell}$ exactly $u(a)$ timeswith reward $y(a)$ Calculate ${\\widehat{\\theta}}_{\\ell}=G_{\\ell}^{-1}\\sum_{a\\in A_{\\ell}}u(a)a y(a)\\quad{\\mathrm{where}}\\quad G_{\\ell}=\\sum_{a\\in A_{\\ell}}u(a)a a^{\\top}$ Update active action set $A_{\\ell+1}=\\left\\{a\\in A_{\\ell}:\\operatorname*{max}_{b\\in{\\mathcal{A}}_{\\ell}}\\langle\\widehat{\\theta}_{\\ell},b\\rangle-\\langle\\widehat{\\theta}_{\\ell},a\\rangle\\leq\\sqrt{\\frac{4d}{m_{\\ell}}\\log\\left(\\frac{|A|}{\\delta}\\right)}+\\frac{1}{2^{\\ell}}\\right\\}$ me+1 \u2190 4me ", "page_idx": 35}, {"type": "text", "text": "Define $\\mathrm{{Gap}}(x)=f_{0}^{\\star}-f_{0}(x)$ as the suboptimal gap at point $x$ . Definition 3 implies the true value function $f_{0}(x)=\\boldsymbol{\\theta}^{\\top}\\boldsymbol{x}+\\Delta(x)$ where $|\\Delta(x)|\\leq\\rho(f_{0}^{\\star}-f_{0}(x))=\\rho\\mathrm{Gap}(x)$ . We further assume that $|\\Delta(x)|\\leq\\rho\\mathrm{Gap}(x)$ which captures both standard uniform misspecification and the gap-dependent misspecification. With this assumption, our main result is summarized in Theorem K.1. ", "page_idx": 35}, {"type": "text", "text": "Theorem K.1. For action $a$ assume $y(a)=f_{0}(a)+\\eta_{a}$ where $\\eta_{a}$ is zero-mean sub-gaussian noise and $f_{0}(a)=\\theta^{\\top}a+\\Delta(a)$ with $|\\Delta(a)|\\leq\\rho{\\bf G a p}(a).$ If $\\textstyle\\rho\\leq{\\frac{1}{64{\\sqrt{d}}}}$ ,with probability of at least $1-\\delta$ we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\mathrm{Reg}_{T}^{\\mathcal{M}^{\\star}}\\leq\\mathcal{O}\\left(\\sqrt{d T\\log\\left|A\\right|/\\delta}\\right)\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Proof. First, with probability of at least $1-\\delta$ , for any $\\ell$ and $b\\in\\mathcal{A}_{\\ell}$ , we have ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\left\\langle b,\\widehat{\\theta}_{\\ell}-\\theta\\right\\rangle\\Big|=\\left|b^{\\top}G_{\\ell}^{-1}\\displaystyle\\sum_{a\\in A_{\\ell}}u(a)a y(a)-b^{\\top}\\theta\\right|}\\\\ {\\displaystyle\\qquad\\qquad=\\left|b^{\\top}G_{\\ell}^{-1}\\displaystyle\\sum_{a\\in A_{\\ell}}u(a)a a^{\\top}{\\eta}_{a}+b^{\\top}G_{\\ell}^{-1}\\displaystyle\\sum_{a\\in A_{\\ell}}u(a)a\\Delta(a)\\right|}\\\\ {\\displaystyle\\qquad\\qquad\\leq\\sqrt{\\frac{4d}{m_{\\ell}}\\log\\left(\\frac{|A|}{\\delta}\\right)}+\\left|b^{\\top}G_{\\ell}^{-1}\\displaystyle\\sum_{a\\in A_{\\ell}}u(a)a\\Delta(a)\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where in the last step, we use standard concentration by Equation (20.2) of Lattimore and Szepesvari (2020) and the apply union bound for all actions. ", "page_idx": 35}, {"type": "text", "text": "For the last term, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\left|b^{\\top}G_{\\ell}^{-1}\\sum_{a\\in A_{\\ell}}u(a)a\\right|\\leq\\operatorname*{max}_{c\\in A_{\\ell}}\\Delta(c)\\cdot{\\sqrt{\\left(\\sum_{a\\in A_{\\ell}}u(a)\\right)b^{\\top}G_{\\ell}^{-1}\\sum_{a\\in A_{\\ell}}u(a)a a^{\\top}\\Sigma_{\\ell}^{-1}b}}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "(Cauchy-Schwarz) ", "page_idx": 36}, {"type": "equation", "text": "$$\n=\\operatorname*{max}_{c\\in{\\cal A}_{\\ell}}\\Delta(c)\\cdot\\sqrt{u\\|b\\|_{{\\cal G}_{\\ell}^{-1}}^{2}}\\leq\\operatorname*{max}_{c\\in{\\cal A}_{\\ell}}\\Delta(c)\\cdot\\sqrt{\\frac{2d u}{m_{\\ell}}}\\leq\\operatorname*{max}_{c\\in{\\cal A}_{\\ell}}\\Delta(c)\\cdot2\\sqrt{d}.\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Thus, for any $b\\in\\mathcal{A}_{\\ell}$ \uff0c", "page_idx": 36}, {"type": "text", "text": "$\\left|\\langle b,{\\widehat{\\theta}}_{\\ell}-\\theta\\rangle\\right|\\leq{\\sqrt{{\\frac{4d}{m_{\\ell}}}\\log\\left({\\frac{|A|}{\\delta}}\\right)}}+2{\\sqrt{d}}\\operatorname*{max}_{a\\in A_{\\ell}}\\Delta(a)={\\sqrt{{\\frac{4d}{m_{\\ell}}}\\log\\left({\\frac{|A|}{\\delta}}\\right)}}+2{\\sqrt{d}}\\rho\\operatorname*{max}_{a\\in A_{\\ell}}\\mathrm{Gap}(a)$ When $\\ell=1$ since $\\begin{array}{r}{m_{1}=\\lceil256d\\log\\log d\\log\\left(\\frac{|A|}{\\delta}\\right)\\rceil\\!+\\!16}\\end{array}$ wehave $\\begin{array}{r}{\\sqrt{\\frac{4d}{m_{1}}\\log\\left(\\frac{|\\mathcal{A}|}{\\delta}\\right)}\\leq\\frac{1}{2^{4}}}\\end{array}$ \u2264. Moreover, by trivial bound, $\\operatorname*{max}_{a\\in\\mathcal{A}_{1}}\\mathbf{Gap}(a)\\leq2$ and $a^{\\star}\\in\\mathcal{A}_{1}$ ", "page_idx": 36}, {"type": "text", "text": "We will jointly do two inductions. Assume for round $\\ell$ we have $a^{\\star}\\in A_{\\ell}$ and $\\operatorname*{max}_{a\\in{\\cal A}_{\\ell}}\\operatorname{Gap}(a)\\leq$ $\\frac{1}{2^{\\ell-2}}$ . We first show $a^{\\star}\\in\\mathcal{A}_{\\ell+1}$ . Thus, for any $b\\in\\mathcal{A}_{\\ell}$ , given $\\textstyle\\rho\\leq{\\frac{1}{64{\\sqrt{d}}}}$ 64Va, since me = 4&-1m1, we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\left|\\langle b,\\widehat{\\theta}_{\\ell}-\\theta\\rangle\\right|\\leq\\sqrt{\\frac{4d}{m_{\\ell}}\\log\\left(\\frac{|{\\cal A}|}{\\delta}\\right)}+2\\sqrt{d}\\rho\\operatorname*{max}_{a\\in{\\cal A}_{\\ell}}{\\mathrm{Gap}}(a)\\leq\\frac{1}{2^{\\ell-1}}\\frac{1}{2^{4}}+\\frac{1}{2^{\\ell+3}}=\\frac{1}{2^{\\ell+2}}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "From the induction hypothesis, let $\\widehat{a}_{\\ell}=\\arg\\operatorname*{max}_{b\\in\\mathcal{A}_{\\ell}}\\langle\\widehat{\\theta}_{\\ell},b\\rangle$ wehave ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{\\theta_{\\ell}^{\\intercal}\\widehat{a}_{\\ell}}-\\widehat{\\theta_{\\ell}^{\\intercal}}a^{\\star}\\leq\\theta^{\\intercal}\\widehat{a}_{\\ell}-\\theta^{\\intercal}a^{\\star}+\\underbrace{\\widehat{\\theta_{\\ell}^{\\intercal}\\widehat{a}_{\\ell}}-\\theta^{\\intercal}\\widehat{a}_{\\ell}}_{\\leq\\frac{1}{2^{\\ell+2}}}+\\underbrace{\\theta^{\\intercal}a^{\\star}-\\widehat{\\theta_{\\ell}^{\\intercal}}a^{\\star}}_{\\leq\\frac{1}{2^{\\ell+2}}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\underbrace{f_{0}(\\widehat{a}_{\\ell})-f_{0}(a^{\\star})}_{\\leq0}+|\\Delta(\\widehat{a}_{\\ell})|+\\frac{1}{2^{\\ell+1}}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\rho\\mathrm{Gap}(\\widehat{a}_{\\ell})+\\frac{1}{2^{\\ell+1}}\\leq\\frac{1}{2^{\\ell}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "For $\\ell+1$ , the remaining actions $a\\in A_{\\ell+1}$ satisfy ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{b\\in\\mathcal{A}_{\\ell}}\\langle\\widehat\\theta_{\\ell},b\\rangle-\\langle\\widehat\\theta_{\\ell},a\\rangle\\leq\\sqrt{\\frac{4d}{m_{\\ell}}\\log\\left(\\frac{|\\mathcal{A}|}{\\delta}\\right)}+\\frac{1}{2^{\\ell}}\\leq\\frac{1}{2^{\\ell+3}}+\\frac{1}{2^{\\ell}}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "This implies $a^{\\star}\\in\\mathcal{A}_{\\ell+1}$ . Moreover, since $a^{\\star}\\in A_{\\ell}$ , for $a\\in A_{\\ell+1}$ , we have ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Gap}(a)=f_{0}^{\\star}-f_{0}(a)=\\theta^{\\top}a^{\\star}-\\theta^{\\top}a+|\\Delta(a)|}\\\\ &{\\qquad\\qquad\\qquad\\leq\\widehat{\\theta_{\\ell}}^{\\top}a^{\\star}-\\widehat{\\theta_{\\ell}^{\\top}}a+\\rho\\mathrm{Gap}(a)+(\\theta-\\widehat{\\theta_{\\ell}})^{\\top}a^{\\star}+(\\widehat{\\theta_{\\ell}}-\\theta)^{\\top}a}\\\\ &{\\qquad\\qquad\\qquad\\leq\\widehat{\\theta_{\\ell}}^{\\top}a^{\\star}-\\widehat{\\theta_{\\ell}^{\\top}}a+\\rho\\mathrm{Gap}(a)+(\\theta-\\widehat{\\theta_{\\ell}})^{\\top}a^{\\star}+(\\widehat{\\theta_{\\ell}}-\\theta)^{\\top}a}\\\\ &{\\qquad\\qquad\\qquad\\leq\\underbrace{\\widehat{\\theta_{\\ell}^{\\top}}a^{\\star}-\\widehat{\\theta_{\\ell}^{\\top}}\\widehat{a}_{\\ell}}_{\\leq0\\mathrm{~given~}a^{\\star}\\in A_{\\ell}}+\\underbrace{\\widehat{\\theta_{\\ell}^{\\top}}\\widehat{a}_{\\ell}-\\widehat{\\theta_{\\ell}^{\\top}}a}_{\\leq\\frac{1}{2^{\\ell+3}}+\\frac{1}{2^{\\ell}}}+\\rho\\mathrm{Gap}(a)+\\frac{1}{2^{\\ell+1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Given $\\begin{array}{r}{\\rho\\leq\\frac{1}{64\\sqrt{d}}\\leq\\frac{1}{64}}\\end{array}$ , this implies ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\mathtt{G a p}(a)\\leq{\\frac{1}{1-\\rho}}({\\frac{1}{2^{\\ell}}}+{\\frac{1}{2^{\\ell+1}}}+{\\frac{1}{2^{\\ell+3}}})\\leq{\\frac{63}{64}}({\\frac{1}{2^{\\ell}}}+{\\frac{1}{2^{\\ell+1}}}+{\\frac{1}{2^{\\ell+3}}})\\leq{\\frac{1}{2^{\\ell-1}}}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The above arguments show that as $\\ell$ increases, $\\operatorname*{max}_{a\\in\\mathcal{A}_{\\ell}}\\mathbf{Gap}(a)$ will shrink by $\\frac{1}{2}$ at every step. Since for $a\\in\\mathcal{A}_{\\ell}$ Gap(a)\u2264 2 =0 (\u221a/ log()) ", "page_idx": 36}, {"type": "text", "text": "Finally, given $L=\\log(T)$ , we have ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathrm{Reg}=\\sum_{\\ell=1}^{L}\\sum_{a\\in A_{\\ell}}u_{\\ell}(a)\\mathrm{Gap}(a)\\leq\\sum_{\\ell=1}^{L}m_{\\ell}\\sqrt{\\frac{4d}{m_{\\ell}}\\log\\left(\\frac{|A|}{\\delta}\\right)}\\leq\\mathcal{O}\\left(\\sqrt{d T\\log|A|/\\delta}\\right)\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "When $|{\\mathcal{A}}|\\geq T^{d}$ , we can apply similar covering number arguments as in Appendix E, replacing $\\mathcal{A}_{1}$ With a $\\textstyle{\\frac{1}{T}}$ -netof $\\boldsymbol{\\mathcal{A}}$ . Combined with Theorem K.1, this yields the result in Theorem 6.2. ", "page_idx": 37}, {"type": "text", "text": "Using the hard instance for $\\epsilon$ -misspecified linear bandits setting in Lattimore et al. (2020), we now show that $\\begin{array}{r}{\\rho=\\tilde{\\Omega}\\big(\\frac{1}{\\sqrt{d}}\\big)}\\end{array}$ fora algorithm to acheve sub-lineareget, proving the above algorithm is optimal in terms of $\\rho$ assumption. ", "page_idx": 37}, {"type": "text", "text": "Theorem K.2. If p \u2265\u2191 $\\rho\\ge\\sqrt{\\frac{8\\log(3T)}{d-1}}$ then there exists an instance that $R_{T}=\\Omega(\\rho T)$ ", "page_idx": 37}, {"type": "text", "text": "Proof. Using Theorem F.5 in Lattimore et al. (2020), there exist a discrete time-invariant action space $\\{a_{i}\\in\\mathbb{R}^{d}\\}_{i=1}^{3\\overline{{T}}}$ thatsatisiesthesetwoconitions? ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{{2}.}&{{}\\langle a_{i},a_{j}\\rangle\\leq\\sqrt{\\frac{8\\log(3T)}{d-1}}}&{\\forall i\\not=j}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "and let $\\begin{array}{r}{\\theta^{*}=\\sqrt{\\frac{d-1}{8\\log(3T)}}\\epsilon a_{i^{*}}}\\end{array}$ , admipatat aalll arms be $\\epsilon$ to make the true expected reward zero. Defining $\\tau:=\\operatorname*{max}(t|i_{s}\\neq i^{*}\\quad\\forall s\\leq t)$ , we have $\\begin{array}{r}{\\mathbb{E}[R_{T}]\\ge\\sqrt{\\frac{d-1}{8\\log(3T)}}\\epsilon\\mathbb{E}[\\tau]}\\end{array}$ Since the observed rewards are independent of $a_{i^{*}}$ before time $\\tau$ and $i^{*}$ is chosen randomly, we have $\\begin{array}{r}{\\mathbb{E}[\\tau]\\geq\\operatorname*{min}\\lbrace T,\\frac{3T-1}{2}\\rbrace}\\end{array}$ .So, ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathbb{E}[R_{T}]\\geq\\epsilon T\\sqrt{\\frac{d-1}{8\\log(3T)}}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Finally, we have $\\begin{array}{r}{\\rho\\,\\ge\\,\\frac{\\epsilon}{\\sqrt{\\frac{d-1}{8\\log(3T)}}\\epsilon-0}=\\sqrt{\\frac{8\\log(3T)}{d-1}}}\\end{array}$ , so choosing $\\begin{array}{r}{\\epsilon=\\operatorname*{min}(\\sqrt{\\frac{8\\log(3T)}{d-1}}}\\end{array}$ 8 log(3T) completes the proof showing linear regret when $\\rho$ is large enough. \u53e3 ", "page_idx": 37}, {"type": "text", "text": "L  General Reduction from Corruption-Robust Algorithms to Misspecification ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "In this section, we extend the results of Section 6 to the reinforcement learning setting. We consider episodicMDPs, denoted bya tuple $\\mathcal{M}=(\\mathcal{S},\\mathcal{A},\\{P_{h}\\}_{h=1}^{H},\\{r_{h}\\}_{h=1}^{H},s_{1})$ for $\\boldsymbol{S}$ the set of states $\\boldsymbol{\\mathcal{A}}$ the set of actions, $P_{h}:S\\times A\\to\\triangle_{S}$ the transition kernel, $r_{h}:\\bar{S}\\times A\\to\\triangle_{[0,1]}$ the reward, and $s_{1}$ the starting state. We assume each episode starts in state $s_{1}$ , where the agent takes action $a_{1}$ \uff0c transitions to $\\dot{s}_{2}\\sim P_{1}(\\cdot\\mid s_{1},a_{1})$ and receives reward $r_{1}\\sim r_{1}(s_{1},a_{1})$ . This proceeds for $H$ steps at which point theisode teminates and the processresetsWe assume that $\\textstyle\\sum_{h=1}^{H}r_{h}\\in[0,1]$ almost surely (note that the linear bandit setting with rewards in [-1,1] can be incorporated into this with a simple rescaling). ", "page_idx": 37}, {"type": "text", "text": "$\\pi$ \uff0cat $\\pi_{h}:S\\to\\triangle_{S}$ $\\pi$ $\\mathcal{M}$ $V_{0}^{\\mathcal{M},\\pi}:=\\mathbb{E}^{\\mathcal{M},\\pi}[\\sum_{h=1}^{H}r_{h}]$ $\\bar{\\mathcal{F}}\\subseteq\\{\\dot{S}\\times\\mathcal{A}\\rightarrow\\mathbb{R}\\}$ . In the MDP setting, we define regret on MDP $\\mathcal{M}$ as: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\mathsf{R e g}_{T}^{\\mathcal{M}}:=T\\cdot\\operatorname*{sup}_{\\pi}V_{0}^{\\mathcal{M},\\pi}-\\sum_{t=1}^{T}V_{0}^{\\mathcal{M},\\pi_{t}}.\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "In the MDP setting, we consider the following notion of misspecification ", "page_idx": 37}, {"type": "text", "text": "Definition 4 (Misspecification). For our environment of interest ${\\mathcal{M}}^{\\star}$ ,thereexists someenvironment $\\mathcal{M}_{0}$ such that, for each $f_{h+1}\\in{\\mathcal{F}}$ $\\pi_{.}$ and $(s,a,h)$ wehave: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|\\mathbb{E}^{\\boldsymbol{M}^{\\star},\\pi}[r_{h}+f_{h+1}(s_{h+1},a_{h+1})\\mid s_{h}=s,a_{h}=a]\\right.}\\\\ &{\\qquad-\\left.\\mathbb{E}^{\\boldsymbol{M}_{0},\\pi}[r_{h}+f_{h+1}(s_{h+1},a_{h+1})\\mid s_{h}=s,a_{h}=a]\\right|\\leq\\epsilon_{h}^{\\mathrm{mis}}(s,a)}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "and ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\exists f_{h}\\in\\mathcal{F}\\,s.t.\\;\\;f_{h}(s,a)=\\mathbb{E}^{\\mathcal{M}_{0}}[r_{h}+\\operatorname*{max}_{a^{\\prime}}f_{h+1}(s_{h+1},a^{\\prime})\\mid s_{h}=s,a_{h}=a]\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "for some $\\epsilon_{h}^{\\mathrm{mis}}(s,a)>0$ ", "page_idx": 38}, {"type": "text", "text": "We make the following assumption on gap-dependent misspecification. ", "page_idx": 38}, {"type": "text", "text": "Assumption 4 (Gap-Dependent Misspecification). For any policy $\\pi$ wehave ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathbb{E}^{\\mathcal{M}^{\\star},\\pi}\\left[\\sum_{h=1}^{H}\\epsilon_{h}^{\\mathrm{mis}}(s_{h},a_{h})\\right]\\leq\\rho\\cdot\\Delta(\\pi)\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "for some $\\rho\\ge[0,1)$ ", "page_idx": 38}, {"type": "text", "text": "We are interested in relating the above misspecification setting to the corruption-robust setting. In the MDP setting, we allow both the reward and transitions to be corrupted. For some MDP $\\mathcal{M}$ define the corruption at episode $t$ andstep $h$ as: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\epsilon_{t,h}(s_{h}^{t},a_{h}^{t}):=\\operatorname*{sup}_{g\\in\\{S\\times A\\to[0,H]\\}}|(\\mathcal{T}^{h}g-\\mathcal{T}_{b}^{h}g)(s_{h}^{t},a_{h}^{t})|\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "where ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathcal{T}^{h}g(s,a):=\\mathbb{E}^{\\mathcal{M}}[r_{h}+\\operatorname*{max}_{a^{\\prime}}g(s_{h+1},a^{\\prime})\\mid s_{h}=s,a_{h}=a]\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "denotes the Bellman operator, and $\\mathcal{T}_{b}^{h}$ denotes the corrupted Bellman operator, i.e. $\\mathcal{T}_{b}^{h}$ denotes the expected reward and next state under the corrupted reward and transition distribution. We denote the total corruption level as ", "page_idx": 38}, {"type": "equation", "text": "$$\nC:=\\sum_{t=1}^{T}\\sum_{h=1}^{H}\\epsilon_{t,h}\\big(s_{h}^{t},a_{h}^{t}\\big).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Note that this definition of corruption encompasses both bandits and RL with function approximation.   \nNow assume we have access to the following oracle. ", "page_idx": 38}, {"type": "text", "text": "Assumption 5. We have access to a regret minimization algorithm which takes as input $\\mathcal{F}$ andsome $C^{\\prime}$ andwithprobability atleast $1-\\delta$ hasregretboundedon $\\mathcal{M}_{\\mathrm{0}}$ as ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathrm{Reg}_{T}^{\\mathcal{M}_{0}}\\le\\mathcal{C}_{1}(\\delta,T)\\sqrt{T}+\\mathcal{C}_{2}(\\delta,T)C^{\\prime}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "$i f{\\cal C}^{\\prime}\\geq{\\cal C}.$ and by HT otherwise, for $C$ as defined above and for (problem-dependent) constants $\\mathcal{C}_{1}(\\delta,T),\\mathcal{C}_{2}(\\delta,T)$ which may scale at most logarithmically with $T$ and $\\frac{1}{\\delta}$ ", "page_idx": 38}, {"type": "text", "text": "Before stating our main reduction from corruption-robust to gap-dependent misspecification, we require the following assumption. ", "page_idx": 38}, {"type": "text", "text": "Assumption 6. For any $\\pi$ ,we have that there exists some $f\\ \\in\\ {\\mathcal{F}}$ such that for all $(s,a,h)$ \uff0c $Q_{h}^{\\mathcal{M}_{0},\\pi}(s,a)=f_{h}(s,a)$ ", "page_idx": 38}, {"type": "text", "text": "We then have the following result. ", "page_idx": 38}, {"type": "text", "text": "Theorem L.1. Assume our environment satisfies Assumption 4. Then under Assumption $^{5}$ and Assumption $\\theta,$ as long as 1/2,wihprobabiliy a ast 2wecanachieverege bounded as: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\mathsf{R e g}_{T}^{\\mathcal{M}^{\\star}}\\leq\\frac{3}{1-\\rho}\\cdot\\mathcal{C}_{1}(\\frac{\\delta}{T},T)\\sqrt{T}+\\frac{2}{1-\\rho}\\cdot\\left(H\\sqrt{2T\\log(1/\\delta)}+H\\right).\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "ProofofTheorem $L.l$ . First, note that by Assumption 4, we can bound ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\mathbb{E}^{\\mathcal{M}^{\\star},\\pi_{t}}\\left[\\sum_{h=1}^{H}\\epsilon_{h}^{\\mathrm{mis}}(s_{h},a_{h})\\right]\\leq\\sum_{t=1}^{T}\\rho\\cdot\\Delta(\\pi_{t})\\leq\\rho\\cdot\\mathrm{Reg}_{T}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Where we abbreviate $\\mathbf{Reg}_{T}:=\\mathbf{Reg}_{T}^{\\mathcal{M}^{\\star}}$ Furthermore, note that under Assumption 4 interating with $\\mathcal{M}^{\\star}$ is equivalent to interacting with $\\mathcal{M}_{\\mathrm{0}}$ but where the rewards and transitions are corrupted up to level $\\epsilon_{h}^{\\mathrm{mis}}(s,a)$ $(s,a,h)$ ", "page_idx": 39}, {"type": "text", "text": "Relating Regret on $\\mathcal{M}_{\\mathrm{0}}$ to ${\\mathcal{M}}^{\\star}$ . Define the regret on $\\mathbf{\\mathcal{M}}_{0}$ as ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathsf{R e g}_{T}^{\\mathcal{M}_{0}}:=T\\cdot\\operatorname*{sup}_{\\pi}\\mathbb{E}^{\\mathcal{M}_{0},\\pi}\\left[\\sum_{h=1}^{H}r_{h}\\right]-\\sum_{t=1}^{T}\\mathbb{E}^{\\mathcal{M}_{0},\\pi_{t}}\\left[\\sum_{h=1}^{H}r_{h}\\right].\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Under Assumption 4, we have that $\\begin{array}{r}{\\mathbb{E}^{\\boldsymbol{\\mathcal{M}}^{\\star},\\pi^{\\star}}\\left[\\sum_{h=1}^{H}\\epsilon_{h}^{\\mathrm{mis}}(s_{h},a_{h})\\right]=0}\\end{array}$ Lemma L.2 then implies that ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbb{E}^{\\mathcal{M}^{\\star},\\pi^{\\star}}\\left[\\sum_{h=1}^{H}r_{h}\\right]=\\mathbb{E}^{\\mathcal{M}_{0},\\pi^{\\star}}\\left[\\sum_{h=1}^{H}r_{h}\\right]\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and so ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathbb{E}^{\\mathcal{M}^{\\star},\\pi^{\\star}}\\left[\\sum_{h=1}^{H}r_{h}\\right]\\leq\\operatorname*{sup}_{\\pi}\\mathbb{E}^{\\mathcal{M}_{0},\\pi}\\left[\\sum_{h=1}^{H}r_{h}\\right].\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Furthermore, Lemma L.2 also implies ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\left|\\mathbb{E}^{\\mathcal{M}_{0},\\pi_{t}}\\left[\\sum_{h=1}^{H}r_{h}\\right]-\\mathbb{E}^{\\mathcal{M}^{\\star},\\pi_{t}}\\left[\\sum_{h=1}^{H}r_{h}\\right]\\right|\\le\\mathbb{E}^{\\mathcal{M}^{\\star},\\pi_{t}}\\left[\\sum_{h=1}^{H}\\epsilon_{h}^{\\operatorname*{mis}}(s_{h},a_{h})\\right].\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Putting these together we can bound ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathrm{Reg}_{T}\\leq\\mathrm{Reg}_{T}^{\\mathcal{M}_{0}}+\\sum_{t=1}^{T}\\mathbb{E}^{\\mathcal{M}^{\\star},\\pi_{t}}\\left[\\sum_{h=1}^{H}\\epsilon_{h}^{\\mathrm{mis}}(s_{h},a_{h})\\right]\\leq\\mathrm{Reg}_{T}^{\\mathcal{M}_{0}}+\\rho\\cdot\\mathrm{Reg}_{T},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where the last inequality holds by Assumption 4. Rearranging this gives ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\mathsf{R e g}_{T}\\leq\\frac{1}{1-\\rho}\\cdot\\mathsf{R e g}_{T}^{\\mathcal{M}_{0}}.\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "Bounding the Regret. Consider running the algorithm of Assumption 5 on ${\\mathcal{M}}^{\\star}$ and assume we run with parameter $C^{\\prime}\\gets\\beta$ which we will choose shortly. From the above observation, this is equivalent to running on $\\mathcal{M}_{\\mathrm{0}}$ with corruption level $\\epsilon_{h}^{\\mathrm{mis}}(s,a)$ at $(s,a,h)$ . Then by Assumption 5, with probability at least $1-\\delta$ we have regret on $\\mathcal{M}_{0}$ bounded as ", "page_idx": 39}, {"type": "equation", "text": "$$\n{\\mathrm{Reg}}_{T}^{\\mathcal{M}_{0}}\\le\\mathcal{C}_{1}(\\delta,T)\\sqrt{T}+\\mathcal{C}_{2}(\\delta,T)\\beta\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "\u00fcb $\\begin{array}{r}{\\beta\\geq\\sum_{t=1}^{T}\\sum_{h=1}^{H}\\epsilon_{h.}^{\\mathrm{mis}}(s_{h}^{t},a_{h}^{t})}\\end{array}$ $H T$ ${\\bf R e g}_{T}$ ", "page_idx": 39}, {"type": "text", "text": "Let $\\mathcal{E}_{1,t}$ denote the event that $\\begin{array}{r}{\\{\\beta\\geq\\sum_{t^{\\prime}=1}^{t}\\sum_{h=1}^{H}\\epsilon_{h}^{\\mathrm{mis}}(s_{h}^{t^{\\prime}},a_{h}^{t^{\\prime}})\\}}\\end{array}$ Let $\\mathcal{E}_{2}$ denote the event that for ll $t\\leq T$ , we have ", "page_idx": 39}, {"type": "equation", "text": "$$\n{\\sf R e g}_{t}\\leq\\frac{1}{1-\\rho}\\cdot\\left(\\mathcal{C}_{1}(\\frac{\\delta}{T},T)\\sqrt{t}+\\mathcal{C}_{2}(\\frac{\\delta}{T},T)\\beta\\right)+\\frac{T H}{1-\\rho}\\cdot\\mathbb{I}\\{\\mathcal{E}_{1,t}^{c}\\},\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "and note that by the above and under Assumption 5 we then have that $\\mathcal{E}_{2}$ occurs with probability at least $1-\\delta$ . For simplicity, for the remainder of the proof we abbreviate $\\begin{array}{r}{\\mathcal{C}_{1}:=\\mathcal{C}_{1}\\bar{(}\\frac{\\delta}{T},T)}\\end{array}$ and $\\begin{array}{r}{\\mathcal{C}_{2}:=\\mathcal{C}_{2}\\big(\\frac{\\delta}{T},T\\big)}\\end{array}$ ", "page_idx": 39}, {"type": "text", "text": "Note that $\\epsilon_{h}(s_{h}^{t},a_{h}^{t})\\in[0,H]$ by construction. It follows that, with probability at least $1-\\delta$ via Azuma-Hoeffding, ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\sum_{i=1}^{T}\\sum_{h=1}^{H}\\epsilon_{h}^{\\mathrm{mis}}(s_{h}^{t},a_{h}^{t})\\leq\\sum_{t=1}^{T}\\mathbb{E}^{\\pi_{t}}\\left[\\sum_{h=1}^{H}\\epsilon_{h}^{\\mathrm{mis}}(s_{h}^{t},a_{h}^{t})\\right]+H\\sqrt{2T\\log1/\\delta}\\leq\\rho\\cdot\\mathbf{Reg}_{T}+H\\sqrt{2T\\log1/\\delta}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Denote this event as ${\\mathcal{E}}_{3}$ ", "page_idx": 40}, {"type": "text", "text": "Now consider choosing ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\beta=\\left(1-{\\frac{\\rho{\\mathcal{C}}_{2}}{1-\\rho}}\\right)^{-1}\\cdot\\left({\\frac{\\rho}{1-\\rho}}\\cdot{\\mathcal{C}}_{1}{\\sqrt{T}}+H{\\sqrt{2T\\log1/\\delta}}+H\\right)\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "so that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\beta=\\frac{\\rho}{1-\\rho}\\cdot\\left(\\mathcal{C}_{1}\\sqrt{T}+\\mathcal{C}_{2}\\beta\\right)+H\\sqrt{2T\\log1/\\delta}+H.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "On $\\mathcal{E}_{2}\\cap\\mathcal{E}_{3}$ , assume that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\beta<\\rho\\cdot{\\mathrm{Reg}_{T}}+H\\sqrt{2T\\log{1/\\delta}}.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Let $t^{\\star}$ denote the minimum time such that ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{t^{\\star}}\\rho\\Delta(\\pi_{t})+H\\sqrt{2T\\log{1/\\delta}}>\\beta\\quad\\mathrm{and}\\quad\\sum_{t=1}^{t^{\\star}-1}\\rho\\Delta(\\pi_{t})+H\\sqrt{2T\\log{1/\\delta}}\\le\\beta,\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "and note that such a time is guaranteed to exist under (29) and since $\\beta\\,\\geq\\,H\\sqrt{2T\\log{1/\\delta}}+H$ by construction so $\\rho\\Delta(\\pi_{1})+H\\sqrt{2T\\log{1/\\delta}}\\,\\le\\,H+H\\sqrt{2T\\log{1/\\delta}}\\,\\le\\,\\beta$ . Furthermore, since $\\Delta(\\pi)\\leq H$ we have here that $\\begin{array}{r}{\\sum_{t=1}^{t^{\\star}-1}\\rho\\Delta(\\pi_{t})>\\beta-\\rho H-H\\sqrt{2T\\log{1/\\delta}}}\\end{array}$ .We then have ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Reg}_{t^{\\star}-1}=\\displaystyle\\sum_{t=1}^{t^{\\star}-1}\\Delta(\\pi_{t})}\\\\ &{\\phantom{a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a}>\\displaystyle\\frac{\\beta}{\\rho}-H-\\frac{H}{\\rho}\\sqrt{2T\\log{1/\\delta}}}\\\\ &{\\phantom{a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a}=\\displaystyle\\frac{1}{1-\\rho}\\cdot\\left(C_{1}\\sqrt{T}+C_{2}\\beta\\right)}\\\\ &{\\phantom{a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a}\\geq\\displaystyle\\frac{1}{1-\\rho}\\cdot\\left(C_{1}\\sqrt{t^{\\star}-1}+C_{2}\\beta\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "However, since by assumtion $\\begin{array}{r}{\\sum_{t=1}^{t^{\\star}-1}\\rho\\Delta(\\pi_{t})+H\\sqrt{2T\\log{1/\\delta}}\\,\\le\\,\\beta}\\end{array}$ on $\\mathcal{E}_{3}\\;\\mathcal{E}_{1,t^{\\star}-1}$ holds so on $\\mathcal{E}_{2}\\cap\\mathcal{E}_{3}$ we have that ", "page_idx": 40}, {"type": "equation", "text": "$$\n{\\mathrm{Reg}}_{t^{\\star}-1}\\leq{\\frac{1}{1-\\rho}}\\cdot\\left({\\mathcal{C}}_{1}{\\sqrt{t^{\\star}-1}}+{\\mathcal{C}}_{2}\\beta\\right).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "This contradicts the above. Therefore, on $\\mathcal{E}_{2}\\cap\\mathcal{E}_{3}$ we must have that $\\beta\\geq\\rho\\cdot{\\mathrm{Reg}}_{T}+H{\\sqrt{2T\\log1/\\delta}}$ $\\mathcal{E}_{1,T}$ holds on $\\mathcal{E}_{3}$ , and so on $\\mathcal{E}_{2}\\cap\\mathcal{E}_{3}$ \uff0c ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\mathrm{Reg}_{T}\\leq\\frac{1}{1-\\rho}\\cdot\\left(\\mathcal{C}_{1}\\sqrt{T}+\\mathcal{C}_{2}\\beta\\right).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "From our setting of $\\beta$ we can bound this as ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\leq\\frac{1}{1-\\rho}\\cdot\\mathcal{C}_{1}\\sqrt{T}+\\frac{1}{1-\\rho}\\cdot\\mathcal{C}_{2}\\cdot\\left(1-\\frac{\\rho\\mathcal{C}_{2}}{1-\\rho}\\right)^{-1}\\cdot\\left(\\frac{\\rho}{1-\\rho}\\cdot\\mathcal{C}_{1}\\sqrt{T}+H\\sqrt{2T\\log{1/\\delta}}+H\\right).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "The result follows from some simplification. ", "page_idx": 40}, {"type": "text", "text": "Lemma L.2. For MDPs ${\\mathcal{M}}^{\\star}$ $\\mathcal{M}_{0}$ satisfying Definition 4, under Assumption $^{6}$ wehave ", "page_idx": 40}, {"type": "equation", "text": "$$\nV_{0}^{\\mathcal{M}_{0},\\pi}-V_{0}^{\\mathcal{M}^{\\star},\\pi}\\leq\\mathbb{E}^{\\mathcal{M}^{\\star},\\pi}\\left[\\sum_{h=1}^{H}\\epsilon_{h}^{\\mathrm{mis}}(s_{h},a_{h})\\right].\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Proof. Let $r^{\\prime}$ denote the reward function on $\\mathcal{M}_{\\mathrm{0}}$ , and note that under Assumption 6 we have that there exs $f\\in\\mathcal F$ such that $V_{h}^{\\mathcal{M}_{0},\\pi}(s)=f_{h}(s,\\pi_{h}(s))$ for all $\\pi,s,h$ Then Lemma E.15 of Dann et al. (2017) gives that ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{V_{0}^{M_{0},\\pi}-V_{0}^{M^{\\star},\\pi}=\\mathbb{E}^{M^{\\star},\\pi}\\bigg[\\sum_{h=1}^{H}(r_{h}^{\\prime}-r_{h})}}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\displaystyle\\sum_{h=1}^{H}\\mathbb{E}^{M_{0},\\pi}[V_{h}^{M_{0},\\pi}(s_{h+1})\\mid s_{h}]-\\mathbb{E}^{M^{\\star},\\pi}[V_{h}^{M_{0},\\pi}(s_{h+1})\\mid s_{h}]\\bigg].}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "By Definition 4, we can bound this as ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\leq\\mathbb{E}^{\\boldsymbol{\\mathcal{M}}^{\\star},\\pi}\\left[\\sum_{h=1}^{H}\\epsilon_{h}^{\\mathrm{mis}}(s_{h},a_{h})\\right].\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Proof of Corollary 6.2.1. First, note that under Assumption 3, we have that Assumption 4 holds for $\\mathcal{F}$ the set of functions linear in $\\phi$ \uff0c $\\mathcal{F}=\\{\\phi(s,a)^{\\top}w^{^{\\mathrm{~\\*~}}}\\colon\\ w\\in\\mathbb{R}^{d}$ s.t. $\\phi(s,a)^{\\top}w\\,\\,\\bar{\\in}\\,\\,[0,H],\\forall s,a\\}$ and $\\epsilon_{h}^{\\mathrm{mis}}(s,a)$ of Assumption 4 set to $H\\epsilon_{h}^{\\mathrm{mis}}(s,a)$ for $\\epsilon_{h}^{\\mathrm{mis}}(s,a)$ of Assumption 3. To see this, let $\\mathcal{M}_{0}$ be the MDP with transitions $\\langle\\phi(s,a),\\mu_{h}(\\cdot)\\rangle$ , and note that this the immediately implies linear realizability on $\\mathcal{M}_{0}$ (and furthermore that Assumption 6 holds). Furthermore, since the total reward is at most $H$ i is asyto seethat under Assumption 3, we can take $\\epsilon_{h}^{\\mathrm{mis}}(s,a)\\gets H\\epsilon_{h}^{\\mathrm{mis}}(s,a)$ ", "page_idx": 41}, {"type": "text", "text": "Next, note that Theorem 4.2 of Ye et al. (2023) gives an algorithm on $\\mathcal{M}_{\\mathrm{0}}$ satisfying Assumption 5 with $\\mathcal{C}_{1}=\\tilde{\\mathcal{O}}(\\sqrt{H^{2}d^{3}})$ and $\\mathcal{C}_{2}=\\widetilde{\\mathcal{O}}(H d)$ (assuming that $\\textstyle\\sum_{h=1}^{H}r_{h}\\in[0,1]$ almost surely). We can then apply Theorem L.1 to obtain the result. ", "page_idx": 41}, {"type": "text", "text": "M Auxiliary Lemmas ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Lemma M1 Lemma 16 ofZimmertad Latimore (2022). Let x =X+aT and $Y=$ $\\left[\\!\\!\\begin{array}{c c}{{Y+y y^{\\top}}}&{{y}}\\\\ {{y^{\\top}}}&{{1}}\\end{array}\\!\\!\\right]$ Then ", "page_idx": 41}, {"type": "equation", "text": "$$\nD_{G}(\\pmb{X},\\pmb{Y})=D_{G}(\\pmb{X},\\pmb{Y})+\\|\\pmb{x}-\\pmb{y}\\|_{Y^{-1}}^{2}\\geq\\|\\pmb{x}-\\pmb{y}\\|_{Y^{-1}}^{2}.\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Lemma M.2 (Lemma 34 of Liu et al. (2023b). Let $G$ be the log-determinant barrier. For any matrix $\\begin{array}{r}{D,\\,i f\\,\\sqrt{\\mathrm{Tr}(H_{t}D H_{t}D)}\\leq\\frac{1}{16\\eta}.}\\end{array}$ then ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{H\\in\\mathcal{H}}\\left\\langle H-H_{t},D\\right\\rangle-\\frac{D_{G}(H,H_{t})}{\\eta}\\leq8\\eta\\mathrm{Tr}\\left(H_{t}D H_{t}D\\right).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Lemma M.3 (Strengthened Freedman's inequality (Theorem 9 of Zimmert and Lattimore (2022)). Let $X_{1},X_{2},\\ldots,X_{T}$ be a martingale difference sequence with a filtration ${\\mathcal{F}}_{1}\\subseteq{\\mathcal{F}}_{2}\\subseteq\\cdot\\cdot\\cdot$ suchthat $\\mathbb{E}[X_{t}|\\mathcal{F}_{t}]=0$ and $\\mathbb{E}[|X_{t}|\\mid\\mathcal{F}_{t}]<\\infty$ almost surely. Then with probability at least $1-\\delta$ ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}X_{t}\\leq3\\sqrt{V_{T}\\log\\left(\\frac{2\\operatorname*{max}\\{U_{T},\\sqrt{V_{T}}\\}}{\\delta}\\right)}+2U_{T}\\log\\left(\\frac{2\\operatorname*{max}\\{U_{T},\\sqrt{V_{T}}\\}}{\\delta}\\right),\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "where $\\begin{array}{r}{V_{T}=\\sum_{t=1}^{T}\\mathbb{E}[X_{t}^{2}\\mid\\mathcal{F}_{t}]}\\end{array}$ and $U_{T}=\\operatorname*{max}\\{1,\\operatorname*{max}_{t\\in[T]}|X_{t}|\\}$ ", "page_idx": 41}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. ", "page_idx": 42}, {"type": "text", "text": "Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 You should answer [Yes] , [No] , or [NA] .   \n\u00b7 [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.   \n\u00b7 Please provide a short (1-2 sentence) justification right after your answer (even for NA). ", "page_idx": 42}, {"type": "text", "text": "The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. ", "page_idx": 42}, {"type": "text", "text": "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found. ", "page_idx": 42}, {"type": "text", "text": "IMPORTANT, please: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 Delete this instruction block, but keep the section heading \u201cNeurIPS paper checklist\", \u00b7 Keep the checklist subsection headings, questions/answers and guidelines below. \u00b7 Do not modify the questions and only use the provided macros for your answers. ", "page_idx": 42}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper's contributions and scope. The claims are validated by detailed proofs. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u00b7 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u00b7 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u00b7 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u00b7 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 42}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] . ", "page_idx": 42}, {"type": "text", "text": "Justification: The paper discuss the limitations of the work when introducing the algorithms. Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u00b7 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u00b7 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should refect on how these assumptions might be violated in practice and what the implications would be.   \n\u00b7 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u00b7 The authors should refect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u00b7 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u00b7 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u00b7 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 43}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 43}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Justification: The paper provides detailed assumptions and proofs. Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include theoretical results.   \n\u00b7 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u00b7 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u00b7 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u00b7 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u00b7 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 43}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] .   \nJustification: This is a theoretical paper.   \nGuidelines: \u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u00b7 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u00b7 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u00b7 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 43}, {"type": "text", "text": "", "page_idx": 44}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 44}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 44}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "\u00b7 The answer NA means that paper does not include experiments requiring code.   \n\u00b7 Please see the NeurIPS code and data submission guidelines (https: //nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u00b7 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https : / /nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u00b7 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u00b7 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u00b7 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ", "page_idx": 44}, {"type": "text", "text": "\u00b7 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 45}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 45}, {"type": "text", "text": "Justification: This paper does not include experiments. Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u00b7 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 45}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 45}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments.   \n\u00b7 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u00b7 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u00b7 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u00b7 The assumptions made should be given (e.g., Normally distributed errors).   \n\u00b7 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u00b7 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u00b7 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u00b7 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 45}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 45}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 45}, {"type": "text", "text": "Justification: This paper does not include experiments. ", "page_idx": 45}, {"type": "text", "text": "Guidelines: ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not include experiments. \u00b7 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ", "page_idx": 45}, {"type": "text", "text": "\u00b7 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u00b7 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper). ", "page_idx": 46}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 46}, {"type": "text", "text": "Answer: [Yes] . ", "page_idx": 46}, {"type": "text", "text": "Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 46}, {"type": "text", "text": "Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u00b7 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u00b7 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 46}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 46}, {"type": "text", "text": "Justification: This is a theoretical work. There is no societal impact of the work performed. Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that there is no societal impact of the work performed.   \n\u00b7 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u00b7 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u00b7 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u00b7 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u00b7 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 46}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 46}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 46}, {"type": "text", "text": "Justification: The paper poses no such risks. Guidelines: ", "page_idx": 46}, {"type": "text", "text": "\u00b7 The answer NA means that the paper poses no such risks.   \n\u00b7 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safetyfilters.   \n\u00b7 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u00b7 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 47}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 47}, {"type": "text", "text": "Justification: This paper does not use existing assets. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not use existing assets.   \n\u00b7 The authors should cite the original paper that produced the code package or dataset.   \n\u00b7 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u00b7 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u00b7 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u00b7 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode . com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u00b7 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u00b7 If this information is not available online, the authors are encouraged to reach out to the asset's creators. ", "page_idx": 47}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 47}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 47}, {"type": "text", "text": "Guidelines: ", "page_idx": 47}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not release new assets.   \n\u00b7 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u00b7 The paper should discuss whether and how consent was obtained from people whose assetisused.   \n\u00b7 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 47}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 47}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 47}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u00b7 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 48}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)were obtained? ", "page_idx": 48}, {"type": "text", "text": "Answer: [NA] . ", "page_idx": 48}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 48}, {"type": "text", "text": "\u00b7 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u00b7 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u00b7 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPs Code of Ethics and the guidelines for their institution.   \n\u00b7 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 48}]