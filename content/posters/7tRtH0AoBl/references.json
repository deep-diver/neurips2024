{"references": [{"fullname_first_author": "Yasin Abbasi-Yadkori", "paper_title": "Improved algorithms for linear stochastic bandits", "publication_date": "2011-01-01", "reason": "This paper is foundational for understanding linear stochastic bandits, which are closely related to the linear MDPs discussed in this paper."}, {"fullname_first_author": "Marc Abeille", "paper_title": "Linear Thompson Sampling Revisited", "publication_date": "2017-04-20", "reason": "This paper provides a refined analysis of Thompson Sampling for linear bandits, an important technique used in many randomized exploration algorithms."}, {"fullname_first_author": "Shipra Agrawal", "paper_title": "Posterior sampling for reinforcement learning: worst-case regret bounds", "publication_date": "2017-01-01", "reason": "This is a seminal work that establishes the theoretical foundation of posterior sampling for reinforcement learning, a core exploration strategy related to this paper's methods."}, {"fullname_first_author": "Mohammad Gheshlaghi Azar", "paper_title": "Minimax regret bounds for reinforcement learning", "publication_date": "2017-01-01", "reason": "This paper provides fundamental minimax regret bounds for reinforcement learning, which are essential for evaluating the efficiency of reinforcement learning algorithms."}, {"fullname_first_author": "Alekh Agarwal", "paper_title": "Model-based RL with optimistic posterior sampling: Structural conditions and sample complexity", "publication_date": "2022-01-01", "reason": "This paper offers a state-of-the-art theoretical analysis for model-based reinforcement learning, which directly addresses function approximation challenges similar to those tackled in this paper."}]}