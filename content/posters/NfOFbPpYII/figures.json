[{"figure_path": "NfOFbPpYII/figures/figures_4_1.jpg", "caption": "Figure 1: The left plot shows the mapping from sentence to the next token. The red rectangle indicates the optimal token in the corresponding sentence. The right plot shows the collocation relationship.", "description": "The figure is composed of two plots. The left plot illustrates the mapping from a sentence to its subsequent token.  The optimal token in each sentence is highlighted by a red rectangle. This visualizes the concept of query-dependent partial orders, where the prediction of the next token depends on the tokens already present in the sentence. The right plot displays the concept of collocation which consists of token pairs where each token is directly paired with its subsequent token. It is a crucial component for training the feed-forward layer in the proposed two-stage training algorithm.", "section": "Realizable Training Dataset and Two-Stage Algorithm"}, {"figure_path": "NfOFbPpYII/figures/figures_8_1.jpg", "caption": "Figure 2: Training dynamics of single-layer transformer for NTP.", "description": "This figure shows the training dynamics of a single-layer transformer for next-token prediction (NTP). It contains six subplots, each illustrating a different aspect of the training process.  The first three plots display the dynamics of the feed-forward layer's training (stage 1), showing the convergence of the loss function to zero, the convergence of the feed-forward layer parameters toward their max-margin solutions, and the linear increase in the norm of the parameters. The last three plots demonstrate the dynamics of the self-attention layer's training (stage 2), showing the convergence of the loss function, the convergence of the self-attention layer parameters toward their max-margin solutions, and the linear increase in the norm of the parameters. Overall, these plots visually confirm the theoretical findings of the paper on the convergence behavior of the two-stage training algorithm.", "section": "Experiment"}]