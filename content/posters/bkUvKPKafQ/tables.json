[{"figure_path": "bkUvKPKafQ/tables/tables_4_1.jpg", "caption": "Table 1: Retrieval results across five multi-turn QA datasets with the average top-1 and top-5 recall scores. Compared to rewriting, fine-tuning performs much better on E5-unsupervised and is comparable on Dragon. *Since the average context length in TopiOCQA and INSCIT is smaller than in other datasets, we report top-5 and top-20 to roughly match the context lengths of top-1 and top-5, respectively, in those datasets. The inputs for these two models are a concatenation of the dialogue history and the current query. The input for this model is the rewritten query.  denotes that the HumanAnnotatedConvQA dataset is replaced with the SyntheticConvQA for fine-tuning. The numbers are not apple-to-apple comparison (e.g., they use the training set for fine-tuning).", "description": "This table presents the results of a comparative study on five different multi-turn Question Answering (QA) datasets.  The study compares two approaches for retrieval: query rewriting and fine-tuning a single-turn retriever on multi-turn data. The table shows the top-1 and top-5 recall scores for each method, across several models. It highlights that fine-tuning generally outperforms rewriting, especially for the E5-unsupervised model, with results that are comparable to state-of-the-art query rewriting on the Dragon model. It also accounts for differences in context lengths between different datasets by using top-20 instead of top-5 recall scores where appropriate.", "section": "4 Retrieval for Multi-Turn QA"}, {"figure_path": "bkUvKPKafQ/tables/tables_6_1.jpg", "caption": "Table 1: Retrieval results across five multi-turn QA datasets with the average top-1 and top-5 recall scores. Compared to rewriting, fine-tuning performs much better on E5-unsupervised and is comparable on Dragon. *Since the average context length in TopiOCQA and INSCIT is smaller than in other datasets, we report top-5 and top-20 to roughly match the context lengths of top-1 and top-5, respectively, in those datasets. The inputs for these two models are a concatenation of the dialogue history and the current query. The input for this model is the rewritten query. denotes that the HumanAnnotatedConvQA dataset is replaced with the SyntheticConvQA for fine-tuning. The numbers are not apple-to-apple comparison (e.g., they use the training set for fine-tuning).", "description": "This table presents the results of retrieval experiments across five multi-turn Question Answering (QA) datasets.  The metrics used are top-1 and top-5 recall scores.  Two different retrieval methods are compared: query rewriting and fine-tuning a single-turn retriever on a conversational QA dataset. The table shows that fine-tuning generally outperforms query rewriting, especially on the E5-unsupervised retriever.  The impact of using synthetic data versus human-annotated data for fine-tuning is also investigated.", "section": "4 Retrieval for Multi-Turn QA"}, {"figure_path": "bkUvKPKafQ/tables/tables_6_2.jpg", "caption": "Table 1: Retrieval results across five multi-turn QA datasets with the average top-1 and top-5 recall scores. Compared to rewriting, fine-tuning performs much better on E5-unsupervised and is comparable on Dragon. *Since the average context length in TopiOCQA and INSCIT is smaller than in other datasets, we report top-5 and top-20 to roughly match the context lengths of top-1 and top-5, respectively, in those datasets. The inputs for these two models are a concatenation of the dialogue history and the current query. The input for this model is the rewritten query. denotes that the HumanAnnotatedConvQA dataset is replaced with the SyntheticConvQA for fine-tuning. The numbers are not apple-to-apple comparison (e.g., they use the training set for fine-tuning).", "description": "This table presents the performance of different retrieval methods (fine-tuning vs. query rewriting) across five multi-turn Question Answering datasets.  It compares top-1 and top-5 recall scores, showing the effectiveness of fine-tuning, especially when compared to the E5-unsupervised baseline.  The table notes differences in average context length across datasets, explaining the use of top-20 scores for TopiOCQA and INSCIT to maintain comparability. It also highlights that the results are not perfectly comparable due to differences in training data used across different methods.", "section": "4 Retrieval for Multi-Turn QA"}, {"figure_path": "bkUvKPKafQ/tables/tables_8_1.jpg", "caption": "Table 4: Accuracies on answerable and unanswerable samples across QuAC and DoQA datasets. Avg-Both is the averaged score between QuAC and DoQA. ChatRAG is the average score on the CHATRAG BENCH. * \u201cno\u201d and \"yes\" denote unanswerable and answerable samples, respectively.", "description": "This table presents the accuracy of various models in identifying and correctly answering both answerable and unanswerable questions within the QuAC and DoQA datasets.  The \"Avg-Both\" column shows the average accuracy across both datasets.  The \"ChatRAG\" column displays the average score achieved by each model across all datasets within the CHATRAG benchmark.  The asterisks (*) indicate that \"no\" represents unanswerable questions and \"yes\" represents answerable questions.", "section": "6.2 Unanswerable Case Evaluation"}, {"figure_path": "bkUvKPKafQ/tables/tables_8_2.jpg", "caption": "Table 5: Zero-shot exact match scores on Natural Questions (NQ), TriviaQA, and HotpotQA, which were evaluated using the data split from the KILT Benchmark (Petroni et al., 2021).", "description": "This table presents the zero-shot exact match scores achieved by various models on three different question answering benchmarks: Natural Questions (NQ), TriviaQA, and HotpotQA.  The scores reflect the models' ability to answer questions without any specific fine-tuning on these datasets.  The data split used for evaluation is from the KILT Benchmark.", "section": "6.3 Evaluation on Single-Turn QA and RAG Benchmark"}, {"figure_path": "bkUvKPKafQ/tables/tables_15_1.jpg", "caption": "Table 1: Retrieval results across five multi-turn QA datasets with the average top-1 and top-5 recall scores. Compared to rewriting, fine-tuning performs much better on E5-unsupervised and is comparable on Dragon. *Since the average context length in TopiOCQA and INSCIT is smaller than in other datasets, we report top-5 and top-20 to roughly match the context lengths of top-1 and top-5, respectively, in those datasets. \u2020The inputs for these two models are a concatenation of the dialogue history and the current query. \u2021The input for this model is the rewritten query. \u00a7 denotes that the HumanAnnotatedConvQA dataset is replaced with the SyntheticConvQA for fine-tuning. The numbers are not apple-to-apple comparison (e.g., they use the training set for fine-tuning).", "description": "This table presents the results of retrieval experiments across five multi-turn Question Answering datasets.  It compares two methods: fine-tuning a single-turn retriever on multi-turn data and using a query rewriting approach. The metrics used are top-1 and top-5 recall scores. The table also notes some nuances in comparing the results, especially concerning the length of contexts used for different models and datasets, as well as differences in the training data used for some methods.  The asterisks indicate the use of top-20 recall to compensate for shorter context lengths in two specific datasets.", "section": "4 Retrieval for Multi-Turn QA"}, {"figure_path": "bkUvKPKafQ/tables/tables_15_2.jpg", "caption": "Table 7: Fine-grained studies on average scores of different dataset types. Avg-text covers datasets where the documents only have text, including Doc2Dial, QuAC, QReCC, COQA, DoQA, TopiOCQA, and INSCIT. Avg-table covers datasets with table in the documents, including ConvFinQA, SQA, and HybriDial. Avg-ret covers datasets with long documents requiring retrieval, including Doc2Dial, QuAC, QReCC, TopiOCQA, and INSCIT. Avg-nonret covers datasets with short documents which do not require retrieval, including COQA, DoQA, ConvFinQA, SQA, and HybriDial.", "description": "This table presents a fine-grained analysis of the model's performance across different categories of datasets within the CHATRAG BENCH.  It breaks down the average scores based on whether the documents are primarily text-based or contain tables, and whether or not retrieval is required for answering the questions.  This allows for a more nuanced understanding of the model's strengths and weaknesses in various scenarios.", "section": "A.2 Fine-grained Analyses"}, {"figure_path": "bkUvKPKafQ/tables/tables_16_1.jpg", "caption": "Table 8: Ablation studies on input context across datasets that require retrieval. All models use SyntheticConvQA. We study the number of contexts used in inputs (# of ctx), context ordering (reverse, swing, random), and the use of retrieved context from the original Dragon. In comparison, ChatQA-1.0-70B (default setting) uses \u201cDragon + Fine-tune\u201d to retrieve the top-5 contexts, and arranges them sequentially from the first to the fifth context in top-5.", "description": "This ablation study investigates the impact of different numbers of retrieved contexts and their ordering on the performance of retrieval-augmented models for conversational QA.  The experiment focuses on datasets where retrieval is necessary, and all models utilize the SyntheticConvQA dataset for training.  The table compares the performance of ChatQA-1.0-70B using different numbers of top-k contexts (top-3, top-5, top-10) and different context orderings (sequential, reverse, swing, random) against a baseline model using the original Dragon retriever.", "section": "A.3 Ablation Studies on Inference Stage"}, {"figure_path": "bkUvKPKafQ/tables/tables_24_1.jpg", "caption": "Table 1: Retrieval results across five multi-turn QA datasets with the average top-1 and top-5 recall scores. Compared to rewriting, fine-tuning performs much better on E5-unsupervised and is comparable on Dragon. *Since the average context length in TopiOCQA and INSCIT is smaller than in other datasets, we report top-5 and top-20 to roughly match the context lengths of top-1 and top-5, respectively, in those datasets. The inputs for these two models are a concatenation of the dialogue history and the current query. The input for this model is the rewritten query. denotes that the HumanAnnotatedConvQA dataset is replaced with the SyntheticConvQA for fine-tuning. The numbers are not apple-to-apple comparison (e.g., they use the training set for fine-tuning).", "description": "This table presents the results of different retrieval methods (fine-tuning and query rewriting) across five multi-turn QA datasets.  It compares the top-1 and top-5 recall scores for each method, highlighting the superior performance of fine-tuning, especially for the E5-unsupervised model.  The table also notes the differences in average context length between datasets and adjusts the top-k metrics accordingly (TopiOCQA and INSCIT).  Finally, it acknowledges that some comparisons aren't perfectly equivalent due to differences in training data.", "section": "4 Retrieval for Multi-Turn QA"}, {"figure_path": "bkUvKPKafQ/tables/tables_28_1.jpg", "caption": "Table 1: Retrieval results across five multi-turn QA datasets with the average top-1 and top-5 recall scores. Compared to rewriting, fine-tuning performs much better on E5-unsupervised and is comparable on Dragon. *Since the average context length in TopiOCQA and INSCIT is smaller than in other datasets, we report top-5 and top-20 to roughly match the context lengths of top-1 and top-5, respectively, in those datasets. \u2020The inputs for these two models are a concatenation of the dialogue history and the current query. \u2021The input for this model is the rewritten query. \u00a7denotes that the HumanAnnotatedConvQA dataset is replaced with the SyntheticConvQA for fine-tuning. The numbers are not apple-to-apple comparison (e.g., they use the training set for fine-tuning).", "description": "This table presents the results of retrieval experiments across five multi-turn Question Answering (QA) datasets.  It compares two approaches: fine-tuning a retriever on conversational queries and using a query rewriting method followed by single-turn retrieval.  The table shows top-1 and top-5 recall scores, indicating the effectiveness of each method in retrieving relevant information.  The asterisk (*) indicates adjustments made for datasets with shorter average context lengths.  The daggers (\u2020, \u2021) denote different input methods, and the section symbol (\u00a7) shows an experimental condition. It highlights the performance difference between query rewriting and fine-tuning, particularly for the E5-unsupervised and Dragon retrievers.", "section": "4 Retrieval for Multi-Turn QA"}, {"figure_path": "bkUvKPKafQ/tables/tables_29_1.jpg", "caption": "Table 1: Retrieval results across five multi-turn QA datasets with the average top-1 and top-5 recall scores. Compared to rewriting, fine-tuning performs much better on E5-unsupervised and is comparable on Dragon. *Since the average context length in TopiOCQA and INSCIT is smaller than in other datasets, we report top-5 and top-20 to roughly match the context lengths of top-1 and top-5, respectively, in those datasets. \u2020The inputs for these two models are a concatenation of the dialogue history and the current query. \u2021The input for this model is the rewritten query. \u00a7denotes that the HumanAnnotatedConvQA dataset is replaced with the SyntheticConvQA for fine-tuning. The numbers are not apple-to-apple comparison (e.g., they use the training set for fine-tuning).", "description": "This table presents the results of retrieval experiments across five multi-turn question answering datasets.  It compares two approaches: fine-tuning a single-turn retriever on conversational query-context pairs and using a query rewriting method.  The table shows top-1 and top-5 recall scores, highlighting that fine-tuning generally outperforms query rewriting, especially for the E5-unsupervised retriever.  It also notes that the context length varies across datasets and that different input methods (concatenated dialogue history and query vs. rewritten query) were used, impacting comparability of results.", "section": "4 Retrieval for Multi-Turn QA"}, {"figure_path": "bkUvKPKafQ/tables/tables_30_1.jpg", "caption": "Table 1: Retrieval results across five multi-turn QA datasets with the average top-1 and top-5 recall scores. Compared to rewriting, fine-tuning performs much better on E5-unsupervised and is comparable on Dragon. *Since the average context length in TopiOCQA and INSCIT is smaller than in other datasets, we report top-5 and top-20 to roughly match the context lengths of top-1 and top-5, respectively, in those datasets. The inputs for these two models are a concatenation of the dialogue history and the current query. The input for this model is the rewritten query. denotes that the HumanAnnotatedConvQA dataset is replaced with the SyntheticConvQA for fine-tuning. The numbers are not apple-to-apple comparison (e.g., they use the training set for fine-tuning).", "description": "This table presents the results of five different multi-turn question answering models across five different datasets.  The metrics used are top-1 and top-5 recall scores.  The models are compared based on two different approaches: query rewriting and fine-tuning a retriever. The table highlights the performance differences between these methods and considers the varying lengths of contexts in some datasets.  Note that some comparisons within the table are not directly comparable due to differences in training methodology.", "section": "4 Retrieval for Multi-Turn QA"}, {"figure_path": "bkUvKPKafQ/tables/tables_31_1.jpg", "caption": "Table 1: Retrieval results across five multi-turn QA datasets with the average top-1 and top-5 recall scores. Compared to rewriting, fine-tuning performs much better on E5-unsupervised and is comparable on Dragon. *Since the average context length in TopiOCQA and INSCIT is smaller than in other datasets, we report top-5 and top-20 to roughly match the context lengths of top-1 and top-5, respectively, in those datasets. The inputs for these two models are a concatenation of the dialogue history and the current query. The input for this model is the rewritten query. denotes that the HumanAnnotatedConvQA dataset is replaced with the SyntheticConvQA for fine-tuning. The numbers are not apple-to-apple comparison (e.g., they use the training set for fine-tuning).", "description": "This table presents the results of retrieval experiments across five multi-turn question answering datasets.  It compares two approaches: fine-tuning a single-turn retriever and using a query rewriting method. The table shows top-1 and top-5 recall scores, highlighting the superior performance of fine-tuning, especially for the E5-unsupervised model.  Note that for datasets with shorter average context lengths, top-5 and top-20 scores are provided for better comparison.  Differences in experimental setups are noted in the caption.", "section": "4 Retrieval for Multi-Turn QA"}, {"figure_path": "bkUvKPKafQ/tables/tables_32_1.jpg", "caption": "Table 1: Retrieval results across five multi-turn QA datasets with the average top-1 and top-5 recall scores. Compared to rewriting, fine-tuning performs much better on E5-unsupervised and is comparable on Dragon. *Since the average context length in TopiOCQA and INSCIT is smaller than in other datasets, we report top-5 and top-20 to roughly match the context lengths of top-1 and top-5, respectively, in those datasets. The inputs for these two models are a concatenation of the dialogue history and the current query. The input for this model is the rewritten query. denotes that the HumanAnnotatedConvQA dataset is replaced with the SyntheticConvQA for fine-tuning. The numbers are not apple-to-apple comparison (e.g., they use the training set for fine-tuning).", "description": "This table shows the performance of different retrieval methods (query rewriting vs. fine-tuning) on five multi-turn question answering datasets.  The metrics used are top-1 and top-5 recall scores. The table highlights that fine-tuning generally outperforms rewriting, especially for the E5-unsupervised retriever.  The asterisk notes that top-5 and top-20 results are reported for TopiOCQA and INSCIT due to their shorter contexts compared to the other datasets.  The table also notes that the results are not directly comparable because different training data was used for certain comparisons.", "section": "4 Retrieval for Multi-Turn QA"}, {"figure_path": "bkUvKPKafQ/tables/tables_33_1.jpg", "caption": "Table 1: Retrieval results across five multi-turn QA datasets with the average top-1 and top-5 recall scores. Compared to rewriting, fine-tuning performs much better on E5-unsupervised and is comparable on Dragon. *Since the average context length in TopiOCQA and INSCIT is smaller than in other datasets, we report top-5 and top-20 to roughly match the context lengths of top-1 and top-5, respectively, in those datasets. The inputs for these two models are a concatenation of the dialogue history and the current query. The input for this model is the rewritten query. denotes that the HumanAnnotatedConvQA dataset is replaced with the SyntheticConvQA for fine-tuning. The numbers are not apple-to-apple comparison (e.g., they use the training set for fine-tuning).", "description": "This table presents the results of retrieval experiments across five multi-turn Question Answering (QA) datasets.  The metrics used are top-1 and top-5 recall scores. Two different retrieval methods are compared: query rewriting and fine-tuning.  The table shows that fine-tuning generally outperforms rewriting, especially for the E5-unsupervised model.  The table also notes differences in context length across the datasets and that some comparisons are not directly comparable because of differences in training data used.", "section": "4 Retrieval for Multi-Turn QA"}, {"figure_path": "bkUvKPKafQ/tables/tables_36_1.jpg", "caption": "Table 1: Retrieval results across five multi-turn QA datasets with the average top-1 and top-5 recall scores. Compared to rewriting, fine-tuning performs much better on E5-unsupervised and is comparable on Dragon. *Since the average context length in TopiOCQA and INSCIT is smaller than in other datasets, we report top-5 and top-20 to roughly match the context lengths of top-1 and top-5, respectively, in those datasets. \u2020The inputs for these two models are a concatenation of the dialogue history and the current query. \u2021The input for this model is the rewritten query. \u00a7 denotes that the HumanAnnotatedConvQA dataset is replaced with the SyntheticConvQA for fine-tuning. The numbers are not apple-to-apple comparison (e.g., they use the training set for fine-tuning).", "description": "This table presents the results of retrieval experiments conducted on five multi-turn question answering datasets.  The experiments compare two approaches: query rewriting and fine-tuning a single-turn retriever.  The table shows top-1 and top-5 recall scores, indicating the effectiveness of each method.  It also notes differences in context lengths across datasets, the inputs used for different methods, and that a synthetic dataset was used in some cases for comparison purposes.  It is important to note that the results are not directly comparable due to differences in experimental setup.", "section": "Retrieval for Multi-Turn QA"}, {"figure_path": "bkUvKPKafQ/tables/tables_36_2.jpg", "caption": "Table 1: Retrieval results across five multi-turn QA datasets with the average top-1 and top-5 recall scores. Compared to rewriting, fine-tuning performs much better on E5-unsupervised and is comparable on Dragon. *Since the average context length in TopiOCQA and INSCIT is smaller than in other datasets, we report top-5 and top-20 to roughly match the context lengths of top-1 and top-5, respectively, in those datasets. The inputs for these two models are a concatenation of the dialogue history and the current query. The input for this model is the rewritten query. denotes that the HumanAnnotatedConvQA dataset is replaced with the SyntheticConvQA for fine-tuning. The numbers are not apple-to-apple comparison (e.g., they use the training set for fine-tuning).", "description": "This table presents the results of retrieval experiments on five multi-turn QA datasets.  Two main methods are compared: fine-tuning a single-turn retriever and using a query rewriting method. The table shows top-1 and top-5 recall scores for each method across the datasets. It also highlights that fine-tuning significantly outperforms query rewriting in one scenario, and performs comparably in another.  Differences in context length across datasets are also addressed.", "section": "4 Retrieval for Multi-Turn QA"}]