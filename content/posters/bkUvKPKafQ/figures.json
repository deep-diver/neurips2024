[{"figure_path": "bkUvKPKafQ/figures/figures_2_1.jpg", "caption": "Figure 1: Two-stage instruction tuning framework for ChatQA.", "description": "This figure illustrates the two-stage instruction tuning process used to develop the ChatQA model.  Stage 1 involves supervised fine-tuning (SFT) of a foundation large language model (LLM) on a dataset combining various instruction-following and dialogue datasets (Soda, ELI5, FLAN, Dolly, and OpenAssistant).  This stage aims to give the model a strong foundation in instruction-following.  Stage 2 applies context-enhanced instruction tuning to improve the model's ability to integrate user-provided or retrieved context for conversational question answering and retrieval-augmented generation (RAG). This stage uses a range of conversational QA datasets, including NarrativeQA, DROP, Quoref, ROPES, SQUAD, NewsQA, TAT-QA, and a dedicated conversational QA dataset, to fine-tune the model and enhance its contextual understanding.", "section": "3 ChatQA"}, {"figure_path": "bkUvKPKafQ/figures/figures_3_1.jpg", "caption": "Figure 2: Illustration of fine-tuning retriever for multi-turn QA.", "description": "This figure illustrates the fine-tuning process of a retriever for multi-turn question answering.  It shows how multi-turn queries (a sequence of questions from the user and their corresponding assistant responses) and corresponding contexts are processed. The query encoder processes the concatenated dialogue history and current query to generate a query embedding. Similarly, the context encoder processes individual context pieces to create context embeddings.  The contrastive fine-tuning is applied to learn effective embeddings by comparing positive (correct context) and negative (incorrect contexts) pairs. This method is used to improve the retriever's ability to handle multi-turn conversations.", "section": "3.1 Stage-1: Supervised Fine-tuning"}, {"figure_path": "bkUvKPKafQ/figures/figures_7_1.jpg", "caption": "Figure 1: Two-stage instruction tuning framework for ChatQA.", "description": "This figure illustrates the two-stage instruction tuning framework used in the ChatQA model.  Stage 1 involves supervised fine-tuning on a foundation LLM using a large dataset of instruction-following and dialogue data. This stage aims to equip the LLM with the ability to follow natural language instructions. Stage 2 involves context-enhanced instruction tuning which integrates contextualized QA datasets into the instruction tuning blend.  This stage aims to enhance the LLM's ability to integrate user-provided or retrieved context for conversational QA and RAG tasks. The framework uses a combination of different datasets (Soda, ELI5, FLAN, Dolly, OpenAssistant, etc.) and techniques (supervised fine-tuning, context-enhanced instruction tuning) to improve the model's performance on various conversational QA tasks.", "section": "3 ChatQA"}, {"figure_path": "bkUvKPKafQ/figures/figures_27_1.jpg", "caption": "Figure 1: Two-stage instruction tuning framework for ChatQA.", "description": "This figure illustrates the two-stage instruction tuning framework used to develop the ChatQA models. Stage 1 involves supervised fine-tuning of a foundation large language model (LLM) on a large dataset of instruction-following and dialogue data. This step equips the model with basic instruction-following capabilities. Stage 2, context-enhanced instruction tuning, further improves the model's performance on conversational question answering and retrieval-augmented generation (RAG) tasks. It integrates contextualized QA datasets and uses a special instruction tuning recipe to enhance the LLM's ability to leverage user-provided or retrieved context during generation.  The framework highlights the iterative process of improving the model's understanding of conversational context and integrating retrieved evidence for more accurate answers.", "section": "3 ChatQA"}, {"figure_path": "bkUvKPKafQ/figures/figures_34_1.jpg", "caption": "Figure 1: Two-stage instruction tuning framework for ChatQA.", "description": "This figure illustrates the two-stage instruction tuning framework used in the ChatQA model. Stage 1 involves supervised fine-tuning on a combined dataset of instruction-following and dialogue datasets.  This equips the LLM with instruction-following capabilities. Stage 2, context-enhanced instruction tuning, improves the LLM's ability to integrate user-provided or retrieved context for conversational QA and RAG tasks. This stage leverages contextualized QA datasets to enhance the model's performance. The framework starts with a foundation LLM, which is then iteratively refined through the two stages to create the final ChatQA model.", "section": "3 ChatQA"}]