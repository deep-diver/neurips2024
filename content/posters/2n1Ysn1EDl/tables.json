[{"figure_path": "2n1Ysn1EDl/tables/tables_7_1.jpg", "caption": "Table 1: Evaluating explanation methods. Higher scores \u2206AF indicate more faithful explanations.", "description": "This table compares the performance of MambaLRP against several baseline explanation methods across various datasets (SST-2, Med-BIOS, SNLI, Emotion, and ImageNet).  The metrics used (\u2206AF) measure the faithfulness of the explanations, with higher scores indicating more faithful explanations.  The table shows that MambaLRP consistently outperforms other methods, including gradient-based and attention-based approaches.", "section": "5 Experiments"}, {"figure_path": "2n1Ysn1EDl/tables/tables_7_2.jpg", "caption": "Table 2: Analyzing the impact of ablating the three proposed propagation rules on \u2206AF for the components in MambaLRP.", "description": "This table presents the results of an ablation study evaluating the impact of removing each of the three novel relevance propagation strategies introduced in MambaLRP. The strategies are for the SiLU activation function, the selective SSM, and the multiplicative gate.  The table shows the \u2206AF (a faithfulness metric, higher is better) achieved on the SST-2 and ImageNet datasets when each component is excluded, one at a time, demonstrating the importance of each component for achieving high explanation faithfulness.  The results clearly show that all three modifications are essential for achieving competitive explanation performance, with the SSM modification having the largest impact.", "section": "Ablation study"}, {"figure_path": "2n1Ysn1EDl/tables/tables_7_3.jpg", "caption": "Table 1: Evaluating explanation methods. Higher scores \u2206AF indicate more faithful explanations.", "description": "This table presents a quantitative comparison of different explanation methods' faithfulness.  Faithfulness is measured using the delta-AF metric (AeRF - AMORF), where higher scores indicate more accurate and reliable explanations. The table compares MambaLRP against several gradient-based methods (Gradient \u00d7 Input, SmoothGrad, Integrated Gradients), a naive LRP implementation (LRP (LN-rule)), and two attention-based methods (Attention Rollout, MambaAttr) across various datasets (SST-2, Medical BIOS, SNLI, Emotion, ImageNet).  The results show MambaLRP consistently outperforms other methods in faithfulness.", "section": "5 Experiments"}, {"figure_path": "2n1Ysn1EDl/tables/tables_16_1.jpg", "caption": "Table 1: Evaluating explanation methods. Higher scores \u0394AF indicate more faithful explanations.", "description": "This table compares the faithfulness of different explanation methods across various datasets (SST-2, Med-BIOS, SNLI, Emotion, and ImageNet) using two metrics: \u0394AF and \u0394AI. Higher scores indicate better faithfulness, meaning the explanations more accurately reflect the model's decision-making process. The methods compared are: Random, GI, SmoothGrad, IG, AttnRoll, MambaAttr, LRP (LN-rule), and MambaLRP (the proposed method).  MambaLRP consistently achieves the highest faithfulness scores, significantly outperforming all other methods.", "section": "Experiments"}, {"figure_path": "2n1Ysn1EDl/tables/tables_16_2.jpg", "caption": "Table 1: Evaluating explanation methods. Higher scores \u2206AF indicate more faithful explanations.", "description": "This table presents a comparison of different explanation methods' performance across various datasets (SST-2, Med-BIOS, SNLI, Emotion, and ImageNet).  The methods are evaluated using two faithfulness metrics (\u2206AF and \u2206AI) calculated using input perturbation with a Most Relevant First (MoRF) and Least Relevant First (LeRF) strategy.  Higher scores in \u2206AF indicate more faithful explanations.  The table shows that MambaLRP consistently outperforms the other methods.", "section": "5 Experiments"}, {"figure_path": "2n1Ysn1EDl/tables/tables_17_1.jpg", "caption": "Table 6: Finding the best LRP composite for Vision Mamba. The layers in which the generalized LRP-y rule is applied are represented with LRP-y and the ones in which the basic LRP rule, i.e. LRP-0, is used are represented with LRP-0.", "description": "This table presents an ablation study on the Vision Mamba model to determine the optimal LRP composite.  Different combinations of applying the generalized LRP-\u03b3 rule and basic LRP-0 rule across various layers (input projection, output projection, and convolutional layers) within each block of the model are tested. The performance metric used is \u0394A<sup>F</sup>, which measures the faithfulness of the explanations. The table shows the \u0394A<sup>F</sup> score for each configuration, helping to identify the best combination of LRP rules for achieving the most faithful explanations.", "section": "C.2.1 LRP composites for Vision Mamba"}, {"figure_path": "2n1Ysn1EDl/tables/tables_18_1.jpg", "caption": "Table 1: Evaluating explanation methods. Higher scores \u2206AF indicate more faithful explanations.", "description": "This table compares the faithfulness of different explanation methods (MambaLRP, Gradient \u00d7 Input, SmoothGrad, Integrated Gradients, Attention Rollout, MambaAttr, and a naive LRP implementation) across four text classification datasets (SST-2, Medical BIOS, SNLI, Emotion) and one image classification dataset (ImageNet).  Faithfulness is measured using two metrics: \u2206AF (AeRF - AMORF) and \u2206AI (AMORF - ALeRF), where higher scores indicate better faithfulness.  The results demonstrate that MambaLRP consistently achieves the highest faithfulness scores, significantly outperforming the other methods.", "section": "5 Experiments"}, {"figure_path": "2n1Ysn1EDl/tables/tables_18_2.jpg", "caption": "Table 1: Evaluating explanation methods. Higher scores \u2206AF indicate more faithful explanations.", "description": "This table compares the faithfulness of different explanation methods (Random, GI, SmoothGrad, IG, AttnRoll, MambaAttr, LRP (LN-rule), and MambaLRP) across multiple datasets (SST-2, Med-BIOS, SNLI, Emotion, and ImageNet).  Faithfulness is measured using the delta-AF metric (AeRF - AMORF), where higher scores indicate more reliable explanations.  The results show that MambaLRP consistently outperforms other methods across all datasets.", "section": "5 Experiments"}, {"figure_path": "2n1Ysn1EDl/tables/tables_19_1.jpg", "caption": "Table 1: Evaluating explanation methods. Higher scores \u2206AF indicate more faithful explanations.", "description": "This table presents a comparison of different explanation methods' performance in terms of faithfulness, evaluated using the delta-F metric (\u2206AF).  Higher \u2206AF scores indicate more faithful explanations.  The methods compared include random baseline, Gradient \u00d7 Input (GI), SmoothGrad, Integrated Gradients (IG), Attention Rollout (AttnRoll), MambaAttr, LRP (LN-rule), and the proposed MambaLRP method. The comparison is performed across multiple datasets including SST-2, Med-BIOS, SNLI, Emotion and ImageNet, for various sizes of Mamba models.", "section": "5 Experiments"}, {"figure_path": "2n1Ysn1EDl/tables/tables_19_2.jpg", "caption": "Table 1: Evaluating explanation methods. Higher scores \u2206AF indicate more faithful explanations.", "description": "This table compares the performance of different explanation methods (Random, Gradient \n\u00d7 Input (GI), SmoothGrad, Integrated Gradients (IG), Attention Rollout (AttnRoll), MambaAttr, LRP (LN-rule), and MambaLRP) across four datasets (SST-2, Med-BIOS, SNLI, and Emotion) and ImageNet. The methods are evaluated using the \u2206AF metric, where a higher score indicates better faithfulness of the explanation. The results demonstrate that MambaLRP consistently achieves the highest faithfulness scores, significantly outperforming other methods.", "section": "5 Experiments"}, {"figure_path": "2n1Ysn1EDl/tables/tables_19_3.jpg", "caption": "Table 1: Evaluating explanation methods. Higher scores \u2206AF indicate more faithful explanations.", "description": "This table presents a comparison of different explanation methods' faithfulness scores across multiple datasets (SST-2, Medical BIOS, SNLI, Emotion, and ImageNet).  Faithfulness is measured using the delta-AF metric (\u2206AF = AeRF - AMORF), where higher scores indicate more faithful explanations. The methods compared include several gradient-based, model-agnostic techniques (GI, SmoothGrad, Integrated Gradients) and two attention-based methods (AttnRoll, MambaAttr), along with a naive implementation of LRP (LRP (LN-rule)) and the proposed MambaLRP. The results show that MambaLRP consistently achieves the highest faithfulness scores across all datasets.", "section": "5 Experiments"}, {"figure_path": "2n1Ysn1EDl/tables/tables_20_1.jpg", "caption": "Table 1: Evaluating explanation methods. Higher scores \u2206AF indicate more faithful explanations.", "description": "This table presents a comparison of different explanation methods' performance on several datasets (SST-2, Medical BIOS, SNLI, Emotion, and ImageNet).  The methods compared include several gradient-based techniques, two attention-based approaches, and the proposed MambaLRP.  The evaluation metric used is \u2206AF, which reflects the faithfulness of the explanation. Higher \u2206AF scores indicate more faithful explanations, meaning the explanation better aligns with the model's actual decision-making process. The table highlights MambaLRP's superior performance compared to the baseline methods.", "section": "5 Experiments"}, {"figure_path": "2n1Ysn1EDl/tables/tables_23_1.jpg", "caption": "Table 1: Evaluating explanation methods. Higher scores \u2206AF indicate more faithful explanations.", "description": "This table presents a comparison of different explanation methods' performance on several datasets. The metrics used are \u2206AF and \u2206AI, which measure the faithfulness of explanations. Higher scores indicate more faithful explanations. The methods compared include baseline methods like Gradient \u00d7 Input, SmoothGrad, Integrated Gradients, and attention-based methods like Attention Rollout and MambaAttr. The proposed method, MambaLRP, is also included and shows superior performance across all datasets. This table provides quantitative evidence supporting the claim that MambaLRP generates more faithful and robust explanations.", "section": "5 Experiments"}, {"figure_path": "2n1Ysn1EDl/tables/tables_23_2.jpg", "caption": "Table 1: Evaluating explanation methods. Higher scores \u2206AF indicate more faithful explanations.", "description": "This table compares the faithfulness of different explanation methods, including Gradient \u00d7 Input (GI), SmoothGrad, Integrated Gradients (IG), Attention Rollout (AttnRoll), MambaAttr, a naive implementation of LRP (LRP (LN-rule)), and the proposed MambaLRP.  Faithfulness is measured using the delta AF metric (\u2206AF = AeRF - AMORF), where AeRF is the area under the curve for the Least Relevant First (LeRF) strategy and AMORF is for the Most Relevant First (MoRF) strategy. Higher \u2206AF scores indicate better faithfulness. The table shows that MambaLRP consistently achieves the highest faithfulness scores across various datasets, significantly outperforming other methods.", "section": "5 Experiments"}, {"figure_path": "2n1Ysn1EDl/tables/tables_23_3.jpg", "caption": "Table 1: Evaluating explanation methods. Higher scores \u2206AF indicate more faithful explanations.", "description": "This table presents a comparison of different explanation methods' faithfulness scores on several datasets.  The \u2206AF score, a metric of faithfulness, is higher for methods producing more reliable explanations.  The results show that MambaLRP consistently outperforms other methods across multiple datasets, demonstrating superior explanation accuracy.", "section": "5 Experiments"}]