[{"figure_path": "lWHe7pmk7C/figures/figures_1_1.jpg", "caption": "Figure 1: Comparative analysis of 3DGS trained with clean raw images, denoted X, versus noisy raw images, denoted X, across various training view counts N. The clean raw images, captured in daylight, are selected from the RawNeRF dataset [23]. The noisy raw images are generated from these clean images using the noise model from PMN [6] with calibrated camera noise parameters. (a) Training with noisy raw images results in decreased PSNR in the test views, with a widening performance gap as the number of training views is reduced. (b) The rendering speed (FPS) shows a similar trend to PSNR. (c) Test view visualizations show that training with noisy images causes 3DGS to produce numerous thin, flat Gaussian shapes, leading to visual artifacts and reduced FPS, especially with fewer training views.", "description": "This figure presents a comparative analysis of 3D Gaussian Splatting (3DGS) trained on clean versus noisy raw images.  It shows that training with noisy raw images leads to a significant decrease in Peak Signal-to-Noise Ratio (PSNR) and Frames Per Second (FPS), especially when the number of training views is limited. The visualization demonstrates that the noisy training data causes 3DGS to generate numerous thin and flat Gaussian shapes, resulting in visual artifacts.", "section": "1 Introduction"}, {"figure_path": "lWHe7pmk7C/figures/figures_1_2.jpg", "caption": "Figure 1: Comparative analysis of 3DGS trained with clean raw images, denoted X, versus noisy raw images, denoted X, across various training view counts N. The clean raw images, captured in daylight, are selected from the RawNeRF dataset [23]. The noisy raw images are generated from these clean images using the noise model from PMN [6] with calibrated camera noise parameters. (a) Training with noisy raw images results in decreased PSNR in the test views, with a widening performance gap as the number of training views is reduced. (b) The rendering speed (FPS) shows a similar trend to PSNR. (c) Test view visualizations show that training with noisy images causes 3DGS to produce numerous thin, flat Gaussian shapes, leading to visual artifacts and reduced FPS, especially with fewer training views.", "description": "This figure compares the performance of 3DGS trained on clean vs. noisy raw images.  It shows that noisy images significantly reduce PSNR and FPS (frames per second), especially when training data is limited.  The visualization shows that noisy training data leads to numerous thin, flat Gaussian shapes in the 3D model, degrading reconstruction quality and speed.", "section": "1 Introduction"}, {"figure_path": "lWHe7pmk7C/figures/figures_1_3.jpg", "caption": "Figure 1: Comparative analysis of 3DGS trained with clean raw images, denoted X, versus noisy raw images, denoted X, across various training view counts N. The clean raw images, captured in daylight, are selected from the RawNeRF dataset [23]. The noisy raw images are generated from these clean images using the noise model from PMN [6] with calibrated camera noise parameters. (a) Training with noisy raw images results in decreased PSNR in the test views, with a widening performance gap as the number of training views is reduced. (b) The rendering speed (FPS) shows a similar trend to PSNR. (c) Test view visualizations show that training with noisy images causes 3DGS to produce numerous thin, flat Gaussian shapes, leading to visual artifacts and reduced FPS, especially with fewer training views.", "description": "This figure presents a comparative analysis of 3D Gaussian Splatting (3DGS) trained with clean versus noisy raw images, showing the impact of noise on PSNR (peak signal-to-noise ratio), FPS (frames per second), and visual quality across different numbers of training views.  The results demonstrate that noise significantly degrades the performance of 3DGS, particularly when the number of training views is limited.  Training with noisy data leads to lower PSNR, lower FPS, and the creation of numerous thin and elongated Gaussian shapes in the 3D representation, leading to visual artifacts.", "section": "1 Introduction"}, {"figure_path": "lWHe7pmk7C/figures/figures_4_1.jpg", "caption": "Figure 2: An illustration of how prevalent noise in raw images impacts the 3DGS optimization. (a) The imaging process inherently introduces additive noise at various stages due to physical principles and hardware limitations, represented as x = x + n, where x and x denote the noisy and clean images, respectively. (b) For a real-world point r, a collection of raw images X = {x1, x2} records its intensity at pixel coordinates {p1, p2}, influenced by noise. The optimal target of 3DGS for this point, denoted as x(p) = Ex(p)~x, has a discrepancy from the clean pixel intensity x(p). The variance of this discrepancy is detailed in Eq. (9).", "description": "This figure illustrates how noise affects the 3DGS optimization process.  Panel (a) shows the different noise sources introduced during image capture, from photon flux to the final noisy image. Panel (b) focuses on a single point in the 3D scene and shows how noise from multiple noisy raw images affects the optimal target for 3D Gaussian Splatting reconstruction.", "section": "4 Methodology"}, {"figure_path": "lWHe7pmk7C/figures/figures_4_2.jpg", "caption": "Figure 2: An illustration of how prevalent noise in raw images impacts the 3DGS optimization. (a) The imaging process inherently introduces additive noise at various stages due to physical principles and hardware limitations, represented as x = x + n, where x and x denote the noisy and clean images, respectively. (b) For a real-world point r, a collection of raw images X = {x1, x2} records its intensity at pixel coordinates {p1, p2}, influenced by noise. The optimal target of 3DGS for this point, denoted as x(p) = Ex(p)~x, has a discrepancy from the clean pixel intensity x(p). The variance of this discrepancy is detailed in Eq. (9).", "description": "This figure illustrates how noise affects the 3D Gaussian Splatting (3DGS) optimization process.  Panel (a) shows the stages of image capture where noise is introduced. Panel (b) shows how noise in multiple raw images of the same scene point (r) leads to a variance in the optimal target for 3DGS, which is represented as the discrepancy between the clean pixel intensity and the expected value of the noisy pixel intensities.", "section": "Methodology"}, {"figure_path": "lWHe7pmk7C/figures/figures_4_3.jpg", "caption": "Figure 3: Visualization of the 3DGS test view changes across optimization iterations. Initially, the 3DGS model fits the clean signal (at 1,000 and 3,000 iterations). However, as the iterations progress (from 10,000 to 30,000), the model starts to overfit the noise.", "description": "This figure shows how a 3D Gaussian Splatting (3DGS) model's reconstruction changes over training iterations. Initially, the model accurately represents the scene. As training progresses, the model starts to fit the noise in the raw image data, leading to a less accurate and more noisy representation.", "section": "4.1 Motivation"}, {"figure_path": "lWHe7pmk7C/figures/figures_5_1.jpg", "caption": "Figure 4: Illustration of the noise-robust reconstruction loss, Lnrr, which comprises three components: the reconstruction loss LRawNeRF, the negative likelihood loss (NLL), and the covariance loss Lcov. A noisy raw image, x, is first input to the noise extractor Fn(\u00b7; \u03a9) to estimate the noise, \u00een. The estimated noise \u00een is then used to calculate the NLL loss relative to the noise distribution. After that, the normalized noise, 2, undergoes a covariance loss, Lcov, to minimize spatial dependencies among noise components. Finally, the reconstruction loss, LRawNeRF, is computed between the rendered distorted image D(x) and the pseudo clean image x \u2013 \u00een.", "description": "This figure illustrates the proposed noise-robust reconstruction loss function, Lnrr.  It's composed of three parts:  the RawNeRF reconstruction loss (comparing the rendered, distortion-corrected image to a denoised version of the input), the negative log-likelihood (NLL) loss (measuring the difference between the predicted noise and the expected noise distribution), and the covariance loss (Lcov) (penalizing spatial correlations in the predicted noise). The process starts with a noisy raw image which passes through a noise extractor to produce an estimate of the noise. This noise estimate is used to calculate the NLL and Lcov losses, and the denoised image is used for the RawNeRF loss. The three losses are combined to form the final Lnrr loss, which guides the training process of the 3DGS model.", "section": "4.3 Noise robust reconstruction loss"}, {"figure_path": "lWHe7pmk7C/figures/figures_6_1.jpg", "caption": "Figure 5: Comparative evaluation of various baselines and our method on rendering quality and speed in limited views training settings. The two-stage denoiser + 3DGS methods are represented by dotted lines, while training on RGB images is indicated by square markers. All metrics are evaluated on test views within the RGB domain.", "description": "The figure shows a comparative analysis of different methods for 3D Gaussian Splatting (3DGS) in terms of PSNR, SSIM, LPIPS, and FPS.  The x-axis represents the number of training views, and the y-axis shows the performance metrics. The results are broken down for different methods, including baselines that use RGB images, two-stage methods combining denoising and 3DGS, and the authors' proposed method. The graph helps visualize how the number of training views affects the quality and speed of 3D scene reconstruction with 3DGS, demonstrating the improvement achieved by the authors' noise-robust approach.", "section": "5 Experiments"}, {"figure_path": "lWHe7pmk7C/figures/figures_6_2.jpg", "caption": "Figure 5: Comparative evaluation of various baselines and our method on rendering quality and speed in limited views training settings. The two-stage denoiser + 3DGS methods are represented by dotted lines, while training on RGB images is indicated by square markers. All metrics are evaluated on test views within the RGB domain.", "description": "This figure presents a comparative analysis of different methods for 3D Gaussian Splatting (3DGS) in terms of rendering quality and speed, specifically focusing on scenarios with limited training views. The methods compared include various baselines and the proposed method.  The metrics used for evaluating rendering quality include Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS). Frames per second (FPS) is used to measure speed.  The results show that the proposed method outperforms all baselines across various metrics and view counts, highlighting its effectiveness in limited data scenarios.", "section": "5 Experiments"}, {"figure_path": "lWHe7pmk7C/figures/figures_7_1.jpg", "caption": "Figure 6: Visual comparison using ours and competing methods under the 12-view training settings.", "description": "This figure provides a visual comparison of the proposed method against several competing methods under limited training views (12 views).  It shows that the proposed method produces significantly higher-quality images, demonstrating its ability to effectively handle noise and reconstruct detailed scenes, unlike the other methods, that either exhibit noise artifacts or overly smooth results. The PSNR, SSIM, and LPIPS values are included beneath each image for quantitative comparison.", "section": "5 Experiments"}, {"figure_path": "lWHe7pmk7C/figures/figures_7_2.jpg", "caption": "Figure 7: Visual comparison across various methods in full views (100-view) training settings.", "description": "This figure presents a visual comparison of different novel view synthesis methods on the \"ar4music\" scene from the RawNeRF dataset.  The methods compared include traditional denoisers (BM3D, ELD, PMN, LGBPN), a self-supervised denoiser (Ne2Ne), a video denoiser (RViDeNet), and 3DGS models (LDR and HDR Scaffold-GS), along with the proposed method. The comparison shows the visual quality of the rendered images and highlights the improvement achieved by the proposed approach in terms of detail, color accuracy and noise reduction compared to existing methods.  The PSNR, SSIM and LPIPS metrics are also provided for each method.", "section": "5 Experiments"}, {"figure_path": "lWHe7pmk7C/figures/figures_13_1.jpg", "caption": "Figure 8: Visualization of our failure cases on the RawNeRF dataset [23]. Despite achieving a higher PSNR than HDR Scaffold-GS by eliminating noise-induced artifacts, our approach resulted in overly smooth outcomes in areas with reflections.", "description": "This figure shows a comparison of results from HDR Scaffold-GS and the proposed method on a scene with reflective surfaces. While the proposed method achieves higher PSNR by reducing noise artifacts, it produces overly smooth results in reflective areas, indicating a limitation of the approach in handling such scenarios.", "section": "5 Experiments"}, {"figure_path": "lWHe7pmk7C/figures/figures_13_2.jpg", "caption": "Figure 8: Visualization of our failure cases on the RawNeRF dataset [23]. Despite achieving a higher PSNR than HDR Scaffold-GS by eliminating noise-induced artifacts, our approach resulted in overly smooth outcomes in areas with reflections.", "description": "This figure showcases a comparison between the proposed method and HDR Scaffold-GS on the RawNeRF dataset.  While the proposed method achieves higher PSNR by reducing noise artifacts, it results in overly smoothed images, particularly in areas containing reflections, highlighting a limitation where high-frequency details are lost in the smoothing process.  The images depict a piano with its brand name visible.  The top images are close-up views of the piano's nameplate, and the bottom images are zoomed-in views of the same region, showcasing the differences in rendering quality between the two methods.", "section": "Failure case"}, {"figure_path": "lWHe7pmk7C/figures/figures_14_1.jpg", "caption": "Figure 9: Visual comparison between NeRF-based and 3DGS-based methods in full views training settings (Zoom in for best view). Noise impact is unique to 3DGS. Compared to RawNeRF, HDR Scaffold-GS is more vulnerable to noise, generating many thin-flat 3D Gaussian points to overfit it. The MLP in RawNeRF, acting as a low-pass filter, does not have this issue. Additionally, the rendering speed of 3DGS is affected by the number of Gaussian points, degrading due to noise points. Our method avoids these noisy Gaussian points, boosting rendering speed and quality.", "description": "This figure compares the results of RawNeRF, HDR Scaffold-GS, and the proposed method on a scene with full views training. The zoomed-in images highlight the difference in noise handling. RawNeRF utilizes an MLP, effectively acting as a low-pass filter, reducing noise impact.  HDR Scaffold-GS shows increased sensitivity to noise, creating thin, flat Gaussian shapes in the rendering. In contrast, the proposed method better handles the noise, resulting in a cleaner image and improved rendering speed.", "section": "G Visual comparison between NeRF-based and 3DGS-based methods"}]