[{"type": "text", "text": "Ex Uno Pluria: Insights on Ensembling in Low Precision Number Systems ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Giung Nam Kim Jaechul Graduate School of AI KAIST, Daejeon, South Korea giung@kaist.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Juho Lee Kim Jaechul Graduate School of AI KAIST, Daejeon, South Korea juholee@kaist.ac.kr ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "While ensembling deep neural networks has shown promise in improving generalization performance, scaling current ensemble methods for large models remains challenging. Given that recent progress in deep learning is largely driven by the scale, exemplified by the widespread adoption of large-scale neural network architectures, scalability emerges an increasingly critical issue for machine learning algorithms in the era of large-scale models. In this work, we first showcase the potential of low precision ensembling, where ensemble members are derived from a single model within low precision number systems in a training-free manner. Our empirical analysis demonstrates the effectiveness of our proposed low precision ensembling method compared to existing ensemble approaches. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "In computer science, it is a de facto standard to represent continuous real numbers using finite precision number systems. While many applications rely on precision formats like FP32 or FP64, the deep learning community is increasingly turning to 16-bit floating-point formats such as FP16 (Micikevicius et al., 2018) or BF16 (Dean et al., 2012) to reduce memory usage during training. More recently, researchers are further exploring low precision optimization, aiming to utilize fewer bits (8 bits or less) to represent weights, activations, and gradients throughout the training process (Gupta et al., 2015; Li et al., 2017; Sun et al., 2020; Wortsman et al., 2023). ", "page_idx": 0}, {"type": "text", "text": "While low precision number systems can aid in training deep neural networks, they are also beneficial for reducing inference costs in real-world deployments of such models (Jacob et al., 2018). In particular, recent advancements in large language models (Brown et al., 2020; Touvron et al., 2023) containing billions of parameters have triggered active exploration of post-training quantization techniques, for deploying pre-trained large language models on hardware with limited memory resources. This exploration encompasses quantizing both weights and activations (Dettmers et al., 2022; Yao et al., 2022), as well as quantizing weights only (Frantar et al., 2023; Lin et al., 2024). ", "page_idx": 0}, {"type": "text", "text": "Originally, the goal of post-training quantization is to specify the best solution represented in low precision number systems, aiming to reduce discrepancies from the original high-precision weights, like perturbations in weight values or increases in loss functions (Nagel et al., 2020). On the other hand, the presence of numerous distinct yet high-performing models within a single basin on loss landscapes (Sadrtdinov et al., 2023; Lion et al., 2024) evokes a Bayesian concept of marginalization instead of optimization, which involves utilizing multiple solutions rather than relying solely on one solution (Wilson and Izmailov, 2020). ", "page_idx": 0}, {"type": "text", "text": "Hinging on this insight, we suggest building ensembles within low precision number systems, as illustrated in Fig. 1. It depicts a proof-of-concept method for ensemble construction using stochastic rounding, a technique commonly used in low precision training to address the issue of rounding weight updates below the minimum precision threshold to zero (Gupta et al., 2015). In our approach, stochastic rounding is employed for ensembling rather than optimization. Indeed, our experimental findings in Section 4 validate that low precision ensembling improves the downstream performance of pre-trained large models without any further training on downstream data. ", "page_idx": 0}, {"type": "image", "img_path": "CbtkDWZzDq/tmp/3eddd69cab349f35a8f2b2f71333ff854f8243d50d3f9cf4126894f429c6f973.jpg", "img_caption": ["Figure 1: Concepts of low precision ensembling. It shows a two-dimensional schematic, where the $\\mathbf{X}$ and y axes represent the neural network weights, while the contours above visualize the loss surface. (a) Let the pre-trained weights, denoted by a yellow star-shaped marker $(\\sun)$ , be positioned within a basin on the loss landscape. In general, (b) post-training quantization methods introduce lower precision number systems, and then (c) choose one candidate from the system, such as the nearest one. (d) However, there are many other highly effective models available, that can contribute to ensemble predictions. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Confirming the potential of low precision ensembling for pre-trained models, we extend our investigation through a comparative study with existing methods involving ensembling. Specifically, we examine Bayesian approaches that approximate a Gaussian posterior over the loss landscape (Maddox et al., 2019; Shen et al., 2024), and sampling techniques that collect model copies from the update trajectory (Huang et al., 2017; Zhang et al., 2020). Our experimental results in Section 4 show that low precision ensembling successfully gathers diverse ensemble members contributing the final ensemble performance within the low precision number system. ", "page_idx": 1}, {"type": "text", "text": "The main contributions of our work can be outlined as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 Introducing low precision number systems inevitably results in quantization errors, usually seen as a flaw to be corrected in neural network quantization. Our work presents a novel viewpoint: these errors can be utilized as a source to improve ensemble diversity. Expanding on our comprehension of diversity, we suggest a simple yet powerful approach to constructing ensembles called Low Precision Ensembling with Bernoulli Stochastic Rounding (LPE-BSR), particularly advantageous for large models.   \n\u2022 The proposed LPE-BSR involves assembling ensemble members with low precision number systems, effectively addressing a notable challenge associated with memory costs inherent in ensemble methods. In this regard, our work holds promise for utilizing low precision number systems to construct ensembles of large models, offering a potential solution for the scalability issue faced by the Bayesian deep learning community in the era of large-scale models (Papamarkou et al., 2024). ", "page_idx": 1}, {"type": "text", "text": "2 Ensemble methods in modern transfer learning scenarios ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Ensembling neural networks is a long-established idea in machine learning (Hansen and Salamon, 1990), where the underlying design principle is to create a strong hypothesis that effectively explains the data by combining a set of weak hypotheses (Kearns, 1988). Notably, even after the transition into the deep learning era, ensemble methods continue to serve as a straightforward yet powerful strategy for boosting the performance of machine learning algorithms involving deep neural networks (Krizhevsky et al., 2012; Ciresan et al., 2012). However, the operational principles of such deep ensembles, i.e., ensembles composed of deep neural networks, deviate from those of classical statistical models and remain not fully comprehended. For instance, both Lee et al. (2015) and Nixon et al. (2020) validated that bagging (Breiman, 1996), built on the theoretically well-motivated bootstrap method (Efron, 1992), does not offer any benefits over the simplest deep ensembles consisting of models obtained from multiple training runs with different random seeds. ", "page_idx": 1}, {"type": "text", "text": "Empirically, it is well-known that we need nothing more than employing different initializations for each ensemble member to construct high-performance deep ensembles (Lakshminarayanan et al., 2017). Fort et al. (2019) delved deeper into this and emphasized the significant role played by a highly non-convex loss function of deep neural networks, where varying initializations for stochastic optimization yield different functional modes. It also aligns with the Bayesian perspective provided by Wilson and Izmailov (2020), which suggests that deep ensembles are involved in approximating multimodal posterior distribution to the Bayesian model averaging. To sum up, while the operational principle of deep ensembles may differ from classical ensembles, the underlying idea regarding diversity remains constant (Krogh and Vedelsby, 1994; Ortega et al., 2022), i.e., ensembles demonstrate improved performance when their individual members offer diverse predictions. ", "page_idx": 2}, {"type": "text", "text": "However, the simple strategy mentioned earlier, which aims to cover multiple modes on the loss landscape by starting from different initializations to achieve ensemble diversity (Fort et al., 2019; Wilson and Izmailov, 2020), faces challenges in modern transfer learning scenarios. It arises from the typical situation that there is only one pre-trained model available for fine-tuning; due to the considerable cost for pre-training of large models, model providers usually do not distribute multiple model copies. In such cases, fine-tuned solutions originating from the same pre-trained weights often inhabit the same pre-trained basin, leading to restricted exploration within the loss landscape (Neyshabur et al., 2020; Mustafa et al., 2020). Consequently, our attention should be directed towards addressing the local structure around the pre-trained basin in modern transfer learning scenarios (Wortsman et al., 2022; Sadrtdinov et al., 2023; Lee et al., 2024), rather than the global multimodal structure of the loss landscape (Fort et al., 2019; Wilson and Izmailov, 2020). ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Finite precision number systems. Computers use binary sequences for data encoding, with FP32 serving as the primary finite precision number system employed to represent real numbers. Given its coverage from approximately $10^{-38}$ to $10^{38}$ with a resolution of $10^{-\\dot{7}}$ , we consider the FP32 system as the continuous set of real numbers $\\mathbb{R}$ . Moreover, we describe INT- $B$ systems, commoly utilized low precision number systems for neural network quantization: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathbb{F}_{\\mathrm{INT-}B}=\\left\\{m\\times\\frac{w_{\\mathrm{absmax}}}{2^{B-1}-1}\\,:\\,m\\in\\left\\{-2^{B-1}+1,\\ldots,0,\\ldots,2^{B-1}-1\\right\\}\\right\\},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the system can represent real numbers in the range $[-w_{\\mathsf{a b s m a x}},w_{\\mathsf{a b s m a x}}]$ with a resolution of $w_{\\mathsf{a b s m a x}}/(2^{B-1}-1)$ , and the integer $m$ can be encoded using $B$ bits. This is the simplest form of integer quantization, with possible variations like a zero offset or non-uniform resolution (Gholami et al., 2021; Yvinec et al., 2023). Unless otherwise specified, our experimental results employ this basic form of symmetric uniform quantization with $B\\,=\\,5$ for simplicity. We also use per-channel granularity, sharing the number systems among the output channels of each linear layer. ", "page_idx": 2}, {"type": "text", "text": "Rounding rules. Let $\\mathbb{F}\\subset\\mathbb{R}$ be a finite precision number system. For any $w\\,\\in\\,\\mathbb{R}$ , there are two rounding options in practice: $\\left\\lfloor w\\right\\rfloor\\;=\\;\\operatorname*{max}\\left\\{\\,\\hat{w}\\in\\mathbb{F}:\\hat{w}\\le w\\right\\}$ and $\\left\\lceil w\\right\\rceil\\;=\\;\\operatorname*{min}\\left\\{\\hat{w}\\in\\mathbb{F}:\\hat{w}\\ge w\\right\\}$ . The rounding-to-nearest (RTN) scheme deterministically selects the closest one, i.e., $\\hat{w}=\\lfloor w\\rceil$ , whereas the Bernoulli stochastic rounding (BSR) scheme randomly chooses one of them, i.e., ", "page_idx": 2}, {"type": "text", "text": "Observing empirically that BSR sometimes produce superior results compared to RTN motivates the neural network quantization community to explore more sophisticated rounding schemes (Nagel et al., 2020; Lee et al., 2023). On the other hand, recognizing the presence of multiple competitive solutions, we consider leveraging them for low precision ensembling. ", "page_idx": 2}, {"type": "text", "text": "Ensemble methods. In ensemble methods for classification problems, the final prediction during testing is obtained by combining $S$ predictions: ", "page_idx": 2}, {"type": "equation", "text": "$$\np(y|x)=\\frac{1}{S}\\sum_{s=1}^{S}p(y|x,\\pmb{w}_{s}),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $y$ is a class label, $x$ is an input, and $p(\\boldsymbol{y}|\\boldsymbol{x},\\boldsymbol{w}_{s})$ is the categorical probabilities predicted by the $s^{\\mathrm{th}}$ member, which is a neural network model with parameters $\\pmb{w}_{s}$ . These $\\pmb{w}_{s}$ could either be maximum a posteriori (MAP) solutions obtained through multiple stochastic optimizations (Lakshminarayanan et al., 2017), or they might be intermediate checkpoints from a single training run (Huang et al., 2017; Garipov et al., 2018). ", "page_idx": 2}, {"type": "table", "img_path": "CbtkDWZzDq/tmp/0f2fc5551139998a27bc30999cde637258979bc895f6cc87a95ef83c5e6c4493.jpg", "table_caption": ["Table 1: Motivating results for low precision ensembling of pre-trained ViT models. Negative log-likelihood (NLL), classification error (ERR), and ensemble ambiguity (AMB) for rounding-tonearest (RTN) and low precision ensembling with Bernoulli stochastic rounding (LPE-BSR) derived from the publicly available pre-trained ImageNet model (\u2606). Blue highlights the areas where LPEBSR excels, particularly in larger models and lower precision settings. "], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "In the Bayesian framework, neural network weights $\\mathbf{\\nabla}w$ are treated as random variables, and $\\pmb{w}_{s}$ are seen as Monte Carlo samples employed to approximate the posterior distribution. More precisely, Eq. 3 can be seen as a simple Monte Carlo method to approximate the posterior with a set of point masses, where the locations are given by samples from another approximate posterior $q$ , i.e., $\\begin{array}{r}{p({\\pmb w}|\\mathcal{D})\\approx\\sum_{s=1}^{S}\\delta({\\pmb w}-{\\pmb w}_{s})/S,\\,{\\pmb w}_{s}\\sim q({\\pmb w})}\\end{array}$ , where $\\mathcal{D}$ denotes the data and $\\delta$ is the Dirac delta function (Wilson and Izmailov, 2020). A common practice is to use a Gaussian approximation $q(\\pmb{w})=\\mathcal{N}(\\pmb{w};\\pmb{\\mu},\\pmb{\\Sigma})$ to generate samples ${\\pmb w}_{s}\\,\\sim\\,q({\\pmb w})$ in a tractable manner (Maddox et al., 2019; Shen et al., 2024), and then approximate the predictive distribution by computing ", "page_idx": 3}, {"type": "equation", "text": "$$\np(y|x)=\\frac{1}{S}\\sum_{s=1}^{S}p(y|x,\\pmb{w}_{s}),\\quad\\pmb{w}_{1},\\pmb{\\ldots},\\pmb{w}_{S}\\sim q(\\pmb{w}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "4 An empirical study of low precision ensembling ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We present a simple yet effective low precision ensemble construction strategy, Low Precision Ensembling with Bernoulli Stochastic Rounding (LPE-BSR), which computes Eq. 4 using ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\boldsymbol{q}(\\boldsymbol{w})=\\prod_{i=1}^{D}\\boldsymbol{q}(\\boldsymbol{w}^{(i)}),\\quad\\boldsymbol{q}(\\boldsymbol{w}^{(i)})=\\lambda_{i}\\cdot\\delta(\\boldsymbol{w}^{(i)}-|\\boldsymbol{w}^{(i)}|)+(1-\\lambda_{i})\\cdot\\delta(\\boldsymbol{w}^{(i)}-\\big[\\boldsymbol{w}^{(i)}|\\big),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "for $i\\ =\\ 1,\\ldots,D$ . Here, $D$ denotes the number of neural network parameters, where each perchannel parameter group shares the same low precision number system, as explained in Section 3. Using the rounding operations $\\lfloor\\cdot\\rfloor$ and $\\lceil\\cdot\\rceil$ defined within each system, the probability is determined by $\\bar{\\lambda_{i}}\\;=\\;\\left\\lceil w_{i}\\right\\rceil-w_{i}$ , as in Eq. 2. Certainly, the proposed LPE-BSR is not Bayesian, and we have simply expressed it in the form of Eq. 4 for the sake of notational simplicity. ", "page_idx": 3}, {"type": "text", "text": "4.1 Motivation: training-free ensemble construction of large ViT models ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We begin with motivating experiments using the publicly available series of pre-trained vision transformer models (ViT; Dosovitskiy et al., 2021). Detailed information about each model can be found in Appendix A. Table 1 summarizes the evaluation results on a subset of the ImageNet validation split, along with the parameter count for each model. In this context, the pre-trained model corresponds to the star-shapred marker (\u2606) in Fig. 1, with RTN using the nearest value in low precision number systems as shown in Fig. 1(c), and LPE-BSR forming an ensemble by selecting $S\\,=\\,10$ nearby samples as illustrated in Fig. 1(d). ", "page_idx": 3}, {"type": "text", "text": "Table 1 provides the following key findings: 1) Larger models experience less performance degradation when reducing the precision of numerical systems. More precisely, in the RTN results, the classification error increases from . $243\\to.247\\to.315$ when transitioning from $\\mathrm{FP}32\\rightarrow\\mathrm{INT}.6\\rightarrow\\mathrm{INT}.4$ at ViT-T/16, whereas at ViT-L/16, it shifts from . $165\\rightarrow$ . $165\\to.167.$ 2) Lower precision systems introduce diversity among samples in LPE-BSR. Specifically, ensemble ambiguity is the metric for quantifying the ensemble diversity (to be defined in Section 4.3), and in all models, the ensemble ambiguity increases when transitioning from INT- $6\\rightarrow\\mathrm{INT}{-}4$ . ", "page_idx": 3}, {"type": "image", "img_path": "CbtkDWZzDq/tmp/989e66198db9e9893a14368508d67afa2fd7c552c09371704708ed5d736b40d4.jpg", "img_caption": ["Figure 2: Comparing low precision ensembling to Bayesian methods. Negative log-likelihood for Bayesian model averaging using an approximate Gaussian posterior derived from SWAG or IVON (BMA, shown in orange) and low precision ensembling with Bernoulli stochastic rounding centered around the MAP solution obtained by each optimizer (LPE-BSR, shown in green). "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Importantly, these findings are in line with the core principle of ensemble methods being effective: a key condition for an ensemble of classifiers to outperform any of its individual members is when the classifiers are both accurate and diverse (Dietterich, 2000). When introducing the low precision number system, 2) suggests that diversity can be achieved by leveraging the quantization error inherent in this process, while 1) emphasizes that larger models maintain accurate individual performance throughout this process. With this compelling motivation for low precision ensembling established, we now proceed to compare it with existing ensemble methods. ", "page_idx": 4}, {"type": "text", "text": "4.2 Comparative study to Bayesian methods ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our initial investigation into the proposed LPE-BSR aims to assess the effectiveness of collecting ensemble members within the discrete space defined by the low precision number system. While using fewer bits to represent samples would certainly reduce ensemble costs, there is a concern that we might overlook potentially good ensemble candidates outside this discrete space. To address this, we conduct a comparative study with two Bayesian deep learning methods: improved variational online newton (IVON; Shen et al., 2024) and stochastic weight averaging Gaussian (SWAG; Maddox et al., 2019). Both methods use samples drawn from an approximate Gaussian posterior in the continuous weight space and perform ensembling in a Bayesian manner. ", "page_idx": 4}, {"type": "text", "text": "To sum up our experiment, we first fine-tune the zero-shot CLIP-ViT model (Radford et al., 2021) on the ImageNet training split to obtain the MAP solution $\\pmb{w}_{\\mathbf{MAP}}^{*}$ . For LPE-BSR, this fine-tuning can employ any of the SGD, SWAG, or IVON optimizers. We then define $q$ according to Eq. 5 using $\\pmb{w}_{\\mathbf{MAP}}^{*}$ and compu\u2217te Eq. 4 with $S$ samples. For SWAG and IVON, $q(\\pmb{w})=\\mathcal{N}(\\pmb{w};\\pmb{\\mu},\\pmb{\\Sigma})$ is defined using the $\\mu\\,=\\,w_{\\mathrm{MAP}}^{*}$ and $\\Sigma$ obtained during their respective optimization processes, followed by the computation of Eq. 4. Consequently, LPE-BSR samples neighboring points of $\\pmb{w}_{\\mathbf{MAP}}^{*}$ within the discrete space defined by the low precision number system, whereas SWAG and IVON sample nearby points of $w_{\\mathrm{MAP}}$ in the continuous space with Gaussian noise added. For more details on SWAG and IVON, including their hyperparameters, please refer to Appendix E. ", "page_idx": 4}, {"type": "text", "text": "Fig. 2 presents the outcomes of our experiments conducted with CLIP-ViT-L/14. In our experimental setup, both SWAG and IVON successfully estimated both the MAP mean and the covariance matrix, resulting in a lower negative log-likelihood compared to MAP through Bayesian model averaging, as illustrated in the second and third subplots of Fig. 2. Remarkably, our LPE-BSR, derived from the MAP solution obtained by the SGD optimizer, achieves competitive results with Bayesian model averaging through SWAG or IVON. Moreover, when using the same MAP solution obtained with the SWAG or IVON optimizer for a fair comparison, it even outperforms these methods. ", "page_idx": 4}, {"type": "text", "text": "From a numerical integration perspective (Wilson and Izmailov, 2020; Wilson, 2021), the conditions for successful approximate Bayesian inference in deep learning are very similar to those for successful ensembling, as discussed in Section 4.1 with reference to Dietterich (2000). Specifically, it entails 1) finding typical points in the posterior that represent regions of substantial mass (cf. accurate); and (ii) ensuring a diverse set of points to give rise to different functions (cf. diverse). Consequently, we proceed to conduct a comparative analysis of these two factors concerning our LPE-BSR method and the Bayesian methods we considered, SWAG and IVON. ", "page_idx": 4}, {"type": "table", "img_path": "CbtkDWZzDq/tmp/9835d55c5e48471ee98762eb155355440bd74e999b415e69cc85a15d8e71dce3.jpg", "table_caption": ["Table 2: Results for low precision ensembling of fine-tuned models. We compute (a) average loss, (b) ambiguity, and (c) ensemble loss for diversity analysis, along with evaluation metrics to assess overall performance. Both BMA and LPE-BSR samples are centered around $\\frac{A}{A}$ within each group (MAP in this context), which are separated by horizontal lines. "], "table_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "CbtkDWZzDq/tmp/793d3ff6c14b08282bead4a04934e0357b44466a2bc49377788fa100540c54a4.jpg", "img_caption": ["Figure 3: Comparison between IVON and LPE-BSR samples. Radial landscape plots visualize a plane subspace defined by three points: the MAP obtained by IVON (depicted as a yellow star \u2606), samples in BMA and LPE-BSR procedures (represented by blue and red circle markers \u25cb). "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "4.3 Diversity analysis of ensemble methods ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Quantitative assessment of ensemble diversity is an essential metric for evaluating ensemble methods. To this end, we adopt the generalized ambiguity decomposition for cross-entropy loss presented by Wood et al. (2023), which can be easily measured using logit ensembling instead of probability ensembling. It should be noted that logit ensembling is solely utilized for diversity analysis, whereas probability ensembling is used for all other experimental results to ensure a fair comparison with Bayesian methods that compute the categorical predictions using the BMA integral. Please refer to Appendix B for more details on our diversity analysis. ", "page_idx": 5}, {"type": "text", "text": "Table 2 presents our experimental outcomes for the generalized ambiguity decomposition (cf. \u2018Diversity analysis\u2019), along with the final ensemble perfomance (cf. \u2018Evaluation metrics\u2019). Lowering the precision of numerical systems naturally amplifies quantization error, as evidenced by the results in the INT- $B$ rows, where reducing $B$ results in higher (a) average loss. However, this also coincides with an increase in ensemble diversity, with smaller $B$ values resulting in greater (b) ambiguity. Consequently, the superior (c) ensemble loss at an appropriate precision level (e.g., $B=5$ ) highlights the fundamental principle of the low precision ensembling: it does not merely perceive quantization error problematic, but rather utilizes it to obtain ensemble diversity. Consequently, the proposed LPE-BSR yields improvements in evaluation metrics, as depicted in Table 2. Definitions for each metric can be found in Appendix B. ", "page_idx": 5}, {"type": "image", "img_path": "CbtkDWZzDq/tmp/f259909b30b908bc012bfeca8cf4378f2a372c1bac3367e93bcb9fdee4f7fac1.jpg", "img_caption": ["Figure 4: Comparison between snapshot and LPE-BSR samples. Radial landscape plots visualize a plane subspace defined by three points: the first and second snapshot samples obtained by SSE (represented by yellow and blue star-shaped marker \u2606), and LPE-BSR sample derived from the first snapshot (depicted as a red circle \u25cb). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Drawing inspiration from Fort et al. (2019), we further provide the radial landscape plot depicting a plane within weight space containing three points (shown as yellow, blue, and orange markers). The z-values for each subplot represent the negative log-likelihood (displayed on the left in a magma colormap), the function differences from the blue model (shown in the center in a blue colormap), and the red model (presented on the right in a red colormap). Namely, the first subplot indicates the placement of each model within the loss landscape, while the subsequent two subplots illustrate the extent to which they differ from each other. ", "page_idx": 6}, {"type": "text", "text": "Fig. 3 depicts radial landscape plots comparing IVON and LPE-BSR samples, showing their parallel roles in ensemble construction. Both show slightly higher individual negative log-likelihoods, shown by the circle markers, compared to the MAP denoted by a star-shaped marker in the first subplot, while also offering diverse function outputs as demonstrated in the subsequent subplots. Ultimately, LPE-BSR can identify samples in a low precision number system that qualitatively resemble the high-quality posterior samples provided by IVON, which leverages Hessian estimate information to compute the covariance of the approximate Gaussian posterior. ", "page_idx": 6}, {"type": "text", "text": "4.4 Combining with fast ensembling methods ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "A key advantage of LPE-BSR is its ability to gather ensemble members without requiring any backward computation. This feature, which eliminates the need for training, aligns with fast ensembling techniques aimed at enhancing the training efficiency of ensemble construction processes (Huang et al., 2017; Garipov et al., 2018; Benton et al., 2021). Consequently, we conduct empirical analysis to further investigate this alignment with snapshot ensembling (SSE; Huang et al., 2017), as well as cyclical stochastic gradient Langevin dynamics (CSGLD; Zhang et al., 2020), a closely related Bayesian posterior sampling algorithm. Both methods involve collecting snapshot samples on the loss landscape around $w_{\\mathrm{MAP}}$ using a cyclical learning rate schedule. For more details, including hyperparameters, please refer to Appendix E. ", "page_idx": 6}, {"type": "text", "text": "We first verify whether LPE-BSR can generate an ensemble component distinct from SSE snapshots using radial landscape analysis. Fig. 4 illustrates a plane subspace containing the first and second SSE snapshots (displayed as star-shaped markers) and the LPE-BSR sample obtained from the first snapshot (represented by a circle marker). Indeed, LPE-BSR provided a novel sample that could contribute to the ensemble; it is clearly diverse from the existing snapshots, as shown in the second and third subplots, while achieving reasonably low individual negative log-likelihoods, as shown in the first subplot. By using such LPE-BSR samples along with SSE snapshots to build an ensemble, we can reduce the cost of achieving target performance in fast ensembling methods or attain better results with the same training budgets. ", "page_idx": 6}, {"type": "text", "text": "However, while fast ensembling techniques usually prioritize evaluating training budgets, particularly the number of backward passes as seen in the literature (Huang et al., 2017; Garipov et al., 2018; Zhang et al., 2020), we also consider memory budgets in our analysis\u2014the total number of bits required to represent the entire ensemble model, which grows with the addition of ensemble members in fast ensembling. From this perspective, we devised a method to eliminate heavy SSE snapshots with high precision from the final ensemble; as a result, each original SSE snapshot is replaced by $S=5$ LPE-BSR samples. This policy also aligns with our reserach objective of exploring ensembling in low precision number systems. Consequently, by forming the ensemble exclusively with LPE-BSR samples in the low precision system, we attained better outcomes compared to SSE regarding both training budgets and memory budgets, as shown in Fig. 5. ", "page_idx": 6}, {"type": "image", "img_path": "CbtkDWZzDq/tmp/89c87950e1bd2358a4d4b500f56f8690a9f8dc712cb42659256b4756c3cc2cbb.jpg", "img_caption": ["Figure 5: Combining with fast ensembling methods. Negative log-likelihood and expected calibration error for fast ensembling methods, SSE and CSGLD, in terms of training budgets, i.e., the number of backward passes, and memory budgets, i.e., the total number of bits for representing ensemble. Top: Results with SSE. Bottom: Results with CSGLD. "], "img_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "CbtkDWZzDq/tmp/934175eb6f19f16e1fadfd195a3c1bb6f74144b73810ae79543ab587762cece5.jpg", "img_caption": ["Figure 6: Radial landscapes for zero-shot CLIP-ViT-L/14 model. Radial landscape plots visuaslize a plane subspace defined by three points: a pre-trained model (depicted as a yellow starshaped marker \u2606) and two LPE-BSR samples derived from the pre-trained weights (represented by blue and red circle markers \u25cb). "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.5 Training-free ensemble construction of pre-trained large models ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our investigation so far, involving deep neural networks up to 300M in size, substantiates the efficacy of the proposed LPE-BSR methodology. In this section, we further extend our validation to confirm that LPE-BSR enables effective ensemble construction without training, even in larger models. To this end, in addition to 300M-scale CLIP-ViT-L/14 model (Radford et al., 2021), we employ a 1B-scale CLIP-ViT-G/14 model (Cherti et al., 2023) and an 8B-scale LLaMa model (Touvron et al., 2023) in a zero-shot manner. Appendix A provides public links for each model. ", "page_idx": 7}, {"type": "text", "text": "We first present a radial landscape analysis of the CLIP-ViT-L/14 model in Fig. 6. As we analyzed previously in Section 4.3, we can confirm that the conditions for effective ensemble construction are met here as well, i.e., ensemble members exhibit slightly higher individual negative log-likelihoods (circle markers) compared to the pre-trained model (star-shaped marker) in the first subplot, and they also offer diverse function outputs, as shown in the subsequent subplots. As depicted in the leftmost subplot of Fig. 7 it leads to achieving lower negative log-likelihood through LPE-BSR compared to the pre-trained model without the need for additional training. ", "page_idx": 7}, {"type": "image", "img_path": "CbtkDWZzDq/tmp/480cc1baaa1415fcd2a8e1d855d972631403001d111931d9e3420127ef5c7f84.jpg", "img_caption": ["Figure 7: Constructing low precision ensemble of large models. Negative log-likelihood for pretrained models (Pre-trained, shown in blue) and low precision ensembling with Bernoulli stochastic rounding centered around the pre-trained model (LPE-BSR, shown in green). The evaluation was conducted on ImageNet for CLIP models and on MMLU for LLaMa-3 in a zero-shot setting. Top: When the $\\mathbf{X}$ -axis represents the ensemble size. Bottom: When the x-axis represents memory budgets, i.e., the total number of bits for representing ensemble. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "CbtkDWZzDq/tmp/1c99b59a429b5ba0c56961b66310a776bcce7a94cbdc739bf1b81137d759c662.jpg", "table_caption": ["Table 3: Results for low precision ensembling of pre-trained models. We compute (a) average loss, (b) ambiguity, and (c) ensemble loss for diversity analysis, along with evaluation metrics to assess overall performance. Our LPE-BSR samples are centered around \u2606within each group (pretrained model in this context), which are separated by horizontal lines. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "Fig. 7 demonstrates that LPE-BSR consistently improves upon the pre-trained checkpoint, even for larger models such as CLIP-ViT-G/14 and LLaMa-3. The subplots at the top of Fig. 7 demonstrate that the performance of LPE-BSR improves with increasing ensemble size, while the subplots at the bottom show that LPE-BSR occupies the preferred lower-left region of the trade-off plots for memory budgets and performance, surpassing the pre-trained checkpoint. Table 3 offers more detailed results for an ensemble size of $S\\,=\\,20$ , including diversity analysis and evaluation results, further confirming the effectiveness of our proposed LPE-BSR for models with billions of parameters. ", "page_idx": 8}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The field of Bayesian deep learning provides the most relevant research for our low precision ensembling strategy; Ferianc et al. (2021) integrated quantization-aware training (Jacob et al., 2018) into established Bayesian deep learning methods; Zhang et al. (2022) introduced a technique for implementing stochastic gradient Langevin dynamics (Welling and Teh, 2011) with reduced precision. However, our work differs significantly from theirs in two key aspects: 1) They focused on training from scratch, which deviates somewhat from the prevalent practice of utilizing pre-trained large models. 2) They employed small-scale models; the largest model they considered, ResNet-18 with 11 million parameters, falls outside our scope as discussed in Section 4.1, as we are interested in larger scales. Nonetheless, the interest in employing low precision ensembling in Bayesian deep learning holds significant promise. Our demonstration of its feasibility for large models constitutes a meaningful advancement for the Bayesian deep learning community (Papamarkou et al., 2024). ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We provided a novel insight on ensembling within low precision number systems. While conventional wisdom perceives quantization errors stemming from representing neural network weights in low precision as obstacles, we introduced an innovative viewpoint suggesting that these errors could serve as a source of ensemble diversity. Our empirical results demonstrated that low precision weights obtained through stochastic rounding of pre-trained weights could effectively form ensembles and improve uncertainty estimates and calibration, especially for large models. Considering the growing scale of models in recent trends reduces the appeal of ensemble methods due to their inherent scalability issue, where memory costs increase with the number of ensemble components, our exploration of low precision ensembling lays the foundation for developing efficient ensemble methods in the era dominated by large models. ", "page_idx": 9}, {"type": "text", "text": "Limitations and future directions. At present, our investigations have centered on the simplest form of low precision number system, known as the symmetric uniform quantization scheme. Similar to the quest in neural network quantization for systems that yield better quantized solutions (e.g., Yvinec et al., 2023; Dettmers et al., 2024), the search for systems conducive to more effective low precision ensembling presents an intriguing avenue for future research. Furthermore, we used fake quantization across all experiments for research purposes, which prevented us from benchmarking the latency of the low precision ensemble due to limited access to specialized hardware and software for accelerating the inference speed of quantized models. Nonetheless, as our work relies on the standard symmetric uniform quantization scheme, it remains compatible with ongoing and future advancements in neural network quantization. Developing practical components such as custom CUDA kernels tailored to low precision ensembles would also be a promising future direction. ", "page_idx": 9}, {"type": "text", "text": "Broader impacts. Our method advocates for the utilization of large models, which could potentially raise ethical concerns (e.g., Weidinger et al., 2021). However, it is important to note that this work primarily focuses on analytical aspects and does not inherently entail significant ethical risks. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was partly supported by Institute of Information & communications Technology Planning & Evaluation(IITP) grant funded by the Korea government(MSIT) (No.RS-2019-II190075, Artificial Intelligence Graduate School Program(KAIST), No.2022-0-00184, Development and Study of AI Technologies to Inexpensively Conform to Evolving Policy on Ethics, No.RS-2024-00509279, Global AI Frontier Lab), and National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIT) (NRF-2021M3E5D9025030). This material is based upon work supported by the Google Cloud Research Credits program with the award GCP19980904 and Cloud TPUs from Google\u2019s TPU Research Cloud (TRC). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Man\u00e9, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi\u00e9gas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL https://www.tensorflow.org/. ", "page_idx": 9}, {"type": "text", "text": "Gregory Benton, Wesley Maddox, Sanae Lotf,i and Andrew Gordon Gordon Wilson. Loss surface simplexes for mode connecting volumes and fast ensembling. In International Conference on Machine Learning (ICML), 2021. ", "page_idx": 10}, {"type": "text", "text": "Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural network. In International Conference on Machine Learning (ICML), 2015. ", "page_idx": 10}, {"type": "text", "text": "James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http: //github.com/google/jax. ", "page_idx": 10}, {"type": "text", "text": "Leo Breiman. Bagging predictors. Machine learning, 24:123\u2013140, 1996. ", "page_idx": 10}, {"type": "text", "text": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems (NeurIPS), 2020. ", "page_idx": 10}, {"type": "text", "text": "Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. ", "page_idx": 10}, {"type": "text", "text": "Dan Ciresan, Alessandro Giusti, Luca Gambardella, and J\u00fcrgen Schmidhuber. Deep neural networks segment neuronal membranes in electron microscopy images. In Advances in Neural Information Processing Systems (NIPS), 2012. ", "page_idx": 10}, {"type": "text", "text": "Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc\u2019aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, et al. Large scale distributed deep networks. In Advances in Neural Information Processing Systems (NIPS), 2012. ", "page_idx": 10}, {"type": "text", "text": "Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. In Advances in Neural Information Processing Systems (NeurIPS), 2022. ", "page_idx": 10}, {"type": "text", "text": "Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. In Advances in Neural Information Processing Systems (NeurIPS), 2024. ", "page_idx": 10}, {"type": "text", "text": "Thomas G Dietterich. Ensemble methods in machine learning. In International workshop on multiple classifier systems, pages 1\u201315. Springer, 2000. ", "page_idx": 10}, {"type": "text", "text": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations (ICLR), 2021. ", "page_idx": 10}, {"type": "text", "text": "Bradley Efron. Bootstrap methods: another look at the jackknife. In Breakthroughs in statistics: Methodology and distribution, pages 569\u2013593. Springer, 1992. ", "page_idx": 10}, {"type": "text", "text": "Martin Ferianc, Partha Maji, Matthew Mattina, and Miguel Rodrigues. On the effects of quantisation on model uncertainty in bayesian neural networks. In Uncertainty in Artificial Intelligence (UAI), 2021. ", "page_idx": 10}, {"type": "text", "text": "Stanislav Fort, Huiyi Hu, and Balaji Lakshminarayanan. Deep ensembles: A loss landscape perspective. arXiv preprint arXiv:1912.02757, 2019. ", "page_idx": 10}, {"type": "text", "text": "Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. OPTQ: Accurate quantization for generative pre-trained transformers. In International Conference on Learning Representations (ICLR), 2023. ", "page_idx": 10}, {"type": "text", "text": "Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In International Conference on Machine Learning (ICML), 2016. ", "page_idx": 10}, {"type": "text", "text": "Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P Vetrov, and Andrew G Wilson. Loss surfaces, mode connectivity, and fast ensembling of dnns. In Advances in Neural Information Processing Systems (NeurIPS), 2018.   \nAmir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. A survey of quantization methods for efficient neural network inference. arXiv preprint arXiv:2103.13630, 2021.   \nAlex Graves. Practical variational inference for neural networks. In Advances in Neural Information Processing Systems (NIPS), 2011.   \nSuyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited numerical precision. In International Conference on Machine Learning (ICML), 2015.   \nLars Kai Hansen and Peter Salamon. Neural network ensembles. IEEE transactions on pattern analysis and machine intelligence, 12(10):993\u20131001, 1990.   \nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations (ICLR), 2021.   \nGao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E. Hopcroft, and Kilian Q. Weinberger. Snapshot ensembles: Train 1, get m for free. In International Conference on Learning Representations (ICLR), 2017.   \nBenoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018.   \nMichael Kearns. Thoughts on hypothesis boosting. Unpublished manuscript, 1988.   \nDiederick P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015.   \nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems (NIPS), 2012.   \nAnders Krogh and Jesper Vedelsby. Neural network ensembles, cross validation, and active learning. In Advances in Neural Information Processing Systems (NIPS), 1994.   \nAnanya Kumar, Ruoqi Shen, Sebastien Bubeck, and Suriya Gunasekar. How to fine-tune vision models with SGD. In International Conference on Learning Representations (ICLR), 2024.   \nBalaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems (NIPS), 2017.   \nHyungi Lee, Giung Nam, Edwin Fong, and Juho Lee. Enhancing transfer learning with flexible nonparametric posterior sampling. In International Conference on Learning Representations (ICLR), 2024.   \nJung Hyun Lee, Jeonghoon Kim, Se Jung Kwon, and Dongsoo Lee. Flexround: Learnable rounding based on element-wise division for post-training quantization. In International Conference on Machine Learning (ICML), 2023.   \nStefan Lee, Senthil Purushwalkam, Michael Cogswell, David Crandall, and Dhruv Batra. Why m heads are better than one: Training a diverse ensemble of deep networks. arXiv preprint arXiv:1511.06314, 2015.   \nHao Li, Soham De, Zheng Xu, Christoph Studer, Hanan Samet, and Tom Goldstein. Training quantized nets: A deeper understanding. In Advances in Neural Information Processing Systems (NIPS), 2017.   \nJi Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. In MLSys, 2024.   \nKai Lion, Lorenzo Noci, Thomas Hofmann, and Gregor Bachmann. How good is a single basin? In International Conference on Artificial Intelligence and Statistics (AISTATS), 2024.   \nWesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P Vetrov, and Andrew Gordon Wilson. A simple baseline for bayesian uncertainty in deep learning. In Advances in Neural Information Processing Systems (NeurIPS), 2019.   \nPaulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. In International Conference on Learning Representations (ICLR), 2018.   \nBasil Mustafa, Carlos Riquelme, Joan Puigcerver, Andr\u00e9 Susano Pinto, Daniel Keysers, and Neil Houlsby. Deep ensembles for low-data transfer learning. arXiv preprint arXiv:2010.06866, 2020.   \nMarkus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training quantization. In International Conference on Machine Learning (ICML), 2020.   \nBehnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang. What is being transferred in transfer learning? In Advances in Neural Information Processing Systems (NeurIPS), 2020.   \nJeremy Nixon, Balaji Lakshminarayanan, and Dustin Tran. Why are bootstrapped deep ensembles not better? In \u201dI Can\u2019t Believe It\u2019s Not Better!\u201d NeurIPS 2020 workshop, 2020.   \nLuis A Ortega, Rafael Caba\u00f1as, and Andres Masegosa. Diversity and generalization in neural network ensembles. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2022.   \nKazuki Osawa, Siddharth Swaroop, Mohammad Emtiyaz E Khan, Anirudh Jain, Runa Eschenhagen, Richard E Turner, and Rio Yokota. Practical deep learning with bayesian principles. In Advances in Neural Information Processing Systems (NeurIPS), 2019.   \nMahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well calibrated probabilities using bayesian binning. In AAAI Conference on Artificial Intelligence (AAAI), 2015.   \nTheodore Papamarkou, Maria Skoularidou, Konstantina Palla, Laurence Aitchison, Julyan Arbel, David Dunson, Maurizio Filippone, Vincent Fortuin, Philipp Hennig, Aliaksandr Hubin, et al. Position: Bayesian deep learning is needed in the age of large-scale ai. In International Conference on Machine Learning (ICML), 2024.   \nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML), 2021.   \nHerbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical Statistics, pages 400\u2013407, 1951.   \nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3), 2015.   \nIldus Sadrtdinov, Dmitrii Pozdeev, Dmitry P. Vetrov, and Ekaterina Lobacheva. To stay or not to stay in the pre-train basin: Insights on ensembling in transfer learning. In Advances in Neural Information Processing Systems (NeurIPS), 2023.   \nYuesong Shen, Nico Daheim, Bai Cong, Peter Nickl, Gian Maria Marconi, Clement Bazan, Rio Yokota, Iryna Gurevych, Daniel Cremers, Mohammad Emtiyaz Khan, et al. Variational learning is effective for large deep networks. In International Conference on Machine Learning (ICML), 2024.   \nXiao Sun, Naigang Wang, Chia-Yu Chen, Jiamin Ni, Ankur Agrawal, Xiaodong Cui, Swagath Venkataramani, Kaoutar El Maghraoui, Vijayalakshmi Viji Srinivasan, and Kailash Gopalakrishnan. Ultra-low precision 4-bit training of deep neural networks. In Advances in Neural Information Processing Systems (NeurIPS), 2020.   \nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \nLaura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et al. Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359, 2021.   \nMax Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In International Conference on Machine Learning (ICML), 2011.   \nYeming Wen, Dustin Tran, and Jimmy Ba. Batchensemble: an alternative approach to efficient ensemble and lifelong learning. In International Conference on Learning Representations (ICLR), 2020.   \nAndrew G Wilson and Pavel Izmailov. Bayesian deep learning and a probabilistic perspective of generalization. In Advances in Neural Information Processing Systems (NeurIPS), 2020.   \nAndrew Gordon Wilson. Deep ensembles as approximate bayesian inference, Oct 2021. URL https://cims.nyu.edu/\\~andrewgw/deepensembles/.   \nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38\u201345, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2020. emnlp-demos.6.   \nDanny Wood, Tingting Mu, Andrew M Webb, Henry WJ Reeve, Mikel Lujan, and Gavin Brown. A unified theory of diversity in ensemble learning. Journal of Machine Learning Research (JMLR), 24(359):1\u201349, 2023.   \nMitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In International Conference on Machine Learning (ICML), 2022.   \nMitchell Wortsman, Tim Dettmers, Luke Zettlemoyer, Ari Morcos, Ali Farhadi, and Ludwig Schmidt. Stable and low-precision training for large-scale vision-language models. In Advances in Neural Information Processing Systems (NeurIPS), 2023.   \nZhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. In Advances in Neural Information Processing Systems (NeurIPS), 2022.   \nEdouard Yvinec, Arnaud Dapogny, Matthieu Cord, and Kevin Bailly. Powerquant: Automorphism search for non-uniform quantization. In International Conference on Learning Representations (ICLR), 2023.   \nRuqi Zhang, Chunyuan Li, Jianyi Zhang, Changyou Chen, and Andrew Gordon Wilson. Cyclical stochastic gradient mcmc for bayesian deep learning. In International Conference on Learning Representations (ICLR), 2020.   \nRuqi Zhang, Andrew Gordon Wilson, and Christopher De Sa. Low-precision stochastic gradient langevin dynamics. In International Conference on Machine Learning (ICML), 2022. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Models and datasets ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The pre-trained weights utilized in our experiments are listed below. We refer readers to the respective papers fdetails on each model: ViT (Dosovitskiy et al., 2021), CLIP (Radford et al., 2021; Cherti et al., 2023), and LLaMa (Touvron et al., 2023). In our CLIP experiments, we obtained zero-shot head weights by following the standard procedure outlined in the official code base1. ", "page_idx": 14}, {"type": "text", "text": "\u2022 ViT-T/16: https://huggingface.co/WinKawaks/vit-tiny-patch16-224/tree/main \u2022 ViT-S/16: https://huggingface.co/WinKawaks/vit-small-patch16-224 \u2022 ViT-B/16: https://huggingface.co/google/vit-base-patch16-224 \u2022 ViT-L/16: https://huggingface.co/google/vit-large-patch16-224 \u2022 CLIP-ViT-L/14: https://huggingface.co/openai/clip-vit-large-patch14 \u2022 CLIP-ViT-G/14: https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k \u2022 LLaMa-3-8B: https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct ", "page_idx": 14}, {"type": "text", "text": "We employed two datasets for our experiments: ImageNet (Russakovsky et al., 2015) for ViT and CLIP-ViT models, and MMLU (Hendrycks et al., 2021) for LLaMa. The evaluation of MMLU was conducted using the template provided in the official repository2, and the computation was based on a micro-average. ", "page_idx": 14}, {"type": "text", "text": "B Evaluation metrics ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Let $p_{i}\\,\\in\\,[0,1]^{K}$ represent the predicted categorical probabilities and $y_{i}\\;\\in\\;\\{1,\\ldots,K\\}$ denote the ground truth label for the $i^{\\mathrm{th}}$ data point, where the total number of data points is $N$ . ", "page_idx": 14}, {"type": "text", "text": "NLL. The negative log-likelihood (NLL) of a categorical distribution, also known as cross-entropy loss, is a fundamental metric for evaluating the performance of a classification model: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{NLL}=\\frac{1}{N}\\sum_{i=1}^{N}\\log p_{i}^{(y)}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "ERR. Another primary metric commonly used to assess the performance of a classification model is the classification error (ERR), also referred to as the 0-1 loss: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{ERR}=\\frac{1}{N}\\sum_{i=1}^{N}\\left[y\\neq\\arg\\operatorname*{max}_{k}p_{i}^{(k)}\\right],\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $[\\cdot]$ denotes the Iverson bracket. ", "page_idx": 14}, {"type": "text", "text": "ECE. The common choice for measuring calibration in machine learning is the expected calibration error (ECE), particularly its empirical variant with binning (Pakdaman Naeini et al., 2015): ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{ECE}=\\sum_{j=1}^{J}\\frac{|B_{j}|\\cdot\\left|\\mathrm{acc}(B_{j})-\\mathrm{conf}(B_{j})\\right|}{N},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $B_{j}$ denotes the $j^{\\mathrm{th}}$ bin comprising $|B_{j}|$ data points whose prediction confidence $\\operatorname*{max}_{k}p_{i}^{(k)}$ falls within the interval $((j-1)/J,j/J]$ . Here, $\\operatorname{acc}(B_{j})$ is the classification accuracy of $B_{j}$ , and $\\mathrm{conf}(B_{j})$ is the average confidence within $B_{j}$ . As a result, it computes a weighted average of calibration gaps, which are the differences between accuracy and confidence, across bins. Throughout our experiments, we employed $J=15$ bins for ECE computation. ", "page_idx": 14}, {"type": "text", "text": "Ensemble ambiguity. Let $\\boldsymbol{z}_{s,i}\\in\\mathbb{R}^{K}$ be categorical logits predicted by the $s^{\\mathrm{th}}$ model for the $i^{\\mathrm{th}}$ data point. The generalized ambiguity decomposition can be written as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\underbrace{\\mathrm{AMB}}_{\\substack{\\mathrm{Analize}\\,\\mathrm{Aize}\\,\\mathrm{Aize}}}=\\frac{1}{S}\\sum_{s=1}^{S}\\frac{1}{N}\\sum_{i=1}^{N}\\log\\pmb{\\sigma}(z_{s,i})^{(y)}-\\frac{1}{N}\\sum_{i=1}^{N}\\log\\pmb{\\sigma}\\left(\\frac{1}{S}\\sum_{s=1}^{S}z_{s,i}\\right)^{(y)},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "1https://github.com/openai/CLIP   \n2https://github.com/hendrycks/test ", "page_idx": 14}, {"type": "table", "img_path": "CbtkDWZzDq/tmp/618e841538610572d037b2d2f8cf8c683f10dfb3d2ae3a4f3f3e79d3a20ad44d.jpg", "table_caption": ["Table 4: Motivating results for low precision ensembling of pre-trained ViT models. Negative log-likelihood (NLL), classification error (ERR), and ensemble ambiguity (AMB) for rounding-tonearest (RTN) and low precision ensembling with Bernoulli stochastic rounding (LPE-BSR) derived from the publicly available pre-trained ImageNet model (\u2606). "], "table_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "CbtkDWZzDq/tmp/051efa3632293141e5c076364e3c367609d93ac32e045af56155795e2e5cb204.jpg", "img_caption": ["Figure 8: Comparing low precision ensembling to Bayesian methods. Negative log-likelihood for Bayesian model averaging using an approximate Gaussian posterior derived from SWAG or IVON (BMA, shown in orange) and low precision ensembling with Bernoulli stochastic rounding centered around the MAP solution obtained by each optimizer (LPE-BSR, shown in green). "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "where $\\pmb{\\sigma}$ denotes a softmax function that maps categorical logits into probabilities. It is worth noting that logit ensembling in (c) is essentially the same as computing a a normalized geometric mean for categorical probabilities. For further information, please see Wood et al. (2023). ", "page_idx": 15}, {"type": "text", "text": "C Additional results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 Motivating results with error bars ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Table 4 is an extended version of Table 1, including standard deviations across four trials. ", "page_idx": 15}, {"type": "text", "text": "C.2 Comparative results with Bayesian methods in terms of memory budgets ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Fig. 8 is a modified version of Fig. 2, where the x-axis has been changed from ensemble size to memory budget, defined as the total number of bits used to represent the ensembles. It can be interpreted as trade-off plots between memory budgets and performance. Compared to SWAG and IVON, which represent ensemble members in the FP32 system, LPE-BSR occupies the preferred lower-left region, where ensemble members are represented in an INT-5 system. ", "page_idx": 15}, {"type": "image", "img_path": "CbtkDWZzDq/tmp/86fcac4f14bd8bcd8666e715319a9b33120c27f3eeafc2925082398d943bacfa.jpg", "img_caption": ["Figure 9: Combining with ensembling methods. Negative log-likelihood and expected calibration error for ensembling methods, SSE, DE, and MultiIVON, in terms of training budgets, i.e., the number of backward passes, and memory budgets, i.e., the total number of bits for representing ensemble. Here, DE represents an ensemble of multiple Adam solutions, while MultiIVON represents an ensemble of multiple IVON solutions. "], "img_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "CbtkDWZzDq/tmp/9744c6f618ffca085edc072bc7f7016851e0413af7493f6dc8ae167286ddaffb.jpg", "table_caption": ["Table 5: Results for low precision ensembling of fine-tuned models. We compute (a) average loss, (b) ambiguity, and (c) ensemble loss for diversity analysis, along with evaluation metrics to assess overall performance. The results are presented in ascending order of memory budgets, i.e., the total number of bits for representing ensemble. The number in parentheses after each method indicates the ensemble size. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "C.3 Further comparisons with non-Bayesian ensembles ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "While the proposed LPE-BSR method does not involve fine-tuning, though we do include finetuning experiments to compare LPE-BSR\u2019s ensemble quality with Bayesian methods in Section 4.2, a comparative study with other non-Bayesian ensemble techniques like deep ensembles (DE; Lakshminarayanan et al., 2017) and batch ensembles (BE; Wen et al., 2020) would be valuable. ", "page_idx": 16}, {"type": "text", "text": "Table 5 summarizes our experimental results using the Adam optimizer (Kingma and Ba, 2015). We observed that in LPE-BSR, (a) each ensemble member had relatively lower performance $(=0.513)$ . However, (b) due to high ensemble diversity $(\\ge0.025)$ , (c) there was a significant improvement in the final ensemble performance. Consequently, it achieves performance comparable to BE, another memory-efficient method available in fine-tuning scenarios. In BE, ensemble members are similarly centered around one solution, with members derived from shared weights by multiplying rank-one matrices, while LPE-BSR members are derived from center weights using stochastic rounding. This comparison with BE, a well-known memory-efficient ensembling strategy, highlights the potential of low precision ensembling with LPE-BSR. ", "page_idx": 16}, {"type": "text", "text": "C.4 Further comparisons with other training-free baselines ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Our LPE-BSR method forms a low precision ensemble from a given checkpoint in a training-free manner, as presented in Section 4. We further conduct additional comparative experiments with two baselines: 1) Gaussian, which builds an ensemble by adding Gaussian noise with fixed variance to the pre-trained weights; and 2) Monte Carlo Dropout (MCD; Gal and Ghahramani, 2016), which constructs an ensemble by applying the dropout technique during inference. MCD is particularly relevant as it uses a $q(w)$ form similar to Eq. 5 of LPE-BSR, employing $\\delta(\\mathbf{0})$ and $\\delta(w)$ . ", "page_idx": 16}, {"type": "text", "text": "Table 6 summarizes the results for CLIP-ViT-L/14 with an ensemble size of $S=20$ . It clearly shows that while both the Gaussian and MCD baselines can perform ensembling in a training-free manner with appropriately tuned noise scales\u2014specifically, the variance of Gaussian noise for the Gaussian baseline and the drop probability for the MCD baseline\u2014our proposed LPE-BSR outperforms them. It is worth noting that LPE-BSR is more memory-efficient, as each of its ensemble members uses INT-5, compared to FP32 used by the baseline methods. Therefore, LPE-BSR not only achieves better performance but also does so with reduced memory usage. ", "page_idx": 16}, {"type": "table", "img_path": "CbtkDWZzDq/tmp/344e2934db74ce0e9cc039de9b28481982e82ae7d61bee17ad106675372dc547.jpg", "table_caption": ["Table 6: Comparative results for training-free ensembles. The training-free ensemble methods, including Gaussian, MCD, and our proposed LPE-BSR, collect ensemble members centered around \u2606(pre-trained CLIP-ViT-L/14 model in this context). Here, $\\sigma^{2}$ denotes the variance of Gaussian noise in the Gaussian baseline, and $p$ refers to the drop probability in the MCD baseline. "], "table_footnote": [], "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "D Experimental details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We built our experimental code using JAX (Bradbury et al., 2018) and Transformers (Wolf et al., 2020), both licensed under Apache-2.0.3 We conducted experiments using TPUv2/v3/v4 cores, with flexibility in selecting the cores based on the memory requirements of each experiment. The code is available at https://github.com/cs-giung/lpe-bsr. ", "page_idx": 17}, {"type": "text", "text": "E Optimization and sampling methods ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "The optimization process for CLIP-ViT-L/14 models concludes after 100,000 iterations with a minibatch size of 64, employing a cosine decaying learning rate schedule. In the experiments described in the main text, we employed the following optimizers: SGD, IVON, and SWAG. All hyperparameter tuning was conducted using a development set created by taking $1\\%$ of the training dataset, i.e., \u2019training $[99\\%:]^{\\prime}$ in TensorFlow Datasets (Abadi et al., 2015). ", "page_idx": 17}, {"type": "text", "text": "The zero-shot head weights were kept entirely fixed, meaning they were not fine-tuned or quantized in any of the experiments. We also froze the embedding layer to enable basic SGD update rules to function with the transformer architecture, as described by Kumar et al. (2024). ", "page_idx": 17}, {"type": "text", "text": "SGD. Stochastic gradient descent (SGD) is a foundational stochastic optimization algorithm in machine learning (Robbins and Monro, 1951). Although it is usually deemed ineffective for transformer architectures, recent findings by Kumar et al. (2024) has shown that a simple modification\u2014freezing the embedding layer\u2014enables pure SGD, even without momentum, to produce results competitive with Adam (Kingma and Ba, 2015). The hyperparameter settings for the SGD optimizer utilized in our experiments are summarized in Table 7. ", "page_idx": 17}, {"type": "text", "text": "IVON. Efforts to develop variational methods for implementing Bayesian inference on neural network models have continued over time (Graves, 2011; Blundell et al., 2015). However, these attempts have frequently proven ineffective in practical scenarios, even for moderately-sized problems (Osawa et al., 2019). Recently, the Improved Variational Online Newton (IVON) algorithm, introduced by Shen et al. (2024), has facilitated variational learning for large models with an update rule closely resembling that of Adam (Kingma and Ba, 2015). In essence, by modifying the update rule of the second momentum in Adam, IVON estimates a diagonal covariance of an approximate Gaussian posterior. The hyperparameters employed for the IVON optimizer in our experiments are presented in Table 7; the notations adhere to those described in Shen et al. (2024). ", "page_idx": 17}, {"type": "table", "img_path": "CbtkDWZzDq/tmp/0233e5f63ee79796f2ef780db27a976612f6b38b45b98c35fa96974ee078a6ff.jpg", "table_caption": ["Table 7: Hyperparameters in SGD and IVON. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "SWAG. Besides variational learning, another notable approach for obtaining an approximate Gaussian posterior is Stochastic Weight Averaging Gaussian (SWAG; Maddox et al., 2019). Essentially, SWAG entails collecting samples throughout the SGD update process and calculating their sample mean and covariance to approximate the Gaussian posterior. For SWAG, we initially applied SGD updates with a cosine decay schedule until reaching a non-zero constant learning rate over 80,000 iterations. Subsequently, we switched to constant learning rate SGD updates, collecting samples every 1,000 iterations. The constant learning rate is determined by multiplying the base learning rate by a decaying factor $\\lambda_{\\mathrm{SWAG}}\\,\\in\\,\\{1.0,0.5,\\bar{0}.2,0.1\\}$ , and the final hyperparameter configuration was $\\alpha_{0}~=~0.003$ and $\\lambda_{\\mathrm{SWAG}}~=~0.2$ . Ultimately, SWAG approximates a Gaussian posterior with non-diagonal covariance matrix with a rank of 10 from these 20 samples. ", "page_idx": 18}, {"type": "text", "text": "SSE and CSGLD. When employing the SGD optimizer, the only difference between Snapshot Ensembling (SSE; Huang et al., 2017) and Cyclical Stochastic Gradient Langevin Dynamics (CSGLD; Zhang et al., 2020) lies in the incorporation of Gaussian noise in the update rule; in CSGLD, a noise term derived from stochastic gradient Langevin dynamics (Welling and Teh, 2011) is added at each iteration. In our experiments on fast ensembling, we initialized both SSE and SGLD using the outcomes obtained from training with SGD for 100,000 iterations. Employing a cosine-decaying learning rate schedule with a cycle duration of 20,000 iterations, we iterated this schedule ten times to produce a total of ten snapshots. ", "page_idx": 18}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: This paper focuses on low precision ensembling, as clearly outlined in both the abstract and introduction. ", "page_idx": 19}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The conclusion section discusses limitations as well as future directions. ", "page_idx": 19}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: This work is more experimental than theoretical, and does not include theoretical assumptions or proofs. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The appendix contains experimental details, and the main text appropriately directs readers to the appendix. ", "page_idx": 19}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: As supplementary material, we provide the code for certain experiments. The full code will be furnished later on. ", "page_idx": 19}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The appendix contains experimental details, and the main text appropriately directs readers to the appendix. ", "page_idx": 19}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: The results with $\\pm$ indicate the average and standard deviation over four measurements. The standard deviation is also shaded in the figure. ", "page_idx": 19}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The experimental details in the appendix include information regarding the hardware used. ", "page_idx": 20}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: This study adheres to a code of ethics. ", "page_idx": 20}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The conclusion section discusses broader impacts. ", "page_idx": 20}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] Justification: The paper poses no such risks. ", "page_idx": 20}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: We acknowledge and credit previous works employed in our research. ", "page_idx": 20}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 20}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 20}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 20}]