[{"figure_path": "TwdX1W3M6S/tables/tables_1_1.jpg", "caption": "Table 1: Comparison of the test accuracy between the BT-based reward model and the preference model. The reward model and preference model are trained with the same base model and preference dataset, where the details are deferred to Section 5. We evaluate the model on Reward-Bench [39].", "description": "This table compares the performance of two reward models: one based on the Bradley-Terry (BT) model and another based on a general preference model.  Both models are trained on the same dataset and evaluated on the Reward-Bench benchmark across four tasks: Chat, Chat Hard, Safety, and Reasoning.  The results show the test accuracy of each model for each task, highlighting the performance difference between the BT and general preference model approaches.", "section": "1 Introduction"}, {"figure_path": "TwdX1W3M6S/tables/tables_8_1.jpg", "caption": "Table 2: The evaluation results of the IPO-aligned models under different KL coefficients. For the first 4 win rates, we use the LLaMA3-8B-based preference model to conduct head-to-head comparisons on the hand-out test set from Ultra-feedback with 3K prompts.", "description": "This table presents the win rates of different IPO-aligned models compared to a Supervised Fine-Tuning (SFT) baseline.  The models are evaluated using a LLaMA3-8B based preference model on a test set of 3000 prompts from the Ultra-feedback dataset.  Four different KL coefficients (0.1, 0.5, 1.0, and a final model using ALPACAEVAL2) are used to train the IPO models, showcasing the impact of this hyperparameter on model performance.", "section": "6 Experiments"}, {"figure_path": "TwdX1W3M6S/tables/tables_8_2.jpg", "caption": "Table 3: The evaluation results of the models from different RLHF algorithms. The gold win rates are computed on the hand-out test set from Ultra-feedback with 3K prompts, with the Offline DPO model as the reference. Details of AlpacaEval2 can be found in Dubois et al. [21].", "description": "This table compares the performance of different Reinforcement Learning from Human Feedback (RLHF) algorithms.  The \"Gold WR\" column shows the win rate against a Supervised Fine-Tuning (SFT) baseline, measured using a LLaMA3-8B preference model on the Ultra-feedback test set (3000 prompts). The \"ALPACAEVAL2 WR\" column shows win rates evaluated against the same SFT baseline using a different test set (AlpacaEval2).  Offline DPO serves as the reference model for comparison. The table highlights the improved performance of the Online ELHF-IPO method.", "section": "6 Experiments"}, {"figure_path": "TwdX1W3M6S/tables/tables_15_1.jpg", "caption": "Table 1: Comparison of the test accuracy between the BT-based reward model and the preference model. The reward model and preference model are trained with the same base model and preference dataset, where the details are deferred to Section 5. We evaluate the model on Reward-Bench [39].", "description": "This table compares the performance of two different reward models (Bradley-Terry based and general preference model) on the Reward-Bench dataset.  Both models are trained using the same base model and preference data, and their test accuracy is evaluated across different tasks (Chat, Chat Hard, Safety, and Reasoning). The table showcases the relative performance improvements offered by the general preference model over the more traditional Bradley-Terry based model.", "section": "1 Introduction"}, {"figure_path": "TwdX1W3M6S/tables/tables_17_1.jpg", "caption": "Table 1: Comparison of the test accuracy between the BT-based reward model and the preference model. The reward model and preference model are trained with the same base model and preference dataset, where the details are deferred to Section 5. We evaluate the model on Reward-Bench [39].", "description": "This table compares the performance of a Bradley-Terry (BT)-based reward model and a general preference model on the Reward-Bench dataset. Both models were trained using the same base model and preference data. The results show the test accuracy for chat, chat hard, safety, and reasoning tasks.", "section": "1 Introduction"}]