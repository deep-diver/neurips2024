[{"type": "text", "text": "Implicit Causal Representation Learning via Switchable Mechanisms ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Learning causal representations from observational and interventional data in the   \n2 absence of known ground-truth graph structures necessitates implicit latent causal   \n3 representation learning. Implicit learning of causal mechanisms typically involves   \n4 two categories of interventional data: hard and soft interventions. In real-world   \n5 scenarios, soft interventions are often more realistic than hard interventions, as the   \n6 latter require fully controlled environments. Unlike hard interventions, which di  \n7 rectly force changes in a causal variable, soft interventions exert influence indirectly   \n8 by affecting the causal mechanism. However, the subtlety of soft interventions   \n9 impose several challenges for learning causal models. One challenge is that soft   \n10 intervention\u2019s effects are ambiguous, since parental relations remain intact. In this   \n11 paper, we tackle the challenges of learning causal models using soft interventions   \n12 while retaining implicit modeling. Our approach models the effects of soft inter  \n13 ventions by employing a causal mechanism switch variable designed to toggle   \n14 between different causal mechanisms. In our experiments, we consistently observe   \n15 improved learning of identifiable, causal representations, compared to baseline   \n16 approaches. ", "page_idx": 0}, {"type": "text", "text": "17 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "18 One of the long-standing challenges in causal   \n19 representation learning is how to recover the   \n20 ground-truth causal graph of a system solely   \n21 from observations. Termed the identifiability   \n22 of causal models problem, this endeavor is cru  \n23 cial. Without achieving identifiability, we risk   \n24 erroneously attributing causal relationships to   \n25 learned representations. Furthermore, statisti  \n26 cal models can masquerade as Directed Acyclic   \n27 Graphs (DAGs) where edges lack causal signif  \n28 icance, further complicating our pursuit.   \n29 When considering the challenge of identifying   \n30 causal models, it is known that the Markov con  \n31 dition in graphs is insufficient for this task [26].   \n32 Thus, without additional assumptions or data,   \n33 we find ourselves limited to learning only a   \n34 Markov Equivalence Class (MEC) of the causal   \n35 model. Existing works have made different   \n36 assumptions about availability of ground-truth   \n37 causal variables labels [34], model parameters ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "PKEmH9ZJfw/tmp/8540d8f4e15fc644e15be939cac257e0733015c75fdbb96088d73d4cfd10ef69.jpg", "img_caption": [], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Figure 1: Difference between hard interventions and soft interventions: As seen in the middle row, hard interventions sever connections with parents. Therefore, an object\u2019s class cannot have any effect on the object\u2019s color when we intervene on color. On the other hand, soft interventions, as shown in the bottom row, allow for such effects. ", "page_idx": 0}, {"type": "text", "text": "38 [1], availability of paired interventional data [3, 31], and availability of intervention targets [17] to   \n39 ensure identifiability of causal models.   \n40 Interventional data are usually obtained through soft or hard interventions. Hard interventions   \n41 usually involve controlled experiments and they severe the connection of an intervened variable   \n42 with its parents [24]. In terms of Structural Causal Models (SCM), hard interventions set the causal   \n43 mechanism relating a causal variable to its parents, to a constant. Due to ethical or safety reasons, it   \n44 may not be possible to perform hard interventions in many real-world applications. On the other hand,   \n45 the effects of soft interventions are more subtle since parent variables can still affect their children.   \n46 These effects can be modeled by a change in the set of parents, the causal mechanisms, and the   \n47 exogenous variables [7]. Consequently, hard interventions can also be seen as a special case of soft   \n48 interventions where the causal mechanism is set to a constant. Illustrated in Figure 1, a prominent   \n49 challenge in causal representation learning lies in dealing with the ambiguity surrounding the effects   \n50 of soft interventions. The observed alterations in object colors fail to distinctly elucidate whether   \n51 they stem from parental influences or the applied interventions.   \n52 Additionally, a lack of comprehension regarding causal graphs can pose significant challenges in   \n53 causal representation learning. In certain applications, the causal graph can be constructed using   \n54 domain knowledge, allowing us to subsequently learn the causal variables [2, 18, 20]. However, this is   \n55 not universally applicable, necessitating the direct learning of the causal graph itself. In a Variational   \n56 AutoEncoder (VAE) framework, there are generally two approaches for causal representation learning:   \n57 Explicit Latent Causal Models (ELCMs) [34, 1, 35, 37, 17, 15] and Implicit Latent Causal Models   \n58 (ILCMs) [3]. In ELCMs, the latents are the causal variables and the adjacency matrix of the causal   \n59 graph is parameterized and integrated into the prior of the latents such that the prior of latents is   \n60 factorized according to the Causal Markov Condition [27]. This approach to causal representation   \n61 learning is highly susceptible to becoming stuck in local minima as it is hard to learn representations   \n62 without knowing the graph, and it is hard to learn the graph without knowing the representations.   \n63 ILCMs [3] were introduced to circumvent this \u201cchicken-and-egg\u201d problem by using solution functions,   \n64 which can implicitly model edges in the causal graph rather than explicitly modeling the entire   \n65 adjacency matrix of the causal model. In ILCMs the latents are the exogenous variables and the there   \n66 is no explicit parameterization for the graph.   \n67 In implicit causal representation learning, the task involves recovering the exogenous variables $\\mathcal{E}$   \n68 from observed variables $\\mathcal{X}$ and learning solution functions. In [3], interventions are assumed to   \n69 be hard, but this is often unrealistic and does not align with real-world problems. In this paper,   \n70 we propose a novel approach for Implicit Causal Representation Learning via Switchable   \n71 Mechanisms (ICRL-SM). We will introduce the causal mechanism switch variable as a way of   \n72 modeling the effect of soft interventions and identifying the causal variables. Our experiments on   \n73 both synthetic and large real-world datasets, highlight the efficacy of proposed method in identifying   \n74 causal variables and promising future directions in implicit causal representation learning. Our key   \n75 contributions can be summarized as follows:   \nI. A novel approach for implicit causal representation learning with soft interventions.   \n77 II. Employing causal mechanisms switch variable to model the effect of soft interventions. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "78 III. Theory for identifiability up to reparameterization from soft interventions. ", "page_idx": 1}, {"type": "text", "text": "79 80 2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "81 Causal representation learning has recently garnered significant attention [27, 14]. The primary   \n82 challenge in this problem lies in achieving identifiability beyond the Markov equivalence class [26].   \n83 Solely relying on observational data necessitates additional assumptions regarding causal mechanisms,   \n84 decoders, latent structure, and the availability of interventional data [22, 28, 36, 25, 15, 1, 40, 13,   \n85 34]. Recent works have focused on identifying causal models from collected interventional data   \n86 instead of making strong assumptions about functions of the causal model. Interventional data   \n87 facilitates identifiability based on relatively weak assumptions [1, 6, 3, 39, 33]. This type of data   \n88 can be further categorized based on whether it involves soft or hard interventions, and whether the   \n89 manipulated variables are observed and specified or latent. Our focus in this paper is on examining   \n90 soft interventions, encompassing both observed and unobserved variables. ", "page_idx": 1}, {"type": "text", "text": "91 2.1 Explicit models vs. Implicit models ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "92 Table 1 presents a comparison of the assumptions and identifiability results between our proposed   \n93 theory and other related works on causal representation learning with interventions. In causal repre  \n94 sentation learning with interventions, one approach assumes a given causal graph and concentrates   \n95 on identifying causal mechanisms and mixing functions. For instance, Causal Component Analysis   \n96 (CauCA) [33] explores soft interventions with a known graph. Alternatively, when the graph is   \n97 not provided, explicit models seek to reconstruct it from interventional data [6, 17], potentially   \n98 resulting in a chicken-and-egg problem in causal representation learning [3]. Current methods face   \n99 the challenge of simultaneously learning the causal graph and other network parameters, especially   \n100 in the absence of information about causal variables or the graph. Addressing these challenges, [3]   \n101 recently introduced ILCM, which performs implicit causal representation learning exclusively using   \n102 hard intervention data. In contrast, our approach introduces a novel method for learning an implicit   \n103 model from soft interventions. [3] describes methods for extracting a causal graph from a learned   \n104 implicit model, which could be applied to our method as well. In our experiments, we will compare   \n105 our method with ILCM and dVAE [21], given their implicit nature and similar experimental settings   \n106 and assumptions. Additionally, to showcase the superiority of our method over explicit models, we   \n107 will employ explicit causal model discovery methods like ENCO [16] and DDS [5], in conjunction   \n108 with various variants of $\\beta$ -VAE. ", "page_idx": 1}, {"type": "table", "img_path": "PKEmH9ZJfw/tmp/dd1d6a844d0d3e0925f16575913cc2b6cfaf362739ed03ec345c38e2cab3b769.jpg", "table_caption": ["Table 1: Comparison of proposed method with other recent related work on causal learning from interventional data "], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "109 2.2 Hard interventions vs Soft interventions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "110 The identification of explicit causal models from hard interventions has been extensively ex  \n111 plored. [29] investigate causal disentanglement in linear causal models with linear mixing functions   \n112 under hard interventions. Similarly, [4] focus on identifying causal models with linear causal mecha  \n113 nisms and nonlinear mixing functions, also utilizing hard interventions. In a more general setting   \n114 with non-parametric causal mechanisms and mixing functions, [32] examine the identifiability of   \n115 causal models, utilizing multi-environment data from unknown interventions. Similarly, [2] explore   \n116 identifiability of causal models using multi-environment data from unknown interventions. [30]   \n117 investigate the identifiability of causal models with nonlinear causal mechanisms and linear mixing   \n118 functions, considering both hard and soft interventions.   \n119 Recent work has expanded the concept of explicit hard interventions to include soft interventions. In   \n120 their study, [38] address the identification of causal models from soft interventions, leveraging the   \n121 sparsity of the adjacency matrix as an inductive bias. However, when dealing with implicit models,   \n122 soft interventions introduce new complexities. Identifiability becomes more challenging, as the   \n123 causal effect of variables on observed variables is less apparent. This ambiguity arises from the dual   \n124 possibility of effects originating from interventions or influences from parent variables on the causal   \n125 variables. Moreover, in scenarios where implicit modeling is retained, the absence of knowledge about   \n126 parent variables further complicates identifiability. While [3] theoretically establishes identifiability   \n127 for hard interventions, practical experiments involving complex causal models with over 10 variables   \n128 reveal increased ambiguity and confounding factors. Consequently, model identification becomes   \n129 less straightforward. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "130 3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "131 3.1 Data Generating Process ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "132 A structural causal model (Definition A1.1) is used to understand and describe the relationships   \n133 between different variables and how they influence each other through causal mechanisms. A decoder   \n134 function, $g(\\mathbf{z})=\\mathbf{x}$ , maps a vector of causal values $z$ to observed values $x$ . The causal variables   \n135 $\\mathcal{Z}$ are unobserved and the goal is to infer them from interventional data. For each causal variable,   \n136 a diffeomorphic solution function, $s_{i}:\\mathcal{E}_{i}\\to\\mathcal{Z}_{i}$ , deterministically maps a value for exogenous   \n137 variable $\\mathcal{E}_{i}$ to a value for causal variable $\\mathcal{Z}_{i}$ . In implicit modeling, we learn the solution functions $s_{i}$   \n138 directly, rather than defining them through local mechanisms $f_{i}$ . We write $\\boldsymbol{S}$ for the set of all solution   \n139 functions $s_{i}\\in\\mathcal S$ , so $S:{\\mathcal{E}}\\rightarrow{\\mathcal{Z}}$ .   \n140 Identifying causal models from data can be complex and is often studied within classes of models   \n141 such as those identifiable up to affine transformations. For example, in the context of nonlinear   \n142 Independent Component Analysis $(I C A)$ , the generative process also involves a mixture function $g$ of   \n143 latent causal variables $\\mathcal{Z}\\in\\mathbb{R}^{n}$ , resulting in observations $\\mathcal{X}\\in\\mathbb{R}^{n}$ [15, 41]. However, a significant   \n144 distinction between causal representation learning and nonlinear-ICA is that in the former, the causal   \n145 variables $\\mathcal{Z}$ may have complex dependencies. Our objective in this paper is to recover $\\mathcal{E}$ from $\\mathcal{X}$ and   \n146 eventually map $\\mathcal{E}$ to $\\mathcal{Z}$ using solution functions.   \n147 Identifying a causal model from observational data is not trivial and requires assumptions on the   \n148 parameters of the model [1]. Adding information about interventions in addition to observations,   \n149 helps to identify causal variables by exhibiting the effect of changing a causal variable on the observed   \n150 variables. An interventional data point $(x,\\tilde{x},i)$ includes the pre-intervention observation $x$ , the post  \n151 intervention observation $\\tilde{x}$ , and intervention target $i\\in\\mathcal{Z}$ where $\\mathcal{T}$ is the set of intervention targets   \n152 selected from the causal variables. The post-intervention data $\\tilde{x}$ is generated by a soft intervention   \n153 that targets one of the causal variables in $\\mathcal{Z}$ . To achieve identifiability up to reparametrization, we   \n154 rely on a series of assumptions within the data generation process, outlined as follows: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "155 Assumption 3.1. (Data generating assumptions) ", "page_idx": 3}, {"type": "text", "text": "156 1. Atomic Interventions: For every sample $(x,\\tilde{x},i)$ , only one causal variable is targeted by an   \n157 intervention.   \n159 3. Post-intervention Exogenous Variables: The exogenous variables\u2019 values change only for the   \n160 corresponding intervened causal variable, while the others maintain their pre-intervention values,   \n161 thus $e_{i}\\neq\\tilde{e}_{i}$ if $i\\in\\mathcal{Z}$ ,and $e_{i}=\\tilde{e}_{i}$ otherwise.   \n162 4. Sufficient Variability: Soft interventions alter causal mechanisms to introduce sufficient variability   \n163 [15]. These interventions should modify causal mechanisms to ensure non-overlapping conditional   \n164 distributions of causal variables (refer to Figure A1).   \n165 5. Diffeomorphic decoder and causal mechanisms: Diffeomorphism guarantees no information loss   \n166 and avoids abrupt changes in the function\u2019s image.   \n167 The known targets assumption can be relaxed in applications where such data is not available   \n168 and the same procedure in [3] can be used to infer the intervention targets. In fact, in our real  \n169 world experiments, intervention targets are not available and based on the nature of the datasets, we   \n170 hypothesize our causal variables to be object attributes and actions to be intervention targets. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "171 3.2 Causal Mechanisms Switch Variable ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "172 The major difference of soft intervention with hard intervention is that post-intervention causal   \n173 variable $\\tilde{\\mathcal{Z}}_{i}$ is no longer disconnected from its parents and its causal mechanism $\\tilde{s}_{i}$ is affected by the   \n174 intervention. This is why identifying the causal mechanisms is more difficult for soft interventions.   \n175 Soft intervention data yield fewer constraints on the causal graph structure than hard intervention   \n176 data. For more details refer to string diagrams of soft and hard interventions depicted in Figure A5.   \n177 Figure 2b shows our main generative model. It includes a data augmentation step that adds the   \n178 intervention displacement $\\tilde{x}-x$ as an observed feature that directly represents the effect of a soft   \n179 intervention in observation space.   \n180 Augmented implicit causal model To model the effect of soft interventions, we introduce the   \n181 causal mechanism switch variable $\\nu$ [26]. By leveraging $\\nu$ , we can effectively switch to the pre  \n182 intervention causal mechanisms within post-intervention data. This facilitates the model\u2019s ability to   \n183 solely focus on discerning alterations in the intrinsic characteristics of each causal variable. These   \n184 changes are encapsulated within their respective exogenous variables, aiding the model in learning   \n185 the causal relationships more accurately. We propose to use a modulated form of $\\nu$ to model the   \n186 soft intervention effects on each causal variable as an additive effect with a nonlinear function $h_{i}$   \n187 such that $\\forall i$ , $\\tilde{\\mathcal{Z}}_{i}=\\tilde{s}_{i}(\\tilde{\\mathcal{E}}_{i};\\tilde{\\mathcal{E}}_{/i})=s_{i}(\\tilde{\\mathcal{E}}_{i};\\mathcal{E}_{/i},h_{i}(\\mathcal{V}))$ . As the parental set for each causal variable is   \n188 not known, we have to use a modulated form of $\\mathcal{V}$ in every causal variable\u2019s solution function and   \n189 the inclusion of $h_{i}(\\nu)$ enables the model to encompass variations in the parental sets of all causal   \n190 variables in $\\mathcal{V}$ . Therefore, there is a switch variable $\\nu_{i}$ for each causal variable $\\mathcal{Z}_{i}$ . Adding switch   \n191 variables to solution functions leads to the concept of an augmented implicit causal model.   \n192 Definition 3.2. (Augmented Implicit Causal Models) $A n$ Augmented Implicit Causal Models (AICMs)   \n193 is defined as $\\boldsymbol{A}=(S,\\mathcal{Z},\\mathcal{E},\\mathcal{V})$ where $\\mathcal{V}\\in\\mathbb{R}^{n}$ is the causal mechanism switch variable which models   \n194 the effect of soft interventions on solution functions $\\boldsymbol{S}$ : ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\forall i,\\,\\tilde{\\mathcal{Z}}_{i}=\\tilde{s}_{i}(\\tilde{\\mathcal{E}}_{i};\\tilde{\\mathcal{E}}_{/i})=s_{i}(\\tilde{\\mathcal{E}}_{i};\\mathcal{E}_{/i},h_{i}(\\mathcal{V})),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "195 where $\\tilde{s_{i}}$ is the new solution function resulting from the soft intervention, $\\tilde{\\mathcal{E}}_{/i}$ is the altered set of all   \n196 exogenous variables except $i$ , including the ancestral exogenous variables, due to intervention, and   \n197 $\\tilde{\\mathcal{E}}_{i}$ is the post-intervention exogenous variable.   \n198 The usage of $\\mathcal{V}$ in soft interventions is analogous to augmented networks in [23] which were mainly   \n199 designed for hard interventions. Pearl [23] even foresaw this possibility by saying: \"One advantage   \n200 of the augmented network representation is that it is applicable to any change in the functional   \n201 relationship $f_{i}$ and not merely to the replacement of $f_{i}$ by a constant.\" ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "202 By using Taylor\u2019s expansion, we can expand the solution functions as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\iota(\\tilde{\\mathcal{E}}_{i};\\mathcal{E}_{j+},h_{i}(\\boldsymbol{V}))=s_{i}(\\tilde{\\mathcal{E}}_{i};\\mathcal{E}_{j+},h_{i}(\\boldsymbol{v}_{0}))+\\sum_{n=1}^{\\infty}\\frac{1}{n!}\\left(\\frac{\\partial^{n}s_{i}}{\\partial h_{i}^{n}}\\bigg|_{h_{i}=h_{i}(\\boldsymbol{v}_{0})}(h_{i}(\\boldsymbol{V})-h_{i}(\\boldsymbol{v}_{0}))^{n}\\right)=s_{i}(\\tilde{\\mathcal{E}}_{i};\\mathcal{E}_{j+},h_{i}(\\boldsymbol{v}_{0}))+R_{i}(\\boldsymbol{v}_{0}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "203 where we\u2019ll use $R_{i}$ as a short-hand for Equation 2. We define the separable dependence property   \n204 for solution functions as $\\exists h_{i}(v_{0}):s_{i}(\\tilde{\\mathcal{E}}_{i};\\bar{\\mathcal{E}}_{/i},h_{i}(v_{0}))=s_{i}(\\tilde{\\mathcal{E}}_{i};\\mathcal{E}_{/i})$ . An example of such a scenario   \n205 could be in location-scale noise models such as, $s_{i}(\\tilde{e_{i}};e_{/i},h_{i}^{'}(v))\\;=\\;\\tilde{e_{i}}+l o c(e_{/i})\\,+\\,h_{i}(v)\\;=\\;$   \n206 $\\tilde{e_{i}}+l o c(e_{/i})+v^{2}+v$ where $v_{0}$ would be zero . By assuming the separable dependence property,   \n207 we can write the solution function in Equation 2 as: ", "page_idx": 4}, {"type": "equation", "text": "$$\ns_{i}(\\tilde{\\mathcal{E}}_{i};\\mathcal{E}_{/i},h_{i}(\\boldsymbol{\\mathcal{V}}))=s_{i}(\\tilde{\\mathcal{E}}_{i};\\mathcal{E}_{/i})+R_{i}=s_{i}(\\tilde{\\mathcal{E}}_{i};\\mathcal{E}_{/i})+s o f t\\,i n t e r v e n t i o n\\,\\,e\\!f\\!\\!f e c t\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "208 As a result, we can switch to pre-intervention solution functions. Subsequently, by modeling soft   \n209 intervention effects using $h_{i}(\\nu)$ , we can recover pre-intervention solution functions. During inference,   \n210 we simply disregard the $h_{i}(\\nu)$ term in the solution functions. Nonetheless, it is possible to train the   \n211 prior $p(\\mathcal{V})$ to ensure that the separable dependence property is maintained for pre-intervention data.   \n212 Observability of switch variable The intuition behind using $\\nu$ is to separate the effect of soft   \n213 intervention on $\\tilde{\\mathcal{Z}}_{i}$ into two: (1) The effect on causal mechanisms and parents, and (2) The effect on   \n214 exogenous variable $\\mathcal{E}_{i}$ . For example, we can say that causal variables in images of objects are the   \n215 objects\u2019 attributes such as shape, color, and size, and performing actions like \"Fold\" change these   \n216 attributes. Furthermore, it can be asserted that the camera angle within a given image may influence   \n217 the shape of the object. If the images were generated from a hard intervention, the camera angle   \n218 remains fixed between pre and post intervention. However, the camera angle changes along with   \n219 the performed actions indicating that the interventions are soft. In this case, if we had a knowledge   \n220 of how the camera angle affects the attributes of objects, then we could separate the effect of soft   \n221 intervention. In other words, if $\\nu$ is observed, then we can extract the effect of the intervention that   \n222 we are interested in (i.e., the effect on the causal variable itself). For more details, refer to Figure A4.   \n223 Lacking an understanding of how soft intervention influences the causal model, a more complex   \n224 model becomes necessary. Consequently, the term $R_{i}$ in Equation 2 would involve a higher order of   \n225 $h_{i}(\\nu)$ . Therefore, we assume the observability of $\\mathcal{V}$ :   \n226 Assumption 3.3. (Observability of $\\mathcal{V}$ ) Given an intervention sample $(x,\\tilde{x},i)$ and linear decoders,   \n227 we can approximate the soft intervention effects $h_{i}(\\nu)$ as follows: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\tilde{z}-z=\\Delta e_{i}+R\\ \\ \\ (u s i n g\\;E q u a t i o n\\;2),\\quad\\tilde{x}-x=g(\\tilde{z})-g(z)\\approx g(\\tilde{z}-z)=g(\\Delta e_{i}+R),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "228 where ${R}=[{R}_{0},{R}_{1},...,{R}_{n}]$ and $n$ is the number of causal variables. $R$ and $\\Delta e_{i}$ are the vectors   \n229 indicating the soft intervention effects and change in effect of the exogenous variable of the intervened   \n230 causal variable, respectively. Note that elements of $R$ will be all zero except for the intervened causal   \n231 variable. Consequently, with linear mixing functions and some pre-processing on observed samples   \n232 (here subtraction), we can observe $R_{i}$ .   \n233 Our synthetic data is generated using a linear decoder, however, the decoder for the real-world   \n234 datasets is not necessarily linear. Therefore, we do not observe $\\mathcal{V}$ from $\\tilde{x}-x$ in the real-world dataset.   \n235 Nevertheless, our findings suggest that incorporating soft interventions through $\\nu$ leads to superior   \n236 performance compared to other implicit modeling approaches. Clearly, understanding the impact of   \n237 soft interventions on the generative system of the dataset would result in improved outcomes. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "238 3.3 Identifiability Theorem for Implicit SCMs with Soft Interventions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "239 In this paper, our focus lies in identifying the causal variables up to reparameterization through soft   \n240 interventions. We first define identifiability up to reparameterization (Definition 3.4) and subsequently   \n241 introduce the identifiability theorem 3.5. The proof of theorem is extensive and is available in full in   \n242 Appendix A1.   \n243 We establish identifiability up to reparameterization, allowing for the mapping of causal variables $\\mathcal{Z}$   \n244 and ${\\mathcal{Z}}^{\\prime}$ between two Latent Causal Models ( $\\mathcal{M}$ and $\\mathcal{M}^{\\prime}$ ) through component-wise transformations   \n245 (Definition A1.2). Given our implicit modeling approach, lacking knowledge of the causal graph, we   \n246 include all exogenous variables in the solution functions, as depicted in Equation 1. Notably, the   \n247 causal graph remains unaltered during learning. To illustrate, we contrast hard interventions,   \n248 which neglect parent influences, with soft interventions that acknowledge parental effects in a simple   \n249 example. Consider a basic causal model $Z_{1}\\to Z_{2}$ alongside a location-scale noise model [12] for the   \n250 solution function, given by $\\begin{array}{r}{\\tilde{z}_{2}=\\frac{\\tilde{e}_{2}-\\widetilde{l o c}(e_{1})}{s c a l e(e_{1})}}\\end{array}$ . The distribution $p(\\tilde{\\mathcal{Z}}_{2})$ mean is $\\begin{array}{r}{\\frac{1}{s c a l e(e_{1})}\\times\\mathrm{mean}(\\tilde{\\mathcal{E}}_{2})\\,-}\\end{array}$   \n251 slco acl(ee(1e)) In the context of hard interventions, we can assume p( Z\u02dc2|Z1) = p( Z\u02dc2) = N(0, 1) as there   \n252 are no parental effects. Consequently, the location and scale networks within the solution function tend   \n253 to dampen parental effects, given the absence of parental influence in the ground-truth data. Contrarily,   \n254 soft interventions exhibit parental influence in the ground-truth data, thus $p(\\tilde{\\mathcal{Z}}_{2}|\\mathcal{Z}_{1})\\neq N(0,1)$ . Due   \n255 to the lack of parental knowledge in implicit modeling, we model $p(\\tilde{\\mathcal{Z}}_{2}|\\mathcal{Z}_{1})\\,=\\,p(\\tilde{\\mathcal{Z}}_{2}|\\mathcal{E}_{2})$ , as $\\mathcal{E}_{2}$   \n256 is a known parent of $\\tilde{\\mathcal{Z}}_{2}$ . Consequently, parental effects are propagated to $\\mathcal{E}_{i}$ (the corresponding   \n257 exogenous variable of each causal variable), violating identifiability up to reparameterization. By   \n258 leveraging $\\mathcal{V}$ , we allow parental effects to propagate to $\\mathcal{V}$ instead of $\\mathcal{E}_{i}$ .   \n259 Definition 3.4. (Equivalence up to component-wise reparameterization) Let $\\mathcal{M}=(\\mathcal{A},\\mathcal{X},g,\\mathcal{T})$   \n260 and $\\mathcal{M}^{\\prime}=(\\mathcal{A}^{\\prime},\\mathcal{X},\\bar{g}^{\\prime},\\mathcal{T})$ be two Latent Causal Models (LCM) based on AICMs $\\boldsymbol{\\mathcal{A}}$ , $\\mathcal{A}^{\\prime}$ with shared   \n261 observation space $\\mathcal{X}$ , shared intervention targets $\\mathcal{T}$ , and respective decoders g and $g^{\\prime}$ . We say that   \n262 $\\mathcal{M}$ and $\\mathcal{M}^{\\prime}$ are equivalent up to component-wise reparameterization $\\mathcal{M}\\sim_{r}\\mathcal{M}^{\\prime}$ if there exists $a$   \n263 component-wise transformation (Definition A1.2) $\\phi_{\\mathcal{Z}}$ from the causal variables $\\mathcal{Z}$ to the causal   \n264 variables ${\\mathcal{Z}}^{\\prime}$ and a component-wise transformation $\\phi_{\\mathcal{E}}$ between $\\mathcal{E}$ and ${\\mathcal{E}}^{\\prime}$ such that:   \n265 1. Indices are preserved (i.e., $\\phi_{i}(z_{i})=z_{i}^{\\prime}$ and $\\phi_{i}(e_{i})=e_{i}^{\\prime},$ ). Corresponding edges are preserved (i.e.,   \n266 $\\mathcal{Z}_{i}\\to\\mathcal{Z}_{j}$ holds in $\\mathcal{G}$ iff $\\mathcal{Z}_{i}^{\\prime}\\to\\mathcal{Z}_{j}^{\\prime}$ holds in $\\mathcal{G}^{\\prime}$ . Edges $\\mathcal{E}_{i}\\to\\mathcal{Z}_{i}$ should be preserved as well.)   \n267 2. The exogenous transformation preserves the probability measure on exogenous variables   \n268 $p\\varepsilon^{\\prime}=(\\phi\\varepsilon)_{*}p\\varepsilon$ (Definition A1.4).   \n269 3. The causal transformation preserves the probability measure on causal variables $p_{\\mathcal{Z}^{\\prime}}=(\\phi_{\\mathcal{Z}})_{*}p_{\\mathcal{Z}}$   \n270 (Definition A1.4).   \n271   \n272 Theorem 3.5. (Identifiability of latent causal models.) Let ${\\mathcal M}\\ =\\ ({\\mathcal A},{\\mathcal X},g,{\\mathcal Z})$ and ${\\mathcal{M}}^{\\prime}\\;=\\;$   \n273 $(\\mathcal{A}^{\\prime},\\mathcal{X},g^{\\prime},\\mathcal{T})$ be two LCMs with shared observation space $\\mathcal{X}$ and shared intervention targets $\\mathcal{T}$ .   \n274 Suppose the following conditions are satisfied:   \n275 1. Data generating assumptions explained in Assumption 3.1.   \n276 2. Soft interventions satisfy Assumption 3.3.   \n277 3. The causal and exogenous variables are real-valued.   \n278 4. The causal and exogenous variables follow a multivariate normal distribution.   \n279 Then the following statements are equivalent:   \n228801 $\\mathcal{M}$ $\\mathcal{M}^{\\prime}$ i.gn the s ood to interventional and observational data i.e.,   \n$p_{\\mathcal M}^{\\mathcal X,\\mathbb Z}(x,\\tilde{x},i)=p_{\\mathcal M^{\\prime}}^{\\mathcal X,\\mathbb Z}(x,\\tilde{x},i)$   \n282 - $\\mathcal{M}$ and $\\mathcal{M}^{\\prime}$ are disentangled, that is $\\mathcal{M}\\sim_{r}\\mathcal{M}^{\\prime}$ ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "image", "img_path": "PKEmH9ZJfw/tmp/d9f6cba95ce4a4393a67a3bd48c37031441ce9a3e011bbda65557e47b512872a.jpg", "img_caption": ["(a) General overview of ICRL-SM "], "img_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "PKEmH9ZJfw/tmp/06fcf5e733780caae1e0f323f375a7ff7f0959efbbbef73526bc9f015a949c7c.jpg", "img_caption": ["(b) Generative model "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "283 3.4 Training Objective ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "284 Consequently, there will be three latent variables in ICRL-SM:   \n285 1. A causal mechanism switch variable $\\nu$ .   \n286 2. The pre-intervention exogenous variables $\\mathcal{E}$ . ", "page_idx": 5}, {"type": "text", "text": "287 3. The post-intervention exogenous variables $\\tilde{\\mathcal{E}}$ . ", "page_idx": 6}, {"type": "text", "text": "288 As the data log-likelihood $\\log p(x,\\tilde{x},x\\,-\\,\\tilde{x})\\;\\equiv\\;\\log p(x,\\tilde{x})$ is intractable, we utilize an ELBO   \n289 approximation as training objective: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{og}\\,p(x,\\tilde{x})\\ge E_{q(e,\\tilde{c},v]\\mid x,\\tilde{x}}\\left[\\log p(x,\\tilde{x}|e,\\tilde{e},v)\\right]-K L D(q(e,\\tilde{e},v|x,\\tilde{x})||p(e,\\tilde{e},x))}\\\\ &{\\qquad\\qquad\\qquad=E_{q(v|\\tilde{x}-x)\\,\\mathcal{q}(e|x)\\cdot q(\\tilde{c}|\\tilde{x})}\\left[\\log(p(x|e)p(\\tilde{x}|\\tilde{e})p(\\tilde{x}-x|v))\\right]-K L D(q(v|\\tilde{x}-x)\\cdot q(e|x)\\cdot q(\\tilde{e}|\\tilde{x})||p(\\tilde{e}|e,v)p(v)p(e)).}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "290 The observations are encoded and decoded independently. The KLD term regularizes the encodings   \n291 to share the latent intervention model $p(\\tilde{e}|e,v)p(v)p(e)$ that is shared across all data points. The   \n292 components of this model can be interpreted as follows: ", "page_idx": 6}, {"type": "text", "text": "293 1. $p(e)$ is the prior distribution over exogenous variables $e$ . ", "page_idx": 6}, {"type": "text", "text": "294 2. $p(v)$ is the prior distribution over switch variables $v$ . ", "page_idx": 6}, {"type": "text", "text": "295 3. $p(\\tilde{e}|e,v)$ is a transition model that shows how the exogeneous variables change as a function of the   \n296 intervention.   \n297 We factorize the posterior with a mean-field approximation $q(v,e,\\tilde{e}|x,\\tilde{x})=q(v|\\tilde{x}-x)\\cdot q(e|x)\\;.$   \n298 $q(\\tilde{e}|\\tilde{{\\boldsymbol x}})$ and, following our data generation model (Figure 2b), the reconstruction probability   \n299 as $\\begin{array}{r}{p(x,\\tilde{x}|e,\\tilde{e},v)\\;=\\;\\bar{p}(x|e)p(\\tilde{x}|\\tilde{e})\\bar{p}(\\tilde{x}\\,-\\,x|v)}\\end{array}$ . The prior over latent variables is factorized as   \n300 $p(\\tilde{e},e,v)=p(\\tilde{e}|e,v)p(v)p(e)$ (Figure 2b). Pre-intervention exogenous variables are mutually inde  \n301 pendent, hence, $p(e)\\,=\\,\\Pi_{i}p(e_{i})$ and $p(v)\\,=\\,\\Pi_{i}p(v_{i})$ . We assume $p(e_{i})$ and $p(v_{i})$ to be standard   \n302 Gaussian. Furthermore, as we assume $e_{i}=\\tilde{e}_{i}$ for all non-intervened variables, the $\\bar{p}(\\tilde{e}|e,v)$ will be   \n303 as follows: ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "equation", "text": "$$\np(\\tilde{e}|e,v)=\\Pi_{i\\notin I}\\delta\\big(\\tilde{e}_{i}-e_{i}\\big)\\Pi_{i\\in I}p(\\tilde{e}_{i}|e,v)=\\Pi_{i\\notin I}\\delta\\big(\\tilde{e}_{i}-e_{i}\\big)\\Pi_{i\\in I}p(\\tilde{z}_{i}|e_{i})\\left|\\frac{\\partial\\tilde{z}_{i}}{\\partial\\tilde{e}_{i}}\\right|\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "304 The last equality is obtained from the Change of Variable Rule in probability theory, applied to the   \n305 solution function $\\tilde{z}_{i}=s_{i}(\\tilde{e}_{i};e_{/i},h_{i}(v))$ . Furthermore, we write $p(\\tilde{z}_{i}|e,v)=p(\\tilde{z}_{i}|e_{i})$ since only $e_{i}$   \n306 is a known parent of $\\tilde{z}_{i}$ in implicit modeling. We assume $p(\\tilde{z}_{i}|e_{i})$ to be a Gaussian whose mean is   \n307 determined by $e_{i}$ . We implement the solution function using a location-scale noise models [12] as   \n308 also practiced in [3], which defines an invertible diffeomorphism. For simplicity, in our experiments,   \n309 we are only going to change the $l o c$ network in post-intervention. Therefore, $h_{i}(v)$ will be used as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\tilde{z}_{i}=\\tilde{s}_{i}(\\tilde{e}_{i};e_{/i},h_{i}(v))=\\frac{\\tilde{e}_{i}-(l o c_{i}(e_{/i})+h_{i}(v))}{s c a l e_{i}(e_{/i})},\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "310 where $l o c_{i}:\\mathbb{R}^{n-1}\\rightarrow\\mathbb{R}$ and $s c a l e_{i}:\\mathbb{R}^{n-1}\\to\\mathbb{R}$ are fully connected networks calculating the first   \n311 and second moments, respectively. The general overview of the model is illustrated in Figure 2a. ", "page_idx": 6}, {"type": "text", "text": "312 4 Experiments and Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "313 The experiments conducted in this paper address two downstream tasks; (1) Causal Disentanglement   \n314 to identify the true causal graph from pairs of observations $(x,\\tilde{x},i)$ , and (2) Action Inference to make   \n315 supervised inferences about actions generated from the post-intervention samples using information   \n316 about the values of the manipulated causal variables. Moreover, we conducted additional experiments   \n317 designed as an ablation study, the results of which are presented in A4. All models are trained using   \n318 the same setting and data with known intervention targets. ", "page_idx": 6}, {"type": "text", "text": "319 4.1 Datasets ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "320 Synthetic Dataset We generate simple synthetic datasets with $\\mathcal{X}=\\mathcal{Z}=\\mathbb{R}^{n}$ . For each value of   \n321 $n$ , we generate ten random DAGs, a random location-scale SCM, then a random dataset from the   \n322 parameterized SCM. To generate random DAGs, each edge is sampled in a fixed topological order   \n323 from a Bernoulli distribution with probability 0.5. The pre-intervention and post-intervention causal   \n324 variables are obtained as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{z_{i}=s c a l e(z_{p a_{i}})e_{i}+l o c(z_{p a_{i}})\\xrightarrow{\\mathrm{Soft.Intervention}}z_{i}=s c a l e(z_{p a_{i}})\\tilde{e}_{i}+\\widetilde{l o c}(z_{p a_{i}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "325 where the $l o c$ and scale networks are changed in post intervention. The pre-intervention $l o c$ and   \n326 post-intervention $\\widetilde{l o c}$ network weights are initialized with samples drawn from ${\\mathcal{N}}(0,1)$ and ${\\mathcal{N}}(3,1)$ ,   \n327 respectively. The scale is constant 1 for both pre-intervention and post-intervention samples. Both   \n328 $e_{i}$ and $\\tilde{e_{i}}$ are sampled from a standard Gaussian. The causal variables are mapped to the data space   \n329 through a randomly sampled $S O(n)$ rotation. For each dataset, we generate 100,000 training samples,   \n330 10,000 validation samples, and 10,000 test samples.   \n331 Action Datasets Causal-Triplet datasets tailored for actionable counterfactuals [19] feature paired   \n332 images where several global scene properties may vary including camera view and object occlusions.   \n333 Thus, the images can be viewed as outcomes of soft interventions, wherein actions affect objects   \n334 alongside subtle alterations. These datasets [19] consist of: images obtained from a photo-realistic   \n335 simulator of embodied agents, ProcTHOR [9], and the other contains images repurposed from a real  \n336 world video dataset of human-object interactions [8]. The former one contains $100\\,\\mathrm{k}$ images in which   \n337 7 types of actions manipulate 24 types of objects in $10\\,\\mathrm{k}$ distinct ProcTHOR indoor environments.   \n338 The latter consists of 2,632 image pairs, collected under a similar setup from the Epic-Kitchens   \n339 dataset with 97 actions manipulating 277 objects.Based on the nature of actions in this dataset, the   \n340 causal variables should represent attributes of objects such as shape and color. As the dataset consists   \n341 of images we train all the methods with ResNet encoder and decoder. For the ProcThor dataset the   \n342 number of causal variables are 7. For the Epic-Kitchens dataset, we randomly chose 20 actions from   \n343 the dataset as 97 causal variables will be too complex in a VAE setup. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "344 4.2 Metrics ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "345 For the causal disentanglement task, we are going to use the DCI scores [10]. Causal disentanglement   \n346 score quantifies the degree to which $\\mathcal{Z}_{i}$ factorises or disentangles the $\\mathcal{Z}^{\\ast}$ . Causal disentanglement $D_{i}$   \n347 for Zi is calculated as Di = (1 \u2212HK(Pi.)) = (1 +  kK=\u221201 Pik logK Pik) where Pij = kK=R\u22120i1 jRik   \n348 and $R_{i j}$ denotes the probability of $\\mathcal{Z}_{i}$ being important for predicting $\\mathcal{Z}_{j}^{\\ast}$ . Total causal disentanglement   \n349 is the weighted average  i \u03c1iDi where \u03c1i =  ijj  RRiijj . Causal Completeness quantifies the degree   \n350 to which each $\\mathcal{Z}_{i}^{\\ast}$ is captured by a single $\\mathcal{Z}_{i}$ . Causal completeness is calculated as $C_{j}\\;=\\;(1\\mathrm{~-~}$   \n351 $\\begin{array}{r}{H_{D}(\\tilde{P}_{.j}))=(1+\\sum_{d=0}^{D-1}\\tilde{P}_{d j}\\log_{D}\\tilde{P}_{i j})}\\end{array}$ . $D$ and $K$ here are equal to the dimension of $\\mathcal{Z}^{\\ast}$ and $\\mathcal{Z}$   \n352 which is $n$ . For the action inference task, we will use classification accuracy as a metric. As we   \n353 assume intervention targets are known, we train all models using known intervention targets for a fair   \n354 comparison. ", "page_idx": 7}, {"type": "text", "text": "355 5 Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "356 5.1 Causal Disentanglement ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "357 We generated a dataset for the soft interventions and trained the models of ICRL-SM, ILCM, $\\beta$ -VAE   \n358 and D-VAE for 10 different seeds, which generated 10 different causal graphs. We selected 4 causal   \n359 variables to encompass complex causal structures, including forks, chains, and colliders. Table 2   \n360 displays the Causal Disentanglement and Causal Completeness scores for all models, computed on   \n361 the test data.   \n362 The results in Table 2 indicate that our method ICRL-SM can identify the true causal graph in most   \n363 cases. The worst results are seen for graphs $G5$ and $G10$ . As mentioned in [27, 25], causal graphs are   \n364 sparse and in the $G5$ case, where the graph is fully connected, the proposed method cannot identify   \n365 the causal variables well. Furthermore, in the next experiment we are going to examine the factors   \n366 affecting causal disentanglement such as the number of edges in the graph and the intensity of soft   \n367 intervention effect. These findings can explain why ICRL-SM cannot identify causal variables in   \n368 $G10$ despite its sparsity. ", "page_idx": 7}, {"type": "table", "img_path": "PKEmH9ZJfw/tmp/5ae8b4a98d1ad7981888e5a0515b9dec5d9dde7c80c7d6a50abeaafcb2fcbc1b.jpg", "table_caption": ["Table 2: Comparison of identifiability results "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Table 3: Table comparing action and object accuracy across various methods on Causal-Triplet datasets under different settings. $Z$ and $z_{i}$ show whether all causal variables $(Z)$ , or only the intervened casual variable $(z_{i})$ are used for the prediction task. $R_{64}$ denote images with resolutions $64\\times64$ . ", "page_idx": 8}, {"type": "table", "img_path": "PKEmH9ZJfw/tmp/250ea3fe271ccc17035f972703e4f9989636a1cc721e5e5dcaf585ab7d241291.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "369 5.2 Factors Affecting Causal Disentanglement ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "370 In this experiment, we consider the graph $G3$ , which has the best identifiability, and change the   \n371 intensity of soft intervention and number of edges in its data generation process. To change the   \n372 intensity, the post-interventionloc network weights are initialized with samples drawn from $N(1,1)$   \n373 (almost similar to $l o c$ ) and $N(10,1)$ (significantly different from loc). To change the number of   \n374 edges, we consider a chain and fully-connected graph. ", "page_idx": 8}, {"type": "text", "text": "Table 4: Left table depicts the action and object accuracy of three explicit models, with experiments conducted applying an image with resolution of $R_{64}$ as the input to the Resnet50 encoder with the intervened causal variable $(z_{i})$ . Right table shows the comparison of ICRL-SM performance on different configurations of $G5$ ", "page_idx": 8}, {"type": "table", "img_path": "PKEmH9ZJfw/tmp/8001a5562ecfd33b3d574f4dd21d095c0f04cdc842932ba3e6c4825298d3ae1d.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "PKEmH9ZJfw/tmp/3fb8d2c5e94626bcc6d660db8d2c132cc0861e00a5fc11bedb8a1032c3eb5202.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "375 The results in Table 4 further confirms the sparsity of causal graphs as the causal disentanglement is   \n376 much worse in the fully-connected graph than the default graph of $G3$ . The result for significantly   \n377 different post-intervention causal mechanisms indicate that the switch variable cannot approximate   \n378 intense effects of soft intervention and more supervision is required to observe $\\nu$ . Similar post  \n379 intervention causal mechanisms also do not have sufficient variability to disentangle the causal   \n380 variables as mentioned in Theory 3.5. ", "page_idx": 8}, {"type": "text", "text": "381 5.3 Action Inference ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "382 In this experiment, we show the performance of ICRL-SM in the real-world Causal-Triplet datasets.   \n383 In these datasets $\\mathcal{V}$ i.e., soft intervention effects, are not directly observable. Nevertheless, our findings   \n384 suggest that incorporating soft interventions through $\\nu$ leads to superior performance compared to   \n385 other implicit modeling approaches. Clearly, understanding the impact of soft interventions on the   \n386 generative system of the dataset would result in improved outcomes.   \n387 The results in Table 3 indicate that when including all causal variables to predict actions, ICRL-SM   \n388 performs at par with the baseline methods. However, including all causal variables in the action   \n389 or object inference may cause spurious correlations. Therefore, we have also experimented with   \n390 including only the related causal variable in action and object inference. In this setting, ICRL  \n391 SM significantly outperforms the baseline methods which means that it can better disentangle the   \n392 causal variables. We have also compared ICRL-SM with explicit causal representation learning   \n393 methods. ENCO [16] and DDS [5] have variable topological order of causal variables during training.   \n394 Furthermore, we have included a specific setting where the topological order is fixed during training.   \n395 As shown in Table 4, our proposed method has superior performance to explicit models as well. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "396 6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "397 ICRL-SM, our novel model, enhances implicit causal representation learning during soft interventions   \n398 by introducing a causal mechanism switch variable. Evaluations on synthetic and real-world datasets   \n399 demonstrate ICRL-SM\u2019s superiority over state-of-the-art methods, highlighting its practical effective  \n400 ness. Our findings emphasize ICRL-SM\u2019s ability to discern causal models from soft interventions,   \n401 marking it as a promising avenue for future research. ", "page_idx": 8}, {"type": "text", "text": "402 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "403 [1] Kartik Ahuja, Divyat Mahajan, Yixin Wang, and Yoshua Bengio. Interventional causal representation   \n404 learning. In International Conference on Machine Learning, ICML, volume 202 of Proceedings of Machine   \n405 Learning Research, pages 372\u2013407. PMLR, 2023.   \n406 [2] Shayan Shirahmad Gale Bagi, Zahra Gharaee, Oliver Schulte, and Mark Crowley. Generative causal   \n407 representation learning for out-of-distribution motion forecasting. In International Conference on Machine   \n408 Learning, ICML, volume 202 of Proceedings of Machine Learning Research, pages 31596\u201331612. PMLR,   \n409 2023.   \n410 [3] Johann Brehmer, Pim de Haan, Phillip Lippe, and Taco S. Cohen. Weakly supervised causal representation   \n411 learning. In NeurIPS, 2022.   \n412 [4] Simon Buchholz, Goutham Rajendran, Elan Rosenfeld, Bryon Aragam, Bernhard Sch\u00f6lkopf, and Pradeep   \n413 Ravikumar. Learning linear causal representations from interventions under general nonlinear mixing,   \n414 2023.   \n415 [5] Bertrand Charpentier, Simon Kibler, and Stephan G\u00fcnnemann. Differentiable DAG sampling. In The Tenth   \n416 International Conference on Learning Representations, ICLR. OpenReview.net, 2022.   \n417 [6] Gregory F. Cooper and Changwon Yoo. Causal discovery from a mixture of experimental and observational   \n418 data, 2013.   \n419 [7] Juan D. Correa and Elias Bareinboim. General transportability of soft interventions: Completeness results.   \n420 In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin,   \n421 editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information   \n422 Processing Systems, NeurIPS, 2020.   \n423 [8] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Evangelos Kazakos, Jian Ma,   \n424 Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Rescaling egocentric   \n425 vision: Collection, pipeline and challenges for EPIC-KITCHENS-100. Int. J. Comput. Vis., 130(1):33\u201355,   \n426 2022.   \n427 [9] Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs, Kiana Ehsani, Jordi Salvador, Winson Han, Eric   \n428 Kolve, Aniruddha Kembhavi, and Roozbeh Mottaghi. Procthor: Large-scale embodied ai using procedural   \n429 generation. Advances in Neural Information Processing Systems, 35:5982\u20135994, 2022.   \n430 [10] Cian Eastwood and Christopher K. I. Williams. A framework for the quantitative evaluation of disentangled   \n431 representations. In 6th International Conference on Learning Representations, ICLR, 2018.   \n432 [11] Irina Higgins, Lo\u00efc Matthey, Arka Pal, Christopher P. Burgess, Xavier Glorot, Matthew M. Botvinick,   \n433 Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained   \n434 variational framework. In 5th International Conference on Learning Representations, ICLR, 2017.   \n435 [12] Alexander Immer, Christoph Schultheiss, Julia E. Vogt, Bernhard Sch\u00f6lkopf, Peter B\u00fchlmann, and Alexan  \n436 der Marx. On the identifiability and estimation of causal location-scale noise models. In International   \n437 Conference on Machine Learning, ICML, volume 202 of Proceedings of Machine Learning Research,   \n438 pages 14316\u201314332. PMLR, 2023.   \n439 [13] Amin Jaber, Murat Kocaoglu, Karthikeyan Shanmugam, and Elias Bareinboim. Causal discovery from soft   \n440 interventions with unknown targets: Characterization and learning. In Advances in Neural Information   \n441 Processing Systems 33: Annual Conference on Neural Information Processing Systems, NeurIPS, 2020.   \n442 [14] Jean Kaddour, Aengus Lynch, Qi Liu, Matt J. Kusner, and Ricardo Silva. Causal machine learning: A   \n443 survey and open problems. CoRR, abs/2206.15475, 2022.   \n444 [15] S\u00e9bastien Lachapelle, Pau Rodr\u00edguez, Yash Sharma, Katie Everett, R\u00e9mi Le Priol, Alexandre Lacoste,   \n445 and Simon Lacoste-Julien. Disentanglement via mechanism sparsity regularization: A new principle for   \n446 nonlinear ICA. In 1st Conference on Causal Learning and Reasoning, CLeaR, volume 177 of Proceedings   \n447 of Machine Learning Research, pages 428\u2013484. PMLR, 2022.   \n448 [16] Phillip Lippe, Taco Cohen, and Efstratios Gavves. Efficient neural causal discovery without acyclicity   \n449 constraints. In The Tenth International Conference on Learning Representations, ICLR. OpenReview.net,   \n450 2022.   \n451 [17] Phillip Lippe, Sara Magliacane, Sindy L\u00f6we, Yuki M. Asano, Taco Cohen, and Stratis Gavves. CITRIS:   \n452 causal identifiability from temporal intervened sequences. In International Conference on Machine   \n453 Learning, ICML, volume 162 of Proceedings of Machine Learning Research, pages 13557\u201313603. PMLR,   \n454 2022.   \n455 [18] Chang Liu, Xinwei Sun, Jindong Wang, Haoyue Tang, Tao Li, Tao Qin, Wei Chen, and Tie-Yan Liu.   \n456 Learning causal semantic representation for out-of-distribution prediction. In M. Ranzato, A. Beygelzimer,   \n457 Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing   \n458 Systems, volume 34, pages 6155\u20136170. Curran Associates, Inc., 2021.   \n459 [19] Yuejiang Liu, Alexandre Alahi, Chris Russell, Max Horn, Dominik Zietlow, Bernhard Sch\u00f6lkopf, and   \n460 Francesco Locatello. Causal triplet: An open challenge for intervention-centric causal representation   \n461 learning. In Conference on Causal Learning and Reasoning, CLeaR, volume 213 of Proceedings of   \n462 Machine Learning Research, pages 553\u2013573. PMLR, 2023.   \n463 [20] Yuejiang Liu, Riccardo Cadei, Jonas Schweizer, Sherwin Bahmani, and Alexandre Alahi. Towards robust   \n464 and adaptive motion forecasting: A causal representation perspective. In IEEE/CVF Conference on   \n465 Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages   \n466 17060\u201317071. IEEE, 2022.   \n467 [21] Francesco Locatello, Ben Poole, Gunnar R\u00e4tsch, Bernhard Sch\u00f6lkopf, Olivier Bachem, and Michael   \n468 Tschannen. Weakly-supervised disentanglement without compromises. In Proceedings of the 37th   \n469 International Conference on Machine Learning,ICML, volume 119 of Proceedings of Machine Learning   \n470 Research, pages 6348\u20136359. PMLR, 2020.   \n471 [22] Chaochao Lu, Yuhuai Wu, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, and Bernhard Sch\u00f6lkopf. Invariant causal   \n472 representation learning for out-of-distribution generalization. In The Tenth International Conference on   \n473 Learning Representations, ICLR, 2022.   \n474 [23] Judea Pearl. Causality, cambridge university press (2000). Artif. Intell., 169(2):174\u2013179, 2005.   \n475 [24] Judea Pearl, Madelyn Glymour, and Nicholas P. Jewell. Causal inference in statistics: A primer. John   \n476 Wiley and Sons, 2016.   \n477 [25] Ronan Perry, Julius von K\u00fcgelgen, and Bernhard Sch\u00f6lkopf. Causal discovery in heterogeneous environ  \n478 ments under the sparse mechanism shift hypothesis. In NeurIPS, 2022.   \n479 [26] Bernhard Sch\u00f6lkopf. Causality for machine learning. CoRR, abs/1911.10500, 2019.   \n480 [27] Bernhard Sch\u00f6lkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh   \n481 Goyal, and Yoshua Bengio. Toward causal representation learning. Proceedings of the IEEE, 109(5):612\u2013   \n482 634, 2021.   \n483 [28] Xinwei Shen, Furui Liu, Hanze Dong, Qing Lian, Zhitang Chen, and Tong Zhang. Weakly supervised   \n484 disentangled generative causal representation learning. J. Mach. Learn. Res., 23:241:1\u2013241:55, 2022.   \n485 [29] Chandler Squires, Anna Seigal, Salil Bhate, and Caroline Uhler. Linear causal disentanglement via   \n486 interventions, 2023.   \n487 [30] Burak Varici, Emre Acarturk, Karthikeyan Shanmugam, Abhishek Kumar, and Ali Tajer. Score-based   \n488 causal representation learning with interventions, 2023.   \n489 [31] Julius von K\u00fcgelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard Sch\u00f6lkopf, Michel Besserve,   \n490 and Francesco Locatello. Self-supervised learning with data augmentations provably isolates content from   \n491 style. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances   \n492 in Neural Information Processing Systems, volume 34, pages 16451\u201316467. Curran Associates, Inc., 2021.   \n493 [32] Julius von K\u00fcgelgen, Michel Besserve, Liang Wendong, Luigi Gresele, Armin Keki\u00b4c, Elias Bareinboim,   \n494 David M. Blei, and Bernhard Sch\u00f6lkopf. Nonparametric identifiability of causal representations from   \n495 unknown interventions, 2023.   \n496 [33] Liang Wendong, Armin Keki\u00b4c, Julius von K\u00fcgelgen, Simon Buchholz, Michel Besserve, Luigi Gresele,   \n497 and Bernhard Sch\u00f6lkopf. Causal component analysis, 2023.   \n498 [34] Mengyue Yang, Furui Liu, Zhitang Chen, Xinwei Shen, Jianye Hao, and Jun Wang. Causalvae: Disentan  \n499 gled representation learning via neural structural causal models. In IEEE Conference on Computer Vision   \n500 and Pattern Recognition, CVPR, pages 9593\u20139602. Computer Vision Foundation / IEEE, 2021.   \n501 [35] Shuai Yang, Kui Yu, Fuyuan Cao, Lin Liu, Hao Wang, and Jiuyong Li. Learning causal representations for   \n502 robust domain adaptation. IEEE Transactions on Knowledge and Data Engineering, pages 1\u20131, 2021.   \n503 [36] Kui Yu, Xianjie Guo, Lin Liu, Jiuyong Li, Hao Wang, Zhaolong Ling, and Xindong Wu. Causality-based   \n504 feature selection: Methods and evaluations. ACM Comput. Surv., 53(5), 2020.   \n505 [37] Yue Yu, Jie Chen, Tian Gao, and Mo Yu. DAG-GNN: DAG structure learning with graph neural networks.   \n506 In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of   \n507 Machine Learning Research, pages 7154\u20137163. PMLR, 2019.   \n508 [38] Jiaqi Zhang, Chandler Squires, Kristjan Greenewald, Akash Srivastava, Karthikeyan Shanmugam, and   \n509 Caroline Uhler. Identifiability guarantees for causal disentanglement from soft interventions, 2023.   \n510 [39] Jiaqi Zhang, Chandler Squires, Kristjan H. Greenewald, Akash Srivastava, Karthikeyan Shanmugam,   \n511 and Caroline Uhler. Identifiability guarantees for causal disentanglement from soft interventions. CoRR,   \n512 abs/2307.06250, 2023.   \n513 [40] Xun Zheng, Bryon Aragam, Pradeep Ravikumar, and Eric P. Xing. Dags with NO TEARS: continuous   \n514 optimization for structure learning. In Advances in Neural Information Processing Systems 31: Annual   \n515 Conference on Neural Information Processing Systems NeurIPS, pages 9492\u20139503, 2018.   \n516 [41] Yujia Zheng, Ignavier Ng, and Kun Zhang. On the identifiability of nonlinear ICA: sparsity and beyond. In   \n517 NeurIPS, 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "518 Appendix ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "519 A1 Proof of Identifiability Theorem ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "520 In order to prove our model is identifiable we need a two additional definitions and some previously   \n521 stated assumptions. ", "page_idx": 11}, {"type": "text", "text": "522 Definition A1.1. Structural Causal Models ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "523 A structural causal model (SCM) is a tuple $\\mathcal{C}=(\\mathcal{F},\\mathcal{Z},\\mathcal{E},\\mathcal{G})$ with the following components:   \n524 1. The domain of causal variables $\\mathcal{Z}=\\mathcal{Z}_{1}\\times\\mathcal{Z}_{2}\\times...\\times\\mathcal{Z}_{n}$ .   \n525 2. The domain of exogenous variables $\\mathcal{E}=\\mathcal{E}_{1}\\times\\mathcal{E}_{2}\\times...\\times\\mathcal{E}_{n}$ .   \n526 3. A directed acyclic graph $\\mathcal{G}(\\mathcal{C})$ over the causal and exogenous variables.   \n527 4. A causal mechanism $f_{i}\\in\\mathcal{F}$ which maps an assignment of parent values for the parents $\\mathcal{Z}_{p a_{i}}$ plus   \n528 an exogenous variable value for $\\mathcal{E}_{i}$ to a value of causal variable $Z_{i}$ .   \n529 Definition A1.2. (Component-wise Transformation) Let $\\phi$ be a transformation (1-1 onto mapping)   \n530 between product spaces $\\phi:\\Pi_{i=1}^{n}\\mathcal{X}_{i}\\to\\Pi_{i=1}^{n}\\mathcal{Y}_{i}$ . If there exist local transformations $\\phi_{i}$ such that   \n531 $\\forall i,j,\\,\\forall x$ , $\\phi(x_{1},x_{2},...,x_{n})_{i}=\\phi_{i}(x_{j})$ , then $\\phi$ is a component-wise transformation.   \n532 Definition A1.3. (Diffeomorphism) A diffeomorphism between smooth manifolds $M$ and $N$ is a   \n533 bijective map $f:M\\to N$ , which is smooth and has a smooth inverse. Diffeomorphisms preserve   \n534 information as they are invertible transformations without discontinuous changes in their image.   \n535 Definition A1.4. (Pushforward measure) Given a measurable function $f:A\\rightarrow B$ between two   \n536 measurable spaces $A$ and $B$ , and a measure $p$ defined on $A$ , the pushforward measure $f_{*}p$ on $B$ is   \n537 defined for measurable sets $E$ in $B$ as: ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "538 $(f_{*}p)(E)=p(f^{-1}(E))$ ", "page_idx": 11}, {"type": "text", "text": "539 where $^*$ denotes the pushforward operation. In other words, the pushforward measure $f_{*}p$ assigns a   \n540 measure to a set in $B$ by measuring the pre-image of that set under $f$ in the space $A$ .   \n541 Lemma A1.5. The transformation $\\phi_{\\mathcal{Z}}:\\mathcal{Z}\\to\\mathcal{Z}^{\\prime}$ between the causal variable of two LCMs $\\mathcal{M}$   \n542 and $\\mathcal{M}^{\\prime}$ defined in Definition 3.4 is a component-wise transformation, $i f\\ \\forall i,j,i\\neq j\\quad\\tilde{\\mathcal{E}}_{i}^{\\prime}\\perp\\tilde{\\mathcal{E}}_{j}^{\\prime}$ and   \n543 the causal variables follow a multivariate normal distribution conditional on the pre-intervention   \n544 exogenous variables where ${\\tilde{E}}_{i}^{\\prime}$ denote the post-intervention exogenous variable of causal variable $i$   \n545 in $\\mathcal{M}^{\\prime}$ .   \n546 proof: We consider the case where the exogenous variables are mapped to causal variables by $a$   \n547 location-scale noise model such that $\\begin{array}{r}{\\tilde{z}_{i}=\\frac{\\tilde{e}_{i}-\\widetilde{l o c}(e_{/i})}{s c a l e(e_{/i})}}\\end{array}$ .   \n548 let\u2019s add these three constants $-E[\\widetilde{\\mathcal{E}_{i}^{\\prime}}]\\widetilde{l o c_{j}^{\\prime}}(e_{/j}^{\\prime}),\\;-E[\\widetilde{\\mathcal{E}_{j}^{\\prime}}]\\widetilde{l o c_{i}^{\\prime}}(e_{/i}^{\\prime}),\\;\\widetilde{l o c_{i}^{\\prime}}(e_{/i}^{\\prime})\\widetilde{l o c_{j}^{\\prime}}(e_{/j}^{\\prime})$ to the both   \n549 sides of the equality and then divide both sides by $\\bar{s c a l e}_{i}^{\\prime}(e_{/i}^{\\prime})\\bar{s c a l e}_{j}^{\\prime}(e_{/j}^{\\prime})$ : ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E\\left[\\frac{\\tilde{E}\\tilde{E}_{j}^{(k)}-\\tilde{E}_{j}^{(k)}\\tilde{E}_{k}^{(r)}(\\ell_{p,t}^{*})-\\tilde{E}_{j}^{(k)}\\tilde{E}_{k}^{(r)}(\\ell_{p,t}^{*})+\\tilde{E}_{k}^{(r)}(\\ell_{p,t}^{*})|\\tilde{E}_{p,t}^{(*)}(\\ell_{p,t}^{*})\\right]=}\\\\ &{\\frac{E[\\tilde{E}_{j}^{(k)}|E_{j}^{(k)}-E_{j}^{(k)}|\\tilde{E}_{p,t}^{(*)}(\\ell_{p,t}^{*})-E_{j}^{(k)}|\\tilde{E}_{p,t}^{(*)}(\\ell_{p,t}^{*})+\\tilde{E}_{k}^{(r)}(\\ell_{p,t}^{*})|\\tilde{E}_{p,t}^{(*)}(\\ell_{p,t}^{*})}{\\sin\\ell_{p}^{*}(r)_{p,t}^{*}\\sin\\ell_{p}^{*}(r)_{p,t}}}\\\\ &{\\to E\\left[\\frac{\\tilde{E}_{j}^{(k)}-\\tilde{E}_{k}^{(r)}(\\ell_{p,t}^{*})}{\\sin\\ell_{p}^{*}(r)_{p,t}^{*}(r)_{p,t}}\\right]\\frac{\\tilde{E}_{p-1}^{(k)}-\\tilde{E}_{p-1}^{(k)}(\\ell_{p,t}^{*})}{\\sin\\ell_{p}^{*}(r)_{p,t}^{*}(r)_{p,t}}\\Big)=(\\frac{E[\\tilde{E}_{j}^{(k)}-\\tilde{E}_{k}^{(r)}(\\ell_{p,t}^{*}))}{\\sin\\ell_{p}^{*}(r)_{p,t}})\\frac{E[\\tilde{E}_{p-1}^{(k)}-\\tilde{E}_{p-1}^{(k)}(\\ell_{p,t}^{*})]}{\\sin\\ell_{p}^{*}(r)_{p,t}^{*}(r)_{p,t}}}\\\\ &{\\to E[\\tilde{E}_{j}^{(k)}\\tilde{E}_{j}^{(k)}-E[\\tilde{E}_{j}^{(k)}|E_{p}^{(k)}|\\ell_{p}^{*}]}\\\\ &{\\to E[\\\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "550 Typically, the aforementioned equalities would be valid for any diffeomorphic solution function   \n551 $\\tilde{s}_{i}:\\tilde{\\mathcal{E}}_{i}\\rightarrow\\tilde{\\mathcal{Z}}_{i}$ . However, in this paper, we specifically focus on solution functions represented by $a$   \n552 location-scale noise model.   \n553 Assuming that the causal variables follow a multivariate normal distribution conditional on the   \n554 pre-intervention exogenous variables, $c o\\nu(\\tilde{Z}_{i}^{\\prime},\\tilde{\\mathcal{Z}}_{j}^{\\prime}|\\mathcal{E}^{\\prime})\\,=\\,0$ would imply that $\\tilde{\\mathcal{Z}}_{i}^{\\prime}\\perp\\!\\!\\!\\perp\\tilde{\\mathcal{Z}}_{j}^{\\prime}|\\mathcal{E}^{\\prime}$ . Let\u2019s   \n555 define $\\phi\\varepsilon\\,=\\,g^{\\prime-1}\\circ g\\,:\\,\\mathcal{E}\\,\\rightarrow\\,\\mathcal{E}^{\\prime}$ where $g$ and $g^{\\prime}$ are the decoders in $\\mathcal{M}$ and $\\mathcal{M}^{\\prime}$ . As stated in   \n556 Assumption 3.1, the decoders are diffeomorphism, hence, $\\phi_{\\mathcal{E}}$ is a diffeomorphism. Furthermore, let\u2019s   \n557 denote $\\tilde{s}$ as the set of all solution functions in post-intervention which are also diffeomorphism as   \n558 stated in Assumption 3.1. Consequently: ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(\\phi_{\\mathcal{E}}^{-1}\\operatorname{\\it~is~diffeomorphic})\\,\\forall i,j,i\\ne j\\quad\\tilde{\\mathcal{Z}}_{i}^{\\prime}\\,\\underline{{\\tilde{z}^{\\prime}}}|\\mathcal{L}^{\\prime}\\!|\\mathcal{E}^{\\prime}\\to\\tilde{\\mathcal{Z}}_{i}^{\\prime}\\,\\underline{{\\mathrm{1i}}}\\,\\tilde{\\mathcal{Z}}_{j}^{\\prime}|\\phi_{\\mathcal{E}}^{-1}(\\mathcal{E}^{\\prime})\\to\\tilde{\\mathcal{Z}}_{i}^{\\prime}\\,\\underline{{\\mathrm{1i}}}\\,\\tilde{\\mathcal{Z}}_{j}^{\\prime}|\\mathcal{E}}\\\\ &{\\to p(\\tilde{\\mathcal{Z}}_{i}^{\\prime}|\\mathcal{E})p(\\tilde{\\mathcal{Z}}_{j}^{\\prime}|\\mathcal{E})=p(\\tilde{\\mathcal{Z}}_{i}^{\\prime},\\tilde{\\mathcal{Z}}_{j}^{\\prime}|\\mathcal{E})}\\\\ &{(a l l\\,f\\!\\,f\\!u n c t i o n s\\operatorname{\\it~in}\\,\\tilde{\\,s}\\mathrm{~are~}d i f\\!f\\!e o m o r p h i s m)\\to p(\\tilde{\\mathcal{Z}}_{i}^{\\prime}|\\tilde{s}(\\mathcal{E}))p(\\tilde{\\mathcal{Z}}_{j}^{\\prime}|\\tilde{s}(E))=p(\\tilde{\\mathcal{Z}}_{i}^{\\prime},\\tilde{\\mathcal{Z}}_{j}^{\\prime}|\\tilde{s}(\\mathcal{E}))}\\\\ &{\\to p(\\tilde{\\mathcal{Z}}_{i}^{\\prime}|\\tilde{\\mathcal{Z}})p(\\tilde{\\mathcal{Z}}_{j}^{\\prime}|\\tilde{\\mathcal{Z}})=p(\\tilde{\\mathcal{Z}}_{i}^{\\prime},\\tilde{\\mathcal{Z}}_{j}^{\\prime}|\\tilde{\\mathcal{Z}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "559 The association between ${\\tilde{\\mathcal{Z}}}^{\\prime}$ and $\\tilde{\\mathcal{Z}}$ arises from their shared observation space. We know that every   \n560 causal variable in $\\mathcal{M}^{\\prime}$ depends at least on one of the causal variables in $\\mathcal{M}$ . If one of the causal   \n561 variables in $\\mathcal{M}^{\\prime}$ depended on more than one causal variable in $\\mathcal{M}$ , it would create dependency   \n562 between two variables in $\\mathcal{M}^{\\prime}$ and violate the above equality. Therefore, no variable in $\\mathcal{M}^{\\prime}$ depends   \n563 on more than one causal variable in $\\mathcal{M}$ . Consequently, the transformation $\\phi_{\\mathcal{Z}}$ is a component-wise   \n564 transformation.   \n565 Theorem A1.6. (Identifiability of latent causal models.) Let $\\mathcal{M}\\,=\\,({\\mathcal{A}},{\\mathcal{X}},g,\\mathbb{Z})$ and ${\\mathcal{M}}^{\\prime}\\,=$   \n566 $(\\mathcal{A}^{\\prime},\\mathcal{X},g^{\\prime},\\mathcal{T})$ be two LCMs with shared observation space $\\mathcal{X}$ and shared intervention targets $\\mathcal{T}$ .   \n567 Suppose the following conditions are satisfied:   \n568 1. Identical correspondence assumptions explained in 3.1.   \n569 2. Soft interventions satisfy Assumption 3.3.   \n570 3. The causal and exogenous variables are real-valued.   \n571 4. The causal and exogenous variables follow a multivariate normal distribution.   \n572 Then the following statements are equivalent:   \n573 -Two LCMs M and $\\mathcal{M}^{\\prime}$ assign the same likelihood to interventional and observational data i.e., ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "574 $p_{\\mathcal{M}}^{\\chi}(x,\\tilde{x})=p_{\\mathcal{M}^{\\prime}}^{\\chi^{\\prime}}(x,\\tilde{x})$ .   \n575 - $\\mathcal{M}$ and $\\mathcal{M}^{\\prime}$ are disentangled, that is $\\mathcal{M}\\sim_{r}\\mathcal{M}^{\\prime}$ according to Definition 3.4.   \n576 Proof We will proceed to prove the equivalence between statements 1 and 2 by showing the implica  \n577 tion is true in each direction. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "578 A1.1 $\\mathcal{M}\\sim_{r}\\mathcal{M}^{\\prime}\\Rightarrow p_{\\mathcal{M}}^{\\chi}(x,\\tilde{x})=p_{\\mathcal{M}^{\\prime}}^{\\chi}(x,\\tilde{x})$ ", "page_idx": 13}, {"type": "text", "text": "579 This direction is fairly straightforward. According to Definition 3.4, the fact that $M\\sim_{r}M^{\\prime}$ implies   \n580 that $\\phi_{\\mathcal{E}}$ is measure preserving. Therefore, $p_{\\mathcal{M}^{\\prime}}^{\\mathcal{E}}(\\bar{e^{\\prime}},\\tilde{e^{\\prime}})=(\\phi_{\\mathcal{E}})_{*}p_{\\mathcal{M}}^{\\mathcal{E}}(e,\\tilde{e})$ . Furthermore, considering   \n581 that ancestry is preserved, $\\phi_{\\mathcal{Z}}$ is measure preserving, and that causal variables are obtained from their   \n582 ancestral exogenous variables in implicit models, we have $p_{\\mathcal{M}^{\\prime}}^{\\mathcal{Z}}(z^{\\prime},\\tilde{z}^{\\prime})\\,=\\,(\\phi_{\\mathcal{Z}})_{*}p_{\\mathcal{M}}^{\\mathcal{Z}}(z,\\tilde{z})$ . Since   \n583 models are trained to maximize the log likelihood of $p(x,\\tilde{x},\\tilde{x}-x)$ and the latent spaces in $M$   \n584 and $M^{\\prime}$ have the same distribution, the decoders should yield the same observational distributions   \n585 $p_{\\mathcal{M}}^{\\mathcal{X}}(x,\\tilde{x})=p_{\\mathcal{M}^{\\prime}}^{\\mathcal{X}}(x,\\tilde{x})$ . ", "page_idx": 13}, {"type": "text", "text": "586 A1.2 $p_{\\mathcal{M}}^{\\mathcal{X}}(x,\\tilde{x})=p_{\\mathcal{M}^{\\prime}}^{\\mathcal{X}}(x,\\tilde{x})\\Rightarrow\\mathcal{M}\\sim_{r}\\mathcal{M}^{\\prime}$ ", "page_idx": 13}, {"type": "text", "text": "587 Let\u2019s define $\\phi\\varepsilon=g^{\\prime-1}\\circ g:\\mathcal{E}\\rightarrow\\mathcal{E}^{\\prime}$ . Since we can express $e=s^{-1}(z)$ , we can now define $\\phi_{\\mathcal{Z}}$ as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\phi\\ z=s^{\\prime}\\circ g^{\\prime-1}\\circ g\\circ s^{-1}:\\mathcal{Z}\\to\\mathcal{Z}^{\\prime}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "588 Therefore, $\\phi\\varepsilon=s^{\\prime-1}\\circ\\phi_{\\mathcal{Z}^{\\circ}s}$ . Because $g$ and $g^{\\prime}$ are diffeomorphisms, $\\phi_{\\mathcal{E}}$ is a diffeomorphism as well.   \n589 Furthermore, since $p_{\\mathcal{M}}^{\\mathcal{X}}=p_{\\mathcal{M}^{\\prime}}^{\\mathcal{X}}$ and $\\phi_{\\mathcal{E}}$ is a diffeomorphism, then $p_{\\mathcal{M}^{\\prime}}^{\\mathcal{E}}=(\\phi_{\\mathcal{E}})_{*}p_{\\mathcal{M}}^{\\mathcal{E}}$ . Consequently,   \n590 $\\phi_{\\mathcal{E}}$ is measure-preserving. Similarly, $\\phi_{\\mathcal{E}}$ is measure-preserving as well since causal mechanisms are   \n591 diffeomorphisms.   \n559923 $\\mathcal{T}$ t :e ps 1u:p Ip $p_{\\mathcal{M}}^{\\mathcal{E},\\mathbb{Z}}(e,\\tilde{e}|I)\\cap s u p p\\,p_{\\mathcal{M}}^{\\mathcal{E},\\mathbb{Z}}(e,\\tilde{e}|J)\\}$ .a nTdh enno, daesss uLmeti\u2019ns gd eaftionem tihc ei sntete $U$ eanst $U=\\{{\\mathcal{E}}\\times{\\mathcal{E}}|\\forall I,J\\in$   \n594 tual exogenous variables, $p_{\\mathcal{M}}^{\\mathcal{E},\\dot{\\mathcal{Z}}}\\dot{(U|I)}=p_{\\mathcal{M}}^{\\mathcal{E},\\mathcal{T}}(U|J)=0$ . Therefore, we can say that $p_{\\mathcal{M}}^{\\varepsilon}(e,\\tilde{e})=$   \n595 $\\begin{array}{r}{\\sum_{I\\in\\mathcal{Z}}p_{\\mathcal{M}}^{\\mathcal{E},\\mathcal{T}}(e,\\tilde{e}|I)p_{\\mathcal{M}}^{\\mathcal{T}}(I)}\\end{array}$ is a discrete mixture of non-overlapping distributions $p_{\\mathcal{M}}^{\\mathcal{E},\\mathcal{T}}(e,\\tilde{e}|I)$ . Sim  \n596 ilarly, we can say that $p_{\\mathcal{M}^{\\prime}}^{\\varepsilon}(e,\\tilde{e})$ is a discrete mixture of non-overlapping distributions. It can be   \n597 concluded that as $\\phi_{\\mathcal{E}}$ must map between these distributions, there exists a bijection that also induces   \n598 a permutation $\\psi:[n]\\to[n]$ . Note: If we had non-atomic interventions or non-counterfactual exoge  \n599 nous variables, then these distributions would have some overlapping. With overlapping distributions,   \n600 we can no longer claim there is a bijection mapping between these distributions.   \n601 In space $\\mathcal{Z}$ , the interventions should also be sufficiently variable in order to have non-overlapping   \n602 $p_{\\mathcal{M}}^{\\mathcal{Z},\\bar{Z}}(z,\\tilde{z}|I)$ distributions. In the case of soft interventions, $\\tilde{z}$ is affected by all ancestral exogenous   \n603 variables which could be ancestors of other causal variables as well. Consequently, if the changes in   \n604 causal mechanisms are not sufficient, the effect of ancestral exogenous variables on causal variables   \n605 will share some similarities and create overlapping distributions. Similar to $p_{\\mathcal{M}}^{\\mathcal{E}}(e,\\tilde{e}|I)$ , we can say   \n606 that there is a permutation between $p_{\\mathcal{M}}^{\\mathcal{Z}}(z,\\tilde{z}|I)$ as well. Furthermore, as we assume the target of   \n607 interventions are known we have: ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "equation", "text": "$$\n\\forall I\\in\\mathcal{Z}:\\,p_{\\mathcal{M}}^{\\mathcal{Z}}(z,\\tilde{z}|I)=p_{\\mathcal{M}^{\\prime}}^{\\mathcal{Z}}(z,\\tilde{z}|I)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "608 Consequently, the permutation $\\psi$ is an identity transformation. The effect of soft intervention with   \n609 known targets on these conditional distributions is shown in Figure A1. ", "page_idx": 13}, {"type": "text", "text": "610 Step 2: Component-wise $\\phi_{\\mathcal{Z}}$ ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "611 According to Lemma A1.5, in order to prove that $\\phi_{\\mathcal{Z}}$ is a component-wise transformation, we need   \n612 to prove that $\\tilde{\\mathcal{E}}_{i}^{\\prime}$ and $\\tilde{\\mathcal{E}}_{j}^{\\prime}$ are independent $\\forall i,j,i\\neq j$ . In implicit modeling we do not know the parents   \n613 of each causal variable, hence, we assume the distribution of $\\tilde{\\mathcal{Z}}_{i}^{\\prime}$ to be conditioned only on $\\mathcal{E}_{i}^{\\prime}$ as in   \n614 Equation 5 since $\\mathcal{E}_{i}^{\\prime}$ is a known parent of $\\tilde{\\mathcal{Z}}_{i}^{\\prime}$ . The mean of a conditional distribution can be calculated   \n615 as: ", "page_idx": 13}, {"type": "equation", "text": "$$\nE[\\tilde{z}_{i}^{\\prime}|e_{i}^{\\prime}]=\\mu_{\\tilde{z}_{i}^{\\prime}}+\\rho\\frac{\\sigma_{\\tilde{z}_{i}^{\\prime}}}{\\sigma_{e_{i}^{\\prime}}}(e_{i}^{\\prime}-\\mu_{e_{i}^{\\prime}})\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "616 where $\\rho$ and $\\sigma$ are the correlation coefficient and variance of the random variables, respectively. On   \n617 the other hand, we model $\\tilde{\\mathcal{Z}}_{i}^{\\prime}$ using switch mechanisms as: ", "page_idx": 13}, {"type": "image", "img_path": "", "img_caption": ["Pre-Intervention "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "PKEmH9ZJfw/tmp/21ea8ba590601346ed55cbe7110566c1c9580fca7f283834ac5ff7aae3558243.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "", "img_caption": ["Figure A1: The distribution of observed and causal variables in two causal models $\\mathcal{M}$ and $\\mathcal{M}^{\\prime}$ , which belong to the equivalence class up to reparameterization. (a) There are 10 observed samples in which $Z_{1}$ or $Z_{2}$ has been intervened on. (b) The distribution of causal variables when $I=0$ (no intervention) is identical to each other but the range of value of causal variables are different and can be mapped to each other using $\\phi_{\\mathcal{Z}}$ . (c) The intervention on $Z_{1}$ $[I=1]$ ). (d) The intervention on $Z_{2}$ $(I=2)$ ). For $I=1$ and $I=2$ the distributions are again identical to each other but are different for different targets of intervention as soft interventions change the conditional distribution (condition on parents) of causal variables. Also, for each value of $I$ , the distributions of $\\mathcal{M}$ and $\\mathcal{M}^{\\prime}$ should move in one direction as targets are known. "], "img_footnote": [], "page_idx": 14}, {"type": "equation", "text": "$$\n\\tilde{z}_{i}^{\\prime}=s_{i}(\\tilde{e}_{i}^{\\prime};e_{/i}^{\\prime},h(v^{\\prime}))\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "618 By using Taylor\u2019s expansion we can write above equation as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{s_{i}(\\tilde{e}_{i}^{\\prime};e_{/i}^{\\prime},h_{i}(v^{\\prime}))=s_{i}(\\tilde{e}_{i}^{\\prime};e_{/i}^{\\prime},h_{i}(v_{0}^{\\prime}))++\\displaystyle\\sum_{n=1}^{\\infty}\\frac{1}{n!}\\left(\\frac{\\partial^{n}s_{i}}{\\partial h_{i}^{n}}\\Big|_{h_{i}=h_{i}(v_{0}^{\\prime})}(h_{i}(v^{\\prime})-h_{i}(v_{0}^{\\prime}))^{n}\\right)}\\\\ &{\\ =s_{i}(\\tilde{e}_{i}^{\\prime};e_{/i}^{\\prime},h_{i}(v_{0}^{\\prime}))+R_{i}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "619 Furthermore, we assume separable dependence such that: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\exists v_{0}^{\\prime}{\\mathrm{~such~that~}}\\forall i\\quad s_{i}(\\tilde{e}_{i}^{\\prime};e_{/i}^{\\prime},h_{i}(v_{0}^{\\prime}))=s_{i}(\\tilde{e}_{i}^{\\prime};e_{/i}^{\\prime})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "620 An example of such a scenario could be in location-scale noise models, where a soft intervention   \n621 changes the location parameter of the model as: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{s_{i}(e_{i}^{\\prime};e_{/i}^{\\prime})=e_{i}^{\\prime}+l o c(e_{/i}^{\\prime})\\rightarrow\\tilde{s}_{i}(\\tilde{e}_{i}^{\\prime};e_{/i}^{\\prime})=s_{i}(\\tilde{e}_{i}^{\\prime};e_{/i}^{\\prime},h_{i}(v^{\\prime}))}}\\\\ {{\\mathrm{\\normalfont~=}\\tilde{e}_{i}^{\\prime}+l o c(e_{/i}^{\\prime})+h_{i}(v^{\\prime})=\\tilde{e}_{i}^{\\prime}+l o c(e_{/i}^{\\prime})+v^{\\prime2}+v^{\\prime}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "622 In this example, for $v_{0}^{\\prime}=0$ , $s_{i}(\\tilde{e}_{i}^{\\prime};e_{\\slash i}^{\\prime},h_{i}(v_{0}^{\\prime}))=s_{i}(\\tilde{e}_{i}^{\\prime};e_{\\slash i}^{\\prime}).$ . ", "page_idx": 15}, {"type": "text", "text": "623 Consequently, we can write the following equality from Equation 10: ", "page_idx": 15}, {"type": "equation", "text": "$$\nE[\\tilde{\\mathcal{Z}}_{i}^{\\prime}|e_{i}^{\\prime}]=E[s_{i}(\\tilde{\\mathcal{E}}_{i}^{\\prime};\\mathcal{E}_{/i}^{\\prime})+R_{i}|e_{i}^{\\prime}]=\\mu_{\\tilde{\\mathcal{Z}}_{i}^{\\prime}}+\\rho\\frac{\\sigma_{\\tilde{\\mathcal{Z}}_{i}^{\\prime}}}{\\sigma_{\\mathcal{E}_{i}^{\\prime}}}(e_{i}^{\\prime}-\\mu\\varepsilon_{i}^{\\prime})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "624 By taking the partial derivative of both side with respect to $\\tilde{\\mathcal{E}}_{j}^{\\prime}$ we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\forall j\\neq i\\quad E[\\frac{\\partial s_{i}(\\tilde{\\mathcal{E}}_{i}^{\\prime};\\mathcal{E}_{/i}^{\\prime})}{\\partial\\tilde{\\mathcal{E}}_{i}^{\\prime}}\\cdot\\frac{\\partial\\tilde{\\mathcal{E}}_{i}^{\\prime}}{\\partial\\tilde{\\mathcal{E}}_{j}^{\\prime}}+\\frac{\\partial s_{i}(\\tilde{\\mathcal{E}}_{i}^{\\prime};\\mathcal{E}_{/i}^{\\prime})}{\\partial\\mathcal{E}_{/i}^{\\prime}}\\cdot\\frac{\\partial\\mathcal{E}_{/i}^{\\prime}}{\\partial\\tilde{\\mathcal{E}}_{j}^{\\prime}}+\\frac{\\partial R_{i}}{\\partial\\tilde{\\mathcal{E}}_{j}^{\\prime}}|e_{i}^{\\prime}]=0\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "625 If we did not have the causal mechanism switch variable $\\left(h_{i}(\\mathcal{V}^{\\prime})\\right)$ , the equation above would only   \n626 hold if $s_{i}$ was constant in parents, which is not the case due to the presence of soft interventions, or if   \n627 $\\begin{array}{r}{\\frac{\\partial s_{i}(\\tilde{\\mathcal{E}}_{i}^{\\prime};\\mathcal{E}_{/i}^{\\prime})}{\\partial\\tilde{\\mathcal{E}}_{i}^{\\prime}}\\cdot\\frac{\\partial\\tilde{\\mathcal{E}}_{i}^{\\prime}}{\\partial\\tilde{\\mathcal{E}}_{j}^{\\prime}}=-\\frac{\\partial s_{i}(\\tilde{\\mathcal{E}}_{i}^{\\prime};\\mathcal{E}_{/i}^{\\prime})}{\\partial\\mathcal{E}_{/i}^{\\prime}}\\cdot\\frac{\\partial\\mathcal{E}_{/i}^{\\prime}}{\\partial\\tilde{\\mathcal{E}}_{j}^{\\prime}},}\\end{array}$ . The latter scenario would imply that $\\frac{\\partial\\tilde{\\mathcal{E}}_{i}^{\\prime}}{\\partial\\tilde{\\mathcal{E}}_{j}^{\\prime}}\\neq0$ , hence, $\\tilde{\\mathcal{E}}_{i}^{\\prime}\\neq\\tilde{\\mathcal{E}}_{j}^{\\prime}$ .   \n628 However, by introducing the causal mechanism switch variable $\\nu$ and assuming it is observed, we   \n629 can account for the effects of soft interventions through hi(V\u2032). In this case, \u2202\u2202EE\u02dc\u02dcji\u2032\u2032 $\\begin{array}{r}{\\frac{\\partial\\tilde{\\mathcal{E}^{\\prime}}_{i}}{\\partial\\tilde{\\mathcal{E}^{\\prime}}_{j}}=0}\\end{array}$ as exogenous   \n630 variables are commonly assumed to be independent in practice. Consequently: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall i,j\\quad\\tilde{\\mathcal{E}}_{i}^{\\prime}\\perp\\!\\!\\!\\perp\\tilde{\\mathcal{E}}_{j}^{\\prime}}\\\\ &{\\rightarrow\\forall i,j\\quad p(\\tilde{\\mathcal{Z}}_{i}^{\\prime},\\tilde{\\mathcal{Z}}_{j}^{\\prime}|\\tilde{\\mathcal{Z}}_{i},\\tilde{\\mathcal{Z}}_{j})=p(\\tilde{\\mathcal{Z}}_{i}^{\\prime}|\\tilde{\\mathcal{Z}}_{i})p(\\tilde{\\mathcal{Z}}_{j}^{\\prime}|\\tilde{\\mathcal{Z}}_{j})}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "image", "img_path": "PKEmH9ZJfw/tmp/10de3d62270d9ecdfafffffddf21b1926010df0b246d1d7878e7f8a8442938ff.jpg", "img_caption": ["Figure A2: (a) String diagram of the causal variables $\\mathcal{Z}$ and ${\\mathcal{Z}}^{\\prime}$ . The triangle indicates sampling $I$ from its distribution. The left-hand side diagram is when $\\phi_{\\mathcal{Z}}$ is applied last and the right-hand side diagram is when $\\phi_{\\mathcal{Z}}$ is applied first. $I$ is the intervention which affects intervened causal variable\u2019s mechanism variable. $V$ is used to model the effect of intervention on mechanisms and parents. (b) String diagrams after discarding $\\tilde{Z}_{o}^{\\prime}$ and the disentangled effect of soft intervention on ${\\tilde{Z}}_{i}$ modeled by $V$ . "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "631 Step 3: Component-wise $\\phi_{\\mathcal{E}}$ ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "632 Using the result from previous step that $\\phi_{\\mathcal{Z}}$ is a component-wise transformation, the string diagrams   \n633 for connections between $\\mathcal{E}$ and ${\\mathcal{E}}^{\\prime}$ will be as shown in Figure A3. $\\phi_{\\mathcal{E}_{i}}$ will only depend on $\\mathcal{E}_{A}$ ,   \n634 where $A=a n c_{i}$ is the ancestors of variable $i$ , and $e_{i}$ . Because $s(e)_{a n c_{i}}$ , $s(e)_{i}$ , and $s^{\\prime-1}(z^{\\prime})_{i}$ only   \n635 depend on ancestors and $\\phi_{\\mathcal{Z}}$ is a component-wise transformation. The first equality in Figure A3   \n636 follows from the definition of $\\phi_{\\mathcal{E}_{i}}$ . The second equality holds when we first apply $\\phi_{\\mathcal{Z}_{A}}$ and then apply   \n637 the causal mechanisms. It can be concluded from the most right-hand side diagram in Figure A3   \n638 that the transformation from $\\mathcal{E}_{i}^{\\prime}\\times\\mathcal{E}_{A}\\rightarrow\\mathcal{E}_{i}^{\\prime}$ is constant in $\\mathcal{E}_{A}$ . Therefore, $\\phi_{\\mathcal{E}_{i}}$ is a component-wise   \n639 transformation. ", "page_idx": 15}, {"type": "image", "img_path": "PKEmH9ZJfw/tmp/9bb87d6690dd96a482aefdd6de238f74c4d4c6b56931249589317e224cffb55e.jpg", "img_caption": ["Figure A3: String diagrams for connections between $\\mathcal{E}$ and ${\\mathcal{E}}^{\\prime}$ . The triangle indicates sampling variables from their corresponding distributions. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "PKEmH9ZJfw/tmp/6f26c1b4e3e4648835164b272510d182e6e1b302561fdfe6280475a2e670b421.jpg", "img_caption": ["Figure A4: In the Causal-Triplet dataset [19], visual representations capture both pre and postintervention scenarios. The first two rows showcase data samples from Epic-Kitchens, while the third and fourth rows feature samples from ProcTHOR. Each image in the post-intervention condition is accompanied by labels specifying the corresponding action and intervened object. In the images in the first two rows, the agent is performing an action on an object but the camera angle has also changed. So we can say that for example the distribution of causal variables conditioned on the camera angle has been changed due to soft intervention. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "640 A2 Soft vs. Hard intervention ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "641 In a causal model, an intervention refers to a deliberate action taken to manipulate or change one or   \n642 more variables in order to observe its impact on other variables within the causal model. Interventions   \n643 help to study how changes in one variable directly cause changes in another, thereby revealing causal   \n644 relationships. ", "page_idx": 16}, {"type": "image", "img_path": "PKEmH9ZJfw/tmp/e9b2c64ab45f13dd1f475bb683454b7f99f5ac32794160a39cdf9a329e15b697.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure A5: Causal graph models in the presence of Hard (a) and Soft (b) interventions. There are no connections from parents to ${\\tilde{Z}}_{i}$ in hard interventions (a). Whereas, parents are connected to ${\\tilde{Z}}_{i}$ in soft interventions (b).Let\u2019s consider an implicit model and use $/i$ to denote all variables except variable i. The major difference of soft intervention (b) with hard intervention (a) is that $\\tilde{\\mathcal{Z}}_{i}$ is no longer disconnected from its parents and its causal mechanism $\\tilde{s}_{i}$ is affected by the intervention. Thus, with a hard intervention, we know the post-intervention parents of a node $\\tilde{\\mathcal{Z}}_{i}$ (there are none), whereas with soft interventions, the parents themselves may not change. ", "page_idx": 17}, {"type": "text", "text": "645 Based on the levels of control and manipulation in a causal intervention, we can have soft vs. hard   \n646 interventions. A hard intervention involves directly manipulating the variables of interest in a   \n647 controlled manner such as Randomized Controlled Trials (RCTs). In other words, a hard intervention   \n648 sets the value of a causal variable $Z$ to a certain value denoted as $d o(Z=z)$ [24].   \n649 On the other hand, soft intervention involves more subtle or less controlled manipulation of variables   \n650 and changes the conditional distribution of the causal variable $p(Z|Z_{p a})\\to\\tilde{p}(Z|Z_{p a})$ which can be   \n651 modeled as $\\tilde{z}_{i}=\\tilde{f}_{i}(z_{p a_{i}},\\tilde{e}_{i})$ [7].   \n652 Looking at interventions from a graphical standpoint, a hard intervention entails that the intervened   \n653 node is solely impacted by the intervention itself, with no influence coming from its ancestral nodes.   \n654 Conversely, in the context of a soft intervention, the representation of the intervened node can be   \n655 influenced not only by the intervention but also by its parent nodes.   \n656 As an example, suppose we are trying to understand the causal relationship between different types   \n657 of diets and weight loss. The soft intervention in this scenario could be a switch from a regular diet to   \n658 a low-carb diet. Switching to a low-carb diet is a voluntary choice made by the individual and there   \n659 are no external forces or regulations compelling them to make this change (non-coercive).   \n660 The intervention involves a modification of the individual\u2019s diet rather than a complete disruption   \n661 since they are adjusting the proportion of macronutrients (fats, proteins, and carbs) they consume,   \n662 which is less disruptive than a radical change in eating habits (gradual modification). The individual   \n663 has autonomy to choose and tailor their diet according to their preferences and health goals so they   \n664 are empowered to make informed decisions about their dietary choices (behavioural empowerment).   \n665 Conversely, if the government or an authority were to intervene and enforce a mandatory low-carb   \n666 diet through legal means, this would constitute a hard intervention. In this scenario, regulations would   \n667 be implemented, prohibiting the consumption of specific carbohydrate-containing foods. Regulatory   \n668 agencies would be established to oversee and ensure adherence to the low-carb diet mandate, taking   \n669 actions such as removing prohibited foods from the market, restricting their import and production,   \n670 and so on. Individuals caught consuming banned foods would be subject to fines, legal repercussions,   \n671 or other penalties. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "672 A3 Experiments ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "673 This section contains additional details about ICRL-SM design architectures, datasets, and experi  \n674 ments settings. ", "page_idx": 17}, {"type": "text", "text": "675 A3.1 Datasets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "676 A3.1.1 Synthetic ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "677 We generate simple synthetic datasets with $\\mathcal{X}=\\mathcal{Z}=\\mathbb{R}^{n}$ . For each value of $n$ , we generate ten   \n678 random DAGs, a random location-scale SCM, then a random dataset from the parameterized SCM.   \n679 To generate random DAGs, each edge is sampled in a fixed topological order from a Bernoulli   \n680 distribution with probability 0.5. The pre-intervention and post-intervention causal variables are   \n681 obtained as: ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "equation", "text": "$$\nz_{i}=s c a l e(z_{p a_{i}})e_{i}+l o c(z_{p a_{i}})\\xrightarrow{\\mathrm{Soft.Intervention}}z_{i}=s c a l e(z_{p a_{i}})\\tilde{e_{i}}+\\widetilde{l o c}(z_{p a_{i}}),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "682 where the loc and scale networks are changed in post intervention. The pre-intervention loc and   \n683 post-interventionlo c network weights are initialized with samples drawn from ${\\mathcal{N}}(0,1)$ and ${\\mathcal{N}}(3,1)$ ,   \n684 respectively. For ablation studies, we change the mean of these Normal distributions. The scale is   \n685 constant 1 for both pre-intervention and post-intervention samples. Both $e_{i}$ and $\\tilde{e_{i}}$ are sampled from   \n686 a standard Gaussian. The causal variables are mapped to the data space through a randomly sampled   \n687 $S O(n)$ rotation. For each dataset, we generate 100,000 training samples, 10,000 validation samples,   \n688 and 10,000 test samples. ", "page_idx": 18}, {"type": "text", "text": "689 A3.1.2 Causal-Triplet ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "690 The Causal-Triplet datasets are consisted of images containing objects in which an action is manipu  \n691 lating the objects shown in Figure A4. Examples of actions and objects in these datasets are given in   \n692 Table A1 and A2.   \n693 Based on the actions and objects, we treat our causal variables as attributes of objects which can be   \n694 changed by actions. Therefore, actions in these datasets are considered as interventions. Assume that   \n695 $z_{1}$ corresponds to the attributes of an object, e.g. a door, the target of opening or closing (action\u2019s   \n696 target) is $z_{1}$ .   \n697 We use actions\u2019 labels in these datasets to detect the targets of interventions to determine which causal   \n698 variable has been intervened upon. Note that informing the model about the target of intervention is   \n699 not same as informing about the action itself (See Table 3). We use 5000 images of these datasets to   \n700 train all models. ", "page_idx": 18}, {"type": "table", "img_path": "PKEmH9ZJfw/tmp/33779d2fe90aa894b439ed92728bc5d2787ed6286e19b1e674a7423c58cd85cb.jpg", "table_caption": ["Table A1: Actions and objects present in the Causal-Triplet images (ProcTHOR Dataset). ", "ProcTHOR Dataset "], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "PKEmH9ZJfw/tmp/57af3a60d250eb348cce78382cb8b5dcd35bca74e034c140805f13f8f2a6db3d.jpg", "table_caption": ["Table A2: Actions and objects present in the Causal-Triplet images (Epic-Kitchens Dataset). ", "Epic-Kitchens Dataset "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "701 A3.2 Architecture Design ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "702 Based on the ICRL-SM architecture depicted in Figure 2a, we devised a location-scale solution   \n703 function (Equation 6) in which the $l o c_{i}$ and $s c a l e_{i}$ , and $h_{i}$ networks each comprise of fully connected   \n704 networks. These networks consist of two layers each, with 64 hidden units per layer and ReLU   \n705 activation functions. The encoder and decoder parameters for latents $\\mathcal{E}$ and $\\tilde{\\mathcal{E}}$ are shared and we use a   \n706 separate encoder and decoder with the same architecture for the latent $\\mathcal{V}$ . For our synthetic dataset   \n707 experiments, the encoder and decoder are consisted of fully connected networks with 2 hidden layers   \n708 and 64 units in each hidden layer. For the Causal-Triplet datasets, we utilized ResNet-based networks.   \n709 The same encoder and decoder architectures are used for all baseline models in the experiments.   \n710 ResNet50 encoder, ResNet50 decoder, and classifiers with 1 hidden layer and 64 hidden units are   \n711 used for predicting actions and objects for experiments in Table 4 and Table 3. ResNet18 encoder,   \n712 ResNet18 decoder, and classifiers with 2 hidden layer and 2 hidden units are used for predicting   \n713 actions and objects for experiments in Table A4 and Table A3. ", "page_idx": 18}, {"type": "text", "text": "714 A3.3 Training ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "715 To enforce the condition described in Equation 5 for $i\\not\\in\\mathcal{T}$ , we assign the post-intervention exogenous   \n716 variables the same value as the pre-intervention exogenous variables. In mathematical terms, this   \n717 translates to $\\forall i\\notin\\mathcal{T}$ , we set $\\tilde{e}_{i}=e_{i}$ .   \n718 In our experiments, we do not pretrain the networks, however, for the baseline models we follow the   \n719 training procedure in [3]. We also use consistency in our experiments to ensure that the encoder and   \n720 decoder are inverse of each other. Consistency regularizer is used as $\\begin{array}{r}{\\sum_{i}E_{\\hat{x}\\sim p(\\hat{x}|e),x\\sim p(x)}[(x-\\hat{x})^{2}]}\\end{array}$   \n721 where $\\hat{x}$ are the reconstructed samples.   \n722 For optimization, Adam optimizer is used with default hyperparamters. In the synthetic experiments,   \n723 learning rate changes from $3e-4$ to $1e-8$ with a cosine scheduler. In the Causal-Triplet experiments   \n724 in Table 4 and Table 3 learning rate changes from 0.002 to $1e-8$ with a cosine scheduler. For Table   \n725 A4 and Table A3 experiments earning rate changes from 0.0001 to $1e-8$ with a cosine scheduler. In   \n726 all experiments the batch size is set to 64. In the main Causal-Triplet experiments we train the models   \n727 for 400 epochs, in the appendix Causal-Triplet experiments we train the models for 2000 epochs, and   \n728 in the synthetic experiments we train the models for 100 epochs. In the appendix experiments, the   \n729 graph parameters for explicit models are frozen after 1000 epochs.   \n730 All models are trained using Nvidia GeForce RTX4090 GPUs. Each of the Causal-Triplet experiments   \n731 takes 3-8 hours to train the models and each of the synthetic experiments takes 2-3 hours to train the   \n732 models.   \n733 We save the models\u2019 weights with best validation loss and evaluate them using those weights with   \n734 test data. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "735 A4 Ablation study ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "736 A4.1 Scalability ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "737 While our primary research objective centered on addressing identifiability challenges in implicit   \n738 causal models under soft interventions, we also conducted an investigation into the scalability of our   \n739 proposed model. To comprehensively assess its performance, we designed experiments covering a   \n740 range of causal graphs, featuring 5 to 10 variables, with 10 different seeds for each variable, following   \n741 a similar experimental setup as our 4-variable causal graph experiments. The outcomes of these   \n742 experiments, comparing ICRL-SM and ILCM, are presented in Figure A6. By increasing the number   \n743 of variables in the graph, confounding factors and ambiguities of causal relations increase as well.   \n744 Consequently, more supervision on $\\nu$ is required to better separate the effect of causal variables   \n745 themselves on the observed variables. ", "page_idx": 19}, {"type": "image", "img_path": "PKEmH9ZJfw/tmp/8081c85319e2788416893c9d45d84250a25413f0d9fd0dc16fc00579fbf8da75.jpg", "img_caption": ["Figure A6: Causal disentanglement for different number of variables "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "746 A4.2 Backbone model ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "747 We trained the models using a simpler backbone model, ResNet18, to see how it affects performance.   \n748 The input image resolution is $64\\times64$ and we use the intervened causal variables to predict action   \n749 and object classes. The results are shown in Table A4 and A3. It can be seen from the results that the   \n750 proposed method outperforms other explicit and implicit models even with a simpler model. ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 20}, {"type": "table", "img_path": "PKEmH9ZJfw/tmp/767f33ff5b1ce63c76f11b7545fd37aa5b21c7b475233681cee5972c5064fd2b.jpg", "table_caption": ["Table A3: Table comparing action and object accuracy across various methods on Causal-Triplet datasets using ResNet18 model. "], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "PKEmH9ZJfw/tmp/175f355413d0e1de1f8df31d646dbb67078c2987e00425688994e0d354b2464e.jpg", "table_caption": ["Table A4: Action and object accuracy of three explicit models are compared with ICRL-SM. Experiments are conducted applying image with resolution of $R_{64}$ as the input to the Resnet18 encoder with the intervened casual variable $(z_{i})$ . "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "751 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "752 1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "53 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n54 paper\u2019s contributions and scope?   \n56 Justification: Our contributions include identifiability of causal models with soft inter  \n57 ventions. In the proposed methods section we give the theory and assumptions for the   \n58 identifiability result and in our experiments we evaluate our method using datasets generated   \n59 by soft interventions. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: We have some strict assumptions on data generation process and model which are given in Assumptions 3.3 and 3.1 which may not be plausible to satisfy in some applications. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "803 3. Theory Assumptions and Proofs ", "page_idx": 21}, {"type": "text", "text": "Justification: We give the full set of our assumptions in the proposed method section and the   \ndetailed proof in Appendix A1.   \nGuidelines: \u2022 The answer NA means that the paper does not include theoretical results. \u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems. \u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. \u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "820 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer:[Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: We provide the full details of our model architecture and training settings in Appendix A3 and in Section 5. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide our anonymized codes which contains the necessary scripts and instructions to run the experiments. Guidelines: \u2022 The answer NA means that paper does not include experiments requiring code. \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. \u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not 2 including code, unless this is central to the contribution (e.g., for a new open-source benchmark). \u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. \u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. \u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. \u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). \u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: We provide the full details of our model architecture and training settings in Appendix A3 and in Section 5. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "99 7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: In our synthetic experiments we initialized the causal graph in the dataests with different seeds. The results of these different seeds are provided in Table 2 and Figure A6. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 23}, {"type": "text", "text": "911 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n912 example, train/test split, initialization, random drawing of some parameter, or overall   \n913 run with given experimental conditions).   \n914 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n915 call to a library function, bootstrap, etc.)   \n916 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n917 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n918 of the mean.   \n919 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n920 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n921 of Normality of errors is not verified.   \n922 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n923 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n924 error rates).   \n925 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n926 they were calculated and reference the corresponding figures or tables in the text.   \n927 8. Experiments Compute Resources   \n928 Question: For each experiment, does the paper provide sufficient information on the com  \n929 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n930 the experiments?   \n931 Answer: [Yes]   \n932 Justification: The details are given in Appendix A3.   \n933 Guidelines:   \n934 \u2022 The answer NA means that the paper does not include experiments.   \n935 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n936 or cloud provider, including relevant memory and storage.   \n937 \u2022 The paper should provide the amount of compute required for each of the individual   \n938 experimental runs as well as estimate the total compute.   \n939 \u2022 The paper should disclose whether the full research project required more compute   \n940 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n941 didn\u2019t make it into the paper).   \n942 9. Code Of Ethics   \n943 Question: Does the research conducted in the paper conform, in every respect, with the   \n944 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n945 Answer: [Yes]   \n946 Justification:   \n947 Guidelines:   \n948 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n949 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n950 deviation from the Code of Ethics.   \n951 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n952 eration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "953 10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "954 Question: Does the paper discuss both potential positive societal impacts and negative   \n955 societal impacts of the work performed?   \n956 Answer: [NA]   \n957 Justification:   \n958 Guidelines:   \n959 \u2022 The answer NA means that there is no societal impact of the work performed.   \n960 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n961 impact or why the paper does not address societal impact.   \n962 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n963 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n964 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n965 groups), privacy considerations, and security considerations.   \n966 \u2022 The conference expects that many papers will be foundational research and not tied   \n967 to particular applications, let alone deployments. However, if there is a direct path to   \n968 any negative applications, the authors should point it out. For example, it is legitimate   \n969 to point out that an improvement in the quality of generative models could be used to   \n970 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n971 that a generic algorithm for optimizing neural networks could enable people to train   \n972 models that generate Deepfakes faster.   \n973 \u2022 The authors should consider possible harms that could arise when the technology is   \n974 being used as intended and functioning correctly, harms that could arise when the   \n975 technology is being used as intended but gives incorrect results, and harms following   \n976 from (intentional or unintentional) misuse of the technology.   \n977 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n978 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n979 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n980 feedback over time, improving the efficiency and accessibility of ML).   \n981 11. Safeguards   \n982 Question: Does the paper describe safeguards that have been put in place for responsible   \n983 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n984 image generators, or scraped datasets)?   \n985 Answer: [NA]   \n986 Justification:   \n987 Guidelines:   \n988 \u2022 The answer NA means that the paper poses no such risks.   \n989 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n990 necessary safeguards to allow for controlled use of the model, for example by requiring   \n991 that users adhere to usage guidelines or restrictions to access the model or implementing   \n992 safety filters.   \n993 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n994 should describe how they avoided releasing unsafe images.   \n995 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n996 not require this, but we encourage authors to take this into account and make a best   \n997 faith effort.   \n998 12. Licenses for existing assets   \n999 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n1000 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n1001 properly respected?   \n1002 Answer: [Yes]   \n1003 Justification:   \n1004 Guidelines:   \n1005 \u2022 The answer NA means that the paper does not use existing assets.   \n1006 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n1007 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n1008 URL.   \n1009 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n1010 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n1011 service of that source should be provided.   \n1012 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n1013 package should be provided. For popular datasets, paperswithcode.com/datasets   \n1014 has curated licenses for some datasets. Their licensing guide can help determine the   \n1015 license of a dataset. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Justification: We only have a code repository for replicating experiments and we have submitted the anonymized zip file with our submission. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 26}, {"type": "text", "text": "1016   \n1017   \n1018   \n1019   \n1020   \n1021   \n1022   \n1023   \n1024   \n1025   \n1026   \n1027   \n1028   \n1029   \n1030   \n1031   \n1032   \n1033   \n1034   \n1035   \n1036   \n1037   \n1038   \n1039   \n1040   \n1041   \n1042   \n1043   \n1044   \n1045   \n1046   \n1047   \n1048   \n1049   \n1050   \n1051   \n1052   \n1053   \n1054   \n1055   \n1056   \n1057   \n1058   \n1059   \n1060   \n1061   \n1062   \n1063   \n1064   \n1065   \n1066   \n1067   \n1068 ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}]