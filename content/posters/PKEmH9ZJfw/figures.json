[{"figure_path": "PKEmH9ZJfw/figures/figures_0_1.jpg", "caption": "Figure 1: Difference between hard interventions and soft interventions: As seen in the middle row, hard interventions sever connections with parents. Therefore, an object's class cannot have any effect on the object's color when we intervene on color. On the other hand, soft interventions, as shown in the bottom row, allow for such effects.", "description": "The figure compares hard and soft interventions by illustrating their effects on a causal graph where an object's class influences its color. Hard interventions directly manipulate the object's color, breaking the causal link with the object's class. Soft interventions, in contrast, exert influence indirectly by altering the causal mechanism that determines the object's color, preserving the link to its class. This difference highlights the subtle nature of soft interventions and the challenges they pose for causal inference.", "section": "1 Introduction"}, {"figure_path": "PKEmH9ZJfw/figures/figures_5_1.jpg", "caption": "Figure 2b: Generative model. It includes a data augmentation step that adds the intervention displacement x as an observed feature that directly represents the effect of a soft intervention in observation space.", "description": "This figure shows the generative model used in the ICRL-SM approach.  The model takes pre-intervention observations (X) and post-intervention observations (X') as input.  The difference between these observations (X-X') is also fed into a separate encoder that outputs a causal mechanism switch variable (V).  Both pre-intervention (E) and post-intervention (E') exogenous variables, along with the causal mechanism switch (V), are used in the solution function (S) which aims to disentangle the causal variables (Z) from the exogenous variables by modeling the effects of soft interventions. A decoder then reconstructs the original observations from the causal variables.", "section": "3.2 Causal Mechanisms Switch Variable"}, {"figure_path": "PKEmH9ZJfw/figures/figures_5_2.jpg", "caption": "Figure 2b: Generative model. It includes a data augmentation step that adds the intervention displacement x as an observed feature that directly represents the effect of a soft intervention in observation space.", "description": "The figure shows a generative model that takes pre-intervention exogenous variables and maps them to causal variables.  A switch variable is then used to model soft interventions which affect the post-intervention exogenous variables. Finally, the post-intervention exogenous variables are mapped to post-intervention causal variables, and then the pre- and post-intervention causal variables are mapped to the pre- and post-intervention observations respectively.  The difference between pre- and post-intervention observations is also used as input to the model.", "section": "3.2 Causal Mechanisms Switch Variable"}, {"figure_path": "PKEmH9ZJfw/figures/figures_14_1.jpg", "caption": "Figure A1: The distribution of observed and causal variables in two causal models M and M', which belong to the equivalence class up to reparameterization. (a) There are 10 observed samples in which Z\u2081 or Z\u2082 has been intervened on. (b) The distribution of causal variables when I = 0 (no intervention) is identical to each other but the range of value of causal variables are different and can be mapped to each other using \u03c6z. (c) The intervention on Z\u2081 (I = 1). (d) The intervention on Z\u2082 (I = 2). For I = 1 and I = 2 the distributions are again identical to each other but are different for different targets of intervention as soft interventions change the conditional distribution (condition on parents) of causal variables. Also, for each value of I, the distributions of M and M' should move in one direction as targets are known.", "description": "This figure demonstrates the identifiability of the proposed model up to reparameterization.  It shows that even with soft interventions, the distributions of observed and causal variables in two equivalent models (M and M') remain consistent, despite variations in the specific values. This consistency holds for both pre-intervention (I=0) and post-intervention scenarios (I=1,2) with different intervention targets. The key is that the distributions shift predictably in the same direction for both models under any given intervention.", "section": "A1 Proof of Identifiability Theorem"}, {"figure_path": "PKEmH9ZJfw/figures/figures_15_1.jpg", "caption": "Figure A3: String diagrams for connections between \u0190 and \u0190'. The triangle indicates sampling variables from their corresponding distributions.", "description": "This figure shows the string diagrams illustrating the component-wise transformation between the pre-intervention and post-intervention exogenous variables. The diagrams illustrate how the transformation preserves the probability measure on exogenous variables, a key requirement for identifiability up to reparameterization.  The left-hand side shows the transformation applied after the intervention, while the right-hand side shows it applied before the intervention, demonstrating the equivalence.", "section": "A1 Proof of Identifiability Theorem"}, {"figure_path": "PKEmH9ZJfw/figures/figures_16_1.jpg", "caption": "Figure A3: String diagrams for connections between \u0190 and \u0190'. The triangle indicates sampling variables from their corresponding distributions.", "description": "This figure presents string diagrams illustrating the transformation between the exogenous variables \u0190 and \u0190'.  The left side shows the original variables, and the right side depicts the transformation after applying the component-wise transformations \u03c6<sub>\u0190i</sub> and \u03c6<sub>Z<sub>A</sub></sub>. The diagrams break down the transformation into steps, highlighting how the ancestral variables (Z<sub>A</sub>) and individual exogenous variables (\u0190<sub>i</sub>) contribute to the overall transformation.", "section": "A1 Proof of Identifiability Theorem"}, {"figure_path": "PKEmH9ZJfw/figures/figures_16_2.jpg", "caption": "Figure 1: Difference between hard interventions and soft interventions: As seen in the middle row, hard interventions sever connections with parents. Therefore, an object's class cannot have any effect on the object's color when we intervene on color. On the other hand, soft interventions, as shown in the bottom row, allow for such effects.", "description": "This figure illustrates the difference between hard and soft interventions. Hard interventions completely sever the connection between a variable and its parents, leading to a clear effect. However, soft interventions subtly influence the mechanism, making the effect less clear. This difference poses challenges for learning causal models.", "section": "1 Introduction"}, {"figure_path": "PKEmH9ZJfw/figures/figures_17_1.jpg", "caption": "Figure A5: Causal graph models in the presence of Hard (a) and Soft (b) interventions. There are no connections from parents to in hard interventions (a). Whereas, parents are connected to in soft interventions (b).Let's consider an implicit model and use /i to denote all variables except variable i. The major difference of soft intervention (b) with hard intervention (a) is that is no longer disconnected from its parents and its causal mechanism is affected by the intervention. Thus, with a hard intervention, we know the post-intervention parents of a node (there are none), whereas with soft interventions, the parents themselves may not change.", "description": "This figure illustrates the difference between hard and soft interventions in a causal graph model. In hard interventions, the intervened variable is completely separated from its parents, effectively severing the connection. However, in soft interventions, the causal relationship between the intervened variable and its parents remains intact, meaning the effect of the intervention is more subtle and indirect.  The figure shows this difference by depicting the causal structure before and after an intervention for both hard and soft scenarios.", "section": "A2 Soft vs. Hard intervention"}, {"figure_path": "PKEmH9ZJfw/figures/figures_19_1.jpg", "caption": "Figure A1: The distribution of observed and causal variables in two causal models M and M', which belong to the equivalence class up to reparameterization. (a) There are 10 observed samples in which Z\u2081 or Z2 has been intervened on. (b) The distribution of causal variables when I = 0 (no intervention) is identical to each other but the range of value of causal variables are different and can be mapped to each other using oz. (c) The intervention on Z\u2081 (I = 1). (d) The intervention on Z2 (I = 2). For I = 1 and I = 2 the distributions are again identical to each other but are different for different targets of intervention as soft interventions change the conditional distribution (condition on parents) of causal variables. Also, for each value of I, the distributions of M and M' should move in one direction as targets are known.", "description": "Figure A1 shows how the distributions of observed and causal variables change when soft interventions are applied in two latent causal models M and M'.  Panels (a) through (d) illustrate the impact of interventions on the distribution of causal variables. In particular, they show that while the distributions in M and M' are different, they're equivalent up to a component-wise reparameterization (which maps variables between the two models), maintaining the overall relationships between variables, despite the differences in ranges.  The figure provides visual evidence supporting the identifiability of the latent causal models under soft interventions, as described in the paper's theoretical results.", "section": "A1 Proof of Identifiability Theorem"}]