[{"figure_path": "jfE7XCE89y/tables/tables_1_1.jpg", "caption": "Table 1: We evaluated the characteristics of FuseMoE against various benchmarks. The pipeline approach [83] relies on a simple feature extraction scheme for each modality, followed by concatenation and classification. It doesn't incorporate irregularities or missingness in its process, but its use of concatenation and zero-imputation for missing modalities allows it to be adapted to FlexiModal settings. Both [101] and [96] tackle multi-modality fusion, but as modalities increase, their method demands exponentially more cross-modal computations and significant model architecture modifications. Finally, [58] presents MoE for language-image alignment, yet it also requires substantial adjustments for the more intricate and universal FlexiModal context we explore.", "description": "This table compares FuseMoE with other state-of-the-art multimodal fusion methods.  It highlights key differences in their ability to handle irregular data, missing modalities, and varying numbers of modalities.  The table also notes whether each method has a theoretical foundation and if it's adaptable to the FlexiModal data setting (FuseMoE's focus).", "section": "1 Introduction"}, {"figure_path": "jfE7XCE89y/tables/tables_6_1.jpg", "caption": "Table 1: We evaluated the characteristics of FuseMoE against various benchmarks. The pipeline approach [83] relies on a simple feature extraction scheme for each modality, followed by concatenation and classification. It doesn't incorporate irregularities or missingness in its process, but its use of concatenation and zero-imputation for missing modalities allows it to be adapted to FlexiModal settings. Both [101] and [96] tackle multi-modality fusion, but as modalities increase, their method demands exponentially more cross-modal computations and significant model architecture modifications. Finally, [58] presents MoE for language-image alignment, yet it also requires substantial adjustments for the more intricate and universal FlexiModal context we explore.", "description": "This table compares FuseMoE against four benchmark methods across several characteristics: data type, handling of irregularity and missingness in data, number of modalities supported, theoretical analysis, and adaptability to FlexiModal data.  It highlights FuseMoE's unique advantages in handling complex, real-world multimodal data.", "section": "Introduction"}, {"figure_path": "jfE7XCE89y/tables/tables_8_1.jpg", "caption": "Table 3: MoE demonstrates improved performance averaged over 5 random experiments on the CMU-MOSI and MOSEI datasets; the best results are highlighted in bold font and the second best results are underlined.", "description": "This table presents the results of the multimodal sentiment analysis experiments conducted on the CMU-MOSI and MOSEI datasets using several methods, including the proposed Mixture-of-Experts (MoE) approach.  The table compares the performance of various methods across multiple metrics such as MAE (Mean Absolute Error), Accuracy, Correlation, and F1-score, highlighting the superior performance of the MoE approach.  The results demonstrate that the MoE model offers better performance than existing methods when dealing with multimodal data in a sentiment analysis task.", "section": "4.1 Main Results"}, {"figure_path": "jfE7XCE89y/tables/tables_9_1.jpg", "caption": "Table 3: MoE demonstrates improved performance averaged over 5 random experiments on the CMU-MOSI and MOSEI datasets; the best results are highlighted in bold font and the second best results are underlined.", "description": "This table presents the results of experiments comparing different methods on the CMU-MOSI and MOSEI datasets, focusing on the performance of MoE (Mixture-of-Experts). The table shows that MoE outperforms other methods in terms of accuracy, correlation, and F1-score. The results are averaged over 5 random experiments, and the best and second-best results are highlighted.", "section": "4.1 Main Results"}, {"figure_path": "jfE7XCE89y/tables/tables_21_1.jpg", "caption": "Table 6: We present the total number of ICU stays in each task, taking into account observations with missing modalities. The total number of stays with at least one observation of the corresponding modality are shown in the three right-most columns.", "description": "This table shows the number of ICU stays included in the study for three different tasks (48-IHM & LOS, 25-PHE).  It also breaks down the total number of stays that had at least one observation for each modality (text, CXR, ECG).  Note that missing modalities were considered in the total count, and this table helps show the prevalence of missing data in the different modalities.", "section": "B FlexiModal Data and Tasks of Interest"}, {"figure_path": "jfE7XCE89y/tables/tables_22_1.jpg", "caption": "Table 3: MoE demonstrates improved performance averaged over 5 random experiments on the CMU-MOSI and MOSEI datasets; the best results are highlighted in bold font and the second best results are underlined.", "description": "This table presents a comparison of the performance of different methods on the CMU-MOSI and MOSEI datasets.  The methods compared include various baselines (MulT, TFN, MAG) and different versions of the MoE model (Softmax-MoE, Joint experts&router, Per-mod router, Disjoint router). The metrics used for evaluation are MAE, Acc-2, Corr, and F1-score. The table shows that the MoE models, especially those with Laplace gating, consistently outperform the baselines across all metrics, highlighting the effectiveness of MoE in multimodal sentiment analysis.", "section": "4.1 Main Results"}, {"figure_path": "jfE7XCE89y/tables/tables_27_1.jpg", "caption": "Table 3: MoE demonstrates improved performance averaged over 5 random experiments on the CMU-MOSI and MOSEI datasets; the best results are highlighted in bold font and the second best results are underlined.", "description": "This table compares the performance of different multimodal fusion methods on the CMU-MOSI and MOSEI datasets.  The methods are evaluated based on several metrics, including MAE, Accuracy-2, Correlation, and F1-score.  The table shows that the Mixture-of-Experts (MoE) approach outperforms other baseline methods across different metrics and datasets. The best results for each metric on each dataset are highlighted in bold font, with second best results underlined. This highlights the effectiveness of the MoE model, especially in handling multi-modal data.", "section": "4.1 Main Results"}, {"figure_path": "jfE7XCE89y/tables/tables_27_2.jpg", "caption": "Table 3: MoE demonstrates improved performance averaged over 5 random experiments on the CMU-MOSI and MOSEI datasets; the best results are highlighted in bold font and the second best results are underlined.", "description": "This table presents the performance comparison of different methods on CMU-MOSI and MOSEI datasets.  The metrics used include Mean Absolute Error (MAE), Accuracy-2 (Acc-2), Pearson Correlation (Corr), and F1-score.  The table highlights the superior performance of the Mixture-of-Experts (MoE) model compared to baseline methods such as MulT, TFN, and MAG.  Different MoE configurations (joint experts & router, per-modality router, disjoint router) are compared, showcasing the impact of architectural choices on performance.", "section": "4.1 Main Results"}, {"figure_path": "jfE7XCE89y/tables/tables_27_3.jpg", "caption": "Table 3: MoE demonstrates improved performance averaged over 5 random experiments on the CMU-MOSI and MOSEI datasets; the best results are highlighted in bold font and the second best results are underlined.", "description": "This table presents a comparison of the performance of different methods on the CMU-MOSI and MOSEI datasets.  The methods include several baselines and different versions of the MoE model, varying in their gating functions (Softmax, Laplace) and router designs.  The performance metrics used include MAE, Acc-2 (accuracy), correlation (Corr), and F1-score.  The best results for each metric are highlighted in bold, indicating that the MoE models generally outperformed the baselines.", "section": "4.1 Main Results"}]