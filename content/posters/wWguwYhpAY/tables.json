[{"figure_path": "wWguwYhpAY/tables/tables_5_1.jpg", "caption": "Table 1: Image reconstruction with different activations. Reporting performance of various activations on the Kodak dataset [12]. Our approach outperforms the baselines (Base, Wider).", "description": "This table compares the performance of different activation functions (Softplus, Softplus+FF, Sine, FINER) on the Kodak image dataset using three different methods: a baseline model, a wider variant with more parameters, and the proposed Neural Experts method.  PSNR, SSIM, and LPIPS scores are provided to evaluate image reconstruction quality.  The table highlights the superior performance of the Neural Experts method, particularly when compared to the wider baseline, even with fewer parameters.", "section": "4.1 Image reconstruction"}, {"figure_path": "wWguwYhpAY/tables/tables_5_2.jpg", "caption": "Table 2: Image reconstruction architecture ablations with Sine activation. Comparing our method to baselines on the Kodak dataset [12] with Sine activations. Our method shows substantial improvement over the naive Vanilla MoE model, random pretraining significantly enhances the Vanilla MoE model's performance, and our smaller Neural Experts model remains competitive.", "description": "This table presents ablation studies on the Sine activation function, comparing different architectural variations of the proposed Neural Experts model with baseline models (Vanilla MoE, Vanilla MoE with random pretraining).  It demonstrates the model's improved performance and the effectiveness of random pretraining.  The table highlights the substantial improvement of the proposed model over baselines, while showing that even a smaller version remains competitive.", "section": "4.1 Image reconstruction"}, {"figure_path": "wWguwYhpAY/tables/tables_5_3.jpg", "caption": "Table 3: Audio reconstruction. Reporting mean squared error (MSE), divided by 10-4 for brevity. The results show that the proposed Neural Experts method is able to significantly outperform non-MoE architectures while utilizing fewer parameters.", "description": "This table presents the mean squared error (MSE) for audio reconstruction using SIREN, SIREN Wider, and the proposed Neural Experts model (Our SIREN MoE). The results are shown for three audio types: Bach (music), Counting (single-speaker speech), and Two Speakers (two-speaker speech). The MSE is divided by 10000 for compactness. The table demonstrates the significant improvement in audio reconstruction performance of the Neural Experts model compared to the baseline SIREN models, while having fewer parameters. ", "section": "4.2 Audio signal reconstruction"}, {"figure_path": "wWguwYhpAY/tables/tables_7_1.jpg", "caption": "Table 4: 3D Shape reconstruction. Reporting the Trimap IoU for d = 0.001 (T-IoU\u2191) and Chamfer distance x1e5 (dc\u2193) for different shapes. The results show a significant boost in performance compared to a larger MLP-based model with the same activation.", "description": "This table presents a comparison of 3D shape reconstruction performance between the proposed Neural Experts method and a baseline SIREN model, using two different sizes (Small and Large).  The evaluation metrics are Trimap IoU (with a boundary distance d of 0.001) and Chamfer distance.  The results demonstrate that Neural Experts achieves significantly better reconstruction with fewer parameters.", "section": "4.3 Surface reconstruction results"}, {"figure_path": "wWguwYhpAY/tables/tables_7_2.jpg", "caption": "Table 5: Conditioning ablation. Without conditioning yields the worst performance however concatenation the experts encoder and manager encoder outperforms all variations.", "description": "This table presents the ablation study on different conditioning options for the proposed Neural Experts model.  It compares the Peak Signal-to-Noise Ratio (PSNR) achieved by four different conditioning methods: no conditioning, max pooling, mean pooling, and concatenation of encoder outputs. The results show that concatenating the encoder outputs yields the best performance, highlighting the importance of this design choice for improving the model's accuracy.", "section": "4.4 Ablation study"}, {"figure_path": "wWguwYhpAY/tables/tables_7_3.jpg", "caption": "Table 5: Conditioning ablation. Without conditioning yields the worst performance however concatenation the experts encoder and manager encoder outperforms all variations.", "description": "This table shows the results of an ablation study on the conditioning of the expert encoder outputs in the Neural Experts model.  Different conditioning methods were tested, including no conditioning, max pooling, mean pooling, and concatenation. The results show that concatenation of the encoder outputs yields the best performance, significantly outperforming the other methods.", "section": "4.4 Ablation study"}, {"figure_path": "wWguwYhpAY/tables/tables_8_1.jpg", "caption": "Table 7: Number of experts ablation. There is a trade-off between increasing the number of experts (leading to higher parameters count and training time), and the improvement in PSNR performance. Time is iteration time (ms).", "description": "This table shows the impact of varying the number of experts on model performance. Increasing the number of experts improves PSNR (peak signal-to-noise ratio), a metric measuring image quality, but also increases the number of parameters and training time.  The results suggest an optimal number of experts that balances performance gains against computational cost. The data shows a trade-off: More experts lead to better PSNR but also more parameters and longer training time.", "section": "4 Neural Experts Experimental Results"}, {"figure_path": "wWguwYhpAY/tables/tables_8_2.jpg", "caption": "Table 8: Experts vs. Layers ablation. The results show a trend that fewer layers and more experts yield better performance.", "description": "This table presents an ablation study on the impact of varying the number of experts and layers in the model's architecture on its performance.  The results suggest a trade-off between these two factors; fewer layers combined with more experts seem to improve performance, although there are diminishing returns with an increasing number of experts.", "section": "4 Neural Experts Experimental Results"}, {"figure_path": "wWguwYhpAY/tables/tables_8_3.jpg", "caption": "Table 9: Parameter allocation ablation. Neural Experts perform comparably (regardless of allocation), except in the case where the majority of the parameters are allocated to the manager.", "description": "This table shows the impact of different parameter allocation strategies on the performance of the Neural Experts model. The model's architecture is kept consistent, except for the distribution of parameters across its three main components: Expert Encoder, Experts, and Manager.  Despite significant variation in the distribution, the PSNR remains relatively consistent, except when a larger portion of the parameters is allocated to the Manager component. This suggests that a balanced distribution of parameters is optimal for the best overall model performance.", "section": "4.4 Ablation study"}, {"figure_path": "wWguwYhpAY/tables/tables_14_1.jpg", "caption": "Table 1: Image reconstruction with different activations. Reporting performance of various activations on the Kodak dataset [12]. Our approach outperforms the baselines (Base, Wider).", "description": "This table presents a comparison of image reconstruction performance using different activation functions (Softplus, Softplus + FF, Sine, FINER) and model architectures (Base, Wider, Ours).  The \"Base\" and \"Wider\" columns represent baseline models with varying numbers of parameters, while the \"Ours\" column shows the performance of the proposed Neural Experts method. The results demonstrate that the Neural Experts method achieves superior PSNR, SSIM, and LPIPS scores compared to the baselines.", "section": "4.1 Image reconstruction"}, {"figure_path": "wWguwYhpAY/tables/tables_14_2.jpg", "caption": "Table 11: Parameter allocation ablation. We update Table 9 to include the sizes for each component.", "description": "This table presents an ablation study on the effect of different parameter allocations across the various components of the Neural Experts model.  It shows the architecture (number of layers and width of each component: expert encoder, experts, manager encoder, manager) and percentage of total parameters allocated to each component for four different allocation strategies: Larger Manager, Larger Experts (the authors' proposed method), Larger Encoder, and Balanced.  The PSNR (Peak Signal-to-Noise Ratio) results for each configuration are also provided, demonstrating that the model performs well with different parameter allocation strategies and highlighting the robustness of the architecture.", "section": "4.1 Image reconstruction"}, {"figure_path": "wWguwYhpAY/tables/tables_18_1.jpg", "caption": "Table 12: IoU and Trimap IoUs of 3D reconstruction. IoU(d) indicates Trimap IoU with boundary region of radius d (IoU is computed within the region of distance d of the ground truth boundary).", "description": "This table presents the Intersection over Union (IoU) and Trimap IoU scores for 3D surface reconstruction using different methods.  Trimap IoU, denoted as IoU(d), is calculated within a boundary region of radius d around the ground truth shape.  The table compares two model sizes (Large and Small) for both SIREN and the proposed MoE (Mixture of Experts) method, showing that the MoE method generally achieves better results for both model sizes.", "section": "4.3 Surface reconstruction results"}, {"figure_path": "wWguwYhpAY/tables/tables_18_2.jpg", "caption": "Table 12: IoU and Trimap IoUs of 3D reconstruction. IoU(d) indicates Trimap IoU with boundary region of radius d (IoU is computed within the region of distance d of the ground truth boundary).", "description": "This table presents the performance of different methods on 3D shape reconstruction using IoU and Trimap IoU metrics.  Trimap IoU is calculated within different distances (d) from the ground truth shape boundary, providing more nuanced evaluation of boundary accuracy. The results show the IoU, IoU(0.1), IoU(0.01), and IoU(0.001) values for different models on various shapes.  The table helps assess how well the models capture the shape's boundary and finer details.", "section": "4.3 Surface reconstruction results"}]