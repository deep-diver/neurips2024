[{"figure_path": "KEe4IUp20I/figures/figures_1_1.jpg", "caption": "Figure 1: An overview of the SpaceByte architecture. The embedding, local transformer blocks, and de-embedding (i.e., a layer norm and linear) are the standard Transformer decoder layers. SpaceByte modifies the standard transformer by applying \"global\" transformer blocks only after certain bytes, such as space characters. The intuition is that the first character of a word is typically the hardest to predict; thus this positioning of the global blocks should make the best use of the global blocks (which use a larger model dimension).", "description": "This figure shows the architecture of SpaceByte.  It's a byte-level Transformer decoder with added \"global\" Transformer blocks inserted between standard Transformer layers. These global blocks are applied selectively after specific bytes (like spaces), aiming to improve prediction accuracy, especially at word beginnings.", "section": "2 SpaceByte"}, {"figure_path": "KEe4IUp20I/figures/figures_6_1.jpg", "caption": "Figure 3: Pareto frontier of the cross-entropy bits-per-byte\u00b3 vs FLOPs-per-byte during inference (details in Appendix A.1) for each model architecture trained using 1018 (connected by thin lines) or 1019 (thick lines) FLOPs on different datasets (on a log-log scale). Each dot describes a model with a different number of layers and/or model dimension. Lower and to the left is better. SpaceByte (red) outperforms all other byte-level architectures across the entire Pareto frontier for all datasets. SpaceByte roughly matches the performance of the subword Transformer using SentencePiece tokens, and outperforms the subword Transformer using GPT2 tokens.", "description": "This figure presents the Pareto frontier showing the trade-off between cross-entropy (a measure of model performance) and FLOPs-per-byte (a measure of computational cost) for various language models.  Different models are trained with varying compute budgets (10^18 and 10^19 FLOPs).  The plot demonstrates that SpaceByte consistently outperforms other byte-level models and achieves performance comparable to subword Transformer models, especially when considering a fixed compute budget.", "section": "5 Results"}, {"figure_path": "KEe4IUp20I/figures/figures_16_1.jpg", "caption": "Figure 3: Pareto frontier of the cross-entropy bits-per-byte\u00b3 vs FLOPs-per-byte during inference (details in Appendix A.1) for each model architecture trained using 1018 (connected by thin lines) or 1019 (thick lines) FLOPs on different datasets (on a log-log scale). Each dot describes a model with a different number of layers and/or model dimension. Lower and to the left is better. SpaceByte (red) outperforms all other byte-level architectures across the entire Pareto frontier for all datasets. SpaceByte roughly matches the performance of the subword Transformer using SentencePiece tokens, and outperforms the subword Transformer using GPT2 tokens.", "description": "This figure shows the Pareto frontier for different language models trained with varying compute budgets.  The x-axis represents the inference FLOPs per byte (a measure of computational cost), and the y-axis represents the cross-entropy (bits per byte), a measure of model performance.  Lower values on both axes are better. The figure compares SpaceByte against other byte-level models (MegaByte, byte-level transformer) and subword models. SpaceByte consistently outperforms other byte-level models and achieves similar performance to the best subword model.", "section": "5 Results"}]