[{"type": "text", "text": "CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations ", "text_level": 1, "page_idx": 0}, {"type": "image", "img_path": "k0qTnbQxzR/tmp/c953e4a7e2c51c7756bba757bc1518138b499d5e0059e4f5cb59d2aee072476b.jpg", "img_caption": ["Figure $\\because\\,\\mathrm{CogCoM}$ solves various visual problems with Chain of Manipulations mechanism. Note that the CoM reasoning generates evidential and explainable steps, without involving external tools. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2 Vision-Language Models (VLMs) have demonstrated their broad effectiveness   \n3 thanks to extensive training in aligning visual instructions to responses. However,   \n4 such training of conclusive alignment leads models to ignore essential visual rea  \n5 soning, further resulting in failures in meticulous visual problems and unfaithful   \n6 responses. Drawing inspiration from human cognition in solving visual prob  \n7 lems (e.g., marking, zoom in), this paper introduces Chain of Manipulations,   \n8 a mechanism that enables VLMs to solve problems step-by-step with evidence.   \n9 After training, models can solve various visual problems by eliciting intrinsic   \n10 manipulations (e.g., grounding, zoom in) with results (e.g., boxes, image) actively   \n11 without involving external tools, while also allowing users to trace error causes. We   \n12 study the roadmap to implement this mechanism, including (1) a flexible design of   \n13 manipulations upon extensive analysis, (2) an efficient automated data generation   \n14 pipeline, (3) a compatible VLM architecture capable of multi-turn multi-image,   \n15 and (4) a model training process for versatile capabilities. With the design, we also   \n16 manually annotate 6K high-quality samples for the challenging graphical mathemat  \n17 ical problems. Our trained model, CogCoM, equipped with this mechanism with   \n18 17B parameters achieves state-of-the-art performance across 9 benchmarks from   \n19 4 categories, demonstrating the effectiveness while preserving the interpretability.   \n20 Our code, model weights, and collected data will be publicly available. ", "page_idx": 1}, {"type": "text", "text": "21 1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "image", "img_path": "k0qTnbQxzR/tmp/1f62339c541d17d97deae55d93fdc855e39ca0655572499223c121211a0ac91a.jpg", "img_caption": ["Figure 2: In comparison with existing VLMs, CogCoM performs the multiple steps of evidential reasoning with chain of manipulations (CoM) to achieve the faithful answer to visual scene. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "22 Beneftiing from the advantage of Large Language Models (LLMs) in broad world knowledge, large   \n23 Vision Language Models (VLMs) (Alayrac et al., 2022; Wang et al., 2023b) that are further trained   \n24 to understand visual inputs have demonstrated viabilities on broad multimodal scenarios, such as   \n25 visual question answering (Liu et al., 2023b), visual grounding (Peng et al., 2023), optical character   \n26 recognition (Zhang et al., 2023b). The research employing VLMs as foundation models (Bai et al.,   \n27 2023; Sun et al., 2023b; Wang et al., 2023b) usually involves two main stages of training, where   \n28 the first stage develops intrinsic visual understanding ability through exposure to massive image  \n29 caption pairs, and the second stage endows the models with problem-solving capabilities through the   \n30 instruction tuning.   \n31 However, existing tuning methods train models to respond to instructions with conclusive language   \n32 responses upon visual inputs, which leads models to ignore the essential intermediate visual reasoning   \n33 and further results in failures in meticulous visual problems, unfaithful responses, and even hallucina  \n34 tions. For example in the left subplot of Figure 2, we test the top-performing model CogVLM (Wang   \n35 et al., 2023b) about the details in the image (i.e., texts written on a pillar), and it directly responds   \n36 an incorrect answer (i.e., NO SMOKING), most likely from bias to visual or linguistic priors (i.e.,   \n37 typical scenes with a pillar in office). The absence of the essential reasoning on the visual scene may   \n38 lead to a rash response (Hwang et al., 2023).   \n39 Humans solve problems regarding visual details by marking or processing the given images for   \n40 convenience and rigor, which we refer to as manipulations. For example, we find targets by sequen  \n41 tially locating references, and concentrate on subtle details by zooming into a corresponding region.   \n42 Most of VLMs have developed numerous intrinsic capabilities (e.g., grounding boxes, recognizing   \n43 texts) during the first stage of training. By further imitating the fundamental human behaviours (e.g.,   \n44 cropping, zoom in), models have the potential to perform this cognitive reasoning process. Three   \n45 major obstacles in eliciting VLMs with such reasoning are (1) flexible definitions of manipulations   \n46 covering most visual problems, (2) an efficient data collection pipeline capable of producing abundant   \n47 training data, and (3) a multi-turn multi-image VLM structure compatible with existing models.   \n48 Inspired by the human cognition in solving visual problems, we introduce Chain of Manipulations   \n49 (CoM), a mechanism that enables VLMs to solve problems step-by-step with evidence, with each   \n50 step potentially involving a manipulation on the visual input and its corresponding result, both   \n51 generated by the model to facilitate the success and fidelity. This paper studies a complete roadmap   \n52 with manipulations design, data collection, model architecture and training process for training   \n53 general VLMs with this mechanism. We first formally design 6 basic manipulations upon the pilot   \n54 experiments, which are capable of handling diverse visual problems. Next, we propose a cascading   \n55 data generation pipeline based on reliable large language models (e.g., LLMs, the linguistic annotators)   \n56 and visual foundational models (e.g., VFMs, the visual annotators), which can automatically produce   \n57 abundant error-free training data. We collect 70K CoM samples with this pipeline. We then devise   \n58 a multi-turn multi-image model architecture compatible with typical VLMs structures. Based on a   \n59 data recipe incorporating the curated corpus, we finally train a general VLM equipped with CoM   \n60 reasoning mechanism, named CogCoM, which possesses capabilities of chat, captioning, grounding   \n61 and reasoning. Additionally, benefiting from the expressive capability of the proposed mechanism,   \n62 we further manually annotated 6K high-quality samples of graphical mathematical problems, each   \n63 accompanied by a CoM reasoning process, to advance the research of VLMs in solving challenging   \n64 mathematical problems.   \n65 We conduct extensive experiments on 9 benchmarks from 4 categories, including TextVQA (Singh   \n66 et al., 2019), ST-VQA (Biten et al., 2019), TallyVQA (Acharya et al., 2019), and GQA Hudson &   \n67 Manning (2019) for detailed visual question answering, RefCOCO (Yu et al., 2016), RefCOCO $^+$ (Yu   \n68 et al., 2016), and $\\mathbf{RefCOCOg}$ (Mao et al., 2016) for visual grounding, POPE (Li et al., 2023c) for   \n69 hallucination validation, and MM-Vet (Yu et al., 2023b) for general multimodal ability. Our model   \n70 achieves up to 9.0 and 1.09 accuracy improvement on the detailed VQA and grounding benchmarks,   \n71 respectively, and the superior performance on the general multimodal benchmark. The results   \n72 demonstrate the effectiveness of the mechanism while maintaining the interpretability of outputs. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "73 2 Terminology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "74 We first conduct pilot experiments to investigate the possible manipulations capable of handling   \n75 diverse visual problems. ", "page_idx": 2}, {"type": "image", "img_path": "k0qTnbQxzR/tmp/2c259cccc5a0c82cbf1a9bc2005abe1bec4a41acf16abaab5f4344bbf83caa0e.jpg", "img_caption": ["91 Figure 3: Distribution of the generated 465 actions 92 base on GPT-4, mapped into 6 manipulations. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Specifically, given a question about an image, we prompt the advanced large language model, GPT-4, to generate solving steps by optionally utilizing possible actions on the image that facilitate problem-solving. We conduct this experiment on 170K questions from TextVQA, a dataset requiring detailed reasoning and recognition on images. To ensure the stability, we manually write 4 demonstrations as priors, The detailed statistics are available at Appendix C.3. ", "page_idx": 2}, {"type": "text", "text": "We utilize the StanfordCoreNLP toolkit to extract verb phrases referring to the actions, and the distribution of frequencies is shown in Figure 3. Through result analysis, we find that most of the actions can be mapped to 6 fundamental manipulations on images: OCR, Grounding, CropZoomIn, Counting, Calculate, and Line. ", "page_idx": 2}, {"type": "text", "text": "93 Based on the observation, we formally predefine a set of 6 manipulations, which can either be   \n94 developed from pre-training or be learned from fine-tuning with the imitation to human behaviors:   \n95 ${\\mathcal{M}}\\ \\subseteq\\{O C R(t g t)\\ \\to\\ t x t,\\,\\,G r o u n d i n g(t g t)\\ \\to\\ b b x,\\,\\,C o u n t i n g(t g t)\\ \\to\\ n u m.}$ , Calculate $\\mathbf{\\nabla}(t g t)~\\rightarrow$   \n96 num, CropZoomIn $(b b x,x)\\;\\;\\rightarrow\\;\\;i m g$ , $L i n e(p t s)\\ \\ \\rightarrow\\ \\ i m g\\}$ , where the parameters or results   \n97 tgt, txt, bbx, num, $x$ , img, pts refer to the bounding boxes, zoom ratio, image, target description,   \n98 numbers, texts, and points, respectively. In addition to the predefined manipulations, we also allow   \n99 trained models to create new manipulations during inference to facilitate problem-solving. We   \n100 empirically find that more complicated goals can be derived from these fundamental manipulations.   \n101 We then define the standard CoM data structure to streamline the subsequent data construction   \n102 and validation process. Given a question $Q$ about an initial input image $I_{0}$ , a VLM equipped with   \n103 chain of manipulations mechanism solves the problem to achieve final answer as $V L M_{\\varsigma}(A,\\bar{C}|I_{0},Q)$ ,   \n104 where $\\varsigma$ refers to the reasoning chain with evidence, ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{c c}{\\varsigma=(s t e p_{1},s t e p_{2},\\ldots)}\\\\ {s t e p_{i}=(f_{i},c_{i}),\\;\\;}&{f_{i}\\in\\mathcal{M}}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "105 where $C=(c_{i},c_{2},...,c_{|C|})$ refers to the free-form textual descriptions incorporating manipulation   \n106 names $f_{i}$ and corresponding results from utilizing $f_{i}$ . This definition explicitly declares the symbolic   \n107 execution process, while also being compatible with linguistic reasoning steps. Based on this   \n108 definition, we can clearly construct standard CoM samples that incorporating the manipulation   \n109 executions and linguistic steps with evidence. After the data construction, we can utilize a simple   \n110 method to convert the standard CoM samples to the compatible VQA samples. ", "page_idx": 3}, {"type": "text", "text": "111 3 Data Collection ", "text_level": 1, "page_idx": 3}, {"type": "image", "img_path": "k0qTnbQxzR/tmp/b058cc152c631d7b4084b7d3757c904c097d3a12ff8dd0f697e71de400f93cc5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 4: A cascading data generation pipeline that automatically produces standard CoM samples. Given an original VQA sample, the linguistic annotator (LLMs) taught with usage of manipulations (prompt) is first asked to provide solving steps for the question $\\mathcal{Q}$ , and the visual foundational models (VFMs) are then engaged to replace the manipulations results, followed by a final traversal on the tree branched by the possible manipulation results to find positive paths terminating to the answer $\\boldsymbol{\\mathcal{A}}$ . ", "page_idx": 3}, {"type": "text", "text": "112 In this section, we first introduces the automated data generation pipeline (illustrated in Figure 4),   \n113 that employs reliable LLMs as linguistic annotators and VFMs as the visual annotators to produce   \n114 error-free CoM samples upon prevalent VQA corpus, and then present the manual annotation of   \n115 high-quality CoM samples for the challenging graphical mathematical problems. ", "page_idx": 3}, {"type": "text", "text": "116 3.1 Automated Data Generation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "117 Given a general corpus $\\mathcal{D}=\\{(I,Q,A)\\}$ consisting of triplet samples of images with corresponding   \n118 visual question-answer pairs, our automated data generation pipeline consists of a linguistic annotator   \n119 and several visual annotators according to the manipulations. For a question $Q$ in each sample, we   \n120 first engage the linguistic annotator to generate manipulations-assisted solving steps with the CoM   \n121 format $(f_{i},c_{i})$ , where the corresponding results of the instantiated manipulation executions are set   \n122 with variables as placeholders. In this paper, we adopt GPT-4 (OpenAI, 2023a), a large language   \n123 model with reliable language understanding and generation abilities as the linguistic annotator. We   \n124 design a comprehensive prompt including the task requirements, usage of manipulations, and output   \n125 data format, and further manually annotate 5 demonstrations for a stable generation. The detailed   \n126 implementations are available at Appendix C.4.   \n127 We then employ essential visual annotators to supply the results of manipulations requested in the   \n128 solving steps by exactly performing the corresponding manipulations. By empirically analyzing   \n129 the manipulations from both predefined set and newly created ones (refers to Appendix C.3 for   \n130 a detailed statistics), we reveal the Grounding and $O C R$ are two fundamental manipulations, and   \n131 most of the others can be consequently derived (e.g., CropZoomIn along a region of box, Counting   \n132 upon recognized boxes, and Calculate for the recognized formula). Therefore, we employ two   \n133 visual foundational models, GroundingDINO (Liu et al., 2023c) and PaddleOCR (Du et al., 2020),   \n134 and develop the implementations of these manipulations1. The execution of the manipulations will   \n135 transform the sequential reasoning steps into a tree $\\tau$ , as the input of current manipulation $f_{1}(x_{a})$   \n136 may rely on one of the multiple results of previous manipulation $f_{2}\\rightarrow(x_{b},x_{c})$ , i.e., $x_{a}$ rely on $x_{b}$   \n137 (e.g., step 2 for finding pillars in Figure 5). We then perform a traversal on each produced tree with   \n138 Depth First Search (DFS) to find all positive paths $\\{\\bar{\\mathcal{P}}_{i}|\\mathcal{P}_{i}\\in\\mathcal{T},i=1,2,...\\}$ that can terminate with   \n139 the final answer $A$ from the result of the last manipulation. Based on this method, the generated   \n140 CoM samples with positive paths are guaranteed to be error-free. We implement this pipeline on 3   \n141 existing datasets that require detailed recognition or objects counting, TextVQA (Singh et al., 2019),   \n142 ST-VQA (Biten et al., 2019), and TDIUC (Shrestha et al., 2019), to build 70K CoM samples 2. The   \n143 designed prompt, a generated example with linguistic and visual results, and detailed algorithm   \n144 illustration are available at AppendixC.1. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "145 3.2 Human Annotation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "146 The analysis from Fig.1 of AlphaGeometry (Trinh et al., 2024) shows that outputting auxiliary lines   \n147 in linguistic reasoning process helps LLMs to solve complex geometry problems. Beneftiing from the   \n148 expressive capability of CoM structure, we have also manually annotated high-quality CoM samples   \n149 for the graphical mathematical problems to facilitate VLMs in solving this challenging scenario.   \n150 Similar to the automated pipeline, we engage 10 human experts as the linguistic annotators and   \n151 visual annotators, where each expert is asked to annotate the linguistic solving steps and the use of   \n152 manipulations, as well as the results of manipulations on images. We perform this annotation on the   \n153 MathVista (Lu et al., 2023) and ChartQA (Masry et al., 2022), which include geometric and chart   \n154 math problems, resulting in the collection of 6K high-quality CoM math samples.   \n155 Finally, we adapt the CoM samples to be compatible with VQA-style training samples. For each CoM   \n156 sample including $n$ images from manipulations outputs $(I_{0},Q,C_{0},I_{1},C_{1},...,I_{n},A)$ , we convert it   \n157 into a multi-turn VQA sample segmented by the images $[(I_{0},Q,C_{0})_{\\underline{{{,}}}}(I_{1},\\bar{Q},C_{1}),...,(I_{n},\\bar{Q},A)]$ ,   \n158 where $C_{i}$ represents the intermediate steps between $I_{i}$ and $I_{i+1}$ , and $Q$ is a simple prompt asking   \n159 model to answer question based on history. This transformation converts CoM samples into multi-turn   \n160 VQA samples that are compatible with existing VLMs training data. The detailed statistics of the   \n161 data generation are available at Appendix C.3. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "162 4 Model Training ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "163 4.1 Architecture ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "164 We use the same model architecture as $\\mathrm{CogVLM}$ (Wang et al., 2023b), a general VLM approach   \n165 that involves four fundamental components: (1) a Visual Encoder, (2) an MLP Adapter, (3) an LLM   \n166 Backbone, and (4) a Visual Expert Module, for a reliable multimodal understanding. Concretely,   \n167 the pre-trained EVA2-CLIP-E (Sun et al., 2023a) with 4B parameters and Vicuna-7B-v1.5 (Chiang   \n168 et al., 2023) are adopted as the visual encoder and LLM backbone, respectively. A two-layer MLP   \n169 (SwiGLU (Shazeer, 2020)) is further engaged to map the output of the visual encoder into the   \n170 linguistic space of the LLM backbone. The visual expert module adds the vision-specific weights   \n171 into the attention layer and feed-forward layer of each block in the LLM backbone, resulting in a   \n172 total of 6.5B additional parameters for the deep fusion of modalities.   \n173 Based on this general architecture, we develop a memory-based multi-turn multi-image VLM   \n174 approach. Specifically, for a multi-turn VQA sample $[(I_{t},\\dot{Q}_{t},A_{t})|t=1,2,...]$ , where $A_{t}$ refers to   \n175 $C_{t}$ in CoM, we keep the accumulated KV memories of each layer in the LLM backbone throughout   \n176 these turns. And at each turn $t$ in training and inference, we calculate the attention function att as: ", "page_idx": 4}, {"type": "image", "img_path": "k0qTnbQxzR/tmp/d1d860056f2daddb54521b0627f4befeaddcc6675416fa19046684d190ff4d58.jpg", "img_caption": ["Figure 5: Left: A compatible VLM architecture capable of multi-turn multi-image understanding. Right: An effective training process to develop a general VLM with versatile capabilities. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{a t t(\\mathbf{X})=s o f t m a x(\\frac{\\mathbf{Q}_{t}\\mathbf{K}_{t}^{\\prime T}}{\\sqrt{d}})V_{t}^{\\prime}}}\\\\ &{}&{\\mathbf{K}_{t}^{\\prime}=\\operatorname{trunc}(\\operatorname{concat}(K_{0},K_{1},...,K_{t}))}\\\\ &{}&{\\mathbf{V}_{t}^{\\prime}=\\operatorname{trunc}(\\operatorname{concat}(V_{0},V_{1},...,V_{t}))}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "177 ", "page_idx": 5}, {"type": "text", "text": "178 where $Q_{t}\\in\\mathbb{R}^{s\\times d}$ is query representation of current layer, and the $K_{t}^{\\prime},V_{t}^{\\prime}\\in\\mathbb{R}^{(s\\times t)\\times d}$ refer to the   \n179 concatenation of accumulated representations and will be further truncated if the sequence length   \n180 $s\\times t$ is greater than a predefined threshold. At $t>0$ , the new image $I_{t}$ will be cropped from $I_{t-1}$   \n181 and amplified with the Bicubic Interpolation (Keys, 1981). ", "page_idx": 5}, {"type": "text", "text": "182 4.2 Training ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "183 The proposed CogCoM-17B relies on two main stages of training, to develop the capabilities of   \n184 general multimodal task-solving as well as the visual reasoning.   \n185 First Stage Pre-Training This stage consists of two ordinal sub-phases of training for foundational   \n186 visual understanding and grounded generation. Following the pre-training of $\\mathrm{CogVLM}$ (Wang et al.,   \n187 2023b), we first train model on 1.5B image-text pairs cleaned from the LAION-2B (Schuhmann et al.,   \n188 2022) and COYO-700M (Byeon et al., 2022) with 120,000 iterations and batch size of 8,192. We   \n189 then train model on 40M grounded image-question-answer triples cleaned from LAION-115M (Li   \n190 et al., 2023b) with 60,000 iterations and batch size of 1,024, where each noun phrase in the answer is   \n191 followed by a list of coordinates $[[x_{0},y_{0},x_{1},y_{1}],\\ldots]^{3}$ referring the phrase to the grounded objects in   \n192 the image. Both phases adopt the next token prediction objective, and train the 6.5B parameters of   \n193 visual experts.   \n194 Second Stage Alignment This stage further trains the model to align with human preferences on   \n195 solving practical visual problems. We fuse the produced CoM data with 3 types of corpus, including   \n196 MultiInstruct $\\mathrm{Xu}$ et al., 2022), LLaVAR (Zhang et al., 2023b), and ShareGPT4V (Chen et al., 2023c),   \n197 referring the abilities of instruction-following, texts-recognizing, and detailed-captioning. This fusion   \n198 results in a total of 570K $(I,Q,A)$ samples, where the answer $A$ in CoM data consists of multiple   \n199 turns. For the training data of CoM, we randomly prepend a lunching prompt4 $P^{M}$ to questions   \n200 $Q=P^{\\mathcal{M}}+Q$ asking models to optionally use manipulations for the adaption of explicitly eliciting.   \n201 We empirically show that the model can effectively learn the evidential visual reasoning by ingesting   \n202 this portion of CoM data. We train model with 14,000 iterations and a batch size of 160, where the   \n203 learning rate reaches $10^{-5}$ after 280 steps of warm-up and then decays linearly. The parameters   \n204 of 6.5B visual experts are trained with the objective of next token prediction. These two stages of   \n205 training result in our standard version of CogCoM involving both chat and reasoning capabilities.   \n206 More training details are available at Appendix D.2. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "207 5 Experiment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "208 To quantitatively validate the suitability and efficiency of the proposed method, we conduct exper  \n209 iments on 9 benchmarks corresponding to 4 categories of multimodal capabilities, as well as on a   \n210 newly constructed testbed that includes the evidential reasoning paths with a keypoints-aware metric.   \n211 Following previous works, we train two generalist versions of CogCoM for adapting to the different   \n212 scenarios of Visual Question Answering and Visual Grounding, and evaluate the standard version   \n213 with a qualitative analysis (Hwang et al., 2023). We also evaluate the time complexity.   \n214 \u2022 Detailed Visual Question Answering. This task involves models to perform detailed   \n215 reasoning or recognition on images. We use 4 prominent benchmarks including, GQA (Hud  \n216 son & Manning, 2019), TextVQA (Singh et al., 2019), ST-VQA (Biten et al., 2019), and   \n217 TallyVQA (Acharya et al., 2019).   \n218 \u2022 Visual Grounding. Visual grounding evaluates the crucial abilities of VLMs on meticulous   \n219 position understanding. We evaluate our model on 3 standard benchmarks, RefCOCO (Yu   \n220 et al., 2016), RefCOCO $^+$ (Yu et al., 2016), and RefCOCOg (Mao et al., 2016).   \n221 \u2022 General Multimodal Capabilities & Hallucination. We also evaluate on a general mul  \n222 timodal benchmark, MM-Vet (Yu et al., 2023b), and a hallucination detection benchmark   \n223 POPE (Li et al., 2023c), to investigate the helpfulness of visual reasoning. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "224 5.1 Experiments on Detailed VQA ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "225 VLMs have demonstrated the well-known superiority in visual scenes with salient content understand  \n226 ing. We evaluate the effectiveness of CogCoM on VQAs on detailed understanding, which typically   \n227 require models to perform multiple actions (find, read) or multiple reasoning steps (recognizing and   \n228 then calculating). Following previous studies (Wang et al., 2023b), we train our model obtained   \n229 from the first-phase of stage-1 on a mixture of data, including an instruction corpus of MultiInstruct,   \n230 13 publicly available VQA datasets (only using training set), a newly created VQA dataset built   \n231 through promoting GPT-4V (OpenAI, 2023b) for image-oriented question-answer generation, and   \n232 the automatically generated 70K CoM corpus. This training results in a generalist VQA model   \n233 incorporating CoM reasoning. For all existing VQA tasks, we directly prompt CogCoM with given   \n234 questions and examine the correctness of outputted answers. ", "page_idx": 6}, {"type": "table", "img_path": "k0qTnbQxzR/tmp/775b65a9205eca3bbc9438a9f20bcf6c61d05c7c02d3b3f1cd97d8ff648a5b4f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Table 1: Performance on Visual Question Answering benchmarks, where the results labeled with \u2020 refer to the few-shot setting. CogCoM achieves SOTA across the board, and demonstrates the effectiveness on the visual reasoning and scene texts recognition benchmarks. ", "page_idx": 6}, {"type": "text", "text": "235 5.1.1 GQA, TextVQA, ST-VQA, TallyVQA ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "236 Settings GQA is a compositional VQA benchmark with diverse reasoning questions coming from   \n237 semantic functional programs. TallyVQA is an objects counting benchmark with human-annotated   \n238 complex counting questions involving challenging non-zero counterparts. TextVQA and ST-VQA are   \n239 two texts understanding benchmarks requiring models to answer questions through textual cues on   \n240 images. We use the official evaluation scripts for GQA and TallyVQA, which calculate the accuracy   \n241 score by the Exact Matching (EM) between model predictions and answers. For TextVQA and   \n242 ST-VQA, we submit our model predictions to the official online websites for calculating the accuracy   \n243 with VQA Score metric (Antol et al., 2015).   \n244 Results As the results shown in Table 2, CogCoM achieves the state-of-the-art performance in   \n245 comparison with all generalist models, and achieves significant improvements over the baseline model.   \n246 Specifically, compared to the baseline model, our model achieves up to 5.97 and 9.0 percentage   \n247 points improvement on the benchmarks that requires complex reasoning and detailed recognition,   \n248 respectively. On GQA and TextVQA, CogCoM also obtains comparable results with the large-scale   \n249 specialist SOTAs. This result demonstrates the effectiveness of the proposed approach in solving   \n250 details recognition problem. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "251 5.1.2 Experiments for Reasoning Accuracy and Time Complexity ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "252 Due to the lack of resource, we build CoM-test, a benchmark with evidential reasoning chains on the   \n253 TextVQA test set based on the proposed data generation pipeline, and also introduce a keypoints  \n254 aware metric to validate the correctness of reasoning paths (see Appendix C.3 for detailed statistics).   \n255 We also evaluate the time complexity for model generation on a held-out benchmark, MM-Vet.   \n256 Reasoning Accuracy To validate the correctness of execution and results of manipulations in   \n257 reasoning paths, we introduce a keypoints-aware evaluation metric that concentrates on these contents   \n258 and their order. Concretely, given a predicted chain-answer pair $(C^{\\prime},A^{\\prime})$ and the ground truth   \n259 pair $(C,A)$ , we first extract the keypoints (i.e., the name, parameters, and results of manipulations)   \n260 in $A^{\\prime},A$ to form two lists, and then discretize these two lists into $K^{\\prime}$ and $K$ based on a bag  \n261 of-words composed of all keypoints. Then, we calculate the normalized Levenshtein Distance   \n262 $s_{K}=L e v e n s h t e i n(K^{\\prime},K)/N$ as the manipulation score. We also compute the BLEU (Papineni   \n263 et al., 2002) score $s_{C}=\\mathrm{BLEU}(C^{\\prime},C)$ as the paragraph score. Finally, a weighted average of these   \n264 two scores serves as the ultimate reasoning score s $a c c=(0.6\\times s_{K}\\dot{+}0.4\\times\\dot{s}_{C})/2$ .   \n265 We train our first-stage model only using the 70K automated CoM data without other supervision   \n266 for qualitatively evaluate the effectiveness of chains, and the results are shown in the left subplot   \n267 of Figure 6. We find that by training with the CoM chains, our model can swiftly achieve the   \n268 satisfactory performance of 48.41 accuracy score with 2k training steps, and obtain the optimal result   \n269 of 55.59 with 8K steps. Additionally, the explanation scores gradually improve along with the model   \n270 performance, indicating that successful reasoning steps contribute to the achieving of final answer.   \n271 Time Complexity We also evaluate the time complexity and average length of tokens during model   \n272 reasoning on a held-out test set, MM-Vet. Specifically, we run CogCoM and the baseline model on   \n273 all 218 questions, and record the time overhead as well as the average number of outputted tokens   \n274 (using the Vicuna-7B-v1.5 tokenizer). We divide the 218 samples into 8 intervals based on the time   \n275 expenditure for each sample and calculate the average values of the time complexity and the number   \n276 of tokens for each interval, with the results presented in the right subplot of Figure 6.   \n277 From the results we find that compared to baseline model, CogCoM produces information-intensive   \n278 reasoning content (e.g., detection boxes, auxiliary lines) without incurring infeasible time overhead.   \n279 For example, without quantitive optimization, CogCoM outputs 262.9 informative tokens in approxi  \n280 mately 9 seconds. With the advantages in long-context optimization techniques (Hooper et al., 2024),   \n281 we believe that it is crucial for models to produce informative content and accurate responses.   \n283 The task of visual grounding requires models to precisely provide the corresponding coordinates   \n284 of regions in an image based on the given target description. Following the existing work (Wang   \n285 et al., 2023b), we train our model obtained by the first stage on a mixture of datasets, including an   \n286 instruction corpus MultiInstruct, a high-quality grounded VQA corpus introduced in $\\mathrm{CogVLM}$ , and   \n287 the 70K CoM data. This training results in a generalist grounding model that is excelling at visual   \n288 grounding while capable of reasoning. For all benchmarks, we prompt $\\mathrm{CogOM}$ in a chat manner to   \n289 ask the model to provide grounded coordinates, such as \u201cWhere is $\\langle e x p r\\rangle$ answer in $[x O,\\mathrm{y}O,x I,y I J]$   \n290 format.\", where the $\\langle e x p r\\rangle$ refers to the target expression. We use the standard metric, that considers   \n291 a prediction as correct when the intersection-over-union (IoU) between boxes is greater than 0.5.   \n292 Results As shown in Figure 2, CogCoM achieves the best performance in 6 out of all 8 sub-sets.   \n293 Based on the training with a mixture of broad capabilities, this result indicates that our model exhibits   \n294 a superior grounding abilities while offers potential to solve a variety of tasks. ", "page_idx": 7}, {"type": "image", "img_path": "k0qTnbQxzR/tmp/f2a4f98125792eca0e17341d05aa83645fae979b4816f10f5937c33230ec1838.jpg", "img_caption": ["Figure 6: Left: Results on a reasoning testbed CoM-test shows CogCoM achieves satisfactory performance with only 70K training data and 2K steps. Right: Results on MM-Vet shows that $\\mathrm{CogCoM}$ produces comprehensive reasoning content without incurring excessive time overhead. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "table", "img_path": "k0qTnbQxzR/tmp/b62e856319caea6bea31a041b74fc3306514b6e5e175ef4c9d7a1972d3f99091.jpg", "table_caption": ["Table 2: Results on VG benchmarks, where the specialist SOTAs are quoted from (Bai et al., 2023). "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "295 5.3 Experiments on General Multimodal Evaluation and Hallucination Examination ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "296 We further examine the general multimodal capabilities, and the hallucination issue. We use the   \n297 generalist VQA model and obtain model predictions by directly asking the original questions in   \n298 benchmarks. We use the challenging adversarial version and official evaluation scripts for POPE.   \n299 Results As shown in Table 3, we can see that $\\mathrm{CogCoM}$ improves the performance by 0.6 points   \n300 compared to the baseline model on MM-Vet, and achieves the superior performance on POPE which   \n301 is in consistent with the baseline model. This result suggests that out model maintains superior   \n302 reasoning capabilities while preserving effectiveness in general multimodal tasks, and simultaneously   \n303 exhibits lower hallucination. ", "page_idx": 8}, {"type": "table", "img_path": "k0qTnbQxzR/tmp/9fc800f7b29cd73ac30a259d03ec220968a501748de66c849d3c439e800c6ccd.jpg", "table_caption": [], "table_footnote": ["Table 3: Evaluation results on the general and hallucination assessment benchmarks. "], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "304 6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "305 This paper studies the problems presented by the conclusive alignment training of VLMs, and   \n306 proposes a mechanism, Chain of Manipulations (CoM), that enables VLMs to solve problems step  \n307 by-step by actively manipulating visual inputs as evidence. We realize this methodology by proposing   \n308 (1) a flexible data structure, (2) an efficient data generation framework capable of producing abundant   \n309 samples, (3) a memory-based architecture compatible with existing VLMs, and (4) a training process   \n310 for versatile capabilities. We also annotate 6K graphical math samples with reasoning chains to   \n311 facilitate the advancement of VLMs in solving mathematical problems. Experiments on 9 public   \n312 benchmarks show that our trained 17B general VLM can produce informative reasoning content   \n313 while achieving superior performance on diverse multimodal problems. ", "page_idx": 8}, {"type": "text", "text": "314 References", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "315 Acharya, M., Kafle, K., and Kanan, C. Tallyqa: Answering complex counting questions. In   \n316 Proceedings of the AAAI conference on artificial intelligence, 2019.   \n317 Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican,   \n318 K., Reynolds, M., et al. Flamingo: a visual language model for few-shot learning. Advances in   \n319 Neural Information Processing Systems, 2022.   \n320 Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C. L., and Parikh, D. Vqa: Visual   \n321 question answering. In Proceedings of the IEEE international conference on computer vision,   \n322 2015.   \n323 Awadalla, A., Gao, I., Gardner, J., Hessel, J., Hanafy, Y., Zhu, W., Marathe, K., Bitton, Y., Gadre,   \n324 S., Sagawa, S., et al. Openflamingo: An open-source framework for training large autoregressive   \n325 vision-language models. arXiv preprint, 2023.   \n326 Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., and Zhou, J. Qwen-vl: A   \n327 frontier large vision-language model with versatile abilities. arXiv preprint, 2023.   \n328 Biten, A. F., Tito, R., Mafla, A., Gomez, L., Rusinol, M., Valveny, E., Jawahar, C., and Karatzas, D.   \n329 Scene text visual question answering. In Proceedings of the IEEE/CVF international conference   \n330 on computer vision, 2019.   \n331 Byeon, M., Park, B., Kim, H., Lee, S., Baek, W., and Kim, S. Coyo-700m: Image-text pair dataset,   \n332 2022.   \n333 Changpinyo, S., Sharma, P., Ding, N., and Soricut, R. Conceptual 12m: Pushing web-scale image-text   \n334 pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference   \n335 on Computer Vision and Pattern Recognition, 2021.   \n336 Chen, D., Liu, J., Dai, W., and Wang, B. Visual instruction tuning with polite flamingo. arXiv   \n337 preprint arXiv:2307.01003, 2023a.   \n338 Chen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., and Zhao, R. Shikra: Unleashing multimodal   \n339 llm\u2019s referential dialogue magic. arXiv preprint, 2023b.   \n340 Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., and Lin, D. Sharegpt4v: Improving   \n341 large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023c.   \n342 Chen, X., Wang, X., Changpinyo, S., Piergiovanni, A., Padlewski, P., Salz, D., Goodman, S., Grycner,   \n343 A., Mustafa, B., Beyer, L., et al. Pali: A jointly-scaled multilingual language-image model. In The   \n344 Eleventh International Conference on Learning Representations, 2022.   \n345 Chen, X., Djolonga, J., Padlewski, P., Mustafa, B., Changpinyo, S., Wu, J., Ruiz, C. R., Goodman, S.,   \n346 Wang, X., Tay, Y., et al. Pali-x: On scaling up a multilingual vision and language model. arXiv   \n347 preprint, 2023d.   \n348 Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y.,   \n349 Gonzalez, J. E., et al. Vicuna: An open-source chatbot impressing gpt-4 with $90\\%^{*}$ chatgpt quality.   \n350 See https://vicuna. lmsys. org (accessed 14 April 2023), 2023.   \n351 Dai, W., Li, J., Li, D., Tiong, A., Zhao, J., Wang, W., Li, B., Fung, P., and Hoi, S. Instructblip:   \n352 Towards general-purpose vision-language models with instruction tuning. arxiv 2023. arXiv   \n353 preprint arXiv:2305.06500.   \n354 Dai, W., Li, J., Li, D., Tiong, A., Zhao, J., Wang, W., Li, B., Fung, P., and Hoi, S. Instructblip:   \n355 Towards general-purpose vision-language models with instruction tuning. arxiv 2023. arXiv   \n356 preprint, 2023.   \n357 Dong, R., Han, C., Peng, Y., Qi, Z., Ge, Z., Yang, J., Zhao, L., Sun, J., Zhou, H., Wei, H., et al.   \n358 Dreamllm: Synergistic multimodal comprehension and creation. arXiv preprint arXiv:2309.11499,   \n359 2023.   \n360 Du, Y., Li, C., Guo, R., Yin, X., Liu, W., Zhou, J., Bai, Y., Yu, Z., Yang, Y., Dang, Q., et al. Pp-ocr:   \n361 A practical ultra lightweight ocr system. arXiv preprint arXiv:2009.09941, 2020.   \n362 Hong, W., Wang, W., Lv, Q., Xu, J., Yu, W., Ji, J., Wang, Y., Wang, Z., Dong, Y., Ding, M., et al.   \n363 Cogagent: A visual language model for gui agents. arXiv preprint arXiv:2312.08914, 2023.   \n364 Hooper, C., Kim, S., Mohammadzadeh, H., Mahoney, M. W., Shao, Y. S., Keutzer, K., and Gholami,   \n365 A. Kvquant: Towards 10 million context length llm inference with kv cache quantization. arXiv   \n366 preprint arXiv:2401.18079, 2024.   \n367 Huang, K.-H., Zhou, M., Chan, H. P., Fung, Y. R., Wang, Z., Zhang, L., Chang, S.-F., and Ji, H.   \n368 Do lvlms understand charts? analyzing and correcting factual errors in chart captioning. arXiv   \n369 preprint arXiv:2312.10160, 2023a.   \n370 Huang, S., Dong, L., Wang, W., Hao, Y., Singhal, S., Ma, S., Lv, T., Cui, L., Mohammed, O. K., Liu,   \n371 Q., et al. Language is not all you need: Aligning perception with language models. arXiv preprint,   \n372 2023b.   \n373 Hudson, D. A. and Manning, C. D. Gqa: A new dataset for real-world visual reasoning and   \n374 compositional question answering. In Proceedings of the IEEE/CVF conference on computer   \n375 vision and pattern recognition, 2019.   \n376 Hwang, A., Head, A., and Callison-Burch, C. Grounded intuition of gpt-vision\u2019s abilities with   \n377 scientific images. arXiv preprint, 2023.   \n378 Jia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham, H., Le, Q., Sung, Y.-H., Li, Z., and Duerig,   \n379 T. Scaling up visual and vision-language representation learning with noisy text supervision. In   \n380 International conference on machine learning, 2021.   \n381 Keys, R. Cubic convolution interpolation for digital image processing. IEEE transactions on   \n382 acoustics, speech, and signal processing, 1981.   \n383 Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.-J.,   \n384 Shamma, D. A., et al. Visual genome: Connecting language and vision using crowdsourced dense   \n385 image annotations. International journal of computer vision, 2017.   \n386 Li, B., Zhang, Y., Chen, L., Wang, J., Yang, J., and Liu, Z. Otter: A multi-modal model with   \n387 in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023a.   \n388 Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping language-image pre-training with   \n389 frozen image encoders and large language models. arXiv preprint, 2023b.   \n390 Li, Y., Du, Y., Zhou, K., Wang, J., Zhao, W. X., and Wen, J.-R. Evaluating object hallucination in   \n391 large vision-language models. arXiv preprint arXiv:2305.10355, 2023c.   \n392 Li, Y., Zhang, C., Yu, G., Wang, Z., Fu, B., Lin, G., Shen, C., Chen, L., and Wei, Y. Sta  \n393 blellava: Enhanced visual instruction tuning with synthesized image-dialogue data. arXiv preprint   \n394 arXiv:2308.10253, 2023d.   \n395 Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll\u00e1r, P., and Zitnick, C. L.   \n396 Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European   \n397 Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, 2014.   \n398 Liu, H., Li, C., Li, Y., and Lee, Y. J. Improved baselines with visual instruction tuning. arXiv preprint   \n399 arXiv:2310.03744, 2023a.   \n400 Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. arXiv preprint, 2023b.   \n401 Liu, S., Zeng, Z., Ren, T., Li, F., Zhang, H., Yang, J., Li, C., Yang, J., Su, H., Zhu, J., et al. Grounding   \n402 dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint   \n403 arXiv:2303.05499, 2023c.   \n404 Lu, P., Bansal, H., Xia, T., Liu, J., Li, C., Hajishirzi, H., Cheng, H., Chang, K.-W., Galley, M., and   \n405 Gao, J. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts.   \n406 arXiv preprint arXiv:2310.02255, 2023.   \n407 Mao, J., Huang, J., Toshev, A., Camburu, O., Yuille, A. L., and Murphy, K. Generation and   \n408 comprehension of unambiguous object descriptions. In Proceedings of the IEEE conference on   \n409 computer vision and pattern recognition, 2016.   \n410 Masry, A., Long, D. X., Tan, J. Q., Joty, S., and Hoque, E. Chartqa: A benchmark for question   \n411 answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022.   \n412 OpenAI, R. Gpt-4 technical report. arxiv 2303.08774. View in Article, 2023a.   \n413 OpenAI, R. Gpt-4v(ision) system card. Citekey: gptvision., 2023b.   \n414 Ordonez, V., Kulkarni, G., and Berg, T. Im2text: Describing images using 1 million captioned   \n415 photographs. Advances in neural information processing systems, 2011.   \n416 Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. Bleu: a method for automatic evaluation of machine   \n417 translation. In Proceedings of the 40th annual meeting of the Association for Computational   \n418 Linguistics, 2002.   \n419 Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma, S., and Wei, F. Kosmos-2: Grounding   \n420 multimodal large language models to the world. arXiv preprint, 2023.   \n421 Schuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk, R., Mullis, C., Katta, A., Coombes, T.,   \n422 Jitsev, J., and Komatsuzaki, A. Laion-400m: Open dataset of clip-filtered 400 million image-text   \n423 pairs. arXiv preprint arXiv:2111.02114, 2021.   \n424 Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T.,   \n425 Katta, A., Mullis, C., Wortsman, M., et al. Laion-5b: An open large-scale dataset for training next   \n426 generation image-text models. Advances in Neural Information Processing Systems, 2022.   \n427 Sharma, P., Ding, N., Goodman, S., and Soricut, R. Conceptual captions: A cleaned, hypernymed,   \n428 image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting   \n429 of the Association for Computational Linguistics (Volume 1: Long Papers), 2018.   \n430 Shazeer, N. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.   \n431 Shrestha, R., Kafle, K., and Kanan, C. Answer them all! toward universal visual question answering   \n432 models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,   \n433 2019.   \n434 Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D., and Rohrbach, M.   \n435 Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision   \n436 and pattern recognition, 2019.   \n437 Sun, Q., Fang, Y., Wu, L., Wang, X., and Cao, Y. Eva-clip: Improved training techniques for clip at   \n438 scale. arXiv preprint arXiv:2303.15389, 2023a.   \n439 Sun, Q., Yu, Q., Cui, Y., Zhang, F., Zhang, X., Wang, Y., Gao, H., Liu, J., Huang, T., and Wang, X.   \n440 Generative pretraining in multimodality. arXiv preprint, 2023b.   \n441 Trinh, T. H., Wu, Y., Le, Q. V., He, H., and Luong, T. Solving olympiad geometry without human   \n442 demonstrations. Nature, 625(7995):476\u2013482, 2024.   \n443 Wang, J., Yang, Z., Hu, X., Li, L., Lin, K., Gan, Z., Liu, Z., Liu, C., and Wang, L. Git: A generative   \n444 image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100, 2022a.   \n445 Wang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou, J., and Yang, H. Ofa:   \n446 Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning   \n447 framework. 2022b.   \n448 Wang, W., Chen, Z., Chen, X., Wu, J., Zhu, X., Zeng, G., Luo, P., Lu, T., Zhou, J., Qiao, Y., et al.   \n449 Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. arXiv   \n450 preprint, 2023a.   \n451 Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao, L., Song, X., et al.   \n452 Cogvlm: Visual expert for pretrained language models. arXiv preprint, 2023b.   \n453 Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct:   \n454 Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560,   \n455 2022c.   \n456 Wu, P. and Xie, S. V\\*: Guided visual search as a core mechanism in multimodal llms. arXiv preprint   \n457 arXiv:2312.14135, 2023.   \n458 Xu, Z., Shen, Y., and Huang, L. Multiinstruct: Improving multi-modal zero-shot learning via   \n459 instruction tuning. arXiv preprint arXiv:2212.10773, 2022.   \n460 Yin, S., Fu, C., Zhao, S., Xu, T., Wang, H., Sui, D., Shen, Y., Li, K., Sun, X., and Chen, E.   \n461 Woodpecker: Hallucination correction for multimodal large language models. arXiv preprint   \n462 arXiv:2310.16045, 2023.   \n463 Yu, L., Poirson, P., Yang, S., Berg, A. C., and Berg, T. L. Modeling context in referring expressions. In   \n464 Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October   \n465 11-14, 2016, Proceedings, Part II 14, 2016.   \n466 Yu, Q., Li, J., Wei, L., Pang, L., Ye, W., Qin, B., Tang, S., Tian, Q., and Zhuang, Y. Hallucidoctor:   \n467 Mitigating hallucinatory toxicity in visual instruction data. arXiv preprint arXiv:2311.13614,   \n468 2023a.   \n469 Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang, X., and Wang, L. Mm-vet: Evaluating   \n470 large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023b.   \n471 Zeng, Y., Zhang, H., Zheng, J., Xia, J., Wei, G., Wei, Y., Zhang, Y., and Kong, T. What matters in   \n472 training a gpt4-style language model with multimodal inputs? arXiv preprint arXiv:2307.02469,   \n473 2023.   \n474 Zhang, S., Sun, P., Chen, S., Xiao, M., Shao, W., Zhang, W., Chen, K., and Luo, P. Gpt4roi:   \n475 Instruction tuning large language model on region-of-interest. arXiv preprint arXiv:2307.03601,   \n476 2023a.   \n477 Zhang, Y., Zhang, R., Gu, J., Zhou, Y., Lipka, N., Yang, D., and Sun, T. Llavar: Enhanced visual   \n478 instruction tuning for text-rich image understanding. arXiv preprint, 2023b. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "479 A Related Works ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "480 A.1 Large Vision-Langauge Models as Foundations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "481 Most of LVLMs rely on the training on publicly available image-caption pairs, including ALIGN (Jia   \n482 et al., 2021), MSCOCO (Lin et al., 2014), VG Krishna et al. (2017), CC3M Sharma et al. (2018),   \n483 CC12M (Changpinyo et al., 2021), SBU (Ordonez et al., 2011), LAION2B (Schuhmann et al., 2022),   \n484 LAION400M Schuhmann et al. (2021). Starting from Flamingo (Alayrac et al., 2022), a series of   \n485 LVLMs have focused on training the adaptation layers to align the visual representation to the frozen   \n486 LLMs on a mixture of image-text pairs with the above corpus, including BLIP2 Li et al. (2023b),   \n487 KOSMOS Huang et al. (2023b), and OpenFlamingo (Awadalla et al., 2023). Inspired by success of   \n488 instruction tuning in LLMs (Wang et al., 2022c), a line of works have devoted efforts to build vision  \n489 oriented instruction-answer pairs through GPT4 and train models for imitation, such as LLAVA (Liu   \n490 et al., 2023b), Otter (Li et al., 2023a), VisionLLM (Wang et al., 2023a), MultiInstruct (Xu et al.,   \n491 2022), Lynx (Zeng et al., 2023), InstructBLIP (Dai et al.), CleverFlamingo (Chen et al., 2023a) and   \n492 StableLLaVA (Li et al., 2023d). Recently, researchers have proven the efficiency of developing   \n493 LVLMs with two stages of training, the first stage of abundant pretraining on image-caption pairs and   \n494 the second stage of alignment on image-question-answer triples, such as PALI (Chen et al., 2022),   \n495 PaLI-X (Chen et al., 2023d), Qwen-VL (Bai et al., 2023), and CogVLM Wang et al. (2023b). ", "page_idx": 13}, {"type": "text", "text": "496 A.2 Large Vision-Language Models with Reasoning ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "497 To further enhance the ability of LVLMs in solving high-level visual problems, research focusing   \n498 on various aspects of reasoning is attracting broad attention. We simply divide existing studies into   \n499 tree broad categories. The first line of research focus on enhance train models with a mastery of   \n500 cross-modal grounded reasoning, where grounded instruction-following supervision is build through   \n501 public visual grounding dataset or GPT4-V for training, including KOSMOS-2 (Peng et al., 2023),   \n502 Shikra (Chen et al., 2023b), and GPT4ROI (Zhang et al., 2023a). The second aspect of efforts have   \n503 been devoted into promoting models to understand artificial visual scenes, such as figures, charts, and   \n504 receipts. These studies includes CogAgent (Hong et al., 2023) and CHARTVE (Huang et al., 2023a).   \n505 Some other studies address the crucial problem of hallucination in LVLMs with counterfactual or   \n506 interpretable reasoning (Yu et al., 2023a; Yin et al., 2023). $\\mathrm{\\DeltaV^{*}}$ (Wu & Xie, 2023) also contributes   \n507 efforts to enhance the details recognition of VLMs based the LLM-guided searching process. ", "page_idx": 13}, {"type": "text", "text": "508 B Limitation and Impact ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "509 Though we try to develop an accurate and robust framework that engages remarkable LLM to provide   \n510 basic solving steps, adopts reliable visual tools to obtain visual contents, and then acquires feasible   \n511 paths based on traversal, there are still limitations in our methodology that we hope to improve in the   \n512 future. First, We find that the diversity of linguistic solving steps is insufficient, and the inaccuracy of   \n513 visual tools (e.g., the rough granularity of grounding boxes, OCR failures on slant letters) will lead   \n514 to a large amount of negative paths (effectively utilizing these paths would beneficial). We suggest   \n515 to promote these limitations with dedicate prompts and improved visual tools. Second, our current   \n516 model re-input the manipulated images with a set of hard prompts, which may bring speed losses.   \n517 This is expected to be improved by implementing the physical manipuations into the calculations in   \n518 vector space. This work presents a general visual reasoning mechanism that alleviate the problems   \n519 caused by existing conclusion-alignment training for VLMs, introduces a data production framework   \n520 involving LLMs and visual tools as reliable annotators, and devises a memory-based compatible VLM   \n521 architecture. We expect this work to bring three beneftis to the community. First, the proposed visual   \n522 reasoning mechanism may push the progress of VLMs in solving complex visual problems. Second,   \n523 the introduced data production framework may be applied to widespread training scenarios to promote   \n524 the development of current data-driven machine learning. Third, we hope that the memory-based   \n525 architecture will be helpful for VLMs in multi-turn long contexts.   \n527 In this section, we further introduce the details of CoM data production, with the overall algorithm of   \n528 a pseudo code, an example of the solving steps generation with LLM and corresponding guideline, an   \n529 example of the reasoning chains completion with visual tools. We also list the details of data statistics   \n530 for the synthesised training data as well as the evaluation data of CoM-test, followed by a limitation   \n531 analysis for the current data production method. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "532 C.1 Algorithm for the Automate Data Generation Pipeline ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "533 We provide the pseudocode of the CoM synthesis algorithm to clearly explain the process of data   \n534 generation, thereby facilitating understanding and reproduction 1. ", "page_idx": 14}, {"type": "text", "text": "Algorithm 1 Synthesising Chain of Manipulations \uf8f1Manipulations : {fi : x \u2192y | fi \u2208M}   \n1: Define:\uf8f2Linguistic Annotator : \u03a8L //We use GPT4 in this work \uf8f3V isual Annotator : \u03a8V //We use PaddleOCR and GroundingDINO in this wor   \n2: Input: Image $I$ , Question $Q$ , Answer $A$   \n3: $//$ Linguistic Annotation   \n4: Prompt $\\Psi_{L}$ with guideline $P^{L}$ to generate reasoning steps: $\\varsigma=\\Psi_{L}(Q|P^{L}),\\quad w h e r e\\left\\{\\varsigma=(s t e p s_{1},s t e p s_{2},...)\\right.$ (3)   \n5: Define tree $\\tau$   \n6: for $i=1$ to $|\\varsigma|$ do   \n7: Extract $x_{i},y_{i}$ instantiated with $f_{i}$ in $s t e p_{i}$   \n8: Extract referential boxes $B$ from $x_{i}$   \n9: for $b$ in $B$ do   \n10: Leverage $\\Psi_{V}$ to acquire corresponding visual content $y_{i}^{\\prime}=\\Psi(x_{i}|I,b)$ , and apply $y_{i}$ to tree $\\mathcal{T}.l e v e l[i].a p p e n d(y_{i})$ (4)   \n11: end for   \n12: end for   \n13: Traverse $\\tau$ to obtain positive chains that leads to given answer with terminal return $\\left[\\varsigma_{1},\\varsigma_{2},...\\right]=D F S(\\mathcal{T}|A)$ (5)   \n14: Return $[\\varsigma_{1},\\varsigma_{2},...]$ ", "page_idx": 14}, {"type": "text", "text": "535 C.2 The CoM-test Benchmark and Evaluation Metric ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "536 To measure the correctness of CoM chains, we introduce a keypoints-aware metric. The intuition   \n537 is that we care about the key elements including actions (i.e., manipulation name), targets (i.e.,   \n538 manipulation input), and visual contents (i.e., manipulation returns) of each step in the path, as well   \n539 as the logical execution order of manipulations. Given a pair of chain-answer annotation $(c,a)$ and   \n540 corresponding model prediction $(c^{\\prime},a^{\\prime})$ , we first sequentially extract the key elements from $c$ and $c^{\\prime}$   \n541 to construct two ordered lists, and then replace the elements in the lists with their fixed indices in a   \n542 Bag-of-Elements $\\mathcal{E}=c\\cup c^{\\prime}$ to result in lists of $k$ and $k^{\\prime}$ . We thus calculate the score as the normalized   \n543 Levenshtein Distance $s_{c}=L e v e n s h t e i n(k,k^{\\prime})/N$ between the two lists, where $N$ is the maximum   \n544 length between $k$ and $k^{\\prime}$ . We adopt this simple discretization strategy with low time complexity   \n545 to concentrate on the key points as well as the solving order. We further consider the linguistic   \n546 matching of paragraphs by calculating the BLEU (Papineni et al., 2002) score between two chains   \n547 $s_{p}=\\mathrm{BLEU}(c,c^{\\prime})$ , and the final sore is a weighted combination as $a c c=(0.6\\times s_{c}+0.4\\times s_{p})/2$ . ", "page_idx": 14}, {"type": "text", "text": "548 C.3 Data Statistics ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "549 We develop a strategy to extract predicate phrases based constituency parsing with StandordCoreNLP,   \n550 in which we extract verb, conjunction-connected verb phrase, preposition-connected verb phrase.   \n551 Besides the standard CoM data incorporating manipulations with explicit visual evidences, the   \n552 proposed data synthesising framework is compatible of producing implicit visual reasoning steps   \n553 $s t e p_{i}^{\\prime}=(d e s c_{i})$ without involving the manipulations. We thereby also build this partial CoM data on   \n554 the corpus consisting of absurd visual questions (i.e., asking unanswerable questions based on the   \n555 given image) to further resist the toxic hallucinations. Specifically, given an image $I$ with a question   \n556 $Q$ ,we prompt GPT-4V (OpenAI, 2023b) to solve the question step-by-step to acquire the reasoning   \n557 chains. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "table", "img_path": "k0qTnbQxzR/tmp/438ab2be3d77430a6abb9ddc01a5728f44e483eaf1144b6dbb1b6bb42b3aec8f.jpg", "table_caption": [], "table_footnote": ["Table 4: Detailed statistics the the training data and evaluation data synthesised with CoM production. "], "page_idx": 15}, {"type": "image", "img_path": "k0qTnbQxzR/tmp/e4bd3df5108faa3628e8c3280817dfeb8fc06e03e4dd88774df5c348dfdd40e5.jpg", "img_caption": ["Figure 7: Distribution of the top-50 generated manipulations out of total 465 based on 4-shot prompting, where the first three bars are scaled with $20\\%$ for a smooth visualization of all data. "], "img_footnote": [], "page_idx": 15}, {"type": "image", "img_path": "k0qTnbQxzR/tmp/c7c91a057819aae4a1e0f5414c3c1b6e7c184b8624863175581868ec3bdf2db0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "k0qTnbQxzR/tmp/4917b195d1867da52b4228bea02e760a28a4cf7c1f318b4ffb72d57746a470d0.jpg", "img_caption": ["Figure 8: An example shows the configuration, inputs, outputs of the linguistic annotation and visual annotation. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "558 C.4 Details of the Linguistic/Visual Annotations ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "559 In this work, we adopt the GPT4-turbo as the linguistic annotator for generating problems solving   \n560 steps, and the API call was conducted during the period of 2023.9 - 2023.12. For the visual annotators,   \n561 we leverage the the currently best-performing tools, GroundingDINO and PaddleOCR, to acquire all   \n562 visual contents requested by the manipulations. For a clear description to the production setting and   \n563 results, we illustrate the guiding prompt, and an example-based linguistic annotation results as well   \n564 as the visual annotation results in Figure 8. ", "page_idx": 16}, {"type": "text", "text": "565 C.5 Limitation Analysis for the Data Production ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "566 For the implemented data framework, we engage the remarkable LLM to provide basic solving steps,   \n567 adopt two reliable visual tools (i.e., GroundingDINO and PaddleOCR) to acquire corresponding   \n568 visual contents, and then perform the traversal to achieve feasible reasoning paths, which ensures the   \n569 correctness and robustness of data synthesizing. However, we also find that there are three major   \n570 limitations caused by the employed models and could be improved in future:   \n571 \u2022 The lack of diversity in linguistic reasoning steps. The 5-shot prompting to the GPT-4 gains   \n572 a stable solving steps, but it also results in the descriptions for executing manipulations or   \n573 general thinking are similar. We suggest that this can be addressed by employing diversified   \n574 prompts or requirements.   \n575 \u2022 The inaccuracy of visual tools. We find that there are a considerable amount of negative   \n576 paths caused by the failures of visual tools, such as the rough granularity of bounding boxes   \n577 and the error recognition of slated letters or long sentences. This issue can be relieved by   \n578 improving the semantic understanding capabilities of visual tools. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "579 D Details of Training ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "580 D.1 Launching Prompts ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "581 \u2022 Please solve the problem gradually via a chain of manipulations, where in each   \n582 step you can selectively adopt one of the following manipulations GROUNDING(a   \n583 phrase) $\\rightarrow$ boxes, OCR(an image or a region) $\\rightarrow$ texts, CROP_AND_ZOOMIN(a region on   \n584 given image) $\\rightarrow$ new_image, CALCULATE(a computable target) $\\rightarrow$ numbers, or invent a new   \n585 manipulation, if that seems helpful. {QUESTION}   \n586 \u2022 Please tackle a given question in a stepbystep manner. For each step one of the following   \n587 manipulations (depicted as Name(Input) $\\rightarrow$ Retrun) can be optionally used: GROUNDING(a   \n588 phrase) $\\rightarrow$ boxes, OCR(an image or a region) $\\rightarrow$ texts, CROP_AND_ZOOMIN(a region on   \n589 given image) $\\rightarrow$ new_image, CALCULATE(a computable target) $\\rightarrow$ numbers, or develop a   \n590 new manipulation yourself (if it is indeed required). {QUESTION}   \n591 \u2022 Please go through the question incrementally with chain of manipulations (optionally use   \n592 manipulation when needed) such as GROUNDING(a phrase) $\\rightarrow$ boxes, OCR(an image or   \n593 a region) $\\rightarrow$ texts, CROP_AND_ZOOMIN(a region on given image) $\\rightarrow$ new_image, CAL  \n594 CULATE(a computable target) $\\rightarrow$ numbers, and create a new manipulation if necessary.   \n595 {QUESTION} ", "page_idx": 17}, {"type": "text", "text": "596 D.2 Training settings ", "text_level": 1, "page_idx": 17}, {"type": "table", "img_path": "k0qTnbQxzR/tmp/3a304a8bd93fbd8ca20804879f5b0f84dce19933d3460b2a0b9301e303768867.jpg", "table_caption": [], "table_footnote": ["Table 5: Training details of all stages. "], "page_idx": 17}, {"type": "text", "text": "597 E Details of Qualitative Analysis ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "598 E.1 Qualitative Analysis ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "599 We investigate the evidential reasoning capability of CogCoM on scenarios that requires different   \n600 types of meticulous reasoning, including recognizing textual details, reading time, understanding   \n601 charts and counting objects. The results are shown in Figure 1. The first case demonstrates that   \n602 CogCoM finds the region corresponding to the plane logo through two steps of grounding and then   \n603 achieves the answer based on zooming in the cropped region. The second case illustrates the ability   \n604 of CogCoM in reading time, by locating the device that displays time and then transforming the time   \n605 into words based on the read_timne manipulation. In the forth example, CogCoM first identifies all   \n606 visible truck wheels through grounding, and then leverages the counting manipulation to consolidate   \n607 the total number. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "image", "img_path": "k0qTnbQxzR/tmp/da4ab14c373664b34edae4eb6669a17d6318cf53f7d6bb0289a1283aa12e782f.jpg", "img_caption": ["608 E.2 Representative Cases "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "CoM: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Given a question, please solve the question step-by-step with a chain of manipulations, where in each step you can optionally use one of the following manipulations (defined as Name(Input)->Return) on the image: GROUNDING(a phrase)->boxes, OCR(an image or a region)->texts, CROP_AND_ZOOMIN(a region on given image)->new_image, CALCULATE(a computable target)->numbers, or a new manipulation created by yourself (if it is indeed needed). ", "page_idx": 18}, {"type": "text", "text": "Question: What is written on the traffic sign near the bus? ", "page_idx": 18}, {"type": "text", "text": "Model:     Using GROUNDING(the traffic sign near the bus) to locate the traffic sign near the bus in current image, which is [[084,157,926,703;676,146,740,273]]. Utilizing OCR(texts in region [[084,157,926,703]]) to read the texts written on the traffic sign within region [[084,157,926,703] resulting Taxi. So ultimately, the conclusive answer to the question in discussion is Taxi. ", "page_idx": 18}, {"type": "image", "img_path": "k0qTnbQxzR/tmp/235b56b5efd5408c094ce2b58834288097c579e573767dfc4a0000d3b6fd1369.jpg", "img_caption": [], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Visual Grounding: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Find the region in image that \"a traffic sign\" describes. Model:     [[678,145,739,270] ", "page_idx": 18}, {"type": "text", "text": "Grounded Captioning: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: The goal of this task is to generate description for one part of the image. The part is specified by [[678,145,739,270]]. Model:     A no tax sign ", "page_idx": 18}, {"type": "text", "text": "Image Captioning: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: In this task, you are given an image and you will need to generate some text to describe it. Please give me a concise answer.   \nAnswer:   Taxi bus with rainbow colors on side. ", "page_idx": 18}, {"type": "text", "text": "Multi Choice: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Select the color that does not appear in the rainbow sign of this bus. (1) red. (2) yellow. (3) pink. (4) black. Please give me a direct answer. Answer:   black ", "page_idx": 18}, {"type": "text", "text": "Detailed Captioning: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Explain the visual content of the image in great detail. Answer:   The image captures a scene on a city street, where a white bus with red and blue accents is parked. The bus bears the text \"Trafalgar\" in bold red letters across its side, indicating it's likely part of a tourist or travel company. The bus is positioned next to a traffic sign that reads \"TAXI\", suggesting it might be waiting for passengers or ready to embark on a journey. In the background, there are buildings and trees, adding depth to the urban landscape. The overall impression is one of a typical day in an urban setting, with a bus waiting for its passengers amidst the hustle and bustle of city life. ", "page_idx": 18}, {"type": "text", "text": "Figure 9: CogCoM demonstrates the flexible capabilities for adapting to different multimodal scenarios, including evidential visual reasoning, Visual Grounding, Grounded Captioning, Image Captioning, Multi Choice, and Detailed Captioning. ", "page_idx": 18}, {"type": "text", "text": "609 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "610 1. Claims   \n611 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n612 paper\u2019s contributions and scope?   \n613 Answer: [Yes]   \n614 Justification: Please refer to Line 6 to Line 20, and Line 48 to Line 72.   \n615 Guidelines:   \n616 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n617 made in the paper.   \n618 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n619 contributions made in the paper and important assumptions and limitations. A No or   \n620 NA answer to this question will not be perceived well by the reviewers.   \n621 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n622 much the results can be expected to generalize to other settings.   \n623 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n624 are not attained by the paper.   \n625 2. Limitations   \n626 Question: Does the paper discuss the limitations of the work performed by the authors?   \n627 Answer: [Yes]   \n628 Justification: Please refer to Appendix B.   \n629 Guidelines:   \n630 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n631 the paper has limitations, but those are not discussed in the paper.   \n632 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n633 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n634 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n635 model well-specification, asymptotic approximations only holding locally). The authors   \n636 should reflect on how these assumptions might be violated in practice and what the   \n637 implications would be.   \n638 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n639 only tested on a few datasets or with a few runs. In general, empirical results often   \n640 depend on implicit assumptions, which should be articulated.   \n641 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n642 For example, a facial recognition algorithm may perform poorly when image resolution   \n643 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n644 used reliably to provide closed captions for online lectures because it fails to handle   \n645 technical jargon.   \n646 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n647 and how they scale with dataset size.   \n648 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n649 address problems of privacy and fairness.   \n650 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n651 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n652 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n653 judgment and recognize that individual actions in favor of transparency play an impor  \n654 tant role in developing norms that preserve the integrity of the community. Reviewers   \n655 will be specifically instructed to not penalize honesty concerning limitations.   \n656 3. Theory Assumptions and Proofs   \n657 Question: For each theoretical result, does the paper provide the full set of assumptions and ", "page_idx": 19}, {"type": "text", "text": "58 a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 19}, {"type": "text", "text": "Justification: Please refer to Line 74 to Line 92 for pilot experiments. Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 20}, {"type": "text", "text": "672 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The data collection with Section 3, model training process with Section 4, and the experimental settings with Section 5. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "715 Answer: [Yes]   \n716 Justification: We will open-source the code, model weights, and all collected data. The   \n717 generation process and statistics of data are available at Section 3 and Appendix C.   \n718 Guidelines:   \n719 \u2022 The answer NA means that paper does not include experiments requiring code.   \n720 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n721 public/guides/CodeSubmissionPolicy) for more details.   \n722 \u2022 While we encourage the release of code and data, we understand that this might not be   \n723 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n724 including code, unless this is central to the contribution (e.g., for a new open-source   \n725 benchmark).   \n726 \u2022 The instructions should contain the exact command and environment needed to run to   \n727 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n728 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n729 \u2022 The authors should provide instructions on data access and preparation, including how   \n730 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n731 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n732 proposed method and baselines. If only a subset of experiments are reproducible, they   \n733 should state which ones are omitted from the script and why.   \n734 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n735 versions (if applicable).   \n736 \u2022 Providing as much information as possible in supplemental material (appended to the   \n737 paper) is recommended, but including URLs to data and code is permitted.   \n738 6. Experimental Setting/Details   \n739 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n740 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n741 results?   \n742 Answer: [Yes]   \n743 Justification: The experimental settings are listed in each subsection of benchmark eval  \n744 uation, which is Section 5.1, Section 5.2 and Section 5.3. The training settings with   \n745 hyperparameters and optimizations are listed at Appendix D.2.   \n746 Guidelines:   \n747 \u2022 The answer NA means that the paper does not include experiments.   \n748 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n749 that is necessary to appreciate the results and make sense of them.   \n750 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n751 material.   \n752 7. Experiment Statistical Significance   \n753 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n754 information about the statistical significance of the experiments?   \n755 Answer: [Yes]   \n756 Justification: We include the error analysis with textual descriptions in Appendix C.5, as   \n757 well as the reasoning accuracy with limitations in Section 5.1.2.   \n758 Guidelines:   \n759 \u2022 The answer NA means that the paper does not include experiments.   \n760 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n761 dence intervals, or statistical significance tests, at least for the experiments that support   \n762 the main claims of the paper.   \n763 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n764 example, train/test split, initialization, random drawing of some parameter, or overall   \n765 run with given experimental conditions).   \n766 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n767 call to a library function, bootstrap, etc.)   \n768 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n769 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n770 of the mean.   \n771 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n772 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n773 of Normality of errors is not verified.   \n774 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n775 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n776 error rates).   \n777 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n778 they were calculated and reference the corresponding figures or tables in the text.   \n779 8. Experiments Compute Resources   \n780 Question: For each experiment, does the paper provide sufficient information on the com  \n781 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n782 the experiments?   \n783 Answer: [Yes]   \n784 Justification: Detailed compute resources are listed in Section D.2.   \n785 Guidelines:   \n786 \u2022 The answer NA means that the paper does not include experiments.   \n787 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n788 or cloud provider, including relevant memory and storage.   \n789 \u2022 The paper should provide the amount of compute required for each of the individual   \n790 experimental runs as well as estimate the total compute.   \n791 \u2022 The paper should disclose whether the full research project required more compute   \n792 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n793 didn\u2019t make it into the paper).   \n794 9. Code Of Ethics   \n795 Question: Does the research conducted in the paper conform, in every respect, with the   \n796 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n797 Answer: [Yes]   \n798 Justification: All code anonymity.   \n799 Guidelines:   \n800 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n801 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n802 deviation from the Code of Ethics.   \n803 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n804 eration due to laws or regulations in their jurisdiction).   \n805 10. Broader Impacts   \n806 Question: Does the paper discuss both potential positive societal impacts and negative   \n807 societal impacts of the work performed?   \n808 Answer: [Yes]   \n809 Justification: We discussed the societal impacts in details in Appendix B.   \n810 Guidelines:   \n811 \u2022 The answer NA means that there is no societal impact of the work performed.   \n812 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n813 impact or why the paper does not address societal impact.   \n814 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n815 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n816 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n817 groups), privacy considerations, and security considerations.   \n818 \u2022 The conference expects that many papers will be foundational research and not tied   \n819 to particular applications, let alone deployments. However, if there is a direct path to   \n820 any negative applications, the authors should point it out. For example, it is legitimate   \n821 to point out that an improvement in the quality of generative models could be used to   \n822 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n823 that a generic algorithm for optimizing neural networks could enable people to train   \n824 models that generate Deepfakes faster.   \n825 \u2022 The authors should consider possible harms that could arise when the technology is   \n826 being used as intended and functioning correctly, harms that could arise when the   \n827 technology is being used as intended but gives incorrect results, and harms following   \n828 from (intentional or unintentional) misuse of the technology.   \n829 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n830 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n831 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n832 feedback over time, improving the efficiency and accessibility of ML).   \n833 11. Safeguards   \n834 Question: Does the paper describe safeguards that have been put in place for responsible   \n835 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n836 image generators, or scraped datasets)?   \n837 Answer: [NA]   \n838 Justification: The training data collected from public datasets with research purpose does   \n839 not face the safety risks.   \n840 Guidelines:   \n841 \u2022 The answer NA means that the paper poses no such risks.   \n842 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n843 necessary safeguards to allow for controlled use of the model, for example by requiring   \n844 that users adhere to usage guidelines or restrictions to access the model or implementing   \n845 safety filters.   \n846 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n847 should describe how they avoided releasing unsafe images.   \n848 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n849 not require this, but we encourage authors to take this into account and make a best   \n850 faith effort.   \n851 12. Licenses for existing assets   \n852 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n853 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n854 properly respected?   \n855 Answer: [Yes]   \n856 Justification: The public available datasets used in this paper are cited properly.   \n857 Guidelines:   \n858 \u2022 The answer NA means that the paper does not use existing assets.   \n859 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n860 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n861 URL.   \n862 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n863 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n864 service of that source should be provided.   \n865 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n866 package should be provided. For popular datasets, paperswithcode.com/datasets   \n867 has curated licenses for some datasets. Their licensing guide can help determine the   \n868 license of a dataset.   \n869 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n870 the derived asset (if it has changed) should be provided.   \n871 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n872 the asset\u2019s creators.   \n873 13. New Assets   \n874 Question: Are new assets introduced in the paper well documented and is the documentation   \n875 provided alongside the assets?   \n876 Answer: [Yes]   \n877 Justification: The newly created benchmark CoM-test is documented in detailed in Section   \n878 5 and Appendix C.2, and the manually annotated math data is described in Section 3.2.   \n879 Guidelines:   \n880 \u2022 The answer NA means that the paper does not release new assets.   \n881 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n882 submissions via structured templates. This includes details about training, license,   \n883 limitations, etc.   \n884 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n885 asset is used.   \n886 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n887 create an anonymized URL or include an anonymized zip file.   \n888 14. Crowdsourcing and Research with Human Subjects   \n889 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n890 include the full text of instructions given to participants and screenshots, if applicable, as   \n891 well as details about compensation (if any)?   \n892 Answer: [NA]   \n893 Justification: This study does not involve crowdsourcing nor research with human subjects.   \n894 Guidelines:   \n895 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n896 human subjects.   \n897 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n898 tion of the paper involves human subjects, then as much detail as possible should be   \n899 included in the main paper.   \n900 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n901 or other labor should be paid at least the minimum wage in the country of the data   \n902 collector.   \n903 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n904 Subjects   \n905 Question: Does the paper describe potential risks incurred by study participants, whether   \n906 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n907 approvals (or an equivalent approval/review based on the requirements of your country or   \n908 institution) were obtained?   \n909 Answer: [NA]   \n910 Justification: This study does not involve crowdsourcing nor research with human subjects.   \n911 Guidelines:   \n912 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n913 human subjects.   \n14 \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)   \n915 may be required for any human subjects research. If you obtained IRB approval, you   \n916 should clearly state this in the paper.   \n917 \u2022 We recognize that the procedures for this may vary significantly between institutions   \n918 and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the   \n919 guidelines for their institution.   \n920 \u2022 For initial submissions, do not include any information that would break anonymity (if   \n21 applicable), such as the institution conducting the review. ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}]