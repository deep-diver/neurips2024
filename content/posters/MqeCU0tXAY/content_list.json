[{"type": "text", "text": "CLIPCEIL: Domain Generalization through CLIP via Channel rEfinement and Image-text aLignment ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Xi Yu, Shinjae Yoo, Yuewei Lin\u2217 Artificial Intelligence Department, Computing and Data Science Directorate Brookhaven National Laboratory, Upton, NY 11973 {xyu1; sjyoo; ywlin}@bnl.gov ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Domain generalization (DG) is a fundamental yet challenging topic in machine learning. Recently, the remarkable zero-shot capabilities of the large pre-trained vision-language model (e.g., CLIP) have made it popular for various downstream tasks. However, the effectiveness of this capacity often degrades when there are shifts in data distribution during testing compared to the training data. In this paper, we propose a novel method, known as CLIPCEIL, a model that utilizes Channel rEfinement and Image-text aLignment to facilitate the CLIP to the inaccessible out-of-distribution test datasets that exhibit domain shifts. Specifically, we refine the feature channels in the visual domain to ensure they contain domain-invariant and class-relevant features by using a lightweight adapter. This is achieved by minimizing the inter-domain variance while maximizing the inter-class variance. In the meantime, we ensure the image-text alignment by aligning text embeddings of the class descriptions and their corresponding image embedding while further removing the domain-specific features. Moreover, our model integrates multi-scale CLIP features by utilizing a self-attention fusion module, technically implemented through one Transformer layer. Extensive experiments on five widely used benchmark datasets demonstrate that CLIPCEIL outperforms the existing state-of-the-art methods. The source code is available at https://github.com/yuxi120407/CLIPCEIL. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Machine learning models inevitably face the challenge of out-of-distribution (OOD) generalization when encountering new tasks with different distributions from the training data. To mitigate this issue, extensive research has been dedicated to domain generalization (DG) [66], aiming to utilize knowledge from source domains to enhance the model\u2019s generalizability to the test dataset with domain shifts. ", "page_idx": 0}, {"type": "text", "text": "Recently, the spotlight has been on advancements in Vision-language models (VLMs), like CLIP [41], which are trained on web-scale image-language pairs containing a diverse range of domains and concepts from an open world, exhibit exceptional zero-shot learning and transferability to various downstream tasks [26, 31, 33, 41, 65]. However, despite their impressive zero-shot performance, supervised fine-tuning on task-specific datasets remains essential for further improving performance on downstream tasks. However, recent works [27, 55] have pointed out that fine-tuning degrades the CLIP\u2019s generalizability on the out-of-distribution test datasets exhibiting domain shift. To tackle this challenge, various methodologies have been proposed. For instance, CoOp [68] and CoCoOp [67] models utilized the prompt learning, DPL [62] learned a lightweight prompt generator, while WiSEFT [55] combined the original zero-shot and fine-tuned models. More recently, CLIPood [44] achieved state-of-the-art performance by employing the beta moving average and margin metric softmax to fine-tune the CLIP. It is noteworthy that these approaches do not explicitly guide the model to learn domain-invariant features, potentially capturing some domain-related information. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "One prominent trend in Domain Generalization (DG) involves acquiring domain-invariant features across variance of source domains [28, 32, 19, 21, 9], as it has been demonstrated that feature representations are general and transferable to different domains if they remain invariant across domains [3]. Intuitively, the domain invariant features are intrinsic to the class while remaining insensitive to the domain changes. However, as shown in Figure 1 (a), many CLIP visual feature channels exhibit unstable activations across domains (illustrated by the blue histogram), indicating a lack of domain invariance. Similarly, as ", "page_idx": 1}, {"type": "image", "img_path": "MqeCU0tXAY/tmp/8f99f96b6f4b7c0256a76d3f1e45e70cbe07fc02340042c78cab6f90617a3dd3.jpg", "img_caption": ["Figure 1: The feature channel sensitivity to domain and class shifts are quantified through employing the histogram of their standard deviations across different domains and classes. We analyze CLIP\u2019s image embeddings using the ViT-B/16 backbone on OfficeHome [52] dataset. For each channel, the average outputs are computed across all samples from each domain/class, and the standard deviations are calculated on domain/class dimension. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "shown in Figure 1 (b), many CLIP visual feature channels show insensitivity, and thus indiscriminative, to class variations. These observations prompt the question: ", "page_idx": 1}, {"type": "text", "text": "Can we enhance the pre-trained model\u2019s generalizability by excluding domain-specific (sensitive) and class-irrelevant (insensitive) features? ", "page_idx": 1}, {"type": "text", "text": "To answer it, we conduct a simple experiment using the pre-trained CLIP model on OfficeHome dataset. Given the original 512 CLIP visual feature channels, we select the ones with low domain variance and high class variance. We calculate the variance to different domains $(V_{d})$ and classes $(V_{c})$ for each feature channel, and then utilize a criterion $J=V_{d}-V_{c}$ to select the top- $Q$ $Q=400)$ ) channels with the smallest values. Assuming effective alignment of visual-language features in CLIP, we use the same $Q$ channels for text features. During inference, we simply use the inner product of the visual and text feature vectors, akin to the approach used in CLIP zero-shot [41]. As shown in Table 1, the simple feature channel selection improves the CLIP zero-shot generalizability. ", "page_idx": 1}, {"type": "text", "text": "Motivated by the above observations, we propose CLIPCEIL, a simple yet effective method aimed at promoting domaininvariant and class-relevant information within CLIP visual features from the perspective of feature channels. Specifically, we freeze the CLIP visual and text encoders and exclusively train a lightweight adapter ", "page_idx": 1}, {"type": "table", "img_path": "MqeCU0tXAY/tmp/e4adcaccb3f765c7e2e66443acf1c4bf448ae5afe8c4f46ab46264cc6c809634.jpg", "table_caption": ["Table 1: Comparison of channel selection $\\langle Q=400\\rangle$ with the CLIP zero-shot on Office Home benchmark "], "table_footnote": [], "page_idx": 1}, {"type": "text", "text": "for visual features, which fuses the multi-scale features, while minimizing the inter-domain variance and maximizing the inter-class variance. Furthermore, we establish alignment between image and text spaces by ensuring the consistency of direction among different classes in both the image and text domains. Our contributions are summarized as follows. ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose to adapt CLIP through Channel rEfinement and Image-text aLignment (CLIPCEIL), ensuring the visual feature channels contain the domain-invariant and classrelevant information while preserving the image-text alignment.   \n\u2022 Our model integrates multi-scale CLIP features by using self-attention mechanism, technically implemented through one Transformer layer.   \n\u2022 We comprehensively evaluate our proposed method on five benchmark datasets. The results demonstrate that our method achieves state-of-the-art performance. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Vision-Language Models (VLM). The VLMs aim to link images and texts by embedding them into a shared space for cross-model learning [45, 12]. Recently, equipped with advanced architecture (e.g., Transformer [51]) and trained on huge web-scale image-text pairs, the VLMs have attracted significant attention and demonstrated superior performance on various downstream tasks like image classification, segmentation, object detection, and image-text retrieval. For instance, CLIP [41] pre-trained on 400M image-text pairs using contrastive loss, demonstrates outstanding zero-shot prediction capability. ALIGN [22], trained on 1.8B noisy image-text pairs with noise-robust contrastive learning, ImageBERT [39], pre-trained on four tasks simultaneously, achieving superior image-text retrieval performance. SLIP [36] incorporates self-supervision into contrastive learning, leading to more efficient pre-training. BLIP [30] and BLIP-2 [29] employ joint optimization with three objectives, achieving state-of-the-art performance on a wide range of vision-language tasks. Instead of developing a new pre-trained model, our work aims to leverage CLIP to enhance domain generalization performance. ", "page_idx": 2}, {"type": "text", "text": "Domain Generalization (DG). DG aims to train a model that generalizes well to the out-of-distribution test (target) domains, solely training on source domains. One typical way is domain augmentation, which either diversifies the source domain or simulates the inaccessible test (target) domain conditions like domain randomization [25, 47, 18, 20], adversarial data augmentation [53, 64, 58] and data generation [46, 43, 57, 40, 23, 69]. Alternatively, methods focus on the learning strategies, including ensemble learning [42] and meta-learning [32]. Another prevalent approach is representation learning, aiming to capture the domain-invariant representations on source domains. [60] extracts the invariant semantic features by jointly learning the semantic and variation encoders. [37] learned style-invariant representation by reducing the intrinsic style from the class categories through the style-agnostic networks. [5] first disentangled the latent representations in domain-specific and domain-invariant and then concatenated them to make final decisions. Similarly, [59] proposed the information theory inspired disentanglement and purification loss functions to explicitly disentangle the latent feature in class-relevant and class-irrelevant components. Most recently, DomainDrop [17] dropped domain-specific channels during training by using additional domain discriminator networks. ", "page_idx": 2}, {"type": "text", "text": "In recent years, research has focused on enhancing the generalization of VLMs, like CLIP. Some studies learn the task-specific prompts [68, 67, 62], while others utilize the ensemble learning [55] or adapter learning [14, 61]. Despite the superior performance of large pre-trained VLMs, they still struggle with out-of-distribution (OOD) generalization. Efforts have been made to enhance their generalizability, e.g., StyLIP [4] and DPL [68] proposed the prompt learning approach for domain generalization. VL2V-SD [1] improved the OOD generalization of the VLM by visual-text alignment and visual encoder distillation. More recently, approaches like inference-time fine-tuning [63] or finetuning the entire visual encoder [35, 44] have been explored to further improve model generalizability. However, the former incurs an additional computational burden during inference, while the latter faces significant computational and storage challenges, requiring a full CLIP-sized model for each task. In contrast, our proposed model, once trained, does not require additional adaptation during inference, and we only need to store a lightweight model for each task. ", "page_idx": 2}, {"type": "text", "text": "3 Methods ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Problem Setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This paper aims to improve the out-of-distribution generalization through the pre-trained VLM. Let $\\boldsymbol{\\chi}\\subset\\overset{\\cdot}{\\mathbb{R}}^{d}$ be the image space and $y\\subset\\mathbb{R}$ the class label space. A domain consists of data sampled from a joint distribution $P_{X Y}$ on $\\mathcal X\\times\\mathcal Y$ . In the context of domain generalization, we have $K$ labeled training (source) domains $\\{\\mathcal{D}_{s}^{k}=\\{(x_{i}^{k},y_{i}^{k})\\}_{i=1}^{n_{k}}\\}_{k=1}^{K}$ , where $n_{k}$ is the number of samples in the $k^{\\mathrm{th}}$ domain, and each domain $\\mathcal{D}_{s}^{k}$ associated with a joint distribution $P_{X Y}^{k}$ . Note that each domain has a different joint distribution: $P_{X Y}^{i}\\neq P_{X Y}^{j},1\\leq i\\neq j\\leq K$ . The goal of domain generalization is to train a model $f:\\mathcal{X}\\to\\mathcal{Y}$ from $K$ training domain $\\mathcal{D}_{s}$ and achieve good generalization on an out-of-distribution inaccessible test (target) domain $\\mathcal{D}_{t}\\;=\\;\\{(x_{i}^{t},y_{i}^{t})\\}_{i=1}^{n_{t}^{-}}$ , where $y^{t}\\in\\mathcal{V}$ , and $P_{X Y}^{t e s t}\\neq P_{X Y}^{i}$ for $i\\in\\{1,...,K\\}$ . ", "page_idx": 2}, {"type": "image", "img_path": "MqeCU0tXAY/tmp/ff168c927514ec4c1191278cc3ad47a5cb1c227bb10b8ca128195f38356539e4.jpg", "img_caption": ["Figure 2: An overview of the proposed framework. We fixed the CLIP visual encoder $I$ and text encoder $T$ and trained a lightweight adapter $g$ during the training. The channel refinement ensures each feature channel contains domain-invariant (minimizing domain variance) and class-relevant (maximizing class variance) information. To further align the image and text, we maximize the image-text similarity and minimize direction loss with the help of text class descriptions based on data pairs from different classes and domains. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.2 Framework Overview ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The overview of our framework is illustrated in Figure. 2, which consists of three primary components. The first one is the lightweight adapter, depicted in the orange block of Figure 2. It fuses the multiscale CLIP visual features and maps them to a latent feature space, aiming to enhance the model\u2019s generalizability. The second component is visual channel refinement, which aims to ensure the visual features contain domain-invariant and class-relevant features. As observed from Figure 1, CLIP\u2019s visual features have numerous channels that exhibit sensitivity to domain variations, which are essentially domain-specific features, as well as channels that exhibit insensitivity to class variations, which are essentially class-irrelevant features. In the context of domain generalization, it is argued that both features are often redundant and may hinder the model\u2019s generalizability. Our framework aim to eliminate these undesirable features by minimizing the feature variance across domains and maximizing feature variance across classes. The third one is the image-text alignment component. The feature channel refinement module, working solely in the image space, has the potential to disrupt the well-aligned image-text feature space from CLIP. Therefore, realigning the image and text spaces becomes necessary. Specifically, we introduce the direction loss to minimize the difference between the direction of two image features and that of their corresponding textual features. We describe each component of our framework thoroughly in the subsequent sections. ", "page_idx": 3}, {"type": "text", "text": "3.3 Adapter $g$ ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A CLIP\u2019s visual encoder consists of several vision transformer layers and a final project layer, as depicted in blue block in Figure 3. Let $I$ denote the visual encoder within CLIP. Given an image $\\mathbf{x}$ , its visual features in CLIP are represented as $I(\\mathbf{x})=[\\{f_{\\mathbf{x}}^{l}\\}_{l=1}^{L};f_{\\mathbf{x}}^{f i n a l}]$ . Here, $f_{\\mathbf{x}}^{l}\\in\\mathbb{R}^{d}$ signifies the feature map derived from the [cls] token in the $l^{\\mathrm{th}}$ layer, with a dimension of $d$ , where $L$ stands for the number of transformer layers. Additionally, $f_{\\mathbf{x}}^{f i n a l}\\in\\mathbb{R}^{D}$ represents the ultimate output of CLIP\u2019s visual encoder, obtained by passing the feature map of the last layer $f_{\\mathbf{x}}^{L}$ through an inherent MLP projector. In this paper, we use ViT-B/16 as the visual encoder backbone with the number of transformer layers $L=12$ , the feature dimensions $d=768$ and $D=512$ . ", "page_idx": 3}, {"type": "text", "text": "We aim to enhance the visual features\u2019 resilience to the domain shifts. Therefore, we propose a lightweight adapter $g$ that consists of a Transformer layer [51] and an MLP projector, specifically utilizing the self-attention mechanism to integrate visual features from different Transformer layers in the CLIP encoder and map these features to a latent feature space that benefits the model\u2019s generalizability. Specifically, the multi-scale features $\\{f_{\\mathbf{x}}^{l}\\}_{l=1}^{L}$ are fed into a Transformer layer $\\operatorname{Tr}$ , the feature obtained from each layer is treated as a token. The feature extracted from the $[\\mathsf{c l s}]$ token in the output of $\\mathrm{Tr}$ is considered as the fusion of multi-scale features. This fused feature is then directed into a single-layer MLP projector $\\mathrm{Pr}$ , which maps it ", "page_idx": 4}, {"type": "image", "img_path": "MqeCU0tXAY/tmp/629d1c7f0d610e4b955617445dca2ad61181f9b3fd465b1dacca1d8a4ddc9996.jpg", "img_caption": ["Figure 3: The architecture of the adapter $g_{\\theta}$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "from dimension $d$ to $D$ . Finally, both the output of $\\mathrm{Pr}$ and the CLIP final feature f xfinalare fused by residual connection to obtain ultimate visual embedding $\\mathbf{z_{x}}$ . More formally, it is formulated as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{z}_{\\mathbf{x}}=g_{\\theta}(I(\\mathbf{x}))=\\operatorname*{Pr}\\left(\\operatorname{Tr}\\left(\\{f_{\\mathbf{x}}^{l}\\}_{l=1}^{L}\\right)\\right)+f_{\\mathbf{x}}^{f i n a l},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\theta$ represents all the learnable parameters within the adapter $g$ . ", "page_idx": 4}, {"type": "text", "text": "3.4 Channel Refinement ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To extract domain-invariant and class-relevant features, while eliminating those that are domainspecific and class-irrelevant, we design a channel refinement loss based on two criteria, 1) interdomain variance: domain-invariant features should exhibit minimal changes across different domains, implying a smaller inter-domain variance; 2) inter-class variance: class-relevant features should change across different classes, while the changes are expected as large as possible to have more discriminative ability, indicating they should have larger inter-class variance. ", "page_idx": 4}, {"type": "text", "text": "Inter-domain Variance. It measures changes in a feature channel across domains. Given the $i^{\\mathrm{th}}$ input image from $k^{\\mathrm{th}}$ domain, $\\mathbf{x}_{i}^{k}$ , its refined feature is zkx $\\mathbf{z}_{\\mathbf{x}_{i}}^{k}\\,=\\,g_{\\theta}(I(\\mathbf{x}_{i}^{k}))$ , and we denote its $m^{\\mathrm{th}}$ dimension as $\\mathbf{z}_{\\mathbf{x}_{i}}^{k^{(m)}}$ . As shown in Figure. 4, we first put features from all the images from the same domain together, i.e., each column indicates the feature of one image. Then, we calculate the Z(km) refers to the $m^{\\mathrm{th}}$ channel-wise average value of all the samples in the $k^{\\mathrm{th}}$ domain: n1k in=k1 zkx(im), where $n_{k}$ is the number of samples in the $k^{\\mathrm{th}}$ domain. Finally, inter-domain variance of the $m^{\\mathrm{th}}$ channels is calculated as follows: ", "page_idx": 4}, {"type": "image", "img_path": "MqeCU0tXAY/tmp/cad4c451cd31d33bd9f93bbe016f0a53de17706d73c12f69ced80c2f693d713b.jpg", "img_caption": ["Figure 4: Diagram of calculating the channel domain sensitivity across different domains. "], "img_footnote": [], "page_idx": 4}, {"type": "equation", "text": "$$\nV_{d}^{(m)}=\\frac{1}{K}\\sum_{k=1}^{K}(\\mathbf{Z}_{k}^{(m)}-\\bar{\\mathbf{Z}}_{d}^{(m)})^{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $K$ is the number of domains, $\\bar{\\mathbf{Z}}_{d}^{(m)}$ represents the average output at $m^{\\mathrm{th}}$ channel across different domains. ", "page_idx": 4}, {"type": "text", "text": "Inter-class Variance. It measures changes in a feature channel across different classes. Similarly to inter-domain variance, we use the same way to compute the inter-class variance, formulated in Eq. 3. ", "page_idx": 5}, {"type": "equation", "text": "$$\nV_{c}^{(m)}=\\frac{1}{L}\\sum_{\\ell=1}^{L}(\\mathbf{Z}_{\\ell}^{(m)}-\\bar{\\mathbf{Z}}_{c}^{(m)})^{2},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $L$ is the number of classes and $\\begin{array}{r}{\\mathbf{Z}_{\\ell}^{(m)}=\\frac{1}{n_{\\ell}}\\sum_{i=1}^{n_{\\ell}}\\mathbf{z}_{\\mathbf{x}_{i}}^{\\ell^{(m)}}}\\end{array}$ denotes the channel-wise average value of all samples from $\\ell^{\\mathrm{th}}$ category, where $n_{\\ell}$ is the number of samples in the $\\ell^{\\mathrm{th}}$ category, and $\\mathbf{z}_{\\mathbf{x}_{i}}^{\\ell^{(m)}}$ denotes the refined feature from $i^{\\mathrm{th}}$ input image in $\\ell^{\\mathrm{th}}$ category. $\\bar{\\mathbf{Z}}_{c}^{(m)}$ represents the average output at $m^{\\mathrm{th}}$ channel across different classes. ", "page_idx": 5}, {"type": "text", "text": "To ensure the image feature channels contain both domain-invariant and class-relevant information, we minimize the inter-domain variance to eliminate the domain-specific information and maximize the inter-class variance to capture more discriminative class-relevant information. Our channel refinement loss combines the above two criteria in the following way: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{ref}}=\\frac{1}{D}\\sum_{m=1}^{D}\\log\\biggl(1+\\frac{\\sqrt{V_{d}^{(m)}}}{\\sqrt{V_{c}^{(m)}}}\\biggr),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $D$ refers to the number of feature channels. ", "page_idx": 5}, {"type": "text", "text": "3.5 Image-Text Alignment ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The adapter $g_{\\theta}$ maps features from the CLIP\u2019s image embedding space $\\mathcal{T}$ to the refined image embedding space $\\mathcal{Z}$ , aiming for capturing domain-invariant and class-relevant features. However, this mapping may disturb the well-alignment between image spaces $\\mathcal{T}$ and text spaces $\\tau$ provided by CLIP, leading to a misalignment between $\\mathcal{Z}$ and $\\tau$ spaces. Therefore, it is necessary to re-align the refined image space $\\mathcal{Z}$ and text space $\\tau$ . To attain this objective, we first simply employ the standard CLIP loss formulated as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{CE}}=\\mathrm{Cross-entropy}\\big(\\mathrm{Softmax}[g_{\\theta}(I(\\mathbf{x}))\\cdot\\mathbf{T}_{y}],y\\big),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where \u201c\u00b7\u201d is inner product, $\\mathbf{T}_{y}=T(\\mathbf{t}_{y})$ denotes the text embedding of a text prompt $\\mathbf{t}_{y}$ of class $y$ . ", "page_idx": 5}, {"type": "text", "text": "However, the standard CLIP loss only aligns image embedding with the correct text embedding on a per-sample basis but overlooks the potential relationship between samples. Thus, we propose to explore semantic structure information to strengthen the image-text alignment. Inspired by prior work [13, 11], we aim to align the pairwise directions in the image and the text spaces. To this end, we first normalize the pairwise distance in image and text space and then directly minimize their cosine similarity. For a pair training samples $\\{(\\bar{\\mathbf{x}_{i}},y_{i}),(\\mathbf{x}_{j},\\bar{y_{j}})\\}$ , the direction loss is defined as: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{dir}}=1-\\left(\\frac{g_{\\theta}(I(\\mathbf{x}_{i}))-g_{\\theta}(I(\\mathbf{x}_{j}))}{\\|g_{\\theta}(I(\\mathbf{x}_{i}))-g_{\\theta}(I(\\mathbf{x}_{j}))\\|}\\cdot\\frac{\\mathbf{T}_{y_{i}}-\\mathbf{T}_{y_{j}}}{\\|\\mathbf{T}_{y_{i}}-\\mathbf{T}_{y_{j}}\\|}\\right),\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "To further remove the domain-specific information in the image space, we sample the pair data from different domains and different classes and align them with the direction of the corresponding classes in the text space. Since the language embedding of the class is naturally domain-invariant. Thus, if the output of $g_{\\theta}(I(\\mathbf{x}_{i}))$ or $g_{\\theta}(I(\\mathbf{x}_{j}))$ contains any domain-specific information, the difference between them will not align with the corresponding class text direction. Therefore, the direction loss strengthens the image-text alignment by exploiting semantic structure information as well as removing domain-specific information in the image space. ", "page_idx": 5}, {"type": "text", "text": "3.6 Training and Inference ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We aggregate all the losses to our overall objective defined as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\mathcal{L}=\\mathcal{L}_{\\mathrm{CE}}+\\mathcal{L}_{\\mathrm{ref}}+\\mathcal{L}_{\\mathrm{dir}},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Input: Pre-trained CLIP image encoder $I$ , text encoder $T$ , adapter $g_{\\theta}$ , initialized with ERM. 1: for $t\\in[1,N]$ do   \n2: Sample data $\\{(\\mathbf{x},y)\\}$ from the source domain set $\\boldsymbol{S}$ .   \n3: Calculate Channel Refinement loss ${\\mathcal{L}}_{\\mathrm{ref}}$ (Eq. 4), and Cross-Entropy loss $\\mathcal{L}_{\\mathrm{CE}}$ (Eq. 5). 4: Sample the pair data $\\{(\\mathbf{x}_{i},y_{i}),(\\mathbf{x}_{j},y_{j})\\}$ from the source domain set $\\boldsymbol{S}$ , where $\\mathbf{x}_{i}$ and $\\mathbf{x}_{j}$ are from different domain and yi \u0338= yj.   \n5: Calculate Direction loss $\\mathscr{L}_{\\mathrm{dir}}$ (Eq. 6) on above pair data samples.   \n6: Update $\\theta$ with total loss $\\mathcal{L}$ (Eq. 7) with Beta Moving Average (BMA).   \n7: end for   \nreturn: ", "page_idx": 6}, {"type": "text", "text": "where $\\theta$ is the parameters of trainable adapter $g_{\\theta}$ . We show the overall training procedure of the proposed CLIPCEIL method in Algorithm 1. ", "page_idx": 6}, {"type": "text", "text": "To incorporate prior knowledge of CLIP, during the inference stage, we ensemble the fine-tuning model\u2019s prediction and CLIP zero-shot prediction to obtain the final classification logits. The logits of sample $\\mathbf{x}_{i}$ are formulated as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{logits}_{\\mathbf{x}_{i}}=\\frac{1}{2}\\big(f_{\\mathbf{x}_{i}}^{f i n a l}\\mathbf{W}+g_{\\theta}(I(\\mathbf{x}_{i}))\\mathbf{W}\\big).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathbf{W}=(\\mathbf{T}_{1},\\dots,\\mathbf{T}_{C})^{\\top}$ , $C$ is the number of classes. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This section showcases the superiority of our method across five widely used DG benchmark datasets. Furthermore, we carry out detailed ablation studies to determine the impacts of different loss terms, the channel refinement strategies, and the architecture of adapter $g$ . ", "page_idx": 6}, {"type": "text", "text": "4.1 Datasets and implementation details ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We evaluate our proposed method on five standard DG benchmarks: PACS [28] contains 9991 images of 7 categories from 4 domains; VLCS [48] comprises 5 categories from 4 domains, 10,729 images in total; OfficeHome [52] contains 15,579 images of 65 categories from 4 domains; TerraIncognita [2] contains 24,788 images with 10 categories from 4 domains; DomainNet [38] is a more recent and the largest one among all five datasets, which contains 0.6 million images in 345 categories from 6 domains. We utilize the CLIP pre-trained model with the ViT-B/16 [10] backbone. More results of other backbones are in Appendix C.1. We fixed the image and text encoders and solely trained adapter $g$ during training. To avoid the influence of different template prompts, the output of the text encoder is calculated by the average of 80 template prompts from ImageNet [41]. In all experiments, we use the open-source code DomainBed [16] and follow the train-validate-test split of each dataset on the DomainBed benchmark. Following the literature, we train our model with 5000 iterations on PACS, VLCS, OfficeHome, and TerraIncognia datasets and 15000 iterations on the DomainNet dataset. Our model is selected based on the source domain validation set. All experiments are conducted on the NVIDIA A100 GPUs. All the results were averaged after five runs with different random seeds. More detailed information are in Appendix A ", "page_idx": 6}, {"type": "text", "text": "4.2 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We evaluate our CLIPCEIL model against the state-of-the-art (SOTA) approaches on five standard benchmark datasets. We initially compare with CLIP zero-shot, which serves as a pre-trained visionlanguage baseline model without any training, which outperforms state-of-the-art ResNet-50 based models, e.g., SAGM [54] and DomainDrop [17], demonstrates the superior of the pre-trained VLMs. We further compare with the standard linear probing, which learns a single-layer linear classifier upon CLIP encoder, and three SOTA VLMs based models, i.e., the mutual-information regularization based MIRO [7] model, the prompt learning based DPL [62] and StyLIP [4] models. To extend the comparison, we adapt three widely-used prompt learning models, i.e., CoOp [68], CoCoOP [67], ", "page_idx": 6}, {"type": "text", "text": "MaPLE [24], and one adapter-based method CLIP-Adapter [15], which are originally designed for few-shot learning, to the DG task using the same experimental setting on the DG benchmark. Furthermore, to ensure a fair comparison with methods that fine-tune the entire visual encoder such as CLIPood [44], CAR-FT [35], and UniDG [63], we train our CLIPCEIL similarly, which we term CLIPCEIL $^{++}$ . Note that UniDG [63] is an inference-time fine-tuning model, which adapts the model with additional information from the target domain. ", "page_idx": 7}, {"type": "text", "text": "Table 2: Comparison of our proposed method with the State-of-the-art methods on the DomainBed benchmark. denotes ResNet-50 backbone; denotes frozen CLIP ViT-B/16 encoder; $\\cdot$ denotes fine-tuning the entire CLIP ViT-B/16 encoder, \\* denotes the two rounds inference-time fine-tuning. Red and indicate the best performance in each group. ", "page_idx": 7}, {"type": "table", "img_path": "MqeCU0tXAY/tmp/d8a6b6ea445df99da8d74ea51c269cd3f1dacf471e147e774c102e62f950a79c.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "As illustrated in Table 2, our proposed CLIPCEIL exhibits significant improvement over the CLIP Zero-Shot and achieves the best average performance on five benchmark datasets among all the compared methods. Specifically, CLIPCEIL exceeds the second-best method DPL [62] by $2.3\\%$ on average, CLIPCEIL $^{++}$ exceeds the second-best method CLIPood [44] by $0.5\\%$ on average. The results prove CLIPCEIL\u2019s effectiveness in enhancing the model generalization through capturing domain-invariant and class-relevant features. More detailed break-down results are in Appendix B. ", "page_idx": 7}, {"type": "text", "text": "4.3 Ablation Studies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "4.3.1 Effectiveness of each loss term ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Firstly, we conduct the ablation study to examine the efficacy of each loss (i.e., channel refinement loss $\\mathcal{L}_{\\mathrm{ref}}$ , and direction loss $\\mathcal{L}_{\\mathrm{dir}}$ ) in our overall objective function. Cross-entropy loss $\\mathcal{L}_{\\mathrm{CE}}$ is very standard and thus we include it by default, similar to multi-scale fusion, which will be investigated in Section 4.3.3. Table 3 presents the results of different CLIPCEIL variants with the pre-trained ViTB/16 model on the OfficeHome dataset. As shown in the table, utilizing multi-scale information alone can enhance performance compared to the CLIP Zero-Shot. Integrating $\\mathcal{L}_{\\mathrm{ref}}$ leads to further enhanced performance, indicating the effectiveness in channel refinement loss to capturing domain-invariant and class-relevant information. Similarly, the improved performance of adding $\\mathcal{L}_{\\mathrm{dir}}$ suggests that the direction loss contributes to enhancing domain-invariant features through the help of text description. As a result, combining all three components results in the best performance, showing that each loss works as an indispensable component for achieving superior generalization of the framework. ", "page_idx": 7}, {"type": "table", "img_path": "MqeCU0tXAY/tmp/b4003749bc9ea490e880805fd5ad86474ff891826274dda79ea168e9a94edee6.jpg", "table_caption": ["Table 3: Ablation study of each loss in our objective function on OfficeHome dataset. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "To further demonstrate the corporation of each loss term, we visualize the image features of the CLIP pre-trained model and our proposed CLIPCEIL on the OfficeHome dataset in Figure 5. Different colors represent different classes or domains. As illustrated in Figure 5 (a) and (b), the image features extracted by CLIPCEIL exhibit more discrimination than the CLIP pre-trained model, proving the effectiveness of CLIPCEIL in capturing the class-relevant features. Meanwhile, the image features corresponding to different domains extracted from CLIPCEIL are distributed almost equally across all classes, demonstrated in Figure 5 (d), indicating that CLIPCEIL definitely extracts domain-invariant features. In contrast, image features from the CLIP pre-trained model are located in various places across different domains, shown in Figure 5 (c), suggesting that it still contains domain-specific information. The visualization of other datasets can be found in Appendix B.2. ", "page_idx": 8}, {"type": "image", "img_path": "MqeCU0tXAY/tmp/f82fbac50e953b11e365ec574d77faefee43d23e86c563a2cca1435ceb652ecf.jpg", "img_caption": ["Figure 5: t-SNE [49] visualization on image features of CLIPCEIL and CLIP pre-trained models across different classes and domains. Different colors indicate different classes or domains "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3.2 The effectiveness of the two criteria in channel refinement loss ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our proposed channel refinement loss $\\mathcal{L}_{\\mathrm{ref}}$ is based on two criteria, namely inter-domain variance and inter-class variance. To demonstrate the effectiveness of these criteria, we conducted experiments on all five datasets. In Figure. 6, the results show that combining inter-domain variance with inter-class variance (represented by the darkest bars) results in better performance than using either criterion alone. This indicates that the two criteria can be effectively blended and both domain-invariant and class-relevant information complement each other and are essential to enhance a model\u2019s generalization ability. More detailed breakdown results are in Appendix B.3. ", "page_idx": 8}, {"type": "image", "img_path": "MqeCU0tXAY/tmp/8209220cfe824695f4155510bbb61028ddeca7c522d7ae362fad7beb7bcfd973.jpg", "img_caption": ["Figure 6: The average accuracy bar of the different channel refinement strategies. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.3.3 Architecture of adapter $g$ ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We investigate the structure of adapter $g$ by comparing the efficacy of multi-scale and bypass connections. As indicated in Table 4, integrating both multi-scale and bypass connections yields the most optimal performance. This can be attributed to two main factors: (1) The multi-scale approach captures a wide range of image features from both lower and higher levels, making it more generalizable than solely using the final layer output. (2) The bypass design preserves the original CLIP pre-trained knowledge and is easier to optimize. More ablation studies for different adapter architecture and integrating Multi-scale text features are in Appendix C.2, and C.3. ", "page_idx": 8}, {"type": "table", "img_path": "MqeCU0tXAY/tmp/3b3c34cc46e70e5c23ebda8375db2f19367fcdf348f7ca01872eb891d26be04e.jpg", "table_caption": ["Table 4: Ablation study of different adapter architectures. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 Discussion: Potential Data Leakage in CLIP on DomainBed Benchmarks ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This section discusses the possibility of data leakage when fine-tuning the pre-trained CLIP model on DomainBed benchmarks. A primary concern is whether the DomainBed datasets truly represent outof-distribution (OOD) data for CLIP, given its extensive pretraining on 400 million image-text pairs. We argue that the data distributions differ significantly: DomainBed datasets, such as DomainNet, display distinct characteristics like imbalance and long-tailed distributions, in contrast to the balanced nature of CLIP\u2019s pretraining dataset [41, 56]. Furthermore, CLIP\u2019s zero-shot performance on benchmarks like TerraIncognita and DomainNet highlights that certain domains (e.g., Infograph and Quickdraw in DomainNet, and camera-trap images in TerraIncognita) remain underrepresented in the CLIP pretraining corpus. These observations suggest that the distribution, style, and specific content of CLIP\u2019s pretraining data diverge meaningfully from those in DomainBed, potentially mitigating concerns about data overlap and preserving the intended OOD nature of DomainBed benchmarks. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduced the CLIPCEIL model to enhance the generalizability of the pre-trained CLIP model to the test datasets undergoing domain shifts. Specifically, we proposed a lightweight adapter for the refinement of visual feature channels to ensure the inclusion of domain-invariant and class-relevant information, which is achieved by minimizing inter-domain variance while maximizing inter-class variance. We maintained image-text alignment by aligning image features with the text features of their corresponding textual descriptions, concurrently eliminating domain-specific features. Comprehensive experiments on five benchmark datasets illustrated that CLIPCEIL surpasses the existing state-of-the-art methods. ", "page_idx": 9}, {"type": "text", "text": "Limitations. Since calculating inter-domain variance involves multiple domains, CLIPCEIL currently only applies to multi-source domain generalization. Exploring its applicability to single-source domain generalization is deferred for future investigation. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the U.S. Department of Energy (DOE), Office of Science (SC), Advanced Scientific Computing Research program under award DE-SC-0012704. This work was supported by the Laboratory Directed Research and Development (LDRD) Program (24-063 and 25-006) of Brookhaven National Laboratory under U.S. Department of Energy Contract No. DE-SC0012704. We are grateful to the anonymous reviewers for their valuable feedback and constructive suggestions, which have significantly improved this paper. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Sravanti Addepalli, Ashish Ramayee Asokan, Lakshay Sharma, and R Venkatesh Babu. Leveraging vision-language models for improving domain generalization in image classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23922\u201323932, 2024.   \n[2] Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In Proceedings of the European conference on computer vision (ECCV), pages 456\u2013473, 2018.   \n[3] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for domain adaptation. Advances in neural information processing systems, 19, 2006.   \n[4] Shirsha Bose, Ankit Jha, Enrico Fini, Mainak Singha, Elisa Ricci, and Biplab Banerjee. Stylip: Multi-scale style-conditioned prompt learning for clip-based domain generalization. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 5542\u20135552, 2024.   \n[5] Manh-Ha Bui, Toan Tran, Anh Tran, and Dinh Phung. Exploiting domain-specific features to enhance domain generalization. Advances in Neural Information Processing Systems, 34:21189\u201321201, 2021.   \n[6] Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung Lee, and Sungrae Park. Swad: Domain generalization by seeking flat minima. Advances in Neural Information Processing Systems, 34:22405\u201322418, 2021.   \n[7] Junbum Cha, Kyungjae Lee, Sungrae Park, and Sanghyuk Chun. Domain generalization by mutualinformation regularization with pre-trained models. In European Conference on Computer Vision, pages 440\u2013457. Springer, 2022.   \n[8] Chia-Yuan Chang, Yu-Neng Chuang, Guanchu Wang, Mengnan Du, and Na Zou. Dispel: Domain generalization via domain-specific liberating. arXiv preprint arXiv:2307.07181, 2023.   \n[9] Ching-Yao Chuang, Antonio Torralba, and Stefanie Jegelka. Estimating generalization under distribution shifts via domain-invariant representations. In Proceedings of the 37th International Conference on Machine Learning, ICML, volume 119 of Proceedings of Machine Learning Research, pages 1984\u20131994. PMLR, 2020.   \n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[11] Lisa Dunlap, Clara Mohri, Devin Guillory, Han Zhang, Trevor Darrell, Joseph E Gonzalez, Aditi Raghunathan, and Anna Rohrbach. Using language to extend to unseen domains. In The Eleventh International Conference on Learning Representations, 2022.   \n[12] Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc\u2019Aurelio Ranzato, and Tomas Mikolov. Devise: A deep visual-semantic embedding model. Advances in neural information processing systems, 26, 2013.   \n[13] Rinon Gal, Or Patashnik, Haggai Maron, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. Stylegannada: Clip-guided domain adaptation of image generators. ACM Transactions on Graphics (TOG), 41(4):1\u201313, 2022.   \n[14] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. Clip-adapter: Better vision-language models with feature adapters. International Journal of Computer Vision, pages 1\u201315, 2023.   \n[15] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. Clip-adapter: Better vision-language models with feature adapters. International Journal of Computer Vision, 132(2):581\u2013595, 2024.   \n[16] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv preprint arXiv:2007.01434, 2020.   \n[17] Jintao Guo, Lei Qi, and Yinghuan Shi. Domaindrop: Suppressing domain-sensitive channels for domain generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023.   \n[18] Narges Honarvar Nazari and Adriana Kovashka. Domain generalization using shape representation. In European Conference on Computer Vision, pages 666\u2013670. Springer, 2020.   \n[19] Shoubo Hu, Kun Zhang, Zhitang Chen, and Laiwan Chan. Domain generalization via multidomain discriminant analysis. In Uncertainty in Artificial Intelligence, pages 292\u2013302. PMLR, 2020.   \n[20] Jiaxing Huang, Dayan Guan, Aoran Xiao, and Shijian Lu. Fsdr: Frequency space domain randomization for domain generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6891\u20136902, 2021.   \n[21] Maximilian Ilse, Jakub M Tomczak, Christos Louizos, and Max Welling. Diva: Domain invariant variational autoencoders. In Medical Imaging with Deep Learning, pages 322\u2013348. PMLR, 2020.   \n[22] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning, pages 4904\u20134916. PMLR, 2021.   \n[23] Juwon Kang, Sohyun Lee, Namyup Kim, and Suha Kwak. Style neophile: Constantly seeking novel styles for domain generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7130\u20137140, 2022.   \n[24] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan. Maple: Multi-modal prompt learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19113\u201319122, 2023.   \n[25] Rawal Khirodkar, Donghyun Yoo, and Kris Kitani. Domain randomization for scene-specific car detection and pose estimation. In 2019 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 1932\u20131940. IEEE, 2019.   \n[26] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023.   \n[27] Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang. Fine-tuning can distort pretrained features and underperform out-of-distribution. arXiv preprint arXiv:2202.10054, 2022.   \n[28] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain generalization. In Proceedings of the IEEE international conference on computer vision, pages 5542\u20135550, 2017.   \n[29] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.   \n[30] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning, pages 12888\u201312900. PMLR, 2022.   \n[31] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learning with momentum distillation. Advances in neural information processing systems, 34:9694\u20139705, 2021.   \n[32] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep domain generalization via conditional invariant adversarial networks. In Proceedings of the European Conference on Computer Vision (ECCV), pages 624\u2013639, 2018.   \n[33] Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, and Junjie Yan. Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm. arXiv preprint arXiv:2110.05208, 2021.   \n[34] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.   \n[35] Xiaofeng Mao, Yufeng Chen, Xiaojun Jia, Rong Zhang, Hui Xue, and Zhao Li. Context-aware robust fine-tuning. International Journal of Computer Vision, pages 1\u201316, 2023.   \n[36] Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie. Slip: Self-supervision meets languageimage pre-training. In European Conference on Computer Vision, pages 529\u2013544. Springer, 2022.   \n[37] Hyeonseob Nam, HyunJae Lee, Jongchan Park, Wonjun Yoon, and Donggeun Yoo. Reducing domain gap by reducing style bias. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8690\u20138699, 2021.   \n[38] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1406\u20131415, 2019.   \n[39] Di Qi, Lin Su, Jia Song, Edward Cui, Taroon Bharti, and Arun Sacheti. Imagebert: Cross-modal pre-training with large-scale weak-supervised image-text data. arXiv preprint arXiv:2001.07966, 2020.   \n[40] Fengchun Qiao and Xi Peng. Uncertainty-guided model generalization to unseen domains. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6790\u20136800, 2021.   \n[41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[42] Mattia Segu, Alessio Tonioni, and Federico Tombari. Batch normalization embeddings for deep domain generalization. Pattern Recognition, page 109115, 2022.   \n[43] Yang Shu, Zhangjie Cao, Chenyu Wang, Jianmin Wang, and Mingsheng Long. Open domain generalization with domain-augmented meta-learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9624\u20139633, 2021.   \n[44] Yang Shu, Xingzhuo Guo, Jialong Wu, Ximei Wang, Jianmin Wang, and Mingsheng Long. Clipood: Generalizing clip to out-of-distributions. In International Conference on Machine Learning, 2023.   \n[45] Richard Socher, Milind Ganjoo, Christopher D Manning, and Andrew Ng. Zero-shot learning through cross-modal transfer. Advances in neural information processing systems, 26, 2013.   \n[46] Nathan Somavarapu, Chih-Yao Ma, and Zsolt Kira. Frustratingly simple domain generalization via image stylization. arXiv preprint arXiv:2006.11207, 2020.   \n[47] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS), pages 23\u201330. IEEE, 2017.   \n[48] Antonio Torralba and Alexei A. Efros. Unbiased look at dataset bias. In CVPR 2011, pages 1521\u20131528, 2011.   \n[49] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008.   \n[50] Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media, 1999.   \n[51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[52] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5018\u20135027, 2017.   \n[53] Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John C Duchi, Vittorio Murino, and Silvio Savarese. Generalizing to unseen domains via adversarial data augmentation. Advances in neural information processing systems, 31, 2018.   \n[54] Pengfei Wang, Zhaoxiang Zhang, Zhen Lei, and Lei Zhang. Sharpness-aware gradient matching for domain generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3769\u20133778, 2023.   \n[55] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al. Robust fine-tuning of zero-shot models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7959\u20137971, 2022.   \n[56] Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying clip data. arXiv preprint arXiv:2309.16671, 2023.   \n[57] Qinwei Xu, Ruipeng Zhang, Ya Zhang, Yanfeng Wang, and Qi Tian. A fourier-based framework for domain generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14383\u201314392, 2021.   \n[58] Fu-En Yang, Yuan-Chia Cheng, Zu-Yun Shiau, and Yu-Chiang Frank Wang. Adversarial teacher-student representation learning for domain generalization. Advances in Neural Information Processing Systems, 34:19448\u201319460, 2021.   \n[59] Xi Yu, Huan-Hsin Tseng, Shinjae Yoo, Haibin Ling, and Yuewei Lin. Insure: an information theory inspired disentanglement and purification model for domain generalization. IEEE Transactions on Image Processing, 2024.   \n[60] Hanlin Zhang, Yi-Fan Zhang, Weiyang Liu, Adrian Weller, Bernhard Sch\u00f6lkopf, and Eric P Xing. Towards principled disentanglement for domain generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8024\u20138034, 2022.   \n[61] Renrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-adapter: Training-free clip-adapter for better vision-language modeling. arXiv preprint arXiv:2111.03930, 2021.   \n[62] Xin Zhang, Shixiang Shane Gu, Yutaka Matsuo, and Yusuke Iwasawa. Domain prompt learning for efficiently adapting clip to unseen domains. Transactions of the Japanese Society for Artificial Intelligence, 38(6):B\u2013MC2_1, 2023.   \n[63] Yiyuan Zhang, Kaixiong Gong, Xiaohan Ding, Kaipeng Zhang, Fangrui Lv, Kurt Keutzer, and Xiangyu Yue. Towards unified and effective domain generalization. arXiv preprint arXiv:2310.10008, 2023.   \n[64] Long Zhao, Ting Liu, Xi Peng, and Dimitris Metaxas. Maximum-entropy adversarial data augmentation for improved generalization and robustness. Advances in Neural Information Processing Systems, 33:14435\u2013 14447, 2020.   \n[65] Ye Zheng, Xi Huang, and Li Cui. Visual language based succinct zero-shot object detection. In Proceedings of the 29th ACM International Conference on Multimedia, pages 5410\u20135418, 2021.   \n[66] Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.   \n[67] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.   \n[68] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. International Journal of Computer Vision, 130(9):2337\u20132348, 2022.   \n[69] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Domain generalization with mixstyle. In 9th International Conference on Learning Representations, ICLR. OpenReview.net, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 14}, {"type": "table", "img_path": "MqeCU0tXAY/tmp/879a3bf1f3125727555690bb55e728224a3c9b7c22367526e1957f5d12ea079b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 14}, {"type": "text", "text": "A Dataset and implementation details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We evaluate our proposed method on five conventional DG benchmarks. PACS [28] contains 9991 images of 7 categories from 4 domains: photo (P), art-painting (A), cartoon (C) and sketch (S). OfficeHome [52] contains 15,579 images in total with 65 categories from 4 domains of styles: Artistic (A), Clip-Art (C), Product (P) and Real-World (R). TerraIncognita [2] contains 24,788 images with 10 categories from 4 domains, i.e., four different locations where the images are taken. VLCS [48] comprises 5 categories from 4 domains, VOC2007 (V), LabelMe (L), Caltech (C) and Sun (S), and 10,729 images in total. DomainNet [38] is a more recent and the largest dataset used in domain generalization task. In total, it contains 0.6 million images in 345 categories from six domains: clipart, infograph, painting, quickdraw, real, and sketch. ", "page_idx": 14}, {"type": "text", "text": "We use the CLIP pre-trained model with ViT-B/16 as the image encoder. We freeze both image and text encoders during the training and only train a lightweight adapter $g$ consisting of one transformer layer and a single-layer MLP projector. The structure details of adapter $g$ are reported in Table 6. To avoid the influence of different template prompts, the output of the text encoder is obtained by averaging 80 template prompts on ImageNet [41] represented in Table 7. Our optimizer is AdamW [34] with a weight decay of $5e-4$ , and the learning rate is initialized to $5e-5$ , gradually decreasing by using the cosine annealing scheduler. We train the model for 5000 iterations for all the datasets except for DomainNet [38] with 15000 iterations. We adopt a batch size of 32 for all datasets, and all images are randomly resized and cropped to $224\\times224$ . Following the same training process of CLIPood [44], we utilize the beta moving average (BMA) to update our parameters during the training. All the default configurations are shown in Table 5. All experiments are conducted on a GPU server equipped with 4 NVIDIA A100-SXM4-80GB GPUs, although only 2 were used for this paper. The server also has an Intel Xeon Gold 6336Y CPU $\\textcircled{a}\\ 2.40\\mathrm{GHz}$ with 24 cores and 48 threads, 1 TB of memory. Our CLIPCEIL model is implemented and evaluated with Python 3.8.13, PyTorch 1.8.0, Torchvision 0.9.0, and CUDA 11.1. ", "page_idx": 14}, {"type": "table", "img_path": "MqeCU0tXAY/tmp/7417da0f0c82511b289bc61c129ace3b63be5e3bb3294e5f91681d54ebc2f26b.jpg", "table_caption": ["Table 5: Default configurations for the experiments. "], "table_footnote": [], "page_idx": 14}, {"type": "table", "img_path": "MqeCU0tXAY/tmp/ad663faeeec4832acaf574a168e745c7fe86a787d455c5de7cfc383a287ebd56.jpg", "table_caption": ["Table 6: Structure details of Adapter g. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Table 7: 80 template prompts on the ImageNet ", "page_idx": 15}, {"type": "table", "img_path": "MqeCU0tXAY/tmp/7c966bc7521eb9b9e4a5f4cad635f2b9cc16cba098dbedca794926b50a92d6d2.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "B Full results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Domain Generalization benchmarks ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In the main paper, we report the average accuracy across each dataset. In the supplementary, we provide a comprehensive breakdown of results for each domain on PACS [28] in Table 8, VLCS [48] in Table 9, OfficeHome [52] in Table 10, TerraIncognita [2] in Table 11, and DomainNet [38] in Table 12. We present the results reported in the original papers on comparison methods. For some methods, such as CoOp [68] and CoCoOp [67] where the original papers do not report results under the domain generalization setting, we reimplement them for a unified comparison. As presented in tables, CLIPCEIL outperforms methods with ResNet pre-trained model by a large margin, indicating that vision-language models pre-trained on huge web-scale image-text pairs provide a promising way to boost OOD generalization. It also outperforms SOTA using CLIP models i.e., MIRO [7] and DPL [62]. In general, our method achieves the best performance on most domains, and our overall average performance on a total of five benchmark datasets exceeds other SOTA DG methods. For each result of CLIPCEIL, we report the average results and the standard deviation of five runs with random seeds. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "table", "img_path": "MqeCU0tXAY/tmp/856b0607cf71d11446bd45be2b2e0aac65ab22bd800ba596781a3dbc6e9d6651.jpg", "table_caption": ["Table 8: Detailed comparison of our proposed method with the State-of-the-art methods on the PACS dataset. \\* denotes the models that utilize the ResNet-50 backbone, and the rest utilize CLIP ViT-B/16 backbone. "], "table_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "MqeCU0tXAY/tmp/bf1695264399c30b6f4d944e5f66955ca968c738207fd77f0cc282ead1a8775f.jpg", "table_caption": ["Table 9: Detailed comparison of our proposed method with the State-of-the-art methods on the VLCS dataset. \\* denotes the models that utilize the ResNet-50 backbone, and the rest utilize CLIP ViT-B/16 backbone. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "B.2 Visualization of visual features ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To further demonstrate the effectiveness of CLIPCEIL, we visualize the image features of CLIP pre-trained model and our proposed CLIPCEIL. We show the t-SNE figures across different domains and classes on PACS, VLCS, and TerrIncognita in Figure 7, Figure 8, and Figure 9, respectively. Note that the OfficeHome results have been reported in the main paper. It is clear to see that the image features extracted by CLIPCEIL exhibit more discrimination with respect to different classes than CLIP pre-trained model. Meanwhile, CLIPCEIL\u2019s image features corresponding to different domains appear in most classes. This proves that CLIPCEIL\u2019s image features contain domain-invariant and class-relevant information. ", "page_idx": 16}, {"type": "text", "text": "B.3 Ablation studies on channel refinement criteria ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To demonstrate the effectiveness of our channel refinement strategy, we compare it with other methods that either consider the inter-domain or inter-class variance criterion. The main paper illustrates the ", "page_idx": 16}, {"type": "text", "text": "Table 10: Detailed comparison of our proposed method with the State-of-the-art methods on the OfficeHome dataset. \\* denotes the models that utilize the ResNet-50 backbone, and the rest utilize CLIP ViT-B/16 backbone. ", "page_idx": 17}, {"type": "table", "img_path": "MqeCU0tXAY/tmp/af522d82c684b7f46bb401ba12b801c6d4a62bcf4064b4663c9d921be050c79a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 17}, {"type": "table", "img_path": "MqeCU0tXAY/tmp/cbaa9f7236f58cc12efc5d538b72a30fb2b4ebaa951810a8a558dcbb4cc650a2.jpg", "table_caption": ["Table 11: Detailed comparison of our proposed method with the State-of-the-art methods on the TerraIncognita dataset. \\* denotes the models that utilize the ResNet-50 backbone, and the rest utilize CLIP ViT-B/16 backbone. "], "table_footnote": ["average accuracy bars of different channel refinement strategies across each dataset. Here, we provide a comprehensive breakdown of results for each domain on five DG datasets in Figure 10. "], "page_idx": 17}, {"type": "text", "text": "C Additional experiments ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "C.1 Performance on different backbones ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In our main experiments, we use ViT-B/16 as the backbone. To further explore performance across different architectures, we conducted additional experiments with ResNet-50, ViT-B/32, and ViT-L/14 on the OfficeHome dataset. The process for extracting latent representations differs between ResNet and ViT-based backbones. For ResNet, we extract latent features from the feature map and apply Attention Pooling to transform the 2D feature map into a 1D vector. These vectors from different layers are then passed into the adapter\u2019s Transformer layer, $g$ . The results, summarized in Table 13, show that CLIPCEIL consistently outperforms zero-shot predictions on ViT backbones and other ResNet-based models, highlighting its strong generalization ability across different architectures. ", "page_idx": 17}, {"type": "text", "text": "C.2 Ablation studies for Adapter ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We conducted ablation studies to explore the effects of the Transformer layer in the adapter $g$ . In this study, we replaced the Transformer layer with Average Pooling and a one-layer MLP projector and used a simple adapter $g$ , i.e., one-layer MLP, that did not consider multi-scale information. As shown in the orange block in Table 14, the Transformer layer outperformed the other fusion strategies, indicating its necessity. The pink block of Table 14 suggests that the inclusion of the reference loss $\\mathcal{L}_{\\mathrm{ref}}$ and the directional loss $\\mathcal{L}_{d i r}$ alongside the simple adapter $g$ leads still improves the performance. ", "page_idx": 17}, {"type": "text", "text": "Table 12: Detailed comparison of our proposed method with the State-of-the-art methods on the DomainNet dataset. \\* denotes the models that utilize the ResNet-50 backbone, and the rest utilize CLIP ViT-B/16 backbone. ", "page_idx": 18}, {"type": "image", "img_path": "MqeCU0tXAY/tmp/e743edd6a67df930ae5d7bbca34975344a17d162e7d00a24aa6b15ac57910bf7.jpg", "img_caption": ["Figure 7: t-SNE [49] visualization on image features of our proposed CLIPCEIL and CLIP pre-trained across different classes and domains on PACS dataset. Different colors indicate different classes or domains "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "C.3 Apply multi-scale mechanism on text encoder ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "To investigate the effectiveness of the multi-scale mechanism on the text encoder. We conducted experiments to incorporate a multi-scale adapter into the text encoder. As shown in Table 15, using both visual and text adapters did not perform as well as only using the visual adapter. This may be due to the increased complexity of optimizing both adapters simultaneously. It also suggests that focusing on image feature adaptation is more crucial for domain generalization tasks since the semantic gap between visual features in pretrained and custom datasets is larger than that of text features. ", "page_idx": 18}, {"type": "image", "img_path": "MqeCU0tXAY/tmp/59bdd7b877d2f34b03bf266babaa0cfebb73f139432a261a5986aab8f08df77a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 8: t-SNE [49] visualization on image features of our proposed CLIPCEIL and CLIP pre-trained across different classes and domains on VLCS dataset. Different colors indicate different classes or domains ", "page_idx": 19}, {"type": "image", "img_path": "MqeCU0tXAY/tmp/e5d5afd4630a41e5a5b51bf55a3009b40ceac89687639047f55b1f8fdbeea3e0.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 9: t-SNE [49] visualization on image features of our proposed CLIPCEIL and CLIP pre-trained across different classes and domains on TerraIncognita dataset. Different colors indicate different classes or domains ", "page_idx": 19}, {"type": "image", "img_path": "MqeCU0tXAY/tmp/11c95ded104f088f8d89dad972dac7cc59893dd3ea8582369d9bccfc11cfbb35.jpg", "img_caption": ["Figure 10: Full accuracy bar results of different channel refinement strategies on the five DG datasets. "], "img_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "MqeCU0tXAY/tmp/9b5497af79f9aa4736ae89f9b34976b96b485b64cbed090402440f49a8accec5.jpg", "table_caption": ["Table 13: Performance with different backbones on OfficeHome datasets. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 14: Performance of a linear layer adapter $g$ on OfficeHome dataset with ViT-B/16 backbone. ", "page_idx": 20}, {"type": "table", "img_path": "MqeCU0tXAY/tmp/487f39a70db60ef5315c3c5b2c3fa28ac8490e075d60f8476c99eb68f0e6e9bb.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "MqeCU0tXAY/tmp/fa3e2b91ebec2be7189d1244953f07e7d89d06889c19af1c3f9902c4f6f5d070.jpg", "table_caption": ["Table 15: Performance comparison with text encoder adapter with ViT-B/16 backbone. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Yes, in both abstract and introduction, we described our contributions and scope. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 21}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: Yes, we discussed the limitations of our work at the end of \u201cConclusion\" section. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 21}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 22}, {"type": "text", "text": "Justification: This paper does not include theoretical results. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 22}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Yes, we provide detailed information about the parameter setting and configuration in the section 4.1 and Appendix A. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 22}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Yes, the source code is available at https://github.com/yuxi120407/ CLIPCEIL, while all the data we use is publicly available. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 23}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Yes, we provide detailed information about experimental setting in the section 4.1 and Appendix A. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 23}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Yes, we provide the mean and standard deviation of the five runs with different random seeds for all the experiments. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 24}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Yes, we provide the detailed information about the computational resource we used in Appendix A. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 24}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: Yes, our research is with the NeurIPS Code of Ethics. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 24}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: As our model is designed for efficient fine-tuning the pre-trained vision language model, we do not anticipate any ethical or social impacts at this point. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 24}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 25}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: As our model is designed for efficient fine-tuning the pre-trained vision language model for the image classification task, we do not anticipate any risk at this point. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 25}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Yes, we cited all the existing assets. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper does not release new assets. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 26}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 26}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing or research with human subjects. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}]