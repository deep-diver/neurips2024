[{"type": "text", "text": "Modeling Latent Neural Dynamics with Gaussian Process Switching Linear Dynamical Systems ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Amber Hu Stanford University amberhu@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "David Zoltowski Stanford University dzoltow@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Aditya Nair Caltech & Howard Hughes Medical Institute adi.nair@caltech.edu ", "page_idx": 0}, {"type": "text", "text": "David Anderson Caltech & Howard Hughes Medical Institute wuwei@caltech.edu ", "page_idx": 0}, {"type": "text", "text": "Lea Duncker\u2217 Columbia University ld3149@columbia.edu ", "page_idx": 0}, {"type": "text", "text": "Scott Linderman\u2217 Stanford University swl1@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Understanding how the collective activity of neural populations relates to computation and ultimately behavior is a key goal in neuroscience. To this end, statistical methods which describe high-dimensional neural time series in terms of lowdimensional latent dynamics have played a fundamental role in characterizing neural systems. Yet, what constitutes a successful method involves two opposing criteria: (1) methods should be expressive enough to capture complex nonlinear dynamics, and (2) they should maintain a notion of interpretability often only warranted by simpler linear models. In this paper, we develop an approach that balances these two objectives: the Gaussian Process Switching Linear Dynamical System (gpSLDS). Our method builds on previous work modeling the latent state evolution via a stochastic differential equation whose nonlinear dynamics are described by a Gaussian process (GP-SDEs). We propose a novel kernel function which enforces smoothly interpolated locally linear dynamics, and therefore expresses flexible \u2013 yet interpretable \u2013 dynamics akin to those of recurrent switching linear dynamical systems (rSLDS). Our approach resolves key limitations of the rSLDS such as artifactual oscillations in dynamics near discrete state boundaries, while also providing posterior uncertainty estimates of the dynamics. To fit our models, we leverage a modified learning objective which improves the estimation accuracy of kernel hyperparameters compared to previous GP-SDE ftiting approaches. We apply our method to synthetic data and data recorded in two neuroscience experiments and demonstrate favorable performance in comparison to the rSLDS. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Computations in the brain are thought to be implemented through the dynamical evolution of neural activity. Such computations are typically studied in a controlled experimental setup, where an animal is engaged in a behavioral task with relatively few relevant variables. Consistent with this, empirical neural activity has been reported to exhibit many fewer degrees of freedom than there are neurons in the measured sample during such simple tasks [1]. These observations have driven the use of latent variable models to characterize low-dimensional structure in high-dimensional neural population activity [2, 3]. In this setting, neural activity is often modeled in terms of a low-dimensional latent state that evolves with Markovian dynamics [4\u201314]. It is thought that the latent state evolution is related to the computation of the system, and therefore, insights into how this evolution is shaped through a dynamical system can help us understand the mechanisms underlying computation [15\u201321]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "In practice, choosing an appropriate modeling approach for a given task requires balancing two key criteria. First, statistical models should be expressive enough to capture potentially complex and nonlinear dynamics required to carry out a particular computation. On the other hand, these models should also be interpretable and allow for straightforward post-hoc analyses of dynamics. One model class that strikes this balance is the recurrent switching linear dynamical system (rSLDS) [8]. The rSLDS approximates arbitrary nonlinear dynamics by switching between a finite number of linear dynamical systems. This leads to a powerful and expressive model which maintains the interpretability of linear systems. Because of their flexibility and interpretability, variants of the rSLDS have been used in many neuroscience applications [19, 22\u201328] and are examples of a general set of models aiming to understand nonlinear dynamics using compact and interpretable components [29, 30]. ", "page_idx": 1}, {"type": "text", "text": "However, rSLDS models suffer from several limitations. First, while the rSLDS is a probabilistic model, typical use cases do not capture posterior uncertainty over inferred dynamics. This makes it difficult to judge the extent to which particular features of a fitted model should be relied upon when making inferences about their role in neural computation. Second, the rSLDS often suffers from producing oscillatory dynamics in regions of high uncertainty in the latent space, such as boundaries between linear dynamical regimes. This artifactual behavior can significantly impact the interpretability and predictive performance of the rSLDS. Lastly, the rSLDS does not impose smoothness or continuity assumptions on the dynamics due to its discrete switching formulation. Such assumptions are often natural and useful in the context of modeling realistic neural systems. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we improve upon the rSLDS by introducing the Gaussian Process Switching Linear Dynamical System (gpSLDS). Our method extends prior work on the Gaussian process stochastic differential equation (GP-SDE) model, a continuous-time method that places a Gaussian process (GP) prior on latent dynamics. By developing a novel GP kernel function, we enforce locally linear, interpretable structure in dynamics akin to that of the rSLDS. Our framework addresses the aforementioned modeling limitations of the rSLDS and contributes a new class of priors in the GP-SDE model class. Our paper is organized as follows. Section 2 provides background on GP-SDE and rSLDS models. Section 3 presents our new gpSLDS model and an inference and learning algorithm for fitting these models. In Section 4 we apply the gpSLDS to a synthetic dataset and two datasets from real neuroscience experiments to demonstrate its practical use and competitive performance. We review related work in Section 5 and conclude our paper with a discussion in Section 6. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "2.1 Gaussian process stochastic differential equation models ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Gaussian processes (GPs) define nonparametric distributions over functions. They are a popular choice in machine learning due to their ability to capture nonlinearities and encode reasonable prior assumptions such as smoothness and continuity [31]. Here, we review the GP-SDE, a Bayesian generative model that leverages the expressivity of GPs for inferring latent dynamics [10]. ", "page_idx": 1}, {"type": "text", "text": "Generative model In a GP-SDE, the evolution of the latent state $\\textbf{\\em x}\\in\\mathbb{R}^{K}$ is modeled as a continuous-time SDE which underlies observed neural activity $\\pmb{y}(t_{i})\\in\\mathbb{R}^{D}$ at time-points $t_{i}\\in[0,T]$ . Mathematically, this is expressed as ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d\\pmb{x}=\\pmb{f}(\\pmb{x})d t+\\pmb{\\Sigma}^{\\frac{1}{2}}d\\pmb{w},\\qquad\\mathbb{E}[\\pmb{y}(t_{i})\\mid\\pmb{x}]=\\pmb{g}\\left(\\pmb{C}\\pmb{x}(t_{i})+\\pmb{d}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "The drift function $\\pmb{f}:\\mathbb{R}^{K}\\rightarrow\\mathbb{R}^{K}$ describes the system dynamics, $\\Sigma$ is a noise covariance matrix, and $d\\pmb{w}\\sim\\mathcal{N}(\\mathbf{0},d t\\pmb{I})$ is a Wiener process increment. Parameters $\\boldsymbol{C}\\in\\mathbb{R}^{D\\times K}$ and $\\pmb{d}\\in\\mathbb{R}^{D}$ define an affine mapping from latent to observed space, which is then passed through a pre-specified inverse link function $g(\\cdot)$ . ", "page_idx": 1}, {"type": "text", "text": "A GP prior is used to model each output dimension of the dynamics $\\pmb{f}(\\cdot)$ independently. More formally, if $\\pmb{f}(\\cdot)=\\left[f_{1}(\\cdot),\\ldots,f_{K}(\\cdot)\\right]^{\\intercal}$ , then ", "page_idx": 2}, {"type": "equation", "text": "$$\nf_{k}(\\cdot)\\stackrel{\\mathrm{iid}}{\\sim}\\mathcal{G P}(0,\\kappa^{\\Theta}(\\cdot,\\cdot)),\\quad\\mathrm{for}\\:k=1,\\cdot\\cdot\\cdot\\,,K,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\kappa^{\\Theta}(\\cdot,\\cdot)$ is the kernel for the GP with hyperparameters $\\Theta$ . ", "page_idx": 2}, {"type": "text", "text": "Interpretability GP-SDEs and their variants can infer complex nonlinear dynamics with posterior uncertainty estimates in physical systems across a variety of applications [32\u201334]. However, one limitation of using this method with standard GP kernels, such as the radial basis function (RBF) kernel, is that its expressivity leads to dynamics that are often challenging to interpret. In Duncker et al. [10], this was addressed by conditioning the GP prior of the dynamics $\\pmb{f}(\\cdot)$ on fixed points $\\pmb{f}(\\pmb{x}^{*})=$ 0 and their\u2217 local Jacobians $\\begin{array}{r}{J({\\pmb x}^{*})\\;=\\;\\frac{\\partial}{\\partial{\\pmb x}}{\\pmb f}({\\pmb x})|_{{\\pmb x}={\\pmb x}^{*}}}\\end{array}$ , and subsequently learning the fixed-point locations and the locally-linearized dynamics $J(\\pmb{x}^{\\ast})$ as model parameters. This approach allows for direct estimation of key features of $\\pmb{f}(\\cdot)$ . However, due to its flexibility, it is also prone to finding more fixed points than those included in the prior conditioning, which undermines its overall interpretability. ", "page_idx": 2}, {"type": "text", "text": "2.2 Recurrent switching linear dynamical systems ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The rSLDS models nonlinear dynamics by switching between different sets of linear dynamics [8]. Accordingly, it retains the simplicity and interpretability of linear dynamical systems while providing much more expressive power. For these reasons, variations of the rSLDS are commonly used to model neural dynamics [19, 22\u201329]. ", "page_idx": 2}, {"type": "text", "text": "Generative model The rSLDS is a discrete-time generative model of the following form: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{x}_{t}\\sim\\mathcal{N}(\\pmb{A}_{s_{t}}\\pmb{x}_{t-1}+\\pmb{b}_{s_{t}},\\pmb{Q}_{s_{t}}),\\qquad\\mathbb{E}[\\pmb{y}_{t}\\mid\\pmb{x}_{t}]=g(\\pmb{C}\\pmb{x}_{t}+\\pmb{d})}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where dynamics switch between $J$ distinct linear systems with parameters $\\{A_{j},b_{j},Q_{j}\\}_{j=1}^{J}$ . This is controlled by a discrete state variable $s_{t}\\in\\{1,\\ldots,J\\}$ , which evolves via transition probabilities modeled by a multiclass logistic regression, ", "page_idx": 2}, {"type": "equation", "text": "$$\np(\\boldsymbol{s}_{t}\\mid\\boldsymbol{s}_{t-1},\\boldsymbol{x}_{t-1})\\propto\\exp(\\boldsymbol{w}^{\\top}\\boldsymbol{x}_{t-1}+\\boldsymbol{r}_{s_{t-1}}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The \u201crecurrent\u201d nature of this model comes from the dependence of eq. (4) on latent space locations $\\pmb{x}_{t}$ As such, the rSLDS can be understood as learning a partition of the latent space into $J$ linear dynamical regimes seprated by linear decision boundaries. This serves as important motivation for the parametrization of the gpSLDS, as we describe later. ", "page_idx": 2}, {"type": "text", "text": "Interpretability While the rSLDS has been successfully used in many applications to model nonlinear dynamical systems, it suffers from a few practical limitations. First, it often produces unnatural artifacts of modeling nonlinear dynamics with discrete switches between linear systems. For example, it may oscillate between discrete modes with different discontinuous dynamics when a trajectory is near a regime boundary. Next, common fitting techniques for rSLDS models with non-conjugate observations typically treat dynamics as learnable hyperparameters rather than as probabilistic quantities [26], which prevents the model from being able to capture posterior uncertainty over the learned dynamics. Inferring a posterior distribution over dynamics is especially important in many neuroscience applications, where scientists often draw conclusions from discovering key features in latent dynamics, such as fixed points or line attractors. ", "page_idx": 2}, {"type": "text", "text": "3 Gaussian process switching linear dynamical systems ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To address these limitations of the rSLDS, we propose a new class of models called the Gaussian Process Switching Linear Dynamical System (gpSLDS). The gpSLDS combines the modeling advantages of the GP-SDE with the structured flexbility of the rSLDS. We achieve this balance by designing a novel GP kernel function that defines a smooth, locally linear prior on dynamics. While our main focus is on providing an alternative to the rSLDS, the gpSLDS also contributes a new prior which allows for more interpretable learning of dynamics and fixed points than standard priors in the GP-SDE framework (e.g., the RBF kernel). Our implementation of the gpSLDS is available at: https://github.com/lindermanlab/gpslds. ", "page_idx": 2}, {"type": "image", "img_path": "LX1lwP90kt/tmp/fa3be1e68b9e70d87556b5f1df83290eaca967714db72a99c0823d5c8e42975c.jpg", "img_caption": ["Figure 1: SSL kernel and generative model. A. 1D function samples, plotted in different colors, from four kernels: two linear kernels with different hyperparameters, the partition kernel, and the SSL kernel. B. (top) An example $\\pi(x)$ in 2D and (bottom) a sample of dynamics from a SSL kernel in 2D with $\\pi(x)$ as hyperparameters. The $x_{1}$ - and $x_{2}$ - directions of the arrows are given by independent 1D samples of the kernel. C. Schematic of the generative model. Simulated trajectories follow the sampled dynamics. Each trajectory is observed via Poisson process or Gaussian observations. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.1 The smoothly switching linear kernel ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The core innovation of our method is a novel GP kernel, which we call the Smoothly Switching Linear (SSL) kernel. The SSL kernel specifies a GP prior over dynamics that maintains the switching linear structure of rSLDS models, while allowing dynamics to smoothly interpolate between different linear regimes. ", "page_idx": 3}, {"type": "text", "text": "For every pair of locations $\\mathbf{\\boldsymbol{x}},\\mathbf{\\boldsymbol{x}}^{\\prime}\\in\\mathbb{R}^{K}$ , the SSL kernel with $J$ linear regimes is defined as, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\kappa_{\\mathrm{ssl}}(\\pmb{x},\\pmb{x}^{\\prime})=\\sum_{j=1}^{J}\\underbrace{((\\pmb{x}-c_{j})^{\\top}\\pmb{M}(\\pmb{x}^{\\prime}-\\pmb{c}_{j})+\\sigma_{0}^{2})}_{\\kappa_{\\mathrm{lin}}^{(j)}(\\pmb{x},\\pmb{x}^{\\prime})}\\underbrace{\\pi_{j}(\\pmb{x})\\pi_{j}(\\pmb{x}^{\\prime})}_{\\kappa_{\\mathrm{part}}^{(j)}(\\pmb{x},\\pmb{x}^{\\prime})}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\pmb{c}_{j}\\in\\mathbb{R}^{K}$ , $M\\in\\mathbb{R}^{K\\times K}$ is a diagonal positive semi-definite matrix, $\\sigma_{0}^{2}\\in\\mathbb{R}_{+}$ , and $\\pi_{j}(\\pmb{x})\\geq0$ with $\\begin{array}{r}{\\sum_{j=1}^{J}\\pi_{j}(\\pmb{x})=1}\\end{array}$ . To gain an intuitive understanding of the SSL kernel, we will separately analyze each of the two terms in the summands. ", "page_idx": 3}, {"type": "text", "text": "The first term, $\\kappa_{\\mathrm{lin}}^{(j)}(\\boldsymbol{x},\\boldsymbol{x}^{\\prime})$ , is a standard linear kernel which defines a GP distribution over linear functions [31]. The superscript $j$ denotes that it is the linear prior on the dynamics in regime $j$ . $_M$ controls the variance of the function\u2019s slope in each input dimension, and $c_{j}$ is such that the variance of the function achieves its minimum value of $\\sigma_{0}^{2}$ at $\\boldsymbol{x}=\\boldsymbol{c}_{j}$ . We expand on the relationship between the linear kernel and linear dynamical systems in Appendix A. ", "page_idx": 3}, {"type": "text", "text": "The second term is what we define as the partition kernel, $\\kappa_{\\mathrm{part}}^{(j)}(\\pmb{x},\\pmb{x}^{\\prime})$ , which gives the gpSLDS its switching structure. We interpret $\\pi(\\pmb{x})=\\left[\\pi_{1}(\\pmb{x})\\quad.\\ldots\\quad\\pi_{J}(\\pmb{x})\\right]^{\\intercal}$ as parametrizing a categorical distribution over $J$ linear regimes akin to the discrete switching variables in the rSLDS. We model $\\pi(x)$ as a multiclass logistic regression with decision boundaries $\\{w_{j}^{\\top}\\phi(x)=0\\mid j=1,\\ldots,J-1\\}$ , where $\\phi(x)$ is any feature transformation of $\\textbf{\\em x}$ . This yields random functions which are locally constant and smoothly interpolate at the decision boundaries. More formally, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pi_{j}(\\pmb{x})=\\frac{\\exp(\\pmb{w}_{j}^{\\top}\\phi(\\pmb{x})/\\tau)}{1+\\sum_{j=1}^{J-1}\\exp(\\pmb{w}_{j}^{\\top}\\phi(\\pmb{x})/\\tau)},\\quad j=1,\\ldots,J\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $w_{J}=0$ . The hyperparameter $\\tau\\in\\mathbb{R}_{+}$ controls the smoothness of the decision boundaries. As $\\tau\\rightarrow0$ , $\\pi(x)$ approaches a one-hot vector which produces piecewise constant functions with sharp boundaries, and as $\\tau\\rightarrow\\infty$ the boundaries become more uniform. While we focus on the parametrization in eq. (6) for the experiments in this paper, we note that in principle any classification method can be used, such as another GP or a neural network. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "The SSL kernel in eq. (5) naturally combines aspects of the linear and partition kernels via $\\kappa_{\\mathrm{lin}}^{(j)}(\\pmb{x},\\pmb{x}^{\\prime})\\bar{\\kappa}_{\\mathrm{part}}^{(j)}(\\pmb{x},\\pmb{x}^{\\prime})$ eknefronrecless,  liwnheiacrhit yh aisn g iiontnusi tiwvhee rien $\\pi_{j}(x)$ aitsi ocnl o[s3e5 t].o  1T. hSe upmromdiuncgt  okveerrn $J$ $J$   \nwith knots determined by $\\pi(x)$ . We note that our kernel is reminiscent of the one in Pfingsten et al. [36], which uses a GP classifier as a prior for $\\pi(x)$ and applies their kernel to a GP regression setting. Here, our work differs in that we explicitly enforce linearity in each regime and draw a novel connection to switching models like the rSLDS. ", "page_idx": 4}, {"type": "text", "text": "Figure 1A depicts samples from each kernel in 1D. Figure 1B shows how a SSL kernel with $J=2$ linear regimes separated by decision boundary $x_{1}^{2}+x_{2}^{2}=4$ (top) produces a structured 2D flow field consisting of two linear systems, with $x_{1}$ - and $x_{2}$ - directions determined by 1D function samples (bottom). ", "page_idx": 4}, {"type": "text", "text": "3.2 The gpSLDS generative model ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The full generative model for the gpSLDS incorporates the SSL kernel in eq. (5) into a GP-SDE modeling framework. Instead of placing a GP prior with a standard kernel on the system dynamics as in eq. (2), we simply plug in our new SSL kernel so that ", "page_idx": 4}, {"type": "equation", "text": "$$\nf_{k}(\\cdot)\\stackrel{\\mathrm{iid}}{\\sim}\\mathcal{G P}(0,\\kappa_{\\mathrm{ssl}}^{\\Theta}(\\cdot,\\cdot)),\\quad\\mathrm{for}\\:k=1,\\cdot\\cdot\\cdot\\,,K,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the kernel hyperparameters are $\\Theta=\\{M,\\sigma_{0}^{2},\\{c_{j}\\}_{j=1}^{J},\\{w_{j}\\}_{j=1}^{J-1},\\tau\\}$ . We then sample latent states and observations according to the GP-SDE via eq. (1). A schematic of the full generative model is depicted in fig. 1C. ", "page_idx": 4}, {"type": "text", "text": "Incorporating inputs In many modern neuroscience applications, we are often interested in how external inputs to the system, such as experimental stimuli, influence latent states. To this end, we also consider an extension of the model in eq. (1) which incorporates additive inputs of the form, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d\\pmb{x}=(\\pmb{f}(\\pmb{x})+\\pmb{B}\\pmb{v}(t))d t+\\pmb{\\Sigma}^{\\frac{1}{2}}d\\pmb{w},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\pmb{v}(t)\\in\\mathbb{R}^{I}$ is a time-varying, known input signal and $B\\in\\mathbb{R}^{K\\times I}$ maps inputs linearly to the latent space. The latent path inference and learning approaches presented in the following section can naturally be extended to this setting, with updates for $_B$ available in closed form. Further details are provided in Appendix B.3-B.6. ", "page_idx": 4}, {"type": "text", "text": "3.3 Latent path inference and parameter learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "For inference and learning in the gpSLDS, we apply and extend a variational expectationmaximization (vEM) algorithm for GP-SDEs from Duncker et al. [10]. In particular, we propose a modification of this algorithm that dramatically improves the learning accuracy of kernel hyperparameters, which are crucial to the interpretability of the gpSLDS. We outline the main ideas of the algorithm here, though full details can be found in Appendix B. ", "page_idx": 4}, {"type": "text", "text": "As in Duncker et al. [10], we consider a factorized variational approximation to the posterior, ", "page_idx": 4}, {"type": "equation", "text": "$$\nq(\\mathbf{x},\\mathbf{f},\\mathbf{u})=q(\\mathbf{x})\\prod_{k=1}^{K}p(f_{k}\\mid\\mathbf{u}_{k},\\Theta)q(\\mathbf{u}_{k}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where we have augmented the model with sparse inducing points to make inference of $\\boldsymbol{\\textbf{\\textit{f}}}$ tractable [37]. The inducing points are located at $\\{z_{m}\\}_{m=1}^{M}\\subset\\mathbb{R}^{K}$ and take values $(f_{k}(z_{1}),\\ldots,f_{k}(z_{M}))^{\\top}={\\pmb u}_{k}$ . Standard vEM maximizes the evidence lower bound (ELBO) to the marginal log-likelihood $\\log p(\\pmb{y}\\mid\\Theta)$ by alternating between updating the variational posterior $q$ and updating model hyperparameters $\\Theta$ [38]. Using the factorization in eq. (9), we will denote the ELBO as $\\mathcal{L}(\\bar{q}(\\mathbf{\\boldsymbol{x}}),q(\\bar{\\mathbf{u}}),\\bar{\\Theta})$ . ", "page_idx": 4}, {"type": "text", "text": "For inference of $q(x)$ , we follow the approach first proposed by Archambeau et al. [39] and extended by Duncker et al. [10]. Computing the ELBO using this approach requires computing variational expectations of the SSL kernel, which we approximate using Gauss-Hermite quadrature as they are not available in closed form. Full derivations of this step are provided in Appendix B.3. For inference of $q(\\boldsymbol{u})$ , we follow Duncker et al. [10] and choose a Gaussian variational posterior for $q(\\boldsymbol{u}_{k})=\\mathcal{N}(\\boldsymbol{u}_{k}\\mid\\boldsymbol{m}_{u}^{k*},S_{u}^{k*})$ , which admits closed-form updates for the mean $m_{u}^{k*}$ and covariance $S_{u}^{k*}$ given $q(x)$ and $\\Theta$ . Duncker et al. [10] perform these updates before updating $\\Theta$ via gradient ascent on the ELBO in each vEM iteration. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "In our setting, this did not work well in practice. The gpSLDS often exhibits strong dependencies between $q(u)$ and $\\Theta$ , which makes standard coordinate-ascent steps in vEM prone to getting stuck in local maxima. These dependencies arise due to the highly structured nature of our GP prior; small cprheavnegnetss  ivn EthMe  fdreocmis ieosnc abpoinugn dsaurbieosp $\\{w_{j}\\}_{j=1}^{J-1}$ ocnasn  olfe apda rtao mlaertgere  s(paadcvee.r sTeo)  cohvaerncgoesm ien t thhees ep rdiiofrf,i cwulhtiiecsh, we propose a different approach for learning $\\Theta$ : instead of fixing $q(\\boldsymbol{u})$ and performing gradient ascent on the ELBO, we optimize $\\Theta$ by maximizing a partially optimized ELBO, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Theta^{*}=\\underset{\\Theta}{\\arg\\operatorname*{max}}\\left\\{\\underset{q(\\mathbf{u})}{\\operatorname*{max}}\\mathcal{L}(q(\\pmb{x}),q(\\pmb{u}),\\Theta)\\right\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Due to the conjugacy of the model, the inner maximization can be performed analytically. This approach circumvents local optima that plague coordinate ascent on the standard ELBO. While other similar approaches exploit model conjugacy for faster vEM convergence in sparse variational GPs [37, 40] and GP-SDEs [41], our approach is the first to our knowledge that leverages this structure specifically for learning the latent dynamics of a GP-SDE model. We empirically demonstrate the superior performance of our learning algorithm in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "3.4 Recovering predicted dynamics ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "It is straightforward to obtain the approximate posterior distribution over $\\pmb{f}^{*}:=\\pmb{f}(\\pmb{x}^{*})$ evaluated at any new location $x^{*}$ . Under the assumption that $f^{*}$ only depends on the inducing points, we can use the approximation $\\begin{array}{r}{q(f^{*})=\\prod_{k=1}^{K}\\int p(f_{k}^{*}\\mid\\pmb{u}_{k},\\Theta)q(\\pmb{u}_{k})\\,\\mathrm{d}\\pmb{u}_{k}}\\end{array}$ which can be computed in closed-form using properties of conditi onal Gaussian distributions. For a batch of points $\\{\\pmb{x}_{i}^{*}\\}_{i=1}^{N}$ , this can be computed in $O(N M^{2})$ time. The full derivation can be found in Appendix B.4. ", "page_idx": 5}, {"type": "text", "text": "This property highlights an appealing feature of the gpSLDS over the rSLDS. The gpSLDS infers a posterior distribution over dynamics at every point in latent space, even in regions of high uncertainty. Meanwhile, as we shall see later, the rSLDS expresses uncertainty by randomly oscillating between different sets of most-likely linear dynamics, which is much harder to interpret. ", "page_idx": 5}, {"type": "text", "text": "4 Results ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "4.1 Synthetic data ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We begin by applying the gpSLDS to a synthetic dataset consisting of two linear rotational systems, one clockwise and one-counterclockwise, which combine smoothly at $x_{1}=0$ (fig. 2A). We simulate 30 trials of latent states from an SDE as in eq. (1) and then generate Poisson process observations given these latent states for $D=50$ output dimensions (i.e. neurons) over $T=2.5$ seconds (Fig. 2B). To initialize $_{C}$ and $\\pmb{d}$ , we fit a Poisson LDS [4] to data binned at $20\\mathrm{ms}$ with identity dynamics. For the rSLDS, we also bin the data at $20\\mathrm{ms}$ . We then fit the gpSLDS and rSLDS models with $J=2$ linear states using 5 different random initializations for 100 vEM iterations, and choose the ftis with the highest ELBOs in each model class. ", "page_idx": 5}, {"type": "text", "text": "We find that the gpSLDS accurately recovers the true latent trajectories (fig. 2C) as well as the true rotation dynamics and the decision boundary between them (fig. 2D). We determine this decision boundary by thresholding the learned $\\pi(x)$ at 0.5. In addition, we can obtain estimates of fixed point locations by computing the posterior probability $\\begin{array}{r}{\\prod_{k=1}^{K}\\mathbb{P}_{q(\\pmb{f})}\\big(|f_{k}(\\pmb{x})|<\\epsilon\\big)}\\end{array}$ for a small $\\epsilon>0$ ; the locations $\\textbf{\\em x}$ with high probability are shaded in purple. This reveals that the gpSLDS finds high-probability fixed points which overlap significantly with the true fixed points, denoted by stars. In comparison, both the rSLDS and the GP-SDE with RBF kernel do not learn the correct decision boundary nor the fixed points as accurately (fig. 2E-F). Of particular note, the RBF kernel incorrectly extrapolates and finds a superfluous fixed point outside the region traversed by the true latent states. ", "page_idx": 5}, {"type": "image", "img_path": "LX1lwP90kt/tmp/8409bd3a15b8b4eaac5dee107c7b09e165b3f2072af313fb4cd6a135ea3de2b5.jpg", "img_caption": ["Figure 2: Synthetic data results. A. True dynamics and latent states used to generate the dataset. Dynamics are clockwise and counterclockwise linear systems separated by $x_{1}\\,=\\,0$ . Two latent trajectories are shown on top of a kernel density estimate of the latent states visited by all 30 trials. B. Poisson process observations from an example trial. C. True vs. inferred latent states for the gpSLDS and rSLDS, with $95\\%$ posterior credible intervals. D. Inferred dynamics (pink/green) and two inferred latent trajectories (gray) corresponding to those in Panel A from a gpSLDS fit with 2 linear regimes. The model finds high-probability fixed points (purple) overlapping with true fixed points (stars). E. Analogous plot to D for the GP-SDE model with RBF kernel. Note that this model does not provide a partition of the dynamics. F. rSLDS inferred latents, dynamics, and fixed points (pink/green dots). G. (top) Sampled latents and corresponding dynamics from the gpSLDS, with $95\\%$ posterior credible intervals. (bottom) Same, but for the rSLDS. The pink/green trace represents the most likely dynamics at the sampled latents, colored by discrete switching variable. $H.$ . MSE between true and inferred latents and dynamics for gpSLDS, GP-SDE with RBF kernel, and rSLDS while varying the number of trials in the dataset. Error bars are $\\pm2\\mathrm{SE}$ over 5 random initializations. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 2G illustrates the differences in how the gpSLDS and the rSLDS express uncertainty over dynamics. To the left of the dashed line, we sample latent states starting from $x_{0}\\,=\\,(7,0)$ and plot the corresponding true dynamics. To the right, we simulate latent states $\\pmb{x}_{\\mathsf{s a m p}}$ from the fitted model and plot the true dynamics (in gray) and the learned most likely dynamics (in color) at $\\pmb{x}_{\\mathsf{s a m p}}$ . For a well-fitting model, we would expect the true and learned dynamics to overlap. We see that the gpSLDS produces smooth simulated dynamics that match the true dynamics at $x_{\\mathsf{s a m p}}\\left(t o p\\right)$ . By contrast, the rSLDS expresses uncertainty by oscillating between the two linear dynamical systems, hence producing uninterpretable dynamics at $\\pmb{x}_{\\mathsf{s a m p}}$ (bottom). This region of uncertainty overlaps with the $x_{1}=0$ boundary, suggesting that the rSLDS fails to capture the smoothly interpolating dynamics present in the true system. ", "page_idx": 6}, {"type": "text", "text": "Next, we perform quantitative comparisons between the three competing methods (fig. 2H). We find that both continuous-time methods consistently outperform the rSLDS on both metrics, suggesting that these methods are likely more suitable for modeling Poisson process data. Moreover, the gpSLDS better recovers dynamics compared to the RBF kernel, illustrating that the correct inductive bias can yield performance gains over a more flexible prior, especially in a data-limited setting. ", "page_idx": 6}, {"type": "text", "text": "Lastly, we note that the gpSLDS can achieve more expressive power than the rSLDS by learning nonlinear decision boundaries between linear regimes, for instance by incorporating nonlinear features into $\\phi(x)$ in eq. (6). We demonstrate this feature for a 2D limit cycle in Appendix D. ", "page_idx": 6}, {"type": "text", "text": "4.2 Application to hypothalamic neural population recordings during aggression ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we revisit the analyses of Nair et al. [27], which applied dynamical systems models to neural recordings during aggressive behavior in mice. To do this, we reanalyze a dataset which consists of calcium imaging of ventromedial hypothalamus neurons from a mouse interacting with two consecutive intruders. The recording was collected from 104 neurons at $15\\ \\mathrm{Hz}$ over ${\\sim}343$ seconds (i.e. 5140 time bins). Nair et al. [27] found that an rSLDS fti to this data learns dynamics that form an approximate line attractor corresponding to an aggressive internal state (fig. 3A). Here, we supplement this analysis by using the gpSLDS to directly assess model confidence about this finding. ", "page_idx": 6}, {"type": "image", "img_path": "LX1lwP90kt/tmp/21ece72add7074205e1a92bf7a8123ff26c3d67e35f87a5d04122089b48f4e02.jpg", "img_caption": ["Figure 3: Results on hypothalamic data from Nair et al. [27]. In each of the panels A-C, flow field arrow widths are scaled by the magnitude of dynamics for clarity of visualization. A. rSLDS inferred latents and most likely dynamics. The presumed location of the line attractor from [27] is marked with a red box. B. gpSLDS inferred latents and most likely dynamics in latent space. Background is colored by posterior standard deviation of dynamics averaged across latent dimensions, which adjusts relative to the presence of data in the latent space. C. Posterior probability of slow points in gpSLDS, which validates line-attractor like dynamics, as marked by a red box. D. Comparison of in-sample forward simulation $R^{2}$ between gpSLDS, rSLDS, and GP-SDE with RBF kernel. To compute this, we choose initial conditions uniformly spaced 100 time bins apart in both trials, simulate latent states $k$ steps forward according to learned dynamics (with $k$ ranging from 100-1500), and evaluate the $R^{2}$ between predicted and true observations as in Nassar et al. [23]. Error bars are $\\pm2\\mathrm{SE}$ over 5 different initializations. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "For our experiments, we $z$ -score and then split the data into two trials, one for each distinct intruder interacting with the mouse. Following Nair et al. [27], we choose $J=4$ linear regimes to compare the gpSLDS and rSLDS. We choose $K=2$ latent dimensions to aid the visualization of the resulting model fits; we find that even in such low dimensions, both models still recover line attractor-like dynamics. We model the calcium imaging traces as Gaussian emissions on an evenly spaced time grid and initialize $_{C}$ and $\\pmb{d}$ using principal component analysis. We fit models with 5 different initializations for 50 vEM iterations and display the runs with highest forward simulation accuracy (as described in the caption of fig. 3). ", "page_idx": 7}, {"type": "text", "text": "In fig. 3A-B, we find that both methods infer similar latent trajectories and find plausible flow fields that are parsed in terms of simpler linear components. We further demonstrate the ability of the gpSLDS to more precisely identify the line attractor from Nair et al. [27]. To do this, we use the learned $q(f)$ to compute the posterior probability of slow dynamics on a dense $(80\\times80)$ grid of points in the latent space using the procedure in Section 3.4. The gpSLDS finds a high-probability region of slow points corresponding to the approximate line attractor found in Nair et al. [27] (fig. 3C). This demonstrates a key advantage of the gpSLDS over the rSLDS: by modeling dynamics probabilistically using a structured GP prior, we can validate the finding of a line attractor with further statistical rigor. Finally, we compare the gpSLDS, rSLDS, and GP-SDE with RBF kernel using an in-sample forward simulation metric (fig. 3D). All three methods retain similar predictive power 500 steps into the future. After that, the gpSLDS performs slightly worse than the RBF kernel; however, it gains interpretability by imposing piecewise linear structure while still outperforming the rSLDS. ", "page_idx": 7}, {"type": "text", "text": "4.3 Application to lateral intraparietal neural recodings during decision making ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In neuroscience, there is considerable interest in understanding how neural dynamics during decision making tasks support the process of making a choice [26, 34, 42\u201347]. In this section, we use the gpSLDS to infer latent dynamics from spiking activity in the lateral intraparietal (LIP) area of monkeys reporting decisions about the direction of motion of a random moving dots stimulus with varying degrees of motion coherence [42, 43]. The animal indicated its choice of net motion direction (either left or right) via a saccade. On some trials, a $100\\mathrm{ms}$ pulse of weak motion, randomly oriented to the left or right, was also presented to the animal. Here, we model the dynamics of 58 neurons recorded across 50 trials consisting of net motion coherence strengths in $\\{-.512,-.128,0.0,.128,.512\\}$ , where the sign corresponds to the net movement direction of the stimulus. We only consider data $200\\mathrm{ms}$ after motion onset, corresponding to the start of decision-related activity. ", "page_idx": 7}, {"type": "image", "img_path": "LX1lwP90kt/tmp/8928cf1cab3c3b16b4be2d44900c8ae52d949b8ecf1edd2fc435e51a93862cb4.jpg", "img_caption": ["Figure 4: Results on LIP spiking data from a decision-making task in Stine et al. [42]. A. gpSLDS inferred latents colored by coherence, inferred dynamics with background colored by most likely linear regime, and the learned input-driven direction depicted by an orange arrow. B. Projection of latents onto the 1D input-driven axis from Panel A, colored by coherence (top) and choice (bottom). C. Inferred latents with $95\\%$ credible intervals and corresponding $100\\mathrm{ms}$ pulse input for an example trial. D. Posterior variance of dynamics produced by the gpSLDS. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "To capture potential input-driven effects, we fit a version of the gpSLDS described in eq. (8) with $K=2$ latent dimensions and $J=2$ linear regimes over 50 vEM iterations. We encoded the input signal as $\\pm1$ with sign corresponding to pulse direction. In Figure 4A, we find that not only does the gpSLDS capture a distinct visual separation between trials by motion coherence, but it also learns a precise separating decision boundary between the two linear regimes. Our finding is consistent with previous work on a related task, which found that average LIP responses can be represented by a 2D curved manifold [48], though here we take a dynamical systems perspective. Additionally, our model learns an input-driven effect which appears to define a separating axis. To verify this, we project the inferred latent states onto the 1D subspace spanned by the input effect vector. Figure 4B shows that the latent states separate by coherence (top) and by choice (bottom), further suggesting that the pulse input relates to meaningful variation in evidence accumulation for this task. Fig. 4C shows an example latent trajectory aligned with pulse input; during the pulse there is a noticeable change in the latent trajectory. Lastly, in Fig. 4D we find that the gpSLDS expresses high confidence in learned dynamics where latent trajectories are present and low confidence in areas further from this region. ", "page_idx": 8}, {"type": "text", "text": "5 Related work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "There are several related approaches to learning nonlinear latent dynamics in discrete or continuous time. Gaussian process state-space models (GP-SSMs) [49\u201353] can be considered a discrete-time analogue to GP-SDEs. In a GP-SSM, observations are assumed to be regularly sampled and latent states evolve according to a discretized dynamical system with a GP prior. Wang et al. [49] and Turner et al. [50] learned the dynamics of a GP-SSM using maximum a posteriori estimation. Frigola et al. [51] and Eleftheriadis et al. [52] employed a variational approximation with sparse inducing points to infer the latent states and dynamics in a fully Bayesian fashion. In our work, we use a continuous-time framework that more naturally handles irregularly sampled data, such as point-process observations commonly encountered in neural spiking data. Neural ODEs and SDEs [54, 55] use deep neural networks to parametrize the dynamics of a continuous-time system, and have emerged as prominent tools for analyzing large datasets, including those in neuroscience [33, 56\u201358]. While these methods can represent flexible function classes, they are likely to overfit to low-data regimes and may be difficult to interpret. In addition, unlike the gpSLDS, neural ODEs and SDEs do not typically quantify uncertainty of the learned dynamics. ", "page_idx": 8}, {"type": "text", "text": "In the context of dynamical mixture models, Ko\u00a8hs et al. [59, 60] proposed a continuous-time switching model in a GP-SDE framework. This model assumes a latent Markov jump process over time which controls the system dynamics by switching between different SDEs. The switching process models dependence on time, but not location in latent space. In contrast, the gpSLDS does not explicitly represent a latent switching process and rather models switching probabilities as part of the kernel function. The dependence of the kernel on the location in latent space allows for the gpSLDS to partition the latent space into different linear regimes. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "While our work has followed the inference approach of Archambeau et al. [61] and Duncker et al. [10], other methods for latent path inference in nonlinear SDEs have been proposed [32, 41, 62, 63]. Verma et al. [41] parameterized the posterior SDE path using an exponential family-based description. The resulting inference algorithm showed improved convergence of the E-step compared to Archambeau et al. [39]. Course and Nair [32, 62] proposed an amortization strategy that allows the variational update of the latent state to be parallelized over sequence length. In principle, any of these approaches could be applied to inference in the gpSLDS and would be an interesting direction for future work. ", "page_idx": 9}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduced the gpSLDS to infer low-dimensional latent dynamical systems from high-dimensional, noisy observations. By developing a novel kernel for GP-SDE models that defines distributions over smooth locally-linear functions, we were able to relate GP-SDEs to rSLDS models and address key limitations of the rSLDS. Using both simulated and real neural datasets, we demonstrated that the gpSLDS can accurately infer true generative parameters and performs favorably in comparison to rSLDS models and GP-SDEs with other kernel choices. Moreover, our real data examples illustrate the variety of potential practical uses of this method. On calcium imaging traces recorded during aggressive behavior, the gpSLDS reveals dynamics consistent with the hypothesis of a line attractor put forward based on previous rSLDS analyses [27]. On a decision making dataset, the gpSLDS finds latent trajectories and dynamics that clearly separate by motion coherence and choice, providing a dynamical systems view consistent with prior studies [42, 48]. ", "page_idx": 9}, {"type": "text", "text": "In our experiments, we demonstrated the ability of the gpSLDS to recover ground-truth dynamical systems and key dynamical features using fixed settings of hyperparameters: the latent dimensionality $K$ and the number of regimes $J$ . For simulated data, we set hyperparameters to their true values; for real data, we chose hyperparameters based on prior studies and did not further optimize these values. However, for most real neural datasets, we do not know the true underlying dimensionality or optimal number of regimes. To tune these hyperparameters, we can resort to standard techniques for model comparison in the neural latent variable modeling literature. Two common evaluation metrics are forward simulation accuracy [23, 27] and co-smoothing performance [6, 64, 65]. ", "page_idx": 9}, {"type": "text", "text": "While these results are promising, we acknowledge a few limitations of the gpSLDS. First, the memory cost scales exponentially with the size of the latent dimension due to using quadrature methods to approximate expectations of the SSL kernel, which are not available in closed form. This computational limitation renders it difficult to fit the gpSLDS with many (i.e. greater than 3) latent dimensions. One potential direction for future work would be to instead use Monte Carlo methods to approximate kernel expectations for models with larger latent dimensionality. In addition, while both the gpSLDS and rSLDS require choosing a discretization timestep for solving dynamical systems, in practice we find that the gpSLDS requires smaller steps for stable model inference. This allows the gpSLDS to more accurately approximate dynamics with continuous-time likelihoods, at the cost of allocating more time bins during inference. Finally, we acknowledge that traditional variational inference approaches \u2013 such as those employed by the gpSLDS\u2013 tend to underestimate posterior variance due to the KL-divergence-based objective [38]. Carefully assessing biases introduced by our variational approximation to the posterior would be an important topic for future work. ", "page_idx": 9}, {"type": "text", "text": "Overall, the gpSLDS provides a general modeling approach for discovering latent dynamics of noisy measurements in an intepretable and fully probabilistic manner. We expect that our extension will provide a useful addition to the rSLDS and related methods on future analyses of neural data. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by grants from the NIH BRAIN Initiative (U19NS113201, R01NS131987, & RF1MH133778) and the NSF/NIH CRCNS Program (R01NS130789), as well as fellowships from the Simons Collaboration on the Global Brain, the Wu Tsai Neurosciences Institute, the Alfred P. Sloan Foundation, and the McKnight Foundation. We thank Barbara Engelhardt, Julia Palacios, and the members of the Linderman Lab for helpful feedback throughout this project. The authors have no competing interests to declare. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Peiran Gao and Surya Ganguli. On simplicity and complexity in the brave new world of large-scale neuroscience. Current Opinion in Neurobiology, 32:148\u2013155, 2015.   \n[2] John P Cunningham and Byron M Yu. Dimensionality reduction for large-scale neural recordings. Nature Neuroscience, 17(11):1500\u20131509, 2014.   \n[3] Shreya Saxena and John P Cunningham. Towards the neural population doctrine. Current Opinion in Neurobiology, 55:103\u2013111, 2019.   \n[4] Anne C Smith and Emery N Brown. Estimating a state-space model from point process observations. Neural Computation, 15(5):965\u2013991, 2003.   \n[5] Liam Paninski, Yashar Ahmadian, Daniel Gil Ferreira, Shinsuke Koyama, Kamiar Rahnama Rad, Michael Vidne, Joshua Vogelstein, and Wei Wu. A new look at state-space models for neural data. Journal of Computational Neuroscience, 29:107\u2013126, 2010.   \n[6] Jakob H Macke, Lars Buesing, John P Cunningham, Byron M Yu, Krishna V Shenoy, and Maneesh Sahani. Empirical models of spiking in neural populations. Advances in Neural Information Processing Systems, 24, 2011.   \n[7] Evan Archer, Il Memming Park, Lars Buesing, John Cunningham, and Liam Paninski. Black box variational inference for state space models. arXiv preprint arXiv:1511.07367, 2015.   \n[8] Scott Linderman, Matthew Johnson, Andrew Miller, Ryan Adams, David Blei, and Liam Paninski. Bayesian learning and inference in recurrent switching linear dynamical systems. In Artificial Intelligence and Statistics, pages 914\u2013922. PMLR, 2017.   \n[9] Chethan Pandarinath, Daniel J O\u2019Shea, Jasmine Collins, Rafal Jozefowicz, Sergey D Stavisky, Jonathan C Kao, Eric M Trautmann, Matthew T Kaufman, Stephen I Ryu, Leigh R Hochberg, et al. Inferring single-trial neural population dynamics using sequential auto-encoders. Nature Methods, 15(10):805\u2013815, 2018.   \n[10] Lea Duncker, Gergo Bohner, Julien Boussard, and Maneesh Sahani. Learning interpretable continuous-time models of latent stochastic dynamical systems. In International Conference on Machine Learning, pages 1726\u20131734. PMLR, 2019.   \n[11] Marine Schimel, Ta-Chu Kao, Kristopher T Jensen, and Guillaume Hennequin. ilqr-vae: controlbased learning of input-driven dynamics with applications to neural data. In International Conference on Learning Representations, 2021.   \n[12] Mikhail Genkin, Owen Hughes, and Tatiana A Engel. Learning non-stationary langevin dynamics from stochastic observations of latent trajectories. Nature Communications, 12(1): 5986, 2021.   \n[13] Christopher Langdon and Tatiana A Engel. Latent circuit inference from heterogeneous neural responses during cognitive tasks. bioRxiv, pages 2022\u201301, 2022.   \n[14] Matthew Dowling, Yuan Zhao, and Il Memming Park. Large-scale variational gaussian statespace models. arXiv preprint arXiv:2403.01371, 2024.   \n[15] Saurabh Vyas, Matthew D Golub, David Sussillo, and Krishna V Shenoy. Computation through neural population dynamics. Annual Review of Neuroscience, 43:249\u2013275, 2020.   \n[16] Lea Duncker and Maneesh Sahani. Dynamics on the manifold: Identifying computational dynamical activity from neural population recordings. Current Opinion in Neurobiology, 70: 163\u2013170, 2021.   \n[17] David Sussillo and Omri Barak. Opening the black box: low-dimensional dynamics in highdimensional recurrent neural networks. Neural Computation, 25(3):626\u2013649, 2013.   \n[18] Daniel J O\u2019Shea, Lea Duncker, Werapong Goo, Xulu Sun, Saurabh Vyas, Eric M Trautmann, Ilka Diester, Charu Ramakrishnan, Karl Deisseroth, Maneesh Sahani, et al. Direct neural perturbations reveal a dynamical mechanism for robust computation. bioRxiv, pages 2022\u201312, 2022.   \n[19] Joana Soldado-Magraner, Valerio Mante, and Maneesh Sahani. Inferring context-dependent computations through linear approximations of prefrontal cortex dynamics. bioRxiv, pages 2023\u201302, 2023.   \n[20] Amit Vinograd, Aditya Nair, Joseph Kim, Scott W Linderman, and David J Anderson. Causal evidence of a line attractor encoding an affective state. Nature, pages 1\u20133, 2024.   \n[21] George Mountoufaris, Aditya Nair, Bin Yang, Dong-Wook Kim, Amit Vinograd, Samuel Kim, Scott W Linderman, and David J Anderson. A line attractor encoding a persistent internal state requires neuropeptide signaling. Cell, 187(21):5998\u20136015, 2024.   \n[22] Biljana Petreska, Byron M Yu, John P Cunningham, Gopal Santhanam, Stephen Ryu, Krishna V Shenoy, and Maneesh Sahani. Dynamical segmentation of single trials from population neural data. Advances in Neural Information Processing Systems, 24, 2011.   \n[23] Josue Nassar, Scott Linderman, Monica Bugallo, and Il Memming Park. Tree-structured recurrent switching linear dynamical systems for multi-scale modeling. In International Conference on Learning Representations, 2018.   \n[24] Jalil Taghia, Weidong Cai, Srikanth Ryali, John Kochalka, Jonathan Nicholas, Tianwen Chen, and Vinod Menon. Uncovering hidden brain state dynamics that regulate performance and decision-making during cognition. Nature Communications, 9(1):2505, 2018.   \n[25] Antonio C Costa, Tosif Ahamed, and Greg J Stephens. Adaptive, locally linear models of complex dynamics. Proceedings of the National Academy of Sciences, 116(5):1501\u20131510, 2019.   \n[26] David Zoltowski, Jonathan Pillow, and Scott Linderman. A general recurrent state space framework for modeling neural dynamics during decision-making. In International Conference on Machine Learning, pages 11680\u201311691. PMLR, 2020.   \n[27] Aditya Nair, Tomomi Karigo, Bin Yang, Surya Ganguli, Mark J Schnitzer, Scott W Linderman, David J Anderson, and Ann Kennedy. An approximate line attractor in the hypothalamus encodes an aggressive state. Cell, 186(1):178\u2013193, 2023.   \n[28] Mengyu Liu, Aditya Nair, Nestor Coria, Scott W Linderman, and David J Anderson. Encoding of female mating dynamics by a hypothalamic line attractor. Nature, pages 1\u20133, 2024.   \n[29] Jimmy Smith, Scott Linderman, and David Sussillo. Reverse engineering recurrent neural networks with Jacobian switching linear dynamical systems. Advances in Neural Information Processing Systems, 34:16700\u201316713, 2021.   \n[30] Noga Mudrik, Yenho Chen, Eva Yezerets, Christopher J Rozell, and Adam S Charles. Decomposed linear dynamical systems (dlds) for learning the latent components of neural dynamics. Journal of Machine Learning Research, 25(59):1\u201344, 2024.   \n[31] Christopher KI Williams and Carl Edward Rasmussen. Gaussian processes for machine learning, volume 2. MIT press Cambridge, MA, 2006.   \n[32] Kevin Course and Prasanth B Nair. State estimation of a physical system with unknown governing equations. Nature, 622(7982):261\u2013267, 2023.   \n[33] Timothy Doyeon Kim, Thomas Zhihao Luo, Tankut Can, Kamesh Krishnamurthy, Jonathan W Pillow, and Carlos D Brody. Flow-field inference from neural data using deep recurrent networks. bioRxiv, 2023.   \n[34] Thomas Zhihao Luo, Timothy Doyeon Kim, Diksha Gupta, Adrian G Bondy, Charles D Kopec, Verity A Elliot, Brian DePasquale, and Carlos D Brody. Transitions in dynamical regime and neural mode underlie perceptual decision-making. bioRxiv, pages 2023\u201310, 2023.   \n[35] David Duvenaud. Automatic model construction with Gaussian processes. PhD thesis, 2014.   \n[36] Tobias Pfingsten, Malte Kuss, and Carl Edward Rasmussen. Nonstationary gaussian process regression using a latent extension of the input space. In Eighth World Meeting of the International Society for Bayesian Analysis (ISBA 2006), 2006.   \n[37] Michalis Titsias. Variational learning of inducing variables in sparse gaussian processes. In Artificial Intelligence and Statistics, pages 567\u2013574. PMLR, 2009.   \n[38] David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. Journal of the American Statistical Association, 112(518):859\u2013877, 2017.   \n[39] Cedric Archambeau, Dan Cornford, Manfred Opper, and John Shawe-Taylor. Gaussian process approximations of stochastic differential equations. In Gaussian Processes in Practice, pages 1\u201316. PMLR, 2007.   \n[40] Vincent Adam, Paul Chang, Mohammad Emtiyaz E Khan, and Arno Solin. Dual parameterization of sparse variational gaussian processes. Advances in Neural Information Processing Systems, 34:11474\u201311486, 2021.   \n[41] Prakhar Verma, Vincent Adam, and Arno Solin. Variational gaussian process diffusion processes. In International Conference on Artificial Intelligence and Statistics, pages 1909\u20131917. PMLR, 2024.   \n[42] Gabriel M Stine, Eric M Trautmann, Danique Jeurissen, and Michael N Shadlen. A neural mechanism for terminating decisions. Neuron, 111(16):2601\u20132613, 2023.   \n[43] Jamie D Roitman and Michael N Shadlen. Response of neurons in the lateral intraparietal area during a combined visual discrimination reaction time task. Journal of Neuroscience, 22(21): 9475\u20139489, 2002.   \n[44] Jochen Ditterich. Stochastic models of decisions about motion direction: behavior and physiology. Neural Networks, 19(8):981\u20131012, 2006.   \n[45] Joshua I Gold and Michael N Shadlen. The neural basis of decision making. Annu. Rev. Neurosci., 30:535\u2013574, 2007.   \n[46] Kenneth W Latimer, Jacob L Yates, Miriam LR Meister, Alexander C Huk, and Jonathan W Pillow. Single-trial spike trains in parietal cortex reveal discrete steps during decision-making. Science, 349(6244):184\u2013187, 2015.   \n[47] Mikhail Genkin, Krishna V Shenoy, Chandramouli Chandrasekaran, and Tatiana A Engel. The dynamics and geometry of choice in premotor cortex. bioRxiv, 2023.   \n[48] Gouki Okazawa, Christina E Hatch, Allan Mancoo, Christian K Machens, and Roozbeh Kiani. Representational geometry of perceptual decisions in the monkey parietal cortex. Cell, 184(14): 3748\u20133761, 2021.   \n[49] Jack Wang, Aaron Hertzmann, and David J Fleet. Gaussian process dynamical models. Advances in Neural Information Processing Systems, 18, 2005.   \n[50] Ryan Turner, Marc Deisenroth, and Carl Rasmussen. State-space inference and learning with gaussian processes. In Artificial Intelligence and Statistics, pages 868\u2013875. JMLR Workshop and Conference Proceedings, 2010.   \n[51] Roger Frigola, Yutian Chen, and Carl Edward Rasmussen. Variational gaussian process statespace models. Advances in Neural Information Processing Systems, 27, 2014.   \n[52] Stefanos Eleftheriadis, Tom Nicholson, Marc Deisenroth, and James Hensman. Identification of gaussian process state space models. Advances in Neural Information Processing Systems, 30, 2017.   \n[53] Gergo Bohner and Maneesh Sahani. Empirical fixed point bifurcation analysis. arXiv preprint arXiv:1807.01486, 2018.   \n[54] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. Advances in Neural Information Processing Systems, 31, 2018.   \n[55] Yulia Rubanova, Ricky TQ Chen, and David K Duvenaud. Latent ordinary differential equations for irregularly-sampled time series. Advances in Neural Information Processing Systems, 32, 2019.   \n[56] Timothy D Kim, Thomas Z Luo, Jonathan W Pillow, and Carlos D Brody. Inferring latent dynamics underlying neural population activity via neural differential equations. In International Conference on Machine Learning, pages 5551\u20135561. PMLR, 2021.   \n[57] Timothy Doyeon Kim, Tankut Can, and Kamesh Krishnamurthy. Trainability, expressivity and interpretability in gated neural odes. In International Conference on Machine Learning, pages 16393\u201316423, 2023.   \n[58] Christopher Versteeg, Andrew R Sedler, Jonathan D McCart, and Chethan Pandarinath. Expressive dynamics models with nonlinear injective readouts enable reliable recovery of latent features from neural activity. arXiv preprint arXiv:2309.06402, 2023.   \n[59] Lukas Ko\u00a8hs, Bastian Alt, and Heinz Koeppl. Variational inference for continuous-time switching dynamical systems. Advances in Neural Information Processing Systems, 34:20545\u201320557, 2021.   \n[60] Lukas K\u00a8ohs, Bastian Alt, and Heinz Koeppl. Markov chain monte carlo for continuous-time switching dynamical systems. In International Conference on Machine Learning, pages 11430\u2013 11454. PMLR, 2022.   \n[61] C\u00b4edric Archambeau, Manfred Opper, Yuan Shen, Dan Cornford, and John Shawe-Taylor. Variational inference for diffusion processes. Advances in Neural Information Processing Systems, 20, 2007.   \n[62] Kevin Course and Prasanth Nair. Amortized reparametrization: Efficient and scalable variational inference for latent sdes. Advances in Neural Information Processing Systems, 36, 2024.   \n[63] Botond Cseke, David Schnoerr, Manfred Opper, and Guido Sanguinetti. Expectation propagation for continuous time stochastic processes. Journal of Physics A: Mathematical and Theoretical, 49(49):494002, 2016.   \n[64] Anqi Wu, Stan Pashkovski, Sandeep R Datta, and Jonathan W Pillow. Learning a latent manifold of odor representations from neural responses in piriform cortex. Advances in Neural Information Processing Systems, 31, 2018.   \n[65] Stephen Keeley, Mikio Aoi, Yiyi Yu, Spencer Smith, and Jonathan W Pillow. Identifying signal and noise structure in neural population activity with gaussian process factor models. Advances in Neural Information Processing Systems, 33:13795\u201313805, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Supplementary Material ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "\u2013 Appendix A: Relationship between linear kernel and linear dynamical systems   \n\u2013 Appendix B: Inference and learning   \n\u2013 Appendix C: Empirical results for new learning objective   \n\u2013 Appendix D: Additional synthetic data results ", "page_idx": 14}, {"type": "text", "text": "A Relationship between linear kernel and linear dynamical systems ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Here, we draw a mathematical connection between functions sampled from a GP with a linear kernel, ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(\\cdot)\\sim\\mathcal{G P}(0,\\kappa_{\\operatorname*{lin}}(\\cdot,\\cdot)),\\qquad\\kappa_{\\operatorname*{lin}}(x,x^{\\prime})=(x-c)^{\\mathsf{T}}M(x-c)+\\sigma_{0}^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "and functions of the form ", "page_idx": 15}, {"type": "equation", "text": "$$\nf(\\pmb{x})=\\beta^{\\top}\\pmb{x}+\\beta_{0}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This connection is useful for understanding the relationship between our model formulation and the typical linear dynamical systems formulation as in the rSLDS. In particular, we will show that the linear kernel in eq. (11) equivalently places a prior on $\\beta$ and $\\beta_{0}$ in eq. (12), in a similar manner to Bayesian linear regression. ", "page_idx": 15}, {"type": "text", "text": "By definition of a GP, for any finite set of input locations $\\{x_{i}\\}_{i=1}^{N}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\left[f(\\pmb{x}_{1})\\quad f(\\pmb{x}_{2})\\quad.\\mathrm{~.~.~}\\quad f(\\pmb{x}_{N})\\right]^{\\intercal}\\sim\\mathcal{N}(\\pmb{0},\\Phi)}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\Phi_{i j}=\\kappa_{\\mathrm{lin}}({\\pmb x}_{i},{\\pmb x}_{j})$ . Equivalently, for every pair $i,j=1,\\ldots,N$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Cov}(f(\\boldsymbol{x}_{i}),f(\\boldsymbol{x}_{j}))=(\\boldsymbol{x}_{i}-\\boldsymbol{c})^{\\top}M(\\boldsymbol{x}_{j}-\\boldsymbol{c})+\\sigma_{0}^{2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad=x_{i}^{\\top}M\\boldsymbol{x}_{j}-c^{\\top}M(\\boldsymbol{x}_{i}+\\boldsymbol{x}_{j})+c^{\\top}c+\\sigma_{0}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Under eq. (12), treating $\\beta$ and $\\beta_{0}$ as random, this would become ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathrm{Cov}(\\beta^{\\mathsf{T}}x_{i}+\\beta_{0},\\beta^{\\mathsf{T}}x_{j}+\\beta_{0})=x_{i}^{\\mathsf{T}}\\mathrm{Cov}(\\beta,\\beta)x_{j}+\\mathrm{Cov}(\\beta_{0},\\beta)(x_{i}+x_{j})+\\mathrm{Var}(\\beta_{0})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Equation (15) and eq. (16) are equivalent and eq. (13) is satified if and only if ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left[\\beta\\atop\\beta_{0}\\right]\\sim{\\mathcal{N}}\\left(\\mathbf{0},\\left[{\\begin{array}{c c}{M}&{-M c}\\\\ {-c^{\\top}M}&{c^{\\top}c+\\sigma_{0}^{2}}\\end{array}}\\right]\\right)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This shows that a GP with a linear kernel is equivalent to a Bayesian linear regression model with a prior on coefficients of the form in eq. (17). ", "page_idx": 15}, {"type": "text", "text": "B Inference and learning ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We present the full inference and learning details for the gpSLDS with additive inputs, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d x=(f(x)+B v(t))d t+\\Sigma^{\\frac{1}{2}}d w,\\qquad\\mathbb{E}[y(t_{i})\\mid x]=g\\left(C x(t_{i})+d\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Our approach primarily follows that of Duncker et al. [10]. We use the following notation from Duncker et al. [10] throughout this section: ", "page_idx": 15}, {"type": "text", "text": "\u2022 $\\langle h(\\cdot)\\rangle_{p(\\cdot)}$ denotes the expectation of $h(\\cdot)$ with respect to the distribution $p(\\cdot)$ . ", "page_idx": 15}, {"type": "text", "text": "$K_{x z}~\\in~\\mathbb{R}^{N_{1}\\times N_{2}}$ denotes the covariance matrix defined by our kernel $\\kappa_{\\mathrm{ssl}}(\\cdot,\\cdot)$ for two batches of points $\\{{\\pmb x}_{i}\\}_{i=1}^{N_{1}}$ and $\\{z_{i}\\}_{i=1}^{N_{2}}$ . Specifically, $[K_{x z}]_{i j}=\\kappa_{\\mathrm{ssl}}(\\pmb{x}_{i},z_{j})$ . If one of these batches has only one point, e.g. $N_{1}=1$ , we denote the corresponding covariance vector as $\\pmb{k}_{x z}\\in\\mathbb{R}^{1\\times N_{2}}$ . Note, these quantities depend on $\\Theta$ through the kernel; we omit writing this dependence for brevity. ", "page_idx": 15}, {"type": "text", "text": "B.1 Augmenting the generative model ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Following Duncker et al. [10], we first augment our generative model with sparse inducing points $\\{\\pmb{u}_{k}\\}_{k=1}^{K}\\subset\\ \\mathbb{R}^{M}$ at input locations $\\{z_{m}\\}_{m=1}^{M}\\subset\\bar{\\mathbb{R}}^{D}$ . Inducing points can be seen as pseudoobservations of $\\pmb{f}(\\cdot)$ at input locations $\\{z_{m}\\}_{m=1}^{M}$ . They allow for tractable inference of $\\pmb{f}$ at any new batch of $N$ input locations by reducing computational complexity from $O(N^{3})$ to $O(N M^{2})$ , where typically $M$ is chosen to be much smaller than $N$ [37]. After this augmentation, the joint likelihood of our model becomes ", "page_idx": 15}, {"type": "equation", "text": "$$\np(\\boldsymbol{y},\\boldsymbol{x},f,\\boldsymbol{u}\\mid\\Theta)=p(\\boldsymbol{y}\\mid\\boldsymbol{x})p(\\boldsymbol{x}\\mid f)\\prod_{k=1}^{K}p(f_{k}\\mid\\boldsymbol{u}_{k},\\Theta)p(\\boldsymbol{u}_{k}\\mid\\Theta).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Treating $\\pmb{u}_{k}$ as pseudo-observations, we assume the following augmented prior: ", "page_idx": 16}, {"type": "equation", "text": "$$\np(\\boldsymbol{u}_{k}\\mid\\Theta)=\\mathcal{N}(\\boldsymbol{u}_{k}\\mid\\mathbf{0},K_{z z}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then we can view $f_{k}(\\cdot)\\mid u_{k}$ as a new GP conditioned on $\\pmb{u}_{k}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\nf_{k}(\\cdot)\\mid\\!\\!u_{k}\\sim\\mathcal{G P}(\\mu_{f_{k}|u_{k}}(\\cdot),\\kappa_{f_{k}|u_{k}}(\\cdot,\\cdot)),\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mu_{f_{k}|{\\pmb u}_{k}}({\\pmb x})=k_{x z}K_{z z}^{-1}{\\pmb u}_{k}\\qquad\\qquad\\qquad}\\\\ {\\kappa_{f_{k}|{\\pmb u}_{k}}({\\pmb x},{\\pmb x}^{\\prime})=\\kappa_{\\mathrm{ssl}}({\\pmb x},{\\pmb x}^{\\prime})-k_{x z}K_{z z}^{-1}k_{z x}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B.2 Variational lower bound ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "As in Duncker et al. [10], we consider a variational approximation to the posterior of the form ", "page_idx": 16}, {"type": "equation", "text": "$$\nq(\\mathbf{x},\\mathbf{f},\\mathbf{u})=q(\\mathbf{x})\\prod_{k=1}^{K}p(f_{k}\\mid\\mathbf{u}_{k},\\Theta)q(\\mathbf{u}_{k}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Using this factorization, we derive the ELBO to the marginal log-likelihood of our model. By Jensen\u2019s inequality, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\log p(\\pmb{y}\\mid\\Theta)=\\log\\int p(\\pmb{y}\\mid\\pmb{x})p(\\pmb{x}\\mid\\pmb{f})p(\\pmb{f}\\mid\\mathfrak{u},\\Theta)p(\\pmb{u}\\mid\\Theta)d\\pmb{x}d\\pmb{f}d\\pmb{u}}\\\\ {\\displaystyle\\ge\\int q(\\pmb{x},\\pmb{f},\\pmb{u})\\log\\frac{p(\\pmb{y}\\mid\\pmb{x})p(\\pmb{x}\\mid\\mathcal{f})p(\\pmb{f}\\mid\\pmb{u},\\Theta)p(\\pmb{u}\\mid\\Theta)}{q(\\pmb{x},\\pmb{f},\\pmb{u})}d\\pmb{x}d\\pmb{f}d\\pmb{u}}\\\\ {\\displaystyle=\\int q(\\pmb{x},\\pmb{f},\\pmb{u})\\log\\frac{p(\\pmb{y}\\mid\\pmb{x})p(\\pmb{x}\\mid\\mathcal{f})\\prod_{k=1}^{K}p(\\pmb{u}_{k}\\mid\\Theta)}{q(\\pmb{x})\\prod_{k=1}^{K}q(\\pmb{u}_{k})}d\\pmb{x}d\\pmb{f}d\\pmb{u}}\\\\ {\\displaystyle=\\langle\\log p(\\pmb{y}\\mid\\pmb{x})\\rangle_{q(\\pmb{x})}-\\langle\\mathrm{KL}[q(\\pmb{x})\\|p(\\pmb{x}\\mid\\mathcal{f})]_{q(\\pmb{f})}-\\sum_{k=1}^{K}\\mathrm{KL}[q(\\pmb{u}_{k})\\|p(\\pmb{u}_{k}\\mid\\Theta)]}\\\\ {\\displaystyle:=\\mathcal{L}(q(\\pmb{x}),q(\\pmb{u}),\\Theta),}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where ", "page_idx": 16}, {"type": "equation", "text": "$$\nq(\\pmb{f})=\\prod_{k=1}^{K}\\int p(f_{k}\\mid\\pmb{u}_{k},\\Theta)q(\\pmb{u}_{k})d\\pmb{u}_{k}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "B.3 Inference of latent paths ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "To perform inference over the posterior of latent paths $q(x)$ , we follow a method first proposed in Archambeau et al. [39] and extended by Duncker et al. [10]. ", "page_idx": 16}, {"type": "text", "text": "As in Archambeau et al. [39], we choose a posterior distribution $q(x)$ characterized by a Markov Gaussian process, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{q(x):\\{d x=\\underbrace{(-A(t)x(t)+b(t))}_{:=f_{q}(x)}d t+\\Sigma^{\\frac{1}{2}}d w,\\quad x_{0}\\sim\\mathcal{N}(m(0),S(0))\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This distribution satisfies posterior marginals $q(\\mathbf{x}_{t})=\\mathcal{N}(\\mathbf{x}_{t}\\mid m_{t},S_{t})$ which satisfy the differential equations ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{d{m}(t)}{d t}=-A(t){m}(t)+{b}(t)}\\\\ {\\displaystyle\\frac{d{\\pmb S}(t)}{d t}=-A(t){\\pmb S}(t)-{\\pmb S}(t){\\pmb A}(t)^{\\top}+{\\pmb\\Sigma}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Archambeau et al. [39] maximize the ELBO with respect to $q(x)$ subject to the constraints in eq. (25) and eq. (26) using the method of Lagrange multipliers. They show that after applying integration by parts, the Lagrangian becomes ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{L}}=\\mathcal{L}(q(\\pmb{x}),q(\\pmb{u}),\\Theta)-\\mathcal{C}_{1}-\\mathcal{C}_{2}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\mathcal{C}}_{1}=\\displaystyle\\int_{0}^{T}\\left\\{\\lambda(t)^{\\top}(A(t)m(t)-b(t))-\\frac{d\\lambda(t)^{\\top}}{d t}m(t)\\right\\}d t+\\lambda(T)^{\\top}m(T)-\\lambda(0)^{\\top}m(0)}\\\\ {{\\mathcal{C}}_{2}=\\displaystyle\\int_{0}^{T}\\mathrm{Tr}\\left[\\Psi(t)(A(t)S(t)+S(t)A(t)^{\\top}-\\Sigma)-\\frac{d\\Psi(t)}{d t}S(t)\\right]d t}\\\\ {\\quad\\,\\,+\\mathrm{Tr}[\\Psi(T)S(T)]-\\mathrm{Tr}[\\Psi(0)S(0)]}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "As in Archambeau et al. [39], we assume that $\\pmb{\\lambda}(T)=\\mathbf{0}$ and $\\Psi(T)=\\mathbf{0}$ . ", "page_idx": 17}, {"type": "text", "text": "To find the stationary points of the Lagrangian, we first take derivatives of $\\widetilde{\\mathcal{L}}$ with respect to $m(0),S(0),m(t),S(\\bar{t}),A(t),b(t)$ and set them to 0. The derivatives with respec t to $m(0)$ and $S(0)$ lead to the updates ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\pmb{m}(0)=\\pmb{\\mu}(0)-\\pmb{V}(0)\\pmb{\\lambda}(0),\\qquad\\pmb{S}(0)=\\left(2\\pmb{\\Psi}(0)+\\pmb{V}(0)^{-1}\\right)^{-1}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we assume a prior on initial conditions $p(\\mathbf{\\boldsymbol{x}}_{0})=\\mathcal{N}(\\mathbf{\\boldsymbol{x}}_{0}\\mid\\pmb{\\mu}(0),V(0))$ . ", "page_idx": 17}, {"type": "text", "text": "The derivatives with respect to $m(t)$ and $S(t)$ lead to the stationary equations ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{d\\pmb{\\lambda}(t)}{d t}=\\pmb{A}(t)^{\\top}\\pmb{\\lambda}(t)-\\frac{\\partial\\mathcal{L}}{\\partial\\pmb{m}(t)}}\\\\ {\\displaystyle\\frac{d\\pmb{\\Psi}(t)}{d t}=\\pmb{A}(t)^{\\top}\\pmb{\\Psi}(t)-\\pmb{\\Psi}(t)\\pmb{A}(t)-\\frac{\\partial\\mathcal{L}}{\\partial\\pmb{S}(t)}\\odot\\mathbb{P}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with $\\begin{array}{r}{\\mathbb{P}_{i j}=\\frac{1}{2}}\\end{array}$ for $i\\neq j$ and 1 otherwise. The inclusion of $\\mathbb{P}$ was proposed by Duncker et al. [10] to adjust for taking derivatives with respect to a symmetric matrix. ", "page_idx": 17}, {"type": "text", "text": "To take derivatives with respect to $\\mathbf{\\boldsymbol{A}}(t)$ and $b(t)$ , we first extend a result from Appendix A of Archambeau et al. [39] to our affine inputs model. This allows us to rewrite the KL-divergence term between the posterior and prior latent paths in the ELBO as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\langle\\mathrm{KL}[q(\\boldsymbol{x})||p(\\boldsymbol{x}\\mid f)]\\rangle_{q(f)}=\\displaystyle\\frac{1}{2}\\int_{0}^{T}\\left\\langle(\\boldsymbol{f}+B\\boldsymbol{v}(t)-f_{q})^{\\top}(\\boldsymbol{f}+B\\boldsymbol{v}(t)-f_{q})\\right\\rangle_{q(\\boldsymbol{x}),q(f)}d t}&{{}}\\\\ {\\displaystyle}&{\\qquad=\\frac{1}{2}\\int_{0}^{T}\\left\\langle(B\\boldsymbol{v}(t)+\\Delta f)^{\\top}(B\\boldsymbol{v}(t)+\\Delta f)\\right\\rangle_{q(\\boldsymbol{x}),q(f)}d t}\\\\ {\\displaystyle}&{\\qquad=\\frac{1}{2}\\int_{0}^{T}\\left\\langle(\\Delta f)^{\\top}(\\Delta f)\\right\\rangle_{q(\\boldsymbol{x}),q(f)}d t}\\\\ {\\displaystyle}&{\\qquad+\\int_{0}^{T}\\boldsymbol{v}(t)^{\\top}B^{\\top}\\left\\langle\\Delta f\\right\\rangle_{q(\\boldsymbol{x}),q(f)}d t+\\frac{1}{2}\\int_{0}^{T}\\boldsymbol{v}(t)^{\\top}B^{\\top}B\\boldsymbol{v}(t)d t}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\Delta f:=f-f_{q}$ . The integrand of the first term in eq. (34) can be expanded as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\big\\langle(\\Delta f)^{\\mathsf{T}}(\\Delta f)\\big\\rangle_{q(x),q(f)}=\\big\\langle f^{\\mathsf{T}}f\\big\\rangle_{q(x),q(f)}+2\\mathrm{Tr}\\left[A(t)^{\\mathsf{T}}\\left\\langle\\frac{\\partial f}{\\partial x}\\right\\rangle_{q(x),q(f)}S(t)\\right]}}&{{\\scriptstyle(3\\leq f)\\mathrm{Tr}\\left\\langle(x)^{\\mathsf{T}}f^{\\mathsf{T}}(t)\\,\\right\\rangle_{q(f)}}}\\\\ &{}&{+\\,\\mathrm{Tr}\\left[A(t)^{\\mathsf{T}}A(t)(S(t)+m(t)m(t)^{\\mathsf{T}}\\right]+2m(t)^{\\mathsf{T}}A(t)^{\\mathsf{T}}\\left\\langle f\\right\\rangle_{q(x),q(f)}\\right.}\\\\ &{}&{\\left.+\\,b(t)^{\\mathsf{T}}b(t)-2b(t)^{\\mathsf{T}}\\left\\langle f\\right\\rangle_{q(x),q(f)}-2b^{\\mathsf{T}}A(t)m(t)\\,\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where we have used the identity ${\\Big\\langle}\\langle f(\\pmb{x})\\rangle_{q(f)}\\,(\\pmb{x}-m)^{\\top}{\\Big\\rangle}_{q(\\pmb{x})}\\,=\\,{\\Big\\langle}{\\frac{\\partial f}{\\partial\\pmb{x}}}{\\Big\\rangle}_{q(\\pmb{x}),q(f)}\\,S$ , which can be derived from Stein\u2019s lemma. Note that computing eq. (35) relies on three quantities, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\langle f\\rangle_{q(x),q(f)}\\,,\\langle f^{\\mathsf{T}}f\\rangle_{q(x),q(f)}\\,,\\biggl\\langle\\frac{\\partial f}{\\partial x}\\biggr\\rangle_{q(x),q(f)}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which can be written as terms which depend on expectations of the kernel with respect to $q(x)$ . We derive these as follows: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\langle{\\pmb f}\\rangle_{q({\\pmb x}),q({\\pmb f})}=\\left\\langle{\\pmb k}_{x z}{\\pmb K}_{z z}^{-1}{\\pmb u}\\right\\rangle_{q({\\pmb u}),q({\\pmb x})}}\\\\ &{}&{=\\langle{\\pmb k}_{x z}\\rangle_{q({\\pmb x})}\\,{\\pmb K}_{z z}^{-1}m_{u}\\,\\,\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\int_{\\mathbb{Z}}^{\\mathbb{E}}\\int_{\\mathbb{Z}}^{\\mathbb{E}}\\biggl(z_{t}|\\tilde{\\mathbf{z}}|_{\\mathcal{H}}^{2}\\biggr)_{\\boldsymbol{\\phi}(z)=\\pm}=\\left\\langle\\sum_{i=1}^{K}\\int_{0}^{\\infty}(f_{i}(\\mathbf{z})^{2})_{\\boldsymbol{\\phi}(z)}\\right\\rangle_{\\mathrm{top}}}}\\\\ &{=\\left\\langle\\sum_{i=1}^{K}\\nabla_{\\mathbf{z}}\\alpha_{i}(f_{i})|f_{i}(\\mathbf{z})\\right\\rangle+\\langle f_{i}(\\mathbf{z})_{\\boldsymbol{\\phi}(z)}^{2}\\right\\rangle_{\\mathrm{top}}}\\\\ &{=\\frac{\\sum_{i=1}^{K}\\langle\\mathbf{z}_{i}|\\gamma_{i}|\\mathbf{z}_{i}\\rangle\\,\\left\\langle\\mathbf{z}_{i}|\\gamma_{i}|\\mathbf{z}_{i}\\rangle-\\mathbf{z}_{i}|\\gamma_{i}|}{\\sum_{i=1}^{K}\\sum_{i=1}^{K}\\mathrm{exp}\\,\\mathrm{d}\\mathbf{z}}+\\underbrace{\\langle\\mathbf{v}_{\\mathbf{z}}(\\mathbf{z}_{i}|\\langle f_{i}(\\mathbf{z})\\rangle_{\\mathcal{H}-1}\\mathbf{z}_{i})\\rangle_{\\mathrm{top}}}_{\\mathrm{Tem~2~}}}\\\\ &{\\quad+\\underbrace{\\langle\\mathcal{L}(f_{i}|\\mathbf{z})_{i}^{2}\\rangle_{\\mathrm{top}}\\,\\mathrm{d}\\mathbf{z}_{i}\\rangle}_{\\mathrm{Var}}}\\\\ &{=\\underbrace{\\sum_{i=1}^{K}\\underbrace{\\langle\\mathbf{z}_{i}|a_{t}\\rangle_{i}^{2}\\,\\mathrm{exp}\\,\\mathrm{d}\\mathbf{z}}}_{\\mathrm{Kim~2~}}+\\underbrace{\\langle\\mathbf{z}_{i}|a_{t}\\rangle_{i}\\,\\mathrm{exp}\\,\\mathrm{d}\\mathbf{z}}_{\\mathrm{Var}}}\\\\ &{\\quad+\\underbrace{\\langle\\mathbf{z}_{i}|a_{t}\\rangle_{i}^{2}\\,\\mathrm{exp}\\,\\mathrm{d}\\mathbf{z}}_{\\mathrm{Var}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\langle\\cfrac{\\partial\\pmb{f}}{\\partial\\pmb{x}}\\right\\rangle_{q(\\pmb{x}),q(\\pmb{f})}=\\left\\langle\\pmb{m}_{u}^{\\top}\\pmb{K}_{z z}^{-1}\\frac{\\partial\\pmb{k}_{z x}}{d\\pmb{x}}\\right\\rangle_{q(\\pmb{x})}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The above three function expectations can thus be expressed in terms of the following four kernel expectations with respect to $\\mathbf{\\bar{\\boldsymbol{q}}}(\\mathbf{\\boldsymbol{x}})=\\mathcal{N}(\\mathbf{\\boldsymbol{x}}\\mid m,S)$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left\\langle\\kappa(\\pmb{x},\\pmb{x})\\right\\rangle_{q(\\pmb{x})},\\quad\\left\\langle\\kappa(\\pmb{x},z)\\right\\rangle_{q(\\pmb{x})},\\quad\\left\\langle\\kappa(z_{1},\\pmb{x})\\kappa(\\pmb{x},z_{2})\\right\\rangle_{q(\\pmb{x})},\\quad\\left\\langle\\frac{\\partial\\kappa(z,\\pmb{x})}{\\partial\\pmb{x}}\\right\\rangle_{q(\\pmb{x})}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "For our SSL kernel these kernel expectations are not available in closed form, so in practice we approximate them using Gauss-Hermite quadrature. ", "page_idx": 18}, {"type": "text", "text": "Next, differentiating $\\widetilde{\\mathcal{L}}$ with respect to $A(t)$ and $b(t)$ yields the updates ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\bf A}(t)=-\\left\\langle\\frac{\\partial f}{\\partial x}\\right\\rangle_{q(x),q(f)}+2\\Sigma\\Psi(t)}\\ ~}\\\\ {{\\displaystyle b(t)=\\left\\langle f(x)\\right\\rangle_{q(x),q(f)}+{\\cal A}(t)m(t)+{\\cal B}v(t)-\\Sigma\\lambda(t)}\\ ~}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that these updates have one key difference from the previously derived updates in Archambeau et al. [39] and Duncker et al. [10]: the input-dependent term $B v(t)$ in eq. (37). Intuitively, this is because the posterior bias term $b(t)$ is fully time-varying, so it captures changes in the latent states due to input-driven effects in the posterior. ", "page_idx": 18}, {"type": "text", "text": "In summary, the inference algorithm for updating $q(x)$ is as follows. In each iteration of vEM, we repeat the following forward-backward style algorithm: ", "page_idx": 18}, {"type": "text", "text": "1. Solve for $\\pmb{m}(t),\\pmb{S}(t)$ forward in time starting from ${m}(0),S(0)$ via eq. (25) and eq. (26).   \n2. Solve for $\\lambda(t),\\Psi(t)$ backward in time starting from $\\pmb{\\lambda}(T),\\pmb{\\Psi}(T)=\\pmb{0}$ via eq. (32) and eq. (33).   \n3. Update $A(t)$ and $b(t)$ via eq. (36) and eq. (37). ", "page_idx": 18}, {"type": "text", "text": "After solving these stationary equations, we update $m(0)$ and $S(0)$ via eq. (31). ", "page_idx": 18}, {"type": "text", "text": "Computational details Solving for ${\\pmb m}(t),{\\pmb S}(t),\\lambda(t)$ , and $\\Psi(t)$ requires integrating continuoustime ODEs. In practice we use Euler integration with a small discretization step $\\Delta t$ relative to the sampling rate of the data, though in principle any ODE solver can be used. We found that the ELBO usually converges within 20 forward-backward iterations. ", "page_idx": 18}, {"type": "text", "text": "The ODEs for solving $\\lambda(t)$ and $\\Psi(t)$ in eq. (32) and eq. (33) depend on evaluating gradients of the ELBO with respect to $m(t)$ and $S(t)$ . We use modern autodifferentiation capabilities in JAX to compute these gradients. ", "page_idx": 19}, {"type": "text", "text": "B.4 Updating dynamics and hyperparameters with a modified learning objective ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "As we describe in Section 3.3, our gpSLDS inference algorithm uses a modified objective for hyperparameter learning. In this section, we discuss this objective in detail and present closed-form updates for the inducing points given the hyperparameters. Then, using the inducing points, we will derive the posterior distribution over $\\pmb{f}(\\cdot)$ at any location in the latent space. ", "page_idx": 19}, {"type": "text", "text": "After updating the latent paths $q(x)$ as described in Appendix B.3, we update hyperparameters $\\Theta$ using a partially optimized ELBO. This update can be written as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\Theta^{*}=\\underset{\\Theta}{\\arg\\operatorname*{max}}\\left\\{\\underset{q(\\mathbf{u})}{\\operatorname*{max}}\\mathcal{L}(q(\\pmb{x}),q(\\pmb{u}),\\Theta)\\right\\}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Following Duncker et al. [10], we choose the variational posterior ", "page_idx": 19}, {"type": "equation", "text": "$$\nq(\\boldsymbol{u}_{k})=\\mathcal{N}(\\boldsymbol{u}_{k}\\mid\\boldsymbol{m}_{u}^{k},\\boldsymbol{S}_{u}^{k}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Given $q(x)$ and $\\Theta$ , this leads to the closed-form updates, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{u}^{k*}=K_{z z}\\left(K_{z z}+\\int_{0}^{T}\\left\\langle k_{z x}k_{x z}\\right\\rangle_{q(x)}d t\\right)^{-1}K_{z z}}\\\\ &{m_{u}^{*}=S_{u}^{k*}K_{z z}^{-1}\\displaystyle\\int_{0}^{T}\\left(\\left\\langle k_{z x}\\right\\rangle_{q(x)}\\left(-A(t)m(t)+b(t)-B v(t)\\right)^{\\sf T}\\right.}\\\\ &{\\left.\\qquad\\quad-\\left\\langle\\frac{\\partial k_{z x}}{\\partial x}\\right\\rangle_{q(x)}S(t)A(t)^{\\sf T}\\right)d t}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In the above equation, $\\pmb{m}_{u}^{*}\\in\\mathbb{R}^{M\\times K}$ contains $\\pmb{m}_{u}^{k*}\\in\\mathbb{R}^{M}$ in each column. The inside maximization of eq. (38) can be computed analytically using these closed-form updates. Note that $m_{u}^{k*}$ and $S_{u}^{k*}$ depend on $\\Theta$ through the prior kernel covariances $\\kappa_{z z}$ and $k_{z x}$ . Therefore, eq. (38) can be understood as performing joint optimization of the ELBO with respect to $\\Theta$ through $\\bar{m}_{u}^{k*}$ and $S_{u}^{k*}$ , as well as through the rest of the ELBO. In practice, this allows vEM to circumvent dependencies between $q(u)$ and $\\Theta$ , leading to more accurate estimation of both quantities. For our experiments, we use the Adam optimizer to solve eq. (38). ", "page_idx": 19}, {"type": "text", "text": "After obtaining $\\Theta^{*}$ in each vEM iteration, we explicitly update $q(u)$ using eq. (41) and eq. (40) for the next iteration. ", "page_idx": 19}, {"type": "text", "text": "Recovering predicted dynamics Given (updated) variational parameters $m_{u}^{k}$ and $S_{u}^{k}$ , it is straightforward to compute the posterior distribution of $\\pmb{f}^{*}:=\\pmb{f}(\\pmb{x}^{*})$ at any location $x^{*}$ in the latent space. Recall the variational approximation from eq. (23). If we apply this approximation to $f^{*}$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\nq(f^{*})=\\prod_{k=1}^{K}q(f_{k}^{*})=\\prod_{k=1}^{K}\\int p(f_{k}^{*}\\mid\\pmb{u}_{k},\\Theta)q(\\pmb{u}_{k})d\\pmb{u}_{k}.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "To evaluate this analytically, we use properties of conditional Gaussian distributions. First note that by our augmented prior, ", "page_idx": 19}, {"type": "equation", "text": "$$\np(f_{k}^{*}\\mid\\boldsymbol{u}_{k},\\Theta)=\\mathcal{N}(f_{k}^{*}\\mid k_{x^{*}z}K_{z z}^{-1}\\boldsymbol{u}_{k},\\kappa_{\\mathrm{ssl}}(x^{*},x^{*})-k_{x^{*}z}K_{z z}^{-1}k_{z x^{*}}).\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Then, by conjugacy of Gaussian distributions, ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{q(f_{k}^{*})=\\int\\mathcal{N}(f_{k}^{*}\\mid k_{x^{*}z}K_{z z}^{-1}u_{k},\\kappa_{\\mathrm{ss}}(x^{*},x^{*})-k_{x^{*}z}K_{z z}^{-1}k_{z x^{*}})\\mathcal{N}(u_{k}\\mid m_{u}^{k},S_{u}^{k})d u_{k}}}\\\\ &{}&{=\\mathcal{N}(f_{k}^{*}\\mid k_{x^{*}z}K_{z z}^{-1}m_{u}^{k},\\kappa_{\\mathrm{ss}}(x^{*},x^{*})-k_{x^{*}z}K_{z z}^{-1}k_{z x^{*}}+k_{x^{*}z}K_{z z}^{-1}S_{u}^{k}K_{z z}^{-1}k_{z x^{*}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "B.5 Learning observation model parameters ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "For the experiments in this paper, we considered two observation models: Gaussian observations and Poisson process observations. ", "page_idx": 20}, {"type": "text", "text": "Gaussian observations We consider the observation model ", "page_idx": 20}, {"type": "equation", "text": "$$\np(\\pmb{y}\\mid\\pmb{x})=\\prod_{t_{i}}\\mathcal{N}(\\pmb{y}(t_{i})\\mid\\pmb{C}\\pmb{x}(t_{i})+\\pmb{d},\\pmb{R}).\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\pmb{R}\\in\\mathbb{R}^{D}$ is a diagonal covariance matrix. The expected log-likelihood is available in closed form and is given by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\langle\\log p(\\pmb{y}\\mid\\pmb{x})\\right\\rangle_{q(\\pmb{x})}=\\sum_{t_{i}}\\left(\\log\\mathcal{N}(\\pmb{y}(t_{i})\\mid\\pmb{C}(t_{i})+\\pmb{d},\\pmb{R})-\\frac{1}{2}\\mathrm{Tr}\\left[\\pmb{S}(t_{i})\\pmb{C}^{\\top}\\pmb{R}^{-1}\\pmb{C}\\right]\\right)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Closed-form updates for $C,d$ and $\\boldsymbol{R}$ are also available: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal C}^{*}=\\left(\\sum_{t_{i}}(y(t_{i})-d)m(t_{i})^{\\top}\\right)\\left(\\sum_{t_{i}}(S(t_{i})+m(t_{i})m(t_{i})^{\\top})\\right)^{-1}}}\\\\ {{\\displaystyle{\\cal d}^{*}=\\frac{1}{n_{t_{i}}}\\sum_{t_{i}}(y(t_{i})-C^{*}m(t_{i}))}}\\\\ {{\\displaystyle R_{d}^{*}=\\frac{1}{n_{t_{i}}}\\sum_{t_{i}}(y_{d}(t_{i})^{2}-2y_{d}(t_{i})c_{d}^{\\top}m(t_{i})+(c_{d}^{\\top}m(t_{i}))^{2}+c_{d}^{\\top}S(t_{i})c_{d})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $n_{t_{i}}$ is the number of observed time points, $R_{d}^{*}$ is the $d_{\\cdot}$ -th entry of $\\boldsymbol{R}$ , and $c_{d}$ is the $d$ -th row of $_{C}$ . ", "page_idx": 20}, {"type": "text", "text": "Poisson process observations The second observation model we consider is Poisson process observations of the form ", "page_idx": 20}, {"type": "equation", "text": "$$\np(\\{t_{i}\\}\\mid x)=\\mathcal{P P}(g(C x(t)+d)),\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where either $g(a)=\\exp(a)$ (exponential inverse link) or $g(a)=\\log(1+\\exp(a))$ (softplus inverse link). For the exponential inverse link, the expected log-likelihood is available in closed form and is given by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\left\\langle\\log p(\\{t_{i}\\}|\\ x)\\right\\rangle_{q(x)}=-\\int_{0}^{T}\\exp\\left(C m(t)+d+{\\frac{1}{2}}\\mathrm{diag}(C S(t)C^{\\top})\\right)d t+\\sum_{t_{i}}(C m(t_{i})+d)\\int_{0}^{T}\\exp\\left(\\ C S(t)+d\\varphi^{\\top}\\right)\\log(\\ C S(t_{i}))d t.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "For the softplus inverse link, the expected log-likelihood is not available in closed-form, but can be approximated by Gauss-Hermite quadrature or a second-order Taylor expansion around $m(t)$ . ", "page_idx": 20}, {"type": "text", "text": "For both Poisson process models, we update $_{C}$ and $^d$ using gradient ascent on the expected loglikelihood with the Adam optimizer. ", "page_idx": 20}, {"type": "text", "text": "B.6 Learning the input effect matrix ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Here we derive a closed-form update for $_B$ , which linearly maps external inputs to the latent space. The only term in the ELBO which depends on $_B$ is $\\langle\\mathrm{KL}[\\dot{q}(\\mathbf{x})|\\bar{|p}(\\mathbf{x}\\mid f)]\\rangle_{q(\\pmb{f})}$ . We differentiate this term as written in eq. (34) and arrive at the update ", "page_idx": 20}, {"type": "equation", "text": "$$\nB^{*}=-\\left(\\int_{0}^{T}(\\langle f\\rangle_{q(x),q(f)}+A(t)m(t)-b(t))v(t)^{\\mathsf{T}}d t\\right)\\left(\\int_{0}^{T}v(t)v(t)^{\\mathsf{T}}d t\\right)^{-1}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Note that the term $\\begin{array}{r}{\\left(\\int_{0}^{T}{\\pmb v}(t){\\pmb v}(t)^{\\top}d t\\right)^{-1}}\\end{array}$ can be pre-computed since ${\\pmb v}(t)$ is known. ", "page_idx": 20}, {"type": "image", "img_path": "LX1lwP90kt/tmp/073278afd8a1da7a840ba01f18c8fe25ce9e0aaea9d62cedaae9d71c1c7c2983.jpg", "img_caption": ["Figure 5: Comparison between the standard vEM approach in Duncker et al. [10] and our modified vEM approach. A. Estimation error between the true and learned decision boundaries, computed as described in Appendix C. For each vEM approach, we fit $5{\\mathrm{~gpSLDS}}$ models with different random initializations. The estimation errors are denoted in light blue/purple dots. The runs that we display in the next two panels are denoted by a solid blue/purple dot. B. The standard vEM approach fails to learn the true decision boundary of $x_{1}=0$ . C. The modified vEM approach precisely learns this decision boundary. "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "C Empirical results for new learning objective ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we empirically compare the standard vEM approach from Duncker et al. [10] to our modified vEM approach in which we learn kernel hyperparameters on a partially optimized ELBO. For this experiment, we use the same synthetic dataset from our main result in Section 4.1. We fit the gpSLDS with 5 different random initializations using both standard vEM and modified vEM. For these fits, we fix the values of $_{C}$ and $\\pmb{d}$ throughout learning to ensure that the resulting models are anchored to the same latent subspace (in general, they are not guaranteed to end up in the same subspace due to rotational unidentifiability). Each run was fit with 50 total vEM iterations; each iteration consisted of 15 forward-backward solves to update $q(x)$ and 300 Adam gradient steps with a learning rate of 0.01 to update kernel hyperparameters. ", "page_idx": 21}, {"type": "text", "text": "To compare the quality of the learned hyperparameters, we quantitatively assess the error between the learned and true decision boundaries. In this simple example with $J=2$ , the decision boundary can be parametrized as $w_{0}+w_{1}x_{1}+w_{2}x_{2}=0$ for some $\\pmb{w}=(w_{0},w_{1},w_{2})^{\\top}$ . The true decision boundary is characterized by $\\pmb{w}_{\\mathrm{true}}=(0,1,0)^{\\top}$ . We denote the learned decision boundary as $\\hat{w}$ . Next, we compute an error metric between the learned and true decision boundaries as follows. We first normalize the learned decision boundary and define $\\begin{array}{r}{\\hat{w}_{\\mathrm{norm}}=\\frac{\\hat{w}}{\\|\\hat{w}\\|_{2}}}\\end{array}$ w\u02c6 . We do not need to do this for $w_{\\mathrm{true}}$ since it is already normalized. Then, we use the error metric ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{err}(\\hat{w},w_{\\mathrm{true}})=\\operatorname*{min}\\left(\\lVert\\hat{w}_{\\mathrm{norm}}-w_{\\mathrm{true}}\\rVert_{2},\\lVert\\hat{w}_{\\mathrm{norm}}+w_{\\mathrm{true}}\\rVert_{2}\\right).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Including both terms in the minimum is necessary due to unidentifiability of the signs of $\\hat{w}$ . ", "page_idx": 21}, {"type": "text", "text": "Figure 5A compares this error metric across the 5 model fits for each vEM method. It is clear that the modified vEM approach consistently outperforms the standard vEM approach in terms of more accurately estimating the decision boundary. In addition, the error for standard vEM has much higher variance, since the algorithm is prone to getting stuck in local maxima of the ELBO. Figures 5B-C display the learned versus true decision boundaries in the latent space for two selected runs. We select the run from each vEM approach which achieved the lowest decision boundary error metric, as denoted by solid dots in fig. 5A. We find that the model fit with standard vEM learns a decision boundary which noticeably deviates from the true boundary. On the other hand, the model with with modified vEM recovers the true boundary almost perfectly. This illustrates that our modified approach dramatically improves kernel hyperparameter estimation in practice and enables the gpSLDS to be much more interpretable in the latent space. ", "page_idx": 21}, {"type": "image", "img_path": "LX1lwP90kt/tmp/c74e6f10cfae134b803fd81d18eb133dafe2974393f89be525459b25be8c9af2.jpg", "img_caption": ["Figure 6: Additional synthetic data results on a 2D limit cycle from a gpSLDS fit with quadratic decision boundaries. A. True dynamics and true latent trajectories on 3 example trials used to generate the dataset. Dynamics are an unstable rotation and a stable rotation with fixed points at $(0,0)$ , separated by $x_{1}^{2}+x_{2}^{2}=4$ . B. Poisson process observations for an example trial. C. gpSLDS inferred latent trajectory with $95\\%$ posterior credible intervals for an example trial. D. The learned $\\pi(x)$ accurately recovers the true circular boundary between the two sets of linear dynamics. E. The gpSLDS learned posterior variance on dynamics. The posterior variance is low in regions heavily traversed by the true latent paths, and is high in regions with little to no data. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "D Additional synthetic data results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "To further demonstrate the expressivity of the gpSLDS over the rSLDS, we apply the gpSLDS to a synthetic dataset where the true decision boundary between linear regimes is nonlinear. The rSLDS can only model linear decision boundaries in order for its inference algorithm to remain tractable. ", "page_idx": 22}, {"type": "text", "text": "For this example, we generate a synthetic dataset consisting of an unstable linear system and a stable linear system separated by the decision boundary $x_{1}^{2}+\\bar{x}_{2}^{2}=4$ . Both of the linear systems have fixed points at $(0,0)$ . The smooth combination of these linear systems results in a 2D limit cycle (fig. 6A). We simulate 30 trials of Poisson process observations from $D=50$ neurons over $T=2$ seconds (fig. 6B). We initialize the observation model parameters $_{C}$ and $^d$ using a Poisson LDS with data binned at $20\\mathrm{ms}$ . Then, we fti a gpSLDS with $J=2$ regimes, and with $\\pi(x)$ modeled using the feature transformation ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\phi(\\pmb{x})=\\left[1\\quad\\pmb{x}_{1}^{2}\\quad\\pmb{x}_{2}^{2}\\right]^{\\mathsf{T}}.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The results of this experiment are shown in fig. 6C-E. In fig. 6C we find that the gpSLDS successfully recovers the the true latent trajectory with accurate posterior credible intervals for an example trial. Furthermore, fig. 6D demonstrates that by using the quadratic feature transformation in eq. (54), the gpSLDS accurately learns the true flow field and true decision boundary $x_{1}^{2}+x_{2}^{2}=4$ . In addition, the values of $\\pi(x)$ smoothly transition between 0 and 1 near this boundary, highlighting the ability of our method to learn smooth dynamics if present. Lastly, in fig. 6E we plot the inferred posterior variance of our method. We find that the gpSLDS is more confident in regions of the latent space with more data (e.g. at the decision boundary), and less confident in regions of latent space with little to no data. ", "page_idx": 22}, {"type": "text", "text": "E Computing resources ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We fti all of our models on a NVIDIA A100 GPU on an internal computing cluster. A breakdown of approximate compute times for the main experiments in this paper includes: ", "page_idx": 22}, {"type": "text", "text": "\u2022 Synthetic data results in Section 4.1: 1.5 hours per model fit, ${\\sim}40$ hours for the entire experiment. \u2022 Real data results in Section 4.2: 1.5 hours per model fti, ${\\sim}8$ hours for the entire experiment. \u2022 Real data results in Section 4.3: 1 hour per model fit, ${\\sim}5\\,$ hours for the entire experiment. ", "page_idx": 23}, {"type": "text", "text": "We note that these estimates do not include the full set of experiments we performed while carrying out this project (such as preliminary or failed experiments). ", "page_idx": 23}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: In our abstract and introduction, we propose a new method for interpretable modeling of neural dynamics. In our paper, we apply our method and demonstrate its interpretability and competitive performance on a synthetic dataset and two real datasets. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 24}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: We discuss limitations of our work in the discussion section of the paper. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \u201dLimitations\u201d section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: Our paper does not contain theoretical results. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 25}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: For our experiments, we provide detailed descriptions and references to datasets, as well as descriptions of the models we fit. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have made our codebase on GitHub publicly available. We provide both data and code for our main synthetic data result, and we also provide references to the dataset and instructions for fitting models to reproduce the second real data result. We do not provide data or code for the first real data result since that data has not been released to the public. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 26}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We provide all details for the experimental setting that are needed to understand the results for all three sets of experiments. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Yes, we report thorough quantitative comparisons with error bars for the experiments in Section 4.1 and 4.2. Throughout the paper, we fit multiple (i.e. at least 5) models with different initializations for our experiments. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 26}, {"type": "text", "text": "\u2022 The authors should answer \u201dYes\u201d if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 27}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We provide information on computing resources in Appendix E. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 27}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: We have reviewed and followed the Code of Ethics. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 27}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: Our work develops a new statistical modeling tool for the analysis of neuroscience data in constrained experiments. We expect that this work will not have broad societal impacts, positive or negative. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 28}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: This paper does not involve data or models which have a high risk of misuse. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 28}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We cite the original papers which analyzed or produced both of the real datasets that we consider in Sections 4.2 and 4.3. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We provide our codebase on GitHub which comes with clear documentation. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 29}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 30}]