[{"type": "text", "text": "The Minimax Rate of HSIC Estimation for Translation-Invariant Kernels ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Florian Kalinke Institute for Program Structures and Data Organization Karlsruhe Institute of Technology Karlsruhe, Germany florian.kalinke@kit.edu ", "page_idx": 0}, {"type": "text", "text": "Zolt\u00e1n Szab\u00f3   \nDepartment of Statistics   \nLondon School of Economics   \nLondon, UK   \nz.szabo@lse.ac.uk ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kernel techniques are among the most influential approaches in data science and statistics. Under mild conditions, the reproducing kernel Hilbert space associated to a kernel is capable of encoding the independence of $M\\geq2$ random variables. Probably the most widespread independence measure relying on kernels is the socalled Hilbert-Schmidt independence criterion (HSIC; also referred to as distance covariance in the statistics literature). Despite various existing HSIC estimators designed since its introduction close to two decades ago, the fundamental question of the rate at which HSIC can be estimated is still open. In this work, we prove that the minimax optimal rate of HSIC estimation on $\\mathbb{R}^{\\dot{d}}$ for Borel measures containing the Gaussians with continuous bounded translation-invariant characteristic kernels is $O\\big(n^{-1/2}\\big)$ . Specifically, our result implies the optimality in the minimax sense of many of the most-frequently used estimators (including the U-statistic, the V-statistic, and the Nystr\u00f6m-based one) on $\\mathbb{R}^{d}$ . ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Kernel methods [Steinwart and Christmann, 2008, Berlinet and Thomas-Agnan, 2004, Saitoh and Sawano, 2016] allow embedding probability measures into reproducing kernel Hilbert spaces (RKHS; [Aronszajn, 1950]) by use of a positive definite function, the kernel function. This approach has gained considerable attention over the last 20 years. Such embeddings induce the so-called maximum mean discrepancy (MMD; [Smola et al., 2007, Gretton et al., 2012]), which quantifies the discrepancy of two probability measures by considering the RKHS norm of the distance of their respective embeddings. MMD is a metric on the space of probability distributions if the kernel is characteristic [Fukumizu et al., 2008, Sriperumbudur et al., 2010]. MMD is also an integral probability metric [Zolotarev, 1983, M\u00fcller, 1997] where the underlying function class is chosen to be the unit ball in the corresponding RKHS. ", "page_idx": 0}, {"type": "text", "text": "MMD allows for the quantification of dependence by considering the distance between the embedding of a joint distribution and that of the product of its marginals. This construction gives rise to the so-called Hilbert-Schmidt independence criterion (HSIC; [Gretton et al., 2005]), which is also equal to the RKHS norm of the centered cross-covariance operator. In fact, one of the most widely-used independence measures in statistics, distance covariance [Sz\u00e9kely et al., 2007, Sz\u00e9kely and Rizzo, 2009, Lyons, 2013], was shown to be equivalent to HSIC [Sejdinovic et al., 2013b] when the latter is specialized to $M=2$ components; Sheng and Sriperumbudur [2023] proved a similar result for the conditional case. For $M>2$ components [Quadrianto et al., 2009, Sejdinovic et al., 2013a, Pfister et al., 2018], universality [Steinwart, 2001, Micchelli et al., 2006, Carmeli et al., 2010, Sriperumbudur et al., 2011] of the kernels $(k_{m})_{m=1}^{M}$ (on the respective domains) underlying HSIC guarantees that this measure captures independence [Szab\u00f3 and Sriperumbudur, 2018]. In the case of $M\\,=\\,2$ , characteristic $(\\bar{k_{m}})_{m=1}^{2}$ suffice [Lyons, 2013]. ", "page_idx": 0}, {"type": "text", "text": "HSIC has been deployed successfully in numerous contexts, including independence testing in batch [Gretton et al., 2008, Wehbe and Ramdas, 2015, Bilodeau and Nangue, 2017, G\u00f3recki et al., 2018, Pfister et al., 2018, Albert et al., 2022, Shekhar et al., 2023] and streaming [Podkopaev et al., 2023] settings, feature selection [Camps-Valls et al., 2010, Song et al., 2012, Yamada et al., 2014, Wang et al., 2022] with applications in biomarker detection [Climente-Gonz\u00e1lez et al., 2019] and wind power prediction [Bouche et al., 2023], clustering [Song et al., 2007, Climente-Gonz\u00e1lez et al., 2019], and causal discovery [Mooij et al., 2016, Pfister et al., 2018, Chakraborty and Zhang, 2019, Sch\u00f6lkopf et al., 2021, Kalinke and Szab\u00f3, 2023]. In addition, HSIC has recently found successful applications in sensitivity analysis [Veiga, 2015, Freitas Gustavo et al., 2023, Fellmann et al., 2024, Herrando-P\u00e9rez and Saltr\u00e9, 2024], in the context of uncertainty quantification [Stenger et al., 2020], for the analysis of data augmentation methods for brain tumor detection [Anaya-Isaza and Mera-Jim\u00e9nez, 2022], and that of multimodal neural networks trained on neuroimaging data [Fedorov et al., 2024]. ", "page_idx": 1}, {"type": "text", "text": "Many estimators for HSIC exist. The classical ones rely on U-statistics or V-statistics [Gretton et al., 2005, Quadrianto et al., 2009, Pfister et al., 2018] and are known to converge at a rate of $\\mathcal{O}_{P}\\left(n^{-1/2}\\right)$ . In fact, the V-statistic-based estimators are obtained by replacing the population kernel mean embedding with its empirical counterpart; estimating the mean embedding can be carried out at a speed $\\mathcal{O}_{P}\\left(n^{-1/2}\\right)$ [Smola et al., 2007, Theorem 2], which implies that HSIC can be estimated at the same rate. Existing approximations such as Nystr\u00f6m HSIC [Kalinke and Szab\u00f3, 2023], also achieve this rate under the assumption of an appropriate rate of decay of the effective dimension. While all of these upper bounds match asymptotically, it is not known whether HSIC can be estimated at a faster rate, that is, whether the upper bound of $\\mathcal{O}_{P}\\left(n^{-1/2}\\right)$ is optimal in the minimax sense, or if designing estimators achieving better rates is possible. Lower bounds for the related MMD are known [Tolstikhin et al., 2016], but the existing analysis considers radial kernels and relies on independent Gaussian distributions. Radial kernels are a special case of the more general class of translation-invariant kernels that we consider.1 The reliance on independent Gaussian distributions renders the analysis of Tolstikhin et al. [2016] inapplicable for HSIC estimation. We tackle both of these severe restrictions in the present article. ", "page_idx": 1}, {"type": "text", "text": "We make the following contributions. ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "\u2022 We establish the minimax lower bound $\\mathcal{O}\\left(n^{-1/2}\\right)$ of HSIC estimation with $M\\geq2$ components on $\\mathbb{R}^{d}$ with continuous bounded translation-invariant characteristic kernels. As this lower bound matches the known upper bounds of the existing \u201cclassical\u201d U-statistic and $\\mathrm{V}.$ -statistic-based estimators, and that of the Nystr\u00f6m HSIC estimator, our result settles their minimax optimality. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Specifically, our result also implies the minimax lower bound of $\\mathcal{O}\\left(n^{-1/2}\\right)$ for the estimation of the cross-covariance operator, which can be further specialized to get back the minimax result [Zhou et al., 2019, Theorem 5] on the estimation of the covariance operator. ", "page_idx": 1}, {"type": "text", "text": "The paper is structured as follows. Notations are introduced in Section 2. Section 3 is dedicated to our main result on the minimax rate of HSIC estimation on $\\mathbb{R}^{d}$ , with proof presented in Section 4. An auxiliary result on the Kullback-Leibler divergence is shown in Appendix A. ", "page_idx": 1}, {"type": "text", "text": "2 Notations ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "In this section, we introduce a few notations $\\mathbb{N}_{>0}$ , $[M]$ , ${\\mathbf{I}}_{n}$ , ${\\bf0}_{n}$ , ${\\mathbf{1}}_{n}$ , $\\mathbf{A}^{\\mathsf{T}}$ , $\\langle\\mathbf{v},\\mathbf{w}\\rangle$ , $\\|\\mathbf{v}\\|_{\\mathbb{R}^{d}}$ , bdiag $({\\bf M}_{1},\\ldots,{\\bf M}_{N})$ , $|\\mathbf{A}|$ , $\\mathcal{M}_{1}^{+}\\left(\\mathbb{R}^{d}\\right)$ , $\\psi_{\\mathbb{P}}$ , $\\operatorname{KL}(\\mathbb{P}||\\mathbb{Q})$ , $L^{2}\\left(\\mathbb{R}^{d},\\Lambda\\right)$ , $\\|f\\|_{L^{2}(\\mathbb{R}^{d},\\Lambda)}$ , $\\operatorname{supp}(\\Lambda)$ , $\\mathcal{H}_{k}$ , $\\phi_{k}$ , $k$ , $\\mu_{k}$ , $\\mathrm{MMD}_{k}$ , $\\otimes_{m=1}^{M}\\mathcal{H}_{k_{m}}$ , $\\otimes_{m=1}^{M}k_{m}$ , $\\mathbb{P}_{m}$ , $\\otimes_{m=1}^{M}\\mathbb{P}_{m}$ , $\\mathbb{P}^{n}$ , $\\mathcal{O}_{P}\\left(r_{n}\\right)$ , $\\mathcal{O}(a_{n})$ , $a_{n}\\asymp b_{n}$ , $\\mathrm{HSIC}_{k}$ , and $C_{X}$ . Throughout the paper we consider random variables, probability measures, and kernels on $\\mathbb{R}^{d}$ . ", "page_idx": 1}, {"type": "text", "text": "For $M\\in\\mathbb{N}_{>0}:=\\{1,2,...\\}$ , let $[M]:=\\{1,\\cdot\\cdot\\cdot,M\\}$ . Denote by ${\\mathbf{I}}_{n}$ the $n\\times n$ -sized identity matrix and by $\\mathbf{0}_{n}=(0,\\ldots,0)^{\\mathsf{T}}\\in\\mathbb{R}^{n}$ (resp. $\\mathbf{1}_{n}=(1,\\ldots,1)^{\\mathsf{T}}\\in\\mathbb{R}^{n})$ a column vector of zeros (resp. ones). The transpose of a matrix $\\mathbf{A}\\in\\mathbb{R}^{d_{1}\\times d_{2}}$ is written as $\\mathbf{A}^{\\top}\\in\\mathbb{R}^{d_{2}\\times d_{1}}$ . For $\\mathbf{v},\\mathbf{w}\\in\\mathbb{R}^{d}$ , $\\langle\\mathbf{v},\\mathbf{w}\\rangle=\\mathbf{v}^{\\mathsf{T}}\\mathbf{w}$ stands for their Euclidean inner product; $\\|\\mathbf{v}\\|_{\\mathbb{R}^{d}}\\;=\\;\\sqrt{\\langle\\mathbf{v},\\mathbf{v}\\rangle}$ is the associated Euclidean norm. ", "page_idx": 1}, {"type": "text", "text": "bdiag $({\\bf M}_{1},\\ldots,{\\bf M}_{N})$ forms a block-diagonal matrix from its arguments $(\\mathbf{M}_{n})_{n=1}^{N}\\,(\\mathbf{M}_{n}\\in\\mathbb{R}^{d_{n}\\,\\times\\,d_{n}}$ , $n\\in[N])$ and $|\\mathbf{A}|$ denotes the determinant of a matrix $\\mathbf{A}\\in\\mathbb{R}^{d\\times\\bar{d}}$ . ", "page_idx": 2}, {"type": "text", "text": "The set of Borel probability measures on $\\mathbb{R}^{d}$ is denoted by $\\mathcal{M}_{1}^{+}\\left(\\mathbb{R}^{d}\\right)$ . For a random variable $X\\,\\sim\\,\\mathbb{P}\\,\\in\\,\\mathcal{M}_{1}^{+}\\left(\\mathbb{R}^{d}\\right)$ , we denote its characteristic function by $\\psi_{\\mathbb{P}}(\\omega)\\,=\\,\\mathbb{E}_{X\\sim\\mathbb{P}}\\left[e^{i\\left\\langle\\omega,X\\right\\rangle}\\right]$ with $\\omega\\,\\in\\,\\mathbb R^{d}$ and $i\\;=\\;\\sqrt{-1}$ . Let $\\mathbb{P},\\mathbb{Q}\\,\\in\\,\\mathcal{M}_{1}^{+}\\left(\\mathbb{R}^{d}\\right)$ , assume that $\\mathbb{P}$ is absolutely continuous w.r.t. $\\mathbb{Q}$ , and let $\\frac{\\mathrm{d}\\mathbb{P}}{\\mathrm{d}\\mathbb{Q}}$ denote the corresponding Radon-Nikodym derivative (of $\\mathbb{P}$ w.r.t. $\\mathbb{Q}$ ). Then, the Kullback-Leibler divergence of P and Q is defined as KL(P||Q) :=  Rd log ddP(x . Given a measure space $\\left(\\mathbb{R}^{d},B\\left(\\mathbb{R}^{d}\\right),\\Lambda\\right)$ , we denote by $L^{2}(\\mathbb{R}^{d},\\Lambda)\\;:=\\;L^{2}\\left(\\mathbb{R}^{d};\\mathcal{B}\\left(\\mathbb{R}^{d}\\right),\\Lambda\\right)$ the Hilbert space of (equivalence classes of) measurable functions $f:(\\mathbb{R}^{d},\\mathcal{B}\\left(\\mathbb{R}^{d}\\right))\\rightarrow(\\mathbb{R},\\mathcal{B}\\left(\\mathbb{R}\\right))$ for which $\\begin{array}{r}{\\|f\\|_{L^{2}(\\mathbb{R}^{d},\\Lambda)}^{2}:=\\int_{\\mathbb{R}^{d}}|f(\\mathbf{x})|^{2}{\\mathrm{d}}\\Lambda(\\mathbf{x})<\\infty}\\end{array}$ . The support of a probability measure $\\Lambda\\in\\mathcal{M}_{1}^{+}\\left(\\mathbb{R}^{d}\\right)$ denoted by $\\operatorname{supp}(\\Lambda)$ is the subset of $\\mathbb{R}^{d}$ for which every open neighborhood of $\\mathbf{x}\\in\\mathbb{R}^{d}$ has positive measure [Cohn, 2013, p. 207]. ", "page_idx": 2}, {"type": "text", "text": "A function $k:\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\,\\to\\,\\mathbb{R}$ is called a kernel if there exists a Hilbert space $\\mathcal{H}$ and a feature map $\\phi:\\,\\mathbb{R}^{d}\\,\\rightarrow\\,\\mathcal{H}$ such that $k(\\mathbf{x},\\mathbf{x}^{\\prime})\\,=\\,\\langle\\phi(\\mathbf{x}),\\phi(\\mathbf{x}^{\\prime})\\rangle_{\\mathcal{H}}$ for all $\\mathbf{x},\\mathbf{x}^{\\prime}\\,\\in\\,\\mathbb{R}^{\\dot{d}}$ . A Hilbert space of functions $h:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ is an RKHS $\\mathcal{H}_{k}$ associated to a kernel $k:\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\to\\mathbb{R}$ if $k(\\cdot,\\mathbf{x})\\in\\mathcal{H}_{k}$ and $\\langle h,k(\\cdot,{\\bf x})\\rangle_{\\mathscr{H}_{k}}\\,=\\,h({\\bf x})$ for all $\\textbf{x}\\in\\mathbb{R}^{d}$ and $h\\in\\mathcal{H}_{k}$ .2 In this work, we assume all kernels to be measurable and bounded.3 The function $\\phi_{k}(\\mathbf{x})\\,:=\\,k(\\cdot,\\mathbf{x})$ is the canonical feature map, and $k(\\mathbf{x},\\mathbf{x^{\\prime}})=\\langle k(\\cdot,\\mathbf{x}),k(\\cdot,\\mathbf{x^{\\prime}})\\rangle_{\\mathcal{H}_{k}}=\\langle\\phi_{k}(\\mathbf{x}),\\phi_{k}(\\mathbf{x^{\\prime}})\\rangle_{\\mathcal{H}_{k}}$ for all $\\mathbf{x},\\mathbf{x}^{\\prime}\\in\\mathbb{R}^{d}$ . A function $\\kappa:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ is called positive definite if $\\begin{array}{r}{\\sum_{i,j\\in[n]}c_{i}c_{j}\\kappa(\\mathbf x_{i}-\\mathbf x_{j})\\,\\geq\\,0}\\end{array}$ for all $n\\,\\in\\,\\mathbb{N}_{>0}$ , $\\mathbf{c}\\,=\\,\\left(c_{i}\\right)_{i=1}^{n}\\,\\in\\,\\mathbb{R}^{n}$ , and $\\{\\mathbf{x}_{i}\\}_{i=1}^{n}\\subset\\mathbb{R}^{d}$ . A kernel $\\boldsymbol{k}:\\dot{\\mathbb{R}}^{\\dot{d}}\\times\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ is said to be translation-invariant if there exists a positive definite function $\\kappa:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ such that $k(\\mathbf{x},\\mathbf{x}^{\\prime})\\,=\\,\\kappa(\\mathbf{x}-\\mathbf{x}^{\\prime})$ for all $\\mathbf{x},\\mathbf{x}^{\\prime}\\in\\mathbb{R}^{d}$ . By Bochner\u2019s theorem [Wendland, 2005, Theorem 6.6] (recalled in Theorem B.1) for a continuous bounded translation-invariant kernel $k:\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\,\\rightarrow\\,\\mathbb{R}$ there exists a finite non-negative Borel measure $\\Lambda_{k}$ such that ", "page_idx": 2}, {"type": "equation", "text": "$$\nk(\\mathbf{x},\\mathbf{y})=\\int_{\\mathbb{R}^{d}}e^{-i\\left\\langle\\mathbf{x}-\\mathbf{y},\\omega\\right\\rangle}\\mathrm{d}\\Lambda_{k}(\\omega)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "for all $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{d}$ . The (kernel) mean embedding of a probability measure $\\mathbb{P}\\in\\mathcal{M}_{1}^{+}\\left(\\mathbb{R}^{d}\\right)$ is ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mu_{k}(\\mathbb{P})=\\int_{\\mathbb{R}^{d}}\\phi_{k}(\\mathbf{x})\\mathrm{d}\\mathbb{P}(\\mathbf{x})\\in\\mathcal{H}_{k},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the integral is meant in Bochner\u2019s sense [Diestel and Uhl, 1977, Chapter II.2]; the boundedness of $k$ ensures that it is well-defined. For $\\mathbb{P},\\mathbb{Q}\\in\\mathcal{M}_{1}^{+}\\left(\\mathbb{R}^{d}\\right)$ one can define the (semi-)metric called maximum mean discrepancy [Smola et al., 2007, Gretton et al., 2012] as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{MMD}_{k}(\\mathbb{P},\\mathbb{Q})=\\|\\mu_{k}(\\mathbb{P})-\\mu_{k}(\\mathbb{Q})\\|_{\\mathcal{H}_{k}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "If the mean embedding $\\mu_{k}$ is injective, MMD is a metric and the kernel $k$ is called characteristic [Fukumizu et al., 2008, Sriperumbudur et al., 2010, Szab\u00f3 and Sriperumbudur, 2018]. ", "page_idx": 2}, {"type": "text", "text": "Let $\\mathbb{R}^{d}~=~\\times_{m=1}^{M}\\mathbb{R}^{d_{m}}$ $\\begin{array}{r}{(d\\,=\\,\\sum_{m=1}^{M}d_{m})}\\end{array}$ and assume that each domain $\\mathbb{R}^{d_{m}}$ is equipped with a kernel $k_{m}:\\mathbb{R}^{d_{m}}\\times\\mathbb{R}^{d_{m}}\\rightarrow\\mathbb{R}$ with associated RKHS $\\mathcal{H}_{k_{m}}$ $(m\\in[M])$ . The tensor product Hilbert space of $(\\mathcal{H}_{k_{m}})_{m=1}^{M}$ is denoted by $\\otimes_{m=1}^{M}\\mathcal{H}_{k_{m}}$ ; it is an RKHS [Berlinet and Thomas-Agnan, 2004, Theorem 13] with the tensor product kernel $k=\\otimes_{m=1}^{M}k_{m}:\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\to\\mathbb{R}$ defined by ", "page_idx": 2}, {"type": "equation", "text": "$$\nk\\left(\\left(\\mathbf{x}_{m}\\right)_{m=1}^{M},(\\mathbf{x}_{m}^{\\prime})_{m=1}^{M}\\right)=\\prod_{m\\in[M]}k_{m}(\\mathbf{x}_{m},\\mathbf{x}_{m}^{\\prime})\\quad\\mathrm{for\\,\\,all}\\quad\\mathbf{x}_{m},\\mathbf{x}_{m}^{\\prime}\\in\\mathbb{R}^{d_{m}},\\,m\\in[M].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The kernel $k$ has the canonical feature map $\\phi_{k}\\left((\\mathbf{x}_{m})_{m=1}^{M}\\right)\\,=\\,\\otimes_{m=1}^{M}\\phi_{k_{m}}\\left(\\mathbf{x}_{m}\\right)\\,\\in\\,\\otimes_{m=1}^{M}\\mathcal{H}_{k_{m}}\\,=:$ $\\mathcal{H}_{k}$ $\\mathbf{\\Phi}(\\mathbf{x}_{m}\\mathbf{\\Phi}\\in\\mathbf{\\Phi}\\mathbb{R}^{d_{m}},m\\mathbf{\\Phi}\\in\\mathbf{\\Phi}[M])$ . Let $X~=~(X_{m})_{m=1}^{M}$ be a random variable taking values in $\\mathbb{R}^{d}$ with joint distribution $\\mathbb{P}\\in\\mathcal{M}_{1}^{+}\\left(\\mathbb{R}^{d}\\right)$ and marginal distributions $\\mathbb{P}_{m}\\,\\in\\,\\mathcal{M}_{1}^{+}\\,\\big(\\mathbb{R}^{d_{m}}\\big)$ $(m\\in[M]$ ; $\\begin{array}{r}{d=\\sum_{m=1}^{M}d_{m})}\\end{array}$ . We write $\\otimes_{m=1}^{M}\\mathbb{P}_{m}\\,\\in\\,\\mathcal{M}_{1}^{+}\\left(\\mathbb{R}^{d}\\right)$ for the product of measures $\\mathbb{P}_{m}$ $(m\\,\\in\\,[M])$ . Specifically, $\\mathbb{P}^{n}:=\\otimes_{i=1}^{n}\\mathbb{P}\\in\\mathcal{M}_{1}^{+}\\left(\\left(\\mathbb{R}^{d}\\right)^{n}\\right)$ denotes the $n$ -fold product of $\\mathbb{P}$ . For a sequence of real-valued random variables $(X_{n})_{n=1}^{\\infty}$ and a sequence $(r_{n})_{n=1}^{\\infty}$ ( $r_{n}>0$ for all $n$ ), $X_{n}=O_{P}\\left(r_{n}\\right)$ denotes that $\\frac{X_{n}}{r_{n}}$ is bounded in probability. For positive sequences $(a_{n})_{n=1}^{\\infty}$ and $(b_{n})_{n=1}^{\\infty}$ , $b_{n}=\\mathcal{O}(a_{n})$ if there exist constants $C\\;>\\;0$ and $n_{0}\\,\\in\\,\\mathbb{N}_{>0}$ such that $b_{n}\\,\\leq\\,C a_{n}$ for all $n~\\geq~n_{0}$ ; $a_{n}\\,\\asymp\\,b_{n}$ if $a_{n}\\,=\\,\\mathcal{O}\\left(b_{n}\\right)$ and $b_{n}\\,=\\,\\mathcal{O}\\left(a_{n}\\right)$ . One can define our quantity of interest, the Hilbert-Schmidt independence criterion (HSIC; [Gretton et al., 2005, Quadrianto et al., 2009, Pfister et al., 2018, Szab\u00f3 and Sriperumbudur, 2018]), as ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{HSIC}_{k}(\\mathbb{P})=\\mathrm{MMD}_{k}\\left(\\mathbb{P},\\otimes_{m=1}^{M}\\mathbb{P}_{m}\\right)=\\|C_{X}\\|_{\\mathcal{H}_{k}}\\,,}\\\\ &{\\qquad C_{X}=\\mu_{k}(\\mathbb{P})-\\mu_{k}\\left(\\otimes_{m=1}^{M}\\mathbb{P}_{m}\\right)\\in\\mathcal{H}_{k},\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "and $C_{X}$ denotes the centered cross-covariance operator. ", "page_idx": 3}, {"type": "text", "text": "3 Results ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This section is dedicated to our results: The minimax lower bound for the estimation of $\\mathrm{HSIC}_{k}(\\mathbb{P})$ , where $k$ is a product of continuous bounded translation-invariant characteristic kernels is given in Theorem 1(ii). For the specific case where $k$ is a product of Gaussian kernels (stated in Theorem 1(i)), the constant in the lower bound is made explicit. Theorem 1(ii) also helps to establish a lower bound on the estimation of the cross-covariance operator (Corollary 1). ", "page_idx": 3}, {"type": "text", "text": "Before presenting our results, we recall the framework of minimax estimation [Tsybakov, 2009] adapted to our setting. Let $\\hat{F}_{n}$ denote any estimator of $\\mathrm{HSIC}_{k}(\\mathbb{P})$ based on $n$ i.i.d. samples from $\\mathbb{P}$ A sequence $(\\xi_{n})_{n=1}^{\\infty}$ ( $\\xi_{n}>0$ for all $n$ ) is said to be a lower bound of HSIC estimation w.r.t. a class $\\mathcal{P}$ of Borel probability measures on $\\mathbb{R}^{d}$ if there exists a constant $c>0$ such that ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\hat{F}_{n}}\\operatorname*{sup}_{\\mathbb{P}\\in\\mathcal{P}}\\mathbb{P}^{n}\\left\\{\\xi_{n}^{-1}\\left|\\mathrm{HSIC}_{k}(\\mathbb{P})-\\hat{F}_{n}\\right|\\geq c\\right\\}>0.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "If a specific estimator of HSIC ${\\tilde{F}}_{n}$ has an upper bound that matches $(\\xi_{n})_{n=1}^{\\infty}$ up to constants, that is, ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\Big\\vert\\mathrm{HSIC}_{k}(\\mathbb{P})-\\tilde{F}_{n}\\Big\\vert=\\mathcal{O}_{P}\\left(\\xi_{n}\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "then ${\\tilde{F}}_{n}$ is called minimax optimal. ", "page_idx": 3}, {"type": "text", "text": "We use Le Cam\u2019s method [Le Cam, 1973, Tsybakov, 2009] (recalled in Theorem B.5) to obtain bounds as in (3); estimators of HSIC achieving the bounds in (4) with $\\xi_{n}=n^{-1/2}$ are quoted in the introduction. The key to the application of the method is to show that there exist $\\alpha\\,>\\,0$ and $n_{0}\\in\\mathbb{N}_{>0}$ such that for all $n\\geq n_{0}$ one can find an adversarial pair of distributions $(\\mathbb{P}_{\\theta_{0}},\\mathbb{P}_{\\theta_{1}})=$ $(\\mathbb{P}_{\\theta_{0}}(n),\\mathbb{P}_{\\theta_{1}}(n))\\in\\mathcal{P}\\times\\mathcal{P}$ and $s_{n}>0$ for which ", "page_idx": 3}, {"type": "text", "text": "1. $\\mathrm{KL}\\left(\\mathbb{P}_{\\theta_{1}}^{n}\\big||\\mathbb{P}_{\\theta_{0}}^{n}\\right)\\leq\\alpha$ , in other words, the corresponding $n$ -fold product measures must be similar in the sense of Kullback-Leibler divergence, but   \n2. $|\\mathrm{HSIC}_{k}(\\mathbb{P}_{\\theta_{1}})-\\mathrm{HSIC}_{k}(\\mathbb{P}_{\\theta_{0}})|\\ \\geq\\ 2s_{n}$ , that is, their corresponding values of HSIC must be dissimilar.   \nIn this case, $\\begin{array}{r}{\\operatorname*{inf}_{\\hat{F}_{n}}\\operatorname*{sup}_{\\mathbb{P}\\in\\mathcal{P}}\\mathbb{P}^{n}\\left\\{\\left|\\mathrm{HSIC}_{k}(\\mathbb{P})-\\hat{F}_{n}\\right|\\geq s_{n}\\right\\}\\geq\\operatorname*{max}\\left(\\frac{e^{-\\alpha}}{4},\\frac{1-\\sqrt{\\alpha/2}}{2}\\right)}\\end{array}$ for all $n\\geq n_{0}$ ;   \nhence to establish the minimax optimality of existing estimators w.r.t. their known upper bounds, it is   \nsufficient to find adversarial pairs $\\{(\\mathbb{P}_{\\theta_{0}}(n),\\mathbb{P}_{\\theta_{1}}(n))\\}_{n\\ge n_{0}}$ that satisfy 1. for some positive constant   \n$\\alpha$ and also fulfill 2. with $s_{n}\\asymp n^{-1/2}$ . ", "page_idx": 3}, {"type": "text", "text": "The proof of the first part of our statement relies on the following Lemma 1 which yields the analytical value of $\\mathrm{HSIC}_{k}$ $(\\mathcal{N}(\\pmb{\\mu},\\pmb{\\Sigma}))$ , where $k=\\otimes_{m=1}^{M}k_{m}$ is the product of Gaussian kernels $k_{m}$ $(m\\in[M])$ and $\\mathcal{N}(\\pmb{\\mu},\\pmb{\\Sigma})$ denotes the multivariate normal distribution with mean $\\pmb{\\mu}\\in\\mathbb{R}^{d}$ and covariance matrix \u03a3 \u2208Rd\u00d7d. ", "page_idx": 3}, {"type": "text", "text": "Lemma 1 (Analytical value of HSIC for the Gaussian setting). Let us consider the Gaussian kernel $k(\\mathbf{x},\\mathbf{\\dot{y}})\\;\\;=\\;\\;e^{-{\\frac{\\gamma}{2}}\\|\\mathbf{x}-\\mathbf{y}\\|_{\\mathbb{R}^{d}}^{2}}\\;\\;(\\gamma\\;\\;>\\;\\;0,\\;\\,\\mathbf{x},\\mathbf{y}\\;\\;\\in\\;\\;\\mathbb{R}^{d})$ and Gaussian random variable $X\\;=\\;(X_{m})_{m=1}^{M}\\:\\sim\\:{\\mathcal{N}}(\\mathbf{m},{\\boldsymbol{\\Sigma}})\\;=:\\;\\mathbb{P},$ , where $X_{m}\\ \\in\\ \\mathbb{R}^{d_{m}}$ $\\mathit{\\Pi}^{\\prime}\\mathit{m}\\;\\in\\;[M])$ , $\\mathbf{m}\\;=\\;(\\mathbf{m}_{m})_{m=1}^{M}\\;\\in\\;\\mathbb{R}^{d}$ $\\begin{array}{r}{\\Sigma=[\\Sigma_{i,j}]_{i,j\\in[M]}\\in\\mathbb{R}^{d\\times d}}\\end{array}$ , $\\pmb{\\Sigma}_{i,j}\\in\\mathbb{R}^{d_{i}\\times d_{j}}$ , and $\\begin{array}{r}{d=\\sum_{m\\in[M]}d_{m}}\\end{array}$ . In this case, with $\\pmb{\\Sigma}_{1}=\\pmb{\\Sigma}$ and $\\Sigma_{2}=\\mathrm{bdiag}(\\Sigma_{1,1},\\ldots,\\Sigma_{M,M})$ , we have ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{HSIC}_{k}^{2}(\\mathbb{P})=\\frac{1}{\\left|2\\gamma\\pmb{\\Sigma}_{1}+\\mathbf{I}_{d}\\right|^{\\frac{1}{2}}}+\\frac{1}{\\left|2\\gamma\\pmb{\\Sigma}_{2}+\\mathbf{I}_{d}\\right|^{\\frac{1}{2}}}-\\frac{2}{\\left|\\gamma\\pmb{\\Sigma}_{1}+\\gamma\\pmb{\\Sigma}_{2}+\\mathbf{I}_{d}\\right|^{\\frac{1}{2}}}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In this work, we focus on continuous bounded translation-invariant kernels, which are fully characterized by Bochner\u2019s theorem [Wendland, 2005, Theorem 6.6]; the theorem states that a function on $\\mathbb{R}^{d}$ is positive definite if and only if it is the Fourier transform of a finite nonnegative measure.4We use this description to obtain our main result, which is as follows. ", "page_idx": 4}, {"type": "text", "text": "Theorem 1 (Lower bound for HSIC estimation on $\\mathbb{R}^{d}$ ). Let $\\mathcal{P}$ be a class of Borel probability measures over $\\mathbb{R}^{d}$ containing the $d$ -dimensional Gaussian distributions. Let $\\begin{array}{r}{d=\\sum_{m\\in[M]}d_{m}}\\end{array}$ and $\\hat{F}_{n}$ denote any estimator of $\\mathrm{HSIC}_{k}(\\mathbb{P})$ with $n\\geq2=:n_{0}$ i.i.d. samples from $\\mathbb{P}\\in\\mathcal{P}$ . Assume further that $k=\\otimes_{m=1}^{M}k_{m}$ where either, for $m\\in[M]$ , ", "page_idx": 4}, {"type": "text", "text": "(i) the kernels $k_{m}:\\mathbb{R}^{d_{m}}\\times\\mathbb{R}^{d_{m}}\\rightarrow\\mathbb{R}$ are Gaussian with common bandwidth parameter $\\gamma>0$ defined by $\\left(\\mathbf{x}_{m},\\mathbf{x}_{m}^{\\prime}\\right)\\mapsto e^{-\\frac{\\gamma}{2}\\left\\Vert\\mathbf{x}_{m}-\\mathbf{x}_{m}^{\\prime}\\right\\Vert_{\\mathbb{R}_{d_{m}}}^{2}}\\left(\\mathbf{x}_{m},\\mathbf{x}_{m}^{\\prime}\\in\\mathbb{R}^{d_{m}}\\right),\\,o r$ ", "page_idx": 4}, {"type": "text", "text": "(ii) the kernels $k_{m}:\\mathbb{R}^{d_{m}}\\times\\mathbb{R}^{d_{m}}\\rightarrow\\mathbb{R}$ are continuous bounded translation-invariant characteristic kernels. ", "page_idx": 4}, {"type": "text", "text": "Then, for any $n\\geq n_{0}$ , it holds that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\hat{F}_{n}}\\operatorname*{sup}_{\\mathbb{P}\\in\\mathcal{P}}\\mathbb{P}^{n}\\left\\{\\left|\\mathrm{HSIC}_{k}\\left(\\mathbb{P}\\right)-\\hat{F}_{n}\\right|\\geq\\frac{c}{\\sqrt{n}}\\right\\}\\geq\\frac{1-\\sqrt{\\frac{5}{8}}}{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with (i) the constant 2(2\u03b3+\u03b31)d4 +1 > 0 (depending on \u03b3 and d only) in the first case, or (ii) some constant $c>0$ in the second case. ", "page_idx": 4}, {"type": "text", "text": "We note that while Theorem 1(ii) applies to the more general class of translation-invariant kernels, we include Theorem 1(i) as it makes the constant $c$ explicit. ", "page_idx": 4}, {"type": "text", "text": "The following corollary allows to recover the recent lower bound on the estimation of the covariance operator by Zhou et al. [2019, Theorem 5] as a special case that we detail in Remark 1(e). ", "page_idx": 4}, {"type": "text", "text": "Corollary 1 (Lower bound on cross-covariance operator estimation). In the setting of Theorem 1(ii), let $\\hat{F}_{n}$ denote any estimator of the centered cross-covariance operator $C_{X}\\in\\mathcal{H}_{k}$ defined in (2) with $n\\geq2=:n_{0}$ i.i.d. samples from $\\mathbb{P}\\in{\\mathcal{P}}$ . Then, for any $n\\geq n_{0}$ , it holds that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\hat{F}_{n}}\\operatorname*{sup}_{\\mathbb{P}\\in\\mathcal{P}}\\mathbb{P}^{n}\\left\\{\\left\\|C_{X}-\\hat{F}_{n}\\right\\|_{\\mathcal{H}_{k}}\\geq\\frac{c}{\\sqrt{n}}\\right\\}\\geq\\frac{1-\\sqrt{\\frac{5}{8}}}{2},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "for some constant $c>0$ . ", "page_idx": 4}, {"type": "text", "text": "Remark 1. ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "(a) Validness of HSIC. Though generally the characteristic property of $(k_{m})_{m=1}^{M}{=}$ is not enough [Szab\u00f3 and Sriperumbudur, 2018, Example 2] for $M>2$ to ensure the $\\mathcal{T}$ -characteristic property of $k\\,=\\,\\otimes_{m=1}^{M}\\dot{k}_{m}$ (in other words, that $H S I C_{k}(\\mathbb{P})\\,=\\,0$ iff. $\\ensuremath{\\mathbb{P}}=\\otimes_{m=1}^{M}\\ensuremath{\\mathbb{P}}_{m})$ , on $\\mathbb{R}^{d}$ under the imposed continuous bounded translation-invariant assumption $(i)\\;k$ being characteristic, $(i i)\\;k$ being $\\mathcal{T}$ -characteristic, and (iii) $(k_{m})_{m=1}^{M}$ -s being characteristic are equivalent (Theorem B.4). ", "page_idx": 4}, {"type": "text", "text": "(b) Minimax optimality of existing HSIC estimators. The lower bounds in Theorem 1 asymptotically match the known upper bounds of the $U$ -statistic and $V\\!\\cdot$ -statistic-based estimators of $\\xi_{n}=n^{-1/\\frac{\\gamma}{2}}$ . ", "page_idx": 4}, {"type": "text", "text": "The Nystr\u00f6m-based HSIC estimator achieves the same rate under an appropriate decay of the eigenspectrum of the respective covariance operator. Hence, Theorem 1 implies the optimality of these estimators on $\\mathbb{R}^{d}$ with continuous bounded translation-invariant characteristic kernels in the minimax sense. ", "page_idx": 5}, {"type": "text", "text": "(c) Difference compared to Tolstikhin et al. [2016] (minimax MMD estimation). We note that $a$ lower bound for the related $M M D_{k}$ exists. However, the adversarial distribution pair $(\\mathbb{P}_{\\theta_{1}},\\mathbb{P}_{\\theta_{0}})$ constructed by Tolstikhin et al. [2016, Theorem $I J$ to obtain the lower bound on MMD estimation has a product structure which implies that $|\\mathrm{HSIC}_{k}({\\mathbb P}_{\\theta_{1}})-\\mathrm{HSIC}_{k}({\\mathbb P}_{\\theta_{0}})|=0$ and hence it is not applicable in our case of HSIC; Tolstikhin et al. [2016, Theorem $2J$ with radial kernels has the same restriction. ", "page_idx": 5}, {"type": "text", "text": "(d) Difference compared to Tolstikhin et al. [2017] (minimax mean embedding estimation). The estimation of the mean embedding $\\mu_{k}(\\mathbb{P})$ is known to have a minimax rate of $O\\left(n^{-1/2}\\right)$ . But, this rate does not imply an optimal lower bound for the estimation of MMD as is evident from the two works [Tolstikhin et al., 2016, 2017]. The same conclusion holds for HSIC estimation. ", "page_idx": 5}, {"type": "text", "text": "(e) Difference compared to Zhou et al. [2019] (minimax covariance operator estimation). For the related problem of estimating the centered covariance operator ", "page_idx": 5}, {"type": "equation", "text": "$$\nC_{X X}=\\int_{\\ensuremath{\\mathbb{R}}^{d}}\\left(\\phi_{k}(x)-\\mu_{k}(\\ensuremath{\\mathbb{P}})\\right)\\otimes\\left(\\phi_{k}(x)-\\mu_{k}(\\ensuremath{\\mathbb{P}})\\right)\\mathrm{d}\\ensuremath{\\mathbb{P}}(x)\\in\\mathcal{H}_{k}\\otimes\\mathcal{H}_{k},\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Zhou et al. [2019, Theorem $5J$ give the lower bound ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\hat{F}_{n}}\\operatorname*{sup}_{\\mathbb{P}\\in\\mathcal{P}}\\mathbb{P}^{n}\\left\\{\\left\\lVert C_{X X}-\\hat{F}_{n}\\right\\rVert_{\\mathcal{H}_{k}\\otimes\\mathcal{H}_{k}}\\geq\\frac{c}{\\sqrt{n}}\\right\\}\\geq1/8\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "in the same setting as in Theorem $I(i i),$ , where $\\hat{F}_{n}$ is any estimator of the centered covariance $C_{X X}$ , and $c$ is a positive constant. By noting that the centered covariance is the centered cross-covariance of a random variable with itself, Corollary $^{\\,I}$ recovers their result. ", "page_idx": 5}, {"type": "text", "text": "The next section contains our proofs. ", "page_idx": 5}, {"type": "text", "text": "4 Proofs ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "This section is dedicated to our proofs. We present the proof of Lemma 1 in Section 4.1, that of Theorem 1 in Section 4.2, and that of Corollary 1 in Section 4.3. ", "page_idx": 5}, {"type": "text", "text": "4.1 Proof of Lemma 1 ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "As ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{HSIC}_{k}^{2}(\\mathbb{P})=\\mathrm{MMD}_{k}^{2}(\\mathbb{P},\\mathbb{Q})=\\|\\mu_{k}(\\mathbb{P})-\\mu_{k}(\\mathbb{Q})\\|_{\\mathcal{H}_{k}}^{2}}\\\\ &{\\qquad\\qquad=\\langle\\mu_{k}(\\mathbb{P}),\\mu_{k}(\\mathbb{P})\\rangle_{\\mathcal{H}_{k}}+\\langle\\mu_{k}(\\mathbb{Q}),\\mu_{k}(\\mathbb{Q})\\rangle_{\\mathcal{H}_{k}}-2\\langle\\mu_{k}(\\mathbb{P}),\\mu_{k}(\\mathbb{Q})\\rangle_{\\mathcal{H}_{k}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "with $\\mathbb{Q}~=~\\otimes_{m=1}^{M}\\mathbb{P}_{m}~=~{\\mathcal N}(\\mathbf{m},\\mathrm{bdiag}(\\Sigma_{1,1},\\dots,\\Sigma_{M,M}))$ , $\\mathbb{P}_{m}~=~\\mathcal{N}(\\mathbf{m}_{m},\\boldsymbol{\\Sigma}_{m,m})$ , it is sufficient to be able to compute $\\langle\\mu_{k}(\\mathbb{P}),\\mu_{k}(\\mathbb{Q})\\rangle_{\\mathcal{H}_{k}}$ -type quantities with $\\mathbb{P}\\;=\\;\\mathcal{N}(\\mathbf{m}_{1},\\pmb{\\Sigma}_{1})$ and $\\mathbb{Q}\\;=\\;\\mathcal{N}(\\mathbf{m}_{2},\\pmb{\\Sigma}_{2})$ . One can show [Muandet et al., 2011, Table 1] that $\\langle\\mu_{k}(\\mathbb{P}),\\mu_{k}(\\mathbb{Q})\\rangle_{\\mathcal{H}_{k}}\\ =$ $\\begin{array}{r}{\\frac{e^{-\\frac{1}{2}(\\mathbf{m}_{1}-\\mathbf{m}_{2})^{\\mathsf{T}}\\left(\\{\\Sigma_{1}+\\Sigma_{2}+\\gamma^{-1}\\mathbf{I}_{d}}\\right)^{-1}(\\mathbf{m}_{1}-\\mathbf{m}_{2})}}{|\\gamma\\Sigma_{1}+\\gamma\\Sigma_{2}+\\mathbf{I}_{d}|^{\\frac{1}{2}}}}\\end{array}$ . Using this fact and that $\\mathbf{m}=\\mathbf{m}_{1}=\\mathbf{m}_{2}$ , the result follows. ", "page_idx": 5}, {"type": "text", "text": "4.2 Proof of Theorem 1 ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "The setup and the upper bound on $\\mathrm{KL}(\\mathbb{P}_{\\theta_{1}}^{n}||\\mathbb{P}_{\\theta_{0}}^{n})$ agree for (i) and (ii) but the methods that we use to lower bound $|\\mathrm{HSIC}_{k}(\\mathbb{P}_{\\theta_{1}})-\\mathrm{HSIC}_{k}(\\mathbb{P}_{\\dot{\\theta_{0}}})|$ differ. We structure the proof accordingly and present the overlapping part before we branch out into (i) and (ii). Both parts of the statement rely on Le Cam\u2019s method, which we state as Theorem B.5 for self-completeness. ", "page_idx": 5}, {"type": "text", "text": "To construct the adversarial pair, we consider a class $\\mathcal{G}$ of Gaussian distributions over $\\mathbb{R}^{d}$ such that every element $\\mathcal{N}(\\mu,\\Sigma)\\in\\dot{\\mathcal{G}}$ , with ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Sigma=\\Sigma(i,j,\\rho)=\\left[\\begin{array}{l l l l l l}{1}&{\\cdots}&{0}&{0}&{\\cdots}&{0}\\\\ {\\vdots}&{\\ddots}&{\\vdots}&{\\vdots}&&{\\vdots}\\\\ {0}&{\\cdots}&{1}&{\\rho}&{\\cdots}&{0}\\\\ {0}&{\\cdots}&{\\rho}&{1}&{\\cdots}&{0}\\\\ {\\vdots}&&{\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {0}&{\\cdots}&{0}&{0}&{\\cdots}&{1}\\end{array}\\right]\\in\\mathbb{R}^{d\\times d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "and (fixed) $i=d_{1},\\,j=d_{1}+1,\\,\\rho\\in(-1,1)$ . In other words, $\\Sigma$ is essentially the $d$ -dimensional matrix $\\pmb{I}_{d}$ except for the $(i,j)$ and $(j,i)$ entry; both entries are identical to $\\rho$ , and they specify the correlation of the respective coordinates. This family of distributions is indexed by a tuple $(\\pmb{\\mu},\\rho)\\in\\mathbb{R}^{d}\\times(-1,1)=:\\dot{\\pmb{A}}$ and, for $a\\in A$ , we write $\\mathbb{P}_{a}$ for the associated distribution. To bring ourselves into the setting of Theorem B.5, we fix $n\\in\\mathbb{N}_{>0}$ , choose $\\mathcal{X}=\\left(\\mathbb{R}^{d}\\right)^{n}$ , set $\\Theta=\\{\\theta_{a}:=$ $\\mathrm{HSIC}_{k}(\\mathbb{P}_{a}):a\\in{\\cal A}\\}$ , ${\\mathcal{P}}_{\\Theta}=\\{\\mathbb{P}_{a}^{n}\\,:\\,a\\in{\\mathcal{A}}\\}=\\{\\mathbb{P}_{a}^{n}\\,:\\,\\theta_{a}\\in\\Theta\\}$ , and use the metric $\\left(x,y\\right)\\mapsto\\left|x\\!-\\!y\\right|$ for $x,y\\in\\mathbb{R}$ . Hence, the data $D\\sim\\mathbb{P}_{\\theta}\\in\\mathcal{P}_{\\Theta}$ . For brevity, let $F:A\\rightarrow\\mathbb{R}$ stand for $a\\mapsto\\mathrm{HSIC}_{k}(\\mathbb{P}_{a})$ , and let $\\hat{F}_{n}$ stand for the corresponding estimator based on $n$ samples. ", "page_idx": 6}, {"type": "text", "text": "As $\\mathcal{G}\\subseteq\\mathcal{P}$ , it holds for every positive $s$ that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\mathbb{P}\\in\\mathcal{P}}\\mathbb{P}^{n}\\left\\{\\left|\\mathrm{HSIC}_{k}\\left(\\mathbb{P}\\right)-\\hat{F}_{n}\\right|\\geq s\\right\\}\\geq\\operatorname*{sup}_{\\mathbb{P}\\in\\mathcal{G}}\\mathbb{P}^{n}\\left\\{\\left|\\mathrm{HSIC}_{k}\\left(\\mathbb{P}\\right)-\\hat{F}_{n}\\right|\\geq s\\right\\}.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Let $\\mathbb{P}_{\\theta_{0}}=\\mathcal{N}\\left(\\mu_{0},\\Sigma_{0}\\right)$ and $\\mathbb{P}_{\\theta_{1}}=\\mathcal{N}\\left(\\mu_{1},\\Sigma_{1}\\right)$ with ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\mu_{0}=\\mathbf{0}_{d}\\in\\mathbb{R}^{d},}&{\\qquad\\qquad\\sum_{0}=\\pm(d_{1},d_{1}+1,0)=\\mathbf{I}_{d}\\in\\mathbb{R}^{d\\times d},}\\\\ {\\displaystyle\\mu_{1}=\\frac{1}{\\sqrt{d}n}\\mathbf{1}_{d}\\in\\mathbb{R}^{d},}&{\\qquad\\qquad\\sum_{1}=\\pmb{\\Sigma}(d_{1},d_{1}+1,\\rho_{n})\\in\\mathbb{R}^{d\\times d},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\rho_{n}~~\\in~(-1,1)$ will be chosen appropriately later.5 We now proceed to upper bound KL $\\left(\\mathbb{P}_{\\theta_{1}}^{n}||\\mathbb{P}_{\\theta_{0}}^{n}\\right)$ and lower bound $|F(\\theta_{1})-F(\\theta_{0})|$ . ", "page_idx": 6}, {"type": "text", "text": "Upper bound for KL divergence Lemma A.1 implies that with $\\textstyle\\rho_{n}^{2}\\,=\\,{\\frac{1}{n}}$ , one has the bound $\\mathrm{KL}\\left(\\mathbb{P}_{\\theta_{1}}^{n}||\\mathbb{P}_{\\theta_{0}}^{n}\\right)\\leq\\alpha:=\\frac{5}{4}$ for $n\\geq2=:n_{0}$ . ", "page_idx": 6}, {"type": "text", "text": "Lower bound (i): Gaussian kernels. Recall that the considered kernel is $k(\\mathbf{x},\\mathbf{y})=e^{-\\frac{\\gamma}{2}\\|\\mathbf{x}-\\mathbf{y}\\|_{\\mathbb{R}^{d}}^{2}}$ $(\\gamma>0)$ . The idea of the proof is as follows. ", "page_idx": 6}, {"type": "text", "text": "1. We express $|F(\\theta_{1})-F(\\theta_{0})|$ in closed form as a function of $\\gamma,\\rho_{n}$ , and $d$ .   \n2. Using the analytical form obtained in the 1st step, we construct the lower bound. ", "page_idx": 6}, {"type": "text", "text": "This is what we detail next. ", "page_idx": 6}, {"type": "text", "text": "\u2022 Analytical form of $|F(\\theta_{1})-F(\\theta_{0})|$ : Using the fact that $\\mathrm{HSIC}_{k}(\\mathbb{P}_{\\theta_{0}})=0$ , we have that ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left|F\\left(\\theta_{1}\\right)-\\underbrace{F\\left(\\theta_{0}\\right)}_{=0}\\right|^{2}=F^{2}\\left(\\theta_{1}\\right)=\\mathrm{HSIC}_{k}^{2}\\left(\\mathbb{P}_{\\theta_{1}}\\right)=\\mathrm{MMD}_{k}^{2}\\left(N\\left(\\mu_{1},\\Sigma_{1}\\right),N\\left(\\mu_{1},\\mathbf{I}_{d}\\right)\\right)}\\\\ &{=\\|\\mu_{k}\\left(N\\left(\\mu_{1},\\Sigma_{1}\\right)\\right)-\\mu_{k}\\left({N\\left(\\mu_{1},\\mathbf{I}_{d}\\right)}\\right)\\|_{\\mathcal{H}_{k}}^{2}}\\\\ &{=\\underbrace{\\left\\langle\\mu_{k}\\left(N\\left(\\mu_{1},\\Sigma_{1}\\right)\\right),\\mu_{k}\\left(N\\left(\\mu_{1},\\Sigma_{1}\\right)\\right)\\right\\rangle_{\\mathcal{H}_{k}}}_{\\left(i\\right)}+\\underbrace{\\left\\langle\\mu_{k}\\left(N\\left(\\mu_{1},\\mathbf{I}_{d}\\right)\\right),\\mu_{k}\\left(N\\left(\\mu_{1},\\mathbf{I}_{d}\\right)\\right)\\right\\rangle_{\\mathcal{H}_{k}}}_{\\left(i i\\right)}}\\\\ &{\\quad-2\\underbrace{\\left\\langle\\mu_{k}\\left(N\\left(\\mu_{1},\\Sigma_{1}\\right)\\right),\\mu_{k}\\left(N\\left(\\mu_{1},\\mathbf{I}_{d}\\right)\\right)\\right\\rangle_{\\mathcal{H}_{k}}}_{\\left(i i i\\right)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "5Notice the dependence of $\\mathbb{P}_{\\theta_{1}}$ on $_n$ . ", "page_idx": 6}, {"type": "text", "text": "which we compute term-by-term with Lemma 1, and obtain ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\left(i\\right)=\\left\\vert2\\gamma\\pmb{\\Sigma}_{1}+\\mathbf{I}_{d}\\right\\vert^{-1/2}=\\left[\\left(2\\gamma+1\\right)^{d-2}\\left(\\left(2\\gamma+1\\right)^{2}-\\left(2\\gamma\\rho_{n}\\right)^{2}\\right)\\right]^{-1/2},}\\\\ &{\\left(i i\\right)=\\left\\vert2\\gamma\\pmb{\\check{\\mathbf{I}}}_{d}+\\mathbf{I}_{d}\\right\\vert^{-1/2}=\\left[\\left(2\\gamma+1\\right)^{d}\\right]^{-1/2},}\\\\ &{\\left(i i i\\right)=\\left\\vert\\gamma\\pmb{\\Sigma}_{1}+\\gamma\\mathbf{I}_{d}+\\mathbf{I}_{d}\\right\\vert^{-1/2}=\\left[\\left(2\\gamma+1\\right)^{d-2}\\left(\\left(2\\gamma+1\\right)^{2}-\\left(\\gamma\\rho_{n}\\right)^{2}\\right)\\right]^{-1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Combining (i), (ii), and (iii) yields that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{HSIC}_{k}^{2}\\left(\\mathbb{P}_{\\theta_{1}}\\right)=(i)+(i i)-2(i i i)}\\\\ &{\\qquad\\qquad\\qquad=\\left[\\left(2\\gamma+1\\right)^{d-2}\\left(\\left(2\\gamma+1\\right)^{2}-\\left(2\\gamma\\rho_{n}\\right)^{2}\\right)\\right]^{-1/2}+\\left[\\left(2\\gamma+1\\right)^{d}\\right]^{-1/2}}\\\\ &{\\qquad\\qquad\\qquad-\\ 2\\left[\\left(2\\gamma+1\\right)^{d-2}\\left(\\left(2\\gamma+1\\right)^{2}-\\left(\\gamma\\rho_{n}\\right)^{2}\\right)\\right]^{-1/2}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "\u2022 Lower bound on $|F(\\theta_{1})-F(\\theta_{0})|$ : Next, we show that there exists $c>0$ such that for any $n\\in\\mathbb{N}_{>0}$ it holds that $\\begin{array}{r}{\\mathrm{HSIC}_{k}^{2}\\left(\\mathbb{P}_{\\theta_{1}}\\right)\\geq\\frac{c}{n}}\\end{array}$ . For $\\begin{array}{r}{0<x<\\left(1+\\frac{1}{2\\gamma}\\right)^{2}}\\end{array}$ , let us consider the function ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{f_{c}(x)=\\left[\\left(2\\gamma+1\\right)^{d-2}\\left(\\left(2\\gamma+1\\right)^{2}-4\\gamma^{2}x\\right)\\right]^{-1/2}+\\left[\\left(2\\gamma+1\\right)^{d}\\right]^{-1/2}}}\\\\ {{\\mathrm{}}}\\\\ {{\\qquad-2\\left[\\left(2\\gamma+1\\right)^{d-2}\\left(\\left(2\\gamma+1\\right)^{2}-\\gamma^{2}x\\right)\\right]^{-1/2}-c x}}\\\\ {{\\mathrm{}}}\\\\ {{\\qquad=\\left[z^{d-2}\\left(z^{2}-4\\gamma^{2}x\\right)\\right]^{-1/2}+\\left(z^{d}\\right)^{-1/2}-2\\left[z^{d-2}\\left(z^{2}-\\gamma^{2}x\\right)\\right]^{-1/2}-c x,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "with the shorthand $z:=2\\gamma+1$ .6 With this notation, $f_{c}(1/n)=\\mathrm{HSIC}_{k}^{2}\\left(\\mathbb{P}_{\\theta_{1}}\\right)-c/n$ ; our aim is to determine $c>0$ such that $f_{c}(1/n)\\ge0$ for any positive integer $n$ . To achieve this goal, notice that $f_{c}(0)=0$ , and ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{f_{c}^{\\prime}(x)=\\frac{2\\gamma^{2}z^{d-2}}{\\left[z^{d-2}\\,\\left(z^{2}-4x\\gamma^{2}\\right)\\right]^{3/2}}-\\frac{\\gamma^{2}z^{d-2}}{\\left[z^{d-2}\\,\\left(z^{2}-x\\gamma^{2}\\right)\\right]^{3/2}}-c}\\\\ &{\\qquad\\gg\\frac{2\\gamma^{2}z^{d-2}}{\\left[z^{d-2}\\,\\left(z^{2}-x\\gamma^{2}\\right)\\right]^{3/2}}-\\frac{\\gamma^{2}z^{d-2}}{\\left[z^{d-2}\\,\\left(z^{2}-x\\gamma^{2}\\right)\\right]^{3/2}}-c=\\frac{\\gamma^{2}z^{d-2}}{\\left[z^{d-2}\\,\\left(z^{2}-x\\gamma^{2}\\right)\\right]^{3/2}}-c}\\\\ &{\\qquad>\\frac{\\gamma^{2}z^{d-2}}{\\left(z^{d-2}z^{2}\\right)^{3/2}}-c=\\frac{\\gamma^{2}}{z^{2}\\sqrt{z^{d}}}-c=\\frac{\\gamma^{2}}{\\left(2\\gamma+1\\right)^{2}\\,\\sqrt{\\left(2\\gamma+1\\right)^{d}}}-c.}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Choosing now c = (2\u03b3+1)2\u03b3\u221a(2\u03b3+1)d > 0, we have f c\u2032(x) \u22650, so f is a nondecreasing function. Note that $f_{c}(1/n)=\\mathrm{HSIC}_{k}^{2}\\left(\\mathbb{P}_{\\theta_{1}}\\right)-c/n\\geq0$ , with $x=1/n$ and $\\begin{array}{r}{\\left(1+\\frac{1}{2\\gamma}\\right)^{-2}<1\\leq n<\\infty}\\end{array}$ By taking the positive square root, this means that ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname{HSIC}_{k}\\left(\\mathbb{P}_{\\theta_{1}}\\right)\\geq{\\frac{\\gamma}{\\left(2\\gamma+1\\right)\\left(\\left(2\\gamma+1\\right)^{d}\\right)^{1/4}{\\sqrt{n}}}}=:2s\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "holds for $n\\geq1$ , implying that $|F(\\theta_{1})-F(\\theta_{0})|\\geq2s>0$ . ", "page_idx": 7}, {"type": "text", "text": "We conclude the proof by Theorem B.5 using that $\\begin{array}{r}{\\alpha=\\frac{5}{4}}\\end{array}$ and $\\begin{array}{r}{\\operatorname*{max}\\left(\\frac{e^{-\\frac{5}{4}}}{4},\\frac{1-\\sqrt{\\frac{5}{8}}}{2}\\right)=\\frac{1-\\sqrt{\\frac{5}{8}}}{2}.}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "Lower bound (ii): translation-invariant kernels. Let $\\Lambda_{k}$ denote the spectral measure associated to the kernel $k$ according to (1). Using the fact that $\\mathrm{HSIC}_{k}(\\mathbb{P}_{\\theta_{0}})=0$ , we have for $|F(\\theta_{1})-F(\\theta_{0})|$ that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F\\left(\\theta_{1}\\right)-\\underset{=0}{\\underbrace{F\\left(\\theta_{0}\\right)}}\\Big|^{2}=F^{2}\\left(\\theta_{1}\\right)=\\mathrm{HST}_{k}^{2}\\left(\\mathbb{P}_{\\theta_{1}}\\right)=\\mathrm{MMD}_{k}^{2}\\left(N\\left(\\mu_{1},\\Sigma_{1}\\right),N\\left(\\mu_{1},\\Sigma_{0}\\right)\\right)}\\\\ &{\\quad\\overset{(i)}{=}{\\left\\lVert{\\psi_{N}(\\mu_{1},\\Sigma_{1})-\\psi_{N}(\\mu_{1},\\Sigma_{0})}\\right\\rVert}_{L^{2}\\left(\\mathbb{R}^{d},\\Lambda_{k}\\right)}^{2}}\\\\ &{\\quad\\overset{(i i)}{=}{\\int_{\\mathbb{R}^{d}}}\\left|e^{i\\left(\\mu_{1},\\omega\\right)-\\frac{1}{2}\\left(\\omega,\\Sigma_{1}\\omega\\right)}-e^{i\\left(\\mu_{1},\\omega\\right)-\\frac{1}{2}\\left(\\omega,\\Sigma_{0}\\right)\\omega}\\right|^{2}\\mathrm{d}\\Lambda_{k}\\left(\\omega\\right)}\\\\ &{\\quad=\\ \\displaystyle\\int_{\\mathbb{R}^{d}}\\left|e^{i\\left(\\mu_{1},\\omega\\right)}\\right|^{2}\\left|e^{-\\frac{1}{2}\\left(\\omega,\\Sigma_{1}\\omega\\right)}-e^{-\\frac{1}{2}\\left(\\omega,\\Sigma_{0}\\right)\\omega}\\right|^{2}\\mathrm{d}\\Lambda_{k}\\left(\\omega\\right)}\\\\ &{\\quad\\overset{(i i i)}{\\geq}\\displaystyle\\int_{A}\\left|e^{-\\frac{1}{2}\\left(\\omega,\\Sigma_{1}\\omega\\right)}-e^{-\\frac{1}{2}\\left(\\omega,\\Sigma_{0}\\right)\\omega}\\right|^{2}\\mathrm{d}\\Lambda_{k}\\left(\\omega\\right)^{\\frac{(i v)}{2}}\\ \\rho_{n}^{2}\\underset{=:\\{-\\tau_{S}^{2}\\}}{\\underbrace{\\left[h_{\\omega}^{\\prime}\\left(0\\right)\\right]^{2}\\mathrm{d}\\Lambda_{k}\\left(\\omega\\right)}}\\stackrel{\\left(\\frac{2C}{\\alpha}\\right)^{2}}{=}\\ \\underbrace{\\frac{\\left(2C\\right)^{2}}{n}}_{=:\\{2\\sqrt{2}\\}\\,,0}\\ ,}\\end{array}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $(i)$ holds by Sriperumbudur et al. [2010, Corollary 4(i)] (recalled in Theorem B.2). $(i i)$ follows from the analytical form $\\psi_{\\mathcal{N}(\\pmb{\\mu},\\pmb{\\Sigma})}(\\mathbf{t})=e^{i\\left\\langle\\pmb{\\mu},\\pmb{t}\\right\\rangle-\\frac{1}{2}\\left\\langle\\mathbf{t},\\pmb{\\Sigma}\\mathbf{t}\\right\\rangle}$ of the characteristic function of a multivariate normal distribution $\\mathcal{N}(\\pmb{\\mu},\\pmb{\\Sigma})$ . For $(i i i)$ , we define the non-empty open set ", "page_idx": 8}, {"type": "equation", "text": "$$\nA=\\left\\{\\omega=(\\omega_{1},\\ldots,\\omega_{d})^{\\mathsf{T}}\\in\\mathbb{R}^{d}\\,:\\,\\omega_{d_{1}}\\omega_{d_{1}+1}<0\\right\\}\\subset\\mathbb{R}^{d},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "and use that the integration of a non-negative function over a subset yields a lower bound. In $(i v)$ , fix $\\omega\\in A$ and let ", "page_idx": 8}, {"type": "equation", "text": "$$\nh_{\\omega}:\\rho\\in[0,1]\\mapsto e^{-\\frac12\\langle\\omega,\\Sigma(d_{1},d_{1}+1,\\rho)\\omega\\rangle}\\in(0,1].\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Note that $h_{\\omega}(\\rho)\\,=\\,e^{-\\frac{1}{2}\\left(\\omega^{\\top}\\omega+2\\rho\\omega_{d_{1}}\\omega_{d_{1}+1}\\right)}$ ; $h_{\\omega}$ is continuous on $[0,1]$ and differentiable on $(0,1)$ Hence for any $\\rho\\in(0,1)$ , by the mean value theorem, there exists $\\tilde{\\rho}\\in\\dot{(0,1)}$ such that ", "page_idx": 8}, {"type": "equation", "text": "$$\nh_{\\omega}(\\rho)-h_{\\omega}(0)=\\rho h_{\\omega}^{\\prime}(\\tilde{\\rho})\\geq\\rho\\operatorname*{min}_{c\\in[0,1]}h_{\\omega}^{\\prime}(c).\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We have the first and second derivatives ", "page_idx": 8}, {"type": "equation", "text": "$$\nh_{\\omega}^{\\prime}(c)=-\\omega_{d_{1}}\\omega_{d_{1}+1}e^{-\\frac{1}{2}\\left(\\omega^{\\mathsf{T}}\\omega+2c\\omega_{d_{1}}\\omega_{d_{1}+1}\\right)},\\quad h_{\\omega}^{\\prime\\prime}(c)=\\omega_{d_{1}}^{2}\\omega_{d_{1}+1}^{2}e^{-\\frac{1}{2}\\left(\\omega^{\\mathsf{T}}\\omega+2c\\omega_{d_{1}}\\omega_{d_{1}+1}\\right)}>0,\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "which implies that $c\\mapsto h_{\\omega}^{\\prime}(c)$ is a strictly increasing function of $c$ and that it attains its minimum at $c=0$ , that is, ", "page_idx": 8}, {"type": "equation", "text": "$$\nh_{\\pmb{\\omega}}(\\rho)-h_{\\pmb{\\omega}}(0)\\geq\\rho h_{\\pmb{\\omega}}^{\\prime}(0)>0,\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where the 2nd inequality holds by $\\rho>0$ and $\\omega\\in A$ . This shows that ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\left[h_{\\omega}(\\rho)-h_{\\omega}(0)\\right]^{2}\\ge\\left[\\rho h_{\\omega}^{\\prime}(0)\\right]^{2},\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "and the monotonicity of integration gives $(i v)$ . For $(v)$ , we note that the kernel $k\\,=\\,\\otimes_{m=1}^{M}k_{m}$ is characteristic [Szab\u00f3 and Sriperumbudur, 2018, Theorem 4] (recalled in Theorem B.4) as the $(k_{m})_{m=1}^{M}$ -s are characteristic. Thus, supp $(\\Lambda_{k})=\\mathbb{R}^{d}$ (see Sriperumbudur et al. [2010, Theorem 9]; recalled in Theorem B.3), implying that $\\Lambda_{k}(A)>0$ . $(v)$ follows from the positivity of $h_{\\omega}^{\\prime}(0)$ (for any $\\omega\\in A$ ), from the fact that the integral of a positive function on a set with positive measure is positive, and from our choice of $\\rho_{n}=\\bar{n}^{-1/2}$ . ", "page_idx": 8}, {"type": "text", "text": "Now, by taking the positive square root, we have ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\left|F\\left(\\theta_{1}\\right)-F\\left(\\theta_{0}\\right)\\right|\\geq{\\frac{2c}{\\sqrt{n}}}=:2s.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We conclude by the application of Theorem B.5 using that $\\begin{array}{r}{\\alpha=\\frac{5}{4}}\\end{array}$ and $\\begin{array}{r}{\\operatorname*{max}\\left(\\frac{e^{-\\frac{5}{4}}}{4},\\frac{1-\\sqrt{\\frac{5}{8}}}{2}\\right)=\\frac{1-\\sqrt{\\frac{5}{8}}}{2}}\\end{array}$ ", "page_idx": 8}, {"type": "text", "text": "4.3 Proof of Corollary 1 ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We use the same argument as in the beginning of the proof of Theorem 1 in Section 4.2 but adjust the setting in which we apply Theorem B.5. Specifically, we now let $\\Theta=\\{\\theta_{a}:=C_{X_{a}}\\,:\\,X_{a}\\sim\\mathbb{P}_{a}$ , $a\\in$ ${\\mathcal{A}}\\}$ with $C_{X}$ defined as in (2) be the set of covariance operators, use the metric $(x,y)\\mapsto\\|x-y\\|_{\\mathcal{H}_{k}}$ for $x,y\\in\\mathcal{H}_{k}$ , and keep the remaining part of the setup the same. Hence, it remains to lower bound $\\left\\|C_{X_{\\theta_{1}}}-C_{X_{\\theta_{0}}}\\right\\|_{\\mathcal{H}_{k}}$ . By using that HSIC is the RKHS norm of the cross-covariance operator, we obtain that ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\left\\|{C_{X_{\\theta_{1}}}-C_{X_{\\theta_{0}}}}\\right\\|_{\\mathcal{H}_{k}}\\stackrel{(i)}{\\geq}\\Big|\\underbrace{\\left\\|{C_{X_{\\theta_{1}}}}\\right\\|_{\\mathcal{H}_{k}}}_{=\\mathrm{HSIC}_{k}\\left(\\mathbb{P}_{\\theta_{1}}\\right)}-\\underbrace{\\left\\|{C_{X_{\\theta_{0}}}}\\right\\|_{\\mathcal{H}_{k}}}_{=\\mathrm{HSIC}_{k}\\left(\\mathbb{P}_{\\theta_{0}}\\right)}\\Big|=\\left|F(\\theta_{1})-F(\\theta_{0})\\right|\\stackrel{(i i)}{\\geq}2s=\\frac{2c}{\\sqrt{n}},\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where $(i)$ holds by the reverse triangle inequality, $F$ is defined as in Section 4.2, and $(i i)$ is guaranteed by (11) for $c>0$ . We conclude as in the proof of Theorem 1(ii) to obtain the stated result. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by the German Research Foundation (DFG) Research Training Group GRK 2153: Energy Status Data \u2014 Informatics Methods for its Collection, Analysis and Exploitation, and by the pilot program Core-Informatics of the Helmholtz Association (HGF). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "M\u00e9lisande Albert, B\u00e9atrice Laurent, Amandine Marrel, and Anouar Meynaoui. Adaptive test of independence based on HSIC measures. The Annals of Statistics, 50(2):858\u2013879, 2022. ", "page_idx": 9}, {"type": "text", "text": "Andr\u00e9s Anaya-Isaza and Leonel Mera-Jim\u00e9nez. Data augmentation and transfer learning for brain tumor detection in magnetic resonance imaging. IEEE Access, 10:23217\u201323233, 2022. ", "page_idx": 9}, {"type": "text", "text": "Nachman Aronszajn. Theory of reproducing kernels. Transactions of the American Mathematical Society, 68:337\u2013404, 1950. ", "page_idx": 9}, {"type": "text", "text": "Alain Berlinet and Christine Thomas-Agnan. Reproducing Kernel Hilbert Spaces in Probability and Statistics. Kluwer, 2004. ", "page_idx": 9}, {"type": "text", "text": "Martin Bilodeau and Aur\u00e9lien Guetsop Nangue. Tests of mutual or serial independence of random vectors with applications. Journal of Machine Learning Research, 18:1\u201340, 2017. ", "page_idx": 9}, {"type": "text", "text": "Dimitri Bouche, R\u00e9mi Flamary, Florence d\u2019Alch\u00e9 Buc, Riwal Plougonven, Marianne Clausel, Jordi Badosa, and Philippe Drobinski. Wind power predictions from nowcasts to 4-hour forecasts: a learning approach with variable selection. Renewable Energy, 211:938\u2013947, 2023. ", "page_idx": 9}, {"type": "text", "text": "Gustavo Camps-Valls, Joris M. Mooij, and Bernhard Sch\u00f6lkopf. Remote sensing feature selection by kernel dependence measures. IEEE Geoscience and Remote Sensing Letters, 7(3):587\u2013591, 2010. ", "page_idx": 9}, {"type": "text", "text": "Claudio Carmeli, Ernesto De Vito, Alessandro Toigo, and Veronica Umanit\u00e1. Vector valued reproducing kernel Hilbert spaces and universality. Analysis and Applications, 8:19\u201361, 2010. ", "page_idx": 9}, {"type": "text", "text": "Shubhadeep Chakraborty and Xianyang Zhang. Distance metrics for measuring joint dependence with application to causal inference. Journal of the American Statistical Association, 114(528): 1638\u20131650, 2019. ", "page_idx": 9}, {"type": "text", "text": "H\u00e9ctor Climente-Gonz\u00e1lez, Chlo\u00e9-Agathe Azencott, Samuel Kaski, and Makoto Yamada. Block HSIC Lasso: model-free biomarker detection for ultra-high dimensional data. Bioinformatics, 35 (14):i427\u2013i435, 2019. ", "page_idx": 9}, {"type": "text", "text": "Donald L. Cohn. Measure Theory. Birkh\u00e4user/Springer, second edition, 2013. ", "page_idx": 9}, {"type": "text", "text": "Joseph Diestel and John Jerry Uhl. Vector Measures. American Mathematical Society. Providence, 1977. ", "page_idx": 9}, {"type": "text", "text": "John Duchi. Derivations for linear algebra and optimization. Berkeley, California, 3(1):2325\u20135870, 2007.   \nAlex Fedorov, Eloy Geenjaar, Lei Wu, Tristan Sylvain, Thomas P DeRamus, Margaux Luck, Maria Misiura, Girish Mittapalle, R Devon Hjelm, Sergey M Plis, et al. Self-supervised multimodal learning for group inferences from MRI data: Discovering disorder-relevant brain regions and multimodal links. NeuroImage, 285:120485, 2024.   \nNo\u00e9 Fellmann, Christophette Blanchet-Scalliet, C\u00e9line Helbert, Adrien Spagnol, and Delphine Sinoquet. Kernel-based sensitivity analysis for (excursion) sets. Technometrics, 2024.   \nMichael Freitas Gustavo, Matti Hellstr\u00f6m, and Toon Verstraelen. Sensitivity analysis for ReaxFF reparametrization using the Hilbert\u2013Schmidt independence criterion. Journal of Chemical Theory and Computation, 19(9):2557\u20132573, 2023.   \nKenji Fukumizu, Arthur Gretton, Xiaohai Sun, and Bernhard Sch\u00f6lkopf. Kernel measures of conditional dependence. In Advances in Neural Information Processing Systems (NIPS), pages 498\u2013496, 2008.   \nTomasz G\u00f3recki, Miroslaw Krzy\u00b4sko, and Waldemar Woly\u00b4nski. Independence test and canonical correlation analysis based on the alignment between kernel matrices for multivariate functional data. Artificial Intelligence Review, pages 1\u201325, 2018.   \nArthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard Sch\u00f6lkopf. Measuring statistical dependence with Hilbert-Schmidt norms. In Algorithmic Learning Theory (ALT), pages 63\u201378, 2005.   \nArthur Gretton, Kenji Fukumizu, Choon Hui Teo, Le Song, Bernhard Sch\u00f6lkopf, and Alexander Smola. A kernel statistical test of independence. In Advances in Neural Information Processing Systems (NIPS), pages 585\u2013592, 2008.   \nArthur Gretton, Karsten Borgwardt, Malte Rasch, Bernhard Sch\u00f6lkopf, and Alexander Smola. A kernel two-sample test. Journal of Machine Learning Research, 13(25):723\u2013773, 2012.   \nSalvador Herrando-P\u00e9rez and Fr\u00e9d\u00e9rik Saltr\u00e9. Estimating extinction time using radiocarbon dates. Quaternary Geochronology, 79:101489, 2024.   \nFlorian Kalinke and Zolt\u00e1n Szab\u00f3. Nystr\u00f6m M-Hilbert-Schmidt independence criterion. In Conference on Uncertainty in Artificial Intelligence (UAI), pages 1005\u20131015, 2023.   \nLucien Le Cam. Convergence of estimates under dimensionality restrictions. The Annals of Statistics, 1:38\u201353, 1973.   \nRussell Lyons. Distance covariance in metric spaces. The Annals of Probability, 41:3284\u20133305, 2013.   \nCharles Micchelli, Yuesheng Xu, and Haizhang Zhang. Universal kernels. Journal of Machine Learning Research, 7:2651\u20132667, 2006.   \nJoris Mooij, Jonas Peters, Dominik Janzing, Jakob Zscheischler, and Bernhard Sch\u00f6lkopf. Distinguishing cause from effect using observational data: Methods and benchmarks. Journal of Machine Learning Research, 17:1\u2013102, 2016.   \nKrikamol Muandet, Kenji Fukumizu, Francesco Dinuzzo, and Bernhard Sch\u00f6lkopf. Learning from distributions via support measure machines. In Advances in Neural Information Processing Systems (NIPS), pages 10\u201318, 2011.   \nAlfred M\u00fcller. Integral probability metrics and their generating classes of functions. Advances in Applied Probability, 29:429\u2013443, 1997.   \nNiklas Pfister, Peter B\u00fchlmann, Bernhard Sch\u00f6lkopf, and Jonas Peters. Kernel-based tests for joint independence. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 80(1): 5\u201331, 2018.   \nAleksandr Podkopaev, Patrick Bl\u00f6baum, Shiva Kasiviswanathan, and Aaditya Ramdas. Sequential kernelized independence testing. In International Conference on Machine Learning (ICML), pages 27957\u201327993, 2023.   \nNovi Quadrianto, Le Song, and Alex Smola. Kernelized sorting. In Advances in Neural Information Processing Systems (NIPS), pages 1289\u20131296, 2009.   \nSaburou Saitoh and Yoshihiro Sawano. Theory of Reproducing Kernels and Applications. Springer Singapore, 2016.   \nBernhard Sch\u00f6lkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, and Yoshua Bengio. Toward causal representation learning. Proceedings of the IEEE, 109(5):612\u2013634, 2021.   \nDino Sejdinovic, Arthur Gretton, and Wicher Bergsma. A kernel test for three-variable interactions. In Advances in Neural Information Processing Systems (NIPS), pages 1124\u20131132, 2013a.   \nDino Sejdinovic, Bharath Sriperumbudur, Arthur Gretton, and Kenji Fukumizu. Equivalence of distance-based and RKHS-based statistics in hypothesis testing. Annals of Statistics, 41:2263\u20132291, 2013b.   \nShubhanshu Shekhar, Ilmun Kim, and Aaditya Ramdas. A permutation-free kernel independence test. Journal of Machine Learning Research, 24(369):1\u201368, 2023.   \nTianhong Sheng and Bharath K. Sriperumbudur. On distance and kernel measures of conditional independence. Journal of Machine Learning Research, 24(7):1\u201316, 2023.   \nAlexander Smola, Arthur Gretton, Le Song, and Bernhard Sch\u00f6lkopf. A Hilbert space embedding for distributions. In Algorithmic Learning Theory (ALT), pages 13\u201331, 2007.   \nLe Song, Alexander J. Smola, Arthur Gretton, and Karsten M. Borgwardt. A dependence maximization view of clustering. In International Conference on Machine Learning (ICML), pages 815\u2013822, 2007.   \nLe Song, Alex Smola, Arthur Gretton, Justin Bedo, and Karsten Borgwardt. Feature selection via dependence maximization. Journal of Machine Learning Research, 13(1):1393\u20131434, 2012.   \nBharath Sriperumbudur, Arthur Gretton, Kenji Fukumizu, Bernhard Sch\u00f6lkopf, and Gert Lanckriet. Hilbert space embeddings and metrics on probability measures. Journal of Machine Learning Research, 11:1517\u20131561, 2010.   \nBharath Sriperumbudur, Kenji Fukumizu, and Gert Lanckriet. Universality, characteristic kernels and RKHS embedding of measures. Journal of Machine Learning Research, 12:2389\u20132410, 2011.   \nIngo Steinwart. On the influence of the kernel on the consistency of support vector machines. Journal of Machine Learning Research, 6(3):67\u201393, 2001.   \nIngo Steinwart and Andreas Christmann. Support Vector Machines. Springer, 2008.   \nJerome Stenger, Fabrice Gamboa, Merlin Keller, and Bertrand Iooss. Optimal uncertainty quantification of a risk measurement from a thermal-hydraulic code using canonical moments. International Journal for Uncertainty Quantification, 10(1), 2020.   \nZolt\u00e1n Szab\u00f3 and Bharath K. Sriperumbudur. Characteristic and universal tensor product kernels. Journal of Machine Learning Research, 18(233):1\u201329, 2018.   \nG\u00e1bor J. Sz\u00e9kely and Maria L. Rizzo. Brownian distance covariance. The Annals of Applied Statistics, 3:1236\u20131265, 2009.   \nG\u00e1bor J. Sz\u00e9kely, Maria L. Rizzo, and Nail K. Bakirov. Measuring and testing dependence by correlation of distances. The Annals of Statistics, 35:2769\u20132794, 2007.   \nIlya Tolstikhin, Bharath Sriperumbudur, and Bernhard Sch\u00f6lkopf. Minimax estimation of maximal mean discrepancy with radial kernels. In Advances in Neural Information Processing Systems (NIPS), pages 1930\u20131938, 2016.   \nIlya Tolstikhin, Bharath Sriperumbudur, and Krikamol Muandet. Minimax estimation of kernel mean embeddings. Journal of Machine Learning Research, 18:1\u201347, 2017.   \nAlexandre B. Tsybakov. Introduction to Nonparametric Estimation. Springer, 2009.   \nSebastien De Veiga. Global sensitivity analysis with dependence measures. Journal of Statistical Computation and Simulation, 85(7):1283\u20131305, 2015.   \nAndi Wang, Juan Du, Xi Zhang, and Jianjun Shi. Ranking features to promote diversity: An approach based on sparse distance correlation. Technometrics, 64(3):384\u2013395, 2022.   \nLeila Wehbe and Aaditya Ramdas. Nonparametric independence testing for small sample sizes. In International Joint Conference on Artificial Intelligence (IJCAI), pages 3777\u20133783, 2015.   \nHolger Wendland. Scattered data approximation. Cambridge University Press, 2005.   \nMakoto Yamada, Wittawat Jitkrittum, Leonid Sigal, Eric P. Xing, and Masashi Sugiyama. Highdimensional feature selection by feature-wise kernelized Lasso. Neural Computation, 26(1): 185\u2013207, 2014.   \nYang Zhou, Di-Rong Chen, and Wei Huang. A class of optimal estimators for the covariance operator in reproducing kernel Hilbert spaces. Journal of Multivariate Analysis, 169:166\u2013178, 2019.   \nV. Zolotarev. Probability metrics. Theory of Probability and its Applications, 28:278\u2013302, 1983. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "A Auxiliary Result ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this section, we collect an auxiliary result. Lemma A.1 presents an upper bound on the KullbackLeibler divergence between multivariate normal distributions. ", "page_idx": 13}, {"type": "text", "text": "Lemma A.1 (Upper bound on KL divergence). Let $\\begin{array}{r}{d=\\sum_{m=1}^{M}d_{m}}\\end{array}$ , with $d_{m}\\in\\mathbb{N}_{>0}$ $m\\in[M])$ . Fix $i\\in[d_{1}]$ . Let $j=\\,i+1_{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!}$ , $\\mathbb{P}_{\\theta_{0}}=\\mathcal{N}(\\mathbf{0}_{d},\\mathbf{I}_{d}),$ , and $\\mathbb{P}_{\\theta_{1}}=\\mathcal{N}(\\mu_{1},\\Sigma_{1})$ , with $\\begin{array}{r}{\\pmb{\\mu}_{1}=\\frac{1}{\\sqrt{d}n}\\mathbf{1}_{d}\\in\\mathbb{R}^{d}}\\end{array}$ , and $\\pmb{\\Sigma}_{1}=\\pmb{\\Sigma}(i,j,\\rho_{n})\\in\\mathbb{R}^{d\\times d}$ defined as in (5) $(\\rho_{n}\\in(0,1),$ ). Then, for $2\\leq n\\in\\mathbb{N}$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathrm{KL}(\\mathbb{P}_{\\theta_{1}}^{n}||\\mathbb{P}_{\\theta_{0}}^{n})\\leq\\frac{1}{2n}+\\frac{n}{2}\\frac{\\rho_{n}^{2}}{1-\\rho_{n}^{2}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In particular, for $\\rho_{n}^{2}=1/n$ , it holds that $\\begin{array}{r}{\\mathrm{KL}(\\mathbb{P}_{\\theta_{1}}^{n}||\\mathbb{P}_{\\theta_{0}}^{n})\\leq\\frac{5}{4}}\\end{array}$ . ", "page_idx": 13}, {"type": "text", "text": "Proof. With $\\pmb{\\mu}_{0}=\\mathbf{0}_{d}$ and $\\Sigma_{0}=\\mathbf{I}_{d}$ , we obtain that ", "page_idx": 13}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{\\operatorname{KL}(\\mathbb{P}_{\\theta_{1}}^{n}|\\|\\mathbb{P}_{\\theta_{0}}^{n})\\leq{\\underset{i\\in[n]}{\\longrightarrow}}\\operatorname{KL}(\\mathbb{P}_{\\theta_{1}}|\\|\\mathbb{P}_{\\theta_{0}})}\\\\ &{\\qquad\\qquad\\leq{\\frac{n}{2}}\\left[\\operatorname{tr}(\\sum_{0}^{-1}\\!\\mathbf{\\Sigma}_{1})+(\\mu_{0}-\\mu_{1})^{\\mathsf{T}}\\mathbf{\\Sigma}_{0}^{-1}(\\mu_{0}-\\mu_{1})-d+\\ln\\left({\\frac{\\left|\\mathbf{\\Sigma}_{0}\\right|}{\\left|\\mathbf{\\Sigma}_{1}\\right|}}\\right)\\right]}\\\\ &{\\qquad={\\frac{n}{2}}\\left[\\underbrace{\\operatorname{tr}(\\sum_{1})}_{=d}+\\underbrace{\\|\\mu_{1}\\|_{\\mathbb{P}_{\\theta}}^{2}}_{={\\frac{1}{n^{2}}}}-d+\\ln\\left({\\begin{array}{c}{\\underbrace{1}_{\\left|\\mathbf{\\Sigma}_{1}\\right|}}\\\\ {\\underbrace{\\left|\\mathbf{\\Sigma}_{2}\\right|}_{=\\,1-\\rho_{n}^{2}}}\\end{array}}\\right)\\right]}\\\\ &{\\qquad={\\frac{1}{2n}}+{\\frac{n}{2}}\\ln\\left({\\frac{1}{1-\\rho_{n}^{2}}}\\right)\\leq{\\frac{1}{2n}}+{\\frac{n}{2}}{\\frac{\\rho_{n}^{2}}{1-\\rho_{n}^{2}}}\\leq{\\frac{5}{4}},}\\end{array}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where (a) is implied by Lemma B.1, (b) follows from Lemma B.2, (c) follows from the definition of the determinant, (d) is the consequence of the inequality $\\ln(x)\\leq x-1$ holding for $x>0$ , and (e) holds for $n\\geq2$ and $\\rho_{n}^{2}=1/n$ as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{n}{2}\\,\\underbrace{\\frac{1/n}{1-1/n}}_{\\frac{1}{n-1}}\\leq1\\iff\\frac{n}{2}\\frac{1}{n-1}\\leq1\\iff n\\leq2(n-1)\\iff n\\geq2,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and in this case (for $n\\geq2$ ) one has that $\\textstyle{\\frac{1}{2n}}\\leq{\\frac{1}{4}}$ ", "page_idx": 13}, {"type": "text", "text": "B External Theorems ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "For self-completeness, we include the external statements that we use. The well-known result by Bochner, stated in Theorem B.1, completely characterizes continuous bounded translation-invariant kernels. Theorem B.2 allows expressing MMD with continuous bounded translation-invariant kernels in terms of characteristic functions, and Theorem B.3 gives an equivalent condition for a continuous bounded translation-invariant kernel to be characteristic. Theorem B.4 connects characteristic kernels to characteristic product kernels and to $\\mathcal{T}$ -characteristic product kernels on $\\mathbb{R}^{d}$ (we include only the part relevant to our paper for brevity). We recall Le Cam\u2019s method in Theorem B.5 and collect results on the Kullback-Leibler divergence in Lemma B.1 and Lemma B.2. ", "page_idx": 13}, {"type": "text", "text": "Theorem B.1 (Bochner; Theorem 6.6; Wendland [2005]). A continuous function $\\kappa:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ is positive definite if and only if it is the Fourier transform of a finite nonnegative Borel measure $\\Lambda$ on $\\bar{\\mathbb{R}}^{d}$ , that is, ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\kappa(\\mathbf{x})=\\int_{\\mathbb{R}^{d}}e^{-i\\langle\\mathbf{x},\\omega\\rangle}\\,\\mathrm{d}\\Lambda(\\omega),\\quad{f o r}\\,a l l\\,\\mathbf{x}\\in\\mathbb{R}^{d}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Theorem B.2 (Corollary 4(i); Sriperumbudur et al. [2010]). Let $k:\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\to\\mathbb{R}$ be a continuous bounded translation-invariant kernel. Then, for any $\\mathbb{P},\\mathbb{Q}\\in\\mathcal{M}_{1}^{+}\\left(\\mathbb{R}^{d}\\right)$ , ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{MMD}_{k}^{2}(\\mathbb{P},\\mathbb{Q})=\\|\\psi_{\\mathbb{P}}-\\psi_{\\mathbb{Q}}\\|_{L^{2}(\\mathbb{R}^{d},\\Lambda_{k})}^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "with $\\psi_{\\mathbb{P}}$ and $\\psi_{\\mathbb{Q}}$ being the characteristic functions of $\\mathbb{P}$ and $\\mathbb{Q},$ , respectively, and $\\Lambda_{k}$ defined in (1). ", "page_idx": 13}, {"type": "text", "text": "Theorem B.3 (Theorem 9; Sriperumbudur et al. [2010]). Suppose $k:\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\to\\mathbb{R}$ is a continuous bounded translation-invariant kernel. Then $k$ is characteristic if and only $i f\\mathrm{supp}(\\Lambda_{k})=\\mathbb{R}^{d}$ , with $\\Lambda_{k}$ defined as in (1). ", "page_idx": 14}, {"type": "text", "text": "Theorem B.4 (Theorem 4; Szab\u00f3 and Sriperumbudur [2018]). Suppose $k_{m}:\\mathbb{R}^{d_{m}}\\times\\mathbb{R}^{d_{m}}\\rightarrow\\mathbb{R}$ is continuous bounded and translation-invariant kernel for all $m\\in[M]$ . Then the following statements are equivalent:   \n(i) $(k_{m})_{m=1}^{M}{=}$ are characteristic;   \n(ii) $\\otimes_{m=1}^{M}k_{m}$ is characteristic;   \n(iii) $\\otimes_{m=1}^{M}k_{m}$ is $\\mathcal{T}$ -characteristic. ", "page_idx": 14}, {"type": "text", "text": "The next statement follows directly from Tsybakov [2009, Eq. (2.9)] and Tsybakov [2009, Theorem 2.2]. ", "page_idx": 14}, {"type": "text", "text": "Theorem B.5 (Theorem 2.2; Tsybakov [2009]). Let $\\mathcal{X}$ be a measurable space, $(\\Theta,d)$ is a semi-metric space, and $\\mathcal{P}_{\\Theta}=\\{\\mathbb{P}_{\\theta}:\\theta\\in\\Theta\\}$ is a class of probability measures on $\\mathcal{X}$ indexed by $\\Theta$ . We observe data $D\\sim\\mathbb{P}_{\\theta}\\in\\mathcal{P}_{\\Theta}$ with some unknown parameter $\\theta$ . The goal is to estimate $\\theta$ . Let ${\\widehat{\\theta}}={\\widehat{\\theta}}(D)$ be an estimator of $\\theta$ based on $D$ . Assume that there exist $\\theta_{0},\\theta_{1}\\in\\Theta$ such that $d(\\theta_{0},\\theta_{1})\\geq2s>0$ and $\\mathrm{KL}(\\mathbb{P}_{\\theta_{1}}||\\mathbb{P}_{\\theta_{0}})\\leq\\alpha<\\infty$ for $\\alpha>0$ . Then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{\\hat{\\theta}}\\operatorname*{sup}_{\\theta\\in\\Theta}\\mathbb{P}_{\\theta}\\left(d\\left({\\hat{\\theta}},\\theta\\right)\\geq s\\right)\\geq\\operatorname*{max}\\left({\\frac{e^{-\\alpha}}{4}},{\\frac{1-{\\sqrt{\\alpha/2}}}{2}}\\right).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We have the following property of the Kullback-Leibler divergence for product measures [Tsybakov, 2009, p. 85]. ", "page_idx": 14}, {"type": "text", "text": "Lemma B.1 (KL divergence of product measures). Let $\\mathbb{P}=\\otimes_{i=1}^{n}\\mathbb{P}_{i}$ and $\\mathbb{Q}=\\otimes_{i=1}^{n}\\mathbb{Q}_{i}$ . Then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{KL}(\\mathbb{P}||\\mathbb{Q})=\\sum_{i\\in[n]}\\mathrm{KL}(\\mathbb{P}_{i}||\\mathbb{Q}_{i}).\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The following lemma [Duchi, 2007, p. 13] shows that the Kullback-Leibler divergence of multivariate Gaussians can be computed in closed form. ", "page_idx": 14}, {"type": "text", "text": "Lemma B.2 (KL divergence of Gaussians). The KL divergence of two normal distributions $\\mathcal{N}(\\pmb{\\mu}_{1},\\pmb{\\Sigma}_{1})$ and $\\mathcal{N}(\\mu_{0},\\bar{\\Sigma}_{0})$ on $\\mathbb{R}^{d}$ is ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathrm{KL}(\\mathcal{N}(\\mu_{1},\\Sigma_{1})||\\mathcal{N}(\\mu_{0},\\Sigma_{0}))=\\frac{\\mathrm{tr}(\\Sigma_{0}^{-1}\\Sigma_{1})+(\\mu_{0}-\\mu_{1})^{\\top}\\Sigma_{0}^{-1}(\\mu_{0}-\\mu_{1})-d+\\ln\\left(\\frac{|\\Sigma_{0}|}{|\\Sigma_{1}|}\\right)}{2}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: The abstract and introduction motivate and summarize the theoretical results that we establish in the article. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 15}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Justification: We make the assumptions explicit in all of our results. Remark 1(a) further elaborates the characteristic assumption. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 15}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We provide explicit proofs to all of our claims. External results that we use are referenced and additionally included as part of the supplement to ensure self-completeness. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 16}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 16}, {"type": "text", "text": "Justification: The paper does not contain experiments. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 16}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 17}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] Justification: The paper does not include experiments requiring code. ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 17}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.   \n\u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 17}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 17}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 17}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 17}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 18}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: The paper does not include experiments. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 18}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The research presented in this work conforms with the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 18}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: This work presents basic research without any tangible societal impact. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 19}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 19}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 19}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. ", "page_idx": 19}, {"type": "text", "text": "\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 19}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks. ", "page_idx": 19}, {"type": "text", "text": "\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. \u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 19}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper does not use existing assets. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 20}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 20}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 20}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 20}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 20}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 21}, {"type": "text", "text": "Guidelines:   \n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 21}]