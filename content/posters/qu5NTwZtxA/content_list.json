[{"type": "text", "text": "Towards Editing Time Series ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Baoyu $\\mathbf{j}\\mathbf{ing}^{*2}$ , Shuqi $\\mathbf{Gu^{*\\,1}}$ , Tianyu Chen1, Zhiyu Yang1, Dongsheng $\\mathbf{Li}^{3}$ , Jingrui $\\mathbf{H}\\mathbf{e}^{2}$ , Kan Ren\u2020 1 1ShanghaiTech University, 2University of Illinois at Urbana-Champaign, 3Microsoft Research ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Synthesizing time series data is pivotal in modern society, aiding effective decisionmaking and ensuring privacy preservation in various scenarios. Time series are associated with various attributes, including trends, seasonality, and external information such as location. Recent research has predominantly focused on random unconditional synthesis or conditional synthesis. Nonetheless, these paradigms generate time series from scratch and are incapable of manipulating existing time series samples. This paper introduces a novel task, called Time Series Editing (TSE), to synthesize time series by manipulating existing time series. The objective is to modify the given time series according to the specified attributes while preserving other properties unchanged. This task is not trivial due to the inadequacy of data coverage and the intricate relationships between time series and their attributes. To address these issues, we introduce a novel diffusion model, called TEdit. The proposed TEdit is trained using a novel bootstrap learning algorithm that effectively enhances the coverage of the original data. It is also equipped with an innovative multi-resolution modeling and generation paradigm to capture the complex relationships between time series and their attributes. Experimental results demonstrate the efficacy of TEdit for editing specified attributes upon the existing time series data. The project page is at https://seqml.github.io/tse. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Time series data analysis plays a crucial role in various modern business sectors, including climate monitoring [22, 13, 14], healthcare [12, 3, 46], urban management [43, 16, 15], and online service [35]. Time series are derived from diverse sources with inherent characteristics, such as system configurations, alongside external influential factors like environmental status. These factors are considered attributes of the time series. In many real-world scenarios, such as healthcare and urban monitoring, time series data tend to be sparse and privacy-sensitive, particularly concerning specific attributes that are rarely observed in real-world scenarios. ", "page_idx": 0}, {"type": "text", "text": "Synthesizing time series data has emerged as a prominent research area aimed at addressing these challenges. Existing methods for time series synthesis range from unconditional generation [47, 31] to conditional generation [39, 27, 4]. Unconditional generation produces outputs based solely on the underlying distribution of the data. Without relying on any input conditions, the generated results are highly uncontrollable. Conditional generation controls the outcomes of the generated data based on the input conditions. However, conditional generation tends to produce samples around the data mean [40], as demonstrated in our experiments, which could easily overlook detailed characteristics of the data, such as high-frequency patterns and local structures. Both unconditional and conditional generation paradigms synthesize time series from scratch, and they are incapable of manipulating existing samples. As a result, these paradigms are unable to answer \u201cwhat if\u201d questions in time series synthesis: given a time series, what would it become if some of its attributes are modified? ", "page_idx": 0}, {"type": "image", "img_path": "qu5NTwZtxA/tmp/2f24f89645a5cd1d109025e035e4406d4b8060cb04a3c8cf12a6886d13dd2ae7.jpg", "img_caption": [], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Illustration of the time series generation paradigms. Conditional generation generates time series from scratch, which usually generates samples around the dataset mean. Time series editing allows for the manipulation of the attributes of an input time series sample, which aligns with the desired target attribute values while preserving other properties. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we introduce a novel task, called Time Series Editing (TSE), for sample-level time series manipulation. More specifically, our objective is to directly manipulate specific attributes of an input time series to target values while maintaining consistency in other characteristics. For instance, consider observing air quality under season conditions in different cities as shown in Fig. 1. On the left side, two example air quality time series for the spring and summer of London are illustrated. The spring one exhibits a lower amplitude with more noise, whereas the summer one features a higher amplitude with less noise. What would the resulting time series look like if we modify the season attribute of the spring one to summer while maintaining all other properties? As illustrated in Fig. 1, it could have a larger amplitude with much noise. ", "page_idx": 1}, {"type": "text", "text": "The task of TSE is complex due to several challenges. Firstly, the time series data distribution over the full composited attribute space is biased and may not be adequately covered, leaving gaps in our understanding, especially concerning unobservable or poorly defined attributes. For example, in climate data analysis, attributes like temperature and humidity are observable and well-defined. However, attributes like atmospheric pressure variations or localized microclimates may be challenging to observe or define accurately. Secondly, different attributes influence time series at varying resolutions. For example, trends have a global impact, while seasonality exerts a more localized influence. Modeling these multi-scale attributes and time series associations, while effectively controlling them, presents significant difficulties. ", "page_idx": 1}, {"type": "text", "text": "To address these challenges, we introduce a novel method called Time Series Editor (TEdit), which is based on predominant generative models, specifically diffusion models [10, 38, 39, 27]. To address the data coverage issue, we propose a novel bootstrap learning algorithm, which leverages the generated data as pseudo-supervision for subsequent model learning. This algorithm helps improve the coverage of the whole attribute space and enhance the generation performance. To capture the intricate multi-scale associations between time series and attributes, we introduce a multi-resolution modeling and generation paradigm. The proposed multi-resolution paradigm can manipulate the given time series and attributes both effectively and efficiently. Our experiments, conducted on both synthetic and real-world datasets, demonstrate the effectiveness of our proposed solution. Specifically, our approach excels in generating precise time series under specified attributes, while keeping consistency in other attributes, showcasing the practical utility of our method. ", "page_idx": 1}, {"type": "text", "text": "In summary, our main contributions are threefold. (1) We introduce and formulate the novel task of Time Series Editing (TSE). To the best of our knowledge, this is the first work exploring editing time series according to the given attribute configuration upon the input data. (2) We propose a novel multi-resolution diffusion model, called TEdit, with a bootstrap learning algorithm, which can flexibly capture the patterns at diverse granularity and gradually improve the generation performance. (3) We conduct and release a benchmark for TSE, including synthetic data and several carefully curated real-world datasets with a comprehensive evaluation protocol, aiming to facilitate further research in the community. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Time Series Generation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "A time series sample $\\mathbf{x}\\in\\mathbb{R}^{L}$ is a series of $L$ chronologically ordered sample points. Each time series sample adheres to some attributes $\\mathbf{a}\\in\\mathbb{N}_{+}^{K}$ , such as trend and the cycle number, and the $k$ -th item $a_{k}$ corresponds to $N_{k}$ categorical options where $k\\in\\{1,...,K\\}$ . In real-world scenarios, each time series may have several attributes, e.g., the air quality time series is associated with $K=2$ attributes: location and season. Note that, not all the attributes can be observed due to practical limitations. ", "page_idx": 2}, {"type": "text", "text": "Before describing the main task of the paper, we first briefly review the background of Time Series Generation. Unconditional Time Series Generation (UTSG) samples the time series data $\\mathbf{x}$ upon the modeled data distribution $p(\\mathbf x)$ such that $\\mathbf{x}\\sim p(\\mathbf{x})$ , of which the generation process is uncontrollable. Conditional Time Series Generation (CTSG) generates the time series sample $\\mathbf{x}$ based on the conditional distribution such that $\\mathbf{x}\\sim p(\\mathbf{x}|\\mathbf{a})$ , where a is the input condition. CTSG tends to produce data samples near the dataset mean [40]. Both UTSG and CTSG paradigms generate time series from scratch and lack the ability to directly manipulate the existing samples. ", "page_idx": 2}, {"type": "text", "text": "2.2 Conditional Diffusion Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Diffusion models [10, 37, 38] learn to estimate and remove the random noise, which was added in the forward process onto the real-world data, through a sequence of sampling processes. Specifically, during training, random noise is gradua\u221ally added to the original data sample $\\mathbf{x}_{0}=\\mathbf{x}^{1}$ via a Gaussian Markov transition $q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1}):\\bar{=}\\mathcal{N}(\\sqrt{1-\\beta_{t}}\\mathbf{x}_{t-1},\\beta_{t}\\mathbf{I}).$ , where $t\\in[1,T]$ indicates the diffusion step, and $\\{\\beta_{t}\\}_{t=0}^{T}$ are the p\u221aredetermi\u221aned variance schedule. The expression of latent variable $\\mathbf{x}_{t}$ can be simplified as $\\mathbf{x}_{t}=\\sqrt[]{\\alpha_{t}}\\mathbf{x}_{0}+\\sqrt{1-\\alpha_{t}}\\epsilon,\\;\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I}),\\;\\alpha_{t}:=\\mathrm{\\hat{\\Pi}}_{s=1}^{t}(1-\\beta_{s})$ . The learnable component of the diffusion models is a noise estimation network $\\epsilon_{\\theta}(\\mathbf{x}_{t},t,\\mathbf{a})$ , where a denotes the additional condition such as attributes. $\\epsilon_{\\theta}(\\mathbf{x}_{t},t,\\mathbf{a})$ is trained by estimating the noise added to $\\mathbf{x}_{t}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\mathcal{L}(\\mathbf{x}_{0}):=\\operatorname*{min}_{\\theta}\\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I}),t\\sim\\mathcal{U}(1,T)}\\left\\|\\epsilon-\\epsilon_{\\theta}\\left(\\mathbf{x}_{t},t,\\mathbf{a}\\right)\\right\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathbf{x}_{0}\\sim q(\\mathbf{x}_{0})$ is sampled from the real data distribution, $\\mathbf{x}_{t}$ is a noisy version of $\\mathbf{x}_{\\mathrm{0}}$ . ", "page_idx": 2}, {"type": "text", "text": "After training, given an attribute a, we can generate a sample $\\hat{\\mathbf{x}}_{0}$ from random noise $\\hat{\\mathbf{x}}_{T}\\sim\\mathcal{N}(0,\\mathbf{I})$ using $\\epsilon_{\\theta}(\\mathbf{x}_{t},t,\\mathbf{a})$ with a sampler, e.g., deterministic Denoising Diffusion Implicit Model (DDIM) [38]: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathbf{x}}_{t-1}=\\sqrt{\\alpha_{t-1}}\\mathbf{f}_{\\theta}(\\hat{\\mathbf{x}}_{t},t,\\mathbf{a})+\\sqrt{1-\\alpha_{t-1}}\\epsilon_{\\theta}(\\hat{\\mathbf{x}}_{t},t,\\mathbf{a})}\\\\ {\\mathbf{f}_{\\theta}(\\hat{\\mathbf{x}}_{t},t,\\mathbf{a})=(\\hat{\\mathbf{x}}_{t}-\\sqrt{1-\\alpha_{t}}\\epsilon_{\\theta}\\left(\\hat{\\mathbf{x}}_{t},t,\\mathbf{a}\\right))/\\sqrt{\\alpha_{t}}\\,\\,\\,\\,\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where ${\\bf f}_{\\theta}(\\hat{{\\bf x}}_{t},t,{\\bf a})$ can be regarded as a prediction of $\\mathbf{x}_{\\mathrm{0}}$ at the diffusion step $t$ . ", "page_idx": 2}, {"type": "text", "text": "Adapting diffusion methods from the computer vision domain to the time series domain is not trivial. Time series is different from images, and it poses unique challenges for modeling and generating time series data, such as the complex multi-scale entanglement between time series and attributes. ", "page_idx": 2}, {"type": "text", "text": "3 Time Series Editing ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Given a time series and its associated attributes $(\\mathbf{x},\\mathbf{a})$ , a natural question arises: \u201cwhat would it become if some of its attributes are modified?\u201d In our work, we take a novel perspective of directly editing the given time series with the specified attribute modifications. ", "page_idx": 2}, {"type": "text", "text": "In this section, we first formally formulate the problem of Time Series Editing (TSE) in Sec. 3.1. Next, we present our method Time Series Editor (TEdit) with the overall procedure of the diffusion model-based TSE in Sec. 3.2, the model architecture in Sec. 3.3 and the learning algorithm in Sec. 3.4. ", "page_idx": 2}, {"type": "text", "text": "3.1 Problem Formulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Recall that, each time series sample $\\mathbf{x}\\in\\mathbb{R}^{L}$ is associated with a set of attributes $\\mathbf{a}\\in\\mathbb{N}_{+}^{K}$ . Each attribute $a_{k}$ has $N_{k}$ options, $k\\in\\{1,...,K\\}$ . For example, the trend type may contain $N_{k}=4$ values like linear, quadratic, exponential, and logistic. Let $(\\mathbf{x}^{\\mathrm{src}},\\mathbf{a}^{\\mathrm{src}})$ be a pair of source time series and source attributes to be edited, and $\\mathbf{a}^{\\mathrm{tgt}}$ be the desired target attributes. Comparing $\\mathbf{a}^{\\mathrm{tgt}}$ and $\\mathbf{a}^{\\mathrm{src}}$ , there are $K_{\\mathrm{edit}}$ edited attributes, e.g., trend types, and $K_{\\mathrm{prsv}}$ preserved attributes, e.g., cycle numbers, where $K_{\\mathrm{edit}}+K_{\\mathrm{prsv}}=K$ . Now, we can formally define the task as below. ", "page_idx": 2}, {"type": "image", "img_path": "qu5NTwZtxA/tmp/bf59a29310628405b0829184311e122b8b7dac90dbbf980b0eb43b7ecf7a1d84.jpg", "img_caption": ["Figure 2: Illustration of the editing function $\\Phi_{\\theta}$ (upper) and the process of self-scoring the generated $\\hat{\\mathbf{x}}_{0}^{\\mathrm{tgt}}$ for bootstrap learning (lower). Upper: $\\Phi_{\\theta}(\\mathbf{x}_{0}^{\\mathrm{src}},\\mathbf{a}^{\\mathrm{src}},\\mathbf{a}^{\\mathrm{tgt}})$ first encodes the source $(\\mathbf{x}_{0}^{\\mathrm{src}},\\mathbf{a}^{\\mathrm{src}})$ into the latent viable ${\\bf x}_{T}^{\\mathrm{src}}$ via the forward DDIM Eq.(4), and then decodes ${\\bf x}_{T}^{\\mathrm{src}}$ with the target attributes $\\mathbf{a}^{\\mathrm{tgt}}$ : $(\\mathbf{x}_{T}^{\\mathrm{src}},\\mathbf{a}^{\\mathrm{tgt}})$ into $\\hat{\\mathbf{x}}_{0}^{\\mathrm{tgt}}$ via the reverse DDIM in Eq.(5). See Sec. 3.2 for more details. Lower: during bootstrap learning, we use $\\Phi_{\\theta}$ to self-score the generated $\\hat{\\mathbf{x}}_{0}^{\\mathrm{tgt}}$ by editing $\\hat{\\mathbf{x}}_{0}^{\\mathrm{tgt}}$ back to $\\hat{\\mathbf{x}}_{0}^{\\mathrm{src}}$ , and obtain the score $s=\\mathrm{MSE}\\big(\\hat{\\mathbf{x}}_{0}^{\\mathrm{src}},\\mathbf{x}_{0}^{\\mathrm{src}}\\big)$ , see Sec. 3.4 for more details. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Definition 1 (Time Series Editing (TSE)) The time series editing task is to build a function $\\Phi_{\\theta}$ to generate a target time series $\\hat{\\mathbf{x}}^{t g t}=\\Phi_{\\theta}\\big(\\mathbf{x}^{s r c},\\mathbf{a}^{s r c},\\mathbf{a}^{t g t}\\big)$ by modifying the set of $K_{e d i t}$ edited attributes $A_{e d i t}$ and maintaining the set of $K_{p r s\\nu}$ preserved attributes $\\mathcal{A}_{p r s\\nu}$ as well as other information of $\\mathbf{x}^{s r c}$ . ", "page_idx": 3}, {"type": "text", "text": "3.2 Editing with Source Modeling and Target Generation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Given a pair of source time series and attributes $(\\mathbf{x}^{\\mathrm{{src}}},\\mathbf{a}^{\\mathrm{{src}}})$ , generating the target time series $\\hat{\\mathbf{x}}^{\\mathrm{tgt}}$ corresponding to the target attributes $\\mathbf{a}^{\\mathrm{tgt}}$ entails two requirements. On one hand, the edited attributes $A_{e d i t}$ should be satisfied after the generation. On the other hand, the preserved attributes $\\mathcal{A}_{p r s v}$ as well as other characteristics, e.g., noise, need to be maintained. Conditional generation [27], such as directly adopting the conditional diffusion model described in Sec. 2.2, would fail the editing task because it does not take into consideration the detail characteristics of the source time series. ", "page_idx": 3}, {"type": "text", "text": "In this paper, we propose a diffusion based two-stage procedure for TSE: $\\Phi_{\\theta}(\\mathbf{x}^{\\mathrm{src}},\\mathbf{a}^{\\mathrm{src}},\\mathbf{a}^{\\mathrm{tgt}})$ , which is illustrated in the upper part of Fig. 2. In the first stage, we encode both the attribute semantics [6] and the detail characteristics [10] of the source time series $\\mathbf{x}_{\\mathrm{0}}^{\\mathrm{src}}$ into the latent variable ${\\bf x}_{T}^{\\mathrm{src}}$ via the deterministic forward DDIM process [18]: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{x}_{t+1}^{\\mathrm{src}}=\\sqrt{\\alpha_{t+1}}\\mathbf{f}_{\\theta}(\\mathbf{x}_{t}^{\\mathrm{src}},t,\\mathbf{a}^{\\mathrm{src}})+\\sqrt{1-\\alpha_{t+1}}\\epsilon_{\\theta}(\\mathbf{x}_{t}^{\\mathrm{src}},t,\\mathbf{a}^{\\mathrm{src}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In the second stage, originating from the latent variable $\\hat{\\mathbf{x}}_{T}^{\\mathrm{tgt}}=\\mathbf{x}_{T}^{\\mathrm{src}}$ , we gradually generate the final target time series $\\hat{\\mathbf{x}}_{0}^{\\mathrm{tgt}}$ with the consideration of the target attribute $\\mathbf{a}^{\\mathrm{tgt}}$ via the deterministic reverse DDIM process [38]: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathbf{x}}_{t-1}^{\\mathrm{tgt}}=\\sqrt{\\alpha_{t-1}}\\mathbf{f}_{\\theta}(\\hat{\\mathbf{x}}_{t}^{\\mathrm{tgt}},t,\\mathbf{a}^{\\mathrm{tgt}})+\\sqrt{1-\\alpha_{t-1}}\\epsilon_{\\theta}(\\hat{\\mathbf{x}}_{t}^{\\mathrm{tgt}},t,\\mathbf{a}^{\\mathrm{tgt}}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{f}_{\\theta}$ is given in Eq. (3). Till now, we have detailed the procedure of the proposed TEdit. In the following subsections, we introduce the model architecture and the training algorithm. ", "page_idx": 3}, {"type": "text", "text": "3.3 Multi-Resolution Noise Estimator ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The key component of our TEdit is the noise estimator $\\epsilon_{\\theta}(\\mathbf{x},t,\\mathbf{a})$ in Eqs. (4)(5). Though many diffusion model realizations have been proposed in other fields [18, 10, 37, 38, 24] and the time series domain [39, 27], we found them inefficacious in modeling and generating time series data, especially ", "page_idx": 3}, {"type": "image", "img_path": "qu5NTwZtxA/tmp/23109bb162b9e17a09da794cfe7ffc432bdc79767ba4ac01531144a03a128a06.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 3: Architecture of the proposed multi-resolution noise estimator $\\epsilon_{\\theta}$ . We illustrate with $R=3$ patching schema, patch length Lrp = 2r\u22121, r \u2208{1, ..., R} and the input length L = 8. N pr = \u230aL\u2212LrpL rp is the patch number. $D$ is the embedding size. Please refer to Sec. 3.3 for details. ", "page_idx": 4}, {"type": "text", "text": "when considering attributes. Few of them consider that different attributes can influence the time series on varying scales. We argue that it is important to take into account these differences. For instance, the trend types have a global impact on time series, while the seasonality affects the time series in local areas. ", "page_idx": 4}, {"type": "text", "text": "In this paper, we propose a multi-resolution noise estimator. It first slices the input time series $\\mathbf{x}_{t}$ into several patch sequences of different resolutions $r\\in\\{1,...,R\\}$ , where $R$ is the total number of resolutions. Then it processes the patch sequences along with other information, e.g., attributes a and diffusion step $t$ , to estimate the noise $\\hat{\\boldsymbol{\\epsilon}}_{t}^{r}$ for resolution $r$ . Finally, the estimated noise for different resolutions are mixed together to obtain the final estimated noise $\\hat{\\epsilon}_{t}$ . An illustration is presented in Fig. 3. We elaborate on the details in the following content2. ", "page_idx": 4}, {"type": "text", "text": "Patchifying. Following [29], we patchify the input time series $\\mathbf{x}\\in\\mathbb{R}^{L}$ , into a sequence of patch tokens $\\mathbf{P}=\\{\\mathbf{p}_{1},...,\\mathbf{p}_{N_{p}}\\}$ , where $\\mathbf{p}_{i}\\in\\mathbb{R}^{L_{p}}$ , $L_{p}$ is the window size and $\\begin{array}{r}{N_{p}=\\left\\lfloor\\frac{L-\\bar{L}_{p}}{L_{p}}\\right\\rfloor}\\end{array}$ is the patch number. After that, we encode them into embeddings $\\bar{\\mathbf{P}}=\\{\\bar{\\mathbf{p}}_{1},...,\\bar{\\mathbf{p}}_{N_{p}}\\}$ , where $\\bar{\\mathbf{p}}_{i}\\in\\mathbb{R}^{D}$ . ", "page_idx": 4}, {"type": "text", "text": "Multi-Resolution Modeling and Generation. To model multi-resolution patterns and better control the conditional generation at multiple scales, we propose a multi-patch design with various patch lengths. Specifically, as shown in the left part of Fig. 3, following the above patchifying operation, we encode the input time series $\\mathbf{x}$ into patch embedding sequences $\\{\\bar{\\mathbf{P}}^{r}\\}_{r=1}^{R}$ of $R$ resolutions, where P\u00afr = {\u00afpr1, ..., \u00afprNpr }, \u00afpir \u2208RD, N pr = \u230aL\u2212LrpL p\u230b is the patch number, $L_{p}^{r}$ is the window size. We set the patch length as $L_{p}^{r}\\,=\\,b^{r-1}$ to produce exponential receptions, where $b\\in\\mathbb{N}_{+}$ is the base. Thereafter, the processing module operates self-attention as the Transformer [42] to capture the input patterns for each resolution $r$ , and it incorporates the attribute a and diffusion step $t$ information to produce the embeddings $\\{\\tilde{\\mathbf{P}}^{r}\\}_{r=1}^{R}$ . The details of the processing module are presented in Appendix D. The processed output $\\{\\tilde{\\mathbf{P}}^{r}\\}_{r=1}^{R}$ is still in the form of multi-resolution sequences. The model subsequently decodes the diffusion noise at different resolutions back into the original space and integrates the outcomes at different resolutions to produce the final noise estimation, as shown on the right side of Fig. 3. Specifically, for each $\\tilde{\\mathbf{P}}^{r}$ , the decoder transforms it to $\\hat{\\epsilon}^{r}\\in\\mathbb{R}^{L}$ in the original time series space. The final estimated noise is obtained via a mixing upon the concatenation of estimated noise of different resolutions as $\\hat{\\pmb{\\epsilon}}=\\mathbf{M}\\mathbf{L}\\mathbf{P}([\\pmb{\\epsilon}^{1},...,\\pmb{\\epsilon}^{R}])$ , where $\\bar{\\mathbf{M}}\\bar{\\mathbf{L}}\\mathbf{P}(\\cdot)$ denotes Multi-Layer Perceptron. ", "page_idx": 4}, {"type": "text", "text": "Parallel Processing. Though more effective, the proposed multi-resolution modeling and generation brings efficiency issues since it derives multiple token sequences with various sequence lengths. Iteratively processing each sequence is of low efficiency. Herein we design a novel parallelization with an attention masking mechanism in the processing module in Fig. 3 to harness the parallelism of the Graphics Processing Unit (GPU). Specifically, we first concatenate the input patch embedding sequences of different resolutions into a single vector $\\bar{\\mathbf{P}}=[\\bar{\\mathbf{P}}^{1},...,\\bar{\\mathbf{P}}^{R}]\\in\\mathbb{R}^{\\bar{D^{\\star}}(N_{p}^{\\bar{1}}+...+N_{p}^{R})}$ . When calculating pair-wise self-attention scores as in the Transformer [42], we use a mask matrix to mask out the inter-sequence attention operations across different sequences while preserving intra-sequence attention. Please refer to Appendix $\\mathrm{D}$ for more details. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "3.4 Bootstrap Learning Algorithm ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Although any pretrained conditional diffusion model $\\epsilon_{\\theta}$ can be directly used to perform editing as shown in Sec. 3.2, the pretrained model is an essentially conditional generator, which has a great ability in generating time series directly from attributes but it might be less effective in modifying the attributes of an existing time series. The simplest way to improve the model\u2019s ability is to finetune $\\epsilon_{\\theta}$ via the ground-truth source $(\\mathbf{x}_{0}^{\\mathrm{src}},\\mathbf{a}^{\\mathrm{src}})$ and target $(\\mathbf{x}_{0}^{\\mathrm{tgt}},\\mathbf{a}^{\\mathrm{tgt}})$ pairs. However, the imaginary target $\\mathbf{x}_{\\mathrm{0}}^{\\mathrm{tgt}}$ , which satisfies both $\\mathbf{a}^{\\mathrm{tgt}}$ and the details of $\\mathbf{x}_{\\mathrm{0}}^{\\mathrm{src}}$ might be very rare or even does not exist in the real world. Thus, a key challenge for finetuning is how to effectively learn the information of $\\mathbf{x}_{0}^{\\mathrm{tgt}}$ . ", "page_idx": 5}, {"type": "text", "text": "Fortunately, the pretrained model is capable of generating some edited samples of a certain quality via $\\hat{\\mathbf{x}}_{0}^{\\mathrm{tgt}}=\\Phi_{\\theta}\\big(\\mathbf{x}_{0}^{\\mathrm{src}},\\mathbf{a}^{\\mathrm{src}},\\mathbf{a}^{\\mathrm{tgt}}\\big)$ . An example is shown in Fig. 2. In this paper, we propose a bootstrap learning algorithm, which first pretrains $\\epsilon_{\\theta}$ based on the noise estimation loss $\\mathcal{L}$ in Eq. (1) and then finetunes $\\epsilon_{\\theta}$ based on $\\hat{\\mathbf{x}}_{0}^{\\mathrm{tgt}}$ with top confidence scores. Here, we briefly present the key steps of the bootstrap learning, and the full algorithm is given in Appendix E. Given a batch $(\\mathbf{X}_{0}^{\\mathrm{src}},\\mathbf{A}^{\\mathrm{src}})$ , where ${\\bf X}_{0}^{\\mathrm{src}}\\in\\mathbb{R}^{B\\times L}$ , $\\bar{\\mathbf{A}^{\\mathrm{src}}}\\in\\mathbb{N}_{+}^{B\\times K}$ , $B$ is the batch size, the finetuning works as follows: ", "page_idx": 5}, {"type": "text", "text": "\u2022 Compose target attributes $\\mathbf{A}^{\\mathbf{tgt}}$ . For each $(\\mathbf{x}_{0}^{\\mathrm{src}},\\mathbf{a}^{\\mathrm{src}})\\in(\\mathbf{X}_{0}^{\\mathrm{src}},\\mathbf{A}^{\\mathrm{src}})$ , we compose the imaginary target attributes $\\mathbf{a}^{\\mathrm{tgt}}$ based on $\\mathbf{a}^{\\mathrm{src}}$ by randomly sampling values for edited attributes $A_{\\mathrm{edit}}$ and keeping the values of the preserved attributes $\\mathcal{A}_{\\mathrm{prsv}}$ . Then we have the tuple $(\\mathbf{X}_{0}^{\\mathrm{src}},\\mathbf{A}^{\\mathrm{src}},\\mathbf{A}^{\\mathrm{tgt}})$ . \u2022 Generate the edited time series $\\hat{\\mathbf{X}}_{\\mathrm{0}}^{\\mathbf{tgt}}$ via $\\Phi_{\\theta}$ . An illustration is shown in the upper part of Fig. 2. \u2022 Self-score $\\hat{\\mathbf{X}}_{0}^{\\mathbf{tgt}}$ via $\\Phi_{\\theta}$ and keep top $\\psi$ samples. An illustration of the self-scoring process is shown in the lower part of Fig. 2. We first edit $\\hat{\\mathbf{X}}_{0}^{\\mathrm{tgt}}$ back to source $\\hat{\\mathbf{X}}_{0}^{\\mathrm{src}}=\\Phi_{\\theta}(\\hat{\\mathbf{X}}_{0}^{\\mathrm{tgt}},\\mathbf{A}^{\\mathrm{tgt}},\\mathbf{A}^{\\mathrm{src}})$ . sTahmenp lews eo fu ${\\bf X}_{0}^{\\mathrm{src}}$ $\\hat{\\mathbf{X}}_{0}^{\\mathrm{src}}$ $\\hat{\\mathbf{X}}_{0}^{\\mathrm{tgt}}$ . The t o.p $\\psi$ $\\hat{\\mathbf{X}}_{0}^{\\mathrm{tgt}}$ with the lowest MSE are selected as the bo0otstrap sa0mples, denoted0 by $\\hat{\\mathbf{X}}_{0,\\mathrm{bs}}^{\\mathrm{tgt}}$ \u2022 Update $\\epsilon_{\\theta}$ by minimizing $\\mathcal{L}_{\\mathrm{BS}}$ . We finetune $\\epsilon_{\\theta}$ by minimizing the noise estimation loss $\\mathcal{L}$ in Eq. (1) for $\\hat{\\mathbf{X}}_{0,\\mathrm{bs}}^{\\mathrm{tgt}}$ . Formally, we minimize $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{BS}}=\\mathcal{L}(\\hat{\\mathbf{X}}_{\\mathrm{0,bs}}^{\\mathrm{tgt}})}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we present experiments aimed at addressing the following research questions: RQ1: How does the proposed TEdit perform in terms of editing and preserving attributes? RQ2: What\u2019s the impact of bootstrap training? RQ3: What\u2019s the impact of multi-resolution modeling? ", "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets. We collect three datasets for TSE, including one synthetic dataset and two real-world datasets. For the Synthetic data, each time series sample has a length of 128, and is associated with 3 attributes, where there are 4 trend types, 2 trend directions, and 4 season cycles. In addition to attributes, noise and bias are added to simulate real-world conditions. The Air Quality [8] dataset contains PM2.5 time series of Beijing and London from 01/01/2017 to 31/03/2018. Each time series has a length of 168 and is affected by two attributes: 2 cities and 4 seasons. The Motor Imagery [1] dataset contains the ElectroEncephaloGram (EEG) data of subjects, who were required to imagine the movements of either the tongue or the left small finger. Each sample takes a length of 150 with 2 attributes: 2 movements and 64 channel ids. ", "page_idx": 5}, {"type": "text", "text": "Each of the three datasets is associated with a pertaining dataset and several finetuning datasets. The pretraining datasets only contain the source time series $\\mathbf{x}^{\\mathrm{{src}}}$ and its attributes $\\mathbf{a}^{\\mathrm{src}}$ . During pertaining, the noise estimator $\\epsilon_{\\theta}$ is trained to denoise ${\\bf x}_{t}^{\\mathrm{src}}$ with $\\mathbf{a}^{\\mathrm{src}}$ via Eq. (1). Each finetuning dataset corresponds to a specific split of $A_{\\mathrm{edit}}$ and $A_{\\mathrm{prsv}}$ . For example, the Synthetic dataset has 3 attributes, and thus it has $\\begin{array}{r}{6=\\sum_{i=1}^{2}\\binom{3}{i}}\\end{array}$ different splits, namely, finetuning datasets. Similarly, both Air Quality and Motor Image ry datasets have 2 finetuning datasets. Finetuning datasets contain the source time series $\\mathbf{x}^{\\mathrm{{src}}}$ , source attributes $\\mathbf{a}^{\\mathrm{src}}$ , and target attributes $\\mathbf{a}^{\\mathrm{tgt}}$ . The process of generating the target attributes mirrors the target attribute composition step in bootstrap learning. Note that for Synthetic, the target time series $\\mathbf{x}^{\\mathrm{{tgt}}}$ is also available. During finetuning, we use the proposed bootstrap learning algorithm to further improve the model. More details can be found in Appendix F. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Evaluation Metrics. A good editor should be able to generate $\\hat{\\mathbf{x}}^{\\mathrm{tgt}}$ that aligns with each $a_{k}^{\\mathrm{tgt}}\\in\\mathbf{a}^{\\mathrm{tgt}}$ . The cornerstone of our evaluation is the Contrastive Time series Attribute Pretraing (CTAP) model, which extracts time series and attribute embeddings by learning their alignment. Our CTAP is similar to CLIP [33]. For details of CTAP, please refer to Appendix G. Based on the CTAP model, we introduce two metrics: CTAP score and Log Ratio of Target-to-Source (RaTS). (1) The CTAP score is similar to the CLIP-I score [23, 33], which measures the alignment between the generated time series and the real-world time series which are associated with the given attribute value atkg t. Specifically, we first use the CTAP model to extract embeddings $\\hat{\\mathbf{h}}_{\\mathbf{x}}$ , which is the embedding of $\\hat{\\mathbf{x}}^{\\mathrm{tgt}}$ , and $\\bar{\\mathbf{h}}_{\\mathbf{x}}|a_{k}^{\\mathrm{tgi}}$ , which is the average embedding of the time series associated with $a_{k}^{\\mathrm{tgt}}$ in the training data. Then we calculate the cosine similarity of $\\hat{\\mathbf{h}}_{\\mathbf{x}}$ and $\\bar{\\mathbf{h}}_{\\mathbf{x}}|a_{k}^{\\mathrm{tgt}}$ , where the higher similarity indicates better alignment. (2) The RaTS score measures whether $\\hat{\\mathbf{x}}^{\\mathrm{tgt}}$ is closer to $a_{k}^{\\mathrm{tgt}}$ than $\\mathbf{x}^{\\mathrm{{src}}}$ . Formally, the RaTS score for a tuple $(\\hat{\\mathbf{x}}^{\\mathrm{tgt}},\\mathbf{x}^{\\mathrm{src}},a_{k}^{\\mathrm{tgt}})$ is defined as: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{RaTS}(\\hat{\\mathbf{x}}^{\\mathrm{tgt}},\\mathbf{x}^{\\mathrm{src}},a_{k}^{\\mathrm{tgt}})=\\log(\\frac{p(a_{k}^{\\mathrm{tgt}}|\\hat{\\mathbf{x}}^{\\mathrm{tgt}})}{p(a_{k}^{\\mathrm{tgt}}|\\mathbf{x}^{\\mathrm{src}})}),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $p(a_{k}|\\mathbf{x})$ is calculated by applying a softmax over the similarity scores of all $(\\mathbf{h}_{\\mathbf{x}},\\mathbf{h}_{a_{k}=i})$ pairs, where $\\mathbf{h}_{\\mathbf{x}}$ is the CTAP embedding of $\\mathbf{x}$ , and $\\mathbf{h}_{a_{k}=i}$ , $i\\in\\{1,...,N_{k}\\}$ , is the CTAP embedding of the $i$ -th possible value for the $k$ -th attribute $a_{k}$ . ", "page_idx": 6}, {"type": "text", "text": "We mainly evaluate $\\hat{\\mathbf{x}}^{\\mathrm{tgt}}$ from two perspectives: editability and preservability for edited attributes $A_{\\mathrm{edit}}$ and preserved attributes $\\mathcal{A}_{\\mathrm{prsv}}$ , respectively. For the edited attributes $A_{\\mathrm{edit}}$ , the higher RaTS and CTAP the better. For the preserved attributes $\\mathcal{A}_{\\mathrm{prsv}}$ , the higher CTAP the better. Since RaTS can take negative values, we use |RaTS| for $\\mathcal{A}_{\\mathrm{prsv}}$ , which measures the semantic divergence of $\\hat{\\mathbf{x}}^{\\mathrm{tgt}}$ from $\\mathbf{x}^{\\mathrm{{src}}}$ w.r.t $a_{k}$ . The lower |RaTS| means the better preservability. For more details, please refer to Appendix H. In addition to the semantic level evaluations, for the synthetic data, since we have ground truth target time series, we also use Mean Square Error (MSE), Mean Absolute Error (MAE) to perform the point level evaluation. ", "page_idx": 6}, {"type": "text", "text": "Compared Methods. Since there is no existing method for TSE, we modify the popular time series diffusion model CSDI [39] and the recent conditional diffusion model Time Weaver [27] for TSE. As CSDI is unable to process attributes, we add an extra attribute encoder to incorporate attribute information. For Time Weaver, we slightly modify its attribute encoder for our settings. After pretraining the modified CSDI and Time Weaver, we use them to edit time series via the editing procedure described in Sec. 3.2. Our proposed TEdit-CSDI and TEdit-TW are implemented by incorporating core processing modules of CSDI and Time Weaver into our proposed multi-resolution noise estimator Sec. 3.3, and trained via Sec. 3.4. Architecture details are presented in Appendix D. ", "page_idx": 6}, {"type": "text", "text": "Implementation Details. For all experiments, we set the number of diffusion steps as $T=50$ , embedding size for attributes and time series as 64, and use Adam optimizer [20] to train the model. For pretraining, we set (batch size, learning rate) as (256, 1e-3); for finetuning, we set them as (64,1e-7) for Synethtic and (32,1e-7) for Air Quality and Motor Imagery. We conduct a grid search for the hyperparameters of the multi-resolution. Considering the balance between performance and efficiency, we chose a compromise ratio for bootstrap. Specifically, $(R,L_{p},\\psi)\\stackrel{\\textstyle-}{=}(3,2,0.5)$ for Synthetic, (3, 2, 0.5) for Air Quality, (3, 3, 0.5) for Motor Imagery. All our experiments were conducted on a single Nvidia-A100 GPU. ", "page_idx": 6}, {"type": "text", "text": "4.2 Main Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we quantitatively evaluate the performance of comparison methods for editing and preserving attributes (RQ1) for all the datasets. For the edited attributes $A_{\\mathrm{edit}}$ , we report the RaTS and CTAP scores to depict the models\u2019 ability to edit attributes. For the preserved attributes $A_{\\mathrm{prsv}}$ , we report the |RaTS| and CTAP scores to depict the models\u2019 ability to preserve attributes. For the Synthetic dataset, since we have the ground truth, we also include MSE and MAE to evaluate the overall disparity between the ground truth $\\mathbf{x}^{\\mathrm{{tgt}}}$ and the generated $\\hat{\\mathbf{x}}^{\\mathrm{tgt}}$ . ", "page_idx": 6}, {"type": "table", "img_path": "qu5NTwZtxA/tmp/f56ce1b6dd5781b0535ddb010e33a98c93eb28d8b0192bf4cdbba07e9542391c.jpg", "table_caption": [], "table_footnote": ["Table 1: Averaged performance over all finetuning sets for Synthetic (left), Air (middle), and Motor (right). \u201cEdited\u201d and \u201cPreserved\u201d are the average results of all edited and preserved attributes. "], "page_idx": 7}, {"type": "table", "img_path": "qu5NTwZtxA/tmp/7f21da02a07340fb039ef1ffda987c55eba203f85786a5881a6e5099aed5f0a4.jpg", "table_caption": [], "table_footnote": ["Table 2: Ablation studies on the Synthetic, Air and Motor datasets. GT, BS and MR refer to Ground Truth source and target pairs, BootStrap and Multi-Resolution. Results are averaged over all finetuning sets. \u201cEdited\u201d and \u201cPreserved\u201d are the average results of all edited and preserved attributes. "], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "In Tab. 1, we present the averaged results over all the finetuning sets for each dataset. The detailed results for each finetuning sets are presented in Appendix I.1. Firstly, for the overall performance (MSE and MAE), the proposed TEdit could significantly outperform baselines. Secondly, for the edited attributes $A_{\\mathrm{edit}}$ , TEdit-CSDI and TEdit-TW could respectively outperform CSDI and Time Weaver on RaTS and TAP, showing that $\\hat{\\mathbf{x}}^{\\mathrm{tgt}}$ generated by TEdit could better fulfill the desired target $A_{\\mathrm{edit}}$ . Thirdly, for the preserved attributes $A_{\\mathrm{prsv}}$ , TEdit could basically maintain |RaTS| and TAP scores, which indicates TEdit is capable of preserving $A_{\\mathrm{prsv}}$ . In summary, these observations show that TEdit, including the multi-resolution and bootstrap learning, could improve the ability of conditional diffusion models to edit $A_{\\mathrm{edit}}$ and preserve $\\mathcal{A}_{\\mathrm{prsv}}$ . ", "page_idx": 7}, {"type": "text", "text": "4.3 Ablation Study ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this subsection, we conduct ablation studies on the Synthetic and Motor datasets based on the TEditTW to investigate the impact of the proposed multi-resolution modeling (RQ2) and the bootstrap training algorithm (RQ3). The averaged results over all the finetuning sets are presented in Tab. 2. ", "page_idx": 7}, {"type": "text", "text": "Firstly, we compare TEdit-TW trained with BS (BootStrap) with TEdit-TW trained with GT (Ground Truth) on the Synthetic dataset3. For GT, we finetune TEdit-TW by minimizing MSE between the generated $\\hat{\\mathbf{x}}^{\\mathrm{tgt}}$ and the ground truth $\\mathbf{x}^{\\mathrm{{tgt}}}$ . It can be observed that GT performs much better than BS on the overall metrics (MSE and MAE), and it is only slightly better than BS on the attribute level metrics for both edited and preserved attributes, demonstrating that BS has a strong ability to capture the attribute semantic information. Secondly, we remove BS from TEdit-TW and compare its performance with the full TEdit-TW model. For the Synthetic dataset, BS could improve the overall scores, and maintain the performance on other attribute level metrics. For the Air dataset, BS primarily improves the scores of the edited attributes, with a trade-off in the scores of the preserved attributes. We hypothesize that there might exist some intrinsic connections between the two attributes: city and season. For the Motor dataset, BS can enhance the performance on the edited attribute while maintaining the performance on the preserved attributes. Thirdly, we investigate the contribution of MR (Multi-Resolution). For the Synthetic and Air datasets, MR could improve the scores on all metrics. For the Motor dataset, MR helps improve the scores for the edited attributes and mostly sustains the performance on the preserved attributes. These improvements highlight the effectiveness of the proposed MR. ", "page_idx": 7}, {"type": "text", "text": "4.4 Extended Investigation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We further conduct an in-depth analysis of the effectiveness and efficiency of our TEdit, aiming to reveal how it works through quantitative comparison and visualization. ", "page_idx": 8}, {"type": "text", "text": "Editing vs. Conditional Generation. Although conditional generation $\\hat{\\mathbf{x}}^{\\mathrm{tgt}}=\\Phi_{\\theta}(\\varnothing,\\varnothing,\\mathbf{a}^{\\mathrm{tgt}})$ can produce time series that satisfy the specified target attribute values, it often struggles to retain sufficient details, particularly for difficult or unobservable attributes, since it generates samples from scratch. In contrast, editing $\\hat{\\mathbf{x}}^{\\mathrm{tgt}}=\\Phi_{\\theta}\\big(\\mathbf{x}^{\\mathrm{src}},\\mathbf{a}^{\\mathrm{src}},\\mathbf{a}^{\\mathrm{tgt}}\\big)$ is designed to modify the attributes of existing time series, and thus is able to preserve much detail information. We first quantitatively compare the two modes on the Synthetic data. The averaged MSE and MAE over all the finetuning datasets are presented in Tab. 3, which shows that editing could significantly outperform conditional generation with much better (lower) MSE and MAE scores. ", "page_idx": 8}, {"type": "image", "img_path": "qu5NTwZtxA/tmp/6ddcf826d05cf939f38c5bd8c1592db9f66ecc5818d58e1bfadd25cb363fd7e2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "qu5NTwZtxA/tmp/ae9283da77ae847cee7d32002641ad2a925f8038ce14357cca12e1a3337d901f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 4: Case study for editing and conditional generation, under two settings: (a) editing the trend type from logistic to linear; (b) editing the trend type from linear to quadratic and the season cycles from 1 to 4. ", "page_idx": 8}, {"type": "text", "text": "Table 3: Quantitative comparison of editing and conditional generation on the Synthetic dataset. The results are averaged over all the finetuning sets. ", "page_idx": 8}, {"type": "text", "text": "We further showcase how editing and conditional generation work at the sample level. Fig. 4, plots the generated results of these two modes under. We can observe that the edited time series could better align with the target attributes, Fig. 4(a)(b), and preserve the characteristics of the source time series, e.g., bias in Fig. 4 (a). In comparison, conditional generation tends to produce smooth samples around the dataset mean. Additionally, we can also see in Fig. 4 that the MSE scores of editing are much lower than conditional generation. More visualization results can be found in Appendix I.4 ", "page_idx": 8}, {"type": "text", "text": "Efficiency of Multi-Resolution Generation. Considering that multi-resolution modeling and generation brings additional efforts for processing different resolutions, as discussed in Sec. 3.3. We compare the running time of serial processing and parallel processing implementations of the multi-resolution paradigm with total resolu$R=3$ ", "page_idx": 8}, {"type": "table", "img_path": "qu5NTwZtxA/tmp/a350e6f67fe088b86ec7860dd64ddf8e99ecca2b3c5728595e38c3bc6b5987a5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 4: Running time (ms) of the multi-resolution modeling and generation module, averaged over 1000 samples. ", "page_idx": 8}, {"type": "text", "text": "tion number and patch length $L_{p}^{r}=3^{r},r\\in\\{0,...,R-1\\}$ . The experiment is conducted on the Synthetic dataset with a batch size of 32. The serial processing iterative processes a single resolution at a time, whereas the parallel processing concatenates multiple resolution sequences along the length dimension and processes them simultaneously. We record the time it takes for the full noise estimator and the processing module to complete the forward diffusion process. As shown in Tab. 4, our designed parallel mechanism can significantly improve the inference efficiency. ", "page_idx": 8}, {"type": "text", "text": "Bootstrap Improves the distributional coverage of the attribution space. Fig. 5 visualizes the dimension reduced distribution of (a) the raw data, (b) the generated data and (c) the mixed data, on the Synthetic dataset using t-SNE [41]. We can observe that the newly generated data fulfill the uncovered region of the original raw data therefore enhance the data coverage over the whole space, which explains how the bootstrap learning helps generative model training. More visualization analysis can be found in Appendix I.5. ", "page_idx": 8}, {"type": "text", "text": "4.5 Sensitivity Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this subsection, we conduct sensitivity analysis to study the impacts of hyper-parameters of the multi-resolution mechanism and bootstrap learning. ", "page_idx": 8}, {"type": "image", "img_path": "qu5NTwZtxA/tmp/3abece3377a1ee4757e25efd3e9590947738ed4432d0d938ab6195ba86a063c9.jpg", "img_caption": ["Figure 5: Visualization of data distribution before and after bootstrapping. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Multi-resolution hyper-parameters. We perform a sensitivity study for the hyper-parameters of the proposed multi-resolution, i.e., the patch length $L_{p}$ and the number of resolutions $R$ , on one Synthetic finetuning set, which aims to edit trend directions while preserving other attributes. The results are presented in Fig. 6. More results on other Synthetic finetuning sets can be found in Appendix I.2. ", "page_idx": 9}, {"type": "text", "text": "To analyze the impact of $R$ , we fix $L_{p}=2$ and vary $R$ . As shown in Fig. 6 (a), trend types prefer $R=3$ , whereas seasonal cycles perform better with $R=4$ . To examine the impact of $L_{p}$ , we fix $R=3$ and vary $L_{p}$ . According to Fig. 6 (b), trend types show a preference for $L_{p}=2$ while seasonal cycles prefer $L_{p}=3$ . Besides, we find that our proposed multi-resolution paradigm always performs better than the vanilla single resolution time-series modeling paradigms, i.e., $L_{p}=1$ or $R=1$ , which corroborates the discussion in Sec. 3.3 that attributes may influence the time-series generation at different scales and granularities. ", "page_idx": 9}, {"type": "text", "text": "Bootstrap learning hyper-parameters. We conduct a sensitivity study on the hyper-parameters of bootstrapping, specifically the bootstrap ratio $\\psi$ within the range $\\left\\{\\bar{0}.1,0.3,0.\\bar{5},0.\\bar{7},0.9\\right\\}$ on a Synthetic finetuning set, which aims to edit the trend direction while preserving others. As shown in Fig. 7, the model exhibits a poor performance then the bootstrap ratio is low, e.g., 0.1. The model has a good performance with medium to high bootstrap ratios. We hypothesize that this is because tuning with only a few top samples might lead to certain mode collapse. ", "page_idx": 9}, {"type": "image", "img_path": "qu5NTwZtxA/tmp/62f05f889006013e3fbaffc5537edbad794e06e234cac9fdbd9d63b75ee31fc7.jpg", "img_caption": ["Figure 6: Sensitivity study for the hyper-parameters of multi- Figure 7: Sensitivity study of bootresolution on the Synthetic finetuning set for editing trend strap ratio $\\psi$ on a Synthetic finetundirections and preserving others. The higher CTAP, the better. ing set aiming to edit trend direction. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduced the task of Time Series Editing (TSE), which enables controllable time series synthesis by manipulating specific attributes of the input time series while maintaining consistency in other properties. There are two major challenges of TSE. Firstly, in the real world, the distribution of time series and attributes is usually biased, and the full space might not be adequately covered. Secondly, time series and attributes exhibit complex multi-scale entanglement. To address these two challenges, we propose a novel diffusion based approach, called TEdit, which incorporates a bootstrap learning algorithm and a multi-resolution modeling and generation paradigm. Comprehensive experiments on both synthetic and real-world datasets demonstrate the effectiveness of our proposed TEdit in generating precise time series with specified attributes. In practice, our method still has certain limitations. For example, different attributes have varying difficulty degrees to edit; attributes may have complex inter-dependencies, and some attributes may be difficult to edit without affecting others. Despite this limitation, we believe our work lays the groundwork for controllable time series synthesis, potentially benefiting applications in fields such as climate monitoring, healthcare, and urban management. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Anthony Bagnall, Hoang Anh Dau, Jason Lines, Michael Flynn, James Large, Aaron Bostrom, Paul Southam, and Eamonn Keogh. The uea multivariate time series classification archive, 2018, 2018.   \n[2] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. Wavegrad: Estimating gradients for waveform generation. arXiv preprint arXiv:2009.00713, 2020.   \n[3] Yuqi Chen, Kan Ren, Yansen Wang, Yuchen Fang, Weiwei Sun, and Dongsheng Li. Contiformer: Continuous-time transformer for irregular time series modeling. Advances in Neural Information Processing Systems, 36, 2024.   \n[4] Andrea Coletta, Sriram Gopalakrishnan, Daniel Borrajo, and Svitlana Vyetrenko. On the constrained time-series generation problem. Advances in Neural Information Processing Systems, 36, 2024.   \n[5] Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. Plug and play language models: A simple approach to controlled text generation. arXiv preprint arXiv:1912.02164, 2019. [6] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780\u20138794, 2021.   \n[7] Dave Epstein, Allan Jabri, Ben Poole, Alexei Efros, and Aleksander Holynski. Diffusion self-guidance for controllable image generation. Advances in Neural Information Processing Systems, 36:16222\u201316239, 2023.   \n[8] Rakshitha Godahewa, Christoph Bergmeir, Geoff Webb, Rob Hyndman, and Pablo MonteroManso. Kdd cup dataset (without missing values). https://zenodo.org/records/4656756, 2020. [9] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022.   \n[10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.   \n[11] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.   \n[12] Daniel Jarrett, Jinsung Yoon, Ioana Bica, Zhaozhi Qian, Ari Ercole, and Mihaela van der Schaar. Clairvoyance: A pipeline toolkit for medical time series. arXiv preprint arXiv:2310.18688, 2023.   \n[13] Baoyu Jing, Hanghang Tong, and Yada Zhu. Network of tensor time series. In Proceedings of the Web Conference 2021, pages 2425\u20132437, 2021.   \n[14] Baoyu Jing, Yansen Wang, Guoxin Sui, Jing Hong, Jingrui He, Yuqing Yang, Dongsheng Li, and Kan Ren. Automated contrastive learning strategy search for time series. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, pages 4612\u20134620, 2024.   \n[15] Baoyu Jing, Si Zhang, Yada Zhu, Bin Peng, Kaiyu Guan, Andrew Margenot, and Hanghang Tong. Retrieval based time series forecasting. arXiv preprint arXiv:2209.13525, 2022.   \n[16] Baoyu Jing, Dawei Zhou, Kan Ren, and Carl Yang. Causality-aware spatiotemporal graph neural networks for spatiotemporal time series imputation. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, pages 1027\u20131037, 2024.   \n[17] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4401\u20134410, 2019.   \n[18] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models for robust image manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2426\u20132435, 2022.   \n[19] Heeseung Kim, Sungwon Kim, and Sungroh Yoon. Guided-tts: A diffusion model for textto-speech via classifier guidance. In International Conference on Machine Learning, pages 11119\u201311133. PMLR, 2022.   \n[20] Diederik P Kingma. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[21] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. arXiv preprint arXiv:2009.09761, 2020.   \n[22] Thorsten Kurth, Shashank Subramanian, Peter Harrington, Jaideep Pathak, Morteza Mardani, David Hall, Andrea Miele, Karthik Kashinath, and Anima Anandkumar. Fourcastnet: Accelerating global high-resolution weather forecasting using adaptive fourier neural operators. In Proceedings of the platform for advanced scientific computing conference, pages 1\u201311, 2023.   \n[23] Dongxu Li, Junnan Li, and Steven Hoi. Blip-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing. Advances in Neural Information Processing Systems, 36, 2024.   \n[24] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang, and Tatsunori B Hashimoto. Diffusion-lm improves controllable text generation. Advances in Neural Information Processing Systems, 35:4328\u20134343, 2022.   \n[25] Huan Ling, Karsten Kreis, Daiqing Li, Seung Wook Kim, Antonio Torralba, and Sanja Fidler. Editgan: High-precision semantic image editing. Advances in Neural Information Processing Systems, 34:16331\u201316345, 2021.   \n[26] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6038\u20136047, 2023.   \n[27] Sai Shankar Narasimhan, Shubhankar Agarwal, Oguzhan Akcin, Sujay Sanghavi, and Sandeep Chinchali. Time weaver: A conditional time series generation model. arXiv preprint arXiv:2403.02682, 2024.   \n[28] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.   \n[29] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. arXiv preprint arXiv:2211.14730, 2022.   \n[30] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195\u20134205, 2023.   \n[31] Hengzhi Pei, Kan Ren, Yuqing Yang, Chang Liu, Tao Qin, and Dongsheng Li. Towards generating real-world time series data. In 2021 IEEE International Conference on Data Mining (ICDM), pages 469\u2013478. IEEE, 2021.   \n[32] Ratish Puduppully, Li Dong, and Mirella Lapata. Data-to-text generation with content selection and planning. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 6908\u20136915, 2019.   \n[33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[34] Kashif Rasul, Calvin Seward, Ingmar Schuster, and Roland Vollgraf. Autoregressive denoising diffusion models for multivariate probabilistic time series forecasting. In International Conference on Machine Learning, pages 8857\u20138868. PMLR, 2021.   \n[35] Hansheng Ren, Bixiong Xu, Yujing Wang, Chao Yi, Congrui Huang, Xiaoyu Kou, Tony Xing, Mao Yang, Jie Tong, and Qi Zhang. Time-series anomaly detection service at microsoft. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pages 3009\u20133017, 2019.   \n[36] Lifeng Shen and James Kwok. Non-autoregressive conditional diffusion models for time series prediction. In International Conference on Machine Learning, pages 31016\u201331029. PMLR, 2023.   \n[37] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 2256\u20132265. PMLR, 2015.   \n[38] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020.   \n[39] Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. Csdi: Conditional score-based diffusion models for probabilistic time series imputation. Advances in Neural Information Processing Systems, 34:24804\u201324816, 2021.   \n[40] Soobin Um and Jong Chul Ye. Don\u2019t play favorites: Minority guidance for diffusion models. arXiv preprint arXiv:2301.12334, 2023.   \n[41] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(Nov):2579\u20132605, 2008.   \n[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[43] Senzhang Wang, Meiyue Zhang, Hao Miao, Zhaohui Peng, and Philip S Yu. Multivariate correlation-aware spatio-temporal graph convolutional networks for multi-scale traffic prediction. ACM Transactions on Intelligent Systems and Technology (TIST), 13(3):1\u201322, 2022.   \n[44] Chunjing Xiao, Zehua Gou, Wenxin Tai, Kunpeng Zhang, and Fan Zhou. Imputation-based timeseries anomaly detection with conditional weight-incremental diffusion models. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 2742\u20132751, 2023.   \n[45] Yiyuan Yang, Ming Jin, Haomin Wen, Chaoli Zhang, Yuxuan Liang, Lintao Ma, Yi Wang, Chenghao Liu, Bin Yang, Zenglin Xu, et al. A survey on diffusion models for time series and spatio-temporal data. arXiv preprint arXiv:2404.18886, 2024.   \n[46] Ke Yi, Yansen Wang, Kan Ren, and Dongsheng Li. Learning topology-agnostic eeg representations with geometry-aware modeling. Advances in Neural Information Processing Systems, 36, 2024.   \n[47] Jinsung Yoon, Daniel Jarrett, and Mihaela Van der Schaar. Time-series generative adversarial networks. Advances in neural information processing systems, 32, 2019.   \n[48] Xinyu Yuan and Yan Qiao. Diffusion-ts: Interpretable diffusion for general time series generation. arXiv preprint arXiv:2403.01742, 2024.   \n[49] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836\u20133847, 2023.   \n[50] Hang Zhou, Yu Liu, Ziwei Liu, Ping Luo, and Xiaogang Wang. Talking face generation by adversarially disentangled audio-visual representation. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 9299\u20139306, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "[51] Hang Zhou, Yasheng Sun, Wayne Wu, Chen Change Loy, Xiaogang Wang, and Ziwei Liu. Pose-controllable talking face generation by implicitly modularized audio-visual representation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4176\u20134186, 2021. ", "page_idx": 13}, {"type": "text", "text": "A Related Work ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Time Series Generation. Time series generation has become a research hotspot in recent years [47]. From the view of modeling, early attempts for time series generation utilize Generative Adversarial Networks (GANs) [47, 31], which trains the generator adversarial to the discriminator of correct classification on real or synthetic data. The latest research moves the focus to Diffusion models (DMs) [27, 36, 39] which have illustrated much higher fidelity and generation performance than GANs [45]. From the perspective of the generation paradigm, there are two branches. The first one is unconditional generation [47, 31, 34, 48], which employs a generative model to generate data samples without relying on any conditions for regulation. The second paradigm is conditional generation [39, 27, 36, 44] making use of additional conditional information like category labels, textual descriptions, features, and other kinds of metadata to guide the generation process and enabling high-quality generation that is tightly correlated with the conditions. Among them, CSDI [39] enhances time-series imputation by treating observed time series as conditions in the diffusion model. TIME WEAVER [27] directly encode the metadata into the diffusion model to achieve controlled generation. Though they achieved better quantitative results, these methods suffer from unexpected outcomes that inadvertently emphasize trivial attributes while overlooking critical ones, especially with a low availability of time-series data over various attributes. ", "page_idx": 14}, {"type": "text", "text": "Diffusion Model. Diffusion model [10, 37] has become increasingly prominent for data generation across various fields [26, 28, 2, 21], which learns to gradually regress and remove the noise that was added to the clean data in the forward process, allowing data generation aligned with real distribution. Our work is highly related to conditional generation, which can be divided into two main streams of techniques. Classifier-guided generation [6, 19, 24] adds the control signal from the gradient of the classifier to bias the generation towards the specific class in the denoising process of the diffusion model, which requires training an additional classification model. Another stream of works [11, 30] stem from classifier-free guidance in data generation, without training any additional classifier. ", "page_idx": 14}, {"type": "text", "text": "Data Editing. Data editing aims to manipulate the content of the given input to meet the requirements of the target data attributes, such as image style [17], semantics [25], and structure [7], with the utilization of multimodal methods such as text-to-image synthesis [9], [28, 49] and audio based image editing [50, 51]. In text-based scenarios, some works also generate texts under the instruction of specific attributes like sentiment [5] and structural information [32]. However, directly adopting the existing solution from other fields to time series editing is not feasible because of the significantly different data formats and poor data coverage of real-world time-series data w.r.t. different attributes. ", "page_idx": 14}, {"type": "text", "text": "B Broader Impacts ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Our method offers several potential positive societal impacts in many real-world scenarios. For example, in climate monitoring, it enhances climate models and forecasts, aiding in climate change mitigation. For urban management, it supports better decision-making in areas such as traffic management, pollution control, and resource allocation. Additionally, in research, it facilitates robust experiments and innovative solutions by providing high-quality, customizable datasets. ", "page_idx": 14}, {"type": "text", "text": "However, there are potential negative societal impacts to consider. There are data privacy risks associated with the manipulation of sensitive information, necessitating strong data protection measures. In financial markets, our method could be exploited for fraudulent activities, undermining market integrity. An overreliance on synthetic data may lead to biased or inaccurate models, resulting in flawed decision-making. Furthermore, there are ethical concerns regarding the potential misuse of our method in creating deceptive artificial intelligence models, which raises issues of transparency and accountability. While our method offers significant benefits, it is crucial to carefully manage these risks to maximize positive impacts and minimize negative consequences. ", "page_idx": 14}, {"type": "text", "text": "C Diffusion Process ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we give a review of the diffusion process. Diffusion models are a class of two-stage generation models, consisting of a forward process and a reverse(denoising) process. The forward process is a Markov chain which gradually adds random noise on the origin data sample $\\mathbf{x}_{\\mathrm{0}}$ and ", "page_idx": 14}, {"type": "image", "img_path": "qu5NTwZtxA/tmp/8cbcabbc786ce007b042c80d55f07c8516dac7c75709764b6d69fe37aa022ff3.jpg", "img_caption": ["Figure 8: Model architecture of our modified CSDI. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "intermediate latent variables $\\mathbf{x}_{t}$ through a Gaussian transition $q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1}):=\\mathcal{N}(\\sqrt{1-\\beta_{t}}\\mathbf{x}_{t-1},\\beta_{t}\\mathbf{I})$ , where $t=1,\\cdot\\cdot\\cdot,T$ indicates the diffusion step, and $\\{\\beta_{t}\\}_{t=0}^{T}$ are predetermined variance schedule. The latent variable $\\mathbf{X}_{t}$ can be described as: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{x}_{t}=\\sqrt{\\alpha_{t}}\\mathbf{x}_{0}+\\sqrt{1-\\alpha_{t}}\\epsilon,\\;\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I}),\\;\\alpha_{t}:=\\Pi_{s=1}^{t}(1-\\beta_{s}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "In the reverse process, a denoiser $\\epsilon_{\\theta}(\\mathbf{x}_{t},t,\\mathbf{a})$ , where $\\mathbf{a}\\in\\mathbb{N}^{K}$ is the given attributes, is needed to approximate the noise in each diffusion step to help to sample backward from $\\mathbf{X}_{T}$ through another Gaussian transition $p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_{t}):=\\mathcal{N}(\\mathbf{x}_{t-1}\\bar{;}\\mu_{\\theta}(\\mathbf{x}_{t},\\bar{t}),\\ \\sigma_{\\theta}(\\bar{\\mathbf{x}_{t}},t)\\mathbf{I})$ . To obtain $\\mu_{\\theta}(\\mathbf{x}_{t},t)$ , the denoiser should be trained by optimizing the loss: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\mathcal{L}:=\\operatorname*{min}_{\\theta}\\mathbb{E}_{\\mathbf{x}_{0}\\sim q\\left(\\mathbf{x}_{0}\\right),\\epsilon\\sim\\mathcal{N}\\left(\\mathbf{0},\\mathbf{I}\\right),t\\sim\\mathcal{U}\\left(1,T\\right)}\\left\\|\\epsilon-\\epsilon_{\\theta}\\left(\\mathbf{x}_{t},t,\\mathbf{a}\\right)\\right\\|_{2}^{2}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Here $q(\\mathbf{x}_{0})$ denotes the data distribution of $\\mathbf{x}_{\\mathrm{0}}$ (or x), $t\\sim\\mathcal{U}(1,T)$ indicates that the diffusion step $t$ follows the uniform distribution between 1 and $T$ . ", "page_idx": 15}, {"type": "text", "text": "After training, given a condition a, we can generate the sample $\\hat{\\mathbf{x}}_{0}$ from a random noise $\\hat{\\mathbf{x}}_{T}\\sim\\mathcal{N}(0,\\mathbf{I})$ using the trained $\\epsilon_{\\theta}(\\mathbf{x}_{t},t,\\mathbf{a})$ with a diffusion sampler, such as the deterministic denoising diffusion implicit model (DDIM) [38]: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathbf{x}}_{t-1}=\\sqrt{\\alpha_{t-1}}\\mathbf{f}_{\\theta}(\\hat{\\mathbf{x}}_{t},t,\\mathbf{a})+\\sqrt{1-\\alpha_{t-1}}\\epsilon_{\\theta}(\\hat{\\mathbf{x}}_{t},t,\\mathbf{a})}\\\\ {\\mathbf{f}_{\\theta}(\\hat{\\mathbf{x}}_{t},t,\\mathbf{a})=(\\hat{\\mathbf{x}}_{t}-\\sqrt{1-\\alpha_{t}}\\epsilon_{\\theta}\\left(\\hat{\\mathbf{x}}_{t},t,\\mathbf{a}\\right))/\\sqrt{\\alpha_{t}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where ${\\bf f}_{\\theta}(\\hat{{\\bf x}}_{t},t,{\\bf a})$ can be regarded as a prediction of $\\mathbf{x}_{\\mathrm{0}}$ at the diffusion step $t$ . ", "page_idx": 15}, {"type": "text", "text": "D Model Architecture ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we will show the architecture utilized in our modified backbone models CSDI 8 and TIME-WEAVER 9, especially how the attributes are incorporated into the models. It should be mentioned that all the attributes in our experiment setting are categorical. ", "page_idx": 15}, {"type": "text", "text": "Consider a batch of time series samples of size $(N_{\\mathrm{batch}},F,L)$ , where $N_{\\mathrm{batch}}$ represents the number of samples per batch, $F$ represents the number of channels in the time series, and $L$ represents the horizon. The corresponding attributes a is of shape $(N_{\\mathrm{batch}},L,K)$ , where $K$ represents the number of the attributes. While performing the time-series inputs, each attribute option will be encoded to a corresponding embedding vector. ", "page_idx": 15}, {"type": "image", "img_path": "qu5NTwZtxA/tmp/beba653ddb7854aff804e315ab40b8ba8b893bebc242154516581bd56e562f8c.jpg", "img_caption": ["Figure 9: Model architecture of Time Weaver. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "As for CSDI, our modified part is highlighted in red. First, the attribute embedding will be linearly projected to shape $(N_{\\mathrm{batch}},d,F,K)$ , then concatenated with the time series embedding to form the prompt with shape $(N_{\\mathrm{batch}},d,F,K+L)$ . Next, the prompt is added with the expanded diffusion step embedding and the sum will be inputted into the transformer layer. After that, the attribute embedding will be discarded, and the shape gets back to $(N_{\\mathrm{batch}},d,F,L)$ . ", "page_idx": 16}, {"type": "text", "text": "For TIME-WEAVER, We only make some slight changes to its original structure: 1. Our experiment setting doesn\u2019t include continuous type attributes, as mentioned. 2. We replace the self-attention modules with multi-layer perceptrons (MLPs). ", "page_idx": 16}, {"type": "text", "text": "As mentioned in Sec 3.3, we introduce a novel parallelization with an attention masking mechanism to accelerate the training and inference. The attention mask is of the following form, denoted as $\\mathbf{M}\\,\\in\\,\\mathbb{R}^{(N_{p}^{0}+\\hdots+N_{p}^{R-1})\\,\\tilde{\\times}\\dot{(N_{p}^{0}+\\hdots+N_{p}^{R-1})}}$ where $\\mathbf{M}^{r}\\,\\in\\,\\mathbf{0}^{N_{p}^{r}}$ , the residual value is negative infinity. Through adding this attention mask into the original attention calculation [42] and softmax operation $\\begin{array}{r}{\\mathrm{softmax}(\\mathbf{z})_{i}\\,=\\,\\frac{e^{z_{i}}}{\\sum_{j=1}^{N}e^{z_{j}}}}\\end{array}$ jNe=z1i ezj receiving z \u2208RN, such attention mask would mask out inter-sequence attention while preserving intra-sequence attention. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{M}=\\left(\\begin{array}{c c c}{\\mathbf{M^{1}}}&{\\cdots}&{-\\infty}\\\\ {\\vdots}&{\\ddots}&{\\vdots}\\\\ {-\\infty}&{\\cdots}&{\\mathbf{M^{R-1}}}\\end{array}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "E Bootstrap Learning Algorithm ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "As mentioned in Sec. 3.4, we propose a bootstrap learning algorithm that enables the model $\\epsilon_{\\theta}$ to learn the information of the imaginary target. Given the pretained model $\\epsilon_{\\theta}$ , the detailed algorithm for a batch $(\\mathbf{X}_{0}^{\\mathrm{src}}\\in\\mathbb{R}^{B\\times L}$ , $\\bar{\\mathbf{A}^{\\mathrm{src}}}\\in\\bar{\\mathbb{N}_{+}^{\\dot{B}\\times K}}\\bar{\\mathbf{\\Xi}}$ , where $B$ is the batch size, $L$ is the length, and $K$ is the number of attributes, is presented below. ", "page_idx": 16}, {"type": "text", "text": "Input: $(\\mathbf{X}_{0}^{\\mathrm{src}}\\in\\mathbb{R}^{B\\times L}$ , $\\mathbf{A}^{\\mathrm{src}}\\in\\mathbb{N}_{+}^{B\\times K})$ , where $B$ , $L$ , $K$ are batch size, length, and attribute number. Attributes to be edited $\\lambda_{\\mathrm{edit}}$ and preserved $\\mathcal{A}_{\\mathrm{prsv}}$ .   \nThe ratio $\\psi$ of the top bootstrapped samples to include for tuning.   \n1: # 1. Compose tuple $(\\mathbf{X}_{0}^{\\mathrm{src}},\\mathbf{A}^{\\mathrm{src}},\\mathbf{A}^{\\mathrm{tgt}})$ .   \n2: for $k=1,\\cdots\\,,K$ do   \n3: if $k\\in{\\mathcal{A}}_{\\mathrm{edit}}$ then   \n4: ${\\bf A}^{\\mathrm{tgt}}[:,k]\\sim\\mathcal{U}\\{1,\\cdots\\,,N_{k}\\}$ \u25b7Random sample a new value for the k-th attribute. 5: else   \n$6!\\begin{array}{l}{{\\bf\\Delta}}\\\\ {{\\bf\\Delta}}\\end{array}\\qquad\\begin{array}{l}{{\\bf A}^{\\mathrm{tgt}}[:,k]={\\bf A}^{\\mathrm{src}}[:,k]$   \n7: # 2. Generate the target $\\hat{\\mathbf{X}}^{\\mathbf{tgt}}$ .   \n8: $\\hat{\\mathbf{X}}_{0}^{\\mathrm{tgt}}\\gets\\Phi_{\\theta}(\\mathbf{X}_{0}^{\\mathrm{src}},\\mathbf{A}^{\\mathrm{src}},\\mathbf{A}^{\\mathrm{tgt}})$   \n9: # 3. Self-Score the target $\\hat{\\mathbf{X}}_{0}^{\\mathbf{tgt}}$ , and keep the top $\\psi$ samples.   \n10: $\\hat{\\mathbf{X}}_{0}^{\\mathrm{src}}\\gets\\Phi_{\\theta}(\\hat{\\mathbf{X}}_{0}^{\\mathrm{tgt}},\\mathbf{A}^{\\mathrm{tgt}},\\mathbf{A}^{\\mathrm{src}})$   \n11: $\\mathbf{s}\\gets\\mathrm{MSE}(\\mathbf{X}_{0}^{\\mathrm{src}},\\hat{\\mathbf{X}}_{0}^{\\mathrm{src}})$   \n12: $\\hat{\\mathbf{X}}_{0}^{\\mathrm{tgt}}\\gets\\mathrm{Sort}(\\hat{\\mathbf{X}}_{0}^{\\mathrm{tgt}}|\\mathbf{s})$ \u25b7Sort $\\hat{\\mathbf{X}}_{0}^{t g t}$ based on s. 13: $\\hat{\\mathbf{X}}_{0,\\mathrm{bs}}^{\\mathrm{tgt}}\\leftarrow\\hat{\\mathbf{X}}_{0}^{\\mathrm{tgt}}[$ : $B\\cdot\\psi]$ \u25b7Keep the top \u03c8 samples. 14: # 4. Calculate the loss.   \n15: $\\mathcal{L}_{B S}\\gets\\mathcal{L}(\\hat{\\mathbf{X}}_{0,\\mathrm{bs}}^{\\mathrm{tgt}})$ $\\vartriangleright$ is the noise estimation loss of diffusion models defined in Eq.(1). ", "page_idx": 17}, {"type": "table", "img_path": "qu5NTwZtxA/tmp/85cfbeef6e2a670d09b8c4bfeed10c320fe58434f24514b4d9bba20ade86b5d2.jpg", "table_caption": [], "table_footnote": ["Table 5: Summary of attribute options "], "page_idx": 17}, {"type": "text", "text": "F Datasets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "F.1 Synthetic Dataset ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We generate the synthetic time series according to the following formula: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{x}=\\mathbf{x}_{\\mathrm{trend}}+\\mathbf{x}_{\\mathrm{season}}+\\mathbf{x}_{\\mathrm{noise}}+\\mathbf{x}_{\\mathrm{bias}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Table 5 shows the summary of the attributes for the synthetic dataset. ", "page_idx": 17}, {"type": "text", "text": "For example, the attribute {Trend Type $=$ linear, Trend Direction $\\mathbf{\\mu}=\\mathbf{up}$ , Season Cycles $=2\\}$ is one of the total 32 (4 Trend Types $\\times\\ 2$ Trend Directions $\\times\\,4$ Season Cycles) attribute combinations. We split the 32 combinations into train, validation, and test sets by 24:4:4. ", "page_idx": 17}, {"type": "text", "text": "In the pretraining dataset, for each attribute combination, we sample 300 time series according to the random variances described in the following subsections. For the finetuning dataset, we treat each attribute combination in the 24:4:4 split as the target attribute. For each target attribute, we compose its source attribute based on the desired attributes to be edited $A_{\\mathrm{edit}}$ and preserved $A_{\\mathrm{prsv}}$ . For example, let $A_{\\mathrm{{edit}}}\\,=$ {trend type}, $\\begin{array}{r}{\\mathcal{A}_{\\mathrm{prsv}}\\;=\\;\\left\\{\\begin{array}{r l r l}\\end{array}\\right.}\\end{array}$ {trend direction, season cycles $\\}$ . Suppose the target attribute $\\mathbf{a}^{\\mathrm{tgt}}\\,=\\,\\{\\mathrm{linear},\\mathrm{up},2\\}$ , then its source could be $\\mathbf{a}_{1}^{\\mathrm{src}}\\ =\\ \\{\\mathrm{linear},\\mathrm{u}\\dot{\\mathrm{p}},2\\}$ , $\\mathbf{\\bar{a}}_{2}^{\\mathrm{{src}}}\\ =\\ \\{\\mathrm{quadratic},\\mathrm{up},2\\}$ , $\\mathbf{a}_{3}^{\\mathrm{src}}=\\{\\mathrm{exponential},\\mathrm{up},2\\}$ and, ${\\mathbf a}_{4}^{\\mathrm{src}}=\\{\\mathrm{logistic},\\mathrm{up},2\\}$ . Then for a source and target attribute pair $(\\mathbf{a}^{\\mathrm{src}},\\mathbf{a}^{\\mathrm{tgt}})$ , we generate the source and target time series $(\\mathbf{x}^{\\mathrm{src}},\\mathbf{x}^{\\mathrm{tgt}})$ based on the process described in the following subsections. Note that for each $(\\mathbf{a}^{\\mathrm{src}},\\mathbf{a}^{\\mathrm{tgt}})$ attribute pair, we first generate the deterministic part, e.g., trend type, for $\\mathbf{a}^{\\mathrm{src}}$ and $\\mathbf{a}^{\\mathrm{tgt}}$ , denoted by $\\mathbf{x}_{d e t e r}^{\\mathrm{{src}}}$ and $\\mathbf{x}_{d e t e r}^{\\mathrm{{tgt}}}$ . Then generate 10 randomnesses $\\mathbf{x}_{r a n d}$ , including noise, bias etc, and then we add these randomnesses to the deterministic part to ensure the source and target time series have the same randomness $\\mathbf{x}_{r a n d}$ : $\\mathbf{x}^{\\mathrm{tgt}}=\\mathbf{x}_{d e t e r}^{\\mathrm{tgt}}+\\mathbf{x}_{r a n d},\\mathbf{x}^{\\mathrm{src}}=\\mathbf{x}_{d e t e r}^{\\mathrm{src}}+\\mathbf{x}_{r a n d}$ . ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "The finetuning dataset contains 6 subsets for editing different attribute combinations. For modifying trend type, there are 760 train samples, 140 validation samples, and 160 test samples. For modifying trend direction, there are 420 train samples, 70 validation samples, and 80 test samples. For modifying season cycles, there are 760 train samples, 140 validation samples, and 160 test samples. For modifying trend type and trend direction, there are 1440 train samples, 280 validation samples, and 320 test samples. For modifying trend type and season cycles, there are 2880 train samples, 560 validation samples, and 640 test samples. For modifying trend direction and season cycles, there are 1440 train samples, 280 validation samples, and 320 test samples. In addition, the same random variance and bias pair is used both on the source time series and target time series in a source-target pair respectively. ", "page_idx": 18}, {"type": "text", "text": "In the following formulas, $t_{i}$ indicates the $i$ -th sampling point in the original synthetic time series(t) and $x_{i}$ indicates the $i$ -th sampling point in a contain form of $\\mathbf{x}$ . For instance, the total length of a time series $L=128$ , then $i=[1,\\dotsc,128]$ . ", "page_idx": 18}, {"type": "text", "text": "F.1.1 Trend ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Trend Types. As described in table 5, there are 4 trend types: linear, quadratic, exponential, and logistic. As described above, $\\mathbf{t}$ is used to obtain $\\mathbf{x}$ . For the linear trend: $\\mathbf{x}_{\\mathrm{trend}}=\\mathbf{t}$ , in this case $t_{i}\\in$ $[0,1]$ , $x_{i}\\in[0,1]$ . For the quadratic trend: $\\mathbf{x}_{\\mathrm{trend}}=\\mathbf{t}^{2}$ , in this case $t_{i}\\in[0,1]$ , $x_{i}\\in[0,1]$ . For the exponential trend: $\\begin{array}{r}{\\mathbf{x}_{\\mathrm{trend}}=\\frac{2^{\\mathbf{t}}}{1024}}\\end{array}$ , in this case $t_{i}\\in[-10,10]$ , $x_{i}\\in[0,1]$ . $\\mathbf{x}_{\\mathrm{trend}}$ is needed to range from 0 to 1, so $t_{i}$ is in [-10,10]. For logistic trend: $\\begin{array}{r}{\\mathbf{x}_{\\mathrm{trend}}=\\frac{1}{1+\\exp(-\\mathbf{t})}}\\end{array}$ , where $t_{i}\\in[-10,10]$ , $x_{i}\\in$ $[0,1]$ . Similar to the exponential trend, we repeat a scaling process in logistic trend. To train the model more easily, $\\mathbf{x}_{\\mathrm{trend}}=(\\mathbf{x}-0.5)\\times2$ is used to normalize $\\mathbf{x}_{\\mathrm{trend}}$ to $[-\\bar{1},1]$ . ", "page_idx": 18}, {"type": "text", "text": "Trend Directions. There are totally 2 directions: up and down. For instance, in the Cartesian coordinate system, a linear line from coordinates $(0,0)$ to $(1,1)$ represents an \u201cup\u201d trend, while another line from coordinates $(0,0)$ to $(1,-1)$ represents a \u201cdown\u201d trend. All 4 trend types introduce above are originally \u201cup\u201d trends. Therefore, for an \u201cup\u201d trend, we set $\\mathbf{x}_{\\mathrm{trend}}=\\mathbf{x}_{\\mathrm{trend}}$ and for a \u201cdown\u201d trend, we set xtrend = \u2212xtrend. ", "page_idx": 18}, {"type": "text", "text": "Random Variance. We randomly perturb the scale of the trend by: $\\mathbf{x}_{\\mathrm{trend}}=s\\cdot\\mathbf{x}_{\\mathrm{trend}}$ where $s$ is the random scale obtained by the following process $s=m\\cdot c+(1-m)\\cdot{\\frac{1}{c}}$ , $m\\sim B e r n(0.5),\\ c\\ \\,$ $c\\sim$ $\\mathcal{U}(0.8,1.0)$ If $m=1$ , then $s=c\\in[0.8,1.0]$ ; else $\\begin{array}{r}{s=\\frac{1}{c}\\in[1.0,1.25]}\\end{array}$ ]. ", "page_idx": 18}, {"type": "text", "text": "F.1.2 Season ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The season attribute is represented by the number of cycles in a time series. [0,1,2,4] sinusoidal wave cycles are randomly added. ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\bf x}_{\\mathrm{season}}=a\\sin(2\\pi t+\\phi)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $t\\,\\in\\,[0,n_{\\mathrm{cycle}}],n_{\\mathrm{cycle}}\\,\\in\\,[0,2^{0},2^{1},2^{2}]$ . The Random Variances of these two variables follow uniform distributions: $\\boldsymbol{\\dot{a}}\\sim\\mathcal{U}(0.4,0.6)$ , $\\phi\\stackrel{\\cdot}{\\sim}\\mathcal{U}(0,2\\pi)$ . ", "page_idx": 18}, {"type": "text", "text": "F.1.3 Noise ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We simulate the real-world noise by a combination of Gaussian noise and high-frequency noise. ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\bf x}_{\\mathrm{noise}}={\\bf x}_{\\mathrm{g}}+{\\bf x}_{\\mathrm{hf}}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The Gaussian noise is sampled through $\\mathbf{x_{g}}=\\mathcal{N}(0,\\sigma),\\sigma\\sim\\mathcal{U}(0.04,0.06)$ . The high-frequency noise is portrayed by a sinusoidal wave: ${\\mathbf{x}}_{\\mathrm{hf}}=a\\sin(2\\pi t+\\phi)$ , $t\\in[0,n_{\\mathrm{cycle}}]$ , $n_{\\mathrm{cycle}}\\in[2^{4},2^{5},2^{6}]$ , $a\\sim$ $\\mathcal{U}(0.08,0.10)$ , $\\phi\\sim\\mathcal{U}(0,2\\pi)$ . ", "page_idx": 18}, {"type": "text", "text": "F.1.4 Bias ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "The last step is to add bias to the previous synthesized sample, which is randomly sampled from a uniform distribution $\\mathbf{x}_{\\mathrm{bias}}\\sim\\mathcal{U}(-0.5,0.5)$ . ", "page_idx": 18}, {"type": "table", "img_path": "qu5NTwZtxA/tmp/fbbde5a5a23d0ab76ceb809bdec8f57c5297fa12bed7e3d35f6866ca0a9c8a23.jpg", "table_caption": [], "table_footnote": ["Table 6: Dataset description in our experiments. Some key information is listed above. The real-world datasets include Air Quality, Motor Imagery. All datasets have been carefully processed. $L$ indicates the time series length. The second column indicates all editable attributes. "], "page_idx": 19}, {"type": "text", "text": "F.2 Real World Datasets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "F.2.1 Air Quality Dataset ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "This dataset was used in the KDD Cup 2018 forecasting competition [8]. It contains lots of hourly sampled time series representing the air quality levels measured from different stations in 2 cities: Beijing (35 stations) and London (24 stations) from 01/01/2017 to 31/03/2018. This air quality level is represented in multiple measurements such as PM2.5, PM10, $\\mathrm{NO_{2}}$ , CO, $\\mathrm{{O_{3}}}$ , and $\\mathrm{{SO_{2}}}$ . ", "page_idx": 19}, {"type": "text", "text": "We processed the raw data and split it into train, validation, and test sets. We only selected the data of PM2.5, which is common in all stations. Season attributes are generated according to month. We slice the series by week to a length of 168 $(24\\mathrm{{hour}\\times7}\\mathrm{{day})}$ ). For pretraining dataset, there are totally 3650 time series samples, in which we randomly pick 2825 time series samples as train, 353 time series samples as validation, and 472 time series samples as the test. For finetuning dataset, it\u2019s composed of multi-subsets where each subset corresponds to an editing setting, e.g. modifying the \u201cseason\u201d. In any subset, a sample is composed of a source time series, source attributes, and target attributes where the source time series are sampled from the corresponding splits of pretraining data. For each source time series, we randomly change the source attributes 2 or 3 times to get the target attributes and pair the source time series and target attributes together. Finally, we get (train: 2000, valid: 600, test: 600) samples for modifying \u201ccity\u201d subset and (train: 3000, validation: 900, test: 900) samples for modifying \u201cseason\u201d subset. ", "page_idx": 19}, {"type": "text", "text": "F.2.2 Motor Imagery Dataset ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "The Motor Imagery dataset [1] contains some Electroencephalogram (EEG) records. During the Brain-Computer Interface experiments, the subjects have to perform imagined movements of either the tongue or the left small finger. All recordings are collected at a sampling rate of $1000\\,\\mathrm{{Hz}}$ with 64 channels. Every record contains 3000 time stamps (3 seconds measurement). For pertaining data, there are 19353 samples in the train set, 2419 in the validation set, 2419 in the test set. For fine-tuning data, the processing operation is similar to the Air Quality. In the subset of modifying movement, there are (train:2000, valid:1000, test:1000) samples. In the subset of modifying channel id, there are (train:3000, valid:1500, test:1500) samples. ", "page_idx": 19}, {"type": "text", "text": "G CTAP Model Details ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Our Contrastive Time-Series Attribute Pretraining (CTAP) model is similar to the popular CLIP model [33]. The purpose of this model is to train the time series encoder and the attribute encoder by learning the alignment between the time series $\\textbf{x}\\in\\mathbb{R}^{L}$ and its associated attributes $\\mathbf{a}\\in\\mathbb{N}_{+}^{K}$ . Note that we use separate attribute encoders for different attributes $a_{k}\\,=\\,\\mathbf{a}[k]$ , $k\\,\\in\\,\\{1,...,E\\}$ . An illustration of CTAP for a batch of $\\mathbf{X}\\in\\mathbb{R}^{B\\times L}$ and their associated $k$ -th attribute $\\mathbf{A}\\in\\mathbb{N}_{+}^{B}$ is presented in Fig. 10. Note that the attribute index $k$ is dropped for clarity. The time series encoder and attribute encoder extract the embeddings $\\mathbf{H}_{\\mathbf{x}}=\\{\\mathbf{h}_{\\mathbf{x}_{i}}\\}_{i=1}^{\\mathbf{\\hat{B}}^{\\dagger}}$ and $\\mathbf{H}_{a}=\\mathbf{\\dot{\\{h}}}_{a_{i}}\\mathbf{\\updownarrow}_{i=1}^{B}$ . Following [33], we calculate the pair-wise similarities between the $\\mathbf{h}_{\\mathbf{x}_{i}}$ and $\\mathbf{h}_{a_{j}}$ , and the encoders are trained by distinguishing whether $\\mathbf{h}_{\\mathbf{x}_{i}}$ and $\\mathbf{h}_{a_{j}}$ are from the same data pair. The pseudocode of the CTAP model is shown in Algorithm 2. ", "page_idx": 19}, {"type": "text", "text": "", "text_level": 1, "page_idx": 20}, {"type": "image", "img_path": "qu5NTwZtxA/tmp/61f1421d6232e1a01f24cda80d0d82ab805785df325998320329bcaba4302f71.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 10: Illustration of the CTAP model for the given pairs of time series $\\mathbf{X}\\,=\\,\\{\\mathbf{x}_{i}\\}_{i=1}^{B}$ and attributes $\\mathbf{A}=\\{a_{i}\\}_{i=1}^{B}$ , where $B$ is the batch size. Here $a=a_{k}=\\mathbf{a}[k]$ , $k\\in\\{1,...,K\\}$ is the $k$ -th attribute of the full attribute vector $\\mathbf{a}\\in\\mathbb{N}_{+}^{K}$ . We use $K$ separate attribute encoders for $K$ attributes. In the illustration, we only show one attribute and thus drop the attribute index $k$ for clarity. After obtaining embeddings $\\{\\mathbf{\\bar{h_{x}}}_{i}\\}_{i=1}^{B}$ and $\\{\\mathbf{h}_{a_{i}}\\}_{i=1}^{B}$ , we calculate the the pair-wise similarities between the $\\mathbf{h}_{\\mathbf{x}_{i}}$ and $\\mathbf{h}_{a_{j}}$ . The encoders are trained by distinguishing the positive pairs (green blocks) and negative pairs (white blocks). ", "page_idx": 20}, {"type": "text", "text": "Input: A batch of paired time series and attributes $\\mathbf{X}\\in\\mathbb{R}^{B\\times L}$ , $\\mathbf{A}\\in\\mathbb{R}^{B}$ ). ", "page_idx": 20}, {"type": "text", "text": "1: # 1. Extract embeddings of time series and attributes.   \n2: $\\mathbf{H}_{\\mathbf{x}}\\gets$ Time Series Encoder $(\\mathbf{X})$   \n3: ${\\bf H}_{a}\\gets$ Attribute Encoder(A)   \n4: # 2. Calculate pairwise similarities.   \n5: $\\mathbf{S}\\gets\\mathrm{Sim}(\\mathbf{H}_{\\mathbf{x}},\\bar{\\mathbf{H}}_{a})$ ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "text", "text": "\u25b7 $\\mathbf{S}\\in\\mathbb{R}^{B\\times B}$ is the similarity score matrix. ", "page_idx": 20}, {"type": "text", "text": "6: # 3. Calculate loss.   \n7: $\\mathcal{L}_{\\mathbf{x}}\\leftarrow$ Cross Entropy(S, I, axis = 1)   \n8: $\\mathcal{L}_{a}\\gets$ Cross Entropy(S, I, axis = 0)   \n9: $\\mathcal{L}\\gets(\\mathcal{L}_{\\mathbf{x}}+\\mathcal{L}_{a})/2$ ", "page_idx": 20}, {"type": "text", "text": "\u25b7 $\\mathbf{I}\\in\\{0,1\\}^{B\\times B}$ is the identity matrix. ", "page_idx": 20}, {"type": "table", "img_path": "qu5NTwZtxA/tmp/66a30f3ed63f8fd2c03503950a311399cc2903e1c48badf960a644931d74b851.jpg", "table_caption": [], "table_footnote": ["Table 7: The performance of CTAP models on different datasets. We report the top-1 and top-2 classification accuracy for each attribute on the test sets of the pertaining dataset. "], "page_idx": 20}, {"type": "text", "text": "H Evaluation Metrics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "H.1 MSE and MAE ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "For the synthetic dataset, MSE and MAE are two of the metrics used in our experiments. For the reason that we have targets in our synthetic datasets, MSE and MAE can be applied to evaluate the generated time series. The two formulas are listed below: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\mathbf{MSE}=\\frac{1}{|\\mathcal{D}|}\\sum_{i=1}^{|\\mathcal{D}|}(\\mathbf{x}_{i}-\\hat{\\mathbf{x}}_{i})^{2}}\\\\ {\\displaystyle\\mathbf{MAE}=\\frac{1}{|\\mathcal{D}|}\\sum_{i=1}^{|\\mathcal{D}|}\\left|\\mathbf{x}_{i}-\\hat{\\mathbf{x}}_{i}\\right|}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Here $|\\mathcal D|$ is the size of the test set, $\\mathbf{x}_{i}$ is the $i$ -th target time series, and $\\hat{\\mathbf{x}}_{i}$ is the corresponding generated time series. ", "page_idx": 21}, {"type": "text", "text": "H.2 CTAP ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "The CTAP score is obtained through the Contrastive Time series - Attribute Pretraining (CTAP) model, which is similar to the popular CLIP model [33]. The details of the CTAP model are given in Appendix G. Suppose $a_{k}^{\\mathrm{tgt}}$ is one of the target attributes, the CTAP score is similar to the CLIP-I score [23], which uses the pretrained CTAP model to measure the alignment, e.g., cosine similarity, between the generated $\\hat{\\mathbf{x}}^{\\mathrm{tgt}}$ and the real-world time series that also associated with the attribute $a_{k}^{\\mathrm{tgt}}$ . Specifically, we first use the CTAP model to extract embeddings $\\hat{\\mathbf{h}}_{\\mathbf{x}}$ , which is the embedding of $\\hat{\\mathbf{x}}^{\\mathrm{tgt}}$ , and $\\bar{\\mathbf{h}}_{\\mathbf{x}}|a_{k}^{\\mathrm{tgi}}$ , which is the average embedding for the time series associated with $a_{k}^{\\mathrm{tgt}}$ in the training data. Then we calculate the cosine similarity of $\\hat{\\mathbf{h}}_{\\mathbf{x}}$ and $\\bar{\\mathbf{h}}_{\\mathbf{x}}|a_{k}^{\\mathrm{tgt}}$ , where the higher similarity indicates better alignment between the generated and real-world time series. ", "page_idx": 21}, {"type": "text", "text": "H.3 RaTS ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We introduce Log Ratio of Target-to-Source probability (RaTS) score to measure whether the generated time series $\\hat{\\mathbf{x}}^{\\mathrm{tgt}}$ is closer to the target attribute $a_{k}^{\\mathrm{tgt}}$ than the source time series $\\mathbf{x}^{\\mathrm{{src}}}$ . Formally, the RaTS score for a tuple $(\\hat{\\mathbf{x}}^{\\mathrm{tgt}},\\mathbf{x}^{\\mathrm{src}},a_{k}^{\\mathrm{tgt}})$ is defined as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathrm{RaTS}(\\hat{\\mathbf{x}}^{\\mathrm{tgt}},\\mathbf{x}^{\\mathrm{src}},a_{k}^{\\mathrm{tgt}})=\\log(\\frac{p(a_{k}^{\\mathrm{tgt}}|\\hat{\\mathbf{x}}^{\\mathrm{tgt}})}{p(a_{k}^{\\mathrm{tgt}}|\\mathbf{x}^{\\mathrm{src}})}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $p(a_{k}|\\mathbf{x})$ is calculated by applying a softmax over the similarity scores of all $(\\mathbf{h}_{\\mathbf{x}},\\mathbf{h}_{a_{k}=i})$ pairs, where $\\mathbf{h}_{\\mathbf{x}}$ is the CTAP embedding of $\\mathbf{x}$ , and $\\mathbf{h}_{a_{k}=i}$ , $i\\in\\{1,...,N_{k}\\}$ , is the CTAP embedding of the $i$ -th possible value for the $k$ -th attribute $a_{k}$ . ", "page_idx": 21}, {"type": "text", "text": "$\\mathrm{RaTS}>0$ represents the model doesn\u2019t retain characteristics of the source time series $\\mathbf{x}^{\\mathrm{{src}}}$ to a certain degree. $\\mathrm{RaTS}<0$ means that the attribute $a_{k}^{\\mathrm{tgt}}$ is reinforced compared to the source $\\mathbf{x}^{\\mathrm{{src}}}$ . For the edited attributes $A_{\\mathrm{edit}}$ , the higher the RaTS score, the closer the edited $\\hat{\\mathbf{x}}^{\\mathrm{tgt}}$ is to the target attributes. For the preserved attributes $A_{\\mathrm{prsv}}$ , the lower the |RaTS| score, the better the preservation of the attributes. ", "page_idx": 21}, {"type": "text", "text": "I More Experimental Results ", "text_level": 1, "page_idx": 22}, {"type": "table", "img_path": "qu5NTwZtxA/tmp/d579e4624ce53c930452aeeb05e5cbb93b1de7ddcff8a2dd7b9a375e812a9950.jpg", "table_caption": ["I.1 Results "], "table_footnote": ["Table 8: Performance on all subsets of attributes combination of Synthetic, \u25b2/\u25b3 denote edited/preserved attributes. "], "page_idx": 22}, {"type": "table", "img_path": "qu5NTwZtxA/tmp/a1f89f76443d74fff08ba22f8998b3fd66db1c61852617eab67b0534a8dfe2d5.jpg", "table_caption": [], "table_footnote": ["Table 9: Performance on all subsets of attributes combination of Motor and Air, $\\pmb{\\triangle}/\\triangle$ denote edited/preserved attributes. "], "page_idx": 22}, {"type": "text", "text": "I.2 Sensitivity Study for multi-resolution ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "In this section, we show the impact of different Multi-resolution parameters on different attributes in Fig. 11. All experiments are performed on 6 subsets of Synthetic, including editing [trend type, trend direction, season cycles, trend type $\\&$ trend direction, trend type $\\&$ season cycles, trend direction & season cycles]. To analyze the influence of $R$ , we fix $L_{p}=2$ and vary $R$ . To examine the impact of $L_{p}$ , we fix $R=3$ and vary $L_{p}$ . We found that different attributes exhibit varying preferences for multi-resolution parameters across different subsets. This observation supports the motivation for proposing a multi-resolution approach: distinct attributes exert different scales of influence on time series data. ", "page_idx": 23}, {"type": "image", "img_path": "qu5NTwZtxA/tmp/f9afddd16ffce43d0e7bb1933d8d437d6a8179d15852233d029a8ad106b56bb2.jpg", "img_caption": ["Figure 11: The impact of multi-resolution on 6 Synthetic subsets. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "In this section, we demonstrate the superiority of our approach by visualizing the results of our method with a baseline on editing tasks. As shown in Fig. 12, we compare our method and baselines on Synthetic dataset. The left column compares the TEdit-CSDI and CSDI while the right column compares the TEdit-TW and Time Weaver. Each row represents different attribute settings, containing: modifying trend type, modifying trend direction, modifying season cycles, modifying trend type and trend direction, modifying trend type and season cycles, modifying trend direction and season cycles. It\u2019s obvious that the editing results of our method are much closer to the target time series with lower MSE and MAE, proving the superiority of our method. ", "page_idx": 24}, {"type": "image", "img_path": "qu5NTwZtxA/tmp/70b645daece47c2a8c1a8d43c1d548e41739ab468bae79a1241e6f4d080240ec.jpg", "img_caption": ["Figure 12: More cases of different attribute subsets of Synthetic dataset "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Here we provide more comparison results of conditional generation and editing in Fig. 13. The editing results are closer to the target time series with lower MSE and MAE. Although the time series generated by condition generation also meets the requirement of the attributes, it still lost the detailed information of input while the editing process better retains the characteristics of the time series to preserve and manipulate only the target attribute to change. Therefore, the editing can achieve a more precise control compared with conditional generation. ", "page_idx": 25}, {"type": "image", "img_path": "qu5NTwZtxA/tmp/69073c7a27ff5be8ad85ced227c541d29865c335b73500a1ff85e438b28f3168.jpg", "img_caption": ["Figure 13: Comparison between editing and conditional generation "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "I.5 Bootstrap influence on different attributes ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "To further explore the impact of bootstrapping on data coverage, we visualize the effect of bootstrapping data on different attributes to complement the original data. Fig. 14,15,16 present the results on trend types, and season cycles, trend directions, respectively. According to the classification accuracy of the TAP model and case study, we can infer that trend directions are a relatively simple pattern for modeling while the trend types and season cycles are more complex. The data coverage improvement brought by bootstrapping is more obvious when the attribute is complex. ", "page_idx": 25}, {"type": "image", "img_path": "qu5NTwZtxA/tmp/b6af76fe68948a1113719213d3d847220b3b4c937f9e843ef4c5c4351e58e3f6.jpg", "img_caption": ["Figure 14: The influence of bootstrapping on different trend types "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "qu5NTwZtxA/tmp/de52e4091ba0cfe8affba9e9034f09f0ccb7633abaa2a260395c077826430acb.jpg", "img_caption": ["Figure 15: The influence of bootstrapping on different season cycles "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "qu5NTwZtxA/tmp/f7242ea9ef088d4d4967032461436ef6d870e49f47f1c9fef1e036800283071f.jpg", "img_caption": ["Figure 16: The influence of bootstrapping on different trend directions "], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper has clearly claimed the contributions and method scope in Abstract and Introduction parts. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or intr2oduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 27}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: This paper has discussed the limitations in Conclusion part. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 27}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: This paper does not include theoretical results. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: This paper has claimed the details of both experimental settings and model implementations. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The code and data are released with paper. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 29}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The training and test details have been provided in the appendix. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 29}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: Standard deviations are provided in Appendix. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The information about the computer resources have been claimed in the appendix. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 30}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: The research conform with the NeurIPS Code of Ethics https://neurips.   \ncc/public/EthicsGuidelines. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 30}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: This paper has discussed broader impacts in appendix. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "", "page_idx": 31}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This paper does not release models or real-world datasets, all the experiments are based on synthetic data and public datasets. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 31}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: All the assets used in this paper are open-source materials and are properly credited. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 32}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 32}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 32}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 32}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 32}]