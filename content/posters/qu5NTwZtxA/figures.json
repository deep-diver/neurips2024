[{"figure_path": "qu5NTwZtxA/figures/figures_1_1.jpg", "caption": "Figure 1: Illustration of the time series generation paradigms. Conditional generation generates time series from scratch, which usually generates samples around the dataset mean. Time series editing allows for the manipulation of the attributes of an input time series sample, which aligns with the desired target attribute values while preserving other properties.", "description": "This figure illustrates the difference between two time series generation paradigms: conditional generation and time series editing. Conditional generation synthesizes time series from scratch, resulting in samples clustered around the data mean. In contrast, time series editing modifies an existing time series sample, adjusting specific attributes to target values while preserving other characteristics.  The figure uses the example of air quality in London during spring and summer to visually represent how each method operates.", "section": "1 Introduction"}, {"figure_path": "qu5NTwZtxA/figures/figures_3_1.jpg", "caption": "Figure 2: Illustration of the editing function \u03a6\u03b8 (upper) and the process of self-scoring the generated for bootstrap learning (lower). Upper: \u03a6\u03b8(xsrc, asrc, atgt) first encodes the source (xsrc, asrc) into the latent viable x via the forward DDIM Eq.(4), and then decodes x with the target attributes atgt: (xsrc, atgt) into x tgt via the reverse DDIM in Eq.(5). See Sec. 3.2 for more details. Lower: during bootstrap learning, we use \u03a6\u03b8 to self-score the generated x tgt by editing it back to xsrc, and obtain the score s = MSE(xsrc, xsrc), see Sec. 3.4 for more details.", "description": "This figure illustrates the two-stage process of the Time Series Editor (TEdit) model. The upper part shows how the model encodes the source time series and its attributes into a latent representation using a forward diffusion process, then decodes the latent representation with the target attributes to generate the edited time series using a reverse diffusion process. The lower part shows the bootstrap learning process, where the generated time series is self-scored by editing it back to the original time series to improve the model's performance.", "section": "3.2 Editing with Source Modeling and Target Generation"}, {"figure_path": "qu5NTwZtxA/figures/figures_4_1.jpg", "caption": "Figure 3: Architecture of the proposed multi-resolution noise estimator \u03b5\u03b8. We illustrate with R = 3 patching schema, patch length Lp = 2\u22121,r \u2208 {1, ..., R} and the input length L = 8. N = [L/Lp]r is the patch number. D is the embedding size. Please refer to Sec. 3.3 for details.", "description": "This figure shows the architecture of the multi-resolution noise estimator, a key component of the TEdit model.  The input noisy time series is first patchified into multiple sequences of different resolutions. These sequences, along with input attributes and diffusion steps, are processed by a multi-patch encoder and a multi-patch decoder.  A processing module captures multi-scale associations between time series and attributes. Finally, a multi-layer perceptron (MLP) combines the estimations from different resolutions to produce the final estimated noise.", "section": "3.3 Multi-Resolution Noise Estimator"}, {"figure_path": "qu5NTwZtxA/figures/figures_8_1.jpg", "caption": "Figure 1: Illustration of the time series generation paradigms. Conditional generation generates time series from scratch, which usually generates samples around the dataset mean. Time series editing allows for the manipulation of the attributes of an input time series sample, which aligns with the desired target attribute values while preserving other properties.", "description": "This figure compares two time series generation methods: conditional generation and time series editing. Conditional generation synthesizes time series from scratch, resulting in samples clustered around the data mean, lacking fine-grained control.  In contrast, time series editing modifies an existing time series sample to match specified attribute values while preserving other characteristics. This allows for more precise control and aligns better with \"what-if\" scenarios in time series analysis.", "section": "1 Introduction"}, {"figure_path": "qu5NTwZtxA/figures/figures_9_1.jpg", "caption": "Figure 5: Visualization of data distribution before and after bootstrapping.", "description": "This figure visualizes the impact of bootstrapping on the distribution of data in the attribute space. It uses t-SNE to reduce the dimensionality of the data and shows the distribution before bootstrapping (original data), the distribution after bootstrapping (bootstrapping data), and a combined visualization of both.  The visualizations help illustrate how bootstrapping expands the coverage of the attribute space by generating samples in previously under-represented regions.", "section": "4.5 Sensitivity Analysis"}, {"figure_path": "qu5NTwZtxA/figures/figures_9_2.jpg", "caption": "Figure 11: The impact of multi-resolution on 6 Synthetic subsets.", "description": "The figure shows the impact of multi-resolution parameters (number of resolutions R and patch length Lp) on different attributes in 6 synthetic subsets.  Each subset involves editing a different combination of attributes: trend type, trend direction, and season cycles. The results (CTAP scores) are presented as bar charts, showing the performance for each attribute across different resolutions and patch lengths. This visualization demonstrates that different attributes have varying sensitivities to different multi-resolution configurations.", "section": "I.2 Sensitivity Study for multi-resolution"}, {"figure_path": "qu5NTwZtxA/figures/figures_15_1.jpg", "caption": "Figure 3: Architecture of the proposed multi-resolution noise estimator  \u03b5\u03b8. We illustrate with R = 3 patching schema, patch length Lp = 2\u22121,r \u2208 {1, ..., R} and the input length L = 8. N = [L/Lp]r is the patch number. D is the embedding size. Please refer to Sec. 3.3 for details.", "description": "The figure depicts the architecture of the multi-resolution noise estimator used in the Time Series Editor (TEdit) model. It shows how the input noisy time series is processed through multiple resolutions using patching, encoding, processing, and decoding modules to estimate the noise.  The different resolutions allow the model to capture multi-scale relationships between the time series and attributes.", "section": "3.3 Multi-Resolution Noise Estimator"}, {"figure_path": "qu5NTwZtxA/figures/figures_16_1.jpg", "caption": "Figure 3: Architecture of the proposed multi-resolution noise estimator  \u03b5\u03b8. We illustrate with R = 3 patching schema, patch length Lp = 2\u22121,r \u2208 {1, ..., R} and the input length L = 8. N = [L/Lp]r is the patch number. D is the embedding size. Please refer to Sec. 3.3 for details.", "description": "This figure shows the architecture of the multi-resolution noise estimator, a key component of the TEdit model.  It uses a multi-patch encoder and decoder to process time series data at multiple resolutions, capturing multi-scale relationships between the time series and its attributes.  The input is a noisy time series, attributes, and the diffusion step.  The output is the estimated noise at different resolutions.  These are then combined using a multi-layer perceptron (MLP).", "section": "3.3 Multi-Resolution Noise Estimator"}, {"figure_path": "qu5NTwZtxA/figures/figures_20_1.jpg", "caption": "Figure 10: Illustration of the CTAP model for the given pairs of time series X = {x}B=1 and attributes A = {ai}B=1, where B is the batch size. Here ai = ak = a[k], k \u2208 {1, ..., K} is the k-th attribute of the full attribute vector a \u2208 NK. We use K separate attribute encoders for K attributes. In the illustration, we only show one attribute and thus drop the attribute index k for clarity. After obtaining embeddings {hx\u2081}B=1 and {ha\u2081}B=1, we calculate the the pair-wise similarities between the hx, and ha,. The encoders are trained by distinguishing the positive pairs (green blocks) and negative pairs (white blocks).", "description": "This figure illustrates the architecture of the Contrastive Time series Attribute Pretraining (CTAP) model.  The model takes pairs of time series and their corresponding attributes as input. It uses separate encoders for time series and attributes to extract their embeddings.  The model then calculates pairwise similarities between the time series and attribute embeddings and is trained to distinguish positive (matching) and negative (non-matching) pairs. This training process helps the model learn the alignment between time series and attributes, which is useful for evaluating the quality of time series editing.", "section": "G CTAP Model Details"}, {"figure_path": "qu5NTwZtxA/figures/figures_23_1.jpg", "caption": "Figure 3: Architecture of the proposed multi-resolution noise estimator  . We illustrate with R = 3 patching schema, patch length Lp = 2\u22121,r \u2208 {1, ..., R} and the input length L = 8. N = [L/Lp] is the patch number. D is the embedding size. Please refer to Sec. 3.3 for details.", "description": "This figure shows the architecture of the multi-resolution noise estimator, a key component of the TEdit model.  It illustrates how the model processes the input noisy time series at multiple resolutions (R=3 in this example) to capture multi-scale relationships between time series and attributes.  The input time series is divided into patches of varying lengths, processed through encoder and decoder modules, and finally combined to produce the estimated noise. The figure highlights the multi-patch encoder, multi-patch decoder, and processing module, illustrating the multi-resolution paradigm.", "section": "3.3 Multi-Resolution Noise Estimator"}, {"figure_path": "qu5NTwZtxA/figures/figures_24_1.jpg", "caption": "Figure 2: Illustration of the editing function \u03a6\u03b8 (upper) and the process of self-scoring the generated for bootstrap learning (lower). Upper: \u03a6\u03b8(xsrc, asrc, atgt) first encodes the source (xsrc, asrc) into the latent viable x via the forward DDIM Eq.(4), and then decodes x with the target attributes atgt: (xsrc, atgt) into xtgt via the reverse DDIM in Eq.(5). See Sec. 3.2 for more details. Lower: during bootstrap learning, we use \u03a6\u03b8 to self-score the generated xtgt by editing xtgt back to xsrc, and obtain the score s = MSE(xsrc, xsrc), see Sec. 3.4 for more details.", "description": "This figure illustrates the two-stage process of the Time Series Editor (TEdit) model. The upper part shows how TEdit modifies the input time series by encoding it into a latent representation and then decoding it with target attributes using the forward and reverse DDIM processes. The lower part illustrates the self-scoring process used in bootstrap learning. This process involves using TEdit to generate a new time series, editing it back to the original input, and comparing it to the original using Mean Squared Error (MSE).", "section": "3.2 Editing with Source Modeling and Target Generation"}, {"figure_path": "qu5NTwZtxA/figures/figures_25_1.jpg", "caption": "Figure 1: Illustration of the time series generation paradigms. Conditional generation generates time series from scratch, which usually generates samples around the dataset mean. Time series editing allows for the manipulation of the attributes of an input time series sample, which aligns with the desired target attribute values while preserving other properties.", "description": "This figure illustrates two different approaches to generating time series data: conditional generation and time series editing.  Conditional generation starts from scratch, creating entirely new time series based on learned data distributions.  The generated samples tend to cluster around the average characteristics of the training data. In contrast, time series editing takes an existing time series as input and modifies specific attributes (e.g., changing the season from spring to summer) while preserving other properties. This allows for a more targeted and controllable synthesis process, enabling the manipulation of existing samples rather than generating entirely new ones.", "section": "1 Introduction"}, {"figure_path": "qu5NTwZtxA/figures/figures_26_1.jpg", "caption": "Figure 2: Illustration of the editing function \u03a6\u03b8 (upper) and the process of self-scoring the generated for bootstrap learning (lower). Upper: \u0424(xrc, asrc, atgt) first encodes the source (x, asrc) into the latent viable x via the forward DDIM Eq.(4), and then decodes x with the target attributes atgt: (xsrc, atgt) into via the reverse DDIM in Eq.(5). See Sec. 3.2 for more details. Lower: during bootstrap learning, we use \u03a6e to self-score the generated by editing back to xsrc, and obtain the score s = MSE(, xsrc), see Sec. 3.4 for more details.", "description": "This figure illustrates the Time Series Editor (TEdit) framework. The upper panel shows the two-stage editing process: forward diffusion to encode the source time series and attributes into a latent variable, followed by reverse diffusion to decode the latent variable with target attributes, generating the edited time series. The lower panel illustrates the self-scoring mechanism used in bootstrap learning, where the generated time series is edited back to the original using the same process, and the mean squared error (MSE) between the original and edited time series serves as the score for evaluating the generated time series.", "section": "3.2 Editing with Source Modeling and Target Generation"}, {"figure_path": "qu5NTwZtxA/figures/figures_26_2.jpg", "caption": "Figure 3: Architecture of the proposed multi-resolution noise estimator \u03f5\u03b8. We illustrate with R = 3 patching schema, patch length Lp = 2\u22121,r \u2208 {1, ..., R} and the input length L = 8. N = [L/Lp] is the patch number. D is the embedding size. Please refer to Sec. 3.3 for details.", "description": "This figure shows the architecture of the multi-resolution noise estimator used in the Time Series Editor (TEdit) model.  It illustrates how the model processes the input time series at multiple resolutions (R=3 in this example) using patches of varying lengths (Lp). Each resolution's patches are encoded and processed separately, capturing different levels of detail in the time series data.  The processed information is then combined to produce a final noise estimation which is used in the diffusion process.", "section": "3.3 Multi-Resolution Noise Estimator"}, {"figure_path": "qu5NTwZtxA/figures/figures_26_3.jpg", "caption": "Figure 5: Visualization of data distribution before and after bootstrapping.", "description": "This figure visualizes the effect of bootstrapping on the data distribution using t-SNE.  It compares the distribution of the original data, the generated data after training with only the original data, and the distribution of both the original and the generated data combined. The visualization aims to show how the bootstrapping process improves the data coverage and fills in gaps in the attribute space. The improved coverage is expected to improve the model's ability to edit time series attributes.", "section": "4.5 Sensitivity Analysis"}]