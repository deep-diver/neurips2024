[{"type": "text", "text": "Breaking Determinism: Fuzzy Modeling of Sequential Recommendation Using Discrete State Space Diffusion Model ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Wenjia Xie Hao Wang\u00a7\u2217Luankang Zhang Rui Zhou Defu Lian Enhong Chen ", "page_idx": 0}, {"type": "text", "text": "University of Science and Technology of China & State Key Laboratory of Cognitive Intelligence {xiaohulu,wanghao3,zhanglk5,zhou_rui,liandefu,cheneh}@mail.ustc.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sequential recommendation (SR) aims to predict items that users may be interested in based on their historical behavior sequences. We revisit SR from a novel information-theoretic perspective and find that conventional sequential modeling methods fail to adequately capture the randomness and unpredictability of user behavior. Inspired by fuzzy information processing theory, this paper introduces the DDSR model, which uses fuzzy sets of interaction sequences to overcome the limitations and better capture the evolution of users\u2019 real interests. Formally based on diffusion transition processes in discrete state spaces, which is unlike common diffusion models such as DDPM that operate in continuous domains. It is better suited for discrete data, using structured transitions instead of arbitrary noise introduction to avoid information loss. Additionally, to address the inefficiency of matrix transformations due to the vast discrete space, we use semantic labels derived from quantization or RQ-VAE to replace item IDs, enhancing efficiency and improving cold start issues. Testing on three public benchmark datasets shows that DDSR outperforms existing state-of-the-art methods in various settings, demonstrating its potential and effectiveness in handling SR tasks. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "For a long time, sequential recommendation (SR) has been attracting increasing attention due to its excellent performance and significant commercial value (Chen et al. [2020], Qiu et al. [2021], Yin et al. [2024]). Unlike traditional collaborative filtering or certain graph-based methods (Wang et al. [2019]), SR systems emphasize the inherent dynamic behaviors of users rather than relying solely on structured data (Chen et al. [2022], Ma et al. [2020], Cen et al. [2020]). This approach enhances the accuracy of personalized recommendations, allowing for more precise tracking of changes in user interests and needs. Typical deep learning-based SR models, such as those utilizing CNN, RNN, and Transformer architectures (Tang and Wang [2018], Hidasi et al. [2015], Kang and McAuley [2018b]), have achieved remarkable success in modeling user historical interaction data. ", "page_idx": 0}, {"type": "text", "text": "However, these methods are formalized models based on a narrow information theory assumption (Shannon [1948]), which only acknowledges determinism (Rosas et al. [2020]). They assume that all phenomena strictly adhere to mechanical laws and that the states of motion of objects at different times can be uniquely determined. In reality, however, user behavior is characterized by randomness and unpredictability. They might change their mind about buying a down jacket due to a sudden warm-up, or they might impulsively buy desserts due to a breakup. As illustrated on the left in Figure 1, a user\u2019s interest at any given moment might be focused on \u2019some items\u2019 with blurred boundaries, only converging finally when the user makes a selection. ", "page_idx": 0}, {"type": "text", "text": "Although increasing the sample size is an effective strategy to address the above issue, in reality, the data in recommendation systems is usually quite sparse (He and McAuley [2016]), limiting the practicality of this strategy. Inspired by the theory of fuzzy information processing (Tanaka et al. [1976], Tanaka and Sommer [1977]), we believe that making the absolute membership relations in traditional sets more flexible is another effective way to solve the problem. In other words, it is not necessary to strictly limit the modeling of user interests to the items they have interacted with. Therefore, we propose using a diffusion model (Ho et al. [2020]) for fuzzy modeling of user interests, which enhances the model\u2019s performance by introducing perturbations during the training process. ", "page_idx": 0}, {"type": "image", "img_path": "h3BdT2UMWQ/tmp/085de101c9f7e8b684fe436927715bc416337cfdf528e3d1010b3bb8e35f9951.jpg", "img_caption": ["Figure 1: Illustration of DDSR constructing fuzzy sets and incorporating semantic IDs to enhance sequential recommendations. In real-world scenarios, a user\u2019s final choice often reflects their immediate interests (left subfigure). We reconstruct the true evolution of interests by constructing fuzzy sets for each item in the interaction sequence (middle subfigure). The right subfigure provides an overview of the process of generating semantic IDs for recommendations based on item-related descriptions. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "We have noticed existing work that introduces diffusion models into SR (Xie et al. [2024]), such as DiffuRec (Li et al. [2023b]) and DreamRec (Yang et al. [2023]), which focus on Gaussian diffusion processes operating within a continuous state space. They add Gaussian noise to the embedded representations of candidate items for recommendation through a forward diffusion process until the noise reaches a pure state (standard normal distribution). Subsequently, they iteratively sample from this noise using a reverse denoising process guided by historical interaction information to recover meaningful representations and recommend items most similar to these representations. ", "page_idx": 1}, {"type": "text", "text": "However, unlike our desire to fuzzily model interaction sequences, the aforementioned methods follow the form of diffusion models in the image domain and operate on candidate items. They introduce the crucial sequence information merely as conditional information, without leveraging the diffusion model\u2019s performance on it. On the other hand, these methods relax discrete interaction data into a continuous space and introduce noise, which may lead to distortion or loss of meaning in the original discrete space, as the addition of noise could push data points away from any meaningful discrete state. Therefore, we hope that state transitions occur under discrete conditions for the entire interaction sequence, which is discrete diffusion. Based on this, we have proposed our DDSR (Discrect Diffusion Sequential Recommendation model), which uses a directed graph to model sequential recommendation. In this model, all interaction items are viewed as nodes, and transitions between items are treated as directed edges. Discrete diffusion is used to enable structured transitions of nodes, with the resulting new sets treated as fuzzy sets, as shown in the middle of Figure 1. By designing the transition matrix, we can achieve uniform transitions or importance-based transitions for the nodes, ensuring controllability. In Section 3.3, we theoretically demonstrate the reliability and effectiveness of modeling on these generated fuzzy sets, based on the principles of information diffusion. During the inference stage, we refer to the sampling formula for discrete diffusion but start from the historical interaction sequence rather than from noise, iteratively generating refined results. ", "page_idx": 1}, {"type": "text", "text": "Furthermore, we have found that the excessive number of items involved in the recommendation problem leads to a high-dimensional transition matrix, resulting in inefficient diffusion transitions. Additionally, item IDs themselves do not contain any prior information, which poses a challenge in determining beneficial transition directions. To address this issue, we have further introduced semantic tags to replace meaningless item IDs, using quantization techniques and VQ-VAE to derive these tags from semantic information, thus reducing the size of the discrete space. We will provide specific details on how this can be achieved in 4.1, and a vivid illustration of this is given on the right side of Figure 1. Simultaneously, the introduction of semantic information has enhanced the model\u2019s generalization capability and effectively solved the cold start problem. We conducted extensive experiments on three public benchmark datasets, comparing DDSR with several state-of-the-art methods. The results demonstrate that DDSR significantly outperforms baseline methods in various settings and effectively handles cold-start recommendations. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Sequence Recommendation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "SR suggests potential subsequent items based on users\u2019 historical interaction records (Chen et al. [2022], Shen et al. [2024]). Early research primarily relied on Markov chains and matrix factorization techniques for recommendation (He et al. [2016]). However, with the development of deep learning, efforts such as GRU4Rec (Hidasi et al. [2015]), Caser (Tang and Wang [2018]), and others have focused on designing neural network models to capture sequential dependencies in user behavior sequences. The introduction of the Transformer architecture (Vaswani et al. [2017]) in SASRec (Kang and McAuley [2018b]) pioneered SR and quickly became the mainstream method in the field. Additionally, BERT4Rec (Sun et al. [2019]) utilizes bidirectional encoders to capture bidirectional dependencies in sequences, using a masked language model to predict the user\u2019s next action. ", "page_idx": 2}, {"type": "text", "text": "Recent studies have shown that high-quality high-dimensional embeddings are crucial for obtaining accurate recommendation results (Hou et al. [2022]). To this end, researchers are striving to leverage the rich attribute information of items to improve data representation. For example, TransFM (Pasricha and McAuley [2018]) introduces arbitrary real-valued features through factorization machines, while S3-Rec (Zhou et al. [2020]) designs four self-supervised learning tasks as pre-training objectives to learn context-aware data representations with attribute awareness. Furthermore, researchers like Hou et al. [2022], Zhao [2022], Harte et al. [2023] further utilize pre-trained language models to process item description texts, obtaining universal item representations with rich semantic information to enhance the performance. VQ-Rec (Hou et al. [2023]) and TIGER (Rajput et al. [2024]) further employ quantization techniques (Jacob et al. [2018]) and RQ-VAE (Lee et al. [2022]) to obtain tokenized semantic IDs for recommendations, replacing semantic embeddings. ", "page_idx": 2}, {"type": "text", "text": "2.2 Discrete Diffusion Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Diffusion models, inspired by non-equilibrium thermodynamics, have been introduced and demonstrated significant results in fields such as computer vision, sequence modeling, and audio processing (Dhariwal and Nichol [2021], Rasul et al. [2021], Ho et al. [2022]). Most diffusion models are based on the Denoising Diffusion Probabilistic Model (DDPM) proposed by Ho et al. [2020], as well as the score-based generative models (SGMs) proposed by Song et al. [2020], targeting continuous data domains. We provide detailed descriptions of DDPM and SGMa in Appendix D to facilitate comparisons with the discrete diffusion approach we employ. Diffusion models in discrete state space are first described in Sohl-Dickstein et al. [2015] and later applied to text and image domains in D3PMs (Austin et al. [2021]). VQ-Diffusion (Gu et al. [2022]) utilizes them to eliminate unidirectional bias in text-to-image generation. ", "page_idx": 2}, {"type": "text", "text": "3 Discrete Diffusion Process of DDSR ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this section, we present the problem definition (Section 3.1) and illustrate how item sequences undergo discrete diffusion to obtain the corresponding fuzzy sets (Section 3.2). Finally, the effectiveness of this fuzzy modeling is theoretically demonstrated (Section 3.3). Please note that the actual diffusion and inference in DDSR occur at the semantic ID level, but this chapter discusses items. ", "page_idx": 2}, {"type": "text", "text": "3.1 Problem Statement ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Let $\\boldsymbol{\\mathcal{U}}$ be the set of users and $\\mathcal{V}$ be the set of discrete items in the dataset, $|\\mathcal{U}|$ and $|\\gamma|$ represent the number of elements in their respective sets. For each user $\\mathbf{u}\\in\\mathcal{U}$ , $v_{1:n-1}=\\left[v_{1},v_{2},\\ldots,v_{n-1}\\right]$ represents his historical interaction sequence sorted by timestamp. The goal of the model is to predict the next item $v_{n}$ that the user is most likely to interact with. To facilitate better discrete diffusion and for the convenience of subsequent theoretical derivations, we model each user\u2019s interaction sequence as a directed graph $g^{u}$ . In this graph, each item represented by a semantic ID is regarded as a node, while transitions between items are viewed as directed edges. Specifically, an edge exists from $v_{i}$ to $v_{j}$ if and only if $v_{j}$ is the next item interacted with by the user after $v_{i}$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "3.2 Node Diffusion Transition ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "A typical diffusion model transforms data $x_{0}\\sim q(x_{0})$ into a sequence of gradually noisier latent variables $x_{1:T}=x_{1},x_{2},...,x_{T}$ via forward process $\\begin{array}{r}{q(\\pmb{x}_{1:T}|\\pmb{x}_{0})=\\prod_{t=1}^{T}q(\\pmb{x}_{t}|\\pmb{x}_{t-1})}\\end{array}$ . In diffusion models within continuous state spaces, the forward distribution is t ypically set with $q(\\pmb{x}_{t}|\\pmb{x}_{t-1})=$ $\\mathcal{N}\\left(\\pmb{x}_{t}|\\sqrt{1-\\beta_{t}}\\pmb{x}_{t-1},\\beta_{t}\\pmb{I}\\right)$ as a hyperparameter controlling the level of noise added at each step. As the number of time steps $T$ approaches infinity, $x_{T}$ converges to a standard Gaussian distribution. Beyond the limitations mentioned above, information loss due to diffusion into pure noise is another reason for unstable training and inadequate alignment of continuous diffusion with SR. ", "page_idx": 3}, {"type": "text", "text": "Continuous diffusion always operates on embeddings, while in diffusion models within discrete state spaces, categories are directly transformed. Transition matrices $[Q_{t}]_{i j}=q(x_{t}=j|x_{t-1}=i)$ are used to describe the probability of single-step diffusion transitions, where $i$ and $j$ represent categories within the domain. Denoting the one-hot version of $x$ with the row vector $\\textbf{\\em x}$ (bold), then the one-step transition probabilities can be expressed as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nq(\\pmb{x}_{t}|\\pmb{x}_{t-1})=\\mathrm{Cat}(\\pmb{x}_{t};\\pmb{p}=\\pmb{x}_{t-1}\\pmb{Q}_{t}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $C a t({\\boldsymbol{x}};{\\boldsymbol{p}})$ is the categorical distribution corresponding to the one-hot row vector $\\textbf{\\em x}$ with probabilities given by the row vector $p$ , and $\\pmb{x}_{t-1}\\pmb{Q}_{t}$ is understood as a row vector-matrix product. Starting from $\\scriptstyle x_{0}$ , we obtain the following $t$ -step marginal and posterior at time $t-1$ : ", "page_idx": 3}, {"type": "equation", "text": "$$\nq(\\pmb{x}_{t}|\\pmb{x}_{0})=\\mathbf{C}\\mathbf{at}\\left(\\pmb{x}_{t};\\pmb{p}=\\pmb{x}_{0}\\pmb{{Q}}_{t}\\right),\\quad\\mathrm{with}\\quad\\pmb{{Q}}_{t}=\\pmb{Q}_{1}\\pmb{Q}_{2}\\ldots\\pmb{Q}_{t},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We take the set $\\mathcal{V}$ as the domain in SR. Each $v_{i}$ in the interaction sequence is represented as a one-hot encoding $\\pmb{x}_{i}^{0}$ . Using the transition form defined by 2 enables transitions to other nodes, denoted as $\\pmb{x}_{i}^{t}$ in the domain with a certain probability at any time step $t$ . Unlike continuous diffusion, which only allows noise addition, discrete diffusion models offer the advantage of controlling the data blurring process by selecting the transition matrix. Here, we present two strategies to select transition matrices, i.e. Uniform transition and Importance transition. ", "page_idx": 3}, {"type": "text", "text": "Uniform transition. Similar to the study by Hoogeboom et al. [2021], the natural idea is to maintain nodes with a certain probability $\\beta_{t}\\in(0,1)$ unchanged. In contrast, in other cases, nodes are randomly transformed into any other node in the domain with equal probability $(1-\\beta_{t})/(|\\mathcal{V}|-1)$ . That is ", "page_idx": 3}, {"type": "equation", "text": "$$\n[Q_{t}]_{i j}={\\binom{\\textstyle\\left(1-\\beta_{t}\\right)/(|\\mathcal{V}|-1)}{\\beta_{t}}}\\quad\\mathrm{if~}i\\neq j\\,\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Uniform transfer can be regarded as a special case of linear information allocation, thus theoretically affected by the size of the discrete space. It can compute the cumulative product $\\bar{\\pmb{Q}}_{t}$ in closed form. ", "page_idx": 3}, {"type": "text", "text": "Importance transition. For data with certain prior knowledge, we propose transitioning between more similar nodes rather than uniformly transitioning to any other state, thus defining the matrix: ", "page_idx": 3}, {"type": "equation", "text": "$$\n[Q_{t}]_{i j}=\\left\\{\\begin{array}{l l}{\\frac{\\exp\\left(-d_{i j}^{2}/2\\sigma^{2}\\right)}{\\sum_{v_{k}\\in\\mathbf{V}}\\exp\\left(-d_{i k}^{2}/2\\sigma^{2}\\right)}}&{\\mathrm{if}\\quad i\\neq j}\\\\ {\\,}\\\\ {1-\\sum_{k=0,k\\neq i}^{|\\mathbf{V}|-1}[Q_{t}]_{i k}}&{\\mathrm{if}\\quad i=j}\\end{array}\\right..\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Here, $d_{i j}$ represents the distance between item $v_{i}$ and $v_{j}$ , calculated using the square of the Euclidean distance. The parameter $\\sigma^{2}$ denotes the variance of the diffusion process. Consequently, the transition probabilities cannot be solved in closed form; instead, they can only be updated alongside the embeddings in the model. The importance transfer matrix adheres to the Gaussian information diffusion function $\\begin{array}{r}{f(x)=\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{x^{2}}{2\\sigma^{2}}}}\\end{array}$ . Therefore, it remains unaffected by the number of discrete points but necessitates the sample point distribution to closely resemble a Gaussian distribution. ", "page_idx": 3}, {"type": "text", "text": "This modeling approach appears to align more closely with our intuition, as a user\u2019s interests at a given moment often form a cluster of similar nodes (items). Only upon the user\u2019s final selection of an item does the \u2018neighborhood\u2019 converge to a single data point. This point represents the representative of the interest cluster and the ambiguity in information naturally dissipates. In recommendation tasks, we can only access the user\u2019s final choice at each moment, without knowledge of the interest cluster, reflecting incomplete knowledge. Regardless of the transition method employed, the key lies in transitioning the sample space from incomplete to complete, as detailed in the subsequent section. ", "page_idx": 4}, {"type": "text", "text": "3.3 Completeness and Reliability ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Here, we aim to demonstrate that the discrete diffusion spaces generated by the two methods in Section 3.2 are completions of the original space, and the models constructed on these fuzzy sets are solvable and effective. We first provide the formal definition of a complete sample space. ", "page_idx": 4}, {"type": "text", "text": "Definition 1. Let W denote the sample space. For any sample $W\\in{\\cal W}$ , $i f W$ is complete, i.e., unbiased estimates can be obtained through certain mathematical processing, then $W$ is called a complete sample space; otherwise, it is called an incomplete sample space. ", "page_idx": 4}, {"type": "text", "text": "In SR, $W$ is a user\u2019s behavior sequence; $W$ is all possible combinations of these behavior sequences; the domain $V$ is all items in the dataset. Datasets in SR systems do not form a complete sample space, as they often consist of incomplete interaction data and potential selection biases. The principle of information diffusion ensures that when the given sample is incomplete, there exist reasonable diffusion functions that can improve non-diffusion estimates. Below we define information diffusion. ", "page_idx": 4}, {"type": "text", "text": "Definition 2. An information diffusion about a set $W$ is defined by a mapping $\\mu:W\\times V\\to[0,1].$ , satisfying the following conditions: ", "page_idx": 4}, {"type": "text", "text": "(1) $\\forall w_{j}\\in W$ , if $v_{j}$ is the observed value of $w_{j}$ , then $\\begin{array}{r}{\\mu(w_{j},v_{j})=\\operatorname*{sup}_{v\\in V}\\mu(w_{j},v)}\\end{array}$ .   \n(2) $\\forall w_{j}\\in W$ , $\\mu(w_{j},v)$ decreases as $\\lVert\\boldsymbol{v}_{j}-\\boldsymbol{v}\\rVert$ increases.   \n(3) $\\forall w\\in W$ , $\\begin{array}{r}{\\sum_{\\nu}\\bar{\\mu}(w,v)\\mathrm{d}v=1}\\end{array}$ . ", "page_idx": 4}, {"type": "text", "text": "The diffusion estimates obtained using uniform transition and importance transition, as defined in Section 3.2, clearly adhere to Definition 2. To illustrate that the space resulting from discrete state transitions provides more information than the original state space, it is necessary to further demonstrate that this space serves as a completion of the original space. In other words, the new metric space is complete, with the original metric space serving as its dense subspace. This will be more precisely discussed in the following theorem. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.1. After information diffusion, the subsequent space must be an entirely separable metric space. Any model constructed in this space will assuredly possess a solution. ", "page_idx": 4}, {"type": "text", "text": "Proof in Appendix A. According to Theorem 3.1, since the space after information diffusion is equidistant isomorphism with the original space, it can be used to replace the sample space with insufficient information in SR to establish a model. On this complete space, predictive models are solvable, demonstrating that modeling on fuzzy sets is a reasonable and effective approach. ", "page_idx": 4}, {"type": "text", "text": "4 Learning and Inference of DDSR ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "4.1 Obtaining Semantic IDs ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As mentioned in Section 3, the indices $i$ and $j$ in the transition matrix $[Q_{t}]_{i j}$ represent categories in the discrete space, making $Q_{t}$ a two-dimensional matrix with dimensions equal to the size of the discrete space. However, in recommendation tasks, the number of items involved can reach tens of thousands, posing a significant challenge in terms of computational resources if we were to use all item IDs as the discrete state space. Inspired by VQ-Rec and the recently proposed Tiger model by Google, we attempt to train recommendation models using semantic IDs instead of item IDs. A semantic ID is a codebook of length $m$ . Assuming we set the size of the codebook to $K$ , the entire codebook can represent $K^{m}$ categories. Though we set each code from a different codebook, the state space only needs $m*K$ nodes to store them. Additionally, the use of semantic IDs further introduces semantic information, addressing the scarcity of information inherent in recommendations, while also allowing the model to extend to unseen items, thus enabling cold-start recommendations. We provide the specific method for obtaining semantic IDs in the Appendix B. ", "page_idx": 4}, {"type": "text", "text": "Input: historical interaction sequence $v_{1:n-1}\\,=\\,c_{1:n-1;1:m}$ ; target item $v_{n}\\,=\\,c_{n;1:m}$ ; transition matrix $Q_{t}$ ; Approximator $f_{\\theta}(\\cdot)$ . ", "page_idx": 5}, {"type": "text", "text": "Output: well-trained Approximator $f_{\\theta}(\\cdot)$ . While not converged do:   \n1: Sample Diffusion Time: $t\\sim[0,1,\\ldots,T]$ ;   \n2: Calculate $t$ -step transition probability: $\\overline{{\\pmb{Q}_{t}}}=\\pmb{Q}_{1}\\pmb{Q}_{2}\\cdot\\cdot\\cdot\\pmb{Q}_{t}$ ;   \n3: Convert cn;1:m to one-hot encoding x0n;1:m;   \n4: Obtain the discrete state $x_{n;1:m}^{t}$ after $t$ steps by Equation 2, thereby obtaining the \u2019fuzzy set\u2019 ct1:n\u22121;1:m;   \n5: Modeling $c_{2;n;1:m}$ based on \u2019fuzzy sets\u2019 through Equation 5;   \n6: Take gradient descent step on $\\nabla L_{C E}\\left(\\hat{c}_{2:n;1:m},c_{2:n;1:m}\\right)$ . ", "page_idx": 5}, {"type": "text", "text": "Algorithm 2 Inference of DDSR. ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Input: historical sequence $c_{1:n-1;1:m}$ ; well-trained Approximator $f_{\\theta}(\\cdot)$ ; sampling step $T$ .   \nOutput: predicted target item $v_{n}$ .   \n1: Let xT = c1:n\u22121;1:m;   \n2: Let $t=T$ ;   \n3: while $t>0$ do   \n4: Use the trained $f_{\\theta}(\\cdot)$ to obtain predictions $\\tilde{\\pmb{x}}_{0}$ with $\\pmb{x}_{t}$ and $t$ as inputs;   \n5: Substitute $\\scriptstyle{\\tilde{x}}_{0}$ into equation 7 to obtain the distribution of $t-1$ step;   \n6: end while   \n7: $\\tilde{v}_{n}=x_{0}[-1;1:m]$ ;   \n8: if the same code project exists: $v_{n}=\\tilde{v}_{n}$ ;   \n9: else: $v_{n}$ is the project in the space closest to $\\tilde{v}_{n}$ . ", "page_idx": 5}, {"type": "text", "text": "4.2 Model Training ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "After introducing the Semantic ID, we convert the historical interaction sequence $v_{1:n-1}$ into sequence $(c_{1,1},\\hdots,c_{1,m};c_{2,1},\\hdots,c_{2,m};\\hdots;c_{n-1,1},\\hdots,c_{n-1,m})$ , abbreviated as $c_{1:n-1;1:m}$ . We convert them into one-hot encodings $(\\pmb{x}_{1,1}^{0},\\allowbreak\\cdot\\cdot\\cdot,\\allowbreak\\pmb{x}_{1,m}^{0};\\allowbreak\\cdot\\cdot\\cdot;\\pmb{x}_{n-1,1}^{0},\\allowbreak\\cdot\\cdot\\cdot,\\allowbreak\\pmb{x}_{n-1,m}^{0})$ x0n\u22121,m), which is considered as the initial state for discrete diffusion. Then we perform discrete diffusion through the state transition formula defined in 2 (for more details, see Section 3.2) to obtain the discrete state after $t$ steps $\\pmb{x}_{i,j}^{t}$ , for any $i\\in\\{1,\\ldots,n-1\\}$ and $j\\,\\in\\,\\{1,\\ldots,m\\}$ . Accordingly, the labels changes from $c_{i,j}$ to $c_{i,j}^{t}$ . Then $(c_{1,1}^{t},\\hdots,c_{1,m}^{t};\\hdots;c_{n-1,1}^{t},\\hdots,c_{n-1,m}^{t})$ forms a \"fuzzy set\" of $c_{1:n-1;1:m}$ , denoted as $c_{1:n-1;1:m}^{t}$ which can also be viewed as the state $x_{t}$ of the diffusion transition at step $t$ . ", "page_idx": 5}, {"type": "text", "text": "Considering the suitability of the Transformer for sequence-to-sequence tasks, along with its welldemonstrated effectiveness in modeling sequential dependencies, we use it with an embedding layer as Approximator $f_{\\theta}(\\cdot)$ to predict $c_{2:n;1:m}$ with $c_{1:n-1;1:m}^{t}$ as input. This approach differs from the common practice in diffusion models, which often focus on modeling noise. It aligns more closely with typical SR tasks that use $v_{1:n-1}$ to predict $v_{2:n}$ , that is, the distribution $\\tilde{p}_{\\theta}(\\tilde{{\\boldsymbol{x}}}_{0}|{\\boldsymbol{x}}_{t})$ . We have adopted sinusoidal time step embeddings, which are added after the embedding layer, allowing the model to capture information about the time steps. This process can be represented by: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\hat{c}_{2:n;1:m}=f_{\\boldsymbol{\\theta}}(c_{1:n-1;1:m}^{t},t).\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Generally, the loss function of diffusion models is designed based on KL divergence, or it can be simplified to mean-squared error. However, guided by the theory of information diffusion, we choose to use a cross-entropy loss function, which is more suitable for recommendation tasks, to optimize our model without being constrained by the aforementioned methods. ", "page_idx": 5}, {"type": "table", "img_path": "h3BdT2UMWQ/tmp/3b4618d5c9eafccbce6135e4e8e21aa04c036789656c55102db7d38f0a7af39a.jpg", "table_caption": ["Table 1: Detailed descriptions and statistics of datasets. \u2019Avg. length\u2019 represents the average length of item sequences, while \u2019Avg. num\u2019 indicates the average number of words in item text. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "4.3 Model Inference ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In the inference phase, we aim to emulate the reverse process of the diffusion model, iteratively producing refined recommendation results. According to Bayes\u2019 theorem, we have ", "page_idx": 6}, {"type": "equation", "text": "$$\nq(x_{t-1}|x_{t},x_{0})=\\frac{q(x_{t}|x_{t-1},x_{0})q(x_{t-1}|x_{0})}{q(x_{t}|x_{0})}=\\mathrm{Cat}\\left(x_{t-1};p=\\frac{x_{t}Q_{t}^{\\top}\\odot x_{0}\\overline{{Q}}_{t-1}}{x_{0}\\overline{{Q}}_{t}x_{t}^{\\top}}\\right),\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\odot$ represents the Hadamard product. Following the approach of Ho et al. [2020] and Hoogeboom et al. [2021], we employ the trained model $f_{\\theta}(\\cdot)$ as described in Section 4.2 to derive the distribution $\\tilde{p}_{\\theta}(\\tilde{\\mathbf{x}}_{0}|\\mathbf{x}_{t})$ . Combining it with $q(\\mathbf{x}_{t-1}|\\mathbf{x}_{t},\\mathbf{x}_{0})$ , we obtain the following parameterized expression: ", "page_idx": 6}, {"type": "equation", "text": "$$\np_{\\theta}(\\mathbf{\\pmbx}_{t-1}|\\mathbf{\\pmbx}_{t})=\\sum_{\\widetilde{\\pmb x}_{0}}q(\\pmb x_{t-1},\\pmb x_{t}|\\widetilde{\\pmb x}_{0})\\widetilde{p}_{\\theta}(\\widetilde{\\pmb x}_{0}|\\pmb x_{t}).\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "For the historical interactions $v_{1:n-1}$ , we use $c_{1:n-1;1:m}$ as $x_{T}$ , and starting from $t=T$ , we iteratively execute Equation 7 until $t=0$ . This parameterization also allows us to perform inference for $k$ steps at a time by predicting $\\begin{array}{r}{p_{\\theta}(\\mathbf{\\boldsymbol{x}}_{t-k}|\\mathbf{\\hat{\\boldsymbol{x}}}_{t})\\,=\\,\\sum q(\\mathbf{\\boldsymbol{x}}_{t-k},\\mathbf{\\boldsymbol{x}}_{t}|\\widetilde{\\mathbf{\\boldsymbol{x}}}_{0})\\widetilde{p_{\\theta}}(\\widetilde{\\mathbf{\\boldsymbol{x}}}_{0}|\\mathbf{\\boldsymbol{x}}_{t}^{\\prime})}\\end{array}$ , leading to efficiency improvements. After obtaining $\\pmb{x}_{0}$ , we take its la st component, whic h  is a semantic ID of length m. If the corresponding item exists, we directly select that item; otherwise, we search for the item closest to it in the embedding space as the final recommendation result. The training and inference phase of DDSR are demonstrated in Algorithm 1 and Algorithm 2. ", "page_idx": 6}, {"type": "text", "text": "5 Experiment ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Experiment Settings ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Datasets. We employ three real-world datasets to evaluate the performance of our DDSR model. Following some works on text-based recommendation (Li et al. [2023a], Hou et al. [2022]), these datasets include two specific subcategories from the Amazon Reviews dataset (Scientific and Office), and a cross-platform dataset known as Online Retail, which operates from the UK. Following the method of Hou et al. [2022], we filter out users and items with fewer than five interactions. Subsequently, interaction behaviors within each sub-dataset are grouped by user and sorted chronologically. For the Amazon sub-datasets, product descriptions are formed by concatenating fields such as title, category, and brand, while for the Online Retail dataset, the description field is used. The product texts are truncated to 512 characters. Please refer to Table 1 for detailed descriptions of these datasets. ", "page_idx": 6}, {"type": "text", "text": "Baselines. We compare DDSR with eight state-of-the-art SR methods, including two conventional SR methods, three methods based on semantic information, and three generative SR methods: ", "page_idx": 6}, {"type": "text", "text": "1). Conventional Baselines: SASRec (Kang and McAuley [2018a]) utilizes a causal Transformer architecture with a self-attention mechanism to model user behavior. BERT4REC (Sun et al. [2019]) proposes a bidirectional Transformer with a cloze task predicting the masked target items for SR. ", "page_idx": 6}, {"type": "text", "text": "2). Semantic-based Baselines: UniSRec (Hou et al. [2022]) utilizes the associated description text of items to learn transferable representations across different recommendation scenarios, using an enhanced mixture-of-experts adaptor to enhance domain fusion and adaptation. VQ-Rec (Hou et al. [2023]) maps item text to a vector of discrete indices for learning transferable sequential recommenders. TIGER (Rajput et al. [2024]) trains a Transformer-based sequence-to-sequence model with semantic IDs obtained from RQ-VAE to enhance its generalization ability. ", "page_idx": 6}, {"type": "text", "text": "3). Generative Baselines: ACVAE (Xie et al. [2021]) proposes an adversarial and contrastive variational autoencoder for SR combining the ideas of CVAE and GAN. DiffuRec (Li et al. [2023b]) ", "page_idx": 6}, {"type": "table", "img_path": "h3BdT2UMWQ/tmp/f9e887d05e20043aa821eaa080a32a3dff504a7d60bc0403d5103026bd2a05ef.jpg", "table_caption": ["Table 2: Performance of different models. Bold (underline) is used to denote the best (second-best) metric, and \u2018\\*\u2019 indicates significant improvements relative to the best baseline (t-test $\\mathrm{P}{<}.05$ ). $\\mathbf{\\nabla}\\mathbf{R}\\mathbb{Q}\\,\\mathbf{K}^{\\prime}$ $(^{\\circ}\\mathrm{N@K^{\\circ}})$ is short for \u2019Recall $@\\,\\mathrm{K}'$ $\\operatorname{\\langle{NDCG}\\rangle}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\$ ). The features of items have been listed, whether ID, text (T), or both $(\\mathrm{ID}+\\mathrm{T})$ . "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "introduces the diffusion model into the field of SR reconstructing target item representation from a Transformer backbone with the user\u2019s historical interaction behaviors. DreamRec (Yang et al. [2024]) uses the historical interaction sequence as conditional guiding information for the diffusion model to enable personalized recommendations. ", "page_idx": 7}, {"type": "text", "text": "Evaluation Settings. Following previous works Hou et al. [2022], Zhao et al. [2022], Zhou et al. [2020], we evaluate all models using metrics Recall $@\\,\\mathrm{K}$ and $\\operatorname{NDCG}@\\operatorname{K}$ , and report experimental results for $K=10$ , 50. We employ the leave-one-out strategy for performance evaluation across all datasets. Concretely, we consider the last interaction as the test set, the second-to-last interaction as the validation set, and all preceding interactions as the training set. The ground-truth item of each sequence is ranked among all the other items while evaluating (Krichene and Rendle [2022]). The implementation details of DDSR are illustrated in Appendix C.1. ", "page_idx": 7}, {"type": "text", "text": "5.2 Overall Performance ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we compare the performance of DDSR with baseline models in terms of Top- $K$ recommendation accuracy under consistent experimental conditions (same data preprocessing), as summarized in Table 2. For models that recommend based on item IDs, we provide semantic information to them by jointly utilizing fixed text embeddings obtained from pre-trained BERT and the embeddings corresponding to item IDs in the model\u2019s embedding layer, to ensure fairness in the experimental setup. For all models, the final table records the better of the three methods, using only ID, only text, or both text and ID. ", "page_idx": 7}, {"type": "text", "text": "We observe that text-enhanced SR methods (UniSRec, VQ-Rec, TIGER) tend to benefit from textual information, leading to improved performance compared to conventional methods in most cases. Notably, VQ-Rec, employing discrete semantic encoding, generally outperforms UniSRec, which relies on continuous text embeddings, across various settings. This is despite UniSRec already using techniques like parameter whitening and MoE-enhanced Adaptor to enhance textual information. We posit that an excessive emphasis on text similarity can yield suboptimal outcomes, while the conversion to codes mitigates the coupling between items and semantic information. The corresponding representations of the codes are relearned in the sequence-to-sequence model, allowing them to include more sequential structural information. While similarly based on discrete semantic encoding, the performance of TIGER is not stable. We do not rule out the possibility that there may be discrepancies between our implementation and the actual model, as it has not been open-sourced. Furthermore, we attribute the instability to TIGER\u2019s semantic ID length, which is limited to only 4 characters, potentially insufficient for expressing complex information. ", "page_idx": 7}, {"type": "text", "text": "In methods grounded in generative models, the performance of DiffuRec and DreamRec, based on diffusion models, surpasses that of ACVAE, relying on GAN and VAE. This disparity arises from the inherent advantages of diffusion models over VAE and GAN, as they circumvent the issue of posterior collapse, wherein the generated hidden representations lack crucial information about user preferences. Notably, DiffuRec achieves superior performance despite its limited capacity to handle semantic information, yet it still exhibits recommendation performance comparable to VQ-Rec. This suggests that diffusion models can yield effective hidden representations of items and users. ", "page_idx": 7}, {"type": "table", "img_path": "h3BdT2UMWQ/tmp/55fa7155db4b49ed447e4e27b1a122fa1a9ef5e0cdc138a9986b43f9bff3bd46.jpg", "table_caption": ["Table 3: Ablation analysis of DDSR. Bold font indicates the best metric. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "In all three datasets, DDSR achieves significant improvements, demonstrating the effectiveness of our approach. We attribute this success to two key factors. Firstly, the integration of semantic information mitigates data sparsity issues, as evidenced by the enhanced performance of semantic-based models on smaller datasets compared to traditional recommendation methods. Secondly, training on fuzzy sets generated by discrete diffusion furnishes the model with additional information. This is consistent with our theoretical analysis in Section 3.3, which posits that the diffused information space constitutes a completion of the original space, rendering models built on this enriched space effectively solvable. Moreover, while DiffuRec, relying on a continuous state space diffusion model, exhibits instability when confronted with larger and more intricate datasets, DDSR maintains a distinct advantage. We attribute this to DDSR\u2019s retention of the discrete space without transitioning it into a continuous one and introducing noise, thus circumventing the loss of meaningful information. ", "page_idx": 8}, {"type": "text", "text": "5.3 Ablation Study ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We analyze the impact of semantic ID and discrete diffusion on final performance and conduct an ablation study to compare the results under different settings, as shown in Table 3. The Uniform transition and importance transition are two discrete diffusion methods provided in Section 3.2, corresponding to different transition matrices. To control variables, these methods, along with the non-diffusion case, are all applied using semantic ID obtained through PQ. It can be observed that on larger datasets, the importance transition has relatively more advantages, and both methods outperform the non-diffusion scenario. ", "page_idx": 8}, {"type": "text", "text": "To control variables and accurately evaluate the performance of IDs, diffusion is applied in the last two rows of the experiments (the method based on PQ ID with diffusion corresponds to the first and second rows, so it is not listed again). The PQ ID and RQ-VAE ID in the table corresponds to the two methods of obtaining semantic identifiers provided in Section 4.1. Random ID represents using a randomly generated codebook to replace the semantic identifiers, where the random identifier of item $c_{i}$ is simply $c_{i}\\,=\\,(c_{i,1},\\ldots,c_{i,m})$ with $c_{i,j}$ uniformly sampled from $1,2,...,K$ . Using Random ID means the model no longer gains semantic information. As observed, PQ ID exhibits greater stability and outperforms RQ-VAE ID across multiple datasets. Nevertheless, RQ-VAE ID requires less memory space due to its ability to achieve satisfactory representation with fewer codebook lengths. Semantic identifiers consistently outperform the Random ID, underscoring the significance of leveraging content-based semantic information. Indeed, models utilizing Random ID even underperform compared to SASRec. This can be attributed to SASRec\u2019s approach of setting independent embeddings for each item, rather than blending unrelated embeddings. ", "page_idx": 8}, {"type": "text", "text": "5.4 Further Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Performance Analysis on Cold-Start Items. In this study, we evaluate the efficacy of DDSR in recommending cold-start items. Generating effective item embeddings without item information poses a challenge for SR models. To evaluate this, we partition the test data into two groups based on item popularity. For the Office dataset, the range [0, 5) demarcates long-tail Items, whereas for the Online Retail dataset, it is [0, 20). All other items are classified as Popular Items. The results are presented in Figure 2a. Notably, DDSR and VQ-Rec demonstrate substantial improvement over SASRec, which solely leverages a Transformer, particularly for long-tail Items, also referred to as the cold-start group. This is attributed to the integration of semantic information, enabling the model to acquire prior knowledge about items to some extent. Furthermore, DDSR demonstrates even greater enhancement compared to VQ-Rec in cold-start scenarios. We attribute this to the discrete diffusion method, which introduces a \u2018fuzziness\u2019 effect in the interaction records, facilitating the inclusion of items with fewer interactions in the training process. ", "page_idx": 8}, {"type": "image", "img_path": "h3BdT2UMWQ/tmp/e8a3146f2f3622d4f4fc48b78d1ca59f40eb037cf7262ae9f15165f556fe8a1a.jpg", "img_caption": ["Figure 2: Impact of Item Popularity and Sampling Step $S$ . "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Impact of Sampling Step $S$ . The sampling step $S$ represents the diffusion step number divided by the number of inference steps executed simultaneously, rounded down to the nearest integer. We illustrate in Figure 2b the influence of various sampling step settings. The model demonstrates optimal performance with approximately 50 sampling steps, with a slight increase for more complex datasets, albeit without significant disparities. Excessive sampling steps prolong the inference time without commensurate performance improvements, while inadequate steps lead to decreased performance. ", "page_idx": 9}, {"type": "text", "text": "Efficiency Analysis. We compared the time complexity and specific running overhead of DDSR with several other baseline algorithms, and the detailed results can be found in Appendix C.2. ", "page_idx": 9}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We proposed the DDSR model for the sequential recommendation, which employed discrete diffusion to construct fuzzy sets of user interaction sequences. This process was iteratively refined during inference, utilizing the sampling formula for discrete diffusion to derive the ultimate recommendation outcomes. Notably, although DDSR had borrowed the form of diffusion and sampling over time steps from diffusion models, it fundamentally differed from directly using diffusion models. If we viewed sequential recommendation through the lens of causality, the interaction sequence was the \u2018cause\u2019 and the recommended item was the \u2018effect\u2019. Diffusion models typically address the target, blurring the \u2018effect\u2019, whereas DDSR has blurred the \u2018cause\u2019, inspired by the theory of fuzzy information processing. Although dual assurances, both theoretical and experimental results, have been provided to substantiate the superior performance of DDSR, it is imperative to recognize its inherent limitations. Despite our efforts to implement efficient computational methods, the nature of diffusion and sampling processes inevitably results in reduced efficiency and increased time complexity. Potential refinements, such as approximating the diffusion process and accelerating the sampling algorithm, could offer effective strategies, which we will explore in future work. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work is supported by the National Natural Science Foundation of China (No. U23A20319, 62202443). Hao Wang also thanks the CCF-Tencent Rhino-Bird Open Research Fund (RAGR20230124). ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Brian D. O. Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3):313\u2013326, 1982. J Austin, DD Johnson, J Ho, et al. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:17981\u201317993, 2021. Yukuo Cen, Jianwei Zhang, Xu Zou, Chang Zhou, Hongxia Yang, and Jie Tang. Controllable multiinterest framework for recommendation. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 2942\u20132951, 2020. Tong Chen, Hongzhi Yin, Quoc Viet Hung Nguyen, Wen-Chih Peng, Xue Li, and Xiaofang Zhou. Sequence-aware factorization machines for temporal predictive analytics. In 2020 IEEE 36th International Conference on Data Engineering (ICDE), pages 1405\u20131416. IEEE, 2020. ", "page_idx": 9}, {"type": "text", "text": "Yongjun Chen, Zhiwei Liu, Jia Li, Julian McAuley, and Caiming Xiong. Intent contrastive learning for sequential recommendation. In Proceedings of the ACM Web Conference 2022, pages 2172\u20132182, 2022.   \nP Dhariwal and A Nichol. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 34:8780\u20138794, 2021.   \nSheng Gu, Dongdong Chen, Jiajun Bao, et al. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10696\u201310706, 2022.   \nJ. Harte, W. Zorgdrager, P. Louridas, et al. Leveraging large language models for sequential recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems, pages 1096\u20131102, 2023.   \nRuining He and Julian McAuley. Fusing similarity models with markov chains for sparse sequential recommendation. In 2016 IEEE 16th International Conference on Data Mining (ICDM), pages 191\u2013200. IEEE, 2016.   \nXiangnan He, Hanwang Zhang, Min-Yen Kan, and Tat-Seng Chua. Fast matrix factorization for online recommendation with implicit feedback. In Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR \u201916, pages 549\u2013558, New York, NY, USA, 2016. Association for Computing Machinery.   \nBal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, et al. Session-based recommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939, 2015.   \nJ Ho, W Chan, C Saharia, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022.   \nJonathan Ho, Ankur Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840\u20136851, 2020.   \nEmiel Hoogeboom, David Nielsen, Priyank Jaini, and et al. Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in Neural Information Processing Systems, 34: 12454\u201312465, 2021.   \nY. Hou, S. Mu, W. X. Zhao, et al. Towards universal sequence representation learning for recommender systems. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 585\u2013593, 2022.   \nY. Hou, Z. He, J. McAuley, et al. Learning vector-quantized item representation for transferable sequential recommenders. In Proceedings of the ACM Web Conference 2023, pages 1162\u20131171, 2023.   \nAapo Hyv\u00e4rinen and Peter Dayan. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(4), 2005.   \nB Jacob, S Kligys, B Chen, et al. Quantization and training of neural networks for efficient integerarithmetic-only inference. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2704\u20132713, 2018.   \nW C Kang and J McAuley. Self-attentive sequential recommendation. In 2018 IEEE International Conference on Data Mining (ICDM), pages 197\u2013206. IEEE, 2018a.   \nWang-Cheng Kang and Julian McAuley. Self-attentive sequential recommendation. In 2018 IEEE International Conference on Data Mining (ICDM), pages 197\u2013206. IEEE, 2018b.   \nWalid Krichene and Steffen Rendle. On sampled metrics for item recommendation. Commun. ACM, 65(7):75\u201383, 2022.   \nD Lee, C Kim, S Kim, et al. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11523\u201311532, 2022.   \nJ Li, M Wang, J Li, et al. Text is all you need: Learning language representations for sequential recommendation. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 1258\u20131267, 2023a.   \nZ. Li, A. Sun, and C. Li. Diffurec: A diffusion model for sequential recommendation. ACM Transactions on Information Systems, 42(3):1\u201328, 2023b.   \nJianxin Ma, Chang Zhou, Hongxia Yang, Peng Cui, Xin Wang, and Wenwu Zhu. Disentangled selfsupervision in sequential recommenders. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 483\u2013491, 2020.   \nRajiv Pasricha and Julian McAuley. Translation-based factorization machines for sequential recommendation. In Proceedings of the 12th ACM Conference on Recommender Systems, pages 63\u201371. ACM, 2018.   \nRuihong Qiu, Zi Huang, and Hongzhi Yin. Memory augmented multi-instance contrastive predictive coding for sequential recommendation. In 2021 IEEE International Conference on Data Mining (ICDM), pages 519\u2013528. IEEE, 2021.   \nS. Rajput, N. Mehta, A. Singh, et al. Recommender systems with generative retrieval. Advances in Neural Information Processing Systems, 36, 2024.   \nK Rasul, C Seward, I Schuster, et al. Autoregressive denoising diffusion models for multivariate probabilistic time series forecasting. In International Conference on Machine Learning, pages 8857\u20138868. PMLR, 2021.   \nFernando E Rosas, Pedro A M Mediano, Henrik J Jensen, et al. Reconciling emergences: An information-theoretic approach to identify causal emergence in multivariate data. PLoS computational biology, 16(12):e1008289, 2020.   \nClaude E Shannon. A mathematical theory of communication. The Bell System Technical Journal, 27 (3):379\u2013423, 1948.   \nTianyu Shen, Haifeng Wang, Jian Zhang, et al. Exploring user retrieval integration towards large language models for cross-domain sequential recommendation. arXiv preprint arXiv:2406.03085, 2024.   \nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, et al. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 2256\u20132265. PMLR, 2015.   \nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, et al. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.   \nF. Sun, J. Liu, J. Wu, et al. Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer. In Proceedings of the 28th ACM international conference on information and knowledge management, pages 1441\u20131450, 2019.   \nH. Tanaka and G. Sommer. On posterior probabilities concerning a fuzzy information. Inst. f\u00fcr Wirtschaftswissenschaften, RWTH, 1977.   \nH. Tanaka, T. Okuda, and K. Asai. A formulation of fuzzy decision problems and its application to an investment problem. Kybernetes, 5(1):25\u201330, 1976.   \nJiaxi Tang and Ke Wang. Personalized top-n sequential recommendation via convolutional sequence embedding. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, pages 565\u2013573. ACM, 2018.   \nAaron Van Den Oord and Oriol Vinyals. Neural discrete representation learning. Advances in Neural Information Processing Systems, 30, 2017.   \nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, volume 30, 2017. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "H. Wang, T. Xu, Q. Liu, et al. Mcne: An end-to-end framework for learning multiple conditional network representations of social network. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1064\u20131072, 2019. ", "page_idx": 12}, {"type": "text", "text": "Wenjia Xie, Ruining Zhou, Hong Wang, et al. Bridging user dynamics: Transforming sequential recommendations with schr\u00f6dinger bridge and diffusion models. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, pages 2618\u20132628, 2024. ", "page_idx": 12}, {"type": "text", "text": "Z. Xie, C. Liu, Y. Zhang, et al. Adversarial and contrastive variational autoencoder for sequential recommendation. In Proceedings of the Web Conference 2021, pages 449\u2013459, 2021. ", "page_idx": 12}, {"type": "text", "text": "Z. Yang, J. Wu, Z. Wang, et al. Generate what you prefer: Reshaping sequential recommendation via guided diffusion. Advances in Neural Information Processing Systems, 36, 2024. ", "page_idx": 12}, {"type": "text", "text": "Zhengyi Yang, Jiancan Wu, Zhicai Wang, Xiang Wang, Yancheng Yuan, and Xiangnan He. Generate what you prefer: Reshaping sequential recommendation via guided diffusion. arXiv preprint arXiv:2310.20453, 2023. ", "page_idx": 12}, {"type": "text", "text": "Mingjia Yin, Hao Wang, Wei Guo, Yong Liu, Suojuan Zhang, Sirui Zhao, Defu Lian, and Enhong Chen. Dataset regeneration for sequential recommendation. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 3954\u20133965, 2024. ", "page_idx": 12}, {"type": "text", "text": "Q. Zhao. Resetbert4rec: A pre-training model integrating time and user historical behavior for sequential recommendation. In Proceedings of the 45th international ACM SIGIR conference on research and development in information retrieval, pages 1812\u20131816, 2022. ", "page_idx": 12}, {"type": "text", "text": "W. X. Zhao, Z. Lin, Z. Feng, et al. A revisiting study of appropriate offline evaluation for top-n recommendation algorithms. ACM Transactions on Information Systems, 41(2):1\u201341, 2022. ", "page_idx": 12}, {"type": "text", "text": "K. Zhou, H. Wang, W. X. Zhao, et al. S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management, pages 1893\u20131902, 2020. ", "page_idx": 12}, {"type": "text", "text": "A Proof of the theory ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Theorem A.1. The model often has the following form: ", "page_idx": 12}, {"type": "equation", "text": "$$\nf:(U\\times V,\\rho)\\to(U^{\\prime}\\times V^{\\prime},\\rho),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "which requires the model to be a continuous mapping from the original input-output space (which can also be the original space). The data distribution on the aggregated model set can be modeled: ", "page_idx": 12}, {"type": "equation", "text": "$$\nf^{\\prime}:(D,\\rho^{\\prime})\\rightarrow(U^{\\prime}\\times V^{\\prime},\\rho),\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "defined as $\\gamma$ such that $f^{\\prime}=\\gamma(\\mu)$ . When the static distribution approximates the dynamic distribution, $\\mu$ is the extended static scattering coefficient; otherwise, it is the linear information distribution coefficient $\\gamma$ , ensuring that the determination of $f^{\\prime}$ is unique. Therefore, $(D,\\rho^{\\prime})$ is the completion space of $(U\\times V,\\rho)$ , consisting of the complete set of $(U\\times V,\\rho)$ and its separation. ", "page_idx": 12}, {"type": "text", "text": "In summary, the information diffusion space must be the completion space of the original space. Due to the same dimensionality as the original space, the modeling on the aggregated set is reliable. ", "page_idx": 12}, {"type": "text", "text": "Next, consider the model (prediction model or simulation model) built on $D(X)$ . In practical applications, the input-output set is generally constructed in two ways based on the original data: ", "page_idx": 12}, {"type": "equation", "text": "$$\ne_{i}=\\frac{x_{i}-x_{\\operatorname*{min}}}{x_{\\operatorname*{max}}-x_{\\operatorname*{min}}},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "where $e_{i}$ is the normalized sequence; $x_{i}$ is the original sequence data; $x_{\\mathrm{{min}}}$ is the minimum value of the original sequence; $x_{\\mathrm{max}}$ is the maximum value of the original sequence. It is obvious that $e_{i}\\;\\in\\;[0,1]$ , meaning $(U\\times V)\\,\\subseteq\\,[0,1]$ , and the function boundary on $D(X)$ . When evaluating risk, the original sequence data generates input-output sets, which clearly have bounded intervals $x_{i}\\in[x_{\\operatorname*{min}},x_{\\operatorname*{max}}],$ , meaning the function boundary on $D(X)$ is also bounded. Therefore, $D(X)$ is necessarily a subset of the real number set. ", "page_idx": 12}, {"type": "text", "text": "In the prediction model, the attribute function obtained from the aggregated set is: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mu_{y_{0}}=\\sum_{u}\\mu_{x_{0}}(u)\\mu_{A_{1}}(u,v),\\quad u\\in U,v\\in V.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Among them, $\\mu_{R_{i}}(u)=\\mu_{A_{i}}(u)\\mu_{A_{i}}(v),\\quad u\\in U,v\\in V.$ . Since the output sequence and the input sequence are generated by the same $\\{e_{i}\\}$ , and $\\mu_{i}=x_{i+1}$ : ", "page_idx": 13}, {"type": "text", "text": "Separately substituting into (6), we can obtain: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\boldsymbol{\\mu}_{x_{0}}\\cdot\\boldsymbol{g}(x_{0})=\\sum_{u}\\mu_{x_{0}}\\mu_{A_{i}}\\mu_{B_{i}}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "or ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mu_{x_{0}}+g(x_{0})=\\sum_{u}\\mu_{x_{0}}\\mu_{A_{i}}\\mu_{B_{i}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Rearranging, we get: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mu_{x_{0}}=\\sum_{u}\\frac{\\mu_{x_{0}}\\mu_{A_{i}}\\mu_{B_{i}}}{g(x_{0})}=\\sum_{u}G(x_{0},\\mu_{x_{0}}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "and ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mu_{x_{0}}=\\sum_{u}\\mu_{x_{0}}\\mu_{A_{i}}\\mu_{B_{i}}-g(x_{0})=\\sum_{u}\\left[\\mu_{x_{0}}\\mu_{A_{i}}\\mu_{B_{i}}-{\\frac{g(x_{0})}{n}}\\right]=\\sum_{u}G(x_{0},\\mu_{x_{0}}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "The initial value problem for the differential equation $\\dot{y}(t)=f(t,y(t)),y(t_{0})=y_{0}$ can be transformed into an integral equation: ", "page_idx": 13}, {"type": "equation", "text": "$$\ny(t)=\\int_{u}f(t,y(t))d t+y_{0},\\quad t\\in u;\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Since the cumulative sum and substitution integrals can be used in the scatter system, (8) is the integral equation\u2019s discrete model on $D(X)$ . ", "page_idx": 13}, {"type": "equation", "text": "$$\n|G(x_{i})-G(x_{j})|=\\left|G(x_{i},\\mu_{x_{i}})-G(x_{j},\\mu_{x_{j}})\\right|=\\left\\{\\left|{\\mu_{x_{i}}\\cdot g(x_{i})-\\mu_{x_{j}}\\cdot g(x_{j})}\\right|,\\,\\,\\,\\,\\,\\,\\,X\\subseteq N(0,1)\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "Both cases satisfy: ", "page_idx": 13}, {"type": "equation", "text": "$$\nK=\\operatorname*{max}_{x\\in U}\\frac{\\Delta}{\\mu_{x_{i}}-\\mu_{x_{j}}}\\;s u c h\\;t h a t\\ |G(x_{i})-G(x_{j})|\\leq K\\left|x_{i}-x_{j}\\right|,\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "i.e., the function $G(X)$ on $D(X)$ satisfies the Lipschitz condition, thus the attribute function must have a solution. ", "page_idx": 13}, {"type": "text", "text": "Since the attribute function has a solution on $D(X)$ , it can be deduced by the \"maximum\" principle that the predicted output value must be: ", "page_idx": 13}, {"type": "equation", "text": "$$\n{\\bar{y_{0}}}=\\left(\\sum_{i=1}^{n}w_{i}v_{i}^{\\prime}\\right)/\\left(\\sum_{i=1}^{n}w_{i}\\right),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where the weights $w_{i}=\\mu_{y_{0}}\\left(v^{\\prime}\\right)=\\operatorname*{max}_{v\\in V}\\left\\{\\mu_{y_{0}}\\left(v\\right)\\right\\}$ ", "page_idx": 13}, {"type": "text", "text": "Therefore, the information diffusion approximation model is established. ", "page_idx": 13}, {"type": "text", "text": "B Model Supplement ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We first obtain fixed text embeddings from the relevant descriptions of items (e.g., product descriptions, item titles, or brands) using the pre-trained BERT model. Specifically, for an item $v_{i}$ with a corresponding description $\\{w_{1},w_{2},\\ldots,w_{c}\\}$ , the corresponding embedding vector is $e_{i}=\\mathrm{BERT}([[\\mathrm{CLS}];w_{1},\\dots,w_{c}])\\in\\mathbb{R}^{d_{W}}$ , where $\"[;]\"$ denotes the concatenation operation. Next, obtaining Semantic IDs from $e_{i}$ can be achieved through the following two methods: ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Product Quantization (PQ). Similar to the VQ-Rec approach, we evenly divided $e_{i}$ into $m$ subvectors $\\boldsymbol{e}_{i}=[e_{i,1};\\ldots;e_{i,m}]$ , each with a dimension of $d_{W}/m$ . Denote $a_{p,j}\\in\\mathbb{R}^{d_{W}/m}$ as the $j$ -th centroid embedding in the codebook corresponding to the $p$ -th sub-vector. For each sub-vector, the index of the nearest centroid from the corresponding PQ codebook is selected to form its discrete code $c_{i,p}=\\arg\\operatorname*{min}_{j}||e_{i,p}-\\pmb{a}_{p,j}||^{2}\\in\\{1,2,\\ldots,K\\}$ . The centroids are obtained through the commonly used Optimized Product Quantization (OPQ) method. Finally, $c_{i}=(c_{i,1},\\ldots,c_{i,m})$ is used as the semantic ID for item $v_{i}$ . ", "page_idx": 14}, {"type": "text", "text": "RQ-VAE. RQ-VAE generate a set of codewords by quantizing the residuals. First, the input $e_{i}$ is encoded into $r_{0}$ using encoder $\\mathbf{E}$ . Next, similar to PQ, the nearest centroid to it in the first codebook, assumed to be $a_{p,1}$ , is found and its index is taken to form the first discrete code $c_{i,1}$ . The residual is defined as $r_{1}:=r_{0}\\!-\\!a_{p_{1},1}$ . The same operation is performed to obtain $c_{i,2}$ and this process is repeated $m$ times to obtain the complete Semantic ID. RQ-VAE use loss function $\\begin{array}{r}{\\mathcal{L}(\\pmb{x}):=\\mathcal{L}_{\\mathrm{recon}}+\\mathcal{L}_{\\mathrm{rqvae}}}\\end{array}$ jointly trains the encoder, decoder, and the codebook, where $\\mathcal{L}_{\\mathrm{recon}}:=\\|e_{i}-\\widehat{e_{i}}\\|^{2}$ is reconstruction loss, $\\begin{array}{r}{\\mathcal{L}_{\\mathrm{rqvae}}:=\\sum_{d=0}^{m-1}\\|\\mathrm{sg}[\\boldsymbol{r}_{i}]-a_{p_{i},i}\\|^{2}+\\beta\\|\\boldsymbol{r}_{i}-\\mathrm{sg}[a_{p_{i},i}]\\|^{2}}\\end{array}$ . Here $\\widehat{e_{i}}$ is the output of the decoder, and sg is the stop-gradient operation Van Den Oord and Vinyals [2017]. Because the norm of the residuals decreases progressively, the importance of the codebooks obtained in this manner also diminishes with each level. Alternatively, it can be said that the encoding at each position has varying levels of granularity. In our experiments, we found that this approach requires a smaller codebook size than PQ, but it is slightly less stable. ", "page_idx": 14}, {"type": "text", "text": "In fact, once the codebook is established, it remains fixed throughout the subsequent model training process. Therefore, the quality of the codebook directly affects the training outcomes. With the rapid development of large language models, considering the use of more advanced pre-training schemes to replace BERT could be an effective way to enhance performance. However, this is not the focus of our current research, we leave this topic for future exploration. ", "page_idx": 14}, {"type": "text", "text": "C Experimental Supplement ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "C.1 Implementation Details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "All of our experiments were conducted on a single RTX 4090. We implemented our models based on PyTorch and the popular open-source recommendation library RecBole. We used $(m=32)\\times(K=$ 256) as the code representation scheme for PQ IDs and $(m=6)\\times(K=256)$ for RQ-VAE IDs. For baseline models, to ensure fair comparison, we optimized all methods using the Adam optimizer and searched for hyperparameters to find the best results. Since we did not find open-source code for the TIGER model, we attempted to replicate it as faithfully as possible based on its paper; however, its performance may have been slightly lower in some experiments due to difficulties in unifying some details. For UniSRec and VQ-Rec, we utilize their code in a form that does not involve pretraining with the entire dataset, as we aim to evaluate all baselines and DDSR considering recommendations on a single dataset rather than cross-domain. The batch size was set to 2,048. The learning rate was adjusted among $\\{0.0001,0.0002,0.0003,0.0005,0.001\\}$ . The model achieving the highest $\\operatorname{NDCG}@10$ result on the validation set was selected for evaluation on the test set. We employed an early stopping strategy with a patience of 10 epochs. ", "page_idx": 14}, {"type": "text", "text": "Regarding diffusion, all models were trained on a diffusion process of 1000 steps, and the time step embeddings are implemented using cosine embeddings, similar to the work of Li et al. [2023b]. For the diffusion with uniform transition, we employ the cosine schedule proposed by Hoogeboom et al. [2021] to set the transition probabilities $(1\\!-\\!\\beta_{t})$ . For the diffusion with importance sampling, we adopt a linear schedule similar to the one used in Ho et al. [2020], where $\\sigma^{2}$ increases linearly from $10^{-4}\\!*\\!\\dot{K}$ to $0.02*K$ . Skip steps in the sampling process were chosen among $\\{100,50,35,28,23,20,17,15\\}.$ While there have been many works on accelerating sampling in continuous state space diffusion models, the development in discrete diffusion is still insufficient. Here, we adopted a basic uniform skip scheme for more efficient and effective sampling, which is one of our next research directions. If the evaluation steps do not divide 1000 evenly, the last step may be skipped. ", "page_idx": 14}, {"type": "table", "img_path": "h3BdT2UMWQ/tmp/f88831112bdd5d435100b947c7553a22a2fe2e91ba3a519969680ee738d9d1e3.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "C.2 Efficiency Analysis ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "We list the time complexities of six baselines in Table 4, where $n$ denotes sequence length, $d$ is the dimension of the hidden layers, and $m$ is the length of the codebook used when semantic IDs are utilized. Most of these models are based on the transformer or its variants, hence the time complexity is $O(n d^{2}+d n^{2})$ . Only TIGER and our proposed DDSR model are trained using semantic IDs, making their time complexity $m$ times that of other methods. We plan to make further improvements in our subsequent work. ", "page_idx": 15}, {"type": "text", "text": "In addition, we compared the actual operational costs of the DDSR model with those of UniSRec and DiffuRec, as shown in Table 5. Although we adopted the method of \u2019performing inference for $k$ steps at a time\u2019, which reduced the sampling steps and lowered the evaluation time compared to DiffuRec, we must acknowledge that DDSR still has certain limitations in terms of operational costs, necessitating further improvements in future work. ", "page_idx": 15}, {"type": "table", "img_path": "h3BdT2UMWQ/tmp/61bc69ecebd9fe8de0328955f6b02e79924e381a0bd92bbdb9b18dc5b9449bd8.jpg", "table_caption": ["Table 5: Comparison of Actual Operational Costs. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "D Diffusion Models in Continuous State Space ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "D.1 DDPM ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Diffusion models comprise a forward diffusion process and a backward denoising process. We begin with the widely recognized denoising diffusion probabilistic model (DDPM) Ho et al. [2020]. We start by defining our data distribution $x_{0}\\sim q(x_{0})$ and a Markovian noising process $q$ which gradually adds noise to the data $x_{0}$ to produce noised samples $x_{T}$ .In particular, each step of the noising process adds Gaussian noise according to a variance schedule given by $\\beta_{t}$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\nq(x_{t}\\mid x_{t-1}):=\\mathcal{N}(x_{t};\\sqrt{1-\\beta_{t}}x_{t-1},\\beta_{t}\\mathbf{I})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Furthermore, $q(x_{t}\\mid x_{0})$ can be expressed as a Gaussian distributio\u221an. With $\\alpha_{t}\\ :=\\ 1\\mathrm{~-~}\\beta_{t}$ and $\\begin{array}{r}{\\bar{\\alpha}_{t}:=\\prod_{s=0}^{t}\\alpha_{s}}\\end{array}$ $q(x_{t}\\mid x_{0})=\\mathcal{N}(x_{t};\\sqrt{\\bar{\\alpha}_{t}}x_{0},(1-\\bar{\\alpha}_{t})\\mathbf{I})=\\sqrt{\\bar{\\alpha}_{t}}x_{0}+\\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon$ , where $\\epsilon\\sim\\mathcal{N}(0,\\bf{I})$ . Here, $1-\\bar{\\alpha}_{t}$ indicates the variance of the noise at an arbitrary timestep, and this can be used to define the noise schedule instead of $\\beta_{t}$ . ", "page_idx": 15}, {"type": "text", "text": "Using Bayes theorem, one finds that the posterior $q(x_{t-1}|x_{t},x_{0})$ is also a Gaussian with mean $\\tilde{\\mu}_{t}(x_{t},x_{0})$ and variance ${\\tilde{\\beta}}_{t}$ defined as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tilde{\\mu}_{t}(x_{t},x_{0}):=\\frac{\\sqrt{\\alpha_{t-1}}\\beta_{t}}{1-\\alpha_{t}}x_{0}+\\frac{\\sqrt{\\alpha_{t}}(1-\\alpha_{t-1})}{1-\\alpha_{t}}x_{t}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\n\\tilde{\\beta}_{t}:=\\frac{1-\\alpha_{t-1}}{1-\\alpha_{t}}\\beta_{t}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "equation", "text": "$$\nq(x_{t-1}|x_{t},x_{0})=\\mathcal{N}(x_{t-1};\\tilde{\\mu}_{t}(x_{t},x_{0}),\\tilde{\\beta}_{t}{\\bf I})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "If we wish to sample from the data distribution $q(x_{0})$ , we can first sample from $q(x_{T})$ and then sample reverse steps $q(x_{t-1}|x_{t})$ until we reach $x_{0}$ . Under reasonable settings for $\\beta_{t}$ and $T$ , the distribution $q(x_{T})$ is nearly an isotropic Gaussian distribution, so sampling $x_{T}$ is trivial. All that is left is to approximate $q(x_{t-1}|x_{t})$ using a neural network, since it cannot be computed exactly when the data distribution is unknown. To this end, Sohl-Dickstein et al. [56] note that $q(x_{t-1}|x_{t})$ approaches a diagonal Gaussian distribution as $T\\rightarrow\\infty$ and correspondingly $\\beta_{t}\\to0$ , so it is sufficient to train a neural network to predict a mean $\\mu_{\\theta}$ and a diagonal covariance matrix $\\Sigma_{\\theta}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\np_{\\theta}(x_{t-1}|x_{t}):=\\mathcal{N}(x_{t-1};\\mu_{\\theta}(x_{t},t),\\Sigma_{\\theta}(x_{t},t))\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To train this model such that $p_{\\theta}(x_{0})$ learns the true data distribution $q(x_{0})$ , we can optimize the following variational lower-bound $L_{\\mathrm{vlb}}$ for $p_{\\theta}(x_{0})$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\nL_{\\mathrm{vlb}}:=L_{0}+L_{1}+.\\ldots+L_{T-1}+L_{T}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\nL_{0}:=-\\log p_{\\theta}(x_{0}|x_{1})\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\nL_{t-1}:=D_{K L}{\\big(}q(x_{t-1}|x_{t},x_{0})\\|p_{\\theta}(x_{t-1}|x_{t}){\\big)}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "equation", "text": "$$\nL_{T}:=D_{K L}\\big(q(x_{T}|x_{0})||p(x_{T})\\big)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "While the above objective is well-justified, Ho et al. [25] found that a different objective produces better samples in practice. In particular, they do not directly parameterize $\\mu_{\\theta}(x_{t},t)$ as a neural network, but instead train a model $\\epsilon_{\\theta}(x_{t},t)$ to predict $\\epsilon$ from Equation 17. This simplified objective is defined as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\nL_{\\mathrm{simple}}:=\\mathbb{E}_{t\\sim[1,T],x_{0}\\sim q(x_{0}),\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I})}\\left[\\Vert\\epsilon-\\epsilon_{\\theta}(x_{t},t)\\Vert_{2}^{2}\\right]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "During sampling, we can use substitution to derive $\\mu_{\\theta}{\\left(x_{t}\\right)}$ from $\\epsilon_{\\theta}(x_{t},t)$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mu_{\\theta}(x_{t})=\\frac{1}{\\sqrt{\\alpha_{t}}}\\left(x_{t}-\\frac{1-\\alpha_{t}}{\\sqrt{1-\\alpha_{t}}}\\epsilon_{\\theta}(x_{t},t)\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that $L_{\\mathrm{simple}}$ does not provide any learning signal for $\\Sigma_{\\theta}(x_{t},t)$ . Ho et al. [25] find that instead of learning $\\Sigma_{\\theta}(x_{t},t)$ , they can fix it to a constant, choosing either $\\beta_{t}$ or ${\\tilde{\\beta}}_{t}$ . These values correspond to learning noise and the reverse process variance respectively. ", "page_idx": 16}, {"type": "text", "text": "D.2 Score-Based Generative Model ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "In this section, we introduce a Score-Based Generative Model (SGMs) Song et al. [2020], specifically a diffusion model represented in the form of Stochastic Differential Equations (SDEs). SGMs model the forward diffusion process using the stochastic differential equation: ", "page_idx": 16}, {"type": "equation", "text": "$$\nd x_{t}=f(x_{t},t)d t+g(t)d w,\\pmb{x}_{0}\\sim p_{0}=p_{\\mathrm{target}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $t\\in[0,T]$ , and $w$ signifies Brownian motion, $p_{\\mathrm{target}}$ represents target distribution. The function $f(\\cdot,t):\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}\\mathbf{i}\\mathrm{s}$ s a vector-valued function called the drift coefficient of $x(t)$ , and $g(\\cdot):\\mathbb{R}\\rightarrow\\mathbb{R}$ is a scalar function known as the diffusion coefficient of $x(t)$ . The functions $f$ and $g$ determine the type of prior distribution $p_{\\mathrm{prior}}$ to which the forward process will diffuse, and they are typically designed to make the prior distribution a Gaussian distribution. As a remarkable result from Anderson [1982], the reverse of the diffusion process is also a diffusion process, given by the following reverse-time SDE: ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d x_{t}=[f(x_{t},t)-g(t)^{2}\\nabla_{x}\\log p_{t}(x)]d t+g(t)d\\bar{w},\\ \\ \\ }\\\\ {x_{T}\\sim p_{T}\\approx p_{\\mathrm{prior}},\\qquad\\qquad\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\bar{w}$ is a standard Wiener process in reverse time. The term $\\nabla_{x}\\log{p_{t}(x)}$ , which represents the score function of the marginal density $p_{t}$ , is the only unknown term in this reverse process. SGMs learns its approximate target $s_{\\theta}(x(t),t)$ through denoising score matching (DSM) Hyv\u00e4rinen and Dayan [2005], with $s_{\\theta}$ referred to as the denoising model: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta^{*}=\\arg\\underset{\\theta}{\\operatorname*{min}}\\,\\mathbb{E}_{t\\sim U(0,T)}\\mathbb{E}_{x(0)}\\mathbb{E}_{x(t)|x(0)}}\\\\ &{\\qquad\\quad\\left[\\|s_{\\theta}(x(t),t)-\\nabla_{x}\\log p_{0t}(x|x(0))\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Here, $\\lambda(t)$ is a positive weighting coefficient, $t\\sim\\mathcal{U}(0,T)$ . The joint distribution $p_{0t}(x|x(0))$ is the conditional transition distribution from $x(0)$ to $x(t)$ , which is determined by the pre-defined forward SDE. To summarize, SGMs first utilize the diffusion process defined in Equation(18) to obtain the distribution $x_{t}$ at intermediate time steps. Then, they minimize the loss defined in Equation(20) to train the denoising model $s_{\\theta}$ and sample iteratively using the formula defined in Equation(19) to obtain the final result. ", "page_idx": 17}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The abstract and introduction clearly outline the primary contributions of the paper, including the background, challenges, motivation, overall framework, and experimental validation. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 18}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Justification: The paper provides a detailed explanation of the problem background in the methods section and validates the hypothesis through multiple runs on various datasets with different characteristics in the experimental section. Additionally, the limitations are thoroughly discussed in Section 6. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 18}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: The paper presents all theoretical results with clearly stated assumptions and complete proofs, which are either included in the main text or supplemented by detailed proofs in the appendix, ensuring both clarity and rigor. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 19}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Justification: The paper provides detailed descriptions of the experimental setup, including dataset specifications, parameter settings, and evaluation metrics, ensuring that the main results can be reproduced even without direct access to the code and data. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 19}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 20}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper includes links to openly accessible data and code repositories that cover the necessary commands and environment setup to reproduce the main experimental results. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 20}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Justification: The paper thoroughly describes all relevant experimental details in a dedicated \"Experiment Setting\" section, including data splits, hyperparameters, and optimization methods, ensuring clarity and reproducibility of the results. ", "page_idx": 20}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 20}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 20}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 20}, {"type": "text", "text": "Justification: The paper appropriately reports statistical significance using t-tests with a significance threshold of $\\mathrm{p}<0.05$ , ensuring transparency and rigor in the experimental analysis. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 21}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The paper thoroughly details the computer resources required for each experiment, including the type of compute workers, memory specifications, and execution times, enabling reproducibility and understanding of the computational demands of the study. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 21}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 21}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 21}, {"type": "text", "text": "Justification: The research ensures that ethical considerations are addressed and integrated into the study\u2019s design and execution, and the authors ensure anonymity is preserved. ", "page_idx": 21}, {"type": "text", "text": "Guidelines: ", "page_idx": 21}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. \u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ", "page_idx": 21}, {"type": "text", "text": "\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 22}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The paper addresses both potential positive and negative societal impacts of the research, highlighting potential misuses and discussing mitigation strategies, thereby ensuring a comprehensive consideration of broader societal implications. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 22}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 22}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: The paper does not involve the release of data or models that pose a high risk for misuse, thus no safeguards are necessary. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 22}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper properly credits the creators of all utilized assets, explicitly adhering to licenses and terms of use by including citations, version details, and relevant licensing information. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 23}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: The code repositories will provide comprehensive documentation for all newly introduced assets, including details about their creation, usage, and limitations, ensuring that other researchers can effectively utilize these resources. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 23}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 23}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects, so this information is not applicable. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 23}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing or research with human subjects, so IRB approval or equivalent is not applicable. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 24}]