[{"figure_path": "nbqvjkOs6S/figures/figures_2_1.jpg", "caption": "Figure 1: (a) The difference between LDM and pixel-space DM lies in the use of the decoder (D). (b) This difference has caused the performance gap in exact inversions. Pixel-space DMs and gradient-based GAN inversion methods are located on the right side due to iterative gradient back-propagations. If gradient-based decoder inversions are used, which are computationally intensive, LDM is located on the rightmost side to address the lossiness of latents. Our proposed gradient-free decoder inversion method allows us to efficiently handle the transformation between latent and pixel spaces.", "description": "The figure compares Latent Diffusion Models (LDMs) and pixel-space Diffusion Models (DMs).  It highlights how LDMs utilize a decoder to map latent representations to pixel space, unlike pixel-space DMs. The figure contrasts the invertibility of different generative models, emphasizing the computational cost and memory requirements of gradient-based inversion methods for LDMs and the advantages of the proposed gradient-free method. The left subfigure shows how latent diffusion models (LDMs) use a decoder to map the low-dimensional latent space to pixel space, while pixel-space DMs operate directly in pixel space. The right subfigure illustrates the difference in the invertibility of various generative models and positions the proposed gradient-free method.", "section": "1 Introduction"}, {"figure_path": "nbqvjkOs6S/figures/figures_2_2.jpg", "caption": "Figure 1: (a) The difference between LDM and pixel-space DM lies in the use of the decoder (D). (b) This difference has caused the performance gap in exact inversions. Pixel-space DMs and gradient-based GAN inversion methods are located on the right side due to iterative gradient back-propagations. If gradient-based decoder inversions are used, which are computationally intensive, LDM is located on the rightmost side to address the lossiness of latents. Our proposed gradient-free decoder inversion method allows us to efficiently handle the transformation between latent and pixel spaces.", "description": "This figure compares different generative models based on their invertibility and computational cost. (a) illustrates the difference between Latent Diffusion Models (LDMs) and pixel-space Diffusion Models, highlighting the role of the decoder. (b) positions various models along two axes: inversion runtime (x-axis) and inversion error (y-axis). It shows that LDMs, due to their latent space representation, present a greater challenge for decoder inversion compared to pixel-space models.  The authors' proposed gradient-free method is presented as an efficient solution that improves inversion for LDMs.", "section": "1 Introduction"}, {"figure_path": "nbqvjkOs6S/figures/figures_5_1.jpg", "caption": "Figure 1: (a) The difference between LDM and pixel-space DM lies in the use of the decoder (D). (b) This difference has caused the performance gap in exact inversions. Pixel-space DMs and gradient-based GAN inversion methods are located on the right side due to iterative gradient back-propagations. If gradient-based decoder inversions are used, which are computationally intensive, LDM is located on the rightmost side to address the lossiness of latents. Our proposed gradient-free decoder inversion method allows us to efficiently handle the transformation between latent and pixel spaces.", "description": "This figure compares latent diffusion models (LDMs) with pixel-space diffusion models in terms of invertibility.  (a) illustrates the core difference: LDMs use a decoder to map latent representations to pixel space, while pixel-space models work directly in pixel space. (b) shows that the introduction of the decoder in LDMs, while efficient for generation, makes inversion more challenging, particularly with gradient-based methods. The authors' proposed gradient-free method addresses this challenge by directly handling the latent-to-pixel space mapping.", "section": "1 Introduction"}, {"figure_path": "nbqvjkOs6S/figures/figures_6_1.jpg", "caption": "Figure 1: (a) The difference between LDM and pixel-space DM lies in the use of the decoder (D). (b) This difference has caused the performance gap in exact inversions. Pixel-space DMs and gradient-based GAN inversion methods are located on the right side due to iterative gradient back-propagations. If gradient-based decoder inversions are used, which are computationally intensive, LDM is located on the rightmost side to address the lossiness of latents. Our proposed gradient-free decoder inversion method allows us to efficiently handle the transformation between latent and pixel spaces.", "description": "This figure illustrates the core difference between Latent Diffusion Models (LDMs) and pixel-space Diffusion Models, highlighting the role of the decoder in the inversion process.  Panel (a) shows the architecture difference, emphasizing how LDMs utilize a latent space, while (b) visualizes the trade-off between speed and accuracy for different inversion methods, showcasing the advantage of the proposed gradient-free approach for LDMs.", "section": "1 Introduction"}, {"figure_path": "nbqvjkOs6S/figures/figures_8_1.jpg", "caption": "Figure 1: (a) The difference between LDM and pixel-space DM lies in the use of the decoder (D). (b) This difference has caused the performance gap in exact inversions. Pixel-space DMs and gradient-based GAN inversion methods are located on the right side due to iterative gradient back-propagations. If gradient-based decoder inversions are used, which are computationally intensive, LDM is located on the rightmost side to address the lossiness of latents. Our proposed gradient-free decoder inversion method allows us to efficiently handle the transformation between latent and pixel spaces.", "description": "This figure illustrates the core difference between Latent Diffusion Models (LDMs) and pixel-space Diffusion Models (DMs) in terms of invertibility.  (a) shows that LDMs utilize a decoder to map from latent space to pixel space, unlike pixel-space DMs which operate directly in pixel space. (b) highlights that while pixel-space DMs and gradient-based GAN inversion methods rely on iterative gradient descent, making them computationally expensive, the proposed gradient-free decoder inversion method offers a more efficient alternative for LDMs, especially beneficial when dealing with the lossy nature of latent representations in LDMs.", "section": "1 Introduction"}, {"figure_path": "nbqvjkOs6S/figures/figures_8_2.jpg", "caption": "Figure 2: The six subfigures represent the relationship between cocoercivity and convergence for 3 models \u00d7 2 algorithms (vanilla forward step method and inertial KM iteration). The x-axis represents the values of mink\u2208[0,100] (EDz\u221e\u2212EDzk,z\u221e\u2212zk) , which informs whether the optimization path satisfies the assumptions of Theorems 1 and 2, while the y-axis represents the convergence (i.e., ||z100\u2212z\u221e||2). The red line shows the linear function fitted by least squares. We set z\u221e=z300", "description": "This figure shows the relationship between cocoercivity and convergence for three different latent diffusion models using two different algorithms: the vanilla forward step method and the inertial KM iteration.  The x-axis represents the minimum cocoercivity, indicating how well the conditions of Theorems 1 and 2 are met. The y-axis represents convergence speed. Each point represents results from a single instance within the given model and algorithm.", "section": "3 Gradient-free decoder inversion in LDMs"}, {"figure_path": "nbqvjkOs6S/figures/figures_17_1.jpg", "caption": "Figure S1: Confusion matrices for watermark classification on LDMs. Ours is better than the encoder and works as well as the grad-based method [14].", "description": "This figure shows three confusion matrices, one for each of the three methods used for watermark classification: Encoder, Grad-based, and Grad-free (ours). Each matrix displays the number of correctly and incorrectly classified images for three different watermark types. The Grad-free method achieves comparable performance to the Grad-based method but outperforms the Encoder method.", "section": "A.2.2 Watermark detection"}, {"figure_path": "nbqvjkOs6S/figures/figures_17_2.jpg", "caption": "Figure S1: Confusion matrices for watermark classification on LDMs. Ours is better than the encoder and works as well as the grad-based method [14].", "description": "This figure presents the confusion matrices for watermark classification experiments using three different methods: Encoder, Gradient-based, and the proposed Gradient-free method.  Each confusion matrix shows the performance of each method in classifying three different tree-ring watermarks (represented by classes 1, 2, and 3). The results indicate that the Gradient-free method achieves comparable accuracy to the Gradient-based method, and significantly outperforms the Encoder method.", "section": "A.2.2 Watermark detection"}, {"figure_path": "nbqvjkOs6S/figures/figures_17_3.jpg", "caption": "Figure S2: Qualitative result on the watermarking classification experiment [14]. Our grad-free decoder inversion aids in the accurate reconstruction of Tree-ring watermarking [48]. Specifically, it either reduces the processing time compared to the grad-based (column 3 vs 5) or achieves better accuracy within the same runtime (column 4 vs 5).", "description": "This figure shows a qualitative comparison of watermark classification results using different methods: encoder, gradient-based method, shortened gradient-based method, and the proposed gradient-free method.  It demonstrates that the gradient-free method achieves comparable or better accuracy with significantly reduced runtime compared to the gradient-based methods.", "section": "A.2.2 Watermark detection"}, {"figure_path": "nbqvjkOs6S/figures/figures_19_1.jpg", "caption": "Figure 2: The six subfigures represent the relationship between cocoercivity and convergence for 3 models \u00d7 2 algorithms (vanilla forward step method and inertial KM iteration). The x-axis represents the values of min k\u2208[0,100] (EDz\u221e\u2212EDzk,z\u221e\u2212zk)/||EDz\u221e\u2212EDzk||2, which informs whether the optimization path satisfies the assumptions of Theorems 1 and 2, while the y-axis represents the convergence (i.e., ||z100\u2212z\u221e||2). The red line shows the linear function fitted by least squares. We set z\u221e=z300.", "description": "This figure shows the relationship between cocoercivity and convergence for three different latent diffusion models using two different algorithms (vanilla forward step method and inertial KM iteration). The x-axis represents the minimum cocoercivity value across iterations, indicating whether the optimization process satisfies certain assumptions. The y-axis shows the convergence rate, represented by the distance between the final latent vector and the ground truth. The red line shows a linear fit, helping visualize the overall trend.", "section": "3 Gradient-free decoder inversion in LDMs"}]