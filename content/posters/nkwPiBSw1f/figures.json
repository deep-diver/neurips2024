[{"figure_path": "nkwPiBSw1f/figures/figures_3_1.jpg", "caption": "Figure 1: The overall framework of FedDPA. Each client contains a frozen LLM, a trainable global adapter (LoRA) and a trainable local adapter (LoRA) with a specific task, where the global adapter (LoRA) is for test-time tasks and the local adapter (LoRA) is for personalization. During the training, only the parameters of the global adapter (LoRA) are transmitted to the server for aggregation.", "description": "This figure illustrates the architecture of the Federated Dual-Personalizing Adapter (FedDPA) framework. Each client has a frozen large language model (LLM) with two adapters: a global adapter for handling unseen test-time tasks and a local adapter for personalization. During training, only the global adapter's parameters are sent to the server for aggregation, enhancing efficiency and privacy. The figure showcases three clients, each with different tasks (Open QA, Question Classification, and Entailment), highlighting the distributed and personalized nature of the FedDPA approach.", "section": "4 Proposed Method"}, {"figure_path": "nkwPiBSw1f/figures/figures_5_1.jpg", "caption": "Figure 2: Frameworks of two personalized methods for local adapter (LoRA) are shown on the left, with their overall learning processes on the right.", "description": "This figure illustrates two different approaches for training the local adapter (LoRA) within the Federated Dual-Personalizing Adapter (FedDPA) framework.  The left side shows the two local adapter training methods: (a) FedDPA-F, where the local adapter is fine-tuned after global adapter training, and (b) FedDPA-T, where the local adapter is trained iteratively alongside the global adapter. The right side depicts the overall learning process for each method, showing the stages of global LoRA learning and local LoRA learning (fine-tuning or training) across communication rounds.  The different training schedules highlight the key difference in how the global and local adapters are updated and how this influences their interaction during prediction.", "section": "4.2 Personalization of Local Model"}, {"figure_path": "nkwPiBSw1f/figures/figures_8_1.jpg", "caption": "Figure 1: The overall framework of FedDPA. Each client contains a frozen LLM, a trainable global adapter (LoRA) and a trainable local adapter (LoRA) with a specific task, where the global adapter (LoRA) is for test-time tasks and the local adapter (LoRA) is for personalization. During the training, only the parameters of the global adapter (LoRA) are transmitted to the server for aggregation.", "description": "This figure illustrates the architecture of the Federated Dual-Personalizing Adapter (FedDPA) framework. Each client in the federated learning system has a frozen large language model (LLM), a global adapter (LoRA) for learning generic knowledge applicable to various tasks, and a local adapter (LoRA) for personalization. During training, only the global adapter's parameters are sent to the server for aggregation, enhancing efficiency and privacy.  The figure shows data flow between clients and the server, highlighting the communication process and model components for both training and testing.", "section": "4 Proposed Method"}, {"figure_path": "nkwPiBSw1f/figures/figures_8_2.jpg", "caption": "Figure 3: Average accuracy varies as communication rounds. ", "description": "This figure presents two subfigures showing the convergence analysis of the proposed FedDPA methods against other baselines for personalization and test-time personalization.  The first subfigure (a) displays the average performance on target local tasks across all clients, demonstrating the faster convergence of FedDPA methods compared to FedIT and superior performance compared to FedLoRA.  The second subfigure (b) illustrates the average performance on all tasks, including test-time tasks, reinforcing that FedDPA achieves faster convergence. ", "section": "6.1 Convergence Analysis"}, {"figure_path": "nkwPiBSw1f/figures/figures_16_1.jpg", "caption": "Figure 2: Frameworks of two personalized methods for local adapter (LoRA) are shown on the left, with their overall learning processes on the right.", "description": "This figure illustrates two different methods for training the local adapter in the Federated Dual-Personalizing Adapter (FedDPA) framework.  The left side shows the two approaches: \n(a) FedDPA-F (fine-tuning): The global adapter is trained first, and then the local adapter is initialized with the global adapter's parameters and fine-tuned.\n(b) FedDPA-T (training): The global and local adapters are trained iteratively in each communication round.\nThe right side shows the overall learning process for each approach, highlighting the interaction between the local and global adapter training and the communication rounds with the server.", "section": "4.2 Personalization of Local Model"}]