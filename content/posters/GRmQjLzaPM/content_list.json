[{"type": "text", "text": "BehaviorGPT: Smart Agent Simulation for Autonomous Driving with Next-Patch Prediction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zikang Zhou1\u2217 Haibo $\\mathbf{H}\\mathbf{u}^{1*}$ Xinhong Chen1 Jianping Wang1 Nan Guan1 Kui Wu2 Yung-Hui Li3 Yu-Kai Huang4 Chun Jason Xue5 ", "page_idx": 0}, {"type": "text", "text": "1City University of Hong Kong 2University of Victoria 3Hon Hai Research Institute   \n4Carnegie Mellon University 5Mohamed bin Zayed University of Artificial Intelligence {zikanzhou2-c, haibohu2-c}@my.cityu.edu.hk {xinhong.chen, jianwang, nanguan}@cityu.edu.hk wkui@uvic.ca yunghui.li@foxconn.com yukaih2@andrew.cmu.edu jason.xue@mbzuai.ac.ae ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Simulating realistic behaviors of traffic agents is pivotal for efficiently validating the safety of autonomous driving systems. Existing data-driven simulators primarily use an encoder-decoder architecture to encode the historical trajectories before decoding the future. However, the heterogeneity between encoders and decoders complicates the models, and the manual separation of historical and future trajectories leads to low data utilization. Given these limitations, we propose BehaviorGPT, a homogeneous and fully autoregressive Transformer designed to simulate the sequential behavior of multiple agents. Crucially, our approach discards the traditional separation between \"history\" and \"future\" by modeling each time step as the \"current\" one for motion generation, leading to a simpler, more parameter- and data-efficient agent simulator. We further introduce the NextPatch Prediction Paradigm (NP3) to mitigate the negative effects of autoregressive modeling, in which models are trained to reason at the patch level of trajectories and capture long-range spatial-temporal interactions. Despite having merely 3M model parameters, BehaviorGPT won first place in the 2024 Waymo Open Sim Agents Challenge with a realism score of 0.7473 and a minADE score of 1.4147, demonstrating its exceptional performance in traffic agent simulation. ", "page_idx": 0}, {"type": "text", "text": "Keywords: Multi-Agent Systems, Transformers, Generative Models, Autonomous Driving ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Autonomous driving has emerged as an unstoppable trend, with its rapid development increasing the demand for faithful evaluation of autonomy systems\u2019 reliability [32]. While on-road testing can measure driving performance by allowing autonomous vehicles (AVs) to interact with the physical world directly, the high testing cost and the scarcity of safety-critical scenarios in the real world have hindered large-scale and comprehensive evaluation. As an alternative, validating system safety via simulation has become increasingly attractive [14, 48, 53, 44] as it enables rapid testing in diverse driving scenarios simulated at a low cost. This work focuses on smart agent simulation, i.e., simulating the behavior of traffic participants such as vehicles, pedestrians, and cyclists in the digital world, which is critical for efficiently validating and iterating behavioral policies for AVs. ", "page_idx": 0}, {"type": "text", "text": "A good simulator should be realistic, matching the real-world distribution of multi-agent behaviors to support the assessment of AVs\u2019 ability to coexist with humans safely. To this end, researchers started by designing naive simulators that mainly replay the driving logs collected in the real world [27, 29]. When testing new driving policies that deviate from the ones during data collection, agents in such simulators often exhibit unrealistic interactions with AVs, owing to the lack of reactivity to AVs\u2019 behavior changes. To simulate reactive agents, traditional approaches [14, 28] apply traffic rules to control agents heuristically [45, 26], which may struggle to capture real-world complexity. Recently, the availability of large-scale driving data [6, 15, 50], the emergence of powerful deep learning tools [19, 47, 20], and the prosperity of related fields such as motion forecasting [16, 46, 59, 42, 58], have spurred the development of data-driven agent simulation [44, 4, 24, 52, 56] towards more precise matching of behavioral distribution. With the establishment of standard benchmarks like the Waymo Open Sim Agents Challenge (WOSAC) [32], which systematically evaluates the realism of agent simulation in terms of kinematics, map compliance, and multi-agent interaction, the research on data-driven simulation approaches has been further advanced [49, 35]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Existing learning-based agent simulators [44, 4, 24, 52, 56, 49, 35] mainly mirror the techniques from motion forecasting [16, 46, 59, 42, 58, 41, 18] and opt for an encoder-decoder architecture, presumably due to the similarity between the two fields. Typically, these models use an encoder to extract historical information and a decoder to predict agents\u2019 future states leveraging the encoded features. This paradigm requires manually splitting the multi-agent time series into a historical and a future segment, with the two segments being processed by separate encoders and decoders with heterogeneous architecture. For example, MVTA [49] constructs training samples by randomly selecting a \u201ccurrent\u201d timestamp to divide sequences into historical and future components. Others [52, 35] use fixed-length agent trajectories as historical scene context, conditioned on which the multi-agent future is sampled from the decoder. Nonetheless, the benefit of employing heterogeneous modules to separately encode the history and decode the future, at the cost of significantly complicating the architecture, is unclear. Moreover, the manual separation of history and future leads to low utilization of data and computation: as every point in the sequence can be used for the separation, we believe a sample-efficient framework should be able to learn from every possible history-future pair from the sequence in parallel, which cannot be easily achieved by encoder-decoder solutions owing to their heterogeneous processing for the historical and the future time steps. ", "page_idx": 1}, {"type": "text", "text": "Inspired by the success of decoder-only Large Language Models (LLMs) [37, 38, 5], we introduce a fully autoregressive Transformer architecture, dubbed BehaviorGPT, into the field of smart agent simulation to overcome the limitations of previous works. By applying homogeneous Transformer blocks [47] to the complete trajectory snippets without differentiating history and future, we arrive at a simpler, more parameter-efficient, and more sample-efficient solution for agent simulation. Utilizing relative spacetime representations [58], BehaviorGPT symmetrically models each agent state in the sequence as if it were the \u201ccurrent\u201d one and tasks each state with modeling subsequent states\u2019 distribution during training. As a result, our framework maximizes the utilization of traffic data for autoregressive modeling, avoiding wasting any learning signals available in the time series. ", "page_idx": 1}, {"type": "text", "text": "Autoregressive modeling with imitation learning, however, suffers from compounding errors [39] and causal confusion [11]. Concerning the behavior simulation task, we observed that blindly mimicking LLMs\u2019 training paradigm of next-token prediction [35], regardless of the difference in tokens\u2019 semantics across tasks, will make these issues more prominent. For a next-token prediction model embedding tokens at $10~\\mathrm{Hz}$ , a low training loss can be achieved by simply copying and pasting the current token as the next one without performing any long-range interaction reasoning in space or time. To mitigate this issue, we introduce the Next-Patch Prediction Paradigm (NP3) that enables models to reason at the patch level of trajectories, as illustrated in Figure 1. By enforcing models to autoregressively generate the next trajectory patch containing multiple time steps, which requires understanding the high-level semantics of agent behaviors and capturing long-range spatialtemporal interactions, we prevent models from leveraging trivial shortcuts during training. We equip BehaviorGPT with NP3 and attain superior performance on WOSAC [32] with merely 3M model parameters, demonstrating the effectiveness of our modeling framework for smart agent simulation. ", "page_idx": 1}, {"type": "text", "text": "Our main contributions are three-fold. First, we propose a fully autoregressive architecture for smart agent simulation, which consists of homogeneous Transformer blocks that process multi-agent long sequences with high parameter and sample efficiency. Second, we develop the Next-Patch Prediction scheme to enhance long-range interaction reasoning, leading to more realistic multi-agent simulation over a long horizon. Third, we achieve remarkable performance on the Waymo Open Motion Dataset, winning first place in the 2024 Waymo Open Sim Agents Challenge. ", "page_idx": 1}, {"type": "image", "img_path": "GRmQjLzaPM/tmp/5ddd04c3150ccd36928ba11fc189c29d926c52f0cc2d6a9a44cdb64c0d48840a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: Next-Patch Prediction Paradigm with patch sizes of 1, 5, and 10 time steps for trajectories sampled at $10\\ \\mathrm{Hz}$ . The capsules in dark red represent the agent states at the current time step $t$ , while the faded red capsules indicate agents\u2019 past states. The grey circles represent the masked agent states required for generation. Our approach groups multi-step agent states as patches, demanding each patch to predict the subsequent patch during training. ", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Multi-Agent Traffic Simulation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Multi-agent traffic simulation is essential for developing and testing autonomous driving systems. From early systems like ALVINN [36] to contemporary simulators such as CARLA [14] and SUMO [28], these platforms have used heuristic driving policies to simulate agents\u2019 reactive behaviors [8, 7, 10]. However, they struggle to capture real-world complexity since policies based on simple heuristics are not robust enough to handle all sorts of scenarios. With the availability of large-scale data and deep learning approaches, generative models like VAEs [44], GANs [24], Diffusion [56], and autoregressive models [49, 41, 35] have gained success in generating multi-agent motions, which greatly enhance the realism of simulations. Given the temporal dependency of agent trajectories, autoregressive models naturally fti the simulation task, while others require extra designs to capture such dependencies. Among the existing autoregressive models, two representatives are MotionLM [41] and Trajeglish [35]. Both of them adopt an encoder-decoder paradigm, designing complicated scene context encoders to extract historical information before autoregressive decoding. In contrast, our approach is fully autoregressive similar to decoder-only LLMs [37, 38, 5], which eliminates the need for using heterogeneous modules to process the historical and future time steps and achieves higher efficiency in terms of data and parameters via simpler architectural design. ", "page_idx": 2}, {"type": "text", "text": "2.2 Patching Operations in Transformers ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The application of patches in Transformer models has demonstrated significant potential across various data modalities. For instance, BERT [12] employs subword tokenization [40] for natural language processing, while ViT [13] segments images into 2D patches for visual understanding. The patching design has also found applications in time-series forecasting [51, 57, 34], aiming at retaining local semantics and reducing computational complexity [34]. Moreover, it has shown the effectiveness in self-supervised learning, which has significantly facilitated representation learning and contributed to excellent fine-tuning results on large datasets [2, 21, 3]. Since the task of agent simulation also involves time-series data, we expect the patching mechanism to help models effectively capture the spatial-temporal interactions in driving scenarios and enhance the realism of the generated motion. Our proposed Next-Patch Prediction Paradigm (NP3) utilizes patch-level tokens in autoregressive modeling and trains each token to generate the next patch that comprises multi-step motions, which shares some similarities to multi-token prediction in LLMs [17]. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section presents the proposed BehaviorGPT for multi-agent behavior simulation, with Figure 2 illustrating the overall framework. To begin with, we provide the formulation of our map-conditioned, multi-agent autoregressive modeling. Then, we detail the architecture of BehaviorGPT, which adopts a Transformer decoder with a triple-attention mechanism to operate sequences at the patch level. Finally, we present the objective for model training. ", "page_idx": 2}, {"type": "image", "img_path": "GRmQjLzaPM/tmp/e2c350fea81400a41dbff8e6483a3728dbc6daa4647c50922a0373e342a75592.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Figure 2: Overview of BehaviorGPT. The model takes as input the agent trajectories and the map elements, which are converted into the embeddings of trajectory patches and map polyline segments, respectively. These embeddings are fed into a Transformer decoder for autoregressive modeling based on next-patch prediction, in which the model is trained to generate the positions, velocities, and yaw angles of trajectory patches. ", "page_idx": 3}, {"type": "text", "text": "3.1 Problem Formulation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In multi-agent traffic simulation, we aim to simulate agents\u2019 future behavior in dynamic and complex environments. Specifically, we define a scenario as the composite of a vector map $M$ and the states of $N_{\\mathrm{agent}}$ agents over $T$ time steps. At each time step, the state of the $i$ -th agent $S_{i}$ includes the agent\u2019s position, velocity, yaw angle, and bounding box size. The semantic type of agents (e.g., vehicles, pedestrians, and cyclists) are also available. Given the sequential nature of agent trajectories, we formulate the problem as sequential predictions over trajectory patches, where the prediction of each patch will affect the subsequent patches. We define an agent-level trajectory patch as ", "page_idx": 3}, {"type": "equation", "text": "$$\nP_{i}^{\\tau}=S_{i}^{((\\tau-1)\\times\\ell+1):(\\tau\\times\\ell)}\\,,\\,i\\in\\{1,\\dots,N_{\\mathrm{agent}}\\}\\,,\\,\\tau\\in\\{1,\\dots,N_{\\mathrm{patch}}\\}\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\ell$ is the number of time steps covered by a patch, $N_{\\mathrm{patch}}=T/\\ell$ indicates the number of patches, and $P_{i}^{\\tau}$ represents the $\\tau$ -th trajectory patch of the $i$ -th agent, with $S_{i}^{((\\tau-1)\\times\\ell+1):(\\tau\\times\\ell)}$ denoting the states within the patch. On top of $P_{i}^{\\tau}$ , we use S1(:(\u03c4N\u22121)\u00d7\u2113+1):(\u03c4\u00d7\u2113)to denote the \u03c4-th multi-agent patch, where $P^{\\tau}$ incorporates all agents\u2019 states at the $\\tau$ -th patch. Next, we factorize the multi-agent joint distribution over patches along the time axis according to the chain rule: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(S_{1:N_{\\mathrm{agent}}}^{1:T}\\mid M\\right)=\\prod_{\\tau=1}^{N_{\\mathrm{patch}}}\\operatorname*{Pr}\\left(P^{\\tau}\\mid P^{1:(\\tau-1)},M\\right)\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\operatorname*{Pr}(S_{1:N_{\\mathrm{agent}}}^{1:T}\\mid M)$ is the joint distribution of all agents\u2019 states over all time steps conditioned on the map $M$ . Further, we factorize over agents the conditional distribution of multi-agent patches based on the assumption that agents plan their motions independently within the horizon of a patch: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(P^{\\tau}\\mid P^{1:(\\tau-1)},M\\right)=\\prod_{i=1}^{N_{\\mathrm{agent}}}\\operatorname*{Pr}\\left(P_{i}^{\\tau}\\mid P^{1:(\\tau-1)},M\\right)\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Considering the multimodality of agents\u2019 behavior within the horizon of a patch, we assume $\\mathrm{Pr}(P_{i}^{\\tau}\\mid$ $P^{1:(\\tau-1)},M)$ to be a mixture model consisting of $N_{\\mathrm{mode}}$ modes: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(P_{i}^{\\tau}\\mid P^{1:(\\tau-1)},M\\right)=\\sum_{k=1}^{N_{\\mathrm{mode}}}\\pi_{i,k}^{\\tau}\\operatorname*{Pr}\\left(P_{i,k}^{\\tau}\\mid P^{1:(\\tau-1)},M\\right)\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\pi_{i,k}^{\\tau}$ is the probability of the $k$ -th mode. Given the sequential nature of the states within a patch, we further conduct factorization over the states per mode using the chain rule: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{Pr}\\left(P_{i,k}^{\\tau}\\mid P^{1:(\\tau-1)},M\\right)=\\prod_{t=(\\tau-1)\\times\\ell+1}^{\\tau\\times\\ell}\\operatorname*{Pr}\\left(S_{i,k}^{t}\\mid S_{i,k}^{((\\tau-1)\\times\\ell+1):(t-1)},P^{1:(\\tau-1)},M\\right)\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Such an autoregressive formulation can be interpreted as planning the patch-level behavior of each agent independently (Eq. (3)), freezing agents\u2019 behavior mode per $\\ell$ time steps (Eq. (4)), and autoregressively unrolling the next state under a specific behavior mode (Eq. (5)). Under this formulation, we can flexibly adjust the replan frequency during inference to control the reactivity of agents. For example, we can let agents execute $\\bar{\\alpha}\\in\\{1,\\ldots,\\bar{\\ell}\\}$ steps of the planned motions and choose a new behavior mode after $\\alpha$ steps to react to the change in environments. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "3.2 Relative Spacetime Representation ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In our autoregressive formulation, we treat each trajectory patch as the \u201ccurrent\u201d patch that is responsible for estimating the next-patch distribution during training, contrasting many existing approaches that designate one current time step per sequence [52, 49, 23]. As a result, it is inefficient to employ the well-established agent- or polyline-centric representation from the field of motion forecasting [46, 59, 33, 42, 25, 54, 43], given that these representations are computed under the reference frames determined by one current time step per sequence. For this reason, we adopt the relative spacetime representation introduced in QCNet [58] to model the patches symmetrically in space and time, achieving simultaneous multi-agent prediction when implementing Eq. (3) and allowing parallel next-patch prediction for the modeling of Eq. (2). Under this representation, the features of each map element and agent state are derived from coordinate-independent attributes, e.g., the semantic category of a map element and the speed of an agent state. On top of this, we effectively maintain the spatial-temporal relationships between input elements via relative positional embeddings. Specifically, we use $i$ and $j$ to index two different input elements and compute the relative spatial-temporal embedding by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{R}_{j\\rightarrow i}=\\mathrm{MLP}\\left(\\|d_{j\\rightarrow i}\\|,\\,\\angle\\left(\\pmb{n}_{i},\\,d_{j\\rightarrow i}\\right),\\,\\Delta\\theta_{j\\rightarrow i},\\,\\Delta z_{j\\rightarrow i},\\,\\Delta\\tau_{j\\rightarrow i}\\right)\\,,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $R_{j\\rightarrow i}$ is the relational embedding from $j$ to $i$ , $\\lVert d_{j\\to i}\\rVert$ is the Euclidean distance between them, $\\angle(n_{i},d_{j\\rightarrow i})$ is the angle between $n_{i}$ (i.e., the orientation of $i$ ) and $d_{j\\to i}$ (i.e., the displacement vector from $j$ to $i$ ), $\\Delta\\theta_{j\\to i}/\\Delta z_{j\\to i}$ is the relative yaw/height from $j$ to $i$ , and $\\Delta\\tau_{j\\rightarrow i}$ is the time difference. ", "page_idx": 4}, {"type": "text", "text": "3.3 Map Tokenization and Agent Patching ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Before performing spatial-temporal relational reasoning among the input elements of a traffic scenario, we must convert the raw information into high-dimensional embeddings. We first embed map information by sampling points along map polylines every 5 meters and tokenizing the semantic category of each 5-meter segment (e.g., lane centerlines, road edges, and crosswalks) via learnable embeddings. The $i$ -th polyline segment\u2019s embedding is denoted by $\\hat{M}_{i}$ , which does not include any information about coordinates. On the other hand, we process agent states using attention-based patching to obtain patch-level embeddings of trajectories. For the $i$ -th agent\u2019s state $S_{i}^{t}$ at time step $t$ , we employ an MLP to transform the speed, the velocity vector\u2019s angle relative to the bounding box\u2019s heading, the size of the bounding box, and the semantic type of the agent, into a feature vector $\\hat{S}_{i}^{t}$ . To further acquire patch embeddings, we collect the feature vectors of $\\ell$ consecutive agent states and apply the attention mechanism with relative positional embeddings to them: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{P}_{i}^{\\tau}=\\mathrm{MHSA}(Q=\\hat{S}_{i}^{\\tau\\times\\ell},K=V=\\{[\\hat{S}_{i}^{t},\\,\\mathcal{R}_{i}^{t\\rightarrow(\\tau\\times\\ell)}]\\}_{t\\in\\{(\\tau-1)\\times\\ell+1,\\dots,\\tau\\times\\ell-1\\}})\\,,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\hat{P}_{i}^{\\tau}$ is the patch embedding of the $i$ -th agent at the $\\tau$ -th patch, $\\mathrm{MHSA(\\cdot)}$ denotes the multi-head self-attention [47], $[;,:]$ denotes concatenation, and $\\mathcal{R}_{i}^{t\\rightarrow(\\tau\\times\\ell)}$ indicates the positional embedding of $S_{i}^{t}$ relative to $S_{i}^{\\tau\\times\\ell}$ computed according to Eq. (6). Such an operation can be viewed as aggregating the features of $S_{i}^{((\\tau-1)\\times\\ell+1):(\\tau\\times\\ell-1)}$ into $\\hat{S}_{i}^{\\tau\\times\\ell}$ and using the embeddings fused with high-level semantics as the agent tokens in the subsequent modules. ", "page_idx": 4}, {"type": "text", "text": "3.4 Triple-Attention Transformer Decoder ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "After obtaining map tokens and the patch embeddings of agents, we employ a Transformer decoder [47] with the triple-attention mechanism to model the spatial-temporal interactions among scene elements. As illustrated in Figure 3, the triple-attention mechanism considers three distinct sources of relations in the scene, including the temporal dependencies over the trajectory patches per agent, the regulations of the map elements on the agents, and the social interactions among agents. ", "page_idx": 4}, {"type": "text", "text": "Temporal Self-Attention. This module captures the relationships among the trajectory patches of each individual agent. Similar to decoder-only LLMs [37, 38, 5], it leverages the multi-head self-attention (MHSA) with a causal mask to enforce each trajectory patch to only attend to the preceding patches of the same agent, accommodating our autoregressive formulation. The temporal MHSA is equipped with relative positional embeddings: ", "page_idx": 4}, {"type": "image", "img_path": "GRmQjLzaPM/tmp/821acbe9a2171e032a20dbea9298245091a65a1cb3a708fc0b73810dc7534841.jpg", "img_caption": ["Figure 3: Triple Attention applies attention mechanisms to model (a) agents\u2019 sequential behaviors, (b) agents\u2019 relationships with the map context, and (c) the interactions among agents. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "equation", "text": "$$\nF_{a2t,i}^{\\tau}=\\mathrm{MHSA}(Q=\\hat{P}_{i}^{\\tau},K=V=\\{[\\hat{P}_{i}^{t},\\mathcal{R}_{i}^{(t\\times\\ell)\\rightarrow(\\tau\\times\\ell)}]\\}_{t\\in\\{1,\\dots,\\tau-1\\}})\\,,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $F_{a2t,i}^{\\tau}$ and $\\hat{P}_{i}^{\\tau}$ are the temporal-aware feature vector and the patch embedding of the $i$ -th agent at the $\\tau$ -th patch, respectively, and $\\mathcal{R}_{i}^{t\\times\\ell\\rightarrow\\tau\\times\\ell}$ embeds the relative position from $S_{i}^{t\\times\\ell}$ to $S_{i}^{\\tau\\times\\ell}$ , which represents the spatial-temporal relationship between the patches $\\bar{P}_{i}^{t}$ and $P_{i}^{\\tau}$ . ", "page_idx": 5}, {"type": "text", "text": "Agent-Map Cross-Attention. Unlike natural language which only has a sequence dimension, we must also conduct spatial reasoning to consider the environmental influence on agents\u2019 behavior. To facilitate the modeling of agent-map interactions, we apply the multi-head cross-attention (MHCA) to each trajectory patch in the scenario. Considering that a scenario may comprise an explosive number of map polyline segments and that an agent would not be influenced by map elements far away, we filter the key/value map elements in MHCA using the $\\mathbf{k}$ -nearest neighbors algorithm [42, 54]. The agent-map cross-attention is formulated as ", "page_idx": 5}, {"type": "equation", "text": "$$\nF_{a2m,i}^{\\tau}={\\bf M}{\\bf H}{\\bf C}{\\bf A}(Q=F_{a2t,i}^{\\tau},K=V=\\{[\\hat{M}_{j},\\,\\mathcal{R}_{j\\rightarrow i}^{\\tau\\times\\ell}]\\}_{j\\in\\mathcal{N}(i,\\tau)})\\,,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $F_{a2m,i}^{\\tau}$ is the map-aware feature vector for the $i$ -th agent at the $\\tau$ -th patch, $\\hat{M}_{j}$ is the embedding of the $j$ -th map polyline segment, $\\mathscr{R}_{j\\rightarrow i}^{\\tau\\times\\ell}$ is the relative positional embedding between the agent state $S_{i}^{\\tau\\times\\ell}$ and the $j$ -th map polyline segment, and $\\mathcal{N}(i,\\tau)$ denotes the $\\boldsymbol{\\mathrm{k}}$ -nearest map neighbors of $S_{i}^{\\tau\\times\\ell}$ ", "page_idx": 5}, {"type": "text", "text": "Agent-Agent Self-Attention. We further capture the social interactions among agents by applying the MHSA to the space dimension of the trajectory patches. In this module, we also utilize the locality assumption induced by the $\\boldsymbol{\\mathrm{k}}$ -nearest neighbor selection for better computational and memory efficiency. Specifically, the map-aware features of trajectory patches are refined by ", "page_idx": 5}, {"type": "equation", "text": "$$\nF_{a2a,i}^{\\tau}=\\mathrm{MHSA}(Q=F_{a2m,i}^{\\tau},K=V=\\{[F_{a2m,j}^{\\tau},\\mathcal{R}_{j\\rightarrow i}^{\\tau\\times\\ell}]\\}_{j\\in N(i,\\tau)})\\,,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $F_{a2a,i}^{\\tau}$ is the feature vector enriched with spatial interaction information among agents for the i-th agent at the \u03c4-th patch, Rj\u03c4\u00d7\u2192\u2113i contains the relative information between the $i$ -th and the $j$ -th agent at the $\\tau$ -th patch, and $\\mathcal{N}(i,\\tau)$ filters the $\\boldsymbol{\\mathrm{k}}$ -nearest agent neighbors of $S_{i}^{\\tau\\times\\ell}$ . ", "page_idx": 5}, {"type": "text", "text": "Overall Decoder Architecture. Each of the attention layers above is enhanced by commonly used components in Transformers [47], including feed-forward networks, residual connections [19], and Layer Normalization [1] in a pre-norm fashion. To enable higher-order relational reasoning, we stack multiple triple-attention blocks by interleaving the three Transformer layers. We denote the ultimate feature of the $i$ -th agent at the $\\tau$ -th patch as $F_{i}^{\\tau}$ , which will serve as the input of the prediction head for next-patch prediction modeling. ", "page_idx": 5}, {"type": "text", "text": "3.5 Next-Patch Prediction Head ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Given the interaction-aware patch features output by the Transformer decoder, we develop a nextpatch prediction head to model the marginal multimodal distribution of agent trajectories, which estimates the distributional parameters of each patch\u2019s successor. ", "page_idx": 5}, {"type": "text", "text": "The following describes the process of next-patch prediction regarding the $\\tau$ -th patch of the $i$ -th agent. Based on the attention output $F_{i}^{\\tau}$ , we intend to estimate the parameters of the next patch\u2019s mixture model pre-defined with $N_{\\mathrm{mode}}$ modes. First, we use an MLP to transform $F_{i}^{\\tau}$ into $\\overline{{\\pi}}_{i}^{\\tau+1}\\in\\mathbb{R}^{N_{\\mathrm{mode}}}$ , the mixing coefficient of the modes. In each mode, the conditional distribution of the next agent state, as depicted in Eq. (5), is considered a multivariate marginal distribution that parameterizes the position and velocity components as Laplace distributions and the yaw angle as a von Mises distribution. Based on this formulation, we employ a GRU-based autoregressive RNN [9] to unroll the states within the next patch step by step, with each step being conditioned on the previously predicted states. Specifically, The hidden state $h_{i,k}^{\\tau,t}$ of the RNN is initialized with $F_{i}^{\\tau}$ at $t=1$ for $\\forall k\\in\\{1,\\ldots,N_{\\mathrm{modes}}\\}$ . At each step of the rollout, we use an MLP to estimate the location and scale parameters of the next agent state\u2019s position and velocity based on the hidden state. On the other hand, the MLP also estimates the location and concentration parameters of the next yaw angle. The location parameters of the newly predicted state, including the 3D positions, the 2D velocities, and the yaw angle, are used to update the RNN\u2019s hidden state directly without relying on the predicted scale/concentration parameters for sampling. The whole process is summarized as follows: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi_{i,k}^{\\tau+1}=\\mathrm{MLP}([F_{i}^{\\tau},Z_{k}])\\,,}\\\\ &{\\qquad\\qquad\\qquad\\quad h_{i,k}^{\\tau,1}=F_{i}^{\\tau}\\,,}\\\\ &{\\mu_{i,k}^{\\tau\\times\\ell+t},\\,\\,b_{i,k}^{\\tau\\times\\ell+t},\\,\\,\\kappa_{i,k}^{\\tau\\times\\ell+t}=\\mathrm{MLP}([h_{i,k}^{\\tau,t},Z_{k}])\\,,}\\\\ &{\\qquad\\qquad\\qquad\\quad h_{i,k}^{\\tau,t+1}=\\mathrm{RNN}(h_{i,k}^{\\tau,t},\\,\\mathrm{MLP}(\\mu_{i,k}^{\\tau\\times\\ell+t}))\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where {\u00b5i\u03c4,k\u00d7\u2113 $\\{\\mu_{i,k}^{\\tau\\times\\ell+t}\\,\\in\\,\\mathbb{R}^{6}\\}_{t\\in\\{1,\\dots,\\ell\\}}$ , $\\{b_{i,k}^{\\tau\\times\\ell+t}\\,\\in\\,\\mathbb{R}^{5}\\}_{t\\in\\{1,\\dots,\\ell\\}}$ , and $\\{\\kappa_{i,k}^{\\tau\\times\\ell+t}\\,\\in\\,\\mathbb{R}\\}_{t\\in\\{1,\\dots,\\ell\\}}$ are the location, scale, and concentration parameters in the $k$ -th mode, and $Z_{k}$ is the $k$ -th learnable mode embedding. ", "page_idx": 6}, {"type": "text", "text": "3.6 Training Objective ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "To train BehaviorGPT, we apply the negative log-likelihood loss $\\mathcal{L}_{\\mathrm{NLL}}$ to the factorized distribution of $\\operatorname*{Pr}(S_{1:N_{\\mathrm{agent}}}^{1:T}\\mid M)$ as formulated previously: ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{NLL}}=\\sum_{\\tau=1}^{N_{\\mathrm{path}}}\\sum_{i=1}^{N_{\\mathrm{grot}}}-\\log\\sum_{k=1}^{N_{\\mathrm{max}}}\\pi_{i,k}^{\\tau}\\prod_{t=(\\tau-1)\\times\\ell+1}^{\\tau\\times\\ell}\\mathrm{Pr}\\left(S_{i,k}^{t}\\mid S_{i,k}^{((\\tau-1)\\times\\ell+1):(t-1)},P^{1:(\\tau-1)},M\\right)\\,.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Note that each ground-truth trajectory patch is transformed into the viewpoint of its previous patch. During training, we utilize teacher forcing to parallelize the modeling of next-patch prediction and ease the learning difficulty, but we do not use the ground-truth agent states when updating the RNN\u2019s hidden states, intending to train the model to recover from its mistakes made in next-state prediction. ", "page_idx": 6}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "This section first introduces the dataset and the evaluation metrics used in our experiments, followed by presenting the implementation details and the rollout results obtained by BehaviorGPT on the Waymo Open Sim Agents Benchmark [32]. Finally, we conduct ablation studies to further compare and analyze the performance of BehaviorGPT under various settings. ", "page_idx": 6}, {"type": "text", "text": "4.1 Dataset and Metrics ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our experiments are conducted on the Waymo Open Motion Dataset (WOMD) [15]. The dataset comprises 486,995/44,097/44,920 training/validation/testing scenarios. Each scenario includes 91- step observations sampled at $10\\,\\mathrm{Hz}$ , totaling 9.1 seconds. Given 11-step initial states of the scenarios, we simulate up to 128 agents and generate 80 simulation steps per agent at 0.1-second intervals in an autoregressive and reactive manner. Each agent requires 32 simulations comprising x/y/z centroid coordinates and a heading value. The results on the test set are obtained by utilizing the full training set, while the performance on the validation set is based on $20\\%$ of training data unless specified. ", "page_idx": 6}, {"type": "text", "text": "We use various metrics for evaluation. The minADE measures the minimum average displacement error over multiple simulated trajectories, assessing trajectory accuracy. REALISM is the meta-metric that expects the simulations to match the real-world distribution. LINEAR SPEED and LINEAR ACCEL evaluate the realism regarding speed and acceleration. Similarly, ANG SPEED and ANG ACCEL measure the realism of angular speed and acceleration. DIST TO OBJ considers the distances to objects, while COLLISION and TTC assess the simulation performance in terms of collision and time to collision. Finally, DIST TO ROAD EDGE and OFFROAD focus on map compliance. ", "page_idx": 6}, {"type": "table", "img_path": "GRmQjLzaPM/tmp/2bc667f886357651a528b8797e7424f0ec8c2cda42390670d554f34b40146301.jpg", "table_caption": ["Table 1: Test set results in the 2024 Waymo Open Sim Agents Challenge. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "GRmQjLzaPM/tmp/fd31ed99ef44f5a539b72820ded7a743e9d45274dc21053cb5882177671ad96b.jpg", "img_caption": ["Figure 4: High-quality simulations produced by BehaviorGPT, where multimodal behaviors of agents are simulated realistically. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.2 Implementation Details ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The optimal patch size we experimented with is 10, corresponding to 1 second. All hidden sizes are set to 128. Each attention layer has 8 attention heads with 16 dimensions per head. To save training resources, we limit the maximum number of agents per scenario to 128 and restrict the maximum number of neighbors in kNN attention layers to 32. The prediction head produces 16 modes per agent and time step. We train the models for 30 epochs on 8 NVIDIA RTX 4090 GPUs with a batch size of 24, utilizing the AdamW optimizer [31]. The weight decay rate and dropout rate are both set to 0.1. The learning rate is initially set to $5\\times10^{-4}$ and decayed to 0 following a cosine annealing schedule [30]. Our results in the 2024 WOSAC are obtained using a single model with 2 decoding blocks and a total of 3M parameters. To produce 32 replicas of rollouts, we randomly sample behavior modes from agents\u2019 next-patch distributions until completing the 8-second multi-agent trajectories, and we repeat this process with different random seeds. The final results on the leaderboard are based on a replan rate of $2\\:\\mathrm{Hz}$ , while the ablation studies are based on a 1-Hz replan rate unless specified. ", "page_idx": 7}, {"type": "text", "text": "4.3 Quantitative Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We report the test set results in Table 1. Notably, BehaviorGPT achieves the lowest minADE and the best REALISM, underscoring the model\u2019s ability to match the real-world distribution. Its excellent performance on COLLISION and OFFROAD also indicates that the model has successfully captured the agent-agent and agent-map interactions in driving scenarios. Besides the benchmarking results, we also compare the number of model parameters in BehaviorGPT and other baselines. Table 1 demonstrates that BehaviorGPT, with only 3M parameters, achieves more realistic simulation than significantly larger models like MVTE [49] and GUMP [22], which demonstrates the parameter efficiency of our approach. Without employing tricks like data augmentation, model ensemble, or post-processing steps, BehaviorGPT won first place in the 2024 Waymo Open Sim Agents Challenge. ", "page_idx": 7}, {"type": "image", "img_path": "GRmQjLzaPM/tmp/619b54bacc579cb047ba01ed423c91e9f4f50f8ca743a1e3f5a3b3b84f1fc35d.jpg", "img_caption": ["Figure 5: A typical failed case produced by BehaviorGPT, where offroad trajectories are generated owing to the compounding error caused by autoregressive modeling. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.4 Qualitative Results ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Figure 4 visualizes some qualitative results of the rollouts produced by our model. In this scenario, BehaviorGPT can generate multiple plausible futures given the same initial states of agents, which demonstrates its capability of simulating diverse yet realistic agent behavior. However, we also note that autoregressive models still suffer from accumulated errors in some cases. As shown in Figure 5, the vehicle in orange gradually goes out of the road as time goes by, which indicates the inherent limitations of autoregressive generation. ", "page_idx": 8}, {"type": "text", "text": "4.5 Ablation Studies ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We conduct some ablation studies to gain a more in-depth understanding of our approach. ", "page_idx": 8}, {"type": "text", "text": "Impact of patch size. Table 2 presents the results of BehaviorGPT with varying patch sizes. According to the results, it is evident that using a patch size of 5, i.e., training and predicting with $2{\\-}\\mathrm{Hz}$ tokens, significantly outperforms the baseline without patching. Moreover, increasing the patch size to 10 further enhances the overall performance. These results demonstrate the benefits of incorporating the NP3 into agent simulation. However, changing the patch size also leads to a variation in replan frequency, which also has an influence on simulation. Next, we investigate the impact of replan frequency on the test set using the model submitted to the 2024 WOSAC. ", "page_idx": 8}, {"type": "table", "img_path": "GRmQjLzaPM/tmp/4ad5290ccb8b4304b552a8b9543e1de3fbd846eb30ce7ec5ec91c3485c3f83c5.jpg", "table_caption": ["Table 2: Impact of patch size on the validation set. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Impact of replan frequency. During inference, we vary the replan frequency of the model with a patch size of 10 by discarding a portion of the predicted states at each simulation step. As shown in Table 3, increasing the replan frequency from $1\\:\\mathrm{Hz}$ to $2\\,\\mathrm{Hz}$ can even improve the overall performance, which may benefti from the enhanced reactivity. This phenomenon demonstrates that the performance gain is not merely due to the lower replan frequency, as the model with a patch size of 10 beats that with a patch size of 5 even harder if using the same replan frequency of $2\\:\\mathrm{Hz}$ . However, using an overly high replan frequency harms the performance, as indicated by the third row of Table 3. Overall, we conclude that using a larger patch indeed helps long-term reasoning, but a moderate replan frequency is important for temporal stability, which may be neglected by prior works. ", "page_idx": 8}, {"type": "table", "img_path": "GRmQjLzaPM/tmp/64e22ea87888bacddf45f9aa857d2a01f2dfb6740ed3373477c476854523d419.jpg", "table_caption": ["Table 3: Impact of replan frequency on the test set. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Impact of multi-agent interaction modeling. We remove all agent-agent self-attention layers in the first row of Table 4 to show that modeling the interactions among agents can boost minADE and REALISM. In particular, the realism in terms of collision is improved by $34.66\\%$ when employing agent-agent self-attention. ", "page_idx": 9}, {"type": "table", "img_path": "GRmQjLzaPM/tmp/329b44d63b8ce07ed18d0fd6092902f6847c3a04150494619f5625ca94ab0281.jpg", "table_caption": ["Table 4: Impact of agent-agent self-attention on the validation set. "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "GRmQjLzaPM/tmp/5951f559b670ea178ccd18905834c99f4e8bd34ac1a48c030c0ec8070bc2a629.jpg", "table_caption": ["Table 5: Effects of training dataTable 6: Effects of model depthTable 7: Effects of model width on the validation set. on the validation set. on the validation set. "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "GRmQjLzaPM/tmp/1133312519724022db96753192211a782884fc4aeb57359441f13dae6e3a93c0.jpg", "table_caption": ["Table 8: Extrapolation ability to generate longer sequences. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Scaling with data. We train our models with different proportions of training data. All the models have 4 decoding blocks and a hidden size of 128, totaling 5M parameters. As shown in Table 5, BehaviorGPT is able to achieve remarkable performance with merely $20\\%$ of training data, which is attributed to the high data efficiency of our approach. Increasing the proportion of training data from $20\\%$ to $50\\%$ further improves the performance on minADE and REALISM, and training on $100\\%$ of the data continues to gain enhancement. Judging from the trend in Table 5, we believe that feeding more data for model training will continuously achieve better simulation performance. ", "page_idx": 9}, {"type": "text", "text": "Scaling with model size. We investigate the effects of scaling up the model size based on some preliminary experiments with $20\\%$ of training data. In Table 6, we vary the number of decoding blocks while fixing the hidden size as 128. On the other hand, we fix the number of decoding blocks as 2 and vary the hidden size, as depicted in Table 7. Based on the experimental results, we can summarize that enlarging the model consistently leads to more realistic simulation, which showcases the potential of BehaviorGPT for scaling up. ", "page_idx": 9}, {"type": "text", "text": "Extrapolation ability. We tried training a model on 5-second sequences and generating 9.1-second sequences during inference. The results in Table 8 show that this model achieves similar performance compared with the baseline trained with 9.1-second sequences, demonstrating our approach\u2019s extrapolation ability to generate longer sequences. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work introduced BehaviorGPT, a fully autoregressive architecture designed to enhance smart agent simulation for autonomous driving. By applying homogeneous Transformer blocks to entire trajectory snippets and utilizing relative spacetime representations, BehaviorGPT simplifies the modeling process and maximizes data utilization. To enable high-level understanding and long-range interaction reasoning in space and time, we developed the Next-Patch Prediction Paradigm, which tasks models with generating trajectory patches instead of single-step states. Experimental results on the Waymo Open Sim Agents Challenge demonstrate that BehaviorGPT achieves outstanding performance with merely 3M model parameters, highlighting its potential to further improve the realism of agent simulation with more data and computation. ", "page_idx": 9}, {"type": "text", "text": "Limitations. First, BehaviorGPT is currently inferior in kinematics-related performance, which can be enhanced by incorporating a kinematic model, e.g., the bicycle model. Second, the current version of BehaviorGPT does not support controlling agent behavior with specific prompts such as language and goal points. However, achieving controllable generation should be trivial given a powerful base model. Finally, we have not verified whether BehaviorGPT will facilitate the development of motion planning, which we leave as future work. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This project is supported by a grant from Hong Kong Research Grant Council under GRF project 11216323 and CRF C1042-23G. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [2] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for self-supervised learning of speech representations. In Advances in Neural Information Processing Systems, 2020.   \n[3] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEit: BERT pre-training of image transformers. In International Conference on Learning Representations, 2022. [4] Luca Bergamini, Yawei Ye, Oliver Scheel, Long Chen, Chih Hu, Luca Del Pero, B\u0142az\u02d9ej Osin\u00b4ski, Hugo Grimmett, and Peter Ondruska. Simnet: Learning reactive self-driving simulations from real-world observations. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 5119\u20135125. IEEE, 2021. [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems, 2020. [6] Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jagjeet Singh, Slawomir Bak, Andrew Hartnett, De Wang, Peter Carr, Simon Lucey, Deva Ramanan, et al. Argoverse: 3d tracking and forecasting with rich maps. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8748\u20138757, 2019.   \n[7] Dian Chen, Vladlen Koltun, and Philipp Kr\u00e4henb\u00fchl. Learning to drive from a world on rails. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15590\u201315599, 2021.   \n[8] Dian Chen, Brady Zhou, Vladlen Koltun, and Philipp Kr\u00e4henb\u00fchl. Learning by cheating. In Conference on Robot Learning, pages 66\u201375. PMLR, 2020.   \n[9] Kyunghyun Cho, Bart van Merrienboer, \u00c7aglar G\u00fcl\u00e7ehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation. In Conference on Empirical Methods in Natural Language Processing, 2014.   \n[10] Felipe Codevilla, Matthias M\u00fcller, Antonio L\u00f3pez, Vladlen Koltun, and Alexey Dosovitskiy. End-to-end driving via conditional imitation learning. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 4693\u20134700. IEEE, 2018.   \n[11] Pim De Haan, Dinesh Jayaraman, and Sergey Levine. Causal confusion in imitation learning. In Advances in Neural Information Processing Systems, 2019.   \n[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In North American Chapter of the Association for Computational Linguistics, 2019.   \n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.   \n[14] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. Carla: An open urban driving simulator. In Conference on Robot Learning, pages 1\u201316. PMLR, 2017.   \n[15] Scott Ettinger, Shuyang Cheng, Benjamin Caine, Chenxi Liu, Hang Zhao, Sabeek Pradhan, Yuning Chai, Ben Sapp, Charles R Qi, Yin Zhou, et al. Large scale interactive motion forecasting for autonomous driving: The waymo open motion dataset. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9710\u20139719, 2021.   \n[16] Jiyang Gao, Chen Sun, Hang Zhao, Yi Shen, Dragomir Anguelov, Congcong Li, and Cordelia Schmid. Vectornet: Encoding hd maps and agent dynamics from vectorized representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11525\u201311533, 2020.   \n[17] Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozi\u00e8re, David Lopez-Paz, and Gabriel Synnaeve. Better & faster large language models via multi-token prediction. arXiv preprint arXiv:2404.19737, 2024.   \n[18] Junru Gu, Chen Sun, and Hang Zhao. Densetnt: End-to-end trajectory prediction from dense goal sets. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15303\u201315312, 2021.   \n[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770\u2013778, 2016.   \n[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, 2020.   \n[21] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:3451\u20133460, 2021.   \n[22] Yihan Hu, Siqi Chai, Zhening Yang, Jingyu Qian, Kun Li, Wenxin Shao, Haichao Zhang, Wei Xu, and Qiang Liu. Solving motion planning tasks with a scalable generative model. In European Conference on Computer Vision, 2024.   \n[23] Zhiyu Huang, Zixu Zhang, Ameya Vaidya, Yuxiao Chen, Chen Lv, and Jaime Fern\u00e1ndez Fisac. Versatile scene-consistent traffic scenario generation as optimization with diffusion. arXiv preprint arXiv:2404.02524, 2024.   \n[24] Maximilian Igl, Daewoo Kim, Alex Kuefler, Paul Mougin, Punit Shah, Kyriacos Shiarlis, Dragomir Anguelov, Mark Palatucci, Brandyn White, and Shimon Whiteson. Symphony: Learning realistic and diverse agents for autonomous driving simulation. In 2022 International Conference on Robotics and Automation (ICRA), pages 2445\u20132451. IEEE, 2022.   \n[25] Xiaosong Jia, Penghao Wu, Li Chen, Yu Liu, Hongyang Li, and Junchi Yan. Hdgt: Heterogeneous driving graph transformer for multi-agent trajectory prediction via scene encoding. IEEE transactions on pattern analysis and machine intelligence, 2023.   \n[26] Arne Kesting, Martin Treiber, and Dirk Helbing. General lane-changing model mobil for car-following models. Transportation Research Record, 1999(1):86\u201394, 2007.   \n[27] Parth Kothari, Christian Perone, Luca Bergamini, Alexandre Alahi, and Peter Ondruska. Drivergym: Democratising reinforcement learning for autonomous driving. arXiv preprint arXiv:2111.06889, 2021.   \n[28] Daniel Krajzewicz, Georg Hertkorn, Christian R\u00f6ssel, and Peter Wagner. Sumo (simulation of urban mobility)-an open-source traffic simulation. In Proceedings of the 4th middle East Symposium on Simulation and Modelling (MESM20002), pages 183\u2013187, 2002.   \n[29] Quanyi Li, Zhenghao Peng, Lan Feng, Qihang Zhang, Zhenghai Xue, and Bolei Zhou. Metadrive: Composing diverse driving scenarios for generalizable reinforcement learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(3):3461\u20133475, 2022.   \n[30] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. In International Conference on Learning Representations, 2017.   \n[31] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019.   \n[32] Nico Montali, John Lambert, Paul Mougin, Alex Kuefler, Nicholas Rhinehart, Michelle Li, Cole Gulino, Tristan Emrich, Zoey Zeyu Yang, Shimon Whiteson, Brandyn White, and Dragomir Anguelov. The waymo open sim agents challenge. In Advances in Neural Information Processing Systems Track on Datasets and Benchmarks, 2023.   \n[33] Nigamaa Nayakanti, Rami Al-Rfou, Aurick Zhou, Kratarth Goel, Khaled S Refaat, and Benjamin Sapp. Wayformer: Motion forecasting via simple & efficient attention networks. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 2980\u20132987. IEEE, 2023.   \n[34] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. In The Eleventh International Conference on Learning Representations, 2023.   \n[35] Jonah Philion, Xue Bin Peng, and Sanja Fidler. Trajeglish: Traffic modeling as next-token prediction. In The Twelfth International Conference on Learning Representations, 2024.   \n[36] Dean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. In Advances in Neural Information Processing Systems, 1988.   \n[37] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. OpenAI blog, 2018.   \n[38] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 2019.   \n[39] St\u00e9phane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pages 627\u2013635, 2011.   \n[40] Mike Schuster and Kaisuke Nakajima. Japanese and korean voice search. In 2012 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 5149\u2013 5152. IEEE, 2012.   \n[41] Ari Seff, Brian Cera, Dian Chen, Mason Ng, Aurick Zhou, Nigamaa Nayakanti, Khaled S Refaat, Rami Al-Rfou, and Benjamin Sapp. Motionlm: Multi-agent motion forecasting as language modeling. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8579\u20138590, 2023.   \n[42] Shaoshuai Shi, Li Jiang, Dengxin Dai, and Bernt Schiele. Motion transformer with global intention localization and local movement refinement. In Advances in Neural Information Processing Systems, 2022.   \n[43] Shaoshuai Shi, Li Jiang, Dengxin Dai, and Bernt Schiele. $\\mathrm{Mtr++}$ : Multi-agent motion prediction with symmetric scene modeling and guided intention querying. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.   \n[44] Simon Suo, Sebastian Regalado, Sergio Casas, and Raquel Urtasun. Trafficsim: Learning to simulate realistic multi-agent behaviors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10400\u201310409, 2021.   \n[45] Martin Treiber, Ansgar Hennecke, and Dirk Helbing. Congested traffic states in empirical observations and microscopic simulations. Physical Review E, 62(2):1805, 2000.   \n[46] Balakrishnan Varadarajan, Ahmed Hefny, Avikalp Srivastava, Khaled S Refaat, Nigamaa Nayakanti, Andre Cornman, Kan Chen, Bertrand Douillard, Chi Pang Lam, Dragomir Anguelov, et al. Multipath $^{++}$ : Efficient information fusion and trajectory aggregation for behavior prediction. In 2022 International Conference on Robotics and Automation (ICRA), pages 7814\u20137821. IEEE, 2022.   \n[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, 2017.   \n[48] Eugene Vinitsky, Nathan Lichtl\u00e9, Xiaomeng Yang, Brandon Amos, and Jakob Nicolaus Foerster. Nocturne: a scalable driving benchmark for bringing multi-agent learning one step closer to the real world. In Advances in Neural Information Processing Systems Track on Datasets and Benchmarks, 2022.   \n[49] Yu Wang, Tiebiao Zhao, and Fan Yi. Multiverse transformer: 1st place solution for waymo open sim agents challenge 2023. arXiv preprint arXiv:2306.11868, 2023.   \n[50] Benjamin Wilson, William Qi, Tanmay Agarwal, John Lambert, Jagjeet Singh, Siddhesh Khandelwal, Bowen Pan, Ratnesh Kumar, Andrew Hartnett, Jhony Kaesemodel Pontes, Deva Ramanan, Peter Carr, and James Hays. Argoverse 2: Next generation datasets for self-driving perception and forecasting. In Advances in Neural Information Processing Systems Track on Datasets and Benchmarks, 2021.   \n[51] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. In Advances in Neural Information Processing Systems, 2021.   \n[52] Danfei Xu, Yuxiao Chen, Boris Ivanovic, and Marco Pavone. Bits: Bi-level imitation for traffic simulation. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 2929\u20132936. IEEE, 2023.   \n[53] Ze Yang, Yun Chen, Jingkang Wang, Sivabalan Manivasagam, Wei-Chiu Ma, Anqi Joyce Yang, and Raquel Urtasun. Unisim: A neural closed-loop sensor simulator. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1389\u20131399, 2023.   \n[54] Zhejun Zhang, Alexander Liniger, Christos Sakaridis, Fisher Yu, and Luc Van Gool. Real-time motion prediction via heterogeneous polyline transformer with relative pose encoding. In Advances in Neural Information Processing Systems, 2023.   \n[55] Zhejun Zhang, Christos Sakaridis, and Luc Van Gool. Trafficbots v1. 5: Traffic simulation via conditional vaes and transformers with relative pose encoding. arXiv preprint arXiv:2406.10898, 2024.   \n[56] Ziyuan Zhong, Davis Rempe, Danfei Xu, Yuxiao Chen, Sushant Veer, Tong Che, Baishakhi Ray, and Marco Pavone. Guided conditional diffusion for controllable traffic simulation. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 3560\u20133566. IEEE, 2023.   \n[57] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI conference on artificial intelligence, 2021.   \n[58] Zikang Zhou, Jianping Wang, Yung-Hui Li, and Yu-Kai Huang. Query-centric trajectory prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17863\u201317873, 2023.   \n[59] Zikang Zhou, Luyao Ye, Jianping Wang, Kui Wu, and Kejie Lu. Hivt: Hierarchical vector transformer for multi-agent motion prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8823\u20138833, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 14}, {"type": "text", "text": "Justification: We have clearly outlined the scope and the targeted task of this work, and presented a concise introduction of our approach. The important contributions are described in detail, and the reported results obtained by our model are tested in the Waymo Open Sim Agents Challenge. This open challenge allows for fair comparisons with other baselines. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 14}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 14}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Justification: We have discussed the limitations of this work in the paper. ", "page_idx": 14}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 14}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 14}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "Justification: BehaviorGPT is an application-oriented approach for autonomous driving. No new theory has been proposed in this work. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 15}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 15}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "Justification: We\u2019ve described our approach in detail, including the model architecture, loss function, datasets, and metrics used for training and testing. We will also make our code publicly available. ", "page_idx": 15}, {"type": "text", "text": "Guidelines: ", "page_idx": 15}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 15}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: The data used in this work is the Waymo Open Motion Dataset, which is publicly available. Our code will also be made public after the paper is published. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 16}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: As the data used in this work is the Waymo Open Motion Dataset, the training and testing details are available on the website of Waymo Open Dataset Challenge. We have also clarified other details in Section 3, Section 4, and the Appendix. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 16}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 16}, {"type": "text", "text": "Answer: [No] ", "page_idx": 16}, {"type": "text", "text": "Justification: While not conducting significance tests over results, our experiments are conducted on the Waymo Open Motion Dataset, which has a large data scale. Thus, the experimental results are stable across multiple trials, and the reported results are reliable and authentic. ", "page_idx": 16}, {"type": "text", "text": "Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 16}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 17}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The resources used for model training have been introduced clearly in the implementation details. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 17}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: There are no violations against the Code of Ethics. ", "page_idx": 17}, {"type": "text", "text": "Guidelines: ", "page_idx": 17}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 17}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 17}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 17}, {"type": "text", "text": "Justification: The proposed BehaviorGPT is particularly designed for smart agent simulation, which can benefti the verification of autonomous driving systems and is expected to promote the development of autonomous driving. While autonomous driving itself may be a doublesided sword, its negative side is out of the scope of this work and the developed BehaviorGPT will not directly bring negative societal impacts. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 18}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 18}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 18}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 18}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 18}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 18}, {"type": "text", "text": "Justification: The paper has cited the original paper that produced the Waymo Open Motion Dataset, and the information about the dataset is available online. ", "page_idx": 18}, {"type": "text", "text": "Guidelines: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 19}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 19}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 19}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 19}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 19}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 19}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 19}, {"type": "text", "text": "Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 19}, {"type": "text", "text": "Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 20}]