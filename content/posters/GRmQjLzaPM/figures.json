[{"figure_path": "GRmQjLzaPM/figures/figures_2_1.jpg", "caption": "Figure 1: Next-Patch Prediction Paradigm with patch sizes of 1, 5, and 10 time steps for trajectories sampled at 10 Hz. The capsules in dark red represent the agent states at the current time step t, while the faded red capsules indicate agents' past states. The grey circles represent the masked agent states required for generation. Our approach groups multi-step agent states as patches, demanding each patch to predict the subsequent patch during training.", "description": "This figure illustrates the Next-Patch Prediction Paradigm (NP3) used in BehaviorGPT.  It shows how the model is trained to predict future trajectory patches (groups of consecutive time steps) rather than individual time steps. Different patch sizes (1, 5, and 10 time steps) are shown, highlighting the model's ability to capture long-range spatial-temporal dependencies.  The dark red capsules represent the current state, faded red capsules the past states, and grey circles masked future states the model must predict.", "section": "3 Methodology"}, {"figure_path": "GRmQjLzaPM/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of BehaviorGPT. The model takes as input the agent trajectories and the map elements, which are converted into the embeddings of trajectory patches and map polyline segments, respectively. These embeddings are fed into a Transformer decoder for autoregressive modeling based on next-patch prediction, in which the model is trained to generate the positions, velocities, and yaw angles of trajectory patches.", "description": "This figure illustrates the overall architecture of the BehaviorGPT model.  It shows how agent trajectories and map data are processed. First, agent data and map data are separately embedded. Then, trajectory patches are created, which are fed into a Transformer decoder along with map embeddings. This decoder uses a triple-attention mechanism to incorporate temporal, agent-map, and agent-agent interactions. Finally, the decoder outputs predictions for the position, velocity, and yaw angle of each agent in subsequent trajectory patches.", "section": "3 Methodology"}, {"figure_path": "GRmQjLzaPM/figures/figures_5_1.jpg", "caption": "Figure 3: Triple Attention applies attention mechanisms to model (a) agents' sequential behaviors, (b) agents' relationships with the map context, and (c) the interactions among agents.", "description": "This figure illustrates the triple-attention mechanism in BehaviorGPT.  It shows how the model processes information from three perspectives to predict agent behavior: (a) Temporal Self-Attention considers the sequential relationship between an agent's past trajectory patches. (b) Agent-Map Cross-Attention focuses on the interaction between agents and the map context, using a k-nearest neighbor approach to efficiently manage the large number of map elements. (c) Agent-Agent Self-Attention models the social interactions between agents, also using a k-nearest neighbor strategy for computational efficiency.  Each attention mechanism uses multi-head self-attention with relative positional embeddings to capture spatial-temporal relationships.", "section": "3.4 Triple-Attention Transformer Decoder"}, {"figure_path": "GRmQjLzaPM/figures/figures_7_1.jpg", "caption": "Figure 4: High-quality simulations produced by BehaviorGPT, where multimodal behaviors of agents are simulated realistically.", "description": "This figure showcases example simulations generated by the BehaviorGPT model.  It visually compares an original scenario with three different predicted scenarios generated by the model. The maps are consistent across all four images.  The plots demonstrate that BehaviorGPT can create diverse and realistic simulations of multi-agent traffic behaviors by producing multiple plausible futures (multiple predicted scenarios) from the same starting point (original scenario).  This highlights the model's ability to handle and generate a range of possible outcomes and not just a single, deterministic prediction.", "section": "4.3 Quantitative Results"}, {"figure_path": "GRmQjLzaPM/figures/figures_8_1.jpg", "caption": "Figure 5: A typical failed case produced by BehaviorGPT, where offroad trajectories are generated owing to the compounding error caused by autoregressive modeling.", "description": "This figure showcases a failure case of the BehaviorGPT model.  The model generates trajectories that deviate from the road, resulting in \"off-road\" driving behavior. This failure is attributed to the compounding errors inherent in the autoregressive modeling approach, where small prediction errors accumulate over time leading to increasingly significant deviations from the expected path. The image highlights the limitations of solely relying on autoregressive prediction for traffic simulation without incorporating mechanisms to handle error propagation or long-range interactions.", "section": "4.4 Qualitative Results"}]