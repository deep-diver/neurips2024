[{"figure_path": "cR2QDzdpEv/figures/figures_6_1.jpg", "caption": "Figure 1: Normalized returns for the baseline (cross-entropy loss) and R\u00b3M across all noise models and noise rates. Error bars represent the standard deviation across 10 different seeds. Learning curves and percentile plots are in Appendix C.1.", "description": "This figure presents the normalized returns for both the baseline (cross-entropy loss) and the proposed R\u00b3M method across three different robotic control tasks under various noise models and noise rates.  The normalized returns are calculated relative to the average returns obtained using the ground truth reward.  Error bars show standard deviation across ten different random seeds, indicating the variability of results.  The figure shows R\u00b3M consistently outperforms the baseline under most scenarios. More detailed results (learning curves and percentile plots) are available in Appendix C.1.", "section": "5.1 Robotic control"}, {"figure_path": "cR2QDzdpEv/figures/figures_8_1.jpg", "caption": "Figure 2: (a) Comparison of the Claude 3 agreement on the annotated labels between sample pairs with zero and positive learned perturbation factors. (b) An example of corrupted annotation in the HH dataset.", "description": "This figure shows the results of comparing Claude 3's agreement with human annotators on labels for two datasets: summarization and dialogue.  Panel (a) is a bar chart showing that Claude 3 agrees more with human annotators when the learned perturbation factor (delta) is zero (no outlier).  Panel (b) shows an example from the HH dataset where an annotator gives a positive label to a response that is clearly unhelpful or harmful, demonstrating a corrupted annotation. This helps illustrate how the model identifies and handles outliers in the preference data.", "section": "Results on Perturbed Datasets"}, {"figure_path": "cR2QDzdpEv/figures/figures_8_2.jpg", "caption": "Figure 3: Comparison of winning scores between R\u00b3M and the DPO baseline across different perturbation percentages on two tasks.", "description": "This figure compares the performance of R\u00b3M and DPO on dialogue and summarization tasks with varying levels of manually introduced label noise (perturbation).  The x-axis represents the percentage of labels that were flipped.  The y-axis shows the winning score, a metric reflecting the models' performance relative to a baseline. The results demonstrate R\u00b3M's superior robustness to noisy labels across both tasks, particularly in the summarization task.", "section": "5.2 Natural Language Generation"}, {"figure_path": "cR2QDzdpEv/figures/figures_8_3.jpg", "caption": "Figure 1: Normalized returns for the baseline (cross-entropy loss) and R\u00b3M across all noise models and noise rates. Error bars represent the standard deviation across 10 different seeds. Learning curves and percentile plots are in Appendix C.1.", "description": "The figure shows the normalized returns for three different robotic control tasks (HalfCheetah, Ant, and Hopper) under three different noise models (stochastic, myopic, and irrational) with varying noise rates.  The baseline uses a standard cross-entropy loss, while R\u00b3M is the proposed robust reward modeling approach.  The results demonstrate the superior performance of R\u00b3M across all tasks, noise models, and noise rates, especially in high noise scenarios. Error bars show standard deviation, and more detailed learning curves and percentile plots are provided in Appendix C.1.", "section": "5.1 Robotic control"}, {"figure_path": "cR2QDzdpEv/figures/figures_15_1.jpg", "caption": "Figure 1: Normalized returns for the baseline (cross-entropy loss) and R\u00b3M across all noise models and noise rates. Error bars represent the standard deviation across 10 different seeds. Learning curves and percentile plots are in Appendix C.1.", "description": "This figure compares the performance of the proposed R\u00b3M method against a baseline (cross-entropy loss) across three robotic control tasks (HalfCheetah, Ant, and Hopper) under different noise models (stochastic, myopic, and irrational) with varying noise levels.  The normalized returns are plotted, showing R\u00b3M's consistent outperformance across various noise conditions, with error bars illustrating the standard deviation across multiple trials.  Appendix C.1 provides more detailed learning curves and percentile plots.", "section": "5.1 Robotic control"}, {"figure_path": "cR2QDzdpEv/figures/figures_16_1.jpg", "caption": "Figure 1: Normalized returns for the baseline (cross-entropy loss) and R\u00b3M across all noise models and noise rates. Error bars represent the standard deviation across 10 different seeds. Learning curves and percentile plots are in Appendix C.1.", "description": "The figure compares the performance of the proposed robust reward modeling method (R\u00b3M) against a baseline method (cross-entropy loss) across three different robotic control tasks under various noise conditions.  The noise is simulated using three different models: stochastic, myopic, and irrational, each with varying noise rates. The normalized returns are plotted, with error bars indicating the standard deviation across multiple trials.  Additional learning curves and percentile plots are available in the appendix.", "section": "5.1 Robotic control"}, {"figure_path": "cR2QDzdpEv/figures/figures_16_2.jpg", "caption": "Figure 5: Comparison of outlier ratios between sample pairs with zero and positive learned perturbation factors for \u03c4 = 1.0, \u03b3 = 0.3, and p = 1/3 for the stochastic, myopic, and irrational noise models, respectively", "description": "This figure compares the outlier ratios for three different noise models (stochastic, myopic, and irrational) in robotic control experiments.  It shows the proportion of outlier data points identified by the R\u00b3M algorithm, categorized by whether the learned perturbation factor (\u03b4) is zero or positive. A higher ratio of positive \u03b4 indicates a greater number of outliers successfully identified by the method.", "section": "Additional experiments"}, {"figure_path": "cR2QDzdpEv/figures/figures_17_1.jpg", "caption": "Figure 1: Normalized returns for the baseline (cross-entropy loss) and R\u00b3M across all noise models and noise rates. Error bars represent the standard deviation across 10 different seeds. Learning curves and percentile plots are in Appendix C.1.", "description": "This figure compares the performance of the proposed R\u00b3M method against a baseline (cross-entropy loss) across three robotic control tasks under various noise models (stochastic, myopic, and irrational) and noise levels.  The normalized returns, representing the average performance relative to the ground truth reward, are shown for each task and noise condition. Error bars illustrate the standard deviation across multiple runs (10 seeds). More detailed learning curves and percentile plots are available in Appendix C.1 for a comprehensive analysis.", "section": "5.1 Robotic control"}, {"figure_path": "cR2QDzdpEv/figures/figures_17_2.jpg", "caption": "Figure 1: Normalized returns for the baseline (cross-entropy loss) and R\u00b3M across all noise models and noise rates. Error bars represent the standard deviation across 10 different seeds. Learning curves and percentile plots are in Appendix C.1.", "description": "This figure displays the normalized returns achieved by the baseline model (using cross-entropy loss) and the proposed R\u00b3M model across three different robotic control tasks (HalfCheetah, Ant, and Hopper) under various noise conditions.  Three types of noise models were applied to the preference data: stochastic, myopic, and irrational, each with varying noise rates. The results are presented for each task and noise model separately. Error bars show the standard deviation across 10 runs for each condition, illustrating the variability in performance. Additional learning curves and percentile plots providing a more detailed performance analysis are available in Appendix C.1 of the paper.", "section": "5.1 Robotic control"}, {"figure_path": "cR2QDzdpEv/figures/figures_18_1.jpg", "caption": "Figure 1: Normalized returns for the baseline (cross-entropy loss) and R\u00b3M across all noise models and noise rates. Error bars represent the standard deviation across 10 different seeds. Learning curves and percentile plots are in Appendix C.1.", "description": "The figure displays the performance comparison between the baseline model (using cross-entropy loss) and the proposed R\u00b3M model across three different robotic control tasks (HalfCheetah, Ant, and Hopper) under various noise conditions (stochastic, myopic, and irrational). Each noise model simulates different types of human preference label corruption with varying noise rates (controlled by parameters \u03c4, \u03b3, and p respectively). The normalized returns of each model under each noise condition are shown, with error bars representing the standard deviation across ten trials.  Appendix C.1 provides additional details like learning curves and percentile plots.", "section": "5.1 Robotic control"}]