[{"type": "text", "text": "Pearls from Pebbles: Improved Confidence Functions for Auto-labeling ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Harit Vishwakarma ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yi Chen yi.chen@wisc.edu University of Wisconsin-Madison ", "page_idx": 0}, {"type": "text", "text": "hvishwakarma@cs.wisc.edu University of Wisconsin-Madison ", "page_idx": 0}, {"type": "text", "text": "Sui Jiet Tay \u2217 st5494@nyu.edu NYU Courant Institute ", "page_idx": 0}, {"type": "text", "text": "Satya Sai Srinath Namburi \u2217 satya.namburi@gehealthcare.com GE HealthCare ", "page_idx": 0}, {"type": "text", "text": "Frederic Sala fredsala@cs.wisc.edu University of Wisconsin-Madison ", "page_idx": 0}, {"type": "text", "text": "Ramya Korlakai Vinayak ramya@ece.wisc.edu University of Wisconsin-Madison ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Auto-labeling is an important family of techniques that produce labeled training sets with minimum manual annotation. A prominent variant, threshold-based auto-labeling (TBAL), works by finding thresholds on a model\u2019s confidence scores above which it can accurately automatically label unlabeled data. However, many models are known to produce overconfident scores, leading to poor TBAL performance. While a natural idea is to apply off-the-shelf calibration methods to alleviate the overconfidence issue, we show that such methods fall short. Rather than experimenting with ad-hoc choices of confidence functions, we propose a framework for studying the optimal TBAL confidence function. We develop a tractable version of the framework to obtain Colander (Confidence functions for Efficient and Reliable Auto-labeling), a new post-hoc method specifically designed to maximize performance in TBAL systems. We perform an extensive empirical evaluation of Colander and compare it against methods designed for calibration. Colander achieves up to $60\\%$ improvement on coverage over the baselines while maintaining error level below $5\\%$ and using the same amount of labeled data. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The demand for labeled data in machine learning (ML) is perpetual. Obtaining it is expensive and time-consuming, creating a bottleneck in ML workflows. Threshold-based auto-labeling (TBAL) is a promising solution to obtain high-quality labeled data at low cost [47, 42, 56]. A TBAL system (Fig. 1) takes unlabeled data as input and outputs a labeled dataset. It works iteratively: in each iteration, it acquires human labels for a small chunk of data to train a model, then auto-labels points using the model\u2019s predictions where its confidence scores are above a certain threshold. The threshold is determined using validation data so that the auto-labeled points meet a desired accuracy criteria. The goal is to maximize coverage\u2014the fraction of points automatically labeled (out of the total)\u2014while maintaining accuracy. TBAL powers industry products like Amazon SageMaker Ground Truth [47]. ", "page_idx": 0}, {"type": "text", "text": "The confidence function is critical to the TBAL workflow (Figure 1). Existing TBAL systems rely on common choices like softmax outputs from neural networks [42, 56]. These functions are not well aligned with the objective of the auto-labeling system. Using them results in substantially suboptimal coverage (Figure 2(a)). For this reason, we ask: ", "page_idx": 0}, {"type": "text", "text": "An ideal confidence function for autolabeling will achieve the maximum coverage at a given auto-labeling error tolerance and thus will bring down the labeling cost significantly. Finding such an ideal function, however, is difficult because of the inherent tension between accuracy and coverage. The models used in auto-labeling are often highly inaccurate so achieving a certain error guarantee is easier when being conservative in terms of confidence\u2014 but this reduces coverage. Conversely, high coverage may appear to require lowering the requirements in confi", "page_idx": 1}, {"type": "image", "img_path": "96gXvFYWSE/tmp/e6e521551317b0184821522f3b406f9ac75d4fa0cc68778c5eec2ba48bbd1ed4.jpg", "img_caption": ["Figure 1: High-level diagram of TBAL system. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "dence, but this may easily lead to overshooting the desired error level. This is compounded by the fact that TBAL is iterative, so even small deviations in error levels can cascade in future iterations. ", "page_idx": 1}, {"type": "text", "text": "Overconfidence may further stymie hopes of balancing accuracy and coverage. While overconfidence is a challenge in general, it is exacerbated in TBAL: since models are trained on a small amount of labeled data, they are often inaccurate, making the problem of designing confidence functions even more challenging. Common choices produce overconfident scores, i.e., high scores for both correct and incorrect predictions [51, 38, 17, 16, 3]. Fig. 2(a) shows that softmax scores are overconfident, resulting in poor auto-labeling performance. ", "page_idx": 1}, {"type": "text", "text": "Several methods have been introduced to address overconfidence, including a variety of calibration techniques [12]. Applying these can nevertheless miss out on significant performance (Figure 2(b)) since the calibration goal differs from auto-labeling. From the auto-labeling standpoint, we seek minimum overlap between the correct and incorrect model prediction scores. Other approaches [6, 35] bake the objective of separating scores into model training or use different optimization procedures [64] that encourage separation. We observe that these do not help TBAL either, since, after some point, the model is correct on almost all the training points, making it hard to train it to discriminate between its own correct and incorrect predictions. ", "page_idx": 1}, {"type": "text", "text": "We tackle these challenges by proposing a framework to learn suitable confidence functions for TBAL. In particular, we express the auto-labeling objective as an optimization problem over the space of confidence functions and thresholds. Our framework subsumes existing methods, i.e., they are points in the space of solutions. The resulting method, Colander (Confidence functions for Efficient and Reliable Auto-labeling), relies on a practical surrogate to the framework that can be used to learn optimal confidence functions for auto-labeling. Using these learned functions in TBAL can achieve up to $60\\%$ improvement in coverage versus baselines like softmax, temperature scaling [12], CRL [35] and FMFP [64]. We summarize our contributions as follows, ", "page_idx": 1}, {"type": "text", "text": "1. We propose a principled framework to study the choices of confidence functions suitable for auto-labeling and provide a practical method (Colander) to learn confidence functions for efficient and reliable auto-labeling.   \n2. We systematically study commonly used choices of scoring functions and calibration methods and demonstrate that they lead to poor auto-labeling performance.   \n3. Through extensive empirical evaluation on real-world datasets, we show that using the confidence scores obtained using our procedure boosts auto-labeling performance significantly in comparison to common choices of confidence functions and calibration methods. ", "page_idx": 1}, {"type": "text", "text": "2 Background and Motivation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We provide notation, background on TBAL and its relationship to other methods, and describe the importance of confidence functions. ", "page_idx": 1}, {"type": "image", "img_path": "96gXvFYWSE/tmp/7a162f29f0510c27ad7727ee9db59605a61e8b027d1455c3839a471dd6de2279.jpg", "img_caption": ["Figure 2: Scores distributions (Kernel Density Estimates) of a CNN model trained on CIFAR-10 data. (a) softmax scores of the vanilla training procedure (SGD) (b) scores after post-hoc calibration using temperature scaling and (c) scores from our Colander procedure applied on the same model. For training the CNN model we use 4000 points drawn randomly and 1000 validation points (of which 500 are used for Temp. Scaling and Colander ). The test accuracy of the model is $55\\%$ . Figures (d) and (e) show the coverage and auto-labeling error of these methods. The dotted-red line corresponds to a user-given error tolerance of $5\\%$ . "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Notation. Let $[m]:=\\{1,2,\\dots,m\\}$ for any natural number $m$ . Let $X_{u}$ be a set of unlabeled points drawn from some instance space $\\mathcal{X}$ . Let $\\mathcal{Y}=\\left\\{1,\\ldots,k\\right\\}$ be the label space. There is an unknown ground truth labeling function $f^{*}:\\mathcal{X}\\rightarrow\\mathcal{Y}$ . Let $\\mathrm{\\textcopyright}$ be a noiseless oracle that provides the true label for any point $\\textbf{x}\\in\\mathcal{X}$ . Denote the model (hypothesis) class by $\\mathcal{H}$ , where each $\\boldsymbol{\\mathscr{h}}\\in{\\mathcal{H}}$ is a function $\\beta:\\mathcal{X}\\rightarrow\\mathcal{Y}$ . Each classifier $h$ also has an associated confidence function $g:\\mathcal{X}\\to\\Delta^{k}$ that quantifies the confidence of the prediction by model $\\boldsymbol{\\mathscr{h}}\\in{\\mathcal{H}}$ on any data point $\\mathbf{x}\\in\\mathcal{X}$ . Here, $\\Delta^{k}$ is a $(k-1)$ -dimensional probability simplex. Let $\\mathbf{v}[i]$ denote the $i^{\\mathrm{th}}$ component for any vector $\\mathbf{v}\\in\\mathbb{R}^{d}$ . For any point $\\mathbf{x}\\in\\mathcal{X}$ the prediction is $\\hat{y}:=\\boldsymbol{\\ell}(\\mathbf{x})$ and the associated confidence is $g(\\mathbf{x})[\\hat{y}]$ . The vector $\\mathbf{t}$ denotes scores over $k$ -classes, and $\\mathbf t[y]$ denotes its $y^{\\mathrm{th}}$ entry, i.e., score for class $y$ . Table 3 (in Appendix B.4) contains a summary of the notation. ", "page_idx": 2}, {"type": "text", "text": "Threshold-based auto-labeling. It seeks to obtain labeled datasets while reducing the labeling burden on humans (Figure 1). The input is a pool of unlabeled data $X_{u}$ . It outputs, for each $\\mathbf{x}\\in X_{u}$ , a label $\\tilde{y}\\in\\mathcal{Y}$ . The output label could be either $y$ , from the oracle (human), or $\\hat{y}$ , from the model. Let $N_{u}$ be the number of unlabeled points, $A\\subseteq[N_{u}]$ the set of indices of auto-labeled points, and $X_{u}(A)$ be these points. Let $N_{a}$ be the size of the auto-labeled set $A$ . The auto-labeling error, denoted by $\\widehat{\\mathcal{E}}(X_{u}(A))$ , and the coverage, denoted by ${\\widehat{\\mathcal{P}}}(X_{u}(A))$ , are defined as follows: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{E}}(X_{u}(A)):=\\frac{1}{N_{a}}\\sum_{i\\in A}\\mathbb{1}(\\tilde{y}_{i}\\neq f^{*}(\\mathbf{x}_{i})),\\quad\\mathrm{~and~}\\quad\\widehat{\\mathcal{P}}(X_{u}(A)):=|A|/N_{u}=N_{a}/N_{u}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "The goal of an auto-labeling algorithm is to label the dataset so that $\\widehat{\\mathcal{E}}(X_{u}(A))\\leq\\epsilon_{a}$ while maximizing coverage ${\\widehat{\\mathcal{P}}}(X_{u}(A))$ for a user-given error tolerance parameter $\\epsilon_{a}\\in[0,1]$ . As depicted in Figure 1, the TBAL algorithm proceeds iteratively. In each iteration, it queries labels for a subset of unlabeled points from the oracle. It trains a classifier from the model class $\\mathcal{H}$ on the oracle-labeled data acquired till that iteration. It then uses the model\u2019s confidence scores on the validation data to identify the region in the instance space, where the current classifier is confidently accurate and automatically labels the points in this region. The auto-labeled points are removed from the unlabeled pool. Similarly, to maintain parity between the validation and unlabeled data in the next round, the validation points in the auto-labeling region are removed as well. These steps are executed in a loop until all the data is labeled or the budget to query oracle labels is exhausted. ", "page_idx": 2}, {"type": "text", "text": "Fundamental differences between TBAL, self-training and active learning. At first glance, TBAL may appear similar to active learning (AL) [46], self-training (ST) [2], and selective classification (SC) [10]. However, as described in [56], it is a fundamentally different technique designed with different goals. Perhaps the most substantial difference is that TBAL\u2019s aim is to create accurately labeled datasets, while the goal in AL and ST is to learn the best (in terms of generalization error) possible classifier in a given model class with limited ground truth labels. This difference is most substantial in the settings where AL converges to a bad classifier, e.g., due to incorrect choice of the model class, sampling bias, etc. [56] illustrates this notion with a scenario where TBAL coverage is above $95\\%$ while the other techniques average around $20\\%$ . See Appendix A.1 for details. ", "page_idx": 2}, {"type": "image", "img_path": "96gXvFYWSE/tmp/c9ac1642ab3f823db933a28d23dfa71e2683457090e01dda7b2a8f9a8736532a.jpg", "img_caption": ["Figure 3: Threshold-based Auto-labeling with Colander: takes unlabeled data as input, selects a small subset of data points, and obtains human labels for them to create $D_{\\mathrm{train}}^{(i)}$ and $D_{\\mathrm{val}}^{(i)}$ for the $i$ th scpolvite iteration. Trains model h\u02c6i on Dt(ri)ain D(i )l s, Dc(ia)l imnatxo itmwioz ipnagr t and Dt(ih) . Colander   takes h\u02c6i and Dc(ia)l . In contrast to the standard TBAL procedure, here we randomly .k iUcksis nign, aabse ilinnpgu tt harneds hleoalrdns are dreatgere mined to ensucroe ntfhied eanucteo -fluanbcetlieodn $\\hat{g}_{i}$ $\\hat{\\beta}_{i}$ $D_{\\mathrm{th}}^{(i)}$ (d $\\hat{g}_{i}$ $\\hat{\\mathbf{t}}_{i}$ $\\epsilon_{a}$ obtaining the thresholds the rest of the steps are the same as standard TBAL. The whole workflow runs until all the data is labeled or another stopping criterion is met. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Problems with confidence functions in TBAL. The success of TBAL hinges on the ability of the classifier\u2019s confidence scores to distinguish between correct and incorrect labels. Prior works on TBAL [56, 42] train the model with stochastic gradient descent (SGD) and use the softmax output of the model as confidence scores, which are known to be overconfident [38]. A natural choice to mitigate this problem is to use post-hoc calibration techniques, e.g., temperature scaling [12]. We evaluate these choices by running TBAL for a single round on the CIFAR-10 [24] dataset with a SimpleCNN model with 5.8M parameters [20] with error threshold $5\\%$ . Details are in Appendix A.2. ", "page_idx": 3}, {"type": "text", "text": "In Figures 2(d) and 2(e) we observe that using softmax scores from the classifier only produces $2.9\\%$ coverage while the error threshold is violated with $10\\%$ error. Using temperature scaling only increases the coverage marginally to $4.9\\%$ and still violates the threshold with error $14\\%$ . Looking closer at the scores for correct versus incorrect examples on validation data, we observe a large overlap for softmax (Figure 2(a)) and a marginal shift with considerable overlap for temperature scaling (Figure 2(b)). To overcome this challenge, we propose a novel framework (Section 3) to learn such confidence functions in a principled way. Our method in this example can achieve $50\\%$ coverage with an error of $3.4\\%$ within the desired threshold (Figure 2(c)). ", "page_idx": 3}, {"type": "text", "text": "3 Proposed Method (Colander) ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The observations in Figure 2(a) and 2(b) suggest that fixed choices of confidence functions can leave significant coverage on the table. To find a better choice in a principled manner, we develop a framework based on auto-labeling objectives\u2014maximizing coverage while having bounded autolabeling error. We instantiate it by using empirical estimates and easy-to-optimize surrogates. We use the overall TBAL workflow from [56] and introduce our method to replace the confidence (scoring) function after training the classifier. ", "page_idx": 3}, {"type": "text", "text": "3.1 Auto-labeling optimization framework ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In any iteration of TBAL, we have a model $\\boldsymbol{\\mathscr{h}}$ trained on a subset of data labeled by the oracle. This model may not be highly accurate. However, it could be accurate in some regions of the instance space, and with the help of a confidence function $g$ , we want to identify the points where the model is correct and auto-label them. As we saw earlier, arbitrary choices of $g$ perform poorly on this task. Instead, we propose a framework to find the right function from a sufficiently rich family. ", "page_idx": 3}, {"type": "text", "text": "Optimal confidence function. To find a confidence function aligned with our objective, we consider a space of thresholds $T$ (e.g. $[0,1].$ ) and functions $\\mathcal{G}:\\mathcal{X}\\rightarrow\\check{T^{k}}$ , where $T^{k}$ is the $k-$ dimensional product space of $T$ . We express the auto-labeling objective as an optimization problem (P1): ", "page_idx": 4}, {"type": "table", "img_path": "96gXvFYWSE/tmp/2aa6b9f681ca095342e1233ac939830ab42dc02e39dcd063f9a538b522ce0e77.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Here $\\mathcal{P}(g,\\mathbf{t}|h)$ and $\\mathcal{E}(g,\\mathbf{t}\\mid h)$ are the population level coverage and auto-labeling error which are defined as follows, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{P}(g,\\mathbf{t}\\mid\\hbar):=\\mathbb{P}_{\\mathbf{x}}\\big(g(\\mathbf{x})[\\widehat{y}]\\geq\\mathbf{t}[\\widehat{y}]\\big)\\quad\\mathrm{~and~}\\quad\\mathcal{E}(g,\\mathbf{t}\\mid\\hbar):=\\mathbb{P}_{\\mathbf{x}}\\big(y\\neq\\widehat{y}\\mid g(\\mathbf{x})[\\widehat{y}]\\geq\\mathbf{t}[\\widehat{y}]\\big).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The optimal $g^{\\star}$ and $\\mathbf{t}^{\\star}$ that achieve the maximum coverage while satisfying the auto-labeling error constraint belong to the solution(s) of this optimization problem. ", "page_idx": 4}, {"type": "text", "text": "3.2 Practical method to learn confidence functions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The framework provides a theoretical characterization of the optimal confidence functions and thresholds for TBAL. However, it is impractical since the distributions and $f^{\\star}$ are unknown. Next, we give a practical method based on the above framework to learn confidence functions for TBAL. ", "page_idx": 4}, {"type": "text", "text": "Empirical optimization problem. Since we do not know the distributions of $\\mathbf{x}$ and $f^{\\star}$ , we use estimates of coverage and auto-labeling errors on a fraction of validation data to solve the optimization problem. Let $D$ be some finite number of labeled samples, and then the empirical coverage and auto-labeling error are defined as follows, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{P}}(g,\\mathbf{t}\\mid h,D):=\\frac{1}{|D|}\\sum_{(\\mathbf{x},y)\\in D}\\mathbb{1}\\big(g(\\mathbf{x})[\\widehat{y}]\\ge\\mathbf{t}[\\widehat{y}]\\big),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widehat{\\mathcal{E}}(g,\\mathbf{t}\\mid\\hbar,D):=\\frac{\\sum_{(\\mathbf{x},y)\\in D}\\mathbb{1}\\!\\left(y\\neq\\hat{y}\\wedge g(\\mathbf{x})[\\hat{y}]\\geq\\mathbf{t}[\\hat{y}]\\right)}{\\sum_{(\\mathbf{x},y)\\in D}\\mathbb{1}\\!\\left(g(\\mathbf{x})[\\hat{y}]\\geq\\mathbf{t}[\\hat{y}]\\right)}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We randomly split the validation data into two parts $D_{\\mathrm{cal}}$ and $D_{\\mathrm{th}}$ and use $D_{\\mathrm{cal}}$ to compute $\\widehat{\\mathcal{P}}(g,\\mathbf{t_{\\theta}}|$ $\\hbar,D_{\\mathrm{cal}})$ and $\\widehat{\\mathcal{E}}(g,\\mathbf{t}\\mid\\boldsymbol{\\hbar},D_{\\mathrm{cal}})$ . Using these estimates, we now seek to solve the following problem, ", "page_idx": 4}, {"type": "table", "img_path": "96gXvFYWSE/tmp/10d0b99552041c3917def6f40df4e697b1864f19b35bc7e84df3452bc52055f9.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Nevertheless, the presence of 0-1 variables means the problem remains challenging. ", "page_idx": 4}, {"type": "text", "text": "Surrogate optimization problem. To make the optimization (P2) tractable using gradientbased methods, we introduce differentiable surrogates for the 0-1 variables. Let $\\bar{\\sigma}(\\bar{\\alpha},z):=$ $1/(1+\\exp(-\\alpha z))$ denote the sigmoid function on $\\mathbb{R}$ with scale parameter $\\alpha\\,\\in\\,\\mathbb{R}$ . It is easy to see that, for any $g,y$ and $\\mathbf{t}$ , $g(\\mathbf{x})[y]\\geq\\mathbf{t}[y]\\iff\\sigma(\\alpha,g(\\mathbf{x})[y]-\\Bar{\\mathbf{t}[y]})\\geq1/2$ . Using this fact, we define the following surrogates of the auto-labeling error and coverage: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widetilde{\\mathcal{P}}(g,\\mathbf{t}|h,D_{\\mathrm{cal}}):=\\frac{1}{|D_{\\mathrm{cal}}|}\\sum_{(\\mathbf{x},y)\\in D_{\\mathrm{cal}}}\\sigma\\big(\\alpha,g(\\mathbf{x})[\\widehat{y}]-\\mathbf{t}[\\widehat{y}]\\big),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\widetilde{\\delta}(g,\\mathbf{t}\\mid\\hbar,D_{\\mathrm{cal}}):=\\frac{\\sum_{(\\mathbf{x},y)\\in D_{\\mathrm{cal}}}\\mathbb{1}\\left(y\\neq\\hat{y}\\right)\\sigma\\big(\\alpha,g(\\mathbf{x})[\\hat{y}]-\\mathbf{t}[\\hat{y}]\\big)}{\\sum_{(\\mathbf{x},y)\\in D_{\\mathrm{cal}}}\\sigma\\big(\\alpha,g(\\mathbf{x})[\\hat{y}]-\\mathbf{t}[\\hat{y}]\\big)},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and the surrogate optimization problem as follows, ", "page_idx": 4}, {"type": "table", "img_path": "96gXvFYWSE/tmp/bc73a65f4250bbea5a1afcd133d6384b3434594354bc7b0dffada79c2b0b7adc.jpg", "table_caption": [], "table_footnote": [], "page_idx": 4}, {"type": "text", "text": "Here, $\\lambda\\in\\mathbb{R}^{+}$ is the penalty term controlling the relative importance of the auto-labeling error and coverage. We tune it with the procedure discussed in Section 4.3. The gap between the surrogate and actual coverage diminishes as $\\alpha\\to\\infty$ . We discuss this in the Appendix. ", "page_idx": 4}, {"type": "text", "text": "Choice of $\\mathcal{G}$ . Our framework is flexible with respect to the choice of function class $\\mathcal{G}$ . In this work, we use neural networks with at least two layers on model class $\\mathcal{H}$ . We use representations from the last two layers as input for the functions in $\\mathcal{G}$ (Figure 4). Let $\\mathbf{z}^{(1)}(\\mathbf{x};\\bar{\\boldsymbol{h}})\\ \\in\\ \\mathbb{R}^{k}$ and $\\mathbf{z}^{(2)}(\\mathbf{x};h)\\in\\mathbb{R}^{d_{2}}$ be the outputs of the last and the second-last layer of the net $h$ for input $\\mathbf{x}$ and let $\\mathbf{z}(\\mathbf{x};h)\\,:=\\,[\\mathbf{z}^{(1)}(\\mathbf{x};h),\\mathbf{z}^{(2)}(\\mathbf{x};h)]$ denote the concatenation. This input is passed to network $\\mathcal{G}_{n n_{2}}:\\mathbb{R}^{k+d_{2}}\\mapsto\\Delta^{k}$ ; it outputs confidence scores for the $k$ classes. Specifically $g$ is defined as $g(\\mathbf{x}):=\\sf s o f t m a x\\big(\\mathbf{W}_{2}t a n h(\\mathbf{W}_{1}\\mathbf{z}(\\mathbf{x};\\hbar))\\big)$ . Here $\\mathbf{W}_{1}\\in\\mathbb{R}^{(k+d_{2})\\times2(k+d_{2}^{-})}$ and $\\mathbb{R}^{2(k+d_{2})\\times k}$ are the learnable weight matrices. As usual, for $\\mathbf{v}\\in\\mathbb{R}^{d}$ , softma $\\begin{array}{r}{\\mathfrak{c}(\\mathbf{v})[i]:=\\exp(\\mathbf{v}[i])/(\\sum_{j}\\exp(\\mathbf{v}[j]))}\\end{array}$ and $\\begin{array}{r}{\\mathbf{tanh}(\\mathbf{v})[i]:=(\\exp(2\\mathbf{v}[i])-1)/(\\exp(2\\mathbf{v}[i])+1).}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "We emphasize, Colander can use any function class for $\\mathcal{G}:\\mathcal{X}\\rightarrow T^{k}$ . Here, we chose 2-layer nets and successfully used the same across all experiments, thus we may not need an exhaustive architecture search. Intuitively, we do not need a large network for $g$ since $\\boldsymbol{\\mathscr{h}}$ already performs the heavier representation learning work. As a result, simple models are preferable to avoid overfitting and to reduce training time since post-hoc methods should be fast. ", "page_idx": 5}, {"type": "text", "text": "Solving the surrogate optimization. The optimization problem (P3) is nonconvex. Nevertheless, it is differentiable and we can apply gradient-based methods. We solve for $g$ and t simultaneously using Adam [23]. Training details, including hyperparameters, are deferred to the Appendix. ", "page_idx": 5}, {"type": "image", "img_path": "96gXvFYWSE/tmp/0bf87a8582600d8781b7614d8c0edd82319053302c73e76b128b14446f138f46.jpg", "img_caption": ["classification model, ", "Figure 4: Our choice of $g$ function. "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "3.3 TBAL procedure with Colander ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "We take the workflow of TBAL and plugin our method Colander to learn the new confidence function and threshold. We discuss the updated workflow below and place the detailed Algorithms 1 and 2 in the Appendix B due to space constraints. ", "page_idx": 5}, {"type": "text", "text": "1. Initialization. First, select $n_{s}$ points randomly from $X_{u}$ and obtain human labels for them to create initial training data $D_{\\mathrm{train}}^{(1)}$ . This is written as RANDOMQUERY $(X_{u},n_{s})$ in Algorithm 1. The procedure RANDOMQUERY $(\\overrightharpoon{X_{u}},n_{s})$ selects $n_{s}$ points randomly from $X_{u}$ and obtains human labels for them to create $D_{\\mathrm{train}}^{(1)}$ . The steps of this procedure are detailed in Algorithm 4 in the Appendix B. ", "page_idx": 5}, {"type": "text", "text": "r2o. uTnrd $i$ ,n t hcel apsrsoicfeicdautiroe nT RmAoIdNelM. OADftEeLr $(\\mathcal{H},D_{\\mathrm{train}}^{(i)})$ utrmaianns- laa bmeoldeedl  tfrraoinmi nmg oddaetl a $D_{\\mathrm{train}}^{(i)}$ $\\mathcal{H}$ foonr  tthhee  tcrauirnrienngt data Dt(ri)ain . Any training procedure can be used here. We use methods listed in Section ?? for model training. This step outputs a model $\\hat{h}_{i}$ trained on $D_{\\mathrm{train}}^{(i)}$ . Note, this model $\\hat{h}_{i}$ may not be highly accurate due to several factors such as the small amount of training data, the choice of $\\mathcal{H}$ , the training algorithm, and its hyperparameters. Indeed, TBAL does not expect the model to have high accuracy but it aims to identify and auto-label points where the model\u2019s accuracy is at least $1-\\epsilon_{a}$ . ", "page_idx": 5}, {"type": "text", "text": "3. Learn new confidence function using Colander . The model $\\hat{h}_{i}$ obtained in the previous step also produces softmax scores that can be used for auto-labeling. However, as we saw earlier in Section 2, using these scores may lead to poor auto-labeling performance. Thus, we plug in our procedure Colander to learn new scores designed to auto-label as many points as possible with the current model $\\hat{\\beta}_{i}$ while respecting the error constraint. We first randomly splits the validation data $D_{\\mathrm{val}}^{(i)}$ into $D_{\\mathrm{cal}}^{(i)}$ and $D_{\\mathrm{th}}^{(i)}$ using procedure $\\mathrm{RANDOMSPLIT}(D_{\\mathrm{val}}^{(i)},\\nu)$ . The part $D_{\\mathrm{cal}}^{(i)}$ has a fraction $\\nu$ of the points from $D_{\\mathrm{val}}^{(i)}$ . Then we consider problem P3 with $\\hat{\\beta}_{i}$ and $D_{\\mathrm{cal}}^{(i)}$ . We solve it to obtain the post-hoc confidence function $\\hat{g}_{i}$ , which is expected to provide the most appropriate scores for TBAL. ", "page_idx": 5}, {"type": "text", "text": "We get thresholds $\\hat{\\mathbf{t}}_{i}^{\\prime}$ as output from Colander, and it is tempting to use these along with $\\hat{g}_{i}$ for auto-labeling. However, these thresholds may violate the auto-labeling error constraint as they are obtained by solving the relaxed optimization problem. Thus, it is crucial to estimate reliable thresholds $\\hat{\\mathbf{t}}_{i}$ from the held-out data $D_{\\mathrm{th}}^{(i)}$ to ensure the auto-labeling error constraint is not violated. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "4. Threshold estimation. The scores from the new confidence function $\\hat{g}_{i}$ on $D_{\\mathrm{th}}^{(i)}$ are used to estimate auto-labeling thresholds in Algorithm 2. This procedure finds thresholds for each class separately. It first splits the points in $D_{\\mathrm{th}}^{(i)}$ according to the ground truth class into subsets $D_{\\mathrm{th}}^{(i,y)}$ Then, for each class $y$ , it finds the auto-labeling threshold by selecting the minimum threshold $t$ such that the estimate of auto-labeling error for class $y$ , $\\widehat{\\mathcal{E}}_{y}(\\widehat{g}_{i},t|\\widehat{\\boldsymbol{h}}_{i},D_{\\mathrm{th}}^{(i,y)})$ plus a confidence interval $\\widehat{\\zeta}(\\widehat{\\mathcal{E}}_{y}(\\widehat{g}_{i},t|\\widehat{\\boldsymbol{h}}_{i},D_{\\mathrm{th}}^{(i,y)}))$ estimated on points in $D_{\\mathrm{th}}^{(i,y)}$ having scores above $t$ , is at most the given error tolerance $\\epsilon_{a}$ . Here $\\widehat{\\zeta}(z)=C_{1}\\sqrt{z(1-z)}$ for $z\\in[0,1]$ and $C_{1}\\geq0$ is a hyperparameter. ", "page_idx": 6}, {"type": "text", "text": "5. Auto-labeling. This is a simple step. We compute the scores on the remaining unlabeled data $X_{u}^{(i)}$ using the function $\\hat{g}_{i}$ and any point $\\mathbf{x}\\in X_{u}^{(i)}$ having score above $\\hat{\\mathbf{t}}[\\hat{y}]$ is assigned auto-label $\\hat{y}=\\hat{\\boldsymbol{h}}_{i}({\\bf x})$ , and the points that did not meet this criterion remain unlabeled. ", "page_idx": 6}, {"type": "text", "text": "6. Remove auto-labeled points. The points that got auto-labeled in the previous steps are removed from the unlabeled pool. To make the validation data consistent with this unlabeled pool for the next round, the points in the validation data that fall into the auto-labeling region are also removed. ", "page_idx": 6}, {"type": "text", "text": "7. Get more human-labeled data. Lastly, it calls the procedure ACTIVEQUERY $(\\hat{\\boldsymbol{h}}_{i},X_{u}^{(i)},n_{b})$ to select $n_{b}$ points from the remaining unlabeled pool using an active learning strategy. This newly acquired human-labeled data is added to the training data $\\bar{D}_{\\mathrm{train}}^{(i)}$ . The details of the querying strategy are in Algorithm 3 Appendix B. Note, that the TBAL procedure is flexible to work with any choice of active querying strategy. We pick a simple strategy based on random sampling from the points where the classifier is most uncertain. To avoid confounding in TBAL with other scores we use the softmax scores from the classifier to determine uncertainty here. ", "page_idx": 6}, {"type": "text", "text": "The procedure then moves to step 2 and runs the loop until there are no more unlabeled points left or it has queried the stipulated number of human labels $N_{t}$ . ", "page_idx": 6}, {"type": "text", "text": "4 Empirical Evaluation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We validate the following claims through extensive empirical evaluation, ", "page_idx": 6}, {"type": "text", "text": "C1. Colander learns better confidence functions for auto-labeling compared to standard training and common post-hoc methods that seek to mitigate the overconfidence problem. Using it in TBAL can boost the coverage significantly while keeping the auto-labeling error low. ", "page_idx": 6}, {"type": "text", "text": "C2. Colander is independent of any particular train-time method and thus should help improve the performance when coupled with different train-time methods. ", "page_idx": 6}, {"type": "text", "text": "4.1 Baselines ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We examine several train-time and post-hoc methods that improve confidence functions from the calibration and ordinal ranking perspectives. Details of these methods are in the Appendix C.7. ", "page_idx": 6}, {"type": "text", "text": "Train-time methods. We use the following methods for training the model $\\widehat{h}$ . Vanilla neural networks are trained with the cross-entropy loss using stochastic gradient descent (SGD) [1, 4, 12]. Squentropy [19] adds the average square loss over the incorrect classes to the cross-entropy loss to improve the calibration and accuracy of the model. Correctness Ranking Loss (CRL) [35] aligns the confidence scores of the model with the ordinal rankings criterion via regularization. FMFP [64] aligns confidence scores with the ordinal rankings criterion by using sharpness-aware minimization (SAM) [7] in lieu of SGD. ", "page_idx": 6}, {"type": "text", "text": "Post-hoc methods. We use the following methods for learning (or updating) the confidence function $\\hat{g}$ after learning $\\widehat{h}$ . Temperature scaling [12] is a variant of Platt scaling [41]. It rescales the logits by a learnable scalar parameter. Top-Label Histogram-Binning [14] builds on the histogram-binning method [62] and focuses on calibrating the scores of the predicted label assigned to unlabeled points. Scaling-Binning [28] applies temperature scaling and then bins the confidence function values. Dirichlet Calibration [26] models the distribution of predicted probability vectors separately on instances of each class and assumes Dirichlet class conditional distributions. Adaptive Temperature Scaling [21] builds on top of temperature scaling and considers that different samples contribute to the calibration error differently. Each train-time method is piped with a post-hoc method, yielding a total of $4\\times6=24$ methods. ", "page_idx": 6}, {"type": "table", "img_path": "96gXvFYWSE/tmp/27704e598f5a75d8e216d9ae7d9c5feaf64e1449ea649685e80f6f2ec009414f.jpg", "table_caption": [], "table_footnote": ["Table 1: Details of the dataset and model we used to evaluate the performance of our method and other calibration methods. For the Tiny-Imagenet and 20 Newsgroup datasets, we use CLIP and FlagEmbedding, respectively, to obtain the embeddings of these datasets and conduct auto-labeling on the embedding space. For Tiny-Imagenet, we use a 3-layer perceptron with 1,000, 500, 300 neurons on each layer as model $\\boldsymbol{\\mathscr{h}}$ ; for 20 Newsgroup, we use a 3-layer perceptron with 1,000, 500, 30 neurons on each layer as model $\\boldsymbol{\\mathscr{h}}$ . "], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "4.2 Datasets and models ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We evaluate the performance of auto-labeling on four datasets. Each is paired with a model for auto-labeling: MNIST [30] is a hand-written digits dataset. We use the LeNet [31] for auto-labeling. CIFAR-10 [24] is an image dataset with 10 classes. We use a CNN with approximately 5.8M parameters [20] for auto-labeling. Tiny-ImageNet [29] is an image dataset comprising 100K images across 200 classes. We use CLIP [43] to derive embeddings for the images in the dataset and use an MLP model. 20 Newsgroups [34] is a natural language dataset comprising around 18K news posts across 20 topics. We use the FlagEmbedding [58] to obtain text embeddings and use an MLP model. ", "page_idx": 7}, {"type": "text", "text": "4.3 Hyperparameter search and evaluation ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The complexity of TBAL workflow and lack of labeled data make hyperparameter search and evaluation challenging. Similar challenges have been observed in active learning [32]. We discuss our practical approach and defer the details to Appendix C.10 and code2. ", "page_idx": 7}, {"type": "text", "text": "Hyperparameter search. We run only the first round of TBAL with each method using a hyperparameter combination 5 times and measure the mean auto-labeling error and mean coverage on $D_{\\mathrm{hyp}}$ , which represents a small part of the held-out human-labeled data. We pick the combination that yields the lowest average auto-labeling error while maximizing the coverage. We first find the best hyperparameters for each train-time method, fix those, and then search the hyperparameters for the post-hoc methods. Note that the best hyperparameter for a post-hoc method depends on the training-time method that it pipes to. The hyperparameter search spaces are in the Appendix $\\textrm{C}$ ; and the selected values used for each setting are in the supplementary material. ", "page_idx": 7}, {"type": "text", "text": "Performance evaluation. After fixing the hyper-parameters, we run TBAL with each combination of train-time and post-hoc method on full $X_{u}$ of size $N$ , with a fixed budget of $N_{t}$ labeled training samples and $N_{v}$ validation samples. The details of these values for each dataset are in Table 1 in Appendix C. Here, we know the ground truth labels for the points in $X_{u}$ , so we measure the auto-labeling error and coverage as defined in (1) and report them in Table 2. ", "page_idx": 7}, {"type": "text", "text": "4.4 Results and discussion ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Our findings, shown in Table 2, are: ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "C1: Colander improves TBAL performance. Our approach aims to optimize the confidence function to maximize coverage while minimizing errors. When applied to TBAL, we expect it to yield substantial coverage enhancement and error reduction compared to vanilla training and softmax scores. Indeed, the results in Table 2 corresponding to the vanilla training match our expectations. We see across all data settings, our method achieves significantly higher coverage while keeping autolabeling error below the tolerance level of $5\\%$ . The improvements are even more pronounced when ", "page_idx": 7}, {"type": "table", "img_path": "96gXvFYWSE/tmp/dd61b9b3ac4ddf53599926f0665d52fd0713a162c9490b525aff0f818947e486.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 2: In every round the error was enforced to be below $5\\%$ ; \u2018TS\u2019 stands for Temperature Scaling, \u2018SB\u2019 stands for Scaling Binning, \u2018Top-HB\u2019 stands for Top-Label Histogram Binning. \u2018AdaTS\u2019 stands for Adaptive Temperature Scaling. The column Err stands for auto-labeling error and Cov stands for coverage. Each cell value is mean $\\pm$ std. deviation on 5 repeated runs with different random seeds. ", "page_idx": 8}, {"type": "text", "text": "the datasets are more complex than MNIST. Also consistent with our expectation and observations in Figure 2(b), the post-hoc calibration methods improve the coverage over using softmax scores but at the cost of slightly higher error. While they are reasonable choices to apply in the TBAL pipeline, they fall short of maximally improving TBAL performance due to the misalignment of goals. ", "page_idx": 8}, {"type": "text", "text": "C2: Colander is compatible with and improves over other train-time methods. Our method is compatible with various choices of train-time methods, and if a train-time method (Squentropy here) provides a better model relative to another train-time method (e.g., Vanilla), then our method exploits this gain and pushes the performance even further. Across different train-time methods, we do not see significant differences in the performance, except for Squentropy. Using Squentropy with softmax improves the coverage by as high as $6.7\\%$ while dropping the auto-labeling error in contrast to using softmax scores obtained with other train-time methods for the Tiny-ImageNet setting. This is unexpected: Squentropy adds the average square loss over the incorrect classes as a regularizer, and it has offered better accuracy and calibration compared to training with cross-entropy loss. ", "page_idx": 8}, {"type": "text", "text": "Train-time methods designed for ordinal ranking objective perform poorly in auto-labeling. CRL and FMFP are state-of-the-art methods designed to produce scores aligned with the ordinal ranking criteria. Ideally, if the scores satisfy this criterion, TBAL\u2019s performance would improve. However, we do not see any significant difference from the Vanilla method. Similar to the other baselines, their evaluation is focused on models trained on large amounts of data. But, in TBAL, we have less data for training. The training error goes to zero after some rounds, and no information is left for the CRL loss to distinguish between correct and incorrect predictions (i.e., count SGD mistakes). On the other hand, FMFP is based on a hypothesis that training models using Sharpness Aware Minimizer (SAM) could lead to scores satisfying the ordinal ranking criteria. However, this phenomenon is still not well understood, especially in settings like ours with limited training data. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Data labeling. We briefly discuss prominent methods for labeling. Crowdsourcing [45, 50] uses a crowd of non-experts to complete a set of labeling tasks. Works in this domain focus on denoising the obtained information, modeling label errors, and designing effective labeling tasks [11, 22, 33, 53, 52, 54, 5]. Weak supervision (WS), in contrast, emphasizes labeling through multiple inexpensive but noisy sources, not necessarily human [44, 48, 55, 18, 49, 60, 63]. Works such as [44, 8] concentrate on binary or multi-class labeling, while [48, 55] extend WS to structured prediction tasks. ", "page_idx": 9}, {"type": "text", "text": "Auto-labeling occupies an intermediate position between weak supervision and crowdsourcing in terms of human dependency. It aims to minimize costs to obtain human labels while generating high-quality labeled data using a specific model. [42] use a TBAL-like algorithm and explore the cost of training for auto-labeling with large-scale model classes. Recent work [56] theoretically analyzes the sample complexity of validation data required to guarantee the quality of auto-labeled data. ", "page_idx": 9}, {"type": "text", "text": "Overconfidence and calibration. The issue of overconfidence [51, 38, 16, 3] is detrimental in several applications (such as robustness to out-of-distribution points [59, 57]), including ours. Many solutions have emerged to mitigate the overconfidence and miscalibration problems. Gawlikowski et al. [9] provide a comprehensive survey on uncertainty quantification and calibration techniques for neural networks. Guo et al. [12] evaluated a variety of solutions ranging from the choice of network architecture, model capacity, weight decay regularization [25], histogram-binning and isotonic regression [61, 62] and temperature scaling [41, 39] which they found to be the most promising solution. The solutions fall into two broad categories: train-time and post-hoc. Train-time solutions modify the loss function, include additional regularization terms, or use different training procedures [27, 37, 36, 19]. On the other hand, post-hoc methods such as top-label histogram-binning [13], scaling binning [28], Dirichlet calibration [26] calibrate the scores directly or learn a model that corrects miscalibrated confidence scores. ", "page_idx": 9}, {"type": "text", "text": "Beyond calibration. While calibration aims to match the confidence scores with a probability of correctness, it is not the precise solution to the overconfidence problem in many applications, including our setting. The desirable criteria for scores for TBAL are closely related to the ordinal ranking criterion [17]. To get such scores, Corbi\u00e8re et al. [6] add a module in the net for failure prediction, Zhu et al. [64] switch to sharpness aware minimization [7] to learn the model; CRL [35] regularizes the loss. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We studied issues with confidence scoring functions used in threshold-based auto-labeling (TBAL). We showed that the commonly used confidence functions and calibration methods can often be a bottleneck, leading to poor performance. We proposed Colander to learn confidence functions that are aligned with the TBAL objective. We evaluated our method extensively against common baselines on several real-world datasets and found that it improves the performance of TBAL significantly in comparison to the several common choices of confidence function. Our method is compatible with several choices of methods used for training the classifier in TBAL and using it in conjunction with them improves TBAL performance further. A limitation of Colander is that, similar to other post-hoc methods it also requires validation data to learn the confidence function. Reducing (or eliminating) this dependence on validation data could be an interesting future work. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was partly supported by funding from the American Family Data Science Institute. We thank Heguang Lin, Changho Shin, Dyah Adila, Tzu-Heng Huang, John Cooper, Aniket Rege, Daiwei Chen and Albert Ge for their valuable inputs. We thank the anonymous reviewers for their valuable comments and constructive feedback on our work. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] S.-i. Amari. Backpropagation and stochastic gradient descent method. Neurocomputing, 5(4-5), 1993.   \n[2] M.-R. Amini, V. Feofanov, L. Pauletto, L. Hadjadj, E. Devijver, and Y. Maximov. Self-training: A survey, 2023.   \n[3] Y. Bai, S. Mei, H. Wang, and C. Xiong. Don\u2019t just blame over-parametrization for overconfidence: Theoretical analysis of calibration in binary classification. In Proceedings of the 38th International Conference on Machine Learning, volume 139, pages 566\u2013576, 2021.   \n[4] L. Bottou. Stochastic Gradient Descent Tricks. Springer Berlin Heidelberg, 2012. ISBN 978-3-642-35289-8.   \n[5] Y. Chen, R. K. Vinayak, and B. Hassibi. Crowdsourced clustering via active querying: Practical algorithm with theoretical guarantees. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, volume 11, pages 27\u201337, 2023.   \n[6] C. Corbi\u00e8re, N. THOME, A. Bar-Hen, M. Cord, and P. P\u00e9rez. Addressing failure prediction by learning model confidence. In Advances in Neural Information Processing Systems 32, pages 2902\u20132913. 2019.   \n[7] P. Foret, A. Kleiner, H. Mobahi, and B. Neyshabur. Sharpness-aware minimization for efficiently improving generalization. In International Conference on Learning Representations, 2021.   \n[8] D. Y. Fu, M. F. Chen, F. Sala, S. M. Hooper, K. Fatahalian, and C. R\u00e9. Fast and three-rious: Speeding up weak supervision with triplet methods. In Proceedings of the 37th International Conference on Machine Learning (ICML 2020), 2020.   \n[9] J. Gawlikowski, C. R. N. Tassi, M. Ali, J. Lee, M. Humt, J. Feng, A. Kruspe, R. Triebel, P. Jung, R. Roscher, et al. A survey of uncertainty in deep neural networks. arXiv preprint arXiv:2107.03342, 2021.   \n[10] Y. Geifman and R. El-Yaniv. Selective classification for deep neural networks. In Advances in Neural Information Processing Systems, volume 30, 2017.   \n[11] R. G. Gomes, P. Welinder, A. Krause, and P. Perona. Crowdclustering. In Advances in Neural Information Processing Systems 24, pages 558\u2013566. 2011.   \n[12] C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger. On calibration of modern neural networks. In International conference on machine learning, pages 1321\u20131330. PMLR, 2017.   \n[13] C. Gupta and A. Ramdas. Distribution-free calibration guarantees for histogram binning without sample splitting. In International Conference on Machine Learning, pages 3942\u20133952. PMLR, 2021.   \n[14] C. Gupta and A. Ramdas. Top-label calibration and multiclass-to-binary reductions. In International Conference on Learning Representations, 2022.   \n[15] S. Hanson and L. Pratt. Comparing biases for minimal network construction with backpropagation. In D. Touretzky, editor, Advances in Neural Information Processing Systems, volume 1. Morgan-Kaufmann, 1988.   \n[16] M. Hein, M. Andriushchenko, and J. Bitterwolf. Why relu networks yield high-confidence predictions far away from the training data and how to mitigate the problem. 2018.   \n[17] D. Hendrycks and K. Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. In International Conference on Learning Representations, 2017.   \n[18] T.-H. Huang, C. Cao, S. Schoenberg, H. Vishwakarma, N. Roberts, and F. Sala. Scriptoriumws: A code generation assistant for weak supervision. In ICLR Deep Learning for Code Workshop, 2023.   \n[19] L. Hui, M. Belkin, and S. Wright. Cut your losses with squentropy. In Proceedings of the 40th International Conference on Machine Learning, 2023.   \n[20] S. Hussain. Cifar 10- cnn using pytorch, Jul 2021. URL https://www.kaggle.com/code/ shadabhussain/cifar-10-cnn-using-pytorch.   \n[21] T. Joy, F. Pinto, S.-N. Lim, P. H. Torr, and P. K. Dokania. Sample-dependent adaptive temperature scaling for improved calibration. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 14919\u201314926, 2023.   \n[22] D. R. Karger, S. Oh, and D. Shah. Budget-optimal crowdsourcing using low-rank matrix approximations. In 2011 49th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 284\u2013291. IEEE, 2011.   \n[23] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[24] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[25] A. Krogh and J. A. Hertz. A simple weight decay can improve generalization. In Proceedings of the 4th International Conference on Neural Information Processing Systems, NIPS\u201991, page 950\u2013957, San Francisco, CA, USA, 1991. Morgan Kaufmann Publishers Inc.   \n[26] M. Kull, M. Perello Nieto, M. K\u00e4ngsepp, T. Silva Filho, H. Song, and P. Flach. Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with dirichlet calibration. In Advances in Neural Information Processing Systems, volume 32, 2019.   \n[27] A. Kumar, S. Sarawagi, and U. Jain. Trainable calibration measures for neural networks from kernel mean embeddings. In Proceedings of the 35th International Conference on Machine Learning. PMLR, 2018.   \n[28] A. Kumar, P. S. Liang, and T. Ma. Verified uncertainty calibration. Advances in Neural Information Processing Systems, 32, 2019.   \n[29] Y. Le and X. Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015.   \n[30] Y. LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.   \n[31] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.   \n[32] D. Lowell, Z. C. Lipton, and B. C. Wallace. Practical obstacles to deploying active learning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 21\u201330. Association for Computational Linguistics, 2019.   \n[33] A. Mazumdar and B. Saha. Clustering with noisy queries. In Advances in Neural Information Processing Systems, volume 30, 2017.   \n[34] T. Mitchell. Twenty Newsgroups. UCI Machine Learning Repository, 1999.   \n[35] J. Moon, J. Kim, Y. Shin, and S. Hwang. Confidence-aware learning for deep neural networks. In Proceedings of the 37th International Conference on Machine Learning, volume 119, 2020.   \n[36] J. Mukhoti, V. Kulharia, A. Sanyal, S. Golodetz, P. Torr, and P. Dokania. Calibrating deep neural networks using focal loss. Advances in Neural Information Processing Systems, 33: 15288\u201315299, 2020.   \n[37] R. M\u00fcller, S. Kornblith, and G. E. Hinton. When does label smoothing help? Advances in neural information processing systems, 32, 2019.   \n[38] A. Nguyen, J. Yosinski, and J. Clune. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 427\u2013436, 2015.   \n[39] A. Niculescu-Mizil and R. Caruana. Predicting good probabilities with supervised learning. In Proceedings of the 22nd International Conference on Machine Learning, 2005.   \n[40] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830, 2011.   \n[41] J. Platt. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in large margin classifiers, 10(3):61\u201374, 1999.   \n[42] H. Qiu, K. Chintalapudi, and R. Govindan. MCAL: Minimum cost human-machine active labeling. In The Eleventh International Conference on Learning Representations, 2023.   \n[43] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[44] A. J. Ratner, C. M. D. Sa, S. Wu, D. Selsam, and C. R\u00e9. Data programming: Creating large training sets, quickly. In Proceedings of the 29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, 2016.   \n[45] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni, and L. Moy. Learning from crowds. Journal of Machine Learning Research, 11(43):1297\u20131322, 2010.   \n[46] B. Settles. Active learning literature survey. 2009.   \n[47] SGT. Aws sagemaker ground truth. https://aws.amazon.com/sagemaker/ data-labeling/, 2022. Accessed: 2024-05-22.   \n[48] C. Shin, W. Li, H. Vishwakarma, N. C. Roberts, and F. Sala. Universalizing weak supervision. In International Conference on Learning Representations, 2022.   \n[49] C. Shin, S. Cromp, D. Adila, and F. Sala. Mitigating source bias for fairer weak supervision. In Advances in Neural Information Processing Systems, volume 36, pages 33564\u201333605, 2023.   \n[50] A. Sorokin and D. Forsyth. Utility data annotation with amazon mechanical turk. In 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, pages 1\u20138, 2008. doi: 10.1109/CVPRW.2008.4562953.   \n[51] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations, 2014.   \n[52] R. K. Vinayak and B. Hassibi. Crowdsourced clustering: Querying edges vs triangles. In Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS\u201916, 2016.   \n[53] R. K. Vinayak, S. Oymak, and B. Hassibi. Graph clustering with missing data: Convex algorithms and analysis. In Advances in Neural Information Processing Systems, 2014.   \n[54] R. K. Vinayak, T. Zrnic, and B. Hassibi. Tensor-based crowdsourced clustering via triangle queries. In 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 2322\u20132326. IEEE, 2017.   \n[55] H. Vishwakarma and F. Sala. Lifting weak supervision to structured prediction. In Advances in Neural Information Processing Systems, volume 35, 2022.   \n[56] H. Vishwakarma, H. Lin, F. Sala, and R. K. Vinayak. Promises and pitfalls of threshold-based auto-labeling. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[57] H. Vishwakarma, H. Lin, and R. K. Vinayak. Taming false positives in out-of-distribution detection with human feedback. In International Conference on Artificial Intelligence and Statistics. PMLR, 2024.   \n[58] S. Xiao, Z. Liu, P. Zhang, and N. Muennighoff. C-pack: Packaged resources to advance general chinese embedding, 2023.   \n[59] J. Yang, P. Wang, D. Zou, Z. Zhou, K. Ding, W. Peng, H. Wang, G. Chen, B. Li, Y. Sun, X. Du, K. Zhou, W. Zhang, D. Hendrycks, Y. Li, and Z. Liu. Openood: Benchmarking generalized out-of-distribution detection, 2022.   \n[60] Y. Yu, S. Zuo, H. Jiang, W. Ren, T. Zhao, and C. Zhang. Fine-tuning pre-trained language model with weak supervision: A contrastive-regularized self-training approach. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1063\u20131077, 2021.   \n[61] B. Zadrozny and C. Elkan. Learning and making decisions when costs and probabilities are both unknown. In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining, pages 204\u2013213, 2001.   \n[62] B. Zadrozny and C. Elkan. Transforming classifier scores into accurate multiclass probability estimates. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 694\u2013699, 2002.   \n[63] D. Zhu, X. Shen, M. Mosbach, A. Stephan, and D. Klakow. Weaker than you think: A critical look at weakly supervised learning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14229\u201314253, 2023.   \n[64] F. Zhu, Z. Cheng, X.-Y. Zhang, and C.-L. Liu. Rethinking confidence calibration for failure prediction. In European Conference on Computer Vision, pages 518\u2013536. Springer, 2022. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Supplementary Material Organization ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "The supplementary material is organized as follows. We provide deferred details of the background and motivation section in Appendix A of the method in Appendix B. Then, in Appendix C, we provide additional experimental results and details of the experiment protocol and hyperparameters used for the experiments. Our code with instructions to run, is uploaded along with the paper. ", "page_idx": 13}, {"type": "text", "text": "A Appendix to the Background and Motivation Section ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A.1 Detailed comparison with active learning and self-training ", "page_idx": 13}, {"type": "text", "text": "To illustrate the differences between TBAL and the combination of active learning (AL) and selftraining for the task of data labeling, we run an experiment on the 2 concentric circles data setting as used in [56]. The details are as follows: ", "page_idx": 13}, {"type": "text", "text": "Data setting. We generate two concentric circles with points in the outer circle belonging to one class and the inner circle belonging to the other class. The total number of points generated is 10,000 of which we use 2000 for validation. ", "page_idx": 13}, {"type": "text", "text": "Methods. We run TBAL, $\\mathrm{AL+Self-}$ Training, and $\\mathrm{AL+}$ Self-Training $+\\mathbf{S}\\mathbf{C}$ , using logistic regression. The combination of AL $^+$ Self-Training means, in each iteration, the algorithm queries human-labeled data points and pseudo-labels the points in the unlabeled data using self-training and adds both the human-labeled and pseudo-labeled points in the training pool. With this procedure, $\\mathrm{AL+Self-}$ Training first learns the best classifier $(\\hat{h}_{\\mathrm{al-st}})$ with the given budget of maximum training points $(N_{t})$ that can be ", "page_idx": 13}, {"type": "image", "img_path": "96gXvFYWSE/tmp/0c2f03e04788ee39e4a216401d4053660387d72a51afea55998d6ce9176c464a.jpg", "img_caption": ["Figure 5: Results of experiment on 2-concentric circles to show the differences between TBAL, AL and ST. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "queried from humans. Then it auto-labels all the remaining unlabeled points with this classifier\u2019s predictions. For AL+Self-Training $\\mathrel{\\+}\\mathbf{S}C$ , we do selective auto-labeling using $\\hat{h}_{\\mathrm{al-st}}$ , i.e., only auto-label the points where the classifier will have an error at most $\\epsilon_{a}$ . We use $\\epsilon_{a}=\\!1\\%$ here. ", "page_idx": 13}, {"type": "text", "text": "Results and discussion. The Figure 5 shows auto-labeling error and coverage achieved by these methods when run with different choices of human-labeled data budget for training. First, we can see that even with linear classifiers TBAL is able to auto-label a huge chunk of the data (high coverage) while maintaining auto-labeling error below the tolerance level of $1\\%$ On the other hand, methods like AL+Self-Training $(+\\mathrm{SC})$ that try to first learn the optimal classifier in the given function class either have high auto-labeling error or very low coverage. These results are also consistent with the observations in [56] on the comparison between TBAL and AL, $\\mathrm{AL+SC}$ . While such findings confirm the notion that there are differences\u2014and, at least in some settings, advantages\u2014for the TBAL approach compared to other techniques, we reiterate that our goal is to understand and improve the role of the confidence function within TBAL, rather than comparing TBAL to other techniques. ", "page_idx": 13}, {"type": "text", "text": "A.2 Details of the motivating experiment in section 2 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "We run TBAL for a single round on the CIFAR-10 dataset with a SimpleCNN classification model with around 5.8M parameters [20]. We randomly sampled 4,000 points for training the classifier and randomly sampled 1,000 points as validation data. We train the model to zero training error using minibatch SGD with learning rate 1e-3, weight decay 1e-3 [15, 25], momentum 0.9, and batch size 32. The trained model has validation accuracy around $55\\%$ , implying we could hope to get coverage around $55\\%$ . We run the auto-labeling procedure with an error tolerance of $5\\%$ . ", "page_idx": 13}, {"type": "text", "text": "B Additional Details on the Method ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Detailed algorithms ", "page_idx": 14}, {"type": "text", "text": "See Algorithms 1, 2 and 3. ", "page_idx": 14}, {"type": "text", "text": "B.2 Tightness of surrogates. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The surrogate auto-labeling error and coverage introduced to relax the optimization problem (P2) is indeed a good approximation of the actual auto-labeling error and coverage. To see this, we use a toy data setting of $x\\sim\\operatorname{Uniform}(0,1)$ with 1\u2212dimensional threshold classifier $\\hbar_{\\theta}(x)=\\mathbb{1}(x\\geq\\theta)$ . For any $x$ , let true labels $y=\\hbar_{0.5}(x)$ and consider the confidence function $g_{w}(x)=|w-x|$ . Let $\\hat{y}=\\hbar_{0.25}(x)$ and consider the points on the side where $\\hat{y}=1$ . We plot actual and surrogate errors in Figure 6(a) and the surrogate and actual coverage in Figure 6(a). ", "page_idx": 14}, {"type": "text", "text": "for three choices of $\\alpha$ . As expected, the gap between the surrogates and the actual functions diminishes as we increase the $\\alpha$ . ", "page_idx": 14}, {"type": "text", "text": "B.3 Active querying strategy. ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We employ the margin-random query approach to select the next batch of training data. This method involves sorting points based on their margin (uncertainty) scores and selecting the top $C n_{b}$ points, from which $n_{b}$ points are randomly chosen. This strategy provides a straightforward and computationally efficient way to balance the exploration-exploitation trade-off. It\u2019s important to acknowledge the existence of alternative active-querying strategies; however, we adopt the margin-random approach as our standard to maintain a focus on evaluating various choices of confidence functions for auto-labeling. Note that while we use the new confidence ", "page_idx": 14}, {"type": "image", "img_path": "96gXvFYWSE/tmp/0e682388da7a25b2f4e6a0e680d8eb562d18c88307f36b6fbf917a1f9f54ccd7.jpg", "img_caption": ["(a) Auto-labeling error and surrogate error at various $\\alpha$ . "], "img_footnote": [], "page_idx": 14}, {"type": "image", "img_path": "96gXvFYWSE/tmp/d5ecc100931bf936d12b6c3d8c34c62f1c3e3b5b39cd60c74327ffd50eaf1482.jpg", "img_caption": ["(b) Auto-labeling coverage and surrogate coverage at various $\\alpha$ . "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "Figure 6: Illustration of the tightness of surrogate error and coverage functions based on the choice of $\\alpha$ . ", "page_idx": 14}, {"type": "text", "text": "scores computed using post-hoc methods for auto-labeling, we do not use these scores in active querying. Instead, we use the softmax scores from the model for this. We do this to avoid conflating the study with the study of active querying strategies. We use $C=2$ for all experiments. ", "page_idx": 14}, {"type": "text", "text": "B.4 Glossary ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The notation is summarized in Table 3 below. ", "page_idx": 15}, {"type": "table", "img_path": "96gXvFYWSE/tmp/a4579421999f54cbbe71a4932a93a71e594b47f174756e03e5cf6a20a5e2f92f.jpg", "table_caption": ["Table 3: Glossary of variables and symbols used in this paper. "], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Algorithm 1 Threshold-based Auto-Labeling (TBAL) ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Input: Unlabeled data $X_{u}$ , labeled validation data $D_{\\mathrm{val}}$ , auto labeling error tolerance $\\epsilon_{a}$ , $N_{t}$ training data query budget, seed data size $n_{s}$ , batch size for active query $n_{b}$ , calibration data fraction $\\nu$ , space of thresholds $T$ , coverage lower bound $\\rho_{0}$ , label space $\\mathcal{Y}$ .   \nOutput: Auto-labeled dataset $D_{\\mathrm{out}}$ . ", "page_idx": 16}, {"type": "text", "text": "1: procedure TBAL $(X_{u},D_{\\mathrm{val}},\\epsilon_{a},N_{t},n_{s},n_{b},\\nu,\\rho_{0},T,\\mathcal{Y})$   \n23:: $D1\\ast\\ast=40$ $D_{\\mathrm{query}}^{(1)}\\leftarrow\\mathrm{RANDOMQUERY}(X_{u},n_{s})$ . \u25b7Randomly select $n_{s}$ points and get human labels.   \n4: $\\begin{array}{r l}&{X_{u}^{(1)}\\leftarrow X_{u}\\setminus\\{\\mathbf{x}:(\\mathbf{x},y)\\in D_{\\mathrm{query}}^{(1)}\\}.}\\\\ &{D_{\\mathrm{val}}^{(1)}\\leftarrow D_{\\mathrm{val}};D_{\\mathrm{train}}^{(0)}\\leftarrow\\emptyset.}\\\\ &{D_{\\mathrm{out}}\\leftarrow D_{\\mathrm{query}}^{(1)};n_{t}^{(1)}\\leftarrow n_{s};i\\leftarrow1.}\\end{array}$ \u25b7Remove these points from the unlabeled pool.   \n5: \u25b7Validation data for the first round is full $D_{\\mathrm{val}}$ .   \n6: \u25b7Add human-labeled data to the output $D_{\\mathrm{out}}$ .   \n7: $D1\\ast\\ast=40$ Run the auto-labeling loop. \\*\\*\\*/   \n8: $\\triangleright/\\ast$ Until no more unlabeled points are left or the budget for training data is exhausted. \\*/   \n9: while $X_{u}^{(i)}\\neq\\emptyset$ and $n_{t}^{(i)}\\le N_{t}$ do   \n10: D(i) train \u2190Dt(ria\u2212i1n) \u222aD(qiu)ery. \u25b7Include human-labeled points in the training data.   \n1112:: h\u02c6Di(i), ,Di) t(ih) \u2190RANDOMSPLIT(D(via)l, \u03bd). \u2190TRAINMODEL(H, Dt(ri)ain \u25b7Randomly\u25b7 sTprlaiitn  cau rcrleansts ivfaicliatdiaotino nm oddateal..   \n13: \u25b7/\\*\\*\\* Colander block, to learn the new confidence function $\\hat{g}_{i}$ . \\*\\*\\*/   \n14: $\\begin{array}{r l}{\\hat{g}_{i},\\hat{\\mathbf{t}}_{i}^{\\prime}}&{{}\\longleftarrow\\arg\\operatorname*{min}_{g\\in\\mathcal{G},\\mathbf{t}\\in T^{k}}-\\widetilde{\\mathcal{P}}(g,\\mathbf{t}\\mid\\hat{h}_{i},D_{\\mathrm{cal}}^{(i)})+\\lambda\\,\\widetilde{\\mathcal{E}}(g,\\mathbf{t}\\mid\\hat{h}_{i},D_{\\mathrm{cal}}^{(i)}).}\\end{array}$ \u25b7Colander .   \n15: $D//4=4:4$ Estimate auto-labeling thresholds using $\\hat{g}_{i}$ and $D_{\\mathrm{th}}^{(i)}$ . See Algorithm 2. \\*\\*\\*/   \n16: $\\begin{array}{r l r}{\\hat{\\mathbf{t}}_{i}}&{{}}&{\\leftarrow\\mathrm{EsTTHRESHOLD}\\big(\\hat{g}_{i},\\hat{\\boldsymbol{h}}_{i},D_{\\mathrm{th}}^{(i)},\\epsilon_{a},\\rho_{0},T,\\mathcal{Y}\\big).}\\end{array}$ .   \n17: \u25b7/\\*\\*\\* Auto-label the points having scores above the thresholds. \\*\\*\\*/   \n18: $\\begin{array}{r l}&{\\widetilde{D}_{u}^{(i)}\\quad\\leftarrow\\{(\\mathbf{x},\\hat{\\beta}_{i}(\\mathbf{x})):\\mathbf{x}\\in X_{u}^{(i)}\\}.}\\\\ &{D_{\\mathrm{auto}}^{(i)}\\,\\leftarrow\\{(\\mathbf{x},\\hat{y})\\in\\tilde{D}_{u}^{(i)}:\\hat{g}_{i}(\\mathbf{x})[\\hat{y}]\\,\\geq\\hat{\\mathbf{t}}_{i}[\\hat{y}]\\,\\}.}\\\\ &{X_{u}^{(i)}\\quad\\leftarrow X_{u}^{(i)}\\setminus\\{\\mathbf{x}:(\\mathbf{x},\\hat{y})\\in D_{\\mathrm{auto}}^{(i)}\\}.\\,\\mathsf{p R e m o}}\\end{array}$   \n19:   \n20: ve auto-labeled points from unlabeled set.   \n21: $\\begin{array}{r l}{\\widetilde{D}_{\\mathrm{val}}^{(i)}}&{\\leftarrow\\{(\\mathbf{x},\\hat{\\boldsymbol{\\beta}}_{i}(\\mathbf{x})):(\\mathbf{x},y)\\in D_{\\mathrm{val}}^{(i)}\\}.}\\end{array}$   \n22: D(vial+1)\u2190{(x, y\u02c6) \u2208D\u02dc(via)l : g\u02c6i(x)[y\u02c6] < t\u02c6i[y\u02c6]}. \u25b7Remove validation points from the   \nauto-labeling region.   \n23: $D144=40$ Get the next batch of manually labeled data using an active querying strategy. \\*\\*\\*/   \n24: $\\begin{array}{r}{D_{\\mathrm{query}}^{(i+1)}\\leftarrow\\mathrm{ACTIVEQUERY}(\\hat{\\boldsymbol{h}}_{i},\\boldsymbol{X}_{u}^{(i)},\\boldsymbol{n}_{b}).}\\\\ {X_{u}^{(i+1)}\\leftarrow X_{u}^{(i)}\\backslash\\{\\mathbf{x}:(\\mathbf{x},y)\\in D_{\\mathrm{query}}^{(i+1)}\\}.}\\end{array}$   \n25: \u25b7Remove human-labeled data from the   \nunlabeled pool.   \n26: $\\begin{array}{r l}{D_{\\mathrm{out}}}&{{}\\leftarrow D_{\\mathrm{out}}\\cup D_{\\mathrm{auto}}^{(i)}\\cup D_{\\mathrm{query}}^{(i+1)}}\\end{array}$ . \u25b7Add the auto-labeled and manually labeled points   \nin the output data.   \n27: $\\begin{array}{l}{{n_{t}^{(i+1)}\\leftarrow n_{t}^{(i)}+n_{b}.}}\\\\ {{i\\leftarrow i+1.}}\\end{array}$   \n28:   \n29: end while   \n30: return $D_{\\mathrm{out}}$ .   \n31: end procedure ", "page_idx": 16}, {"type": "text", "text": "Input: Confidence function $\\hat{g}_{i}$ , classifier $\\hat{\\beta}_{i}$ , Part of validation data $D_{\\mathrm{th}}^{(i)}$ for threshold estimation, auto labeling error tolerance $\\epsilon_{a}$ , space of thresholds $T$ , coverage lower bound $\\rho_{0}$ , label space $y$ . Output: Auto-labeling thresholds $\\hat{\\mathbf{t}}_{i}$ , where $\\hat{\\mathbf{t}}_{i}[y]$ is the threshold for class $y$ .   \n1: procedure ESTTHRESHOLD $(\\hat{g}_{i},\\hat{\\hat{h}}_{i},D_{\\mathrm{th}}^{(i)},\\epsilon_{a},\\rho_{0},T,\\mathcal{Y})$   \n2: \u25b7/\\*\\*\\* Estimate thresholds for each class. \\*\\*\\*/   \n3: for $y\\in\\mathcal{Y}$ do   \n4: $D_{\\mathrm{th}}^{(i,y)}\\leftarrow\\{(\\mathbf{x}^{\\prime},y^{\\prime})\\in D_{\\mathrm{th}}^{(i)}:y^{\\prime}=y\\}.$ \u25b7Group points class-wise. 5: $D//4=4:4$ Only evaluate thresholds with est. coverage at least $\\rho_{0}$ . \\*\\*\\*/   \n6: $\\begin{array}{r l}{T_{y}^{\\prime}}&{{}\\gets\\{t\\in T:\\widehat{\\mathcal{P}}\\big(\\hat{g}_{i},t\\mid\\hat{\\hat{h}}_{i},D_{\\mathrm{th}}^{(i,y)}\\big)\\geq\\rho_{0}\\}\\cup\\{\\infty\\}.}\\end{array}$ .   \n7: $D144=40$ Estimate auto-labeling error at each threshold. Pick the smallest threshold with the sum of estimated error and $C_{1}$ times the std. deviation is below $\\epsilon_{a}$ . $C_{1}$ is set to 0.25 here. \\*\\*\\*/ 8: $\\begin{array}{r l}{\\widehat{\\mathbf{t}}_{i}[y]}&{\\gets\\operatorname*{min}\\{t\\in T_{y}^{\\prime}:\\widehat{\\mathcal{E}}_{y}(\\widehat{g}_{i},t|\\widehat{h}_{i},D_{\\mathrm{th}}^{(i,y)})+C_{1}\\widehat{\\zeta}(\\widehat{\\mathcal{E}}_{y}(\\widehat{g}_{i},t|\\widehat{h}_{i},D_{\\mathrm{th}}^{(i,y)}))\\leq\\epsilon_{a}\\}.}\\end{array}$ 9: end for   \n10: return $\\hat{\\mathbf{t}}_{i}$ .   \n11: end procedure ", "page_idx": 17}, {"type": "text", "text": "Algorithm 3 Active Querying Strategy to Acquire Human-labeled Samples for Training ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Input: Classifier $\\hat{\\beta}_{i}$ , unlabeled data $X_{u}^{(i)}$ , batch size $n_{b}$ , constant $C\\geq1$ .   \nOutput: $D_{\\mathrm{query}}^{(i+1)}$ , a subset of $X_{u}^{(i)}$ of size at most $n_{b}$ with human (groundtruth) labels. 1: procedure ACTIVEQUERY $(\\hat{\\boldsymbol{\\beta}}_{i},X_{u}^{(i)},n_{b})$   \n2: $S_{u}^{(i)}\\gets$ Softmax scores from $\\hat{\\beta}_{i}$ for all points in $X_{u}^{(i)}$ .   \n3: $X^{\\prime}\\leftarrow\\mathrm{Top}\\;C\\times n_{b}$ points from $X_{u}^{(i)}$ sorted in ascending order on the scores $S_{u}^{(i)}$ . 4: $D_{\\mathrm{query}}^{(i+1)}\\gets\\mathsf{R A N D O M Q U E R Y}(X^{\\prime},n_{b})$ .   \n5: return D(qiu+er1y).   \n6: end procedure ", "page_idx": 17}, {"type": "text", "text": "Algorithm 4 Select a Subset of Points Randomly and Obtain Human Labels ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Input: $X,n$ .   \nOutput: $D$ , a subset of $X$ of size at most $n$ with human (groundtruth) labels.   \n1: procedure RANDOMQUERY $(X,n)$   \n2: if then $\\vert\\vert X\\vert>n$   \n3: $X^{\\prime\\prime}\\gets$ randomly select $n$ points from $X$ .   \n4: else   \n5: $X^{\\prime\\prime}\\gets X.$ .   \n6: end if   \n7: $D\\gets\\{(\\mathbf{x},\\mathtt{h u m a n\\mathrm{_-}1a b e1}(\\mathbf{x}):\\mathbf{x}\\in X^{\\prime\\prime}\\}.$   \n8: return $D$ .   \n9: end procedure ", "page_idx": 17}, {"type": "text", "text": "C Additional Experiments and Details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "C.1 Experiments on $N_{t}$ , $N_{v}$ and $\\nu$ ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We need to understand the effect of training data query budget i.e. $N_{t}$ , the total validation data $N_{v}$ , and the data that can be used for calibrating the model i.e. the calibration data fraction $\\nu$ on the auto-labeling objective. As varying these hyperparameters on each train-time method is expensive, we experimented with only Squentropy as it was the best-performing method across settings for various datasets. ", "page_idx": 18}, {"type": "text", "text": "When we vary the budget for training data $N_{t}$ , we observe from Figure 7 that our method does not require a lot of data to train the base model, i.e. achieving low auto-labeling error and high coverage with a low budget. While other methods benefit from having more training data for auto-labeling objectives, it comes at the expense of reducing the available data for validation. ", "page_idx": 18}, {"type": "text", "text": "From Figure 8, we observe that, while the coverage of our method remains the same across different $N_{v}$ , it reduces for other methods. The cause of this phenomenon can be attributed to the fact that we are borrowing the data from the training budget as it limits the performance of the base model, which in turn limits the auto-labeling objective. ", "page_idx": 18}, {"type": "text", "text": "As we increase the percentage of data that can be used to calibrate the model, i.e., $\\nu$ , we note from Figure 9 that other methods improve the coverage, which can be understood from the fact that when more data is available for calibrating the model, the model becomes better in terms of the auto-labeling objective. But it\u2019s interesting to note that even with a low calibration fraction, our method achieves superior coverage compared to other methods. It is also important to note that the auto-labeling error increases as we increase $\\nu$ . This is because when $\\nu$ increases, the number of data points used to estimate the threshold decreases, leading to a less granular and precise threshold. ", "page_idx": 18}, {"type": "table", "img_path": "96gXvFYWSE/tmp/5c75b33a2350f0c7b1953f54772b170239cd6812a0987002205a832fb2527c41.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "table", "img_path": "96gXvFYWSE/tmp/c876f3b5241ad8a908b4f25b7730e6417bbf601cffc2c43f9c9c5f720dc660bc.jpg", "table_caption": ["Table 4: Auto-labeling error and coverage for the 3 feature representations we could use for 20 Newsgroup. As we can see, the feature representation does not lead to a significant difference in auto-labeling error and coverage. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "Table 5: Auto-labeling error and coverage for the 3 feature representations we could use for CIFAR10 SimpleCNN. As we can see, the feature representation does not lead to a significant difference in auto-labeling error and coverage. ", "page_idx": 18}, {"type": "text", "text": "C.2 Experiments on Colander input ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Figure 4 illustrates that we could use logits (last layer\u2019s representations), pre-logits (second last layer\u2019s representations), or the concatenation of these two as the input to $g$ . To help us decide which one we should use, we conduct a hyperparameter search for input features on the CIFAR-10 and 20 Newsgroup dataset using the Squentropy train-time method. Table 4 and 5 present the autolabeling error and coverage of using the 3 types of feature representations. As we can see, all feature representation leads to a similar auto-labeling error and coverage, and in some cases, it is better to include pre-logits as well. Thus, we use concatenated representation (Concat), for more flexibility. ", "page_idx": 18}, {"type": "text", "text": "C.3 Experiments on $\\epsilon_{a}$ ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We run TBAL with five values of $\\epsilon_{a}\\in\\{0.01,0.025,0.05,0.075,0.1\\}$ and report the results in Table 6. As expected the auto-labeling error is high with larger values of and smaller with small $\\epsilon_{a}$ . ", "page_idx": 19}, {"type": "table", "img_path": "96gXvFYWSE/tmp/df5d0d944c24bd73c16d3a82a48243b5a6bbd387b3c7aaad1a38d9656aab0837.jpg", "table_caption": [], "table_footnote": ["Table 6: $\\epsilon_{a}$ variation. Dataset: CIFAR-10, Train-time method: Vanilla. "], "page_idx": 19}, {"type": "text", "text": "C.4 Single vs multi-round TBAL ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We further demonstrate that the performance gains are due to the use of Colander, even if methods use multiple rounds. To do so, we show the evolution of coverage and error over multiple rounds in Figure 16. The effects of using Colander are visible from the first round itself, and the following rounds improve performance further. We also run a single round (passive) variant of TBAL where we sample all the human-labeled points for ", "page_idx": 19}, {"type": "image", "img_path": "96gXvFYWSE/tmp/7036b0c9e84cdbac971f659b6ef2bab419cd19c53d50471efb53cfe8046a13cb.jpg", "img_caption": ["Figure 16: Per-epoch metrics for all post-hoc methods for CIFAR10 setting. (left) Auto-labeling accuracy (right) Coverage. Train-time method is vanilla. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "training $(N_{t})$ randomly at once, train a classifier, do auto-labeling, and then stop. This setting avoids confounding due to multiple rounds. We observe that using Colander yields significantly higher coverage in comparison to the baselines (see Table 7). This reinforces the fact that the gains in the multi-round TBAL are directly due to Colander, while multiple rounds of data selection, training, and auto-labeling are superior to this single round version. ", "page_idx": 19}, {"type": "text", "text": "C.5 Experiments on different architectures ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "In TBAL it is not a priori clear what model the practitioner should use. The overall system is flexible enough to work with any chosen model class. Our focus is on evaluating the effect of various training time and post-hoc methods designed to improve the confidence functions for any given model. To answer the query, we ran experiments with Resnet18 and ViT models in the CIFAR-10 setting (see Table 8). As we expected there are variations in the results in the baselines due to model choices but our method maintains high performance irrespective of the classification model used. This is due to its ability to learn confidence scores tailored for TBAL. ", "page_idx": 19}, {"type": "table", "img_path": "96gXvFYWSE/tmp/b3bd0a864a5fe81b483c690c56f2750580507ca0b2fcc942043b257ecafc4b31.jpg", "table_caption": [], "table_footnote": ["Table 7: Results with single round of auto-labeling. Dataset and model: CIFAR-10 setting in the paper. "], "page_idx": 19}, {"type": "table", "img_path": "96gXvFYWSE/tmp/6acf2b00dadf0ccb127e84c97af62a46463476192e2fbe7ccfe1d73d9a0e4445.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "96gXvFYWSE/tmp/bd36e9f84d752703312b5824a220d75d2498cc71c1660fa895ac3c8879ee2fa5.jpg", "table_caption": ["Table 8: Model variation. CIFAR-10 dataset with ViT (Left) and ResNet18 (Right), Train-time method Vanilla. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Table 9: Hyperparameters swept over for train-time methods. Those listed next to Common are the hyperparameters for the four train-time methods: Vanilla, CRL, FMFP, and Squentropy. Therefore, we do not list those again for each method. Note that for FMFP, we used SAM optimizer instead of SGD. For each method, we swept through all possible combinations of the possible values for each hyperparameter. Underlined values are only used on TinyImageNet since it is a complicated dataset containing 200 classes. ", "page_idx": 20}, {"type": "text", "text": "C.6 Hyperparameters ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The hyperparameters and their values we swept over are listed in Table 9 and 10 for train-time and post-hoc methods, respectively. ", "page_idx": 20}, {"type": "text", "text": "C.7 Train-time and post-hoc methods ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "C.7.1 Train-time methods ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "1. Vanilla: Neural networks are commonly trained by minimizing the cross entropy loss using stochastic gradient descent (SGD) with momentum [1, 4]. We refer to this as the Vanilla training method. We also include weight decay to mitigate the overconfidence issue associated with this method [12]. ", "page_idx": 20}, {"type": "text", "text": "2. Squentropy [19]: This method adds the average square loss over the incorrect classes to the cross-entropy loss. This simple modification to the Vanilla method leads to the end model with better test accuracy and calibration.   \n3. Correctness Ranking Loss (CRL) [35]: This method includes a term in the loss function of the vanilla training method so that the confidence scores of the model are aligned with the ordinal rankings criterion [17, 6]. The confidence functions satisfying this criterion produce high scores on points where the probability of correctness is high and low scores on points with low probabilities of being correct.   \n4. FMFP [64] aims to align confidence scores with the ordinal rankings criterion. It uses Sharpness Aware Minimizer (SAM) [7] to train the model, with the expectation that the flat minima would benefit the ordinal rankings objective of the confidence function. ", "page_idx": 20}, {"type": "text", "text": "C.7.2 Post-hoc methods ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. Temperature scaling [12]: This is a variant of Platt scaling [12], a classic and one of the easiest parametric methods for post-hoc calibration. It rescales the logits by a learnable scalar parameter and has been shown to work well for neural networks.   \n2. Top-Label Histogram-Binning [14]: Since TBAL assigns the top labels (predicted labels) to the selected unlabeled points, it is appealing to only calibrate the scores of the predicted label. Building upon a rich line of histogram-binning methods (non-parametric) for post-hoc calibration [62], this method focuses on calibrating the scores of predicted labels.   \n3. Scaling-Binning [28]: This method combines parametric and non-parametric methods. It first applies temperature scaling and then bins the confidence function values to ensure calibration.   \n4. Dirichlet Calibration [26]: This method models the distribution of predicted probability vectors separately on instances of each class and assumes the class conditional distributions are Dirichlet distributions with different parameters. It uses linear parameterization for the distributions, which allows easy implementation in neural networks as additional layers and softmax output. ", "page_idx": 21}, {"type": "text", "text": "Note: For binning methods, uniform mass binning [62] has been a better choice over uniform width binning. Hence, we use uniform mass binning as well. ", "page_idx": 21}, {"type": "text", "text": "C.8 Compute resources and time ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Our experiments were conducted on machines equipped with the NVIDIA RTX A6000 and NVIDIA GeForce RTX 4090 GPUs. The wall clock time of our method is similar to other post-hoc methods. For instance, a single run on the CIFAR-10 setting on NVIDIA RTX A6000 takes around 1.5 hours with post-hoc methods and roughly 1 hour without post-hoc methods. The additional time taken by our method over the baselines not doing any post-hoc calibration is traded-off the by the quality and quantity of the auto-labeled data it outputs. We leave a thorough benchmarking of wall clock time and its optimization for future work. ", "page_idx": 21}, {"type": "text", "text": "C.9 Detailed dataset and model ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "1. The MNIST dataset [30] consists of $28\\times28$ grayscale images of hand-written digits across 10 classes. It was used alongside the LeNet5 [31], a convolutional neural network, for auto-labeling.   \n2. The CIFAR-10 dataset [24] contains $3\\times32\\times32$ color images across 10 classes. We utilized its raw pixel matrix in conjunction with SimpleCNN [20], a convolutional neural network with approximately 5.8M parameters, for auto-labeling.   \n3. Tiny-ImageNet [29] is a color image dataset that consists of 100K images across 200 classes. Instead of using the $3\\times64\\times64$ raw pixel matrices as input, we utilized CLIP [43] to derive embeddings within the $\\mathbb{R}^{512}$ vector space. We used a 3-layer perceptron (1,000-500-300) as the auto-labeling model.   \n4. 20 Newsgroups [34, 40] is a natural language dataset comprising around 18,000 news posts across 20 topics. We used the FlagEmbedding [58] to map the textual data into $\\mathbb{R}^{1024}$ embeddings. We used a 3-layer perceptron (1,000-500-30) as the auto-labeling model. ", "page_idx": 21}, {"type": "text", "text": "C.10 Detailed experiments protocol ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We predefined TBAL hyperparameters for each dataset-model pair and the hyperparameters we will sweep for each train-time and post-hoc method in Table 9 and Table 10 respectively. For a datasetmodel pair, initially, we perform a hyperparameter search for the train-time method. Subsequently, we optimize the hyperparameters for post-hoc methods while keeping the train-time method fixed with the previously found optimum hyperparameter for that dataset-model pair. ", "page_idx": 21}, {"type": "text", "text": "We fix the hyperparameters for the train-time method while searching hyperparameters for the posthoc method to alleviate computational budget throttle. We effectively reduce the search space to the sum of the cardinalities of unique hyper-parameter combinations across the two methods instead of a larger multiplicative product. Furthermore, due to the independent nature of these hyper-parameter combinations, TBAL runs can be highly parallelized to expedite the search process. ", "page_idx": 21}, {"type": "text", "text": "Since TBAL operates iteratively to acquire human labels for model training, selecting hyperparameters at each round of TBAL could quickly become intractable and lose its practical significance. To better align with its practical usage, we only conducted a hyperparameter search for the initial TBAL round. The specific set of hyperparameters used for the search are reported in Table 10. ", "page_idx": 22}, {"type": "text", "text": "After completing the hyperparameter search for train-time and post-hoc methods, the determined hyperparameter combinations are subjected to a full evaluation across all iterations of TBAL. At the end of each iteration, the auto-labeled points are evaluated against their ground truth labels to determine their auto-labeling error. These points are then added to the auto-labeled set, where their ratio to the total amount of unlabeled data determines the coverage. This iterative process continues until all unlabeled data are exhaustively labeled by either the oracle or through auto-labeling in the final iteration. The auto-labeling error and coverage at the final iteration of TBAL are then recorded. ", "page_idx": 22}, {"type": "text", "text": "Since TBAL incorporates randomized components as detailed in Algorithm 1, we ran the algorithm 5 times, each with a unique random seed while maintaining the same hyperparameter combination. We then recorded the results from the final iteration of these runs and calculated the mean and standard deviation of both auto-labeling error and coverage. These figures are reported in Table 2. ", "page_idx": 22}, {"type": "text", "text": "A limitation of the grid search approach in hyper-parameter optimization becomes apparent when our predefined hyper-parameter choices result in sub-optimal coverage and auto-labeling errors. Using these sub-optimal hyper-parameters can adversely affect the multi-round iterative process in TBAL, prompting the need for repetitive searches to find more effective hyper-parameters. When encountering such scenarios, TBAL users should explore additional hyper-parameter options until satisfactory performance is achieved in the initial round. However, we opted for a more straightforward approach to hyper-parameter selection, mindful of the computational demands of repeatedly optimizing multiple hyper-parameters across different methods. In scenarios expressed conditionally, we retained the top-1 hyper-parameter combination for any given method if it achieved the highest coverage while adhering to the specified error margin $(\\epsilon_{a})$ . If no hyper-parameter combinations yielded an auto-labeling error at most equal to the error margin $(\\epsilon_{a})$ , we then chose the hyper-parameter combination with the lowest auto-labeling error, regardless of its coverage. In the case of ties, we resolved them through random selection. This process results in obtaining singular values for each choice of hyper-parameter after completing each method\u2019s hyper-parameter search. ", "page_idx": 22}, {"type": "text", "text": "D Broader Impact ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "This paper contributes to the advancement of the practice of creating labeled datasets in machine learning. While our work has various possible societal implications, we do not identify any specific concerns that require special attention in this context. ", "page_idx": 23}, {"type": "text", "text": "E NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: Our claims are backed by our novel technique in Section $\\mathrm{C}$ and thorough empirical evaluation in Section 4. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: We discuss them briefly. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important ", "page_idx": 23}, {"type": "text", "text": "role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 24}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 24}, {"type": "text", "text": "Justification: It is an empirical paper, it does not have theoretical results. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: All the necessary details are provided in Section 4 and in the Appendix C. We have also uploaded the code along with the submission. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). ", "page_idx": 24}, {"type": "text", "text": "(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 25}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We use publicly available datasets and uploaded the code as supplementary material. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/ guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: These details are provided in the Section 4 and Appendix C. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: We run each setting with multiple random seeds and report the mean, standard deviations of the evaluation metrics. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 25}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Provided in the Appendix C. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have followed the NeurIPS Code of Ethics to the best of our knowledge. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] Justification: The paper has a brief discussion on the broader impacts. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 26}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper does not release such data or models that have high risk for misuse. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: We have appropriately credited them along with citations. Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The code is well documented along with instructions to run. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: We did not use crowdsourcing or human subjects in the paper. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: We did not use crowdsourcing or human subjects in the paper. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}, {"type": "image", "img_path": "96gXvFYWSE/tmp/d00985fd3367f7111d48591f9c1d74b6b5dbc156fe6c38c635ea48d2bf7a4f25.jpg", "img_caption": ["Figure 7: Autolabeling error and coverage of different post-hoc methods on CIFAR-10 for various $N_{t}$ "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "96gXvFYWSE/tmp/40a26c0212c691e0ffc7da62871dd24206913c45aa2cd42d13cbc446c79215c0.jpg", "img_caption": ["Figure 8: Autolabeling error and coverage of different post-hoc methods on CIFAR-10 for various $N_{v}$ "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "96gXvFYWSE/tmp/b351d0b7eb3fe8a772e2edfe7caee2bc28b92b79f5e00ddde3764de104b3ab7c.jpg", "img_caption": ["Figure 9: Autolabeling error and coverage of different post-hoc methods on CIFAR-10 for various $\\nu$ "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "96gXvFYWSE/tmp/6da728e743631435c7fd38099e81f6bb51004c33ab7081fc47f4c9cad5595ca3.jpg", "img_caption": ["Figure 10: Auto-labeling error and coverage for different post-hoc methods on CIFAR-10 while we vary $N_{t}$ . $N_{u}=40,000$ is the size of the given unlabeled pool. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "96gXvFYWSE/tmp/f76ae66f575198265ce0eb8b61520cecdf0022719e66efbcd6b7947034af9f1a.jpg", "img_caption": ["Figure 11: Auto-labeling error and coverage for different post-hoc methods on Tiny-ImageNet while we vary $N_{t}$ . $N_{u}=90,000$ is the size of the given unlabeled pool. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "96gXvFYWSE/tmp/82d20f583526d83685b54802671fab0071d4470d9c985fd83312650f956ae964.jpg", "img_caption": ["Figure 12: Auto-labeling error and coverage for different post-hoc methods on $20\\,\\mathrm{N}$ ewsgroups while we vary $N_{t}$ . $N_{u}=9,052$ is the size of the given unlabeled pool. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "96gXvFYWSE/tmp/3711e506067be1584501785957318406e26be3b784c3e9eede31d99b319687b2.jpg", "img_caption": ["Figure 13: Auto-labeling error and coverage for different post-hoc methods on CIFAR-10 while we vary $N_{v}$ . $N_{v_{\\mathrm{max}}}=8,000$ is the maximum number of points available for validation. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "96gXvFYWSE/tmp/fc1a92966441f144469befaa043e659e04035458137489f0c60b7758f3b76d8e.jpg", "img_caption": ["Figure 14: Auto-labeling error and coverage for different post-hoc methods on Tiny-ImageNet while we vary $N_{v}$ . $N_{v_{\\mathrm{max}}}=18,000$ is the maximum number of points available for validation. "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "96gXvFYWSE/tmp/29576e41b6c34c447587bf0dd918336cf857181cf60d486210e448f31978b516.jpg", "img_caption": ["Figure 15: Auto-labeling error and coverage for different post-hoc methods on 20 Newsgroups while we vary $N_{v}$ . $N_{v_{\\mathrm{max}}}=1,600$ is the maximum number of points available for validation. "], "img_footnote": [], "page_idx": 31}, {"type": "table", "img_path": "96gXvFYWSE/tmp/87f03c8a0be0d9017e4d2bdd091ce716f785c914b37ac037f900ba8831888674.jpg", "table_caption": [], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "Table 10: Hyperparamters swept over for post-hoc methods. For each method, we swept through all possible combinations of the possible values for each hyperparameter. ", "page_idx": 32}]