[{"figure_path": "uM3rQ14iex/figures/figures_5_1.jpg", "caption": "Figure 1: True Causal Graph G with four other graphs each with one missing bi-directed edge.", "description": "This figure presents five causal graphs. Graph G is considered the true graph, and the other four graphs (G1, G2, G3, G4) each have one missing bi-directed edge compared to G.  This is used to illustrate that learning the full causal structure isn't necessary for identifying all possibly optimal arms (POMISs) in a causal bandit setting.  While missing some bi-directed edges (like in G1 and G2) leads to incorrect identification of POMISs, others (G3) might not affect the resulting POMISs. This highlights the paper's core argument of only needing to learn a sufficient subset of the causal graph for effective regret minimization.", "section": "Possibly Optimal Arms in Causal Bandits with Unknown Causal Graph"}, {"figure_path": "uM3rQ14iex/figures/figures_6_1.jpg", "caption": "Figure 1: True Causal Graph G with four other graphs each with one missing bi-directed edge.", "description": "This figure shows five causal graphs. The first graph (a) is the true causal graph G. The other four graphs (b-e) each have one missing bi-directed edge compared to the true graph.  This is used to illustrate that not all latent confounders need to be identified for learning possibly optimal arms in causal bandits. Missing some bi-directed edges may not affect the set of possibly optimal arms, while missing others will.", "section": "Possibly Optimal Arms in Causal Bandits with Unknown Causal Graph"}, {"figure_path": "uM3rQ14iex/figures/figures_8_1.jpg", "caption": "Figure 2: Simulations to demonstrate the advantage of Algorithm 4 over full graph discovery (Learning all possible latents)", "description": "This figure compares the number of interventional samples required by Algorithm 4 (learning POMISs) and the full graph learning approach (learning all latents) for learning the induced subgraph on the ancestors of the reward node Y.  The results are shown for different graph densities (p) and confounder probabilities (pL), and the number of nodes (n) in the graph are varied from 5 to 20.  The plots show that Algorithm 4 generally requires fewer samples than the full graph learning approach, demonstrating its sample efficiency.  The difference becomes less pronounced as the probability of confounders increases.", "section": "6 Experiments"}, {"figure_path": "uM3rQ14iex/figures/figures_9_1.jpg", "caption": "Figure 2: Simulations to demonstrate the advantage of Algorithm 4 over full graph discovery (Learning all possible latents)", "description": "The figure shows the number of interventional samples required for learning the possibly optimal minimum intervention sets (POMISs) using Algorithm 4, compared to learning the induced subgraph on the ancestors of the reward node with all latent confounders.  The plots demonstrate that Algorithm 4 requires fewer samples, especially when the probability of confounders (PL) is lower. As PL increases, the advantage diminishes, but Algorithm 4 still generally outperforms full graph discovery.  This highlights the efficiency of Algorithm 4 in reducing the sample complexity needed to learn the sufficient subgraph for regret minimization in causal bandits.", "section": "Experiments"}, {"figure_path": "uM3rQ14iex/figures/figures_9_2.jpg", "caption": "Figure 2: Simulations to demonstrate the advantage of Algorithm 4 over full graph discovery (Learning all possible latents)", "description": "This figure compares the number of interventional samples required by Algorithm 4 (which learns a subset of confounders) and a full graph learning approach (which learns all confounders) for learning the induced subgraph on the ancestors of the reward node and the POMIS set. The results are shown for various graph densities (p) and latent confounder probabilities (PL). The figure demonstrates that Algorithm 4 requires significantly fewer samples compared to the full graph learning method, especially when the probability of latent confounders is low. This advantage diminishes as the latent confounder probability increases.", "section": "Experiments"}]