{"references": [{"fullname_first_author": "A. Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduced the Transformer architecture, which is the foundation for many large language models and is the primary target for distillation in this work."}, {"fullname_first_author": "T. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper demonstrated the effectiveness of large language models in few-shot learning, establishing the direction for many subsequent language modeling works."}, {"fullname_first_author": "A. Gu", "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces", "publication_date": "2023-12-01", "reason": "This paper introduced the Mamba architecture, the main linear RNN used in the proposed method, providing the basis for the improved efficiency."}, {"fullname_first_author": "H. Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-13", "reason": "This paper introduced the Llama model, one of the main large language models used as a teacher in this paper, demonstrating the progress in creating efficient foundation models."}, {"fullname_first_author": "G. Hinton", "paper_title": "Distilling the knowledge in a neural network", "publication_date": "2015-03-02", "reason": "This paper introduced the concept of knowledge distillation, a core technique used in this work to transfer the knowledge from a large Transformer model to a smaller linear RNN."}]}