[{"figure_path": "uAzhODjALU/tables/tables_2_1.jpg", "caption": "Table 2: Chat benchmark results for open-access and proprietary models on MT-Bench and AlpacaEval. MT-Bench scores model responses using GPT-4. AlpacaEval version two measures the win-loss rate between baseline models and GPT-4 scored by GPT-4 Turbo.", "description": "This table compares the performance of different language models, including open-source and proprietary ones, on two benchmark tasks: MT-Bench (multi-turn chat) and AlpacaEval (single-turn chat).  MT-Bench scores are based on GPT-4 judgments, while AlpacaEval uses GPT-4 Turbo to assess win/loss rates against GPT-4. The table shows the models' performance in terms of scores and win rates, allowing for a comparison of different model architectures and training methods.", "section": "5.2 Evaluation on Chat Benchmarks"}, {"figure_path": "uAzhODjALU/tables/tables_6_1.jpg", "caption": "Table 2: Chat benchmark results for open-access and proprietary models on MT-Bench and AlpacaEval. MT-Bench scores model responses using GPT-4. AlpacaEval version two measures the win-loss rate between baseline models and GPT-4 scored by GPT-4 Turbo.", "description": "This table compares the performance of different language models, including both open-source and proprietary models, on two distinct chat benchmarks: MT-Bench and AlpacaEval.  MT-Bench assesses the quality of model responses using GPT-4 as a judge, while AlpacaEval v2 utilizes GPT-4 Turbo to determine a win/loss rate against baseline models. The table allows for a comparison of various model architectures and training methodologies, highlighting their strengths and weaknesses in different evaluation settings.", "section": "5.2 Evaluation on Chat Benchmarks"}, {"figure_path": "uAzhODjALU/tables/tables_6_2.jpg", "caption": "Table 1: Speedup results for speculative decoding with pure Mamba models. The 2.8B verifier uses a 130M Mamba draft. The 7B verifier uses a Llama3 1B draft we trained. Data is from The Pile. K is number of draft tokens produced, # Gen includes an additional token from the last verifier logits.", "description": "This table presents the speedup achieved by using speculative decoding with pure Mamba models of different sizes (2.8B and 7B parameters) on different GPUs (3090 and H100).  The speedup is calculated by comparing the throughput (tokens per second) of speculative decoding against a baseline non-speculative decoding approach. It shows that speculative decoding provides substantial speed improvements, particularly with the H100 GPU.  The number of draft tokens generated and the model sizes used are also indicated.", "section": "4.3 Speculation Analysis and Hardware Specific Optimization"}, {"figure_path": "uAzhODjALU/tables/tables_8_1.jpg", "caption": "Table 2: Chat benchmark results for open-access and proprietary models on MT-Bench and AlpacaEval. MT-Bench scores model responses using GPT-4. AlpacaEval version two measures the win-loss rate between baseline models and GPT-4 scored by GPT-4 Turbo.", "description": "This table compares the performance of various language models, including the distilled hybrid Mamba models, on two chat benchmarks: MT-Bench and AlpacaEval.  MT-Bench uses GPT-4 to score the model's responses, while AlpacaEval uses GPT-4 Turbo to measure the win/loss rate against GPT-4. The table shows the scores achieved by different models, highlighting the performance of the distilled models compared to the original models and other baselines. The model size, alignment method, and architecture are also specified.", "section": "5.2 Evaluation on Chat Benchmarks"}, {"figure_path": "uAzhODjALU/tables/tables_8_2.jpg", "caption": "Table 3: Evaluation on LM Eval benchmark for Mamba and Mamba2 distilled from Llama-3 Instruct 8B.", "description": "This table presents the results of zero-shot evaluation on the LM-Eval benchmark for Mamba and Mamba2 models distilled from Llama-3 Instruct 8B.  It compares the performance of different configurations of these models (varying the percentage of attention layers replaced with linear RNNs) across ten tasks, showing their accuracy or normalized accuracy on each. The table also includes results for baseline models TRI Mamba-7B and Nvidia Hybrid Mamba-8B for comparison.  The purpose is to demonstrate the effectiveness of the distillation method and the performance of the resulting hybrid models in a general language modeling evaluation setting.", "section": "5.3 Evaluation on General Benchmarks"}, {"figure_path": "uAzhODjALU/tables/tables_9_1.jpg", "caption": "Table 4: Results on the Open LLM Leaderboard and ZeroEval Leaderboard. For GSM8K and CRUX, we chose the zero-shot evaluation using ZeroEval, which is designed for evaluating instruct models. We evaluated the corresponding instruct-tuned models for Falcon Mamba-7b and RecurrentGemma-9B, specifically Falcon Mamba-7b-instruct and RecurrentGemma-9B-it.", "description": "This table presents the zero-shot performance of different language models on various benchmarks from the Open LLM Leaderboard and ZeroEval Leaderboard.  The benchmarks cover diverse tasks including commonsense reasoning, knowledge, and code understanding.  The table compares the performance of distilled hybrid Mamba models with different percentages of attention layers retained against several strong baselines, including Falcon Mamba and RecurrentGemma. The results highlight the competitive performance of the distilled models, particularly those with a higher percentage of attention layers.", "section": "5.3 Evaluation on General Benchmarks"}, {"figure_path": "uAzhODjALU/tables/tables_9_2.jpg", "caption": "Table 5: Performance metrics for different draft and target model configurations for K = 4 on data from OpenHermes2.5. # Gen is the average number of generated tokens per speculative decoding step and includes an additional token from the last verifier logits.", "description": "This table presents the results of speculative decoding experiments using different configurations of draft and target models.  It shows the speedup achieved by speculative decoding compared to a non-speculative baseline for different models and numbers of generated tokens.  The experiments were performed on the OpenHermes2.5 dataset.", "section": "5.4 Hybrid speculative decoding"}, {"figure_path": "uAzhODjALU/tables/tables_15_1.jpg", "caption": "Table 6: (Left) Perplexity comparison between our distillation approach and [59]. (Right) Ablation study of different alignment methods of the Distilled Hybrid Mamba on the MT-benchmark using OpenHermes 2.5 as the SFT dataset.", "description": "This table presents a comparison of perplexity scores between the proposed distillation method and a previous approach [59], showing the impact of removing attention layers from the model.  The right side shows an ablation study investigating the effects of different alignment methods (distillation, supervised fine-tuning (SFT), and directed preference optimization (DPO)) on the performance of the distilled hybrid Mamba model using a specific dataset. ", "section": "A Analysis"}, {"figure_path": "uAzhODjALU/tables/tables_15_2.jpg", "caption": "Table 6: (Left) Perplexity comparison between our distillation approach and [59]. (Right) Ablation study of different alignment methods of the Distilled Hybrid Mamba on the MT-benchmark using OpenHermes 2.5 as the SFT dataset.", "description": "This table presents a comparison of perplexity scores between the proposed distillation method and a previous method from the literature ([59]). It also shows an ablation study on different alignment methods for the distilled hybrid Mamba model, using the MT-benchmark and the OpenHermes 2.5 dataset for supervised fine-tuning.", "section": "A Analysis"}, {"figure_path": "uAzhODjALU/tables/tables_15_3.jpg", "caption": "Table 7: (Left) Perplexity comparison with different initialization. (Right) Perplexity comparison with different Mamba interleaving layers and stepwise distillation.", "description": "This table presents the results of ablation studies on the knowledge distillation process for the Mamba model. The left side shows the impact of different initialization methods on perplexity, comparing models initialized with transformer weights versus random initialization.  The right side shows the effect of different distillation strategies (interleaving attention and Mamba layers versus a stepwise approach) on perplexity.", "section": "A Analysis"}, {"figure_path": "uAzhODjALU/tables/tables_16_1.jpg", "caption": "Table 8: Performance of Zephyr-Mamba (50% attention) with different initialization.", "description": "This table compares the performance of the hybrid model (Zephyr-Mamba with 50% attention layers) using two different initialization methods: default random initialization and reusing the linear projection from the attention layers of the original transformer model.  The results show that using the linear projection from the attention layers leads to significantly better performance across all evaluated benchmarks, highlighting the importance of proper weight initialization for effective knowledge distillation.", "section": "5 Results"}, {"figure_path": "uAzhODjALU/tables/tables_16_2.jpg", "caption": "Table 9: Performance of Hybrid-Mamba with different initialization.", "description": "This table compares the performance of the hybrid model (50% attention with Mamba and 50% attention without Mamba) using two different initialization methods: default random initialization and reusing the linear projection from the attention.  The results show that initializing from attention weights is crucial for good performance.", "section": "5.3 Evaluation on General Benchmarks"}]