[{"figure_path": "bJddXCyosA/tables/tables_5_1.jpg", "caption": "Table 1: Comparison of benchmarks offering visual hard negatives (HN) across: minimal HN, visual complexity, textual complexity, human-approved captions (!) and images (), and size. : criterion holds for a subset of the benchmark.", "description": "This table compares several existing benchmarks for evaluating fine-grained visual understanding in Vision-Language Models (VLMs).  It assesses benchmarks across five key criteria: 1) whether the hard-negative examples present minimal changes (Visual Minimal HN); 2) complexity of the visual scenes (Visual Complexity), considering aspects like image source and scene realism; 3) complexity of the captions (Textual Complexity), differentiating between free-form and template-based captions; 4) whether captions and images were verified and approved by human annotators; and 5) the size of the benchmark dataset.  VisMin is presented alongside existing methods, highlighting its unique characteristics.", "section": "4 Training and Benchmark sets"}, {"figure_path": "bJddXCyosA/tables/tables_6_1.jpg", "caption": "Table 2: Performance of foundational variants and MLLMs across categories on the VisMin Dataset. Columns 'I,' 'T,' and 'G' denote Image, Text, and Group scores from Winoground [36]. AVG denotes the average across columns. The best results are highlighted in bold.", "description": "This table presents the performance of various foundational vision-language models (VLMs) and multimodal large language models (MLLMs) on the VisMin benchmark.  It shows the image (I), text (T), and group (G) scores for each model across four categories of minimal changes (object, attribute, spatial relation, and count). The scores are compared to random chance and human performance on the same task, indicating each model's strengths and weaknesses in fine-grained visual understanding.  The best-performing model in each category and overall is highlighted in bold.", "section": "5 Benchmarking VLMs on VisMin Benchmark"}, {"figure_path": "bJddXCyosA/tables/tables_7_1.jpg", "caption": "Table 2: Performance of foundational variants and MLLMs across categories on the VisMin Dataset. Columns 'I,' 'T,' and 'G' denote Image, Text, and Group scores from Winoground [36]. AVG denotes the average across columns. The best results are highlighted in bold.", "description": "This table presents the performance of various foundational vision-language models (VLMs) and multimodal large language models (MLLMs) on the VisMin benchmark.  It shows the image (I), text (T), and group (G) scores for each model across four categories: object, attribute, spatial relation, and count.  The Winoground scores are included for comparison.  The average (AVG) score across all four categories is also provided. The best-performing models in each category are highlighted in bold.", "section": "5 Benchmarking VLMs on VisMin Benchmark"}, {"figure_path": "bJddXCyosA/tables/tables_8_1.jpg", "caption": "Table 2: Performance of foundational variants and MLLMs across categories on the VisMin Dataset. Columns 'I', 'T', and 'G' denote Image, Text, and Group scores from Winoground [36]. AVG denotes the average across columns. The best results are highlighted in bold.", "description": "This table presents the performance of various foundational vision-language models (VLMs) and large multimodal language models (MLLMs) on the VisMin benchmark.  The benchmark evaluates the models' ability to identify minimal changes (object, attribute, spatial relation, count) between image-caption pairs.  The table shows individual scores for Image (I), Text (T), and Group (G) accuracy, along with an average (AVG) score.  The Winoground scoring metric is used, and the best results for each category and model are highlighted in bold.", "section": "Results"}, {"figure_path": "bJddXCyosA/tables/tables_17_1.jpg", "caption": "Table 2: Performance of foundational variants and MLLMs across categories on the VisMin Dataset. Columns 'I,' 'T,' and 'G' denote Image, Text, and Group scores from Winoground [36]. AVG denotes the average across columns. The best results are highlighted in bold.", "description": "This table presents the performance of various foundational and multimodal large language models (VLMs) on the VisMin benchmark, categorized by object, attribute, spatial relation, and count changes.  For each category, the table displays the image score (I), text score (T), and group score (G) achieved by each model, representing its ability to correctly match images and captions. The average score across all categories is also provided.  The best performance for each category is highlighted in bold.", "section": "Results"}, {"figure_path": "bJddXCyosA/tables/tables_17_2.jpg", "caption": "Table 2: Performance of foundational variants and MLLMs across categories on the VisMin Dataset. Columns 'I,' 'T,' and 'G' denote Image, Text, and Group scores from Winoground [36]. AVG denotes the average across columns. The best results are highlighted in bold.", "description": "This table presents the performance of different foundational vision-language models (VLMs) and multimodal large language models (MLLMs) on the VisMin benchmark.  It shows the image (I), text (T), and group (G) scores for each model across four categories of minimal changes: object, attribute, spatial relation, and count. The Winoground scores are included for comparison, and the average score across all categories is also provided.  The best-performing models in each category are highlighted in bold.", "section": "Results"}, {"figure_path": "bJddXCyosA/tables/tables_19_1.jpg", "caption": "Table 2: Performance of foundational variants and MLLMs across categories on the VisMin Dataset. Columns 'I,' 'T,' and 'G' denote Image, Text, and Group scores from Winoground [36]. AVG denotes the average across columns. The best results are highlighted in bold.", "description": "This table presents the performance of various foundational vision-language models (VLMs) and multimodal large language models (MLLMs) on the VisMin benchmark.  The benchmark evaluates the models' ability to detect minimal changes in four categories: object, attribute, spatial relation, and count.  The table shows the image (I), text (T), and group (G) scores for each model across these categories, along with the average score (AVG).  Image, Text, and Group scores are adapted from the Winoground benchmark, and the best-performing models in each category are highlighted in bold.", "section": "Results"}, {"figure_path": "bJddXCyosA/tables/tables_22_1.jpg", "caption": "Table 2: Performance of foundational variants and MLLMs across categories on the VisMin Dataset. Columns 'I,' 'T,' and 'G' denote Image, Text, and Group scores from Winoground [36]. AVG denotes the average across columns. The best results are highlighted in bold.", "description": "This table presents the performance of various foundational vision-language models (VLMs) and multimodal large language models (MLLMs) on the VisMin benchmark.  The results are broken down by four categories of minimal changes: Object, Attribute, Spatial Relation, and Count. For each category, image (I), text (T), and group (G) scores are shown, along with an average (AVG) across these three scores.  The scores are compared to random chance and human performance.  The table highlights the best-performing models for each category and overall, indicating the relative strengths and weaknesses of different model architectures in fine-grained visual understanding.", "section": "Results"}, {"figure_path": "bJddXCyosA/tables/tables_23_1.jpg", "caption": "Table 2: Performance of foundational variants and MLLMs across categories on the VisMin Dataset. Columns 'I', 'T', and 'G' denote Image, Text, and Group scores from Winoground [36]. AVG denotes the average across columns. The best results are highlighted in bold.", "description": "This table presents the performance of various foundational Vision-Language Models (VLMs) and Multimodal Large Language Models (MLLMs) on the VisMin benchmark.  The models are evaluated across four categories of minimal changes (object, attribute, spatial relation, count), using three scoring metrics: Image score, Text score, and Group score.  The Image score measures the model's ability to select the correct image given two captions; the Text score measures its ability to choose the correct caption given two images; and the Group score combines both. The table shows the performance (in percentages) for each model on each metric across all four categories and also provides the average score across all categories. The best results in each category and metric are highlighted in bold, providing a direct comparison of model performance in fine-grained visual-linguistic understanding.", "section": "Results"}]