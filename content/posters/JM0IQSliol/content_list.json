[{"type": "text", "text": "Shapes analysis for time series ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Thibaut Germain1\u2217 Centre Borelli, ENS Paris-Saclay 4 av. des sciences, 91190 ", "page_idx": 0}, {"type": "text", "text": "Samuel Gruffaz1 Centre Borelli, ENS Paris-Saclay 4 av. des sciences, 91190 ", "page_idx": 0}, {"type": "text", "text": "Charles Truong1 Centre Borelli, ENS Paris-Saclay 4 av. des sciences, 91190 ", "page_idx": 0}, {"type": "text", "text": "Laurent Oudre1 Centre Borelli, ENS Paris-Saclay 4 av. des sciences, 91190 ", "page_idx": 0}, {"type": "text", "text": "Alain Durmus CMAP, CNRS, Ecole polytechnique Institut Polytechnique de Paris 91120 Palaiseau, France ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Analyzing inter-individual variability of physiological functions is particularly appealing in medical and biological contexts to describe or quantify health conditions. Such analysis can be done by comparing individuals to a reference one with time series as biomedical data. This paper introduces an unsupervised representation learning (URL) algorithm for time series tailored to inter-individual studies. The idea is to represent time series as deformations of a reference time series. The deformations are diffeomorphisms parameterized and learned by our method called TS-LDDMM. Once the deformations and the reference time series are learned, the vector representations of individual time series are given by the parametrization of their corresponding deformation. At the crossroads between URL for time series and shape analysis, the proposed algorithm handles irregularly sampled multivariate time series of variable lengths and provides shape-based representations of temporal data. In this work, we establish a representation theorem for the graph of a time series and derive its consequences on the LDDMM framework. We showcase the advantages of our representation compared to existing methods using synthetic data and real-world examples motivated by biomedical applications. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Our goal is to analyze the inter-individual variability within a time series dataset, an approach of significant interest in physiological contexts [25, 58, 4, 21]. Specifically, we aim to develop an unsupervised feature representation method that encodes the specificities of individual time series in comparison to a reference time series. In physiology, examining the various \"shapes\" in a time series related to biological phenomena and their variations due to individual differences or pathological conditions is common. However, the term \"shape\" lacks a precise definition and is more intuitively understood as the silhouette of a pattern in a time series. In this paper, we refer to the shape of a time series as the graph of this signal. ", "page_idx": 0}, {"type": "image", "img_path": "JM0IQSliol/tmp/423df714a4473005a5f73508cb6c8c126dd636b5b43a49e34b387af656941357.jpg", "img_caption": ["Figure 1: A time series\u2019 graph ${\\mathsf{G}}=\\{(t,s(t)):\\;t\\in|\\}$ can lose its structure after applying a general diffeomorphism $\\phi.6$ : a time value can be related to two values on the space axis. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Although community structures with representatives can be learned in an unsupervised manner [55, 39] using contrastive loss [20, 54, 39] or similarity measures [2, 21, 45, 62], the study of interindividual variability of shapes within a cluster [42, 51] remains an open problem in unsupervised representation learning (URL), particularly for irregularly sampled time series with variable lengths. ", "page_idx": 1}, {"type": "text", "text": "Our work explicitly focuses on learning shape-based representation of time series. First, we propose to view the shape of a time series not merely as its curve $\\{s_{t}:~t\\in{\\mathsf{I}}\\}$ , but as its graph $\\mathsf{G}(s)=$ $\\{(t,s(t))~\\colon~t~\\bar{\\in}~{\\sf I}\\}$ . Then, building on the shape analysis literature [5, 57], we adopt the Large Deformation Diffeomorphic Metric Mapping (LDDMM) framework [5, 57] to analyze these graphs. The core idea is to represent each element $\\mathsf{\\bar{G}}(s^{j})$ of a dataset $(s^{j})_{j\\in[N]}$ as the transformation of a reference graph $\\mathsf{G}(\\mathbf{s}_{0})$ by a diffeomorphism $\\phi_{j}$ , i.e. $\\mathsf{G}(s^{j})\\sim\\phi_{j}.\\mathsf{G}(\\mathbf{s}_{0})$ . The diffeomorphism $\\phi_{j}$ is learned by integrating an ordinary differential equation parameterized by a Reproducing Kernel Hilbert Space (RKHS). The parameters $(\\alpha_{j})_{j\\in[N]}$ encoding the diffemorphisms $(\\phi_{j})_{j\\in[N]}$ yield the representation features of the graphs $(\\mathsf{G}(s^{j}))_{j\\in[N]}$ . Finally, these shape-encoding features can be used as inputs to any statistical or machine-learning model. ", "page_idx": 1}, {"type": "text", "text": "However, a time series graph transformation by a general diffeomorphism is not always a time series graph, see e.g. Figure 1, thus a time series graph is more than a simple curve [23]. Our contributions arise from this observation: we specify the class of diffeomorphisms to consider and show how to learn them. This change is fruitful in representing transformations of time series graphs as illustrated in Figure 2. ", "page_idx": 1}, {"type": "text", "text": "Our contributions can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "\u2022 We propose an unsupervised method (TS-LDDMM) to analyze the inter-individual variability of shapes in a time series dataset (Section 4). In particular, the method can handle multivariate time series irregularly sampled and with variable sizes.   \n\u2022 We motivate our extension of LDDMM to time series by introducing a theoretical framework with a representation theorem for time series graph (Theorem 1) and kernels related to their structure (Lemma 1).   \n\u2022 We demonstrate the identifiability of the model by estimating the true generating parameter of synthetic data, and we highlight the sensitivity of our method concerning its hyperparameters (Appendix G.1), also providing guidelines for tuning (Appendix D).   \n\u2022 We highlight the interpretability of TS-LDDMM for studying the inter-individual variability in a clinical dataset (Section 5).   \n\u2022 We illustrate the quantitative interest of such representation on classification tasks on real shape-based datasets with regular and irregular sampling (Appendices $\\mathrm{H}$ and I). ", "page_idx": 1}, {"type": "text", "text": "2 Notations ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We denote by integer ranges by $[k:l]\\,=\\,\\{k,\\dots,l\\}\\,\\subset\\,\\mathscr{P}(\\mathbb{Z})$ and $[l]=[1:l]$ with $k,l\\in\\mathbb{N}$ , by $\\mathrm{C}^{m}(\\mathsf{I},\\mathsf{E})$ the set of $m$ -times continously differentiable function defined on an open set $\\mathsf{U}$ to a normed vector space $\\mathsf{E}$ , by $\\begin{array}{r}{||u||_{\\infty}=\\operatorname*{sup}_{x\\in\\mathsf{U}}|\\dot{u}(x)|}\\end{array}$ for any bounded function $u:\\mathsf{U}\\to\\mathsf{E}$ , and by $\\mathbb{N}_{>0}$ is the set of positive integers. ", "page_idx": 1}, {"type": "image", "img_path": "JM0IQSliol/tmp/c817218d604ed4bc10800ca6b02741cf90e3d69f4042cb7f624409496b29aca4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 2: LDDMM and TS-LDDMM are applied to ECG data. We observe that LDDMM, using a general Gaussian kernel, does not learn the time translation of the first spike but changes the space values, i.e., one spike disappears before emerging at a translated position. At the same time, TSLDDMM handles the time change in the shape. This difference of deformations implies differences in features representations. ", "page_idx": 2}, {"type": "text", "text": "3 Background on LDDMM ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In this part, we expose how to learn the diffeomorphisms $(\\phi_{j})_{j\\in[N]}$ using LDDMM, initially introduced in [5]. In a nutshell, for any $j\\in[N]$ , $\\phi_{j}$ corresponds to a differential flow related to a learnable velocity field belonging to a well-chosen Reproducing Kernel Hilbert Space (RKHS). ", "page_idx": 2}, {"type": "text", "text": "In the next section, time series are going to be represented by diffeomorphism parameters $(\\alpha_{j})_{j\\in[N]}$ . That is why LDDMM is chosen since it offers a parametrization for diffeomorphisms that is sparse and interpretable, two features particularly relevant in the biomedical context. ", "page_idx": 2}, {"type": "text", "text": "The basic problem that we consider in this section is the following. Given a set of targets $\\textbf{y}=$ $(y_{i})_{i\\in[T_{2}]}$ in $\\mathbb{R}^{d^{\\prime}2}$ , a set of starting points $\\mathbf{x}=(x_{i})_{i\\in[T_{1}]}$ in $\\mathbb{R}^{d^{\\prime}}$ , we aim to find a diffeomorphism $\\phi$ such that the finite set of points $\\mathbf{y}$ is similar in a certain sense to the set of finite sets of transformed points $\\phi\\cdot\\mathbf{x}=(\\phi(x_{i}))_{i\\in[T_{1}]}$ . The function $\\phi$ is occasionally referred to as a deformation. In general, these sets $\\mathbf x,\\mathbf y$ are meshes of continuous objects, e.g., surfaces, curves, images, and so on. ", "page_idx": 2}, {"type": "text", "text": "Representing diffeomorpshims as deformations. Such deformations $\\phi$ are constructed via differential flow equations, for any $\\boldsymbol{x}_{0}\\in\\mathbb{R}^{d^{\\prime}}$ and $\\tau\\in[0,1]$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\frac{\\mathrm{d}X(\\tau)}{\\mathrm{d}\\tau}=v_{\\tau}(X(\\tau)),\\quad X(0)=x_{0}\\;,\\phi_{\\tau}^{v}(x_{0})=X(\\tau),\\quad\\phi^{v}=\\phi_{1}^{v}\\;,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where the velocity field is $v\\,:\\,\\tau\\,\\in\\,[0,1]\\,\\mapsto\\,v_{\\tau}\\,\\in\\,\\vee$ and $\\vee$ is a Hilbert space of continuously differentiable function on $\\mathbb{R}^{d^{\\prime}}$ . If $||\\,\\mathrm{d}u||_{\\infty}+||u||_{\\infty}\\leq||u||_{\\mathsf{v}}$ for any $u\\in\\mathsf{V}$ and $v\\in\\mathrm{L}^{2}([0,1],\\lor)=$ $\\begin{array}{r}{\\{v\\in\\mathrm{C}^{0}([0,1],\\mathsf{V}):\\int_{0}^{1}||v_{\\tau}||_{\\mathsf{V}}^{2}\\,\\mathrm{d}\\tau<\\infty\\}}\\end{array}$ , by [22, Theorem 5] $\\phi^{v}$ exists and belongs to $\\mathcal{D}(\\mathbb{R}^{d^{\\prime}})$ , where we denote by $\\mathcal{D}(0)$ the set of diffeomorpshim defined on an open set $\\textsf{O}$ to O. Therefore, for any choice of $v$ , $\\phi^{v}$ defines a valid deformation. This offers a general recipe to construct diffeomorphism given a functional space $\\vee$ . ", "page_idx": 2}, {"type": "text", "text": "With this in mind, the velocity field $v$ fitting the data can be estimated by minimizing $v\\in$ $\\mathrm{L}^{2}([0,1],\\vee)\\,\\mapsto\\,\\mathcal{L}(\\phi^{v}.{\\bf x},{\\bf y})$ , where $\\mathcal{L}$ is an appropriate loss function. However, two computational challenges arise. First, this optimization problem is ill-posed, and a penalty term is needed to obtain a unique solution. In addition, a parametric family $\\mathsf{V}_{\\Theta}\\\\\\dot{\\subset}\\,\\mathrm{L}^{2}([0,1],\\dot{\\mathsf{V}})$ , parameterized by $\\Theta$ , is sought to efficiently solve this minimization problem. ", "page_idx": 2}, {"type": "text", "text": "From deformations to geodesics. It has been proposed in [40] to interpret $\\vee$ as a tangent space relative to the group of diffeomorphisms ${\\sf H}=\\{\\hat{\\phi}^{v}:^{\\star}v\\in\\mathrm{L}^{2}([0,\\bar{1}],\\vee)\\}$ . Following this geometric point of view, geodesics can be constructed on $\\mathsf{H}$ by using the following squared norm ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{R}^{2}:g\\in\\mathsf{H}\\mapsto\\operatorname*{inf}_{\\substack{v\\in\\mathrm{L}^{2}([0,1],\\mathsf{V}):\\,g=\\phi^{v}}}\\int_{0}^{1}||v_{\\tau}||_{\\mathsf{V}}^{2}\\,\\mathrm{d}\\tau\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "By deriving differential constraints related to the minimum of (2) and using Cauchy-Lipschitz conditions, geodesics can be defined only by giving the starting point and the initial velocity $v_{0}\\in\\mathsf{V}$ [40], as straight lines in Euclidean space. Denoting by $\\tau\\mapsto\\rho_{v_{0}}(\\tau)\\in\\mathsf{H}$ the geodesic starting from ", "page_idx": 2}, {"type": "text", "text": "the identity with inital velocity $v_{0}\\in\\mathsf{V}$ , the exponential map is defined as $\\varphi^{\\{v_{0}\\}}\\triangleq\\rho_{v_{0}}(1)$ . Using $\\varphi^{\\{v_{0}\\}}$ instead of $\\phi^{v}$ , the previous matching problem becomes a geodesic shooting problem: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{v_{0}\\in\\mathbf{V}}\\mathcal{L}(\\varphi^{\\{v_{0}\\}}.\\mathbf{x},\\mathbf{y}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Using $\\varphi^{\\{v_{0}\\}}$ instead of $\\phi^{v}$ for any $v\\in\\mathrm{L}^{2}([0,1],\\lor)$ regularizes the problem and induces a sparse representation for the learning diffeomorphisms. Moreover, by setting $\\vee$ as an RKHS, the geodesic shooting problem has a unique solution and becomes tractable, as described in the next section. ", "page_idx": 3}, {"type": "text", "text": "Discrete parametrization of diffeomorpshim. In this part, $\\vee$ is chosen as an RKHS [6] generated by a smooth kernel $K$ (e.g., Gaussian). We follow [17] and define a discrete parameterization of the velocity fields to perform geodesics shooting (3). The initial velocity field $v_{0}$ is chosen as a finite linear combination of the RKHS basis vector fields, $\\mathbf{n}_{0}$ control points $\\mathsf{X}_{0}=(\\boldsymbol{x}_{k,0})_{k\\in[\\mathbf{n}_{0}]}\\in(\\mathbb{R}^{d^{\\prime}})^{\\mathbf{n}_{0}}$ and momentum vectors $\\alpha_{0}=(\\alpha_{k,0})_{k\\in[{\\bf n}_{0}]}\\in(\\mathbb{R}^{d^{\\prime}})^{{\\bf n}_{0}}$ are defined such that for any $\\boldsymbol{x}\\in\\mathbb{R}^{d^{\\prime}}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\nv_{0}\\left(\\alpha_{0},\\mathsf{X}_{0}\\right)(x)=\\sum_{k=1}^{\\mathbf{n}_{0}}K(x,x_{k,0})\\alpha_{k,0}\\ .\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In our applications, the control points $(x_{k,0})_{k\\in[\\mathbf{n}_{0}]}$ can be understood as the discretized graph $(t_{k},\\mathbf{s}_{0}(t_{k}))_{k\\in[\\mathbf{n}_{0}]}$ of a starting time series $\\mathbf{s}_{\\mathrm{0}}$ . With this parametrization of $v_{0}$ , [40] show that the velocity field $v$ of the solution of (3) keeps the same structure along time, such that for any $\\boldsymbol{x}\\in\\mathbb{R}^{d^{\\prime}}$ and $\\tau\\in[0,1]$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle v_{\\tau}(\\boldsymbol{x})=\\sum_{k=1}^{\\mathbf{n}_{0}}K(\\boldsymbol{x},\\boldsymbol{x}_{k}(\\tau))\\alpha_{k}(\\tau)\\;,}\\\\ &{\\displaystyle\\left\\{\\frac{\\mathrm{d}x_{k}(\\tau)}{\\mathrm{d}\\tau}=v_{\\tau}(x_{k}(\\tau))\\;,\\quad\\frac{\\mathrm{d}\\alpha_{k}(\\tau)}{\\mathrm{d}\\tau}=-\\sum_{k=1}^{\\mathbf{n}_{0}}\\mathrm{d}_{x_{k}(\\tau)}K(x_{k}(\\tau),x_{l}(\\tau))\\alpha_{l}(\\tau)^{\\top}\\alpha_{k}(\\tau)\\right.}\\\\ &{\\displaystyle\\left.\\alpha_{k}(0)=\\alpha_{k,0},\\quad x_{k}(0)=x_{k,0}\\;,k\\in[\\mathbf{n}_{0}]\\right.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "These equations are derived from the hamiltonian $\\begin{array}{r}{H:(\\alpha_{k},x_{k})_{k\\in[\\mathbf{n}_{0}]}\\mapsto\\sum_{k,l=1}^{\\mathbf{n}_{0}}\\alpha_{k}^{\\top}K(x_{k},x_{l})\\alpha_{l}}\\end{array}$ such that the velocity norm is preserved $||\\boldsymbol{v}_{\\tau}||_{\\mathsf{v}}=||\\boldsymbol{v}_{0}||_{\\mathsf{v}}$ for any $\\tau\\in[0,1]$ . By (5), the velocity field related to a geodesic $v^{*}$ is fully parametrized by its initial control points and momentum $(x_{k,0},\\alpha_{k,0})_{k\\in[{\\bf n}_{0}]}$ . Thus, given a set of targets $\\mathbf{y}=(y_{i})_{i\\in[T_{2}]}$ in $\\mathbb{R}^{d^{\\prime}}$ , a set of starting points $\\textbf{x}=$ $(x_{i,0})_{i\\in[T_{1}]}$ in $\\mathbb{R}^{d^{\\prime}}$ , a RKHS\u2019s kernel $K:\\mathbb{R}^{d^{\\prime}}\\times\\mathbb{R}^{d^{\\prime}}\\rightarrow\\mathbb{R}^{d^{\\prime}\\times\\bar{d}^{\\prime}}$ , a distance on sets $\\mathcal{L}$ , a numerical integration scheme of ODE and a penalty factor $\\lambda>0$ , the basic geodesic shooting step minimizes the following function using a gradient descent method: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{F}_{\\mathbf{x},\\mathbf{y}}:(\\alpha_{k})_{k\\in[T_{1}]}\\mapsto\\mathcal{L}\\left(\\varphi^{\\{v_{0}\\}}.\\mathbf{x},\\mathbf{y}\\right)+\\lambda||v_{0}||_{\\mathsf{V}}^{2}~,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $v_{0}$ is defined by (4) and $\\varphi^{\\{v_{0}\\}}.\\mathbf{x}$ is the result of the numerical integration of (5) using control points $\\mathbf{x}$ and initial momentums $(\\alpha_{k})_{k\\in[T_{1}]}$ . ", "page_idx": 3}, {"type": "text", "text": "Relation to Continuous Normalizing Flows. One particular popular choice to address the problem of learning a diffeomorphism or a velocity field is Normalizing Flows [47, 32] (NF) or their continuous counterpart [13, 24, 48] (CNF). However, we do not rely on this class of learning algorithms for several reasons. Indeed, existing and simple normalizing flows are not suitable for the type of data that we are interested in this paper [19, 16]. In addition, they are primarily designed to have tractable Jacobian functions, while we do not require such property in our applications. Finally, the use of a differential flow solution of an ODE (1) trick is also at the basis of CNF, which then consists of learning a velocity field to address in ftiting the data through a loss aiming to address the problem at hand. Nevertheless, the main difference between CNF and LDDMM lies in the parametrization of the velocity field. LDDMM uses kernels to derive closed form formula and enhance interpretability while NF and CNF take advantage of deep neural networks to scale with large dataset in high dimensions. ", "page_idx": 3}, {"type": "text", "text": "4 Methodology ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We consider in this paper observations which consist in a population of $N$ multivariate time series, for any $j\\in[N]$ , $s^{j}\\in\\mathrm{C}^{1}(\\mathsf{I}_{j},\\mathbb{R}^{d})$ . However, we can only access a $n_{j}$ -samples $\\tilde{s}^{j}=(\\tilde{s}_{i}^{j}=s^{j}(t_{i}^{j}))_{i\\in[n_{j}]}$ collected at timestamps $(t_{i}^{j})_{i\\in[n_{j}]}$ for any $j\\,\\in\\,[N]$ . Note that the number of samples $n_{j}$ is not necessarily the same across individuals and the timestamps can be irregularly sampled. We assume the time series population is globally homogeneous regarding their \"shapes\" even if inter-individual variability exists. Intuitively speaking, the \"shape\" of a time series $s:\\mathsf{I}\\to\\mathbb{R}^{d}$ is encoded in its graphs ${\\sf G}(s)$ defined as the set $\\{(i,s(t)):\\;\\bar{t}\\in\\mathsf{I}\\}$ and not only in its values $s(\\mathfrak{l})=\\{s(t):\\;t\\in\\mathfrak{l}\\}$ since the time axis is crucial. As a motivating use-case, $s^{j}$ can be the time series of a heartbeat extracted from an individual\u2019s electrocardiogram (ECG), see Figure 2. The homogeneity in a resulting dataset comes from the fact that humans have similar shapes of heartbeat [61, 37]. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "The deformation problem. In this paper, we aim to study the inter-individual variability in the dataset by finding a relevant representation of each time series. Inspired from the framework of shape analysis [57], addressing similar problems in morphology, we suggest to represent each time series\u2019 graph ${\\sf G}(s^{j})$ as the transformation of a reference graph ${\\sf G}({\\bf s}_{0})$ , related to a time series $\\mathbf{s}_{0}:\\mathsf{I}\\to\\mathbb{R}^{d}$ , by a diffeomorphism $\\phi_{j}$ on $\\mathbb{R}^{d+1}$ , for any $j\\in[N]$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\phi_{j}.\\mathsf{G}(\\mathbf{s}_{0})=\\left\\{\\phi_{j}\\left(t,\\mathbf{s}_{0}(t)\\right),\\;t\\in\\mathsf{I}\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "$\\mathbf{s}_{\\mathrm{0}}$ will be understood as the typical representative shape common to the collection of time series $(s^{j})_{j\\in[N]}$ . As $\\mathbf{s}_{\\mathrm{0}}$ is supposed to be fixed, then the representation of the time series $(s^{j})_{j\\in[N]}$ boils down to the one of the transformation $(\\phi_{j})_{j\\in[N]}$ . We aim to learn $\\mathsf{G}(\\mathbf{s}_{0})$ and $(\\phi_{j})_{j\\in[N]}$ . ", "page_idx": 4}, {"type": "text", "text": "Optimization related to (7). Defining the discretized graphs of the time series $(s^{j})_{j\\in[N]}$ and a discretization of the reference graph $\\mathsf{G}(\\mathbf{s}_{0})$ as, for any $j\\in[N]$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{y}_{j}=\\mathsf{G}(\\widetilde{s}^{j})=(t_{i}^{j},\\widetilde{s}_{i}^{j})_{i\\in[n_{j}]}\\in(\\mathbb{R}^{d+1})^{n_{j}},\\quad\\widetilde{\\mathsf{G}}_{0}=(t_{i}^{0},\\widetilde{s}_{i}^{0})_{i\\in[\\mathbf{n}_{0}]}\\in(\\mathbb{R}^{d+1})^{\\mathbf{n}_{0}}\\;,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with ${\\bf n}_{0}=\\mathrm{median}((n_{j})_{j\\in[N]})$ , the representation problem given in (7) boils down solving: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathrm{argmin}_{\\tilde{\\mathsf{G}}_{0},(\\alpha_{k}^{j})_{k\\in[\\mathsf{n}_{0}]}^{j\\in[N]}}\\sum_{j=1}^{N}\\mathcal{F}_{\\tilde{\\mathsf{G}}_{0},\\mathbf{y}_{j}}\\left((\\alpha_{k}^{j})_{k\\in[\\mathsf{n}_{0}]}\\right)\\,,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which is carried out by gradient descent on the control points $\\tilde{\\mathsf{G}}_{0}$ and the momentums $\\boldsymbol{\\alpha}_{j}=(\\alpha_{k}^{j})_{k\\in[\\mathbf{n}_{0}]}$ for any $j\\in[N]$ , initialized by a dataset\u2019s time series graph of size $\\mathbf{n}_{0}$ and by $0_{(d+1)\\mathbf{n}_{0}}$ respectively. The optimization hyperparameter details are given in Appendix E.1. The result of the minimization $\\tilde{\\mathsf{G}}_{0}$ is then considered as the $\\mathbf{n}_{\\mathrm{0}}$ -samples of a common time series $\\mathbf{s}_{\\mathrm{0}}$ and the momentums $\\alpha_{j}$ encoding $\\phi_{j}$ yields a feature vector in $\\mathbb{R}^{d\\mathbf{n}_{0}}$ of $s^{j}$ for any $j~\\in~[N]$ . Finally, the vectors $(\\alpha_{j})_{j\\in[N]}$ can be analyzed with any statistical or machine learning tools such as Principal Components Analysis (PCA), Latent Discriminant Analysis (LDA), longitudinal data analysis and so on. ", "page_idx": 4}, {"type": "text", "text": "Nevertheless, (8) asks to define a kernel and a loss in order to perform geodesic shooting (6), which is the purpose of the following subsection. ", "page_idx": 4}, {"type": "text", "text": "4.1 Application of LDDMM to time series analysis: TS-LDDMM ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "This section presents our theoretical contribution: we tailor the LDDMM framework to handle time series data. The reason is that applying a general diffeomorphism $\\phi$ from $\\mathbb{R}^{d+1}$ to a time series\u2019 graph ${\\sf G}(s)$ can result in a set $\\phi.\\mathsf{G}(s)$ that does not correspond to the graph of any time series, as illustrated in the Figure 1. Thus, time series graphs have more structure than a simple 1D curve [23] and deserve their unique analysis, which will prove fruitful as demonstrated in Section 5. ", "page_idx": 4}, {"type": "text", "text": "To address this challenge, we need to identify an RKHS kernel $K:\\mathbb{R}^{d+1}\\times\\mathbb{R}^{d+1}\\rightarrow\\mathbb{R}^{(d+1)^{2}}$ that generates deformations preserving the structure of the time series graph. This goal motivates us to clarify, in Theorem 1, the specific representation of diffeomorphisms we require before presenting a class of kernels that produce deformations with this representation. ", "page_idx": 4}, {"type": "text", "text": "Similarly, selecting a loss function on sets $\\mathcal{L}$ that considers the temporal evolution in a time series\u2019 graph is crucial for meaningful comparisons with time series data. Consequently, we introduce the oriented Varifold distance. ", "page_idx": 4}, {"type": "text", "text": "A representation separating space and time. We prove that two time series graphs can always be linked by a time transformation composed with a space transformation. Moreover, a time series graph transformed by this kind of transformation is always a time series graph. We define $\\Psi_{\\gamma}\\in$ $\\mathcal{D}(\\mathbb{R}^{d+1}):(t,x)\\in\\mathbb{R}^{d+1}\\rightarrow(\\gamma(t),x)$ for any $\\gamma\\in{\\mathcal{D}}(\\mathbb{R})$ and $\\Phi_{f}:(t,x)\\in\\mathbb{R}^{d+1}\\to(t,f(t,x))$ for any $f\\in\\mathrm{C}^{1}(\\mathbb R^{d+1},\\mathbb R^{d})$ . We have the following representation theorem. All proofs are given in Appendix B. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "Denote by ${\\sf G}(s)\\triangleq\\{(t,s(t)):\\;t\\in1\\}$ the graph of a time series $s:\\mathsf{I}\\to\\mathbb{R}^{d}$ and $\\phi.\\mathsf{G}(s)\\triangleq\\{\\phi(t,s(t)):$ $t\\in|\\}$ the action of $\\phi\\in{\\mathcal{D}}(\\mathbb{R}^{d+1})$ on ${\\sf G}(s)$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 1. Let $s:\\,\\mathsf{J}\\,\\to\\,\\mathbb{R}^{d}$ and $\\mathbf{s}_{0}:\\mathsf{I}\\to\\mathbb{R}^{d}$ be two continuously differentiable time seriess with I, J two intervals of $\\mathbb{R}$ . There exist $f\\in\\mathrm{C}^{1}(\\mathbb R^{d+1},\\mathbb R^{d})$ and $\\gamma\\in{\\mathcal{D}}(\\mathbb{R})$ such that $\\gamma(\\mathsf{l})=\\mathsf{J}$ and $\\Phi_{f}\\in{\\cal D}(\\mathbb{R}^{d+1})$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathsf{G}(s)=\\Pi_{\\gamma,f}.\\mathsf{G}(\\mathbf{s}_{0}),\\;\\Pi_{\\gamma,f}=\\Psi_{\\gamma}\\circ\\Phi_{f}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Moreover, for any $\\bar{f}\\in\\mathrm{C}^{1}(\\mathbb R^{d+1},\\mathbb R^{d})$ and $\\bar{\\gamma}\\in\\mathcal{D}(\\mathbb{R})$ , there exists a continously differentiable time series s\u00af such that $\\mathsf{G}(\\bar{s})=\\Pi_{\\bar{\\gamma},\\bar{f}}.\\mathsf{G}(\\mathbf{s}_{0})$ ", "page_idx": 5}, {"type": "text", "text": "Remark 2. Note that for any $\\gamma\\in{\\mathcal{D}}(\\mathbb{R})$ and $s\\in\\mathrm{C}^{0}(1,\\mathbb{R}^{d})$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\{(\\gamma(t),s(t)),\\ t\\in1\\}=\\{(t,s\\circ\\gamma^{-1}(t)):\\ t\\in\\gamma(1)\\}\\ .\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "As a result, $\\Psi_{\\gamma}$ can be understood as a temporal reparametrization and $\\Phi_{f}$ encodes the transformation about the space. ", "page_idx": 5}, {"type": "text", "text": "Choice for the kernel associated with the RKHS V As depicted on Figure 1-2, we can not use any kernel $K$ to apply the previous methodology to learn deformations on time series\u2019 graphs. We describe and motivate our choice in this paragraph. Denote the one-dimensional Gaussian kernel by $K_{\\sigma}^{(a)}(x,y)=\\exp(-|x-y|^{2}/\\sigma)$ for any $(x,y)\\in(\\mathbb{R}^{a})^{2}$ , $a\\in\\mathbb N$ and $\\sigma>0$ . To solve the geodesic shooting problem (6) on $\\mathbb{R}^{d+1}$ , we consider for $\\vee$ the RKHS associated with the kernel defined for any $(t,\\tilde{x),(t^{\\prime},x^{\\prime})\\in(\\mathbb{R}^{d+1})^{2}}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{K_{\\mathsf{G}}((t,x),(t^{\\prime},x^{\\prime}))=\\left(\\!\\!\\begin{array}{c c}{c_{0}K_{\\mathrm{time}}}&{0}\\\\ {0}&{c_{1}K_{\\mathrm{space}}\\!}\\end{array}\\!\\!\\right)\\;,}\\\\ &{K_{\\mathrm{space}}=K_{\\sigma_{T,1}}^{(1)}(t,t^{\\prime})K_{\\sigma_{x}}^{(d)}(x,x^{\\prime})\\mathrm{I}_{d}\\;,K_{\\mathrm{time}}=K_{\\sigma_{T,0}}^{(1)}(t,t^{\\prime})\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "parametrized by the widths $\\sigma_{T,0},\\sigma_{T,1},\\sigma_{x}>0$ and the constants $c_{0},c_{1}>0$ . This choice for $K_{\\mathsf{G}}$ is motivated by the representation Theorem 1 and the following result. ", "page_idx": 5}, {"type": "text", "text": "Lemma 1. If we denote by $\\vee$ the RKHS associated with the kernel $K_{G}$ , then for any vector field $v$ generated by (5) with $v_{0}$ satisfying (4), there exist $\\gamma\\in\\mathsf{D}(\\mathbb{R})$ and $f\\,\\in\\,\\mathrm{C}^{1}(\\dot{\\mathbb R}^{d+1},\\dot{\\mathbb R}^{d})$ such that $\\phi^{v}=\\Psi_{\\gamma}\\circ\\Phi_{f}$ . ", "page_idx": 5}, {"type": "text", "text": "Instead of Gaussian kernels, other types of smooth kernels can be selected as long as the structure (9) is respected. ", "page_idx": 5}, {"type": "text", "text": "Remark 3. With this choice of kernel, the features associated with the time transformation can be extracted from the momentums $(\\alpha_{k,0})_{k\\in[{\\bf n}_{0}]}\\in(\\mathbb{R}^{d+1})^{{\\bf n}_{0}}$ in (4) by taking the coordinates related to time. However, the features related to the space transformation are not only in the space coordinates since the related kernel $K_{s p a c e}$ depends on time as well.The kernel\u2019s representation has been carefully designed to integrate both space and time, while ensuring that time remains independent of space. Initially, we considered separating the spatial and temporal components. However, post-hoc analysis of such a representation proved to be challenging. The separated spatial and temporal representations are correlated, and understanding this correlation is essential for interpreting the data. As a result, concatenating the two representations becomes necessary, though there is no straightforward method for doing so, as they are not commensurable. Consequently, we opted for a representation that inherently integrates both space and time. ", "page_idx": 5}, {"type": "text", "text": "In Appendix D, we give guidelines for selecting the hyperparameters $(\\sigma_{T,0},\\sigma_{T,1},\\sigma_{x},c_{0},c_{1})$ . ", "page_idx": 5}, {"type": "text", "text": "Loss This section specifies the distance function $\\mathcal{L}$ introduced in the loss function defined in (6). ", "page_idx": 5}, {"type": "text", "text": "In practice, we can only access discretized graphs of time series, $(t_{i}^{j},\\tilde{s}_{i}^{j})_{i\\in[n_{j}]}$ for any $j\\in[N]$ , that are potentially of different sizes $n_{j}$ and sampled at different timestamps $(t_{i}^{j})_{i\\in[n_{j}]}$ for any $j\\in[N]$ . Usual metrics, such as the Euclidean distance, are not appealing as they make the underlying assumptions of equal size sets and the existence of a pairing between points. Distances between measures on sets (taking the empirical distribution), such as Maximum Mean Discaprency (MMD) [18, 9], alleviate those issues; however, MMD only accounts for positional information and lacks information about the time evolution between sampled points. A classical data fidelity metric from shape analysis corresponding to the distance between oriented varifolds associated with curves alleviates this last issue [30]. Intuitively, an oriented varifold is a measure that accounts for positional and tangential information about the underlying curves at sample points. More details and information about oriented varifolds can be found in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "More precisely, given two sets ${\\sf G}_{0}\\,=\\,(g_{i}^{0})_{i\\in[T_{0}]}$ , $\\mathsf{G}_{1}\\,=\\,(g_{i}^{1})_{i\\in[T_{1}]}\\,\\,\\in\\,(\\mathbb{R}^{d+1})^{T_{1}}$ and a kernel3 $k$ : $(\\mathbb{R}^{d+1}\\times\\mathbb{S}^{d})^{2}\\rightarrow\\mathbb{R}$ verifying [30, Proposition 2 & 4], for any $\\xi\\in\\{0,1\\}$ and $i\\in[T_{\\xi}-1]$ , denoting the center and length of the $i^{t h}$ segment $[g_{i}^{\\xi},g_{i+1}^{\\xi}]$ by $c_{i}^{\\xi}=(g_{i}^{\\xi}+g_{i+1}^{\\xi})/2$ , $l_{i}^{\\xi}=\\|g_{i+1}^{\\xi}-g_{i}^{\\xi}\\|$ , and $\\overrightarrow{v_{i}}^{\\xi}=(g_{i+1}^{\\xi}-g_{i}^{\\xi})/l_{i}^{\\xi}$ , the varifold distance between $\\mathsf{G}_{0}$ and $\\mathsf{G}_{1}$ is defined as, ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{d_{\\mathbb{W}^{*}}^{2}(\\mathsf{G}_{0},\\mathsf{G}_{1})=\\displaystyle\\sum_{i,j=1}^{T_{0}-1}l_{i}^{0}k((c_{i}^{0},\\overrightarrow{v_{i}^{\\flat}}),(c_{j}^{0},\\overrightarrow{v_{j}^{\\flat}}))l_{j}^{0}-2\\displaystyle\\sum_{i=1}^{T_{0}-1}\\sum_{j=1}^{T_{1}-1}l_{i}^{0}k((c_{i}^{0},\\overrightarrow{v_{i}^{\\flat}}),(c_{j}^{1},\\overrightarrow{v_{j}^{\\flat}}))l_{j}^{1}}}\\\\ {{\\displaystyle+\\sum_{i,j=1}^{T_{1}-1}l_{i}^{1}k((c_{i}^{1},\\overrightarrow{v_{i}^{\\flat}}),(c_{j}^{1},\\overrightarrow{v_{j}^{\\flat}}))l_{j}^{1}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "In practice, we set the kernel $k$ as the product of two anisotropic Gaussian kernels, $k_{\\mathrm{pos}}$ and $k_{\\mathrm{dir}}$ , such that for any ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{1},(y,\\overrightarrow{\\boldsymbol{v}})\\in(\\mathbb{R}^{d+1}\\times\\mathbb{S}^{d})^{2}}\\\\ &{\\quad\\,\\,k((x,\\overrightarrow{\\boldsymbol{u}}),(y,\\overrightarrow{\\boldsymbol{v}}))=k_{\\mathrm{pos}}(x,y)k_{\\mathrm{dir}}(\\overrightarrow{\\boldsymbol{u}},\\overrightarrow{\\boldsymbol{v}})\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Note that the loss kernel $k$ has nothing to do with the velocity field kernel denoted by $K_{G}$ or $K$ specified in Section 4.1. Finally, we define the data fidelity loss function, $\\mathcal{L}$ , as a sum of $d_{\\mathsf{W}^{*}}^{2}$ using different kernel\u2019s width parameters $\\sigma$ to incorporate multiscale information. $\\mathcal{L}$ is indeed differentiable with respect to its first variable. The specific kernels $k_{\\mathrm{pos}},k_{\\mathrm{dir}}$ that we use in our experiments are given Appendix C.1. For further readings on curves and surface representation as varifolds, readers can refer to [30, 12]. ", "page_idx": 6}, {"type": "text", "text": "A pedagogical online application is available to inspect the effect of hyperprameters on geodesic shooting (5) and registration (6). ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The source code is available on Github4. For conciseness, several experiments are relegated in appendix: ", "page_idx": 6}, {"type": "text", "text": "1. TS-LDDMM representation identifiability, Appendix G: On synthetic data, we evaluate the ability of our method to retrieve the parameter $v_{0}^{*}$ that encodes the deformation $\\varphi^{\\{v_{0}^{*}\\}}$ acting on a time series graph G by solving the geodesic shooting problem (6) between G and $\\bar{\\varphi}^{\\{v_{0}^{*}\\}}$ .G. Results show that TS-LDDMM representations are identifiable or weakly identifiable depending on the velocity field kernel $K_{G}$ specification. ", "page_idx": 6}, {"type": "text", "text": "2. Robustness to irregular sampling, Appendix H: We compare the robustness of TSLDDMM representation with 9 URL methods handling irregularly sampled multivariate time series on 15 shape-based datasets (7 univariates & 8 multivariates). We assess methods\u2019 classification performances under regular sampling ( $0\\%$ missing rate) and three irregular sampling regimes ( $30\\%$ , $50\\%$ , and $70\\%$ missing rates), according to the protocol depicted in [31]. Results show that our method, TS-LDDMM, outperforms all methods for sampling regimes with missing rates: $0\\%$ , $30\\%$ , and $50\\%$ . ", "page_idx": 6}, {"type": "text", "text": "3. Classification benchmark on regularly sampled datasets, Appendix I: We compare performances of a kernel support vector machine (SVC) algorithm based on TS-LDDMM representation with 3 state-of-the-art classification methods from shape analysis on 15 shape-based datasets (7 univariates & 8 multivariates). Results show that the TS-LDDMMbased method outperforms other methods (best performances over 13 datasets), making TS-LDDMM representation relevant for time series shape analysis. ", "page_idx": 6}, {"type": "table", "img_path": "JM0IQSliol/tmp/fdfa69d4778cb1a647c1ac8e0175122009aaf504db6f05fbbc25d06b262a09f8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 3: Analysis of the two principal components (PC) related to mice ventilation before exposure with TS-LDDMM representations (a), and LDDMM (b). In both cases and for all PC, the left plot displays PC densities according to mice genotype and right plot displays deformations of the reference graph ${\\bf c}_{0}$ along each PC. ", "page_idx": 7}, {"type": "image", "img_path": "JM0IQSliol/tmp/7276fb7798ba28e926ea1a8e7b97f02b2ec6ce82f80b9a37bad2979789cafeee.jpg", "img_caption": ["(a) ColQ cycle (b) PC1 vs PC2 (c) WT cycle "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 4: (a) a ColQ respiratory cycle sample. (b) Referent respiratory cycle of individual mouse $\\mathbf{c}_{0}^{j}$ in the TS-LDDMM PC1-PC2 coordinates system of $\\mathbf{c}_{\\mathrm{0}}$ . (c) a WT respiratory cycle sample. ", "page_idx": 7}, {"type": "text", "text": "4. Noise sensitivity for learning the reference graph, Appendix J: We evaluate the noise sensitivity of TS-LDDMM and Shape-FPCA [60] for learning the reference graph on a synthetic dataset and for several levels of additive Gaussian noise. Results show that both methods are sensitive to noise. However, TS-LDDMM preserves the overall shape while shape-FPCA alters the shape depending on the noise level. ", "page_idx": 7}, {"type": "text", "text": "5.1 Interpretability: mice ventilation analysis ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "This experiment highlights the interpretability of TS-LDDMM representation for studying the interindividual variability in biomedical applications. We consider a time series dataset monitoring the evolution of mice\u2019s nasal and thoracic airflow when exposed to a drug altering respiration [41]. The dataset includes recordings of 7 control mice (WT) and 7 mutant mice (ColQ) with an enzyme deficiency. The enzyme is involved in the respiration regulation, and the drug inhibits its activity. For each mouse, airflows were monitored for 15 to 20 minutes before the drug exposure and then for 35 to 40 minutes. A complete description of the dataset is given in the Appendix F.1. ", "page_idx": 7}, {"type": "text", "text": "Experimental protocol. We considered two experimental scenarios; the first focuses on mice ventilation before exposure to explore the inter-individual and genotype-specific variabilities. The second focuses on whole recordings to analyze the evolution of mice\u2019s ventilation after drug exposure. In both cases, the baseline protocol consists of first extracting $N$ respiratory cycles from the datasets with the procedure described in [21]. Then, learning the referent respiratory cycle $\\mathbf{c}_{0}$ and the representations of respiratory cycles $(\\pmb{\\alpha}_{0}^{j})_{j\\in[N]}$ by solving (8) using TS-LDDMM. $\\alpha_{0}^{j}$ being the momentum of the initial velocity field of the geodesic encodings the diffeomorphisms mapping $\\mathbf{c}_{0}$ to the $j^{t h}$ respiratory cycle. Finally, performing a Kernel-PCA on the initial velocity fields (4) belonging to $\\vee$ and encoded by the pairs $(\\alpha_{0}^{j},\\mathbf{c}_{0})_{j\\in[N]}^{-}$ . The first experiment includes $N_{1}=700$ cycles collected before exposure. The second experiment includes $N_{2}=1400$ cycles with $25\\%$ (resp. $75\\%)$ ) before (resp. after) exposure. We also performed the first experimental scenario with LDDMM representation, and Appendix K describes the settings of both methods. Essentially, varifold losses are identical for both methods, and the velocity field kernels are set to encompass time and space scales. in addition, In addition, Appendix K presents a comparison between TS-LDDMM and Shape-FPCA on the second scenario. ", "page_idx": 7}, {"type": "text", "text": "Geodesic shooting along principal component directions. Any principal component (PC), noted $v_{0}^{p c}$ , from a kernel-PCA in V, is itself an initial velocity field encoded by a pair $(\\mathbf{c}_{0},\\bar{\\mathbf{\\alpha}}_{0}^{p c})$ . PCs encode the principal axis of deformations, and it is possible to shoot along the geodesic they encode with the differential equations (5), enabling interpretation of the main sources of deformations. ", "page_idx": 7}, {"type": "image", "img_path": "JM0IQSliol/tmp/b1cd66fe8c9e2f79fe9aa55a49d77d22e655ccfe2eed21530dda362decbda75e.jpg", "img_caption": ["Figure 5: Analysis of the first Principal Component (PC1) related to mice ventilation before and after exposure with TS-LDDMM representations. (a) displays PC densities per mice genotype, (b) illustrates deformations of the reference respiratory cycle ${\\bf c}_{0}$ along PC1, and (c) displays all respiratory cycles with respect to time in PC1 and PC3 coordinates "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Mice ventilation before exposure. We focus on the analysis of the two first Principal Components (PC) for TS-LDDMM (Figure 3a) and LDDMM (Figure 3b). Looking at the geodesic shooting along PCs, Figure 3 shows that principal components learned with TS-LDMM lead to deformations that remain respiratory cycles. In contrast, deformations learned with LDDMM are challenging to interpret as respiratory cycles. The LDDMM velocity field kernel is a Gaussian anisotropic kernel that accounts for time and space scales; however, the entanglement of time and space dimensions in the kernel does not guarantee the graph structure, and it makes the convergence of the method complex (relative varifold loss error: TS-LDDMM: 0.06, LDDMM: 0.11). ", "page_idx": 8}, {"type": "text", "text": "Regarding TS-LDDMM Figure 3a, its PCs refer to deformations directions carrying different physiological meanings. Indeed, the geodesic shooting along these directions indicates that PC1 accounts for variations of the total duration of a respiratory cycle, while PC2 expresses the trade-off between inspiration and expiration duration. In addition, the distribution of ColQ respiratory cycles along PC1 is wider than in WT mice, indicating that the adaptation of mutant mice to their enzyme deficiency is variable. This observation can also be seen in Figure 4b where a referent respiratory cycle $\\mathbf{c}_{0}^{j}$ is learned by solving (8) for each mouse and is encoded in the (PC1,PC2) coordinate system of ${\\bf c}_{0}$ by registration (3). Indeed, the average respiratory cycles of ColQ mice are more spread out than WT mice\u2019s. Going back to the densities of PC1, ColQ mice distribution has a heavier tail toward negative values compared to WT mice. When shooting in the opposite direction of PC1, we can observe that the inspiration is divided into two steps. Congruently with [21], such inspirations indicate motor control difficulties due to enzyme deficiency. Figure 4a is an example of ColQ respiratory cycle with negative PC1 coordinate. ", "page_idx": 8}, {"type": "text", "text": "Mice ventilation evolution after drug exposure. This experiment focuses on the first principal components learned from TS-LDDDM representations of respiratory cycles randomly sampled before and after drug exposure. Figure 5a illustrates the geodesic shootings along PC1. Again, PC1 accounts for variations in respiratory cycle duration, but more importantly, it can be observed on the deformation at $-1.5\\ \\sigma_{\\mathrm{PC}}$ the apparition of a long pause after inspiration. Congruently, Figure 5c indicates that pauses appear after drug exposure as cycles with negative PC1 values mainly occur after 20 minutes and present more variability along PC3. In addition, Figure 5b shows a bimodal distribution for WT mice with one of the peaks in the negative values. This peak was not observed in the previous experiment Figure 3a. It indicates that pauses after inspiration are prevalent in WT mice after drug exposure. On the other hand, the distributions of ColQ mice\u2019s respiratory cycles along PC1 in both experiments are similar and account for the same deformation, suggesting that ColQ mice weakly react to the drug exposure as they already adapt their enzyme deficiency. ", "page_idx": 8}, {"type": "text", "text": "Experiment Conclusion. Analyzing mice ventilation with TS-LDDMM representation highlights the method\u2019s ability to create meaningful interaction between experts and the data. Indeed, combining statistical and visual results shows that main deformations carry physiological meaning, enabling the characterization of some mice genotypes and the effects of drug exposure. ", "page_idx": 8}, {"type": "text", "text": "6 Related Works ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Shape analysis focuses on statistical analysis of mathematical objects invariant under some deformations like rotations, dilations, or time parameterization. The main idea is to represent these objects in a complete Riemannian manifold $\\left(\\mathcal{M},\\mathbf{g}\\right)$ with a metric $\\mathbf{g}$ adapted to the geometry of the problem [40]. Then, any set of points in $\\mathcal{M}$ can be represented as points in the tangent space of their Frechet mean $\\mathbf{m}_{0}$ [44, 33] by considering their logarithms. The goal is to find a well-suited Riemannian structure according to the nature of the studied object. ", "page_idx": 8}, {"type": "text", "text": "LDDMM framework is a relevant shape analysis tool to represent curves as depicted in [23]. However, graphs of time series are a well-structured type of curve due to the inclusion of the temporal dimension that requires specific care (Figure 1). In a similar vein, Qiu et al [46] proposes a method for tracking anatomical shape changes in serial images using LDDMM. They include temporal evolution, but not for the same purpose: the aim is to perform longitudinal modeling of brain images. ", "page_idx": 9}, {"type": "text", "text": "Leaving the LDDMM representation, the results of [53, 26] address the representation of curves with the Square-Root Velocity (SRV) representation. However, the SRV representation is applied after reparametrization of the temporal dimension of the unit length segment. Consequently, the graph structure of the time series is not respected, and the original time evolution of the time series is not encoded in the final representation. Very recently, in a functional data analysis (FDA) framework, a paper [60] (Shape-FPCA) improved by representing the original time evolution. However, the space and time representations remain correlated, complicating post-hoc analysis, as discussed in Remark 3. Additionally, this method is tailored for continuous objects and applies only to time series of the same length, making the estimation more sensitive to noise. This issue can be addressed through interpolation, but this approach is not always reliable in sparse and irregular sampling scenarios. Most FDA approaches, as seen in [50, 63, 59], address this challenge using interpolation or basis function expansion. In summary, FDA methods typically separate space and time representations for continuous objects, whereas TS-LDDMM algorithm maintain a discrete-to-discrete analysis, inherently integrating both space and time representations. ", "page_idx": 9}, {"type": "text", "text": "Balancing between discrete and continuous elements is a challenging task. In the deep learning literature [13, 31, 56, 29, 36, 1], Neural Ordinary Differential Equations (Neural ODEs) [13] learn continuous latent representations using a vector field parameterized by a neural network, serving as a continuous analog to Residual Networks [64]. This approach was further enhanced by Neural Controlled Differential Equations (Neural CDEs) [31] for handling irregular time series, functioning as continuous-time analogs of RNNs [49]. Extending Neural ODEs, Neural Stochastic Differential Equations (Neural SDEs) introduce regularization effects [36], although optimization remains challenging. Leveraging techniques from continuous-discrete filtering theory, Ansari et al. [1] applied successfully Neural SDEs to irregular time series. Oh et al. [43] improved these results by incorporating the concept of controlled paths into the drift term, similar to how Neural CDEs outperform Neural ODEs. With TS-LDDMM, the representation is also derived from an ODE, but the velocity field is parameterized with kernels and optimized to have a minimal norm, which enhances interpretability. ", "page_idx": 9}, {"type": "text", "text": "All these state-of-the-art methods previously mentionned [23, 43, 60, 26] are compared to TSLDDMM in Appendix H and Appendix I. ", "page_idx": 9}, {"type": "text", "text": "Compared to the Metamorphosis framework [7], LDDMM framework has weaker assumptions. The 3DMM framework requires that each mesh be re-parametrized into a consistent form where the number of vertices, triangulation, and the anatomical meaning of each vertex are consistent across all meshes, as stated in the introduction of [8]. In our context, we do not need such pre-processing; the time series graph can have different sizes. ", "page_idx": 9}, {"type": "text", "text": "7 Limitations and conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper proposes a feature representation method, TS-LDDMM, designed for shape comparison on homogeneous time series datasets. We show on a real dataset its ability to study, with high interpretability, the inter-individual shape variability. As an unsupervised approach, it is user-friendly and enables knowledge transfer for different supervised tasks such as classification. ", "page_idx": 9}, {"type": "text", "text": "Although TS-LDDMM is already competitive for classification, its performances can be leveraged on more heterogeneous datasets using a hierarchical clustering extension, which is relegated for future work. ", "page_idx": 9}, {"type": "text", "text": "TS-LDDMM employs kernel computations, which require specific libraries (e.g., KeOps [11]) to be efficient and scalable. However, in our experiments, the time complexity of TS-LDDMM is comparable to that of competitors. It is clear that TS-LDDMM needs to be extended to handle very large datasets with high-dimensional time series (such as videos). ", "page_idx": 9}, {"type": "text", "text": "Additionally, TS-LDDMM requires tuning several hyperparameters, though this is a common requirement among competitors [23, 43, 60, 26]. In future work, adaptive methods are expected to be developed to provide a more user-friendly interface. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported by grants from R\u00e9gion Ile-de-France (DIM MathInnov). Charles Truong is funded by the PhLAMES chair of ENS Paris-Saclay. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Abdul Fatir Ansari, Alvin Heng, Andre Lim, and Harold Soh. Neural continuous-discrete state space models for irregularly-sampled time series. In International Conference on Machine Learning, pages 926\u2013951. PMLR, 2023. [2] Asal Asgari. Clustering of clinical multivariate time-series utilizing recent advances in machinelearning. 2023. [3] Anthony Bagnall, Hoang Anh Dau, Jason Lines, Michael Flynn, James Large, Aaron Bostrom, Paul Southam, and Eamonn Keogh. The uea multivariate time series classification archive, 2018. arXiv preprint arXiv:1811.00075, 2018. [4] Ziv Bar-Joseph, Anthony Gitter, and Itamar Simon. Studying and modelling dynamic biological processes using time-series gene expression data. Nature Reviews Genetics, 13(8):552\u2013564, 2012.   \n[5] M Faisal Beg, Michael I Miller, Alain Trouv\u00e9, and Laurent Younes. Computing large deformation metric mappings via geodesic flows of diffeomorphisms. International journal of computer vision, 61:139\u2013157, 2005. [6] Alain Berlinet and Christine Thomas-Agnan. Reproducing kernel Hilbert spaces in probability and statistics. Springer Science & Business Media, 2011. [7] Volker Blanz and Thomas Vetter. Face recognition based on ftiting a 3d morphable model. IEEE Transactions on pattern analysis and machine intelligence, 25(9):1063\u20131074, 2003. [8] James Booth, Anastasios Roussos, Stefanos Zafeiriou, Allan Ponniah, and David Dunaway. A 3d morphable model learnt from 10,000 faces. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5543\u20135552, 2016.   \n[9] Karsten M Borgwardt, Arthur Gretton, Malte J Rasch, Hans-Peter Kriegel, Bernhard Sch\u00f6lkopf, and Alex J Smola. Integrating structured biological data by kernel maximum mean discrepancy. Bioinformatics, 22(14):e49\u2013e57, 2006.   \n[10] Claudio Carmeli, Ernesto De Vito, Alessandro Toigo, and Veronica Umanit\u00e1. Vector valued reproducing kernel hilbert spaces and universality. Analysis and Applications, 8(01):19\u201361, 2010.   \n[11] Benjamin Charlier, Jean Feydy, Joan Alexis Glaunes, Fran\u00e7ois-David Collin, and Ghislain Durif. Kernel operations on the gpu, with autodiff, without memory overflows. Journal of Machine Learning Research, 22(74):1\u20136, 2021.   \n[12] Nicolas Charon and Alain Trouv\u00e9. The varifold representation of nonoriented shapes for diffeomorphic registration. SIAM journal on Imaging Sciences, 6(4):2547\u20132580, 2013.   \n[13] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. Advances in neural information processing systems, 31, 2018.   \n[14] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.   \n[15] Hoang Anh Dau, Anthony Bagnall, Kaveh Kamgar, Chin-Chia Michael Yeh, Yan Zhu, Shaghayegh Gharghabi, Chotirat Ann Ratanamahatana, and Eamonn Keogh. The ucr time series archive. IEEE/CAA Journal of Automatica Sinica, 6(6):1293\u20131305, 2019.   \n[16] Ruizhi Deng, Bo Chang, Marcus A Brubaker, Greg Mori, and Andreas Lehrmann. Modeling continuous stochastic processes with dynamic normalizing flows. Advances in Neural Information Processing Systems, 33:7805\u20137815, 2020.   \n[17] Stanley Durrleman, St\u00e9phanie Allassonni\u00e8re, and Sarang Joshi. Sparse adaptive parameterization of variability in image ensembles. International Journal of Computer Vision, 101:161\u2013183, 2013.   \n[18] Gintare Karolina Dziugaite, Daniel M Roy, and Zoubin Ghahramani. Training generative neural networks via maximum mean discrepancy optimization. arXiv preprint arXiv:1505.03906, 2015.   \n[19] Shibo Feng, Chunyan Miao, Ke Xu, Jiaxiang Wu, Pengcheng Wu, Yang Zhang, and Peilin Zhao. Multi-scale attention flow for probabilistic time series forecasting. IEEE Transactions on Knowledge and Data Engineering, 2023.   \n[20] Jean-Yves Franceschi, Aymeric Dieuleveut, and Martin Jaggi. Unsupervised scalable representation learning for multivariate time series. Advances in neural information processing systems, 32, 2019.   \n[21] Thibaut Germain, Charles Truong, Laurent Oudre, and Eric Krejci. Unsupervised classification of plethysmography signals with advanced visual representations. Frontiers in Physiology, 14:781, 2023.   \n[22] Joan Glaunes. Transport par diff\u00e9omorphismes de points, de mesures et de courants pour la comparaison de formes et l\u2019anatomie num\u00e9rique. These de sciences, Universit\u00e9 Paris, 13, 2005.   \n[23] Joan Glaunes, Anqi Qiu, Michael I Miller, and Laurent Younes. Large deformation diffeomorphic metric curve mapping. International journal of computer vision, 80:317\u2013336, 2008.   \n[24] Will Grathwohl, Ricky TQ Chen, Jesse Bettencourt, and David Duvenaud. Scalable reversible generative models with free-form continuous dynamics. In International Conference on Learning Representations, page 7, 2019.   \n[25] Ella Guscelli, John I Spicer, and Piero Calosi. The importance of inter-individual variation in predicting species\u2019 responses to global change drivers. Ecology and Evolution, 9(8):4327\u20134339, 2019.   \n[26] Tae-Young Heo, Joon Myoung Lee, Myung Hun Woo, Hyeongseok Lee, and Min Ho Cho. Logistic regression models for elastic shape of curves based on tangent representations. Journal of the Korean Statistical Society, pages 1\u201319, 2024.   \n[27] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997.   \n[28] Heinz Gerd Hoymann. Lung function measurements in rodents in safety pharmacology studies. Frontiers in pharmacology, 3:156, 2012.   \n[29] Junteng Jia and Austin R Benson. Neural jump stochastic differential equations. Advances in Neural Information Processing Systems, 32, 2019.   \n[30] Irene Kaltenmark, Benjamin Charlier, and Nicolas Charon. A general framework for curve and surface comparison and registration with oriented varifolds. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3346\u20133355, 2017.   \n[31] Patrick Kidger, James Morrill, James Foster, and Terry Lyons. Neural controlled differential equations for irregular time series. Advances in Neural Information Processing Systems, 33:6696\u20136707, 2020.   \n[32] Ivan Kobyzev, Simon JD Prince, and Marcus A Brubaker. Normalizing flows: An introduction and review of current methods. IEEE transactions on pattern analysis and machine intelligence, 43(11):3964\u20133979, 2020.   \n[33] Huiling Le. Locating fr\u00e9chet means with application to shape spaces. Advances in Applied Probability, 33(2):324\u2013338, 2001.   \n[34] Mathias Lechner and Ramin Hasani. Learning long-term dependencies in irregularly-sampled time series. arXiv preprint arXiv:2006.04418, 2020.   \n[35] Yurim Lee, Eunji Jun, Jaehun Choi, and Heung-Il Suk. Multi-view integrative attention-based deep representation learning for irregular clinical time-series data. IEEE Journal of Biomedical and Health Informatics, 26(8):4270\u20134280, 2022.   \n[36] Xuanqing Liu, Tesi Xiao, Si Si, Qin Cao, Sanjiv Kumar, and Cho-Jui Hsieh. Neural sde: Stabilizing neural ode networks with stochastic noise. arXiv preprint arXiv:1906.02355, 2019.   \n[37] Putri Madona, Rahmat Ilias Basti, and Muhammad Mahrus Zain. Pqrst wave detection on ecg signals. Gaceta Sanitaria, 35:S364\u2013S369, 2021.   \n[38] Larry Medsker and Lakhmi C Jain. Recurrent neural networks: design and applications. CRC press, 1999.   \n[39] Qianwen Meng, Hangwei Qian, Yong Liu, Yonghui Xu, Zhiqi Shen, and Lizhen Cui. Unsupervised representation learning for time series: A review. arXiv preprint arXiv:2308.01578, 2023.   \n[40] Michael I Miller, Alain Trouv\u00e9, and Laurent Younes. Geodesic shooting for computational anatomy. Journal of mathematical imaging and vision, 24:209\u2013228, 2006.   \n[41] Aur\u00e9lie Nervo, Andr\u00e9-Guilhem Calas, Florian Nachon, and Eric Krejci. Respiratory failure triggered by cholinesterase inhibitors may involve activation of a reflex sensory pathway by acetylcholine spillover. Toxicology, 424:152232, 2019.   \n[42] Vit Niennattrakul and Chotirat Ann Ratanamahatana. Inaccuracies of shape averaging method using dynamic time warping for time series data. In Computational Science\u2013ICCS 2007: 7th International Conference, Beijing, China, May 27-30, 2007, Proceedings, Part I 7, pages 513\u2013520. Springer, 2007.   \n[43] YongKyung Oh, Dongyoung Lim, and Sungil Kim. Stable neural stochastic differential equations in analyzing irregular time series data. In The Twelfth International Conference on Learning Representations, 2024.   \n[44] Susovan Pal, Roger P Woods, Suchit Panjiyar, Elizabeth Sowell, Katherine L Narr, and Shantanu H Joshi. A riemannian framework for linear and quadratic discriminant analysis on the tangent space of shapes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 47\u201355, 2017.   \n[45] John Paparrizos and Luis Gravano. k-shape: Efficient and accurate clustering of time series. In Proceedings of the 2015 ACM SIGMOD international conference on management of data, pages 1855\u20131870, 2015.   \n[46] Anqi Qiu, Marilyn Albert, Laurent Younes, and Michael I Miller. Time sequence diffeomorphic metric mapping and parallel transport track time-dependent shape changes. NeuroImage, 45(1):S51\u2013S60, 2009.   \n[47] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International conference on machine learning, pages 1530\u20131538. PMLR, 2015.   \n[48] Hadi Salman, Payman Yadollahpour, Tom Fletcher, and Kayhan Batmanghelich. Deep diffeomorphic normalizing flows. arXiv preprint arXiv:1810.03256, 2018.   \n[49] Mike Schuster and Kuldip K Paliwal. Bidirectional recurrent neural networks. IEEE transactions on Signal Processing, 45(11):2673\u20132681, 1997.   \n[50] Han Lin Shang. A survey of functional principal component analysis. AStA Advances in Statistical Analysis, 98:121\u2013142, 2014.   \n[51] Gota Shirato, Natalia Andrienko, and Gennady Andrienko. Identifying, exploring, and interpreting time series shapes in multivariate time intervals. Visual Informatics, 7(1):77\u201391, 2023.   \n[52] Satya Narayan Shukla and Benjamin M Marlin. Multi-time attention networks for irregularly sampled time series. arXiv preprint arXiv:2101.10318, 2021.   \n[53] Anuj Srivastava, Eric Klassen, Shantanu H Joshi, and Ian H Jermyn. Shape analysis of elastic curves in euclidean spaces. IEEE transactions on pattern analysis and machine intelligence, 33(7):1415\u20131428, 2010.   \n[54] Sana Tonekaboni, Danny Eytan, and Anna Goldenberg. Unsupervised representation learning for time series with temporal neighborhood coding. arXiv preprint arXiv:2106.00750, 2021.   \n[55] Patara Trirat, Yooju Shin, Junhyeok Kang, Youngeun Nam, Jihye Na, Minyoung Bae, Joeun Kim, Byunghyun Kim, and Jae-Gil Lee. Universal time-series representation learning: A survey. arXiv preprint arXiv:2401.03717, 2024.   \n[56] Belinda Tzen and Maxim Raginsky. Neural stochastic differential equations: Deep latent gaussian models in the diffusion limit. arXiv preprint arXiv:1905.09883, 2019.   \n[57] Marc Vaillant, Michael I Miller, Laurent Younes, and Alain Trouv\u00e9. Statistics on diffeomorphisms via tangent space representations. NeuroImage, 23:S161\u2013S169, 2004.   \n[58] Kai Wang, Youjin Zhao, Qingyu Xiong, Min Fan, Guotan Sun, Longkun Ma, Tong Liu, et al. Research on healthy anomaly detection model based on deep learning from multiple time-series physiological signals. Scientific Programming, 2016, 2016.   \n[59] John Warmenhoven, Norma Bargary, Dominik Liebl, Andrew Harrison, Mark A Robinson, Edward Gunning, and Giles Hooker. Pca of waveforms and functional pca: A primer for biomechanics. Journal of Biomechanics, 116:110106, 2021.   \n[60] Yuexuan Wu, Chao Huang, and Anuj Srivastava. Shape-based functional data analysis. TEST, 33(1):1\u201347, 2024.   \n[61] Can Ye, BVK Vijaya Kumar, and Miguel Tavares Coimbra. Heartbeat classification using morphological and dynamic features of ecg signals. IEEE Transactions on Biomedical Engineering, 59(10):2930\u20132941, 2012.   \n[62] Lexiang Ye and Eamonn Keogh. Time series shapelets: a new primitive for data mining. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 947\u2013956, 2009.   \n[63] Qunqun Yu, Xiaosun Lu, and JS Marron. Principal nested spheres for time-warped functional data analysis. Journal of Computational and Graphical Statistics, 26(1):144\u2013151, 2017.   \n[64] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.   \n[65] Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C Tatikonda, Nicha Dvornek, Xenophon Papademetris, and James Duncan. Adabelief optimizer: Adapting stepsizes by the belief in observed gradients. Advances in neural information processing systems, 33:18795\u201318806, 2020. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Societal impact ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We believe that the paper has a positive societal impact for the following reasons: ", "page_idx": 14}, {"type": "text", "text": "\u2022 TS-LDDMM is an interpretable method for understanding inter-individual variability in biomedical datasets, potentially offering new insights in medicine.   \n\u2022 TS-LDDMM bridges the gap between the shape analysis community and the unsupervised representation learning (URL) community, fostering potential future collaborations between these fields. ", "page_idx": 14}, {"type": "text", "text": "However, the computational cost of the method may raise environmental concerns similar to those associated with deep learning [43]. Additionally, while TS-LDDMM has promising biomedical applications, it could also be misused for creating poison. ", "page_idx": 14}, {"type": "text", "text": "B Proofs ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Denote by ${\\sf G}(s)\\triangleq\\{(t,s(t)):\\;t\\in1\\}$ the graph of a time series $s:\\mathsf{I}\\to\\mathbb{R}^{d}$ and $\\phi.\\mathsf{G}(s)\\triangleq\\{\\phi(t,s(t)):$ $t\\in|\\}$ the action of $\\phi\\in{\\mathcal{D}}(\\mathbb{R}^{d+1})$ on ${\\sf G}(s)$ . ", "page_idx": 14}, {"type": "text", "text": "Theorem 4. Let $s:\\,\\mathsf{J}\\,\\to\\,\\mathbb{R}^{d}$ and $\\mathbf{s}_{0}:\\mathsf{I}\\to\\mathbb{R}^{d}$ be two continuously differentiable time seriess with I, J two intervals of $\\mathbb{R}$ . There exist $f\\in\\mathrm{C}^{1}(\\mathbb R^{d+1},\\mathbb R^{d})$ and $\\gamma\\in{\\mathcal{D}}(\\mathbb{R})$ such that $\\gamma(\\mathsf{l})=\\mathsf{J}$ and $\\Phi_{f}\\in{\\cal D}(\\mathbb{R}^{d+1})$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathsf{G}(s)=\\Pi_{\\gamma,f}.\\mathsf{G}(\\mathbf{s}_{0}),\\;\\Pi_{\\gamma,f}=\\Psi_{\\gamma}\\circ\\Phi_{f}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Moreover, for any $\\bar{f}\\in\\mathrm{C}^{1}(\\mathbb R^{d+1},\\mathbb R^{d})$ and $\\bar{\\gamma}\\in\\mathcal{D}(\\mathbb{R})$ , there exists a continously differentiable time series s\u00af such that $\\mathsf{G}(\\bar{s})=\\Pi_{\\bar{\\gamma},\\bar{f}}.\\mathsf{G}(\\mathbf{s}_{0})$ ", "page_idx": 14}, {"type": "text", "text": "Proof. Let $s:\\ J\\,\\rightarrow\\,\\mathbb{R}^{d}$ and $\\mathbf{s}_{0}:\\mathsf{I}\\to\\mathbb{R}^{d}$ be two continuously differentiable time seriess with ${\\sf I}=(a,b),{\\sf J}=(\\alpha,\\beta)$ two intervals of $\\mathbb{R}$ . By setting $\\gamma:t\\in\\mathbb{R}\\mapsto(\\beta-\\alpha)(t-a)/(b-a)+\\alpha\\in\\mathbb{R}$ , we have $\\gamma(1)\\,=\\,\\mathsf{J}$ and $\\gamma\\,\\in\\,{\\mathcal{D}}(\\mathbb{R})$ . By defining $f:(t,x)\\,\\in\\,\\mathbb{R}^{d+1}\\,\\mapsto\\,x-{\\mathbf s}_{0}(t)+s\\,\\circ\\,\\gamma(t),$ the map $\\Phi_{f}\\,\\in\\,\\mathcal{D}(\\mathbb{R}^{d+1})$ , indeed, its inverse is $\\Phi_{f}^{-1}:(t,x)\\,\\in\\,\\mathbb{R}^{d+1}\\mapsto(t,x+\\mathbf{s}_{0}(t)-s(t))$ and is continuously differentiable. Moreover, we have $\\Pi_{\\gamma,f}.\\mathsf{G}(\\mathbf{s}_{0})=\\{(\\gamma(t),s\\circ\\gamma(t)):\\,t\\in1\\}=\\mathsf{G}(s)$ . ", "page_idx": 14}, {"type": "text", "text": "Let $\\bar{f}\\in\\mathrm{C}^{1}(\\mathbb R^{d+1},\\mathbb R^{d})$ , $\\bar{\\gamma}\\in\\mathcal{D}(\\mathbb{R})$ and $\\mathbf{s}_{0}\\in\\mathrm{C}^{1}(1,\\mathbb{R}^{d})$ with I an interval of $\\mathbb{R}$ . We have : ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Pi_{\\gamma,f}.\\mathsf{G}(\\mathbf{s}_{0})=\\{(\\gamma(t),f(t,\\mathbf{s}_{0}(t))),\\;t\\in1\\}}\\\\ &{\\qquad\\qquad=\\{(t,f\\left(\\gamma^{-1}(t),\\mathbf{s}_{0}(\\gamma^{-1}(t))\\right),\\;t\\in\\gamma(1)\\;\\}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "By defining $\\bar{s}:t\\in\\gamma(1)\\to f\\left(\\gamma^{-1}(t),\\mathbf{s}_{0}(\\gamma^{-1}(t))\\right)$ , we have $\\bar{s}\\in\\mathrm{C}^{1}(\\gamma(\\mathbb{I}),\\mathbb{R}^{d})$ by composition of $C^{1}$ functions and $\\mathsf{G}(\\bar{s})=\\Pi_{\\gamma,f}.\\mathsf{G}(\\mathbf{s}_{0})$ by (10), which concludes the proof. \u53e3 ", "page_idx": 14}, {"type": "text", "text": "Lemma 2. If we denote by $\\vee$ the RKHS associated with the kernel $K_{G}$ , then for any vector field $v$ generated by (5) with $v_{0}$ satisfying (4), there exist $\\gamma\\in\\mathsf{D}(\\mathbb{R})$ and $f\\,\\in\\,\\mathrm{C}^{1}(\\dot{\\mathbb R}^{d+1},\\dot{\\mathbb R}^{d})$ such that $\\phi^{v}=\\Psi_{\\gamma}\\circ\\Phi_{f}$ . ", "page_idx": 14}, {"type": "text", "text": "Proof. Let $v$ be a vector field generated by (5) with $v_{0}$ satisfying (4). We remark that the first coordinate of the velocity field $v_{\\tau}$ denoted by $v_{\\tau}^{\\mathrm{time}}$ only depends on the time variable $t$ for any $\\tau\\,\\in\\,[0,1]$ . Thus, when computing the first coordinate of the deformation $\\phi^{v}$ , denoted by $\\gamma$ , we integrate (1) with $v_{\\tau}$ replaced by $v_{\\tau}^{\\mathrm{time}}$ , thus $\\gamma$ is independant of the variable $x$ . Moreover, $\\gamma\\in{\\mathcal{D}}(\\mathbb{R})$ since a Gaussian kernel induced an Hilbert space $\\vee$ satisfying $|f|_{V}\\leq|f|_{\\infty}+|\\,\\mathrm{d}f|_{\\infty}$ for any $f\\in\\mathsf{V}$ by [22, Theorem 9]. For the same reason, we have $\\phi^{v}\\,\\in\\,\\mathcal{D}(\\mathbb{R}^{d+1})$ , and thus its last coordinates denoted by $f$ belongs to $\\mathrm{C}^{1}(\\mathbb{R}^{d+1},\\mathbb{R}^{d})$ , and by construction $\\phi^{v}=\\Psi_{\\gamma}\\circ\\Phi_{f}$ . \u53e3 ", "page_idx": 14}, {"type": "text", "text": "C Oriented varifold ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section, we introduce the oriented varifold associated with curves. For further readings on curves and surfaces representation as varifolds, readers can refer to [30, 12]. We associate to ", "page_idx": 14}, {"type": "text", "text": "$\\gamma\\in\\mathrm{C}^{1}((a,b),\\mathbb{R}^{d+1})$ an oriented varifold $\\mu_{\\gamma}$ , i.e. a distribution on the space $\\mathbb{R}^{d+1}\\times\\mathbb{S}^{d}$ defined as follows, for any smooth test function $\\omega:\\mathbb{R}^{d+1}\\times\\mathbb{S}^{d}\\rightarrow\\mathbb{R}.$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}_{Y\\sim\\mu_{\\gamma}}\\left[\\omega(Y)\\right]=\\mu_{\\gamma}(\\omega)=\\int_{a}^{b}\\omega\\left(\\gamma(t),\\frac{\\dot{\\gamma}(t)}{|\\dot{\\gamma}(t)|}\\right)|\\dot{\\gamma}(t)|\\,\\mathrm{d}t\\ .\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Denoting by $\\mathsf W$ the space of smooth test function, we have that $\\mu_{\\gamma}$ belongs to its dual $\\mathsf{W}^{*}$ . Thus, a distance on $\\mathsf{W}^{*}$ is sufficient to set a distance on oriented varifolds associated to curve and thus on $\\mathrm{C}^{1}((a,b),\\mathbb{R}^{d+1})$ by the identification $\\gamma\\to\\mu_{\\gamma}$ . Remark that in (TS-LDDMM), $\\gamma$ should be the parametrization of a time series\u2019 graph ${\\sf G}(s)$ , i.e. $\\gamma:t\\,\\in\\,\\mathsf{I}\\to(t,s(t))\\,\\in\\,\\mathbb{R}^{d+1}$ denoting by $s:\\mathsf{I}\\to\\mathbb{R}^{d}$ the time series. However, in practice, we work with discrete objects. That is why, we set $W$ as an RKHS to use its representation theorem. More specifically [30, Proposition 2 & 4] encourages us to consider a kernel $k:(\\mathbb{R}^{d+1}\\times\\mathbb{S}^{d})^{2}\\rightarrow\\mathbb{R}$ such that there exist two positive and continuously differentiable kernels $k_{\\mathrm{pos}}$ and $k_{\\mathrm{dir}}$ , such that for any $(x,\\overrightarrow{\\,u}),(y,\\overrightarrow{\\,v})\\in(\\bar{\\mathbb{R}}^{d+1}\\times\\mathbb{S}^{d})^{2}$ ", "page_idx": 15}, {"type": "equation", "text": "$$\nk((x,\\overrightarrow{\\mathcal{U}}),(y,\\overrightarrow{\\mathcal{v}}))=k_{\\mathrm{pos}}(x,y)k_{\\mathrm{dir}}(\\overrightarrow{\\mathcal{U}},\\overrightarrow{\\mathcal{v}})\\;,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with moreover $k_{\\mathrm{dir}}\\,>\\,0$ and $k_{\\mathrm{pos}}$ which admits an RKHS $\\mathsf{W_{p o s}}$ dense in the space of continous function on $\\mathbb{R}^{d+1}$ vanishing at infinite [10]. ", "page_idx": 15}, {"type": "text", "text": "Given such a kernel $k:(\\mathbb{R}^{d+1}\\times\\mathbb{S}^{d})^{2}\\rightarrow\\mathbb{R}$ verifying [30, Proposition 2 & 4], we have that for any $(x,v)\\in\\mathbb{R}^{d+1}\\times\\mathbb{S}^{d}$ , $\\delta_{(x,\\vec{v})}$ belongs to $\\mathsf{W}^{\\ast}$ as a distribution and that the dual metric $\\langle\\cdot,\\cdot\\rangle_{\\mathsf{W}^{*}}$ satisfies for any $(x_{1},v_{1}),(x_{2},v_{2})\\in\\left(\\mathbb R^{d+1}\\times\\mathbb S^{d}\\right)^{2}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\langle\\delta_{(x_{1},\\,\\overrightarrow{v}_{\\,1})},\\delta_{(x_{2},\\,\\overrightarrow{v}_{\\,2})}\\rangle_{\\mathsf{W}^{*}}=k\\big((x_{1},\\,\\overrightarrow{v}_{\\,1}),(x_{2},\\,\\overrightarrow{v}_{\\,2})\\big)\\;.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Thus, given two sets of triplets $X\\ =\\ (l_{i},x_{i},\\overrightarrow{\\,v}_{\\,i})_{{i\\in[T_{0}-1]}}\\ \\in\\ (\\mathbb{R}\\,\\times\\,\\mathbb{R}^{d+1}\\,\\times\\,\\mathbb{S}^{d})^{T_{0}-1},Y\\ =$ $(l_{i}^{\\prime},y_{i},\\overrightarrow{w}_{i})_{i\\in[T_{1}]}\\in(\\mathbb{R}\\times\\mathbb{R}^{d+1}\\times\\mathbb{S}^{d})^{T_{1}-1}$ and denoting by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mu_{X}=\\sum_{i=1}^{T_{0}}l_{i}\\delta_{(x_{i},\\overrightarrow{v}_{i})},\\mu_{Y}=\\sum_{i=1}^{T_{1}}l_{i}^{\\prime}\\delta_{(y_{i},\\overrightarrow{w}_{i})}\\ ,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "we have, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{|\\mu_{X}-\\mu_{Y}|_{\\mathbb{W}^{*}}^{2}=}&{\\sum_{i,j=1}^{T_{0}-1}l_{i}k((x_{i},\\overrightarrow{v_{i}}),(x_{i},\\overrightarrow{v_{i}}^{0}))l_{j}+\\sum_{i,j=1}^{T_{1}-1}l_{i}^{\\prime}k((y_{i},\\overrightarrow{w_{i}}),(y_{i},\\overrightarrow{w_{i}}))l_{j}^{\\prime}}\\\\ &{-2\\sum_{i=1}^{T_{0}-1}\\sum_{j=1}^{T_{1}-1}l_{i}k((x_{i},\\overrightarrow{v_{i}}),(y_{i},\\overrightarrow{w_{i}}))l_{j}^{\\prime}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Then, using the identification $X\\to\\mu_{X},Y\\to\\mu_{Y}$ , we can define a distance on sets of triplets as $d_{\\mathsf{W}^{*},3}(X,\\bar{Y})=|\\mu_{X}-\\mu_{Y}|_{\\mathsf{W}^{*}}^{2}$ . ", "page_idx": 15}, {"type": "text", "text": "Now, we aim to discretize the oriented varifold $\\mu_{\\mathsf{G}}$ related to a time series\u2019 graph ${\\sf G}(s)$ by using a set of triplets. This is carried out by using a discretized version of ${\\sf G}(s)$ , i.e. $\\tilde{\\mathsf{G}}=(g_{i}=(t_{i},s(t_{i})))_{i\\in[T]}\\in$ $(\\mathbb{R}^{d+1})^{T}$ , in the following way: For any $i\\in[T-1]$ , denoting the center and length of the $i^{t h}$ segment $[g_{i},g_{i+1}]$ by $c_{i}=(g_{i}+g_{i+1})/2$ , $l_{i}=\\left\\Vert{g_{i+1}-g_{i}}\\right\\Vert$ , and the unit norm vector of direction $\\overrightarrow{g_{i}g_{i+1}}$ by $\\overrightarrow{v_{i}}=(g_{i+1}-g_{i})/l_{i}$ , we define the set of triplets $\\overset{\\cdot}{X}(\\tilde{\\mathsf{G}})=(l_{i},c_{i},\\overrightarrow{v_{i}^{\\prime}})_{i\\in[T-1]}$ and its related oriented varifold $\\begin{array}{r}{\\mu_{X(\\tilde{\\mathsf{G}})}=\\sum_{i=1}^{T-1}l_{i}\\delta_{c_{i},\\overrightarrow{v_{i}^{\\star}}}}\\end{array}$ as in (11). This is a valid discretization of the oriented varifold $\\mu_{\\mathsf{G}}$ according to [30, Proposition 1]: $\\mu_{X(\\tilde{\\mathsf{G}})}$ converges towards $\\mu_{\\mathsf{G}}$ as the size of the descretization mesh $\\operatorname*{sup}_{i\\in[T-1]}\\left|t_{i+1}-t_{i}\\right|$ converges to 0. ", "page_idx": 15}, {"type": "text", "text": "Finally, we define a distance on discretized time series\u2019 graphs $\\tilde{\\mathsf{G}}_{1},\\tilde{\\mathsf{G}}_{2}$ as $d_{\\mathsf{W}^{*}}(\\tilde{\\mathsf{G}}_{1},\\tilde{\\mathsf{G}}_{2})\\;\\;=\\;\\;$ $d_{\\mathsf{W}^{*},3}\\'(X(\\tilde{\\mathsf{G}}_{1}),X(\\tilde{\\mathsf{G}}_{2}))$ . ", "page_idx": 15}, {"type": "text", "text": "C.1 Varifold kernels ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Denote the one-dimensional Gaussian kernel by $K_{\\sigma}^{(a)}(x,y)=\\exp(-|x-y|^{2}/\\sigma)$ for any $(x,y)\\in$ $(\\mathbb{R}^{a})^{2},~a~\\in~\\mathbb{N}$ and $\\sigma\\mathrm{~\\,~>~0~}$ . In the implementation, we use the following kernels, for any $\\begin{array}{r}{((t_{1},x_{1}),(t_{2},x_{2}))\\in(\\mathbb{R}^{d+1})^{2},((w_{1},v_{1}),(w_{2},v_{2}))\\in(\\mathbb{S}^{d})^{2},}\\end{array}$ , ", "page_idx": 15}, {"type": "equation", "text": "$$\nk_{\\mathrm{pos}}(x,y)=K_{\\sigma_{\\mathrm{pos},t}}^{(1)}(t_{1},t_{2})K_{\\sigma_{\\mathrm{pos},x}}^{(d)}(x_{1},x_{2}),\\quad k_{\\mathrm{pos}}(x,y)=K_{\\sigma_{\\mathrm{dir},t}}^{(1)}(w_{1},w_{2})K_{\\sigma_{\\mathrm{dir},x}}^{(d)}(v_{1},v_{2})\\;,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where $\\sigma_{\\mathrm{pos},t},\\sigma_{\\mathrm{pos},x},\\sigma_{\\mathrm{dir},t},\\sigma_{\\mathrm{dir},x}>0$ are hyperparameters. In practice, we select $\\sigma_{\\mathrm{pos},x}\\approx\\sigma_{\\mathrm{dir},x}\\approx$ 1 when the times series are centered and normalized. Otherwise we select $\\sigma_{\\mathrm{pos},x}\\approx\\sigma_{\\mathrm{dir},x}\\approx\\bar{\\sigma}_{s}$ with $\\bar{\\sigma}_{s}$ the average standard deviation of the time series. We choose $\\sigma_{\\mathrm{pos},t}\\approx\\sigma_{\\mathrm{dir},t}=m f_{e}$ with $f_{e}$ the sampling frequency of the time series and $m\\in[5]$ an integer depending on the time change between the starting and the target time series graph. The more significant the time change, the higher $m$ should be. The intuition comes from the fact that the width $\\sigma_{\\mathrm{pos},t},\\sigma_{\\mathrm{dir},t}$ rules the time windows used to perform the comparison, and $\\sigma_{\\mathrm{pos},x},\\sigma_{\\mathrm{dir},x}$ affects the space window. The size of the windows should be selected depending on the variations in the data. ", "page_idx": 16}, {"type": "text", "text": "D Tuning the hyperparameters of the TS-LDDMM velocity field kernel ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "The parameter $\\sigma_{T,0}$ should be chosen large compared the sampling frequency $f_{e}$ and compared to average standard deviation $\\bar{\\sigma}_{s}$ of the time series, e.g $\\sigma_{T,0}\\,=\\,100$ as $\\bar{\\sigma}_{s}\\approx\\,f_{e}\\,\\approx\\,1$ . It makes the time transformation smoother. If $\\sigma_{T,0}$ is too small, for instance, $\\sigma_{T,0}=f_{e}$ , the effect of the time deformation is too localized, and there are not enough samples to make it visible. ", "page_idx": 16}, {"type": "text", "text": "The parameter $\\sigma_{T,1}$ should be of the same order as $f_{e}$ : two different points in time can have various space transformations. $\\sigma_{x}$ should be of the same order of $\\bar{\\sigma}_{s}$ : two points with a big difference regarding space compared to $\\bar{\\sigma}_{s}$ can have very different space transformations. ", "page_idx": 16}, {"type": "text", "text": "We take $c_{0}\\approx10c_{1}$ , we want to encourage time transformation before space transformation. We take $(c_{0},c_{1})=(1,0.1)$ in all experiments. ", "page_idx": 16}, {"type": "text", "text": "E Experimental settings ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "All experiments were performed on a Debian 6.1.69-1 server with NVIDIA RTX A2000 12GB GPU, Intel(R) Xeon(R) Gold 5220R CPU $\\textcircled{a}2.20\\mathrm{GHz}$ , and 250 GB of RAM. The source code is available on Github5. ", "page_idx": 16}, {"type": "text", "text": "E.1 Optimization details of TS-LDDMM & LDDMM ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We implemented TS-LDDMM in Python with the JAX library 6. ", "page_idx": 16}, {"type": "text", "text": "Initialization. As initialization of (8), all momentum parameters are set to 0, and the initial graph of reference is picked from the dataset such that its length is equal to the median length observed in the dataset. ", "page_idx": 16}, {"type": "text", "text": "Gradient descent. The chosen gradient descent method is \"adabelief\" [65] implemented in the OPTAX library 7. The gradient descent has two main parameters: the number of steps (nb_steps) and the maximum stepsize value $(\\eta_{M})$ . The stepsize has a scheduling scheme: ", "page_idx": 16}, {"type": "text", "text": "\u2022 Warmup period on $0.1\\times$ nb_steps steps: the stepsize increases linearly from 0 to $\\eta_{M}$ . The goal is to learn progressively the parameters. If the step size is too large at the start, smaller steps at the end cannot make up for the mistakes made at the beginning. ", "page_idx": 16}, {"type": "text", "text": "\u2022 Fine tuning periode on $0.9\\times$ nb_steps $:$ the stepsize decreases from $\\eta_{M}$ to 0 with a cosine decay implemented in the OPTAX scheduler, i.e. the decreasing factor as the form $0.5(1+$ $\\cos(\\dot{\\pi}t/\\bar{T})$ ). ", "page_idx": 16}, {"type": "image", "img_path": "JM0IQSliol/tmp/99852415cfc886fb4e3ec64f3917c89e5f45f74738589f5d89ec00d71f84b1cf.jpg", "img_caption": [], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Figure 6: A: Illustration of a double-chamber plethysmograph. The term dpt stands for differential pressure transducer which measures the pressure in each compartment, the pressure then being converted to flow. B: Nasal airflow (top) and lung volume (bottom). During inspiration, airflow is positive (grey) and during expiration, airflow is negative (pink). ", "page_idx": 17}, {"type": "text", "text": "F Datasets ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "F.1 Mouse respiratory cycle dataset ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Ventilation is a simple physiological function that ensures a vital supply of oxygen and the elimination of CO2. Acetylcholine (Ach) is a neurotransmitter that plays an important role in muscular activity, notably for breathing. Indeed, muscle contraction information passes from the brain to the muscle through the nervous system. Achs are located in synapses of the nervous system (central and peripheral) and skeletal muscles. They ensure the information transmission from nerve to nerve. However, the transmission cannot end without the hydrolysis of Ach by the enzyme Acetylcholinesterase (AchE), allowing nerves to return to their resting state. Inhibition of (AchE) with, for instance, nerve gas, pesticide, or drug intoxication leads to respiratory arrests. ", "page_idx": 17}, {"type": "text", "text": "The dataset comes from the experiment [41], where they studied the consequences of partial deficits in AChE and AChE inhibition on mice respiration. AchE inhibition was induced with an irritant molecule called physostigmine (an AchE inhibitor). Mice nasal airflows were sampled at ${2000}\\mathrm{Hz}$ with a Double Chamber plethysmograph [28], as depicted in Figure 6-A). The flow is expressed in $m l.s^{-1}$ ; it has a positive value during inspiration and a negative value expiration Figure 6-B). Among the mice population, we selected 7 control mice (wt) and 7 ColQ mice (colq), which do not have AChE anchoring in muscles and some tissues. As described in [41], mice experiments were as follows: ", "page_idx": 17}, {"type": "text", "text": "1. The mouse is placed in a DCP for 15 or $20\\,\\mathrm{min}$ to serve as an internal control. ", "page_idx": 17}, {"type": "text", "text": "2. The mouse is removed from the DCP and injected with physostigmine.   \n3. The mouse is placed back into the DCP, and its nasal flow is recorded for 35 or $40\\;\\mathrm{min}$ . ", "page_idx": 17}, {"type": "text", "text": "Respiratory cycles were extracted following procedure [21]. We removed respiratory cycles whose duration exceeds 1 second; the average respiratory cycle duration is $300\\,\\mathrm{ms}$ . We randomly sampled 10 respiratory cycles per minute and mouse. It leads to a dataset of 12,732 (time, genotype)-annotated respiratory cycles. ", "page_idx": 17}, {"type": "text", "text": "F.2 Shape-based UCR/UEA time series classification datasets ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We selected 15 shape-based datasets (7 univariates and 8 multivariates) from the from the University of East Anglia (UEA) and the University of California Riverside (UCR) Time Series Classification Repository8 [15, 3]. All datasets were downloaded with the python package aeon9. Essential datasets information are summarized in Table 1 and further can be found in [15, 3]. ", "page_idx": 18}, {"type": "table", "img_path": "JM0IQSliol/tmp/4387d28c6db7d620f543f0919ee9766cf30bdbecaa0ee724efb952c4a82f105c.jpg", "table_caption": ["Table 1: UCR/UEA shape-based time series datasets for classification. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "G Appendix for experiment: TS-LDDMM representation identifiability ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this experiment, we evaluate the ability of TS-LDDMM to retrieve the parameter $v_{0}^{*}$ that encodes the deformation $\\varphi^{\\{v_{0}^{*}\\}}$ acting on a time series graph G by solving the geodesic shooting problem (6) between $\\mathsf{G}$ and $\\varphi^{\\{v_{0}^{*}\\}}$ .G. Parameter identifiability is an important property for subsequent statistical analysis. Results show that TS-LDDMM representations are identifiable or weakly identifiable depending on the velocity field kernel $K_{G}$ specification. ", "page_idx": 18}, {"type": "text", "text": "G.1 Settings ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "This experiment only involves the TS-LDDMM method in two different settings: ", "page_idx": 18}, {"type": "text", "text": "\u2022 The velocity field kernel $K_{G}$ is well-specified: The velocity field kernel $K_{G}$ is set to $(c_{0},c_{1},\\sigma_{T,0},\\sigma_{T,1},\\sigma_{x})\\,=\\,(1,0.1,100,1,1)$ , the varifold loss kernels $(k_{p o s},k_{d i r})$ are set to $(\\sigma_{\\mathrm{pos},t},\\sigma_{\\mathrm{pos},t},\\sigma_{\\mathrm{dir},t},\\sigma_{\\mathrm{dir},x})\\;=\\;(2,1,2,0.6)$ , and the optimizer has 400 steps with a maximum stepsize $\\eta_{M}$ of 0.05.   \n\u2022 The velocity field kernel $K_{G}$ is missspecified: The velocity field kernel $K_{G}$ is set with $(c_{0},c_{1},\\sigma_{T,1})\\ =\\ (1,0.1,1)$ , $\\sigma_{T,0}$ ranging in $(1,5,10,50,100,200,300)$ , and $\\sigma_{x}$ ranging in $(0.1,1,10,100)$ . The varifold loss kernels $(k_{p o s},k_{d i r})$ are set to $(\\sigma_{\\mathrm{pos},t},\\sigma_{\\mathrm{pos},t},\\sigma_{\\mathrm{dir},t},\\sigma_{\\mathrm{dir},x})=(2,1,2,0.6)$ , and the optimizer has 400 steps with a maximum stepsize $\\eta_{M}$ of 0.05. ", "page_idx": 18}, {"type": "text", "text": "provided that the hyperparameters and the reference graph are wisely selected, i.e., the parameter $v_{0}^{*}$ generating a deformation $\\varphi^{\\{v_{0}^{*}\\}}$ of a time series graph G can be estimated from the data ${\\sf G},\\varphi^{\\{v_{0}^{*}\\}}.\\bar{\\sf G}$ by solving the geodesic shooting problem (6). ", "page_idx": 18}, {"type": "text", "text": "The velocity field kernel $K_{G}$ is well specified. First, we show the model identifiability when the kernel $K_{G}$ is well specified: the estimated parameter is a good approximation of the generating parameter when the generation and the estimation procedure use the same hyperparameters for the RKHS kernel $K_{G}$ . All the hyperparameter values for generation and estimation are given in Appendix G.1. ", "page_idx": 18}, {"type": "image", "img_path": "JM0IQSliol/tmp/546b9580a2a7a576132aeae3324d11b0a6c065f66d3d0718fd4677abc9eebc88.jpg", "img_caption": ["Figure 7: Plots of $\\varphi^{\\{v_{0}(\\alpha^{*},\\mathsf{X})\\}}.\\mathsf{X}$ for different values of $\\alpha^{*}$ according to its sampling parameter $t_{a},s_{a},m_{s}$ , taking $\\mathsf{X}=\\mathsf{G}(s_{0})$ with $s_{0}:k\\in[300]\\to\\sin(2\\pi k/300)$ . "], "img_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "JM0IQSliol/tmp/cbed1f6d544b39cb0be1bbc5265f1fb69930f279a2af404db7d4352d8113ea26.jpg", "table_caption": ["Table 2: Values of $\\mathcal{L}(\\varphi^{\\{v_{0}(\\alpha^{*},\\mathsf{X})\\}}.\\mathsf{X},\\varphi^{\\{\\hat{v}_{0}\\}}.\\mathsf{X})$ as $\\alpha^{*}$ is sampled according to Gen(10,10,50) and $\\hat{v}_{0}$ is estimated using $K_{G}$ with varying parameters $\\sigma_{T,1},\\sigma_{x}$ . "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "We fix the initial control points as $\\sf X=(x_{k}=(k,\\sin(2\\pi k/300)))_{k\\in[300]}$ . Given $m_{s}\\in\\mathbb{N}_{>0}$ and $t_{a},s_{a}\\,>\\,0$ , we randomly generate initial momentums $\\alpha^{*}\\,=\\,(\\alpha_{k}^{*})_{k\\in[{\\mathbf{n}}_{0}]}$ with the following sampling, called ${\\mathrm{Gen}}(m_{s},t_{a},s_{a})$ : For any $k\\,\\in\\,[\\mathbf{n}_{0}]$ , $\\alpha_{k}^{\\prime}$ is sampled according to a Gaussian normal distribution $\\mathcal{N}(0_{d+1},I_{d+1})$ . Then, $(\\alpha_{k}^{\\prime})_{k\\in[\\mathbf{n}_{0}]}$ is regularized by a rolling average of size $m_{s}$ , we get $\\bar{\\alpha}^{\\prime}=(\\bar{\\alpha}_{k}^{\\prime})_{k\\in[{\\bf n}_{0}]}$ . Finally, we normalize $\\bar{\\alpha}^{\\prime}$ to derive $\\alpha^{*}$ such that $|([\\alpha_{k}^{*}]_{t})_{k\\in[\\mathbf{n}_{0}]}|\\,=\\,t_{\\mathrm{amp}}$ and $|([\\alpha_{k}^{*}]_{s})_{k\\in[\\mathbf{n}_{0}]}|=\\dot{s}_{\\mathrm{amp}}$ for any $k\\,\\in\\,[\\mathbf{n}_{0}]$ , denoting by $[\\alpha_{k}^{*}]_{t},[\\alpha_{k}^{*}]_{s}$ the time and space coordinates of $\\alpha_{k}^{*}$ respectively. Note that the regularizing step $(\\alpha_{k}^{\\prime})_{k\\in[{\\bf n}_{0}]}\\rightarrow\\bar{\\alpha}^{\\prime}$ is necessary to obtain realistic deformations which take into account the regularity induced by the RKHS V. ", "page_idx": 19}, {"type": "text", "text": "Then, using $v_{0}(\\alpha^{*},\\mathsf X)$ as defined in (4) with initial momentums $\\alpha^{*}$ and control points $\\mathsf{X}$ , we apply the induced deformation $\\varphi^{\\{v_{0}\\}}$ by (5) to $\\mathsf{X}$ and obtain $\\varphi^{\\{v_{0}\\}}$ .X. Finally, we solve (6) to recover an estimation $\\hat{\\alpha}$ of $\\alpha^{*}$ and report the average relative error (ARE) $|v_{0}(\\hat{\\alpha},\\mathsf{X})-v_{0}(\\alpha^{*},\\mathsf{X})|_{\\mathsf{V}}/|v_{0}(\\alpha^{*},\\mathsf{X})|_{\\mathsf{Y}}$ on 50 repetitions. This procedure is performed for any $m_{s},t_{a},s_{a}\\in\\{10,50,100\\}\\times\\{5,10,15,20\\}^{2}$ . Mean, standard deviation, and maximum of the ARE on all these hyperparameters choices are respectively 0.10, 0.03, 0.17. Therefore, the estimation procedure (6) offers a good approximation of the true parameter when the kernel $K_{\\mathsf{G}}$ is well specified. We observe that the estimation is difficult when $t_{a}\\ll s_{a}$ because the time series can be very noisy as illustrated in Figure 7: this impacts the Varifold loss which is sensitive to tangents. ", "page_idx": 19}, {"type": "text", "text": "The velocity field kernel $K_{G}$ is misspecified. We demonstrate a weak identifiability when the kernel $K_{\\mathsf{G}}$ is misspecified: we can reconstruct the graph time series\u2019 after deformations even if the hyperparameters of $K_{\\mathsf{G}}$ are different during the generation and the estimation. The hyperparameters of $K_{\\mathsf{G}}$ during generation are $(c_{0},c_{1},\\sigma_{T,0},\\sigma_{T,1},\\sigma_{x})\\;=\\;(1,0.1,100,1,1)$ and we fix $\\sigma_{T,1},c_{0},c_{1}=(1,1,0.1)$ for $K_{G}$ during estimation. We aim to understand the impact of $\\sigma_{T,1},\\sigma_{x}$ on the reconstruction since they are encoding the smoothness of the transformation according to time and space. ", "page_idx": 19}, {"type": "text", "text": "For any choice of the hyperparameters $\\sigma_{T,1},\\sigma_{x}\\in\\{1,10,50,100,200,300\\}\\times\\{0.1,1,100\\}$ related to $K_{\\mathsf{G}}$ in the estimation, we average $\\mathcal{L}(\\varphi^{\\{v_{0}(\\alpha^{*},\\mathsf{X})\\}}.\\mathsf{X},\\varphi^{\\{\\hat{v}_{0}\\}}.\\mathsf{X})$ on 50 repetitions when $\\alpha^{*}$ is sampled according to ${\\mathrm{Gen}}(10,10,50)$ and $\\hat{v}_{0}=v_{0}(\\hat{\\alpha},\\sf X)$ denoting by $\\hat{\\alpha}$ the result of the minimization (6). We observe in Table 2 that the reconstruction is almost perfect except in the case when $\\sigma_{t,0}=1$ during estimation, while $\\sigma_{t,0}=100$ during generation. Compared to $\\sigma_{T,0},\\sigma_{x}$ has nearly no impact on the reconstruction. In Appendix C.1-D, we propose guidelines to drive future hyperparameters tuning and further discussions related to $\\sigma_{T,1},c_{0},c_{1}$ . ", "page_idx": 19}, {"type": "text", "text": "H Appendix for experiment: Robustness to irregular sampling ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "This experiment is inspired by [43] where the authors perform an extensive comparison of Neural Ordinary Differential Equations (Neural ODEs) methods [31]. We assess the classification performances of several methods under regular sampling ( $0\\%$ missing rate) and three irregular sampling regimes on 15 shape-based datasets (7 univariate & 8 multivariate). Methods and training strategy are taken from its associated Github10 and described in what follows. We conclude with the results, which show that our method, TS-LDDMM, outperforms all methods for sampling regimes with missing rates: $0\\%$ , $30\\%$ , and $50\\%$ . ", "page_idx": 20}, {"type": "text", "text": "H.1 Benchmark methods ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "In related work, we give an overview of Neurals ODEs methods and their relation with TS-LDDMM. ", "page_idx": 20}, {"type": "text", "text": "\u2022 RNN-based methods: Baseline reccurent neural networks including RNN [38], LSTM [27], and GRU [14].   \n\u2022 Attention-based methods: Multi-Time Attention Networks (MTAN) [52] and MultiIntegration Attention Module (MIAM) [35]. Both handle multivariate time series irregularly sampled with attention mechanisms.   \n\u2022 Neural ODEs: ODE-LSTM [34] a form of Neural-ODEs used to learn continuous latent representations.   \n\u2022 Neural SDEs: Neural SDE [36] and Neural LNSDE [43] have been proposed to model randomness in time-series using drift and diffusion terms as an extension of Neural-ODEs.   \n\u2022 Shape-Analysis methods: TS-LDDMM (ours) and LDDMM [23]. From shape analysis, both methods learn representations by solving ODEs parametrized with Kernels. While both methods handle multivariate signals irregularly sampled, TS-LDDMM is specifically designed for time series. ", "page_idx": 20}, {"type": "text", "text": "H.2 Model architecture ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Neural ODEs methods As depicted in [43], any Neural ODEs layer in Appendix H.1 is followed by an MLP with two fully connected layers with ReLU activations. The risk of overfitting and the model regularization are handled with a dropout rate of $10\\%$ and an early-stopping mechanism, ceasing the training when the validation loss does not improve for 10 successive epochs. ", "page_idx": 20}, {"type": "text", "text": "For each method and dataset, the learning rate, the hidden vector dimensions, and the number of layers are optimized to minimize the CrossEntropy loss on a validation set using the Ray 11 Python library. The learning rate varies from $10^{-4}$ to $\\bar{10}^{-1}$ using log uniform search, the hidden vector dimension ranges from 16, 32, 64, 128 using grid search, and the number of layers ranges from $1,2,3,4$ using grid search. The batch size was selected from 16, 32, 64, 128 according to the size of the dataset. All methods were trained for 100 epochs, and the best method was selected based on the lowest validation loss. ", "page_idx": 20}, {"type": "text", "text": "TS-LDDMM and LDDMM Representations learned with TS-LDDMM or LDDMM are fed to a Support Vector Classifier (SVC) from scikit-learn 12. All SVC\u2019s hyperparameters are set to default except the regularization term C, which is set through grid search on a validation set with the macro f1-score 13. ", "page_idx": 20}, {"type": "text", "text": "To learn TS-LDDMM (resp. LDDMM) representations, the velocity field kernel $K_{G}$ is set to $(c_{0},c_{1},\\sigma_{T,0},\\sigma_{T,1},\\sigma_{x})=(\\bar{1},\\bar{0}.1,0.33\\bar{l},1,n_{d})$ , (resp. $(\\sigma_{T},\\bar{\\sigma}_{x})=(0.33\\bar{l},n_{d}))$ where $\\bar{l}$ is the average time series length and $n_{d}$ the number of dimensions. For both methods and all datasets, the varifold loss kernels $(k_{p o s},k_{d i r})$ are identical and set to $(\\sigma_{\\mathrm{pos},t},\\sigma_{\\mathrm{pos},t},\\sigma_{\\mathrm{dir},t},\\sigma_{\\mathrm{dir},x})\\,=\\,(2,n_{d},2,n_{d})$ . For TS-LDDMM (resp. LDDMM), the optimizer is set with 400 epochs (resp. 400) and a maximum learning rate $\\eta_{M}=0.1$ (resp. $\\eta_{M}=0.01)$ ). In all cases, the initial reference graph is selected in the dataset as a time series with the median length. ", "page_idx": 20}, {"type": "text", "text": "H.3 Protocol ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this experiment, we investigate the robustness to missing samples and the classification performance of TS-LDDMM compared to Neural ODEs on 15 datasets described in Appendix F.2. For fairness between methods of different architectures, the evaluation protocol on each dataset and method is as follows: ", "page_idx": 21}, {"type": "text", "text": "1. Spilt the dataset in train $75\\%$ , validation $15\\%$ , and test $15\\%$ .   \n2. Tune hyperparameters with train and validation sets and a missing rate of $0\\%$ .   \n3. For each missing rate in $[0\\%,30\\%,50\\%,70\\%]$ \u2022 Remove samples in time series in the train and test sets according to the missing rate and the drop procedure described in [31]. \u2022 Train the model on the train set \u2022 Evaluate the macro f1-score on the test set ", "page_idx": 21}, {"type": "text", "text": "H.4 Results ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this experiment, we investigate the robstuness to missing samples and the classification performance of TS-LDDMM representations. We compare TS-LDDMM with LDDMM and 8 neural ODEs networks. Performances are evaluated in terms of average macro f1-score and rank on four different regimes of missing rate $0\\%,30\\%,50\\%$ , and $70\\%$ . Results are aggregated in Table 3. ", "page_idx": 21}, {"type": "text", "text": "On three out of four regimes $(0\\%,30\\%$ , and $50\\%$ ) TS-LDDMM classifier is the best performer in terms of f1-score and rank. For missing rates of $0\\%$ and $30\\%$ , the score increases by $10\\%$ compared to the second-best performer, LDDMM. However, LDDMM is not the second-best performer in rank (Neural LNSDE), showing its sensitivity to parameterization, unlike TS-LDDMM, which remains consistent. Performances of Neural LNSDE remain constant with the increase of the missing rate as observed in [43], and it becomes the best performer for missing rate $70\\%$ . The decrease in TS-LDDMM performances with the increasing missing rate is due to the varifold loss, which poorly approximates the time series shape. Other losses might be more relevant for high missing rates. ", "page_idx": 21}, {"type": "text", "text": "Overall, TS-LDDMM is a relevant and consistent shape-based representation for irregularly sampled multivariate time series for missing rates up to $50\\%$ . ", "page_idx": 21}, {"type": "table", "img_path": "JM0IQSliol/tmp/700a67bb5265c49057e42256a0aa16ac18bf461892d6503d33038c9829c700ed.jpg", "table_caption": ["Table 3: Comparison of average macro f1-score and rank as the sample dropping rate increases. First & second best performers. TS-LDDMM is the best performer on three out of four regimes. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "I Appendix for experiment: Classification benchmark on regularly sampled datasets ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we compare the classification performances of TS-LDDMM with other methods from shape analysis on 15 shape-based datasets of time series regularly sampled. TS-LDDMM outperforms other methods on 12 out of 15, highlighting its relevance for shape analysis when dealing with time series. ", "page_idx": 21}, {"type": "text", "text": "I.1 Benchmark methods ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 SRV-based method: we include TCLR [26] a logistic regression on the tangent space of the Frechet mean with Square Root Velocity (SRV representation). We also include Shape-FPCA [60] that encodes both the time series and its time parameterization. \u2022 LDDMM-Based : TS-LDDMM (ours) and LDDMM [23]. Both methods learn representations by solving ODEs parametrized with Kernels. While both methods handle multivariate signals, TS-LDDMM is specifically designed for time series. ", "page_idx": 22}, {"type": "text", "text": "I.2 Model settings ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "TCLR & Shape-FPCA Shape-FPCA is available in the Python library FDASRSF 14. Once the shapeFPCA representations are learned, they are fed to an SVC from scikit-learn. FDASRSF provides SRV representation methods that we combined with a logistic regression from scikit-learn to implement TCLR. For both methods, the number of steps to learn the Frechet mean is set to 50, and the regularization hyperparameter C is set through grid search on a validation set with the macro f1-score. Other parameters are set to default. ", "page_idx": 22}, {"type": "text", "text": "TS-LDDMM & LDDMM Representations learned with TS-LDDMM or LDDMM are fed to an SVC from scikit-learn. All SVC\u2019s hyperparameters are set to default except the regularization term C, which is set through grid search on a validation set with the macro f1-score. ", "page_idx": 22}, {"type": "text", "text": "To learn TS-LDDMM (resp. LDDMM) representations, the velocity field kernel $K_{G}$ is set to $(c_{0},c_{1},\\sigma_{T,0},\\sigma_{T,1},\\sigma_{x})=(\\bar{1},\\bar{0}.1,0.33\\bar{l},1,n_{d})$ , (resp. $(\\sigma_{T},\\sigma_{x})=(0.33\\bar{l},n_{d}))$ where $\\bar{l}$ is the average time series length and $n_{d}$ the number of dimensions. For both methods and all datasets, the varifold loss kernels $(k_{p o s},k_{d i r})$ are identical and set to $(\\sigma_{\\mathrm{pos},t},\\sigma_{\\mathrm{pos},t},\\sigma_{\\mathrm{dir},t},\\sigma_{\\mathrm{dir},x})\\,=\\,(2,n_{d},2,n_{d})$ . For TS-LDDMM (resp. LDDMM), the optimizer is set with 400 epochs (resp. 400) and a maximum learning rate $\\eta_{M}=0.1$ (resp. $\\eta_{M}=0.01)$ ). In all cases, the initial reference graph is selected in the dataset as a time series with the median length. ", "page_idx": 22}, {"type": "text", "text": "I.3 Protocol ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "For each dataset and method, the evaluation protocol is a simple train,validation test with hyperparameter tuning: ", "page_idx": 22}, {"type": "text", "text": "1. Split The dataset in train $75\\%$ , validation $15\\%$ , and test $15\\%$ .   \n2. Training and hyperparameters tuning with train and validation sets   \n3. Evaluate the macro f1-score on the test set ", "page_idx": 22}, {"type": "text", "text": "I.4 Results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this experiment, we investigate the classification performances of several methods from shape analysis on 15 shape-based time series datasets (7 univariate and 8 multivariate). The performances are evaluated in terms of macro f1-score. Results are aggregated in Table 4. ", "page_idx": 22}, {"type": "text", "text": "The TS-LDDMM-based classifier outperforms other methods on 12 out of 15 datasets. TCLR is the second-best performer on univariate datasets; however, its current implementation with FDASRSF does not extend to the multivariate case, which limits usage. LDDMM performances are lower than TCLR, and Shape-FPCA is the worst performer. ", "page_idx": 22}, {"type": "text", "text": "Overall, TS-LDDMM representations are well suited for shape-based time series classification, and its extension to multivariate irregularly sampled time series makes it a relevant option for time series shape analysis. ", "page_idx": 22}, {"type": "text", "text": "J Appendix for the experiment: Noise sensitivity ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "This experiment evaluates the influence of noise on the learning of the reference sequence for TS-LDDMM and SRVF Kacher-mean, a subroutine of Shape-FPCA [60]. ", "page_idx": 22}, {"type": "table", "img_path": "JM0IQSliol/tmp/b4548fa4c45a0826078a09123b1f0f85687e867cc5e37feac5b7074f93e55871.jpg", "table_caption": ["Table 4: F1-score comparison between methods from shape analysis on 15 datasets. First and second best performers. "], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "J.1 Protocol ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The dataset includes 100 sine waves with randomly generated time parametrization by following the procedure described in Appendix G.1 with ${\\mathrm{Gen}}(50,1,0)$ and uniformly resampled. The dataset has been altered under four scenarios with an additive Gaussian noise centered and with standard deviation $\\sigma_{\\epsilon}\\in\\{0,0.05,0.1,0.2\\}$ . The referent sequence is learned for each scenario, and the $L_{2}$ -norm error between the exact and the learned barycenter is computed. ", "page_idx": 23}, {"type": "text", "text": "J.2 Method settings ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "For SRVF, the number of steps to learn the Kacher-mean is set to 20. Regarding TS-LDDMM, the velocity field kernel $K_{G}$ is set to $(c_{0},c_{1},\\sigma_{T,0},\\sigma_{T,1},\\sigma_{x})\\,=\\,(1,0.1,65,\\bar{1},1)$ , and the varifold loss kernels $(k_{p o s},k_{d i r})$ are set to $(\\sigma_{\\mathrm{pos},t},\\sigma_{\\mathrm{pos},t},\\sigma_{\\mathrm{dir},t},\\sigma_{\\mathrm{dir},x})=(5,1,1,1)$ . For TS-LDDMM, the optimizer is set with 400 epochs and a maximum learning rate $\\eta_{M}=0.1$ . In all cases, the initial reference graph is selected in the dataset as a time series with the median length. ", "page_idx": 23}, {"type": "text", "text": "J.3 Results ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Figure 8 illustrates the results. Noise level affects the learning reference graph in both cases, as depicted by the increasing error and the illustrations. However, the overall sine wave shape is better preserved by TS-LDDMM compared to SRVF Kacher-mean, for which the sine wave amplitude decreases as the noise increases. In addition, for TS-LDDMM, the regularity of the reference graph can be controlled by penalizing the norm of the velocity fields in the loss function. Further work on penalization will be conducted to handle noisy data better. ", "page_idx": 23}, {"type": "text", "text": "K Appendix for experiment: Analysis of respiratory behavior in mice ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "K.1 Settings ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "This experiment involves TS-LDDMM, LDDMM [23] and Shape-FPCA [60] methods. Two scenarios are investigated: before drug exposure and before/after drug exposure. All methods are investigated on both scenarios. ", "page_idx": 23}, {"type": "text", "text": "TS-LDDMM parameters. ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 Before exposure: The velocity field kernel $K_{G}$ is set to $\\left(c_{0},c_{1},\\sigma_{T,0},\\sigma_{T,1},\\sigma_{x}\\right)\\ =$ $(1,0.1,150,1,2)$ . The varifold loss is the sum of three varifolds to capture shapes variations at different scales with parameters: (Varifold 1,Varifold 2,Varifold 3): $((5,2,5,1),(2,1,2,0.6),(1,0.6,1,0.6))$ and the mapper $(\\sigma_{\\mathrm{pos},t},\\sigma_{\\mathrm{pos},t},\\sigma_{\\mathrm{dir},t},\\sigma_{\\mathrm{dir},x})$ . The optimizer has 800 steps with a maximum stepsize $\\eta_{M}$ of 0.3. \u2022 Before/after exposure: The velocity field kernel $K_{G}$ is set to $\\left(c_{0},c_{1},\\sigma_{T,0},\\sigma_{T,1},\\sigma_{x}\\right)=$ $(1,0.1,220,1,2)$ . The varifold loss is the sum of four varifolds to capture shapes ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 23}, {"type": "text", "text": "(a) TS-LDDMM barycenter ", "text_level": 1, "page_idx": 24}, {"type": "image", "img_path": "JM0IQSliol/tmp/b4865ab5a5a68e31125c6bb5f1fe2a8d467e47285944cbe9591705d68a0c544a.jpg", "img_caption": ["Figure 8: Illustration of the learned barycenter (red) compared to the exact barycenter (green) for both TS-LDDMM (a) and Shape-FPCA (b). The computation has been done for different level of noise $\\epsilon\\sim\\mathcal{N}(0,\\sigma_{\\epsilon})$ with $\\sigma_{\\epsilon}\\in\\{0,0.05,0.1,0.2\\}$ . "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "variations at different scales with parameters: (Varifold 1,Varifold 2,Varifold 3, Varifold 4): $((30,2,30,1),(5,2,5,\\bar{1}),(2,1,2,0.6),(1,0.1,1,0.1))$ and the mapper $(\\sigma_{\\mathrm{pos},t},\\sigma_{\\mathrm{pos},t},\\sigma_{\\mathrm{dir},t},\\sigma_{\\mathrm{dir},x})$ . The optimizer has 800 steps with a maximum stepsize $\\eta_{M}$ of 0.3. ", "page_idx": 24}, {"type": "text", "text": "LDDMM parameters. Note that varifold losses are unchanged between TS-LDDMM and LDDMM. Compared to TS-LDDMM, the convergence of LDDMM is more sensitive to the maximum stepsize $\\eta_{m}$ , which must remain small for LDDMM to guarantee the convergence. ", "page_idx": 24}, {"type": "text", "text": "\u2022 Before exposure: The velocity field kernel $K_{G}$ is an anysotropic Gaussian kernel with parameters $\\sigma_{T}\\,=\\,150$ for the time dimension and $\\sigma_{x}\\,=\\,2$ for space dimensions. The varifold loss is the sum of three varifolds to capture shapes variations at different scales with parameters: (Varifold 1,Varifold 2,Varifold 3): $((5,\\bar{2},5,1),(2,1,2,0.6),(1,0.6,1,0.6))$ and the mapper $(\\sigma_{\\mathrm{pos},t},\\sigma_{\\mathrm{pos},t},\\sigma_{\\mathrm{dir},t},\\sigma_{\\mathrm{dir},x})$ . The optimizer has 800 steps with a maximum stepsize $\\eta_{M}$ of 0.01. \u2022 Before/after exposure: The velocity field kernel $K_{G}$ is an anysotropic Gaussian kernel with parameters $\\sigma_{T}\\mathrm{~\\,~=~\\,~}220$ for the time dimension and $\\sigma_{x}\\textrm{\\,\\ensuremath{\\mathscr{D}}}=\\textrm{\\,2}$ for space dimensions. The varifold loss is the sum of four varifolds to capture shapes variations at different scales with parameters: (Varifold 1,Varifold 2,Varifold   \n3, Varifold 4): $((30,2,30,1),(5,2,5,1),(\\bar{2},1,2,0.6),(1,0.1,1,0.1))$ and the mapper $(\\sigma_{\\mathrm{pos},t},\\sigma_{\\mathrm{pos},t},\\sigma_{\\mathrm{dir},t},\\sigma_{\\mathrm{dir},x})$ . The optimizer has 800 steps with a maximum stepsize $\\eta_{M}$ of   \n0.01. ", "page_idx": 24}, {"type": "text", "text": "Shape-FPCA parameters. For both scenarios, respiratory cycles are linearly interpolated and resampled to 200 points, and the length of the original time interval is kept. The computation of the Kacher-mean is done in a maximum of 50 iterations, and srv representations of the realigned time series and time parametrization are concatenated with cycle durations. When concatenating these vectors, the choice of amplitude factors is made to minimize the reconstruction error from the ", "page_idx": 24}, {"type": "image", "img_path": "JM0IQSliol/tmp/af2d9174b14ed6670f06d30fa14e3408da53e63566d9122620b5c60801a8244d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 9: Analysis of the first Principal Component (PC1) related to mice ventilation before and after exposure with TS-LDDMM representations. (a) displays PC densities per mice genotype, (b) illustrates deformations of the reference respiratory cycle ${\\bf c}_{0}$ along PC1, and (c) displays all respiratory cycles with respect to time in PC1 and PC3 coordinates ", "page_idx": 25}, {"type": "image", "img_path": "JM0IQSliol/tmp/2f1d8d47a4baaeb6aef62ca56a431cdea01c9e49d2ec6a9004f5ac91b366e4a9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 10: Analysis of the first Principal Component (PC1) related to mice ventilation before and after exposure with shape-FPCA representations. (a) displays PC densities per mice genotype, (b) illustrates deformations of the reference respiratory cycle ${\\bf c}_{0}$ along PC1, and (c) displays all respiratory cycles with respect to time in PC1 and PC2 coordinates ", "page_idx": 25}, {"type": "text", "text": "principal components analysis by following the procedure described in [60]. Shape-FPCA does not handle multivariate data, and we only kept the nasal airflow for this method. ", "page_idx": 25}, {"type": "text", "text": "K.2 Addiotinal results ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Figure 9 presents results for TS-LDDMM and Figure 10 presents results for Shape-FPCA. The main components look similar. However, a subtle difference, yet important, can be noticed. With Shape-FPCA, the deformation tends to be a uniform time scaling, whereas, with TS-LDDMM, the time dilatation mainly occurs during the pause between inspiration and expiration. Qualitatively, this last deformation ftis the physiological phenomenon: Mice\u2019s muscles cannot relax after exposure to the irritant molecule, leading to pauses between inspiration and expiration [41]. Qualitatively, contrary to Shape-FPCA, which manages to represent the main phenomena in the data, the deformations of TS-LDDMM capture subtle physiological behaviors essential for understanding the phenomenon at hand. ", "page_idx": 25}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Each claim in the introduction is referring to the part where it is tackled. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 25}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: We have provided a special section for this purpose Section 7. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 26}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Justification: All the proofs are given in Appendix B and each proof state all its arguments in a logical order. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 26}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The optimization methodology is described in Section 4 and all numerical details and protocol are given in Section 5 and in Appendix E.1-G-Appendix H-Appendix I\u2013 Appendix K. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 27}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Justification: Synthetic data can be generated, benchmark data are publicly available (Appendix F.2), but the mouse dataset is not. The code is provided as supplementary material and will be made publicly available if the paper is accepted. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: The experiments setting is clearly presented in Section 5 for the experiments on the mouse dataset. The full details of all experiments are presented in Appendix GAppendix H-Appendix I-E.1. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Justification: Error bars are given for the synthetic experiments, but not for the real experiments due to limited computational budget. There is no randomness in the train/test split but there is randomness in the optimization method (Adabelief), but its impact is not significant on the results. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: This is given at the beginning of Appendix E. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 29}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: Every authors are funded for their work and preserve research integrity. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Justification: A special section is given on this subject Appendix A ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. ", "page_idx": 29}, {"type": "text", "text": "\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 30}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 30}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 30}, {"type": "text", "text": "Justification: There is no risk. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 30}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: All dataset and python library were cited when necessary. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 30}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 30}, {"type": "text", "text": "Justification: The mouse dataset is described in Appendix F.1. Guidelines: ", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets. ", "page_idx": 30}, {"type": "text", "text": "\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 31}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 31}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 31}]