[{"figure_path": "EMstukR5J4/figures/figures_1_1.jpg", "caption": "Figure 1: Pre-trained models are fine-tuned into thousands of model variants and stored in cloud.", "description": "This figure shows the growth trend of the total number of models stored on HuggingFace from March 2022 to March 2024.  It highlights the increasing number of fine-tuned models compared to pre-trained models.  The illustration emphasizes that pre-trained models are fine-tuned to create numerous variants which are then stored in the cloud. This signifies the large storage overhead created by this growing trend, and is the main challenge addressed by the FM-Delta model presented in the paper.", "section": "1 Introduction"}, {"figure_path": "EMstukR5J4/figures/figures_3_1.jpg", "caption": "Figure 2: Difference information between the fine-tuned and pre-trained models.", "description": "This figure shows the cosine similarity between fine-tuned and pre-trained models for four model families (Stable Diffusion, GPT2, Bert-large, ResNet50), the distribution of the weight difference between them for four other models (Pokemon Stable Diffusion, Wikitext103 GPT2, SST2 BERT, FER2013 ResNet50), and the residual matrix of different layers on Wikitext103 GPT2.  The results indicate high similarity between fine-tuned and pre-trained models.", "section": "3 Difference between Fine-tuned and Pre-trained Models"}, {"figure_path": "EMstukR5J4/figures/figures_3_2.jpg", "caption": "Figure 9: Fine-tuning results on different models.", "description": "This figure visualizes the fine-tuning process for four different model families: Stable Diffusion, GPT2, Bert-large-uncased, and ResNet50. For each model family, it presents four sub-figures: (a) Cosine Similarity, (b) Distribution of the Weight Difference, (c) Residual Matrix of GPT-2 on Wikitext103, (d) Fine-tuning different models. These sub-figures show various metrics like cosine similarity between fine-tuned and pre-trained models, distribution of weight differences, and the residual matrix of the model's weight parameters across epochs.  The plots provide insights into how the changes in the model's weights evolve during the fine-tuning process.", "section": "3 Difference between Fine-tuned and Pre-trained Models"}, {"figure_path": "EMstukR5J4/figures/figures_4_1.jpg", "caption": "Figure 4: Most significant bit distribution of the first convolutional-layer delta.", "description": "This figure shows the distribution of the most significant bit (MSB) in the difference between the integer representations of the parameters of a fine-tuned model and its corresponding pre-trained model, specifically focusing on the first convolutional layer. The x-axis represents the MSB position (0-32), and the y-axis shows the count of parameters with that MSB position.  The distribution is heavily skewed towards lower MSB values, indicating that a significant portion of the parameter differences have many leading zeros. This observation directly supports the effectiveness of the FM-Delta compression method, which leverages this bit redundancy by entropy coding the integer delta.", "section": "3 Difference between Fine-tuned and Pre-trained Models"}, {"figure_path": "EMstukR5J4/figures/figures_5_1.jpg", "caption": "Figure 5: The lossless compression workflow of FM-Delta. The FM-Delta scheme (1) maps the two floating-point parameter elements at the same position of fine-tuned and pre-trained models into unsigned integers, and performs integer subtraction to obtain the bit-redundant delta element. Then it (2) regards the sign s and the most significant bit k of delta as symbols. With a quasi-static probability modeler, it encodes the symbols and scales the range to involve raw bits on all delta elements, leading to the compressed fine-tuned model.", "description": "This figure illustrates the workflow of the FM-Delta lossless compression algorithm.  It starts by mapping the floating-point parameters of fine-tuned and pre-trained models into unsigned integers.  Subtraction then yields a bit-redundant delta.  The algorithm then uses range coding to compress the delta further.  The sign and most significant bit of the delta are treated as symbols and encoded using a quasi-static probability model. Finally, the encoded symbols are combined with the remaining raw bits to form the compressed fine-tuned model.", "section": "4 FM-Delta"}, {"figure_path": "EMstukR5J4/figures/figures_8_1.jpg", "caption": "Figure 7: Three metrics over the iteration steps T when fine-tuning GPT-2 on different datasets.", "description": "This figure shows how three metrics (perplexity, Euclidean distance, and compression rate) change as the number of fine-tuning steps (T) increases during the fine-tuning of GPT-2 on five different datasets. The results illustrate how the model's performance, its difference from the pre-trained model, and the effectiveness of the compression method evolve during the training process.", "section": "3 Difference between Fine-tuned and Pre-trained Models"}, {"figure_path": "EMstukR5J4/figures/figures_8_2.jpg", "caption": "Figure 13: End-to-end time under different user bandwidths on GPT-NeoX-20B.", "description": "This figure shows the total time taken for model upload and download under various user bandwidths for the GPT-NeoX-20B model.  The three sub-figures break down the timings: (a) Time for key procedures (loading the pre-trained model, compression, decompression and transfer); (b) Total time for upload; (c) Total time for download.  It demonstrates that FM-Delta achieves similar total times to the non-compressed method when bandwidth is below 800Mbps, and significantly faster download/upload times at higher bandwidths.", "section": "E.6 Extended Time Results"}, {"figure_path": "EMstukR5J4/figures/figures_15_1.jpg", "caption": "Figure 9: Fine-tuning results on different models.", "description": "This figure presents the results of fine-tuning four different models (Stable Diffusion, GPT2, Bert-large-uncased, and ResNet50) on various datasets.  Each subfigure shows the loss and the average parameter element difference (avg_w_distance) between the fine-tuned and pre-trained models over the training epochs. The plots illustrate how the difference between the fine-tuned and pre-trained models changes during the fine-tuning process, providing empirical evidence supporting the paper's claim that this difference grows slowly with the number of fine-tuning steps. This slow growth is a key finding that motivates their proposed lossless compression method.", "section": "3 Difference between Fine-tuned and Pre-trained Models"}, {"figure_path": "EMstukR5J4/figures/figures_15_2.jpg", "caption": "Figure 10: Residual matrix of GPT-2 on Wikitext103.", "description": "This figure shows a heatmap visualization of the residual matrix for different layers of the GPT-2 model trained on the Wikitext103 dataset. The heatmap displays the element-wise difference between the fine-tuned and pre-trained model parameters for each layer. Each cell's color intensity represents the magnitude of the difference, with darker colors indicating larger differences. This visualization helps to understand the distribution of changes in model parameters after fine-tuning, supporting the paper's claim that the difference between fine-tuned and pre-trained models is relatively small.", "section": "3 Difference between Fine-tuned and Pre-trained Models"}, {"figure_path": "EMstukR5J4/figures/figures_20_1.jpg", "caption": "Figure 11: Compression rates of FM-Delta on different model layers.", "description": "This figure visualizes the compression rates achieved by the FM-Delta algorithm across different layers of various neural network models.  The three subfigures show the compression rates for different model types: (a) UNet of Stable Diffusion, (b) Transformer layers of GPT2, and (c) sublayers within a transformer.  Each subfigure plots the compression rate against the layer number, revealing how the effectiveness of the compression technique varies across the layers of a network architecture.  The patterns observed offer insights into the characteristics of different model layers and the suitability of FM-Delta for compressing them.", "section": "E.2 Compression Rates on Different Layers"}, {"figure_path": "EMstukR5J4/figures/figures_21_1.jpg", "caption": "Figure 12: Three metrics over the iteration steps T when fine-tuning GPT-2-1.5B on different datasets.", "description": "This figure shows the perplexity, Euclidean distance, and compression rate during the fine-tuning process of GPT-2-1.5B on five different datasets (PTB, Wikitext2, Wikitext103, LAMBADA, and 1BW).  The x-axis represents the number of fine-tuning steps (T), while the y-axis shows the corresponding metric values for each dataset. The figure illustrates how these metrics change as the model is fine-tuned on different datasets, highlighting the relationship between the number of fine-tuning steps and model performance and compression characteristics. It helps to visualize the model's learning progress and the effectiveness of the FM-Delta compression method across various datasets.", "section": "E.4 Extended Fine-tuning Results"}, {"figure_path": "EMstukR5J4/figures/figures_22_1.jpg", "caption": "Figure 13: End-to-end time under different user bandwidths on GPT-NeoX-20B.", "description": "The figure shows the detailed time for model upload and download under different user bandwidths on <EleutherAI/gpt-neox-20b, KoboldAI/GPT-NeoX-20B-Erebus>. When the user's bandwidth is below approximately 800Mbps, the total time is nearly equivalent to that of the non-compression solution for FM-Delta, and it is significantly reduced for FM-Deltau due to the decreased data transfer volume. When the user's bandwidth exceeds around 800Mbps, the total time is limited by the compression throughput due to the transmission speed being faster than the compression speed (approximately 100MB/s).", "section": "E.6 Extended Time Results"}]