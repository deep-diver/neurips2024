[{"figure_path": "EMstukR5J4/tables/tables_1_1.jpg", "caption": "Table 1: Fine-tuning statistical information in HuggingFace for the six most popular models on different tasks. \"Inactive\" refers to models with less than 10 monthly downloads.", "description": "This table presents the fine-tuning statistics from HuggingFace for six popular model families.  It shows the number of full fine-tuned models, the number of parameter-efficient fine-tuned (PEFT) models, the total size of the models, and the percentage of inactive models (those with fewer than 10 monthly downloads). This data highlights the significant storage overhead imposed by full fine-tuned models on cloud platforms.", "section": "1 Introduction"}, {"figure_path": "EMstukR5J4/tables/tables_4_1.jpg", "caption": "Table 2: Comparison of a certain element value in the ith position of the pre-trained model (wp) and the fine-tuned model (wf) respectively. The delta of the two original element bytes contains a large number of redundant \"0\" bits.", "description": "This table compares the values of a specific element from the pre-trained and fine-tuned models.  It shows the original floating-point values, their integer representations, and the difference between those integer representations.  The key observation is that the integer difference has many leading zeros (redundant \"0\" bits), which motivates the compression scheme in the paper.", "section": "3 Difference between Fine-tuned and Pre-trained Models"}, {"figure_path": "EMstukR5J4/tables/tables_6_1.jpg", "caption": "Table 3: Given a base value 0.001, the most significant bit position r of the integer delta, corresponding to the range intervals of different tuned values.", "description": "This table shows the most significant bit position (r) of the integer delta for different ranges of tuned values, given a base value of 0.001.  The most significant bit position is a key component in the FM-Delta compression algorithm, indicating the number of leading zeros in the difference between the fine-tuned and pre-trained model parameters.  The table helps illustrate the relationship between the magnitude of the difference and the bit redundancy that FM-Delta leverages for compression. This relationship is crucial for the algorithm's efficiency and robustness across a range of fine-tuning scenarios.", "section": "4.1 Algorithm"}, {"figure_path": "EMstukR5J4/tables/tables_7_1.jpg", "caption": "Table 4: Overall compression rates and throughput of six lossless compression schemes on different model families.", "description": "This table presents a comparison of six lossless compression algorithms (LZMA, Gzip, Zlib, FPZip, BZip2, and FM-Delta) applied to seven different pre-trained model families with varying numbers of fine-tuned models.  For each model family and number of fine-tuned models, it shows the original storage size in GB, the storage size after compression using each algorithm in GB, and the compression and decompression throughputs (in MB/s) achieved by each algorithm. The table highlights FM-Delta's superior compression rates compared to the other algorithms, offering significant storage savings with good compression and decompression speed.", "section": "5 Experiments"}, {"figure_path": "EMstukR5J4/tables/tables_8_1.jpg", "caption": "Table 5: Compression rates the five baselines on different objects. The compression rate of FM-Delta is 68%.", "description": "This table presents the compression rate achieved by five different baseline compression methods (LZMA, Gzip, Zlib, FPzip, and Bzip2) when applied to different data representations of fine-tuned and pre-trained model parameters.  The data representations include float parameters, float delta (difference between fine-tuned and pre-trained), integer delta, and unsigned integer delta.  The table highlights that FM-Delta achieves a 68% compression rate on unsigned integer delta.", "section": "5 Experiments"}, {"figure_path": "EMstukR5J4/tables/tables_8_2.jpg", "caption": "Table 6: Compression rates of FM-Delta under three different data types on Bert-large-uncased.", "description": "This table presents the compression rates achieved by the FM-Delta algorithm when applied to the Bert-large-uncased model using three different data types: FP32, FP16, and BF16.  The results show how the compression rate varies depending on the precision of the floating-point numbers used to represent the model parameters. Lower precision generally leads to higher compression rates because there is less information to represent in the smaller number of bits.", "section": "Different Data Types"}, {"figure_path": "EMstukR5J4/tables/tables_8_3.jpg", "caption": "Table 4: Overall compression rates and throughput of six lossless compression schemes on different model families.", "description": "This table presents a comparison of six lossless compression algorithms (LZMA, Gzip, Zlib, FPZip, BZip2, and FM-Delta) applied to seven different model families with varying numbers of fine-tuned models.  For each model family and number of fine-tuned models, the table shows the original storage size, and the storage size after compression using each algorithm. It also provides the average compression and decompression throughput in MB/s for each algorithm. This allows for a comprehensive comparison of the performance of different compression techniques in reducing storage space and maintaining reasonable compression and decompression speeds for massive fine-tuned models.", "section": "5 Experiments"}, {"figure_path": "EMstukR5J4/tables/tables_14_1.jpg", "caption": "Table 8: The number of full fine-tuned and PEFT models in the ten additional model families, along with the proportion of full models on these families.", "description": "This table presents statistics on ten different large language models, showing the counts of fully fine-tuned models and parameter-efficient fine-tuned (PEFT) models for each.  The \"Proportion of Full\" column indicates the percentage of models in each family that are fully fine-tuned, providing insights into the prevalence of fully fine-tuned models compared to PEFT models across different model architectures.", "section": "A Extended Statistics from HuggingFace"}, {"figure_path": "EMstukR5J4/tables/tables_14_2.jpg", "caption": "Table 9: The portion of pre-trained and fine-tuned models in the 10,000 models from HuggingFace, counted in ascending and descending order.", "description": "This table shows the proportion of pre-trained and fine-tuned models among 10,000 models from HuggingFace. The data is divided into two sets: ascending (oldest to newest) and descending (newest to oldest) order to show the trend in model uploads.  The results indicate a significant increase in the number of fine-tuned models over time.", "section": "A Extended Statistics from HuggingFace"}, {"figure_path": "EMstukR5J4/tables/tables_19_1.jpg", "caption": "Table 2: Comparison of a certain element value in the ith position of the pre-trained model (wp) and the fine-tuned model (wf) respectively. The delta of the two original element bytes contains a large number of redundant \"0\" bits.", "description": "This table compares the values of a specific element (at the ith position) in both the pre-trained and fine-tuned models.  It shows the original float values, their integer representations, and the resulting delta. The key observation is the presence of many redundant zeros in the integer delta, highlighting the potential for compression.", "section": "3 Difference between Fine-tuned and Pre-trained Models"}, {"figure_path": "EMstukR5J4/tables/tables_20_1.jpg", "caption": "Table 4: Overall compression rates and throughput of six lossless compression schemes on different model families.", "description": "This table presents a comparison of the compression performance of six different lossless compression algorithms (LZMA, Gzip, Zlib, FPZip, BZip2, and FM-Delta) on seven distinct model families.  For each model family, the table shows the original storage size, the number of fine-tuned models, and the compressed storage size achieved by each algorithm.  Additionally, the average compression and decompression throughput (in MB/s) for each algorithm is provided. The table highlights the significant storage reduction achieved by FM-Delta compared to traditional methods, especially with a larger number of fine-tuned models.", "section": "5 Experiments"}, {"figure_path": "EMstukR5J4/tables/tables_21_1.jpg", "caption": "Table 4: Overall compression rates and throughput of six lossless compression schemes on different model families.", "description": "This table presents a comparison of six lossless compression algorithms (LZMA, Gzip, Zlib, FPZip, BZip2, and FM-Delta) on seven different pre-trained model families (Falcon-40B, GPT-NeoX-20B, GPT-J-6B, GPT-2-124M, Bert-large-uncased-336M, Stable-Diffusion-860M, ResNet50-26M).  For each model family, it shows the original storage size, the number of fine-tuned models used in the experiment, and the storage size after compression using each algorithm. It also shows the average compression and decompression throughput (in MB/s) for each algorithm.  This table highlights the superior compression rate of FM-Delta compared to traditional lossless compression methods for fine-tuned language models.", "section": "5 Experiments"}, {"figure_path": "EMstukR5J4/tables/tables_22_1.jpg", "caption": "Table 1: Fine-tuning statistical information in HuggingFace for the six most popular models on different tasks. \"Inactive\" refers to models with less than 10 monthly downloads.", "description": "This table presents the fine-tuning statistics from HuggingFace for six popular models.  It shows the number of full fine-tuned models and parameter-efficient fine-tuned (PEFT) models for each pre-trained model. The table also indicates the proportion of \"inactive\" models (those with fewer than 10 monthly downloads), highlighting the significant storage overhead caused by inactive, full fine-tuned models.  The table is used to demonstrate the problem FM-Delta seeks to solve, namely the inefficiency of storing numerous, full fine-tuned models that are rarely accessed in the cloud.", "section": "1 Introduction"}]