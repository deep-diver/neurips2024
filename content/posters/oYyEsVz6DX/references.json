{"references": [{"fullname_first_author": "Judy Borowski", "paper_title": "Exemplary natural images explain CNN activations better than state-of-the-art feature visualization", "publication_date": "2021", "reason": "This paper introduces the 2-AFC psychophysics task that the current work builds upon and automates, forming the basis for the machine interpretability score."}, {"fullname_first_author": "Roland S.", "paper_title": "How well do feature visualizations support causal understanding of CNN activations?", "publication_date": "2021", "reason": "This paper extends the 2-AFC task to quantify the impact of interventions and introduces a human interpretability score which the current work seeks to automate."}, {"fullname_first_author": "Roland S.", "paper_title": "Scale alone does not improve mechanistic interpretability in vision models", "publication_date": "2023", "reason": "This paper introduces the ImageNet Mechanistic Interpretability dataset (IMI) used for validation and comparison in the current work."}, {"fullname_first_author": "Stephanie Fu", "paper_title": "DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data", "publication_date": "2023", "reason": "This paper provides the DreamSim image similarity measure used in the automated interpretability metric, a key component of the current work."}, {"fullname_first_author": "Alex Krizhevsky", "paper_title": "ImageNet Classification with Deep Convolutional Neural Networks", "publication_date": "2012", "reason": "This foundational paper in deep learning describes the ImageNet dataset, which is heavily used in the current work for training and testing."}]}