[{"figure_path": "41lovPOCo5/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison between table prompting techniques for LMs. (a) - (d): Data included in the LM prompt (shaded region). (a) Read Table: The LM reads the entire table, which is often infeasible for large tables. (b) Read Schema: The LM reads only the schema, consisting of column names and data types, resulting in a loss of information from the table content. (c) Row-Column Retrieval: Rows and columns are encoded and then selected based on their similarity to the question. Only the intersection of these rows and columns is presented to the LM. It is still infeasible to encode all rows and columns for large tables. (d) Schema-Cell Retrieval (our work): Column names and cells are encoded and retrieved based on their relevance to LM-generated queries about the question. Only the retrieved schema and cells are provided to the LM, enhancing efficiency in both encoding and reasoning. (e) Retrieval results on the ArcadeQA dataset show that TableRAG outperforms other methods in both column and cell retrieval, thereby enhancing the subsequent table reasoning process. The Read Table technique is excluded as reading entire tables is typically infeasible in this context.", "description": "This figure compares different table prompting techniques used with Language Models (LMs) for table understanding tasks. It shows how different methods (Read Table, Read Schema, Row-Column Retrieval, and Schema-Cell Retrieval (TableRAG)) handle the input data and their efficiency. TableRAG, the proposed method, is highlighted for its superior performance in both column and cell retrieval, which leads to better overall table reasoning.", "section": "1 Introduction"}, {"figure_path": "41lovPOCo5/figures/figures_1_2.jpg", "caption": "Figure 1: Comparison between table prompting techniques for LMs. (a) - (d): Data included in the LM prompt (shaded region). (a) Read Table: The LM reads the entire table, which is often infeasible for large tables. (b) Read Schema: The LM reads only the schema, consisting of column names and data types, resulting in a loss of information from the table content. (c) Row-Column Retrieval: Rows and columns are encoded and then selected based on their similarity to the question. Only the intersection of these rows and columns is presented to the LM. It is still infeasible to encode all rows and columns for large tables. (d) Schema-Cell Retrieval (our work): Column names and cells are encoded and retrieved based on their relevance to LM-generated queries about the question. Only the retrieved schema and cells are provided to the LM, enhancing efficiency in both encoding and reasoning. (e) Retrieval results on the ArcadeQA dataset show that TableRAG outperforms other methods in both column and cell retrieval, thereby enhancing the subsequent table reasoning process. The Read Table technique is excluded as reading entire tables is typically infeasible in this context.", "description": "This figure compares different table prompting techniques used with Language Models (LMs) for table understanding. It shows that simply reading the whole table is not feasible for large tables due to context length limits. Reading only the schema loses information, while row/column retrieval is still computationally expensive for large tables. The proposed approach, Schema-Cell Retrieval (TableRAG), only retrieves relevant schema and cells for efficient and accurate processing.", "section": "1 Introduction"}, {"figure_path": "41lovPOCo5/figures/figures_3_1.jpg", "caption": "Figure 2: Workflow of the TableRAG Framework. The table is utilized to build the schema and cell databases. A question is then expanded into multiple schema and cell queries by LMs. These queries are sequentially utilized to retrieve schemas and column-cell pairs. The top K candidates from each query are combined and fed into the LM solver's prompt to answer the question. The pseudocode and an answering example on ArcadeQA can be found in Alg. 1 and Fig. 8 respectively.", "description": "This figure illustrates the workflow of the TableRAG framework.  It starts with a question, which is processed by an LM to generate schema and cell queries. These queries are then used to query pre-built schema and cell databases, retrieving the top K results for each. The retrieved schema and cell information are then combined and sent to a program-aided LM, which uses the information to generate a program to answer the question.  The process is iterative, involving multiple rounds of query generation and retrieval until a final answer is reached. Algorithm 1 and Figure 8 in the paper provide more details.", "section": "3 TableRAG"}, {"figure_path": "41lovPOCo5/figures/figures_4_1.jpg", "caption": "Figure 3: Histogram of the proportion of number of distinct values to number of cells in ArcadeQA and BirdQA. The figure indicates that for most tables, the number of distinct values (D) are much smaller than the number of cells (NM).", "description": "This histogram shows the ratio of the number of distinct values to the total number of cells in tables from the ArcadeQA and BirdQA datasets.  The data demonstrates that in most tables, the number of unique cell values is significantly smaller than the total number of cells. This observation supports the efficiency of TableRAG's cell retrieval method, which only encodes distinct values, significantly reducing the computational cost compared to methods that encode all cells.", "section": "3 TableRAG"}, {"figure_path": "41lovPOCo5/figures/figures_7_1.jpg", "caption": "Figure 4: Performance evaluation of Synthetic Tabfact in varying scales. TableRAG shows consistently superior results, and it decreases gracefully compared to competitive methods.", "description": "This figure shows the performance of different table prompting methods (ReadTable, ReadSchema, RandRowSampling, RowColRetrieval, and TableRAG) on a synthetic dataset derived from TabFact with varying table sizes.  The x-axis represents the synthetic table size, increasing from the original size to 1000. The y-axis shows the accuracy of each method.  TableRAG consistently outperforms other methods across all table sizes, demonstrating its scalability and robustness. The accuracy of other methods decreases significantly with increasing table size, highlighting the scalability challenge of traditional methods when dealing with large tables.", "section": "4.6 Scalability Test on TabFact"}, {"figure_path": "41lovPOCo5/figures/figures_7_2.jpg", "caption": "Figure 5: Impact of varying top retrieval results (K). Different K values influence both prompt length and accuracy. Each point is labeled with its corresponding K value. TableRAG retrieves the top K schema and cell values, RandRowSampling selects K random rows, and RowColRetrieval retrieves K rows and K columns.", "description": "This figure compares the performance of three different table prompting methods (TableRAG, RandRowSampling, and RowColRetrieval) across various prompt lengths, which are directly influenced by the number of top retrieval results (K).  It shows that TableRAG maintains higher accuracy with fewer tokens compared to others.  Increasing K values increase prompt length but do not always correlate with higher accuracy, especially for RandRowSampling and RowColRetrieval, which indicates that TableRAG's selective retrieval strategy is more efficient.", "section": "4.5 Retrieval Performance Analysis"}, {"figure_path": "41lovPOCo5/figures/figures_8_1.jpg", "caption": "Figure 6: Impact of cell encoding budget B. TableRAG retrieves from the B most frequent cells, maintaining robust performance even with a smaller budget. RowColSampling truncates more rows as B decreases, showing greater sensitivity to budget changes and generally underperforming compared to TableRAG.", "description": "This figure shows the impact of the cell encoding budget (B) on the performance of TableRAG and RowColRetrieval on ArcadeQA and BirdQA datasets. TableRAG demonstrates consistent performance across varying budgets, while RowColRetrieval shows a decline in performance with increased budgets. This highlights TableRAG's ability to maintain accuracy even with limited encoding budgets.", "section": "4.6 Scalability Test on TabFact"}, {"figure_path": "41lovPOCo5/figures/figures_8_2.jpg", "caption": "Figure 7: Effect of query expansion. Query expansion consistently improves TableRAG across all scenarios, indicating that it provides better coverage of user intents.", "description": "This figure shows the result of an ablation study on the impact of query expansion on TableRAG's performance.  It compares the accuracy of TableRAG with and without query expansion on two datasets, ArcadeQA and BirdQA, using two different language models, GPT-3.5-turbo and Gemini-Pro.  The results consistently demonstrate that query expansion significantly improves TableRAG's accuracy, highlighting its value in enhancing the model's ability to understand and effectively process user queries. ", "section": "4.6 Scalability Test on TabFact"}, {"figure_path": "41lovPOCo5/figures/figures_12_1.jpg", "caption": "Figure 8: Example of TableRAG in ArcadeQA.", "description": "This figure demonstrates a complete workflow of TableRAG using an example from the ArcadeQA dataset. It starts with a question, \"What is the average price for wallets?\", and shows how TableRAG expands this question into schema and cell queries.  These queries are then used to retrieve relevant information from the schema and cell databases built from the table. Finally, the relevant information is passed to a program-aided Language Model (LM) to generate the final answer. The figure shows a step-by-step breakdown of the process, including the queries generated, the relevant information retrieved, and the actions performed by the LM solver to arrive at the final answer.", "section": "3 TableRAG"}, {"figure_path": "41lovPOCo5/figures/figures_15_1.jpg", "caption": "Figure 9: Prompt of Query Expansion for Schema Retrieval", "description": "This figure shows the prompt used for the schema retrieval part of the TableRAG framework.  The prompt instructs a large language model (LLM) to suggest column names from a large table (described as \"amazon seller order status prediction orders data\") that are relevant to answering the question: \"What is the average price for leather wallets?\" The LLM is instructed to respond with a JSON array of column names, without any further explanation.  This is an example of how TableRAG uses the LLM to generate queries for relevant schema information rather than processing the entire table directly. The expected output is a list of column names (as a JSON array) that likely contain the price of leather wallets.", "section": "3.3 Core Components of TableRAG"}, {"figure_path": "41lovPOCo5/figures/figures_15_2.jpg", "caption": "Figure 10: Prompt of Query Expansion for Cell Retrieval", "description": "This figure shows the prompt and completion for the query expansion of cell retrieval in TableRAG.  The prompt instructs a large language model (LLM) to extract keywords from a hypothetical table about Amazon seller order data that are relevant to answering the question \"What is the average price for leather wallets?\"  The keywords should be categorical rather than numerical and present in the question. The completion provides a JSON array of keywords that the LLM generated.", "section": "3.3 Core Components of TableRAG"}, {"figure_path": "41lovPOCo5/figures/figures_16_1.jpg", "caption": "Figure 8: Example of TableRAG in ArcadeQA.", "description": "This figure shows an example of how TableRAG works on a real-world table from the ArcadeQA dataset. It illustrates the workflow, starting from the question, then expanding it into multiple queries for schema and cell retrieval. The relevant information extracted from the table using these queries is then presented to the program-aided LM solver, which generates the final answer. This example demonstrates TableRAG's ability to efficiently handle complex table-based questions.", "section": "3 TableRAG"}]