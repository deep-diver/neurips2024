[{"type": "text", "text": "PiCO: Peer Review in LLMs based on the Consistency Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Existing large language models (LLMs) evaluation methods typically focus on test  \n2 ing the performance on some closed-environment and domain-specific benchmarks   \n3 with human annotations. In this paper, we explore a novel unsupervised evalua  \n4 tion direction, utilizing peer-review mechanisms to measure LLMs automatically   \n5 without any human feedback. In this setting, both open-source and closed-source   \n6 LLMs lie in the same environment, capable of answering unlabeled questions and   \n7 evaluating each other, where each LLM\u2019s response score is jointly determined   \n8 by other anonymous ones. To obtain the ability hierarchy among these models,   \n9 we assign each LLM a learnable capability parameter to adjust the final ranking.   \n10 We formalize it as a constrained optimization problem, intending to maximize the   \n11 consistency of each LLM\u2019s capabilities and scores. The key assumption behind is   \n12 that high-level LLM can evaluate others\u2019 answers more accurately than low-level   \n13 ones, while higher-level LLM can also achieve higher response scores. Moreover,   \n14 we propose three metrics called PEN, CIN, and LIS to evaluate the gap in aligning   \n15 human rankings. We perform experiments on multiple datasets with these metrics,   \n16 validating the effectiveness of the proposed approach. ", "page_idx": 0}, {"type": "text", "text": "17 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Goodhart\u2019s Law: \u201cWhen a measure becomes a target, it ceases to be a good measure.\u201d ", "page_idx": 0}, {"type": "text", "text": "20 Large language models (LLMs)[11, 2, 12, 43] have achieved remarkable success across a variety   \n21 of real-world applications [54, 32, 36, 52]. With the increasingly widespread application of these   \n22 models, there is an urgent need for an effective evaluation method to ensure that their performance   \n23 and usability meet the growing demands. To assess the ability level of LLMs, a large number of   \n24 evaluation benchmarks have been proposed by using some small and domain-specific datasets with   \n25 human-curated labels, such as MMLU [26], HELM [30], Big-Bench[39], GLUE[45]. However, these   \n26 benchmarks can only measure LLMs\u2019 core capability on a confined set of tasks (e.g. multi-choice   \n27 knowledge or retrieval questions), which fails to assess their alignment with human preference in   \n28 open-ended tasks adequately [16, 28, 34]. On the other hand, these evaluations may suffer from   \n29 benchmark leakage issue, referring that the evaluation data is unknowingly used for model training,   \n30 which can also lead to misleading evaluations [49, 56]. Therefore, blindly improving scores on   \n31 these public benchmarks cannot always yield a large language model that truly satisfies human   \n32 requirements.   \n33 For assessing human preferences, recent studies have focused on building crowdsourced battle   \n34 platforms with human ratings as the primary evaluation metric. Typical platforms include Chatbot   \n35 Arena [55], MT-Bench [55], and AlpacaEval [29]. It constructs anonymous battles between chatbots   \n36 in real-world scenarios, where users engage in conversations with two chatbots at the same time and   \n37 rate their responses based on personal preferences. While human evaluation is the gold standard for   \n38 measuring human preferences, it is exceptionally slow and costly[55]. In addition, adding a new   \n39 LLM to the crowdsourced battle platforms also poses a cold-start issue [15]. Thus, a fundamental   \n40 question arises: can we construct an unsupervised LLMs evaluation system without relying on any   \n41 human feedback?   \n42 Actually, in real human evaluation systems, people build their ability hierarchy based on different   \n43 empirical assumptions. For example, majority voting [22, 10, 40] and rating voting [5] methods   \n44 are widely used during the decision-making process, which are based on the wisdom of the crowds   \n45 [40, 13, 50] and have been proven to lead to better results than that of an individual. Moreover, in   \n46 the established practice of peer-review in academic research, scholars evaluate their academic level   \n47 rankings based on the consistency assumption, i.e., scholars with stronger abilities have stronger   \n48 persuasiveness for evaluating others, and can also obtain higher achievements. This paper attempts to   \n49 explore whether similar phenomena exist in the LLMs evaluation systems.   \n50 In this work, we propose PiCO, a Peer review approach in LLMs based on Consistency Optimization.   \n51 In this setting, LLMs themselves act as \u201creviewers\u201d, engaging in mutual assessments to achieve   \n52 comprehensive, efficient, and performance evaluations without relying on manually annotated data.   \n53 This method aims to address the limitations of existing evaluation approaches and provide insights   \n54 into LLMs\u2019 real-world capabilities. As shown in Figure 1, both open-source and closed-source   \n55 LLMs lie in the same environment and answer the open-ended questions from an unlabeled dataset.   \n56 Then, we construct anonymous answer pairs, while randomly selecting other LLMs as \u201creviewers\u201d to   \n57 evaluate both responses with a learnable confidence weight $w$ . Finally, we employ this weight and   \n58 calculate the response scores $G$ for each LLM based on the weighted joint evaluation. It is worth   \n59 noting that the whole peer-review process works in an unsupervised way, and our goal is to optimize   \n60 the confidence weights that re-rank the LLMs to be closer to human rankings.   \n61 To achieve this, we formalize it as a constrained optimization based on the consistency assumption. We   \n62 maximize the consistency of each LLM\u2019s capability $w$ and score $G$ while adjusting the final ranking   \n63 to align with human preference more closely. The key assumption behind this is that high-level LLM   \n64 can evaluate others\u2019 answers more accurately (confidence) than low-level ones, while higher-level   \n65 LLM can also achieve higher answer-ranking scores. As a result, the entropy (controversy) of the   \n66 whole peer-review evaluation system can be minimized. In other words, the consistency optimization   \n67 aims to find a final score ranking that all LLMs have no \u201cdisputes\u201d regarding.   \n68 To evaluate the gap in aligning human rankings, we propose three metrics called PEN (Permutation   \n69 Entropy), CIN (Count Inversions), LIS (Longest Increasing Subsequence). The experiments are   \n70 conducted on multiple crowdsourcing datasets and validated on these three metrics. The experimental   \n71 results demonstrate that the proposed PiCO framework can effectively obtain a large language models   \n72 leaderboard closer to human preferences. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "image", "img_path": "uNZpvFlsg9/tmp/d9f20d2f618d0b833a0436c05680db38c23850df3d68c28862051fe22800aa3f.jpg", "img_caption": ["Figure 1: The framework of PiCO. In this framework, both open-source and closed-source LLMs lie in the same environment, capable of answering unlabeled questions and evaluating each other, where each LLM\u2019s response score is jointly determined by other anonymous ones. We assign each LLM a learnable capability weight to optimize the score ranking based on the consistency assumption, while reducing the entropy of the peer-review evaluation system. The consistency optimization aims to find a final score ranking that all LLMs \u201cagree\u201d it. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "uNZpvFlsg9/tmp/793f73b98180137e485acbdd5ff475fab4db360aebea974b252b91b0d0c6bcdd.jpg", "img_caption": ["Figure 2: Preference alignment metric. Three metrics for evaluating the gap with human preferences called PEN, CIN, and LIS, respectively "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "73 The contributions of this paper can be summarized as follows. ", "page_idx": 2}, {"type": "text", "text": "74 \u2022 We explore a novel unsupervised LLM evaluation direction without human feedback, uti  \n75 lizing peer-review mechanisms to measure LLMs automatically. All LLMs can answer   \n76 unlabeled questions and evaluate each other.   \n77 \u2022 A constrained optimization based on the consistency assumption is proposed to re-rank the   \n78 LLMs to be closer to human rankings.   \n79 \u2022 We propose three metrics called PEN, CIN, and LIS on the PiCO framework for evaluating   \n80 the gap with human preferences.   \n81 \u2022 The experiments with these metrics on three crowdsourcing datasets validate the effective  \n82 ness of the proposed approach. ", "page_idx": 2}, {"type": "text", "text": "83 2 The Proposed Approach ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "84 In this section, we first describe the problem definition and preference alignment evaluation, and then   \n85 introduce the proposed PiCO framework in detail. ", "page_idx": 2}, {"type": "text", "text": "86 2.1 Definition and Metrics ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "87 Problem Definition. In this subsection, we aim to measure the ability level of LLMs automatically   \n88 without relying on human annotations. Thus we consider an unsupervised LLM evaluation scenario   \n89 with an unlabeled dataset $\\mathcal{Q}$ consisting of $n$ open-ended questions, where $\\mathcal{Q}=\\{Q_{i}\\}_{i=1}^{n}$ . In addition,   \n90 we have a large language model pool $\\mathcal{M}=\\{M_{j}\\}_{j=1}^{m}$ , which includes both open-source and closed  \n91 source models. Write $M_{1}\\succ M_{2}$ to indicate that the LLM $M_{1}$ has stronger capabilities than the LLM   \n92 $M_{2}$ . Thus, we can assume that the ground-truth ranking $\\mathcal{R}^{*}$ alignment with human preferences, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{R}^{*}:=[M_{1}\\succ M_{2}\\succ M_{3}\\succ...\\succ M_{m}],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "93 and assume that the learned ranking $\\hat{\\mathcal{R}}$ by different evaluation methods is as follows, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{R}}:=[M_{3}\\succ M_{1}\\succ M_{2}\\succ...\\succ M_{m}].\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "94 The goal is to build an LLM ranking $\\hat{\\mathcal{R}}$ that aligns with human ranking $\\mathcal{R}^{*}$ , making the loss $\\mathcal{L}$ of the   \n95 both rankings tend towards 0, i.e., $\\mathcal{L}(\\hat{\\mathcal{R}},\\mathcal{R}^{*})\\to0$   \n96 Preference Alignment Metrics. Before building LLM rankings, we first need to discuss how to   \n97 evaluate aligned human rankings. Intuitively, the metrics we want mainly describe the differences   \n98 between two arrays composed of ranking indices. Assuming that human ranking $\\mathcal{R}^{*}$ is defined as   \n99 being well-ranked in ascending order $([1,2,3,...,m])$ as shown in Eq 1. Thus the metric is to quantify   \n100 the randomness of the learned ranking array $([3,1,2,...,m])$ as shown in $\\operatorname{Eq}\\,2$ . Based on this, we   \n101 propose three metrics called PEN, CIN, and LIS, respectively.   \n102 PEN (Permutation Entropy). Permutation entropy [8] is a concept used to quantify the complexity or   \n103 randomness of time series data. It provides a measure of the irregularity or unpredictability of the   \n104 order of values in a sequence. We thus utilize it to measure the gap with human rankings as follows, ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{L}_{P E N}(\\hat{\\mathcal{R}},\\mathcal{R}^{*}):=-\\sum p(\\pi)\\log p(\\pi),\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "105 where ", "page_idx": 2}, {"type": "equation", "text": "$$\np(\\pi)=\\frac{\\#\\{t|0\\leq t\\leq m-k,(M_{t+1},...,M_{t+k})\\in\\pi\\}}{m-k+1}.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "image", "img_path": "uNZpvFlsg9/tmp/6d7c7edaff70c4a59e24c5439a5d342dedd0b2b38c2abbe5c8d107226df95e1c.jpg", "img_caption": ["Figure 3: The pipeline of the PiCO. It is mainly composed of two components: the peer-review and consistency optimization stages. Specifically, in the peer-review stage, the unlabeled dataset $\\mathcal{Q}$ and the LLMs pool $\\mathcal{M}$ are given. Then, we let all LLMs answer each unlabeled question to obtain the response set $\\mathcal{A}$ . We shuffle the set and construct anonymous answer pairs, while randomly selecting other LLMs to evaluate both responses with a learnable confidence $w$ . As a result, we can obtain the answer-ranking data $\\mathcal{D}$ which is a quadruple that records the partial order between two answers and the evaluator\u2019s confidence weight. In the consistency optimization stage, we update the parameter $w$ by maximizing the consistency of each LLM\u2019s capability and score, while re-ranking the LLMs to be closer to human rankings. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "106 $\\pi$ denotes different permutations, $k$ is a hyper-parameter recommended to be set to 3 to 7, and we   \n107 set $k=3$ in this paper. Intuitively, it samples some subsequences and calculates the entropy for all   \n108 permutation types. And the lower the permutation entropy in the learned LLM rankings, the closer it   \n109 is to the ground-truth human rankings.   \n110 CIN (Count Inversions). Counting inversions [27] aims to measure the degree of disorder or   \n111 \"invertedness\" in an array or sequence of elements. We thus define it as follows, ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{L}_{C I N}(\\hat{\\mathcal{R}},\\mathcal{R}^{*}):=\\sum_{M_{i},M_{j}\\sim\\mathcal{M}}\\mathbf{1}\\{M_{i}\\succ M_{j}\\land i<j\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "112 Where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function that the value is 1 when the condition is met, otherwise it is 0.   \n113 Intuitively, the fewer inverse pairs in the learned LLM rankings, the closer it is to the ground-truth   \n114 human rankings.   \n115 LIS (Longest Increasing Subsequence). The longest increasing subsequence aims to find the length   \n116 of the longest subsequence in a given sequence of elements, where the subsequence is in increasing   \n117 order. We utilize it to measure the degree of match with human rankings as follows, ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{L}_{L I S}(\\hat{\\mathcal{R}},\\mathcal{R}^{*}):=\\operatorname*{max}\\left\\{d p[i]\\ |\\ 1\\le i\\le m\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "118 where ", "page_idx": 3}, {"type": "equation", "text": "$$\nd p[i]=1+\\operatorname*{max}\\left\\{d p[j]\\mid1\\leq j<i\\wedge M_{j}\\prec M_{i}\\right\\}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "119 $d p[i]$ represents the length of the longest increasing subsequence that ends with $M_{i}$ . LIS allows for   \n120 a nuanced understanding of the degree to which the learned ranking aligns with the ideal human   \n121 ranking, with a higher LIS length indicating greater alignment. ", "page_idx": 3}, {"type": "text", "text": "122 2.2 Algorithm Details ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "123 The PiCO framework, depicted in Figure 3, involves peer-review and consistency optimization stages.   \n124 In the peer-review stage, we first collect an unlabeled dataset $\\mathcal{Q}$ consisting of open-ended questions,   \n125 and construct a large language model pool $\\mathcal{M}$ that includes both open-source and closed-source   \n126 LLMs. Then, we let all LLMs answer each unlabeled question to obtain the response set $\\boldsymbol{\\mathcal{A}}$ . We   \n127 shuffle the set and construct anonymous answer pairs, while randomly selecting other LLMs as   \n128 \u201creviewers\u201d to evaluate both responses with a learnable confidence $w$ . Finally, we can obtain the   \n129 answer-ranking data $\\mathcal{D}$ and calculate the response score $G$ for each large language model. In the   \n130 consistency optimization phase, we maximize the consistency of each LLM\u2019s capability $w$ and score   \n131 $G$ with constrained optimization, while re-ranking the LLMs to be closer to human rankings. ", "page_idx": 3}, {"type": "text", "text": "132 2.2.1 Peer Review Stage ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "133 Data Collection and LLMs Pool Construction. Benefiting from the creation of crowdsourced   \n134 battle platforms, we accessed open assessment datasets from Chatbot Arena[55], MT-Bench[55],   \n135 and AlpacaEval[29]. These open datasets include critical fields such as \"question_id\" and   \n136 \"question_content.\" Utilizing the Chatbot Arena dataset, which features pairwise data from twenty   \n137 LLMs with human preference annotations, we assembled an LLM pool $\\bar{\\mathcal{M}}=\\{M_{j}\\}_{j=1}^{m}$ . Leveraging   \n138 33K human-annotated interactions from this dataset, we established a ground-truth ranking $\\mathcal{R}^{*}$ and   \n139 gathered responses $\\mathcal{A}=\\{\\{A_{i}^{j}\\}_{i=1}^{n}\\}_{j=1}^{m}$ for our dataset $\\mathcal{Q}=\\{Q_{i}\\}_{i=1}^{n}$ .   \n140 Answer-Ranking Data Construction Based on Peer Review. After obtaining the responses set $\\boldsymbol{\\mathcal{A}}$ ,   \n141 we aim to generate answer-ranking data $\\mathcal{D}$ through the peer-review mechanism. Specifically, for the   \n142 same question $Q_{i}\\in\\mathcal{Q}$ , we randomly construct a battle pair $<A_{i}^{j},A_{i}^{k}>$ for review. Each battle pair   \n143 will be randomly assigned five models (\u201creviewers\u201d) to determine the winners or declare ties. Note   \n144 that the model may evaluate its own answers, but the entire process is anonymous. As a result, we   \n145 can obtain the quadruples $(A_{i}^{j},A_{i}^{k},>w^{s})$ , indicating the \u201creviewer\u201d $M_{s}$ believes that the answer $A_{i}^{j}$   \n146 is better than answer $A_{i}^{k}$ with a confidence $w^{s}$ . Therefore, the answer-ranking data $\\mathcal{D}$ can be defined   \n147 as follows, ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{D}=\\left\\{(A_{i}^{j},A_{i}^{k},>,w^{s})\\right\\}_{i\\sim\\mathcal{Q},j,k,s\\sim\\mathcal{M}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "148 where $i$ denotes the question index, and $j,k,s$ indicate the model indices. $w^{s}$ is a learnable confidence   \n149 of model $M_{s}$ , and $>$ is a partial order relationship from $\\{>,<,=\\}$ . ", "page_idx": 4}, {"type": "text", "text": "150 2.2.2 Consistency Optimization Stage ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "151 As shown in $\\operatorname{Eq}6$ , following the peer-review mechanism, we construct anonymous answer pairs and   \n152 randomly select other LLMs as \u201creviewers\u201d to evaluate both responses with a learnable confidence $w$ .   \n153 Next, we expect to optimize the confidence $w$ and re-rank the LLMs to be closer to human rankings.   \n154 We thus propose the consistency assumption, i.e., high-level LLM can evaluate others\u2019 answers   \n155 more accurately (confidence) than low-level ones, while higher-level LLM can also achieve higher   \n156 answer-ranking scores. Formally, we maximize the consistency of each LLM\u2019s capability $w$ and   \n157 score $G$ with constrained optimization as follows, ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\operatorname{argmax}\\operatorname{Consistency}(G,w)}}\\\\ &{}&{\\mathrm{s.t.~}G_{j}=\\sum_{(A_{i}^{j},A_{i}^{k},>,w^{s})\\sim\\mathcal{D}}\\mathbf{1}\\{A_{i}^{j}>A_{i}^{k}\\}*w^{s},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "158 where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function that the value is 1 when the condition is met, otherwise, it is 0.   \n159 $G_{j}$ denotes the response score of model $M_{j}$ , which is calculated by joint evaluation of other models.   \n160 Moreover, we employ Pearson correlation [38] to measure the consistency between $w$ and $G$ . Note   \n161 that we only introduce this straightforward implementation to validate our idea of PiCO. Other more   \n162 advanced strategies may be employed to further improve the performance.   \n163 Discussion: It is worth noting that the whole process (Eq. 6 and 7) works in an unsupervised way.   \n164 The only thing we do is to adaptively assign each LLM a score that matches its abilities. An intuitive   \n165 example is as follows: in a real peer-review system, if the academic level of three scholars $a,b$ , and $c$   \n166 satisfies the following relationship, $w^{a}>w^{\\stackrel{.}{b}}>w^{c}$ . So, in the ultimate ideal scenario, the ranking   \n167 of the scores submitted by these three scholars should also be, $G_{a}>G_{b}>G_{c}$ . In other words, the   \n168 sorting of $G$ and $w$ satisfies high consistency. On the other hand, scholars with stronger abilities $(i.e.,$ ,   \n169 scholar $a$ ) evaluate $A^{b}>A^{c}$ have stronger persuasiveness, so scholar $b$ should also receive higher   \n170 weighted scores $1*w^{a}$ .   \n171 Reviewer Elimination Mechanism. Realizing that not all LLMs have sufficient ability to evaluate   \n172 the responses of other models. We thus introduce an unsupervised elimination mechanism to remove   \n173 those LLMs that have low scores. It iteratively removes the lowest-scoring LLM from the \u201creviewer   \n174 queue\u201d for the next consistency optimization stage, until $60\\%$ of models are eliminated. The whole   \n175 process of the approach is summarized in Algorithm 1, and the details can be found in Appendix D. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "176 3 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "177 Datasets. To validate the effectiveness of the proposed approach, we perform experiments on Chatbot   \n178 Arena[55], MT-Bench[55], and AlpacaEval[29]. The MT-Bench dataset assesses six LLMs\u2019 responses   \n179 to 80 multi-category questions. The Chatbot Arena Conversations Dataset, with 33K conversations   \n180 from 13K IPs during April-June 2023, evaluates real dialogue performance. AlpacaEval dataset   \n181 integrates 805 evaluations from diverse tests (e.g., Self-Instruct[48], OASST, Anthropic\u2019s helpful[7],   \n182 Vicuna[16] and Koala[25] test sets) to align evaluations real-world interactions[21]. These datasets   \n183 are collected by crowdsourcing platforms from human feedback, so they have a ground-truth ranking   \n184 LLMs $\\mathcal{R}^{*}$ aligned with human preferences.   \n185 LLMs Pool. In our experiments, we employ 15 LLMs with diverse architectures to construct the   \n186 LLMs pool, including GPT-3.5-Turbo[35], WizardLM-13B[51], Guanaco-33B[1], Vicuna-7B[16],   \n187 Vicuna-13B[16], Koala-13B[24], Mpt-7B[42], gpt4all-13B[6], ChatGLM-6B[53], Oasst-sft-4-pythia  \n188 12B[19], FastChat-T5-3B[55], StableLM-7B[3], Dolly-12B[18], LLaMA-13B[43], Alpaca-13B[41].   \n189 All models use the same evaluation template, they can be found in Appendix B   \n190 Baselines. To validate the effectiveness of the proposed PiCO approach, we compare the following   \n191 methods in the experiments.   \n92 \u2022 The wisdom of the crowds: The two methods that perform LLMs evaluation based on the   \n93 wisdom of the crowds [40, 13, 50] are compared in this experiment. 1) Majority Voting   \n4 [40]: Multiple review models vote for the better answer for the same response pair, and the   \n95 model with the most votes gets 1 score; 2) Rating Voting [5]: Multiple review models also   \n6 vote on the same response pair, and the number of votes obtained is the score.   \n7 \u2022 State-of-the-art methods: The four recent SOTA methods of using either single or multiple   \n98 models for self-evaluation are compared in this experiment. PandaLM[46]: It is a fine-tuned   \n9 language model based on Llama-7b designed for the preference judgment tasks to evaluate   \n0 and optimize LLMs. GPTScore[23]: It employs generative pre-trained models to assess the   \n01 quality of generated text. It calculates the likelihood that the text was generated in response   \n02 to specific instructions and context, indicative of high quality. In our implementation, GPT-3   \n(davinci-002) and flan-t5-xxl serve as the base models. PRD[28]: It transforms the LLMs   \n4 win rates into weights for competitive ranking, while evaluating each LLM based on its   \n05 preference for all possible pairs of answers, enabling a tournament-style ranking system.   \n6 PRE[17]: It employs a supervised process to evaluate LLMs using a qualification exam,   \naggregates their scores based on accuracy, and assigns weights accordingly. PiCO (Ours):   \n08 the proposed approach in this paper.   \n209 Metrics. For all experiments, we employ three metrics to evaluate the aforementioned experimental   \n210 setups and our Peer Review method: PEN, CIN, and LIS. Moreover, we perform the experiments for   \n211 4 runs and record the average results over 4 seeds $'s e e d=1,2,3,4,$ . ", "page_idx": 4}, {"type": "table", "img_path": "uNZpvFlsg9/tmp/c480a4b108cc667f147eadc5913fb5532deda209992814ded19274e5c0366f9c.jpg", "table_caption": ["Table 1: Comparison of all methods on three datasets under data volumes of 1, 0.7 and 0.4, where the top value is highlighted by blod font. Lower PEN and CIN scores indicate better performance, while a higher LIS score signifies improved performance. "], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "image", "img_path": "uNZpvFlsg9/tmp/bb1156b0423b921fe2839df991b9cc2bc4b6ed6ee6f0649796412d6ba71e3419.jpg", "img_caption": ["Figure 4: Heatmap distribution of preference gap (PG) metric among seven LLMs across three datasets. Higher values (above 0) indicate greater evaluation bias[17]. The first row shows original PG values in three datasets, while the second row displays PG values re-weighted using our learned confidence weights. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "212 3.1 Performance Comparison ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "213 We validate the effectiveness of the proposed PiCO method on three datasets by comparing the   \n214 following two types of methods, i.e., the wisdom of the crowds and recent SOTA LLMs evaluation   \n215 methods. The average results of PEN, CIN and LIS are demonstrated in Table 1. The ratios of   \n216 response sets $\\mathcal{D}$ are 1, 0.7, and 0.4, respectively.   \n217 The results presented in Table 1 illustrate the proposed PiCO method consistently surpasses com  \n218 peting approaches across the majority of evaluated metrics Notably, PiCO achieves performance   \n219 improvements of 0.1, 2.5, and 0.92 on the PEN, CIN, and LIS metrics, respectively, compared to the   \n220 Runner-up. These results underscore the superiority of aggregating evaluations from multiple models,   \n221 such as Majority Voting, Rating Voting, PRD, and PRE, as opposed to relying solely on single-model   \n222 methods like GPTScore and PandaLM. This collective model approach, leveraging \u2019the wisdom of   \n223 the crowds\u2019, more accurately aligns with human rankings in our open-question evaluation framework.   \n224 In comparison with existing peer review evaluation methods(i.e., PRD and PRE), it is evident that   \n225 PiCO exhibits improvements across various evaluation metrics. Despite PRD\u2019s adjustment of model   \n226 weights based on their win rates and PRE\u2019s reliance on supervised human feedback data to assign   \n227 weights through a qualification exam, neither method achieves performance superior to the fully   \n228 unsupervised PiCO approach. These methods rely on predefined criteria and human feedback,   \n229 potentially leading to biases or suboptimal performance. In contrast, PiCO leverages unsupervised   \n230 learning techniques, allowing it to autonomously adapt and discover patterns in the data without   \n231 explicit human intervention.   \n232 It is important to highlight that PandaLM, a language model equipped with 7 billion parameters, was   \n233 fine-tuned using labels generated by GPT-3.5-turbo as the ground truth, achieving stable performance   \n234 across various datasets. However, in our unsupervised, open-ended experimental setup, which focuses   \n235 on ranking-based metrics, GPTScore exhibits less robustness regardless of whether the base model is   \n236 GPT-3 (davinci-002) or flan-t5-xx. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "237 3.2 Exploring the Role of Confidence Weight ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "238 In this subsection, we will show that the confidence weight $w$ learned by our consistency optimization   \n239 can reduce the system evaluation bias. Specifically, we first study whether the \u201creview\u201d model would   \n240 prefer a particular model\u2019s response. Following [17], we employ the preference gap (PG) to evaluate   \n241 the bias as follows, (8) ", "page_idx": 6}, {"type": "image", "img_path": "uNZpvFlsg9/tmp/fc7955d5a371d7e270c469bbbfd7d9eb42f1162603d5d5474485979b7f47e3e9.jpg", "img_caption": ["Figure 5: Performance comparison of the PiCO (Ours) and PRE[17] methods on the Chatbot Arena, MT-Bench, and AlpacaEval datasets, with the number of eliminated reviewers on the $\\mathbf{X}$ -axis. The y-axis is CIN, where lower values indicate better performance. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\nP G(i,j)=P_{i}(i>j)-P_{j}(i>j),\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "242 where $P_{i}(i>j)$ represents the winning rate of model $i$ as the \u201creviewer\u201d believes that $i$ defeated   \n243 $j$ . The heatmap distribution of the PG value $P G(i,j)$ among seven LLMs across three datasets is   \n244 demonstrated in the first row of Figure 4. It can be observed that the evaluation system exhibits severe   \n245 bias. Especially on ChatGLM-6B and Mpt-7B models, they often believe that their results are better   \n246 than other ones, as their PG values are greater than 0 across three datasets.   \n247 After the consistency optimization, we assign the learned confidence weight $w$ to the corresponding   \n248 model and ultimately obtain the re-weighting PG value $\\hat{P G}(i,j)$ as follows, ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "equation", "text": "$$\n\\hat{P G}(i,j)=w_{i}\\times P_{i}(i>j)-w_{j}\\times P_{j}(i>j).\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "249 The results of the re-weighting PG value $\\hat{P G}(i,j)$ are displayed on the second row of Figure 4. It can   \n250 be observed that the learned confidence weight $w$ can significantly mitigate the preference gaps of the   \n251 whole evaluation system. In our consistency optimization, LLMs such as ChatGLM-6B and Mpt-7B   \n252 have lower weights, and reducing their confidence can effectively alleviate the system evaluation bias. ", "page_idx": 7}, {"type": "text", "text": "253 3.3 Study of Elimination Mechanism ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "254 The PiCO and PRE[17] methods both employ elimination mechanisms to remove those weakest   \n255 LLMs from the \u201creviewer queue\u201d during the evaluation process. As shown in Figure 5, the $\\mathbf{X}$ -axis   \n256 quantifies the number of reviewers eliminated, and the y-axis measures the CIN, where lower scores   \n257 denote higher performance. Due to space limitations, more results on PEN and LIS metrics can be   \n258 found in Appendix E. It can be observed that both PiCO and PRE exhibit better performance with   \n259 an increasing number of eliminated \u201creviewers\u201d. The proposed PiCO approach can achieve better   \n260 performance than PRE in most cases. It is worth noting that the PRE method employs the accuracy   \n261 of \u201cqualification exams\u201d to eliminate weak LLMs, and this process requires human annotation [17].   \n262 On the contrary, the elimination process of our PiCO method is unsupervised and can still achieve   \n263 better evaluation results than PRE. ", "page_idx": 7}, {"type": "text", "text": "264 3.4 Validation of Consistency Assumption ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "265 In this subsection, we conduct the ablation study to validate the effectiveness of the consistency   \n266 assumption. Specifically, we first manually construct three methods: Forward Weight Voting,   \n267 Uniform Weight Voting, and Reverse Weight Voting. That is, the ability weights of the model are   \n268 respectively weighted forward $(w\\,=\\,[1,0.9,...,0])$ , uniformly $(w\\,=\\,[1,1,...,1])$ , and backward   \n269 $(w\\stackrel{.}{=}[0,0.\\dot{1},...,\\dot{1}])$ according to the ground-truth human ranking. Then, we randomly initialize the   \n270 ability weights and employ our consistency optimization to adjust the weight. In addition, we also   \n271 collect the average performance of \u201creviewer queue\u201d, i.e., employing a single LLM as the \u201creviewer\u201d   \n272 to evaluate all response pairs and then calculate the average results of all LLMs.   \n273 As shown in Table 2, it can be observed that the Forward Weight Voting achieves better results than   \n274 the Uniform and Backward ones in all cases, while the Backward one achieves worse results. It   \n275 validates that assigning larger weights to those models with stronger capabilities can obtain better   \n276 results. Most importantly, employing our consistency optimization algorithm to assign weights to   \n277 different review models can further improve the performance of the evaluation system, i.e., lower PEN   \n278 and CIN, as well as higher LIS in all cases. Moreover, it is worth noting that the average performance   \n279 of the \u201creviewer queue\u201d is very poor, even worse than the Backward Weight Voting. This means   \n280 that the answer-ranking data $\\mathcal{D}$ contains a lot of evaluation noise, while the proposed approach can   \n281 still optimize weights and obtain better ranking results. In summary, the above experimental results   \n282 validate the effectiveness of the consistency assumption from various perspectives. ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "uNZpvFlsg9/tmp/a3a35b0d39a06fab1a9344f85888877dcae6413fd4ef6ab9f17ecba0ff379ca6.jpg", "table_caption": ["Table 2: Ablation study comparing Backward, Uniform, Forward weight voting, and Consistency Optimization methods with the Average Performance of Reviewer Queue across three datasets. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "283 4 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "284 Evaluation Benchmarks for Diversity. LLMs are designed to handle a variety of tasks, necessitat  \n285 ing comprehensive benchmarks[15]. Notable benchmarks include GLUE[45] and SuperGLUE[44],   \n286 which simulate real-world scenarios across tasks such as text classification, translation, reading   \n287 comprehension, and dialogue generation. HELM[30] provides a holistic evaluation of LLMs, as  \n288 sessing language understanding, generation, coherence, and reasoning. BIG-bench[39] pushes LLM   \n289 capabilities with 204 diverse tasks. MMLU[26] measures multitask accuracy across domains like   \n290 mathematics and law. However, these evaluations can be compromised by benchmark leakage, where   \n291 evaluation data inadvertently used for training leads to inflated performance metrics[4, 56].   \n292 Human Evaluation. Human evaluation provides reliable feedback that closely aligns with real  \n293 world applications[15]. Liang et al.[30] evaluated summary and misinformation scenarios across   \n294 multiple models. Ziems et al.[57] involved experts to assess model outputs in various domain-specific   \n295 tasks. Bang et al.[9] examined ChatGPT\u2019s performance in summarization, translation, and reasoning   \n296 using human-annotated datasets. The LMSYS initiative introduced platforms like Chatbot Arena[55],   \n297 relying on human ratings as the primary evaluation metric. Despite its effectiveness, human evaluation   \n298 is costly and subject to bias and cultural differences[37].   \n299 Large Language Models for Evaluation. The development of open-source LLMs has led to the   \n300 use of LLMs as evaluators. GPTScore[23] uses models like GPT-3 to assign probabilities to high  \n301 quality content through multidimensional evaluation. Bubeck et al.[12] tested GPT-4, finding it   \n302 rivaling human capabilities. Lin and Chen introduced LLM-EVAL[31] for evaluating dialogue quality   \n303 with single prompts. PandaLM[46] employs LLMs as \"judges\" for evaluating instruction tuning.   \n304 However, reliance on a single model can introduce biases such as positional[20], verbosity[47], and   \n305 self-favoring biases[33, 55]. ChatEval[14] proposes a multi-agent framework to simulate human   \n306 evaluation processes. Similarly, PRE[17] and PRD[28] use LLMs as evaluators, combining multiple   \n307 evaluation outcomes for automated assessment. However, the PRE method, which relies on human   \n308 feedback for supervised evaluation throughout the process, still incurs relatively high costs. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "309 5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "310 In this paper, we propose the novel Peer Review method based on the Consistency Optimization   \n311 (PiCO) to automatically evaluate Large Language Models (LLMs) without relying on human feedback.   \n312 PiCO utilizes peer-review mechanisms to autonomously assess LLMs in a shared environment, where   \n313 both open-source and closed-source models can respond to unlabeled questions and evaluate each   \n314 other. In this setup, each LLM\u2019s response score is determined collectively by other anonymous   \n315 models, aiming to maximize consistency across capabilities and scores. We propose three metrics,   \n316 i.e., PEN, CIN, and LIS, to quantify the disparity from human preferences. The extensive experiment   \n317 results across multiple datasets and metrics demonstrate that PiCO effectively generates an LLM   \n318 leaderboard that aligns closely with human preferences. In the future, we plan to extend the peer  \n319 review mechanism to evaluate the capabilities of multi-modality large models. ", "page_idx": 8}, {"type": "text", "text": "320 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "321 [1] Guanaco - generative universal assistant for natural-language adaptive context-aware omnilin  \n322 gual outputs. https://guanaco-model.github.io/, 2023. Accessed: 15 April 2024.   \n323 [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni   \n324 Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4   \n325 technical report. arXiv preprint arXiv:2303.08774, 2023.   \n326 [3] Stability AI. Stablelm-tuned-alpha-7b: A fine-tuned language model for diverse applications.   \n327 https://huggingface.co/stabilityai/stablelm-tuned-alpha-7b, 2023. Accessed:   \n328 15 April 2024.   \n329 [4] Rachith Aiyappa, Jisun An, Haewoon Kwak, and Yong-Yeol Ahn. Can we trust the evaluation   \n330 on chatgpt?, 2023.   \n331 [5] Mohammad Allahbakhsh and Aleksandar Ignjatovic. Rating through voting: An iterative   \n332 method for robust rating. arXiv preprint arXiv:1211.0390, 2012.   \n333 [6] Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar.   \n334 Gpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo.   \n335 https://github.com/nomic-ai/gpt4all, 2023.   \n336 [7] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn   \n337 Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless   \n338 assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862,   \n339 2022.   \n340 [8] Christoph Bandt and Bernd Pompe. Permutation entropy: a natural complexity measure for   \n341 time series. Physical review letters, 88(17):174102, 2002.   \n342 [9] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Love  \n343 nia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. A multitask, multilingual, multimodal evaluation   \n344 of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023,   \n345 2023.   \n346 [10] Robert S Boyer and J Strother Moore. Mjrty\u2014a fast majority vote algorithm. In Automated   \n347 reasoning: essays in honor of Woody Bledsoe, pages 105\u2013117. Springer, 1991.   \n348 [11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,   \n349 Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are   \n350 few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.   \n351 [12] S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece   \n352 Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general   \n353 intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.   \n354 [13] David V Budescu and Eva Chen. Identifying expertise to extract the wisdom of crowds.   \n355 Management science, 61(2):267\u2013280, 2015.   \n356 [14] Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu,   \n357 and Zhiyuan Liu. Chateval: Towards better llm-based evaluators through multi-agent debate.   \n358 arXiv preprint arXiv:2308.07201, 2023.   \n359 [15] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen,   \n360 Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large language   \n361 models. ACM Transactions on Intelligent Systems and Technology, 2023.   \n362 [16] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,   \n363 Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot   \n364 impressing gpt-4 with $90\\%$ chatgpt quality. https://vicuna.lmsys.org, 2023. Accessed:   \n365 15 April 2024.   \n366 [17] Zhumin Chu, Qingyao Ai, Yiteng Tu, Haitao Li, and Yiqun Liu. Pre: A peer review based large   \n367 language model evaluator. arXiv preprint arXiv:2401.15641, 2024.   \n368 [18] Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick   \n369 Wendell, Matei Zaharia, and Reynold Xin. Free dolly: Introducing the world\u2019s first truly open   \n370 instruction-tuned llm, 2023.   \n371 [19] Open-Assistant Contributors. Oasst-sft-4-pythia-12b: A supervised fine-tuning   \n372 model for language understanding. https://huggingface.co/OpenAssistant/   \n373 oasst-sft-4-pythia-12b-epoch-3.5, 2023. Accessed: 15 April 2024.   \n374 [20] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient   \n375 finetuning of quantized llms. Advances in Neural Information Processing Systems, 36, 2024.   \n376 [21] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos   \n377 Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for   \n378 methods that learn from human feedback. arXiv preprint arXiv:2305.14387, 2023.   \n379 [22] Allan M. Feldman. Majority voting. SpringerLink, 2006.   \n380 [23] Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire.   \n381 arXiv preprint arXiv:2302.04166, 2023.   \n382 [24] Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine,   \n383 and Dawn Song. Koala-13b: Dialogue model for effective human-ai interaction. https:   \n384 //bair.berkeley.edu/blog/2023/04/03/koala/, 2023. Accessed: 15 April 2024.   \n385 [25] Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and   \n386 Dawn Song. Koala: A dialogue model for academic research. Blog post, April, 1, 2023.   \n387 [26] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and   \n388 Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint   \n389 arXiv:2009.03300, 2020.   \n390 [27] Charles Eric Leiserson, Ronald L Rivest, Thomas H Cormen, and Clifford Stein. Introduction   \n391 to algorithms, volume 3. MIT press Cambridge, MA, USA, 1994.   \n392 [28] Ruosen Li, Teerth Patel, and Xinya Du. Prd: Peer rank and discussion improve large language   \n393 model based evaluations. arXiv preprint arXiv:2307.02762, 2023.   \n394 [29] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy   \n395 Liang, and Tatsunori B Hashimoto. Alpacaeval: An automatic evaluator of instruction-following   \n396 models, 2023.   \n397 [30] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga,   \n398 Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of   \n399 language models. arXiv preprint arXiv:2211.09110, 2022.   \n400 [31] Yen-Ting Lin and Yun-Nung Chen. Llm-eval: Unified multi-dimensional automatic evaluation   \n401 for open-domain conversations with large language models. arXiv preprint arXiv:2305.13711,   \n402 2023.   \n403 [32] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig.   \n404 Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language   \n405 processing. ACM Computing Surveys, 55(9):1\u201335, 2023.   \n406 [33] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. Gpteval:   \n407 Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634,   \n408 2023.   \n409 [34] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christo  \n410 pher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted   \n411 question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.   \n412 [35] OpenAI. Introducing chatgpt. https://openai.com/blog/chatgpt, 2022. Accessed:   \n413 [insert date here].   \n414 [36] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,   \n415 Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to   \n416 follow instructions with human feedback. Advances in Neural Information Processing Systems,   \n417 35:27730\u201327744, 2022.   \n418 [37] Kaiping Peng, Richard E Nisbett, and Nancy YC Wong. Validity problems comparing values   \n419 across cultures and possible solutions. Psychological methods, 2(4):329, 1997.   \n420 [38] Philip Sedgwick. Pearson\u2019s correlation coefficient. Bmj, 345, 2012.   \n421 [39] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid,   \n422 Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, et al.   \n423 Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.   \n424 arXiv preprint arXiv:2206.04615, 2022.   \n425 [40] James Surowiecki. The wisdom of crowds. Anchor, 2005.   \n426 [41] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy   \n427 Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.   \n428 https://github.com/tatsu-lab/stanford_alpaca, 2023.   \n429 [42] MosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commercially   \n430 usable llms, 2023. Accessed: 2023-05-05.   \n431 [43] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo  \n432 th\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open   \n433 and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n434 [44] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix   \n435 Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose   \n436 language understanding systems. Advances in neural information processing systems, 32, 2019.   \n437 [45] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.   \n438 Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv   \n439 preprint arXiv:1804.07461, 2018.   \n440 [46] Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya   \n441 Jiang, Rui Xie, Jindong Wang, Xing Xie, et al. Pandalm: An automatic evaluation benchmark   \n442 for llm instruction tuning optimization. arXiv preprint arXiv:2306.05087, 2023.   \n443 [47] Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu,   \n444 David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go?   \n445 exploring the state of instruction tuning on open resources. Advances in Neural Information   \n446 Processing Systems, 36, 2024.   \n447 [48] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi,   \n448 and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instruc  \n449 tions. arXiv preprint arXiv:2212.10560, 2022.   \n450 [49] Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng   \n451 Cheng, Weiwei L\u00fc, Rui Hu, et al. Skywork: A more open bilingual foundation model. arXiv   \n452 preprint arXiv:2310.19341, 2023.   \n453 [50] Susan C Weller. Cultural consensus theory: Applications and frequently asked questions. Field   \n454 methods, 19(4):339\u2013368, 2007.   \n455 [51] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and   \n456 Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions,   \n457 2023.   \n458 [52] Jia-Yu Yao, Kun-Peng Ning, Zhen-Hui Liu, Mu-Nan Ning, and Li Yuan. Llm lies: Hallucinations   \n459 are not bugs, but features as adversarial examples. arXiv preprint arXiv:2310.01469, 2023.   \n460 [53] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang,   \n461 Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv   \n462 preprint arXiv:2210.02414, 2022.   \n463 [54] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min,   \n464 Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv   \n465 preprint arXiv:2303.18223, 2023.   \n466 [55] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,   \n467 Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.   \n468 Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.   \n469 [56] Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai Lin,   \n470 Ji-Rong Wen, and Jiawei Han. Don\u2019t make your llm an evaluation benchmark cheater. arXiv   \n471 preprint arXiv:2311.01964, 2023.   \n472 [57] Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, and Diyi Yang. Can large   \n473 language models transform computational social science? arXiv preprint arXiv:2305.03514,   \n474 2023. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "475 A Dataset Format ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "476 Focusing on the MT-Bench dataset, we demonstrate the ensuing data format utilizing dataset $\\mathcal{Q}$ .   \n477 As Figure 6 illustrates, the Question dataset $\\mathcal{Q}$ contains \"Question id,\" \"Category,\" \"Question,\"   \n478 and \"Reference.\" In categories with definitive answers like \"reasoning\" or \"math,\" the \"Reference\"   \n479 field is populated with standard answers; otherwise, it remains blank. Each model M in our pool   \n480 processes the Question dataset $\\mathcal{Q}$ to generate the LLMs answer data $\\boldsymbol{\\mathcal{A}}$ , consisting of \"Question   \n481 id,\" \"Answer id,\" \"Model id,\" and \"Answer.\" Finally, we combine pairs in $\\boldsymbol{\\mathcal{A}}$ and appoint judges to   \n482 evaluate, creating the Answer-Ranking data $\\mathcal{D}$ , featuring \"Question id,\" \"Model 1,\" \"Model 2,\" \"G1   \n483 winner,\" \"G2 winner,\" and \"Judge.\" Here, \"G1 winner\" and \"G2 winner\" indicate the outcomes of   \n484 inputting reversed order responses of Model 1 and Model 2 into the judge model, a method employed   \n485 to mitigate biases stemming from models\u2019 preferences for input order. ", "page_idx": 12}, {"type": "image", "img_path": "uNZpvFlsg9/tmp/06b62985407fa62afe703a8ebc2bc56c6d6be5cfe66394c7597dcce72a687b9c.jpg", "img_caption": ["Figure 6: Format of the Question dataset $\\mathcal{Q}$ , LLMs responses data $\\mathcal{A}$ , and the Answer-Ranking data $\\mathcal{D}$ for Peer Review "], "img_footnote": [], "page_idx": 12}, {"type": "text", "text": "486 B Detailed Prompt for Reviewers ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "487 The evaluation prompts, as detailed in Section 2.2.1, are employed during the Peer Review Stage.   \n8 These prompts are provided to the Reviewer Language Model Systems (LLMs), enabling them to   \n489 generate evaluative preferences. In our experimental framework, we devised four distinct prompt   \n490 settings. For each setting, a tailored prompt template was meticulously crafted as illustrated below:   \nTemplate for Single-Turn Interaction: This template is designed for single-turn interactions   \n492 between users and LLMs, where there is no predetermined correct answer. It facilitates open-ended   \n493 dialogue, allowing for a wide range of user inquiries without the expectation of specific responses.   \n494 Referenced Template for Single-Turn Interaction: Tailored for single-turn dialogues between   \n495 users and LLMs, this template incorporates predefined correct answers. It is particularly suited for   \n496 interactions involving factual inquiries, such as mathematics or logic problems, where accuracy and   \n497 reference to correct information are paramount.   \n498 Template for Multi-Turn Interaction: This template caters to multi-turn conversations between   \n499 users and LLMs, without predefined answers. It supports extended interactions, enabling users to   \n500 explore topics in depth through a series of interconnected questions and responses.   \n501 Referenced Template for Multi-Turn Interaction: Designed for multi-turn dialogues with prede  \n502 fined correct answers, this template is ideal for complex inquiries requiring sequential reasoning or   \n503 problem-solving, such as mathematical computations or logical deductions.   \n504 Each template is carefully constructed to match its intended use-case, providing a structured frame  \n505 work that guides the interaction between users and LLMs towards achieving desired outcomes,   \n506 whether for open-ended exploration or precise problem-solving. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "Template for Single-Turn Answer ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "System prompt: Please act as a judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You do not need to explain, just give your judgment. Output your final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\" if assistant B is better, and \"[[C]]\" for a tie.   \nUser Question: {question}   \nAssistant A\u2019s Answer: {answer a}   \nAssistant B\u2019s Answer: {answer b} ", "page_idx": 13}, {"type": "text", "text": "507 ", "page_idx": 13}, {"type": "text", "text": "Referenced Template for Single-Turn Answer ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "System prompt: Please act as a judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below, with reference to the provided reference answers. You do not need to explain, just give your judgment. Output your final verdict by strictly following this format: \"[[A]]\"if assistant A is better, \"[[B]]\" if assistant B is better, and \"[[C]]\" for a tie. ", "page_idx": 13}, {"type": "text", "text": "User Question: {question} Reference Answer: {reference answer} Assistant A\u2019s Answer: {answer a} Assistant B\u2019s Answer: {answer b} ", "page_idx": 13}, {"type": "text", "text": "Template for Multi-Turn Answer ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "System prompt: Please act as a judge and evaluate the quality of the responses provided by   \ntwo AI assistants to the user question displayed below. You do not need to explain, just give   \nyour judgment. Output your final verdict by strictly following this format: \"[[A]]\" if assistant   \nA is better, \"[[B]]\" if assistant B is better, and \"[[C]]\" for a tie   \nAssistant A\u2019s Conversation with User: User: {question 1} Assistant A: {answer a1} User: {question 2} Assistant A: {answer a2}   \nAssistant B\u2019s Conversation with User: User: {question 1} Assistant B: {answer b1} User: {question 2} Assistant B: {answer b2} ", "page_idx": 13}, {"type": "image", "img_path": "uNZpvFlsg9/tmp/9e6dcc3b1b7545b6302ce436fd64def0079b2da2656a9cc94ef0f9b782526d60.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "511 C Scoring Methodology ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "512 In Section 2.2.2, Equation 7 delineates the methodology for optimizing scores. Within this frame  \n513 work, the function ${\\mathbf{1}}\\{A_{i}^{j}>A_{i}^{k}\\}$ is more precisely defined as $f(A_{i}^{j},A_{i}^{k})$ . Additionally, the function   \n514 $f(A_{i}^{j},A_{i}^{k})$ is not fixed and can be implemented using various computational strategies. We introduce   \n515 two distinct methodologies in this context: the Elo mechanism and the Rank mechanism.   \n516 Within the framework of the Elo mechanism, as specified by Equation 10, the $B A S E$ value is set to   \n517 10, and the $S C A L E$ factor is determined to be 400. This approach facilitates a dynamic adjustment   \n518 of scores based on the outcomes of pairwise comparisons, allowing for a nuanced reflection of   \n519 performance variations among models.   \n520 Conversely, in the context of the Rank mechanism, as outlined by Equation 11 $,r a n k(j)$ signifies the   \n521 current ranking of model $j$ , with the constant $K$ assigned a value of 200. This mechanism employs   \n522 a model\u2019s ranking within a predefined hierarchy as a pivotal factor in score calculation, thereby   \n523 providing a straightforward, yet effective, method for evaluating comparative model performance. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "equation", "text": "$$\nf(A_{i}^{j},A_{i}^{k})=\\left\\{\\begin{array}{l l}{1-\\frac{1}{1+\\mathrm{BASE}^{((G(k)-G(j))/\\mathrm{SCALE})}}}&{\\mathrm{if~}A_{i}^{j}>A_{i}^{k}}\\\\ {0.5-\\frac{1}{1+\\mathrm{BASE}^{((G(k)-G(j))/\\mathrm{SCALE})}}}&{\\mathrm{if~}A_{i}^{j}=A_{i}^{k}}\\\\ {0-\\frac{1}{1+\\mathrm{BASE}^{((G(k)-G(j))/\\mathrm{SCALE})}}}&{\\mathrm{if~}A_{i}^{j}<A_{i}^{k}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "equation", "text": "$$\nf(A_{i}^{j},A_{i}^{k})=\\left\\{\\begin{array}{l l}{1+(r a n k(j)-r a n k(k))/K}&{\\mathrm{if}\\;A_{i}^{j}>A_{i}^{k}}\\\\ {0.5}&{\\mathrm{if}\\;A_{i}^{j}=A_{i}^{k}}\\\\ {0}&{\\mathrm{if}\\;A_{i}^{j}<A_{i}^{k}}\\end{array}\\right.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "524 D Overall Algorithm of Peer Review ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "525 The overall algorithm, as delineated in Algorithm 1, encapsulates the comprehensive process outlined   \n526 in Section 2.2. This sequence commences with \"Data Collection and LLMs Pool Construction,\"   \n527 progresses through \"Answer-Ranking Data Construction Based on Peer Review,\" advances to \"Con  \n528 sistency Optimization,\" and culminates with the \"Unsupervised Elimination Mechanism.\"   \nRequire: Unlabeled dataset $\\mathcal{Q}$ , Pool of LLMs $\\mathcal{M}$ , Active LLM pool $\\mathcal{M}^{\\ast}=\\mathcal{M}$   \nEnsure: Consistency-optimized ranking of LLMs $\\mathcal{R}^{*}$   \n1: Initialize response matrix $A\\leftarrow\\emptyset$   \n2: for each question $q_{i}\\in\\mathcal{Q}$ do   \n3: Initialize response vector for question $q_{i}$ , $A^{i}\\gets\\emptyset$   \n4: for each model $m_{j}\\in\\mathcal{M}$ do   \n5: $A_{j}^{i}\\leftarrow$ response of model $m_{j}$ to question $q_{i}$   \n6: $\\ddot{A^{i}}\\leftarrow A^{i}\\cup\\{A_{j}^{i}\\}$   \n7: end for   \n8: Shuffle $A^{i}$ to obtain permuted response vector $A^{i}$   \n9: $A\\leftarrow A\\cup\\{A^{i}\\}$   \n10: end for   \n11: Initialize answer-ranking data $D\\gets\\emptyset$   \n12: Initialize model weights vector $w$ with Gaussian distribution   \n13: for each permuted response vector $A^{i}$ do   \n14: for each pair of responses $(A_{i}^{j},A_{i}^{k})$ in $A^{i}$ do   \n15: for $s\\gets1$ to 5 do $\\triangleright$ Randomly select 5 models for evaluation   \n16: Evaluate the pair $(A_{i}^{j},A_{i}^{k})$ with model $m_{s}$   \n17: $D\\leftarrow D\\cup\\{(A_{i}^{j},A_{i}^{k},>w^{s})\\}$   \n18: end for   \n19: end for   \n20: end for   \n21: Initialize scores $G_{j}$ for each model $m_{j}\\in\\mathcal{M}$ to the Elo initial score   \n22: repeat   \n23: while not converged do   \n24: for each model $m_{j}\\in\\mathcal{M}$ do   \n25: Compute $G_{j}$ using updated formula:   \n26: $\\begin{array}{r}{G_{j}=\\sum_{i}\\dot{\\sum_{k\\neq j}}\\dot{\\sum_{s\\neq k,s\\neq j}}\\,\\mathbf{1}\\{A_{i}^{j},A_{i}^{k}\\}\\times w^{s}\\quad(A_{i}^{j},A_{i}^{k},>w^{s},s\\in\\mathcal{M}^{*})\\in D}\\end{array}$   \n27: end for   \n28: Update weight vector $w$ to maximize the consistency of $w$ and $G$   \n29: end while   \n30: Sort $\\mathcal{M}^{\\ast}$ by $G_{j}$ to identify $\\mathcal{M}_{m i n}$ , the lowest-scoring model   \n31: if size of $\\mathcal{M}^{\\ast}>$ threshold then   \n32: Remove $\\mathcal{M}_{m i n}$ from $\\mathcal{M}^{\\ast}$   \n33: end if   \n34: until size of $\\mathcal{M}^{\\ast}<$ threshold   \n35: Compute the final ranking $\\mathcal{R}^{*}$ based on the optimized scores $G_{j}$   \n36: return $\\mathcal{R}^{*}$ ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "529 E Complete Experimental Results ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "530 In Section 3.4, we both employ elimination mechanisms to cull the weakest LLMs from the \u2019reviewer   \n531 queue\u2019 during the evaluation process. In Figures 7 and 8, we present the results for the PEN and   \n532 LIS metrics, where lower PEN scores indicate better performance, and higher LIS scores denote   \n533 superior performance. It is evident that both the \u2019PiCO\u2019 and PRE approaches demonstrate enhanced   \n534 performance as the number of eliminated \u2019reviewers\u2019 increases. In most cases, the proposed \u2019PiCO\u2019   \n535 method outperforms PRE.   \n536 In Section 3.5, we validate the effectiveness of the consistency assumption and compare it with the   \n537 Average Performance of the Reviewer Queue, i.e., employing a single LLM as the \u2019reviewer\u2019 to   \n538 evaluate all response pairs and then calculating the average results of all LLMs. The comprehensive   \n539 results compared with the Reviewer Queue are illustrated in Table3, Figure 9, 10 and 11, revealing   \n540 that in the full Reviewer Queue, the performance of the vast majority of LLMs is very poor, indicating   \n541 that the evaluations from most LLMs are noise. However, our \u2019PiCO\u2019 approach nearly matches the   \n542 evaluative prowess of the pool\u2019s most capable LLM, GPT-3.5. Remarkably, given its unsupervised   \n543 nature, the \u2019PiCO\u2019 method demonstrates the capability to mitigate the influence of noise, reaching the   \n544 evaluation upper bound (the strongest LLM) within any given unknown LLM pool $M$ , even in the   \n545 absence of prior ranking information. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "image", "img_path": "uNZpvFlsg9/tmp/a74c006c844ec05417413217039a6eb3361e9657656c6419c2c885bc581ea3a9.jpg", "img_caption": ["Figure 7: Performance comparison of the PiCO (Ours) and PRE[17] methods on the MT-Bench, Chatbot Arena, and AlpacaEval datasets, with the number of eliminated reviewers on the $\\mathbf{X}$ -axis. The y-axis is PEN, where lower values indicate better performance. "], "img_footnote": [], "page_idx": 16}, {"type": "image", "img_path": "uNZpvFlsg9/tmp/b8f005aa7e73e57dd8cac3461c8d472e2aa1033067014cd2c252026af4539f8f.jpg", "img_caption": ["Figure 8: Performance comparison of the PiCO (Ours) and PRE[17] methods on the MT-Bench, Chatbot Arena, and AlpacaEval datasets, with the number of eliminated reviewers on the $\\mathbf{X}{\\cdot}$ -axis. The y-axis is LIS, where upper values indicate better performance. "], "img_footnote": [], "page_idx": 16}, {"type": "table", "img_path": "uNZpvFlsg9/tmp/923627a25c08fa8318dcb7a3ca4568fbe6382eb0350ac801bd72d41b996e8aa1.jpg", "table_caption": ["Table 3: Comparison of performance across three datasets using Unsupervised methods versus using single models in reviewer queue. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "image", "img_path": "uNZpvFlsg9/tmp/2bf39ed854f1fca3ec542c87a5660bc9fe51a219bbbe6b875e2366991d6c05dd.jpg", "img_caption": ["Figure 9: Comparison of performance on the CIN metric across three datasets using Unsupervised methods versus using single models, with Unsupervised methods on the left and Supervised methods on the right. The dotted line represents the average value using single models. "], "img_footnote": [], "page_idx": 17}, {"type": "image", "img_path": "uNZpvFlsg9/tmp/79804da5ccb7e6dddc3f269ab3337ec1eb587c3302e2f348ea7ca7032fc7ad6a.jpg", "img_caption": ["Figure 10: Comparison of performance on the PEN metric across three datasets using Unsupervised methods versus using single models, with Unsupervised methods on the left and Supervised methods on the right. The dotted line represents the average value using single models. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "546 F Selected Models and Optimized Ranking ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "547 For our analysis, we meticulously selected 15 LLMs spanning a variety of architectures, encompassing   \n548 both open-source and closed-source models, as detailed in the subsequent table. Our curated selection   \n549 features prominent LLMs including the closed-source \"gpt-3.5-turbo,\" \"chatglm\" which is predicated   \n550 on the encoder-decoder framework, \"fastchat-t5-3b\" that leverages Google\u2019s T5 (Text-to-Text Transfer   \n551 Transformer) architecture, and \"llama-13b\" founded on the GPT architectural principles.   \n552 We have comprehensively detailed the ranking outcomes across three distinct datasets for our   \n553 comparative analysis, incorporating the optimized model rankings, names, and their respective scores.   \n554 As delineated in Appendix C, the PiCO (Ours) is capable of employing various scoring mechanisms,   \n555 thereby facilitating the presentation of ranking outcomes on three datasets utilizing both the Elo and   \n556 Rank mechanisms. Furthermore, we have also enumerated the ranking results for PRD and PRE   \n557 methodologies across the three datasets, offering a holistic view of the competitive landscape. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "image", "img_path": "uNZpvFlsg9/tmp/3a291e1004a229dcfec85f3d998dea06e04770cd774fffba363e97a3f41cc83b.jpg", "img_caption": ["Figure 11: Comparison of performance on the LIS metric across three datasets using Unsupervised methods versus using single models, with Unsupervised methods on the left and Supervised methods on the right. The dotted line represents the average value using single models. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "table", "img_path": "uNZpvFlsg9/tmp/5aa2e6e6b8d7a837afdc9bacb533b4da1de31cd81e3a973d79d5afd1817085bd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "559 ", "page_idx": 19}, {"type": "text", "text": "Grade-Elo-AlpacaEval ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "#1 WizardLM-13B | Grade: 8662.7158203125   \n#2 Vicuna-13B | Grade: 5586.46630859375   \n#3 Guanaco-33B | Grade: 5445.341796875   \n#4 Vicuna-7B | Grade: 5374.2314453125   \n#5 Gpt-3.5 | Grade: 4845.91552734375   \n#6 Koala-13B | Grade: 4338.77783203125 | Eliminated   \n#7 Chatglm-6B | Grade: 2293.4208984375 | Eliminated   \n#8 Gpt4all-13B | Grade: 2080.511962890625 | Eliminated   \n#9 Mpt-7B | Grade: 1694.4945068359375 | Eliminated   \n#10 Fastchat-t5-3B | Grade: 1371.94287109375 | Eliminated   \n#11 Oasst-pythia-12B | Grade: -665.8685302734375 | Eliminated   \n#12 StableLM-7B | Grade: -1343.5838623046875 | Eliminated   \n#13 Dolly-12B | Grade: -5377.13427734375 | Eliminated   \n#14 Llama-13B | Grade: -5847.59130859375 | Eliminated   \n#15 Alpaca-13B | Grade: -13459.6162109375 | Eliminated ", "page_idx": 19}, {"type": "text", "text": "560 ", "page_idx": 19}, {"type": "image", "img_path": "uNZpvFlsg9/tmp/933c33e94a6cd103679b6181dea02577eecf6861a8e0ceddbb0a3375a7050ed5.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "table", "img_path": "uNZpvFlsg9/tmp/9e95175ba395857c685aa4d54cb83ad6fe55b1e3ef8cffefd075d6bc29e5137f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Grade-Rank-AlpacaEval ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "#1 WizardLM-13B | Grade: 0.4019235074520111 #2 Vicuna-13B | Grade: 0.36745429039001465 #3 Guanaco-33B | Grade: 0.3664878010749817 #4 Vicuna-7B | Grade: 0.36541733145713806 #5 Gpt-3.5 | Grade: 0.36000365018844604 #6 Koala-13B | Grade: 0.3544933795928955 | Eliminated #7 Chatglm-6B | Grade: 0.3319571018218994 | Eliminated #8 Gpt4all-13B | Grade: 0.3306528627872467 | Eliminated #9 Mpt-7B | Grade: 0.32641729712486267 | Eliminated #10 Fastchat-t5-3B | Grade: 0.32173293828964233 | Eliminated #11 Oasst-pythia-12B | Grade: 0.2999681532382965 | Eliminated #12 StableLM-7B | Grade: 0.2932431995868683 | Eliminated #13 Dolly-12B | Grade: 0.24777530133724213 | Eliminated #14 Llama-13B | Grade: 0.24381506443023682 | Eliminated #15 Alpaca-13B | Grade: 0.16114839911460876 ", "page_idx": 20}, {"type": "text", "text": "563 ", "page_idx": 20}, {"type": "text", "text": "Grade-Rank-MT_Bench ", "text_level": 1, "page_idx": 20}, {"type": "image", "img_path": "uNZpvFlsg9/tmp/510f86d60247a1a4d02122684c778eec25d19d657d0244e6cde0e5d9556b5908.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "table", "img_path": "uNZpvFlsg9/tmp/7d2bec19c2e480f268abce90df40e6736596f0bdc8bcf98442ff42ac033681cd.jpg", "table_caption": [], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "566 ", "page_idx": 21}, {"type": "text", "text": "PRD-AlpacaEval ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "#1 WizardLM-13B | Grade: 5469.75634765625   \n#2 Guanaco-33B | Grade: 3707.014892578125   \n#3 Vicuna-13B | Grade: 3618.63427734375   \n#4 Vicuna-7B | Grade: 3569.389892578125   \n#5 Gpt-3.5 | Grade: 3197.755615234375   \n#6 Koala-13B | Grade: 2893.642578125   \n#7 Chatglm-6B | Grade: 1847.1300048828125   \n#8 Fastchat-t5-3B | Grade: 1585.66943359375   \n#9 Gpt4all-13B | Grade: 1561.145751953125   \n#10 Mpt-7B | Grade: 1332.3753662109375   \n#11 StableLM-7B | Grade: -33.00855255126953   \n#12 Oasst-pythia-12B | Grade: -92.68387603759766   \n#13 Dolly-12B | Grade: -3013.588623046875   \n#14 Llama-13B | Grade: -3211.0302734375   \n#15 Alpaca-13B | Grade: -7432.3701171875 ", "page_idx": 21}, {"type": "text", "text": "567 ", "page_idx": 21}, {"type": "text", "text": "PRD-MT_Bench ", "text_level": 1, "page_idx": 21}, {"type": "image", "img_path": "uNZpvFlsg9/tmp/3a2b1a8fbb98cb703b4ed80d8e34a99310e2b30e4428af7bc4c0cfddaec1e31c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "uNZpvFlsg9/tmp/95eced3760a33af524ae83b7765159295797e6fe062e92dd22986f755a62fb8e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "570 ", "page_idx": 22}, {"type": "table", "img_path": "uNZpvFlsg9/tmp/bd190e578a5706973fded253eef96fd8abba9f3379dba9b167bd49a633b7651b.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "text", "text": "571 ", "page_idx": 22}, {"type": "image", "img_path": "uNZpvFlsg9/tmp/a6d7fe4e917d0d3f864a2f75af9f320e8516d2c752c05729a6f8c0935048cc62.jpg", "img_caption": [], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "573 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "5 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n6 paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Justification: We clearly state our claims in the abstract and introduction, such as a novel unsupervised LLM evaluation method and a consistency-based constrained optimization approach. These are substantiated in Section 3, demonstrating the alignment between our theoretical contributions and empirical results. ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [No] ", "page_idx": 23}, {"type": "text", "text": "Justification: Although this paper does not have a separate \u2019Limitations\u2019 section, the consistency assumptions on which the work is based are clearly stated in the introduction, and their validity is experimentally verified in Section 3.5. Moreover, the limitations of our work are discussed in the conclusion, noting that the current study is conducted solely within a text-based llm evaluation environment, and exploring the potential for future expansion into multimodal large model assessments. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best ", "page_idx": 23}, {"type": "text", "text": "625 judgment and recognize that individual actions in favor of transparency play an impor  \n626 tant role in developing norms that preserve the integrity of the community. Reviewers   \n627 will be specifically instructed to not penalize honesty concerning limitations.   \n628 3. Theory Assumptions and Proofs   \n629 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n630 a complete (and correct) proof?   \n631 Answer: [Yes]   \n632 Justification: We thoroughly detail the Consistency Assumption which underpins our the  \n633 oretical results and provide a complete proof in Section 3.5. Furthermore, we ensure that   \n634 all necessary assumptions are explicitly stated and each theorem and proof is carefully   \n635 numbered and cross-referenced for clarity and accessibility.   \n636 Guidelines:   \n637 \u2022 The answer NA means that the paper does not include theoretical results.   \n638 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n639 referenced.   \n640 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems   \n641 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n642 they appear in the supplemental material, the authors are encouraged to provide a short   \n643 proof sketch to provide intuition.   \n644 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n645 by formal proofs provided in appendix or supplemental material.   \n646 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n647 4. Experimental Result Reproducibility   \n648 Question: Does the paper fully disclose all the information needed to reproduce the main ex   \n649 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n650 of the paper (regardless of whether the code and data are provided or not)?   \n651 Answer: [Yes]   \n652 Justification: We provide detailed pseudocode of our new LLM evaluation algorithm in   \n653 Appendix D and have made all relevant data and code publicly accessible on GitHub,   \n654 ensuring anonymity during the review process. This comprehensive disclosure allows othe   \n655 researchers to reproduce our experimental results, fully aligning with our paper\u2019s claims and   \n656 enhancing the credibility of our findings.   \n657 Guidelines:   \n658 \u2022 The answer NA means that the paper does not include experiments.   \n659 \u2022 If the paper includes experiments, a No answer to this question will not be perceived   \n660 well by the reviewers: Making the paper reproducible is important, regardless of   \n661 whether the code and data are provided or not.   \n662 \u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken   \n663 to make their results reproducible or verifiable.   \n664 \u2022 Depending on the contribution, reproducibility can be accomplished in various ways.   \n665 For example, if the contribution is a novel architecture, describing the architecture fully   \n666 might suffice, or if the contribution is a specific model and empirical evaluation, it may   \n667 be necessary to either make it possible for others to replicate the model with the same   \n668 dataset, or provide access to the model. In general. releasing code and data is often   \n669 one good way to accomplish this, but reproducibility can also be provided via detailed   \n670 instructions for how to replicate the results, access to a hosted model (e.g., in the case   \n671 of a large language model), releasing of a model checkpoint, or other means that are   \n672 appropriate to the research performed.   \n673 \u2022 While NeurIPS does not require releasing code, the conference does require all submis  \n674 sions to provide some reasonable avenue for reproducibility, which may depend on the   \n675 nature of the contribution. For example   \n676 (a) If the contribution is primarily a new algorithm, the paper should make it clear how   \n677 to reproduce that algorithm.   \n678 (b) If the contribution is primarily a new model architecture, the paper should describe   \n679 the architecture clearly and fully.   \n680 (c) If the contribution is a new model (e.g., a large language model), then there should   \n681 either be a way to access this model for reproducing the results or a way to reproduce   \n682 the model (e.g., with an open-source dataset or instructions for how to construct   \n683 the dataset).   \n684 (d) We recognize that reproducibility may be tricky in some cases, in which case   \n685 authors are welcome to describe the particular way they provide for reproducibility.   \n686 In the case of closed-source models, it may be that access to the model is limited in   \n687 some way (e.g., to registered users), but it should be possible for other researchers   \n688 to have some path to reproducing or verifying the results.   \n689 5. Open access to data and code   \n690 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n691 tions to faithfully reproduce the main experimental results, as described in supplemental   \n692 material?   \n693 Answer: [Yes]   \n694 Justification: All necessary data and code have been made publicly available on GitHub,   \n695 with detailed instructions for installation, environment setup, and execution commands. This   \n696 includes all raw, pre-processed, intermediate, and generated data needed to reproduce our   \n697 experimental results. The repository is anonymous during the review process to ensure   \n698 compliance with double-blind requirements. This thorough documentation ensures that   \n699 other researchers can faithfully replicate our study.   \n700 Guidelines:   \n701 \u2022 The answer NA means that paper does not include experiments requiring code.   \n702 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n703 public/guides/CodeSubmissionPolicy) for more details.   \n704 \u2022 While we encourage the release of code and data, we understand that this might not be   \n705 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n706 including code, unless this is central to the contribution (e.g., for a new open-source   \n707 benchmark).   \n708 \u2022 The instructions should contain the exact command and environment needed to run to   \n709 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n710 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n711 \u2022 The authors should provide instructions on data access and preparation, including how   \n712 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n713 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n714 proposed method and baselines. If only a subset of experiments are reproducible, they   \n715 should state which ones are omitted from the script and why.   \n716 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n717 versions (if applicable).   \n718 \u2022 Providing as much information as possible in supplemental material (appended to the   \n719 paper) is recommended, but including URLs to data and code is permitted.   \n720 6. Experimental Setting/Details   \n721 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n722 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n723 results?   \n724 Answer: [Yes]   \n725 Justification: We have detailed the data processing and training procedures in Sections 2.2   \n726 and Appendices A, B, and D. For comprehensive understanding, additional information   \n727 such as hyperparameters, optimizer types, and detailed data splits are provided alongside   \n728 the code due to space constraints in the paper.   \n729 Guidelines:   \n730 \u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 24}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "\u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 26}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 26}, {"type": "text", "text": "Justification: We conducted each experiment four times using different seeds $s e e d=$ 1, 2, 3, 4) to ensure robustness. The results, presented as averages, are accompanied by standard deviations as error bars in Tables 1 and 2. This approach captures the variability due to different initializations and confirms the reproducibility of our results. The standard deviations used help clarify the extent of variability in the experiments, ensuring that our statistical analysis aligns with best practices for empirical research. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 26}, {"type": "text", "text": "731   \n732   \n733   \n734   \n735   \n736   \n737   \n738   \n739   \n740   \n741   \n742   \n743   \n744   \n745   \n746   \n747   \n748   \n749   \n750   \n751   \n752   \n753   \n754   \n755   \n756   \n757   \n758   \n759   \n760   \n761   \n762   \n763   \n764   \n765   \n766   \n767   \n768   \n769   \n770   \n771   \n772   \n773   \n774   \n775   \n776   \n777   \n778   \n779   \n780   \n781   \n782 ", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Justification: Although we did not detail the exact compute resources for each experimental setup in the paper, we used NVIDIA A6000 graphics cards for open-source models and API calls for proprietary models. To facilitate reproducibility, we have provided all necessary data, ensuring that the experiments can be replicated on consumer-grade computers. This approach allows readers to reproduce the results without requiring high-end computational resources. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. ", "page_idx": 26}, {"type": "text", "text": "783 \u2022 The paper should disclose whether the full research project required more compute   \n784 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n785 didn\u2019t make it into the paper).   \n786 9. Code Of Ethics   \n787 Question: Does the research conducted in the paper conform, in every respect, with the   \n788 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n789 Answer: [Yes]   \n790 Justification: The research conducted in this paper complies with the NeurIPS ethics   \n791 guidelines in all respects.   \n792 Guidelines:   \n793 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n794 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n795 deviation from the Code of Ethics.   \n796 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n797 eration due to laws or regulations in their jurisdiction).   \n798 10. Broader Impacts   \n799 Question: Does the paper discuss both potential positive societal impacts and negative   \n800 societal impacts of the work performed?   \n801 Answer: [Yes]   \n802 Justification: In the introduction, we discuss the potential positive impact of our novel   \n803 unsupervised LLM evaluation approach, which could significantly advance the field of LLM   \n804 evaluation. However, we also recognize potential negative societal impacts, such as the   \n805 misuse of this technology to unfairly or inaccurately assess LLM systems, which might   \n806 lead to biased or misleading outcomes. We suggest potential mitigation strategies, such as   \n807 implementing robust validation protocols and ethical guidelines to govern the application of   \n808 this evaluation methodology.   \n809 Guidelines:   \n810 \u2022 The answer NA means that there is no societal impact of the work performed.   \n811 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n812 impact or why the paper does not address societal impact.   \n813 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n814 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n815 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n816 groups), privacy considerations, and security considerations.   \n817 \u2022 The conference expects that many papers will be foundational research and not tied   \n818 to particular applications, let alone deployments. However, if there is a direct path to   \n819 any negative applications, the authors should point it out. For example, it is legitimate   \n820 to point out that an improvement in the quality of generative models could be used to   \n821 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n822 that a generic algorithm for optimizing neural networks could enable people to train   \n823 models that generate Deepfakes faster.   \n824 \u2022 The authors should consider possible harms that could arise when the technology is   \n825 being used as intended and functioning correctly, harms that could arise when the   \n826 technology is being used as intended but gives incorrect results, and harms following   \n827 from (intentional or unintentional) misuse of the technology.   \n828 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n829 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n830 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n831 feedback over time, improving the efficiency and accessibility of ML).   \n832 11. Safeguards   \n833 Question: Does the paper describe safeguards that have been put in place for responsible   \n834 release of data or models that have a high risk for misuse (e.g., pretrained language models, ", "page_idx": 27}, {"type": "text", "text": "835 image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "836 Answer: [NA]   \n837 Justification: This paper introduces a new approach for unsupervised LLM evaluation and   \n838 does not involve the release of pre-trained models, image generators, or newly collected   \n839 datasets. Therefore, there are no direct risks associated with misuse or dual-use of such   \n840 resources, making safeguards for controlled release irrelevant to this study.   \n841 Guidelines:   \n842 \u2022 The answer NA means that the paper poses no such risks.   \n843 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n844 necessary safeguards to allow for controlled use of the model, for example by requiring   \n845 that users adhere to usage guidelines or restrictions to access the model or implementing   \n846 safety filters.   \n847 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n848 should describe how they avoided releasing unsafe images.   \n849 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n850 not require this, but we encourage authors to take this into account and make a best   \n851 faith effort.   \n852 12. Licenses for existing assets   \n853 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n854 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n855 properly respected?   \n856 Answer: [Yes]   \n857 Justification: This paper utilizes the FastChat project\u2019s code, along with several other pre  \n858 trained models and datasets. The FastChat project adheres to the Apache License 2.0. In   \n859 compliance with the licensing requirements, we have included the original project\u2019s licensing   \n860 information in all derivative works and have clearly marked any modifications made to the   \n861 code. Additionally, we have ensured that all utilized pre-trained models and datasets are   \n862 appropriately cited.   \n863 Guidelines:   \n864 \u2022 The answer NA means that the paper does not use existing assets.   \n865 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n866 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n867 URL.   \n868 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n869 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n870 service of that source should be provided.   \n871 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n872 package should be provided. For popular datasets, paperswithcode.com/datasets   \n873 has curated licenses for some datasets. Their licensing guide can help determine the   \n874 license of a dataset.   \n875 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n876 the derived asset (if it has changed) should be provided.   \n877 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n878 the asset\u2019s creators.   \n879 13. New Assets   \n880 Question: Are new assets introduced in the paper well documented and is the documentation   \n881 provided alongside the assets?   \n882 Answer: [NA]   \n883 Justification: The paper does not release new assets.   \n884 Guidelines:   \n885 \u2022 The answer NA means that the paper does not release new assets.   \n886 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n887 submissions via structured templates. This includes details about training, license,   \n888 limitations, etc. ", "page_idx": 28}, {"type": "text", "text": "\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Justification: This paper focuses on an unsupervised evaluation method for LLMs that does not require human feedback or interaction. Consequently, there is no involvement of crowdsourcing or research with human subjects, making details about participant instructions and compensation irrelevant. ", "page_idx": 29}, {"type": "text", "text": "Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 29}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 29}]