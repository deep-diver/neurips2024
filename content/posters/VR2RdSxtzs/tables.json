[{"figure_path": "VR2RdSxtzs/tables/tables_3_1.jpg", "caption": "Table 1: Accuracy (%) of GPT-4 Turbo on MATH dataset with different prompting methods.", "description": "This table compares the accuracy of GPT-4 Turbo on the MATH dataset using various prompting methods, including I-O, CoT, SC-CoT, CSV, CSV+Voting, and MACM.  It shows the accuracy percentages achieved by each method across different problem types within the MATH dataset (Algebra, Geometry, Probability, Number Theory, etc.) and an overall accuracy.  The results highlight MACM's significant improvement over other methods.", "section": "4.1 Performance on MATH benchmark"}, {"figure_path": "VR2RdSxtzs/tables/tables_5_1.jpg", "caption": "Table 1: Accuracy (%) of GPT-4 Turbo on MATH dataset with different prompting methods.", "description": "This table presents the accuracy percentages achieved by the GPT-4 Turbo model on the MATH dataset when using various prompting methods.  It compares the performance of the Input-Output (I-O) method, Chain of Thought (CoT), Self-Consistency Chain of Thought (SC-CoT), and the proposed Multi-Agent System for Conditional Mining (MACM) method, as well as results from previous work using the CSV method. The accuracy is broken down by different categories of mathematical problems within the MATH dataset (Algebra, Counting and Probability, Geometry, Intermediate Algebra, Number Theory, Prealgebra, and Precalculus).", "section": "4.1 Performance on MATH benchmark"}, {"figure_path": "VR2RdSxtzs/tables/tables_6_1.jpg", "caption": "Table 2: Accuracy (%) comparison of different methods on various tasks.", "description": "This table compares the accuracy of various prompting methods (I-O, CoT, SC-CoT, ToT, GoT, and MACM) on two specific tasks: the 24-points game and sequence sorting (with 64 elements).  It shows the model used (GPT-3.5, GPT-4, GPT-4 Turbo), whether code verification was used, and the resulting accuracy percentage. Note that the GoT accuracy for sequence sorting is marked with an asterisk, indicating it may be an estimate from a graph rather than a precise value.", "section": "4.2 Comparison with ToT and GoT"}, {"figure_path": "VR2RdSxtzs/tables/tables_7_1.jpg", "caption": "Table 1: Accuracy (%) of GPT-4 Turbo on MATH dataset with different prompting methods.", "description": "This table presents the accuracy of GPT-4 Turbo on the MATH dataset using various prompting methods, including I-O, CoT, SC-CoT, and MACM.  It shows the accuracy across different subcategories of math problems within the MATH dataset (Algebra, Counting and Probability, Geometry, Intermediate Algebra, Number Theory, Prealgebra, and Precalculus). The results demonstrate the performance improvement achieved by MACM compared to the baseline and other prompting methods.", "section": "4.1 Performance on MATH benchmark"}]