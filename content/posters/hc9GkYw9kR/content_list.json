[{"type": "text", "text": "LORA-MOO: Learning Ordinal Relations and Angles for Expensive Many-Objective Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Many-objective optimization (MOO) simultaneously optimizes many conflicting   \n2 objectives to identify the Pareto front - a set of diverse solutions that represent   \n3 different optimal balances between conflicting objectives. For expensive MOO   \n4 problems, due to their costly function evaluations, computationally cheap surrogates   \n5 have been widely used in MOO to save evaluation budget. However, as the number   \n6 of objectives increases, the cost of learning and surrogation, as well as the difficulty   \n7 of maintaining solution diversity, increases rapidly. In this paper, we propose   \n8 LORA-MOO, a surrogate-assisted MOO algorithm that learns surrogates from   \n9 spherical coordinates. This includes an ordinal-regression-based surrogate for   \n10 convergence and $M-1$ regression-based surrogates for diversity. $M$ is the number   \n11 of objectives. Such a surrogate modeling method makes it possible to use a   \n12 single ordinal surrogate to do the surrogate-assisted search, and the remaining   \n13 surrogates are used to select solution for expensive evaluations, which enhances the   \n14 optimization efficiency. The ordinal regression surrogate is developed to predict   \n15 ordinal relation values as radial coordinates, estimating how desirable the candidate   \n16 solutions are in terms of convergence. The solution diversity is maintained via   \n17 angles between solutions, which is a parameter-free. Experimental results show   \n18 that LORA-MOO significantly outperforms other surrogate-assisted MOO methods   \n19 on most MOO benchmark problems and real-world applications. ", "page_idx": 0}, {"type": "text", "text": "20 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "21 Many-objective optimization problems (MOOPs) are widely exist in many real-world applications,   \n22 such as production scheduling [26], traffic signal control [33], and water resource engineering [21].   \n23 These MOOPs have conflicting objectives to optimize, and thus all objectives cannot reach their   \n24 optimum simultaneously. As a result the optimum of MOOPs is the Pareto front $(P F)$ : A set of   \n25 non-dominated solutions that represent different optimal balance between conflicting objectives.   \n26 Multi-/many-objective optimization (MOO) 1 aims to find non-dominated solutions that are close to   \n27 the PF and also well distributed along the PF, indicating that MOO should consider both convergence   \n28 and diversity.   \n29 Various evolutionary optimization algorithms have been proposed to solve MOOPs [10]. These   \n30 optimization algorithms usually require plenty of solution samplings and evaluations to find converged   \n31 and diverse non-dominated solutions. However, in many real-world MOOPs, the evaluation of solution   \n32 performance could be expensive [41]. In these expensive MOOPs, the evaluation budget only allows   \n33 a limited number of solutions to be evaluated on the expensive objective functions. To address   \n34 expensive MOOPs, evolutionary optimization algorithms are combined with computationally cheap   \n35 surrogates to enhance sampling efficiency and save evaluations, which are known as surrogate-assisted   \n36 evolutionary algorithms (SAEAs).   \n37 Yet, it is a perennial challenge to use surrogates in a more effective and efficient way for SAEAs,   \n38 especially when optimization problems have many objectives. For example, conventional SAEAs   \n39 usually use regression-based surrogates to approximate each objective function separately [5, 34].   \n40 For MOOPs, many objectives indicate maintaining many surrogates for surrogate-assisted search and   \n41 selection, which results in a low efficiency of SAEAs. In addition, it is difficult to maintain solution   \n42 diversity in high-dimensional objective space. Some SAEAs [24, 43, 5] need to investigate proper   \n43 parametric strategies to generate reference vectors or divide objective space into subspaces. Recently,   \n44 a family of classification-based SAEAs [31, 17] attempted to use a single surrogate to learn pairwise   \n45 dominance relations. However, the training with pairwise relations implies an exponential increase in   \n46 the size of training dataset. Therefore, a natural question is that whether we can reduce the cost of   \n47 maintaining many surrogates without increasing the cost of training a single surrogate. Furthermore,   \n48 whether we can use an non-parametric diversity maintenance strategy to handle the objective space of   \n49 MOOPs, instead of designing complex reference vectors or points?   \n50 In this paper, we propose a different way to implement surrogate-assisted evolutionary optimization   \n51 for expensive MOOPs, named LORA-MOO, where a single surrogate is developed to learn ordinal   \n52 relations for convergence purpose, and several angular surrogates are generated from spherical   \n53 coordinates to maintain diversity. Our major contributions are summarized as follows:   \n54 \u2022 We develop a novel ordinal-regression-based model to approximate the ordinal landscape of   \n55 expensive MOOPs. Our ordinal surrogate is able to handle many objectives simultaneously   \n56 and assist MOO algorithms to complete the model-based search. Artificial ordinal relations   \n57 are generated via a clustering method to improve the learning quality of ordinal relations for   \n58 many objectives. Unlike the pairwise relations learned through classification, the ordinal   \n59 relations would not increase the size of training dataset, hence high efficiency.   \n60 \u2022 We introduce the idea of spherical coordinates approximation into surrogate-assisted evo  \n61 lutionary optimization and proposed LORA-MOO to solve expensive MOOPs. Different   \n62 from existing SAEAs which learn approximation models from Cartesian coordinates, we fti   \n63 several regression-based surrogates to approximate angular coordinates, while our ordinal   \n64 surrogate can be treated as a radial coordinate. An non-parametric approach is developed to   \n65 select diverse solutions for expensive evaluations via our angular coordinate surrogates.   \n66 \u2022 Extensive experiments on benchmark and real-world optimization problems are conducted   \n67 under a range of scales and numbers of objectives. Empirical results show that our LORA  \n68 MOO is effective. It is able to obtain a well-distributed solution set that outperforms the   \n69 state-of-the-arts. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "70 2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "71 2.1 Multi-/Many-Objective Surrogate-Assisted Evolutionary Algorithms ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "72 Regression-based SAEAs. Regression-based SAEAs employ regression-based surrogates such as   \n73 Kriging [36, 39] to approximate either the objective values of solutions or the objective functions   \n74 of expensive problems [22]. To maintain solution diversity, ParEGO [24] employs a Kriging model   \n75 to iteratively approximate an aggregate objective function which aggregates all objectives into one   \n76 via a set of pre-defined scale vectors. In MOEA/D-EGO [43], plenty of scale vectors are generated   \n77 uniformly to decompose the target MOOP into many single-objective subproblems. K-RVEA [5] also   \n78 designs a set of scale vectors as reference vectors to maintain solution diversity. Similarity or density   \n79 estimation is an alternative option for maintaining diversity. For instance, KTA2 [34] estimates the   \n80 distribution status of non-dominated solutions by defining a similarity or density indicator.   \n81 Classification-based SAEAs. In model-based optimization, the optimization is guided by the relation   \n82 between solutions rather than accurate objective values. Therefore, there is a tendency for recently   \n83 proposed SAEAs to use classification-based surrogates to learn the relation between solutions directly.   \n84 CSEA [31] trains a neural network to justify whether candidate solutions can be dominated by given   \n85 reference points or not. $\\theta$ -DEA-DP [42] uses two neural networks to predict the Pareto dominance   \n86 relation and $\\theta^{}$ -dominance relation between two solutions, respectively. REMO [17] employs a   \n87 neural network to fit a ternary classifier, which is able to learn the dominance relation between   \n88 pairs of solutions. Compared with regression-based SAEAs, although classification-based SAEAs   \n89 take advantage of learning solution relations directly, their drawbacks are also clear: The prediction   \n90 of solution relations lacks the information of how solutions are distributed in the objective space,   \n91 making it difficult for classification-based SAEAs to maintain solution diversity. In [31, 17], a radial   \n92 projection selection approach is adapted to select diverse reference points. However, its effect on   \n93 diversity maintenance is limited. In addition, although classification-based SAEAs maintain only one   \n94 surrogate, the cost of learning pairwise relations from large datasets is inevitably increased.   \n95 SAEAs based on Other Surrogates. HSMEA [15] uses an ensemble of multiple surrogates in the   \n96 optimization. In addition, a new category of surrogates, namely ordinal regression surrogate [40] or   \n97 level-based classification surrogate [28], is proposed recently to combine regression-based surrogates   \n98 with classification-based surrogates. However, the shortcoming remains the same as these surrogates   \n99 lack the information of solution distribution, especially when the number of objectives is large. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "100 2.2 Multi-Objective Bayesian Optimization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "101 MOBO. Bayesian Optimization (BO) [35, 18] is also a typical model-based optimization method   \n102 for expensive optimization, while multi-objective BO (MOBO) methods are designed for expensive   \n103 MOOPs [7, 8, 27, 1]. Some MOBO generalizes the acquisition functions such as upper confidence   \n104 bound (UCB) [46], expected improvement (EI) [14], Thompson sampling [3], to solve expensive   \n105 MOOPs. In addition, entropy search methods have also been employed in MOBO [2, 37]. To   \n106 maintain solution diversity, the EI of a multi-objective performance indicator, Hypervolume (HV)   \n107 [45], was used as the acquisition function in recent MOBO [6, 27]. Based on the Hypervolume   \n108 improvement (HVI), PSL [27] proposes a learning method to approximate the whole Pareto set for   \n109 MOBO, and PDBO [1] automatically selects the best acquisition function for objective functions   \n110 in each iteration. However, the time complexity of computing HV increases exponentially with the   \n111 number of objectives, which may limit the application of MOBO methods on optimization problems   \n112 with many objectives.   \n113 Connection to SAEAs. Both SAEAs and MOBO are model-based optimization methods. A SAEA   \n114 is also a MOBO if it uses probability models as surrogates, and a MOBO is also a SAEA if it searches   \n115 candidate solutions with evolutionary search algorithms. Therefore, some model-based optimization   \n116 methods belong to both SAEAs and MOBO [24, 14, 43]. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "117 3 LORA-MOO: Optimization via Learning Ordinal Relations and Angles ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "118 This section first introduces the LORA-MOO framework, followed by detailed algorithm descriptions. ", "page_idx": 2}, {"type": "text", "text": "119 3.1 LORA-MOO Framework ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "120 The pseudocode of LORA-MOO is depicted in Alg. 1, it consists of four phases: ", "page_idx": 2}, {"type": "text", "text": "121 1. Initialization: An initial dataset of size $11D\\mathrm{~-~}1$ (As suggested in the literature [24]) are   \n122 sampled from the decision space using the Latin hypercube sampling (LHS) [30] (line 1),   \n123 where $D$ is the dimensionality of decision variables. The sampled solutions are evaluated on   \n124 objective functions $f$ and then saved in an archive $S_{A}$ (line 2).   \n125 2. Surrogate modeling: For all solutions $\\pmb{x}\\in S_{A}$ , quantify their ordinal values (line 4) and   \n126 calculate their angular coordinates (line 9). The set of ordinal values $S_{o}$ is used to train   \n127 the ordinal surrogate $h_{o}$ (line 5). The angular coordinates are used to fit $M-1$ angular   \n128 surrogates $h_{a i}$ separately (line 10).   \n129 3. Sampling (Search and Selection): Run an optimizer on surrogate $h_{o}$ to generate a population   \n130 of candidate solutions $P$ (line 6). Select optimal candidate solutions $\\pmb{x}_{1}^{*}$ , $\\pmb{x}_{2}^{*}$ from $P$ based   \n131 on surrogates $h_{o}$ , $h_{a i}$ , respectively (lines 7 and 11).   \n132 4. Update: Evaluate new optimal candidate solutions $\\pmb{x}_{1}^{*}$ , $\\pmb{x}_{2}^{*}$ on expensive objective functions   \n133 $f$ , update archive $S_{A}$ and the number of used function evaluations $F E$ (lines 8 and 12). The   \n134 algorithm will go to phase 2 until the evaluation budget $F E_{m a x}$ has run out. ", "page_idx": 2}, {"type": "text", "text": "Input: $M$ objective functions of the optimization problem $f(\\pmb{x})=(f_{1}(\\pmb{x}),\\dots,f_{M}(\\pmb{x}))$ ; Evaluation budget: The number of allowed function evaluations $F E_{m a x}$ . ", "page_idx": 3}, {"type": "text", "text": "Procedure: ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "1: Sample a set of solutions $\\{x_{1},\\dots,x_{11D-1}\\}$ and evaluate them on $f$ .   \n2: Save all evaluated solutions $({\\pmb x},f({\\pmb x}))$ in an archive $S_{A}$ . Set the number of used function   \nevaluations $F E=|S_{A}|$ .   \n3: while $F E<F E_{m a x}$ do   \n4: Ordinal training set $S_{o}\\gets$ Quantify ordinal values for all $\\mathbf{\\boldsymbol{x}}_{i}\\in S_{A}$ (Alg. 2).   \n5: Ordinal surrogate $h_{o}\\gets\\mathrm{Train}\\ \\mathrm{Kriging}(S_{A},S_{o})$ .   \n6: Population of candidate solutions $P\\leftarrow\\mathrm{Run}$ an optimizer on $h_{o}$ (Alg. 3).   \n7: $x_{1}^{*}\\gets\\mathrm{Use}$ the ordinal surrogate to select a solution from $P$ by convergence criterion.   \n8: Evaluate $\\pmb{x}_{1}^{*}$ and update $S_{A}\\stackrel{\\cdot}{=}S_{A}\\cup\\{(\\pmb{x}_{1}^{*},f(\\pmb{x}_{1}^{*}))\\}$ , $F E=F E+1$ .   \n9: Angular training set $S_{a}\\gets$ Calculate angular coordinates for all $\\mathbf{\\boldsymbol{x}}_{i}\\in S_{A}$ .   \n10: $M\\!-\\!1$ angular surrogates $h_{a i}\\leftarrow$ Train Kriging $(S_{A},S_{a})$ , $i=1,\\dots,M-1.$ .   \n11: $\\pmb{x}_{2}^{*}\\leftarrow\\mathrm{Use}$ angular surrogates to select a solution from $P$ by diversity criterion (Alg. 4).   \n12: Evaluate $\\pmb{x}_{2}^{*}$ and update $\\bar{S}_{A}=S_{A}\\cup\\{(\\pmb{x}_{2}^{*},f(\\pmb{x}_{2}^{*}))\\}$ , $F E=F E+1$ .   \n13: end while ", "page_idx": 3}, {"type": "text", "text": "", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Output: Non-dominated solutions in archive $S_{A}$ . ", "page_idx": 3}, {"type": "text", "text": "135 3.2 Surrogate Modeling ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "136 The ordinal surrogate $h_{o}$ is mainly trained on dominance-based ordinal relations, additional clustering  \n137 based artificial ordinal relations will be introduced for training if the number of objectives $M$ is   \n138 large. In addition, for an $M$ -objective problem, $M\\!-\\!1$ angular surrogates $h_{a i}$ are trained on angular   \n139 coordinates. These surrogates are used in the selection procedure for solution diversity but are idle in   \n140 the search procedure. ", "page_idx": 3}, {"type": "text", "text": "141 3.2.1 Learning dominance-based ordinal relations. ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "142 In LORA-MOO, the concept of ordinal regression [40] is adapted to learn dominance-based ordinal   \n143 relations. Clearly, the dominance-based ordinal relation between a set of reference points $S_{R P}$ and a   \n144 given solution $\\textbf{\\em x}$ is quantified as a relation value. Such a relation value is a numerical value that used   \n145 for training the ordinal-regression surrogate $h_{o}$ . The quantification of relation values consists of two   \n146 steps: The selection of reference points $S_{R P}$ and the computation of relation values.   \n147 Selection of Reference Points. We propose the definition of $\\lambda$ -dominance relationship to simplify   \n148 the selection of reference points. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "149 Definition 1. ( $\\lambda$ -Dominance Relationship) ", "page_idx": 3}, {"type": "text", "text": "150 A solution $x^{1}$ is said to $\\lambda$ -dominate another solution $x^{2}$ (denoted by $x^{1}\\prec_{\\lambda}x^{2})$ if and only if: ", "page_idx": 3}, {"type": "equation", "text": "$$\ng_{\\lambda}(\\pmb{x}^{1})\\prec g_{\\lambda}(\\pmb{x}^{2}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "151 where $\\lambda\\geq0$ is the dominance coefficient and $g_{\\lambda}$ is a smooth objective function defined as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nf_{i n}({\\pmb x})=\\frac{f_{i}({\\pmb x})-z_{i}^{*}}{|z_{i}^{n a d}-z_{i}^{*}|},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "152 ", "page_idx": 3}, {"type": "equation", "text": "$$\ng_{\\lambda,i}(\\pmb{x})=f_{i n}(\\pmb{x})+\\lambda m a x(f_{j n}(\\pmb{x})),j\\in\\{1,\\dots,M\\},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "153 where $f_{i n}$ denotes a normalized objective function, $z^{*}=\\{z_{1}^{*},\\dots,z_{M}^{*}\\}$ , $z^{n a d}=\\{z_{1}^{n a d},\\dots,z_{M}^{n a d}\\}$   \n154 are ideal point and nadir point for the current non-dominated solutions, respectively.   \n155 More detailed definitions about the background of MOO are available in Appendix A. All non- $\\cdot\\lambda$ -   \n156 dominated solutions in $S_{A}$ are selected as reference points $S_{R P}$ . There are two reasons to introduce   \n157 the definition of $\\lambda$ -dominance:   \n158   \n159   \n160   \n161   \n162 ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "\u2022 The $\\lambda$ -dominance can smoothen the original PF by excluding dominance resistant solutions (DRSs) [16, 38]. DRSs are solutions that are best or close to best on one or several objectives but extremely poor on at least one of the remaining objectives. Such a solution is apparently not desirable but may be regarded as one of the best solutions since there may not exist any other solutions dominating it in the solution set. ", "page_idx": 3}, {"type": "text", "text": "163 \u2022 Second, $\\lambda$ -dominance can eliminate some similar non-dominated solutions from the Pareto   \n164 set, which can be used to adjust the size of Pareto set. When the number of objectives $M$   \n165 is large, it is possible that a majority of past evaluated samples are non-dominated to each   \n166 other. To balance the number of reference points and remaining samples, we introduce the   \n167 dominance coefficient $\\lambda$ to sightly reduce the ratio of reference points in $S_{A}$ . This alleviates   \n168 the situation of extreme imbalance of samples in different ordinal levels (see the division of   \n69 ordinal levels below).   \n170 Computation of Relation Values. To quantify ordinal relation values, we first calculate extension   \n171 coefficients $e c({\\pmb x})$ for each $\\pmb{x}\\in S_{A}$ . $e c({\\pmb x})$ is defined as the minimal coefficient $e c\\geq1$ to make a   \n172 solution $\\textbf{\\em x}$ non- $\\lambda$ -dominated to all solutions $\\mathbf{\\nabla}x^{\\prime}$ in the extended reference: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\ne c(\\pmb{x})=\\arg\\operatorname*{min}_{e c\\geq1}\\nexists\\pmb{x}^{\\prime}\\in S_{R P}:(\\pmb{x}^{\\prime}\\ast e c)\\prec_{\\lambda}\\pmb{x}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "173 Although extension coefficient $e c({\\pmb x})$ quantifies the distance between a solution $\\textbf{\\em x}$ and reference $S_{R P}$ ,   \n174 it has not been used to train the ordinal regression-based surrogate directly. To generate a stable   \n175 ordinal regression-based surrogate, solutions in $S_{A}$ are divided into $N_{o}=m a x(n_{o},|S_{A}|/|S_{R P}|)$   \n176 ordinal levels, where $n_{o}$ is a pre-defined parameter denoting the minimal number of ordinal levels.   \n177 The solutions in $S_{R P}$ are classified into the non-dominated ordinal level, thus the relation value $v_{1}=$   \n178 1.0 is assigned to them. Remaining solutions in $S_{A}$ are sorted by their extension coefficients $e c({\\pmb x})$   \n179 and then divided into $N_{o}$ -1 ordinal levels uniformly. The relation value $\\begin{array}{r}{v_{i}\\,=\\,1\\,-\\,\\frac{i-1}{N_{o}-1}}\\end{array}$ will be   \n180 assigned to the solutions $\\textbf{\\em x}$ in the $i^{t h}$ ordinal level. Lastly, relation values serve as radial coordinates   \n181 and a Kriging model is employed to approximate them. ", "page_idx": 4}, {"type": "text", "text": "182 3.2.2 Artificial clustering-based ordinal relations. ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "183 When the number of objectives $M$ is large, most evaluated solutions in archive $S_{A}$ could be non  \n184 dominated solutions, indicating that these solutions will be divided into the same non-dominated   \n185 ordinal level and thus treated as reference points $S_{R P}$ . This is harmful to the ordinal surrogate   \n186 modeling due to the extreme imbalance between the numbers of training samples in different ordinal   \n187 levels. To reduce the ratio of $S_{R P}$ , we use a clustering method to generate $n_{\\ast}$ _clusters clusters   \n188 for $S_{R P}$ , where $n_{\\ast}$ _clusters is the half of the size of $S_{R P}$ . All solutions $\\pmb{x}\\in S_{R P}$ are mapped to   \n189 the closest cluster centers. The solutions with the shortest projection on each cluster center will be   \n190 selected as the new $S_{R P}$ , while the remaining solutions will be moved to the next ordinal level. Such   \n191 artificial ordinal relations greatly reduce the ratio of $S_{R P}$ in $S_{A}$ . In LORA-MOO, we set a ratio   \n192 threshold rp_ratio for $S_{R P}$ , once the ratio of $S_{R P}$ is larger than rp_ratio, artificial ordinal relations   \n193 will be generated for surrogate modeling. Details are available in Appendix C, Alg. 2 and Fig. 5. ", "page_idx": 4}, {"type": "text", "text": "194 3.2.3 Surrogates for Angular Coordinates. ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "195 Given a solution $\\pmb{x}\\in S_{A}$ with Cartesian coordinates $(f_{1}({\\pmb x}),\\dots,f_{M}({\\pmb x}))$ , The angular coordinates   \n196 of solution $\\textbf{\\em x}$ are transformed with the following rules: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\varphi_{i}=a r c c o s\\frac{f_{i}(x)-z_{i}^{*}}{\\sqrt{(f_{i}(x)-z_{i}^{*})^{2}+\\cdot\\cdot\\cdot+(f_{M}(x)-z_{M}^{*})^{2}}},i=1,\\ldots,M-1,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "197 where $z^{*}$ is the ideal point. The resulting angular coordinates $\\displaystyle\\left(\\varphi_{1},\\ldots,\\varphi_{M-1}\\right)$ are used to fti $M-1$   \n198 regression-based surrogates separately. In LORA-MOO, we use the Kriging model to approximate   \n199 angular coordinates. The introduction and usage of Kriging model is given in Appendix $\\mathbf{B}$ . ", "page_idx": 4}, {"type": "text", "text": "200 3.3 Sampling: Search and Selection ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "201 In this subsection, we describe how to use surrogate $h_{o}$ to search for candidate solutions and how to   \n202 use surrogates $h_{o}$ and $h_{a i}$ to select optimal ones from candidate solutions for expensive evaluations. ", "page_idx": 4}, {"type": "text", "text": "203 3.3.1 Search: Generation of Candidate Solutions. ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "204 An advantage of LORA-MOO is that it searches for candidate solutions on ordinal surrogate $h_{o}$   \n205 only, leaving all angular surrogates $h_{a i}$ idle in this search procedure. This saves a lot of time from   \n206 predicting with all surrogates. LORA-MOO employs an optimizer (e.g. PSO [13]) to generate a   \n207 population of candidate solutions $P$ (Detailed pseudo-code is available in Appendix C, Alg. 3). The   \n208 initial population for optimization search consists of two parts. The first half initial solutions are   \n209 generated randomly from the decision space, while the remaining initial solutions are mutants of   \n210 current reference points $S_{R P}$ . To ensure the diversity of initial candidate solutions, a KNN clustering   \n211 method is applied to divide $S_{R P}$ into several different clusters, from each cluster, an equal number of   \n212 mutants are generated as initial candidate solutions. The global optimal population $P$ produced by   \n213 PSO is the candidate solutions for further environmental selection. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "214 3.3.2 Selection Criteria. ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "215 To take both convergence and diversity into consideration, in each iteration, LORA-MOO selects two   \n216 optimal candidate solutions $\\mathbf{\\boldsymbol{x}}_{1}^{*},\\mathbf{\\boldsymbol{x}}_{2}^{*}$ from $P$ for objective function evaluations. $\\mathbf{\\boldsymbol{x}}_{1}^{*},\\mathbf{\\boldsymbol{x}}_{2}^{*}$ are sampled on   \n217 the basis of convergence and diversity, respectively.   \n218 Convergence Criterion for environmental selection is the expected improvement (EI) [14] of ordinal   \n219 values, which is similar to many MOBO methods [24, 43]. Since the output of our ordinal surrogate   \n220 $h_{o}(x)$ is an 1-D numerical value, the solution with maximal 1-D EI in $P$ is selected as $\\pmb{x}_{1}^{*}$ .   \n221 Diversity Criterion to sample $\\pmb{x}_{2}^{*}$ from $P$ is defined as angles $d_{a n g}$ between candidate solutions   \n222 and reference points $S_{R P}$ . Firstly, the minimal degree between each candidate solution and $S_{R P}$ is   \n223 measured. Among these minimal degrees $m d_{a n g}$ , the solution with ma $\\mathbf{\\nabla}\\mathbf{x}(m d_{a n g})$ is selected as $\\pmb{x}_{2}^{*}$   \n224 (Detailed pseudo-code is available in Appendix C, Alg. 4). ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "225 4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "226 To evaluate the optimization performance of LORA-MOO on expensive MOOPs, we conduct   \n227 experiments to compare LORA-MOO with other SAEAs on different MOOPs, including a series   \n228 of scalable multi-/many-objective benchmark optimization problems DTLZ [11], WFG [19], and a   \n229 real-world network architecture search (NAS) problem. ", "page_idx": 5}, {"type": "text", "text": "230 4.1 Experimental Setups ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "231 Optimization Problem Setup. To ensure a fair comparison, the following optimization problem   \n232 setup is the same as the setup that has been widely used in the literature [5, 31, 34, 17]. In our   \n233 experiments, initial datasets of size $F E_{i n i t}=11\\mathrm{~}D\\cdot1$ are used to initialize surrogates, while the   \n234 maximum number of allowed evaluations $F E_{m a x}$ is 300. The statistical results are obtained from 30   \n235 independent runs. For each run, different comparison algorithms share the same initial dataset.   \n236 Comparison Algorithms. We compare LORA-MOO with 6 state-of-the-art SAEAs, some of them   \n237 also known as MOBO methods. These comparison algorithms can be classified into three categories:   \n238 \u2022 Regression-based MOO methods: ParEGO [24], K-RVEA [5], and KTA2 [34]. ParEGO is a   \n239 classic regression-based SAEA and also a MOBO, which serves as a baseline. K-RVEA is a   \n240 typical SAEA which uses reference vector to guide the diversity maintenance. KTA2 is a   \n241 newly proposed algorithm to use an independent archive to keep solution diversity.   \n242 \u2022 Classification-based MOO methods: CSEA [31], REMO [17]. CSEA is a classic   \n243 classification-based SAEA which serves as a baseline. REMO is a newly proposed SAEA   \n244 which represents the state-of-the-art performance of classification-based SAEAs.   \n245 \u2022 Ordinal-regression-based MOO method: OREA [40] is a new category of SAEA that is   \n246 different from common regression-based and classification-based SAEAs. We compare with   \n247 it since it is directly related to our radial surrogate.   \n248 Note that some classic SAEAs and MOBO methods such as MOEA/D-EGO [43] and CPS-MOEA   \n249 [44] are not compared in our experiments as they failed to outperform other comparison algorithms   \n250 on any DTLZ problem [17]. Some HV-based MOBO methods are not compared as they are failed to   \n251 solve many objectives.   \n252 Parameter Setup. For the surrogate modeling, the Kriging models used in all comparison algorithms   \n253 are implemented using DACE [32], just as [24] suggested. For regression-based Kriging surrogates,   \n254 the range of hyper-parameter $\\theta\\in[10^{-5}$ , 100]. And for the neural networks in CSEA and REMO, the   \n255 parameters are the same as suggested in the literature. In the sampling strategy, the mutation operator   \n256 used to initialize candidate solutions is polynomial mutation [9], the mutation probability $p_{m}=1/d$   \n257 and mutation index $\\eta_{m}=20$ , as recommended in [34, 17]. The size of offspring population is 100.   \n258 The settings of the PSO optimizer are the range of hyper-parameter in the ordinal-regression-based   \n259 surrogate are the same as suggested in [40].   \n260 For the specific parameters exist in LORA-MOO, such as the dominance coefficient $\\lambda$ and the   \n261 threshold ratio of reference points to introduce clustering-based ordinal relations rp_ratio. As there   \n262 is no relevant study in the literature for their setups, we conducted ablation studies to investigate   \n263 the effect of these parameters on the performance of LORA-MOO. The results are summarized in   \n264 Section 4.2 and reported in Appendix F. The source code of LORA-MOO 2 will be available online.   \n265 Performance Indicator. To have a comprehensive estimation of optimization performance, we use   \n266 three different performance indicators in our experiments: The inverted generational distance (IGD)   \n267 [4], the inverted generational distance plus $(\\mathrm{IGD+})$ [20], and the Hypervolume (HV) [45]. IGD and   \n268 $\\mathrm{IGD+}$ use a set of truth Pareto front to measure the quality of a set of non-dominated solutions in   \n269 terms of convergence and diversity. A smaller IGD or $\\mathrm{IGD+}$ value indicates better MOO performance.   \n270 HV use a reference point to calculate the area covered by a set of non-dominated solutions, a large   \n271 HV value is preferable to MOO. See Appendix D for details and setups about performance indicators. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "image", "img_path": "hc9GkYw9kR/tmp/5bed366ec85226633f21380734ecd22e172af1749381c993b991a7add6b536f1.jpg", "img_caption": ["Figure 1: IGD curves averaged over 15 runs on the WFG5 problem instances for LORA-MOO with different parameter setups (shaded area is $\\pm$ std of the mean). "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "272 4.2 Ablation Studies ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "273 We conduct ablation studies on DTLZ and WFG benchmark problems with $D=10$ variables and   \n274 $M{=}\\{3,6,10\\}$ objectives. LHS [30] is used to sample initial dataset. The effects of four parameters   \n275 are investigated: They are the minimal number of ordinal levels $n_{o}$ , the dominance coefficient $\\lambda$ , the   \n276 ratio threshold of reference points $r p.$ _ratio, and the clustering number for reproduction $n_{c}$ . Three   \n277 representative results obtained on the WFG5 problem with 3 and 10 objectives are depicted in Fig. 1.   \n278 Complete results and statistical analysis of ablation studies are reported in Appendix F.   \n279 As shown in Fig. 1 (left), when $M=10$ , a large $n_{o}$ results in poor optimization performance. This is   \n280 because the ratio of non-dominated solutions in the archive tends to be large when $M$ is large, hence,   \n281 setting a large $n_{o}$ will lead to a lack of training samples in each dominated ordinal levels, which is   \n282 detrimental to the performance of surrogate modeling. As such, $n_{o}$ in LORA-MOO is set to 4.   \n283 The result in Fig. 1 (middle) shows that using $\\lambda$ -dominance to sightly modify the original dominance   \n284 relations is beneficial to the effectiveness of LORA-MOO. When $\\lambda=0$ , no $\\lambda$ -dominance would be   \n285 used and the corresponding LORA-MOO variant has the worst performance among all the variants. In   \n286 addition, setting a large $\\lambda$ could cause severe damage to the original dominance relations. Therefore,   \n287 we set $\\lambda$ to 0.2.   \n288 The effect of introducing artificial ordinal relations via clustering is demonstrated in Fig. 1 (right).   \n289 When the ratio threshold of reference points rp_ratio is 1 and $M=10$ , no artificial ordinal relations   \n290 are introduced to further divide ordinal levels for plenty of non-dominated solutions in the archive.   \n291 Consequently, the imbalance of sample numbers in different ordinal levels leads to poor optimization   \n292 performance. However, dominance relations are preferable to artificial ordinal relations when $M=3$   \n293 and the size of ordinal levels are well balanced. Hence, we set $r p\\_r a t i o=0.5$ . ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "294 4.3 Optimization on Benchmark Problems ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "295 The optimization performance of LORA-MOO is evaluated on DTLZ and WFG benchmark problems   \n296 with $D=10$ variables and $M{=}\\{3,\\,4,\\,6,\\,8,\\,10\\}$ objectives. The IGD values obtained on DTLZ   \n297 problems with different $M$ are reported in Table 1. It shows that LORA-MOO achieves the best   \n298 optimization results among all the comparison algorithms in terms of IGD values, followed by KTA2   \n299 and KRVEA. The IGD values obtained on the WFG problems, the $\\mathrm{IGD+}$ and HV results, and the   \n300 results obtained under different scales ( $D\\!=\\!5$ or 20) are reported in Appendix H. A consistent result   \n301 can be concluded from the $\\mathrm{IGD+}$ and HV values. The results on the 3- and 10-objective problems are   \nplotted in Fig. 2. ", "page_idx": 6}, {"type": "table", "img_path": "hc9GkYw9kR/tmp/1e47f08eee52a141b7e233fa0f38d7b02ffdbce47627d08ccd62cb6727e2f080.jpg", "table_caption": ["Table 1: Statistical results of the IGD value obtained by the comparison algorithms on the 35 DTLZ optimization problems over 30 runs. Symbols $\\mathbf{\\omega}^{\\star}+\\mathbf{\\omega}^{\\star}$ , $\\surd\\approx\\mathrel{}$ , \u2018\u2212\u2019 denote LORA-MOO is statistically significantly superior to, equivalent to, and inferior to the compared algorithms in the Wilcoxon rank sum test (significance level is 0.05), respectively. The last three rows are the total win/tie/loss results on DTLZ, WFG, and both of them, respectively. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "hc9GkYw9kR/tmp/9d365f1d87b47b7e7d83bab8dc727ee3d66457f1761f3c918ff2c515b41e7160.jpg", "img_caption": ["Figure 2: $\\mathrm{IGD}(\\mathrm{log})$ curves averaged over 30 runs on the DTLZ problems for the comparison algorithms (shaded area is $\\pm$ std of the mean). Top: 10 variables and 3 objectives. Bottom: 10 variables and 10 objectives. More figures are displayed in Appendices $\\mathrm{G}$ and $\\mathrm{H}$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Further comparison is conducted on a real-world network architecture search (NAS) problem, the best three algorithms listed in Table 1 are compared: LORA-MOO, KTA2, and KRVEA. The NAS problem tested is the NASbench201 implemented in EvoXBench [29], it has 6 variables and 5 objectives. Details of this NAS problem is provided in Appendix E. Considering NASbench201 is a real-world application and we do not know its exact PF, we use HV to evaluate optimization performance since HV can be calculated without the exact PF. In practice, $l o g(H V_{\\mathrm{diff}})$ is employed to amplify the visual difference of the obtained HV values: ", "page_idx": 8}, {"type": "equation", "text": "$$\nl o g(H V_{\\mathrm{diff}})=l o g(H V_{\\mathrm{max}}-H V)\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "304 where $H V_{\\mathrm{max}}$ is the maximal HV value on this problem that is provided in EvoXBench. ", "page_idx": 8}, {"type": "text", "text": "305 Fig. 3 plots the result. As can be seen in the figure, LORA-MOO outperforms KTA2 and KRVEA on this NAS problem. Although KTA2 and KRVEA have quicker convergence rate than LORA-MOO at the beginning of the optimization, both of them slow down their convergence speed as the number of evaluations increases. Particularly, KTA2 is trapped on local optima and thus fails to reach better results. In comparison, LORA-MOO reaches better NAS results when the evaluation number is larger than 250. ", "page_idx": 8}, {"type": "image", "img_path": "hc9GkYw9kR/tmp/dc7d65a4dce5af54f69909021bec8640846e4004cde1b522c8a093d14a67aec0.jpg", "img_caption": ["Figure 3: $L o g(H V_{\\mathrm{diff}})$ curves averaged over 30 runs on the NAS problem for the comparison algorithms. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "306 4.5 Runtime Comparison ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "307 We compare the runtime on benchmark problems for all the comparison algorithms to in  \n308 vestigate the relation between their optimization efficiency and the number of objectives $M$ . ", "page_idx": 8}, {"type": "text", "text": "09 Fig. 4 illustrates how the runtime of each comparison algorithm varies as the $M$ increases. It can be observed that the runtime of KTA2 increases exactly in the same rate as $M$ increases. In comparison, the runtime of LORA-MOO increases slightly when $M$ increases. This demonstrates that using angular surrogates only at the end of environmental selection process is beneficial to the optimization efficiency of LORA-MOO. In addition, the runtimes of ParEGO, CSEA, REMO, and OREA do not increase significantly with $M$ since they do not maintain specific surrogates to manage the diversity of non-dominated solutions. Consequently, their overall performance reported in Table 1 is not desirable. Overall, LORA-MOO finds a good trade-off between optimization efficiency and optimization results. ", "page_idx": 8}, {"type": "image", "img_path": "hc9GkYw9kR/tmp/6696c6be405970d4b33a9a1747589e367350e2caf5e0fd43f2bccaafc46ee366.jpg", "img_caption": ["Figure 4: Comparison of runtime averaged over 30 runs on benchmark problems $D=10$ variables and $M=3$ , 4, 6, 8, and 10 objectives for the comparison algorithms. For each algorithm, its runtimes are normalized by the runtime it costed on 3-objective problems. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "310 5 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "311 In this paper, we propose an efficient MOO method, LORA-MOO, to solve expensive MOOPs.   \n312 Different from existing surrogate modeling approaches, our LORA-MOO learns surrogate models   \n313 from ordinal relations and spherical coordinates. Only one ordinal surrogate is used in the model  \n314 based search, which hugely improve the efficiency of optimization. Our empirical studies have   \n315 demonstrated that our LORA-MOO significantly outperforms other state-of-the-art efficient MOO   \n316 methods, including SAEAs and MOBO methods. ", "page_idx": 8}, {"type": "text", "text": "317 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "318 [1] Alaleh Ahmadianshalchi, Syrine Belakaria, and Janardhan Rao Doppa. Pareto front-diverse batch   \n319 multi-objective Bayesian optimization. In Proceedings of the 38th AAAI Conference on Artificial   \n320 Intelligence (AAAI\u201924), pages 10784\u201310794, 2024.   \n321 [2] Syrine Belakaria, Aryan Deshwal, and Janardhan Rao Doppa. Max-value entropy search for   \n322 multi-objective Bayesian optimization. In Advances in Neural Information Processing Systems 32   \n323 (NeurIPS\u201919), pages 7825\u20137835, 2019.   \n324 [3] Syrine Belakaria, Aryan Deshwal, Nitthilan Kannappan Jayakodi, and Janardhan Rao Doppa.   \n325 Uncertainty-aware search framework for multi-objective Bayesian optimization. In Proceedings   \n326 of the 34th AAAI Conference on Artificial Intelligence (AAAI\u201920), pages 10044\u201310052, 2020.   \n327 [4] Peter AN Bosman and Dirk Thierens. The balance between proximity and diversity in multiob  \n328 jective evolutionary algorithms. IEEE Transactions on Evolutionary Computation, 7(2):174\u2013188,   \n329 2003.   \n330 [5] Tinkle Chugh, Yaochu Jin, Kaisa Miettinen, Jussi Hakanen, and Karthik Sindhya. A surrogate  \n331 assisted reference vector guided evolutionary algorithm for computationally expensive many  \n332 objective optimization. IEEE Transactions on Evolutionary Computation, 22(1):129\u2013142, 2016.   \n333 [6] Samuel Daulton, Maximilian Balandat, and Eytan Bakshy. Differentiable expected hypervol  \n334 ume improvement for parallel multi-objective Bayesian optimization. In Advances in Neural   \n335 Information Processing Systems 33 (NeurIPS\u201920), pages 9851\u20139864, 2020.   \n336 [7] Samuel Daulton, Maximilian Balandat, and Eytan Bakshy. Parallel Bayesian optimization   \n337 of multiple noisy objectives with expected hypervolume improvement. In Advances in Neural   \n338 Information Processing Systems 34 (NeurIPS\u201921), pages 2187\u20132200, 2021.   \n339 [8] Samuel Daulton, David Eriksson, Maximilian Balandat, and Eytan Bakshy. Multi-objective   \n340 Bayesian optimization over high-dimensional search spaces. In Proceedings of the 38th Conference   \n341 on Uncertainty in Artificial Intelligence (UAI\u201922), pages 507\u2013517, 2022.   \n342 [9] Kalyanmoy Deb and Mayank Goyal. A combined genetic adaptive search (GeneAS) for engi  \n343 neering design. Computer Science and Informatics, 26(4):30\u201345, 1996.   \n344 [10] Kalyanmoy Deb and Himanshu Jain. An evolutionary many-objective optimization algorithm   \n345 using reference-point-based nondominated sorting approach, part I: solving problems with box   \n346 constraints. IEEE Transactions on Evolutionary Computation, 18(4):577\u2013601, 2013.   \n347 [11] Kalyanmoy Deb, Lothar Thiele, Marco Laumanns, and Eckart Zitzler. Scalable test problems   \n348 for evolutionary multiobjective optimization. In Evolutionary Multiobjective Optimization, pages   \n349 105\u2013145. Springer, London, U.K., 2005.   \n350 [12] Xuanyi Dong and Yi Yang. Nas-bench-201: Extending the scope of reproducible neural archi  \n351 tecture search. In Proceedings of the 8th International Conference on Learning Representations   \n352 (ICLR\u201920), 2020.   \n353 [13] Russell Eberhart and James Kennedy. Particle swarm optimization. In Proceedings of the 1995   \n354 IEEE International Conference on Neural Networks (ICNN\u201995), pages 1942\u20131948, 1995.   \n355 [14] Michael TM Emmerich, Kyriakos C Giannakoglou, and Boris Naujoks. Single-and multiobjec  \n356 tive evolutionary optimization assisted by Gaussian random field metamodels. IEEE Transactions   \n357 on Evolutionary Computation, 10(4):421\u2013439, 2006.   \n358 [15] Ahsanul Habib, Hemant Kumar Singh, Tinkle Chugh, Tapabrata Ray, and Kaisa Miettinen. A   \n359 multiple surrogate assisted decomposition-based evolutionary algorithm for expensive multi/many  \n360 objective optimization. IEEE Transactions on Evolutionary Computation, 23(6):1000\u20131014,   \n361 2019.   \n362 [16] Thomas Hanne. On the convergence of multiobjective evolutionary algorithms. European   \n363 Journal of Operational Research, 117(3):553\u2013564, 1999.   \n364 [17] Hao Hao, Aimin Zhou, Hong Qian, and Hu Zhang. Expensive multiobjective optimization by   \n365 relation learning and prediction. IEEE Transactions on Evolutionary Computation, 26(5):1157\u2013   \n366 1170, 2022.   \n367 [18] Xiaobin Huang, Lei Song, Ke Xue, and Chao Qian. Stochastic Bayesian optimization with   \n368 unknown continuous context distribution via kernel density estimation. In Proceedings of the 38th   \n369 AAAI Conference on Artificial Intelligence (AAAI\u201924), pages 12635\u201312643, 2024.   \n370 [19] Simon Huband, Philip Hingston, Luigi Barone, and Lyndon While. A review of multiobjective   \n371 test problems and a scalable test problem toolkit. IEEE Transactions on Evolutionary Computation,   \n372 10(5):477\u2013506, 2006.   \n373 [20] Hisao Ishibuchi, Hiroyuki Masuda, Yuki Tanigaki, and Yusuke Nojima. Modified distance   \n374 calculation in generational distance and inverted generational distance. In Proceedings of the   \n375 8th International Conference on Evolutionary Multi-criterion Optimization (EMO\u201915), pages   \n376 110\u2013125, 2015.   \n377 [21] M. Janga Reddy and D. Nagesh Kumar. Evolutionary algorithms, swarm intelligence methods,   \n378 and their applications in water resources engineering: A state-of-the-art review. H2Open Journal,   \n379 3(1):135\u2013188, 2021.   \n380 [22] Yaochu Jin. A comprehensive survey of fitness approximation in evolutionary computation.   \n381 Soft Computing, 9(1):3\u201312, 2005.   \n382 [23] Donald R. Jones, Matthias Schonlau, and William J. Welch. Efficient global optimization of   \n383 expensive black-box functions. Journal of Global Optimization, 13(4):455\u2013492, 1998.   \n384 [24] Joshua Knowles. ParEGO: A hybrid algorithm with on-line landscape approximation for   \n385 expensive multiobjective optimization problems. IEEE Transactions on Evolutionary Computation,   \n386 10(1):50\u201366, 2006.   \n387 [25] Ke Li, Kalyanmoy Deb, Qingfu Zhang, and Sam Kwong. An evolutionary many-objective opti  \n388 mization algorithm based on dominance and decomposition. IEEE Transactions on Evolutionary   \n389 Computation, 19(5):694\u2013716, 2014.   \n390 [26] Lin Lin and Mitsuo Gen. Hybrid evolutionary optimisation with learning for production   \n391 scheduling: State-of-the-art survey on algorithms and applications. International Journal of   \n392 Production Research, 56(1-2):193\u2013223, 2018.   \n393 [27] Xi Lin, Zhiyuan Yang, Xiaoyuan Zhang, and Qingfu Zhang. Pareto set learning for expen  \n394 sive multi-objective optimization. In Advances in Neural Information Processing Systems 35   \n395 (NeurIPS\u201922), pages 19231\u201319247, 2022.   \n396 [28] Zhuo Liu, Xiaolin Xiao, Feng-Feng Wei, and Wei-Neng Chen. A classification-assisted level  \n397 based learning evolutionary algorithm for expensive multiobjective optimization problems. In Pro  \n398 ceedings of the 24th Annual Conference on Genetic and Evolutionary Computation (GECCO\u201922),   \n399 pages 547\u2013555, 2022.   \n400 [29] Zhichao Lu, Ran Cheng, Yaochu Jin, Kay Chen Tan, and Kalyanmoy Deb. Neural architec  \n401 ture search as multiobjective optimization benchmarks: Problem formulation and performance   \n402 assessment. IEEE Transactions on Evolutionary Computation (Early Access), 2023.   \n403 [30] Michael D. McKay, Richard J. Beckman, and William J. Conover. A comparison of three   \n404 methods for selecting values of input variables in the analysis of output from a computer code.   \n405 Technometrics, 42(1):55\u201361, 2000.   \n406 [31] Linqiang Pan, Cheng He, Ye Tian, Handing Wang, Xingyi Zhang, and Yaochu Jin. A   \n407 classification-based surrogate-assisted evolutionary algorithm for expensive many-objective opti  \n408 mization. IEEE Transactions on Evolutionary Computation, 23(1):74\u201388, 2018.   \n409 [32] Jerome Sacks, William J. Welch, Toby J. Mitchell, and Henry P. Wynn. Design and analysis of   \n410 computer experiments. Statistical Science, 4(4):409\u2013423, 1989.   \n411 [33] Palwasha W. Shaikh, Mohammed El-Abd, Mounib Khanafer, and Kaizhou Gao. A review on   \n412 swarm intelligence and evolutionary algorithms for solving the traffic signal control problem.   \n413 IEEE Transactions on Intelligent Transportation Systems, 23(1):48\u201363, 2020.   \n414 [34] Zhenshou Song, Handing Wang, Cheng He, and Yaochu Jin. A Kriging-assisted two-archive   \n415 evolutionary algorithm for expensive many-objective optimization. IEEE Transactions on Evolu  \n416 tionary Computation, 25(6):1013\u20131027, 2021.   \n417 [35] Lei Song, Ke Xue, Xiaobin Huang, and Chao Qian. Monte Carlo tree search based variable se  \n418 lection for high dimensional Bayesian optimization. In Advances in Neural Information Processing   \n419 Systems 35 (NeurIPS\u201922), pages 28488\u201328501, 2022.   \n420 [36] Michael L. Stein. Interpolation of Spatial Data: Some Theory for Kriging. Springer Science &   \n421 Business Media, New York, NY, 1999.   \n422 [37] Shinya Suzuki, Shion Takeno, Tomoyuki Tamura, Kazuki Shitara, and Masayuki Karasuyama.   \n423 Multi-objective Bayesian optimization using Pareto-frontier entropy. In Proceedings of the 37th   \n424 International Conference on Machine Learning (ICML\u201920), pages 9279\u20139288, 2020.   \n425 [38] Zhenkun Wang, Yew-Soon Ong, and Hisao Ishibuchi. On scalable multiobjective test problems   \n426 with hardly dominated boundaries. IEEE Transactions on Evolutionary Computation, 23(2):217\u2013   \n427 231, 2018.   \n428 [39] Christopher KI Williams and Carl Edward Rasmussen. Gaussian Processes for Machine   \n429 Learning. MIT press, Cambridge, MA, 2006.   \n430 [40] Xunzhao Yu, Xin Yao, Yan Wang, Ling Zhu, and Dimitar Filev. Domination-based ordinal   \n431 regression for expensive multi-objective optimization. In Proceedings of the 2019 IEEE Symposium   \n432 Series on Computational Intelligence (SSCI\u201919), pages 2058\u20132065, 2019.   \n433 [41] Xunzhao Yu, Ling Zhu, Yan Wang, Dimitar Filev, and Xin Yao. Internal combustion engine   \n434 calibration using optimization algorithms. Applied Energy, 305:117894, 2022.   \n435 [42] Yuan Yuan and Wolfgang Banzhaf. Expensive multi-objective evolutionary optimization assisted   \n436 by dominance prediction. IEEE Transactions on Evolutionary Computation, 26(1):159\u2013173, 2022.   \n437 [43] Qingfu Zhang, Wudong Liu, Edward Tsang, and Botond Virginas. Expensive multiobjective   \n438 optimization by MOEA/D with gaussian process model. IEEE Transactions on Evolutionary   \n439 Computation, 14(3):456\u2013474, 2010.   \n440 [44] Jinyuan Zhang, Aimin Zhou, and Guixu Zhang. A classification and pareto domination based   \n441 multiobjective evolutionary algorithm. In Proceedings of the 17th IEEE Congress on Evolutionary   \n442 Computation (CEC\u201915), pages 2883\u20132890, 2015.   \n443 [45] Eckart Zitzler and Lothar Thiele. Multiobjective optimization using evolutionary algorithms - a   \n444 comparative case study. In Proceedings of the 5th International Conference on Parallel Problem   \n445 Solving from Nature (PPSN V), pages 292\u2013301, 1998.   \n446 [46] Marcela Zuluaga, Andreas Krause, et al. $\\epsilon$ -pal: An active learning approach to the multi  \n447 objective optimization problem. Journal of Machine Learning Research, 17(104):1\u201332, 2016. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "448 A Background of Many-Objective Optimization ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "449 We consider minimization problems and many-objective optimization problems (MOOPs) can be   \n450 formulated as follows: ", "page_idx": 12}, {"type": "text", "text": "Definition 2. (Expensive Many-Objective Optimization Problem) ", "page_idx": 12}, {"type": "text", "text": "Given $M$ expensive objective functions $f_{1},\\ldots,f_{M}$ and an evaluation budget $F E_{m a x}$ , obtain the Pareto set for the following many-objective optimization problem: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\operatorname*{argmin}_{\\pmb{x}\\in X}f(\\pmb{x})=(f_{1}(\\pmb{x}),\\dots,f_{M}(\\pmb{x}))\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "451 where $X\\subseteq\\mathbb{R}^{D}$ is the decision space of the problem. ", "page_idx": 12}, {"type": "text", "text": "452 The Pareto set is defined through the following definitions: Pareto set and Pareto front are defined as   \n453 follows: ", "page_idx": 12}, {"type": "text", "text": "Definition 3. Pareto dominance: ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A solution $x^{1}$ is said to dominate another solution $x^{2}$ (denoted by $x^{1}\\prec x^{2}$ ) if and only if: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall k\\in\\{1,2,\\ldots,M\\}:f_{k}(\\pmb{x}^{1})\\leq f_{k}(\\pmb{x}^{2})\\wedge}\\\\ &{\\exists k\\in\\{1,2,\\ldots,M\\}:f_{k}(\\pmb{x}^{1})<f_{k}(\\pmb{x}^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Definition 4. Non-dominated solution: ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "A non-dominated solution $x^{\\star}$ in the decision space $X$ is a solution that cannot be dominated by any other solutions in $X$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\nexists{\\pmb x}\\in X:{\\pmb x}\\prec{\\pmb x}^{\\star}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Definition 5. Pareto set: ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Pareto set $S_{p s}$ is the set of all non-dominated solutions in the decision space $X$ : ", "page_idx": 12}, {"type": "equation", "text": "$$\nS_{p s}=\\{\\pmb{x}^{\\star}\\in X|\\exists\\pmb{x}\\in X:\\pmb{x}\\prec\\pmb{x}^{\\star}\\}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "Definition 6. Pareto front: ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Pareto front $S_{p f}$ is the corresponding unique set of the Pareto set in the objective space: ", "page_idx": 12}, {"type": "equation", "text": "$$\nS_{p f}=\\{f(x)|x\\in S_{p s}\\}\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "454 B Kriging Model ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "455 Kriging model, also known as Gaussian process model [23] or design and analysis of computer   \n456 experiments (DACE) model [32], is a stochastic process model used to approximate an unknown   \n457 objective function. LORA-MOO uses Kriging models to implement angular surrogates and the radial   \n458 surrogate, to avoid potential confusion and help the understanding of our algorithm, the working   \n459 mechanism of the Kriging model is described below.   \n460 A common way to approximate an unknown objective function with $n$ observations is linear regression:   \n461 ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "equation", "text": "$$\ny(\\pmb{x}^{i})=\\sum_{k=1}^{N}\\beta_{k}f_{k}(\\pmb{x}^{i})+\\epsilon^{i},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "462 where $\\pmb{x}^{i}$ is the $i^{t h}$ sample point observed from the objective function. $f_{k}({\\pmb x}^{i}),\\,\\beta_{k}$ are a linear or   \n463 nonlinear function of $\\pmb{x}^{i}$ and its coefficient, respectively. $N$ is the number of functions $f({\\boldsymbol{x}})$ . $\\epsilon^{i}$ is an   \n464 independent error term, which is normally distributed with mean zero and variance $\\sigma^{2}$ .   \n465 However, a stochastic process model such as Kriging does not assume that the error terms $\\epsilon$ are   \n466 independent. Hence, an error term $\\epsilon^{i}$ is rewritten as $\\epsilon(\\bar{\\pmb{x}}^{i})$ . Moreover, these error terms are assumed   \n467 to be related or correlated to each other. The correlation between two error terms $\\epsilon(\\pmb{x}^{i})$ and $\\epsilon(\\pmb{x}^{j})$ is   \n468 inversely proportional to the distance between the corresponding points [23]. The correlation function   \n469 in the Kriging model is defined as: ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "equation", "text": "$$\nC o r r(\\epsilon({\\pmb x}^{i}),\\epsilon({\\pmb x}^{j}))=e x p[-d i s({\\pmb x}^{i},{\\pmb x}^{j})],\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "470 where the distance between two points $\\pmb{x}^{i}$ and $x^{j}$ are measured using the special weighted distance   \n471 formula shown below: ", "page_idx": 12}, {"type": "equation", "text": "$$\nd i s({\\pmb x}^{i},{\\pmb x}^{j})=\\sum_{k=1}^{D}\\theta_{i}|x_{k}^{i}-x_{k}^{j}|^{p_{k}},\n$$", "text_format": "latex", "page_idx": 12}, {"type": "text", "text": "472 where $D$ is the number of decision variables, $\\pmb{\\theta}\\in\\mathbb{R}_{\\geq0}^{D}$ and $\\mathbf{p}\\in[1,2]^{D}$ are parameters of the Kriging   \n473 model. It can be seen from Eq.(7) that the correlation is ranged within $(0,1]$ and is increasing as the   \n474 distance between two points decreases. Particularly, in Eq.(8), the parameter $\\theta_{k}$ can be explained as   \n475 the importance of the decision variable $x_{k}$ , and the parameter $p_{k}$ can be interpreted as the smoothness   \n476 of the correlation function in the $k^{t h}$ coordinate direction.   \n477 Due to the effectiveness of correlation modelling, the regression model in Eq.(6) can be simplified   \n478 without degrading modelling performance [23]. Clearly, all regression terms are replaced with a   \n479 constant term, thus the Kriging regression model can be rewritten as follows: ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "equation", "text": "$$\ny(\\pmb{x}^{i})=\\mu+\\epsilon(\\pmb{x}^{i}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "480 where $\\mu$ is the mean of this stochastic process, $\\epsilon(\\pmb{x}^{i})\\sim\\mathcal{N}(0,\\sigma^{2})$ . ", "page_idx": 13}, {"type": "text", "text": "481 B.1 Training the Kriging model ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "482 To train the Kriging model and estimate the parameters $\\mathbf{\\nabla}\\theta,\\mathbf{p}$ in Eq.(8), the following likelihood   \n483 function is maximised: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\frac{1}{(2\\pi)^{n/2}(\\sigma^{2})^{n/2}|\\mathbf{R}|^{1/2}}e x p[-\\frac{(\\mathbf{y}-\\mathbf{1}\\mu)^{T}\\mathbf{R}^{-1}(\\mathbf{y}-\\mathbf{1}\\mu)}{2\\sigma^{2}}],\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "484 where $|\\bf R|$ is the determinant of the correlation matrix, each element in the matrix is obtained using   \n485 Eq.(7). $\\mathbf{y}$ is the $n$ -dimensional vector of dependent variables that observed from the objective function.   \n486 The mean value $\\mu$ and variance $\\sigma^{2}$ in Eq.(9) and Eq.(10) can be estimated by: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\hat{\\mu}=\\frac{\\mathbf{1}^{T}\\mathbf{R}^{-1}\\mathbf{y}}{\\mathbf{1}^{T}\\mathbf{R}^{-1}\\mathbf{1}},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "487 ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\hat{\\sigma}=\\frac{1}{n}(\\mathbf{y}-\\mathbf{1}\\hat{\\mu})^{T}\\mathbf{R}^{-1}(\\mathbf{y}-\\mathbf{1}\\hat{\\mu}).\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "488 B.2 Prediction with the Kriging model ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "489 For a new solution $x^{*}$ , the Kriging model predicts the approximation of $\\hat{y}(\\pmb{x}^{\\ast})$ and the uncertainty   \n490 $\\hat{s}^{2}(x^{\\ast})$ as follows: ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\hat{y}(\\pmb{x}^{*})=\\hat{\\mu}+\\pmb{\\mathrm{r}}^{\\prime}\\pmb{\\mathrm{R}}^{-1}(\\pmb{\\mathrm{y}}-\\pmb{1}\\hat{\\mu}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "491 ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\hat{s}^{2}(x^{*})=\\hat{\\sigma}^{2}(1-\\mathbf{r}^{\\prime}\\mathbf{R}^{-1}\\mathbf{r}),\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "492 where $\\mathbf{r}$ is a $n$ -dimensional vector of correlations between $\\epsilon(x^{\\ast})$ and the error terms at the training   \n493 data, which can be calculated via Eq.(7).   \n494 Further details and a comprehensive description of the Kriging model and Gaussian Process can be   \n495 found in [39]. In this paper, all regression-based Kriging models have $\\pmb{\\theta}\\in[10^{-5},100]^{D}$ , ${\\mathfrak{p}}=2^{D}$ . ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "496 C Additional Description of LORA-MOO ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "497 This section describes LORA-MOO with more details. ", "page_idx": 13}, {"type": "text", "text": "498 C.1 Quantification of Ordinal Relations ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "499 In order to learn the ordinal landscape of MOOPs, we need to quantify the ordinal relations between   \n500 solutions into numerical values. Alg. 2 illustrates the pseudocode of quantifying ordinal relations3,   \n501 it describes line 4 in Alg. 1 of the main file. It can be seen that Alg. 2 is mainly working on the   \n502 quantification of dominance-based ordinal relations. Artificial ordinal relations will not be added   \n503 unless the ratio of reference points is larger than ratio threshold rpratio (line 5).   \n504 An illustration of artificial clustering-based ordinal relations is given in Fig. 5. By using clustering   \n505 methods, artificial ordinal relations are generated for training ordinal regression surrogates. Picking   \n506 one solution from each cluster ensures the diversity of non-dominated solutions in the first ordinal   \n507 level $L_{1}$ . Meanwhile, the selection within each cluster is based on the projection length on cluster   \n508 center, which is beneficial to the convergence of non-dominated solutions. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "$S_{A}$ : Archive of evaluated solutions;   \nrp_ratio: Ratio threshold of reference points in $S_{A}$ ;   \n$n_{o}$ : Minimal number of ordinal levels. ", "page_idx": 14}, {"type": "text", "text": "1: $S_{R P}\\leftarrow\\mathrm{Non}$ -dominated solutions in $S_{A}$ that are non- $\\lambda$ -dominated to any other solution in $S_{A}$ .   \n2: Non-dominated level (The first ordinal level) $L_{1}\\leftarrow S_{R P}$ .   \n3: The number of non-dominated ordinal levels $n_{n d l}=1$ .   \n4: Ratio of reference points $\\begin{array}{r}{r a t i o=\\frac{|S_{R P}|}{|S_{A}|}}\\end{array}$   \n5: if $r a t i o>r p_{r a t i o}$ then   \n6: $n_{n d l}=n_{n d l}+1$ .   \n$/*$ Add Artificial Ordinal Relations. $^{*}\\!/$   \n7: Divide $S_{R P}$ into |SR2P |clusters via KNN clustering.   \n8: For $\\textbf{\\em x}$ in each cluster, calculate the projection length of $\\textbf{\\em x}$ on the corresponding cluster center.   \n9: $L_{1}\\leftarrow$ Solutions $\\textbf{\\em x}$ with the shortest projection on each cluster.   \n10: $L_{2}\\leftarrow$ Remaining $\\frac{|S_{R P}|}{2}$ solutions in $S_{R P}$ . ", "page_idx": 14}, {"type": "text", "text": "11: end if ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "12: Calculate extension coefficient $e c({\\pmb x})$ for all $\\pmb{x}\\in S_{A}$ .   \n13: The number of ordinal levels $\\begin{array}{r}{N_{o}=\\operatorname*{max}(n_{o},\\frac{|S_{A}|}{|S_{R P}|})}\\end{array}$ .   \n14: According to the order of $e c({\\pmb x})$ , uniformly divide solutions $\\pmb{x}\\in(S_{A}-S_{R P})$ into $N_{o}$ -   \n$n_{n d l}$ levels.   \n15: Ordinal relation value $\\begin{array}{r}{v_{i}=1-\\frac{i-1}{N_{o}-1}}\\end{array}$ for $\\pmb{x}\\in L_{i}$ . ", "page_idx": 14}, {"type": "text", "text": "Output: An ordinal training set $S_{o}$ consisting of ordinal relation values $v_{i}$ ", "page_idx": 14}, {"type": "image", "img_path": "hc9GkYw9kR/tmp/a27f4a2a5cfa2dd09739871a2511dff49d6ac88caffda659d193504b501242af.jpg", "img_caption": ["Figure 5: Illustration of artificial clustering-based ordinal relations. Left: Non-dominated solutions without artificial ordinal relations. Right: Non-dominated solutions with artificial ordinal relations. Red solutions are new non-dominated solutions in $L_{1}$ , remaining blue solutions are moved to next ordinal level $L_{2}$ . Dash circles are clusters, green vectors are cluster centers. "], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "509 C.2 Generation of candidate solutions ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "510 Algo. 3 gives the pseudocode of generating candidate solutions, it is the implementation of line 6 in   \n511 Alg. 1 of the main flie. In lines 1-9, a population $P_{0}$ is generated. Since reference points $S_{R P}$ are the   \n512 optimal solutions in $S_{A}$ in terms of convergence, a half initial solutions are generated from $S_{R P}$ (lines   \n513 2-8). To obtain a diverse subset of $S_{R P}$ , LORA-MOO divides $S_{R P}$ into $n_{c}$ clusters before sampling   \n514 solutions (line 2). Once population initialization is completed (line 9), a normal PSO is conducted to   \n515 produce candidate solutions (lines 11-16). Please be noted that, although we are solving expensive   \n516 MOOPs, only a single ordinal surrogate $h_{o}$ is used in the reproduction process (line 14). This is a   \n517 great advantage of LORA-MOO since existing regression-based SAEAs involve all $M$ surrogates in   \n518 the reproduction process. Hence, LORA-MOO is more efficient than these regression-based SAEAs. ", "page_idx": 14}, {"type": "text", "text": "519 C.3 Angle-Based Diversity Selection ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "520 Alg. 4 gives the pseudocode of selecting the second optimal solution $\\pmb{x}_{2}^{*}$ from $P$ via our angle-based   \n521 diversity criterion, it is the implementation of line 11 in Alg. 1 of the main file. This angle-based ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "table", "img_path": "hc9GkYw9kR/tmp/edbe9a23a88a0739d3b693b3db5e5e8dee95dba8ca86d87a97eee0f290415897.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "Algorithm 4 Angle-Based Diversity Selection in LORA-MOO ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "$S_{R P}$ : Reference points used in the ordinal regression;   \n$P$ : Population of candidate solutions;   \n$h_{a1},\\ldots,h_{a(M-1)}\\colon M\\mathrm{-}1$ angular surrogates; ", "page_idx": 15}, {"type": "text", "text": "Procedure: ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1: $h(a i)(P)\\leftarrow$ Evaluate $P$ on angular surrogates $h_{a i}$ , $\\mathrm{i}=1,\\,.\\,.\\,,M-1$ .   \n2: for $j=2$ to $|P|$ do   \n3: $x_{j}\\gets$ The $j^{t h}$ solution in $P,\\,/^{*}$ Assume the first solution in $P$ is selected as $\\pmb{x}_{1}^{*}$ already. $^{\\ast}/$   \n4: $d_{a n g}\\leftarrow$ Calculate the angles between $\\pmb{x}_{j}$ and all reference points in $S_{R P}$ .   \n5: $m d_{a n g}\\leftarrow$ The angle between $\\pmb{x}_{j}$ and its nearest reference point.   \n6: end for   \n7: $\\pmb{x}_{2}^{*}\\gets$ The candidate solution in $P$ with maximal $m d_{a n g}$ .   \nOutput: The second candidate solution $\\pmb{x}_{2}^{*}$ .   \n522 diversity selection does not require extra parameters for generating guidance vectors, it selects the   \n523 candidate solution that is mostly deviate from solutions in $S_{R P}$ . Note that all angular surrogates are   \n524 only used to evaluate one population $P$ during the whole reproduction and environmental selection   \n525 procedures. Therefore, although LORA-MOO fits $M$ surrogates in total (one ordinal surrogate and   \n526 M-1 angular surrogates), its runtime cost is less than other SAEAs which fit $M$ surrogates from   \n527 Cartesian coordinates. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "528 D Details of Performance Indicators Used in Our Experiments ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "529 In our experiments, we use IGD [4], $\\mathrm{IGD+}$ [20], and HV [45] to measure the performance of many   \n530 objective optimization. Both IGD and $\\mathrm{IGD+}$ require a subset of Pareto front as reference points. In   \n531 our experiments, the number of IGD/IGD $^+$ reference points is set to 5000 for 3-, 4-, and 6-objective   \n532 optimization problems, as widely used in the literature [40]. Considering the large objective space,   \n533 we set the number of $\\mathrm{IGD/IGD+}$ reference points to 10000 for 8- and 10-objective optimization   \n534 problems to achieve a more accurate estimation of optimization performance. The method proposed   \n535 in [25] is employed to generate well-distributed $\\mathrm{IGD/IGD+}$ reference points.   \n536 In comparison, the calculation of HV values does not require a subset of Pareto front as reference   \n537 points. For a set of non-dominated solutions, its HV is the volume in the objective space it dominates   \n538 from the set to a single reference point. Table 2 lists the reference point used for calculating HV   \n539 values. All HV values are calculated using the reference point and the normalized solutions. A   \n540 solution $\\textbf{\\em x}$ is normalize by the upper bound and lower bound of Pareto front: ", "page_idx": 15}, {"type": "table", "img_path": "hc9GkYw9kR/tmp/f240f5fa5021290ec856345320a5ee6ce63ffc2e03d9faa3d182a3af1d0402b5.jpg", "table_caption": ["Table 2: The HV reference points for all problems in this work. "], "table_footnote": [], "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "equation", "text": "$$\n\\frac{\\mathbf{\\nabla}x-l b_{p f}}{u b_{p f}-l b_{p f}},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "541 where $u b_{p f}$ , $l b_{p f}$ are the upper bound and lower bound of Pareto front, respectively. ", "page_idx": 16}, {"type": "text", "text": "542 E Details of the NASbench201 Problem ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "543 NASbench201 [12] are discrete optimization problems that aim to identify the optimal architecture 544 for neural networks. The search space is defined by a cell with 4 nodes inside, forming a directed acyclic graph as illustrated in Fig. 6. The decision variables are 6 edges, each edge is associated ", "page_idx": 16}, {"type": "image", "img_path": "hc9GkYw9kR/tmp/836b4ef219e0a5daa425a349f4195c10173ec5fd16121f4d1ff0101f50106e2a.jpg", "img_caption": ["Figure 6: Diagram of a network architecture in NASbench201. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "545 ", "page_idx": 16}, {"type": "text", "text": "546 with an operation selected from a predefined operation set {zeroize, skip-connect, 1x1 convolution,   \n547 $3\\mathrm{x}3$ convolution, 3x3 average pool}. Therefore, a network architecture can be encoded into a 6-   \n548 dimensional decision vector with 5 discrete numbers. In total, there are $\\mathrm{5^{6}}{=}15{,}625$ different candidates   \n549 for neural architecture search.   \n550 The optimization objectives in NASbench201 varies in different optimization problems. In this   \n551 paper, our NASbench201 problem consider 5 objectives, including the accuracy in CI-FAR10 dataset,   \n552 groundtruth floating point operations (FLOPs), the number of parameters, latency, and energy cost.   \n553 All these objectives are normalized to [0, 1] in the optimization. The optimization problem can be   \n554 formulated as ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "equation", "text": "$$\nF({\\pmb x})=\\{f_{a c c}({\\pmb x}),f_{F L O P s}({\\pmb x}),f_{p a r a m}({\\pmb x}),f_{l a t e n c y}({\\pmb x}),f_{e n e r g y}({\\pmb x})\\},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "555 where decision vector $\\pmb{x}\\in\\{0,1,2,3,4\\}^{6}$ . ", "page_idx": 16}, {"type": "text", "text": "556 F Complete Results of Ablation Studies ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "557 In this section, we report complete results of our ablation studies that are not displayed in the main   \n558 paper. We conduct four ablation studies to investigate the effect of the following four parameters on   \n559 the optimization performance of LORA-MOO. ", "page_idx": 16}, {"type": "text", "text": "1. $n_{o}$ : The minimal number of ordinal levels. A parameter in the modeling of our ordinalregression-based surrogate $h_{o}$ . ", "page_idx": 16}, {"type": "text", "text": "562 2. $\\lambda$ : The dominance coefficient. A parameter in the modeling of our ordinal-regression-based   \n563 surrogate $h_{o}$ .   \n564 3. $r p_{r a t i o}$ : The ratio threshold of reference points $S_{R P}$ . A parameter to determine whether to   \n565 introduce artificial ordinal relations via clustering.   \n566 4. $n_{c}$ : The number of clusters generated from reference points $S_{R P}$ to initialize PSO population.   \n567 A parameter in the generation of candidate solutions.   \n568 Setup of Ablation Studies. Our ablation studies are conducted on 7 DTLZ and 9 WFG benchmark   \n569 optimization problems. These benchmark problems have different features, such as unimodal, multi  \n570 modal, scaled, degenerated, and discontinuous. Therefore, the effect of four parameters can be   \n571 investigated comprehensively. Considering our paper focuses on many-objective optimization instead   \n572 of scalable optimization, we are interested in the optimization performance under different numbers   \n573 of objectives $M$ rather than the performance under different numbers of decision variables $D$ . Hence,   \n574 we set $D=10$ for all benchmark optimization problems, as suggested in literature [5, 31, 34, 17]. In   \n575 comparison, we set $M=\\left\\{3,6,10\\right\\}$ to observe the optimization performance with different objectives.   \n576 Other setups are the same as described in Section 4.1 of the main file. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "577 F.1 Influence of Minimal Number of Ordinal Levels $n_{o}$ . ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "578 This subsection investigates the influence of minimal number of ordinal levels $n_{o}$ on the optimization   \n579 performance. We set $n_{o}=\\{10,8,6,4,3\\}$ to generate five LORA-MOO variants. For all variants, in   \n580 this ablation study, we tentatively set $\\lambda=0.2$ , $r p_{r a t i o}=2/3$ , $n_{c}=5$ for a fair comparison. The $\\mathrm{IGD+}$   \n581 values obtained by five LORA-MOO variants with different $n_{o}$ are reported in Table 3.   \n582 In the last five rows of Table 3, the summary of statistical test results shows that $n_{o}=4$ is the optimal   \n583 parameter setup for LORA-MOO, because it is the only variant that is significantly superior to or   \n584 equivalent to all other variants. In comparison, the LORA-MOO variant with $n_{o}=10,\\,8,\\,6,\\,3$ are   \n585 significantly inferior to other 4, 1, 1, 2 LORA-MOO variants, respectively. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "586 F.2 Influence of Dominance Coefficient $\\lambda$ . ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "587 In this subsection, we analyze the influence of $\\lambda$ -dominance coefficient $\\lambda$ on the optimization   \n588 performance. We set $\\lambda=\\{0,0.1,0.2,0.3\\}$ to generate four LORA-MOO variants. As determined in   \n589 the previous ablation study, we set $n_{o}=4$ for all variants. The remaining two parameters $r p_{r a t i o}$ and   \n590 $n_{c}$ are set to $2/3$ and 5, respectively. The $\\mathrm{IGD+}$ values obtained by four LORA-MOO variants with   \n591 different $\\lambda$ are reported in Table 4.   \n592 The last four rows of Table 4 shows that $\\lambda=0.2$ is the optimal parameter setup for LORA-MOO.   \n593 The variant of $\\lambda=0.2$ is significantly superior to both the variants of $\\lambda=0$ and $\\lambda=0.1$ , and it is   \n594 equivalent to the variant of $\\lambda=0.3$ . We note that the variant of $\\lambda=0.3$ is also significantly superior   \n595 to both the variants of $\\lambda=0$ and $\\lambda=0.1$ . However, this variant wins/ties/losses 30/105/9 statistical   \n596 tests in total, while the variant of $\\lambda=0.2$ wins/ties/losses 32/109/3 statistical tests in total. Therefore,   \n597 setting $\\lambda=0.2$ is preferable to setting $\\lambda=0.3$ .   \n598 Note that all other LORA-MOO variants outperform the variant of $\\lambda=0$ , this implies that excluding   \n599 some samples from the set of non-dominated solutions is beneficial to the performance of ordinal   \n600 regression. The effectiveness of using our $\\lambda$ -dominance approach in LORA-MOO is demonstrated. ", "page_idx": 17}, {"type": "text", "text": "601 F.3 Influence of Ratio Threshold $r p_{r a t i o}$ . ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "602 In this subsection, we investigate the influence of ratio threshold $r p_{r a t i o}$ on the optimization perfor  \n603 mance. $r p_{r a t i o}$ is the threshold to determine when to add artificial ordinal relations for the training   \n604 of ordinal surrogate $h_{o}$ . We set $r p_{r a t i o}=\\{1,2/3,1/2,1/3\\}$ to generate four LORA-MOO variants.   \n605 For all variants, we set $n_{o}$ , $\\lambda$ to 4, 0.2, respectively, which are consistent with our conclusions in   \n606 previous ablation studies. Parameter $n_{c}$ is tentatively set to 5. The $\\mathrm{IGD+}$ values obtained by four   \n607 LORA-MOO variants with different $r p_{r a t i o}$ are reported in Table 5. It should be noted that, when the   \n608 number of objectives $M=3$ , the results of $r p_{r a t i o}=1$ are the same as the results of $r p_{r a t i o}=2/3$ ,   \n609 because the ratio of reference points in archive $S_{A}$ is always lower than $2/3$ . Consequently, when $M$   \n610 $=3$ , setting ratio threshold $r p_{r a t i o}$ to either 1 or $2/3$ makes no difference to the optimization process   \n611 of LORA-MOO. Similarly, the results of $r p_{r a t i o}=1/3$ on some problems are the same as the results   \n612 obtained by setting $r p_{r a t i o}$ to $1/2$ , because on these problems, the ratio of reference points in $S_{A}$ is   \n613 always higher than $1/2$ .   \n614 As shown in Table 5, the variant of $r p_{r a t i o}=1/2$ outperforms other variants and achieves the optimal   \n615 behavior. Therefore, we set $r p_{r a t i o}=1/2$ for LORA-MOO. In comparison, the variants of $r p_{r a t i o}$   \n616 $=2/3$ and $r p_{r a t i o}=1/3$ have competitive performance, both of them are inferior to the variant of   \n617 $r p_{r a t i o}=1/2$ but significantly superior to the variant of $r p_{r a t i o}=1$ .   \n618 Setting $r p_{r a t i o}=1$ indicates this LORA-MOO variant will never introduce artificial ordinal relations   \n619 for the learning of the ordinal surrogate. The ordinal surrogate in this variant is trained completely on   \n620 the basis of dominance ordinal relations. When the number of objectives $M$ is large, a majority of   \n621 evaluated solutions in archive $S_{A}$ are non-dominated, leading to a large ratio of reference points $S_{R P}$   \n622 in $S_{A}$ . As a result, there would be a significant imbalance between the number of evaluated solutions   \n623 in each ordinal level, which causes a poor performance on ordinal surrogate and LORA-MOO. In   \n624 particular, on most 10-objective WFG problems, the variant of $r p_{r a t i o}=1$ performs worse than all   \n625 other variants. This observation shows the detrimental effect of imbalance solutions in ordinal levels   \n626 on the optimization performance, which also demonstrates the effectiveness of using artificial ordinal   \n627 relations in LORA-MOO to address many-objective optimization problems. ", "page_idx": 17}, {"type": "table", "img_path": "hc9GkYw9kR/tmp/1a7dc7018e022acbfa562f937f35adb05a02c9ac6c2e57255f2ec3e091c9b885.jpg", "table_caption": ["Table 3: Statistical results of the $\\mathrm{IGD+}$ value obtained by LORA-MOO with different $n_{o}$ on 48 benchmark optimization problems over 15 runs. The last five rows count the total results of Wilcoxon rank sum tests (significance level is 0.05). $\\cdot\\cdot\\approx$ , and \u2018\u2212\u2019 denote the corresponding LORA-MOO variant is statistically significantly superior to, almost equivalent to, and inferior to the compared variants in Wilcoxon tests, respectively. "], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 18}, {"type": "table", "img_path": "hc9GkYw9kR/tmp/5f7f0f345c3f4aeb552c8acd26a0c9a343fdaccfdc3eefeaf1e01de7c17be17b.jpg", "table_caption": ["Table 4: Statistical results of the $\\mathrm{IGD+}$ value obtained by LORA-MOO with different $\\lambda$ on 48 benchmark optimization problems over 15 runs. The last four rows count the total results of Wilcoxon rank sum tests (significance level is 0.05). $\\cdot\\cdot\\approx$ , and \u2018\u2212\u2019 denote the corresponding LORA-MOO variant is statistically significantly superior to, almost equivalent to, and inferior to the compared variants in Wilcoxon tests, respectively. "], "table_footnote": [], "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "table", "img_path": "hc9GkYw9kR/tmp/d56d1f7865d821dc5b553f93361f52927bab0277804ade8d527b2d35fda51090.jpg", "table_caption": ["Table 5: Statistical results of the $\\mathrm{IGD+}$ value obtained by LORA-MOO with different $r p_{r a t i o}$ on 48 benchmark optimization problems over 15 runs. The last four rows count the total results of Wilcoxon rank sum tests (significance level is 0.05). $\\cdot\\cdot\\approx$ , and \u2018\u2212\u2019 denote the corresponding LORA-MOO variant is statistically significantly superior to, almost equivalent to, and inferior to the compared variants in Wilcoxon tests, respectively. "], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "628 F.4 Influence of Clustering Number for Reproduction $n_{c}$ . ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "629 This subsection analyzes the influence of clustering number $n_{c}$ on the optimization performance. $n_{c}$   \n630 is used in the reproduction process to initialize the PSO population. We set $n_{c}=\\{1,3,5,7,10\\}$ to   \n631 generate five LORA-MOO variants. According to the conclusions of previous ablation studies, in this   \n632 ablation study, we set $n_{o}=4$ , $\\lambda=0.2$ , $r p_{r a t i o}=1/2$ for all variants. The $\\mathrm{IGD+}$ values obtained by   \n633 five LORA-MOO variants with different $n_{c}$ are reported in Table 6.   \n634 It can be observed that both the variants of $n_{c}=5$ and $n_{c}=7$ outperform three other variants and are   \n635 inferior to one variant, showing the optimal performance over other variants in this ablation study.   \n636 In comparison, the variants of $n_{c}=3$ and $n_{c}=10$ are significantly superior to two variants but are   \n637 also significantly inferior to two other variants. The variant of $n_{c}=1$ reaches the worst optimization   \n638 results as it is significantly inferior to all other variants. In addition, considering that the variant of $n_{c}$   \n639 $=7$ wins/ties/losses 2/45/1 statistical tests when compared with the variant of $n_{c}=5$ , we set $n_{c}=7$   \n640 for LORA-MOO.   \n641 The result of this ablation study demonstrates the influence of population initialization on the   \n642 optimization results. By clustering the evaluated solutions into several clusters and sampling the same   \n643 amount of initial solutions from each cluster, the solutions in the initial population are distributed   \n644 in a more diverse way than the solutions sampled from the set of reference points $S_{R P}$ directly.   \n645 Consequently, all variants of $n_{c}>1$ have achieved better optimization results than the variant of $n_{c}$   \n646 $=1$ . ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 20}, {"type": "table", "img_path": "hc9GkYw9kR/tmp/c4080e5fe6278d7ecebf71d0f17be63eac7ca302d9def7aef0e108730a99b769.jpg", "table_caption": ["Table 6: Statistical results of the $\\mathrm{IGD+}$ value obtained by LORA-MOO with different $n_{c}$ on 48 benchmark optimization problems over 15 runs. The last five rows count the total results of Wilcoxon rank sum tests (significance level is 0.05). $\\cdot\\cdot\\approx$ , and \u2018\u2212\u2019 denote the corresponding LORA-MOO variant is statistically significantly superior to, almost equivalent to, and inferior to the compared variants in Wilcoxon tests, respectively. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "image", "img_path": "hc9GkYw9kR/tmp/0b3bf7fb6f448f644616f1ffbbdc9a0e795554d031c1db526f485cd64e1fe284.jpg", "img_caption": ["Figure 7: Distribution of obtained non-dominated solutions on DTLZ2 with 10 variables and 3 objectives. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "hc9GkYw9kR/tmp/a2650ce1c2ae4bd0ffbef6ca2afbaed8d5a023c633e84487901e2b8aee1e62a2.jpg", "img_caption": ["Figure 8: Distribution of obtained non-dominated solutions on DTLZ4 with 10 variables and 3 objectives. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "647 G Solution Distribution ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "648 The solution distribution we obtained on some 3-objective DTLZ problems are plotted. ", "page_idx": 22}, {"type": "text", "text": "649 H Complete Results of Benchmark Optimization ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "650 In Section 4.3 of the main file, we display the optimization results of comparison algorithms on   \n651 DTLZ problems in terms of IGD values. In this section, we provide detailed IGD results on WFG   \n652 problems and more results on $\\mathrm{IGD+}$ and HV values. In addition, the optimization results on DTLZ   \n653 problems with different scales, such as $D=5$ and 20, are reported. ", "page_idx": 22}, {"type": "text", "text": "654 H.1 IGD Results on WFG Optimization Problems ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "655 Table 7 shows the optimization results on WFG problems in terms of IGD values. The last row   \n656 summarizes the results of statistical tests, which has reported at the end of Table 1 in the main file.   \n657 It can be seen that LORA-MOO outperforms all comparison algorithms, followed by KTA2 and ", "page_idx": 22}, {"type": "image", "img_path": "hc9GkYw9kR/tmp/07e2b37f982296630d2276ddfd5db684f81e0e49bd558f56f437a1f7fa53b512.jpg", "img_caption": ["Figure 9: Distribution of obtained non-dominated solutions on DTLZ6 with 10 variables and 3 objectives. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Table 7: Statistical results of the IGD value obtained by comparison algorithms on 45 WFG optimization problems over 30 runs. Symbols $\\mathbf{\\omega}^{\\star}+\\mathbf{\\omega}^{\\star}$ , $\\surd\\approx\\ '$ , \u2018\u2212\u2019 denote LORA-MOO is statistically significantly superior to, almost equivalent to, and inferior to the compared algorithms in the Wilcoxon rank sum test (significance level is 0.05), respectively. The last row counts the total win/tie/loss results. ", "page_idx": 23}, {"type": "table", "img_path": "hc9GkYw9kR/tmp/cae2361440daf54471749895ece296bc6e674735d70f78b53730e059afb83c2e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "hc9GkYw9kR/tmp/3753f26172a8af7b524b00541bf0a38ffebf1dce4c03db4649fb7f1eeaf208f0.jpg", "img_caption": ["Figure 10: Log (IGD) curves averaged over 30 runs on six WFG problems for comparison algorithms (shaded area is $\\pm$ std of the mean). Top: 10 variables and 3 objectives. Bottom: 10 variables and 10 objectives. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Table 8: Statistical results of the $\\mathrm{IGD+}$ value obtained by comparison algorithms on 35 DTLZ optimization problems over 30 runs. Symbols $\\mathbf{\\omega}^{\\star}+\\mathbf{\\omega}^{\\star}$ , \u2018\u2248\u2019, \u2018\u2212\u2019 denote LORA-MOO is statistically significantly superior to, almost equivalent to, and inferior to the compared algorithms in the Wilcoxon rank sum test (significance level is 0.05), respectively. The last row counts the total win/tie/loss results. ", "page_idx": 24}, {"type": "table", "img_path": "hc9GkYw9kR/tmp/b3db0081dc82923d6d51b4d683c6f187db954a9140c4e3ca29a7ad5d5a29524f.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "660 H.2 IGD $^+$ Results on DTLZ and WFG Optimization Problems ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "661 Tables 8 and 9 display the $\\mathrm{IGD+}$ optimization results of comparison algorithms on DTLZ and WFG   \n662 optimization problems, respectively. Different from IGD results, although LORA-MOO achieves the   \n663 smallest $\\mathrm{IGD+}$ values on most DTLZ problems, its perform is competitive to KRVEA and KTA2 on   \n664 WFG problems. However, from the perspective of overall performance, we can still conclude that our   \n665 LORA-MOO outperforms all comparison algorithms on benchmark optimization problems in terms   \n666 of $\\mathrm{IGD+}$ values. Such a observation is consistent with the results we observed from IGD values. ", "page_idx": 24}, {"type": "text", "text": "Table 9: Statistical results of the $\\mathrm{IGD+}$ value obtained by comparison algorithms on 45 WFG optimization problems over 30 runs. Symbols $\\mathbf{\\omega}^{\\star}+\\mathbf{\\omega}^{\\star}$ , \u2018\u2248\u2019, \u2018\u2212\u2019 denote LORA-MOO is statistically significantly superior to, almost equivalent to, and inferior to the compared algorithms in the Wilcoxon rank sum test (significance level is 0.05), respectively. The last row counts the total win/tie/loss results. ", "page_idx": 25}, {"type": "table", "img_path": "hc9GkYw9kR/tmp/95580ee0ec0818a0c8fdb93f421d6bc1e063d43e433074336d782da0c93c9fbf.jpg", "table_caption": [], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "667 H.3 HV Results on DTLZ and WFG Optimization Problems ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "668 Tables 10 and 11 report the HV optimization results of comparison algorithms on DTLZ and WFG   \n669 optimization problems, respectively. Since the calculation of HV values on 8- and 10-obj optimization   \n670 problems is very time-consuming, only the results obtained on 3-, 4-, and 6-objective optimization   \n671 problems are displayed. Consistent with the IGD an $\\mathrm{IGD+}$ results obtained on 3-, 4-, and 6-objectives,   \n672 our LORA-MOO achieves the best overall performance over all comparison algorithms, showing the   \n673 effectiveness of LORA-MOO on addressing expensive many-objective optimization problems. ", "page_idx": 25}, {"type": "text", "text": "674 H.4 Problems with Different Scales ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "675 In this subsection, we investigate the optimization performance of LORA-MOO when the number   \n676 of decision variables $D$ is different. The experimental setups for all comparison algorithms are the   \n677 same as the setups used in previous benchmark optimization problems, but the setup for optimization   \n678 problems is different:   \n679 \u2022 The optimization problems have $D=\\{5,10,20\\}$ decision variables and $M=3$ objectives.   \n680 \u2022 When $D=5$ or 10, a dataset of size $11\\,D-1$ is used for surrogate initialization. When $D$   \n681 $=20$ , since $11\\,D-1$ would be greater than our evaluation budget (300), the size of initial   \n682 dataset is set to 100.   \n683 Tables 12, 13, and 14 report the obtained IGD, $\\mathrm{IGD+}$ , and HV values on benchmark optimization   \n684 problems with different numbers of decision variables $D$ , respectively. It can be seen from Table 12 ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 25}, {"type": "text", "text": "Table 10: Statistical results of the HV value obtained by comparison algorithms on 21 DTLZ optimization problems over 30 runs. Symbols $\\mathbf{\\omega}^{\\star}+\\mathbf{\\omega}^{\\star}$ , \u2018\u2248\u2019, \u2018\u2212\u2019 denote LORA-MOO is statistically significantly superior to, almost equivalent to, and inferior to the compared algorithms in the Wilcoxon rank sum test (significance level is 0.05), respectively. The last row counts the total win/tie/loss results. ", "page_idx": 26}, {"type": "table", "img_path": "hc9GkYw9kR/tmp/9ab65a51cbe6a13c5c8d630b4852e83f7513fd62f8189d6b8a39ced455d770ea.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "Table 11: Statistical results of the HV value obtained by comparison algorithms on 27 WFG optimization problems over 30 runs. Symbols $\\mathbf{\\omega}^{\\star}+\\mathbf{\\omega}^{\\star}$ \u2019, $\\surd\\approx\\ '$ , \u2018\u2212\u2019 denote LORA-MOO is statistically significantly superior to, almost equivalent to, and inferior to the compared algorithms in the Wilcoxon rank sum test (significance level is 0.05), respectively. The last row counts the total win/tie/loss results. ", "page_idx": 26}, {"type": "table", "img_path": "hc9GkYw9kR/tmp/19c722981bf78fdfc3210703c7d9fde1203a39b0c1ad7cd8988fd5ff429038f8.jpg", "table_caption": [], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "685 that LORA-MOO outperforms all comparison algorithms on DTLZ optimization problems when $D$   \n686 $=5$ , 10, and 20. In addition, KTA2 reaches competitive optimization results on many optimization   \n687 problems. The observations from Tables 13 and 14 have demonstrated consistent conclusions. ", "page_idx": 26}, {"type": "table", "img_path": "hc9GkYw9kR/tmp/3d592a2e26ee534a2b4f0d4822720090f8b9f71ffe0a22cb713d4c1625348e27.jpg", "table_caption": ["Table 12: Statistical results of the IGD value obtained by comparison algorithms on $5D$ , $10D$ , and $20D$ DTLZ optimization problems over 30 runs. Symbols $\"+\"$ , \u2018\u2248\u2019, \u2018\u2212\u2019 denote LORA-MOO is statistically significantly superior to, almost equivalent to, and inferior to the compared algorithms in the Wilcoxon rank sum test (significance level is 0.05), respectively. The last row counts the total win/tie/loss results. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Table 13: Statistical results of the $\\mathrm{IGD+}$ value obtained by comparison algorithms on $5D$ , $10D$ , and $20D$ DTLZ optimization problems over 30 runs. Symbols $\\mathbf{\\omega}^{\\star}+\\mathbf{\\dot{\\omega}}$ \u2019, \u2018\u2248\u2019, \u2018\u2212\u2019 denote LORA-MOO is statistically significantly superior to, almost equivalent to, and inferior to the compared algorithms in the Wilcoxon rank sum test (significance level is 0.05), respectively. The last row counts the total win/tie/loss results. ", "page_idx": 27}, {"type": "table", "img_path": "hc9GkYw9kR/tmp/cd31c30b8fcecd1b0f8bae775794519c92b6e5355e6ce86899a416c9650c72b0.jpg", "table_caption": [], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "Table 14: Statistical results of the HV value obtained by comparison algorithms on $5D$ , $10D$ , and $20D$ DTLZ optimization problems over 30 runs. Symbols $\"+\"$ , $\\surd\\approx\\mathrel{\\hat{\\,}}$ , \u2018\u2212\u2019 denote LORA-MOO is statistically significantly superior to, almost equivalent to, and inferior to the compared algorithms in the Wilcoxon rank sum test (significance level is 0.05), respectively. The last row counts the total win/tie/loss results. ", "page_idx": 28}, {"type": "table", "img_path": "hc9GkYw9kR/tmp/09e5821c799fb81eb1f84a027a14bca865b11d01e229a46d857bc8a1b8f0c28a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "689 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Justification: Claims we made accurately reflect the paper\u2019s contributions and scope. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: When the number of objectives is large, there would be many non-dominated solutions in the archive, however, we have introduced artificial ordinal relations in our surrogate modeling procedure to alleviate this limitation. ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. ", "page_idx": 28}, {"type": "text", "text": "\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. ", "page_idx": 28}, {"type": "text", "text": "720 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n721 only tested on a few datasets or with a few runs. In general, empirical results often   \n722 depend on implicit assumptions, which should be articulated.   \n723 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n724 For example, a facial recognition algorithm may perform poorly when image resolution   \n725 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n726 used reliably to provide closed captions for online lectures because it fails to handle   \n727 technical jargon.   \n728 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n729 and how they scale with dataset size.   \n730 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n731 address problems of privacy and fairness.   \n732 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n733 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n734 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n735 judgment and recognize that individual actions in favor of transparency play an impor  \n736 tant role in developing norms that preserve the integrity of the community. Reviewers   \n737 will be specifically instructed to not penalize honesty concerning limitations.   \n738 3. Theory Assumptions and Proofs   \n739 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n740 a complete (and correct) proof?   \n741 Answer: [NA]   \n742 Justification: Not applicable.   \n743 Guidelines:   \n744 \u2022 The answer NA means that the paper does not include theoretical results.   \n745 \u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross  \n746 referenced.   \n747 \u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n748 \u2022 The proofs can either appear in the main paper or the supplemental material, but if   \n749 they appear in the supplemental material, the authors are encouraged to provide a short   \n750 proof sketch to provide intuition.   \n751 \u2022 Inversely, any informal proof provided in the core of the paper should be complemented   \n752 by formal proofs provided in appendix or supplemental material.   \n753 \u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.   \n754 4. Experimental Result Reproducibility   \n755 Question: Does the paper fully disclose all the information needed to reproduce the main ex  \n756 perimental results of the paper to the extent that it affects the main claims and/or conclusions   \n757 of the paper (regardless of whether the code and data are provided or not)?   \n758 Answer: [Yes]   \n759 Justification: Experimental setups are described in detail.   \n760 Guidelines:   \n761 \u2022 The answer NA means that the paper does not include experiments.   \n762 \u2022 If the paper includes experiments, a No answer to this question will not be perceived   \n763 well by the reviewers: Making the paper reproducible is important, regardless of   \n764 whether the code and data are provided or not.   \n765 \u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken   \n766 to make their results reproducible or verifiable.   \n767 \u2022 Depending on the contribution, reproducibility can be accomplished in various ways.   \n768 For example, if the contribution is a novel architecture, describing the architecture fully   \n769 might suffice, or if the contribution is a specific model and empirical evaluation, it may   \n770 be necessary to either make it possible for others to replicate the model with the same   \n771 dataset, or provide access to the model. In general. releasing code and data is often   \n772 one good way to accomplish this, but reproducibility can also be provided via detailed   \n773 instructions for how to replicate the results, access to a hosted model (e.g., in the case   \n774 of a large language model), releasing of a model checkpoint, or other means that are   \n775 appropriate to the research performed.   \n776 \u2022 While NeurIPS does not require releasing code, the conference does require all submis  \n777 sions to provide some reasonable avenue for reproducibility, which may depend on the   \n778 nature of the contribution. For example   \n779 (a) If the contribution is primarily a new algorithm, the paper should make it clear how   \n780 to reproduce that algorithm.   \n781 (b) If the contribution is primarily a new model architecture, the paper should describe   \n782 the architecture clearly and fully.   \n783 (c) If the contribution is a new model (e.g., a large language model), then there should   \n784 either be a way to access this model for reproducing the results or a way to reproduce   \n785 the model (e.g., with an open-source dataset or instructions for how to construct   \n786 the dataset).   \n787 (d) We recognize that reproducibility may be tricky in some cases, in which case   \n788 authors are welcome to describe the particular way they provide for reproducibility.   \n789 In the case of closed-source models, it may be that access to the model is limited in   \n790 some way (e.g., to registered users), but it should be possible for other researchers   \n791 to have some path to reproducing or verifying the results.   \n792 5. Open access to data and code   \n793 Question: Does the paper provide open access to the data and code, with sufficient instruc  \n794 tions to faithfully reproduce the main experimental results, as described in supplemental   \n795 material?   \n796 Answer: [No]   \n797 Justification: Will release our code after acceptation, or we can provide the code if any   \n798 reviewers are interested in it during the review process. Anyway, the details about the code   \n799 have already described in the paper.   \n800 Guidelines:   \n801 \u2022 The answer NA means that paper does not include experiments requiring code.   \n802 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n803 public/guides/CodeSubmissionPolicy) for more details.   \n804 \u2022 While we encourage the release of code and data, we understand that this might not be   \n805 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n806 including code, unless this is central to the contribution (e.g., for a new open-source   \n807 benchmark).   \n808 \u2022 The instructions should contain the exact command and environment needed to run to   \n809 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n810 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n811 \u2022 The authors should provide instructions on data access and preparation, including how   \n812 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n813 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n814 proposed method and baselines. If only a subset of experiments are reproducible, they   \n815 should state which ones are omitted from the script and why.   \n816 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n817 versions (if applicable).   \n818 \u2022 Providing as much information as possible in supplemental material (appended to the   \n819 paper) is recommended, but including URLs to data and code is permitted.   \n820 6. Experimental Setting/Details   \n821 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n822 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n823 results?   \n824 Answer: [Yes]   \n825 Justification: We have described all the details about of experiments.   \nGuidelines: ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 31}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We have conducted statistical tests in our experiments, error bars are plotted in figures. ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 31}, {"type": "text", "text": "827   \n828   \n829   \n830   \n831   \n832   \n833   \n834   \n835   \n836   \n837   \n838   \n839   \n840   \n841   \n842   \n843   \n844   \n845   \n846   \n847   \n848   \n849   \n850   \n851   \n852   \n853   \n854   \n855   \n856   \n857   \n858   \n859   \n860   \n861   \n862   \n863   \n864   \n865   \n866   \n867   \n868   \n869   \n870   \n871   \n872   \n873   \n874   \n875   \n876   \n877   \n878 ", "page_idx": 31}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: A runtime comparison experiment is reported in the end of our experiment section. We did not provide information about compute workers and memory since our experiments do not have specific requirements on memory or other computation resource. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 31}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 31}, {"type": "text", "text": "880 Justification: Not applicable.   \n881 Guidelines:   \n882 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n883 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n884 deviation from the Code of Ethics.   \n885 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n886 eration due to laws or regulations in their jurisdiction).   \n887 10. Broader Impacts   \n888 Question: Does the paper discuss both potential positive societal impacts and negative   \n889 societal impacts of the work performed?   \n890 Answer: [No]   \n891 Justification: Our algorithm has no potential negative social impacts.   \n892 Guidelines:   \n893 \u2022 The answer NA means that there is no societal impact of the work performed.   \n894 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n895 impact or why the paper does not address societal impact.   \n896 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n897 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n898 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n899 groups), privacy considerations, and security considerations.   \n900 \u2022 The conference expects that many papers will be foundational research and not tied   \n901 to particular applications, let alone deployments. However, if there is a direct path to   \n902 any negative applications, the authors should point it out. For example, it is legitimate   \n903 to point out that an improvement in the quality of generative models could be used to   \n904 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n905 that a generic algorithm for optimizing neural networks could enable people to train   \n906 models that generate Deepfakes faster.   \n907 \u2022 The authors should consider possible harms that could arise when the technology is   \n908 being used as intended and functioning correctly, harms that could arise when the   \n909 technology is being used as intended but gives incorrect results, and harms following   \n910 from (intentional or unintentional) misuse of the technology.   \n911 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n912 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n913 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n914 feedback over time, improving the efficiency and accessibility of ML).   \n915 11. Safeguards   \n916 Question: Does the paper describe safeguards that have been put in place for responsible   \n917 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n918 image generators, or scraped datasets)?   \n919 Answer: [No]   \n920 Justification: Code will be released after acceptation, it would be open access, no safeguards   \n921 are required.   \n922 Guidelines:   \n923 \u2022 The answer NA means that the paper poses no such risks.   \n924 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n925 necessary safeguards to allow for controlled use of the model, for example by requiring   \n926 that users adhere to usage guidelines or restrictions to access the model or implementing   \n927 safety filters.   \n928 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n929 should describe how they avoided releasing unsafe images. ", "page_idx": 32}, {"type": "text", "text": "\u2022 We recognize that providing effective safeguards is challenging, and many papers do ", "page_idx": 33}, {"type": "text", "text": "931 not require this, but we encourage authors to take this into account and make a best   \n932 faith effort.   \n933 12. Licenses for existing assets   \n934 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n935 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n936 properly respected?   \n937 Answer: [Yes]   \n938 Justification: We have cited the existing assets we used in our paper.   \n939 Guidelines:   \n940 \u2022 The answer NA means that the paper does not use existing assets.   \n941 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n942 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n943 URL.   \n944 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n945 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n946 service of that source should be provided.   \n947 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n948 package should be provided. For popular datasets, paperswithcode.com/datasets   \n949 has curated licenses for some datasets. Their licensing guide can help determine the   \n950 license of a dataset.   \n951 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n952 the derived asset (if it has changed) should be provided.   \n953 \u2022 If this information is not available online, the authors are encouraged to reach out to   \n954 the asset\u2019s creators. ", "page_idx": 33}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA] Justification: We did not introduce any new assets. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 33}, {"type": "text", "text": "969 14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "970 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n971 include the full text of instructions given to participants and screenshots, if applicable, as   \n972 well as details about compensation (if any)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [NA]   \nJustification: We do not have any experiments or research with human subjects. Guidelines:   \n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: We do not have any experiments or research with human subjects. Guidelines: \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. \u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. \u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. \u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 34}]