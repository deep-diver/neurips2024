[{"figure_path": "WPPC7FHtaM/figures/figures_1_1.jpg", "caption": "Figure 1: Comparison between traditional gradient-based prompt optimization (a) and our interpretable prompt optimization (b) for vision-language models. Traditional gradient descent-based prompt learning methods [8, 11] treat the text prompt as learnable parameters V. By minimizing the loss through gradient descent on the training set, an optimized prompt V is obtained after I iterations, which is not interpretable by humans. In contrast, our interpretable prompt optimization leverages an LLM as optimizer to optimize the loss and accuracy. After I iterations, the resulting optimized top prompt is effective and human-readable.", "description": "This figure compares two prompt optimization methods for vision-language models. The left panel illustrates gradient-based optimization, where the prompt is treated as a parameter and optimized using gradient descent. This often leads to prompts that are hard to interpret by humans. The right panel shows the proposed interpretable prompt optimizer, which uses large language models (LLMs) to dynamically generate human-readable and effective prompts, improving model performance and interpretability. The key difference is that gradient-based methods overfit easily on base class, while the LLMs approach does not.", "section": "Methods"}, {"figure_path": "WPPC7FHtaM/figures/figures_4_1.jpg", "caption": "Figure 1: Comparison between traditional gradient-based prompt optimization (a) and our interpretable prompt optimization (b) for vision-language models. Traditional gradient descent-based prompt learning methods [8, 11] treat the text prompt as learnable parameters V. By minimizing the loss through gradient descent on the training set, an optimized prompt V is obtained after I iterations, which is not interpretable by humans. In contrast, our interpretable prompt optimization leverages an LLM as optimizer to optimize the loss and accuracy. After I iterations, the resulting optimized top prompt is effective and human-readable.", "description": "This figure compares two prompt optimization approaches for vision-language models.  (a) shows the traditional gradient-descent method, where prompts are treated as parameters and optimized via gradient descent.  This often results in prompts that are hard for humans to understand. (b) shows the authors' proposed interpretable prompt optimization (IPO) method, which uses a Large Language Model (LLM) to generate human-readable prompts that improve accuracy. The IPO method incorporates a prompt history and image descriptions to improve prompt quality and dataset-specific performance.", "section": "Methods"}, {"figure_path": "WPPC7FHtaM/figures/figures_8_1.jpg", "caption": "Figure 1: Comparison between traditional gradient-based prompt optimization (a) and our interpretable prompt optimization (b) for vision-language models. Traditional gradient descent-based prompt learning methods [8, 11] treat the text prompt as learnable parameters V. By minimizing the loss through gradient descent on the training set, an optimized prompt V is obtained after I iterations, which is not interpretable by humans. In contrast, our interpretable prompt optimization leverages an LLM as optimizer to optimize the loss and accuracy. After I iterations, the resulting optimized top prompt is effective and human-readable.", "description": "The figure compares two prompt optimization methods for vision-language models: gradient-based and interpretable. The gradient-based method treats prompts as parameters and optimizes them using gradient descent, often leading to overfitting and incomprehensible prompts.  In contrast, the interpretable method uses a large language model (LLM) to generate human-understandable prompts, leveraging past performance data and multimodal interaction for improved accuracy and interpretability.", "section": "Methods"}, {"figure_path": "WPPC7FHtaM/figures/figures_14_1.jpg", "caption": "Figure 1: Comparison between traditional gradient-based prompt optimization (a) and our interpretable prompt optimization (b) for vision-language models. Traditional gradient descent-based prompt learning methods [8, 11] treat the text prompt as learnable parameters V. By minimizing the loss through gradient descent on the training set, an optimized prompt V is obtained after I iterations, which is not interpretable by humans. In contrast, our interpretable prompt optimization leverages an LLM as optimizer to optimize the loss and accuracy. After I iterations, the resulting optimized top prompt is effective and human-readable.", "description": "This figure compares two prompt optimization methods for vision-language models: gradient-based and interpretable prompt optimization. The gradient-based method treats prompts as parameters and optimizes them using gradient descent, leading to overfitted and uninterpretable prompts.  In contrast, the interpretable method uses LLMs to dynamically generate human-understandable and effective prompts. The figure illustrates the processes and results of both methods, highlighting the advantages of using LLMs for prompt optimization.", "section": "Methods"}, {"figure_path": "WPPC7FHtaM/figures/figures_14_2.jpg", "caption": "Figure 1: Comparison between traditional gradient-based prompt optimization (a) and our interpretable prompt optimization (b) for vision-language models.", "description": "This figure compares gradient-based prompt optimization and the proposed interpretable prompt optimization (IPO).  (a) shows the traditional approach where prompts are treated as learnable parameters and optimized via gradient descent, leading to overfitting and non-interpretable prompts. (b) illustrates the IPO method, which utilizes a large language model (LLM) and a large multimodal model (LMM) to dynamically generate human-interpretable prompts.  The IPO method incorporates a Prompt Optimization Prompt that guides the LLM and uses episodic memory to improve performance and interpretability.", "section": "Methods"}, {"figure_path": "WPPC7FHtaM/figures/figures_18_1.jpg", "caption": "Figure 1: Comparison between traditional gradient-based prompt optimization (a) and our interpretable prompt optimization (b) for vision-language models. Traditional gradient descent-based prompt learning methods [8, 11] treat the text prompt as learnable parameters V. By minimizing the loss through gradient descent on the training set, an optimized prompt V is obtained after I iterations, which is not interpretable by humans. In contrast, our interpretable prompt optimization leverages an LLM as optimizer to optimize the loss and accuracy. After I iterations, the resulting optimized top prompt is effective and human-readable.", "description": "This figure compares gradient-based and interpretable prompt optimization methods for vision-language models.  The gradient-based method treats prompts as parameters and optimizes them using gradient descent, often resulting in uninterpretable prompts.  In contrast, the interpretable method uses a large language model (LLM) to generate and refine prompts iteratively, leading to human-understandable and effective prompts. The figure illustrates the process of each method, highlighting the differences in prompt generation and evaluation.", "section": "Methods"}, {"figure_path": "WPPC7FHtaM/figures/figures_19_1.jpg", "caption": "Figure 1: Comparison between traditional gradient-based prompt optimization (a) and our interpretable prompt optimization (b) for vision-language models. Traditional gradient descent-based prompt learning methods [8, 11] treat the text prompt as learnable parameters V. By minimizing the loss through gradient descent on the training set, an optimized prompt V is obtained after I iterations, which is not interpretable by humans. In contrast, our interpretable prompt optimization leverages an LLM as optimizer to optimize the loss and accuracy. After I iterations, the resulting optimized top prompt is effective and human-readable.", "description": "This figure compares two prompt optimization methods: gradient-based and interpretable.  Gradient-based methods treat prompts as parameters, optimizing them via gradient descent.  This often leads to overfitting and non-human-interpretable prompts. In contrast, the proposed interpretable method uses a large language model (LLM) to dynamically generate and refine prompts, resulting in human-understandable and effective prompts.", "section": "Methods"}, {"figure_path": "WPPC7FHtaM/figures/figures_20_1.jpg", "caption": "Figure 1: Comparison between traditional gradient-based prompt optimization (a) and our interpretable prompt optimization (b) for vision-language models. Traditional gradient descent-based prompt learning methods [8, 11] treat the text prompt as learnable parameters V. By minimizing the loss through gradient descent on the training set, an optimized prompt V is obtained after I iterations, which is not interpretable by humans. In contrast, our interpretable prompt optimization leverages an LLM as optimizer to optimize the loss and accuracy. After I iterations, the resulting optimized top prompt is effective and human-readable.", "description": "The figure compares traditional gradient-based prompt optimization with the proposed interpretable prompt optimization method.  Gradient-based methods treat prompts as parameters, leading to uninterpretable results.  In contrast, the proposed method uses LLMs to generate human-readable and effective prompts.", "section": "Methods"}, {"figure_path": "WPPC7FHtaM/figures/figures_22_1.jpg", "caption": "Figure 1: Comparison between traditional gradient-based prompt optimization (a) and our interpretable prompt optimization (b) for vision-language models. Traditional gradient descent-based prompt learning methods [8, 11] treat the text prompt as learnable parameters V. By minimizing the loss through gradient descent on the training set, an optimized prompt V is obtained after I iterations, which is not interpretable by humans. In contrast, our interpretable prompt optimization leverages an LLM as optimizer to optimize the loss and accuracy. After I iterations, the resulting optimized top prompt is effective and human-readable.", "description": "This figure compares two different prompt optimization methods for vision-language models. The traditional gradient-based method treats prompts as parameters and optimizes them using gradient descent. This often leads to uninterpretable prompts.  The proposed interpretable method uses a large language model (LLM) to generate and refine prompts dynamically, resulting in prompts that are both effective and human-understandable.", "section": "Methods"}, {"figure_path": "WPPC7FHtaM/figures/figures_22_2.jpg", "caption": "Figure 1: Comparison between traditional gradient-based prompt optimization (a) and our interpretable prompt optimization (b) for vision-language models. Traditional gradient descent-based prompt learning methods [8, 11] treat the text prompt as learnable parameters V. By minimizing the loss through gradient descent on the training set, an optimized prompt V is obtained after I iterations, which is not interpretable by humans. In contrast, our interpretable prompt optimization leverages an LLM as optimizer to optimize the loss and accuracy. After I iterations, the resulting optimized top prompt is effective and human-readable.", "description": "The figure compares two approaches to prompt optimization for vision-language models: gradient-based and interpretable.  Gradient-based methods treat prompts as parameters, leading to overfitting and uninterpretable prompts. The interpretable approach uses LLMs to dynamically generate human-understandable prompts, improving accuracy and interpretability.  The figure illustrates the process of each method through diagrams.", "section": "Methods"}]