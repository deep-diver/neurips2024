[{"figure_path": "34dHGTri2w/figures/figures_2_1.jpg", "caption": "Figure 1: A good anchor point could help improve convergence even if the gradient is unexpectedly disturbed from original gradient to the disturbed gradient, getting closer to the optimal point.", "description": "This figure illustrates how a well-chosen anchor point can aid convergence, even when gradients are unexpectedly perturbed.  The black dot represents the starting point (x1).  The red dot represents a point with a disturbed gradient, where the gradient unexpectedly changes direction (\u2207xf(xp)). The blue arrow (-\u03b7||\u2207xf(x1)||) illustrates the negative gradient direction from the starting point, while the red arrow (-\u03b7||\u2207xf(xp)||) shows how the disturbed gradient changes the direction. By incorporating an anchor point (the gray dot), the algorithm is guided toward the optimal point, improving convergence despite the gradient disturbance. The green, blue, and yellow arrows represent the steps of the process.", "section": "3 Motivation"}, {"figure_path": "34dHGTri2w/figures/figures_2_2.jpg", "caption": "Figure 2: To enhance exploratory capabilities, it's important to encourage particles to explore the landscape.", "description": "The figure illustrates the concept of metastability in energy-based models. Particles can get trapped in local minima during sampling, hindering exploration of the entire energy landscape. The figure shows how the introduction of elastic forces helps particles escape these local minima and better explore the space.", "section": "3 Motivation"}, {"figure_path": "34dHGTri2w/figures/figures_2_3.jpg", "caption": "Figure 3: There is a potential for particles to unintentionally follow the gradient flow towards these regions of high energy. A more comprehensive description could be found at Section 5.1.3.", "description": "This figure illustrates a scenario where particles might get trapped in pseudo-stable minima due to misleading gradients. The gradients point towards areas of high energy instead of low energy areas, which are the true modes of the distribution. This is an example of pseudo-stability.", "section": "3 Motivation"}, {"figure_path": "34dHGTri2w/figures/figures_5_1.jpg", "caption": "(a) Sampling from the original distribution in the form of e-U(x). T = 1000 in all experiments. (b) Sampling from the approximated distribution in the form of e-Uo(x). T = 2000 in all experiments. Figure 4: Sample from a Gaussian distribution N(\u03bc, \u03a3) where \u03bc\u2208 Rd corresponds to the clean image. For each method, we plot the lowest-energy particle (in terms of U(x) among all particles in XT). The upper-left image represents a direct sample from the distribution N(\u03bc, \u03a3); The lower-left image is generated by performing HMC sampling for T steps on the function Ue(x), with an initial point set to xo = \u03bc.", "description": "This figure compares different sampling methods (FHL, U-LMC, U-HMC, LMC, HMC) on two different sampling tasks. The first task is to sample from the original Gaussian distribution; the second task is to sample from an approximated distribution obtained by convolving the original distribution with an isotropic Gaussian distribution. The figure shows that FHL performs better on both tasks.", "section": "5.1.1 Instability"}, {"figure_path": "34dHGTri2w/figures/figures_6_1.jpg", "caption": "Figure 4: Sample from a Gaussian distribution N(\u03bc, \u03a3) where \u03bc\u2208 Rd corresponds to the clean image. For each method, we plot the lowest-energy particle (in terms of U(x) among all particles in XT). The upper-left image represents a direct sample from the distribution N(\u03bc, \u03a3); The lower-left image is generated by performing HMC sampling for T steps on the function Ue(x), with an initial point set to xo = \u03bc.", "description": "This figure compares the sampling performance of different methods (FHL, U-LMC, U-HMC, LMC, and HMC) from a Gaussian distribution.  The top row shows sampling from the original distribution, while the bottom row shows sampling from an approximated distribution. The leftmost image in each row shows a direct sample from the respective distribution. The remaining images show the lowest energy particle found by each sampling method after a specified number of steps. This illustrates the efficiency and accuracy of FHL in converging to the mode of the distribution, particularly when compared to other baseline methods.", "section": "5.1.1 Instability"}, {"figure_path": "34dHGTri2w/figures/figures_6_2.jpg", "caption": "Figure 5: Plot of N = 256 particles of Xr on d = 2 starting from random initialization N(0, 4 \n\u00b7 I). The target distribution is N([1, 1], I). Energy State corresponds to the target density \u03c0. The baseline methods U-LMC, LMC, U-HMC, HMC and our proposed method FHL generate XT after T = 1000 steps. There are no gradient flows and the samplers are only able to sample by the energy information.", "description": "This figure compares the performance of different sampling methods (U-LMC, LMC, U-HMC, HMC, and FHL) on a 2D Gaussian distribution with two modes.  The methods only use energy information, as gradients are absent.  The plot shows the final positions of 256 particles after 1000 steps, illustrating that FHL is better at identifying the modes of the target distribution than the other methods.", "section": "5.1.2 Metastability"}, {"figure_path": "34dHGTri2w/figures/figures_7_1.jpg", "caption": "Figure 7: Sample from joint energy model by different classes (Left: HMC; Right: FTH).", "description": "This figure compares the image samples generated by the Hamiltonian Monte Carlo (HMC) method and the Follow Hamiltonian Leader (FTH) method from a joint energy model for four different classes: airplanes, birds, frogs, and dogs.  Each class is represented by a grid of small images, with those on the left generated by HMC and those on the right by FTH. The visual comparison allows assessment of the quality and diversity of the generated images for both methods.", "section": "5.2 Energy-Based Generative Model"}, {"figure_path": "34dHGTri2w/figures/figures_8_1.jpg", "caption": "Figure 8: Compose sampling with DDPM.", "description": "This figure compares the performance of different sampling methods in generating samples from a composition of two distributions (p1(x) and p2(x)).  The left column shows the original distributions. The remaining columns display the results obtained using different sampling algorithms: Ground Truth, U-LMC, LMC, FTH, U-HMC, and HMC. The figure highlights that FTH demonstrates a strong ability to converge to the correct composition, with fewer particles falling outside the high-density regions compared to other methods.", "section": "5.3.1 Synthetic Dataset"}, {"figure_path": "34dHGTri2w/figures/figures_8_2.jpg", "caption": "Figure 17: Generation of cube and cylinder.", "description": "This figure compares the results of generating cube and cylinder images using four different sampling methods: Baseline, MALA, HMC, and FTH.  The Baseline represents a standard approach, while MALA and HMC are established Markov Chain Monte Carlo methods. FTH is the novel method proposed in the paper.  The figure visually shows the generated images from each method, allowing a comparison of image quality and accuracy. It demonstrates the superior performance of the FTH algorithm in this task.", "section": "5.3.2 CLEVR Dataset"}, {"figure_path": "34dHGTri2w/figures/figures_8_3.jpg", "caption": "Figure 10: Generation of sphere and cylinder. The zoomed images could be found at Figure 20.", "description": "This figure compares the results of generating images of spheres and cylinders using four different sampling methods: a baseline method, MALA, HMC, and the proposed FTH method.  The zoomed versions of these images can be found in Figure 20, allowing for a more detailed comparison of the generated shapes and quality.", "section": "5.3.2 CLEVR Dataset"}, {"figure_path": "34dHGTri2w/figures/figures_12_1.jpg", "caption": "Figure 11: U(x\u00b9) with gradient descent method [6]. The learning rate is set to 0.1.", "description": "This figure shows the trajectory and loss function plots for the gradient descent method and the proposed method (gradient + leader) on the U(x\u00b9) function. The gradient descent method struggles to converge efficiently, while the proposed method exhibits improved convergence behavior, reaching the minimum faster.", "section": "A.1 Instability & Metastability"}, {"figure_path": "34dHGTri2w/figures/figures_13_1.jpg", "caption": "Figure 11: U(x\u00b9) with gradient descent method [6]. The learning rate is set to 0.1.", "description": "This figure shows the trajectory and loss plots for a 2D optimization problem using the gradient descent method.  The objective function is U(x) = x[1]^2 + 0.01 * x[2]^2, which is ill-conditioned (condition number of 100). The trajectory plot demonstrates the path taken by the gradient descent algorithm, starting from an initial point and converging toward the minimum. The loss plot shows the value of U(x) at each iteration, illustrating the convergence of the algorithm.  The figure includes a comparison to the proposed method, where the addition of the leader-pulling scheme is shown to improve convergence and efficiency.", "section": "A.1 Instability & Metastability"}, {"figure_path": "34dHGTri2w/figures/figures_14_1.jpg", "caption": "Figure 13: Plot of N = 512 particles of XT for a 4-mode compositional Gaussian mixture model \u03c0\u03b1\u03c01\u00b7 \u03c02 on d = 2. We sample by gradient \u2207 log \u03c0\u2081 and energy \u03c01\u00b7 \u03c02. The baseline methods LMC, HMC and our proposed method FHL generate XT after T = 4000 steps, using the initial particles Xo = {x} with x sampled from a common distribution.", "description": "This figure compares the sampling performance of the proposed FHL method against LMC and HMC on a 2-dimensional, 4-mode compositional Gaussian mixture model.  The initial particle positions are randomly sampled from a common distribution. The target distribution is a mixture of four Gaussian distributions. The figure visually demonstrates the superior ability of FHL to accurately converge to the modes of the target distribution compared to the baseline methods, which struggle to escape incorrect modes due to misleading gradients.", "section": "A.2.1 Low-dimensional Example"}, {"figure_path": "34dHGTri2w/figures/figures_15_1.jpg", "caption": "Figure 14: Sample from a 100-mode compositional Gaussian mixture model \u03c0 \u221d \u03c0\u2081 \u22c5 \u03c0\u2082 on d = 784, where each mode corresponds to a clean image from coat category. We sample by gradient \u2207 log \u03c0\u2081 and energy \u03c0\u2081 \u22c5 \u03c0\u2082. For each method, we plot the smallest-energy particle (in terms of U(x) among all particles in X). The correct samples are displayed in the upper-left corner.", "description": "This figure compares the sampling results of different algorithms (FHL, LMC, and HMC) from a 100-mode compositional Gaussian mixture model in a 784-dimensional space. Each mode represents a clean image from the 'coat' category. The algorithms sample using both gradient information (\u2207 log \u03c0\u2081) and energy information (\u03c0\u2081 \u22c5 \u03c0\u2082). The figure shows that FHL is better at generating images similar to the 'coat' category samples, compared to LMC and HMC, which produce samples from other categories.", "section": "5.3.2 CLEVR Dataset"}, {"figure_path": "34dHGTri2w/figures/figures_16_1.jpg", "caption": "Figure 15: Sample from joint energy model by different classes (Left: HMC; Right: FTH).", "description": "This figure shows samples generated by HMC and FTH from a joint energy model for different classes (cats and horses).  It visually compares the quality and diversity of samples produced by each method, illustrating the performance differences in generating images of various classes.", "section": "5.2 Energy-Based Generative Model"}, {"figure_path": "34dHGTri2w/figures/figures_16_2.jpg", "caption": "Figure 17: Generation of cube and cylinder.", "description": "This figure compares the results of generating cube and cylinder images using four different methods: Baseline, MALA, HMC, and FTH (Follow Hamiltonian Leader).  Each method's output is shown in a separate panel.  The figure likely illustrates the relative quality and accuracy of the generated images by each method, highlighting the performance of the proposed FTH approach in generating high-quality images free from artifacts or extra shapes.", "section": "5.3.2 CLEVR Dataset"}, {"figure_path": "34dHGTri2w/figures/figures_16_3.jpg", "caption": "Figure 17: Generation of cube and cylinder.", "description": "This figure compares the results of generating images of cubes and cylinders using four different sampling methods: Baseline, MALA, HMC, and FTH. The images generated by FTH show a better generation quality compared to the other methods.", "section": "5.3.2 CLEVR Dataset"}, {"figure_path": "34dHGTri2w/figures/figures_16_4.jpg", "caption": "Figure 10: Generation of sphere and cylinder. The zoomed images could be found at Figure 20.", "description": "This figure compares the results of generating images of spheres and cylinders using four different sampling methods: Baseline, MALA, HMC, and FTH.  The zoomed-in versions of these images are available in Figure 20 for a more detailed comparison.  The goal is to assess the quality and accuracy of the generated images by each method.", "section": "5.3.2 CLEVR Dataset"}, {"figure_path": "34dHGTri2w/figures/figures_17_1.jpg", "caption": "Figure 19: Generation of cube.", "description": "This figure compares the generation of cube images using four different methods: Baseline, MALA, HMC, and FTH.  The images show that FTH produces higher-quality images with less noise and fewer artifacts compared to other methods.", "section": "5.3.2 CLEVR Dataset"}]