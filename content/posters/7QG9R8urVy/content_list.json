[{"type": "text", "text": "Doubly Mild Generalization for Offline Reinforcement Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yixiu Mao1, Qi Wang1, Yun $\\mathbf{Qu}^{1}$ , Yuhang Jiang1, Xiangyang Ji1 1Department of Automation, Tsinghua University myx21@mails.tsinghua.edu.cn, xyji@tsinghua.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Offilne Reinforcement Learning (RL) suffers from the extrapolation error and value overestimation. From a generalization perspective, this issue can be attributed to the over-generalization of value functions or policies towards out-of-distribution (OOD) actions. Significant efforts have been devoted to mitigating such generalization, and recent in-sample learning approaches have further succeeded in entirely eschewing it. Nevertheless, we show that mild generalization beyond the dataset can be trusted and leveraged to improve performance under certain conditions. To appropriately exploit generalization in offline RL, we propose Doubly Mild Generalization (DMG), comprising (i) mild action generalization and (ii) mild generalization propagation. The former refers to selecting actions in a close neighborhood of the dataset to maximize the Q values. Even so, the potential erroneous generalization can still be propagated, accumulated, and exacerbated by bootstrapping. In light of this, the latter concept is introduced to mitigate the generalization propagation without impeding the propagation of RL learning signals. Theoretically, DMG guarantees better performance than the in-sample optimal policy in the oracle generalization scenario. Even under worst-case generalization, DMG can still control value overestimation at a certain level and lower bound the performance. Empirically, DMG achieves state-of-the-art performance across Gym-MuJoCo locomotion tasks and challenging AntMaze tasks. Moreover, benefiting from its flexibility in both generalization aspects, DMG enjoys a seamless transition from offline to online learning and attains strong online fine-tuning performance. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Reinforcement learning (RL) aims to solve sequential decision-making problems and has garnered significant attention in recent years [53, 67, 74, 63, 12]. However, its practical applications encounter several challenges, such as risky exploration attempts [20] and time-consuming data collection phases [35]. Offline RL emerges as a promising paradigm to alleviate these challenges by learning without interaction with the environment [40, 42]. It eliminates the need for unsafe exploration and facilitates the utilization of pre-existing large-scale datasets [31, 48, 59]. ", "page_idx": 0}, {"type": "text", "text": "However, offline RL suffers from the out-of-distribution (OOD) issue and extrapolation error [19]. From a generalization perspective, this well-known challenge can be regarded as a consequence of the over-generalization of value functions or policies towards OOD actions [47]. Specifically, the potential value over-estimation at OOD actions caused by intricate generalization is often improperly captured by the max operation [73]. This over-estimation will propagate to values of in-distribution samples through Bellman backups and further spread to values of OOD ones via generalization. In mitigating value overestimation caused by OOD actions, substantial efforts have been dedicated [19, 39, 38, 17] and recent advancements in in-sample learning have successfully formulated the Bellman target solely with the actions present in the dataset [37, 85, 92, 88, 21] and extracted policies by weighted behavior cloning [57, 80]. As a result, these algorithms completely eschew generalization and avoid the extrapolation error. Despite simplicity, this way can not take advantage of the generalization ability of neural networks, which could be beneficial for performance improvement. Until now, how to appropriately exploit generalization in offline RL remains a lasting issue. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "This work demonstrates that mild generalization beyond the dataset can be trusted and leveraged to improve performance under certain conditions. For appropriate exploitation of mild generalization, we propose Doubly Mild Generalization (DMG) for offilne RL, comprising (i) mild action generalization and (ii) mild generalization propagation. The former concept refers to choosing actions in the vicinity of the dataset to maximize the Q values. However, the mere utilization of mild action generalization still falls short in adequately circumventing potential erroneous generalization, which can be propagated, accumulated, and exacerbated through the process of bootstrapping. To address this, we propose a novel concept, mild generalization propagation, which involves reducing the generalization propagation while preserving the propagation of RL learning signals. Regarding DMG\u2019s implementation, this work presents a simple yet effective scheme. Specifically, we blend the mildly generalized max with the in-sample max in the Bellman target, where the former is achieved by actor-critic learning with regularization towards high-value in-sample actions, and the latter is accomplished using in-sample learning techniques such as expectile regression [37]. ", "page_idx": 1}, {"type": "text", "text": "We conduct a thorough theoretical analysis of our approach DMG in both oracle and worst-case generalization scenarios. Under oracle generalization, DMG guarantees better performance than the in-sample optimal policy in the dataset [38, 37]. Even under worst-case generalization, DMG can still upper bound the overestimation of value functions and guarantee to output a safe policy with a performance lower bound. Empirically1, DMG achieves state-of-the-art performance on standard offline RL benchmarks [16], including Gym-MuJoCo locomotion tasks and challenging AntMaze tasks. Moreover, benefiting from its flexibility in both generalization aspects, DMG can seamlessly transition from offline to online learning and attain superior online fine-tuning performance. ", "page_idx": 1}, {"type": "text", "text": "2 Preliminaries ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "RL. The environment in RL is mostly characterized as a Markov decision process (MDP), which can be represented as a tuple $\\mathcal{M}=(\\mathcal{S},\\mathcal{A},P,R,\\gamma,d_{0})$ , comprising the state space $\\boldsymbol{S}$ , action space $\\boldsymbol{\\mathcal{A}}$ , transition dynamics $P:S\\times A\\to\\Delta(S)$ , reward function $\\bar{R}:\\bar{S}\\ \\times\\bar{A}\\rightarrow[0,\\bar{R}_{\\mathrm{max}}]$ , discount factor $\\gamma\\in[0,1)$ , and initial state distribution $d_{0}$ [70]. The goal of RL is to find a policy $\\pi:S\\to\\Delta(A)$ that can maximize the expected discounted return, denoted as $J(\\pi)$ : ", "page_idx": 1}, {"type": "equation", "text": "$$\nJ(\\pi)=\\mathbb{E}_{s_{0}\\sim d_{0},a_{t}\\sim\\pi(\\cdot|s_{t}),s_{t+1}\\sim P(\\cdot|s_{t},a_{t})}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R(s_{t},a_{t})\\right].\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "For any policy $\\pi$ , we define the value function as $\\begin{array}{r}{V^{\\pi}(s)=\\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R(s_{t},a_{t})|s_{0}=s\\right]}\\end{array}$ and the state-action value function ( $\\mathrm{\\Delta}Q$ -value function) as $\\begin{array}{r}{\\mathcal{Q}^{\\pi}(s,a)=\\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}R(s_{t},a_{t})|s_{0}=s,a_{0}=a\\right]}\\end{array}$ ", "page_idx": 1}, {"type": "text", "text": "Offline RL. Distinguished from traditional online RL training, offline RL handles a static dataset of transitions $\\mathcal{D}\\,=\\,\\bar{\\{}(s_{i},a_{i},r_{i},s_{i}^{\\prime})\\}_{i=0}^{n-1}$ and seeks an optimal policy without any additional data collection [40, 42]. We use ${\\hat{\\beta}}(a|s)$ to denote the empirical behavior policy observed in $\\mathcal{D}$ , which depicts the conditional distributions in the dataset [19]. Ordinary approximate dynamic programming methods minimize temporal difference error, according to the following loss [70]: ", "page_idx": 1}, {"type": "equation", "text": "$$\nL_{T D}(\\theta)=\\mathbb{E}_{(s,a,s^{\\prime})\\sim\\mathcal{D}}\\left[(Q_{\\theta}(s,a)-R(s,a)-\\gamma\\operatorname*{max}_{a^{\\prime}}Q_{\\theta^{\\prime}}(s^{\\prime},a^{\\prime}))^{2}\\right],\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\pi_{\\phi}$ is a parameterized policy, $Q_{\\theta}(s,a)$ is a parameterized $Q$ function, and $Q_{\\theta^{\\prime}}(s,a)$ is a target $Q$ function whose parameters are updated via Polyak averaging [53]. ", "page_idx": 1}, {"type": "text", "text": "3 Doubly Mild Generalization for Offline RL ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "This section discusses the strategy to appropriately exploit generalization in offilne RL. In Section 3.1, we introduce a formal perspective on how generalization impacts offilne RL and discuss the issues of over-generalization and non-generalization. Subsequently, we propose the DMG concept, comprising mild action generalization and mild generalization propagation in Section 3.2. Following this, we conduct a comprehensive analysis of DMG in both oracle generalization (Section 3.3) and worst-case generalization scenarios (Section 3.4). Finally, we present the practical algorithm in Section 3.5. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "3.1 Generalization Issues in Offline RL ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Offilne RL training typically involves a complex interaction between Bellman backup and generalization [47]. Offilne RL algorithms vary in backup mechanisms to train the Q function. Here we denote a generic form of Bellman backup as $\\tau_{u}$ , where $u$ is a distribution in the action space. ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\mathcal{T}_{u}Q(s,a):=R(s,a)+\\gamma\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot|s,a)}\\left[\\operatorname*{max}_{a^{\\prime}\\sim u(\\cdot|s^{\\prime})}Q(s^{\\prime},a^{\\prime})\\right]\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "During offilne training, this backup is exclusively executed on $(s,a)\\in\\mathcal{D}$ , and the values of $(s,a)\\notin D$ are influenced solely via generalization. A crucial aspect is that $(s^{\\prime},a^{\\prime})$ in the Bellman target can be absent from the dataset $\\mathcal{D}$ , depending on the choice of $u$ . As a result, Bellman backup and generalization exhibit an intricate interaction: the backups on $(s,a)\\,\\in\\,\\mathcal{D}$ impact the values of $\\left(s,a\\right)\\notin D$ via generalization; the values of $(s,a)\\notin D$ participates in the computation of Bellman target, thereby affecting the values of $(s,a)\\in\\mathcal{D}$ . ", "page_idx": 2}, {"type": "text", "text": "This interaction poses a key challenge in offilne RL, value overestimation. The potential overestimation of values of $(s,a)\\notin D$ , induced by intricate generalization, tends to be improperly captured by the max operation, a phenomenon known as maximization bias [73]. This overestimation propagates to values of $(s,a)\\in\\mathcal{D}$ through backups and further extends to values of $(s,a)\\notin D$ via generalization. This cyclic process consistently amplifies value overestimation, potentially resulting in value divergence. The crux of this detrimental process can be summarized as over-generalization. ", "page_idx": 2}, {"type": "text", "text": "To address value overestimation, recent advancements in the field have introduced a paradigm known as in-sample learning, which formulates the Bellman target solely with the actions present in the dataset [37, 85, 92, 88, 21]. Its effect is equivalent to choosing $u$ in $\\tau_{u}$ to be exactly $\\hat{\\beta}$ , i.e., the empirical behavior policy observed in the dataset. Following in-sample value learning, policies are extracted from the learned Q functions using weighted behavior cloning [57, 9, 55]. By entirely eschewing generalization in offline RL training, they effectively avoid the extrapolation error [19], a strategy we term non-generalization. However, the ability to generalize is a critical factor contributing to the extensive utilization of neural networks [41]. In this sense, in-sample learning methods seem too conservative without utilizing generalization, particularly when the offilne datasets do not cover the optimal actions in large or continuous spaces. ", "page_idx": 2}, {"type": "text", "text": "3.2 Doubly Mild Generalization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The following focuses on the appropriate exploitation of generalization in offline RL. ", "page_idx": 2}, {"type": "text", "text": "We start by analyzing the generalization effect under the generic backup operator $\\tau_{u}$ . We consider a straightforward scenario, where $Q_{\\theta}$ is updated to $Q_{\\theta^{\\prime}}$ by one gradient step on a single $(s,a)\\in\\mathcal{D}$ with learning rate $\\alpha$ . We characterize the resulting generalization effect on any $(s,\\tilde{a})\\notin\\mathcal{D}^{2}$ as follows. ", "page_idx": 2}, {"type": "text", "text": "Theorem 1 (Informal). Under certain continuity conditions, the following equation holds when the learning rate $\\alpha$ is sufficiently small and $\\tilde{a}$ is sufficiently close to $a$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\nQ_{\\theta^{\\prime}}(s,\\tilde{a})=Q_{\\theta}(s,\\tilde{a})+C_{1}\\left(\\mathcal T_{u}Q_{\\theta}(s,\\tilde{a})-Q_{\\theta}(s,\\tilde{a})+C_{2}\\|\\tilde{a}-a\\|\\right)+\\mathcal O\\left(\\|\\theta^{\\prime}-\\theta\\|^{2}\\right)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $C_{1}\\in[0,1]$ and $C_{2}$ is a bounded constant. ", "page_idx": 2}, {"type": "text", "text": "The formal theorem and all proofs are deferred to Appendix B. ", "page_idx": 2}, {"type": "text", "text": "Note that Eq. (4) is the update of the parametric $\\mathrm{^Q}$ function $\\mathit{\\Pi}(Q_{\\theta}\\,\\to\\,Q_{\\theta^{\\prime}})$ at state-action pairs $(s,\\tilde{a})\\notin D$ , which is exclusively caused by generalization. If $\\tilde{a}$ is within a close neighborhood of $a$ , then $C_{2}\\|\\tilde{a}-a\\|$ is small. Moreover, as $C_{1}\\in[0,1]$ , Eq. (4) approximates an update towards the true objective $\\pi_{u}\\dot{Q}_{\\theta}(s,\\tilde{a})$ , as if $Q_{\\theta}(s,\\tilde{a})$ is updated by a true gradient step at $(s,\\bar{a})\\notin{\\cal D}$ . Therefore, ", "page_idx": 2}, {"type": "text", "text": "Theorem 1 shows that, under certain continuity conditions, $\\mathrm{^Q}$ functions can generalize well and approximate true updates in a close neighborhood of samples in the dataset. This implies that mild generalizations beyond the dataset can be leveraged to potentially pursue better performance. Inspired by Theorem 1, we define a mildly generalized policy $\\tilde{\\beta}$ as follows. ", "page_idx": 3}, {"type": "text", "text": "Definition 1 (Mildly generalized policy). Policy $\\tilde{\\beta}$ is termed a mildly generalized policy if it satisfies ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname{supp}(\\hat{\\beta}(\\cdot|s))\\subseteq\\operatorname{supp}(\\tilde{\\beta}(\\cdot|s)),\\;\\;a n d\\quad\\operatorname*{max}_{a_{1}\\sim\\tilde{\\beta}(\\cdot|s)}\\operatorname*{min}_{a_{2}\\sim\\hat{\\beta}(\\cdot|s)}\\|a_{1}-a_{2}\\|\\leq\\epsilon_{a},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\hat{\\beta}$ is the empirical behavior policy observed in the offline dataset. ", "page_idx": 3}, {"type": "text", "text": "It means that $\\tilde{\\beta}$ has a wider support than $\\hat{\\beta}$ (the dataset), and for any $a_{1}\\,\\sim\\,\\tilde{\\beta}(\\cdot|s)$ , we can find $a_{2}\\sim\\hat{\\beta}(\\cdot|s)$ (in dataset) such that $\\|a_{1}-a_{2}\\|\\leq\\epsilon_{a}$ . In other words, the generalization of $\\tilde{\\beta}$ beyond the dataset is bounded by $\\epsilon_{a}$ when measured in the action space distance. According to Theorem 1, there is a high chance that $Q_{\\theta}$ can generalize well in this mild generalization area $\\bar{\\beta}(a|s)>0$ . ", "page_idx": 3}, {"type": "text", "text": "However, even in this mild generalization area, it is inevitable that the learned value function will incur some degree of generalization error. The possible erroneous generalization can still be propagated and exacerbated by value bootstrapping as discussed in Section 3.1. To this end, we introduce an additional level of mild generalization, termed mild generalization propagation, and propose a novel Doubly Mildly Generalization (DMG) operator as follows. ", "page_idx": 3}, {"type": "text", "text": "Definition 2. The Doubly Mild Generalization (DMG) operator is defined as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathcal{T}_{\\mathrm{DMG}}Q(s,a):=R(s,a)+\\gamma\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot|s,a)}\\left[\\lambda\\operatorname*{max}_{a^{\\prime}\\sim\\tilde{\\beta}(\\cdot|s^{\\prime})}Q(s^{\\prime},a^{\\prime})+(1-\\lambda)\\operatorname*{max}_{a^{\\prime}\\sim\\tilde{\\beta}(\\cdot|s^{\\prime})}Q(s^{\\prime},a^{\\prime})\\right]\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\hat{\\beta}$ is the empirical behavior policy in the dataset and $\\tilde{\\beta}$ is a mildly generalized policy. ", "page_idx": 3}, {"type": "text", "text": "Note that in typical offline RL algorithms, extrapolation error and value overestimation caused by erroneous generalization are propagated through bootstrapping, and the discount factor of this process is $\\gamma$ . DMG reduces this discount factor to $\\lambda\\gamma$ , mitigating the amplification of value overestimation. On the other hand, in contrast to in-sample methods, DMG allows mild generalization, utilizing the generalization ability of neural networks to seek better performance, as Theorem 1 suggests that value functions are highly likely to generalize well in the mild generalization area. ", "page_idx": 3}, {"type": "text", "text": "To summarize, the generalization of DMG is mild in two aspects: (i) mild action generalization: based on the mildly generalized policy $\\tilde{\\beta}$ , which generalizes beyond $\\hat{\\beta}$ , DMG selects actions in a close neighborhood of the dataset to maximize the $\\mathrm{\\DeltaQ}$ values in the first part of the Bellman target; and (ii) mild generalization propagation: DMG mitigates the generalization propagation without hindering the propagation of RL learning signals by blending the mildly generalized max with the in-sample max in the Bellman target. This reduces the discount factor through which generalization propagates, mitigating the amplification of value overestimation caused by bootstrapping. ", "page_idx": 3}, {"type": "text", "text": "To support the above claims, we provide a comprehensive analysis of DMG in both oracle and worst-case generalization scenarios, with particular emphasis on value estimation and performance. ", "page_idx": 3}, {"type": "text", "text": "3.3 Oracle Generalization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "This section conducts analyses under the assumption that the learned value functions can achieve oracle generalization in the mild generalization area $\\tilde{\\beta}(a|s)>0$ , formally defined as follows. ", "page_idx": 3}, {"type": "text", "text": "Assumption 1 (Oracle generalization). The generalization of learned $Q$ functions in the mild generalization area $\\bar{\\beta}(a|\\bar{s})>0$ reflects the true value updates according to $\\mathcal{T}_{\\mathrm{DMG}}$ . ", "page_idx": 3}, {"type": "text", "text": "The mild generalization area $\\tilde{\\beta}(a|s)>0$ may contain some points outside the offline dataset, and ${\\mathcal{T}}_{\\mathrm{DMG}}$ might query $\\mathrm{\\DeltaQ}$ values of such points. This assumption assumes that the generalization at such points reflects the true value updates according to $\\mathcal{T}_{\\mathrm{DMG}}$ . The rationale for such an assumption comes from Theorem 1, which characterizes the generalization effect of value functions in the mild generalization area. Now we analyze the dynamic programming properties of the operators ${\\mathcal{T}}_{\\mathrm{DMG}}$ and $\\mathcal{T}_{\\mathrm{In}}$ , where $\\mathcal{T}_{\\mathrm{In}}$ is the in-sample Q learning operator [37, 88, 21] defined as follows. ", "page_idx": 3}, {"type": "text", "text": "Definition 3. The In-sample $Q$ Learning operator $I37J$ is defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{T}_{\\mathrm{In}}Q(s,a):=R(s,a)+\\gamma\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot|s,a)}\\left[\\operatorname*{max}_{a^{\\prime}\\sim\\hat{\\beta}(\\cdot|s^{\\prime})}Q(s^{\\prime},a^{\\prime})\\right]\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\hat{\\beta}$ is the empirical behavior policy in the dataset. ", "page_idx": 4}, {"type": "text", "text": "Lemma 1. $\\mathcal{T}_{\\mathrm{In}}$ is a $\\gamma$ -contraction operator in the in-sample area $\\hat{\\beta}(a|s)>0$ under the $\\mathcal{L}_{\\infty}$ norm. ", "page_idx": 4}, {"type": "text", "text": "Following Lemma 1, we denote the fixed point of $\\mathcal{T}_{\\mathrm{In}}$ as $Q_{\\mathrm{In}}^{*}$ , and its induced policy as $\\pi_{\\mathrm{In}}^{*}$ . Here $Q_{\\mathrm{In}}^{*}$ is known as the in-sample optimal value function [37], which is the value function of the in-sample optimal policy $\\pi_{\\mathrm{In}}^{*}$ . We refer readers to [37, 38, 88] for more discussions on the in-sample optimality. ", "page_idx": 4}, {"type": "text", "text": "Now we present the theoretical properties of DMG for comparison. ", "page_idx": 4}, {"type": "text", "text": "Theorem 2 (Contraction). Under Assumption $^{\\,l}$ , $\\mathcal{T}_{\\mathrm{DMG}}$ is a $\\gamma$ -contraction operator in the mild generalization area $\\tilde{\\beta}(a|s)>0$ under the $\\mathcal{L}_{\\infty}$ norm. Therefore, by repeatedly applying $\\mathcal{T}_{\\mathrm{DMG}}$ , any initial $Q$ function can converge to the unique fixed point $Q_{\\mathrm{DMG}}^{*}$ . ", "page_idx": 4}, {"type": "text", "text": "We denote the induced policy of $Q_{\\mathrm{DMG}}^{*}$ as $\\pi_{\\mathrm{DMG}}^{*}$ , whose performance is guaranteed as follows. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3 (Performance). Under Assumption $^{\\,l}$ , the value functions of $\\pi_{\\mathrm{DMG}}^{*}$ and $\\pi_{\\mathrm{In}}^{*}$ satisfy: ", "page_idx": 4}, {"type": "equation", "text": "$$\nV^{\\pi_{\\mathrm{DMG}}^{*}}(s)\\geq V^{\\pi_{\\mathrm{In}}^{*}}(s),\\quad\\forall s\\in\\mathcal{D}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Theorem 3 indicates that the policy learned by DMG can achieve better performance than the in-sample optimal policy under the oracle generalization condition. ", "page_idx": 4}, {"type": "text", "text": "3.4 Worst-case Generalization ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "This section turns to the analyses in the worst-case generalization scenario, where the learned value functions may exhibit poor generalization in the mild generalization area $\\tilde{\\beta}(a|s)>0$ . In other words, this section considers that ${\\mathcal{T}}_{\\mathrm{DMG}}$ is only defined in the in-sample area $\\hat{\\beta}(a|s)>0$ and the learned value functions may have any generalization error at other state-action pairs. In this case, we use the notation $\\hat{\\mathcal{T}}_{\\mathrm{DMG}}$ to tell the difference. ", "page_idx": 4}, {"type": "text", "text": "We make continuity assumptions about the learned Q function and the transition dynamics. ", "page_idx": 4}, {"type": "text", "text": "Assumption 2 (Lipschitz $Q$ ). The learned $Q$ function is $K_{Q}$ -Lipschitz. $\\forall s\\,\\sim\\,\\mathcal{D},\\,\\forall a_{1},a_{2}\\,\\sim\\,\\mathcal{A},$ $|Q(s,a_{1}^{-})-Q(s,a_{2}^{-})|\\leq K_{Q}\\|a_{1}-a_{2}\\|$ ", "page_idx": 4}, {"type": "text", "text": "Assumption 3 (Lipschitz $P$ ). The transition dynamics $P$ is $K_{P}$ -Lipschitz. $\\forall s,s^{\\prime}\\sim{\\cal S},\\,\\forall a_{1},a_{2}\\sim{\\cal A},$ , $|P(s^{\\prime}|s,a_{1})-P(\\bar{s}^{\\prime}|s,a_{2})|\\leq K_{P}\\|a_{1}-a_{2}\\|$ ", "page_idx": 4}, {"type": "text", "text": "For Assumption 2, a continuous learned Q function is particularly necessary for analyzing value function generalization and can be relatively easily satisfied using neural networks or linear models [24]. Assumption 3 is also a common assumption in theoretical studies of RL [13, 87, 61]. ", "page_idx": 4}, {"type": "text", "text": "Now we consider the iteration starting from arbitrary function $\\mathcal{Q}^{0}\\colon\\hat{Q}_{\\mathrm{DMG}}^{k}\\,=\\,\\hat{T}_{\\mathrm{DMG}}\\hat{Q}_{\\mathrm{DMG}}^{k-1}$ and $Q_{\\mathrm{In}}^{k}=\\mathcal{T}_{\\mathrm{In}}Q_{\\mathrm{In}}^{k-1}$ , $\\forall k\\in\\mathbb{Z}^{+}$ . The possible value of $\\hat{Q}_{\\mathrm{DMG}}^{k}$ is bounded by the following results. ", "page_idx": 4}, {"type": "text", "text": "Theorem 4 (Limited overestimation). Under Assumption 2, the learned $Q$ function of DMG by iterating $\\hat{\\mathcal{T}}_{\\mathrm{DMG}}$ satisfies the following inequality ", "page_idx": 4}, {"type": "equation", "text": "$$\nQ_{\\mathrm{In}}^{k}(s,a)\\leq\\hat{Q}_{\\mathrm{DMG}}^{k}(s,a)\\leq Q_{\\mathrm{In}}^{k}(s,a)+\\frac{\\lambda\\epsilon_{a}K_{Q}\\gamma}{1-\\gamma}(1-\\gamma^{k}),\\ \\forall s,a\\sim\\mathcal{D},\\ \\forall k\\in\\mathbb{Z}^{+}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Since in-sample training eliminates the extrapolation error [37, 92], $Q_{\\mathrm{In}}^{k}$ can be considered a relatively accurate estimate [37]. Therefore, Theorem 4 suggests that DMG exhibits limited value overestimation under the worst-case generalization scenario. Moreover, the bound becomes tighter as $\\epsilon_{a}$ decreases (milder action generalization) and $\\lambda$ decreases (milder generalization propagation). This is consistent with our intuitions in Section 3.2. ", "page_idx": 4}, {"type": "text", "text": "Finally, we show in Theorem 5 that even under worst-case generalization, DMG guarantees to output a safe policy with a performance lower bound. ", "page_idx": 4}, {"type": "text", "text": "Theorem 5 (Performance lower bound). Let $\\ensuremath{\\hat{\\pi}}_{\\mathrm{DMG}}$ be the learned policy of DMG by iterating $\\hat{\\mathcal{T}}_{\\mathrm{DMG}}$ , $\\pi^{*}$ be the optimal policy, and $\\epsilon_{\\mathcal{D}}$ be the inherent performance gap of the in-sample optimal policy $\\epsilon_{\\mathscr D}:={\\cal J}(\\pi^{*})-{\\cal J}(\\pi_{\\mathrm{In}}^{*})$ . Under Assumptions 2 and $3$ , for sufficiently small $\\epsilon_{a}$ , we have ", "page_idx": 5}, {"type": "equation", "text": "$$\nJ(\\hat{\\pi}_{\\mathrm{DMG}})\\geq J(\\pi^{*})-\\frac{C K_{P}R_{\\operatorname*{max}}}{1-\\gamma}\\epsilon_{a}-\\epsilon_{\\mathcal{D}}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $C$ is a positive constant. ", "page_idx": 5}, {"type": "text", "text": "3.5 Practical Algorithm ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "This section puts DMG into implementation and presents a simple yet effective practical algorithm. The algorithm comprises the following networks: policy $\\pi_{\\phi}$ , target policy $\\pi_{\\phi^{\\prime}}$ , $Q$ network $Q_{\\theta}$ , target $Q$ network $Q_{\\theta^{\\prime}}$ , and $V$ network $V_{\\psi}$ . ", "page_idx": 5}, {"type": "text", "text": "Policy learning. Practically, we expect DMG to exhibit a tendency towards mild generalization around good actions in the dataset. To this end, we first consider reshaping the empirical behavior policy $\\hat{\\beta}$ to be skewed towards actions with high advantage values $\\hat{\\beta}^{*}(a|s)\\propto\\hat{\\beta}(a|s)\\exp(A(s,a))$ . Then we enforce the proximity between the trained policy and the reshaped behavior policy to constrain the generalization area. We define the generalization set $\\Pi_{G}$ as follows. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\Pi_{G}=\\{\\pi\\mid\\mathrm{KL}(\\hat{\\beta}^{*}(\\cdot|s)\\|\\pi(\\cdot|s))\\leq\\epsilon\\}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that forward $\\mathrm{KL}$ allows $\\pi$ to select actions outside the support of ${\\hat{\\beta}}^{*}$ , enabling $\\Pi_{G}$ to generalize beyond the actions in the dataset. With $\\Pi_{G}$ defined, the next step is to compute the maximal $Q$ within $\\Pi_{G}$ . To accomplish this, we adopt Actor-Critic style training [70] for this part. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\phi}\\mathbb{E}_{s\\sim\\mathcal{D},a\\sim\\pi_{\\phi}(\\cdot|s)}Q_{\\theta}(s,a),\\quad s.t.\\;\\pi_{\\phi}\\in\\Pi_{G}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "By treating the constraint term as a penalty, we maximize the following objective. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{\\phi}\\mathbb{E}_{s\\sim\\mathcal{D},a\\sim\\pi_{\\phi}(\\cdot|s)}Q_{\\theta}(s,a)-\\nu\\mathbb{E}_{s\\sim\\mathcal{D}}\\mathrm{KL}(\\hat{\\beta}^{*}(\\cdot|s)\\|\\pi_{\\phi}(\\cdot|s))\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Through straightforward derivations, Eq. (13) is equivalent to the following policy training objective. ", "page_idx": 5}, {"type": "equation", "text": "$$\nJ_{\\pi}(\\phi)=\\mathbb{E}_{s\\sim\\mathcal{D},a\\sim\\pi_{\\phi}(\\cdot\\vert s)}Q_{\\theta}(s,a)-\\nu\\mathbb{E}_{(s,a)\\sim\\mathcal{D}}\\left[\\exp(\\alpha(Q_{\\theta^{\\prime}}(s,a)-V_{\\psi}(s)))\\log\\pi_{\\phi}(a\\vert s)\\right]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\alpha$ is an inverse temperature and $Q_{\\theta^{\\prime}}(s,a)-V_{\\psi}(s)$ computes the advantage function $A(s,a)$ . ", "page_idx": 5}, {"type": "text", "text": "Value learning. Now we turn to the implementation of the $\\mathcal{T}_{\\mathrm{DMG}}$ operator for training value functions. By introducing the aforementioned policy, we can substitute $\\operatorname*{max}_{a\\sim\\tilde{\\beta}}$ in $\\mathcal{T}_{\\mathrm{DMG}}$ with $\\mathbb{E}_{a\\sim\\pi}$ . Regarding $\\operatorname*{max}_{a\\sim\\hat{\\beta}}$ in $\\mathcal{T}_{\\mathrm{DMG}}$ , any in-sample learning techniques can be employed to compute the in-sample maximum [37, 88, 85, 21]. In particular, based on IQL [37], we perform expectile regression. ", "page_idx": 5}, {"type": "table", "img_path": "7QG9R8urVy/tmp/c30913b647f76d99e70c4f95af803e321d641153557b6a60ea63438dc143bdab.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "equation", "text": "$$\nL_{V}(\\psi)=\\underset{(s,a)\\sim\\mathcal{D}}{\\mathbb{E}}\\left[L_{2}^{\\tau}\\left(Q_{\\theta^{\\prime}}(s,a)-V_{\\psi}(s)\\right)\\right]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $L_{2}^{\\tau}(u)=|\\tau-\\mathbb{1}(u<0)|u^{2}$ and $\\tau\\in(0,1)$ . For $\\tau\\approx1$ , $V_{\\psi}$ can capture the in-sample maximal $Q$ [37]. Finally, we have the following value training loss. ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{L_{Q}(\\theta)=\\mathbb{E}_{(s,a,s^{\\prime})\\sim\\mathcal{D}}\\left[\\left(Q_{\\theta}(s,a)-R(s,a)-\\gamma\\lambda\\mathbb{E}_{a^{\\prime}\\sim\\pi_{\\phi^{\\prime}}}Q_{\\theta^{\\prime}}(s^{\\prime},a^{\\prime})-\\gamma(1-\\lambda)V_{\\psi}(s^{\\prime})\\right)^{2}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Overall algorithm. Integrating all components, we present our practical algorithm in Algorithm 1. ", "page_idx": 5}, {"type": "text", "text": "4 Discussions and Related Work ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Summary of offline RL work from a generalization perspective. As analyzed above, DMG is featured in both mild action generalization and mild generalization propagation. Within the actorcritic framework upon which most offline RL algorithms are built, these two aspects correspond to the policy and value training phases, respectively. Action generalization concerns whether the policy training intentionally selects actions beyond the dataset to maximize Q values, while generalization propagation involves whether value training propagates generalization through bootstrapping. Table 1 presents a clear comparison of offline RL works in this generalization view. The table shows one representative method of each category and we elaborate on others as follows. ", "page_idx": 6}, {"type": "table", "img_path": "7QG9R8urVy/tmp/d41bec3ea9dfa4f45f994deac5b876bab4565d6b2e9d0bb59cea2605b8cd42ad.jpg", "table_caption": ["Table 1: Comparison of offline RL work from the generalization perspective. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "Concerning policy learning, AWR [57], AWAC [55], CRR [80], $10\\%$ BC [8], IQL [37], and other works such as [78, 9, 66, 21, 88] extract policies through weighted or filtered behavior cloning, thereby lacking intentional action generalization to maximize Q values beyond the dataset. Typical policy-regularized offline RL methods like TD3BC [17], BRAC [84], BEAR [38], SPOT [83], and others such as [79, 61, 72] introduce regularization terms to Q maximization objectives to regularize the trained policy towards the behavior policy and allows mild action generalization. Online RL algorithms like TD3 [18] and SAC [27] have no constraints and maximize Q values in the entire action space, corresponding to full action generalization. Regarding value training, in-sample learning methods including OneStep RL [7], IQL [37], InAC [85], IAC [92], $\\mathcal{X}\\mathrm{QL}$ [21], and SQL [88] completely avoid generalization propagation and accumulation via bootstrapping, whereas typical offilne and online RL approaches allow full generalization propagation through bootstrapping. In the proposed approach DMG, generalization is mild in both aspects. ", "page_idx": 6}, {"type": "text", "text": "Recently, Ma et al. [47] have also drawn attention to generalization in offilne RL and the issue of overgeneralization. They mitigate over-generalization from a representation perspective, differentiating between the representations of in-sample and OOD state-action pairs. Lyu et al. [44] argue that conventional value penalization like CQL [39] tends to harm the generalization of value functions and hinder performance improvement. They propose mild value penalization to mitigate the detrimental effects of value penalization on generalization. ", "page_idx": 6}, {"type": "text", "text": "Connection to heuristic blending approaches. Our approach also relates to the framework of blending heuristics into bootstrapping [10, 81, 71, 28, 82, 22]. In offline RL, HUBL [22] blends Monte-Carlo returns into bootstrapping and acts as a data relabeling step, which reduces the degree of bootstrapping and thereby increases its performance. In contrast, DMG blends the in-sample maximal values into the bootstrapping operator. DMG does not reduce the discount for RL learning but reduces the discount for generalization propagation. ", "page_idx": 6}, {"type": "text", "text": "For extended discussions on related work, please refer to Appendix A. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we conduct several experiments to justify the validity of the proposed method DMG.   \nExperimental details and extended results are provided in Appendices C and D, respectively. ", "page_idx": 6}, {"type": "text", "text": "5.1 Main Results on Offline RL Benchmarks ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Tasks. We evaluate the proposed approach on Gym-MuJoCo locomotion domains and challenging AntMaze domains in D4RL [16]. The latter involves sparse-reward tasks and necessitates \u201cstitching\u201d fragments of suboptimal trajectories traveling undirectedly to find a path to the goal of the maze. ", "page_idx": 6}, {"type": "text", "text": "Baselines. Our offline RL baselines include both typical bootstrapping methods and in-sample learning approaches. For the former, we compare to BCQ [19], BEAR [38], AWAC [55], TD3BC [17], and CQL [39]. For the latter, we compare to BC [58], OneStep RL [7], IQL [37], $\\mathcal{X}\\mathrm{QL}$ [21], and SQL [88]. We also include the sequence-modeling method Decision Transformer (DT) [8]. ", "page_idx": 6}, {"type": "table", "img_path": "7QG9R8urVy/tmp/2c886d380c2cd1fcdc00e4ec357de88447e1e701354b097a88a6943ffa12d331.jpg", "table_caption": ["Table 2: Averaged normalized scores on Gym locomotion and Antmaze tasks over five random seeds. $\\mathbf{m}=$ medium, $\\mathbf{m-r=}$ medium-replay, $\\mathbf{m-e=}$ medium-expert, $\\mathbf{e}=$ expert, $\\mathbf{r}=$ random; $\\mathfrak{u}=$ umaze, $\\mathbf{u-d=}$ umaze-diverse, $\\scriptstyle{\\mathrm{m-p}}=$ medium-play, $\\ m{\\mathrm{-d}}=$ medium-diverse, $\\mathsf{l}_{-\\mathrm{p}=}$ large-play, $\\left\\lvert-\\mathrm{d}=\\right\\lvert$ large-diverse. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Comparison with baselines. Aggregated results are displayed in Table 2. On the Gym locomotion tasks, DMG outperforms prior methods on most tasks and achieves the highest total score. On the much more challenging AntMaze tasks, DMG outperforms all the baselines by a large margin, especially in the most difficult large mazes. For detailed learning curves, please refer to Appendix D.3. According to [56], we also report the results of DMG over more random seeds in Appendix D.2. ", "page_idx": 7}, {"type": "text", "text": "Runtime. We test the runtime of DMG and other baselines on a GeForce RTX 3090. As shown in Appendix D.1, the runtime of DMG is comparable to that of the fastest offilne RL algorithm TD3BC. ", "page_idx": 7}, {"type": "text", "text": "5.2 Performance Improvement over In-sample Learning Approaches ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "DMG can be combined with various insample learning approaches. Besides IQL [37], we also apply DMG to two recent state-of-the-art in-sample algorithms, $\\mathcal{X}\\mathrm{QL}$ [21] and SQL [88]. As shown in Table 3 (and Table 2), DMG consistently and substantially improves upon these in-sample methods, particularly on sub-optimal datasets where generalization plays a crucial role in the pursuit of a better policy. This provides compelling empirical evidence that the performance of in-sample methods is largely confined by eschewing generalization beyond the dataset, while DMG effectively exploits generalization, achieving significantly improved performance across tasks. ", "page_idx": 7}, {"type": "table", "img_path": "7QG9R8urVy/tmp/6390f1d960ea4f39de53be5e2bc09195921f788071eddf0421dcaecf06be60a2.jpg", "table_caption": ["Table 3: DMG combined with various in-sample approaches, showing averaged scores over 5 seeds. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "7QG9R8urVy/tmp/3cf7d269feb5111152b6ec0c814c6340c51d815c3b6028a2ff2746f73061426c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 1: Performance and Q values of DMG with varying mixture coefficient $\\lambda$ over 5 random seeds. The crosses $\\times$ mean that the value functions diverge in several seeds. As $\\lambda$ increases, DMG enables stronger generalization propagation, resulting in higher and probably divergent learned Q values. Mild generalization propagation plays a crucial role in achieving strong performance. ", "page_idx": 8}, {"type": "image", "img_path": "7QG9R8urVy/tmp/b89586188f71ad1186dc8429951fe14d71123b06f8bc6dc8a651b4578c8cea6c.jpg", "img_caption": ["Figure 2: Performance and Q values of DMG with varying penalty coefficient $\\nu$ over 5 random seeds. As $\\nu$ decreases, DMG allows broader action generalization, leading to larger learned Q values. Mild action generalization is also critical for attaining superior performance. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.3 Ablation Study for Performance and Value Estimation ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Mixture coefficient $\\lambda$ . The mixture coefficient $\\lambda$ controls the extent of generalization propagation. We fix $\\nu=0.1$ and vary $\\lambda\\in[0,1]$ , presenting the learned $\\mathrm{^Q}$ values and performance on several tasks in Figure 1. As $\\lambda$ increases, DMG enables increased generalization propagation through bootstrapping, and the learned Q values become larger and probably diverge. A moderate $\\lambda$ (mild generalization propagation) is crucial for achieving strong performance across datasets. Under the same degree of action generalization, mild generalization propagation effectively suppresses value overestimation, facilitating more stable policy learning. ", "page_idx": 8}, {"type": "text", "text": "Penalty coefficient $\\nu$ . The penalty coefficient $\\nu$ regulates the degree of action generalization. We fix $\\lambda=0.25$ and vary $\\nu$ . The results are shown in Figure 2. As $\\nu$ decreases, DMG allows broader action generalization beyond the dataset, which results in higher learned values. Regarding performance, a moderate $\\nu$ (mild action generalization) is also crucial for achieving superior performance. ", "page_idx": 8}, {"type": "text", "text": "5.4 Online Fine-tuning after Offline RL ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Benefiting from its flexibility in both generalization aspects, DMG enjoys a seamless transition from offilne to online learning. This is accomplished through a gradual enhancement of both action generalization and generalization propagation. Since IQL [37] has demonstrated superior online fine-tuning performance compared to previous methods [55, 39] in its paper, we follow the experimental setup of IQL and compare to IQL. We also train online RL algorithm TD3 [18] from scratch for comparison. We use the challenging AntMaze domains [16], given DMG\u2019s already high offline perfor", "page_idx": 8}, {"type": "table", "img_path": "7QG9R8urVy/tmp/7cd78149b13f677a5dbd273c854a5d55e314baba5b86b1f4423b79cb9461ce54.jpg", "table_caption": ["Table 4: Online fine-tuning results on AntMaze tasks, showing normalized scores of offline training and 1M steps online fine-tuning, averaged over 5 seeds. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "mance in Gym locomotion domains. Results are presented in Table 4. While online training from scratch fails in the challenging sparse reward AntMaze tasks, DMG initialized with offilne pretraining succeeds in learning near-optimal policies, outperforming IQL by a significant margin. Please refer to Appendix C.2 for experimental details, and to Appendix D.4 for learning curves. ", "page_idx": 8}, {"type": "text", "text": "6 Conclusion and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work scrutinizes offline RL through the lens of generalization and proposes DMG, comprising mild action generalization and mild generalization propagation, to exploit generalization in offilne RL appropriately. We theoretically analyze DMG in oracle and worst-case generalization scenarios, and empirically demonstrate its SOTA performance in offilne training and online fine-tuning experiments. ", "page_idx": 9}, {"type": "text", "text": "While our work contributes valuable insights, it also has limitations. The DMG principle is shown to be effective across most scenarios. However, when the function approximator employed is highly compatible with a specific task setting, the learned value functions may generalize well in the entire action space. In such case, DMG may underperform full generalization methods due to conservatism. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgment ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We thank the anonymous reviewers for feedback on an early version of this paper. This work was supported by the National Key R&D Program of China under Grant 2018AAA0102801, National Natural Science Foundation of China under Grant 61827804. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "[1] Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In International conference on machine learning, pages 22\u201331. PMLR, 2017.   \n[2] Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offline reinforcement learning with diversified q-ensemble. Advances in neural information processing systems, 34:7436\u20137447, 2021.   \n[3] Chenjia Bai, Lingxiao Wang, Zhuoran Yang, Zhi-Hong Deng, Animesh Garg, Peng Liu, and Zhaoran Wang. Pessimistic bootstrapping for uncertainty-driven offilne reinforcement learning. In International Conference on Learning Representations, 2022. URL https://openreview. net/forum?id $\\equiv$ Y4cs1Z3HnqL.   \n[4] Jacob Beck, Risto Vuorio, Evan Zheran Liu, Zheng Xiong, Luisa Zintgraf, Chelsea Finn, and Shimon Whiteson. A survey of meta-reinforcement learning. arXiv preprint arXiv:2301.08028, 2023.   \n[5] Avinandan Bose, Simon Shaolei Du, and Maryam Fazel. Offline multi-task transfer rl with representational penalization. arXiv preprint arXiv:2402.12570, 2024.   \n[6] Stephen P Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.   \n[7] David Brandfonbrener, Will Whitney, Rajesh Ranganath, and Joan Bruna. Offline rl without off-policy evaluation. Advances in Neural Information Processing Systems, 34:4933\u20134946, 2021.   \n[8] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084\u201315097, 2021.   \n[9] Xinyue Chen, Zijian Zhou, Zheng Wang, Che Wang, Yanqiu Wu, and Keith Ross. Bail: Bestaction imitation learning for batch deep reinforcement learning. Advances in Neural Information Processing Systems, 33:18353\u201318363, 2020.   \n[10] Ching-An Cheng, Andrey Kolobov, and Adith Swaminathan. Heuristic-guided reinforcement learning. Advances in Neural Information Processing Systems, 34:13550\u201313563, 2021.   \n[11] Ching-An Cheng, Tengyang Xie, Nan Jiang, and Alekh Agarwal. Adversarially trained actor critic for offline reinforcement learning. In International Conference on Machine Learning, pages 3852\u20133878. PMLR, 2022.   \n[12] Jonas Degrave, Federico Felici, Jonas Buchli, Michael Neunert, Brendan Tracey, Francesco Carpanese, Timo Ewalds, Roland Hafner, Abbas Abdolmaleki, Diego de Las Casas, et al. Magnetic control of tokamak plasmas through deep reinforcement learning. Nature, 602(7897): 414\u2013419, 2022.   \n[13] Francois Dufour and Tomas Prieto-Rumeau. Finite linear programming approximations of constrained discounted markov decision processes. SIAM Journal on Control and Optimization, 51(2):1298\u20131324, 2013.   \n[14] Francois Dufour and Tomas Prieto-Rumeau. Approximation of average cost markov decision processes using empirical distributions and concentration inequalities. Stochastics An International Journal of Probability and Stochastic Processes, 87(2):273\u2013307, 2015.   \n[15] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pages 1126\u20131135. PMLR, 2017.   \n[16] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.   \n[17] Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offilne reinforcement learning. Advances in neural information processing systems, 34:20132\u201320145, 2021.   \n[18] Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In International conference on machine learning, pages 1587\u20131596. PMLR, 2018.   \n[19] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In International conference on machine learning, pages 2052\u20132062. PMLR, 2019.   \n[20] Javier Garc\u0131a and Fernando Fern\u00e1ndez. A comprehensive survey on safe reinforcement learning. Journal of Machine Learning Research, 16(1):1437\u20131480, 2015.   \n[21] Divyansh Garg, Joey Hejna, Matthieu Geist, and Stefano Ermon. Extreme q-learning: Maxent RL without entropy. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id $\\equiv$ SJ0Lde3tRL.   \n[22] Sinong Geng, Aldo Pacchiano, Andrey Kolobov, and Ching-An Cheng. Improving offline RL by blending heuristics. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=MCl0TLboP1.   \n[23] Seyed Kamyar Seyed Ghasemipour, Dale Schuurmans, and Shixiang Shane Gu. Emaq: Expected-max q-learning operator for simple yet effective offilne and online rl. In International Conference on Machine Learning, pages 3682\u20133691. PMLR, 2021.   \n[24] Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael J Cree. Regularisation of neural networks by enforcing lipschitz continuity. Machine Learning, 110:393\u2013416, 2021.   \n[25] Sven Gronauer and Klaus Diepold. Multi-agent deep reinforcement learning: a survey. Artificial Intelligence Review, 55(2):895\u2013943, 2022.   \n[26] Shangding Gu, Long Yang, Yali Du, Guang Chen, Florian Walter, Jun Wang, and Alois Knoll. A review of safe reinforcement learning: Methods, theory and applications. arXiv preprint arXiv:2205.10330, 2022.   \n[27] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pages 1861\u20131870. PMLR, 2018.   \n[28] Ehsan Imani, Eric Graves, and Martha White. An off-policy policy gradient theorem using emphatic weightings. Advances in neural information processing systems, 31, 2018.   \n[29] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy optimization. Advances in neural information processing systems, 32, 2019.   \n[30] Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard. Way off-policy batch deep reinforcement learning of implicit human preferences in dialog. arXiv preprint arXiv:1907.00456, 2019.   \n[31] Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a freely accessible critical care database. Scientific data, 3(1):1\u20139, 2016.   \n[32] Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Modelbased reinforcement learning for atari. arXiv preprint arXiv:1903.00374, 2019.   \n[33] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-based offilne reinforcement learning. Advances in neural information processing systems, 33:21810\u201321823, 2020.   \n[34] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[35] Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The International Journal of Robotics Research, 32(11):1238\u20131274, 2013.   \n[36] Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. Offline reinforcement learning with fisher divergence critic regularization. In International Conference on Machine Learning, pages 5774\u20135783. PMLR, 2021.   \n[37] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning. In International Conference on Learning Representations, 2022. URL https: //openreview.net/forum?id $\\equiv$ 68n2s9ZJWF8.   \n[38] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing offpolicy q-learning via bootstrapping error reduction. Advances in Neural Information Processing Systems, 32, 2019.   \n[39] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. Advances in Neural Information Processing Systems, 33: 1179\u20131191, 2020.   \n[40] Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. Reinforcement learning: State-of-the-art, pages 45\u201373, 2012.   \n[41] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436\u2013444, 2015.   \n[42] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.   \n[43] Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. Advances in neural information processing systems, 30, 2017.   \n[44] Jiafei Lyu, Xiaoteng Ma, Xiu Li, and Zongqing Lu. Mildly conservative q-learning for offline reinforcement learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https:// openreview.net/forum?id=VYYf6S67pQc.   \n[45] Xiaoteng Ma, Yiqin Yang, Hao Hu, Qihan Liu, Jun Yang, Chongjie Zhang, Qianchuan Zhao, and Bin Liang. Offline reinforcement learning with value-based episodic memory. arXiv preprint arXiv:2110.09796, 2021.   \n[46] Yecheng Ma, Dinesh Jayaraman, and Osbert Bastani. Conservative offline distributional reinforcement learning. Advances in Neural Information Processing Systems, 34:19235\u201319247, 2021.   \n[47] Yi Ma, Hongyao Tang, Dong Li, and Zhaopeng Meng. Reining generalization in offline reinforcement learning via representation distinction. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id= mVywRIDNIl.   \n[48] Will Maddern, Geoffrey Pascoe, Chris Linegar, and Paul Newman. 1 year, $1000\\,\\mathrm{km}$ : The oxford robotcar dataset. The International Journal of Robotics Research, 36(1):3\u201315, 2017.   \n[49] Yixiu Mao, Hongchang Zhang, Chen Chen, Yi Xu, and Xiangyang Ji. Supported trust region optimization for offline reinforcement learning. In International Conference on Machine Learning, pages 23829\u201323851. PMLR, 2023.   \n[50] Yixiu Mao, Qi Wang, Chen Chen, Yun Qu, and Xiangyang Ji. Offilne reinforcement learning with ood state correction and ood action suppression. arXiv preprint arXiv:2410.19400, 2024.   \n[51] Yixiu Mao, Hongchang Zhang, Chen Chen, Yi Xu, and Xiangyang Ji. Supported value regularization for offline reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024.   \n[52] Tatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Ofir Nachum, and Shixiang Gu. Deployment-efficient reinforcement learning via model-based offline optimization. In International Conference on Learning Representations, 2021. URL https://openreview. net/forum?id $\\equiv$ 3hGNqpI4WS.   \n[53] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529\u2013533, 2015.   \n[54] Thomas M Moerland, Joost Broekens, Aske Plaat, Catholijn M Jonker, et al. Model-based reinforcement learning: A survey. Foundations and Trends\u00ae in Machine Learning, 16(1):1\u2013118, 2023.   \n[55] Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating online reinforcement learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020.   \n[56] Andrew Patterson, Samuel Neumann, Martha White, and Adam White. Empirical design in reinforcement learning. arXiv preprint arXiv:2304.01315, 2023.   \n[57] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.   \n[58] Dean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. Advances in neural information processing systems, 1, 1988.   \n[59] Yun Qu, Boyuan Wang, Jianzhun Shao, Yuhang Jiang, Chen Chen, Zhenbin Ye, Linc Liu, Junfeng Yang, Lin Lai, Hongyang Qin, et al. Hokoff: Real game dataset from honor of kings and its offline reinforcement learning benchmarks. In Thirty-seventh Conference on Neural Information Processing Systems Track on Datasets and Benchmarks, 2023.   \n[60] Yun Qu, Boyuan Wang, Yuhang Jiang, Jianzhun Shao, Yixiu Mao, Cheems Wang, Chang Liu, and Xiangyang Ji. Choices are more important than efforts: Llm enables efficient multi-agent exploration. arXiv preprint arXiv:2410.02511, 2024.   \n[61] Yuhang Ran, Yi-Chen Li, Fuxiang Zhang, Zongzhang Zhang, and Yang Yu. Policy regularization with dataset constraint for offline reinforcement learning. In International Conference on Machine Learning, pages 28701\u201328717. PMLR, 2023.   \n[62] Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Monotonic value function factorisation for deep multi-agent reinforcement learning. Journal of Machine Learning Research, 21(178):1\u201351, 2020.   \n[63] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604\u2013609, 2020.   \n[64] Jianzhun Shao, Yun Qu, Chen Chen, Hongchang Zhang, and Xiangyang Ji. Counterfactual conservative q learning for offline multi-agent reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/ forum?id $\\equiv$ 62zmO4mv8X.   \n[65] Jianzhun Shao, Hongchang Zhang, Yun Qu, Chang Liu, Shuncheng He, Yuhang Jiang, and Xiangyang Ji. Complementary attention for multi-agent reinforcement learning. In International Conference on Machine Learning, pages 30776\u201330793. PMLR, 2023.   \n[66] Noah Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Neunert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin Riedmiller. Keep doing what worked: Behavior modelling priors for offline reinforcement learning. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum? id=rke7geHtwH.   \n[67] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. nature, 550(7676):354\u2013359, 2017.   \n[68] Yihao Sun, Jiaji Zhang, Chengxing Jia, Haoxin Lin, Junyin Ye, and Yang Yu. Model-bellman inconsistency for model-based offline reinforcement learning. In International Conference on Machine Learning, pages 33177\u201333194. PMLR, 2023.   \n[69] Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM Sigart Bulletin, 2(4):160\u2013163, 1991.   \n[70] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.   \n[71] Richard S Sutton, A Rupam Mahmood, and Martha White. An emphatic approach to the problem of off-policy temporal-difference learning. Journal of Machine Learning Research, 17 (73):1\u201329, 2016.   \n[72] Denis Tarasov, Vladislav Kurenkov, Alexander Nikulin, and Sergey Kolesnikov. Revisiting the minimalist approach to offline reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024.   \n[73] Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. In Proceedings of the AAAI conference on artificial intelligence, volume 30, 2016.   \n[74] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha\u00ebl Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350\u2013354, 2019.   \n[75] Cheems Wang, Yiqin Lv, Yixiu Mao, Yun Qu, Yi Xu, and Xiangyang Ji. Robust fast adaptation from adversarially explicit task distribution generation. arXiv preprint arXiv:2407.19523, 2024.   \n[76] Qi Wang and Herke Van Hoof. Model-based meta reinforcement learning using graph structured surrogate models and amortized policy search. In International Conference on Machine Learning, pages 23055\u201323077. PMLR, 2022.   \n[77] Qi Wang, Yiqin Lv, Zheng Xie, Jincai Huang, et al. A simple yet effective strategy to robustify the meta learning paradigm. Advances in Neural Information Processing Systems, 36, 2024.   \n[78] Qing Wang, Jiechao Xiong, Lei Han, Han Liu, Tong Zhang, et al. Exponentially weighted imitation learning for batched historical data. Advances in Neural Information Processing Systems, 31, 2018.   \n[79] Zhendong Wang, Jonathan J Hunt, and Mingyuan Zhou. Diffusion policies as an expressive policy class for offline reinforcement learning. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=AHvFDPi-FA.   \n[80] Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh S Merel, Jost Tobias Springenberg, Scott E Reed, Bobak Shahriari, Noah Siegel, Caglar Gulcehre, Nicolas Heess, et al. Critic regularized regression. Advances in Neural Information Processing Systems, 33:7768\u20137778, 2020.   \n[81] Albert Wilcox, Ashwin Balakrishna, Jules Dedieu, Wyame Benslimane, Daniel Brown, and Ken Goldberg. Monte carlo augmented actor-critic for sparse reward deep reinforcement learning from suboptimal demonstrations. Advances in Neural Information Processing Systems, 35: 2254\u20132267, 2022.   \n[82] Robert Wright, Steven Loscalzo, Philip Dexter, and Lei Yu. Exploiting multi-step sample trajectories for approximate value iteration. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2013, Prague, Czech Republic, September 23-27, 2013, Proceedings, Part I 13, pages 113\u2013128. Springer, 2013.   \n[83] Jialong Wu, Haixu Wu, Zihan Qiu, Jianmin Wang, and Mingsheng Long. Supported policy optimization for offline reinforcement learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id $\\equiv$ KCXQ5HoM-fy.   \n[84] Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning. arXiv preprint arXiv:1911.11361, 2019.   \n[85] Chenjun Xiao, Han Wang, Yangchen Pan, Adam White, and Martha White. The in-sample softmax for offilne reinforcement learning. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=u-RuvyDYqCM.   \n[86] Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellmanconsistent pessimism for offline reinforcement learning. Advances in neural information processing systems, 34:6683\u20136694, 2021.   \n[87] Huaqing Xiong, Tengyu Xu, Lin Zhao, Yingbin Liang, and Wei Zhang. Deterministic policy gradient: Convergence analysis. In Uncertainty in Artificial Intelligence, pages 2159\u20132169. PMLR, 2022.   \n[88] Haoran Xu, Li Jiang, Jianxiong Li, Zhuoran Yang, Zhaoran Wang, Victor Wai Kin Chan, and Xianyuan Zhan. Offline RL with no OOD actions: In-sample learning via implicit value regularization. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=ueYYgo2pSSU.   \n[89] Rui Yang, Chenjia Bai, Xiaoteng Ma, Zhaoran Wang, Chongjie Zhang, and Lei Han. RORL: Robust offline reinforcement learning via conservative smoothing. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=_QzJJGH_KE.   \n[90] Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. Mopo: Model-based offline policy optimization. Advances in Neural Information Processing Systems, 33:14129\u201314142, 2020.   \n[91] Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn. Combo: Conservative offline model-based policy optimization. Advances in neural information processing systems, 34:28954\u201328967, 2021.   \n[92] Hongchang Zhang, Yixiu Mao, Boyuan Wang, Shuncheng He, Yi Xu, and Xiangyang Ji. In-sample actor critic for offline reinforcement learning. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id= dfDv0WU853R.   \n[93] Wenxuan Zhou, Sujay Bajracharya, and David Held. Plas: Latent action space for offline reinforcement learning. In Conference on Robot Learning, pages 1719\u20131735. PMLR, 2021. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Extended Related Work ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Model-free offline RL. In offline RL, a fixed dataset is provided and no further interactions are allowed [40, 42]. As a result, conventional off-policy RL algorithms suffer from the extrapolation error due to OOD actions and exhibit poor performance [19]. To address this challenge, various offline RL algorithms have been developed, primarily categorized into model-free and model-based approaches. In model-free solutions, value regularization methods introduce conservatism in value estimation through direct penalization [39, 36, 46, 86, 11, 64, 51], or via value ensembles [2, 3, 89]. Policy constraint approaches enforce proximity between the trained policy and the behavior policy, either explicitly via divergence penalties [84, 38, 30, 17, 83], implicitly by weighted behavior cloning [9, 57, 55, 80, 49], or directly through specific parameterization of the policy [19, 23, 93]. Some recent efforts focus on learning the optimal policy within the dataset\u2019s support (known as insupport or in-sample optimal policy) in a theoretically sound manner [49, 51, 83]. These approaches are less influenced by the the dataset\u2019s average quality. Another popular branch of algorithms opts for in-sample learning, which formulates the Bellman target without querying the values of any unseen actions [7, 45, 37, 85, 92, 88, 21]. Among these, OneStep RL [7] evaluates the behavior policy via SARSA [70] and performs only one step of constrained policy improvement without off-policy evaluation. IQL [37] modifies the SARSA update, using expectile regression to approximate an upper expectile of the value distribution and enables multi-step dynamic programming. Following IQL, several recent works such as InAC [85], IAC [92], $\\mathcal{X}\\mathrm{QL}$ [21], and SQL [88] have developed different in-sample learning frameworks, further enhancing the performance of in-sample learning approaches. However, this work shows that the performance of in-sample approaches is confined by eschewing generalization beyond the offline dataset. In contrast, the proposed approach DMG utilizes doubly mild generalization to appropriately exploit generalization and achieves significantly stronger performance across tasks. ", "page_idx": 15}, {"type": "text", "text": "Model-based offline RL. Model-based offline RL methods involve training an environmental dynamics model, from which synthetic data is generated to facilitate policy optimization [69, 29, 32]. In the context of offline RL, algorithms such as MOPO [90] and MOReL [33] propose to estimate the uncertainty within the trained model and subsequently impose penalties or constraints on stateaction pairs characterized by high uncertainty levels, thus achieving conservatism in the learning process. Some model-based approaches incorporate conservatism in a similar way to those modelfree ones. For example, COMBO [91] leverages value penalization, while BREMEN [52] employs behavior regularization. More recently, MOBILE [68] introduces uncertainty quantification via the inconsistency of Bellman estimations within a learned dynamics ensemble. SCAS [50] proposes a generic model-based regularizer that unifies OOD state correction and OOD action suppression in offilne RL. However, typical model-based methods often involve heavy computational overhead [29], and their effectiveness hinges on the accuracy of the trained dynamics model [54]. ", "page_idx": 15}, {"type": "text", "text": "Recently, Bose et al. [5] explores multi-task offilne RL from the perspective of representation learning and introduced a notion of neighborhood occupancy density. The neighborhood occupancy density at a given stata-action pair in the dataset for a source task is defined as the fraction of points in the dataset within a certain distance from that stata-action pair in the representation space. Bose et al. [5] use this concept to bound the representational transfer error in the downstream target task. In contrast, DMG is a wildly compatible idea in offline RL and provides insights into many offline RL methods. DMG balances the need for generalization with the risk of over-generalization in offline RL. Generalization to stata-action pairs in the neighborhood of the dataset corresponds to mild action generalization in the DMG framework. ", "page_idx": 15}, {"type": "text", "text": "B Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we provide the proofs of all the theories in the paper. ", "page_idx": 15}, {"type": "text", "text": "B.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "This section presents the formal theorem for the Theorem 1 in the main paper, along with its proof.   \nWe first make several common continuity assumptions for Theorem 1. ", "page_idx": 15}, {"type": "text", "text": "Assumption 4 (Lipschitz $Q$ ). The learned value function $Q_{\\theta}$ is $K_{Q}$ -Lipschitz and is upper bounded by $Q_{\\mathrm{max}}$ . $\\forall s\\sim\\mathcal{D}$ , $\\forall a_{1},a_{2}\\sim A_{!}$ , $|Q_{\\theta}(s,a_{1})-Q_{\\theta}(s,a_{2})|\\leq K_{Q}\\|\\dot{a}_{1}-a_{2}\\|$ . ", "page_idx": 16}, {"type": "text", "text": "Assumption 5 (Lipschitz $Q$ gradient). The learned value function $Q_{\\theta}$ is smooth, i.e, has a $K_{g}$ - Lipschitz continuous gradient. $\\forall s\\sim\\mathcal{D}$ , $\\forall a_{1},a_{2}\\sim A$ , $\\begin{array}{r}{\\|\\nabla_{\\theta}Q_{\\theta}(s,a_{1})-\\nabla_{\\theta}Q_{\\theta}(s,a_{2})\\|\\leq K_{g}\\|a_{1}-}\\end{array}$ $a_{2}\\|$ . ", "page_idx": 16}, {"type": "text", "text": "Assumption 6 (Bounded $Q$ and $Q$ gradient). $\\forall s,a,\\,|Q_{\\theta}(s,a)|\\leq Q_{\\operatorname*{max}}$ and $\\|\\nabla_{\\theta}Q_{\\theta}(s,a)\\|\\leq g_{\\operatorname*{max}}.$ . ", "page_idx": 16}, {"type": "text", "text": "Assumption 7 (Lipschitz $P$ ). The transition dynamics $P$ is $K_{P}$ -Lipschitz. $\\forall s,s^{\\prime}\\sim{\\cal S},\\,\\forall a_{1},a_{2}\\sim{\\cal A},$ , $|P(s^{\\prime}|s,a_{1})-P(\\bar{s}^{\\prime}|s,a_{2})|\\leq K_{P}\\|a_{1}-a_{2}\\|$ . ", "page_idx": 16}, {"type": "text", "text": "Assumption 8 (Lipschitz $R$ ). The reward function $R$ is $K_{R}$ -Lipschitz. $\\forall s\\;\\sim\\;{\\cal S},\\;\\forall a_{1},a_{2}\\:\\sim\\:{\\cal A},$ , $|R(s|a_{1}^{-})-R(s,a_{2}^{-})|\\leq K_{R}\\|a_{1}-a_{2}\\|$ . ", "page_idx": 16}, {"type": "text", "text": "A continuous learned Q function is particularly necessary for the analysis of value function generalization. Since we often use neural networks or linear models to parameterize the value function $Q_{\\theta}$ , Assumptions 4 and 5 can be relatively easily satisfied [24]. Assumptions 6, 7, and 8 are also common in the theoretical studies of RL [13, 87, 61] and optimization [6]. ", "page_idx": 16}, {"type": "text", "text": "Before we start the proof of Theorem 1, we prove the following lemma. ", "page_idx": 16}, {"type": "text", "text": "Lemma 2. $\\forall s\\sim\\mathcal{D}$ , $\\forall a_{1},a_{2}\\sim A,\\,|T_{u}Q_{\\theta}(s,a_{1})-T_{u}Q_{\\theta}(s,a_{2})|\\leq K_{T}\\|a_{1}-a_{2}\\|$ . where $K\\tau$ is $a$ positive bounded constant. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P v o r_{f}\\nabla s\\sim P\\nabla u_{1},\\,a_{2}\\sim A,}\\\\ &{\\left|\\mathcal{T}_{\\mathcal{R}}Q_{\\theta}(s,a_{1})-\\mathcal{T}_{\\mathcal{R}}Q(s,a_{2})\\right|}\\\\ &{=\\left|R(s,a_{1})-R(s,a_{2})+\\gamma\\mathbb{E}_{s^{\\sim}\\sim P(\\cdot|s,a_{1})}\\left[\\underset{\\omega^{\\star\\star}\\sim\\omega(\\cdot|s^{\\star})}{\\operatorname*{max}}Q(s^{\\prime},a^{\\prime})\\right]-\\gamma\\mathbb{E}_{s^{\\sim}P(\\cdot|s,a_{2})}\\left[\\underset{\\omega^{\\star\\star}\\sim\\omega(\\cdot|s^{\\star})}{\\operatorname*{max}}Q(s^{\\prime},a^{\\prime})\\right]\\right|}\\\\ &{\\leq\\left|R(s,a_{1})-R(s,a_{2})\\right|+\\gamma\\left|\\mathbb{E}_{s^{\\sim}P(\\cdot|s,a_{1})}\\left[\\underset{\\omega^{\\star\\star}\\sim\\omega(\\cdot|s^{\\star})}{\\operatorname*{max}}Q(s^{\\prime},a^{\\prime})\\right]-\\mathbb{E}_{s^{\\sim}P(\\cdot|s,a_{2})}\\left[\\underset{\\omega^{\\star\\star}\\sim\\omega(\\cdot|s^{\\star})}{\\operatorname*{max}}Q(s^{\\prime},a^{\\prime})\\right]}\\\\ &{=\\left|R(s,a_{1})-R(s,a_{2})\\right|+\\gamma\\left|\\sum_{\\nu}^{*}\\left(P(s^{\\prime}|s,a_{1})-P(s^{\\prime}|s,a_{2})\\right)\\underset{\\omega^{\\star\\star}\\sim\\omega(\\cdot|s^{\\star})}{\\operatorname*{max}}Q(s^{\\prime},a^{\\prime})\\right|}\\\\ &{\\leq\\left|R(s,a_{1})-R(s,a_{2})\\right|+\\gamma\\sum_{\\nu}^{*}\\left|(P(s^{\\prime}|s,a_{1})-P(s^{\\prime}|s,a_{2}))\\right|\\underset{\\omega^{\\star\\star}\\sim\\omega(\\cdot|s^{\\star})}{\\operatorname*{max}}Q(s^{\\prime},a^{\\prime})\\right|}\\\\ &{\\leq K_{\\mathcal{R}}\\|a_{1}-a_{2}\\|+\\gamma\\sum_{\\nu}^{*}P\\|a_{1}-a_{2}\\|Q_{\\operatorname*{max}}}\\\\ &{=(K_{\\mathcal{R}}+\\gamma K_{\\nu}|s)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the last inequality holds by Assumptions 6, 7, and 8. ", "page_idx": 16}, {"type": "text", "text": "Therefore, for any $s\\sim\\mathcal{D}$ , $a_{1},a_{2}\\sim A$ , it holds that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{|\\mathcal{T}_{u}Q_{\\theta}(s,a_{1})-\\mathcal{T}_{u}Q_{\\theta}(s,a_{2})|\\leq K_{T}\\|a_{1}-a_{2}\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $K_{\\mathcal{T}}:=K_{R}+\\gamma K_{P}|S|Q_{\\operatorname*{max}}$ is a positive bounded constant. ", "page_idx": 16}, {"type": "text", "text": "We restate the scenario analyzed in Theorem 1: $Q_{\\theta}$ is updated to $Q_{\\theta^{\\prime}}$ by one gradient step on a single state-action pair $(s,a)\\in\\mathcal{D}$ , which affects the Q-value of an arbitrary state-action pair $(s,\\tilde{a})\\notin\\overline{{D}}$ . The parameter update is ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\theta^{\\prime}=\\theta+\\alpha(\\mathcal{T}_{u}Q_{\\theta}(s,a)-Q_{\\theta}(s,a))\\nabla_{\\theta}Q_{\\theta}(s,a)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $\\alpha$ is the learning rate. ", "page_idx": 16}, {"type": "text", "text": "Now we start the proof of Theorem 1 in the main paper. ", "page_idx": 16}, {"type": "text", "text": "Theorem 6 (Theorem 1). Under Assumptions 4 to 8, the following equation holds when the learning rate $\\alpha$ is sufficiently small and $\\tilde{a}$ is sufficiently close to $a$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\nQ_{\\theta^{\\prime}}(s,\\tilde{a})=Q_{\\theta}(s,\\tilde{a})+C_{1}\\left(\\mathcal T_{u}Q_{\\theta}(s,\\tilde{a})-Q_{\\theta}(s,\\tilde{a})+C_{2}\\|\\tilde{a}-a\\|\\right)+\\mathcal O\\left(\\|\\theta^{\\prime}-\\theta\\|^{2}\\right)\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $C_{1}\\in[0,1]$ and $C_{2}\\in[-K_{Q}-K_{R}-\\gamma K_{P}|S|Q_{\\operatorname*{max}},K_{Q}+K_{R}+\\gamma K_{P}|S|Q_{\\operatorname*{max}}].$ . ", "page_idx": 16}, {"type": "text", "text": "Proof. We formalize $Q_{\\theta^{\\prime}}(s,\\tilde{a})$ by Taylor expansion at the parameter $\\theta$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\nQ_{\\theta^{\\prime}}(s,\\tilde{a})=Q_{\\theta}(s,\\tilde{a})+\\nabla_{\\theta}Q_{\\theta}(s,\\tilde{a})^{\\top}(\\theta^{\\prime}-\\theta)+\\mathcal{O}\\left(\\|\\theta^{\\prime}-\\theta\\|^{2}\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By plugging Eq. (18) into Eq. (20), we have ", "page_idx": 17}, {"type": "equation", "text": "$$\nQ_{\\theta^{\\prime}}(s,\\tilde{a})=Q_{\\theta}(s,\\tilde{a})+\\alpha\\nabla_{\\theta}Q_{\\theta}(s,\\tilde{a})^{\\top}\\nabla_{\\theta}Q_{\\theta}(s,a)\\left(\\mathcal{T}_{u}Q_{\\theta}(s,a)-Q_{\\theta}(s,a)\\right)+\\mathcal{O}\\left(\\|\\theta^{\\prime}-\\theta\\|^{2}\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "According to Assumption 4 and Lemma 2, it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~~|Q_{\\theta}(s,\\tilde{a})-Q_{\\theta}(s,a)|\\leq K_{Q}\\|\\tilde{a}-a\\|}\\\\ &{|T_{u}Q_{\\theta}(s,\\tilde{a})-T_{u}Q_{\\theta}(s,a)|\\leq K_{T}\\|\\tilde{a}-a\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $K_{\\mathcal{T}}:=K_{R}+\\gamma K_{P}|S|Q_{\\operatorname*{max}}$ is a positive bounded constant. ", "page_idx": 17}, {"type": "text", "text": "Therefore, ", "text_level": 1, "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad|({\\mathcal{T}}_{u}Q_{\\theta}(s,\\tilde{a})-Q_{\\theta}(s,\\tilde{a}))-({\\mathcal{T}}_{u}Q_{\\theta}(s,a)-Q_{\\theta}(s,a))|}\\\\ &{=|({\\mathcal{T}}_{u}Q_{\\theta}(s,\\tilde{a})-{\\mathcal{T}}_{u}Q_{\\theta}(s,a))+(Q_{\\theta}(s,a)-Q_{\\theta}(s,\\tilde{a}))|}\\\\ &{\\le|({\\mathcal{T}}_{u}Q_{\\theta}(s,\\tilde{a})-{\\mathcal{T}}_{u}Q_{\\theta}(s,a))|+|(Q_{\\theta}(s,a)-Q_{\\theta}(s,\\tilde{a}))|}\\\\ &{\\le K_{T}\\|\\tilde{a}-a\\|+K_{Q}\\|\\tilde{a}-a\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "As a result, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{T}_{u}Q_{\\theta}(s,a)-Q_{\\theta}(s,a)\\leq\\mathcal{T}_{u}Q_{\\theta}(s,\\tilde{a})-Q_{\\theta}(s,\\tilde{a})+(K_{Q}+K_{T})\\|\\tilde{a}-a\\|}\\\\ &{\\mathcal{T}_{u}Q_{\\theta}(s,a)-Q_{\\theta}(s,a)\\geq\\mathcal{T}_{u}Q_{\\theta}(s,\\tilde{a})-Q_{\\theta}(s,\\tilde{a})-(K_{Q}+K_{T})\\|\\tilde{a}-a\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Thus we can let ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{T}_{u}Q_{\\theta}(s,a)-Q_{\\theta}(s,a)=\\mathcal{T}_{u}Q_{\\theta}(s,\\tilde{a})-Q_{\\theta}(s,\\tilde{a})+C_{2}\\|\\tilde{a}-a\\|,}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $C_{2}\\in[-K_{Q}-K_{T},K_{Q}+K_{T}]$ is a bounded constant. ", "page_idx": 17}, {"type": "text", "text": "Now we shift our focus to $\\alpha\\nabla_{\\theta}Q_{\\theta}(s,\\tilde{a})^{\\top}\\nabla_{\\theta}Q_{\\theta}(s,a)$ . Let $\\boldsymbol{v}=\\nabla_{\\boldsymbol{\\theta}}Q_{\\boldsymbol{\\theta}}(s,\\tilde{a}){-}\\nabla_{\\boldsymbol{\\theta}}Q_{\\boldsymbol{\\theta}}(s,a)$ . According to the smoothness of $Q_{\\theta}$ in Assumption 5, it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\|v\\|=\\|\\nabla_{\\theta}Q_{\\theta}(s,\\tilde{a})-\\nabla_{\\theta}Q_{\\theta}(s,a)\\|\\leq K_{g}\\|\\tilde{a}-a\\|.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\nabla_{\\theta}Q_{\\theta}(s,\\tilde{a})^{\\top}\\nabla_{\\theta}Q_{\\theta}(s,a)}\\\\ &{{=}(\\nabla_{\\theta}Q_{\\theta}(s,a)+v)^{\\top}\\nabla_{\\theta}Q_{\\theta}(s,a)}\\\\ &{{=}\\|\\nabla_{\\theta}Q_{\\theta}(s,a)\\|^{2}+v^{\\top}\\nabla_{\\theta}Q_{\\theta}(s,a)}\\\\ &{{\\geq}\\|\\nabla_{\\theta}Q_{\\theta}(s,a)\\|^{2}-\\|v\\|\\|\\nabla_{\\theta}Q_{\\theta}(s,a)\\|}\\\\ &{{\\geq}\\|\\nabla_{\\theta}Q_{\\theta}(s,a)\\|^{2}-K_{g}\\|\\tilde{a}-a\\|\\|\\nabla_{\\theta}Q_{\\theta}(s,a)\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, for sufficiently close $\\tilde{a}$ and $a$ such that $\\|\\tilde{a}\\mathrm{~-~}a\\|\\;\\le\\;\\|\\nabla_{\\theta}Q_{\\theta}(s,a)\\|/K_{g}$ , it holds that $\\alpha\\nabla_{\\boldsymbol{\\theta}}Q_{\\boldsymbol{\\theta}}(s,\\tilde{a})^{\\top}\\nabla_{\\boldsymbol{\\theta}}Q_{\\boldsymbol{\\theta}}(s,a)\\geq0$ . ", "page_idx": 17}, {"type": "text", "text": "On the other hand, because $\\|\\nabla_{\\theta}Q_{\\theta}\\|$ is bounded by $g_{\\mathrm{max}}$ according to Assumption 6, it holds that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\alpha\\nabla_{\\boldsymbol{\\theta}}Q_{\\boldsymbol{\\theta}}(s,\\tilde{a})^{\\top}\\nabla_{\\boldsymbol{\\theta}}Q_{\\boldsymbol{\\theta}}(s,a)\\leq\\alpha g_{\\mathrm{max}}^{2}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "By choosing a small learning rate $\\alpha$ such that $\\alpha\\leq1/g_{\\operatorname*{max}}^{2}$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\alpha\\nabla_{\\boldsymbol{\\theta}}Q_{\\boldsymbol{\\theta}}(s,\\tilde{a})^{\\top}\\nabla_{\\boldsymbol{\\theta}}Q_{\\boldsymbol{\\theta}}(s,a)\\leq1\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In such cases (sufficiently close $\\tilde{a}$ and $a$ , and sufficiently small $\\alpha$ ), let ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{C_{1}:=\\alpha\\nabla_{\\theta}Q_{\\theta}(s,\\tilde{a})^{\\top}\\nabla_{\\theta}Q_{\\theta}(s,a)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We have $C_{1}\\in[0,1]$ ", "page_idx": 17}, {"type": "text", "text": "By plugging Equations (24) and (26) into Equation (21), the following equation holds. ", "page_idx": 17}, {"type": "equation", "text": "$$\nQ_{\\theta^{\\prime}}(s,\\tilde{a})=Q_{\\theta}(s,\\tilde{a})+C_{1}\\left(\\mathcal T_{u}Q_{\\theta}(s,\\tilde{a})-Q_{\\theta}(s,\\tilde{a})+C_{2}\\|\\tilde{a}-a\\|\\right)+\\mathcal O\\left(\\|\\theta^{\\prime}-\\theta\\|^{2}\\right)\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 17}, {"type": "text", "text": "B.2 Proofs under Oracle Generalization ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "We first restate the several definitions in the main paper. ", "page_idx": 18}, {"type": "text", "text": "Definition 4 (Mildly generalized policy, Definition 1). Policy $\\tilde{\\beta}$ is termed a mildly generalized policy if it satisfies ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname{supp}(\\hat{\\beta}(\\cdot|s))\\subseteq\\operatorname{supp}(\\tilde{\\beta}(\\cdot|s)),\\;\\;a n d\\quad\\operatorname*{max}_{a_{1}\\sim\\tilde{\\beta}(\\cdot|s)}\\operatorname*{min}_{a_{2}\\sim\\tilde{\\beta}(\\cdot|s)}\\|a_{1}-a_{2}\\|\\leq\\epsilon_{a},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\hat{\\beta}$ is the empirical behavior policy in the offline dataset. ", "page_idx": 18}, {"type": "text", "text": "Definition 5 (Definition 2). The Doubly Mildly Generalization (DMG) operator is defined as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{T}_{\\mathrm{DMG}}Q(s,a):=R(s,a)+\\gamma\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot|s,a)}\\left[\\lambda\\operatorname*{max}_{a^{\\prime}\\sim\\tilde{\\beta}(\\cdot|s^{\\prime})}Q(s^{\\prime},a^{\\prime})+(1-\\lambda)\\operatorname*{max}_{a^{\\prime}\\sim\\tilde{\\beta}(\\cdot|s^{\\prime})}Q(s^{\\prime},a^{\\prime})\\right].\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\hat{\\beta}$ is the empirical behavior policy in the dataset and $\\tilde{\\beta}$ is a mildly generalized policy. ", "page_idx": 18}, {"type": "text", "text": "Definition 6 (Definition 3). The In-sample $Q$ Learning operator $I37J$ is defined as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{T}_{\\mathrm{In}}Q(s,a):=R(s,a)+\\gamma\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot|s,a)}\\left[\\operatorname*{max}_{a^{\\prime}\\sim\\hat{\\beta}(\\cdot|s^{\\prime})}Q(s^{\\prime},a^{\\prime})\\right]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\hat{\\beta}$ is the empirical behavior policy in the dataset. ", "page_idx": 18}, {"type": "text", "text": "In this subsection, we assume that the learned value function can make oracle generalization in the mild generalization area $\\tilde{\\beta}(a|s)>0$ , which is formally defined as follows. ", "page_idx": 18}, {"type": "text", "text": "Assumption 9 (Oracle generalization, Assumption 1). The generalization of learned $Q$ functions in the mild generalization area $\\tilde{\\beta}(a|s)>0$ reflects the true value updates according to ${\\mathcal{T}}_{\\mathrm{DMG}}$ . In other words, $\\mathcal{T}_{\\mathrm{DMG}}$ is well defined in the mild generalization area ${\\tilde{\\beta}}(a|s)>0$ . ", "page_idx": 18}, {"type": "text", "text": "This assumption can be considered reasonable according to the results presented in Theorem 6 above.   \nIn such cases, we can analyze the dynamic programming properties of operators $\\mathcal{T}_{\\mathrm{In}}$ and $\\mathcal{T}_{\\mathrm{DMG}}$ . ", "page_idx": 18}, {"type": "text", "text": "Before we start the proofs of Lemma 1 and Theorem 2 in the main paper, we prove a lemma. ", "page_idx": 18}, {"type": "text", "text": "Lemma 3. For any function $f_{1},\\;f_{2}$ , any variant $x\\in\\mathscr{X}$ , the following inequality holds: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left|\\operatorname*{max}_{x\\in{\\mathcal{X}}}f_{1}(x)-\\operatorname*{max}_{x\\in{\\mathcal{X}}}f_{2}(x)\\right|\\leq\\operatorname*{max}_{x\\in{\\mathcal{X}}}|f_{1}(x)-f_{2}(x)|\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Proof. Define $x_{1}:=\\operatorname{argmax}_{x\\in\\mathcal{X}}f_{1}(x)$ and $x_{2}:=\\operatorname{argmax}_{x\\in\\mathcal{X}}f_{2}(x)$ . ", "page_idx": 18}, {"type": "text", "text": "According to the definition, the following inequality holds: ", "page_idx": 18}, {"type": "equation", "text": "$$\nf_{1}(x_{2})-f_{2}(x_{2})\\leq f_{1}(x_{1})-f_{2}(x_{2})\\leq f_{1}(x_{1})-f_{2}(x_{1})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Therefore, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~\\left|\\underset{x\\in\\mathcal{X}}{\\mathrm{max}}\\,f_{1}(x)-\\underset{x\\in\\mathcal{X}}{\\mathrm{max}}\\,f_{2}(x)\\right|}\\\\ &{=\\left|f_{1}(x_{1})-f_{2}(x_{2})\\right|}\\\\ &{\\leq\\operatorname*{max}\\left\\{|f_{1}(x_{2})-f_{2}(x_{2})|\\,,|f_{1}(x_{1})-f_{2}(x_{1})|\\right\\}}\\\\ &{\\leq\\underset{x\\in\\mathcal{X}}{\\mathrm{max}}\\,|f_{1}(x)-f_{2}(x)|}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This concludes the proof of Lemma 3. ", "page_idx": 18}, {"type": "text", "text": "Lemma 4 (Lemma 1). $\\mathcal{T}_{\\mathrm{In}}$ is a $\\gamma$ -contraction operator in the in-sample area $\\hat{\\beta}(a|s)>0$ under the $\\mathcal{L}_{\\infty}$ norm. ", "page_idx": 18}, {"type": "text", "text": "Proof. Let $f_{1}$ and $f_{2}$ be two arbitrary functions. ", "page_idx": 19}, {"type": "text", "text": "For all $(s,a)$ s.t. $\\hat{\\beta}(a|s)>0$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad|T_{\\ln}f_{1}(s,a)-T_{\\ln}f_{2}(s,a)|}\\\\ &{=\\left|R(s,a)+\\gamma\\mathbb{E}_{s^{\\geq}\\sim P(\\cdot|s,a)}\\left[\\underset{a^{\\prime}\\sim\\beta(\\cdot,\\cdot,s^{\\prime})}{\\operatorname*{max}}~f_{1}(s^{\\prime},a^{\\prime})\\right]-R(s,a)-\\gamma\\mathbb{E}_{s^{\\geq}\\sim P(\\cdot|s,a)}\\left[\\underset{a^{\\prime}\\sim\\beta(\\cdot,\\cdot,s^{\\prime})}{\\operatorname*{max}}\\int_{a}(s^{\\prime},a^{\\prime})\\right]\\right.}\\\\ &{=\\gamma\\left|\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot|s,a)}\\left[\\underset{a^{\\prime}\\sim\\beta(\\cdot,\\cdot,s^{\\prime})}{\\operatorname*{max}}f_{1}(s^{\\prime},a^{\\prime})-\\underset{a^{\\prime}\\sim\\beta(\\cdot,\\cdot,s^{\\prime})}{\\operatorname*{max}}~f_{2}(s^{\\prime},a^{\\prime})\\right]\\right|}\\\\ &{\\leq\\gamma\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot|s,a)}\\left[\\underset{a^{\\prime}\\sim\\beta(\\cdot,\\cdot,s^{\\prime})}{\\operatorname*{max}}f_{1}(s^{\\prime},a^{\\prime})-\\underset{a^{\\prime}\\sim\\beta(\\cdot,\\cdot,s^{\\prime})}{\\operatorname*{max}}f_{2}(s^{\\prime},a^{\\prime})\\right]\\right|}\\\\ &{\\leq\\gamma\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot|s,a)}\\left[\\underset{a^{\\prime}\\sim\\beta(\\cdot,\\cdot,s^{\\prime})}{\\operatorname*{max}}~f_{1}(s^{\\prime},a^{\\prime})-f_{2}(s^{\\prime},a^{\\prime})\\right]}\\\\ &{\\leq\\gamma\\mathbb{E}_{s^{\\geq}\\sim P(\\cdot|s,a)}\\left[\\underset{a^{\\prime}\\sim\\beta(\\cdot,\\cdot,s^{\\prime})}{\\operatorname*{max}}~|f_{1}(s^{\\prime},a^{\\prime})-f_{2}(s^{\\prime},a^{\\prime})\\right]}\\\\ &{\\leq\\gamma\\sum_{\\scriptstyle(\\ a,a)\\lambda\\lambda\\geq0}\\left[f_{1}(s,a)-f_{2}(s,a)\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where the second inequality holds by Lemma 3. ", "page_idx": 19}, {"type": "text", "text": "Therefore, in the in-sample area $\\tilde{\\beta}(a|s)>0$ , $\\mathcal{T}_{\\mathrm{In}}$ is a $\\gamma$ -contraction operator under the $\\mathcal{L}_{\\infty}$ norm. This concludes the proof for $\\mathcal{T}_{\\mathrm{In}}$ . \u53e3 ", "page_idx": 19}, {"type": "text", "text": "Thus, by repeatedly applying $\\mathcal{T}_{\\mathrm{In}}$ , any initial Q function can converge to the unique fixed point $Q_{\\mathrm{In}}^{*}$ . We denote its induced policy by \u03c0I\u2217n: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{Q_{\\mathrm{In}}^{*}(s,a)=R(s,a)+\\gamma\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot|s,a)}\\left[\\underset{a^{\\prime}\\sim\\hat{\\beta}(\\cdot|s^{\\prime})}{\\operatorname*{max}}Q_{\\mathrm{In}}^{*}(s^{\\prime},a^{\\prime})\\right],\\quad\\hat{\\beta}(a|s)>0,}\\\\ &{\\qquad\\qquad\\qquad\\quad\\pi_{\\mathrm{In}}^{*}(s):=\\underset{a\\sim\\hat{\\beta}(\\cdot|s)}{\\operatorname{argmax}}Q_{\\mathrm{In}}^{*}(s,a).}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Here, $Q_{\\mathrm{In}}^{*}$ is known as the in-sample optimal value function [38, 37], which is the value function of the in-sample optimal policy $\\pi_{\\mathrm{In}}^{*}$ . We refer readers to [83, 37, 49, 51] for more discussions on the in-sample or in-support optimality. ", "page_idx": 19}, {"type": "text", "text": "Now we start the proof of Theorem 2 in the main paper. ", "page_idx": 19}, {"type": "text", "text": "Theorem 7 (Contraction, Theorem 2). Under Assumption 9, TDMG is a $\\gamma$ -contraction operator in the mild generalization area $\\tilde{\\beta}(a|s)>0$ under the $\\mathcal{L}_{\\infty}$ norm. Therefore, by repeatedly applying ${\\mathcal{T}}_{\\mathrm{DMG}}$ , any initial $Q$ function can converge to the unique fixed point $Q_{\\mathrm{DMG}}^{*}$ . ", "page_idx": 19}, {"type": "text", "text": "Proof. By the oracle generalization assumption (Assumption 9), $\\mathcal{T}_{\\mathrm{DMG}}$ is well defined in the mild generalization area $\\tilde{\\beta}(a|s)>0$ . ", "page_idx": 19}, {"type": "text", "text": "Let $f_{1}$ and $f_{2}$ be two arbitrary functions. For all $(s,a)$ s.t. ${\\tilde{\\beta}}(a|s)>0$ , we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{T}_{\\mathrm{DMG}}f_{1}(s,a)-\\mathcal{T}_{\\mathrm{DMG}}f_{2}(s,a)}\\\\ &{=R(s,a)+\\gamma\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot\\vert s,a)}\\left[\\displaystyle\\sum_{\\alpha^{\\prime}\\sim\\beta(\\cdot\\vert s^{\\prime})}f_{1}(s^{\\prime},a^{\\prime})+(1-\\lambda)\\operatorname*{max}_{\\alpha^{\\prime}\\sim\\beta(\\cdot\\vert s^{\\prime})}f_{1}(s^{\\prime},a^{\\prime})\\right]}\\\\ &{\\quad-R(s,a)-\\gamma\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot\\vert s,a)}\\left[\\displaystyle\\sum_{\\lambda^{\\prime}\\sim\\beta(\\cdot\\vert s^{\\prime})}f_{2}(s^{\\prime},a^{\\prime})+(1-\\lambda)\\operatorname*{max}_{\\alpha^{\\prime}\\sim\\beta(\\cdot\\vert s^{\\prime})}f_{2}(s^{\\prime},a^{\\prime})\\right]}\\\\ &{=\\gamma\\lambda\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot\\vert s,a)}\\left[\\displaystyle\\operatorname*{max}_{\\alpha^{\\prime}\\sim\\beta(\\cdot\\vert s^{\\prime})}f_{1}(s^{\\prime},a^{\\prime})-\\displaystyle\\operatorname*{max}_{\\alpha^{\\prime}\\sim\\beta(\\cdot\\vert s^{\\prime})}f_{2}(s^{\\prime},a^{\\prime})\\right]}\\\\ &{\\quad+\\,\\gamma(1-\\lambda)\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot\\vert s,a)}\\left[\\displaystyle\\operatorname*{max}_{\\alpha^{\\prime}\\sim\\beta(\\cdot\\vert s^{\\prime})}f_{1}(s^{\\prime},a^{\\prime})-\\displaystyle\\operatorname*{max}_{\\alpha^{\\prime}\\sim\\beta(\\cdot\\vert s^{\\prime})}f_{2}(s^{\\prime},a^{\\prime})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Therefore, for all $(s,a)\\;s.t.\\;\\tilde{\\beta}(a|s)>0$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad[\\langle P_{k a}a\\rangle\\hat{L}_{2},\\langle P_{k b}a\\rangle-\\hat{L}_{2},\\langle P_{k b}a\\rangle]}\\\\ &{\\leq\\hat{\\mathcal{E}}[\\mathcal{R}(\\hat{\\mathcal{T}}_{i},\\hat{\\mathcal{H}}_{i})]\\Bigg[\\phantom{\\hat{\\mathcal{T}}_{i},\\hat{\\mathcal{H}}_{i}}\\bigg[\\phantom{\\mathcal{T}_{i},\\hat{\\mathcal{H}}_{i}}\\hat{L}_{i}(i\\alpha,\\hat{\\mathcal{T}}_{i})-\\phantom{\\frac{(\\hat{\\mathcal{H}}_{i},\\hat{\\mathcal{H}}_{i})}{2}}\\hat{L}_{i}(i\\alpha,\\hat{\\mathcal{H}}_{i})\\bigg]\\Bigg]}\\\\ &{\\quad+\\left[\\phantom{\\hat{\\mathcal{T}}_{i}}\\!\\!\\!\\!\\!+\\!\\!\\frac{1}{n}\\beta_{i}\\,\\overline{{\\kappa}}_{\\alpha}-\\hat{\\kappa}_{i}\\hat{L}_{i}(i\\alpha)\\right]\\Bigg[\\phantom{\\hat{\\mathcal{T}}_{i}}\\!\\!\\!\\!+\\!\\!\\frac{\\kappa_{i}}{n}\\beta_{i}-\\hat{L}_{i}(i\\alpha,\\hat{\\mathcal{H}}_{i})\\bigg]}\\\\ &{\\quad+\\left[\\phantom{\\hat{\\mathcal{T}}_{i}}\\!\\!\\!\\!+\\!\\!\\frac{1}{n}\\beta_{i}-\\hat{L}_{i}(i\\alpha,\\hat{\\mathcal{H}}_{i})\\right]\\Bigg[\\phantom{\\hat{\\mathcal{T}}_{i}}\\!\\!\\!\\!+\\!\\!\\hat{\\mathcal{E}}_{i}\\!\\!\\!\\!+\\!\\hat{\\mathcal{E}}_{i}\\!\\!\\!\\!+\\!\\hat{\\mathcal{E}}_{i}\\!\\!\\!\\!-\\!\\hat{L}_{i}(i\\alpha,\\hat{\\mathcal{H}}_{i})}\\\\ &{\\quad+\\gamma_{i}\\,\\overline{{\\kappa}}_{\\alpha}-\\hat{\\kappa}_{i}\\hat{L}_{i}(i\\alpha)\\bigg]\\Bigg[\\phantom{\\hat{\\mathcal{T}}_{i}}\\!\\!\\!\\!+\\!\\hat{\\mathcal{E}}_{i}\\!\\!\\!\\!+\\!\\hat{\\mathcal{E}}_{i}\\!\\!\\!\\!-\\!\\hat{L}_{i}(i\\alpha,\\hat{\\mathcal{H}}_{i})}\\\\ &{\\quad+\\gamma_{i}\\,\\overline{{\\kappa}}_{\\alpha}-\\hat{\\kappa}_{i}\\hat{L}_{i}(i\\alpha,\\hat{\\mathcal{H}}_{i})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the third inequality holds by Lemma 3. ", "page_idx": 20}, {"type": "text", "text": "Therefore, in the mild generalization area $\\tilde{\\beta}(a|s)>0$ , $\\mathcal{T}_{\\mathrm{DMG}}$ is a $\\gamma$ -contraction operator under the $\\mathcal{L}_{\\infty}$ norm. This concludes the proof. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "As a result, by repeatedly applying $\\mathcal{T}_{\\mathrm{DMG}}$ , any initial $\\mathrm{\\DeltaQ}$ function can converge to the unique fixed point $Q_{\\mathrm{DMG}}^{*}$ . We denote the induced policy of $Q_{\\mathrm{DMG}}^{*}$ by $\\pi_{\\mathrm{DMG}}^{*}$ . ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\begin{array}{r l}{Q_{\\mathrm{DMG}}^{*}(s,a)=R(s,a)+\\gamma\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot|s,a)}\\left[\\underset{a^{\\prime}\\sim\\tilde{\\beta}(\\cdot|s^{\\prime})}{\\operatorname*{max}}Q_{\\mathrm{DMG}}^{*}(s^{\\prime},a^{\\prime})\\right],}&{\\tilde{\\beta}(a|s)>0,}\\end{array}}\\\\ &{\\begin{array}{r l}{\\pi_{\\mathrm{DMG}}^{*}(s):=\\underset{a\\sim\\tilde{\\beta}(\\cdot|s)}{\\operatorname{argmax}}Q_{\\mathrm{DMG}}^{*}(s,a).}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Before we start the proof of Theorem 3, we prove two lemmas. ", "page_idx": 20}, {"type": "text", "text": "Lemma 5. Under Assumption 9, for any function $f$ , the following inequality holds: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{T}_{\\mathrm{DMG}}f(s,a)\\geq\\mathcal{T}_{\\mathrm{In}}f(s,a),~\\forall(s,a)~s.t.~\\tilde{\\beta}(a|s)>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. The oracle generalization assumption (Assumption 9) implies that $\\mathcal{T}_{\\mathrm{In}}$ is also well defined in the mild generalization area ${\\tilde{\\beta}}(a|s)>0$ . Because $\\mathrm{supp}(\\hat{\\beta}(\\cdot|s))\\subseteq\\mathrm{supp}(\\tilde{\\beta}(\\cdot|s)),$ $\\mathcal{T}_{\\mathrm{In}}$ requires less information than $\\mathcal{T}_{\\mathrm{DMG}}$ . Therefore, ${\\mathcal{T}}_{\\mathrm{DMG}}$ being well defined in the mild generalization area implies $\\mathcal{T}_{\\mathrm{In}}$ also being well defined in that area. ", "page_idx": 20}, {"type": "text", "text": "According to the definitions, for all $(s,a)$ s.t. ${\\tilde{\\beta}}(a|s)>0$ , ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{T}_{\\mathrm{DMG}}f(s,a)=R(s,a)+\\gamma\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot|s,a)}\\left[\\lambda\\operatorname*{max}_{a^{\\prime}\\sim\\tilde{\\beta}(\\cdot|s^{\\prime})}f(s^{\\prime},a^{\\prime})+(1-\\lambda)\\operatorname*{max}_{a^{\\prime}\\sim\\tilde{\\beta}(\\cdot|s^{\\prime})}f(s^{\\prime},a^{\\prime})\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{T}_{\\mathrm{In}}f(s,a)=R(s,a)+\\gamma\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot|s,a)}\\left[\\operatorname*{max}_{a^{\\prime}\\sim\\hat{\\beta}(\\cdot|s^{\\prime})}f(s^{\\prime},a^{\\prime})\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Therefore, for all $(s,a)$ s.t. ${\\tilde{\\beta}}(a|s)>0$ , we have ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{T}_{\\mathrm{DMG}}f(s,a)-\\mathcal{T}_{\\mathrm{In}}f(s,a)}\\\\ &{=\\!\\!\\gamma\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot|s,a)}\\left[\\lambda\\operatorname*{max}_{a^{\\prime}\\sim\\tilde{\\beta}(\\cdot|s^{\\prime})}f(s^{\\prime},a^{\\prime})-\\lambda\\operatorname*{max}_{a^{\\prime}\\sim\\hat{\\beta}(\\cdot|s^{\\prime})}f(s^{\\prime},a^{\\prime})\\right]}\\\\ &{\\geq\\!\\!0}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last inequality holds because $\\tilde{\\beta}$ has a wider support than $\\hat{\\beta}$ . ", "page_idx": 21}, {"type": "text", "text": "Lemma 6. Under Assumption $\\boldsymbol{9}$ , for any function $f_{1},f_{2}$ such that $f_{1}(s,a)\\;\\;\\geq\\;\\;\\;f_{2}(s,a),$ , $\\forall(s,a)$ s.t. $\\tilde{\\beta}(a|s)>0,$ , the following inequality holds: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{T}_{\\mathrm{DMG}}f_{1}(s,a)\\geq\\mathcal{T}_{\\mathrm{DMG}}f_{2}(s,a),\\;\\;\\forall(s,a)\\;s.t.\\;\\tilde{\\beta}(a|s)>0}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. By Assumption 9, ${\\mathcal{T}}_{\\mathrm{DMG}}$ is well defined in the mild generalization area $\\tilde{\\beta}(a|s)>0$ . ", "page_idx": 21}, {"type": "text", "text": "According to the definition, for all $(s,a)$ s.t. ${\\tilde{\\beta}}(a|s)>0$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{T}_{\\mathrm{DMG}}f(s,a)=R(s,a)+\\gamma\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot|s,a)}\\left[\\lambda\\operatorname*{max}_{a^{\\prime}\\sim\\tilde{\\beta}(\\cdot|s^{\\prime})}f(s^{\\prime},a^{\\prime})+(1-\\lambda)\\operatorname*{max}_{a^{\\prime}\\sim\\tilde{\\beta}(\\cdot|s^{\\prime})}f(s^{\\prime},a^{\\prime})\\right]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "$f_{1}$ and $f_{2}$ satisfy ", "page_idx": 21}, {"type": "equation", "text": "$$\nf_{1}(s,a)\\geq f_{2}(s,a),\\forall(s,a)\\ \\ s.t.\\ \\tilde{\\beta}(a|s)>0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Therefore, for all $(s,a)\\;s.t.\\;\\tilde{\\beta}(a|s)>0$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{T}_{\\mathrm{DMG}}f_{1}(s,a)-\\mathcal{T}_{\\mathrm{DMG}}f_{2}(s,a)}\\\\ &{=\\gamma\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot\\vert s,a)}\\left[\\lambda\\operatorname*{max}_{a^{\\prime}\\sim\\tilde{\\beta}(\\cdot\\vert s^{\\prime})}f_{1}(s^{\\prime},a^{\\prime})-\\lambda\\operatorname*{max}_{a^{\\prime}\\sim\\tilde{\\beta}(\\cdot\\vert s^{\\prime})}f_{2}(s^{\\prime},a^{\\prime})\\right]}\\\\ &{\\quad+\\,\\gamma\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot\\vert s,a)}\\left[(1-\\lambda)\\operatorname*{max}_{a^{\\prime}\\sim\\tilde{\\beta}(\\cdot\\vert s^{\\prime})}f_{1}(s^{\\prime},a^{\\prime})-(1-\\lambda)\\operatorname*{max}_{a^{\\prime}\\sim\\tilde{\\beta}(\\cdot\\vert s^{\\prime})}f_{2}(s^{\\prime},a^{\\prime})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now we start the proof of Theorem 3 in the main paper. ", "page_idx": 21}, {"type": "text", "text": "Theorem 8 (Performance, Theorem 3). Under Assumption 9, the value functions of $\\pi_{\\mathrm{DMG}}^{*}$ and $\\pi_{\\mathrm{In}}^{*}$ satisfy: ", "page_idx": 21}, {"type": "equation", "text": "$$\nV^{\\pi_{\\mathrm{DMG}}^{*}}(s)\\geq V^{\\pi_{\\mathrm{In}}^{*}}(s),\\quad\\forall s\\in\\mathcal{D}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. We first prove the following inequality: ", "page_idx": 21}, {"type": "equation", "text": "$$\n(\\mathcal{T}_{\\mathrm{DMG}})^{k}f(s,a)\\geq(\\mathcal{T}_{\\mathrm{In}})^{k}f(s,a),\\;\\forall k\\in\\mathbb{Z}^{+},\\;\\forall f,\\;\\,\\forall(s,a)\\;s.t.\\;\\tilde{\\beta}(a|s)>0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "When $k=1$ , according to Lemma 5, it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n(\\mathcal{T}_{\\mathrm{DMG}})^{1}f(s,a)\\geq(\\mathcal{T}_{\\mathrm{In}})^{1}f(s,a),\\;\\forall f,\\;\\,\\forall(s,a)\\;s.t.\\;\\tilde{\\beta}(a|s)>0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Suppose when $k=i$ , the following inequality holds: ", "page_idx": 21}, {"type": "equation", "text": "$$\n(\\mathcal{T}_{\\mathrm{DMG}})^{i}\\,f(s,a)\\geq(\\mathcal{T}_{\\mathrm{In}})^{i}\\,f(s,a),\\;\\forall f,\\;\\,\\forall(s,a)\\;s.t.\\;\\tilde{\\beta}(a|s)>0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Then $(\\mathcal{T}_{\\mathrm{DMG}})^{i}f$ and $(\\mathcal{T}_{\\mathrm{In}})^{i}f$ are the two functions $f_{1},f_{2}$ that satisfy the condition in Lemma 6. Therefore, by Lemma 6, it holds that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathcal{T}_{\\mathrm{DMG}}(\\mathcal{T}_{\\mathrm{DMG}})^{i}f(s,a)\\geq\\mathcal{T}_{\\mathrm{DMG}}(\\mathcal{T}_{\\mathrm{In}})^{i}f(s,a),\\;\\forall f,\\;\\,\\forall(s,a)\\;s.t.\\;\\tilde{\\beta}(a|s)>0.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Now considering $(\\mathcal{T}_{\\mathrm{In}})^{i}f$ as the function $f$ in Lemma 5. By Lemma 5, it holds that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{T}_{\\mathrm{DMG}}(\\mathcal{T}_{\\mathrm{In}})^{i}f(s,a)\\geq\\mathcal{T}_{\\mathrm{In}}(\\mathcal{T}_{\\mathrm{In}})^{i}f(s,a),\\;\\forall f,\\;\\;\\forall(s,a)\\;s.t.\\;\\tilde{\\beta}(a|s)>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Combining Equations (45) and (46), we have ", "page_idx": 22}, {"type": "equation", "text": "$$\n(\\mathcal{T}_{\\mathrm{DMG}})^{i+1}f(s,a)\\geq(\\mathcal{T}_{\\mathrm{In}})^{i+1}f(s,a),\\;\\forall f,\\;\\;\\forall(s,a)\\;s.t.\\;\\tilde{\\beta}(a|s)>0.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, for all $k\\in\\mathbb{Z}^{+}$ , the following inequality holds: ", "page_idx": 22}, {"type": "equation", "text": "$$\n(\\mathcal{T}_{\\mathrm{DMG}})^{k}f(s,a)\\geq(\\mathcal{T}_{\\mathrm{In}})^{k}f(s,a),\\;\\forall f,\\;\\,\\forall(s,a)\\;s.t.\\;\\tilde{\\beta}(a|s)>0.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Lemma 4 states that $\\mathcal{T}_{\\mathrm{In}}$ is a $\\gamma$ -contraction operator in the in-sample area $\\hat{\\beta}(a|s)>0$ . Thus we have ", "page_idx": 22}, {"type": "equation", "text": "$$\nQ_{\\mathrm{In}}^{*}(s,a)=\\operatorname*{lim}_{k\\to\\infty}(\\mathcal{T}_{\\mathrm{In}})^{k}f(s,a),\\;\\forall(s,a)\\;s.t.\\;\\hat{\\beta}(a|s)>0.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Under Assumption 9, Theorem 7 states that $\\mathcal{T}_{\\mathrm{DMG}}$ is a $\\gamma$ -contraction operator in the mild generalization area $\\bar{\\beta}(a|\\bar{s})>0$ . Thus we have ", "page_idx": 22}, {"type": "equation", "text": "$$\nQ_{\\mathrm{DMG}}^{*}(s,a)=\\operatorname*{lim}_{k\\to\\infty}(\\mathcal{T}_{\\mathrm{DMG}})^{k}f(s,a),\\;\\forall(s,a)\\;s.t.\\;\\tilde{\\beta}(a|s)>0.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "As $\\tilde{\\beta}$ has a wider support than $\\hat{\\beta},\\operatorname{supp}(\\hat{\\beta}(\\cdot|s))\\subseteq\\operatorname{supp}(\\tilde{\\beta}(\\cdot|s))$ , the following inequality holds by combining Equations (47) to (49): ", "page_idx": 22}, {"type": "equation", "text": "$$\nQ_{\\mathrm{DMG}}^{*}(s,a)\\geq Q_{\\mathrm{In}}^{*}(s,a),\\;\\forall(s,a)\\;s.t.\\;\\hat{\\beta}(a|s)>0.\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, for any $s\\sim\\mathcal{D}$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad V^{\\pi_{\\mathrm{DMG}}^{*}}(s)=V_{\\mathrm{DMG}}^{*}(s)=Q_{\\mathrm{DMG}}^{*}(s,\\pi_{\\mathrm{DMG}}^{*}(s))}\\\\ &{\\geq\\!Q_{\\mathrm{DMG}}^{*}(s,\\pi_{\\mathrm{In}}^{*}(s))}\\\\ &{\\geq\\!Q_{\\mathrm{In}}^{*}(s,\\pi_{\\mathrm{In}}^{*}(s))=V_{\\mathrm{In}}^{*}(s)=V^{\\pi_{\\mathrm{In}}^{*}}(s)}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the first inequality holds because $\\pi_{\\mathrm{DMG}}^{*}(s):=\\,\\mathrm{argmax}_{a\\sim\\tilde{\\beta}(\\cdot\\vert s)}\\,Q_{\\mathrm{DMG}}^{*}(s,a)$ and $\\pi_{\\mathrm{In}}^{*}(s)\\in$ $\\hat{\\beta}(\\cdot|s)$ (thus $\\pi_{\\mathrm{In}}^{*}(s)\\in\\tilde{\\beta}(\\cdot|s))$ , and the second inequality holds by Equation (50). ", "page_idx": 22}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 22}, {"type": "text", "text": "Theorem 8 indicates that the policy induced by the DMG operator can behave better than the in-sample optimal policy under the oracle generalization condition. ", "page_idx": 22}, {"type": "text", "text": "B.3 Proofs under Worst-case Generalization ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this section, we focus on the analyses in the worst-case generalization scenario, where the learned value functions may exhibit poor generalization in the mild generalization area ${\\tilde{\\beta}}(a|s)>0$ . In other words, this section considers that ${\\mathcal{T}}_{\\mathrm{DMG}}$ is only defined in the in-sample area $\\hat{\\beta}(a|s)>0$ and the learned value functions may have any generalization error at other state-action pairs. In this case, we use the notation $\\hat{\\mathcal{T}}_{\\mathrm{DMG}}$ for differentiation. ", "page_idx": 22}, {"type": "text", "text": "In this case, we make the following continuity assumptions about the learned $Q$ function and the transition dynamics $P$ . ", "page_idx": 22}, {"type": "text", "text": "Assumption 10 (Lipschitz $Q$ ). The learned $Q$ function is $K_{Q}$ -Lipschitz. $\\forall s\\sim\\mathcal{D}$ , $\\forall a_{1},a_{2}\\sim A_{\\cdot}$ , $|Q(s,a_{1})-Q(s,a_{2})|\\leq K_{Q}\\|a_{1}-a_{2}\\|$ ", "page_idx": 22}, {"type": "text", "text": "Assumption 11 (Lipschitz $P$ ). The transition dynamics $P$ is $K_{P}$ -Lipschitz. $\\forall s,s^{\\prime}\\sim{\\cal S},\\,\\forall a_{1},a_{2}\\sim{\\cal A},$ , $|P(s^{\\prime}|s,a_{1})-P(s^{\\bar{\\prime}}|s,a_{2})|\\leq K_{P}\\|a_{1}-a_{2}\\|$ ", "page_idx": 22}, {"type": "text", "text": "For Assumption 10, a continuous learned Q function is particularly necessary for the analysis of value function generalization and can be relatively easily satisfied [24], since we often use neural networks or linear models to parameterize the value function. For Assumption 11, continuous transition dynamics is also a standard assumption in the theoretical studies of RL [13, 14, 87, 61]. Several previous works assume the transition to be Lipschitz continuous with respect to (w.r.t) both state and action [13, 14]. In our paper, we need the Lipschitz continuity to hold only w.r.t. action. ", "page_idx": 22}, {"type": "text", "text": "Before we start the proof of Theorem 4, we prove two lemmas. ", "page_idx": 22}, {"type": "text", "text": "Lemma 7. Under Assumption $I O_{;}$ , for any function $f$ and $s\\sim\\mathcal{D}$ , the following inequality holds: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname*{max}_{a\\sim\\tilde{\\beta}(\\cdot\\vert s)}f(s,a)-\\operatorname*{max}_{a\\sim\\hat{\\beta}(\\cdot\\vert s)}f(s,a)\\leq\\epsilon_{a}K_{Q}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. For any $s\\sim\\mathcal{D}$ , we define $\\tilde{a}^{*},\\hat{a}^{*},\\hat{a}^{\\prime}$ as follows: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{a}^{*}=\\underset{a\\sim\\tilde{\\beta}(\\cdot\\vert s)}{\\mathrm{argmax}}\\,f(s,a)}\\\\ &{\\hat{a}^{*}=\\underset{a\\sim\\tilde{\\beta}(\\cdot\\vert s)}{\\mathrm{argmax}}\\,f(s,a)}\\\\ &{\\hat{a}^{\\prime}=\\underset{a\\sim\\tilde{\\beta}(\\cdot\\vert s)}{\\mathrm{argmin}}\\,\\Vert\\tilde{a}^{*}-a\\Vert}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "According to the definition of mildly generalized policy $\\tilde{\\beta}$ (Definition 4), it holds that $\\lVert\\tilde{{\\boldsymbol{a}}}^{*}-\\hat{{\\boldsymbol{a}}}^{\\prime}\\rVert\\leq\\epsilon_{a}$ . Further by Assumption 10, it holds that ", "page_idx": 23}, {"type": "equation", "text": "$$\n|f(s,\\tilde{a}^{*})-f(s,\\hat{a}^{\\prime})|\\leq K_{Q}\\|\\tilde{a}^{*}-\\hat{a}^{\\prime}\\|\\leq\\epsilon_{a}K_{Q},\\,\\,\\,\\forall s\\sim\\mathcal{D}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, ", "page_idx": 23}, {"type": "equation", "text": "$$\nf(s,\\tilde{a}^{*})-f(s,\\hat{a}^{*})\\leq f(s,\\tilde{a}^{*})-f(s,\\hat{a}^{\\prime})\\leq\\epsilon_{a}K_{Q},\\,\\,\\,\\forall s\\sim\\mathcal{D}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Lemma 8. For any function $f_{1},f_{2}$ such that $f_{1}(s,a)\\;\\geq\\;f_{2}(s,a),\\;\\forall(s,a)$ s.t. $\\hat{\\beta}(a|s)\\,>\\,0,$ , the following inequality holds: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{T}_{\\mathrm{In}}f_{1}(s,a)\\geq\\mathcal{T}_{\\mathrm{In}}f_{2}(s,a),~\\forall(s,a)~s.t.~\\hat{\\beta}(a|s)>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. According to the definitions, for all $(s,a)$ s.t. $\\hat{\\beta}(a|s)>0$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{T}_{\\mathrm{In}}f(s,a)=R(s,a)+\\gamma\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot|s,a)}\\left[\\operatorname*{max}_{a^{\\prime}\\sim\\hat{\\beta}(\\cdot|s^{\\prime})}f(s^{\\prime},a^{\\prime})\\right]\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "$f_{1}$ and $f_{2}$ satisfy ", "page_idx": 23}, {"type": "equation", "text": "$$\nf_{1}(s,a)\\geq f_{2}(s,a),\\forall(s,a)\\ \\ s.t.\\ \\hat{\\beta}(a|s)>0.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, for all $(s,a)$ s.t. $\\hat{\\beta}(a|s)>0$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathcal{T}_{\\mathrm{In}}f_{1}(s,a)-\\mathcal{T}_{\\mathrm{In}}f_{2}(s,a)}\\\\ &{=\\!\\!\\gamma\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot|s,a)}\\left[\\operatorname*{max}_{a^{\\prime}\\sim\\hat{\\beta}(\\cdot|s^{\\prime})}f_{1}(s^{\\prime},a^{\\prime})-\\operatorname*{max}_{a^{\\prime}\\sim\\hat{\\beta}(\\cdot|s^{\\prime})}f_{2}(s^{\\prime},a^{\\prime})\\right]}\\\\ &{\\geq\\!\\!0}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Now we start the proof of Theorem 4 in the main paper. ", "page_idx": 23}, {"type": "text", "text": "We consider the iteration starting from arbitrary function $Q^{0}$ $2^{0}\\colon\\hat{Q}_{\\mathrm{DMG}}^{k}=\\hat{T}_{\\mathrm{DMG}}\\hat{Q}_{\\mathrm{DMG}}^{k-1}$ and $Q_{\\mathrm{In}}^{k}=$ $\\tau_{\\mathrm{In}}Q_{\\mathrm{In}}^{k-1}$ , $\\forall k\\in\\mathbb{Z}^{+}$ . The possible value of $\\hat{Q}_{\\mathrm{DMG}}^{k}$ is upper bounded by the following results. ", "page_idx": 23}, {"type": "text", "text": "Theorem 9 (Limited over-estimation, Theorem 4). Under Assumption $I O_{\\cdot}$ , the learned $Q$ function of DMG by iterating $\\hat{\\mathcal{T}}_{\\mathrm{DMG}}$ satisfies the following inequality ", "page_idx": 23}, {"type": "equation", "text": "$$\nQ_{\\mathrm{In}}^{k}(s,a)\\leq\\hat{Q}_{\\mathrm{DMG}}^{k}(s,a)\\leq Q_{\\mathrm{In}}^{k}(s,a)+\\frac{\\lambda\\epsilon_{a}K_{Q}\\gamma}{1-\\gamma}(1-\\gamma^{k}),\\ \\forall s,a\\sim\\mathcal{D},\\ \\forall k\\in\\mathbb{Z}^{+}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Proof. Under worst-case generalization, $\\ensuremath{\\hat{\\mathcal{T}}}_{\\mathrm{DMG}}$ is only defined in the area $\\hat{\\beta}(a|s)\\,>\\,0$ , i.e., the dataset, and may have any generalization error at other $(s,a)$ . ", "page_idx": 24}, {"type": "text", "text": "For any function $f$ and any $s,a\\sim\\mathcal{D}$ , ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\phantom{=}\\hat{\\mathcal{T}}_{\\mathrm{DMG}}f(s,a)-\\mathcal{T}_{\\mathrm{In}}f(s,a)}\\\\ &{=\\!R(s,a)+\\gamma\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot\\vert s,a)}\\left[\\lambda_{\\alpha^{\\prime}\\sim\\hat{\\beta}(\\cdot\\vert s^{\\prime})}\\,f(s^{\\prime},a^{\\prime})+(1-\\lambda)\\operatorname*{max}_{a^{\\prime}\\sim\\hat{\\beta}(\\cdot\\vert s^{\\prime})}\\,f(s^{\\prime},a^{\\prime})\\right]}\\\\ &{\\phantom{=}-R(s,a)-\\gamma\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot\\vert s,a)}\\left[\\operatorname*{max}_{a^{\\prime}\\sim\\hat{\\beta}(\\cdot\\vert s^{\\prime})}\\,f(s^{\\prime},a^{\\prime})\\right]}\\\\ &{\\phantom{=}-\\gamma\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot\\vert s,a)}\\left[\\lambda_{\\alpha^{\\prime}\\sim\\hat{\\beta}(\\cdot\\vert s^{\\prime})}\\,f(s^{\\prime},a^{\\prime})-\\lambda_{\\alpha^{\\prime}\\sim\\hat{\\beta}(\\cdot\\vert s^{\\prime})}\\,f(s^{\\prime},a^{\\prime})\\right]}\\\\ &{\\overset{=}-\\gamma\\mathbb{E}_{s^{\\prime}\\sim P(\\cdot\\vert s,a)}\\left[\\lambda_{\\alpha^{\\prime}\\sim\\hat{\\beta}(\\cdot\\vert s^{\\prime})}\\,f(s^{\\prime},a^{\\prime})-\\lambda_{\\alpha^{\\prime}\\sim\\hat{\\beta}(\\cdot\\vert s^{\\prime})}\\,f(s^{\\prime},a^{\\prime})\\right]}\\\\ &{\\le\\!\\gamma\\lambda_{\\epsilon}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where the last inequality holds by Lemma 7. ", "page_idx": 24}, {"type": "text", "text": "On the other hand, because $\\tilde{\\beta}$ has a wider support than $\\hat{\\beta}$ , we also have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\hat{\\mathcal{T}}_{\\mathrm{DMG}}f(s,a)-\\mathcal{T}_{\\mathrm{In}}f(s,a)\\geq0\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Therefore, for any function $f$ , the following inequality holds: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{T}_{\\mathrm{In}}f(s,a)\\leq\\hat{\\mathcal{T}}_{\\mathrm{DMG}}f(s,a)\\leq\\mathcal{T}_{\\mathrm{In}}f(s,a)+\\gamma\\lambda\\epsilon_{a}K_{Q},\\ \\forall s,a\\sim\\mathcal{D}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Let $f$ in Equation (58) be $Q^{0}$ . We have ", "page_idx": 24}, {"type": "equation", "text": "$$\nQ_{\\mathrm{In}}^{1}(s,a)\\leq\\hat{Q}_{\\mathrm{DMG}}^{1}(s,a)\\leq Q_{\\mathrm{In}}^{1}(s,a)+\\frac{\\lambda\\epsilon_{a}K_{Q}\\gamma}{1-\\gamma}(1-\\gamma),\\ \\forall s,a\\sim\\mathcal{D}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "This is the same as Equation (57) with $k=1$ . Therefore, Equation (57) holds when $k=1$ . Suppose when $k=i$ , Equation (57) holds: ", "page_idx": 24}, {"type": "equation", "text": "$$\nQ_{\\mathrm{In}}^{i}(s,a)\\leq\\hat{Q}_{\\mathrm{DMG}}^{i}(s,a)\\leq Q_{\\mathrm{In}}^{i}(s,a)+\\frac{\\lambda\\epsilon_{a}K_{Q}\\gamma}{1-\\gamma}(1-\\gamma^{i}),\\ \\forall s,a\\sim\\mathcal{D}.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then let $f$ in Equation (58) be $\\hat{Q}_{\\mathrm{DMG}}^{i}$ . We have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{\\mathrm{In}}\\hat{Q}_{\\mathrm{DMG}}^{i}(s,a)\\leq\\hat{Q}_{\\mathrm{DMG}}^{i+1}(s,a)=\\hat{T}_{\\mathrm{DMG}}\\hat{Q}_{\\mathrm{DMG}}^{i}(s,a)\\leq T_{\\mathrm{In}}\\hat{Q}_{\\mathrm{DMG}}^{i}(s,a)+\\gamma\\lambda\\epsilon_{a}K_{Q},\\,\\,\\forall s,a\\sim\\mathcal{D}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "On the one hand, according to Lemma 8 and Equation (60), for any $s,a\\sim\\mathcal{D}$ , we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{T_{\\mathrm{In}}\\hat{Q}_{\\mathrm{IMG}}^{\\mathrm{in}}(s,a)}\\\\ &{\\leq T_{\\mathrm{In}}\\left(Q_{\\mathrm{in}}^{\\mathrm{i}}(s,a)+\\frac{\\lambda\\epsilon_{a}K_{Q}\\gamma}{1-\\gamma}(1-\\gamma^{i})\\right)}\\\\ &{=R(s,a)+\\gamma\\mathbb{E}_{s^{\\prime}\\sim P^{\\dagger}(\\cdot\\vert s,a)}\\left[\\underset{a^{\\prime}\\sim\\beta(\\cdot\\vert s^{\\prime})}{\\operatorname*{max}}\\left(Q_{\\mathrm{in}}^{\\mathrm{i}}(s^{\\prime},a^{\\prime})+\\frac{\\lambda\\epsilon_{a}K_{Q}\\gamma}{1-\\gamma}(1-\\gamma^{i})\\right)\\right]}\\\\ &{=R(s,a)+\\gamma\\mathbb{E}_{s^{\\prime}\\sim P^{\\dagger}(\\cdot\\vert s,a)}\\left[\\underset{a^{\\prime}\\sim\\beta(\\cdot\\vert s^{\\prime})}{\\operatorname*{max}}Q_{\\mathrm{in}}^{i}(s^{\\prime},a^{\\prime})\\right]+\\gamma\\frac{\\lambda\\epsilon_{a}K_{Q}\\gamma}{1-\\gamma}(1-\\gamma^{i})}\\\\ &{=T_{\\mathrm{In}}Q_{\\mathrm{in}}^{i}(s,a)+\\gamma\\frac{\\lambda\\epsilon_{a}K_{Q}\\gamma}{1-\\gamma}(1-\\gamma^{i})}\\\\ &{=Q_{\\mathrm{in}}^{i+1}(s,a)+\\gamma\\frac{\\lambda\\epsilon_{a}K_{Q}\\gamma}{1-\\gamma}(1-\\gamma^{i})}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Combining Equations (61) and (62), for any $s,a\\sim\\mathcal{D}$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\hat{Q}_{\\mathrm{DMG}}^{i+1}(s,a)}\\\\ &{\\le\\!Q_{\\mathrm{In}}^{i+1}(s,a)+\\gamma\\frac{\\lambda\\epsilon_{a}K_{Q}\\gamma}{1-\\gamma}(1-\\gamma^{i})+\\gamma\\lambda\\epsilon_{a}K_{Q}}\\\\ &{=\\!Q_{\\mathrm{In}}^{i+1}(s,a)+\\lambda\\epsilon_{a}K_{Q}\\gamma\\left(\\frac{\\gamma(1-\\gamma^{i})}{1-\\gamma}+1\\right)}\\\\ &{=\\!Q_{\\mathrm{In}}^{i+1}(s,a)+\\frac{\\lambda\\epsilon_{a}K_{Q}\\gamma}{1-\\gamma}(1-\\gamma^{i+1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "On the other hand, according to Lemma 8 and Equation (60), for any $s,a\\sim\\mathcal{D}$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{T_{\\mathrm{In}}\\hat{Q}_{\\mathrm{DMG}}^{i}(s,a)\\geq\\mathcal{T}_{\\mathrm{In}}Q_{\\mathrm{In}}^{i}(s,a)=Q_{\\mathrm{In}}^{i+1}(s,a)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Combining Equations (61) and (63), for any $s,a\\sim\\mathcal{D}$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{Q}_{\\mathrm{DMG}}^{i+1}(s,a)\\geq Q_{\\mathrm{In}}^{i+1}(s,a).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Hence, Equation (57) still holds when $k=i+1$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\nQ_{\\mathrm{In}}^{i+1}(s,a)\\leq\\hat{Q}_{\\mathrm{DMG}}^{i+1}(s,a)\\leq Q_{\\mathrm{In}}^{i+1}(s,a)+\\frac{\\lambda\\epsilon_{a}K_{Q}\\gamma}{1-\\gamma}(1-\\gamma^{i+1}),\\,\\forall s,a\\sim\\ensuremath{\\mathcal{D}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore, Equation (57) holds for all $k\\in\\mathbb{Z}^{+}$ , which concludes the proof. ", "page_idx": 25}, {"type": "text", "text": "Since in-sample training eliminates extrapolation error completely [37, 92], $Q_{\\mathrm{In}}^{k}$ can be considered a relatively accurate estimate. Therefore, Theorem 9 indicates that DMG has limited over-estimation under the worst generalization case. Moreover, the bound gets tighter as $\\epsilon_{a}$ gets smaller (more mild action generalization) and $\\lambda$ gets smaller (more mild generalization propagation). This is consistent with our intuitions in Section 3.2. ", "page_idx": 25}, {"type": "text", "text": "Finally, Theorem 5 in the main paper shows that even under worst-case generalization, DMG is guaranteed to output a safe policy with a performance lower bound. ", "page_idx": 25}, {"type": "text", "text": "We give a lemma before we start the proof of Theorem 5, ", "page_idx": 25}, {"type": "text", "text": "Lemma 9. Let $\\pi_{1}$ and $\\pi_{2}$ be two deterministic policies. Under Assumption $_{l l}$ , the following inequality holds: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathrm{TV}\\left(d^{\\pi_{1}}||d^{\\pi_{2}}\\right)\\leq C K_{P}\\operatorname*{max}_{s}\\left\\|\\pi_{1}(s)-\\pi_{2}(s)\\right\\|\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $C$ is a positive constant and $d^{\\pi}(s)$ is the state occupancy induced by $\\pi$ . ", "page_idx": 25}, {"type": "equation", "text": "$$\nd^{\\pi}(s)=(1-\\gamma)\\sum_{t=0}^{\\infty}\\gamma^{t}\\mathbb{E}_{\\pi}\\left[\\mathbb{I}\\left[s_{t}=s\\right]\\right].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Proof. Please refer to Lemma A.5 in [61] and Lemma 1 in [87]. ", "page_idx": 25}, {"type": "text", "text": "Theorem 10 (Performance lower bound, Theorem 5). Let $\\hat{\\pi}$ DMG be the learned policy of DMG by iterating $\\hat{\\mathcal{T}}_{\\mathrm{DMG}}$ , $\\pi^{*}$ be the optimal policy, and $\\epsilon_{\\mathcal{D}}$ be the inherent performance gap of the in-sample optimal policy $\\epsilon_{\\mathscr D}:={\\cal J}(\\pi^{*})-{\\cal J}(\\pi_{\\mathrm{In}}^{*})$ . Under Assumptions $I O$ and $_{l l}$ , for sufficiently small $\\epsilon_{a}$ , we have ", "page_idx": 25}, {"type": "equation", "text": "$$\nJ(\\hat{\\pi}_{\\mathrm{DMG}})\\geq J(\\pi^{*})-\\frac{C K_{P}R_{\\operatorname*{max}}}{1-\\gamma}\\epsilon_{a}-\\epsilon_{\\mathcal{D}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $C$ is a positive constant. ", "page_idx": 25}, {"type": "text", "text": "Proof. Following previous works [38, 83, 37, 49], we define the in-sample optimal policy as $\\pi_{\\mathrm{In}}^{*}$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\pi_{\\mathrm{In}}^{*}(s)=\\operatorname*{argmax}_{a\\sim\\hat{\\beta}(\\cdot\\vert s)}Q_{\\mathrm{In}}^{*}(s,a)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We also use $\\epsilon_{\\mathcal{D}}$ to denote the performance gap between the in-sample optimal policy and the globally optimal policy, which is fixed once the dataset is provided. ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\epsilon_{\\mathcal{D}}=J(\\pi^{*})-J(\\pi_{\\mathrm{In}}^{*}).\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "We use $\\hat{Q}_{\\mathrm{DMG}}$ to denote the learned $\\mathrm{^Q}$ function of DMG with sufficient iteration steps $\\hat{Q}_{\\mathrm{DMG}}^{k}$ , $k\\rightarrow\\infty$ . And $\\ensuremath{\\hat{\\pi}}_{\\mathrm{DMG}}$ is the output policy of $\\hat{Q}_{\\mathrm{DMG}}$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\hat{\\pi}_{\\mathrm{DMG}}(s)=\\underset{a\\sim\\tilde{\\beta}(\\cdot\\vert s)}{\\mathrm{argmax}}\\,\\hat{Q}_{\\mathrm{DMG}}(s,a)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "It holds that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad|J(\\pi^{*})-J(\\hat{\\pi}_{\\mathrm{DMG}})|}\\\\ &{=\\!|J(\\pi^{*})-J(\\pi_{\\mathrm{In}}^{*})+J(\\pi_{\\mathrm{In}}^{*})-J(\\hat{\\pi}_{\\mathrm{DMG}})|}\\\\ &{\\leq\\!|J(\\pi^{*})-J(\\pi_{\\mathrm{In}}^{*})|+|J(\\pi_{\\mathrm{In}}^{*})-J(\\hat{\\pi}_{\\mathrm{DMG}})|}\\\\ &{=\\!\\epsilon_{D}+|J(\\pi_{\\mathrm{In}}^{*})-J(\\hat{\\pi}_{\\mathrm{DMG}})|}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "In the following, we bound the term $|J(\\pi_{\\mathrm{In}}^{*})-J(\\hat{\\pi}_{\\mathrm{DMG}})|$ . ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad|J(\\pi_{\\mathrm{in}}^{*})-J(\\bar{n}_{\\mathrm{NMG}})|}\\\\ &{=\\left|\\displaystyle\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim d^{*}\\mathrm{prac}}[r(s)]-\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim d^{*}\\mathrm{in}}[r(s)]\\right|}\\\\ &{=\\displaystyle\\frac{1}{1-\\gamma}\\left|\\sum_{s}\\left(d^{\\bar{s}_{\\mathrm{DMG}}}(s)-d^{\\bar{r}_{\\mathrm{in}}^{*}}(s)\\right)r(s)\\right|}\\\\ &{\\leq\\displaystyle\\frac{1}{1-\\gamma}\\sum_{s}\\left|\\left(d^{\\bar{s}_{\\mathrm{DMG}}}(s)-d^{\\bar{r}_{\\mathrm{in}}^{*}}(s)\\right)\\right||r(s)|}\\\\ &{\\leq\\displaystyle\\frac{R_{\\mathrm{max}}}{1-\\gamma}\\mathrm{Tv}\\left(d^{\\bar{s}_{\\mathrm{DMG}}}(s)\\|d^{\\bar{r}_{\\mathrm{in}}^{*}}(s)\\right)}\\\\ &{\\leq\\displaystyle\\frac{R_{\\mathrm{max}}}{1-\\gamma}C K r\\operatorname*{max}_{s}\\|\\bar{r}_{\\mathrm{DMG}}(s)-\\pi_{\\mathrm{in}}^{*}(s)\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the last inequality holds by Lemma 9. ", "page_idx": 26}, {"type": "text", "text": "According to Theorem 9, $\\hat{Q}_{\\mathrm{DMG}}$ satisfies the following inequality: ", "page_idx": 26}, {"type": "equation", "text": "$$\nQ_{\\mathrm{In}}^{*}(s,a)\\leq\\hat{Q}_{\\mathrm{DMG}}(s,a)\\leq Q_{\\mathrm{In}}^{*}(s,a)+\\frac{\\lambda\\epsilon_{a}K_{Q}\\gamma}{1-\\gamma},\\,\\forall s,a\\sim\\mathcal{D}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "It means that for any $(s,a)\\sim\\mathcal{D}$ , with sufficiently small $\\epsilon_{a}$ , $\\hat{Q}_{\\mathrm{DMG}}(s,a)$ sufficiently approximates $Q_{\\mathrm{In}}^{*}(s,a)$ . By Definition 4, $\\tilde{\\beta}$ is a mildly generalized policy. That is, for any $s\\sim\\mathcal{D},\\tilde{\\beta}$ satisfies ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname{supp}(\\hat{\\beta}(\\cdot|s))\\subseteq\\operatorname{supp}(\\tilde{\\beta}(\\cdot|s)),\\;\\;\\mathrm{and}\\quad\\operatorname*{max}_{a_{1}\\sim\\tilde{\\beta}(\\cdot|s)}\\operatorname*{min}_{a_{2}\\sim\\tilde{\\beta}(\\cdot|s)}\\|a_{1}-a_{2}\\|\\leq\\epsilon_{a},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "As $\\hat{\\pi}_{\\mathrm{DMG}}(s)\\in\\tilde{\\beta}(\\cdot|s)$ , it implies that we can find $a_{\\mathrm{in}}\\in\\hat{\\beta}(\\cdot|s)$ (in dataset) such that $\\|\\hat{\\pi}_{\\mathrm{DMG}}(s)-$ $a_{\\mathrm{in}}\\|\\leq\\epsilon_{a}$ . ", "page_idx": 26}, {"type": "text", "text": "Now suppose $a_{\\mathrm{in}}$ is not the maximum point of $Q_{\\mathrm{In}}^{*}(s,\\cdot)$ at a certain $s$ . We use $\\pi_{\\mathrm{In}}^{*}(s)$ to denote the maximum point of $Q_{\\mathrm{In}}^{*}(s,\\cdot)$ . Let $\\epsilon_{Q_{\\mathrm{In}}^{*}}$ be the gap between $Q_{\\mathrm{In}}^{*}(s,a_{\\mathrm{in}})$ and $Q_{\\mathrm{In}}^{*}(s,\\overline{{\\pi}}_{\\mathrm{In}}^{*}(s))$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\epsilon_{Q_{\\mathrm{In}}^{*}}(s):=Q_{\\mathrm{In}}^{*}(s,\\pi_{\\mathrm{In}}^{*}(s))-Q_{\\mathrm{In}}^{*}(s,a_{\\mathrm{in}})>0.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By Assumption 10 (Lipschitz $Q$ ), we have ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{Q}_{\\mathrm{DMG}}(s,\\hat{\\pi}_{\\mathrm{DMG}}(s))-\\hat{Q}_{\\mathrm{DMG}}(s,a_{\\mathrm{in}})\\leq K_{Q}\\|\\hat{\\pi}_{\\mathrm{DMG}}(s)-a_{\\mathrm{in}}\\|\\leq K_{Q}\\epsilon_{a}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Therefore, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\hat{Q}_{\\mathrm{DMG}}\\big(s,\\pi_{\\mathrm{In}}^{*}(s)\\big)-\\hat{Q}_{\\mathrm{DMG}}\\big(s,\\hat{\\pi}_{\\mathrm{DMG}}(s)\\big)}\\\\ &{\\geq\\!\\hat{Q}_{\\mathrm{DMG}}\\big(s,\\pi_{\\mathrm{In}}^{*}(s)\\big)-\\hat{Q}_{\\mathrm{DMG}}\\big(s,a_{\\mathrm{in}}\\big)-K_{Q}\\epsilon_{a}}\\\\ &{\\geq\\!Q_{\\mathrm{In}}^{*}\\big(s,\\pi_{\\mathrm{In}}^{*}(s)\\big)-Q_{\\mathrm{In}}^{*}\\big(s,a_{\\mathrm{in}}\\big)-\\displaystyle\\frac{\\lambda\\epsilon_{a}K_{Q}\\gamma}{1-\\gamma}-K_{Q}\\epsilon_{a}}\\\\ &{=\\!\\epsilon_{Q_{\\mathrm{In}}^{*}}\\!\\left(s\\right)-\\displaystyle\\frac{\\lambda\\epsilon_{a}K_{Q}\\gamma}{1-\\gamma}-K_{Q}\\epsilon_{a}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the first inequality holds by Equation (76), the second inequality holds by Equation (74), and the last equality holds by Equation (75). ", "page_idx": 27}, {"type": "text", "text": "Hence, for sufficiently small $\\epsilon_{a}$ such that $\\begin{array}{r}{\\epsilon_{Q_{\\mathrm{In}}^{*}}(s)-\\frac{\\lambda\\epsilon_{a}K_{Q}\\gamma}{1-\\gamma}-K_{Q}\\epsilon_{a}>0}\\end{array}$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\epsilon_{a}<\\frac{(1-\\gamma)\\epsilon_{Q_{\\mathrm{In}}^{*}}(s)}{K_{Q}(1-\\gamma+\\lambda\\gamma)},\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "it holds that $\\hat{Q}_{\\mathrm{DMG}}(s,\\pi_{\\mathrm{In}}^{*}(s))-\\hat{Q}_{\\mathrm{DMG}}(s,\\hat{\\pi}_{\\mathrm{DMG}}(s))\\,>\\,0$ . As $\\pi_{\\mathrm{In}}^{*}(s)\\;\\in\\;\\hat{\\beta}(\\cdot|s)$ , it also satisfies $\\pi_{\\mathrm{In}}^{*}(s)\\in\\tilde{\\beta}(\\cdot|s)$ . This contradicts the definition of $\\hat{\\pi}_{\\mathrm{DMG}}(s)$ in Equation (71): ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\hat{\\pi}_{\\mathrm{DMG}}(s)=\\underset{a\\sim\\tilde{\\beta}(\\cdot\\vert s)}{\\mathrm{argmax}}\\,\\hat{Q}_{\\mathrm{DMG}}(s,a)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Therefore, $a_{\\mathrm{in}}$ is the maximum point of $Q_{\\mathrm{In}}^{*}(s,\\cdot)$ . In other words, the maximum point of $Q_{\\mathrm{In}}^{*}(s,\\cdot)$ (denoted by $\\pi_{\\mathrm{In}}^{*}(s)$ ) is the closest neighbor of $\\hat{\\pi}_{\\mathrm{DMG}}(s)$ in the dataset $(\\hat{\\beta}(\\cdot|s)>0)$ : ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\pi_{\\mathrm{In}}^{*}(s)=\\underset{a\\sim\\hat{\\beta}(\\cdot\\vert s)}{\\operatorname{argmin}}\\,\\Vert a-\\hat{\\pi}_{\\mathrm{DMG}}(s)\\Vert\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "As $\\hat{\\pi}_{\\mathrm{DMG}}(s)\\in\\tilde{\\beta}(\\cdot|s)$ , the following inequality holds by Definition 4: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\hat{\\pi}_{\\mathrm{DMG}}(s)-\\pi_{\\mathrm{In}}^{*}(s)\\right\\|\\le\\epsilon_{a}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Therefore, we have ", "page_idx": 27}, {"type": "equation", "text": "$$\n|J(\\pi_{\\mathrm{In}}^{*})-J(\\hat{\\pi}_{\\mathrm{DMG}})|\\leq\\frac{R_{\\mathrm{max}}}{1-\\gamma}C K_{P}\\epsilon_{a}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "By combining Equations (72) and (78), we have ", "page_idx": 27}, {"type": "equation", "text": "$$\nJ(\\hat{\\pi}_{\\mathrm{DMG}})\\geq J(\\pi^{*})-\\frac{C K_{P}R_{\\operatorname*{max}}}{1-\\gamma}\\epsilon_{a}-\\epsilon_{\\mathcal{D}}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 27}, {"type": "text", "text": "C Experimental Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "C.1 Experimental Details in Offline Experiments ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Our evaluation criteria follow those used in most previous works. For the Gym locomotion tasks, we average returns over 10 evaluation trajectories and 5 random seeds, while for the AntMaze tasks, we average over 100 evaluation trajectories and 5 random seeds. Following the suggestions in the benchmark [16], we subtract 1 from the rewards for the AntMaze datasets. And following previous works [17, 37, 83, 88], we normalize the states in Gym locomotion datasets. We choose TD3 [18] as our base algorithm and optimize a deterministic policy. Thus we replace the log likelihood in Eq. (14) with mean squared error in practice, which is equivalent to optimizing a Gaussian policy with fixed variance [17]. The reported results are the normalized scores, which are offered by the D4RL benchmark [16] to measure how the learned policy compared with random and expert policy: ", "page_idx": 27}, {"type": "table", "img_path": "7QG9R8urVy/tmp/d321e5a91434e840dd0479c9009104a801fadc36e0fc4267276b88bc4caddf11.jpg", "table_caption": ["Table 5: Hyperparameters of DMG. "], "table_footnote": [], "page_idx": 28}, {"type": "text", "text": "As we implement our main algorithm based on IQL [37], we use the hyperparameters suggested in their paper for fair comparisons, i.e., $\\tau=0.7$ and $\\alpha=3$ for Gym locomotion tasks and $\\tau=0.9$ and $\\alpha=10$ for AntMaze tasks. For the results of $\\mathcal{X}\\mathrm{QL}\\mathrm{+}\\mathrm{DMG}$ and $\\scriptstyle\\mathrm{SQL+DMG}$ , we also adopt the suggested hyperparameters in their papers [21, 88] for fair comparisons. In detail, we choose $\\beta$ in $\\mathcal{X}\\mathrm{QL}$ [21] as 5.0 in medium, medium-replay, and medium-expert datasets, and $\\alpha$ in SQL [88] as 2.0 for medium, medium-replay datasets, and 5.0 for medium-expert datasets. ", "page_idx": 28}, {"type": "text", "text": "DMG has two main hyperparameters: mixture coefficient $\\lambda$ and penalty coefficient $\\nu$ . We use $\\lambda=0.25$ for all tasks. We use $\\nu=0.5$ for Antmaze tasks and $\\nu\\in\\{0.1,10\\}$ for Gym locomotion tasks (0.1 for medium, medium-replay, random datasets; 10 for expert and medium-expert datasets). All hyperparameters of DMG are included in Table 5. ", "page_idx": 28}, {"type": "text", "text": "C.2 Experimental Details in Offline-to-online Experiments ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "For online fine-tuning experiments, we first run offline RL for $1\\times10^{6}$ gradient steps. Then we continue training while collecting data actively in the environment and adding the data to the replay buffer. We perform online fine-tuning for $\\mathrm{1\\stackrel{\\cdot}{\\times}10^{6}}$ steps with 1 update-to-data (UTD) ratio, and collect data with exploration noise 0.1 as suggested by TD3 [18]. During offline pre-training, we fix the mixture coefficient $\\lambda=0.25$ and the penalty coefficient $\\nu=0.5$ , while in the online phase, we exponentially adjust $\\lambda$ and $\\nu$ , as DMG with $\\lambda=1$ and $\\nu=0$ corresponds to standard online RL. In the challenging AntMaze domains characterized by high-dimensional state and action spaces, as well as sparse rewards, the extrapolation error remains significant even during the online phase [83]. Therefore, we decay $\\lambda$ from 0.25 to 0.5 and $\\nu$ from 0.5 to 0.005 $1\\%$ of its initial value), employing a decay rate of 0.99 every 1000 gradient steps. Additionally, following previous works [83, 72], we set $\\gamma=0.995$ when fine-tuning on antmaze-large datasets, for both DMG and IQL to ensure a fair comparison. All other training details remain consistent between the offilne RL phase and the online fine-tuning phase. ", "page_idx": 28}, {"type": "text", "text": "D Additional Experimental Results ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "D.1 Computational Cost ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "We test the runtime of offline RL algorithms on halfcheetah-medium-replay-v2 on a GeForce RTX 3090. The results of DMG and other baselines are shown in Figure 3. It takes 1.7h for DMG to finish the task, which is comparable to the fastest offline RL algorithm TD3BC [17]. ", "page_idx": 28}, {"type": "image", "img_path": "7QG9R8urVy/tmp/699751a67edc7b43c3ef8c215cdf2fe0cd77558b6229b0966765619307c063ce.jpg", "img_caption": ["Figure 3: Runtime of algorithms on halfcheetah-medium-replay-v2 on a GeForce RTX 3090. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "D.2 Offline Training Results of DMG on More Random Seeds ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "The experimental results in the main paper show the mean and standard deviation (SD) over five random seeds. According to [56], we conduct experiments to test DMG on additional random seeds, reporting $95\\%$ confidence interval (CI) over 10 random seeds. Table 6 shows the comparison between the new results (10seeds $/95\\%\\mathrm{CI})$ ) and the previously reported results (5seeds/SD in Table 2) on the D4RL offline training tasks. The results show that our method achieves about the same performance as under the previous evaluation criterion. ", "page_idx": 29}, {"type": "table", "img_path": "7QG9R8urVy/tmp/8075e0602c0c60ddaeb44d22d819e39d68797890d88e0812636289aeb25a27d8.jpg", "table_caption": ["Table 6: Comparison of DMG under different evaluation criteria on D4RL offline training tasks. "], "table_footnote": [], "page_idx": 29}, {"type": "text", "text": "D.3 Learning Curves of DMG during Offline Training ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Learning curves during offline training on Gym-MuJoCo locomotion tasks and Antmaze tasks are presented in Figure 4 and Figure 5, respectively. The curves are averaged over 5 random seeds, with the shaded area representing the standard deviation across seeds. ", "page_idx": 29}, {"type": "text", "text": "D.4 Learning Curves of DMG during Online Fine-tuning ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Learning curves during online fine-tuning on Antmaze tasks are presented in Figure 6. The curves are averaged over 5 random seeds, with the shaded area representing the standard deviation across seeds. ", "page_idx": 30}, {"type": "text", "text": "E Broader Impact ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Offline reinforcement learning (RL) presents a promising avenue for enhancing and broadening the practical applicability of RL across various domains including robotics, recommendation systems, healthcare, and education, characterized by costly or hazardous data collection processes. However, it is imperative to recognize the potential adverse societal ramifications associated with any offilne RL algorithm. One such concern pertains to the possibility that the offilne data utilized for training may harbor inherent biases, which could subsequently permeate into the acquired policy. Furthermore, it is essential to contemplate the potential implications of offilne RL on employment, given its contribution to automating tasks conventionally executed by human experts, such as factory automation or autonomous driving. Addressing these challenges is essential for fostering the responsible development and deployment of offline RL algorithms, with the aim of maximizing their positive impact while mitigating negative societal consequences. ", "page_idx": 30}, {"type": "text", "text": "From an academic perspective, this research scrutinizes offilne RL through the lens of generalization, balancing the need for generalization with the risk of over-generalization. The proposed approach DMG potentially offers researchers a new perspective on appropriately exploiting generalization in offilne RL. Besides, DMG also holds the promise to be extended to safe RL [1, 26, 20], multi-agent RL [43, 62, 65, 60, 25], and meta RL [15, 76, 77, 75, 4]. ", "page_idx": 30}, {"type": "image", "img_path": "7QG9R8urVy/tmp/a22eab596191e180edf551dead0aebd4c78c3bb0721f2f68794e3cafdd4a7923.jpg", "img_caption": ["Figure 4: Learning curves of DMG on Gym locomotion tasks during offilne training. The curves are averaged over 5 random seeds, with the shaded area representing the standard deviation across seeds. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "7QG9R8urVy/tmp/e0ef3514e6e3bbb371dace7b678521d505ae3fa9f9fd2145a80680d694c66387.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Figure 5: Learning curves of DMG on Antmaze tasks during offilne training. The curves are averaged over 5 random seeds, with the shaded area representing the standard deviation across seeds. ", "page_idx": 31}, {"type": "image", "img_path": "7QG9R8urVy/tmp/d503cb499da9cda2147cf9a7adb795870592b77bc0e88556a5136e709b98bd3b.jpg", "img_caption": ["Figure 6: Learning curves of DMG on Antmaze tasks during online fine-tuning. The curves are averaged over 5 random seeds, with the shaded area representing the standard deviation across seeds. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 32}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Justification: Please refer to Section 6. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 32}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Please refer to Appendix B. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 33}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: Please refer to Appendix C. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 33}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: Please refer to the code in the supplemental material. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 34}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: Please refer to Appendix C. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 34}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: The results in the paper are accompanied by standard deviations across multiple seeds. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 34}, {"type": "text", "text": "", "page_idx": 35}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Please refer to Appendix D.1. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 35}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 35}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: Please refer to Appendix E. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "", "page_idx": 36}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 36}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The creators or original owners of assets used in the paper are properly credited and the license and terms of use are explicitly mentioned and properly respected. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 36}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 37}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The code is well documented and anonymized. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 37}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 37}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 37}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 37}]