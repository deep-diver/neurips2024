[{"figure_path": "7QG9R8urVy/tables/tables_5_1.jpg", "caption": "Table 1: Comparison of offline RL work from the generalization perspective.", "description": "This table compares different offline reinforcement learning (RL) methods based on two aspects of generalization: action generalization (whether the policy selects actions beyond the dataset) and generalization propagation (whether value training propagates generalization).  It categorizes several offline RL algorithms as having none, mild, or full generalization in each aspect, highlighting the unique approach of the proposed Doubly Mild Generalization (DMG) method.", "section": "4 Discussions and Related Work"}, {"figure_path": "7QG9R8urVy/tables/tables_6_1.jpg", "caption": "Table 1: Comparison of offline RL work from the generalization perspective.", "description": "This table summarizes various offline reinforcement learning (RL) methods from the perspective of generalization.  It categorizes the methods based on two key aspects of generalization: Action Generalization (whether the policy training intentionally selects actions beyond the dataset to maximize Q-values) and Generalization Propagation (whether value training propagates generalization through bootstrapping).  The table shows the different levels of generalization utilized by each method (none, mild, or full).  It helps to illustrate the unique approach of Doubly Mild Generalization (DMG) in mitigating the issues of over-generalization often seen in offline RL.", "section": "4 Discussions and Related Work"}, {"figure_path": "7QG9R8urVy/tables/tables_7_1.jpg", "caption": "Table 2: Averaged normalized scores on Gym locomotion and Antmaze tasks over five random seeds. m = medium, m-r = medium-replay, m-e = medium-expert, e = expert, r = random; u = umaze, u-d = umaze-diverse, m-p = medium-play, m-d = medium-diverse, l-p= large-play, l-d = large-diverse.", "description": "This table presents the average normalized scores achieved by different offline reinforcement learning algorithms on various Gym locomotion and Antmaze tasks.  The results are averaged over five different random seeds for each task.  The table provides a comparison of the performance of DMG (Doubly Mild Generalization), the proposed method, against several baseline algorithms including BCQ, BEAR, AWAC, TD3BC, CQL, IQL, and others.  Different dataset variations (e.g., medium, expert, random) are also included for comparison.", "section": "5.1 Main Results on Offline RL Benchmarks"}, {"figure_path": "7QG9R8urVy/tables/tables_7_2.jpg", "caption": "Table 3: DMG combined with various in-sample approaches, showing averaged scores over 5 seeds.", "description": "This table presents the results of combining the Doubly Mild Generalization (DMG) method with three different in-sample learning approaches (XQL, SQL, and IQL) on several benchmark tasks.  It demonstrates the performance improvement achieved by incorporating DMG into existing in-sample methods, highlighting the benefits of mild generalization in offline reinforcement learning.", "section": "5.2 Performance Improvement over In-sample Learning Approaches"}, {"figure_path": "7QG9R8urVy/tables/tables_8_1.jpg", "caption": "Table 4: Online fine-tuning results on AntMaze tasks, showing normalized scores of offline training and 1M steps online fine-tuning, averaged over 5 seeds.", "description": "This table presents the results of online fine-tuning experiments on AntMaze tasks.  It compares the performance of three algorithms: TD3 (trained from scratch), IQL, and DMG.  The table shows the normalized scores achieved after offline training and then after an additional 1 million steps of online fine-tuning.  The results highlight DMG's superior online fine-tuning performance, demonstrating a seamless transition from offline to online learning.", "section": "5.4 Online Fine-tuning after Offline RL"}, {"figure_path": "7QG9R8urVy/tables/tables_28_1.jpg", "caption": "Table 1: Comparison of offline RL work from the generalization perspective.", "description": "The table compares several offline reinforcement learning methods based on two aspects: action generalization (whether the policy selects actions beyond the dataset) and generalization propagation (whether value training propagates generalization through bootstrapping).  It categorizes existing methods into those with none, mild, or full generalization in each aspect, highlighting the unique position of the proposed DMG method.", "section": "4 Discussions and Related Work"}, {"figure_path": "7QG9R8urVy/tables/tables_29_1.jpg", "caption": "Table 6: Comparison of DMG under different evaluation criteria on D4RL offline training tasks.", "description": "This table compares the performance of the Doubly Mild Generalization (DMG) algorithm on D4RL offline training tasks using two different evaluation criteria.  The first uses the mean and standard deviation (SD) calculated over five random seeds, while the second uses the mean and 95% confidence interval (CI) calculated over ten random seeds. This allows for a comparison of the algorithm's robustness and consistency across different runs and evaluation methods.", "section": "5.2 Performance Improvement over In-sample Learning Approaches"}]