[{"figure_path": "PWzB2V2b6R/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of the online action detection. Models trained on closed-set actions (e.g., discus and brush toilet) are unable to detect the novel action class (e.g., fry eggs). We train a visual-text dual-encoder on web-collected video-text pairs without using frame-scale labels. It can discriminate arbitrary action classes.", "description": "This figure compares the closed-set online action detection and open-vocabulary online action detection methods.  The left side shows a traditional approach where the model is trained on a limited set of actions and struggles to recognize novel actions. In contrast, the right side showcases the proposed OV-OAD model, which leverages vision-language models and text supervision to detect novel actions in real-time without relying on frame-level labels.", "section": "1 Introduction"}, {"figure_path": "PWzB2V2b6R/figures/figures_3_1.jpg", "caption": "Figure 2: The illustration of our OV-OAD (best viewed in color), is formulated in a visual-text dual-encoder manner. Specifically, the visual encoder consists of a distant neighboring-frame transformer block (DNTR, light grey backdrop) and an action clustering block (AC, yellowish backdrop). The DNTR is built with Transformer Decoder units, which take the neighboring tokens and distant past tokens as inputs. The AC is built with our Object-Centric Decoder and vanilla Transformer Encoder units, which take the output tokens (orange squares) and learnable group embeddings (purple gradient squares) as inputs. During testing, the OV-OAD handles each incoming video snippet online, absent future context.", "description": "This figure illustrates the architecture of the proposed Open-Vocabulary Online Action Detection (OV-OAD) model.  The model uses a dual-encoder approach, with separate visual and text encoders. The visual encoder processes video frames, using a distant neighboring-frame transformer to leverage information from both neighboring and past frames and an object-centric decoder to group similar frames.  The text encoder processes captions.  The model is trained using three proxy tasks: video-text alignment, current frame-text matching, and background frame mask prediction to improve zero-shot performance on unseen actions. The diagram visually represents the flow of information and the various components, highlighting key elements like attention mechanisms and group embeddings.", "section": "3.1 Architecture"}, {"figure_path": "PWzB2V2b6R/figures/figures_9_1.jpg", "caption": "Figure 3: Failure recognition cases on THUMOS'14. We use the red box to indicate the location of the action that is taking place.", "description": "This figure shows four examples of failure cases from the THUMOS'14 dataset where the model's action recognition performance was poor.  The red boxes highlight the locations of the actions that the model failed to correctly identify. These failures are likely due to challenges such as subtle actions, similar backgrounds, and cluttered scenes. The examples illustrate limitations in distinguishing between similar actions that are visually close, particularly when the foreground and background are indistinct. This highlights the need for further model improvements, potentially incorporating a more robust representation of spatio-temporal information.", "section": "4.3 Limitations"}]