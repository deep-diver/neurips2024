[{"figure_path": "PWzB2V2b6R/tables/tables_6_1.jpg", "caption": "Table 1: Benchmark evaluation on THUMOS'14 and TVSeries. \"IVid\" denotes InternVid-5K.", "description": "This table presents the benchmark evaluation results of different methods on two datasets: THUMOS'14 and TVSeries.  The methods compared include various versions of CLIP and the proposed OV-OAD model, using both InternVid-5K and ANet for pre-training.  The evaluation metrics are mean average precision (mAP) for THUMOS'14 and calibrated average precision (cAP) for TVSeries.  The results show that OV-OAD significantly outperforms the baseline CLIP methods.", "section": "4.1 Comparison with Existing Methods"}, {"figure_path": "PWzB2V2b6R/tables/tables_6_2.jpg", "caption": "Table 2: Benchmark evaluation on FineAction and EK100.", "description": "This table presents the benchmark evaluation results of the proposed OV-OAD model and the baseline CLIP model on two action detection datasets: FineAction and EK100.  The results show the mean average precision (mAP) and calibrated average precision (cAP) for both models on each dataset, allowing for a comparison of their performance on different scales and complexities of action data.", "section": "4.1 Comparison with Existing Methods"}, {"figure_path": "PWzB2V2b6R/tables/tables_6_3.jpg", "caption": "Table 3: Base-to-novel and fully-supervised evaluation on THUMOS\u201914 dataset.", "description": "This table presents the results of experiments conducted on the THUMOS\u201914 dataset to evaluate the performance of various methods, including base-to-novel and fully supervised approaches.  It compares the mean average precision (mAP) achieved by different models under three train-test split scenarios: 100% seen (0% unseen), 75% seen (25% unseen), and 50% seen (50% unseen).  The table highlights how the OV-OAD model performs comparatively to other existing methods under these different data conditions.", "section": "4.1 Comparison with Existing Methods"}, {"figure_path": "PWzB2V2b6R/tables/tables_7_1.jpg", "caption": "Table 4: Ablation study on the current frame-caption contrastive loss (Lcurrent) and background mask loss (Lmask). The baseline only uses the multi-label video clip-text contrastive loss (Lcontras).", "description": "This table presents the ablation study results on the three proxy tasks used in the OV-OAD model: multi-label video-text contrastive loss (Lcontras), current frame-text contrastive loss (Lcurrent), and background mask loss (Lmask).  The table shows the mean average precision (mAP) achieved with different combinations of these loss functions, demonstrating the contribution of each component to the overall performance.", "section": "4.2 Ablations"}, {"figure_path": "PWzB2V2b6R/tables/tables_7_2.jpg", "caption": "Table 6: Results of different numbers of Transformer units for AC and DNTR.", "description": "This table presents the results of experiments conducted to determine the optimal number of Transformer units for both the Action Clustering block (AC) and the Distant Neighboring-frame Transformer block (DNTR) within the OV-OAD model.  The table shows how different configurations of layers impacted the model's performance, measured by mean Average Precision (mAP).  The results indicate an optimal configuration leading to the highest mAP.", "section": "4.2 Ablations"}, {"figure_path": "PWzB2V2b6R/tables/tables_7_3.jpg", "caption": "Table 5: Results of different number of frame tokens about V and Vp.", "description": "This table presents the results of experiments conducted to determine the optimal number of frame tokens to use for both neighboring frames (V) and past frames (Vp) in the OV-OAD model.  Different numbers of tokens were tested (\"4\", \"8\", \"16\", \"24\", \"28\", \"32\"), and the resulting mean average precision (mAP) values are shown in the table. The goal was to find the balance between incorporating enough contextual information from previous and neighboring frames while maintaining computational efficiency.", "section": "4.2 Ablations"}, {"figure_path": "PWzB2V2b6R/tables/tables_7_4.jpg", "caption": "Table 7: Results of different designs of the DNTR block. \"TR\" denotes Transformer. \u201cOC\u201d means Object-Centric. The penultimate row is our proposed OV-OAD design.", "description": "This table presents the ablation study on the design of the Distant Neighboring-Frame Transformer (DNTR) block within the OV-OAD model.  It compares different architectures for the DNTR block, including using Transformer encoders and decoders, with and without cross-attention mechanisms.  The results show the mean average precision (mAP) achieved by each architecture, demonstrating the effectiveness of the chosen design of 4xTR Decoder with an Object-Centric Decoder.", "section": "4.2 Ablations"}, {"figure_path": "PWzB2V2b6R/tables/tables_9_1.jpg", "caption": "Table 8: Ablation study on Text Encoder.", "description": "This table presents the ablation study on the Text Encoder, showing the performance (mAP) with different configurations: pre-trained weights used or not, fixed weights or fine-tuned, and adapter used or not.  The results indicate that using pre-trained weights with an adapter yields the best performance.", "section": "4.2 Ablations"}, {"figure_path": "PWzB2V2b6R/tables/tables_9_2.jpg", "caption": "Table 9: Efficiency comparison on parameter (M) and inference speed (FPS)", "description": "This table compares the model parameters and inference speed (frames per second) of four different online action detection models: OadTR, LSTR, MAT, and OV-OAD.  The comparison is broken down into the number of parameters (in millions), the speed of optical flow computation, the speed of RGB feature extraction, the speed of flow feature extraction, and the overall model inference speed. OV-OAD shows significantly faster inference speed compared to the others,  demonstrating its efficiency for real-time applications.", "section": "4 Experiments"}, {"figure_path": "PWzB2V2b6R/tables/tables_14_1.jpg", "caption": "Table 11: Video-Text model evaluation on THUMOS\u201914.", "description": "This table presents the benchmark results of various video-text models on the THUMOS\u201914 dataset.  The models tested include ViCLIP (with both ViT/B and ViT/L architectures) and CLIP (also with ViT/B). The table compares the mean average precision (mAP) achieved by each model, demonstrating the superior performance of the proposed OV-OAD model compared to existing video-text models.", "section": "4 Experiments"}, {"figure_path": "PWzB2V2b6R/tables/tables_14_2.jpg", "caption": "Table 12: The impact of  PAC  for zero-shot performance on THUMOS\u201914.", "description": "This table shows the impact of the Action Clustering block's output (PAC) on the zero-shot performance of the OV-OAD model on the THUMOS'14 dataset.  The results demonstrate that including PAC significantly improves performance compared to excluding it.", "section": "4.1 Comparison with Existing Methods"}]