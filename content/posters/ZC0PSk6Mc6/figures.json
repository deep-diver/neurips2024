[{"figure_path": "ZC0PSk6Mc6/figures/figures_1_1.jpg", "caption": "Figure 1: Successive Concept Bottlenecks Agents (SCoBots) allow for easy inspection and revision. Top: Deep RL agents trained on Pong produce high playing scores with importance map explanations that suggest sensible underlying reasons for taking an action (). However, when the enemy is hidden, the deep RL agent fails to even catch the ball without clear reasons (). Bottom: SCoBots, on the other hand, allow for multi-level inspection of the reasoning behind the action selection, e.g., at a relational concept, but also action level. Moreover, they allow users to easily intervene on them () to prevent the agents from focusing on potentially misleading concepts. In this way, SCoBots can mitigate RL specific caveats like goal misalignment.", "description": "This figure illustrates the core idea of the paper: introducing SCoBots, a new approach to reinforcement learning agents.  The top part shows that standard deep RL agents can achieve high scores in the game Pong but fail when the opponent is hidden, highlighting the problem of misalignment. The bottom part showcases SCoBots, which utilize successive concept bottleneck layers allowing for a multi-level inspection and understanding of the agent's decision-making process, enabling easy intervention and mitigation of misalignment issues.", "section": "1 Introduction"}, {"figure_path": "ZC0PSk6Mc6/figures/figures_2_1.jpg", "caption": "Figure 2: An overview of Successive Concept Bottlenecks Agents (SCoBots). SCoBots decompose the policy into consecutive interpretable concept bottlenecks (ICB). Objects and their properties are first extracted from the raw input, human-understandable functions are then employed to derive relational concepts, used to select an action. The understandable concepts enable interactivity. Each bottleneck allows expert users to, e.g., prune or utilize concepts to define additional reward signals.", "description": "This figure illustrates the architecture of Successive Concept Bottleneck Agents (SCoBots).  SCoBots break down the reinforcement learning policy into several interpretable concept bottleneck layers. The process begins with extracting object properties from raw inputs (e.g., images).  Then, relational concepts are derived using human-understandable functions applied to these objects (e.g., distance between objects). These relational concepts are used by an action selector to choose the optimal action. The key feature is the interactivity at each layer; experts can modify the concepts or reward signals to align the agent's behavior with desired goals.", "section": "2 Successive Concept Bottleneck Agents"}, {"figure_path": "ZC0PSk6Mc6/figures/figures_5_1.jpg", "caption": "Figure 3: Object-centric agents can master different Atari environments and interactive SCoBots allow for corrections. Human-normalized scores of different agents trained using PPO on 9 ALE environments, including deep agents (i.e. using CNNs), guided decision tree policy (SCoBots), their neural object-centric baseline (NN-), and these baselines without guidance (NG). SCoBots obtain similar or better scores than the deep agents, showing that object-centric agents can also solve RL tasks while making use of human-understandable concepts (left). Guiding SCoBots allow to correct misalignment in Pong (center) and to obtain the originally intended agents, depicted by a level completion score of 100% on the intended goal's evaluation in Kangaroo (right).", "description": "This figure presents a comparison of the performance of different RL agents across various Atari games.  The left panel shows that SCoBots achieve comparable or better performance than standard deep RL agents. The middle panel demonstrates SCoBots' ability to correct for misalignment issues, specifically in the Pong game. The right panel illustrates SCoBots' capacity for achieving the intended goals in complex scenarios, as seen in the Kangaroo game.", "section": "3 Experimental Evaluations"}, {"figure_path": "ZC0PSk6Mc6/figures/figures_6_1.jpg", "caption": "Figure 4: Interpretable SCoBots allow to follow their decision process, thanks to their interpretable concepts. The states and associated decision processes of SCoBots (extracted from the decision trees) on Skiing (left), and from unguided SCoBots on Pong (middle) and Kangaroo (right). For example, in this Skiing state, our SCoBot selects RIGHT, as the signed distance between Player and the (left) Flag1 (on the x axis) is bigger than 15. This agent selects the correct action for the right reasons.", "description": "This figure shows three examples of how SCoBots' interpretable concepts enable tracing the decision-making process.  The left panel depicts Skiing, where the agent correctly chooses \"RIGHT\" because the player's distance from the left flag exceeds 15 pixels.  The center panel illustrates the Pong misalignment; the agent focuses on the enemy's paddle, not the ball.  The right panel showcases Kangaroo, where the agent's decision to \"FIRE\" is based on the proximity to a monkey.", "section": "Inspectable SCoBots to detect misalignments (Q2)"}, {"figure_path": "ZC0PSk6Mc6/figures/figures_7_1.jpg", "caption": "Figure 5: SCoBots can learn with noisy object detectors, transparent SCoBots rely on relations. Final human normalized scores (with stds) comparing SCoBots and the object-centric neural baselines (NN-SCoBots), with and without relations. We also provide the scores of NN-SCoBots that learned on noisy environments. The noise only noticeably affects the agents on Kangaroo. Ablating the relations is harmless on NN-SCobots, as neural networks can recompute them, but impacts SCoBots performances on 6 games. (*For better visualization, we used a human score of 100 in Boxing.)", "description": "This figure compares the performance of different SCoBots configurations, including those with and without relational concepts and those trained with noisy object detectors. It shows that relational concepts improve performance, except in boxing and that the models are relatively robust to noisy inputs, except on the Kangaroo game.", "section": "3 Experimental Evaluations"}, {"figure_path": "ZC0PSk6Mc6/figures/figures_15_1.jpg", "caption": "Figure 6: The Atari Learning Environments is more used in scientific research than the next 8 other benchmarks together. Graph borrowed from [Delfosse et al., 2024b].", "description": "This figure is a bar chart showing the number of publications using different reinforcement learning benchmarks.  The Atari benchmark significantly outperforms all other benchmarks in terms of publication count, indicating its widespread use in scientific research.", "section": "A.1 Atari games are most common set of environments"}, {"figure_path": "ZC0PSk6Mc6/figures/figures_16_1.jpg", "caption": "Figure 3: Object-centric agents can master different Atari environments and interactive SCoBots allow for corrections. Human-normalized scores of different agents trained using PPO on 9 ALE environments, including deep agents (i.e., using CNNs), guided decision tree policy (SCoBots), their neural object-centric baseline (NN-), and these baselines without guidance (NG). SCoBots obtain similar or better scores than the deep agents, showing that object-centric agents can also solve RL tasks while making use of human-understandable concepts (left). Guiding SCoBots allow to correct misalignment in Pong (center) and to obtain the originally intended agents, depicted by a level completion score of 100% on the intended goal's evaluation in Kangaroo (right).", "description": "This figure demonstrates the performance of SCoBots and other RL agents (deep RL agents, NN-SCoBots, and NN-SCoBots(NG)) across 9 Atari games. The left panel compares the performance of SCoBots and deep RL agents, showing similar or better performance with SCoBots. The center panel showcases SCoBots ability to mitigate goal misalignment in Pong.  The right panel demonstrates that guiding SCoBots can achieve the intended goal (100% level completion) in the Kangaroo game, suggesting that human-understandable concepts can resolve issues such as goal misalignment.", "section": "Experimental Evaluations"}, {"figure_path": "ZC0PSk6Mc6/figures/figures_17_1.jpg", "caption": "Figure 1: Successive Concept Bottlenecks Agents (SCoBots) allow for easy inspection and revision. Top: Deep RL agents trained on Pong produce high playing scores with importance map explanations that suggest sensible underlying reasons for taking an action (). However, when the enemy is hidden, the deep RL agent fails to even catch the ball without clear reasons (). Bottom: SCoBots, on the other hand, allow for multi-level inspection of the reasoning behind the action selection, e.g., at a relational concept, but also action level. Moreover, they allow users to easily intervene on them () to prevent the agents from focusing on potentially misleading concepts. In this way, SCoBots can mitigate RL specific caveats like goal misalignment.", "description": "This figure demonstrates the advantages of SCoBots over traditional deep RL agents in terms of interpretability and goal alignment.  The top panel shows that while a deep RL agent performs well in the standard Pong game, it fails when the opponent is hidden, highlighting a hidden misalignment problem. The bottom panel illustrates how SCoBots, through their multi-level concept bottleneck architecture, allow for easy inspection and intervention, enabling better understanding and mitigation of such issues.", "section": "1 Introduction"}, {"figure_path": "ZC0PSk6Mc6/figures/figures_20_1.jpg", "caption": "Figure 4: Interpretable SCoBots allow to follow their decision process, thanks to their interpretable concepts. The states and associated decision processes of SCoBots (extracted from the decision trees) on Skiing (left), and from unguided SCoBots on Pong (middle) and Kangaroo (right). For example, in this Skiing state, our SCoBot selects RIGHT, as the signed distance between Player and the (left) Flag1 (on the x axis) is bigger than 15. This agent selects the correct action for the right reasons.", "description": "This figure shows three examples of how SCoBots make decisions in different Atari games: Skiing, Pong, and Kangaroo.  Each example displays a game state and the corresponding decision tree path used by the SCoBot agent to reach its chosen action.  The key takeaway is the interpretability of the SCoBots model, allowing for easy inspection and understanding of the reasoning behind the agent's actions.  The Pong example, in particular, highlights a case of goal misalignment where the agent focuses on a spurious correlation between the positions of paddles instead of directly using the ball's position, showcasing a previously unknown issue with the game.", "section": "Inspectable SCoBots to detect misalignments (Q2)"}, {"figure_path": "ZC0PSk6Mc6/figures/figures_20_2.jpg", "caption": "Figure 1: Successive Concept Bottlenecks Agents (SCoBots) allow for easy inspection and revision. Top: Deep RL agents trained on Pong produce high playing scores with importance map explanations that suggest sensible underlying reasons for taking an action (). However, when the enemy is hidden, the deep RL agent fails to even catch the ball without clear reasons (). Bottom: SCoBots, on the other hand, allow for multi-level inspection of the reasoning behind the action selection, e.g., at a relational concept, but also action level. Moreover, they allow users to easily intervene on them () to prevent the agents from focusing on potentially misleading concepts. In this way, SCoBots can mitigate RL specific caveats like goal misalignment.", "description": "This figure illustrates the core idea of the paper. The top part shows that standard deep RL agents, when trained on the Pong game, may develop strategies that rely on the enemy's paddle position, even though the ball's position is more relevant. When the enemy is hidden, the agent fails. The bottom part illustrates how SCoBots, by introducing intermediate concept layers, allow for better interpretability and mitigation of such issues. The figure depicts how SCoBots allow inspecting and revising the reasoning process at multiple levels.  This enables users to identify and address misleading concepts. It highlights that SCoBots allow for human intervention in model learning to prevent the agent from relying on shortcuts.", "section": "1 Introduction"}, {"figure_path": "ZC0PSk6Mc6/figures/figures_21_1.jpg", "caption": "Figure 4: Interpretable SCoBots allow to follow their decision process, thanks to their interpretable concepts. The states and associated decision processes of SCoBots (extracted from the decision trees) on Skiing (left), and from unguided SCoBots on Pong (middle) and Kangaroo (right). For example, in this Skiing state, our SCoBot selects RIGHT, as the signed distance between Player and the (left) Flag1 (on the x axis) is bigger than 15. This agent selects the correct action for the right reasons.", "description": "This figure shows examples of the decision-making process used by SCoBots agents on three different Atari games: Skiing, Pong, and Kangaroo.  The decision trees, extracted from the trained models, show how the agent selects an action based on a series of interpretable concepts.  Each game example highlights how different relational concepts, such as distances between objects, are used by the agent in their reasoning process. This is particularly important for highlighting the interpretability advantage of SCoBots over traditional deep RL agents, which often provide opaque and inscrutable explanations.", "section": "Inspectable SCoBots to detect misalignments (Q2)"}, {"figure_path": "ZC0PSk6Mc6/figures/figures_21_2.jpg", "caption": "Figure 13: Expert user feedback allow for faster discovering of the reward signal in the sparse reward Pong environment.", "description": "The figure shows the number of steps needed for a random agent and a guided agent to receive a reward in the Pong game. The original Pong environment has a sparse reward, meaning that the agent only gets a reward when it scores a point. The assisted agent has an additional reward signal that is inversely proportional to the distance between its paddle and the ball, which incentivizes the agent to keep a vertical position close to the ball. As a result, the assisted agent receives rewards more frequently than the original agent.", "section": "A.8.3 The reward sparsity of Pong"}]