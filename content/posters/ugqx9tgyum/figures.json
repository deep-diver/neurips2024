[{"figure_path": "ugqx9tgyum/figures/figures_3_1.jpg", "caption": "Figure 1: Overview of the dual-network meta-learning HMR method, composed of a main HMR regression network fw and an auxiliary network fu. Both networks have the same architecture but different parameters. Given ith batch of images, test-time optimization is first executed for each training image Ii,j in the batch individually, updating fw to fw by performing a gradient descent step w.r.t. the test-time loss function Ltest-u. Then based on {fw|j \u2208 [1,M]} (M is the batch size), the training optimization is executed to update the parameters of both main and auxiliary networks by Ltrain with different arguments respectively. wmeta and umeta are the finally generated meta-parameters. fu generates \u201cPseudo SMPLs", "description": "This figure illustrates the dual-network meta-learning approach for Human Mesh Recovery (HMR). It shows how test-time optimization is integrated into the training process. The main network (fw) and an auxiliary network (fu) are used. For each image in a batch, test-time optimization is performed to update the main network's parameters. These updated parameters, along with the auxiliary network's output (pseudo ground truth SMPL meshes), are then used in a training optimization step to update both networks' parameters. This process leads to the generation of meta-parameters (wmeta and umeta) that are effective for test-time optimization.", "section": "3 Our Method"}, {"figure_path": "ugqx9tgyum/figures/figures_7_1.jpg", "caption": "Figure 2: Qualitative comparison with SOTA methods. We show results produced by CLIFF [33], ReFit [59], and our method (\u2020: OpenPose, *: RSN). All the three methods use HRNet-W48 as the backbone. In the novel views, green represents the ground truth, orange represents CLIFF, purple represents ReFit, pink and blue represent the two variants of our method, respectively.", "description": "This figure compares the qualitative results of the proposed method against two state-of-the-art methods (CLIFF and ReFit) on several examples.  It shows input images and the 3D human mesh estimations of each method, with novel views showing the accuracy from different angles. The ground truth meshes are shown in green for comparison.  The different colors highlight the variation in results between the different approaches.", "section": "4.3 Comparison with Previous Approaches"}, {"figure_path": "ugqx9tgyum/figures/figures_7_2.jpg", "caption": "Figure 3: Influence of optimization steps during inference. Our method outperforms EFT when using the same regression model. As optimization proceeds, our results continuously become better, while those of EFT become better at first and then become worse (see (a) and (b)). (c) shows that our method achieves faster convergence compared to EFT.", "description": "This figure shows the impact of the number of optimization steps during the inference phase on the performance of the proposed method and EFT.  The plots (a) and (b) illustrate that the proposed method consistently improves with more optimization steps, unlike EFT which shows initial improvement followed by degradation. Plot (c) demonstrates the faster convergence of the proposed method compared to EFT.", "section": "3.1 Test-time Optimization"}, {"figure_path": "ugqx9tgyum/figures/figures_8_1.jpg", "caption": "Figure 1: Overview of the dual-network meta-learning HMR method, composed of a main HMR regression network fw and an auxiliary network fu. Both networks have the same architecture but different parameters. Given ith batch of images, test-time optimization is first executed for each training image Ii,j in the batch individually, updating fw to fw by performing a gradient descent step w.r.t. the test-time loss function Ltest-u. Then based on {fw|j \u2208 [1,M]} (M is the batch size), the training optimization is executed to update the parameters of both main and auxiliary networks by Ltrain with different arguments respectively. wmeta and umeta are the finally generated meta-parameters. fu generates \u201cPseudo SMPLs", "description": "This figure illustrates the dual-network meta-learning HMR method. It shows two networks (main and auxiliary) with the same architecture but different parameters.  The process begins with test-time optimization on each image in a batch, updating the main network's parameters. This is followed by a training optimization step that uses the updated parameters from all images in the batch. This dual-network approach unifies training and testing objectives by generating pseudo SMPLs. These pseudo SMPLs are utilized in the test-time loss to supervise the learning of 'Estimated SMPL Inner,' while the ground truth SMPLs are employed in the training loss to supervise the 'Estimated SMPL Outer'. The resulting meta-parameters (wmeta and umeta) are then used for human mesh recovery at test time.", "section": "3.2 Incorporating Test-time Optimization into Training"}, {"figure_path": "ugqx9tgyum/figures/figures_9_1.jpg", "caption": "Figure 2: Qualitative comparison with SOTA methods. We show results produced by CLIFF [33], ReFit [59], and our method (\u2020: OpenPose, *: RSN). All the three methods use HRNet-W48 as the backbone. In the novel views, green represents the ground truth, orange represents CLIFF, purple represents ReFit, pink and blue represent the two variants of our method, respectively.", "description": "This figure presents a qualitative comparison of the proposed method's performance against state-of-the-art approaches (CLIFF and Refit) on human mesh recovery. It uses HRNet-W48 as the backbone network for all three methods.  The comparison includes both standard views and novel viewpoints, to assess the robustness of the approaches. The color-coding in the novel views highlights the differences between the ground truth, CLIFF, Refit and the proposed method's results.", "section": "4.3 Comparison with Previous Approaches"}, {"figure_path": "ugqx9tgyum/figures/figures_16_1.jpg", "caption": "Figure 1: Overview of the dual-network meta-learning HMR method, composed of a main HMR regression network fw and an auxiliary network fu. Both networks have the same architecture but different parameters. Given ith batch of images, test-time optimization is first executed for each training image Ii,j in the batch individually, updating fw to fw by performing a gradient descent step w.r.t. the test-time loss function Ltest-u. Then based on {fw i,j |j \u2208 [1,M]} (M is the batch size), the training optimization is executed to update the parameters of both main and auxiliary networks by Ltrain with different arguments respectively. wmeta and umeta are the finally generated meta-parameters. fu generates \u201cPseudo SMPLs", "description": "This figure illustrates the dual-network meta-learning HMR method.  It shows how a main HMR regression network (fw) and an auxiliary network (fu) are used together.  The test-time optimization is performed individually for each image in a batch, updating fw. Then, training optimization is performed using the results of the test-time optimization to further update both networks. The auxiliary network (fu) generates pseudo SMPLs used in the training and test losses, while GT SMPLs are used in the training loss.  The final meta-parameters (wmeta and umeta) are used for inference. ", "section": "3.2 Incorporating Test-time Optimization into Training"}, {"figure_path": "ugqx9tgyum/figures/figures_18_1.jpg", "caption": "Figure 7: Stepwise visualization. From left to right, we showcase results after different steps of test-time optimization during testing.", "description": "This figure shows a series of images demonstrating the stepwise process of test-time optimization. Each column represents a different step in the optimization process, starting from an initial guess (Step 0) and progressing towards a refined estimate (Final). The results show the progressive refinement of the 3D human mesh estimation over several steps of test-time optimization, and illustrates how the initial estimates are progressively corrected to better align with the input image data, demonstrating the effectiveness of this method.", "section": "A.7 More Intermediate Results"}, {"figure_path": "ugqx9tgyum/figures/figures_20_1.jpg", "caption": "Figure 8: More qualitative comparisons with SOTA methods. We show results produced by HybrIK [32], NIKI [31], ProPose [13], ReFit [59], CLIFF [33], EFTCLIFF, and our method (\u2020: OpenPose, *: RSN ).", "description": "This figure compares the qualitative results of the proposed method with several state-of-the-art methods on Human Mesh Recovery.  It shows the input image and the 3D human mesh estimations produced by different methods. The goal is to visually demonstrate the performance differences between the approaches in terms of accuracy and detail in reconstructing the human mesh. The methods being compared are HybrIK [32], NIKI [31], ProPose [13], ReFit [59], CLIFF [33], EFTCLIFF, and the proposed method.  Two versions of the proposed method are shown, one using OpenPose and the other using RSN for 2D joint detection.", "section": "4.3 Comparison with Previous Approaches"}, {"figure_path": "ugqx9tgyum/figures/figures_21_1.jpg", "caption": "Figure 8: More qualitative comparisons with SOTA methods. We show results produced by HybrIK [32], NIKI [31], ProPose [13], ReFit [59], CLIFF [33], EFTCLIFF, and our method (\u2020: OpenPose, *: RSN ).", "description": "This figure presents a qualitative comparison of the proposed method's performance against several state-of-the-art (SOTA) human mesh recovery methods.  It shows the input images and the 3D mesh recovery results from various methods, including HybrIK, NIKI, ProPose, ReFit, CLIFF, EFTCLIFF and the authors' method using two different 2D joint detectors (OpenPose and RSN).  The visualization allows for a direct comparison of the mesh accuracy and fidelity of the different approaches, highlighting the strengths and weaknesses of each method in terms of accuracy and robustness.", "section": "4.3 Comparison with Previous Approaches"}, {"figure_path": "ugqx9tgyum/figures/figures_22_1.jpg", "caption": "Figure 2: Qualitative comparison with SOTA methods. We show results produced by CLIFF [33], ReFit [59], and our method (\u2020: OpenPose, *: RSN). All the three methods use HRNet-W48 as the backbone. In the novel views, green represents the ground truth, orange represents CLIFF, purple represents ReFit, pink and blue represent the two variants of our method, respectively.", "description": "This figure compares the qualitative results of the proposed method with two state-of-the-art methods (CLIFF and ReFit) on several examples.  The input images are shown along with the 3D mesh reconstruction results from each method, including novel views to better assess the accuracy of the mesh generation.  Different colors represent different methods for easy comparison. The ground truth is shown in green.", "section": "4.3 Comparison with Previous Approaches"}, {"figure_path": "ugqx9tgyum/figures/figures_22_2.jpg", "caption": "Figure 2: Qualitative comparison with SOTA methods. We show results produced by CLIFF [33], ReFit [59], and our method (\u2020: OpenPose, *: RSN). All the three methods use HRNet-W48 as the backbone. In the novel views, green represents the ground truth, orange represents CLIFF, purple represents ReFit, pink and blue represent the two variants of our method, respectively.", "description": "This figure compares the qualitative results of the proposed method with two state-of-the-art methods, CLIFF and ReFit.  It visually demonstrates the superiority of the proposed method by presenting both original images and the 3D human mesh recovery results from each method. Different color schemes are used to easily distinguish results generated by different methods, with a clear indication of the ground truth for each example.", "section": "4.3 Comparison with Previous Approaches"}, {"figure_path": "ugqx9tgyum/figures/figures_22_3.jpg", "caption": "Figure 1: Overview of the dual-network meta-learning HMR method, composed of a main HMR regression network fw and an auxiliary network fu. Both networks have the same architecture but different parameters. Given ith batch of images, test-time optimization is first executed for each training image Ii,j in the batch individually, updating fw to fw by performing a gradient descent step w.r.t. the test-time loss function Ltest-u. Then based on {fw|j \u2208 [1,M]} (M is the batch size), the training optimization is executed to update the parameters of both main and auxiliary networks by Ltrain with different arguments respectively. wmeta and umeta are the finally generated meta-parameters. fu generates \u201cPseudo SMPLs", "description": "This figure illustrates the dual-network meta-learning framework for Human Mesh Recovery (HMR). It shows two networks: a main HMR regression network (fw) and an auxiliary network (fu). The process begins with test-time optimization for each image in a batch, updating fw's parameters.  Then, training optimization is performed, updating both networks' parameters based on results from the test-time optimization. This process generates meta-parameters (wmeta and umeta).  The auxiliary network (fu) creates pseudo SMPL (Surface-based Mannequin Parameterized Linear) meshes, used in the loss function to supervise the learning of the inner SMPL meshes.  The ground-truth SMPL meshes are used to supervise the outer SMPL mesh, providing a unified objective for both training and testing phases. ", "section": "3.2 Incorporating Test-time Optimization into Training"}]