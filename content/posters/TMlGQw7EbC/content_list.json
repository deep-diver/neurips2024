[{"type": "text", "text": "Markov Equivalence and Consistency in Differentiable Structure Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Chang Deng\u2020\u2217 Kevin Bello\u2020,\u2021 Pradeep Ravikumar\u2021 Bryon Aragam\u2020 ", "page_idx": 0}, {"type": "text", "text": "\u2020Booth School of Business, University of Chicago, Chicago, IL 60637 \u2021Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15213 ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Existing approaches to differentiable structure learning of directed acyclic graphs (DAGs) rely on strong identifiability assumptions in order to guarantee that global minimizers of the acyclicity-constrained optimization problem identifies the true DAG. Moreover, it has been observed empirically that the optimizer may exploit undesirable artifacts in the loss function. We explain and remedy these issues by studying the behavior of differentiable acyclicity-constrained programs under general likelihoods with multiple global minimizers. By carefully regularizing the likelihood, it is possible to identify the sparsest model in the Markov equivalence class, even in the absence of an identifiable parametrization. We first study the Gaussian case in detail, showing how proper regularization of the likelihood defines a score that identifies the sparsest model. Assuming faithfulness, it also recovers the Markov equivalence class. These results are then generalized to general models and likelihoods, where the same claims hold. These theoretical results are validated empirically, showing how this can be done using standard gradient-based optimizers (without resorting to approximations such as Gumbel-Softmax), thus paving the way for differentiable structure learning under general models and losses. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Directed acyclic graphs (DAGs) are the most common graphical representation for causal models [48, 60, 50], where nodes represent variables and directed edges represent cause-effect relationships among variables. We are interested in the problem of structure learning, i.e. learning DAGs from passively observed data, also known as causal discovery. Our focus will mainly be on scorebased approaches to DAG learning [11, 23], where the structure learning problem is formulated as optimizing a given score or loss function $s(B;{\\mathbf{X}})$ that measures how well the graph, represented as an adjacency matrix $B\\;\\in\\;\\{0,1\\}^{p\\times p}$ , fits the observed data $\\mathbf{X}$ , constrained to the graphical structure $B$ being acyclic. This combinatorial optimization problem is generally known to be NP-complete [10, 12]. ", "page_idx": 0}, {"type": "text", "text": "Recent advances in score-based methods have introduced a continuous representation of DAGs, transforming the combinatorial acyclicity constraint into a continuous constraint via a differentiable function that exactly characterizes DAGs [73]. In this case, the discrete adjacency matrix $B\\ \\in$ $\\{0,1\\}^{p\\times p}$ is first relaxed to the space of real matrices, i.e., $\\boldsymbol{B}\\,\\in\\,\\mathbb{R}^{p\\times p}$ , and then a differentiable function $h:\\mathbb{R}^{p\\times p}\\rightarrow[0,\\infty)$ is devised so that $h(B)=0$ if and only if $B$ is a DAG [73, 4]. This results in the following optimization problem: ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{B\\in\\mathbb{R}^{p\\times p}}s(B;\\mathbf{X})\\quad{\\mathrm{subject~to}}\\quad h(B)=0.\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "Considering a differentiable score function $s$ , the differentiable program (1) facilitates the use of gradient-based optimization techniques along with the use of richer models, such as neural networks, for modeling the functional relationships among the variables [74, 67, 41, 29, 44, 28, 76]. One of the most attractive features of this approach is that it applies to general models, losses, and optimizers, in contrast to prior work. Moreover, it cleanly separates computational and statistical concerns, so that each can be studied in isolation, in the same spirit as the graphical lasso [36, 68, 17]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Looking back at the inception of the continuous DAG learning framework by Zheng et al. [73, 74], however, most developments in this framework have focused on the design of alternative differentiable acyclicity functions $h$ with better numerical/computational properties [4, 32, 72, 67], placing little emphasis on which score function to use [41]. In fact, and unfortunately, regardless of the modeling assumptions, it has become a rather standard practice [67, 74, 4, 13, 32, 28] to simply use the least squares (LS) loss (a.k.a. \u201creconstruction loss\u201d) as the score by default, following the original paper by Zheng et al. [73], despite its known statistical limitations [63, 33, 1]. ", "page_idx": 1}, {"type": "text", "text": "As a result, Reisach et al. [55] flagged the empirical successes of continuous structure learning (CSL) methods as largely due to the high agreement between the order of marginal variances of the nodes and the topological order of the underlying simulated DAGs, a concept they describe as \u201cvarsortability\u201d. Then, Reisach et al. [55] empirically showed that the performance in structure recovery of CSL methods drops significantly after simple data standardization. More recently, $\\mathrm{Ng}$ et al. [42] demonstrated that this phenomenon may not be explained by varsortability, and instead pointed out that the explanations are due to the score function, albeit without proposing which score function to use. These observations motivate a deeper consideration of the choice of score. ", "page_idx": 1}, {"type": "text", "text": "Unfortunately, despite the fact that several score functions have been proposed for learning Bayesian networks (such as BIC [23], BDeu [35], and MDL [6]), their application to CSL methods is not well understood. This paper is precisely concerned with finding a suitable and general score function with strong statistical properties for CSL methods. That is, our objective is to find a score function that is: (1) intrinsically differentiable so that it is amenable to gradient-based optimization without approximations; (2) applicable to general models; (3) scale-invariant; (4) capable of identifying the sparsest model under proper regularization; and (5) connects nicely with classical concepts from Bayesian networks such as faithfulness and Markov equivalence classes. ", "page_idx": 1}, {"type": "text", "text": "Contributions. The main contribution of our work is to show that a properly regularized, likelihoodbased score function has the five properties outlined above. We begin with Gaussian models to convey the main ideas, and then discuss generalizations. In more detail: ", "page_idx": 1}, {"type": "text", "text": "1. (Section 4) Starting with Gaussian models, we show that using the log-likelihood with a quasi-MCP penalty (10) as the scoring function leads to optimal solutions of (1) that correspond to the sparsest DAG structure which is Markov to $P(X)$ (Theorem 1). Furthermore, under the faithfulness assumption, all optimal solutions are the sparsest within the same Markov equivalence class (Theorem 2).   \n2. (Section 5) We provide general conditions on the log-likelihood under which similar results hold for general models (Theorem 4).   \n3. (Section 4.5) We show that for Gaussian models, the log-likelihood score is scale-invariant. This means that rescaling or standardizing the data does not change the DAG structure (Theorem 3), and hence is not susceptible to varsortability.   \n4. We conduct experiments in multiple settings to evaluate the advantages of using a likelihoodbased scoring method. The findings from these experiments are detailed in in Section 6 and D. The empirical results support our theoretical claims: The likelihood-based score is robust and scale invariant. ", "page_idx": 1}, {"type": "text", "text": "2 Related work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Most methods for learning DAGs fall into two primary categories: Constraint-based algorithms, which depend on tests of conditional independence, and score-based algorithms, which aim to optimize a specific score or loss function. As our focus is on score-based methods, we only briefly mention classical constraint-based methods [59, 34, 62]. Within the umbrella of score-based methods, the linear Gaussian models is covered in works such as [1, 2, 19, 20, 37, 49], while studies on linear non-Gaussian SEMs are found in [33, 57]. Regarding nonlinear SEMs, significant contributions have been made in additive models [9, 15, 64], additive noise models [24, 49, 39], generalized linear models [47, 46, 22], and broader nonlinear SEMs [38, 21]. ", "page_idx": 1}, {"type": "text", "text": "Works that are more directly connected to our research include those developed in the continuous structure learning (CSL) framework [e.g. 73, 74, 13, 4, 14, 29, 76, 41, 40, 28, 44]. Most of these papers focus on empirical and computational aspects, and only a few study the theoretical properties of the CSL framework in (1). These include: [65, 43] studied the optimization and convergence subtleties of problem (1); [13] studied optimality guarantees for more general types of score functions and proposed a bi-level optimization method to guarantee local minima; [14] designed an optimization scheme that converges to the global minimum of the least squares score in the bivariate case. Finally, among the few works that study score functions under this framework, we note: [41] studied the properties of the $\\ell_{1}$ -regularized proflie log-likelihood, which leads to quasi-equivalent models to the ground-truth DAG; and the authors in [56] claim that a family of likelihood-based scores reduce to the least square loss, although this only holds under knowledge of the noise variances [33]. Perhaps most closely related to our work is [8], who proved a similar identifiability result under the likelihood score. However, they used an $\\ell_{0}$ regularizer along with the faithfulness assumption, which leads to an inherently non-differentiable optimization problem that is much simpler to analyze but requires approximations (e.g. Gumbel-Softmax) to optimize. On the other hand, they also consider interventional data, which we do not pursue in this work. Extending our results to include interventional data and interventional Markov equivalence is an important direction for future work. In contrast to the aforementioned works, we also prove that the log-likelihood has desirable properties such as being scale invariant, and when regularized by nonconvex and differentiable approximations of the $\\ell_{0}$ function, it provably leads to useful solutions that are minimal models and Markov equivalent to the underlying structure, without assuming faithfulness. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We let $G=(V,E)$ denote a directed graph on $p$ nodes, with vertex set $V=[p]:=\\{1,\\dots,p\\}$ and edge set $E\\subset V\\times V$ , where $(i,j)\\in E$ indicates the presence of a directed edge from node $i$ to node $j$ . We associate each node $i\\in V$ to a random variable $X_{i}$ , and let $\\boldsymbol{X}=(X_{1},\\bar{\\ldots},X_{p})$ . ", "page_idx": 2}, {"type": "text", "text": "Structural equation models (SEMs). An SEM $(X,f,P(N))$ over the random vector $X\\,=$ $(X_{1},\\allowbreak...,X_{p})$ is a collection of $p$ structural equations of the form: ", "page_idx": 2}, {"type": "equation", "text": "$$\nX_{j}=f_{j}(X,N_{j}),\\quad\\partial_{k}f_{j}=0\\;\\mathrm{if}\\;k\\notin\\mathrm{PA}_{j},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $f=(f_{j})_{j=1}^{p}$ is a collection of functions $f_{j}:\\mathbb{R}^{p+1}\\rightarrow\\mathbb{R}$ , here $N=(N_{1},\\ldots,N_{p})$ is a vector of independent noises with distribution $P(N)$ , and $\\mathrm{PA}_{j}$ denotes the set of parents of node $j$ . Here, $\\partial_{k}f_{j}$ denotes the partial derivative of $f_{j}$ w.r.t. $X_{k}$ , which is identically zero when $f_{j}$ is independent of $X_{k}$ , i.e. $f_{j}(X,N_{j})=f_{j}(\\mathrm{PA}_{j},N_{j})$ . The graphical structure induced by the SEM, assumed to be a DAG, will be represented by the following $p\\times p$ weighted adjacency matrix $B$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\nB=B(f),\\qquad B_{i j}=\\|\\partial_{i}f_{j}\\|_{2},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "and we use $G(B)$ to denote the corresponding binary adjacency matrix. For any set $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ of SEMs, let ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\mathcal{G}}(B):=\\{G(B(f)):(X,f,P(N))\\in B\\},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "i.e. $\\mathcal{G}(B)$ is the collection of all the DAGs implied by $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}_{\\boldsymbol{B}}$ . If $\\mathcal{D}$ is a set of DAGs and $\\boldsymbol{\\mathrm{\\Delta}}\\boldsymbol{\\mathrm{\\Omega}}^{\\beta}$ is a set of SEM, we also abuse notation by writing $\\mathcal{D}=\\mathcal{B}$ to indicate $\\mathcal{D}=\\mathcal{G}(\\boldsymbol{B})$ . ", "page_idx": 2}, {"type": "text", "text": "The SEM (2) is general enough to include many well-known models, such as linear SEMs [e.g., 33, 49], generalized linear models [47, 45, 18], and additive noise models [24, 51], post-nonlinear models [70, 71] and general nonlinear SEM [38, 21, 26, 74]. To illustrate some of these models: In linear SEMs we have $X_{j}=f_{j}(\\mathrm{PA}_{j})+N_{j}$ , where $f_{j}$ is a linear map; in causal additive models (CAM) we have $\\begin{array}{r}{X_{j}=\\sum_{k\\in\\mathrm{PA}_{j}}\\bar{f}_{j,k}(\\bar{X}_{k})+\\bar{N}_{j}}\\end{array}$ , where $f_{j,k}$ is a univariate function; in post-nonlinear models we have $X_{j}=f_{j,1}(f_{j,2}(\\mathrm{PA}_{j})+N_{j}).$ In fact, essentially any distribution can be represented as an SCM of the form (2); see Proposition 7.1 in Peters et al. [50]. ", "page_idx": 2}, {"type": "text", "text": "Faithfulness and sparsest representations. It is well-known that the DAG $G$ is not always identifiable from $X$ , and there is a well-developed theory on what can be identified based on $X$ under certain assumptions. This leads to the concepts of faithfulness and sparsest representations, which we briefly recall here; we refer the reader to [60, 48, 50] for details. Let $\\mathcal{T}(P)$ denote the set of conditional independence relations implioed by the distribution $P$ , and let ${\\mathcal{T}}(G)$ denote the set of $d$ -separations implied by the graph $G$ . Then $P$ is Markov to $G$ if ${\\mathcal{T}}(G)\\subset{\\mathcal{T}}(P)$ , and faithful to $G$ if $\\bar{\\mathcal{Z}}(P)\\bar{\\mathsf{\\Omega}}\\subset\\bar{\\mathcal{Z}}(G)$ . When both conditions hold, i.e. $\\qquad\\mathbb{Z}(P)=\\mathbb{Z}(G)$ , then $G$ is called a perfect map of $P$ . Following common convention, we will simply call $P$ faithful when $\\scriptstyle{\\mathcal{T}}(G)\\;=\\;{\\mathcal{T}}(P)$ . When $P$ is faithful to $G$ , the Markov equivalence class (MEC) of $G$ is identifiable and can be represented by a CPDAG. ", "page_idx": 2}, {"type": "text", "text": "Since faithfulness may not always hold, there has been progress in understanding what can be identified under weaker conditions. One approach which we will use is the notion of a sparsest (Markov) representation (SMR), introduced in [54]. A sparsest representation of $P$ is a Markovian DAG $G$ that has strictly fewer edges than any other Markovian DAG $G^{\\prime}$ , and such sparsest representation is unique up to Markov equivalence class. Theorem 2.4 in [54] shows that if $P$ is faithful to $G$ , then $G$ must be a sparsest representation of $P$ . This notion is closely related to the notion of minimality we adopt in Definition 2 (cf. Lemma 4 in the Appendix). These ideas can be generalized and weakened even further; see [31, 30] for details. ", "page_idx": 3}, {"type": "text", "text": "Parameters and the negative log-likelihood (NLL). For positive integers $m,s$ , we will use $\\psi\\in$ $\\Psi\\subseteq\\mathbb{R}^{m}$ and $\\xi\\in\\Xi\\subseteq\\mathbb{R}^{s}$ to denote the model parameters for $f=(f_{1},\\ldots,f_{p})$ and $N$ , respectively.2 Then we denote the distribution of $X$ by $P(X;\\psi,\\xi)$ . Let $\\textbf{x}\\in\\mathbb{R}^{p}$ denote one observation of $X$ . Given $n$ i.i.d. samples $\\mathbf{X}=(\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n})^{\\top}$ where $\\mathbf{x}_{i}\\sim P(X;\\psi,\\xi)$ , the negative log-likelihood and expected negative log-likelihood can be written as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\ell_{n}(\\psi,\\xi)=-\\frac{1}{n}\\sum_{i=1}^{n}\\log P(\\mathbf{x}_{i};\\psi,\\xi),\\qquad\\ell(\\psi,\\xi)=-\\mathbb{E}[\\log P(\\mathbf{x};\\psi,\\xi)],\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where the subscript $n$ in $\\ell_{n}$ is used to indicate the sample version of the log-likelihood. ", "page_idx": 3}, {"type": "text", "text": "Identifiability. Let $\\psi^{0}$ (resp. $\\xi^{0}$ ) denote the model parameters for the ground truth $f^{0}$ (resp. $N^{0}\\mathrm{,}$ ), let $B^{0}=B^{0}(\\psi^{0})\\in\\mathbb R^{p\\times p}$ denote the induced weighted adjacency matrix, and let $G(B^{0})\\in\\{0,1\\}^{p\\times p}$ denote the induced binary adjacency matrix. For example, in the general linear Gaussian model (6), $\\psi=B$ represents the adjacency matrix, and $\\xi=\\Omega$ denotes the variance of the Gaussian noise. In another case, if $f_{j}$ is approximated by a multilayer perceptron (MLP), with $N_{j}$ as Gaussian noise, then $\\psi$ includes all the parameters of the MLP, while $\\xi$ represents the variance of the Gaussian noise. Additionally, $(\\boldsymbol{B})_{i j}=[B(\\boldsymbol{\\psi})]_{i j}=\\|\\mathbf{i}$ -th column of $A_{j}^{(1)}\\|$ , where $A_{j}^{(1)}$ is the first hidden layer in $f_{j}$ [74]. Thus, by our definitions, $P(X;\\psi^{0},\\xi^{0})$ is the true distribution. Here, there are two types of identifiability questions: ", "page_idx": 3}, {"type": "text", "text": "1. Parameter identifiability: Is it possible to uniquely determine the parameters $(\\psi^{0},\\xi^{0})$ based on observations from $P(X;\\psi^{0},\\xi^{0})!$ ? Formally, is there any $(\\widetilde{\\psi},\\widetilde{\\xi})\\neq(\\psi^{0},\\dot{\\xi}^{0})$ , such that $P(X,\\psi^{0},\\xi^{0})=P(X,\\widetilde\\psi,\\widetilde\\xi)$ almost surely?   \n2. Structural identifiability: Is it possible to uniquely determine the DAG $G(B^{0})$ based on observations from $P(X;\\psi^{0},\\xi^{0})?$ In other words, is there any $(\\widetilde{\\psi},\\widetilde{\\xi})\\neq(\\psi^{0},\\xi^{0})$ such that $P(X,\\psi^{0},\\xi^{0})=P(X,\\widetilde\\psi,\\widetilde\\xi)$ but $G(B^{0})\\neq G(B(\\tilde{\\psi}))$ . ", "page_idx": 3}, {"type": "text", "text": "In general, parameter identifiability implies structural identifiability since the ability to uniquely determine parameter values often means that the structure they induce is also identifiable. However, the converse is not generally true, i.e. structural identifiability does not always imply parameter identifiability, as different parameter values can lead to the same structure. Classical results on identifiability of SEMs include: linear SEM with equal variance [33], linear SEM with non-Gaussian noises [57, 58], causal additive models with Gaussian noises [9], additive models with continuous noise [51], and post-nonlinear models [70, 71, 25]. ", "page_idx": 3}, {"type": "text", "text": "In models where parameter identifiability is possible, the population NLL $\\ell(\\psi,\\xi)$ serves as a natural choice for the score function because it attains a unique minimum at the true parameters $(\\psi^{0},\\xi^{0})$ . However, this approach is not straightforward for nonidentifiable models, where multiple parameter sets can induce the same data distribution $P(X;\\psi^{0},\\xi^{0})$ , leading to ambiguities in parameter or structure estimation. In such cases, regularizing the log-likelihood can alleviate this issue. These regularizers enforce specific characteristics like sparsity, guiding the model towards more meaningful solutions (e.g. faithful or sparsest), despite the lack of identifiability. ", "page_idx": 3}, {"type": "text", "text": "4 General linear Gaussian SEMs: A nonidentifiable model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Although our results apply to general models, we begin by outlining the main idea with one of the simplest nonidentifiable models, the Gaussian model. Our goal in this section is to theoretically show how the NLL with nonconvex differentiable regularizers can lead to meaningful solutions such as minimal-edge models and elements of Markov equivalent classes. We also discuss and prove the scale invariance of NLL, making it amenable to CSL approaches and addressing concerns raised in previous work [55, 42]. Then, in Section 5, we extend these results to general models. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "4.1 Gaussian DAG models ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "A linear SEM $(B,\\Omega)$ over $X$ with independent Gaussian noises $N$ , a special case of (2), is wellknown to be nonidentifiable in terms of parameters and structure [see 2, for discussion]. We write the model as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\boldsymbol{X}=\\boldsymbol{B}^{\\intercal}\\boldsymbol{X}+\\boldsymbol{N},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\b{B}\\in\\mathbb{R}^{p\\times p}$ is a matrix of coefficients with $G(B)$ being a DAG, and $N\\in\\mathbb{R}^{p}$ is the vector of independent noises with covariance matrix $\\Omega=\\mathrm{diag}(\\omega_{1}^{2},\\dots,\\omega_{p}^{2})$ .3 ", "page_idx": 4}, {"type": "text", "text": "Given the model (6) it is easy to see that the distribution $P(X)$ is Gaussian and is fully characterized by the pair $(B,\\Omega)$ . That is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\ensuremath{\\boldsymbol{X}}\\sim\\ensuremath{\\boldsymbol{\\mathcal{N}}}(0,\\Sigma),\\quad\\Sigma=\\Sigma_{f}(\\ensuremath{\\boldsymbol{B}},\\Omega):=(I-\\ensuremath{\\boldsymbol{B}})^{-\\top}\\Omega(I-\\ensuremath{\\boldsymbol{B}})^{-1},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\Sigma$ is the covariance matrix of $X$ . In the sequel, we use the subscript $f$ to refer to a function. In this case, $\\Sigma_{f}$ denotes a function with arguments $(B,\\Omega)$ and returns the covariance matrix. Moreover, we use $\\Theta$ to denote the corresponding precision matrix (inverse of the covariance matrix): ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Theta=\\Theta_{f}(B,\\Omega):=(I-B)\\Omega^{-1}(I-B)^{\\top}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Let $\\mathbf{X}\\,=\\,(\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n})^{\\top}$ be $n$ i.i.d. samples of $X$ . Then, let the sample covariance matrix be $\\begin{array}{r}{\\widehat{\\Sigma}=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}}\\end{array}$ . The sample NLL function is given by: ", "page_idx": 4}, {"type": "equation", "text": "$$\n{\\displaystyle{\\bf\\Gamma}_{n}(B,\\Omega)=-\\frac{1}{n}\\log\\prod_{i=1}^{n}P({\\bf x}_{i};B,\\Omega)=\\frac{1}{2}\\log\\operatorname*{det}\\Omega-\\log\\operatorname*{det}(I-B)+\\frac{1}{2}\\operatorname*{Tr}(\\widehat\\Sigma\\Theta(B,\\Omega))+\\mathrm{const}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The corresponding population NLL function is ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\ell(B,\\Omega)=-\\mathbb{E}_{X}\\log P(X;B,\\Omega)=\\frac{1}{2}\\log\\operatorname*{det}\\Omega-\\log\\operatorname*{det}(I-B)+\\frac{1}{2}\\operatorname{Tr}(\\Sigma\\Theta(B,\\Omega))+\\mathrm{const}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The full derivation can be found in Appendix C.1. Here, it is important to note that the distribution of $X$ is fully determined by either the precision matrix $\\Theta$ or the covariance matrix $\\Sigma$ . ", "page_idx": 4}, {"type": "text", "text": "4.2 Equivalence and nonidentifiability ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our goal is to identify $(B,\\Omega)$ : Unfortunately, the model is inherently nonidentifiable in terms of both parameter and structure. This means that multiple pairs $(B,\\Omega)$ for model (6) can induce the same data distribution $P(X)$ given in (7), thus resulting also in the same precision matrix $\\Theta$ . To address this, we define the equivalence class $\\mathcal{E}(\\Theta)$ as the set of all pairs $(B,\\Omega)$ such that $\\Theta_{f}(B,\\Omega)=\\Theta$ . ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal E(\\Theta):=\\{(B,\\Omega):\\Theta_{f}(B,\\Omega)=\\Theta\\}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "It is worth noting that the size of $\\mathcal{E}(\\Theta)$ is finite and at most $p!$ , which corresponds to the number of permutations for $p$ variables [2]. For more comprehensive details on this class, see Appendix C.2. ", "page_idx": 4}, {"type": "text", "text": "This ambiguity naturally leads to the question: which pair $(B,\\Omega)$ should we estimate? Since any pair would be indistinguishable based only on observational data, a natural objective is to estimate the \u201csimplest\u201d DAG, for example, a DAG that induces the precision matrix $\\Theta$ with the smallest number of edges. In other words, our goal is to estimate the matrix $B$ that has the minimal number of nonzero entries in the equivalence class. Let $s_{B}=|\\{(i,j):B_{i j}\\neq0\\}|$ . ", "page_idx": 4}, {"type": "text", "text": "Definition 2 (Minimality). $(B,\\Omega)$ is called a minimal-edge $I.$ -map4 in the equivalence class $\\mathcal{E}(\\Theta)$ $i f s_{B}\\leq s_{\\widetilde{B}},\\forall(\\widetilde{B},\\widetilde{\\Omega})\\in\\mathcal{E}(\\Theta)$ . The set of all minimal-edge $I.$ -maps in the equivalence class $\\mathcal{E}(\\Theta)$ is referred to as the minimal equivalence class $\\ensuremath{\\mathcal{E}_{\\mathrm{min}}}(\\Theta)$ : ", "page_idx": 4}, {"type": "text", "text": "In the sequel, for brevity, we will often refer to such models as \u201cminimal models\u201d. ", "page_idx": 5}, {"type": "text", "text": "Unlike faithfulness, which may not always hold, the minimal equivalence class $\\ensuremath{\\mathcal{E}_{\\mathrm{min}}}(\\Theta)$ is always well-defined. Moreover, as detailed in Lemma 4 in the Appendix, Definition 2 is closely related to the SMR assumption [54]: Under the SMR assumption (and hence also faithfulness) for $G$ , we have $\\mathcal{M}(G)=\\mathcal{E}_{\\operatorname*{min}}(\\Theta).$ , i.e., $\\ensuremath{\\mathcal{E}_{\\mathrm{min}}}(\\Theta)$ is the Markov equivalence class of $G$ . However, there could be multiple pairs $(B,\\Omega)$ within $\\mathcal{E}_{\\mathrm{min}}(\\Theta)$ . Nevertheless, our goal is to recover one element from $\\ensuremath{\\mathcal{E}_{\\mathrm{min}}}(\\Theta)$ . The elements in $\\ensuremath{\\mathcal{E}_{\\mathrm{min}}}(\\Theta)$ not only represent the \u201csimplest\u201d DAG model for $X$ in terms of edge count, but also bear a deep connection to classical notions such as Markov equivalence. For example, under faithfulness, all these elements describe the same independence statements. ", "page_idx": 5}, {"type": "text", "text": "Lemma 1. Let $X$ follow model (6) with $(B^{0},\\Omega^{0})$ and $\\Theta^{0}=\\Theta_{f}(B^{0},\\Omega^{0})$ . Assume that $P(X)$ is faithful to $G^{0}:=G(B^{0})$ . Then $\\mathcal{M}(G^{0})=\\mathcal{E}_{\\operatorname*{min}}(\\Theta^{0})$ . ", "page_idx": 5}, {"type": "text", "text": "Recall our convention that this means that $\\mathcal{M}(G^{0})=\\mathcal{G}(\\mathcal{E}_{\\operatorname*{min}}(\\Theta^{0}))$ , i.e. the DAG structures contained in $\\mathcal{E}_{\\mathrm{min}}(\\Theta^{0})$ coincide with $\\mathcal{M}(G^{0})$ . Thus, under the faithfulness assumption, recovering $\\mathcal{E}_{\\mathrm{min}}(\\Theta^{0})$ is the same as recovering the MEC, which is the usual goal in causal discovery. Moreover, we emphasize that these apply generally: For non-Gaussian $X$ following the model specified in (2), the same conclusion can be made; see Lemma 3 in the Appendix. Finally, we note that the commonly used LS loss does not have the same minimizers as the log-likelihood when the noise variances are different; see Appendix C.3. ", "page_idx": 5}, {"type": "text", "text": "4.3 Regularization ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In order to distinguish elements in $\\mathcal{E}(\\Theta)$ from the minimal elements in $\\ensuremath{\\mathcal{E}_{\\mathrm{min}}}(\\Theta)$ , we need to somehow account for the number of edges when evaluating the score function. The common approach to this is to use BIC, or equivalently the $\\ell_{0}$ penalty. Although both approaches effectively penalize the number of nonzero entries in $B$ , their non-differentiability makes them unsuitable for differentiable structure learning. The $\\ell_{1}$ penalty, while amenable to differentiable approaches,5 is not effective in precisely counting the number of edges, and also biased in parameter estimation6. To mitigate these shortcomings alternatives such as the smoothly clipped absolute deviation (SCAD) penalty [16] and the minimax concave penalty (MCP) [69] have been proposed. We choose to use a reparametrized version of MCP, termed quasi-MCP, defined as follows: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathrm{quasi-MCP}:\\qquad p_{\\lambda,\\delta}(t)=\\lambda[(|t|-\\frac{t^{2}}{2\\delta})\\mathbb{1}(|t|<\\delta)+\\frac{\\delta}{2}\\mathbb{1}(|t|>\\delta)]\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Here, $\\mathbb{1}(\\cdot)$ is the indicator function; corresponding plot can be found in Appendix C.5. Similar to MCP, quasi-MCP is a symmetric function that takes on a quadratic form between $[0,\\delta]$ and remains constant for values greater than $\\delta$ . The function is smooth, and for values $|t|>\\delta$ , it approximates the behavior of the $\\ell_{0}$ penalty, thus serving to penalize the number of non-zero coefficients in $B$ . ", "page_idx": 5}, {"type": "text", "text": "The score function in (1) can be naturally written as ", "page_idx": 5}, {"type": "equation", "text": "$$\ns(B,\\boldsymbol{\\Omega};\\lambda,\\delta,\\mathbf{X})=\\ell_{n}(B,\\boldsymbol{\\Omega})+p_{\\lambda,\\delta}(B)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\begin{array}{r}{p_{\\lambda,\\delta}(B)=\\sum_{i\\neq j}p_{\\lambda,\\delta}(B_{i j})}\\end{array}$ . Then, the optimization problem can be written as ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{B,\\Omega}s(B,\\Omega;\\lambda,\\delta,{\\mathbf X})\\quad\\mathrm{subject}\\;\\mathrm{to}\\quad h(B)=0,\\;\\Omega>0.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "It is worth noting that for any $B$ , the corresponding optimal $\\Omega$ that minimizes $s(B,\\Omega;\\lambda,\\delta,{\\bf X})$ can be easily be expressed in terms of $B$ as $\\Omega_{f}(B)$ (see Appendix C.1). Therefore, we can always plug $\\Omega_{f}(B)$ into (12) to profile out $\\Omega$ . ", "page_idx": 5}, {"type": "text", "text": "4.4 Provably recovering minimal models ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Solving problem (12) requires minimizing $\\ell_{n}(B,\\Omega)$ and $p_{\\lambda,\\delta}$ simultaneously. To study the behavior of these minimizers, let us define the set of global minimizers, ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\mathcal{O}_{n,\\lambda,\\delta}=\\{(B^{*},\\Omega^{*}):(B^{*},\\Omega^{*})\\mathrm{~is~a~minimizer~of~}(12)\\}.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Ideally, we would like $\\mathcal{O}_{n,\\lambda,\\delta}=\\mathcal{E}_{\\mathrm{min}}(\\Theta^{0})$ , however, it is unclear whether there exist values of $\\lambda$ and $\\delta$ such that any optimal solution $(B^{*},\\Omega^{*})$ lies within $\\ensuremath{\\mathcal{E}_{\\mathrm{min}}}(\\Theta)$ . The following theorem provides an affirmative answer to this question. In the sequel, we say that a property $S(x)$ holds for all sufficiently small $x>0$ if there is some fixed $\\epsilon>0$ such that for every $x\\le\\epsilon$ , the property $S(x)$ holds. ", "page_idx": 6}, {"type": "text", "text": "Theorem 1. Let $X$ follow model (6) with $(B^{0},\\Omega^{0})$ and $\\Theta^{0}=\\Theta_{f}(B^{0},\\Omega^{0})$ . Let X be n i.i.d. samples from $P(X)$ , and $\\mathcal{O}_{n,\\lambda,\\delta}$ be defined as in (13). Then, for all sufficiently small $\\lambda,\\delta>0$ (independent of $n$ ), it holds that $P(\\mathcal O_{n,\\lambda,\\delta}=\\mathcal{E}_{\\mathrm{min}}(\\Theta^{0}))\\to1$ as $n\\to\\infty$ . ", "page_idx": 6}, {"type": "text", "text": "In other words, we can always guarantee that $\\mathcal{O}_{n,\\lambda,\\delta}=\\mathcal{E}_{\\operatorname*{min}}(\\Theta^{0})$ by taking $\\lambda,\\delta$ sufficiently small, which is easily accomplished in practice. In the following, we use the superscript 0 to denote ground truth parameters. Additionally, we can assume that $B^{0}$ always belongs to $\\bar{\\mathcal{E}}_{\\operatorname*{min}}(\\bar{\\Theta}^{0})$ , ensuring that our reference to the ground truth aligns with the simplest or minimal representation within the equivalence class. Moreover, by Lemma 1, under the faithfulness assumption, Theorem 1 can be interpreted as recovering the Markov equivalence class $\\mathcal{M}(G^{0})$ : ", "page_idx": 6}, {"type": "text", "text": "Theorem 2. Consider the setup in Theorem $^{\\,l}$ and assume additionally that $P(X)$ is faithful to $G^{0}:=G(B^{0})$ . Then, for all sufficiently small $\\lambda,\\delta>0$ (independent of $n$ ), it holds that $P(\\mathcal{O}_{n,\\lambda,\\delta}=$ $\\mathcal{M}(G^{0}))\\to1$ as $n\\to\\infty$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 2 indicates with properly chosen hyperparameters, the optimal solution from optimization (12) will produce a graph that adheres to the same independence statements as $G^{0}$ . This implies that the structure learned through the optimization process accurately reflect the underlying causal or conditional independence structure of underlying data generating process. ", "page_idx": 6}, {"type": "text", "text": "Remark 1. Although we use quasi-MCP (mainly for its simplicity), it turns out MCP or SCAD can also be used. See Corollary 1 in Appendix A for details. ", "page_idx": 6}, {"type": "text", "text": "4.5 Scale invariance and standardization ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "It is known that the LS loss is not scale-invariant, i.e. re-scaling the data (and in particular, standardizing it) can drastically change the structure [33], a fact which Reisach et al. [55] use to argue that differentiable DAG learning with the LS Loss is also not scale-invariant. Here we show that by using a different score\u2014in this case the log-likelihood\u2014fixes this and results in (provable) scale invariance. Thus, the choice of score function is crucial if certain properties such as scale invariance are desired. The following result restates the well-known fact that Gaussian DAGs are invariant to re-scaling (i.e. re-scaling does not change the support for any $(B,\\Omega)\\in\\mathcal{E}(\\Theta))$ ) using our notation: ", "page_idx": 6}, {"type": "text", "text": "Lemma 2. Let $\\boldsymbol{X}\\sim\\mathcal{N}(\\boldsymbol{0},\\boldsymbol{\\Sigma})$ , suppose $\\Sigma$ is a positive definite covariance matrix and let $\\Theta:=\\Sigma^{-1}$ , suppose $D$ is a diagonal matrix with positive diagonal entries. Then $\\mathcal{G}(\\mathcal{E}(\\Theta))=\\mathcal{G}(\\mathcal{E}(D\\Theta D))$ . ", "page_idx": 6}, {"type": "text", "text": "Lemma 2 has appealing consequences for standardization. Given raw data $\\mathbf{X}$ , denote its standardized version by $\\mathbf{Z}$ (cf. Appendix C.6). Ideally, structure learning algorithms will output the same structure whether $\\mathbf{X}$ or $\\mathbf{Z}$ is used as input, and Lemma 2 suggests that re-scaling $\\mathbf{X}$ will not alter the structure of the DAG that is recovered from optimizing (12). The following theorem formalizes this: ", "page_idx": 6}, {"type": "text", "text": "Theorem 3. Under the same setting as Theorem $^{l}$ , the solutions to (12) are scale-invariant. That is, for any $n\\geq0$ , let ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{O}_{n,\\lambda,\\delta}(\\mathbf{X})=\\{(B^{*},\\Omega^{*}):(B^{*},\\Omega^{*})\\;i s\\;a\\;m i n i m i z e r\\;o f\\;(12)\\;w i t h\\;d a t a\\;\\mathbf{X}\\},}\\\\ &{\\mathcal{O}_{n,\\lambda,\\delta}(\\mathbf{Z})=\\{(B^{*},\\Omega^{*}):(B^{*},\\Omega^{*})\\;i s\\;a\\;m i n i m i z e r\\;o f\\;(12)\\;w i t h\\;d a t a\\;\\mathbf{Z}\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathbf{Z}$ is the standardized version of $\\mathbf{X}$ . Then, for all sufficiently small $\\lambda,\\delta\\geq0$ and all $n$ , we have $\\mathcal{G}(\\mathcal{O}_{n,\\lambda,\\delta}(\\mathbf{X}))=\\mathcal{G}(\\mathcal{O}_{n,\\lambda,\\delta}(\\mathbf{Z}))$ . Moreover, for all sufficiently small $\\lambda,\\delta>0$ we have ", "page_idx": 6}, {"type": "equation", "text": "$$\nP\\left[\\mathcal{G}(\\mathcal{O}_{n,\\lambda,\\delta}(\\mathbf{X}))=\\mathcal{G}(\\mathcal{O}_{n,\\lambda,\\delta}(\\mathbf{Z}))=\\mathcal{G}(\\mathcal{E}_{\\operatorname*{min}}(\\Theta_{f}(B^{0},\\Omega^{0})))\\right]\\to1\\quad a s\\;n\\to\\infty.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Thus, even on finite samples, the set of DAG structures $\\mathcal{G}(O_{n,\\lambda,\\delta}(\\mathbf{X}))$ derived from the raw (unstandardized) data $\\mathbf{X}$ will always be the same as $\\mathcal{G}(\\mathcal{O}_{n,\\lambda,\\delta}(\\mathbf{\\dot{Z}}))$ , which is derived from standardized data $\\mathbf{Z}$ . As a result, standardizing Gaussian data does not affect the recovered DAG structure if the optimization problem (12) can be solved exactly. ", "page_idx": 6}, {"type": "text", "text": "Remark 2. Theorem 3 applies to global optimization of the objective (12). Of course, in practice, algorithms can get stuck in local optima, but the global solutions (even for finite samples $n$ ) will always be scale invariant. ", "page_idx": 6}, {"type": "text", "text": "5 Nonconvex regularized log-likelihood for general models ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The results in the previous section are not specific to Gaussian models, although this helps with interpretability in a familiar setting. We now extend these results from linear Gaussian SEMs to more general SEMs. Here, we assume that $X$ follows model (2) and the induced distribution is denoted by ${\\bar{P}}(X;\\psi^{0},\\xi^{0})$ . Let us define the equivalence class $\\mathcal{E}(\\psi^{0},\\xi^{0})$ , ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{E}(\\psi^{0},\\xi^{0})=\\{(\\psi,\\xi):P(x;\\psi,\\xi)=P(x;\\psi^{0},\\xi^{0}),\\forall x\\in\\mathbb{R}^{p}\\}.\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "That is, $\\mathcal{E}(\\psi^{0},\\xi^{0})$ is a set of pairs $(\\psi,\\xi)$ that induce the same distribution $P(X;\\psi^{0},\\xi^{0})$ . As a result, any pair $(\\psi,\\xi)$ within this equivalence class will be a minimizer of the NLL $\\ell(\\psi,\\xi)$ . Analogously to Definition 2, we can also define the collection of minimal elements in the equivalence class $\\bar{\\mathcal{E}}(\\psi^{0},\\bar{\\xi}^{0})$ . ", "page_idx": 7}, {"type": "text", "text": "Definition 3. $(\\psi,\\xi)$ is called a minimal-edge $I$ -map in the equivalence class $\\mathcal{E}(\\psi^{0},\\xi^{0})\\;i f s_{B(\\psi)}\\leq$ $s_{B(\\widetilde{\\psi})},\\forall(\\widetilde{\\psi},\\widetilde{\\xi})\\in\\mathcal{E}(\\psi^{0},\\xi^{0})$ . We further define ", "page_idx": 7}, {"type": "text", "text": "Here, it is crucial that our concept of minimality concerns $s_{B(\\psi)}$ , which is the number of nonzero entries in the weighted adjacency matrix $B(\\psi)$ , rather than the number of nonzero entries in the parameter $\\psi$ itself. Therefore, $s_{B(\\psi)}$ essentially counts the number of edges in the adjacency matrix. ", "page_idx": 7}, {"type": "text", "text": "Assumption A. $(I)\\,|\\mathcal{E}(\\psi^{0},\\xi^{0})|$ is finite. (2) $B(\\psi)$ is $L$ -Lipschitz w.r.t. $\\psi_{i}$ , i.e. $\\begin{array}{r}{\\frac{\\|B(\\psi_{1})-B(\\psi_{2})\\|_{2}}{\\|\\psi_{1}-\\psi_{2}\\|_{2}}\\leq L.}\\end{array}$ . Assumption B. For any $\\alpha$ such that $\\ell(\\psi^{0},\\xi^{0})<\\alpha$ , the level set $\\{(\\psi,\\xi):\\ell(\\psi,\\xi)\\leq\\alpha\\}$ is bounded, where $\\ell(\\psi,\\xi)$ is the expected NLL defined in (5). ", "page_idx": 7}, {"type": "text", "text": "Assumption A(1) is relatively mild; it requires that the equivalence class contains only finitely many points. This assumption is satisfied by Gaussian models, generalized linear models with continuous output [66], binary output [74, 13], and most exponential families. It is also obviously satisfied by any identifiable model since $|\\mathcal{E}(\\psi^{0},\\xi^{0})|\\,=\\,1$ . Assumption A(2) is a mild continuity requirement on $B(\\psi)$ . Assumption B simply guarantees that the optimization problem has a minimizer, and is standard [7]. More discussions about the assumptions are included in Appendix C.7. Without this type of assumption, score-based learning is not even well-defined. ", "page_idx": 7}, {"type": "text", "text": "Similar in spirit to Theorem 1, we can show that by combining the NLL with quasi-MCP for appropriate $\\lambda,\\delta$ , solving the following problem, we recover elements of $\\mathcal{E}_{\\mathrm{min}}(\\psi^{0},\\xi^{0})$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\psi\\in\\Psi,\\xi\\in\\Xi}\\ell_{n}(\\psi,\\xi)+p_{\\lambda,\\delta}(B(\\psi))\\quad\\mathrm{subject}\\;\\mathrm{to}\\quad h(B(\\psi))=0,\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $p_{\\lambda,\\delta}(\\cdot)$ is quasi-MCP defined in (10). Next, define its set of global minimizers. ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathcal{O}_{n,\\lambda,\\delta}=\\{(\\psi^{*},\\xi^{*}):(\\psi^{*},\\xi^{*})\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Theorem 4. Let $X$ follow model (2) with parameters $(\\psi^{0},\\xi^{0})$ and let $\\mathbf{X}$ be n i.i.d. samples from $P(X;\\psi^{0},\\xi^{0})$ . Under Assumptions A-B, for all sufficiently small $\\lambda,\\delta>0$ (independent of $n$ ), it holds that $P(\\mathcal{O}_{n,\\lambda,\\delta}=\\mathcal{E}_{\\mathrm{min}}(\\psi^{0},\\bar{\\xi^{0}}))\\to1$ as $n\\to\\infty$ . ", "page_idx": 7}, {"type": "text", "text": "Theorem 5. Under the setting in Theorem 4 and assuming that $P(X;\\xi^{0},\\psi^{0})$ is faithful with respect to $G^{0}\\,:=\\,G(B(\\psi^{0}))$ . Then, for all sufficiently small $\\lambda,\\delta\\,>\\,0$ (independent of $n$ ), it holds that $P(\\mathcal{O}_{n,\\lambda,\\delta}=\\mathcal{M}(G^{0}))\\to1$ as $n\\to\\infty$ . ", "page_idx": 7}, {"type": "text", "text": "6 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "To solve (12) and (14), we employ the augmented Lagrangian algorithm [5] from NOTEARS [73, 74], modifying their least squares score with $\\ell_{1}$ penalty into the log-likelihood with MCP (10). We compare our approach to relevant baselines, e.g. NOTEARS [73], GOLEM [41], DAGMA [4], VarSort [55], FGES [52] and PC [59]. For our variation of NOTEARS that employs a score function based on the NLL with MCP, we name it as LOGLL-NOTEARS. The suffixes \u2018POPULATION\u2019 and \u2018SAMPLE\u2019 denote the use of the population and sample covariance matrix, respectively. Full details of the experiments are given in Appendix D. ", "page_idx": 7}, {"type": "text", "text": "Our primary empirical results are shown in Figures 1 and 2. We use the structural Hamming distance (SHD) as the main metric to evaluate the difference between the estimated graph and the ground truth graph. Lower SHD values indicate better estimation accuracy. Given that the model specified in (6) is nonidentifiable, we compare the CPDAGs of the estimated graph and the ground truth graph. ", "page_idx": 8}, {"type": "text", "text": "In Figure 1(a), we observe that using the NLL $+\\mathbf{M}\\mathbf{C}\\mathbf{P}$ achieves the best performance for the different types of graphs and ranks second best for sparse graphs {ER1, SF1}. In Figure 1(b), standardizing $\\mathbf{X}$ significantly impacts the performance of GOLEM, NOTEARS, and DAGMA; the SHD values are not any better than an empty graph, exactly as predicted by prior theory. The performance of LOGLL-NOTEARS-SAMPLE and LOGLL-NOTEARS-POPULATION are also affected by standardization, but these methods remain robust and continue to make meaningful discoveries. It is important to note that this observation does not contradict our Lemma 2. The challenges arise because solving the optimization problems (12) and (14) to find global solutions becomes inherently difficult as $p$ increases.To verify the scale invariance property in Theorem 3, we also conduct experiments on small graphs and include exact method that solve (12) and (14) to global optimal, see Figure 5. ", "page_idx": 8}, {"type": "text", "text": "In Figure 2, we replicate the Figure 1 in [55], providing a more direct comparison between various methods applied to raw data $({\\mathbf X})$ and standardized data ( $\\mathbf{X}$ standardized). We include VarSort (referred to as sortnregress in [55]) as a baseline. Notably, for smaller graphs $(p=10)$ ), both LOGLL(- NOTEARS)-SAMPLE and LOGLL(-NOTEARS)-POPULATION exhibit the scale-invariant property alongside PC and FGES, in alignment with Lemma 2. This contrasts sharply with other methods, which completely deteriorate. For larger graphs $(p=50)$ ), standardizing the data mildly degrades the performance of LOGLL(-NOTEARS)-SAMPLE and LOGLL(-NOTEARS)-POPULATION. This can be attributed to the increased complexity of optimization as the size of the graph grows. ", "page_idx": 8}, {"type": "text", "text": "In Figure 3, we use a concrete toy example to investigate two key factors in the implementation: (1) the impact of random initialization, and (2) the upper limit for $\\delta_{0}$ that can be applied according to Theorem 1. We generate $10^{5}$ initializations $B_{\\mathrm{random}}$ with weight for each edge uniformly sampled within $[-5,5]$ , and perform optimization using LOGLL-NOTEARS starting from these points. The \u201cmaximal $\\delta\"$ is the theoretical maximum $\\delta_{0}$ that ensures the validity of Theorem 1. We computed the SHD and the distances between the estimated $\\mathcal{M}(B_{\\mathrm{est}})$ and $\\dot{\\mathcal{M}}(B^{0})$ . The red line in Figure 3 represents the average SHD and distances. The distribution of these $10^{5}$ estimated SHD and distances is visualized using dots of varying sizes, where larger dots indicate a higher frequency of points. In some cases where SHD takes a value of $-1$ , this value is used to indicate that the estimated $B_{\\mathrm{est}}$ does not form a valid DAG, which is an artifact of thresholding and affects $<0.5\\%$ of models. For the remaining models, the optimization (12) can typically be solved very close to a globally optimal, and according to Theorem 2, the SHD should ideally be zero, which is consistent with the figure. ", "page_idx": 8}, {"type": "text", "text": "Our results are not limited to the linear model with Gaussian noise. In Appendix E.3, we provide additional experiments on a logistic model (binary $X_{j}$ ) and neural networks. Further details on the experimental settings and additional experiments can be found in Appendix D and E. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Continuous score-based structure learning is a relative newcomer to the literature on causal structure learning, which goes back several decades. It has attracted significant attention due to its simplicity and generality, however, its theoretical properties are often misunderstood. We have sought to fill in this gap by studying its statistical aspects (to complement ongoing computational studies, e.g. [41, 65, 4, 13, 14, 43]). To this end, we proposed a fully differentiable score function for structure learning, composed of log-likelihood and quasi-MCP. We demonstrated that the global solution corresponds to the sparsest DAG structure that is Markov to the data distribution. Under mild assumptions, we conclude that all optimal solutions are the sparsest within the same Markov equivalence class. Additionally, the proposed score is scale-invariant, producing the same structure regardless of the data scale under the linear Gaussian model. Experimental results validate our theory, showing that our score provides better and more robust structure recovery compared to other scores. ", "page_idx": 8}, {"type": "text", "text": "We hope that this work stimulates further statistical inquiry into the properties of CSL. For example, we have focused on parametric models, and left extensions to nonparametric models to future work. Certain assumptions such as the finiteness of the equivalence class and the boundedness of the level set of the log-likelihood become more interesting in this regime. We have mentioned already that extensions to richer data types including interventions is an important direction. It would be of great interest to explore ways to relax our assumptions to expand our statistical understanding of CSL in broader scenarios. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "image", "img_path": "TMlGQw7EbC/tmp/059b74eb658cfeae2ce486018cf5361a9c7353a4b88720fa74a58b352056e6dd.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "Figure 1: Results in terms of SHD between MECs of estimated graph and ground truth. Lower is better. Column: $k=\\{1,2,4\\}$ . Row: random graph types. {ER,SF}- $k=$ {Scale-Free,Erd\u02ddos-R\u00e9nyi } graphs with $k d$ expected edges. Here $p=\\{10,2\\bar{0},50,70,100\\}$ , $n=1000$ . ", "page_idx": 9}, {"type": "image", "img_path": "TMlGQw7EbC/tmp/f6cc11c5c4c919bf08c817e89069c2e5c6795c848803ca00d2872aaa9208f9aa.jpg", "img_caption": [], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "TMlGQw7EbC/tmp/338e692eb9210226cb22ced365178b268be60a51640717fecf980b3103601b95.jpg", "img_caption": ["Figure 2: Comparison of raw (orange) vs. standardized (green) data. SHD (lower is better) between Markov equivalence classes (MEC) of recovered and ground truth graphs for ER-2 graphs with 10 (left) or 50 (right) nodes. In (b), SHD for VarSort with standardized data is omitted due to its average exceeding 300. "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "TMlGQw7EbC/tmp/13442668a58754226c9f8d6e19c3ed8bddac11d3c4d1b04d5cc9af1cca18f1d1.jpg", "img_caption": ["Figure 3: Graph: fork structure $X_{0}\\ \\to\\ X_{1}$ and $X_{0}\\ \\rightarrow\\ X_{2}$ . For $0~<~\\delta~<~\\delta_{0}$ , the estimated $(\\bar{B_{\\mathrm{est}}},\\bar{\\Omega}_{\\mathrm{est}})\\in\\bar{\\mathcal{E}_{\\mathrm{min}}}(\\Theta^{0})$ because SHD and distance are close to 0. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Aragam, B., Amini, A. and Zhou, Q. [2019], \u2018Globally optimal score-based learning of directed acyclic graphs in high-dimensions\u2019, Advances in Neural Information Processing Systems 32. [2] Aragam, B. and Zhou, Q. [2015], \u2018Concave penalized estimation of sparse Gaussian Bayesian networks\u2019, The Journal of Machine Learning Research 16(1), 2273\u20132328.   \n[3] Barab\u00e1si, A.-L. and Albert, R. [1999], \u2018Emergence of scaling in random networks\u2019, science 286(5439), 509\u2013512. [4] Bello, K., Aragam, B. and Ravikumar, P. [2022], \u2018Dagma: Learning dags via m-matrices and a log-determinant acyclicity characterization\u2019, Advances in Neural Information Processing Systems 35, 8226\u20138239.   \n[5] Bertsekas, D. P. [1997], \u2018Nonlinear programming\u2019, Journal of the Operational Research Society 48(3), 334\u2013334.   \n[6] Bouckaert, R. R. [1993], Probabilistic network construction using the minimum description length principle, in \u2018European conference on symbolic and quantitative approaches to reasoning and uncertainty\u2019, Springer, pp. 41\u201348.   \n[7] Boyd, S., Boyd, S. P. and Vandenberghe, L. [2004], Convex optimization, Cambridge university press.   \n[8] Brouillard, P., Lachapelle, S., Lacoste, A., Lacoste-Julien, S. and Drouin, A. [2020], \u2018Differentiable causal discovery from interventional data\u2019, Advances in Neural Information Processing Systems 33, 21865\u201321877. [9] B\u00fchlmann, P., Peters, J. and Ernest, J. [2014], \u2018Cam: Causal additive models, high-dimensional order search and penalized regression\u2019, The Annals of Statistics 42(6), 2526\u20132556.   \n[10] Chickering, D. M. [1996], Learning bayesian networks is np-complete, in \u2018Learning from data\u2019, Springer, pp. 121\u2013130.   \n[11] Chickering, D. M. [2003], \u2018Optimal structure identification with greedy search\u2019, JMLR 3, 507\u2013554.   \n[12] Chickering, D. M., Heckerman, D. and Meek, C. [2004], \u2018Large-sample learning of Bayesian networks is NP-hard\u2019, Journal of Machine Learning Research 5, 1287\u20131330.   \n[13] Deng, C., Bello, K., Aragam, B. and Ravikumar, P. K. [2023], Optimizing notears objectives via topological swaps, in \u2018International Conference on Machine Learning\u2019, PMLR, pp. 7563\u20137595.   \n[14] Deng, C., Bello, K., Ravikumar, P. and Aragam, B. [2023], \u2018Global optimality in bivariate gradient-based dag learning\u2019, Advances in Neural Information Processing Systems 36.   \n[15] Ernest, J., Rothenh\u00e4usler, D. and B\u00fchlmann, P. [2016], \u2018Causal inference in partially linear structural equation models: identifiability and estimation\u2019, arXiv preprint arXiv:1607.05980 .   \n[16] Fan, J. and Li, R. [2001], \u2018Variable selection via nonconcave penalized likelihood and its oracle properties\u2019, Journal of the American statistical Association 96(456), 1348\u20131360.   \n[17] Friedman, J., Hastie, T. and Tibshirani, R. [2008], \u2018Sparse inverse covariance estimation with the graphical lasso\u2019, Biostatistics 9(3), 432\u2013441.   \n[18] Gao, M., Ding, Y. and Aragam, B. [2020], \u2018A polynomial-time algorithm for learning nonparametric causal graphs\u2019, Advances in Neural Information Processing Systems 33, 11599\u201311611.   \n[19] Ghoshal, A. and Honorio, J. [2017], Learning identifiable gaussian bayesian networks in polynomial time and sample complexity, in \u2018Proceedings of the 31st International Conference on Neural Information Processing Systems\u2019, pp. 6460\u20136469.   \n[20] Ghoshal, A. and Honorio, J. [2018], Learning linear structural equation models in polynomial time and sample complexity, in \u2018Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics\u2019, Vol. 84 of Proceedings of Machine Learning Research, PMLR, pp. 1466\u20131475.   \n[21] Goudet, O., Kalainathan, D., Caillou, P., Guyon, I., Lopez-Paz, D. and Sebag, M. [2018], \u2018Learning functional causal models with generative neural networks\u2019, Explainable and interpretable models in computer vision and machine learning pp. 39\u201380.   \n[22] Gu, J., Fu, F. and Zhou, Q. [2019], \u2018Penalized estimation of directed acyclic graphs from discrete data\u2019, Statistics and Computing 29(1), 161\u2013176.   \n[23] Heckerman, D., Geiger, D. and Chickering, D. M. [1995], \u2018Learning bayesian networks: The combination of knowledge and statistical data\u2019, Machine learning 20(3), 197\u2013243.   \n[24] Hoyer, P., Janzing, D., Mooij, J. M., Peters, J. and Sch\u00f6lkopf, B. [2008], \u2018Nonlinear causal discovery with additive noise models\u2019, Advances in neural information processing systems 21.   \n[25] Immer, A., Schultheiss, C., Vogt, J. E., Sch\u00f6lkopf, B., B\u00fchlmann, P. and Marx, A. [2023], On the identifiability and estimation of causal location-scale noise models, in \u2018International Conference on Machine Learning\u2019, PMLR, pp. 14316\u201314332.   \n[26] Kalainathan, D., Goudet, O., Guyon, I., Lopez-Paz, D. and Sebag, M. [2022], \u2018Structural agnostic modeling: Adversarial learning of causal graphs\u2019, Journal of Machine Learning Research 23(219), 1\u201362.   \n[27] Kingma, D. P. and Ba, J. [2014], \u2018Adam: A method for stochastic optimization\u2019, arXiv preprint arXiv:1412.6980 .   \n[28] Kyono, T., Zhang, Y. and van der Schaar, M. [2020], \u2018Castle: Regularization via auxiliary causal graph discovery\u2019, Advances in Neural Information Processing Systems 33, 1501\u20131512.   \n[29] Lachapelle, S., Brouillard, P., Deleu, T. and Lacoste-Julien, S. [2020], Gradient-based neural dag learning, in \u2018International Conference on Learning Representations\u2019.   \n[30] Lam, W. Y. [2023], Causal Razors and Causal Search Algorithms, PhD thesis, Carnegie Mellon University.   \n[31] Lam, W.-Y., Andrews, B. and Ramsey, J. [2022], Greedy relaxations of the sparsest permutation algorithm, in \u2018Uncertainty in Artificial Intelligence\u2019, PMLR, pp. 1052\u20131062.   \n[32] Lee, H.-C., Danieletto, M., Miotto, R., Cherng, S. T. and Dudley, J. T. [2019], Scaling structural learning with no-bears to infer causal transcriptome networks, in \u2018Pacific Symposium on Biocomputing 2020\u2019, World Scientific, pp. 391\u2013402.   \n[33] Loh, P.-L. and B\u00fchlmann, P. [2014], \u2018High-dimensional learning of linear causal networks via inverse covariance estimation\u2019, The Journal of Machine Learning Research 15(1), 3065\u20133105.   \n[34] Margaritis, D. and Thrun, S. [1999], Bayesian network induction via local neighborhoods, in \u2018Proceedings of the 12th International Conference on Neural Information Processing Systems\u2019, pp. 505\u2013511.   \n[35] Maxwell Chickering, D. and Heckerman, D. [1997], \u2018Efficient approximations for the marginal likelihood of bayesian networks with hidden variables\u2019, Machine learning 29(2), 181\u2013212.   \n[36] Meinshausen, N. and B\u00fchlmann, P. [2006], \u2018High-dimensional graphs and variable selection with the lasso\u2019.   \n[37] Meinshausen, N. and B\u00fchlmann, P. [2006], \u2018High-dimensional graphs and variable selection with the Lasso\u2019, The Annals of Statistics 34(3).   \n[38] Monti, R. P., Zhang, K. and Hyv\u00e4rinen, A. [2020], Causal discovery with general non-linear relationships using non-linear ica, in \u2018Uncertainty in artificial intelligence\u2019, PMLR, pp. 186\u2013195.   \n[39] Mooij, J. M., Peters, J., Janzing, D., Zscheischler, J. and Sch\u00f6lkopf, B. [2016], \u2018Distinguishing cause from effect using observational data: methods and benchmarks\u2019, The Journal of Machine Learning Research 17(1), 1103\u20131204.   \n[40] Moraffah, R., Moraffah, B., Karami, M., Raglin, A. and Liu, H. [2020], \u2018Causal adversarial network for learning conditional and interventional distributions\u2019, arXiv:2008.11376 .   \n[41] Ng, I., Ghassami, A. and Zhang, K. [2020], \u2018On the role of sparsity and dag constraints for learning linear dags\u2019, Advances in Neural Information Processing Systems 33, 17943\u201317954.   \n[42] Ng, I., Huang, B. and Zhang, K. [2024], Structure Learning with Continuous Optimization: A Sober Look and Beyond, in \u2018Proceedings of the Third Conference on Causal Learning and Reasoning\u2019, PMLR, pp. 71\u2013105.   \n[43] Ng, I., Lachapelle, S., Ke, N. R., Lacoste-Julien, S. and Zhang, K. [2022], On the convergence of continuous constrained optimization for structure learning, in \u2018International Conference on Artificial Intelligence and Statistics\u2019, PMLR, pp. 8176\u20138198.   \n[44] Pamfil, R., Sriwattanaworachai, N., Desai, S., Pilgerstorfer, P., Georgatzis, K., Beaumont, P. and Aragam, B. [2020], Dynotears: Structure learning from time-series data, in \u2018International Conference on Artificial Intelligence and Statistics\u2019, PMLR, pp. 1595\u20131605.   \n[45] Park, G. and Park, H. [2019a], Identifiability of generalized hypergeometric distribution (ghd) directed acyclic graphical models, in \u2018The 22nd International Conference on Artificial Intelligence and Statistics\u2019, PMLR, pp. 158\u2013166.   \n[46] Park, G. and Park, S. [2019b], \u2018High-dimensional poisson structural equation model learning via \\ell_1-regularized regression.\u2019, J. Mach. Learn. Res. 20, 95\u20131.   \n[47] Park, G. and Raskutti, G. [2017], \u2018Learning quadratic variance function (qvf) dag models via overdispersion scoring (ods).\u2019, J. Mach. Learn. Res. 18, 224\u20131.   \n[48] Pearl, J. [2009], Causality, Cambridge university press.   \n[49] Peters, J. and B\u00fchlmann, P. [2014], \u2018Identifiability of gaussian structural equation models with equal error variances\u2019, Biometrika 101(1), 219\u2013228.   \n[50] Peters, J., Janzing, D. and Sch\u00f6lkopf, B. [2017], Elements of causal inference: foundations and learning algorithms, MIT press.   \n[51] Peters, J., Mooij, J. M., Janzing, D. and Sch\u00f6lkopf, B. [2014], \u2018Causal discovery with continuous additive noise models\u2019, JMLR .   \n[52] Ramsey, J., Glymour, M., Sanchez-Romero, R. and Glymour, C. [2017], \u2018A million variables and more: the fast greedy equivalence search algorithm for learning high-dimensional graphical causal models, with an application to functional magnetic resonance images\u2019, International journal of data science and analytics 3, 121\u2013129.   \n[53] Ramsey, J., Zhang, J. and Spirtes, P. L. [2012], \u2018Adjacency-faithfulness and conservative causal inference\u2019, arXiv preprint arXiv:1206.6843 .   \n[54] Raskutti, G. and Uhler, C. [2018], \u2018Learning directed acyclic graph models based on sparsest permutations\u2019, Stat 7(1), e183.   \n[55] Reisach, A., Seiler, C. and Weichwald, S. [2021], \u2018Beware of the simulated dag! causal discovery benchmarks may be easy to game\u2019, Advances in Neural Information Processing Systems 34, 27772\u201327784.   \n[56] Seng, J., Ze\u02c7cevi\u00b4c, M., Dhami, D. S. and Kersting, K. [2023], Learning large dags is harder than you think: Many losses are minimal for the wrong dag, in \u2018The Twelfth International Conference on Learning Representations\u2019.   \n[57] Shimizu, S., Hoyer, P. O., Hyv\u00e4rinen, A., Kerminen, A. and Jordan, M. [2006], \u2018A linear non-gaussian acyclic model for causal discovery.\u2019, Journal of Machine Learning Research 7(10).   \n[58] Shimizu, S., Inazumi, T., Sogawa, Y., Hyvarinen, A., Kawahara, Y., Washio, T., Hoyer, P. O., Bollen, K. and Hoyer, P. [2011], \u2018Directlingam: A direct method for learning a linear non-gaussian structural equation model\u2019, Journal of Machine Learning Research-JMLR 12(Apr), 1225\u20131248.   \n[59] Spirtes, P. and Glymour, C. [1991], \u2018An algorithm for fast recovery of sparse causal graphs\u2019, Social science computer review 9(1), 62\u201372.   \n[60] Spirtes, P., Glymour, C. N., Scheines, R. and Heckerman, D. [2000], Causation, prediction, and search, MIT press.   \n[61] Tibshirani, R. [1996], \u2018Regression shrinkage and selection via the lasso\u2019, Journal of the Royal Statistical Society Series B: Statistical Methodology 58(1), 267\u2013288.   \n[62] Tsamardinos, I., Aliferis, C. F., Statnikov, A. R. and Statnikov, E. [2003], Algorithms for large scale markov blanket discovery, in \u2018FLAIRS conference\u2019, Vol. 2, pp. 376\u2013380.   \n[63] Van de Geer, S. and B\u00fchlmann, P. [2013], ${\\boldsymbol{\\cdot}}{\\boldsymbol{\\ell}}_{0}$ -penalized maximum likelihood for sparse directed acyclic graphs\u2019, The Annals of Statistics 41(2), 536\u2013567.   \n[64] Voorman, A., Shojaie, A. and Witten, D. [2014], \u2018Graph estimation with joint additive models\u2019, Biometrika 101(1), 85\u2013101.   \n[65] Wei, D., Gao, T. and Yu, Y. [2020], DAGs with no fears: A closer look at continuous optimization for learning bayesian networks, in \u2018Advances in Neural Information Processing Systems\u2019.   \n[66] Ye, Q., Amini, A. A. and Zhou, Q. [2024], \u2018Federated learning of generalized linear causal networks\u2019, IEEE Transactions on Pattern Analysis and Machine Intelligence .   \n[67] Yu, Y., Chen, J., Gao, T. and Yu, M. [2019], Dag-gnn: Dag structure learning with graph neural networks, in \u2018International Conference on Machine Learning\u2019, PMLR, pp. 7154\u20137163.   \n[68] Yuan, M. and Lin, Y. [2007], \u2018Model selection and estimation in the gaussian graphical model\u2019, Biometrika 94(1), 19\u201335.   \n[69] Zhang, C.-H. [2010], \u2018Nearly unbiased variable selection under minimax concave penalty\u2019, The Annals of Statistics 38(2), 894 \u2013 942.   \n[70] Zhang, K. and Hyvarinen, A. [2012], \u2018On the identifiability of the post-nonlinear causal model\u2019, arXiv preprint arXiv:1205.2599 .   \n[71] Zhang, K., Wang, Z., Zhang, J. and Sch\u00f6lkopf, B. [2015], \u2018On estimation of functional causal models: general results and application to the post-nonlinear causal model\u2019, ACM Transactions on Intelligent Systems and Technology (TIST) 7(2), 1\u201322.   \n[72] Zhang, Z., Ng, I., Gong, D., Liu, Y., Abbasnejad, E., Gong, M., Zhang, K. and Shi, J. Q. [2022], \u2018Truncated matrix power iteration for differentiable dag learning\u2019, Advances in Neural Information Processing Systems 35, 18390\u201318402.   \n[73] Zheng, X., Aragam, B., Ravikumar, P. K. and Xing, E. P. [2018], \u2018Dags with no tears: Continuous optimization for structure learning\u2019, Advances in neural information processing systems 31.   \n[74] Zheng, X., Dan, C., Aragam, B., Ravikumar, P. and Xing, E. [2020], Learning sparse nonparametric dags, in \u2018International Conference on Artificial Intelligence and Statistics\u2019, Pmlr, pp. 3414\u20133425.   \n[75] Zheng, Y., Huang, B., Chen, W., Ramsey, J., Gong, M., Cai, R., Shimizu, S., Spirtes, P. and Zhang, K. [2024], \u2018Causal-learn: Causal discovery in python\u2019, Journal of Machine Learning Research 25(60), 1\u20138.   \n[76] Zhu, S., Ng, I. and Chen, Z. [2020], Causal discovery with reinforcement learning, in \u2018International Conference on Learning Representations\u2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "SUPPLEMENTARY MATERIAL Markov Equivalence and Consistency in Differentiable Structure Learning ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A Preliminary Technical Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this appendix, we include various technical results used to prove the main theorems of the paper.   \nProofs can be found in Appendix B. ", "page_idx": 14}, {"type": "text", "text": "The following corollary supports Remark 1. In the main paper, we use quasi-MCP (10) as a penalty in the optimization problems (12) and (14) for simplicity. However, similar conclusions hold when MCP or SCAD is used as the penalty term. ", "page_idx": 14}, {"type": "text", "text": "Corollary 1 (MCP/SCAD). Under the same setting as Theorem 1. Let optimal solutions collection be ", "page_idx": 14}, {"type": "text", "text": "Then, for all sufficiently small $\\lambda,a\\,>\\,0$ (independent of $n$ ), it holds that $\\mathcal{O}_{n,\\lambda,a}\\,=\\,\\mathcal{E}_{\\mathrm{min}}(\\Theta^{0})$ as $n\\to\\infty$ , where MCP $p_{\\lambda,a}^{M C P}(\\cdot)$ and SCAD $p_{\\lambda,a}^{S C A D}(\\cdot)$ are defined in Appendix C.5. ", "page_idx": 14}, {"type": "text", "text": "The following lemma is a generalization of Lemma 1. Even for the general model, under the faithfulness assumption, all elements in the minimal equivalence class $\\bar{\\mathcal{E}}_{\\operatorname*{min}}(\\psi^{0},\\xi^{0})$ belong to the same Markov equivalence class, as is the case in the general linear Gaussian model (6). ", "page_idx": 14}, {"type": "text", "text": "Lemma 3. Consider that $X$ is generated by (2) with $(\\psi^{0},\\xi^{0})$ . Assume that $P(X;\\xi^{0},\\psi^{0})$ is faithful to $G^{0}:=G(B(\\psi^{0}))$ . Then ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathcal{M}(G^{0})=\\mathcal{G}(\\mathcal{E}_{\\operatorname*{min}}(\\psi^{0},\\xi^{0}))=\\{G(B(\\psi)):(\\psi,\\xi)\\in\\mathcal{E}_{\\operatorname*{min}}(\\psi^{0},\\xi^{0})\\}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $B(\\psi)$ is the adjacency matrix implied by the parameterization $(\\psi,\\xi)$ , see (3). $\\mathcal{M}(G^{0})$ is the Markov equivalence class of $G^{0}$ , see Definition $^{\\,l}$ . ", "page_idx": 14}, {"type": "text", "text": "Under the Sparsest Markov representation assumption, all elements in the minimal equivalence class are also in the same Markov equivalence class. It is important to note that the faithfulness assumption is stronger than the Sparsest Markov representation assumption. Specifically, if $P$ is faithful with respect to $G$ , then the pair $(G,P)$ satisfies the Sparsest Markov representation assumption. ", "page_idx": 14}, {"type": "text", "text": "Lemma 4. If a pair $\\left(G,P(X;B,\\Omega)\\right)$ ) satisfies Sparsest Markov representation (SMR) (see Definition 4), then $\\mathcal{M}(G)=\\mathcal{G}(\\mathcal{E}_{\\operatorname*{min}}(\\Theta))=\\{G(B):(B,\\Omega)\\in\\mathcal{E}_{\\operatorname*{min}}(\\Theta)\\}$ where $\\mathcal{M}(G)$ is Markov equivalence class of $G$ (see Definition $^{l}$ ). ", "page_idx": 14}, {"type": "text", "text": "The following lemma provides the formulation for the standardization of $X$ , along with its covariance and precision matrices. ", "page_idx": 14}, {"type": "text", "text": "Lemma 5 (standardization). Let $\\boldsymbol{X}\\sim\\mathcal{N}(\\boldsymbol{0},\\boldsymbol{\\Sigma})$ , $\\sigma_{i}^{2}:=\\mathrm{Var}(X_{i})$ and $D:=\\mathrm{diag}(\\sigma_{1},\\ldots,\\sigma_{p})$ . Then the standardization of $X$ , corresponding covariance matrix and precision matrix can be expressed as ", "page_idx": 14}, {"type": "equation", "text": "$$\nX_{\\mathrm{std}}:=D^{-1}(X-\\mathbb{E}X),\\quad\\mathrm{Cov}(X_{\\mathrm{std}})=D^{-1}\\Sigma D^{-1}\\quad[\\mathrm{Cov}(X_{\\mathrm{std}})]^{-1}=D\\Theta D\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The following lemma establishes an useful identity that holds for any adjacency matrix of a DAG, which is used in the derivation of the log-likelihood function for the model in Equation (6). ", "page_idx": 14}, {"type": "text", "text": "Lemma 6. If $B$ is adjacency matrix of a DAG, then $\\log\\operatorname*{det}(I-B)=0.$ ", "page_idx": 14}, {"type": "text", "text": "The following lemma provides a condition under which the optimization problem (12) is well-defined, ensuring that $\\ell(B,\\Omega)>-\\infty$ for any $(B,\\Omega)$ . ", "page_idx": 14}, {"type": "text", "text": "Lemma 7. For any $(B,\\Omega)$ , if $\\Omega>0,$ , then $\\Sigma:=\\Sigma_{f}(B,\\Omega)$ is positive definite. Moreover, if $X$ is generated by Equation (6) with $(B^{0},\\Omega^{0})$ , then $\\ell(B,\\Omega)>-\\infty$ for any $(B,\\Omega)$ . ", "page_idx": 14}, {"type": "text", "text": "The following lemma is used in the proof of Theorem 1. It justifies that the loss of every element in $A_{3}$ is strictly greater than the loss of the ground truth, i.e., $\\check{(}B^{0},\\Omega^{0})$ . ", "page_idx": 15}, {"type": "text", "text": "Lemma 8. Under the same setting and notation as in the proof of Theorem 1, see Section B.1. If for any $(\\bar{B},\\bar{\\Omega})\\,\\in\\,\\mathcal{E}(\\Theta^{0})$ , it holds that dis $\\langle\\bar{B},A_{3}\\rangle\\,>\\,0,$ , there exists $\\alpha\\,>\\,0$ such that $\\ell(B,\\Omega)\\,-$ $\\ell(B^{0},\\dot{\\Omega}^{0})>\\dot{\\alpha}$ for all $(B,\\Omega)\\in A_{3}$ . ", "page_idx": 15}, {"type": "text", "text": "B Detailed Proofs ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. It suffices to consider the population case, i.e., $\\ell_{n}(B,\\Omega)$ is replaced by its population counterpart $\\ell(B,\\Omega)$ . By Lemma 7, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\ell(B,\\Omega)>-\\infty\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Also, $p_{\\lambda,\\delta}(B)\\geq0$ for any $B$ . Consequently, optimization problem (12) is well-defined. ", "page_idx": 15}, {"type": "text", "text": "By convention, we assume that $(B^{0},\\Omega^{0})\\in\\mathcal{E}_{\\operatorname*{min}}(\\Theta^{0})$ . Now, consider the case where $p_{\\lambda,\\delta}(B^{0})=0$ , which is equivalent to $B^{0}=0$ , since $\\ell(B,\\Omega)\\,\\ge\\,\\ell(B^{0},\\Omega^{0})$ and $p_{\\lambda,\\delta}(B)\\,\\ge\\,p_{\\lambda,\\delta}(B^{0})$ for any $B$ . Therefore, for all $\\lambda>0$ and $\\delta>0$ , $B^{0}$ is the unique optimal solution to optimization problem (12), proving the conclusion. ", "page_idx": 15}, {"type": "text", "text": "In the subsequent proof, we assume that $\\vert\\mathcal{E}_{\\mathrm{min}}(\\Theta^{0})\\vert\\,=\\,1$ , that is, $\\mathcal{E}_{\\mathrm{min}}(\\Theta^{0})\\,=\\,\\{(B^{0},\\Omega^{0})\\}$ . This assumption simplifies the proof because any element of $\\ensuremath{\\mathcal{E}_{\\mathrm{min}}}(\\Theta^{0})$ is indistinguishable based on the value of $\\ell(B,\\Omega)$ and the penalty for the chosen $(\\delta,\\lambda)$ , as shown below. Our goal is to identify one element via optimization problem (12), which significantly simplifies the argument. ", "page_idx": 15}, {"type": "text", "text": "First, let us define ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\delta_{0}=\\!\\frac{\\tau}{1+\\Delta}\\qquad\\mathrm{where}\\ \\tau:=\\operatorname*{min}_{(B,\\Omega)\\in\\mathcal{E}(\\Theta^{0})}\\operatorname*{min}_{\\{(i,j)|B_{i j}\\neq0\\}}|B_{i j}|\\stackrel{(a)}{=}\\operatorname*{min}_{\\pi}\\operatorname*{min}_{\\{(i,j)|[\\tilde{B}^{0}(\\pi)]_{i j}\\neq0\\}}\\left|\\left[\\widetilde{B}^{0}(\\pi)\\right]_{i j}\\right|\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "with any $\\Delta>0.\\ (a)$ is due to the fact that each element in equivalence class $\\mathcal{E}(\\Theta^{0})$ is one-to-one associated with $\\widetilde{B}^{0}(\\pi)$ , see Section C.2 or [2] for detailed discussion. Then, for any $\\lambda>0$ and $0<\\delta<\\delta_{0}$ , con sider the set $A_{1}=\\{(B,\\Omega)\\mid{{p}_{\\lambda,\\delta}}(B)={{p}_{\\lambda,\\delta}}(B^{0})\\}$ . For any $(B,\\Omega)\\in A_{1}$ , we have $(B,\\Omega)\\not\\in\\mathcal{E}(\\Theta^{0})\\setminus\\{(B^{0},\\Omega^{0})\\}$ . This follows from the fact that ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle p_{\\lambda,\\delta}(B)=\\frac{\\lambda\\delta}{2}s_{B}>\\frac{\\lambda\\delta}{2}s_{B^{0}}=p_{\\lambda,\\delta}(B^{0})\\qquad\\forall(B,\\Omega)\\in\\mathcal{E}(\\Theta^{0})\\backslash\\left\\{(B^{0},\\Omega^{0})\\right\\}\\!.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "As a consequence, this implies that $\\ell(B^{0},\\Omega^{0})<\\ell(B,\\Omega),\\forall(B,\\Omega)\\in A_{1}$ . Therefore, ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\ell(B^{0},\\Omega^{0})+p_{\\lambda,\\delta}(B^{0})<\\ell(B,\\Omega)+p_{\\lambda,\\delta}(B)\\qquad\\forall(B,\\Omega)\\in A_{1}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Next, we define $A_{2}=\\{(B,\\Omega)\\mid p_{\\lambda,\\delta}(B)>p_{\\lambda,\\delta}(B^{0})\\}$ . Since $\\ell(B^{0},\\Omega^{0})\\le\\ell(B,\\Omega)$ , it follows that for all $(B,\\Omega)\\in A_{2}$ , the following inequality holds: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\ell(B^{0},\\Omega^{0})+p_{\\lambda,\\delta}(B^{0})<\\ell(B,\\Omega)+p_{\\lambda,\\delta}(B)\\qquad\\forall(B,\\Omega)\\in A_{2}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Therefore, we need to examine the set $A_{3}\\,=\\,\\{(B,\\Omega)\\,\\mid\\,p_{\\lambda,\\delta}(B)\\,<\\,p_{\\lambda,\\delta}(B^{0})\\}$ . For $(B^{0},\\Omega^{0})$ to achieve the minimum value of the score function, it is crucial that the following condition is satisfied: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\ell(B^{0},\\Omega^{0})+p_{\\lambda,\\delta}(B^{0})<\\ell(B,\\Omega)+p_{\\lambda,\\delta}(B)\\qquad\\forall(B,\\Omega)\\in A_{3}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This condition guarantees that the ground truth parameters $(B^{0},\\Omega^{0})$ correspond to the optimal solution by comparing their score with any other parameters in the subset $A_{3}$ . ", "page_idx": 15}, {"type": "text", "text": "It is important to note that $p_{\\lambda,\\delta}(t)=\\lambda p_{1,\\delta}(t),\\forall t$ . Thus, a necessary and sufficient condition for this to hold is: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\lambda<\\operatorname*{min}_{(B,\\Omega)\\in A_{3}}\\frac{\\ell(B,\\Omega)-\\ell(B^{0},\\Omega^{0})}{p_{1,\\delta}(B^{0})-p_{1,\\delta}(B)}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that for all $(B,\\Omega)\\in A_{3}$ , we have $\\begin{array}{r}{p_{1,\\delta}(B^{0})-p_{1,\\delta}(B)\\,\\le\\,\\frac\\delta2s_{0}}\\end{array}$ , with equality achieved when $B\\;=\\;0$ . Therefore, the denominator on the RHS cannot be arbitrarily large. Moreover, since $(B,\\Omega)\\in A_{3}$ , it follows that $(B,\\Omega)\\not\\in\\mathcal{E}(\\Theta^{0})$ , as $A_{3}\\cap\\mathcal{E}(\\Theta^{0})=\\emptyset$ . ", "page_idx": 16}, {"type": "text", "text": "We define the distance from $\\bar{B}$ to the set $A_{3}$ as: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{dist}(\\bar{B},A_{3})=\\operatorname*{inf}_{(B,\\Omega)\\in A_{3}}\\|B-\\bar{B}\\|_{2}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "For all $(\\bar{B},\\bar{\\Omega})\\in\\mathcal{E}(\\Theta^{0})$ , it turns out that dist $(\\bar{B},A_{3})$ must be positive due to the design of $\\delta_{0}$ , giving: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\mathrm{dist}(\\bar{B},A_{3})>\\displaystyle\\operatorname*{min}_{(B,\\Omega)\\in\\mathcal E(\\Theta^{0})}\\displaystyle\\operatorname*{min}_{\\{(i,j)|B_{i j}\\neq0\\}}|B_{i j}|-\\delta_{0}}}\\\\ {{=\\tau-\\displaystyle\\frac{\\tau}{1+\\Delta}=\\displaystyle\\frac{\\Delta}{1+\\Delta}\\tau>0.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "By Lemma 8, there exists some $\\alpha\\,>\\,0$ such that $\\ell(B,\\Omega)-\\ell(B^{0},\\Omega^{0})>\\alpha$ for all $(B,\\Omega)\\,\\in\\,A_{3}$ . Consequently, we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{(B,\\Omega)\\in A_{3}}\\frac{\\ell(B,\\Omega)-\\ell(B^{0},\\Omega^{0})}{p_{1,\\delta}(B^{0})-p_{1,\\delta}(B)}>0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, we can define ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\lambda_{0}=\\operatorname*{inf}_{(B,\\Omega)\\in A_{3}}\\frac{\\ell(B,\\Omega)-\\ell(B^{0},\\Omega^{0})}{p_{1,\\delta}(B^{0})-p_{1,\\delta}(B)}>0.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In summary, for all $0\\ <\\ \\lambda\\ <\\ \\lambda_{0}$ and $0~<~\\delta~<~\\delta_{0}$ , for any $(\\widehat{B},\\widehat{\\Omega})\\;\\in\\;\\mathcal{E}_{\\operatorname*{min}}(\\Theta^{0})$ , and for all $(B,\\Omega)\\not\\in\\dot{\\mathcal{E}_{\\operatorname*{min}}}(\\Theta^{0})$ , the following holds: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\ell(\\widehat{B},\\widehat{\\Omega})+p_{\\lambda,\\delta}(\\widehat{B})<\\ell(B,\\Omega)+p_{\\lambda,\\delta}(B).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 16}, {"type": "text", "text": "B.2 Proof of Theorem 2 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. Here, $\\Theta_{f}(B^{0},\\Omega^{0})=\\Theta^{0}$ . From Theorem 1, we know that when $n\\to\\infty$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{O}_{n,\\lambda,\\delta}=\\mathcal{E}_{\\mathrm{min}}(\\Theta^{0}).\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Given the additional assumption that $p(X)$ is faithful with respect to $G^{0}:=G(B^{0})$ , by Lemma 1, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{M}(G^{0})=\\mathcal{E}_{\\operatorname*{min}}(\\Theta^{0})=\\{G(B):(B,\\Omega)\\in\\mathcal{E}_{\\operatorname*{min}}(\\Theta^{0})\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\{G(B):(B,\\Omega)\\in\\mathcal E_{\\operatorname*{min}}(\\Theta^{0})\\}=\\{G(B):(B,\\Omega)\\in\\mathcal O_{n,\\lambda,\\delta}\\}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, we conclude that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{M}(G^{0})=\\mathcal{O}_{n,\\lambda,\\delta}.\\qquad\\mathrm{as~}n\\to\\infty\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "This completes the proof. ", "page_idx": 16}, {"type": "text", "text": "B.3 Proof of Theorem 3 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. In this proof, we use the notation introduced in Section C.6. Note that when $\\mathbf{X}$ is used in (12), we essentially compute the sample covariance matrix $\\widehat{\\Sigma}$ based on $\\mathbf{X}$ as follows: ", "page_idx": 16}, {"type": "equation", "text": "$$\n{\\widehat{\\boldsymbol{\\Sigma}}}={\\frac{1}{n}}\\left[{\\mathbf{X}}-\\mathbf{1}_{n}\\cdot({\\widehat{\\mu}}_{1},\\ldots,{\\widehat{\\mu}}_{p})\\right]^{\\top}\\left[{\\mathbf{X}}-\\mathbf{1}_{n}\\cdot({\\widehat{\\mu}}_{1},\\ldots,{\\widehat{\\mu}}_{p})\\right]\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and plug it into the negative sample log-likelihood function. The same procedure applies to $\\mathbf{Z}$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\widehat{\\boldsymbol{\\Sigma}}_{\\mathrm{std}}=\\displaystyle\\frac{1}{n}D^{-1}\\left[\\mathbf{X}-\\mathbf{1}_{n}(\\widehat{\\mu}_{1},\\ldots,\\widehat{\\mu}_{p})\\right]^{\\intercal}\\left[\\mathbf{X}-\\mathbf{1}_{n}(\\widehat{\\mu}_{1},\\ldots,\\widehat{\\mu}_{p})\\right]D^{-1}}}\\\\ {{\\phantom{\\widehat{\\mathbf{X}}_{\\mathrm{std}}=\\widehat{\\mathbf{\\Sigma}}}=D^{-1}\\widehat{\\boldsymbol{\\Sigma}}D^{-1}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Denote $\\widehat\\Theta\\,=\\,(\\widehat\\Sigma)^{-1}$ and $\\widehat{\\Theta}_{\\mathrm{std}}\\,=\\,(\\widehat{\\Sigma}_{\\mathrm{std}})^{-1}\\,=\\,D\\widehat{\\Theta}D$ . For $\\widehat{\\Theta}$ , by applying Theorem 1, there exist $\\lambda_{0}^{\\mathrm{raw},n}\\,>\\,0$ and $\\delta_{0}^{\\mathrm{raw},n}\\,>\\,0$ such t hat for any $0\\ <\\ \\lambda\\ <\\ \\lambda_{0}^{\\mathrm{raw},n}$ and $\\overline{{{0}}}^{\\overline{{{\\,}}}}\\:<\\:\\delta\\ <\\ \\delta_{0}^{\\mathrm{raw},n}$ , we have $\\mathcal{O}_{n,\\lambda,\\delta}(\\mathbf{X})\\,=\\,\\mathcal{E}_{\\mathrm{min}}(\\widehat{\\Theta})$ . For $D\\widehat{\\Theta}D$ , we apply Theorem 1 again, and there exist $\\lambda_{0}^{\\mathrm{std},n}\\,>\\,0$ and $\\delta_{0}^{\\mathrm{std},n}>0$ such that for any $0<\\lambda<\\lambda_{0}^{\\mathrm{std},n}$ and $0<\\delta<\\delta_{0}^{\\mathrm{std},n}$ , we have $\\mathcal{O}_{n,\\lambda,\\delta}(\\mathbf{Z})=\\mathcal{E}_{\\mathrm{min}}(D\\widehat{\\Theta}D)$ . ", "page_idx": 17}, {"type": "text", "text": "We can select $\\delta_{0}=\\operatorname*{min}\\{\\delta_{0}^{\\mathrm{raw},n},\\delta_{0}^{\\mathrm{std},n}\\}$ and $\\lambda_{0}=\\operatorname*{min}\\{\\lambda_{0}^{\\mathrm{raw},n},\\lambda_{0}^{\\mathrm{std},n}\\}$ in optimization (12) to ensure that $\\mathcal{O}_{n,\\lambda,\\delta}(\\mathbf{X})=\\mathcal{E}_{\\operatorname*{min}}(\\widehat{\\Theta})$ and $\\mathcal{O}_{n,\\lambda,\\delta}(\\mathbf{Z})=\\mathcal{E}_{\\mathrm{min}}(D\\widehat{\\Theta}D)$ hold simultaneously. ", "page_idx": 17}, {"type": "text", "text": "By applying Lemma 2, we conclude that: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{G}(\\mathcal{O}_{n,\\lambda,\\delta}(\\mathbf{X}))=\\mathcal{G}(\\mathcal{E}_{\\operatorname*{min}}(\\widehat{\\Theta}))=\\mathcal{G}(\\mathcal{E}_{\\operatorname*{min}}(D\\widehat{\\Theta}D))=\\mathcal{G}(\\mathcal{O}_{n,\\lambda,\\delta}(\\mathbf{Z})).\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Furthermore, as $n\\rightarrow\\infty$ , we have $\\widehat{\\Sigma}\\rightarrow\\Sigma$ and $\\widehat\\Theta\\to\\Theta$ . Therefore, $\\mathcal{E}_{\\mathrm{min}}(\\ensuremath{\\widehat{\\Theta}})\\to\\mathcal{E}_{\\mathrm{min}}(\\Theta)$ , and $\\mathcal{E}_{\\mathrm{min}}(\\widehat{\\Theta}_{\\mathrm{std}})\\to\\mathcal{E}_{\\mathrm{min}}(\\Theta_{\\mathrm{std}})$ as $n\\to\\infty$ . Thus, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal G(\\mathcal O_{n,\\lambda,\\delta}(\\mathbf X))=\\mathcal G(\\mathcal E_{\\operatorname*{min}}(\\Theta))=\\mathcal G(\\mathcal O_{n,\\lambda,\\delta}(\\mathbf Z)).\\qquad\\mathrm{where~}n\\to\\infty\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that we use $n\\to\\infty$ to indicate we consider the result in population level. ", "page_idx": 17}, {"type": "text", "text": "B.4 Proof of Theorem 4 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. This proof shares many similarities with the proof of Theorem 1. First, when $n\\to\\infty$ , we consider the result at the population level. Thus, $\\ell_{n}(\\bar{\\psi_{,}}\\xi)\\to\\ell(\\psi,\\xi)$ , and we will focus on $\\ell(\\psi,\\xi)$ in the following. As a result, we only work with $\\ell(\\psi,\\xi)$ instead of $\\ell_{n}(\\psi,\\xi)$ . ", "page_idx": 17}, {"type": "text", "text": "By convention, we can always assume that $(\\psi^{0},\\xi^{0})\\in\\mathcal{E}_{\\mathrm{min}}(\\psi^{0},\\xi^{0})$ ", "page_idx": 17}, {"type": "text", "text": "Now, consider the case where $p_{\\lambda,\\delta}(B(\\psi^{0}))\\,=\\,0$ , which implies that $B(\\psi^{0})\\,=\\,0$ . Since $\\textit{X}\\sim$ $P(X,\\psi^{0},\\xi^{0})$ , we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\ell(\\psi^{0},\\xi^{0})\\leq\\ell(\\psi,\\xi),\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and thus ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\ell(\\psi^{0},\\xi^{0})+p_{\\lambda,\\delta}(B(\\psi^{0}))\\leq\\ell(\\psi,\\xi)+p_{\\lambda,\\delta}(B(\\psi))}&{}&{\\forall(\\psi,\\xi)\\in\\Psi\\times\\Xi,\\forall\\lambda>0,\\forall\\delta>0.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, $(\\psi^{0},\\xi^{0})$ is the optimal solution to optimization (12). ", "page_idx": 17}, {"type": "text", "text": "As we iterate, we can assume that $|\\mathcal{E}_{\\mathrm{min}}(\\psi^{0},\\xi^{0})|=1$ , meaning $\\mathcal{E}_{\\mathrm{min}}(\\psi^{0},\\xi^{0})=\\{(\\psi^{0},\\xi^{0})\\}$ . This assumption simplifies the proof because any element in $\\mathcal{E}_{\\mathrm{min}}(\\psi^{\\bar{0}},\\xi^{0})$ is indistinguishable based on the value of $\\ell(\\psi,\\xi)$ and the penalty for the chosen parameters $(\\delta,\\lambda)$ . Our goal is to find one element by solving the optimization problem (14), and this assumption simplifies the argument significantly. First, we define ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\delta_{0}=\\frac{\\tau}{1+\\Delta}\\qquad\\tau:=\\operatorname*{min}_{(\\psi,\\xi)\\in\\mathcal{E}(\\psi^{0},\\xi^{0})}\\operatorname*{min}_{\\{(i,j):B(\\psi)_{i j}\\neq0\\}}|B(\\psi)|_{i j}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "It is important to note that, under Assumption A (1), since $|\\mathcal{E}(\\psi^{0},\\xi^{0})|$ is finite, we have $\\tau>0$ . ", "page_idx": 17}, {"type": "text", "text": "Then, for any $\\lambda>0$ and $0<\\delta<\\delta_{0}$ , consider the set ", "page_idx": 17}, {"type": "equation", "text": "$$\n{\\cal A}_{1}=\\{(\\psi,\\xi)\\mid p_{\\lambda,\\delta}(B(\\psi))=p_{\\lambda,\\delta}(B(\\psi^{0}))\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "It is clear that for any $(\\psi,\\xi)\\in A_{1}$ , we must have $(\\psi,\\xi)\\not\\in\\mathcal{E}(\\psi^{0},\\xi^{0})$ , since for any $(\\psi,\\xi)\\in\\mathcal{E}(\\psi^{0},\\xi^{0})$ , we have $p_{\\lambda,\\delta}(B(\\psi))>p_{\\lambda,\\delta}(B(\\psi^{0}))$ . As a result, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\ell(\\psi^{0},\\xi^{0})<\\ell(\\psi,\\xi)\\qquad\\forall(\\psi,\\xi)\\in A_{1}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\ell(\\psi^{0},\\xi^{0})+p_{\\lambda,\\delta}(B(\\psi^{0}))<\\ell(\\psi,\\xi)+p_{\\lambda,\\delta}(B(\\psi))\\qquad\\forall(\\psi,\\xi)\\in A_{1}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Next, we consider the se $\\mathrm{~\\boldmath~t~}{\\cal A}_{2}\\;=\\;\\{(\\psi,\\xi)\\;\\mid\\;p_{\\lambda,\\delta}(B(\\psi))\\;>\\;p_{\\lambda,\\delta}(B(\\psi^{0}))\\},$ and we know that $\\ell(\\psi^{0},\\xi^{0})\\leq\\ell(\\psi,\\xi)$ . Therefore, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\ell(\\psi^{0},\\xi^{0})+p_{\\lambda,\\delta}(B(\\psi^{0}))<\\ell(\\psi,\\xi)+p_{\\lambda,\\delta}(B(\\psi))\\qquad\\forall(\\psi,\\xi)\\in A_{2}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Consequently, we need to check ${\\cal A}_{3}=\\{(\\psi,\\xi)\\mid p_{\\lambda,\\delta}(B(\\psi))<p_{\\lambda,\\delta}(B(\\psi^{0}))\\}$ . For $(\\psi^{0},\\xi^{0})$ to be the minimizer of (14), we require that the following condition holds: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\ell(\\psi^{0},\\xi^{0})+p_{\\lambda,\\delta}(B(\\psi^{0}))<\\ell(\\psi,\\xi)+p_{\\lambda,\\delta}(B(\\psi))\\qquad\\forall(\\psi,\\xi)\\in A_{3}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "This condition ensures that the ground truth parameters $(\\psi^{0},\\xi^{0})$ correspond to the optimal solution by comparing their score with that of any other parameters in the subset $A_{3}$ . ", "page_idx": 18}, {"type": "text", "text": "It is also worth noting that $p_{\\lambda,\\delta}(t)=\\lambda p_{1,\\delta}(t)$ for all $t$ . Therefore, a necessary and sufficient condition for this to hold is: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\lambda<\\operatorname*{inf}_{(\\psi,\\xi)\\in A_{3}}\\frac{\\ell(\\psi,\\xi)-\\ell(\\psi^{0},\\xi^{0})}{p_{1,\\delta}(B(\\psi^{0}))-p_{1,\\delta}(B(\\psi))}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that for $(\\psi,\\xi)\\;\\in\\;A_{3}$ , we have $\\begin{array}{r}{p_{1,\\delta}(B(\\psi^{0}))\\,-\\,p_{1,\\delta}(B(\\psi))\\;\\le\\;\\frac{\\delta}{2}s_{B(\\psi^{0})}}\\end{array}$ . Therefore, the denominator on the RHS cannot be arbitrarily large. Moreover, for any $0\\:\\dot{<}\\:\\delta\\:<\\:\\delta_{0}$ , the following holds: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\Delta}{1+\\Delta}\\tau\\leq\\|B(\\psi^{0})-B(\\psi)\\|_{2}\\leq L\\|\\psi^{0}-\\psi\\|_{2}\\qquad\\forall(\\psi,\\xi)\\in A_{3}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The second inequality follows from Assumption A (b). As a consequence, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\|(\\psi,\\xi)-(\\psi^{0},\\xi^{0})\\|_{2}\\geq\\|\\psi^{0}-\\psi\\|_{2}\\geq\\frac{\\tau\\Delta}{L(1+\\Delta)}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Thus, we obtain ", "page_idx": 18}, {"type": "equation", "text": "$$\nA_{3}\\subseteq\\{(\\psi,\\xi)\\mid\\|(\\psi,\\xi)-(\\psi^{0},\\xi^{0})\\|_{2}\\geq\\frac{\\Delta\\tau}{L(1+\\Delta)}\\}=A_{4}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "First, note that $A_{3}$ is a nonempty set. Otherwise, the conclusion would hold immediately. Let us select any $(\\bar{\\psi},\\bar{\\xi})\\in A_{3}\\neq\\emptyset$ , and define ${\\cal A}_{5}=\\{(\\psi,\\xi)\\mid\\ell(\\psi,\\xi)\\leq\\ell(\\bar{\\psi},\\bar{\\xi})\\}$ . Then, ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{(\\psi,\\xi)\\in A_{3}}\\ell(\\psi,\\xi)=\\operatorname*{inf}_{(\\psi,\\xi)\\in A_{3}\\cap A_{5}}\\ell(\\psi,\\xi)\\geq\\operatorname*{inf}_{(\\psi,\\xi)\\in A_{4}\\cap A_{5}}\\ell(\\psi,\\xi).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "It is important to note that $A_{4}\\cap A_{5}$ is nonempty, since $(\\bar{\\psi},\\bar{\\xi})\\;\\in\\;A_{4}\\cap A_{5}$ . By Assumption B and the properties of $\\ell(\\psi,\\xi)$ , we know that $A_{5}$ is a bounded and closed set, and $A_{4}$ is a closed set. Consequently, $A_{4}\\cap A_{5}$ is compact. Furthermore, for all $(\\psi,\\xi)\\in\\mathcal{E}(\\psi^{0},\\xi^{0})$ , we have $(\\psi,\\xi)\\notin A_{4}\\cap A_{5}$ . All of this leads to the following conclusion: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{(\\psi,\\xi)\\in A_{3}}\\ell(\\psi,\\xi)\\geq\\operatorname*{min}_{(\\psi,\\xi)\\in A_{4}\\cap A_{5}}\\ell(\\psi,\\xi)=\\operatorname*{inf}_{(\\psi,\\xi)\\in A_{4}\\cap A_{5}}\\ell(\\psi,\\xi)>\\ell(\\psi^{0},\\xi^{0}).\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "As a result, we define $\\lambda_{0}$ as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\lambda_{0}=\\operatorname*{inf}_{(\\psi,\\xi)\\in A_{3}}\\frac{\\ell(\\psi,\\xi)-\\ell(\\psi^{0},\\xi^{0})}{p_{1,\\delta}(B(\\psi^{0}))-p_{1,\\delta}(B(\\psi))}>0.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "B.5 Proof of Theorem 5 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. The proof is combination of Lemma 3 and Theorem 4, similar to Proof of Theorem 2. ", "page_idx": 18}, {"type": "text", "text": "B.6 Proof of Lemma 1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Before proving the result, we introduce the definitions of the Sparsest Markov representation assumption and restricted faithfulness, along with a few useful theorems. ", "page_idx": 18}, {"type": "text", "text": "Definition 4 (Sparsest Markov representation [54]). A pair $(G^{0},P)$ satisfies the Sparsest Markov Representation (SMR) assumption $\\bar{\\imath}f(G^{0},P)$ satisfies the Markov property and $|G|>|G^{0}|$ for every DAG $G$ such that $(G,P)$ satisfies the Markov property and $G\\not\\in\\bar{\\mathcal{M}}(\\bar{G}^{0})$ . ", "page_idx": 18}, {"type": "text", "text": "In other words, the SMR assumption asserts that the true DAG $G^{0}$ is the (unique up to Markov equivalence) sparsest DAG satisfying the Markov property. ", "page_idx": 18}, {"type": "text", "text": "Definition 5 (Restricted-faithfulness [53, 54]). A distribution $P$ satisfies the restricted-faithfulness assumption with respect to a DAG $G$ if it is Markov to $G$ and following two conditions hold: ", "page_idx": 19}, {"type": "text", "text": "\u2022 Adjacency-faithfulness: for all $(j,k)\\in E$ and all subsets $S\\,\\subset\\,[p]\\backslash\\{j,k\\}$ it holds that $X_{j}$ \u0338 $L\\ X_{k}\\mid X_{S}$ \u2022 Orientation-faithfulness: for all triples $(j,k,l)$ with skeleton $j-l-k$ and all subsets $S\\subset[p]\\backslash\\{j,\\bar{k}\\}$ such that $j$ is $d$ -connected to $k$ given $S$ it holds that $X_{j}$ \u0338\u22a5\u22a5 $X_{k}\\mid X_{S}$ ", "page_idx": 19}, {"type": "text", "text": "Theorem 6 ([53]). If a distribution $P$ is faithful to $G$ , then such distribution $P$ also satisfies the restricted-faithfulness assumption with respect to $G$ . ", "page_idx": 19}, {"type": "text", "text": "Theorem 7 (Theorem 2.4 in [54]). Let $(G,P)$ satisfy the Markov property. Then the restrictedfaithfulness assumption implies the SMR assumption. ", "page_idx": 19}, {"type": "text", "text": "Proof. First, by Theorem 6, the faithfulness assumption implies the restricted faithfulness assumption. Second, by Theorem 7, the restricted faithfulness assumption implies the Sparsest Markov representation assumption. Furthermore, note that for any $(B,\\bar{\\Omega})\\in\\mathcal{G}(\\bar{\\mathcal{E}}_{\\operatorname*{min}}(\\Theta^{0}))$ , the distribution $\\bar{P(X)}$ is Markov to $G(B)$ , since $(B,\\Omega)\\in\\mathcal{G}(\\mathcal{E}(\\Theta^{0}))$ . According to the definition of the Sparsest Markov representation assumption, all sparsest DAGs that satisfy the Markov property must belong to the same Markov equivalence class. In our case, this means $\\dot{\\mathcal{G}}(\\mathcal{E}_{\\operatorname*{min}}(\\Theta^{0}))\\overset{\\cdot}{=}\\dot{\\mathcal{M}}(\\dot{G^{0}})$ . \u53e3 ", "page_idx": 19}, {"type": "text", "text": "B.7 Proof of Lemma 2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. For $\\ensuremath{\\boldsymbol{X}}\\sim\\mathcal{N}(\\ensuremath{\\boldsymbol{0}},\\ensuremath{\\boldsymbol{\\Sigma}})$ , let $\\bar{\\Theta}=D\\Theta D$ , and denote the inverse of $\\bar{\\Theta}$ as $\\bar{\\Sigma}=(\\bar{\\Theta})^{-1}=D^{-1}\\Sigma D^{-1}$ . It follows that $\\bar{X}:=D^{-1}X\\sim{\\mathcal{N}}(0,\\bar{\\Sigma})$ . Now, consider the following least squares regression for $j\\in\\{1,\\ldots,p\\}$ and $S\\subseteq\\{1,\\ldots,p\\}\\setminus\\{j\\}$ . Let $\\beta\\in\\mathbb{R}^{|S|}$ . Then the following relationships hold: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\beta_{S j}=\\arg\\underset{\\beta}{\\operatorname*{min}}\\,\\mathbb{E}\\|X_{j}-\\beta^{\\top}X_{S}\\|_{2}^{2}\\Rightarrow\\beta_{S j}=\\Sigma_{S S}^{-1}\\Sigma_{S j}}\\\\ &{\\bar{\\beta}_{S j}=\\arg\\underset{\\beta}{\\operatorname*{min}}\\,\\mathbb{E}\\|\\bar{X}_{j}-\\bar{\\beta}^{\\top}\\bar{X}_{S}\\|_{2}^{2}\\Rightarrow\\bar{\\beta}_{S j}=\\bar{\\Sigma}_{S S}^{-1}\\bar{\\Sigma}_{S j}}\\\\ &{\\bar{\\Sigma}_{S S}^{-1}=([D^{-1}\\Sigma D^{-1}]_{S S})^{-1}=D_{S S}\\Sigma_{S S}^{-1}D_{S S}}\\\\ &{\\bar{\\Sigma}_{S j}=[D^{-1}\\Sigma D^{-1}]_{S j}=D_{S S}^{-1}\\Sigma_{S j}D_{j j}^{-1}}\\\\ &{\\bar{\\beta}_{S j}=\\bar{\\Sigma}_{S S}^{-1}\\bar{\\Sigma}_{S j}=D_{S S}\\Sigma_{S S}^{-1}D_{S S}D_{S S}^{-1}\\Sigma_{S j}D_{j j}^{-1}=D_{S S}\\beta_{S j}D_{j j}^{-1}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "As a consequence, $\\mathrm{supp}(\\beta_{S j})=\\mathrm{supp}(\\bar{\\beta}_{S j})$ . Note that for all $(B,\\Omega)\\in\\mathcal{E}(\\Theta)$ , we know from Section C.2 that there exists a $\\pi\\in\\mathcal{P}$ such that $B=\\widetilde{B}(\\pi)$ . Moreover, $B$ can be recovered by least squares regression using $X$ with its topological sort [2, 13] that is consistent with $\\pi$ . For such a $\\pi$ , we can find a pair $(\\bar{B},\\bar{\\bar{\\Omega}})\\in\\mathcal E(D\\Theta D)$ , where $\\bar{B}$ has the same topological sort as $\\pi$ , and it can be recovered by least squares regression on $\\bar{X}$ . We have shown that, for the same $S,j$ , $\\mathrm{supp}(\\beta_{S j})=\\mathrm{supp}(\\bar{\\beta}_{S j})$ . Therefore, $\\operatorname{supp}(B)=\\operatorname{supp}({\\bar{B}})$ . \u53e3 ", "page_idx": 19}, {"type": "text", "text": "B.8 Proof of Lemma 3 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. The proof is the same as Lemma 1. ", "page_idx": 19}, {"type": "text", "text": "B.9 Proof Lemma 4 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. This follows directly from the definition of the Sparsest Markov Representation (SMR) assumption. Since for all $(B,\\Omega)\\in\\mathcal{E}_{\\operatorname*{min}}(\\Theta),G(B)$ is Markovian to $P$ and $G(B)$ is the Sparsest, by Definition 2, all $G(B)$ must belong to the same Markov equivalence class by the definition of SMR. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "B.10 Proof of Lemma 5 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. $X_{\\mathrm{std}}=D^{-1}(X-\\mathbb{E}X)$ is based on definition of standardization. ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Cov}(X_{\\mathrm{std}})=\\mathrm{Cov}(D^{-1}(X-\\mathbb{E}X))=D^{-1}\\mathrm{Cov}((X-\\mathbb{E}X))D^{-1}=D^{-1}\\Sigma D^{-1}}\\\\ &{[\\mathrm{Cov}(X_{\\mathrm{std}})]^{-1}=[D^{-1}\\Sigma D^{-1}]^{-1}=D\\Sigma^{-1}D=D\\Theta D.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "B.11 Proof of Lemma 6 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof. Detailed proof can be found in [41], Appendix Section D. ", "page_idx": 20}, {"type": "text", "text": "B.12 Proof of Lemma 7 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof. From the definition of $\\Sigma_{f}(B,\\Omega)$ : ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Sigma_{f}(B,\\Omega):=(I-B)^{-\\top}\\Omega(I-B)^{-1}=(I-B)^{-\\top}\\Omega^{1/2}\\Omega^{1/2}(I-B)^{-1},\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\Omega^{1/2}=\\mathrm{diag}(\\omega_{1},\\ldots,\\omega_{p})$ . It is clear that $\\Sigma(B,\\Omega)$ is positive semidefinite, as ", "page_idx": 20}, {"type": "equation", "text": "$$\nx^{\\top}\\Sigma(B,\\Omega)x=\\Vert\\Omega^{1/2}(I-B)^{-1}x\\Vert_{2}^{2}\\geq0,\\qquad\\forall x\\in\\mathbb{R}^{p}.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Next, we just need to show that $\\Omega^{1/2}(I-B)^{-1}x\\neq0$ for all $x\\neq0$ . ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\Omega^{1/2}(I-B)^{-1}x\\neq0\\Leftrightarrow(I-B)^{-1}x\\neq0\\Leftrightarrow x\\neq0.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Here, $\\omega_{j}^{2}>0$ for all $j$ , so $\\Omega^{1/2}$ is invertible. As $(I-B)$ is a full rank matrix, then $(I-B)^{-1}$ is also a full rank matrix, it indicates that $\\Sigma(B,\\Omega)$ is positive definite matrix. ", "page_idx": 20}, {"type": "text", "text": "Since $\\Omega^{0}>0$ , it follows that $\\Sigma^{0}$ is positive definite. By Lemma 6, we have: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\ell({\\cal B},\\Omega)=\\frac{1}{2}\\log\\operatorname*{det}\\Omega-\\log\\operatorname*{det}(I-{\\cal B})+\\frac{1}{2}\\,\\mathrm{Tr}(\\Sigma^{0}\\Theta({\\cal B},\\Omega))+\\mathrm{const.}}}\\\\ {{\\displaystyle\\qquad\\qquad=\\frac{1}{2}\\log\\operatorname*{det}\\Omega+\\frac{1}{2}\\,\\mathrm{Tr}(\\Sigma^{0}\\Theta({\\cal B},\\Omega))+\\mathrm{const.}}}\\\\ {{\\displaystyle\\qquad\\qquad\\ge\\ell({\\cal B},\\Omega_{f}({\\cal B}))=\\ell({\\cal B})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\Omega_{f}(B)$ and $\\ell(B)$ are defined in Equations (15) and (17), respectively. The last inequality follows from Section C.1. Next, we need to prove that $\\ell(B)>-\\infty$ . ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell(B)=\\displaystyle\\frac{1}{2}\\log\\operatorname*{det}\\mathrm{diag}\\big((I-B)^{\\top}\\Sigma^{0}(I-B)\\big)+\\mathrm{const.}}\\\\ &{\\qquad=\\displaystyle\\frac{1}{2}\\sum_{j=1}^{N}\\log\\mathbb{E}\\|X_{j}-B_{j}^{\\top}X\\|_{2}^{2}+\\mathrm{const.}}\\\\ &{\\qquad=\\displaystyle\\frac{1}{2}\\sum_{j=1}^{N}\\log\\mathbb{E}\\|(e_{j}-B_{j})^{\\top}X\\|_{2}^{2}+\\mathrm{const.}}\\\\ &{\\qquad=\\displaystyle\\frac{1}{2}\\sum_{j=1}^{N}\\log(e_{j}-B_{j})^{\\top}\\Sigma^{0}(e_{j}-B_{j})+\\mathrm{const.}}\\\\ &{\\qquad=\\displaystyle\\frac{1}{2}\\sum_{j=1}^{N}\\log(e_{j}-B_{j})^{\\top}\\Sigma^{0}(e_{j}-B_{j})+\\mathrm{const.}}\\\\ &{\\qquad\\ge\\displaystyle\\frac{1}{2}\\sum_{j=1}^{N}\\log\\|(e_{j}-B_{j})\\|_{2}^{2}\\Delta\\mathrm{min}(\\Sigma^{0})+\\mathrm{const.}}\\\\ &{\\qquad\\ge2\\displaystyle\\frac{1}{2}\\sum_{j=1}^{N}\\log\\Lambda_{\\mathrm{min}}(\\Sigma^{0})>-\\infty}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Here, $e_{j}\\in\\mathbb{R}^{p}$ is a unit vector with the $j$ -th position equal to 1 and all other positions being zero, and $\\Lambda_{\\mathrm{min}}(\\Sigma^{0})$ is the minimum eigenvalue of $\\Sigma^{0}$ . Since $\\Sigma^{0}$ is positive definite, we have $\\Lambda_{\\mathrm{min}}(\\Sigma^{0})>0$ . Because $B$ is the adjacency matrix of a DAG, it follows that $B_{j j}=0$ , which implies $\\lVert e_{j}-B_{j}\\rVert\\geq$ $\\|e_{j}\\|=1$ . As a result, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\ell(B,\\Omega)\\ge\\ell(B)>-\\infty.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "B.13 Proof of Lemma 8 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof. Note that for a fixed $B$ , the corresponding optimal $\\Omega_{f}(B)=\\mathrm{diag}((I-B)^{\\top}\\Sigma^{0}(I-B))$ is the solution with respect to $\\ell(B,\\Omega)$ . Therefore, without causing confusion, we take $\\Omega_{f}(B)=$ ", "page_idx": 20}, {"type": "text", "text": "$\\mathrm{diag}((I-B)^{\\top}\\Sigma^{0}(I-B))$ and consider the log-likelihood as a function of $B$ only, i.e., $\\ell(B)$ , for simpler representation. See Equation (17) in Section C.1 for details. It is clear that $0\\in A_{3}$ , so we define $A_{4}\\,{\\overset{.}{=}}\\,\\{B\\mid\\ell(B)\\leq\\ell(0)\\}$ . Note that $\\ell(0)$ is finite. ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\ell(0)\\geq\\ell(B)=\\displaystyle\\sum_{j=1}^{p}\\log(e_{j}-B_{j})^{\\top}\\Sigma^{0}(e_{j}-B_{j})}}\\\\ &{}&{\\geq\\displaystyle\\sum_{j=1}^{p}\\log\\|e_{j}-B_{j}\\|^{2}\\Lambda_{\\operatorname*{min}}(\\Sigma^{0})}\\\\ &{}&{=\\log\\left[(\\Lambda_{\\operatorname*{min}}(\\Sigma^{0}))^{p}\\displaystyle\\prod_{j=1}^{p}\\|e_{j}-B_{j}\\|^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This indicates that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\prod_{j=1}^{p}\\|e_{j}-B_{j}\\|^{2}\\leq\\frac{\\exp(\\ell(0))}{(\\Lambda_{\\mathrm{min}}(\\Sigma^{0}))^{p}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Moreover, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|e_{k}-B_{k}\\|^{2}\\leq\\prod_{j=1}^{p}\\|e_{j}-B_{j}\\|^{2}\\leq{\\frac{\\exp(\\ell(0))}{\\Lambda_{\\operatorname*{min}}^{p}}}\\qquad\\forall k\\in\\{1,\\ldots,p\\}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This implies that $B_{k}$ must be bounded, and therefore every $B$ in $A_{4}$ is bounded. It is clear that ar $\\mathrm{g}\\,\\mathrm{min}_{B\\in A_{3}}\\,\\ell(B)\\,\\in\\,A_{4}$ . Thus, we need to show that $\\begin{array}{r}{\\operatorname*{min}_{B\\in A_{3}}\\ell(B)\\,=\\,\\operatorname*{min}_{B\\in A_{3}\\cap A_{4}}\\ell(B)\\,>\\,}\\end{array}$ $\\ell(B^{0})$ . Define ", "page_idx": 21}, {"type": "equation", "text": "$$\nA_{5}=\\{\\breve{B}\\mid\\mathrm{dist}(\\breve{B},\\mathcal{E}(\\Theta^{0}))\\geq\\frac{1}{2}\\operatorname*{min}_{B\\in\\mathcal{E}(\\Theta^{0})}\\mathrm{dist}(B,A_{3})\\}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "It is easy to see that $A_{3}\\subseteq A_{5}$ . Then, ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{B\\in A_{3}\\cap A_{4}}\\ell(B)\\ge\\operatorname*{min}_{B\\in A_{4}\\cap A_{5}}\\ell(B).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Note that $A_{4}\\cap A_{5}$ is closed, bounded, and nonempty $(0\\,\\in\\,A_{4}\\cap A_{5})$ , and $\\ell(B)$ is a continuous function of $B$ . Consequently, there exists at least one minimizer of $\\ell(B)$ in $A_{4}\\cap A_{5}$ . Combining this with the fact that $B\\notin A_{4}\\cap A_{5}$ for all $B\\in\\mathcal{E}(\\Theta^{0})$ , we conclude that: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{B\\in A_{3}}\\ell(B)\\ge\\operatorname*{min}_{B\\in A_{4}\\cap A_{5}}\\ell(B)>\\ell(B^{0}).\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "B.14 Proof of Corollary 1 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proof. By Theorem 1, we know there exists $\\lambda_{0}>0$ and $\\delta_{0}>0$ . For MCP, it can be transformed into quasi-MCP, by reparameterization from Section C.5. Then, combining these results together. ", "page_idx": 21}, {"type": "equation", "text": "$$\n0<\\lambda=\\lambda_{\\mathrm{mcp}}<\\lambda_{0},0<\\delta=a\\lambda_{\\mathrm{mcp}}<\\delta_{0}\\Rightarrow0<\\lambda_{\\mathrm{mcp}}<\\lambda_{0},0<a<\\frac{\\delta_{0}}{\\lambda_{\\mathrm{mcp}}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We could simple set a0 : \u03b4\u03bb0 and $(\\lambda_{\\mathrm{mcp}})_{0}=\\lambda_{0}$ . For SCAD, we just requires the following is satisfied to satisfies the pattern in the proof of Theorem 1. ", "page_idx": 21}, {"type": "equation", "text": "$$\n0<a\\lambda_{\\mathrm{scad}}<\\delta_{0},0<\\frac{\\lambda_{\\mathrm{scad}}^{2}(a+1)}{2}<\\lambda_{0}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "One simple choice is to let ", "page_idx": 21}, {"type": "equation", "text": "$$\na_{0}=(\\lambda_{\\mathrm{scad}})_{0}<\\operatorname*{min}\\{\\sqrt{\\delta_{0}},\\sqrt{\\lambda_{0}},1\\}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This completes the proof of Corollary 1. ", "page_idx": 21}, {"type": "text", "text": "C Additional Examples and Details ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this appendix, we provide the additional details of derivations, examples, concepts, and discussions referenced in the main paper. These include: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The derivation of the log-likelihood function for the model in Equation (6) (Appendix C.1).   \n\u2022 A brief introduction to the characterization of the equivalence class $\\mathcal{E}(\\Theta)$ (Appendix C.2).   \n\u2022 Examples demonstrating that the optimal solution for the least squares loss differs from the optimal solution of the log-likelihood (Appendix C.3).   \n\u2022 An example illustrating the estimation bias when the $\\ell_{1}$ penalty is applied (Appendix C.4).   \n\u2022 The formulations for quasi-MCP, MCP, and SCAD (Appendix C.5).   \n\u2022 The standardization of the random variable $X$ and the dataset $\\mathbf{X}$ (Appendix C.6).   \n\u2022 A detailed discussion of Assumptions A and B (Appendix C.7). ", "page_idx": 22}, {"type": "text", "text": "C.1 Log-likelihood of Model (6) ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "In this subsection, we detail the negative log-likelihood of the model in Equation (6). ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\ell_{n}(B,\\Omega)=-\\frac{1}{n}\\log\\frac{1}{\\sqrt{n}}(f(\\mathbf{x};B,\\Omega))}\\\\ &{=-\\frac{1}{n}\\log\\frac{1}{\\sqrt{n}}\\frac{1}{(2\\pi)^{n/2}(\\operatorname*{det}\\mathbb{C}/B,\\Omega))^{1/2}}\\exp\\left(-\\frac{\\mathbf{x}^{\\top}\\Theta(B,\\Omega)\\mathbf{x}_{\\perp}}{2}\\right)}\\\\ &{=\\frac{p}{2}\\log2\\pi+\\frac{1}{2}\\log\\mathrm{det}\\Sigma(B,\\Omega)+\\frac{1}{2n}\\sum_{i=1}^{n}\\exp(B,\\Omega)\\times_{i}}\\\\ &{=\\frac{1}{2}\\log\\mathrm{det}(I-B)^{-\\pi}\\Omega(I-B)^{-1}+\\frac{1}{2}\\operatorname{Tr}(\\Theta(B,\\Omega)(\\frac{\\sum_{i=1}^{n}\\mathbf{x}_{i}^{\\top}\\mathbf{x}_{i}}{n}))+\\cos\\mathrm{t}}\\\\ &{=\\frac{1}{2}\\log\\mathrm{det}\\Omega-\\log\\mathrm{det}(I-B)+\\frac{1}{2}\\operatorname{Tr}(\\hat{\\Sigma}\\Theta(B,\\Omega))+\\cos\\alpha.}\\\\ &{=\\frac{1}{2}\\displaystyle\\sum_{i=1}^{n}\\log\\mathrm{t}\\frac{1}{2}\\sum_{i=1}^{n}(\\mathbf{\\Gamma}(\\mathbf{\\mathcal{I}}^{-1}(I-B)^{\\top}\\hat{\\Sigma}(I-B))-\\log\\mathrm{det}(I-B)+\\cot\\alpha.}\\\\ &{=\\frac{1}{2}\\displaystyle\\sum_{i=1}^{n}\\log\\mathrm{t}\\alpha^{2}+\\frac{1}{2}\\displaystyle\\sum_{i=1}^{n}\\frac{(I-B)^{\\top}\\hat{\\Sigma}(I-B))_{i j}}{\\omega_{j}^{2}}-\\log\\mathrm{det}(I-B)+\\cot\\alpha.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "It is easy to know that for any fix $B$ , the optimal solution of $(\\omega_{j}^{*})^{2}$ can be written as: ", "page_idx": 22}, {"type": "equation", "text": "$$\n(\\omega_{j}^{*})^{2}=[(I-B)^{\\top}\\widehat\\Sigma(I-B)]_{j j}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, optimal solution $\\Omega_{f}(B)$ for any fixed $B$ can be written as: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\Omega_{f}(B)=\\mathrm{diag}((I-B)^{\\top}\\widehat\\Sigma(I-B))\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Let us define proflie sample log-likelihood $\\ell_{n}(B)$ as function of $B$ with such optimal $\\Omega(B)$ plugged in ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ell_{n}(B)=\\!\\frac{1}{2}\\log\\operatorname*{det}\\operatorname{diag}((I-B)^{\\top}\\widehat\\Sigma(I-B))-\\log\\operatorname*{det}(I-B)+\\operatorname{const}.}\\\\ &{\\qquad\\quad=\\!\\frac{1}{2}\\log\\frac{1}{n}\\|{\\mathbf{X}}_{j}-{\\mathbf{X}}B_{j}\\|_{2}^{2}-\\log\\operatorname*{det}(I-B)+\\operatorname{const}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Where $\\mathbf{X}=(\\mathbf{X}_{1},\\ldots,\\mathbf{X}_{p})=\\left(\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n}\\right)^{\\top}$ and corresponding profile population log-likelihood ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\ell(B)=\\frac{1}{2}\\log\\operatorname*{det}\\operatorname{diag}((I-B)^{\\top}\\Sigma(I-B))-\\log\\operatorname*{det}(I-B)+\\mathrm{const.}}\\\\ {\\displaystyle\\qquad=\\frac{1}{2}\\log\\mathbb{E}\\|X_{j}-B_{j}^{\\top}X\\|-\\log\\operatorname*{det}(I-B)+\\mathrm{const.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "C.2 Equivalence class $\\mathcal{E}(\\Theta)$ ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We provide a brief introduction to the equivalence class $\\mathcal{E}(\\Theta)$ , which has been extensively studied in [2]. We adopt the notation from [2], and further details can be found in that work. ", "page_idx": 23}, {"type": "text", "text": "Definition 6 (topological sort). a topological sort of a directed graph is an ordering on the nodes, often denoted by $\\prec$ , such that the existence of a directed edge $X_{k}\\to X_{j}$ implies that $X_{k}\\prec X_{j}$ in the ordering. ", "page_idx": 23}, {"type": "text", "text": "Let $\\mathcal{P}$ denote the collection of all permutations of the indices $\\{1,\\ldots,p\\}$ . For an arbitrary matrix $A$ and any $\\pi\\in\\mathcal{P}$ , let $P_{\\pi}A$ represent the matrix obtained by permuting the rows and columns of $A$ according to \u03c0, such that (P\u03c0A)ij = a\u03c0(i)\u03c0(j). ", "page_idx": 23}, {"type": "text", "text": "A DAG $B$ is said to be compatible with permutation $\\pi$ if $P_{\\pi}B$ is a lower-triangular matrix, which is equivalent to saying that $X_{k}\\to X_{j}$ in $B$ implies that $\\pi^{-1}(k)>\\pi^{-1}(j)$ . Similarly, $\\pi$ is also called compatible with $B$ . ", "page_idx": 23}, {"type": "text", "text": "For any positive definite matrix $\\Theta$ and $\\pi\\,\\in\\,\\mathcal{P}$ , the matrix $P_{\\pi}\\Theta$ represents the same covariance structure as $\\Theta$ , up to a reordering of the variables. The Cholesky decomposition of $P_{\\pi}(\\Theta)$ can be uniquely written as: ", "page_idx": 23}, {"type": "equation", "text": "$$\nP_{\\pi}\\Theta=(I-L)D^{-1}(I-L)^{\\top}=\\Theta_{f}(L,D),\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $L$ is strictly lower triangular and $D$ is diagonal. By Lemma 8 in [2], the following holds: ", "page_idx": 23}, {"type": "equation", "text": "$$\nP_{\\pi}\\Theta(L,D)=\\Theta(P_{\\pi}L,P_{\\pi}D)\\qquad\\forall\\pi\\in\\mathcal{P}.\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Therefore, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\Theta=\\Theta_{f}(P_{\\pi^{-1}}L,P_{\\pi^{-1}}D).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "For each $\\pi$ , we define: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\widetilde{B}(\\pi):=P_{\\pi^{-1}}L,}\\\\ {\\widetilde{\\Omega}(\\pi):=P_{\\pi^{-1}}D.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This suggests that for any $\\pi\\,\\in\\,\\mathcal{P}$ , there exists a pair $(\\widetilde{B}(\\pi),\\widetilde{\\Omega}(\\pi))\\,\\in\\,\\mathcal{E}(\\Theta)$ , where $\\widetilde{B}(\\pi)$ can be uniquely determined based on the permutation $\\pi$ and $\\Theta$ [2 ]. It is  important to emphasize  that different permutations, $\\pi_{1}\\neq\\pi_{2}$ , can still result in the same pairs, i.e., $(\\tilde{\\cal B}(\\pi_{1}),\\widetilde\\Omega(\\pi_{1})\\bar{)}=(\\widetilde B(\\pi_{2}),\\widetilde\\Omega(\\pi_{2}))$ . Furthermore, this indicates that for any $(B,\\Omega)\\in\\mathcal{E}(\\bar{\\Theta})$ , there exi sts at le ast one perm utation $\\pi$ such that $(B,\\Omega)=(\\widetilde{B}(\\pi),\\widetilde{\\Omega}(\\pi))$ . Moreover, it turns out that the collection of pair of $(\\widetilde{B}(\\pi),\\widetilde{\\Omega}(\\pi))$ forms the entire equiv alence  class $\\mathcal{E}(\\Theta)$ . ", "page_idx": 23}, {"type": "text", "text": "Lemma 9 (Lemma $1,2$ ). Suppose $\\Sigma$ is a positive definite covariance matrix and $\\Theta=\\Sigma^{-1}$ . Then, ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{E}(\\Theta)=\\{(P_{\\pi^{-1}}L,P_{\\pi^{-1}}D):P_{\\pi}\\Theta=\\Theta_{f}(L,D),\\pi\\in\\mathcal{P}\\}}\\\\ &{\\qquad=\\{(\\widetilde{B}(\\pi),\\widetilde{\\Omega}(\\pi)):\\pi\\in\\mathcal{P}\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This result indicates that the size of $\\mathcal{E}(\\Theta)$ is at most $p!$ , which is large but finite. ", "page_idx": 23}, {"type": "text", "text": "C.3 LS loss vs. log-likelihood ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "The following examples show that when the variances are unequal, the LS loss will not in general have the same minimizers as the log-likelihood. The first example is just Example 1 in [33]. Suppose $(X_{1},X_{2})$ is distributed according to the following linear SEM with unequal variances: ", "page_idx": 23}, {"type": "equation", "text": "$$\nX_{1}=\\epsilon_{1},\\qquad X_{2}=-\\frac{X_{1}}2+\\epsilon_{2},\\qquad\\epsilon_{1}\\sim N(0,1),\\qquad\\epsilon_{2}\\sim N(0,1/4).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Thus ", "page_idx": 23}, {"type": "equation", "text": "$$\nB^{0}=\\left(\\!\\!\\begin{array}{c c}{{0}}&{{-1/2}}\\\\ {{0}}&{{0}}\\end{array}\\!\\!\\right),\\quad\\Omega^{0}=\\left(\\!\\!\\begin{array}{c c}{{1}}&{{0}}\\\\ {{0}}&{{1/4}}\\end{array}\\!\\!\\right),\\quad\\Sigma^{0}=\\Sigma_{f}(B^{0},\\Omega^{0})=\\left(\\!\\!\\begin{array}{c c}{{1}}&{{-1/2}}\\\\ {{-1/2}}&{{1/2}}\\end{array}\\!\\!\\right)\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and also ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathcal{E}(\\Theta^{0})=\\mathcal{E}_{\\mathrm{min}}(\\Theta^{0})=\\{B^{0},B_{1}\\}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where ", "page_idx": 24}, {"type": "equation", "text": "$$\nB_{1}=\\left(\\!\\!\\begin{array}{l l}{{0}}&{{0}}\\\\ {{-1}}&{{0}}\\end{array}\\!\\!\\right),\\quad\\Omega_{1}=\\left(\\!\\!\\begin{array}{l l}{{1}}&{{0}}\\\\ {{0}}&{{1}}\\end{array}\\!\\!\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Moreover, $\\ell(B^{0},\\Omega^{0})=\\ell(B_{1},\\Omega_{1})$ , since both SEM represent the same covariance. ", "page_idx": 24}, {"type": "text", "text": "But it turns out that $\\mathbb{E}[\\|X-B_{1}^{\\top}X\\|^{2}]<\\mathbb{E}[\\|X-(B^{0})^{\\top}X\\|^{2}]$ : More precisely, it is easy to check that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~~\\mathbb{E}[\\|X-B_{1}^{\\top}X\\|^{2}]=\\mathrm{Tr}((I-B_{1})^{\\top}\\Sigma^{0}(I-B_{1}))=1,}\\\\ &{~~~~\\mathbb{E}[\\|X-(B^{0})^{\\top}X\\|^{2}]=\\mathrm{Tr}((I-B^{0})^{\\top}\\Sigma^{0}(I-B^{0}))=5/4,}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and moreover $B_{1}$ is the global minimizer of the LS loss $\\mathbb{E}[\\|X-B^{\\top}X\\|^{2}]$ . It follows that when the variances are different, the log-likelihood and LS loss have different global minimizers. ", "page_idx": 24}, {"type": "text", "text": "Similar calculations can be carried out for $d\\,>\\,2$ , but are tedious owing to the size of $\\mathcal{E}(\\Theta)$ . For example, here is an example of an SEM over 3 nodes such that the LS loss has a different set of global minimizers, but also the LS-global minimizer has more edges than the sparsest Markov representation: ", "page_idx": 24}, {"type": "equation", "text": "$$\nB^{0}=\\left(\\!\\!\\begin{array}{c c c}{{0}}&{{0}}&{{-3/10}}\\\\ {{0}}&{{0}}&{{-2}}\\\\ {{0}}&{{0}}&{{0}}\\end{array}\\!\\!\\right),\\quad\\Omega^{0}=\\left(\\!\\!\\begin{array}{c c c}{{7}}&{{0}}&{{0}}\\\\ {{0}}&{{3}}&{{0}}\\\\ {{0}}&{{0}}&{{2}}\\end{array}\\!\\!\\right),\\quad\\Sigma^{0}=\\Sigma_{f}(B^{0},\\Omega^{0})=\\left(\\!\\!\\begin{array}{c c c}{{7}}&{{0}}&{{-2}}\\\\ {{0}}&{{3}}&{{-5}}\\\\ {{-2}}&{{-5}}&{{10}}\\end{array}\\!\\!\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "For this model, LS loss selects the following SEM with 3 edges: ", "page_idx": 24}, {"type": "equation", "text": "$$\nB_{1}=\\left(\\begin{array}{c c c}{{0}}&{{0}}&{{0}}\\\\ {{-1.197}}&{{0}}&{{-1.589}}\\\\ {{-0.7532}}&{{0}}&{{0}}\\end{array}\\right).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "We have $B^{0}\\in\\mathcal{E}_{\\operatorname*{min}}(\\Theta^{0})$ , but $B_{1}\\not\\in\\mathcal{E}_{\\operatorname*{min}}(\\Theta^{0})$ . ", "page_idx": 24}, {"type": "text", "text": "C.4 Estimation bias under $\\ell_{1}$ ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We provide an example showing that when the $\\ell_{1}$ penalty is applied, the estimation becomes biased. Therefore, $\\ell_{1}$ should not be used. Consider the following linear Structural Equation Model (SEM): ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\left\\{{\\begin{array}{l}{X_{1}={\\mathcal{N}}(0,1)}\\\\ {X_{2}=X_{1}+{\\mathcal{N}}(0,\\sigma^{2})}\\end{array}}\\right.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "If the topological sort is known, i.e., $X_{1}\\rightarrow X_{2}$ , and an $\\ell_{1}$ penalty is used for minimizing the negative log-likelihood: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{a}\\log\\mathbb{E}[\\|X_{2}-a X_{1}\\|_{2}^{2}]+\\log\\mathbb{E}[\\|X_{1}\\|_{2}^{2}]+\\lambda|a|\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Ideally, we would expect $a=1$ to be the minimal solution to the loss function. However, the problem is equivalent to: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\log((1-a)^{2}+\\sigma^{2})+\\lambda|a|\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "It is clear that $a=1$ is not the minimal solution to the loss function, as the derivative at $a=1$ is nonzero for any $\\lambda>0$ , indicating that the $\\ell_{1}$ penalty leads to a biased estimator. This bias does not occur when using MCP or SCAD with appropriate hyperparameters. ", "page_idx": 24}, {"type": "text", "text": "C.5 quasi-MCP, MCP and SCAD ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "We present the formulas for quasi-MCP, MCP, and SCAD, and demonstrate that quasi-MCP and MCP are equivalent. ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{\\lambda,\\delta}(t)=\\lambda\\left[\\left(|t|-\\frac{t^{2}}{2\\delta}\\right)\\mathbb{1}(|t|<\\delta)+\\frac{\\delta}{2}\\mathbb{1}(|t|>\\delta)\\right]}\\\\ &{p_{\\lambda,a}^{M C P}(t)=\\mathbb{1}(|t|<a\\lambda)\\left(\\lambda|t|-\\frac{t^{2}}{2a}\\right)+\\mathbb{1}(|t|\\geq\\lambda a)\\frac{\\lambda^{2}a}{2}}\\\\ &{p_{\\lambda,a}^{S C A D}(t)=\\lambda|t|\\mathbb{1}(|t|<\\lambda)+\\mathbb{1}(\\lambda<|t|<a\\lambda)\\frac{2a\\lambda|t|-t^{2}-\\lambda^{2}}{2(a-1)}+\\mathbb{1}(|t|\\geq\\lambda a)\\frac{\\lambda^{2}(a+1)}{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "image", "img_path": "TMlGQw7EbC/tmp/f2555e22bc74c040fbbf251b34b8fb969e1f88b26a88331bf47da8f903cfbee3.jpg", "img_caption": ["Figure 4: The plot of $p_{\\lambda,\\delta}(t)$ with $\\lambda=2,\\delta=1$ "], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Iwt ei ss ewt tiinn g MthCaPt,  itfh ewne $\\delta$ $a\\lambda$ . MTChuP,s ,t hqeuna $p_{\\lambda,a\\lambda}(t)=p_{\\lambda,a}^{M C P}(t)$ .  eIqn uaivnaoltehnetr  two aeya, cihf $\\begin{array}{r}{a=\\frac{\\delta}{\\lambda}}\\end{array}$ $p_{\\lambda,\\frac{\\delta}{\\lambda}}^{M C P}(t)=p_{\\lambda,a}(t)$   \nother. ", "page_idx": 25}, {"type": "text", "text": "C.6 Standardization of $X$ and X ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We present the formulas for the standardization of $X$ and the standardization of the corresponding dataset $\\mathbf{X}$ . ", "page_idx": 25}, {"type": "text", "text": "Let $\\sigma_{i}^{2}\\,:=\\,\\mathrm{Var}(X_{i})$ and $D:=\\,\\mathrm{diag}(\\sigma_{1},\\ldots,\\sigma_{p})$ . Denote the standardized version of $X$ as $X_{\\mathrm{std}}$ , which can be expressed as: ", "page_idx": 25}, {"type": "equation", "text": "$$\nX_{\\mathrm{std}}=D^{-1}(X-\\mathbb{E}X).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For $\\mathbf{X}\\in\\mathbb{R}^{n\\times p}$ , we can write $\\mathbf{X}=(\\mathbf{X}_{i j})=(\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n})^{\\top}=(\\mathbf{X}_{1},\\ldots,\\mathbf{X}_{p})$ , and define sample average for node $j$ as ${\\widehat{\\mu}}_{j}$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\widehat{\\mu}_{j}=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{X}_{i j}\\qquad\\forall j\\in[p].\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Next, we define the sample variance for node $j$ as ", "page_idx": 25}, {"type": "equation", "text": "$$\n{\\widehat{\\sigma}}_{j}^{2}={\\frac{1}{n-1}}\\sum_{i=1}^{n}(\\mathbf{X}_{i j}-{\\widehat{\\mu}}_{j})^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The diagonal matrix of sample standard deviations is then ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\widehat{\\boldsymbol{D}}=\\mathrm{diag}(\\widehat{\\sigma}_{1},\\ldots,\\widehat{\\sigma}_{p}).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Finally, we standardize $\\mathbf{X}$ by subtracting the sample means and scaling by the inverse of $\\widehat{D}$ : ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbf{Z}=[\\mathbf{X}-\\mathbf{1}_{n}\\cdot({\\widehat{\\mu}}_{1},\\ldots,{\\widehat{\\mu}}_{p})]{\\widehat{D}}^{-1},\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\mathbf{1}_{n}\\in\\mathbb{R}^{n}$ is an $n$ -dimensional vector with all entries equal to 1. ", "page_idx": 25}, {"type": "text", "text": "C.7 Discussion of Assumption A, B ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this subsection, we provide a more detailed discussion of Assumptions A and B. ", "page_idx": 25}, {"type": "text", "text": "First, it is important to emphasize that if $p_{\\lambda,\\delta}$ is replaced by the $\\ell_{0}$ penalty in (12) or (14), then Assumptions A and B can be omitted, and all the results still hold. In this case, the proof would be significantly simplified. However, the use of the differentiable quasi-MCP, in contrast to the $\\ell_{0}$ penalty, introduces substantial complications, necessitating some additional assumptions that are fundamentally different from the results in [8]. Specifically, Assumptions A and B are exactly what is required to make the problem suitable for gradient-based optimization. ", "page_idx": 26}, {"type": "text", "text": "Assumption A is not restrictive and is satisfied by all identifiable models, including linear Gaussian models, generalized linear models with continuous output [66], binary output [74, 4], and most exponential families. However, the requirement for the finiteness of the equivalence class can be relaxed. What is truly needed is that the minimal nonzero edge has sufficient \u201csignal,\u201d i.e., ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{(\\psi,\\xi)\\in\\mathcal{E}(\\psi^{0},\\xi^{0})}\\operatorname*{min}_{\\{(i,j):B(\\psi)_{i j}\\neq0\\}}|B(\\psi)|_{i j}>0\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "This is trivially true when $|\\mathcal{E}(\\psi^{0},\\xi^{0})|$ is finite. When $|\\mathcal{E}(\\psi^{0},\\xi^{0})|$ is infinite, each $|B(\\psi)|_{i j}$ could be positive, but it is possible lim $\\operatorname*{inf}_{\\substack{\\ldots}}(\\psi,\\xi){\\underline{{\\in}}}\\mathcal{E}(\\psi^{0},\\xi^{0})\\operatorname*{min}_{\\substack{\\ldots}}\\{(i,j){\\colon}B(\\psi)_{i j}{\\neq}0\\}.|B(\\psi)|_{i j}=0$ , because $|B(\\psi)|_{i j}$ can be arbitrarily small. The $\\ell_{0}$ penalty deals with this with its discontinuity at zero, whereas the continuity of quasi-MPC makes this more challenging. This is the cost of differentiability, which we argue is worthwhile ", "page_idx": 26}, {"type": "text", "text": "Assumption B is a standard assumption in the optimization literature [7] and is generally quite weak. Moreover, it is almost necessary because quasi-MCP cannot exactly count the number of edges in $B(\\psi)$ . The magnitude of the quasi-MCP penalty does not directly reveal the number of edges. This is the trade-off for replacing the $\\ell_{0}$ penalty with a fully differentiable sparsity-inducing penalty. Finally, it is worth noting that this assumption can also be relaxed: what is truly required is that for any $\\epsilon>0$ , there exists $\\delta>0$ such that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\ell(\\psi,\\xi)-\\ell(\\psi^{0},\\xi^{0})>\\delta\\quad\\mathrm{for~all~}\\{(\\psi,\\xi)\\mid\\mathrm{dist}((\\psi,\\xi),\\mathcal{E}(\\psi^{0},\\xi^{0}))>\\epsilon\\}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "In other words, we require a loss gap when $(\\psi,\\xi)$ is not in $\\mathcal{E}(\\psi^{0},\\xi^{0})$ . This can be inferred from Assumption B. ", "page_idx": 26}, {"type": "text", "text": "The following is the proof when $p_{\\lambda,\\delta}$ is replaced with the $\\ell_{0}$ penalty. ", "page_idx": 26}, {"type": "text", "text": "Proof when $p_{\\lambda,\\delta}$ is replaced with $\\ell_{0}$ : ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Proof. We can also assume that $|\\mathcal{E}_{\\mathrm{min}}(\\psi^{0},\\xi^{0})|=1$ , meaning $\\mathcal{E}_{\\mathrm{min}}(\\psi^{0},\\xi^{0})=\\{(\\psi^{0},\\xi^{0})\\}$ . In other words, there is a unique element in the minimal equivalence class. This is because any element in $\\mathcal{E}_{\\mathrm{min}}(\\psi^{0},\\xi^{0})$ is indistinguishable based on the score function, i.e., the value of $\\ell(\\psi,\\xi)$ and the penalty for the number of edges in $B(\\psi)$ . Our objective is to find this unique element by solving equation (12) in the paper, which simplifies the proof. ", "page_idx": 26}, {"type": "text", "text": "When $s_{B(\\psi^{0})}=0$ , the result is straightforward: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\ell(\\psi^{0},\\xi^{0})+s_{B(\\psi^{0})}\\leq\\ell(\\psi,\\xi)+s_{B(\\psi)}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Now, let us consider the more general case where $s_{B(\\psi^{0})}>0$ and divide the parameter space into three regions: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{4_{1}=\\{(\\psi,\\xi)\\mid s_{B(\\psi)}>s_{B(\\psi^{0})}\\},\\quad A_{2}=\\{(\\psi,\\xi)\\mid s_{B(\\psi)}=s_{B(\\psi^{0})}\\},\\quad A_{3}=\\{(\\psi,\\xi)\\mid s_{B(\\psi)}<s_{B(\\psi^{0})}\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Case 1: Consider $A_{1}$ . Since $\\ell(\\psi^{0},\\xi^{0})\\leq\\ell(\\psi,\\xi)$ , the following holds for any $\\lambda>0$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\ell(\\psi^{0},\\xi^{0})+\\lambda s_{B(\\psi^{0})}<\\ell(\\psi,\\xi)+\\lambda s_{B(\\psi)}\\quad\\forall(\\psi,\\xi)\\in A_{1}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Case 2: Consider $A_{2}$ . Since $|\\mathcal{E}_{\\mathrm{min}}(\\psi^{0},\\xi^{0})|\\,=\\,1$ , it follows that for all $(\\psi,\\xi)\\,\\in\\,\\mathcal{E}(\\psi^{0},\\xi^{0})$ and $(\\psi,\\xi)\\neq(\\psi^{0},\\xi^{0})$ , we have $s_{B(\\psi)}>s_{B(\\psi^{0})}$ . Therefore, for all $(\\psi,\\xi)\\in A_{2}$ , it holds that $\\ell(\\psi^{0},\\dot{\\xi}^{0})<$ $\\ell(\\psi,\\xi)$ . Consequently, for any $\\lambda>0$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\ell(\\psi^{0},\\xi^{0})+\\lambda s_{B(\\psi^{0})}<\\ell(\\psi,\\xi)+\\lambda s_{B(\\psi)}\\quad\\forall(\\psi,\\xi)\\in A_{2}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Case 3: Consider $A_{3}$ . We need to prove that: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\ell(\\psi^{0},\\xi^{0})+\\lambda s_{B(\\psi^{0})}<\\ell(\\psi,\\xi)+\\lambda s_{B(\\psi)}\\quad\\forall(\\psi,\\xi)\\in A_{3}.\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "This is equivalent to showing that there exists a positive $\\lambda$ such that: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\lambda<\\frac{\\ell(\\psi,\\xi)-\\ell(\\psi^{0},\\xi^{0})}{s_{B(\\psi^{0})}-s_{B(\\psi)}}\\quad\\forall(\\psi,\\xi)\\in A_{3}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since ${\\mathcal{S}}_{B(\\psi^{0})}$ is the minimal number of edges in the equivalence class, any $(\\psi,\\xi)\\in A_{3}$ corresponds to $B(\\psi)$ with a number of edges strictly less than ${\\mathcal{S}}_{B(\\psi^{0})}$ . This implies that: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\ell(\\psi,\\xi)-\\ell(\\psi^{0},\\xi^{0})>0\\quad\\forall(\\psi,\\xi)\\in A_{3}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Furthermore, we have $1\\,\\leq\\,s_{B(\\psi^{0})}-s_{B(\\psi)}\\,\\leq\\,s_{B(\\psi^{0})}$ , which implies that there exists a small but positive $\\lambda$ that satisfies the inequality. ", "page_idx": 27}, {"type": "text", "text": "D Experiment Details ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section, we provide all the details about the experiments. These include: (1) the types of graphs used, (2) the process for generating the samples, (3) the baseline methods we compare against and where to find the code for these methods, (4) the implementation details of our method and how to replicate the results, and (5) the metrics used to evaluate the estimation. ", "page_idx": 27}, {"type": "text", "text": "D.1 Experimental Setting ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In this section, we outline the process for generating graphs and data for Structural Equation Models (SEMs) in (2). For each model, a random graph $G$ is generated using one of two types of random graph models: Erd\u02ddos-R\u00e9nyi (ER) or Scale-Free (SF). The models are specified to have, on average, $k p$ edges, where $k\\in\\{1,2,4\\}$ . These configurations are denoted as $\\mathrm{ER}k$ or $\\mathrm{SF}k$ , respectively. ", "page_idx": 27}, {"type": "text", "text": "\u2022 Erdo\u02dds-R\u00e9nyi (ER), Random graphs whose edges are add independently with equal probability. We simulated models with $p,2p$ and $4p$ edges (in expectation) each, denoted by $E R1,E R2$ , and $E R4$ respectively.   \n\u2022 Scale-free network(SF). Network simulated according to the preferential attachment process [3]. We simulated scale-free network with $p,2p$ and $4p$ edges and $\\beta=1$ , where $\\beta$ is the exponent used in the preferential attachment process. ", "page_idx": 27}, {"type": "text", "text": "Linear SEMs. Given a random DAG $B\\in\\{0,1\\}^{p\\times p}$ from one of these two graph models, edge weights were assigned independently from $\\mathrm{Jnif}([-1.5,-0.5]\\cup[0.5,1.5])$ to obtain a weight matrix $\\b{B}\\in\\mathbb{R}^{p\\times p}$ . Given $B$ , we sampled $X=B^{\\top}X+z\\in\\mathbb{R}^{p}$ according to: ", "page_idx": 27}, {"type": "text", "text": "\u2022 Gaussian noise with unequal variance (Gauss-NV): $z_{i}\\,\\sim\\mathcal{N}(0,\\sigma_{i}^{2}),i\\,=\\,1,\\ldots,p$ where $\\sigma_{i}\\sim\\mathrm{Unif}[0.1,0.7]$ ", "page_idx": 27}, {"type": "text", "text": "We chose to set $\\sigma_{i}$ , the noise variances in our models, to be relatively smaller compared to the settings used in previous studies such as [73], [41], and [4]. This decision aims to mitigate the potential exploitation of accumulated variance along the topological sort, as highlighted in [55]. ", "page_idx": 27}, {"type": "text", "text": "Generalized Linear Model with Binary Output Given a random DAG $B\\in\\{0,1\\}^{p\\times p}$ from one of these two graph models, edge weights were assigned independently from $\\mathrm{Unif}([-1.5,-0.5]\\cup$ [0.5, 1.5]) to obtain a weight matrix $\\bar{B_{}}\\in\\mathbb{R}^{p\\times p}$ . Given $B$ , we sample $X_{j}$ according to the following ", "page_idx": 27}, {"type": "equation", "text": "$$\nX_{j}=\\operatorname{Bernoulli}(\\exp(B_{j}^{\\top}X)/(1+\\exp(B_{j}^{\\top}X)))\\quad j=1,\\ldots,p\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $B_{j}$ is $j$ -th column of $B$ . The corresponding negative log-likelihood function: ", "page_idx": 27}, {"type": "equation", "text": "$$\ns(B;{\\mathbf X})=\\frac{1}{n}\\sum_{i=1}^{p}\\mathbf{1}_{n}^{\\top}\\left(\\log(\\mathbf{1}_{n}+\\exp({\\mathbf X}B))-{\\mathbf X}_{i}\\circ({\\mathbf X}B)\\right)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where $\\mathbf{X}=(\\mathbf{X}_{i j})=(\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n})^{\\top}=(\\mathbf{X}_{1},\\ldots,\\mathbf{X}_{p})$ ", "page_idx": 27}, {"type": "text", "text": "Nonlinear Models with Neural Networks. We primarily follow the nonlinear setting described in Zheng et al. [74]. Given $G$ , we simulate the SEM as follows: ", "page_idx": 28}, {"type": "equation", "text": "$$\nX_{j}=f_{j}(X_{\\mathrm{pa}(j)})+N_{j}\\qquad\\forall j\\in[p],\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $N_{j}\\,\\sim\\,{\\mathcal N}(0,\\sigma_{i}^{2})$ and $\\sigma_{i}\\,\\sim\\,\\mathrm{Uni}[0.1,1]$ . Here, $f_{j}$ is a randomly initialized MLP with one hidden layer of size 100 and sigmoid activation. It is worth noting that the score function used in nonlinear-NOTEARS [74] is least square loss: ", "page_idx": 28}, {"type": "equation", "text": "$$\ns(f,\\mathbf{X})=\\frac{1}{2n}\\sum_{i=1}^{p}\\|\\mathbf{x}_{i}-\\widehat{f}_{i}(\\mathbf{X})\\|^{2},\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where each $\\widehat{f}_{i}$ is an MLP with one hidden layer of size 30 and sigmoid activation. ", "page_idx": 28}, {"type": "text", "text": "Simulation We generated random datasets $\\mathbf{X}\\in\\mathbb{R}^{n\\times p}$ by sampling rows i.i.d. from the models described above. For each simulation, we produced datasets with $n$ samples across graphs with $p$ nodes. ", "page_idx": 28}, {"type": "text", "text": "\u2022 Linear Model: $p=\\{10,20,50,70,100\\}$ , $k=\\{1,2,4\\}$ , $n=1000$ and graph types $=$ {ER, SF}.   \n\u2022 Generalized Linear Model: $p=\\{10,20,40\\}$ , $k=\\{1,2\\}$ , $n=10000$ and graph types $=$ {ER, SF}.   \n\u2022 Nonlinear Model: $p=\\{10,20,40\\}$ , $k=\\{1,2\\}$ , $n=1000$ and graph type $\\mathbf{\\sigma}_{\\mathrm{3}}=\\{{\\mathrm{ER}},{\\mathrm{SF}}\\}$ . ", "page_idx": 28}, {"type": "text", "text": "For each dataset, we applied several structural learning algorithms, including fast greedy equivalence search (FGES [52]), constraint-based methods (PC [60]), NOTEARS [73, 74] (using least squares loss), GOLEM [41] (using NLL with $\\ell_{1}$ penalty), VarSort [55], causal additive models (CAM [9]), LOGLL(-NOTEARS/DAGMA)-SAMPLE (utilizing the sample covariance matrix $\\widehat{\\Sigma}^{\\prime}$ ), LOGLL(- NOTEARS/DAGMA)-POPULATION (using the population covariance matrix $\\Sigma$ ) and exact method (EXACT-SEARCH). Implementation details are provided in the following paragraph. After running the algorithms, a post-processing threshold of 0.3 was applied to the estimated matrix $B_{\\mathrm{est}}$ to prune small values, following the methodology in [73, 74]. ", "page_idx": 28}, {"type": "text", "text": "Implementation The implementation details of baseline are listed below: ", "page_idx": 28}, {"type": "text", "text": "\u2022 Fast Greedy Equivalence Search (FGES [52]) is based on greedy search and assumes linear dependency between variables. The implementation is based on the py-tetrad package, available at https://github.com/cmu-phil/py-tetrad. We use BIC as the score function with default parameters.   \n\u2022 PC [60] is constraint-based method and based on uses conditional independence induced by causal relationships to learn those causal relationships. The implementation is based on the py-tetrad package, available at https://github.com/cmu-phil/py-tetrad. We use Fisher- $\\mathcal{L}$ test with $\\alpha=0.5$ .   \n\u2022 NOTEARS[73, 74] is the continuous DAG learning algorithm using least square loss with $\\ell_{1}$ regularization. It is implemented in python: https://github.com/xunzheng/notears.   \n\u2022 GOLEM [41] is implemented using Python and TensorFlow. The code is available https://github.com/ignavierng/golem.   \n\u2022 VarSort [55] is based on the observation that variances tend to accumulate along the topological sort. It uses Lasso [61] to recover the coefficients. The code is implemented in Python and is available https://github.com/Scriddie/Varsortability.   \n\u2022 DAGMA[4] is a continues DAG learning algorithm with better accuracy and faster computational speed. It also use least square loss with $\\ell_{1}$ penalty as NOTEARS. The implementation is available at https://github.com/kevinsbello/dagma.   \n\u2022 Causal additive model (CAM [9]) learns an addititve SEM by leveraging efficient nonparametric regression techiques and greedy search over edges. The code is implemented in R, and avaiable at https://rdrr.io/cran/CAM/man/CAM.html ", "page_idx": 28}, {"type": "text", "text": "\u2022 LOGLL(-NOTEARS/DAGMA)-SAMPLE/POPULATION is our approach, which modifies the original NOTEARS or DAGMA algorithm by replacing its scoring function. Instead of using the least squares loss with an $\\ell_{1}$ penalty, it employs a log-likelihood function that includes a quasi-MCP penalty, as defined in (10). For LOGLL(-NOTEARS/DAGMA)- SAMPLE, we use the sample covariance matrix $\\hat{\\Sigma}$ in the score function. In contrast, LOGLL(- NOTEARS/DAGMA)-POPULATION uses the true covariance matrix $\\Sigma$ as a baseline approach. In this paper, LOGLL-NOTEARS refers to solving (12) using NOTEARS with NLL and quasiMCP as the score function, while LOGLL-DAGMA refers to solving (12) using DAGMA with NLL and quasi-MCP as the score function. ", "page_idx": 29}, {"type": "text", "text": "\u2022 EXACT-SEARCH is used to indicate that the optimization problem (12) is solved exactly. This approach is feasible only for small graphs, where we attempt to calculate all possible configurations $\\tilde{B}(\\pi)$ as defined in Section C.2. These calculations can be performed using Cholesky Decomposition or Ordinary Least Squares (OLS). The label POPULATION signifies that the operation is based on the population covariance matrix $\\Sigma$ , while SAMPLE denotes that it is based on the sample covariance matrix $\\widehat{\\Sigma}$ . The Structural Hamming Distance (SHD) for EXACT-SEARCH is calculated on an average basis. This involves identifying the set $\\mathcal{M}_{\\operatorname*{min}}(\\Theta)$ or $\\mathcal{M}_{\\mathrm{min}}(\\widehat{\\Theta})$ , calculating the SHD for each DAG within this set, and then computing the average SHD. ", "page_idx": 29}, {"type": "text", "text": "D.2 Implementation of LogLL(-NOTEARS/DAGMA)-population/sample ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Linear model There are two main challenges in solving (14). The first challenge is that (12) is a highly nonconvex optimization problem and is sensitive to initialization. If we randomly initialize or set the initialization to zero, as done in [73, 74, 41], LOGLL-NOTEARS/DAGMA often gets stuck at a local optimal solution. The second challenge arises from Theorem 1, where we are advised to select $\\lambda$ and $\\delta$ such that $0<\\lambda<\\lambda_{0}$ and $0<\\delta<\\delta_{0}$ . Theoretically, smaller values for $\\lambda$ and $\\delta$ should be used to adhere to the theorem\u2019s guidelines, however, in practice, solving the optimization problem (12) to global optimality is not always feasible. ", "page_idx": 29}, {"type": "text", "text": "To address the first challenge, we adopt the approach from $\\mathrm{Ng}$ et al. [41]. We first run NOTEARS (with least squares loss and $\\ell_{1}$ penalty) or DAGMA (with least squares loss and $\\ell_{1}$ penalty) to obtain a \"good\" initialization point. Then, we apply LOGLL(-NOTEARS/DAGMA)-POPULATION/SAMPLE to obtain the final output. ", "page_idx": 29}, {"type": "text", "text": "To address second challenge, we use warm starts. We begin with larger values for $\\lambda$ and $\\delta$ and solve (12) using LOGLL-NOTEARS/DAGMA to obtain an initial $B_{\\mathrm{est}}$ . We then reduce $\\lambda$ and $\\delta$ by a factor of $\\gamma<1$ and use the previous output as the starting point for the next iteration of LOGLLNOTEARS/DAGMA. This process is repeated until the negative log-likelihood $\\ell_{n}(B_{\\mathrm{est}},\\Omega_{\\mathrm{est}})$ begins to increase. This iterative approach helps to refine the solutions gradually, ensuring that each step starts from a potentially better approximation, as formally outlined in Algorithm 1. ", "page_idx": 29}, {"type": "text", "text": "In Algorithm 1, we detail the complete implementation of LOGLL(-NOTEARS/DAGMA)-SAMPLE. By replacing $\\widehat{\\Sigma}$ with $\\Sigma$ and substituting $\\ell_{n}$ with $\\ell$ , this algorithm is adapted to the full implementation of LOGLL(-NOTEARS/DAGMA)-POPULATION. ", "page_idx": 29}, {"type": "text", "text": "It turns out that LOGLL-DAGMA outperforms LOGLL-NOTEARS in our experiments. Therefore, we present the results from LOGLL-DAGMA. ", "page_idx": 29}, {"type": "text", "text": "Nonlinear model We utilize LOGLL-NOTEARS, which uses the same optimization framework in [74] with replacement of least square loss to negative log-likelihood loss, we keep other hyperparameter unchanged. The score function (negative log-likelihood) we use ", "page_idx": 29}, {"type": "equation", "text": "$$\ns_{N L L}(f,\\mathbf{X})=\\frac{1}{2n}\\sum_{i=1}^{d}\\log\\left(\\|\\mathbf{x}_{i}-\\widehat{f}_{i}(\\mathbf{X})\\|^{2}\\right)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Here $\\hat{f}_{i}$ is $i$ -th MLP with one hidden layer of size 40 and sigmoid activation. ", "page_idx": 29}, {"type": "text", "text": "Standardized data $\\mathbf{Z}$ Although it has been shown that the log-likelihood score is scale-invariant for the linear model with Gaussian noise (see Theorem 3), it was observed that using standardized ", "page_idx": 29}, {"type": "text", "text": "Input: Sample covariance $\\widehat{\\Sigma}$ , decay factor $\\gamma\\in(0,1)$ , and $\\lambda,\\delta$ , initial point $(B_{\\mathrm{in}},\\Omega_{\\mathrm{in}})$ , initial loss $\\ell_{\\mathrm{in}}$ (typically v ery large) // initial point $(B_{\\mathrm{in}},\\Omega_{\\mathrm{in}})$ is obtained from NOTEARS or DAGMA Output: $(B_{\\mathrm{est}},\\Omega_{\\mathrm{est}})$ 1 while True do 2 Solve LOGLL-NOTEARS or LOGLL-DAGMA with input $(B_{\\mathrm{in}},\\Omega_{\\mathrm{in}})$ , and get output $(B_{\\mathrm{out}},\\Omega_{\\mathrm{out}})$ 3 Calculate $\\ell_{n}(B_{\\mathrm{out}},\\Omega_{\\mathrm{out}})$ 4 if $\\ell_{i n}>\\ell_{n}(B_{o u t},\\Omega_{o u t})$ then 5 $\\ell_{\\mathrm{in}}\\gets\\ell_{n}(B_{\\mathrm{out}},\\Omega_{\\mathrm{out}})$ 6 \u03bb \u2190\u03b3\u03bb 7 \u03b4 \u2190\u03b3\u03b4 8 $(B_{\\mathrm{in}},\\Omega_{\\mathrm{in}})\\leftarrow(B_{\\mathrm{out}},\\Omega_{\\mathrm{out}})$ 9 else 10 return $(B_{\\mathrm{in}},\\Omega_{\\mathrm{in}})$ 11 end 12 end ", "page_idx": 30}, {"type": "text", "text": "data $\\mathbf{Z}$ makes solving the optimization problem (12) significantly more challenging. For the LOGLLNOTEARS implementation, the LBFGS-B algorithm fails to produce meaningful solutions. As a result, we replaced LBFGS-B with ADAM [27], an optimizer better suited for handling the difficulties of standardized data, to solve the subproblem in NOTEARS. Alternatively, directly using LOGLLDAGMA is another effective way to address this challenge. Empirically, we find that setting $\\gamma=0.8$ , $\\lambda\\:=\\:0.4$ , and $\\delta\\:=\\:0.2$ usually serves as a good choice for the parameters in our optimization procedures. ", "page_idx": 30}, {"type": "text", "text": "D.3 Metrics ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We evaluate the performance of each algorithm with the following three metrics: ", "page_idx": 30}, {"type": "text", "text": "\u2022 Structure Hamming distance (SHD): A standard benchmark in the structure learning literature that counts the total number of edges additions, deletions, and reversals needed to convert the estimated graph into the true graph. Since our model specified in (6) is unidentifiable, the Structural Hamming Distance (SHD) is calculated with respect to the completed partially directed acyclic graph (CPDAG) of the ground truth and $B_{\\mathrm{est}}$ . We utilize the code from Zheng et al. [75]. ", "page_idx": 30}, {"type": "text", "text": "\u2022 Times: The amount of time the algorithm takes to run, measured in seconds. This metric is used to evaluate the speed of the algorithms. ", "page_idx": 30}, {"type": "text", "text": "E Additional Results ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "E.1 Linear Model (SHD) ", "page_idx": 31}, {"type": "image", "img_path": "TMlGQw7EbC/tmp/8333c01f8ace8efccc417befa6af87bae872c456476fee0e950b5e205d6a27ae.jpg", "img_caption": [], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "Figure 5: Structural Hamming Distance (SHD, with lower values indicating better performance) between Markov equivalence classes (MEC) of recovered and ground truth graphs for ER-2 graphs with 8 nodes. Here Exact-search is added to illustrate Theorem 3. Standardization does not affect the DAG structure if the optimization (10) can be solved globally. Both Exact-sample and Exactpopulation produce the same DAG structure for raw data $\\mathbf{X}$ and standardized data $\\mathbf{Z}$ . When the population covariance matrix is known, $\\mathcal{E}_{\\operatorname*{min}}(\\Theta^{0})=\\mathcal{M}(G^{0})$ , resulting in an SHD of zero. The poor performance of Exact-sample can be attributed to the lack of thresholding applied to the coefficients recovered from Ordinary Least Squares (OLS). Since $\\widehat{\\Sigma}$ is only an approximation of $\\Sigma$ , coefficients derived from OLS based on different permutations $\\pi$ may shift from zero to nonzero, even though such coefficients might be very small. However, since Exact is impractical for real-world applications, we use this example primarily for illustrative purposes, and thus no threshold is applied to this method. ", "page_idx": 31}, {"type": "image", "img_path": "TMlGQw7EbC/tmp/bb4e16514b79221706e407b2d9ceb3a9fa4c9db2a486ecdfe00ac6dfca769883.jpg", "img_caption": [], "img_footnote": [], "page_idx": 32}, {"type": "text", "text": "Figure 6: Comparison of raw (orange) vs. standardized (green) data. Structural Hamming Distance (SHD, with lower values indicating better performance) between Markov equivalence classes (MEC) of recovered and ground truth graphs for ER-2 graphs with 5 nodes ", "page_idx": 32}, {"type": "image", "img_path": "TMlGQw7EbC/tmp/63c5a75054b0b20d0ff30059af92c11bf87a79913c42e46672a48efed36be160.jpg", "img_caption": ["Figure 7: Comparison of raw (orange) vs. standardized (green) data. Structural Hamming Distance (SHD, with lower values indicating better performance) between Markov equivalence classes (MEC) of recovered and ground truth graphs for ER-2 graphs with 20 nodes "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "TMlGQw7EbC/tmp/eea02f4ee4e17f2b7d6410b504979f5ec4ebf1c3c4ffe1f1e8de82862686bb38.jpg", "img_caption": [], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Figure 8: Graph: structure $X_{0}\\,\\rightarrow\\,X_{1},X_{0}\\,\\rightarrow\\,X_{2},X_{1}\\,\\rightarrow\\,X_{3},X_{2}\\,\\rightarrow\\,X_{3}$ . For $0\\:<\\:\\delta\\:<\\:\\delta_{0}$ , the estimated $(B_{\\mathrm{est}}^{\\bar{\\mathbf{\\Gamma}}},\\bar{\\Omega}_{\\mathrm{est}})\\in\\mathcal{E}_{\\mathrm{min}}(\\Theta^{0})$ because SHD and distance are closed to 0. ", "page_idx": 33}, {"type": "image", "img_path": "TMlGQw7EbC/tmp/a22ebb9ce7beb60a382f2d9c07cca0ffb43708b89ddca1c814f2da01cb6a84d4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "Figure 9: Graph: structure $X_{0}\\rightarrow X_{1},X_{2}\\rightarrow X_{1}$ . For $0\\:<\\:\\delta\\:<\\:\\delta_{0}$ , the estimated $(B_{\\mathrm{est}},\\Omega_{\\mathrm{est}})\\in$ $\\bar{\\mathcal{E}_{\\mathrm{min}}}(\\Theta^{0})$ because SHD and distance are closed to 0. ", "page_idx": 33}, {"type": "image", "img_path": "TMlGQw7EbC/tmp/3ffe24901e8ca8a04a2329e48817bb8cf9733a3fe5f9e439041426d5e4826cd7.jpg", "img_caption": ["Figure 10: Results in term of Time. Lower is better. Column: $k\\;=\\;\\{1,2,4\\}$ . Row: random graph types. {ER,SF}- $k=$ {Scale-Free,Erd\u02ddos-R\u00e9nyi } graphs with $k d$ expected edges. Here $d=$ $\\{10,20,50,70,100\\}$ , $n=1000$ . Standard error is removed for better visualization. It is for different methods on raw data $\\mathbf{X}$ "], "img_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "TMlGQw7EbC/tmp/acbc0654f5fec6b3786a0002a1279333774d59d9d2ac8b9fbcf39aeb06b9cf14.jpg", "img_caption": ["Figure 11: Results in term of Time. Lower is better. Column: $k\\,=\\,\\{1,2,4\\}$ . Row: random graph types. {ER,SF}- $k=$ {Scale-Free,Erd\u02ddos-R\u00e9nyi } graphs with $k d$ expected edges. Here $d=$ $\\{10,20,50,70,100\\}$ , $n=1000$ . Standard error is removed for better visualization. It is for different methods on standardized data $\\mathbf{Z}$ "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "E.3 Nonlinear Model (SHD) ", "text_level": 1, "page_idx": 35}, {"type": "image", "img_path": "TMlGQw7EbC/tmp/470d2023a776cf9cbb8455e3731a226f6102282df216dd4ba94699d11aa092de.jpg", "img_caption": ["E.3.1 Neural Network "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "Figure 12: Structural Hamming distance (SHD) between Markov equivalence classes (MEC) of recovered and ground truth graphs. LOGLL (i.e. LOGLL-NOTEARS) stands for NOTEARS method with log-likelihood and quasi-MCP, L2 (i.e. NOTEARS) stands for NOTEARS method with least square and $\\ell_{1}$ . ", "page_idx": 35}, {"type": "image", "img_path": "TMlGQw7EbC/tmp/ce8cd28c734f6dc4bddd4e4ae06ba055567922b2f0adb9bcaf918ec3e5f7f5a2.jpg", "img_caption": ["E.3.2 General Linear Model with Binary Output (Logistic Model) ", "Figure 13: Structural Hamming distance (SHD) for Logistic Model, Row: random graph types, {SF, ER}- $.k{=}$ {Scale-Free,Erdo\u02dds-R\u00e9nyi } graphs. Columns: $k d$ expected edges. NOTEARS_LOGLL (i.e. LOGLL-NOTEARS) uses log-likelihood with quasi-MCP, NOTEARS use log-likelihood with $\\ell_{1}$ . Error bars represent standard errors over 10 simulations. "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We are confident about this point. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 36}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: This is the last section of main paper. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 36}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We include all the important assumptions. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 37}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We provide the detailed discussion in the appendix. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 37}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: It is simple adaption of open source code. We will release our the code. Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 38}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: It is included in the experinments section. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 38}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: We include the error bar to illustrate this point. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 38}, {"type": "text", "text": "", "page_idx": 39}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [No] ", "page_idx": 39}, {"type": "text", "text": "Justification: We run the algorithm on personal computer. It should be easy to replicate the results. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 39}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 39}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 39}, {"type": "text", "text": "Answer: [No] ", "page_idx": 39}, {"type": "text", "text": "Justification: We mainly focus on the theoretical findings. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 39}, {"type": "text", "text": "", "page_idx": 40}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: This is unrelated to the topic of paper. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 40}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: We cite the related paper. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ", "page_idx": 40}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 41}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 41}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 41}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 41}]