[{"figure_path": "TMlGQw7EbC/figures/figures_9_1.jpg", "caption": "Figure 1: Results in terms of SHD between MECs of estimated graph and ground truth. Lower is better. Column: k = {1, 2, 4}. Row: random graph types. {ER,SF}-k = {Scale-Free,Erd\u0151s-R\u00e9nyi } graphs with kd expected edges. Here p = {10, 20, 50, 70, 100}, n = 1000.", "description": "This figure displays the results of structure learning experiments using different methods. The x-axis represents the number of nodes in the graph, and the y-axis represents the structural Hamming distance (SHD) between the estimated graph and the ground truth. A lower SHD indicates better performance.  The figure is divided into subplots based on the number of expected edges (k) in the graph, and the type of graph (Erd\u0151s-R\u00e9nyi or Scale-Free).  Data is shown for both raw data and standardized data. The results demonstrate how well different algorithms recover causal structure in varying settings.", "section": "6 Experiments"}, {"figure_path": "TMlGQw7EbC/figures/figures_9_2.jpg", "caption": "Figure 1: Results in terms of SHD between MECs of estimated graph and ground truth. Lower is better. Column: k = {1, 2, 4}. Row: random graph types. {ER,SF}-k = {Scale-Free,Erd\u0151s-R\u00e9nyi } graphs with kd expected edges. Here p = {10, 20, 50, 70, 100}, n = 1000.", "description": "The figure compares the performance of different methods in recovering the true causal structure, represented as a directed acyclic graph (DAG), from data generated by structural equation models (SEMs). The performance is measured using the Structural Hamming Distance (SHD), which quantifies the difference between the estimated DAG and the true DAG. Lower SHD values indicate better accuracy. The figure shows results for different graph types (Erd\u0151s-R\u00e9nyi and Scale-Free) with varying numbers of edges, demonstrating how the methods perform with different levels of sparsity and network structure. It also shows results for different numbers of nodes (p) and sample sizes (n), demonstrating the scalability of the methods. The results suggest that the LOGLL-NOTEARS method performs best overall. The results for the standardized data are shown in (b).", "section": "6 Experiments"}, {"figure_path": "TMlGQw7EbC/figures/figures_9_3.jpg", "caption": "Figure 2: Comparison of raw (orange) vs. standardized (green) data. SHD (lower is better) between Markov equivalence classes (MEC) of recovered and ground truth graphs for ER-2 graphs with 10 (left) or 50 (right) nodes. In (b), SHD for VarSort with standardized data is omitted due to its average exceeding 300.", "description": "This figure compares the performance of various causal structure learning algorithms on raw and standardized data.  The Structural Hamming Distance (SHD) is used to measure the difference between the estimated causal graph structure and the ground truth. Lower SHD values indicate better performance. The figure shows that standardizing the data significantly impacts some algorithms (GOLEM, NOTEARS, DAGMA) causing them to perform much worse compared to using raw data; while others (LOGLL-NOTEARS) maintain robustness to data standardization.", "section": "6 Experiments"}, {"figure_path": "TMlGQw7EbC/figures/figures_9_4.jpg", "caption": "Figure 3: Graph: fork structure X0 \u2192 X1 and X0 \u2192 X2. For 0 < \u03b4 < \u03b40, the estimated (Best, \u03a9est) \u2208 Emin(\u0398\u00ba) because SHD and distance are closed to 0.", "description": "This figure shows the results of an experiment evaluating the impact of the hyperparameter \u03b4 on the quality of solutions obtained by the LOGLL-NOTEARS algorithm. The experiment used a simple fork graph structure (X0 \u2192 X1 and X0 \u2192 X2) and varied \u03b4, while keeping other hyperparameters fixed. The plots show the structural Hamming distance (SHD) between the estimated graph and the true graph, along with the distance between the estimated model parameters and the true parameters. For values of \u03b4 less than a certain threshold (\u03b40), the SHD and distance are both close to zero, indicating that the algorithm successfully recovers the minimal model in the Markov equivalence class. This supports the theoretical findings of the paper which demonstrate that under certain conditions, the LOGLL-NOTEARS algorithm consistently recovers the minimal model within its Markov equivalence class.", "section": "4.5 Scale invariance and standardization"}, {"figure_path": "TMlGQw7EbC/figures/figures_25_1.jpg", "caption": "Figure 4: The plot of p\u03bb,\u03b4(t) with \u03bb = 2, \u03b4 = 1", "description": "This figure shows a plot of the quasi-MCP penalty function, p\u03bb,\u03b4(t), with parameters \u03bb = 2 and \u03b4 = 1.  The plot illustrates the function's behavior: it is constant at 1 for |t| \u2265 \u03b4, quadratic for |t| < \u03b4, and smooth throughout its domain. The quasi-MCP penalty combines the properties of a quadratic function near zero, approximating the L2 penalty, with a constant value for larger values of |t|, approximating the L0 penalty. This balance results in a penalty that is both differentiable and encourages sparsity.", "section": "C.5 quasi-MCP, MCP and SCAD"}, {"figure_path": "TMlGQw7EbC/figures/figures_31_1.jpg", "caption": "Figure 1: Results in terms of SHD between MECs of estimated graph and ground truth. Lower is better. Column: k = {1, 2, 4}. Row: random graph types. {ER,SF}-k = {Scale-Free,Erd\u0151s-R\u00e9nyi } graphs with kd expected edges. Here p = {10, 20, 50, 70, 100}, n = 1000.", "description": "This figure shows the results of the experiment comparing different methods for estimating the structure of directed acyclic graphs (DAGs). The results are evaluated using Structural Hamming Distance (SHD), which measures the difference between the estimated DAG and the true DAG. The experiment varies the number of nodes (p) and edges (k) in the DAG, as well as the method used for estimation.  Lower SHD values indicate better performance.", "section": "6 Experiments"}, {"figure_path": "TMlGQw7EbC/figures/figures_32_1.jpg", "caption": "Figure 6: Comparison of raw (orange) vs. standardized (green) data. Structural Hamming Distance (SHD, with lower values indicating better performance) between Markov equivalence classes (MEC) of recovered and ground truth graphs for ER-2 graphs with 5 nodes", "description": "This figure compares the performance of various causal discovery methods on datasets with 5 nodes and 2k expected edges, generated using the Erd\u0151s-R\u00e9nyi (ER) random graph model. The comparison is made between using raw data and standardized data. The results show that the Structural Hamming Distance (SHD), a measure of the difference between the estimated graph and the true graph, is generally lower for methods using raw data, especially for the LOGLL methods (which are the main focus of the paper). This indicates that standardizing the data can negatively impact the performance of some causal discovery methods.", "section": "6 Experiments"}, {"figure_path": "TMlGQw7EbC/figures/figures_32_2.jpg", "caption": "Figure 1: Results in terms of SHD between MECs of estimated graph and ground truth. Lower is better. Column: k = {1, 2, 4}. Row: random graph types. {ER,SF}-k = {Scale-Free,Erd\u0151s-R\u00e9nyi } graphs with kd expected edges. Here p = {10, 20, 50, 70, 100}, n = 1000.", "description": "This figure displays the results of an experiment comparing various methods for learning the structure of directed acyclic graphs (DAGs).  The experiment uses several types of random graphs with varying numbers of nodes (10, 20, 50, 70, 100) and edges, and evaluates the performance using the Structural Hamming Distance (SHD), a measure of how similar the learned structure is to the true structure.  Lower SHD values indicate better performance. The figure is organized with columns representing different graph density levels (k = 1, 2, 4) and rows representing different graph types (Erd\u0151s-R\u00e9nyi and Scale-Free). The number of samples used in each experiment is 1000 (n=1000).", "section": "6 Experiments"}, {"figure_path": "TMlGQw7EbC/figures/figures_33_1.jpg", "caption": "Figure 3: Graph: structure X0 \u2192 X1 and X0 \u2192 X2. For 0 < \u03b4 < \u03b40, the estimated (Best, \u03a9est) \u2208 Emin(\u0398\u00ba) because SHD and distance are closed to 0.", "description": "This figure shows the results of an experiment to test the impact of the hyperparameter \u03b4 (delta) on the performance of an algorithm for learning causal graphs. The experiment uses a simple fork structure graph (X0 \u2192 X1, X0 \u2192 X2) as the ground truth graph.  The results show that when \u03b4 is smaller than a certain threshold (\u03b40), the algorithm finds the minimal model (Best) within the Markov equivalence class and the structural Hamming distance (SHD) between the estimated graph and the true graph is close to zero. This suggests that choosing \u03b4 appropriately leads to accurate causal discovery.", "section": "Experiments"}, {"figure_path": "TMlGQw7EbC/figures/figures_33_2.jpg", "caption": "Figure 9: Graph: structure X0 \u2192 X1, X2 \u2192 X\u2081. For 0 < \u03b4 < \u03b40, the estimated (Best, \u03a9est) \u2208 Emin(\u0398\u00ba) because SHD and distance are closed to 0.", "description": "This figure shows the results of an experiment to evaluate the impact of the hyperparameter \u03b4 on the performance of a structure learning algorithm. The algorithm aims to recover the minimal equivalence class of a DAG. The X-axis shows the values of \u03b4, while the Y-axes show the structural Hamming distance (SHD) and the distance between the estimated model and the true model. The dots represent the results from 100000 different initializations of the algorithm, showing the distribution of SHD and distance values for each \u03b4 value. The red line shows the average values, highlighting that for small enough \u03b4 values (0 < \u03b4 < \u03b40), the algorithm consistently recovers the minimal equivalence class.", "section": "Experiments"}, {"figure_path": "TMlGQw7EbC/figures/figures_34_1.jpg", "caption": "Figure 10: Results in term of Time. Lower is better. Column: k = {1,2,4}. Row: random graph types. {ER,SF}-k = {Scale-Free,Erd\u0151s-R\u00e9nyi } graphs with kd expected edges. Here d = {10, 20, 50, 70, 100}, n = 1000. Standard error is removed for better visualization. It is for different methods on raw data X", "description": "The figure displays the time taken by different causal discovery algorithms to learn the graph structure for various graph sizes (number of nodes) and edge densities. The algorithms are compared using the Structural Hamming Distance (SHD), which measures how close the estimated graph structure is to the ground truth.  The graph types considered are Erd\u0151s-R\u00e9nyi (ER) and Scale-Free (SF) networks, characterized by different connectivity patterns. The results show how the computation time increases with the number of nodes and edges.  The time taken by our proposed method is comparable to other state-of-the-art methods.", "section": "6 Experiments"}, {"figure_path": "TMlGQw7EbC/figures/figures_34_2.jpg", "caption": "Figure 10: Results in term of Time. Lower is better. Column: k = {1,2,4}. Row: random graph types. {ER,SF}-k = {Scale-Free,Erd\u0151s-R\u00e9nyi } graphs with kd expected edges. Here d = {10, 20, 50, 70, 100}, n = 1000. Standard error is removed for better visualization. It is for different methods on raw data X", "description": "This figure shows the computation time for different causal structure learning algorithms on raw data.  The x-axis represents the number of nodes in the graph, and the y-axis shows the time taken (in seconds).  Different colors represent different algorithms. The results are broken down by the type of graph (Erd\u0151s-R\u00e9nyi or Scale-free) and the average number of edges.  The standard error bars have been removed for clarity.", "section": "6 Experiments"}, {"figure_path": "TMlGQw7EbC/figures/figures_35_1.jpg", "caption": "Figure 12: Structural Hamming distance (SHD) between Markov equivalence classes (MEC) of recovered and ground truth graphs. LOGLL (i.e. LOGLL-NOTEARS) stands for NOTEARS method with log-likelihood and quasi-MCP, L2 (i.e. NOTEARS) stands for NOTEARS method with least square and l1.", "description": "This figure displays the results of the Structural Hamming Distance (SHD) for neural network models with different graph types (scale-free and Erd\u0151s-R\u00e9nyi) and numbers of nodes.  The SHD measures the difference between the estimated causal graph structure and the ground truth.  Two methods are compared: LOGLL (using log-likelihood with quasi-MCP penalty) and L2 (using least squares loss with L1 penalty), along with a CAM baseline. The results show the impact of standardization on the SHD for both methods, across various network settings and sizes.", "section": "E.3.1 Neural Network"}, {"figure_path": "TMlGQw7EbC/figures/figures_35_2.jpg", "caption": "Figure 13: Structural Hamming distance (SHD) for Logistic Model, Row: random graph types, {SF, ER}-k= {Scale-Free,Erd\u0151s-R\u00e9nyi } graphs. Columns: kd expected edges. NOTEARS_LOGLL (i.e. LOGLL-NOTEARS) uses log-likelihood with quasi-MCP, NOTEARS use log-likelihood with l\u2081. Error bars represent standard errors over 10 simulations.", "description": "The figure shows the comparison of the SHD for the logistic model between NOTEARS and LOGLL-NOTEARS with different number of nodes.  The results are shown for scale-free and Erdos-Renyi graphs with different numbers of expected edges. The error bars show the standard error over 10 simulations.", "section": "6 Experiments"}]