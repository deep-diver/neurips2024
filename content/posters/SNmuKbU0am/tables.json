[{"figure_path": "SNmuKbU0am/tables/tables_6_1.jpg", "caption": "Table 1: Audio-Visual Event Localization. Comparison with previous methods in a fully-supervised manner. The performance is evaluated on the test set of AVE dataset with classification accuracy. The visual and audio encoders are pre-trained on the ImageNet and Audioset, respectively. \u2020 means that no official code was provided to report some of the baseline-specific metrics.", "description": "This table compares the performance of the proposed AVMoE model with several existing state-of-the-art methods on the Audio-Visual Event Localization (AVE) task.  It shows the accuracy achieved by each method, along with details about the visual and audio encoders used (pre-trained on ImageNet and AudioSet, respectively), and the number of trainable parameters in millions.  The table highlights the parameter efficiency of AVMoE by showcasing a lower number of trainable parameters while maintaining comparable or superior performance.", "section": "4.1 Audio-Visual Event Localization"}, {"figure_path": "SNmuKbU0am/tables/tables_7_1.jpg", "caption": "Table 2: Audio-Visual Video Parsing. Comparison with previous methods on the test set of LLP dataset. We conduct comparative experiments on two kinds of annotation: segment-level and event-level. The Type computes the average audio, visual, and audio-visual event evaluation results, and the Event considers all audio and visual events for each sample to compute the F-score.", "description": "This table compares the performance of the proposed AVMoE model with several other state-of-the-art methods on the Audio-Visual Video Parsing (AVVP) task using the LLP dataset. The results are broken down by two evaluation metrics (Type and Event) and two annotation levels (segment-level and event-level).  Each metric shows results for audio (A), visual (V), audio-visual (AV) modalities separately, offering a comprehensive view of the model's performance across different aspects of the AVVP task.", "section": "4.2 Audio-Visual Video Parsing"}, {"figure_path": "SNmuKbU0am/tables/tables_7_2.jpg", "caption": "Table 3: Audio-Visual Segmentation. Comparison with previous methods under the S4 and MS3 settings of AVSBench dataset. MJ and MF denote the mean I and F metric values over the whole dataset.", "description": "This table compares the performance of the proposed AVMoE model with several existing state-of-the-art methods on the Audio-Visual Segmentation (AVS) task.  The comparison is made using two different settings of the AVSBench dataset: S4 and MS3.  The S4 setting is for single-sound sources while the MS3 setting is for multiple-sound sources, representing a more challenging scenario. The results are evaluated using the Jaccard Index (MJ) and F-score (MF),  representing region similarity and contour accuracy, respectively.  The table also shows the number of trainable parameters for each model, providing an indication of the model's complexity.", "section": "4.3 Audio-Visual Segmentation"}, {"figure_path": "SNmuKbU0am/tables/tables_8_1.jpg", "caption": "Table 4: Audio-Visual Question Answering. Comparison with previous methods on the test set of MUSIC-AVQA dataset. We report accuracy on three types of questions, i.e., Audio Question (AQ), Visual Question (VQ), and Audio-Visual Question (AVQ). LAVisH* denotes our implementation version of LAVisH.", "description": "This table compares the performance of the proposed AVMoE model against several state-of-the-art methods on the MUSIC-AVQA dataset.  The comparison is done across three question types: Audio Question (AQ), Visual Question (VQ), and Audio-Visual Question (AVQ).  The table shows the accuracy achieved by each method for each question type, along with details on the model architecture, including the visual and audio encoders used, the percentage of trainable parameters, and the total number of parameters in millions.", "section": "4.4 Audio-Visual Question Answering"}, {"figure_path": "SNmuKbU0am/tables/tables_9_1.jpg", "caption": "Table 5: AVMoE design. CMA and UA denote cross-modal adapter and uni-modal adapter, respectively.", "description": "This table presents the results of ablation studies on the number of experts (cross-modal and unimodal adapters) used in the Audio-Visual Mixture of Experts (AVMoE) model.  It shows how varying the number of each type of adapter affects the performance on three different audio-visual tasks: Audio-Visual Segmentation (AVS) under two settings (S4 and MS3), Audio-Visual Question Answering (AVQA), and Audio-Visual Event Localization (AVE).  The metrics used are Mean Jaccard Index (MJ) and Mean F-score (MF) for AVS, and accuracy for AVQA and AVE. The results illustrate the impact of different expert configurations on model performance and the trade-offs between using more cross-modal versus unimodal adapters. ", "section": "5 Ablation Studies"}, {"figure_path": "SNmuKbU0am/tables/tables_9_2.jpg", "caption": "Table 6: Modality Ablation. \"A\" and \"V\" denote audio and visual, respectively. All models are well-trained on audio-visual modalities and tested on different modalities.", "description": "This table presents the results of an ablation study evaluating the performance of the proposed AVMoE model and the baseline DG-SCT model under different modality conditions.  It compares performance on three audio-visual tasks (AVE, AVS, AVQA) when using only visual data (V) versus both audio and visual data (A+V).  The goal is to assess the robustness of each model when one modality is missing. ", "section": "5.2 Modality Ablation"}]