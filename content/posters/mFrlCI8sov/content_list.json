[{"type": "text", "text": "Interventional Causal Discovery in a Mixture of DAGs ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Burak Var\u0131c\u0131\u2217 Dmitriy A. Katz Dennis Wei Carnegie Mellon University IBM Research IBM Research ", "page_idx": 0}, {"type": "text", "text": "Prasanna Sattigeri Ali Tajer IBM Research Rensselaer Polytechnic Institute ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Causal interactions among a group of variables are often modeled by a single causal graph. In some domains, however, these interactions are best described by multiple co-existing causal graphs, e.g., in dynamical systems or genomics. This paper addresses the hitherto unknown role of interventions in learning causal interactions among variables governed by a mixture of causal systems, each modeled by one directed acyclic graph (DAG). Causal discovery from mixtures is fundamentally more challenging than single-DAG causal discovery. Two major difficulties stem from (i) an inherent uncertainty about the skeletons of the component DAGs that constitute the mixture and (ii) possibly cyclic relationships across these component DAGs. This paper addresses these challenges and aims to identify edges that exist in at least one component DAG of the mixture, referred to as the true edges. First, it establishes matching necessary and sufficient conditions on the size of interventions required to identify the true edges. Next, guided by the necessity results, an adaptive algorithm is designed that learns all true edges using $O(n^{2})$ interventions, where $n$ is the number of nodes. Remarkably, the size of the interventions is optimal if the underlying mixture model does not contain cycles across its components. More generally, the gap between the intervention size used by the algorithm and the optimal size is quantified. It is shown to be bounded by the cyclic complexity number of the mixture model, defined as the size of the minimal intervention that can break the cycles in the mixture, which is upper bounded by the number of cycles among the ancestors of a node. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "The causal interactions in a system of causally related variables are often abstracted by a directed acyclic graph (DAG). This is the common practice in various disciplines, including biology [1], social sciences [2], and economics [3]. In a wide range of applications, however, the complexities of the observed data cannot be reduced to conform to a single DAG, and they are best described by a mixture of multiple co-existing DAGs over the same set of variables. For instance, gene expression of certain cancer types comprises multiple subtypes with different causal relationships [4]. In another example, mixture models are often more accurate than unimodal distributions in representing dynamical systems [5], including time-series trajectories in psychology [6] and data from complex robotics environments [7]. ", "page_idx": 0}, {"type": "text", "text": "Despite the widespread applications, causal discovery for a mixture of DAGs remains an underinvestigated domain. Furthermore, the existing studies on the subject are also limited to using only observational data [8\u201311]. Observational data alone is highly insufficient in uncovering causal relationships. It is well-established that even for learning a single DAG, observational data can learn a DAG only up to its Markov equivalence class (MEC) [12]. Hence, interventions, which refer to altering the causal mechanisms of a set of target nodes, have a potentially significant role in improving identifiability guarantees in mixture DAG models. Specifically, interventional data can be used to learn specific cause-effect relationships and refine the equivalence classes. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Using interventions for learning a single DAG is well-investigated for various causal models and interventions [13\u201317]. In this paper, we investigate using interventions for causal discovery in a mixture of DAGs, a fundamentally more challenging problem. The major difficulties stem from (i) an inherent uncertainty about the skeletons of the DAGs that constitute the mixture and (ii) possibly cyclic relationships across these DAGs. For a single DAG, the skeleton can be learned from observational data via conditional independence (CI) tests and the role of interventions is limited to orienting the edges. On the contrary, in a mixture of DAGs, the skeleton cannot be learned from observational data alone, making interventions essential for both learning the skeleton and orienting the edges. Uncertainty in the skeleton arises because, in addition to true edges present in at least one individual DAG, there are inseparable random variable pairs that cannot be made conditionally independent via CI tests, even though they are nonadjacent in every DAG of the mixture. These types of inseparable node pairs, referred to as emergent edges [11], cannot be distinguished from true edges using observational data alone. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we aim to characterize the fundamental limits of interventions needed for learning the true edges in a mixture of DAGs. The two main aspects of these limits are the minimum size and number of the interventions. To this end, we first investigate the necessary and sufficient size of interventions for identifying a true edge. Subsequently, we design an adaptive algorithm that learns the true edges using interventions guided by the necessary and sufficient intervention sizes. We quantify the optimality gap of the maximum intervention size used by the algorithm as a function of the structure of the cyclic relationships across the mixture model. We note that the component DAGs of the mixture cannot be identified without further assumptions even when using interventions (see examples in Appendix D.1). Hence, our focus is on learning the set of true edges in the mixture as specified above. Our contributions are summarized as follows. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Intervention size: We establish matching necessary and sufficient intervention size to identify each node\u2019s mixture parents (i.e., the union of its parents across all DAGs). Specifically, we show that this size is one more than the number of mixture parents of the said node. ", "page_idx": 1}, {"type": "text", "text": "\u2022 Tree DAGs: For the special case of a mixture of directed trees, we show that the necessary and sufficient intervention size is one more than the number of DAGs in the mixture. \u2022 Algorithm: We design an adaptive algorithm that identifies all directed edges of the individual DAGs in the mixture by using $O(n^{2})$ interventions, where $n$ is the number of variables. Remarkably, the maximum size of the interventions used in our algorithm is optimal if the mixture ancestors of a node (i.e., the union of its ancestors across all DAGs) do not form a cycle. \u2022 Optimality gap: We show that the gap between the maximum intervention size used by the proposed algorithm for a given node and the optimal size is bounded by the cyclic complexity number of the node, which is defined as the number of nodes needing intervention to break cycles among the ancestors of the node, and is upper bounded by the number of such cycles. ", "page_idx": 1}, {"type": "text", "text": "We provide an overview of the closely related literature, the majority of which is focused on the causal discovery of single DAGs. ", "page_idx": 1}, {"type": "text", "text": "Causal discovery of a mixture of DAGs. The relevant literature on the causal discovery of a mixture of DAGs focuses on developing graphical models to represent CI relationships in the observed mixture distribution [8\u201311]. Among them, [8] proposes a fused graph and shows that the mixture distribution is Markov with respect to it. The study in [9] proposes a similar mixture graph but relies on longitudinal data to orient any edges. The study in [10] constructs a mixture $D A G$ that represents the mixture distribution and designed an algorithm for learning a maximal ancestral graph. The algorithm of [10] requires the component DAGs of the mixture to be poset compatible, which rules out any cyclic relationships across the DAGs. The study in [11] introduces the notion of emergent edges to investigate the inseparability conditions arising in the mixture of DAGs. The study in [18] proposes a variational inference-based approach for causal discovery from a mixture of time-series data. Despite their differences, all these studies are limited to using observational data. ", "page_idx": 1}, {"type": "text", "text": "Intervention design for causal discovery of a single DAG. We note that the structure of a single DAG without latent variables can be learned using single-node interventions. Hence, the majority of the literature focuses on minimizing the number of interventions. Worst-case bounds on the number of interventions with unconstrained size are established in [13], and heuristic adaptive algorithms are proposed in [14]. Intervention design on causal graphs with latent variables is studied in [19\u201321]. The study in [20] also shows that single-node interventions are not sufficient for exact graph recovery in the presence of latent variables. In another direction, [16] studies interventions under size constraints, establishes a lower bound for the number of interventions, and shows that $\\mathcal{O}(\\frac{n}{k}\\log\\log k)$ randomized interventions with size $k$ suffice for identifying the DAG with high probability. In the case of single-node interventions, adaptive and non-adaptive algorithms are proposed in [22], active learning of directed trees is studied in [23], and a universal lower bound for the number of interventions is established in [17]. [24] also studies the universal lower bound problem and [25] provides an exact characterization for the number of interventions required to recover the DAG from the observational essential graph. A linear cost model, where the cost of an intervention is proportional to its size, is proposed in [26]. It is shown that learning the DAG with optimal cost under the linear cost model is NP-hard [27]. The size of the minimal intervention sets is studied for cyclic directed models in [28]. Specifically, it is shown that the required intervention size is at least $\\zeta-1$ where $\\zeta$ denotes the size of the largest strongly connected component in the cyclic model. A related problem to intervention design is causal discovery from a combination of observational and interventional data. In this setting, the characterization of the equivalence classes and designing algorithms for learning them is well-explored for a single DAG [29\u201332]. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Causal discovery from multiple clusters/contexts. Another approach to causal discovery from a mixture of DAGs is clustering the observed samples and performing structure learning on each cluster separately [33\u201337]. Learning from multiple contexts is also studied in the interventional causal discovery literature [38\u201341]. However, these studies assume that domain indexes are known. In a similar problem, [42] aims to learn the domain indexes and perform causal discovery simultaneously. ", "page_idx": 2}, {"type": "text", "text": "2 Preliminaries and definitions ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "2.1 Observational mixture model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "DAG models. We consider $K\\ge2$ distinct DAGs $\\mathcal{G}_{\\ell}\\triangleq(\\mathbf{V},\\mathbf{E}_{\\ell})$ for $\\ell\\in\\{1,\\ldots,K\\}$ defined over the same set of nodes $\\mathbf{V}\\triangleq\\{1,\\...\\,,n\\}$ . $\\mathbf{E}_{\\ell}$ denotes the set of directed edges in graph $g_{\\ell}$ . Throughout the paper, we refer to these as the mixture component DAGs. We use $\\mathrm{pa}_{\\ell}(i),\\,\\mathrm{ch}_{\\ell}(i),\\,\\mathrm{an}_{\\ell}(i)$ , and $\\mathrm{de}_{\\ell}(i)$ to refer to the parents, children, ancestors, and descendants of node $i$ in DAG $g_{\\ell}$ , respectively. For each node $i\\in\\mathbf{V}$ , we also define $\\mathrm{pa}_{\\mathrm{m}}(i)$ as the union of the nodes that are parents of $i$ in at least one component DAG and refer to $\\mathrm{pa}_{\\mathrm{m}}(i)$ as the mixture parents of node $i$ . Similarly, for each node $i\\in\\mathbf{V}$ we define $\\mathrm{ch}_{\\mathrm{m}}(i)$ , $\\mathrm{an}_{\\mathrm{m}}(i)$ , and $\\mathrm{de}_{\\mathrm{m}}(i)$ . ", "page_idx": 2}, {"type": "text", "text": "Mixture model. Each of the component DAGs represents a Bayesian network. We denote the random variable generated by node $i\\in\\mathbf{V}$ by $X_{i}$ and define the random vector $X\\triangleq(X_{1},\\ldots,X_{n})^{\\top}$ . For any subset of nodes $A\\subseteq\\mathbf{V}$ , we use $X_{A}$ to denote the vector formed by $X_{i}$ for $i\\in A$ . We denote the probability density function (pdf) of $X$ under $g_{\\ell}$ by $p_{\\ell}$ , which factorizes according to $g_{\\ell}$ as ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{\\ell}(x)=\\prod_{i\\in[n]}p_{\\ell}(x_{i}\\mid x_{{\\mathrm{pa}}_{\\ell}(i)})\\;,\\quad\\forall\\ell\\in[K]\\;.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "For distinct $\\ell,\\ell^{\\prime}\\in[K]$ , $p_{\\ell}$ and $p_{\\ell^{\\prime}}$ can be distinct even when $\\mathbf{E}_{\\ell}=\\mathbf{E}_{\\ell^{\\prime}}$ . The differences between any two DAGs are captured by the nodes with distinct causal mechanisms (i.e., conditional distributions) in the DAGs. To formalize such distinctions, we define the following set, which contains all the nodes with at least two different conditional distributions across component distributions. ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\Delta\\triangleq\\left\\{i\\in{\\bf V}:\\exists\\ell,\\,\\ell^{\\prime}\\in[K]\\ :\\ p_{\\ell}(X_{i}\\ |\\ X_{\\mathrm{pa}_{\\ell}(i)})\\neq p_{\\ell^{\\prime}}(X_{i}\\ |\\ X_{\\mathrm{pa}_{\\ell^{\\prime}}(i)})\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "We adopt the same mixture model as the prior work on causal discovery of mixture of DAGs $[8-$ 11, 18]. Specifically, observed data is generated by a mixture of distributions $\\{p_{\\ell}\\,:\\,\\ell\\,\\in\\,[K]\\}$ . It is unknown to the learner which model is generating the observations $X$ . To formalize this, we define $L\\in\\{1,\\ldots,K\\}$ as a latent random variable where $L=\\ell$ specifies that the true model is $p_{\\ell}$ We denote the probability mass function (pmf) of $L$ by $r$ . Hence, we have the following mixture distribution for the observed samples $X$ . ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{\\mathrm{m}}(x)\\triangleq\\sum_{\\ell\\in[K]}r(\\ell)\\cdot p_{\\ell}(x)\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Next, we provide several definitions that are instrumental to formalizing causal discovery objectives. ", "page_idx": 3}, {"type": "text", "text": "Definition 1 (True edge). We say that $j\\rightarrow i$ is $a$ true edge if $j\\in\\mathrm{pa}_{\\mathrm{m}}(i)$ . The set of all true edges is denoted by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{E}_{\\mathrm{t}}\\triangleq\\{(j\\to i):i,j\\in\\mathbf{V},\\ \\exists\\,\\mathcal{G}_{\\ell}:j\\in\\mathrm{pa}_{\\ell}(i)\\}\\ .\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "A common approach to causal discovery is the class of constraint-based approaches, which perform conditional independence (CI) tests on the observed data to infer (partial) knowledge about the DAGs\u2019 structure [43\u201345]. In this paper, we adopt a constraint-based $\\mathrm{CI}$ testing approach. Following this approach, the following definition formally specifies the set of node pairs that cannot be made conditionally independent in the mixture distribution. ", "page_idx": 3}, {"type": "text", "text": "Definition 2 (Inseparable pair). The node pair $(i,j)$ is called inseparable $i f X_{i}$ and $X_{j}$ are always statistically dependent in the mixture distribution $p_{\\mathrm{m}}$ under any conditioning set. The set of inseparable node pairs is specified by ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\bf E}_{\\mathrm{i}}\\triangleq\\{(i-j):i,j\\in{\\bf V},\\,\\,\\,\\rlap/\\sharp A\\subseteq{\\bf V}\\setminus\\{i,j\\}:\\,\\,X_{i}\\perp\\,\\bot\\!\\!\\!\\perp X_{j}\\mid X_{A}\\,\\,\\,i n\\,\\,\\,p_{\\mathrm{m}}\\}\\;.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Note that when $(j\\rightarrow i)$ is a true edge, the pair $(i,j)$ will be inseparable. A significant difference between independence tests for mixture models and single-DAG models is that not all inseparable pairs have an associated true edge in the former. More specifically, due to the mixing of multiple distributions, a pair of nodes can be nonadjacent in all component DAGs but still be inseparable in mixture distribution $p_{\\mathrm{m}}$ . We refer to such inseparable node pairs as emergent pairs, formalized next. ", "page_idx": 3}, {"type": "text", "text": "Definition 3 (Emergent pair). An inseparable pair $(i,j)\\in\\mathbf{E}_{\\mathrm{i}}$ is called an emergent pair if there is no true edge associated with the pair. The set of emergent pairs is denoted by ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{E}_{\\mathrm{e}}\\triangleq\\{(i,j)\\in\\mathbf{E}_{i}:\\ i\\notin\\mathrm{pa}_{\\mathrm{m}}(j)\\ \\wedge\\ j\\notin\\mathrm{pa}_{\\mathrm{m}}(i)\\}\\;.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The conditions under which emergent edges arise in mixture models are recently investigated in [11], where it is shown that the causal paths that pass through a node in the set $\\Delta$ defined in (2) are instrumental for their analysis. These paths are specified next. ", "page_idx": 3}, {"type": "text", "text": "Definition 4 ( $\\Delta$ -through path). We say that a causal path in $g_{\\ell}$ between i and $j$ is a $\\Delta$ -through path if it passes through at least one node in $\\Delta_{\\pm}$ , i.e., there exists $u\\in\\Delta$ such that $i\\stackrel{\\ell}{\\sim}u\\stackrel{\\ell}{\\sim}j.\\ I\\!\\!\\!/$ f $u\\in\\mathrm{ch}_{\\ell}(i)$ , the path is also called a $\\Delta$ -child-through path. ", "page_idx": 3}, {"type": "text", "text": "2.2 Intervention model ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "In this section, we describe the intervention model we use for causal discovery on a mixture of DAGs. We consider hard stochastic interventions on component DAGs of the mixture model. A hard intervention on a set of nodes ${\\mathcal{T}}\\subseteq\\mathbf{V}$ cuts off the edges incident on nodes $i\\in\\mathcal{Z}$ in all component DAGs $g_{\\ell}$ for $\\ell\\in[K]$ . We denote the post-intervention component DAGs upon an intervention $\\mathcal{T}$ by $\\{\\mathcal{G}_{\\ell,\\mathcal{Z}}:\\ell\\in[K]\\}$ . We note that hard interventions are less restrictive than $d o$ interventions, which not only remove ancestral dependencies but also remove randomness by assigning constant values to the intervened nodes. Specifically, in $\\boldsymbol{\\mathcal{G}}_{\\ell,\\mathcal{T}}$ , the causal mechanism of an intervened node $i\\in\\mathcal{T}$ changes from $p_{\\ell}(x_{i}\\mid x_{\\mathrm{pa}_{\\ell}(i)})$ to $q_{i}(x_{i})$ . Therefore, upon an intervention ${\\mathcal{T}}\\subseteq\\mathbf{V}$ , the interventional component DAG distributions are given by ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{\\ell,{\\mathbb Z}}(x)\\triangleq\\prod_{i\\in{\\cal Z}}q_{i}(x_{i})\\prod_{i\\in{\\bf V}\\setminus{\\cal I}}p_{\\ell}(x_{i}\\mid x_{{\\mathrm{pa}}_{\\ell}(i)})\\;,\\qquad\\forall\\ell\\in[K]\\;.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Subsequently, the interventional mixture distribution $p_{\\mathrm{m},\\mathbb{Z}}(x)$ is given by ", "page_idx": 3}, {"type": "equation", "text": "$$\np_{\\mathrm{m},\\mathbb{Z}}(x)\\triangleq\\sum_{\\ell\\in[K]}r(\\ell)\\cdot p_{\\ell,\\mathbb{Z}}(x)\\;.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "We note that an intervened node $i\\in\\mathcal{Z}$ has the same causal mechanism $q_{i}(x_{i})$ for all interventions ${\\mathcal{T}}\\subseteq\\mathbf{V}$ that contain $i$ . This is because an intervention procedure targets a set of nodes in all mixture components at the same time. Hence, resulting $q_{i}(X_{i})$ is shared for all component models, owing to the same intervention mechanism, e.g., gene knockout experiments [46]. Hence, the set of nodes with distinct causal mechanisms across the components of the interventional mixture model becomes $\\Delta_{\\mathcal{T}}\\triangleq\\Delta\\setminus\\mathcal{T}$ . Next, we specify the $\\mathcal{T}$ -mixture DAG, which extends the mixture DAG defined for observational data in [10, 11] and will facilitate our analysis. ", "page_idx": 3}, {"type": "image", "img_path": "mFrlCI8sov/tmp/e00f88ef3789d268ae1bbdf78571adde180cefc1ba7cc9bdd35550dc6e282718.jpg", "img_caption": ["Figure 1: (a)- ${\\bf\\delta}({\\bf b})$ : sample component DAGs; (c) the mixture DAG for $\\mathcal{T}=\\emptyset$ , note that $\\Delta=\\{2,3,4\\}$ (when the distribution of node 1 remains the same) ; (d)-(e): post-intervention component DAGs for ${\\mathcal{T}}=\\{2\\}$ ; (f): corresponding $\\mathcal{T}$ -mixture DAG. Also note that true edges $\\mathbf{E}_{\\mathrm{t}}=\\{(\\dot{1}\\rightarrow2),(2\\rightarrow$ 3), $(3\\rightarrow2)$ , $[3\\rightarrow4]$ ), $(1\\rightarrow4)\\}$ , inseparable pairs $\\mathbf{E}_{\\mathrm{i}}=\\{(1-2),(1-3),(1-4),(2-3),(2$ \u2212 4), $(3-4)\\}$ , and emergent edges $\\bar{\\mathbf{E}_{\\mathrm{e}}}\\bar{=}\\,\\{(1,3)\\bar{,}(2,4)\\}$ . "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Definition 5 $\\mathcal{Z}$ -mixture DAG). Given an intervention $\\mathcal{T}$ on a mixture of DAGs, $\\mathcal{T}$ -mixture DAG $\\mathcal{G}_{\\mathrm{m},\\mathcal{Z}}$ is a graph with $n K+1$ nodes constructed by first concatenating the $K$ component DAGs and then adding a single node $y$ to the concatenation. Furthermore, there will be a directed edge from y to every node in $\\Delta_{\\mathbb{Z}}$ in every $D A G$ $\\{\\mathcal{G}_{\\ell,\\mathcal{Z}}:\\ell\\in[K]\\}$ . In the $\\mathcal{T}$ -mixture DAG $\\mathcal{G}_{\\mathrm{m},\\mathcal{Z}}$ , we use $i_{\\ell}$ to denote the copy of node $i$ in $\\boldsymbol{\\mathcal{G}}_{\\ell,\\mathcal{T}}$ . Accordingly, for any $A\\subseteq\\mathbf{V}$ we define ${\\bar{A}}\\triangleq\\{i_{\\ell}:i\\in A,\\;\\ell\\in[K]\\}$ . ", "page_idx": 4}, {"type": "text", "text": "Figure 1 illustrates an example of a mixture of $K=2$ component DAGs, different edge types, an intervention $\\mathcal{T}$ on the mixture, and the construction of the $\\mathcal{T}$ -mixture DAG from post-intervention component DAGs $\\mathcal{G}_{1,\\mathcal{Z}}$ and $\\mathcal{G}_{2,\\mathcal{Z}}$ . We define the observational mixture $D A G$ as the $\\mathcal{T}$ -mixture DAG when the intervention set is $\\mathcal{T}=\\emptyset$ and denote it by $\\mathcal{G}_{\\mathrm{m}}$ . It is known that $p_{\\mathrm{m}}$ specified in (3) satisfies the global Markov property with respect to observational mixture DAG, [10, Theorem 3.2]. It can be readily verified that this result extends to the interventional setting for $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ and $\\mathcal{G}_{\\mathrm{m},\\mathcal{Z}}$ . We make the following faithfulness assumption to facilitate causal discovery via statistical independence tests. ", "page_idx": 4}, {"type": "text", "text": "Assumption 1 $\\mathcal{Z}$ -mixture faithfulness). For any intervention ${\\mathcal{T}}\\subseteq\\mathbf{V}$ , the interventional mixture distribution $p_{\\mathrm{m},\\mathbb{Z}}(x)$ is faithful to $\\mathcal{G}_{\\mathrm{m},\\mathcal{Z}}$ , that is if $X_{A}$ \u22a5\u22a5 $X_{B}\\mid X_{C}$ in $p_{\\mathrm{m},\\mathbb{Z}}(x)$ , then $\\bar{A}$ and $\\bar{B}$ are $d$ -separated given $\\bar{C}$ in $\\mathcal{G}_{\\mathrm{m},\\mathcal{Z}}$ . ", "page_idx": 4}, {"type": "text", "text": "Finally, we note that the observational counterpart of Assumption 1, i.e., when $\\mathcal{T}=\\emptyset$ , is standard in the literature for analyzing a mixture of DAGs [9\u201311]. In working with interventions, we naturally extend it to interventional mixture distributions. Also note that Assumption 1 does not compare observational and interventional distributions. Hence, it is not comparable to various faithfulness assumptions in the literature on the interventional causal discovery of a single DAG, e.g., [29, 31]. ", "page_idx": 4}, {"type": "text", "text": "2.3 Causal discovery objectives ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We aim to address the following question: how can we use interventions to perform causal discovery in a mixture of $D A G s$ , with the objectives specified next. ", "page_idx": 4}, {"type": "text", "text": "The counterpart of this question is well-studied for the causal discovery of a single DAG. Since the unoriented skeleton of the single DAG can already be identified by CI tests on observational data, interventions are leveraged to orient the edges. Interventions are generally bounded by a pre-specified budget, measured by the number of interventions. The extent of causal relationships that observational data can uncover in a mixture of DAGs is significantly narrower than those in single DAGs. The striking difference is the existence of emergent pairs specified in (6). Therefore, the objective of intervention design extends to distinguishing true cause-effect relationships from the emergent pairs as well as determining the direction of causality. Specifically, we focus on identifying the true edges specified in (4) as the edges exist in at least one component DAG of the mixture. For this purpose, two central objectives of our investigation are: ", "page_idx": 4}, {"type": "text", "text": "1. Determining the necessary and sufficient size of the interventions for identifying true edges $\\mathbf{E_{\\mathrm{t}}}$ ,   \n2. Designing efficient algorithms with near-optimal intervention sizes. ", "page_idx": 4}, {"type": "text", "text": "In this section, we investigate the first key question of interventional causal discovery on a mixture of DAGs and investigate the size of the necessary and sufficient interventions for identifying mixture parents of a node. First, we consider a mixture of general DAGs without imposing structural constraints and establish matching necessary and sufficient intervention size for distinguishing a true edge from an emergent pair. Then, we strengthen the results for a mixture of directed trees. The results established in this section are pivotal for understanding the fundamental limits of causal discovery of a mixture of DAGs. These results guide the intervention design in Section 4. ", "page_idx": 5}, {"type": "text", "text": "Our analysis uncovers the connections between the mixture distribution under an intervention $\\mathcal{T}$ and the structure of post-intervention component DAGs $\\{\\mathcal{G}_{\\ell,\\mathcal{T}}:\\ell\\in[K]\\}$ . We know that the interventional mixture distribution $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ satisfies the Markov property with respect to $\\mathcal{T}$ -mixture DAG $\\mathcal{G}_{\\mathrm{m},\\mathcal{Z}}$ specified in Definition 5. Therefore, in conjunction with the $\\mathcal{T}$ -mixture faithfulness assumption, the separation statements in $\\mathcal{G}_{\\mathrm{m},\\mathcal{Z}}$ can be inferred exactly by testing the conditional independencies in $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ . To establish the necessary and sufficient intervention sizes, we recall that set $\\Delta$ plays an important role in the separability conditions in $\\mathcal{T}$ -mixture DAG $\\mathcal{G}_{\\mathrm{m},\\mathcal{Z}}$ since $\\Delta$ allows paths across different component DAGs. The following result serves as an intermediate step in obtaining our main result. ", "page_idx": 5}, {"type": "text", "text": "Lemma 1. Consider an inseparable pair $(i,j)\\in\\mathbf{E}_{\\mathrm{i}}$ and an intervention $\\mathcal{T}\\subseteq\\mathbf{V}$ . We have the following identifiability guarantees using the interventional mixture distribution $p_{\\mathrm{m},\\mathbb{Z}}(x)$ . ", "page_idx": 5}, {"type": "text", "text": "(i) Identifiability: $I t$ is possible to determine whether $j\\in\\mathrm{pa}_{\\mathrm{m}}(i)$ if $j\\in\\mathcal{T}$ and there do not exist $\\Delta$ -through paths from $j$ to $i$ in $\\boldsymbol{\\mathcal{G}}_{\\ell,\\mathcal{T}}$ for any $\\ell\\in[K]$ .   \n(ii) Non-identifiability: It is impossible to determine whether $j\\in\\mathrm{pa}_{\\mathrm{m}}(i)\\:i f j\\in\\Delta_{\\mathbb{Z}}$ or there exists a $\\Delta$ -child-through path from $j$ to $i$ in at least one $\\boldsymbol{\\mathcal{G}}_{\\ell,\\mathcal{T}}$ where $\\ell\\in[K]$ . ", "page_idx": 5}, {"type": "text", "text": "Lemma 1 provides intuition for characterizing sufficient and necessary conditions for identifying a true edge. The identifiability result implies that it suffices to choose an intervention $\\mathcal{T}$ that reduces the viable $\\Delta$ -through paths in $\\mathcal{G}_{\\mathrm{m},\\mathcal{Z}}$ to true edges from $j$ to $i$ . Similarly, the non-identifiability result implies the necessity of intervening on $\\Delta$ -child nodes. Building on these properties, our main result in this section establishes matching necessary and sufficient intervention sizes for identifying true edges. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1 (Intervention sizes). Consider nodes $i,j\\in\\mathbf{V}$ in a mixture of $D A G s$ . ", "page_idx": 5}, {"type": "text", "text": "(i) Sufficiency: For any mixture of DAGs, there exists an intervention $\\mathcal{T}$ with $|\\mathcal{Z}|\\leq|\\mathrm{pa}_{\\mathrm{m}}(i)|+1$ that ensures the determination of whether $j\\in\\mathrm{pa}_{\\mathrm{m}}(i)$ using $C I$ tests on $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ . ", "page_idx": 5}, {"type": "text", "text": "(ii) Necessity: There exist DAG mixtures for which it is impossible to determine whether $j\\in\\mathrm{pa}_{\\mathrm{m}}(i)$ using CI tests on $p_{\\mathrm{m},\\mathbb{Z}}(x)$ for any intervention $\\mathcal{T}$ with $|\\mathcal{T}|\\leq|\\mathrm{pa}_{\\mathrm{m}}(i)|$ . ", "page_idx": 5}, {"type": "text", "text": "Theorem 1 represents a fundamental step for understanding the intricacies of mixture causal discovery and serves as a guide for evaluating the optimality and efficiency of any learning algorithm. We also note that the necessity statement reflects a worst-case scenario. As such, we present the following refined sufficiency results that can guide efficient algorithm designs. ", "page_idx": 5}, {"type": "text", "text": "Lemma 2. Consider nodes $i,j\\in\\mathbf{V}$ in a mixture of DAGs. It is possible to determine whether $j\\in\\mathrm{pa}_{\\mathrm{m}}(i)$ using $C I$ tests on $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ and any of the following interventions: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{Z}=\\{j\\}\\cup\\bigcup_{\\ell\\in[K]}\\left\\{\\mathrm{pa}_{\\ell}(i)\\cap\\mathrm{de}_{\\ell}(j)\\right\\}\\,;\\,o r}\\\\ &{\\mathcal{Z}=\\{j\\}\\cup\\bigcup_{\\ell\\in[K]}\\left\\{\\mathrm{an}_{\\ell}(i)\\cap\\mathrm{ch}_{\\ell}(j)\\right\\}\\,;\\,o r}\\\\ &{\\mathcal{Z}=\\{j\\}\\cup\\bigcup_{\\ell\\in[K]}\\left\\{\\mathrm{an}_{\\ell}(i)\\cap\\mathrm{de}_{\\ell}(j)\\cap\\Delta\\right\\}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Note that the three interventions in Lemma 2 can coincide when parents of $i$ in a component DAG are also children of $j$ and are in $\\Delta$ . This case yields the set $\\mathcal{Z}\\,=\\,\\mathrm{pa}_{\\mathrm{m}}(i)\\cup\\{j\\}$ with size $(|\\mathrm{pa}_{\\mathrm{m}}(i)|+1)$ . Since this can be a rare occurrence for realistic mixture models, partial knowledge about the underlying component DAGs, e.g., ancestral relations or the knowledge of $\\Delta$ , can prove to be useful for identifying $\\mathrm{pa}_{\\mathrm{m}}(i)$ using interventions with smaller sizes. Finally, we note that our results in Theorem 1 and Lemma 2 are given for a mixture of general DAGs, and they can be improved for special classes of DAGs. In the next result, we focus on mixtures of directed trees. ", "page_idx": 5}, {"type": "text", "text": "Theorem 2 (Intervention sizes \u2013 trees). Consider nodes $i,j\\in\\mathbf{V}$ in a mixture of $K$ directed trees. ", "page_idx": 5}, {"type": "text", "text": "(i) Sufficiency: For any mixture of directed trees, there exists an intervention $\\mathcal{T}$ with $|Z|\\le K+1$ such that it is possible to determine whether $j\\in\\mathrm{pa}_{\\mathrm{m}}(i)$ using $C I$ tests on $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ . ", "page_idx": 5}, {"type": "text", "text": "(ii) Necessity: There exist mixtures of directed trees such that it is impossible to determine whether $j\\in\\mathrm{pa}_{\\mathrm{m}}(i)$ using CI tests on $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ for any intervention $\\mathcal{T}$ with $|\\mathcal{T}|\\leq K$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 2 shows that, unlike the general result in Theorem 1, the number of mixture components plays a key role when considering a mixture of directed trees. Hence, prior knowledge of the number of mixture components can be useful for the causal discovery of a mixture of directed trees. ", "page_idx": 6}, {"type": "text", "text": "4 Learning algorithm and its analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we design an adaptive algorithm that identifies and orients all true edges, referred to as Causal Discovery from Interventions on Mixture Models (CADIM). The algorithm is summarized in Algorithm 1, and its steps are described in Section 4.1. We also analyze the performance guarantees of the algorithm and the optimality of the interventions used in the algorithm in Section 4.2. ", "page_idx": 6}, {"type": "text", "text": "4.1 Causal discovery from interventions on mixture models ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The proposed CADIM algorithm designs interventions for performing causal discovery on a mixture of DAGs. The algorithm is designed to be general and demonstrate feasible time complexity for any mixture of DAGs without imposing structural constraints. Therefore, we forego the computationally expensive task of learning the inseparable pairs from observational data, which requires $\\bar{O}(n^{2}\\cdot2^{n})$ CI tests [11], and entirely focus on leveraging interventions for discovering the true causal relationships. The key idea of the algorithm is to use interventions to decompose the ancestors of a node into topological layers and identify the mixture parents by sequentially processing the topological layers using carefully selected interventions. The algorithm consists of four main steps, which are described next. ", "page_idx": 6}, {"type": "text", "text": "Step 1: Identifying mixture ancestors. We start by identifying the set of mixture ancestors $\\mathrm{an}_{\\mathrm{m}}(i)$ for each node $i\\in\\mathbf{V}$ , i.e., the union of ancestors of $i$ in the component DAGs. For this purpose, we use single-node interventions. Specifically, for each node $i\\in\\mathbf{V}$ , we intervene on $\\mathcal{T}=\\{i\\}$ and construct the set of nodes that are marginally dependent on $X_{i}$ in $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ , i.e., ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{\\mathrm{de}}(i)=\\{j:X_{j}\\,\\,\\nmid\\!\\!\\perp X_{i}\\,\\,\\,\\mathrm{in}\\,\\,\\,p_{\\mathrm{m},\\{i\\}}\\}\\;,\\quad\\forall i\\in\\mathbf{V}\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Then, we construct the sets $\\hat{\\mathbf{a}}(i)=\\{j:i\\in\\hat{\\mathrm{de}}(j)\\}$ for all $i\\in\\mathbf{V}$ . Under $\\mathcal{T}$ -mixture faithfulness, this procedure ensures that $\\hat{\\mathrm{de}}(i)=\\mathrm{de}_{\\mathrm{m}}(i)$ , and $\\hat{\\mathrm{an}}(i)=\\mathrm{an}_{\\mathrm{m}}(i)$ (see Lemma 3). The rest of the algorithm steps aim to identify mixture parents of a single node $i$ , $\\mathrm{pa}_{\\mathrm{m}}(i)$ , within the set $\\hat{\\bf a n}(i)$ . Hence, the following steps can be repeated for all $i\\in\\mathbf{V}$ to identify all true edges. ", "page_idx": 6}, {"type": "text", "text": "Step 2: Obtaining cycle-free descendants. In this step, we consider a given node $i\\in\\mathbf{V}$ and aim to break the cycles across the nodes in $\\hat{\\bf a n}(i)$ by careful interventions. Once this is achieved, for all $j\\in\\hat{\\mathrm{an}}(i)$ , we will refine $j$ \u2019s descendant set $\\hat{\\operatorname*{de}}(j)$ to cycle-free descendant set $\\mathrm{de}_{i}(j)$ . The motivation is that these refined descendant sets can be used to topologically order the nodes in $\\mathrm{i}\\hat{\\bf n}(i)$ . The details of this step work as follows. First, we construct the set of cycles ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathcal{C}(i)\\triangleq\\{\\pi=(\\pi_{1},\\ldots,\\pi_{\\ell})\\,:\\,\\pi_{1}=\\pi_{\\ell}\\,,\\forall u\\in[\\ell-1]\\,\\,\\,\\pi_{u}\\in\\mathrm{\\hat{an}}(i)\\,\\,\\land\\,\\,\\pi_{u}\\in\\mathrm{\\hat{an}}(\\pi_{u+1})\\}\\;.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Subsequently, if $\\mathcal{C}(i)$ is not empty, we define a minimal set that shares at least one node with each cycle in $\\mathcal{C}(i)$ , ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{B}(i)\\triangleq\\mathrm{~a~minimal~set~such~that~}\\,\\forall\\pi\\in\\mathcal{C}(i)\\,\\,\\,|\\mathcal{B}(i)\\cap\\pi|\\ge1\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "We refer to $B(i)$ as the breaking set of node $i$ since intervening on any set $\\mathcal{T}$ that contains $B(i)$ breaks all the cyclic relationships in $\\mathcal{C}(i)$ . Then, if $\\mathcal{C}(i)$ is not empty, we sequentially intervene on $\\mathcal{T}=B(i)\\cup\\{j\\}$ for all $j\\in\\hat{\\mathrm{an}}(i)$ , and construct the cycle-free descendant sets defined as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{de}_{i}(j)\\gets\\{k\\in\\mathrm{an}(i)\\cup\\{i\\}:X_{j}\\ |\\ \\varLambda_{k}\\ \\mathrm{~in~}\\,p_{\\mathrm{m},\\tau}\\}\\;,\\quad\\mathrm{where}\\ \\,\\mathcal{T}=\\mathcal{B}(i)\\cup\\{j\\}\\;,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Note that $\\mathrm{de}_{i}(j)$ is a subset of $\\mathrm{de}_{\\mathrm{m}}(j)$ since intervening on $j$ makes it independent of all its nondescendants. Finally, we construct the set $\\mathcal{A}=\\{j\\in\\hat{\\mathrm{an}(i)}:i\\in\\mathrm{de}_{i}(j)\\}$ . ", "page_idx": 6}, {"type": "text", "text": "Step 3: Topological layering. In this step, we decompose $\\hat{\\bf a n}(i)$ into topological layers by using the cycle-free descendant sets constructed in Step 2. We start by constructing the first layer as ", "page_idx": 6}, {"type": "equation", "text": "$$\nS_{1}(i)=\\{j\\in\\ensuremath{\\mathcal{A}}:\\ensuremath{\\mathrm{de}}_{i}(j)\\cap\\ensuremath{\\mathcal{A}}=\\emptyset\\}\\;.\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "1: Step 1: Identify mixture ancestors   \n2: for $i\\in\\mathbf{V}$ do   \n3: Intervene on $\\mathcal{T}=\\{i\\}$ , observe samples from $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$   \n4: $\\hat{\\mathrm{de}}(i)\\leftarrow\\{j:X_{j}$ \u0338\u22a5\u22a5 $X_{i}$ in $p_{\\mathrm{m},\\mathcal{I}}\\}$ \u25b7mixture descendants of node $i$   \n5: for $i\\in\\mathbf{V}$ do   \n6: $\\hat{\\mathrm{an}}(i)\\leftarrow\\{j:i\\in\\hat{\\mathrm{de}}(j)\\}$ \u25b7mixture ancestors of node $i$   \n7: Repeat Steps 2, 3, 4 for all $i\\in\\mathbf{V}$   \n8: Step 2: Obtain cycle-free descendants   \n9: Find cycles among $\\hat{\\bf a n}(i)$   \n10: $\\mathcal{C}(i)\\stackrel{\\cdot}{\\leftarrow}\\{\\pi=(\\pi_{1}^{\\circ},\\ldots,\\pi_{t+1})\\;:\\;\\pi_{1}=\\pi_{t+1}\\;,\\forall u\\in[t]\\;\\;\\pi_{u}\\in\\mathrm{\\normalfont{a}}\\Lambda(i)\\;\\wedge\\;\\pi_{u}\\in\\mathrm{\\normalfont{a}}\\Lambda(\\pi_{u+1})\\}$   \n11: if $\\mathcal{C}(i)$ is empty then   \n12: $B(i)\\gets\\bar{\\emptyset}$   \n13: for $j\\in\\hat{\\mathrm{an}}(i)$ do   \n14: $\\mathrm{de}_{i}(j)\\xleftarrow{}\\{\\mathrm{de}(j)\\cap\\mathrm{a\\hat{n}}(i)\\}\\cup\\{i\\}$   \n15: else   \n16: $B(i)\\gets\\mathbf{a}$ minimal set such that $\\forall\\pi\\in\\mathcal{C}(i)\\ |\\mathcal{B}(i)\\cap\\pi|\\geq1$   \n17: for $j\\in\\hat{\\mathrm{an}}(i)$ do   \n18: Intervene on $\\mathcal{T}=B(i)\\cup\\{j\\}$ $\\triangleright$ break cycles among $\\hat{\\bf a n}(i)$   \n19: $\\mathrm{de}_{i}(j)\\gets\\{k\\in\\mathrm{a}\\mathrm{in}(i)\\cup\\{i\\}:X_{j}$ \u0338\u22a5\u22a5 $X_{k}$ in $p_{\\mathrm{m},\\mathcal{I}}\\}$ \u25b7cycle-free descendants of node $j$   \n20: $A\\leftarrow\\{j\\in\\hat{\\mathrm{an}}(i)\\ :\\ i\\in\\mathrm{de}_{i}(j)\\}$ $\\triangleright$ refined ancestors ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "21: Step 3: Topological layering ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "22: $t\\leftarrow0$   \n23: while $|{\\mathcal{A}}|\\geq1$ do   \n24: $\\begin{array}{r l}&{t\\leftarrow t+1}\\\\ &{S_{t}(i)\\leftarrow\\{j\\in\\mathcal{A}:\\mathrm{de}_{i}(j)\\cap\\mathcal{A}=\\emptyset\\}}\\\\ &{\\mathcal{A}\\leftarrow\\mathcal{A}\\setminus S_{t}(i)}\\end{array}$   \n25:   \n26:   \n27: Step 4: Identify mixture parents   \n28: $\\hat{\\mathrm{pa}(i)}\\gets\\emptyset$   \n29: for $\\dot{u}\\in(1,\\ldots,t)$ do   \n30: for $j\\in S_{u}(i)$ do   \n31: Intervene on $\\mathcal{T}=\\hat{\\mathrm{pa}}(i)\\cup B(i)\\cup\\{j\\}$   \n32: if $X_{j}$ \u0338\u22a5\u22a5 $X_{i}$ in $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ then   \n33: $\\bar{\\mathrm{pa}}(i)\\leftarrow\\hat{\\mathrm{pa}}(i)\\cup\\{j\\}$   \n34: Return p\u02c6a(i) ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "The construction of cycle-free descendant sets ensures that $S_{1}(i)$ is not empty. Next, we update $A\\leftarrow A\\setminus S_{1}(i)$ by removing layer $S_{1}(i)$ to conclude the first step. Then, we iteratively construct the layers $S_{u}(i)=\\{j\\in\\mathcal{A}:\\mathop{\\mathrm{de}}(j)\\cap\\mathcal{A}=\\emptyset\\}$ and update $A\\leftarrow A\\setminus S_{u}(i)$ as in Line 26 of the algorithm. We continue until the set $\\boldsymbol{\\mathcal{A}}$ is exhausted, and denote these topological layers by $\\{S_{1}(i),\\allowbreak\\dots,S_{t}(i)\\}$ . ", "page_idx": 7}, {"type": "text", "text": "Step 4: Identifying the mixture parents. Finally, we process the topological layers sequentially to identify the mixture parents in each layer. For a node $j\\,\\in\\,S_{1}(i)$ , whether $j\\,\\in\\,\\mathrm{pa}_{\\mathrm{m}}(i)$ can be determined from a marginal independence test on $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ where $\\mathcal{T}\\,=\\,\\mathcal{B}(i)\\cup\\{j\\}$ . Leveraging this result, when processing each $S_{u}(i)$ , we consider the nodes $j\\in S_{u}(i)$ sequentially and intervene on $\\mathcal{T}=\\hat{\\mathrm{pa}}(i)\\cup\\bar{B}(i)\\cup\\{j\\}$ , where $\\hat{\\mathrm{pa}}(i)$ denotes the estimated mixture parents. Under this intervention, a statistical dependence implies a true edge from $j$ to $i$ . Hence, we update the set $\\hat{\\mathrm{pa}}(i)$ as follows. ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathrm{\\hat{pa}}(i)\\leftarrow\\mathrm{\\hat{pa}}(i)\\cup\\{j\\}\\ \\ \\mathrm{if}\\ \\ X_{j}\\ \\mathrm{\\mathcal{N}}_{i}\\ X_{i}\\ \\ \\mathrm{in}\\ p_{\\mathrm{m},{\\mathbb{Z}}}\\ \\ \\mathrm{where}\\ \\ \\mathrm{\\mathbb{Z}}=\\mathrm{\\hat{pa}}(i)\\cup\\mathcal{B}(i)\\cup\\{j\\}\\ .\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "After the last layer $S_{t}(i)$ is processed, the algorithm returns the estimated mixture parents $\\hat{\\mathrm{pa}}(i)$ . By repeating Steps 2, 3, and 4 for all $i\\in\\mathbf{V}$ , we determine the true edges with their orientations. ", "page_idx": 7}, {"type": "text", "text": "4.2 Guarantees of the CADIM algorithm ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this section, we establish the guarantees of the CADIM algorithm and interpret them vis- $\\hat{\\mathbf{\\sigma}}_{\\mathbf{\\hat{a}}}$ -vis the results in Section 3. We start by providing the following result to show the correctness of identifying mixture ancestors. ", "page_idx": 8}, {"type": "text", "text": "Lemma 3. Given $\\mathcal{T}$ -mixture faithfulness, Step 1 of Algorithm 1 identifies $\\{\\mathrm{an}_{\\mathrm{m}}(i):i\\in[n]\\}$ using $n$ single-node interventions. ", "page_idx": 8}, {"type": "text", "text": "Note that the mixture ancestor sets $\\{\\mathrm{an}_{\\mathrm{m}}(i)\\}$ do not imply a topological order over the nodes $\\mathbf{V}$ , e.g., there may exist nodes $u,v$ such that $u\\in\\mathrm{{an}}_{\\mathrm{{m}}}(v)$ and $v\\,\\in\\,\\mathrm{an}_{\\mathrm{m}}(u)$ . As such, a major difficulty in learning a mixture of DAGs compared to learning a single DAG is the possible cyclic relationships formed by the combination of components of the mixture. Recall that the breaking set is specified in (11) to treat such possible cycles carefully. We refer to the size of $B(i)$ as the cyclic complexity number of node $i$ , denoted by $\\tau_{i}$ , and the size of the largest breaking set by $\\tau_{\\mathrm{m}}$ as ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\tau_{i}\\triangleq|\\boldsymbol{\\mathcal{B}}(i)|\\;,\\quad\\forall i\\in{\\mathbf{V}}\\;,\\qquad\\mathrm{and}\\quad\\tau_{\\mathrm{m}}\\triangleq\\operatorname*{max}_{i\\in{\\mathbf{V}}}\\tau_{i}\\;.\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Note that $\\tau_{i}$ is readily bounded by the number of cycles in $\\mathcal{C}(i)$ . Next, we analyze the guarantees of the algorithm for a node $i$ in two cases: $\\tau_{i}=0$ (cycle-free case) and $\\tau_{i}\\geq1$ (nonzero cyclic complexity). ", "page_idx": 8}, {"type": "text", "text": "Cycle-free case. Our next result shows that if $\\tau_{i}=0$ , i.e., there are no cycles among the nodes in $\\mathrm{an}_{\\mathrm{m}}(i)$ , then we identify the mixture parents $\\mathrm{pa}_{\\mathrm{m}}(i)$ , i.e., the union of the nodes that are parents of $i$ in at least one component DAG, using interventions with the optimal size. ", "page_idx": 8}, {"type": "text", "text": "Theorem 3 (Guarantees for cycle-free ancestors). If the cyclic complexity of node i is zero, then Algorithm 1 ensures that $\\hat{\\mathrm{pa}}(\\dot{i})=\\mathrm{pa}_{\\mathrm{m}}(i)$ by using $|\\mathrm{an}_{\\mathrm{m}}(i)|$ interventions where the size of each intervention is at most $|\\mathrm{pa}_{\\mathrm{m}}(i)|+1$ . ", "page_idx": 8}, {"type": "text", "text": "Theorem 3 shows that by repeating the algorithm steps for each node $i\\in\\mathbf{V}$ , we can identify all true edges with their orientations using $\\begin{array}{r}{n+\\sum_{i\\in\\mathbf{V}}\\left|\\mathrm{an}_{\\mathrm{m}}(i)\\right|\\le n+n(n-1)=n^{2}}\\end{array}$ interventions, where the size of each intervention is bounded  by the worst-case necessary size established in Theorem 1. ", "page_idx": 8}, {"type": "text", "text": "Nonzero cyclic complexity. Finally, we address the most general case, in which the mixture ancestors of node $i$ might contain cycles. In this case, our algorithm performs additional interventions to break the cycles among $\\mathrm{an}_{\\mathrm{m}}(i)$ . Hence, the number and size of the interventions will be greater than the cycle-free case, which is established in the following result. ", "page_idx": 8}, {"type": "text", "text": "Theorem 4 (Guarantees for general mixtures). Algorithm 1 ensures that $\\hat{\\mathrm{pa}}(i)=\\mathrm{pa}_{\\mathrm{m}}(i)$ by using $|\\mathrm{an}_{\\mathrm{m}}(i)|$ interventions with size $\\tau_{i}+1$ , and $|\\mathrm{an}_{\\mathrm{m}}(i)|$ interventions with size at most $|\\mathrm{pa}_{\\mathrm{m}}(i)|+\\tau_{i}+1$ . ", "page_idx": 8}, {"type": "text", "text": "Theorem 4 shows that, Algorithm 1 achieves the causal discovery objectives by using a total of $\\begin{array}{r}{n+2\\sum_{i\\in\\mathbf{V}}\\left|\\mathrm{an}_{\\mathrm{m}}(i)\\right|\\le n+2n(n-1)=\\mathcal{O}(n^{2})}\\end{array}$ interventions, where the maximum intervention size for learning each $\\mathrm{pa}_{\\mathrm{m}}(i)$ is at most $\\tau_{i}$ larger than the necessary and sufficient size $\\vert\\mathrm{pa}_{\\mathrm{m}}(i)\\vert+1$ This optimality gap reflects the challenges of accommodating cyclic relationships in intervention design for learning in mixtures while also maintaining a quadratic number of interventions ${\\mathcal{O}}(n^{2})$ . ", "page_idx": 8}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We evaluate the performance of Algorithm 1 for estimating the true edges in a mixture of DAGs using synthetic data and investigate the need for interventions, the effect of the graph size, and the cyclic complexity. Additional results for varying the number of components, parameterization, and number of samples are provided in Appendix $\\dot{\\mathrm{~E~}}^{1}$ . ", "page_idx": 8}, {"type": "text", "text": "Experimental setup. We use an Erd\u02ddos-R\u00e9nyi model $G(n,p)$ with density $p\\,=\\,2/n$ to generate the component DAGs $\\{\\mathcal{G}_{\\ell}:\\ell\\in[K]\\}$ for different values of nodes $n$ and mixture components $K$ . We adopt linear structural equation models (SEMs) with Gaussian noise for the causal models, in which the noise for node $i$ is sampled from $\\mathcal{N}(\\mu_{i},\\sigma_{i}^{2})$ where $\\mu_{i}$ is sampled uniformly in $[-1,1]$ and $\\sigma_{i}^{2}$ is sampled uniformly in [0.5, 1.5]. The edge weights are sampled uniformly in $\\pm[0.25,2]$ . We consider the case where a change in the conditional distribution of node $i$ is only caused by changes in the parents of $i$ across different DAGs. We use a partial correlation test to check (conditional) independence in the algorithm steps, similar to the related work [10, 11]. We repeat this procedure for 100 randomly generated DAG mixtures for each of the following settings. ", "page_idx": 8}, {"type": "image", "img_path": "mFrlCI8sov/tmp/0cb4de022b38d88467cbb48309aa70e24cc44e67010382c05c9d32b8dd62c775.jpg", "img_caption": ["Figure 2: Mean true edge recovery rates and quantification of mean cyclic complexity of a node. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Need for interventions. We demonstrate the need for interventions for learning the skeleton in the mixture of DAGs, unlike the case of single DAGs. To this end, we consider a mixture of $K=2$ DAGs and learn the inseparable node pairs via exhaustive CI tests (see Algorithm 2 in Appendix E). Figure 2a empirically verifies the claim that true edges (even their undirected versions) cannot be learned using observational data only. ", "page_idx": 9}, {"type": "text", "text": "Recovery of true edges. We evaluate the performance of Algorithm 1 on the central task of learning the true edges in the mixture. For this purpose, we report average precision and recall rates for recovering the true edges. We look into the performance of Algorithm 1 under a varying number of nodes $n\\in[5,30]$ for a mixture of $K=3$ DAGs and using 5000 samples from each DAG. Figure 2b demonstrates that Algorithm 1 maintains a strong performance even under $n=30$ nodes. We provide additional results for the number of DAGs in the range $K\\in[2,10]$ and varying number of samples in Appendix E. ", "page_idx": 9}, {"type": "text", "text": "Quantification of cyclic complexity. We recall that for finding the mixture parents of a node $i$ , the maximum size of the intervention used in Algorithm 1 is at most $\\tau_{i}$ , i.e., cyclic complexity, larger than the necessary size. In Figure 2c, we plot the empirical values of average cyclic complexity \u2013 both the ground truth and estimated by the algorithm. Figure 2c shows that even though average $\\tau_{i}$ increases with $K$ , it still remains very small, e.g., approximately 1.5 for a mixture of $K=3$ DAGs with $n=10$ nodes. Furthermore, on average, the estimated $\\tau_{i}$ values used in the algorithm are almost identical to the ground truth $\\tau_{i}$ . Therefore, Algorithm 1 maintains its close to optimal intervention size guarantees in the finite-sample regime. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we have conducted the first analysis of using interventions to learn causal relationships in a mixture of DAGs. First, we have established the matching necessary and sufficient size of interventions needed for learning the true edges in a mixture. Subsequently, guided by this result, we have designed an algorithm that learns the true edges using interventions with close to optimal sizes. We have also analyzed the optimality gap of our algorithm in terms of the cyclic relationships within the mixture model. The proposed algorithm uses a total of ${\\mathcal{O}}(n^{2})$ interventions. Establishing lower bounds for the number of interventions with constrained sizes remains an important direction for future work, which can draw connections to intervention design for single-DAG and further characterize the differences of causal discovery in mixtures. Finally, generalizing the mixture model to accommodate partial knowledge of the underlying domains can be useful in disciplines where such knowledge can be acquired a priori. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and disclosure of funding ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This work was supported by IBM through the IBM-Rensselaer Future of Computing Research Collaboration. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Nir Friedman, Michal Linial, Iftach Nachman, and Dana Pe\u2019er. Using Bayesian networks to analyze expression data. In Proc. International Conference on Computational Molecular Biology, Tokyo, Japan, April 2000.   \n[2] Nicolai Meinshausen, Alain Hauser, Joris M Mooij, Jonas Peters, Philip Versteeg, and Peter B\u00fchlmann. Methods for causal inference from gene perturbation experiments and validation. Proceedings of the National Academy of Sciences, 113(27):7361\u20137368, 2016.   \n[3] Guido W Imbens. Potential outcome and directed acyclic graph approaches to causality: Relevance for empirical practice in economics. Journal of Economic Literature, 58(4):1129\u2013 1179, 2020. [4] Brett M Reid, Jennifer B Permuth, and Thomas A Sellers. Epidemiology of ovarian cancer: A review. Cancer Biology & Medicine, 14(1):9, 2017.   \n[5] Yanxi Chen and H. Vincent Poor. Learning mixtures of linear dynamical systems. In Proc. International Conference on Machine Learning, Baltimore, Maryland, July 2022. [6] Kirsten Bulteel, Francis Tuerlinckx, Annette Brose, and Eva Ceulemans. Clustering vector autoregressive models: Capturing qualitative differences in within-person dynamics. Frontiers in Psychology, 7:1540, 2016. [7] Emma Brunskill, Bethany R. Leffler, Lihong Li, Michael L. Littman, and Nicholas Roy. Provably efficient learning with typed parametric models. Journal of Machine Learning Research, 10 (68):1955\u20131988, 2009. [8] Peter Spirtes. Directed cyclic graphical representations of feedback models. In Proc. Conference on Uncertainty in Artificial Intelligence, Montr\u00e9al, Canada, August 1995.   \n[9] Eric V. Strobl. Causal discovery with a mixture of dags. Machine Learning, 112(11):4201\u20134225, 2023.   \n[10] Basil Saeed, Snigdha Panigrahi, and Caroline Uhler. Causal structure discovery from distributions arising from mixtures of dags. In Proc. International Conference on Machine Learning, virtual, July 2020.   \n[11] Burak Var\u0131c\u0131, Dmitriy Katz, Dennis Wei, Prasanna Sattigeri, and Ali Tajer. Separability analysis for causal discovery in mixture of DAGs. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https://openreview.net/forum?id=ALRWXT1RLZ.   \n[12] Thomas Verma and Judea Pearl. An algorithm for deciding if a set of observed independencies has a causal explanation. In Proc. Conference on Uncertainty in Artificial Intelligence, Stanford, CA, July 1992.   \n[13] Frederick Eberhardt and Richard Scheines. Interventions and causal inference. Philosopy of Science, 74(5):981\u2013995, December 2007.   \n[14] Alain Hauser and Peter B\u00fchlmann. Two optimal strategies for active learning of causal models from interventional data. International Journal of Approximate Reasoning, 55(4):926\u2013939, June 2014.   \n[15] Antti Hyttinen, Frederick Eberhardt, and Patrik O Hoyer. Experiment selection for causal discovery. Journal of Machine Learning Research, 14(1):3041\u20133071, 2013.   \n[16] Karthikeyan Shanmugam, Murat Kocaoglu, Alexandros G Dimakis, and Sriram Vishwanath. Learning causal graphs with small interventions. In Proc. Advances in Neural Information Processing Systems, Montr\u00e9al, Canada, December 2015.   \n[17] Chandler Squires, Sara Magliacane, Kristjan Greenewald, Dmitriy Katz, Murat Kocaoglu, and Karthikeyan Shanmugam. Active structure learning of causal DAGs via directed clique trees. In Proc. Advances in Neural Information Processing Systems, December 2020.   \n[18] Sumanth Varambally, Yi-An Ma, and Rose Yu. Discovering mixtures of structural causal models from time series data. arXiv:2310.06312, 2023.   \n[19] Murat Kocaoglu, Karthikeyan Shanmugam, and Elias Bareinboim. Experimental design for learning causal graphs with latent variables. Proc. Advances in Neural Information Processing Systems, December 2017.   \n[20] Raghavendra Addanki, Shiva Kasiviswanathan, Andrew McGregor, and Cameron Musco. Efficient intervention design for causal discovery with latents. In Proc. International Conference on Machine Learning, July 2020.   \n[21] Raghavendra Addanki, Andrew McGregor, and Cameron Musco. Intervention efficient algorithms for approximate learning of causal graphs. In Algorithmic Learning Theory, March 2021.   \n[22] Yang-Bo He and Zhi Geng. Active learning of causal networks with intervention experiments and optimal designs. Journal of Machine Learning Research, 9(November):2523\u20132547, 2008.   \n[23] Kristjan Greenewald, Dmitriy Katz, Karthikeyan Shanmugam, Sara Magliacane, Murat Kocaoglu, Enric Boix Adsera, and Guy Bresler. Sample efficient active learning of causal trees. In Proc. Advances in Neural Information Processing Systems, Vancouver, Canada, December 2019.   \n[24] Vibhor Porwal, Piyush Srivastava, and Gaurav Sinha. Almost optimal universal lower bound for learning causal dags with atomic interventions. In Proc. International Conference on Artificial Intelligence and Statistics, virtual, March 2022.   \n[25] Davin Choo, Kirankumar Shiragur, and Arnab Bhattacharyya. Verification and search algorithms for causal dags. In Proc. Advances in Neural Information Processing Systems, New Orleans, LA, December 2022.   \n[26] Murat Kocaoglu, Alex Dimakis, and Sriram Vishwanath. Cost-optimal learning of causal graphs. In Proc. International Conference on Machine Learning, Sydney, Australia, August 2017.   \n[27] Erik Lindgren, Murat Kocaoglu, Alexandros G Dimakis, and Sriram Vishwanath. Experimental design for cost-aware learning of causal graphs. In Proc. Advances in Neural Information Processing Systems, Montr\u00e9al, Canada, December 2018.   \n[28] Ehsan Mokhtarian, Saber Salehkaleybar, AmirEmad Ghassami, and Negar Kiyavash. A unified experiment design approach for cyclic and acyclic causal models. Journal of Machine Learning Research, 24(354):1\u201331, 2023.   \n[29] Karren Yang, Abigail Katcoff, and Caroline Uhler. Characterizing and learning equivalence classes of causal DAGs under interventions. In Proc. International Conference on Machine Learning, Stockholm, Sweden, July 2018.   \n[30] Amin Jaber, Murat Kocaoglu, Karthikeyan Shanmugam, and Elias Bareinboim. Causal discovery from soft interventions with unknown targets: Characterization and learning. In Proc. Advances in Neural Information Processing Systems, December 2020.   \n[31] Chandler Squires, Yuhao Wang, and Caroline Uhler. Permutation-based causal structure learning with unknown intervention targets. In Proc. Conference in Uncertainty in Artificial Intelligence, August 2020.   \n[32] Philippe Brouillard, S\u00e9bastien Lachapelle, Alexandre Lacoste, Simon Lacoste-Julien, and Alexandre Drouin. Differentiable causal discovery from interventional data. In Proc. Advances in Neural Information Processing Systems, December 2020.   \n[33] Bo Thiesson, Christopher Meek, David Maxwell Chickering, and David Heckerman. Learning mixtures of DAG models. In Proc. Conference on Uncertainty in Artificial Intelligence, Madison, WI, July 1998.   \n[34] Biwei Huang, Kun Zhang, Pengtao Xie, Mingming Gong, Eric P Xing, and Clark Glymour. Specific and shared causal relation modeling and mechanism-based clustering. In Proc. Advances in Neural Information Processing Systems, Vancouver, Canada, December 2019.   \n[35] Kun Zhang and Madelyn RK Glymour. Unmixing for causal inference: Thoughts on Mccaffrey and Danks. The British Journal for the Philosophy of Science, 2020.   \n[36] Wei Chen, Yunjin Wu, Ruichu Cai, Yueguo Chen, and Zhifeng Hao. CCSL: A causal structure learning method from multiple unknown environments. arXiv:2111.09666, 2021.   \n[37] Alex Markham, Richeek Das, and Moritz Grosse-Wentrup. A distance covariance-based kernel for nonlinear causal clustering in heterogeneous populations. In Proc. Conference on Causal Learning and Reasoning, Eureka, CA, April 2022.   \n[38] Biwei Huang, Kun Zhang, Jiji Zhang, Joseph D Ramsey, Ruben Sanchez-Romero, Clark Glymour, and Bernhard Sch\u00f6lkopf. Causal discovery from heterogeneous/nonstationary data. Journal of Machine Learning Research, 21(89):1\u201353, 2020.   \n[39] Kun Zhang, Biwei Huang, Jiji Zhang, Clark Glymour, and Bernhard Sch\u00f6lkopf. Causal discovery from nonstationary/heterogeneous data: Skeleton estimation and orientation determination. In Proc. International Joint Conference on Artificial Intelligence, Melbourne, Australia, August 2017.   \n[40] Joris M. Mooij, Sara Magliacane, and Tom Claassen. Joint causal inference from multiple contexts. Journal of Machine Learning Research, 21(99):1\u2013108, 2020.   \n[41] Amin Jaber, Murat Kocaoglu, Karthikeyan Shanmugam, and Elias Bareinboim. Causal discovery from soft interventions with unknown targets: Characterization and learning. In Proc. Advances in Neural Information Processing Systems, virtual, December 2020.   \n[42] Chenxi Liu and Kun Kuang. Causal structure learning for latent intervened non-stationary data. In Proc. International Conference on Machine Learning, Honolulu, Hawaii, July 2023.   \n[43] Peter Spirtes, Clark N Glymour, Richard Scheines, and David Heckerman. Causation, Prediction, and Search. MIT Press, Cambridge, MA, 2000.   \n[44] Judea Pearl. Causality. Cambridge University Press, Cambridge, UK, 2009.   \n[45] Joris M Mooij and Tom Claassen. Constraint-based causal discovery using partial ancestral graphs in the presence of cycles. In Proc. Conference on Uncertainty in Artificial Intelligence, virtual, August 2020.   \n[46] F Ann Ran, Patrick D Hsu, Jason Wright, Vineeta Agarwala, David A Scott, and Feng Zhang. Genome engineering using the crispr-cas9 system. Nature Protocols, 8(11):2281\u20132308, 2013.   \n[47] Geoffrey J McLachlan, Sharon X Lee, and Suren I Rathnayake. Finite mixture models. Annual Review of Statistics and Its Application, 6(1):355\u2013378, 2019.   \n[48] Abhinav Kumar and Gaurav Sinha. Disentangling mixtures of unknown causal interventions. In Proc. Uncertainty in Artificial Intelligence, July 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Interventional Causal Discovery in a Mixture of DAGs Appendices ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Table of Contents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "A Auxiliary results 14 ", "page_idx": 13}, {"type": "text", "text": "B Proofs for Section 3 15   \nB.1 Proof of Lemma 1 15   \nB.2 Proof of Lemma 2 15   \nB.3 Proof of Theorem 1 15   \nB.4 Proof of Theorem 2 16   \nC Proofs for Section 4 16   \nC.1 Proof of Lemma 3 16   \nC.2 Proof of Theorem 3 16   \nC.3 Proof of Theorem 4 18   \nD Additional examples 20   \nD.1 Partitioning true edges into component DAGs . 20   \nD.2 An example of cyclic complexity 20 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Auxiliary results ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "Lemma 4 (Markov property, [10, Theorem 3.2]). Let $A,B,C\\subseteq\\mathbf{V}$ be disjoint. If $\\bar{A}$ and $\\bar{B}$ are $d$ -separated given $\\bar{C}$ in the mixture $D A G,$ , then $X_{A}$ and $X_{B}$ are conditionally independent given $X_{C}$ in mixture distribution. ", "page_idx": 13}, {"type": "text", "text": "Lemma 5 ([11, Theorem 5]). Consider nodes $i,j\\in\\mathbf{V}$ such that i and $j$ are not adjacent in any of the component $D A G s$ , i.e., $i\\notin\\mathrm{pa}_{\\mathrm{m}}(j)$ and $j\\not\\in\\mathrm{pa}_{\\mathrm{m}}(i)$ . ", "page_idx": 13}, {"type": "text", "text": "(i) If $i\\in\\Delta$ and $j\\in\\Delta$ : $i$ and $j$ are always inseparable, i.e., $(i-j)$ is an emergent edge. ", "page_idx": 13}, {"type": "text", "text": "(ii) If $i\\not\\in\\Delta$ and $j\\not\\in\\Delta$ : If $i$ and $j$ are inseparable, then there exist two component DAGs G\u2113, G\u2113\u2032 such that $g_{\\ell}$ contains a $\\Delta$ -through path from $i$ to $j$ and $\\mathcal{G}_{\\ell^{\\prime}}$ contains a $\\Delta$ -through path from $j$ to $i$ . ", "page_idx": 13}, {"type": "text", "text": "(iii) If $i\\not\\in\\Delta$ and $j\\in\\Delta$ : If $i$ and $j$ are inseparable, then at least one component DAG contains a $\\Delta$ -through path from $i$ to $j$ . ", "page_idx": 13}, {"type": "text", "text": "Lemma 6 ([11, Theorem 6]). Suppose that $\\mathcal{G}_{1},\\ldots,\\mathcal{G}_{K}$ are directed trees. Consider nodes $i,j\\in\\mathbf{V}$ such that i and $j$ are not adjacent in any component DAG, i.e., $i\\notin\\mathrm{pa}_{\\mathrm{m}}(j)$ and $j\\not\\in\\mathrm{pa}_{\\mathrm{m}}(i)$ . ", "page_idx": 13}, {"type": "text", "text": "(i) If $i\\in\\Delta$ and $j\\in\\Delta$ : $i$ and $j$ are always inseparable. ", "page_idx": 13}, {"type": "text", "text": "(ii) If $i\\not\\in\\Delta$ and $j\\not\\in\\Delta$ : $i$ and $j$ are separable if and only if there does not exist $g_{\\ell}$ , G\u2113\u2032 such that the two DAGs contain $\\Delta$ -child-through paths between i and $j$ in opposite directions. ", "page_idx": 13}, {"type": "text", "text": "(iii) If $i\\not\\in\\Delta$ and $j\\in\\Delta$ : $i$ and $j$ are separable if and only if none of the component DAGs contains a $\\Delta$ -child-through path from $i$ to $j$ . ", "page_idx": 13}, {"type": "text", "text": "B Proofs for Section 3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "B.1 Proof of Lemma 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof of identifiability: Since $j\\in\\mathcal{T}$ , we have $j\\notin\\Delta_{\\mathbb{Z}}$ . Then, Lemma 5 (iii) implies that if $(j\\rightarrow i)$ is not a true edge and there does not exist a $\\Delta$ -through path from $j$ to $i$ in any $\\boldsymbol{\\mathcal{G}_{\\ell,\\mathcal{T}}}$ , then $i$ and $j$ are separable in $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ . Consequently, whether $j\\in\\mathrm{pa}_{\\mathrm{m}}(i)$ can be determined from $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ . ", "page_idx": 14}, {"type": "text", "text": "Proof of non-identifiability: First, note that using Lemma 4, for any intervention $\\mathcal{T}$ , $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ is Markov with respect to the $\\mathcal{T}$ -mixture DAG $\\mathcal{G}_{\\mathrm{m},\\mathcal{Z}}$ . Then, for $j\\in\\Delta_{\\mathbb{Z}}$ , if also $i\\in\\Delta_{\\mathbb{Z}}$ , $i$ and $j$ are inseparable in $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ regardless of whether there is a true edge between $i$ and $j$ . Hence, whether $j\\in\\mathrm{pa}_{\\mathrm{m}}(i)$ cannot be determined using CI tests on $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ . For the second statement, recall that intervention $\\mathcal{T}$ cuts off all incoming edges to nodes of $\\mathcal{T}$ in all component DAGs. Then, if $i\\in\\mathcal{T}$ , we cannot determine whether $j\\in\\mathrm{pa}_{\\mathrm{m}}(i)$ since the possible influence of $j$ on $i$ is cut off by the intervention. Suppose that $i\\in\\Delta_{\\mathbb{Z}}$ , and let $\\pi$ be a $\\Delta$ -child-through path from $j$ to $i$ in some component DAG $\\boldsymbol{\\mathcal{G}_{\\ell,\\mathcal{T}}}$ , i.e., $\\pi$ is given by $j\\stackrel{\\ell}{\\to}k\\stackrel{\\ell}{\\sim}i$ for some $k\\in\\Delta_{\\mathbb{Z}}$ . Since $i\\in\\Delta_{\\mathbb{Z}}$ , the $\\mathcal{T}$ -mixture DAG $\\mathcal{G}_{\\mathrm{m},\\mathcal{Z}}$ also contains the path $j\\stackrel{\\ell}{\\to}k\\stackrel{\\ell}{\\leftarrow}y\\stackrel{\\ell}{\\to}i$ . Since these two paths cannot be blocked simultaneously by conditioning on a set of nodes, $i$ and $j$ are inseparable regardless of whether $j\\in\\mathrm{pa}_{\\mathrm{m}}(i)$ . Therefore, whether $j\\in\\mathrm{pa}_{\\mathrm{m}}(i)$ cannot be determined using CI tests on $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ . ", "page_idx": 14}, {"type": "text", "text": "B.2 Proof of Lemma 2 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "We start by providing a general statement that will be used for the proof of the three subcases. Let $\\mathcal{T}$ be an intervention such that $j\\in\\mathcal{T}$ and there does not exist a $\\Delta$ -through path from $j$ to $i$ in any component DAG $\\boldsymbol{\\mathcal{G}}_{\\ell,\\mathcal{T}}$ . In this case, using Lemma 5, if $(j\\rightarrow i)$ is not a true edge, then $i$ and $j$ are separable in $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ . Subsequently, if $i$ and $j$ are inseparable in $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ , then $(j\\rightarrow i)$ is a true edge. ", "page_idx": 14}, {"type": "text", "text": "Let $\\pi$ be a $\\Delta$ -through path from $j$ to $i$ in some $\\boldsymbol{\\mathcal{G}_{\\ell,\\!\\mathcal{Z}}}$ . Note that, for any of the following three intervention sets, ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\mathcal{Z}}=\\{j\\}\\cup\\bigcup_{\\ell\\in[K]}\\left\\{\\mathrm{pa}_{\\ell}(i)\\cap\\mathrm{de}_{\\ell}(j)\\right\\}\\;,}\\\\ &{\\bar{\\mathcal{Z}}=\\{j\\}\\cup\\bigcup_{\\ell\\in[K]}\\left\\{\\mathrm{an}_{\\ell}(i)\\cap\\mathrm{ch}_{\\ell}(j)\\right\\}\\;,}\\\\ &{\\bar{\\mathcal{Z}}=\\{j\\}\\cup\\bigcup_{\\ell\\in[K]}\\left\\{\\mathrm{an}_{\\ell}(i)\\cap\\mathrm{de}_{\\ell}(j)\\cap\\Delta\\right\\}\\;,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "$\\pi$ cannot contain a node between $j$ and $i$ . Therefore, if $j\\not\\in\\mathrm{pa}_{\\mathrm{m}}(i)$ , there does not exist a $\\Delta$ -through path from $j$ to $i$ . Subsequently, if $i$ and $j$ are not separable in $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ for any of these three interventions, it means there exists $j\\xrightarrow{\\ell}i$ for some component DAG $g_{\\ell}$ . Therefore, whether $j\\in\\mathrm{pa}_{\\mathrm{m}}(i)$ can be determined by checking whether $i$ and $j$ are separable in $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ for any of these three interventions. ", "page_idx": 14}, {"type": "text", "text": "B.3 Proof of Theorem 1 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "The sufficiency result immediately follows from Lemma 2 since each of the three interventions in stated in Lemma 2 is a subset of $\\mathrm{\\dot{\\pa}}_{\\mathrm{m}}(i)\\cup\\{j\\}$ . To show the worst-case necessity of an intervention with size at least $\\vert\\mathrm{pa}_{\\mathrm{m}}(i)\\vert+1$ , we construct the following example. Consider component DAGs $\\{\\mathcal{G}_{\\ell}:\\ell\\in[K]\\}$ such that $\\mathcal{G}_{1}$ contains a single edge $i\\stackrel{1}{\\rightarrow}j$ . In the rest of the component DAGs, for any $k\\xrightarrow{\\ell}i$ edge, let us also draw $j\\xrightarrow{\\ell}k$ . We do not put any constraints on the other possible connections in $\\{{\\mathcal{G}}_{\\ell}:{\\bar{\\ell}}\\in\\{2,\\ldots,K\\}\\}$ . Note that this construction yields that $\\mathrm{pa}_{\\mathrm{m}}(i)\\cup\\{i,j\\}\\subseteq\\Delta$ . Consider the paths ", "page_idx": 14}, {"type": "equation", "text": "$$\nj\\,\\nleftarrow\\,y\\stackrel{1}{\\rightarrow}\\,i\\ ,\\quad\\mathrm{and}\\quad\\{j\\stackrel{\\ell}{\\rightarrow}k\\stackrel{\\ell}{\\rightarrow}i:k\\in\\mathrm{pa}_{\\mathrm{m}}(i)\\}\\;.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "For any intervention ${\\mathcal{T}}\\subseteq\\mathbf{V}\\setminus\\{i\\}$ that does not contain all nodes in $\\mathrm{{pa}}_{\\mathrm{{m}}}(i)\\cup\\{j\\}$ , at least one of these paths will be active in the $\\mathcal{T}$ -mixture DAG $\\mathcal{G}_{\\mathrm{m},\\mathcal{Z}}$ , regardless of whether there exists a $j\\xrightarrow{\\ell}i$ , $\\ell\\in\\{2,\\ldots,K\\}$ edge. Therefore, at the worst-case, whether $j\\in\\mathrm{pa}_{\\mathrm{m}}(i)$ cannot be determined from $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ for any intervention $\\mathcal{T}$ with size $|\\mathcal{T}|\\leq|\\mathrm{pa}_{\\mathrm{m}}(i)|$ . ", "page_idx": 14}, {"type": "text", "text": "B.4 Proof of Theorem 2 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof of sufficiency. Consider an inseparable pair $(i-j)$ in $p_{\\mathrm{m}}$ . For a mixture of directed trees, Lemma 6 implies that if there does not exist a $\\Delta$ -child-through path from $j$ to $i$ in any component DAG, then $(i-j)$ corresponds to a true edge. Consider the intervention ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathcal{Z}=\\{j\\}\\cup\\bigcup_{\\ell\\in[K]}\\left\\{\\mathrm{an}_{\\ell}(i)\\cap\\mathrm{ch}_{\\ell}(j)\\cap\\Delta\\right\\},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which cuts off all $\\Delta$ -child-through paths from $j$ to $i$ in component DAGs. Also note that $\\lvert\\mathrm{an}_{\\ell}(i)\\cap$ $\\mathrm{ch}_{\\ell}(j)\\cap\\Delta|\\leq1$ since each $g_{\\ell}$ contains at most one causal path from $j$ to $i$ . Then, $i$ and $j$ are inseparable in $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ if and only if $j\\in\\mathrm{pa}_{\\mathrm{m}}(i)$ , since $j\\in\\mathcal{T}$ cuts off all $i\\stackrel{\\ell}{\\to}j$ edges. Therefore, we can determine whether $j\\in\\mathrm{pa}_{\\mathrm{m}}(i)$ from $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ with an intervention $\\mathcal{T}$ where ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\left|\\mathbb{Z}\\right|=1+\\sum_{\\ell\\in[K]}{\\mathbb{1}}\\left(\\left\\{\\operatorname{an}_{\\ell}(i)\\cap\\operatorname{ch}_{\\ell}(j)\\cap\\Delta\\right\\}\\neq\\varnothing\\right)\\leq K+1\\;.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof of necessity. For the worst-case necessity, consider component trees $\\{\\mathcal{G}_{\\ell}:\\ell\\in[K]\\}$ such that each graph contains a $\\Delta$ -child-through path from $j$ to $i$ in which the children of $j$ in each graph is distinct. Also, let $k\\in\\mathrm{pa}_{1}(j)$ but $k\\ \\bar{\\notin\\mathrm{\\pa}_{\\ell}}(j)$ for all $\\ell\\in\\{2,\\ldots,K\\}$ . This construction yields that ", "page_idx": 15}, {"type": "equation", "text": "$$\n{\\mathcal{I}}\\triangleq\\{i,j\\}\\cup\\bigcup_{\\ell\\in[K]}\\{{\\mathrm{an}}_{\\ell}(i)\\cap{\\mathrm{ch}}_{\\ell}(j)\\cap\\Delta\\}\\subseteq\\Delta\\ .\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Consider the paths ", "page_idx": 15}, {"type": "equation", "text": "$$\nj\\leftarrow y\\rightarrow i\\;,\\quad{\\mathrm{and}}\\quad\\{j\\rightarrow k\\stackrel{\\ell}{\\sim}i:k\\in{\\mathcal{I}}\\backslash\\left\\{i\\right\\}\\}\\;.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "For any intervention ${\\mathcal{T}}\\subseteq\\mathbf{V}\\setminus\\{i\\}$ that does not contain all nodes in $\\mathcal{I}\\backslash\\{i\\}$ , at least one of these paths will be active in the $\\mathcal{T}$ -mixture DAG $\\mathcal{G}_{\\mathrm{m},\\mathcal{Z}}$ , regardless of whether one of the $\\Delta$ -child-through paths is $j\\xrightarrow{\\ell}i$ itself. Note that if $j\\not\\in\\mathrm{pa}_{\\mathrm{m}}(i)$ , then this specific construction yields that $|{\\mathcal{I}}\\setminus\\{i\\}|=K+1$ Therefore, at the worst-case, whether $j\\in\\mathrm{pa}_{\\mathrm{m}}(i)$ cannot be determined from $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ for any intervention $\\mathcal{T}$ with size $|\\mathcal{T}|\\leq K$ . ", "page_idx": 15}, {"type": "text", "text": "C Proofs for Section 4 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 Proof of Lemma 3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Consider intervention $\\mathcal{T}=\\{i\\}$ and the corresponding interventional mixture distribution $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ and $\\mathcal{T}$ - mixture DAG $\\mathcal{G}_{\\mathrm{m},\\mathcal{Z}}$ . We will show that $\\{j:X_{j}$ \u0338\u22a5\u22a5 $X_{i}$ in $p_{\\mathrm{m},\\mathcal{I}}\\}$ is equal to the mixture descendants $\\mathrm{de}_{\\mathrm{m}}(i)$ . First, let $i\\in\\mathrm{an}_{\\mathrm{m}}(j)$ , i.e., there exists a path $i\\stackrel{\\ell}{\\sim}j$ for some $\\ell\\in[K]$ . Then, by $\\mathcal{T}$ -mixture faithfulness, $X_{j}$ \u0338\u22a5\u22a5 $X_{i}$ in $p_{\\underline{{\\mathrm{m}}},\\mathcal{T}}$ . For the other direction, let $X_{j}$ \u0338\u22a5\u22a5 $X_{i}$ in $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ . Then, there must be an active path between $\\bar{j}$ and $\\bar{i}$ in $\\mathcal{T}$ -mixture DAG $\\mathcal{G}_{\\mathrm{m},\\mathcal{Z}}$ . Since $i$ is intervened and there is no conditioning set, the path cannot contain any collider, which implies that the path is in the form $i\\stackrel{\\ell}{\\sim}j$ for some $\\ell\\in[K]$ . Therefore, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\ni\\in\\mathrm{an}_{\\mathrm{m}}(j)\\quad\\iff\\quad X_{j}\\ \\rlap/\\!\\!\\perp X_{i}\\ \\mathrm{in}\\ \\,p_{\\mathrm{m},\\{i\\}}\\ ,\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "which concludes the proof. ", "page_idx": 15}, {"type": "text", "text": "The choice of single-node interventions in Step 1. We note that the choice of single-node interventions for learning the mixture ancestors $\\{\\operatorname{an}_{\\operatorname{m}}(i):i\\in\\mathbf{V}\\}$ is deliberate for simplicity. It is possible to achieve the same guarantee using fewer than $n$ interventions by designing multi-node interventions, e.g., using separating systems with restricted intervention sizes [16]. Nevertheless, using $n$ intervention does not compromise our main results since as we show in the sequel, the number of total interventions in the algorithm will be dominated by the number of interventions in the subsequent steps. ", "page_idx": 15}, {"type": "text", "text": "C.2 Proof of Theorem 3 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Lemma 3 ensures that Step 1 of Algorithm 1 identifies $\\mathrm{an}_{\\mathrm{m}}(i)$ and $\\mathrm{de}_{\\mathrm{m}}(i)$ correctly for all $i\\in[n]$ . Hence, in this proof we use $\\mathrm{an}_{\\mathrm{m}}(i)$ and $\\mathrm{de}_{\\mathrm{m}}(i)$ for $\\hat{\\bf a n}(i)$ and $\\hat{\\mathrm{de}}(i)$ , respectively. We consider the ", "page_idx": 15}, {"type": "text", "text": "case where there are no cycles among the mixture ancestors of node $i$ induced by the mixture ancestral relationships, i.e., the following set of cycles is empty. ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{C}(i)\\gets\\{\\pi=(\\pi_{1},\\ldots,\\pi_{\\ell})\\,:\\,\\pi_{1}=\\pi_{\\ell}\\,,\\forall u\\in[\\ell-1]\\,\\,\\,\\pi_{u}\\in\\mathrm{an}_{\\mathrm{m}}(i)\\,\\,\\wedge\\,\\,\\pi_{u}\\in\\mathrm{an}_{\\mathrm{m}}(\\pi_{u+1})\\}\\;.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "In this case, Step 2 of Algorithm 1 only creates the sets ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathrm{de}_{i}(j)\\triangleq\\hat{\\mathrm{de}}(j)\\cap\\{\\hat{\\mathrm{an}}(i)\\cup i\\}=\\mathrm{de}_{\\mathrm{m}}(j)\\cap\\{\\mathrm{an}_{\\mathrm{m}}(i)\\cup i\\}\\:,\\quad\\forall i\\in\\mathrm{an}_{\\mathrm{m}}(i)\\:,\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "and $A=\\operatorname{an}_{\\operatorname{m}}(i)$ . The lack of cycles implies that the nodes in $\\boldsymbol{\\mathcal{A}}$ can be topologically ordered, i.e., there exists an ordering $(\\mathcal{A}_{1},\\ldots,\\mathcal{A}_{|\\mathrm{an}_{\\mathrm{m}}(i)|})$ such that $\\mathcal{A}_{j}\\in\\mathrm{de}_{\\mathrm{m}}(\\mathcal{A}_{k})$ implies $j<k$ . In Step 3, we leverage this key property for constructing hierarchically ordered topological layers. ", "page_idx": 16}, {"type": "text", "text": "Next, recall the definition of $S_{1}(i)$ in (13), ", "page_idx": 16}, {"type": "equation", "text": "$$\nS_{1}(i)=\\{j\\in\\mathrm{an}_{\\mathrm{m}}(i):\\mathrm{de}_{\\mathrm{m}}(j)\\cap\\mathrm{an}_{\\mathrm{m}}(i)=\\emptyset\\}\\ .\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Then, since there are no cycles among the nodes in $\\mathrm{an}_{\\mathrm{m}}(i),S_{1}(i)$ is not empty, e.g., the first node of the topological order described above has no mixture descendant within $\\mathrm{an}_{\\mathrm{m}}(i)$ . Consider $j\\in S_{1}(i)$ . Since $\\mathrm{an}_{\\mathrm{m}}\\overline{{(\\it i)}}\\cap\\mathrm{de}_{\\mathrm{m}}(\\it j)=\\emptyset$ , $j$ must be a mixture parent of $i$ . Therefore, $S_{1}(\\dot{\\iota})\\subseteq\\mathrm{pa}_{\\mathrm{m}}(i)$ . We will use induction to prove that topological layering in Step 3 and the sequential interventions in Step 4 ensure identifying $\\mathrm{pa}_{\\mathrm{m}}(i)$ . ", "page_idx": 16}, {"type": "text", "text": "Base case. Consider $S_{2}(i)$ defined as ", "page_idx": 16}, {"type": "equation", "text": "$$\nS_{2}(i)=\\{j\\in\\mathrm{an}_{\\mathrm{m}}(i)\\setminus S_{1}(i):\\mathrm{de}_{\\mathrm{m}}(j)\\cap\\{\\mathrm{an}_{\\mathrm{m}}(i)\\setminus S_{1}(i)\\}=\\emptyset\\}\\;.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that we have $\\mathrm{\\hat{pa}}(i)=S_{1}(i)$ . Consider a node $j\\in S_{2}(i)$ and intervene on $\\mathcal{T}=S_{1}(i)\\cup\\{j\\}$ . If $j\\in\\mathrm{pa}_{\\mathrm{m}}(i)$ , then by $\\mathcal{T}$ -mixture faithfulness, $X_{j}\\ {\\underline{{\\mathbb{M}}}}\\ X_{i}$ in $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ . On the other direction, suppose that $X_{j}\\ {\\underline{{\\!\\!\\lambda\\!\\!\\!\\perp}}}\\ X_{i}$ in $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ . If $j\\not\\in\\mathrm{pa}_{\\mathrm{m}}(i)$ , then there exists an active path between $\\bar{j}$ and $\\bar{i}$ in $\\mathcal{G}_{\\mathrm{m},\\mathcal{Z}}$ . Since the conditioning set is empty, there cannot be any colliders on the path. Then, since $j\\not\\in\\mathrm{pa}_{\\mathrm{m}}(i)$ , the path has the form $j\\stackrel{\\ell}{\\sim}k\\stackrel{\\ell}{\\rightarrow}i$ for some $k\\,\\in\\,\\mathrm{an}_{\\mathrm{m}}(i)$ and $\\ell\\,\\in\\,[K]$ . We know that $k\\,\\in\\,S_{1}(i)$ since intervention $\\mathcal{T}$ contains $S_{1}(i)$ . Then, $k\\;\\in\\;\\mathrm{de}_{\\mathrm{m}}(j)\\cap\\{\\mathrm{an}_{\\mathrm{m}}(\\bar{i})\\;\\backslash\\;S_{1}(i)\\}$ , which contradicts with $j~\\in~S_{2}(i)$ by definition of $\\dot{S_{2}}(i)$ . Therefore, $X_{j}$ \u0338\u22a5\u22a5 $X_{i}$ in $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ implies that $j\\,\\in\\,\\mathrm{pa}_{\\mathrm{m}}(i)$ . Subsequently, for $j\\in S_{2}(i)$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\nj\\in\\mathrm{pa}_{\\mathrm{m}}(i)\\quad\\iff\\quad X_{j}\\ \\underbrace{\\mathcal{N}}_{\\hbar}\\ X_{i}\\ \\operatorname{in}p_{\\mathrm{m},{\\mathbb{Z}}}\\ .\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that this step uses $|S_{2}(i)|$ interventions, one for each $j\\in S_{2}(i)$ , and each intervention has size $|\\mathcal{T}|=|S_{1}(i)|+1$ . ", "page_idx": 16}, {"type": "text", "text": "Induction hypothesis. Assume that we have identified the set $S_{u}(i)\\cap\\mathrm{pa}_{\\mathrm{m}}(i)$ correctly for $u\\in$ $\\{1,\\ldots,v-1\\}$ , i.e., we have $\\hat{\\mathrm{{pa}}}(i)\\,=\\bigcup_{k=1}^{v-1}S_{k}(i)\\cap\\mathrm{{pa}}_{\\mathrm{m}}(i)$ . We will show that the algorithm also identifies $S_{v}(i)\\cap\\mathrm{pa}_{\\mathrm{m}}(i)$ correctly. ", "page_idx": 16}, {"type": "text", "text": "Let $A=\\operatorname{an}_{\\operatorname{m}}(i)\\setminus\\bigcup_{k=1}^{v-1}S_{k}(i)$ and consider $S_{v}(i)$ defined as ", "page_idx": 16}, {"type": "equation", "text": "$$\nS_{v}(i)=\\{j\\in\\mathsf{\\boldsymbol{A}}:\\mathrm{de}_{\\mathrm{m}}(j)\\cap\\mathsf{\\boldsymbol{A}}=\\emptyset\\}\\;.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Consider a node $j\\in S_{v}(i)$ and intervene on ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathcal{T}=\\{j\\}\\cup\\hat{\\mathrm{pa}}(i)=\\{j\\}\\cup\\bigcup_{k=1}^{v-1}S_{k}(i)\\cap\\mathrm{pa}_{\\mathrm{m}}(i)\\;.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "If $j\\in\\mathrm{pa}_{\\mathrm{m}}(i)$ , then by $\\mathcal{T}$ -mixture faithfulness, $X_{j}$ \u0338\u22a5\u22a5 $X_{i}$ in $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ . For the other direction, suppose that $X_{j}$ \u0338\u22a5\u22a5 $X_{i}$ in $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ . If $j\\not\\in\\mathrm{pa}_{\\mathrm{m}}(i)$ , then there exists an active path between $\\bar{j}$ and $\\bar{i}$ in $\\mathcal{T}$ -mixture DAG $\\mathcal{G}_{\\mathrm{m},\\mathcal{Z}}$ . Since the conditioning set is empty, this path has the form $j\\;\\stackrel{\\ell}{\\sim}\\;k\\;\\stackrel{\\ell}{\\rightarrow}\\;i$ for some $k\\in\\deg(j)\\cap A$ , which contradicts with $j\\,\\in\\,S_{v}(i)$ by the definition of $S_{v}(i)$ . Subsequently, for $j\\in S_{v}(i)$ , we have ", "page_idx": 16}, {"type": "equation", "text": "$$\nj\\in\\mathrm{{pa}}_{\\mathrm{m}}(i)\\quad\\iff X_{j}\\not\\perp X_{i}\\:\\:\\mathrm{in}\\:p_{\\mathrm{m},\\mathbb{Z}}\\:.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Therefore, by induction, the algorithm identifies $S_{u}(i)\\cap\\mathrm{pa}_{\\mathrm{m}}(i)$ correctly for all $u\\in\\{1,\\ldots,t\\}$ . ", "page_idx": 16}, {"type": "text", "text": "Finally, note that while processing each layer $S_{u}(i)$ , the algorithm uses $|S_{u}(i)|$ interventions, one for each $j\\in S_{u}(i)$ , with size $\\left|\\mathcal{Z}\\right|=\\left|\\operatorname{pa}_{\\mathrm{m}}(i)\\cap\\bigcup_{k=1}^{u-1}S_{k}(i)\\right|+1$ . This is upper bounded by $|\\mathrm{pa}_{\\mathrm{m}}(i)|+1$ , which is shown to be the worst-case necessary intervention size in Theorem 1. Then, including $n$ single-node interventions performed in Step 1, for identifying $\\mathrm{pa}_{\\mathrm{m}}(i)$ for all $i\\in\\mathbf{V}$ , Algorithm 1 uses a total of ", "page_idx": 17}, {"type": "equation", "text": "$$\nn+\\sum_{i=1}^{n}\\sum_{u=1}^{t}|S_{u}(i)|=n+\\sum_{i=1}^{n}|\\mathrm{an}_{\\mathrm{m}}(i)|=\\mathcal{O}(n^{2})\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "interventions, which completes the proof of the theorem. ", "page_idx": 17}, {"type": "text", "text": "C.3 Proof of Theorem 4 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We start by giving a synopsis of the proof. Lemma 3 ensures that Step 1 of Algorithm 1 identifies $\\mathrm{an}_{\\mathrm{m}}(i)$ and $\\mathrm{de}_{\\mathrm{m}}(i)$ correctly for all $i\\in\\mathbf{V}$ . Hence, in this proof we use $\\mathrm{an}_{\\mathrm{m}}(i)$ and $\\mathrm{de}_{\\mathrm{m}}(i)$ for $\\hat{\\bf a n}(i)$ and $\\hat{\\mathrm{de}_{\\mathrm{m}}}(i)$ , respectively. In this theorem, we consider the most general case in which the nodes in mixture ancestors $\\mathrm{an}_{\\mathrm{m}}(i)$ can form cycles via their mixture ancestral relationships. These cycles will be accommodated by the procedure in Step 2. Intuitively, by intervening on a small number of nodes, we can break all the cycles in $\\mathcal{C}(i)$ in the new interventional mixture graphs. Then, we would be able to follow Steps 3 and 4 similarly to the proof of Theorem 3, albeit using interventions with larger sizes. ", "page_idx": 17}, {"type": "text", "text": "Step 2. First, we recall the definition of cycles among mixture ancestors of $i$ , ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{C}(i)\\gets\\{\\pi=(\\pi_{1},\\ldots,\\pi_{\\ell})\\,:\\,\\pi_{1}=\\pi_{\\ell}\\,,\\forall u\\in[\\ell-1]\\,\\,\\,\\pi_{u}\\in\\mathrm{\\normalfont\\sf{a}n}(i)\\,\\,\\wedge\\,\\,\\pi_{u}\\in\\mathrm{\\normalfont\\sf{a}n}(\\pi_{u+1})\\}\\;,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and the associated breaking set, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{B}(i)\\triangleq\\mathrm{a\\,minimal\\;set\\;s.t.}\\;\\;\\forall\\pi\\in\\mathcal{C}(i),\\;\\;|\\mathcal{B}(i)\\cap\\pi|\\geq1\\;.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "We denote the size of the breaking set by $\\tau_{i}\\triangleq|\\boldsymbol{\\beta}(i)|$ and refer to it as the cyclic complexity of node $i$ . The intervention $\\mathcal{T}=\\mathcal{B}(i)$ breaks all cycles in $\\mathcal{C}(i)$ . To see this consider a cycle $\\pi=(\\pi_{1},\\ldots,\\pi_{\\ell})$ in $\\mathcal{C}(i)$ and suppose that $p i_{u}\\in B(i)$ . Then, intervening on $\\pi_{u}$ breaks all causal paths from $\\pi_{u-1}$ to $\\pi_{u}$ , which breaks the cycle. In Step 2, we leverage this property to obtain cycle-free descendants of each node $j\\in\\mathrm{an}_{\\mathrm{m}}(i)$ . Specifically, for each each $j\\in\\mathrm{an}_{\\mathrm{m}}(i)$ , we intervene on $\\mathcal{T}=B(i)\\cup\\{j\\}$ and set ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathrm{de}_{i}(j)=\\left\\{k\\in\\mathrm{an}_{\\mathrm{m}}(i)\\cup\\{i\\}:X_{j}\\ \\rlap/\\!\\!\\perp X_{k}\\ \\mathrm{~in}\\ p_{\\mathrm{m},\\mathbb Z}\\right\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Note that if $j\\in\\mathrm{pa}_{\\mathrm{m}}(i)$ , then $i\\in\\mathrm{de}_{i}(j)$ . Hence, after constructing these cycle-free descendant sets, we refine the ancestor set ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{A}=\\left\\{j\\in\\mathrm{an}_{\\mathrm{m}}(i):i\\in\\mathrm{de}_{i}(j)\\right\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which contains all $\\mathrm{pa}_{\\mathrm{m}}(i)$ . We will use induction to prove that topological layering in Step 3 and the sequential interventions on Step 4 ensure to identify $\\mathrm{pa}_{\\mathrm{m}}(i)$ from $\\boldsymbol{\\mathcal{A}}$ . ", "page_idx": 17}, {"type": "text", "text": "Base case. Consider $S_{1}(i)$ defined as ", "page_idx": 17}, {"type": "equation", "text": "$$\nS_{1}(i)=\\{j\\in\\ensuremath{\\mathcal{A}}:\\ensuremath{\\mathrm{de}}_{i}(j)\\cap\\ensuremath{\\mathcal{A}}=\\emptyset\\}\\;.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "First, we show that $S_{1}(i)$ is not empty. Otherwise, starting from a node $\\pi_{1}\\in A$ , we would have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\mathrm{de}_{i}(\\pi_{1})\\cap A\\neq\\emptyset\\;}&{\\implies}&{\\exists\\;\\pi_{2}\\in\\mathrm{de}_{i}(\\pi_{1})\\cap A}\\\\ {\\mathrm{de}_{i}(\\pi_{2})\\cap A\\neq\\emptyset\\;}&{\\implies}&{\\exists\\;\\pi_{3}\\in\\mathrm{de}_{i}(\\pi_{2})\\cap A}\\\\ &{\\colon}&\\\\ &{\\vdots\\;}&\\\\ {\\mathrm{de}_{i}(\\pi_{\\ell})\\cap A\\neq\\emptyset\\;}&{\\implies}&{\\exists\\;\\pi_{1}\\in\\mathrm{de}_{i}(\\pi_{\\ell})\\cap A}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "since $\\boldsymbol{\\mathcal{A}}$ has finite elements. However, this implies that none of the $\\{\\pi_{1},\\ldots,\\pi_{\\ell}\\}$ are contained in $B(i)$ due to the construction of $\\mathrm{de}_{i}(j)$ sets with interventions $\\mathcal{T}=B(i)\\cup\\{j\\}$ . This contradicts with the definition of the breaking set $B(i)$ as it does not contain any node from the cycle $\\{\\pi_{1},\\ldots,\\pi_{\\ell}\\}$ . ", "page_idx": 17}, {"type": "text", "text": "Next, consider a node $j\\in S_{1}(i)$ and intervene on $\\mathcal{T}=B(i)\\cup\\{j\\}$ . We will show that ", "page_idx": 18}, {"type": "equation", "text": "$$\nj\\in\\mathrm{pa}_{\\mathrm{m}}(i)\\quad\\iff\\quad X_{j}\\ \\mathbb{\\1}\\ X_{i}\\ \\mathrm{in}\\ p_{\\mathrm{m},\\mathbb{Z}}\\;.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "If $j\\in\\mathrm{pa}_{\\mathrm{m}}(i)$ , then by $\\mathcal{T}$ -mixture faithfulness, $X_{j}$ \u0338\u22a5\u22a5 $X_{i}$ in $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ . We prove the other direction, that is $X_{j}\\ {\\underline{{\\mathbb{M}}}}\\ X_{i}$ in $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ implies that $j\\in\\mathrm{pa}_{\\mathrm{m}}(i)$ as follows. First, note that $X_{j}$ \u0338\u22a5\u22a5 $X_{i}$ in $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ does not have a conditioning set. Then, it implies that there exists an active path $j\\stackrel{\\ell}{\\sim}i$ in $\\boldsymbol{\\mathcal{G}}_{\\ell,\\mathcal{T}}$ for some $\\ell\\in[K]$ . Suppose that $j\\not\\in\\mathrm{pa}_{\\mathrm{m}}(i)$ , which implies that $j\\stackrel{\\ell}{\\to}k\\stackrel{\\ell}{\\sim}i$ in $\\boldsymbol{\\mathcal{G}}_{\\ell,\\mathcal{T}}$ , and $k\\notin\\mathcal{B}(i)$ for path being active. However, in this case we have $k\\in\\deg_{i}(j)\\cap A$ , which contradicts with $j\\in\\dot{S}_{1}(i)$ due to definition of $S_{1}(i)$ . Hence, for $j\\in S_{1}(i),X_{j}$ \u0338\u22a5\u22a5 $X_{i}$ in $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ implies that $j\\in\\mathrm{pa}_{\\mathrm{m}}(i)$ , which concludes the proof of the base case. ", "page_idx": 18}, {"type": "text", "text": "Induction step. Assume that we have identified the set $S_{u}(i)\\cap\\mathrm{pa}_{\\mathrm{m}}(i)$ correctly for $u\\in$ $\\{1,\\ldots,v-1\\}$ . Let $\\boldsymbol{\\mathcal{A}}=\\mathrm{an}_{\\mathrm{m}}(i)\\setminus\\bigcup_{k=1}^{v-1}S_{k}(i)$ and consider $S_{v}(i)$ defined as ", "page_idx": 18}, {"type": "equation", "text": "$$\nS_{v}(i)=\\left\\{j\\in{\\cal A}:\\mathrm{de}_{i}(j)\\cap{\\cal A}=\\emptyset\\right\\}\\,.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that, after processing $\\{S_{1},\\dotsc,S_{v-1}\\}$ correctly, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\hat{\\mathrm{\\pa}}(i)=\\mathrm{\\pa}_{\\mathrm{m}}(i)\\cap\\bigcup_{k=1}^{v-1}S_{k}(i)\\ .\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Consider a node $j\\in S_{v}(i)$ and intervene on ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathcal{Z}=\\{j\\}\\cup\\hat{\\mathrm{pa}}(i)\\cup\\mathcal{B}(i)=\\{j\\}\\cup\\bigcup_{k=1}^{v-1}S_{k}(i)\\cap\\mathrm{pa}_{\\mathrm{m}}(i)\\;.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We will show that ", "page_idx": 18}, {"type": "equation", "text": "$$\nj\\in\\mathrm{pa}_{\\mathrm{m}}(i)\\quad\\iff\\quad X_{j}\\ \\mathbb{\\1}\\ X_{i}\\ \\mathrm{in}\\ p_{\\mathrm{m},\\mathbb{Z}}\\;.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "If $j\\in\\mathrm{pa}_{\\mathrm{m}}(i)$ , then by $\\mathcal{T}$ -mixture faithfulness, $X_{j}$ \u0338\u22a5\u22a5 $X_{i}$ in $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ . We will prove the other direction, that is $X_{j}$ \u0338\u22a5\u22a5 $X_{i}$ in $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ implies $j\\in\\mathrm{pa}_{\\mathrm{m}}(i)$ , similarly to the base case. First, note that $X_{j}$ \u0338\u22a5\u22a5 $X_{i}$ in $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ does not have a conditioning set. Then, it implies that there exists an active path $j\\stackrel{\\ell}{\\sim}i$ in $\\mathscr{G}_{\\ell,\\mathcal{T}}$ for some $\\ell\\in[K]$ . Now, suppose that $j\\not\\in\\mathrm{pa}_{\\mathrm{m}}^{\\overline{{\\mathbf{\\Lambda}}}}(i)$ , which implies that the active path has the form $j\\stackrel{\\ell}{\\sim}k\\stackrel{\\ell}{\\rightarrow}i$ in $\\mathcal{G}_{\\ell,\\mathcal{Z}}$ for some $\\ell\\in[K]$ and $k\\not\\in\\mathcal{T}$ . Since $k\\in\\mathrm{\\mathrm{pa}}_{\\mathrm{m}}(i),\\,k\\not\\in\\mathbb{Z}$ implies that $v\\!-\\!1$   \n$k\\not\\in\\bigcup_{u=1}S_{u}(i)$ . Then, we have $k\\in\\deg_{i}(j)\\cap A$ , which contradicts with $k\\in S_{v}(i)$ due to definition of $S_{v}(i)$ . Therefore, for $j\\,\\in\\,S_{v}(i)$ and ${\\mathcal{T}}={\\mathcal{B}}(i)\\cup{\\hat{\\mathrm{pa}}}(i)\\cup\\{j\\},\\,.$ $X_{j}$ \u0338\u22a5\u22a5 $X_{i}$ in $p_{\\mathrm{m},\\ensuremath{\\mathbb{Z}}}$ implies that $j\\in\\mathrm{pa}_{\\mathrm{m}}(i)$ , which concludes the proof of the induction step. Therefore, by induction, the algorithm identifies $S_{u}\\cap\\mathrm{pa}_{\\mathrm{m}}(i)$ correctly for all $u\\in\\{1,\\ldots,t\\}$ .   \nFinally, note that while processing each layer $S_{u}(i)$ , the algorithm uses $|S_{u}(i)|$ interventions, one for ", "page_idx": 18}, {"type": "text", "text": "each $j\\in S_{u}(i)$ , with size $\\left|\\mathcal{Z}\\right|=\\left|\\mathrm{pa}_{\\mathrm{m}}(i)\\cap\\bigcup_{k=1}^{u-1}S_{k}(i)\\right|+\\mathcal{B}(i)+1$ , where $\\tau_{i}=|\\boldsymbol{B}(i)|$ is referred to as the cyclic complexity of node $i$ . Therefore, the size of the largest intervention set is ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\left|\\mathrm{pa}_{\\mathrm{m}}(i)\\right|+\\tau_{i}+1\\;.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We note that this upper bound on the intervention size is $\\tau_{i}$ larger than the necessary size $\\vert\\mathrm{pa}_{\\mathrm{m}}(i)\\vert+1$ shown in Theorem 1. This optimality gap reflects the effect of the cyclic complexity of the problem. Finally, adding $n$ single-node interventions performed in Step 1, for identifying $\\mathrm{pa}_{\\mathrm{m}}(i)$ for all $i\\in\\mathbf{V}$ , Algorithm 1 uses a total of ", "page_idx": 18}, {"type": "equation", "text": "$$\nn+\\sum_{i=1}^{n}|\\mathrm{an}_{\\mathrm{m}}(i)|+\\sum_{i=1}^{n}\\sum_{u=1}^{t}|S_{u}(i)|=n+2\\sum_{i=1}^{n}|\\mathrm{an}_{\\mathrm{m}}(i)|\\leq2n^{2}-n=\\mathcal{O}(n^{2})\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "interventions, which completes the proof of the theorem. ", "page_idx": 18}, {"type": "text", "text": "D Additional examples ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "D.1 Partitioning true edges into component DAGs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We have emphasized in Section 1 that the component DAGs of the mixture cannot be identified without further assumptions even under our interventional setting. We discuss a few examples of this matter. ", "page_idx": 19}, {"type": "text", "text": "1. Consider the following two mixtures of $K=2$ DAGs \u2022 Mixture 1: Edge sets $\\mathbf{E}_{1}=\\{1\\rightarrow2,1\\rightarrow3\\}$ and $\\mathbf{E}_{2}=\\varnothing$ \u2022 Mixture 2: Edge sets $\\mathbf{E}_{1}=\\{1\\rightarrow2\\}$ and $\\mathbf{E}_{2}=\\{1\\rightarrow3\\}$ ", "page_idx": 19}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "In this case, distributions of the two mixtures can still be the same under all intervention sets ${\\mathcal{T}}\\subseteq\\{1,2,3\\}$ . Hence, without additional assumptions (e.g., model parameterization), we cannot distinguish the two mixtures via only conditional independence tests. Instead, we can only learn the set of true edges, $\\mathbf{E}_{\\mathrm{t}}=\\{1\\rightarrow2,1\\rightarrow3\\}$ for both mixtures. ", "page_idx": 19}, {"type": "text", "text": "2. Consider a mixture of two DAGs with edge sets $\\mathbf{E}_{1}=\\{1\\rightarrow2,2\\rightarrow3\\}$ and $\\mathbf{E}_{2}=\\{3\\rightarrow1\\}$ . Recall that in Stage 1 of Algorithm 1, we learn the mixture ancestors of the nodes as an intermediate step. Hence, after learning the set of true edges in the mixture via the rest of the algorithm, we have the following information: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{1\\in\\mathrm{an}_{\\mathrm{m}}(3)\\;,\\;\\;1\\notin\\mathrm{pa}_{\\mathrm{m}}(3),\\;\\;1\\in\\mathrm{pa}_{\\mathrm{m}}(2)\\;,}\\\\ &{2\\in\\mathrm{pa}_{\\mathrm{m}}(3)\\;,\\;\\;2\\notin\\mathrm{an}_{\\mathrm{m}}(1)\\;,}\\\\ &{3\\in\\mathrm{pa}_{\\mathrm{m}}(1)\\;,\\;\\;3\\notin\\mathrm{an}_{\\mathrm{m}}(2)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Suppose that we know $K=2$ . Then, we can see that the only possible component DAGs that result in this mixture are $\\mathbf{E}_{1}=\\{1\\rightarrow2,2\\rightarrow3\\}$ and $\\mathbf{E}_{2}=\\{3\\rightarrow1\\}$ . This is because $3\\rightarrow1$ cannot be in the same DAG as the other two edges due to the known ancestral relationships. Hence, we learn the component DAGs in this case. However, without learning the true edges, we would not be able to know that we can learn the component DAGs of the mixture model. ", "page_idx": 19}, {"type": "text", "text": "These examples show that our work is a necessary first step into the interventional causal discovery of mixtures. Furthermore, we hope that it can inspire future work for the use of interventions in a mixture of models, e.g., establishing graphical conditions for (partial) recovery of individual DAGs, leveraging the side information. ", "page_idx": 19}, {"type": "text", "text": "Finally, we note two things regarding the possible side information that can enable stronger results. First, mixture distributions (the underlying component distributions $p_{\\ell}(x)\\mathbf{s})$ can be identified under some assumptions, e.g., Gaussian mixture models [47]. Second, in a related line of work, disentangling mixtures of unknown interventional datasets is studied under specific conditions on the intervention sets and given the distribution of the observational DAG [48]. Establishing the necessary and sufficient conditions for achieving similar disentangling objectives tasks in our mixture model is an open problem for future work. ", "page_idx": 19}, {"type": "text", "text": "D.2 An example of cyclic complexity ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We have empirically quantified the average cyclic complexity in Section 5. In addition, we give a visual example here. Consider the mixture of two DAGs in Figure 3. By definition of mixture ancestors, we have ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{an}_{\\mathrm{m}}(1)=\\{2,5\\}\\ ,}\\\\ &{\\mathrm{an}_{\\mathrm{m}}(2)=\\{3,5\\}\\ ,}\\\\ &{\\mathrm{an}_{\\mathrm{m}}(3)=\\emptyset\\ ,}\\\\ &{\\mathrm{an}_{\\mathrm{m}}(4)=\\{1,2,3,5\\},}\\\\ &{\\mathrm{an}_{\\mathrm{m}}(5)=\\{1,2\\}\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "image", "img_path": "mFrlCI8sov/tmp/efefd46d1e7c61d9d0ad2506ac455434cc3f394363d9204e1d65c29a9cefc97e.jpg", "img_caption": ["Figure 3: Sample DAGs for a mixture of two DAGs "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Then, by definition of $\\mathcal{C}(i)$ in (10), we have ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{C}(1)=\\{(2,5,2)\\}\\;,}\\\\ &{\\mathcal{C}(2)=\\emptyset\\;,}\\\\ &{\\mathcal{C}(3)=\\emptyset\\;,}\\\\ &{\\mathcal{C}(4)=\\{(2,5,2),(2,1,5,2),(1,5,1)\\},}\\\\ &{\\mathcal{C}(5)=\\emptyset\\;.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Subsequently, an example of minimal breaking sets and cycle complexities are given by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\mathcal{B}(1)=\\{2\\}\\implies\\quad\\tau_{1}=1}}\\\\ {{\\mathcal{B}(2)=\\emptyset\\implies\\quad\\tau_{2}=0}}\\\\ {{\\mathcal{B}(3)=\\emptyset\\implies\\quad\\tau_{3}=0}}\\\\ {{\\mathcal{B}(4)=\\{5\\}\\implies\\quad\\tau_{4}=1}}\\\\ {{\\mathcal{B}(5)=\\emptyset\\implies\\quad\\tau_{5}=0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "This example illustrates that even though the mixture model can contain many cycles, the cyclic complexity of the nodes can be small. ", "page_idx": 20}, {"type": "text", "text": "E Additional experiments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Effect of the number of samples. For the same experimental setting in Section 5, we investigate the effect of the number of samples on the performance of Algorithm 1. Figure 4a demonstrates that the algorithm achieves almost perfect precision even with as few as $s\\,=\\,1000$ samples under the parameterization described in Section 5. The recall rates are lower than the precision; however, when the number of samples is increased to $s=10000$ , the gap is closed, and the recall rates also become closer to perfect. ", "page_idx": 20}, {"type": "text", "text": "Varying the true edge weights. Recall that in Section 5, we have considered the case where the weight of a true edge is constant across the component DAGs it belongs to. However, our theory and algorithm can handle the general case, in which conditional distributions $p_{\\ell}(X_{i}\\mid X_{\\mathrm{pa}_{\\ell}(i)})$ and $p_{\\ell^{\\prime}}\\big(X_{i}\\mid X_{\\mathrm{pa}_{\\ell^{\\prime}}(i)}\\big)$ can be different even if the parent sets $\\mathrm{pa}_{\\ell}(i)=\\mathrm{pa}_{\\ell^{\\prime}}(i)$ for two component DAGs $g_{\\ell}$ and $\\mathcal{G}_{\\ell^{\\prime}}$ . For instance, when considering a mixture of DAGs where $(1\\rightarrow2)\\in\\mathbf{E}_{1}$ , $(1\\rightarrow2)\\notin\\mathbf{E}_{1}$ , and $\\left(1\\rightarrow2\\right)\\in\\mathbf{E}_{3}$ relations, the edge weight from node 1 to node 2 can be different in $\\mathcal{G}_{1}$ and $\\mathcal{G}_{3}$ . To investigate the performance of our algorithm in this general setting, we consider the following parameterization. When randomly generating the weight of a true edge $\\left(i\\rightarrow j\\right)\\in\\mathbf{E}_{\\mathrm{t}}$ , it has two options: (i) With probability 0.5, it is constant across the component DAGs it belongs to, (ii) with probability 0.5, it is different (randomly sampled) for every component DAG it belongs to. Figure 4b shows that the performance of the algorithm is virtually the same for this setting compared to the main setting considered in Section 5. ", "page_idx": 20}, {"type": "text", "text": "Skeleton (true edges) versus inseparable pairs. In Section 5, we demonstrate the need for interventions by learning inseparable pairs from observational mixture data and comparing them to the skeleton of the true edges. To learn the inseparable pairs specified in (5), we perform exhaustive conditional independent tests as in [11], summarized in Algorithm 2. Note that as mentioned in ", "page_idx": 20}, {"type": "image", "img_path": "mFrlCI8sov/tmp/b09a469229ebdff17aa143c10f2c3dcaea4b5c73f822af2ce2612ad64df1b53e.jpg", "img_caption": ["(a) Varying number of samples for a mixture of $K=3$ DAGs "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "mFrlCI8sov/tmp/1b952db60eee03b04ab24515caac4037f15117991374f8e1a6b747879a582254.jpg", "img_caption": ["(b) Recovery rates for varying true edge weights for a mixture of $K=3$ DAGs "], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 4: Additional experiment results for true edge recovery ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 21}, {"type": "image", "img_path": "mFrlCI8sov/tmp/65e5108f8c3c549f9d424be8f661fd6ce6d774d8d66b0a48c412eea5398732ca.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Section 4.1, this exhaustive search requires $O(n^{2}\\cdot2^{n})$ CI tests, which we perform only for this experiment setting and omit in our proposed Algorithm 1. ", "page_idx": 21}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: Sections 3 and 4 establish the results claimed in the abstract and introduction. Guidelines: ", "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 22}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Justification: The limitations of the work are clarified throughout the paper and discussed in Section 6. ", "page_idx": 22}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 22}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 22}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 22}, {"type": "text", "text": "Justification: The assumptions for the theoretical results are clearly stated in the theorem statements, and the proofs of the results are provided in Appendix B and C. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 23}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The algorithm details are summarized in Algorithm 1, and described in Section 4. The experimental procedure is described in Section 5, and detailed parameterization is given in Appendix E. The code for reproducing the main experimental results can be found at https://github.com/bvarici/intervention-mixture-DAG. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 23}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The code for reproducing the main experimental results can be found at https://github.com/bvarici/intervention-mixture-DAG. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 24}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The experimental procedure is described in Section 5, and detailed parameterization is provided in Appendix E. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 24}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 24}, {"type": "text", "text": "Justification: The plots in all figures are given for an average of 100 experiments. Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 24}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: Experiments are run on a single commercial CPU. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 25}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The authors have reviewed the NeurIPS Code of Ethics and confirm that the research in this paper conforms with the code of ethics. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 25}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 25}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 25}, {"type": "text", "text": "Justification: This paper is mostly theoretical and does not pose potential negative societal impacts. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that there is no societal impact of the work performed. ", "page_idx": 25}, {"type": "text", "text": "\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 26}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: This paper is mostly theoretical and does not pose such risks. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 26}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: Portions of the publicly available code of [11] is adopted in the code of our experiments. ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The codebase for the experiments can be found at https://github.com/ bvarici/intervention-mixture-DAG, and is released under Apache 2.0 license. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 27}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] Justification: [NA] ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 27}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: [NA] ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}]