[{"figure_path": "ypggxVWIv2/figures/figures_1_1.jpg", "caption": "Figure 1: The overall schematic of GTBENCH. There are three main components from right to left: Environments (c) for game hosting, observation providing, and action execution; Prompt Adapter (b) for converting observation to prompt and extracting actions from participants' generations; Participants (a) for reasoning and action generation.", "description": "This figure shows the overall architecture of GTBENCH, a game-theoretic evaluation environment for LLMs.  It highlights three main components: Participants (LLMs and other agents), Prompt Adapter (responsible for converting observations into prompts and actions), and Environments (the game environments themselves). The figure visually depicts the flow of information and actions between these components, illustrating the process of LLMs participating in and responding to game-theoretic tasks. The diagram also categorizes the games included in GTBENCH into complete/deterministic and incomplete/probabilistic categories, and provides example games for each category.", "section": "3 GTBENCH: Game-Theoretic Evaluation of LLMs"}, {"figure_path": "ypggxVWIv2/figures/figures_5_1.jpg", "caption": "Figure 2: The NRA of state-of-the-art LLM-driven reasoning agents when against MCTS Agents and Random Agents, over complete and deterministic scenarios. Red and gray lines mean the maximum NRA achieved by LLM agents.", "description": "This figure shows the Normalized Relative Advantage (NRA) of various LLMs against both random agents and Monte Carlo Tree Search (MCTS) agents across four deterministic games.  The LLMs employ different reasoning methods (Prompt, CoT, SC-CoT, ToT).  The results reveal that LLMs struggle significantly against MCTS agents, indicating limitations in strategic reasoning in deterministic scenarios. However, they outperform random agents, highlighting their basic strategic capabilities.", "section": "4 Are LLMs Capable of Strategic Reasoning?"}, {"figure_path": "ypggxVWIv2/figures/figures_5_2.jpg", "caption": "Figure 3: The game-wise NRA of LLMs when against MCTS/TfT Agents and Random Agents, over incomplete and probabilistic scenarios. Error bars are obtained over different reasoning methods. Green and gray lines mean the maximum NRA achieved by LLM agents.", "description": "This figure shows the Normalized Relative Advantage (NRA) of different LLMs across various probabilistic and incomplete information games.  It compares their performance against both MCTS (Monte Carlo Tree Search) and random agents, highlighting how different reasoning methods (Prompt, CoT, SC-COT) affect their strategic capabilities in these scenarios. The error bars represent the variability in performance across different reasoning methods. The green and grey lines indicate the maximum NRA achieved by the LLMs in each game.", "section": "4 Are LLMs Capable of Strategic Reasoning?"}, {"figure_path": "ypggxVWIv2/figures/figures_6_1.jpg", "caption": "Figure 2: The NRA of state-of-the-art LLM-driven reasoning agents when against MCTS Agents and Random Agents, over complete and deterministic scenarios. Red and gray lines mean the maximum NRA achieved by LLM agents.", "description": "The figure shows the Normalized Relative Advantage (NRA) of several large language models (LLMs) when playing against Monte Carlo Tree Search (MCTS) agents and random agents in complete and deterministic game scenarios.  The NRA is a metric that measures the relative advantage of one player over another.  Positive NRA indicates that the LLM outperforms the opponent, negative values indicate the opposite, and a value near zero means that both perform comparably.  The figure displays the results across different reasoning methods used by the LLMs, showing the effectiveness of each approach in these games.  The red and grey lines represent the maximum NRA achieved by each LLM, highlighting their best performances.", "section": "4 Are LLMs Capable of Strategic Reasoning?"}, {"figure_path": "ypggxVWIv2/figures/figures_8_1.jpg", "caption": "Figure 5: Game-theoretic properties. The results are obtained when competing against GPT-3.5-turbo w/ Prompt Agent as the opponent. In (b), each dot (x, y) represents an agreement in a resource distribution with Player 1 obtaining reward x and Player 2 obtaining reward y. In (c), the system reward is calculated by the sum of the payoffs of all players.", "description": "This figure shows the results of evaluating the game-theoretic properties of LLMs.  Panel (a) displays the regret values for different LLMs in Blind Auction and Iterated Prisoner's Dilemma.  Panel (b) illustrates the resource distribution agreements reached by the LLMs in a negotiation game, showing the rewards obtained by each player. Panel (c) demonstrates Pareto efficiency, comparing the system rewards achieved in both non-repeated and repeated gameplay scenarios. The overall goal is to understand the strategic behavior and decision-making of LLMs under different game-theoretic settings.", "section": "The Game-Theoretic Properties of LLMs"}, {"figure_path": "ypggxVWIv2/figures/figures_17_1.jpg", "caption": "Figure 2: The NRA of state-of-the-art LLM-driven reasoning agents when against MCTS Agents and Random Agents, over complete and deterministic scenarios. Red and gray lines mean the maximum NRA achieved by LLM agents.", "description": "The figure shows the Normalized Relative Advantage (NRA) of several LLMs in complete and deterministic game scenarios, such as Tic-Tac-Toe and Connect 4.  The LLMs are compared against both a Monte Carlo Tree Search (MCTS) agent (a strong baseline) and a random agent.  The results are presented for different reasoning methods (Prompt, CoT, SC-COT, and ToT) used with each LLM.  The red and gray lines highlight the best performance obtained by each LLM across all methods.", "section": "4 Are LLMs Capable of Strategic Reasoning?"}, {"figure_path": "ypggxVWIv2/figures/figures_23_1.jpg", "caption": "Figure A7: Investigating the sensitivity of Chain-of-Thought prompt. Prompt (used) and CoT (used) refer to the prompts utilized by the Prompt Agent and the CoT Agent in this paper. Results are obtained from the model GPT-3.5-turbo over all the game-theoretic tasks. Please refer to Table A9 for Template 0 to Template 4.", "description": "The bar chart compares the performance of different prompting methods on the GPT-3.5-turbo model across various game-theoretic tasks.  The \"Prompt\" method represents a simple prompt without any explicit reasoning instructions.  \"CoT (used)\" shows the performance of the Chain-of-Thought (CoT) prompting method used in the main paper.  \"Other CoT\" encompasses the results from four additional CoT templates (templates 1-4, detailed in Table A9). The y-axis represents the Normalized Relative Advantage (NRA), a measure of the model's performance against a conventional solver.  The chart reveals the relative effectiveness of these different prompting strategies in influencing the strategic reasoning capabilities of the model. ", "section": "A8 Chain-of-Thought Sensitivity"}, {"figure_path": "ypggxVWIv2/figures/figures_27_1.jpg", "caption": "Figure 1: The overall schematic of GTBENCH. There are three main components from right to left: Environments (c) for game hosting, observation providing, and action execution; Prompt Adapter (b) for converting observation to prompt and extracting actions from participants' generations; Participants (a) for reasoning and action generation.", "description": "This figure shows the overall architecture of GTBENCH, a language-driven environment for evaluating LLMs' strategic reasoning abilities. It highlights three main components: Participants (LLMs or other agents), Prompt Adapter (for converting observations to prompts and actions), and Environments (game environments). The flow of information and actions among these components is visually represented, demonstrating how GTBENCH facilitates game-theoretic evaluations of LLMs.", "section": "3 GTBENCH: Game-Theoretic Evaluation of LLMs"}, {"figure_path": "ypggxVWIv2/figures/figures_27_2.jpg", "caption": "Figure 2: The NRA of state-of-the-art LLM-driven reasoning agents when against MCTS Agents and Random Agents, over complete and deterministic scenarios. Red and gray lines mean the maximum NRA achieved by LLM agents.", "description": "This figure shows the normalized relative advantage (NRA) of various LLMs against both MCTS (Monte Carlo Tree Search) and random agents across four games: Tic-Tac-Toe, Connect Four, Breakthrough, and Nim. These games represent scenarios with complete and deterministic information.  The NRA values indicate the relative performance of the LLMs compared to the benchmarks. Red and gray lines highlight the peak NRA scores achieved by each LLM across different reasoning methods.", "section": "4 Are LLMs Capable of Strategic Reasoning?"}, {"figure_path": "ypggxVWIv2/figures/figures_28_1.jpg", "caption": "Figure 1: The overall schematic of GTBENCH. There are three main components from right to left: Environments (c) for game hosting, observation providing, and action execution; Prompt Adapter (b) for converting observation to prompt and extracting actions from participants' generations; Participants (a) for reasoning and action generation.", "description": "This figure shows the overall architecture of GTBENCH, a language-driven environment for evaluating LLMs' strategic reasoning abilities.  It highlights the three main components:  Participants (LLMs or other agents), Prompt Adapter (which translates observations into prompts for the agents and extracts actions from their responses), and Environments (which host the games and provide observations and action execution). The diagram visually depicts the flow of information and control between these components, illustrating how observations are converted into prompts, how participants generate actions, and how those actions are executed within the game environments.", "section": "3 GTBENCH: Game-Theoretic Evaluation of LLMs"}, {"figure_path": "ypggxVWIv2/figures/figures_28_2.jpg", "caption": "Figure 2: The NRA of state-of-the-art LLM-driven reasoning agents when against MCTS Agents and Random Agents, over complete and deterministic scenarios. Red and gray lines mean the maximum NRA achieved by LLM agents.", "description": "This figure displays the Normalized Relative Advantage (NRA) scores for various LLMs against both MCTS (Monte Carlo Tree Search) and Random agents across four complete and deterministic games.  The NRA score indicates the relative performance of the LLM compared to the other agents;  positive scores show that the LLM outperforms the other agent, while negative scores indicate underperformance. The red and gray lines in each bar highlight the best NRA achieved by any of the tested reasoning methods for that LLM in that game.", "section": "4 Are LLMs Capable of Strategic Reasoning?"}]