[{"type": "text", "text": "Near-Optimal Streaming Heavy-Tailed Statistical Estimation with Clipped SGD ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Aniket Das\u2217 Dheeraj Nagaraj Stanford University Google DeepMind aniketd@cs.stanford.edu dheerajnagaraj@google.com ", "page_idx": 0}, {"type": "text", "text": "Soumyabrata Pal\u2217 Arun Sai Suggala Prateek Varshney\u2217 Adobe Research Google DeepMind Stanford University soumyabratap@adobe.com arunss@google.com vprateek@stanford.edu ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "We consider the problem of high-dimensional heavy-tailed statistical estimation in the streaming setting, which is much harder than the traditional batch setting due to memory constraints. We cast this problem as stochastic convex optimization with heavy tailed stochastic gradients, and prove that the widely used ClippedSGD algorithm attains near-optimal sub-Gaussian statistical rates whenever the second moment of the stochastic gradient noise is finite. More precisely, with $T$ samples, we show that Clipped-SGD, for smooth and strongly convex objectives, achieves an error of $\\scriptstyle{\\sqrt{\\frac{\\mathsf{T r}(\\Sigma)+{\\sqrt{\\mathsf{T r}(\\Sigma)\\|\\Sigma\\|_{2}}}\\ln\\left(\\ln\\left(T\\right)/\\delta\\right)}{T}}}$ with probability $1-\\delta$ , where $\\Sigma$ is the covariance of the clipped gradient. Note that the fluctuations (depending on $^1\\!/\\!\\delta$ ) are of lower order than the term ${\\mathsf{T r}}(\\Sigma)$ . This improves upon the current best rate of Tr(\u03a3) Tln(1/\u03b4)for Clipped-SGD, known only for smooth and strongly convex objectives. Our results also extend to smooth convex and lipschitz convex objectives. Key to our result is a novel iterative refinement strategy for martingale concentration, improving upon the PAC-Bayes approach of Catoni and Giulini [8]. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "A fundamental problem in machine learning and statistics is the estimation of an unknown parameter of a probability distribution, given samples from that distribution. This can be expressed as the minimization of the expected loss: $\\begin{array}{r}{\\operatorname*{min}_{\\mathbf{x}}\\bar{F}(\\mathbf{x}):=\\mathbb{E}_{\\xi\\sim P}[f(\\mathbf{x};\\xi)]}\\end{array}$ , where $\\mathbf{x}$ represents the parameter to be estimated, $P$ is the underlying probability distribution which can only be accessed through samples, and $f(\\mathbf{x};\\boldsymbol{\\xi})$ is a function which quantifies the loss incurred at a point $\\xi$ by parameter x. In this paper, we focus on the setting where $P$ is a heavy-tailed distribution for which the extreme values are more likely than in distributions like the Gaussian, $f(\\cdot;\\cdot)$ is convex and the learner only has access to $O(d)$ memory. ", "page_idx": 0}, {"type": "text", "text": "The heavy-tailed statistical estimation problem has received increased attention of late because of the prevalence of heavy-tailed distributions in many statistical applications dealing with real world data [19, 49, 57, 23]. The presence of such heavy-tailed distributions can significantly degrade the performance of statistical estimation and testing procedures designed under Gaussian (or sub-Gaussian) tail assumptions [30, 24, 53, 24]. This has spurred recent research efforts towards developing estimators specifically tailored for heavy-tailed settings (e.g., [10, 44, 16, 36]; see Section 1.2 for a more detailed literature review). Despite substantial progress on this problem in recent years, much of the existing work has concentrated on batch learning, where the entire dataset is available upfront, and the learner can revisit data points multiple times, without memory constraints. However, the streaming setting, where data arrives sequentially and must be processed with limited memory, is increasingly pertinent in the era of large-scale models. Consequently, in this work, we focus on understanding estimators for statistical estimation under heavy-tailed distributions, in the streaming setting. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "A popular approach to study heavy-tailed streaming statistical estimation casts it as a stochastic convex optimization (SCO) problem with heavy-tailed gradients [17, 44, 52, 48] - with Clipped-SGD as the favored solution due to its simplicity [42]. Indeed, clipping has become a standard component in the training of modern deep neural networks and thus, the properties of Clipped-SGD have been studied widely in the literature [1, 56, 38, 48, 52] in various contexts. Specifically, several works have shown that Clipped-SGD has sub-Exponential or sub-Gaussian tails despite the presence of heavy tailed noise in the gradient [45, 21, 52, 49]. Despite this progress, the best known rates for Clipped-SGD with smooth and strongly convex losses, under a bounded $2^{\\mathrm{nd}}$ moment assumption on gradient distribution, are of the order $\\sqrt{\\frac{\\mathsf{T r}(\\Sigma)\\ln(1/\\delta)}{T}}$ , where $\\delta$ is the failure probability [52]. Note that this is still far from the optimal sub-Gaussian rates of Tr(\u03a3)+\u2225\u03a3\u2225ln(1/\u03b4). In this work, we bridge this gap with a sharper analysis of Clipped-SGD for SCO problems, achieving nearly sub-Gaussian rates (see Section 1.1). Our approach leverages a novel technique obtained by bootstrapping the Donsker-Varadhan Variational Principle to Freedman\u2019s inequality, yielding tighter concentration inequalities for vector martingales compared to those in [8]. This enables us to derive more refined rates for a variety of settings than a direct application of Freedman\u2019s inequality as in [52]. ", "page_idx": 1}, {"type": "text", "text": "1.1 Sub-Gaussian Error Guarantees for Statistical Estimation ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Mean Estimation We motivate our style of results with the case of mean estimation. The Central Limit Theorem (CLT) posits that that the empirical mean of $T$ independent and identically distributed (i.i.d) random variables with a finite covariance, behaves roughly like the empirical mean of Gaussian random variables with the same covariance, as $T\\to\\infty$ . That is, the empirical mean $\\hat{\\mu}$ , the true mean $\\mu$ and the covariance $\\Sigma$ are such that $\\begin{array}{r}{\\operatorname*{lim}_{T\\to\\infty}\\mathbb{P}\\left(\\sqrt{T}\\|\\hat{\\mu}-\\mu\\|>\\sqrt{\\mathsf{T r}(\\Sigma)+\\|\\Sigma\\|_{2}\\log(\\frac{1}{\\delta})}\\right)\\,\\leq\\,\\delta}\\end{array}$ . However, these asymptotic rates need not hold with a practical number of samples. Therefore, recent works on heavy-tailed high dimensional mean estimation consider algorithms and non-asymptotic guarantees which move beyond the empirical mean (see [36, 10, 9, 27, 28, 15]). Estimators such as the clipped mean estimator [8, 55], trimmed mean estimator [45], and the geometric median-of-means estimator [39, 29] achieve an error of at-most $\\sqrt{\\mathsf{T r}(\\Sigma)\\log(\\frac{1}{\\delta})/T}$ with probability $1-\\delta$ with a finite covariance assumption. Recent ground breaking works [37, 28, 8, 36] further improve upon these results to construct estimators which can achieve the CLT convergence rates of $C\\sqrt{\\mathsf{T r}(\\Sigma)\\!+\\!\\|\\Sigma\\|_{2}\\log(\\frac{1}{\\delta})/T}$ for every $T$ and $\\delta$ . Some of these estimators work under just the assumption that the second moment is bounded [37, 28, 9] and some even provide a nearly linear time algorithm [15]. ", "page_idx": 1}, {"type": "text", "text": "General Statistical Estimation In this work, we are interested in the general statistical estimation problem. Among the various approaches, framing this problem as SCO with heavy-tailed gradients has gained traction recently (see [52] and references there in). While one obvious candidate is to use SGD with state-of-the-art optimal mean estimators for robust gradient estimation, such methods can face significant challenges. First, most optimal mean estimators aren\u2019t designed for the streaming data setting with batch-size being 1. Second, these estimators can be complex, frequently relying on semi-definite programming or other demanding techniques. Third, and perhaps most importantly, they don\u2019t typically provide guarantees on the bias of their estimates. This lack of bias control is problematic because SGD-style algorithms, even when equipped with accurate gradient estimates, can perform poorly if those estimates are systematically biased (See [3, Theorem 4], where bias does not cancel across iterations). Given these challenges, the clipped mean estimator of [8] has emerged as a popular choice for gradient estimation in SCO, due mainly to is simplicity. Several recent works analyze the performance of SGD with clipped mean estimator for the gradients (i.e, Clipped SGD). However, as previously mentioned, the best known analysis for clipped SGD achieves a sub-optimal rate of $\\sqrt{\\mathsf{T r}(\\Sigma)\\ln(^{1}\\!/\\delta)/T}$ , under bounded $2^{\\mathrm{nd}}$ moment assumption. In this work, we improv\u221ae upon these rates and show that with $T$ samples, clipped-SGD obtains a sharper rate of $\\frac{{\\sf T r}(\\Sigma)+\\sqrt{{\\sf T r}(\\Sigma)\\|\\Sigma\\|}\\ln(\\frac{\\ln(T)}{\\delta})}{T}$ with probability $1-\\delta$ , which is closer to the truly sub-Gaussian rates. ", "page_idx": 1}, {"type": "text", "text": "Table 1: Sample complexity bounds (for converging to an $\\epsilon$ approximate solution) of various algorithms for SCO under heavy tailed stochastic gradients. Results are instantiated for smooth and strongly convex losses, and for the case where the gradient noise has bounded covariance equal to the Identity matrix. $D_{1}$ is the distance of the initial iterate from the optimal solution. For readability, we ignore the dependence of rates on the condition number. Observe all prior works have $d\\log\\delta^{-1}$ dependence in the sample complexity. ", "page_idx": 2}, {"type": "table", "img_path": "8JauriwDeH/tmp/ef8a9d7e5c20220b3a53ffd3c89145b892034aacc84edb3d38347b3980569337.jpg", "table_caption": [], "table_footnote": [], "page_idx": 2}, {"type": "text", "text": "1.2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Clipped SGD Clipped SGD and it\u2019s variants have been studied under a variety of settings including convex, strongly-convex, non-convex losses, with various assumptions on the moments of stochastic gradients. The estimators of [21, 45, 14, 40] work under the assumption of bounded $2^{n d}$ moments, but require $O(1/\\epsilon)$ batch size, to converge to an $\\epsilon$ -approximate solution. Consequently, they are not suitable for streaming setting. The recent work of [52], which is closest to our work, addresses this issue by analysing Clipped-SGD for batch size 1 for smooth, strongly convex losses. But they achieve a sub-optimal rate of $\\sqrt{\\mathsf{T r}(\\Sigma)\\ln(^{1}\\!/\\delta)/T}$ . These rates are improved in our work (see Table 1 for a detailed comparison). Additionally, our work provides convergence rates for convex objectives that are not strongly convex. Recent works [48, 46, 41, 34, 13] have studied Clipped-SGD with the assumption that the stochastic gradient has a finite $p$ -th moment for some $p\\in(1,2]$ . They derive fine-grained near optimal results in terms of dependence of $T$ and $p$ (but their dependence on $\\log\\delta^{-1}$ is sub-optimal). In contrast, our work specifically the case considers $p=2$ with a focus on improving the sub-Gaussian dependence in the high probability bounds in these works from $\\mathsf{T r}(\\Sigma)\\log(\\bar{1/\\delta})$ and approaching the truly sub-Gaussian rates for estimation 1.1. ", "page_idx": 2}, {"type": "text", "text": "Heavy-tailed Estimation Heavy-tailed estimation has a rich history in statistics and we only review some of the recent advances. Several recent works have studied the problem of heavy-tailed mean estimation, and have derived estimators that achieve sub-Gaussian rates under the bounded $2^{\\mathrm{nd}}$ moment assumption [36, 10, 9, 27, 28, 15, 45]. Among these, the works of [15, 32] are particularly relevant to our work. The algorithm of [15] runs in linear time while requiring $O(d\\log\\delta^{-1})$ memory. But it is not immediately clear how to use their estimator in the framework of SGD. [32] study the trimmed mean estimator (an estimator that is closely related to clipped mean estimator, where outliers are removed instead of being clipped) and show that when $T=\\bar{\\omega}(\\log^{3}\\delta^{-1})$ , $d=\\omega(\\log^{2}(\\delta^{-1}))$ , the estimator achieves the optimal rates. We not that our analysis of clipped SGD, when instantiated for mean estimation, leads to similar rates. But unlike [32] which is primarily focused on mean estimation, we focus on the more general SCO problem. ", "page_idx": 2}, {"type": "text", "text": "Heavy-tailed linear regression has been widely studied, with classical estimators based on Huber regression [30, 50, 33] known to provide optimal rates when the response variables are heavy-tailed, but the covariates are light-tailed. Recently, there has been a surge of interest in developing estimators when both covariates and response variables are heavy tailed [5, 44, 17, 43]. However, most of these works are in the batch setting. Another line of work has considered streaming algorithms in the Huber-contamination model, which is a much harder contamination model than heavy-tails [18]. However, these algorithms when adapted to heavy-tailed setting, do not provide optimal rates. ", "page_idx": 2}, {"type": "text", "text": "1.3 Contributions ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Iteratively Refined Martingale Concentration via PAC Bayes Our key technical result obtains fine-grained concentration guarantees for vector-valued martingales by using the Donsker-Varadhan Variational Principle to iteratively refine baseline concentration inequalities. This allows us to sharpen the PAC Bayes bounds of Catoni and Giulini [8] (and its martingale based extensions like [11]), which were used to analyze the clipped mean estimator. We believe these iterative refinement arguments could be of independent interest for developing fine-grained concentration bounds. ", "page_idx": 3}, {"type": "text", "text": "Sharp Analysis of Clipped SGD Leveraging these fine-grained concentration results, We perform a fine-grained analysis of clipped SGD for heavy-tailed SCO problem obtain nearly subgaussian performance guarantees in the streaming setting with a batchsize of 1 and $O(d)$ space complexity. In particular, we demonstrate that the sub-optimality gap after $T$ steps scales as $\\mathsf{T r}(\\Sigma)\\,+\\,\\sqrt{\\|\\Sigma\\|_{2}\\mathsf{T r}(\\Sigma)}\\log(1/\\delta)$ , improving upon the best known scaling of $\\mathsf{T r}(\\Sigma)\\log(1/\\delta)$ obtained by prior works [52] only for smooth strongly convex problems. To the best of our knowledge, we derive the first such guarantees for smooth convex and lipschitz convex problems in the streaming setting. ", "page_idx": 3}, {"type": "text", "text": "Streaming Heavy Tailed Statistical Estimation We use the above results to develop streaming estimators for various heavy-tailed statistical estimation problems including heavy-tailed mean estimation as well as linear, logistic and Least Absolute Deviation (LAD) regression with heavy tailed covariates, all of which exhibit nearly subgaussian performance. Our mean estimation results improve upon the previous best known guarantees for trimmed mean based estimators [8, 52, 32] (either in performance or in generality) For heavy-tailed linear regression under the assumption of bounded $4^{\\mathrm{th}}$ moments for the covariates and bounded $2^{\\mathsf{n d}}$ moments for the response, our rates significantly improve upon that of the previous best known streaming estimator [52]. To the best of our knowledge, we develop the first known streaming estimators for heavy-tailed logistic regression and LAD regression which attain nearly subgaussian rates ", "page_idx": 3}, {"type": "text", "text": "2 Notation and Organization ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We work with Euclidean spaces $\\mathbb{R}^{d}$ equipped with the standard inner product $\\langle\\cdot,\\cdot\\rangle$ and the induced $\\ell_{2}$ norm $\\|\\cdot\\|$ . For any matrix $A\\,\\in\\,\\mathbb{R}^{m\\times n}$ , we use $\\lVert\\mathbf{A}\\rVert_{2}$ to denote its Euclidean operator norm $\\|\\mathbf{A}\\|=\\operatorname*{sup}_{\\mathbf{x}\\neq0}\\|\\mathbf{A}\\mathbf{x}\\|/\\|\\mathbf{x}\\|$ . For $A\\in\\mathbb{R}^{d\\times d}$ , we denote its trace as ${\\mathsf{T r}}(A)$ . For any random vector $\\mathbf{x}$ , we denote its covariance matrix as $\\mathsf{C o v}[\\mathbf{x}]$ . We use $\\lesssim,\\gtrsim\\mathrm{and}\\asymp1$ o denote $\\leq,\\geq{\\mathrm{and}}=$ respectively, upto universal multiplicative constants. We use $\\nabla f(\\mathbf{x})$ to denote the gradient of a differentiable function For any convex function $f$ , we use $\\partial f(\\mathbf{x})$ to denote an arbitrary subgradient of $f$ at $\\mathbf{x}$ . ", "page_idx": 3}, {"type": "text", "text": "3 Background and Problem Formulation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Our work studies the Stochastic Convex Optimization (SCO) problem, described as follows: Let $\\mathcal{C}$ denote a closed convex subset of $\\mathbb{R}^{d}$ and let $F:{\\mathcal{C}}\\rightarrow\\mathbb{R}$ be a convex function. We aim to solve: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\mathbf{x}\\in\\mathcal{C}}F(\\mathbf{x}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "assuming access to a convex projection oracle $\\Pi_{\\mathcal{C}}$ and a stochastic gradient oracle, which we define as follows: Let $P$ denote a probability measure supported on an arbitrary domain $\\Xi$ from which we can draw samples. A stochastic gradient oracle for $F$ is a function $g:{\\mathcal{C}}\\times{\\mathcal{C}}$ , which, given a point $\\mathbf{x}\\in{\\mathcal{C}}$ and a sample $\\xi\\sim P$ returns an unbiased estimate $g(\\mathbf{x};\\boldsymbol{\\xi})$ of $\\nabla F(\\mathbf{x})$ i.e., $\\mathbb{E}_{\\xi\\sim P}\\left[g(\\mathbf{x};\\xi)\\bar{]}=\\nabla F(\\mathbf{x})$ . If $F$ is nondifferentiable, $\\mathbb{E}_{\\xi\\sim P}\\left[g(\\mathbf{x};\\xi)\\right]=\\partial F(\\mathbf{x})$ . Note that we do not assume direct access to $\\nabla F(\\mathbf{x})$ , which may be expensive or intractable to compute. Our objective is to (approximately) solve SCO subject to a constraint on the number of samples we can draw from $P$ . ", "page_idx": 3}, {"type": "text", "text": "This is an alternative formulation of the statistical estimation problem by recognizing $P$ as the data distribution, $\\mathcal{C}$ as the parameter space and defining the population risk $F(\\mathbf{x}):=\\mathbb{E}_{\\xi\\sim P}[f(\\mathbf{x};\\xi)]$ , where $f$ denotes the sample-level loss function. The associated stochastic gradient oracle is $g(\\bar{\\bf x};\\xi):=$ $\\nabla f(\\mathbf{x};\\boldsymbol{\\xi}),\\;\\boldsymbol{\\xi}\\,\\sim\\,P$ , which is usually easy to compute. As we shall discuss in Section 5, several statistical estimation problems such as mean estimation, linear regression, logistic regression and least absolute deviation regression naturally fit into the SCO framework. ", "page_idx": 3}, {"type": "text", "text": "We use $\\begin{array}{r}{\\mathbf{n}(\\mathbf{x};\\boldsymbol{\\xi})=\\mathbf{g}(\\mathbf{x};\\boldsymbol{\\xi})-\\nabla F(\\mathbf{x})}\\end{array}$ to denote the stochastic gradient noise and assume it has finite second moment, i.e., $\\Sigma(\\overset{.}{\\mathbf{x}})=\\mathbb{E}_{\\xi\\sim P}[\\mathbf{n}(\\mathbf{x};\\xi)\\mathbf{n}(\\mathbf{x};\\xi)^{T}]$ exists for every $\\mathbf{x}\\in{\\mathcal{C}}$ . Our results make use of either of the following assumptions on $\\Sigma(\\mathbf{x})$ . ", "page_idx": 4}, {"type": "text", "text": "Assumption 1 (Bounded Second Moment). The exists a positive semidefinite matrix $\\Sigma$ such that: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\Sigma(\\mathbf{x})\\preceq\\Sigma\\quad\\forall\\,\\mathbf{x}\\in\\mathcal{C}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Similar assumption has been made by several prior works [21, 40, 14, 45]. We also consider the following generalized assumption, which is as a refinement of the one made in Tsai et al. [52]. ", "page_idx": 4}, {"type": "text", "text": "Assumption 2 (Second Moment with Quadratic Growth). There exist constants $\\alpha,\\beta~\\geq~0$ and $1\\leq d_{\\mathsf{e f f}}d$ such that the following holds for every $\\mathbf{x}\\in{\\mathcal{C}}$ ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\|\\Sigma(\\mathbf{x})\\|_{2}\\leq\\alpha\\|\\mathbf{x}-\\mathbf{x}^{*}\\|^{2}+\\beta;\\qquad\\mathsf{T r}(\\Sigma(\\mathbf{x}))\\leq d_{\\mathrm{eff}}\\left(\\alpha\\|\\mathbf{x}-\\mathbf{x}^{*}\\|^{2}+\\beta\\right)\\qquad(\\mathrm{QG~}2^{\\mathrm{nd}}\\ \\mathrm{Moment})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{x}^{*}$ denotes any arbitrary minimizer of $F$ . ", "page_idx": 4}, {"type": "text", "text": "Since we consider streaming statistical estimators that are robust to heavy tailed data, we only assume the existence of the second moment of the stochastic gradient noise and allow its higher moments to be infinite. That is, our results hold even when $\\mathbb{E}_{{\\xi}\\sim{P}}[|\\left.\\left\\langle\\mathbf{n}(\\mathbf{x};{\\xi}),\\mathbf{v}\\right\\rangle|^{2+\\epsilon}\\right]=\\infty$ for every $\\epsilon>0,\\mathbf{v}\\in\\mathbb{R}^{d}$ ", "page_idx": 4}, {"type": "text", "text": "Our work analyzes SCO under either of the following structural assumptions assumptions on $F$ ", "page_idx": 4}, {"type": "text", "text": "Assumption 3 (Convexity). $F:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ is a convex function if the following holds for any $t\\in[0,1]$ ", "page_idx": 4}, {"type": "equation", "text": "$$\nF(t\\mathbf{x}+(1-t)\\mathbf{y})\\leq t F(\\mathbf{x})+(1-t)F(\\mathbf{y})\\quad\\forall\\,\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{d}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Assumption 4 ( $\\mu$ -Strong Convexity). $F:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ is a $\\mu$ -strongly convex function for $\\mu\\geq0$ if the following holds for every $t\\in[0,1]$ ", "page_idx": 4}, {"type": "equation", "text": "$$\nF(t\\mathbf{x}+(1-t)\\mathbf{y})\\leq t F(\\mathbf{x})+(1-t)F(\\mathbf{y})-t(1-t)\\cdot{\\frac{\\mu}{2}}\\|\\mathbf{x}-\\mathbf{y}\\|^{2}\\quad\\forall\\,\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{d}\\quad(\\mu\\cdot\\mathrm{Strong~Convexity})\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In addition, we also consider either of the two regularity assumptions on $F$ ", "page_idx": 4}, {"type": "text", "text": "Assumption 5 ( $L$ -smoothness). $F:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ is $L$ -smooth for some $L\\ge0\\;i f\\,F$ is continuously differentiable and satisfies the following: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\|\\nabla F(\\mathbf{x})-\\nabla F(\\mathbf{y})\\|\\leq L\\|\\mathbf{x}-\\mathbf{y}\\|\\quad\\forall\\,\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{d}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Assumption 6 $G$ -Lipschitzness). $F:\\ensuremath{\\mathbb{R}^{d}}\\to\\ensuremath{\\mathbb{R}}$ is $G$ -Lipschitz for some $G\\geq0$ , i.e., $F$ is continuous and satisfies the following: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\|F(\\mathbf{x})-F(\\mathbf{y})\\|\\leq G\\|\\mathbf{x}-\\mathbf{y}\\|\\quad\\forall\\,\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{d}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "4 Results ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Under the Bdd. $2^{\\mathsf{n d}}$ Moment and QG $2^{\\mathsf{n d}}$ Moment assumptions, streaming algorithms for SCO such as Stochastic Gradient Descent (SGD) typically convergence bounds guarantees that hold in expectation [56, 26, 22]. However, high probability guarantees require strong assumptions on the tail behavior of the stochastic gradients (e.g. boundedness or subgaussianity) [25, 47, 31]. Our work analyzes SCO under heavy tailed stochastic gradients, which typically exhibit large fluctuations from their expected value due to its higher order moments being potentially infinite. Clipped SGD mitigates the large fluctuations typically observed in the heavy tailed stochastic gradient $\\hat{g}(\\mathbf{x};\\boldsymbol{\\xi})$ by thresholding its norm as follows. The full algorithm is described in Algorithm 1. ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathsf{c l i p}_{\\Gamma}(g(\\mathbf{x};\\boldsymbol{\\xi})):=\\frac{g(\\mathbf{x};\\boldsymbol{\\xi})}{\\|g(\\mathbf{x};\\boldsymbol{\\xi})\\|}\\cdot\\operatorname*{min}\\{\\Gamma,\\|g(\\mathbf{x};\\boldsymbol{\\xi})\\|\\}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We now present our performance guarantees for clipped SGD for streaming heavy tailed SCO, wherein Algorithm 1 is subject to an $O(d)$ memory constraint and can access only one stochastic gradient sample per iteration. For the remainder, of this section, we use $\\mathbf{x}^{*}\\in\\mathcal{C}$ to denote an arbitrary minimizer of $F$ , which is assumed to always exist, and guaranteed to be unique if $F$ satisfies $\\mu$ -Strong Convexity. We use $\\mathbf{x}_{1}$ to denote the initialization of Algorithm 1 and let $D_{1}=\\|\\mathbf{x}-\\mathbf{x}^{*}\\|$ . ", "page_idx": 4}, {"type": "text", "text": "Algorithm 1 Clipped Stochastic Gradient Descent ", "page_idx": 5}, {"type": "text", "text": "Input: Initialization $\\mathbf{x}_{1}$ , Horizon $T$ , Step Sizes $(\\eta_{t})_{t\\in[T]}$ , Clipping Level \u0393 ", "page_idx": 5}, {"type": "text", "text": "1: for $t\\in[T]$ do   \n2: $\\mathbf{g}_{t}\\leftarrow\\operatorname{\\overline{{g}}}(\\mathbf{x}_{t};\\xi_{t}),\\quad\\xi_{t}\\sim P$   \n3: $\\mathbf{x}_{t+1}\\gets\\Pi_{\\mathcal{C}}(\\mathbf{x}_{t}-\\eta_{t}\\cdot\\mathsf{c}|\\mathsf{i}\\mathsf{p}_{\\Gamma}(\\mathbf{g}_{t}))$   \n4: end for   \n5: Last Iterate : Output xT +1   \n6: Average Iterate : Output $\\begin{array}{r}{\\hat{\\mathbf{x}}_{T}=\\frac{1}{T}\\sum_{t=1}^{T}\\mathbf{x}_{t}}\\end{array}$ ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "4.1 Smooth Strongly Convex Objectives ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Theorems 1 and 2, proved in, Appendix B and C respectively, derive high probability convergence bounds for smooth and strongly convex objectives with second moment assumption. ", "page_idx": 5}, {"type": "text", "text": "Theorem 1 (Smooth Strongly Convex Objectives). Let the $L$ -smoothness, $\\mu$ -Strong Convexity and Bdd. $2^{\\mathsf{n d}}$ Moment assumptions be satisfied. Then, for any $\\delta~\\in~(0,1/2)$ , the last iterate of Algorithm $^{\\,l}$ run for $T\\,\\gtrsim\\,\\ln(\\ln(d))$ iterations with stepsize $\\begin{array}{r}{\\eta_{t}~=~\\frac{4}{\\mu(t+\\gamma)}}\\end{array}$ and clipping level $\\begin{array}{r}{\\Gamma\\ =\\ \\frac{\\mu}{\\ln(\\ln(T)/\\delta)}\\sqrt{(\\gamma+1)^{2}D_{1}^{2}+\\frac{(T+\\gamma)}{\\mu^{2}}(\\mathsf{T r}(\\Sigma)+\\sqrt{\\mathsf{T r}(\\Sigma)\\|\\Sigma\\|_{2}}\\ln\\bigl(\\ln(T)/\\delta\\bigr)\\bigr)}.}\\end{array}$ satisfies the following with probability at least $1-\\delta$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\quad\\quad\\quad\\|\\mathbf{x}_{T+1}-\\mathbf{x}^{*}\\|\\lesssim\\frac{\\gamma D_{1}}{T+\\gamma}+\\frac{1}{\\mu}\\sqrt{\\frac{\\mathsf{T r}(\\Sigma)+\\sqrt{\\mathsf{T r}(\\Sigma)\\|\\Sigma\\|_{2}}\\ln\\left(\\ln(T)/\\delta\\right)}{T+\\gamma}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\gamma\\asymp\\operatorname*{max}\\{\\frac{\\|\\Sigma\\|_{2^{\\kappa^{2}}}\\ln\\left(\\ln(T)/\\delta\\right)^{2}}{\\mathsf{T r}(\\Sigma)},\\kappa^{3/2}\\ln\\bigl(\\ln(T)/\\delta\\bigr),\\kappa\\ln\\bigl(\\ln(T)/\\delta\\bigr)^{2}\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We use Theorem 1 to derive sharp rates for streaming heavy tailed mean estimation in Section 5.1 and the following result to derive sharp rates for streaming heavy tailed linear regression in section 5.2 ", "page_idx": 5}, {"type": "text", "text": "Theorem 2 (Smooth Strongly Convex Objectives with Quadratic Growth Noise Model). Let Assumptions $\\mu$ -Strong Convexity, $L$ -smoothness and $Q G\\;2^{\\mathsf{n d}}$ Moment be satisfied and let $\\kappa=L/\\mu$ . For any \u03b4 \u2208(0, 1/2), the last iterate of Algorithm 1 run for T \u2273ln(ln(d)) iterations with step-size \u03b7t = \u00b5(t4+\u03b3) and clipping level $\\begin{array}{r}{\\Gamma=\\frac{\\mu}{\\ln(^{(\\ln(T)/\\delta)})}\\sqrt{(\\gamma+1)^{2}D_{1}^{2}+\\frac{\\beta}{\\mu^{2}}\\cdot(T+\\gamma)(d_{\\mathsf{e f f}}+\\sqrt{d_{\\mathsf{e f f}}}\\ln(^{\\ln(T)}/\\delta))}}\\end{array}$ satisfies the following with probability at least $1-\\delta$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\|\\mathbf{x}_{T+1}-\\mathbf{x}^{*}\\|\\lesssim\\frac{\\gamma D_{1}}{T+\\gamma}+\\frac{1}{\\mu}\\sqrt{\\frac{\\beta(d_{\\mathrm{eff}}+\\sqrt{d_{\\mathrm{eff}}}\\ln\\bigl(\\ln(T)/\\delta\\bigr))}{T+\\gamma}}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{w h e r e\\uparrow\\asymp\\operatorname*{max}\\{\\frac{\\alpha d_{\\mathrm{eff}}}{\\mu^{2}},\\frac{\\alpha\\sqrt{d_{\\mathrm{eff}}}}{\\mu^{2}}\\ln\\bigl(\\ln(T)/\\delta\\bigr),\\frac{\\kappa\\sqrt{\\alpha}}{\\mu}\\ln\\bigl(\\ln(T)/\\delta\\bigr),\\frac{\\sqrt{\\kappa\\alpha d_{\\mathrm{eff}}}}{\\mu}\\ln\\bigl(\\ln(T)/\\delta\\bigr),}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\frac{\\kappa^{2/3}\\alpha^{1/3}d_{\\mathrm{eff}}^{1/3}}{\\mu^{2/3}}\\ln\\bigl(\\ln(T)/\\delta\\bigr),\\kappa^{3/2}\\ln\\bigl(\\ln(T)/\\delta\\bigr),\\kappa\\ln\\bigl(\\ln(T)/\\delta\\bigr)^{2},\\frac{\\kappa^{2}}{d_{\\mathrm{eff}}}\\ln\\bigl(\\ln(T)/\\delta\\bigr)\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Comparison to Prior Works To the best of our knowledge, the result closest to Theorem 2 is [52, Theorem 1] which analyzes streaming strongly convex SCO and obtains a $\\begin{array}{r}{\\frac{\\zeta D_{1}}{T+\\zeta}+\\frac{1}{\\mu}\\sqrt{\\frac{\\beta d_{\\mathrm{eff}}\\ln\\left(1/\\delta\\right)}{T+\\zeta}}}\\end{array}$ for \u03b1deff l\u00b5o2g(1/\u03b4). We note that Theorem 2 obtains a significantly better confidence bound which is closer to the optimal subgaussian rate compared [52, Theorem 1]. ", "page_idx": 5}, {"type": "text", "text": "Extra $\\log\\log T$ term: Our bounds for the statistical error is of the form $\\begin{array}{r}{\\frac{1}{\\mu}\\sqrt{\\frac{\\beta\\left(d_{\\mathsf{e f f}}+\\sqrt{d_{\\mathsf{e f f}}}\\ln\\left(\\ln\\left(T\\right)/\\delta\\right)\\right)}{T+\\gamma}}}\\end{array}$ which has an extra $\\log\\log T$ factor in the lower order term. This is still sharper than prior works with bounds of the form \u03b2defTf  l+n(\u03b31/\u03b4)as long as log log T \u226a deff log( \u03b41 ). ", "page_idx": 5}, {"type": "text", "text": "4.2 Beyond Strongly Convex Objectives ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Moving beyond strong convexity, we present Theorems 3 for smooth convex functions and 4 for Lipschitz convex function, proved in Appendix $\\mathrm{D}$ and $\\boldsymbol{\\mathrm E}$ respectively. To the best of our knowledge, these are the first results for streaming heavy-tailed convex SCO that exhibits near-subgaussian concentration without strong convexity. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Theorem 3 (Smooth Convex Objectives). Let Convexity, $L$ -smoothness and Bdd. $2^{\\mathsf{n d}}$ Moment be satisfied. Then, for any $\\delta\\,\\in\\,(0,^{\\bullet}1/2)$ and $T\\,\\geq\\,\\ln(\\ln(d))$ , there exists an $\\eta\\,\\in\\,(0,1/2L]$ such that the average iterate of Algorithm $^{\\,l}$ run for $T$ iterations with step-size $\\eta_{t}\\,=\\,\\eta$ and clipping level $\\begin{array}{r}{\\Gamma=\\sqrt{\\frac{T\\sqrt{\\|\\Sigma\\|_{2}}(\\sqrt{\\mathsf{T r}(\\Sigma)}+L D_{1})}{\\ln\\left(\\ln{(T)}/\\delta\\right)}}}\\end{array}$ satisfies the following with probability at least $1-\\delta$ : ", "page_idx": 6}, {"type": "equation", "text": "$$\nF(\\hat{\\mathbf{x}}_{T})-F(\\mathbf{x^{*}})\\lesssim\\frac{L D_{1}^{2}}{T}+D_{1}\\sqrt{\\frac{\\mathsf{T r}(\\Sigma)+\\sqrt{\\|\\Sigma\\|_{2}}\\left(\\sqrt{\\mathsf{T r}(\\Sigma)}+L D_{1}\\right)\\ln\\bigl(\\ln(T)/\\delta\\bigr)}{T}}+o_{T}(L,D_{1},\\Sigma)\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $o_{T}(L,D_{1},\\Sigma)$ represents terms that are of lower order in $T$ (explicated in Appendix $D$ ) ", "page_idx": 6}, {"type": "text", "text": "Theorem 4 (Lipschitz Convex Objectives). Let Assumptions Convexity, $G$ -Lipschitzness and Bdd. $2^{\\mathsf{n d}}$ Moment be satisfied. Then, for any $\\delta\\in(0,{^1\\mathord{/}{\\vphantom{^{1}}^{2}}})$ and $T\\geq\\ln(\\ln(d))$ , there exists an $\\eta\\in(0,G/\\sqrt{T}]$ such that the average iterate of Algorithm $^{\\,l}$ run for $T$ iterations with step-size $\\eta_{t}=\\eta$ and clipping level $\\begin{array}{r}{\\Gamma=\\sqrt{\\frac{T\\sqrt{\\|\\Sigma\\|_{2}}(\\sqrt{\\mathsf{T r}(\\Sigma)}+G)}{\\ln\\left(\\ln\\left(T\\right)/\\delta\\right)}}}\\end{array}$ satisfies the following with probability at least $1-\\delta$ $F(\\hat{\\mathbf{x}}_{T})-F(\\mathbf{x}^{*})\\lesssim\\frac{D_{1}G}{\\sqrt{T}}+D_{1}\\sqrt{\\frac{\\mathsf{T r}(\\Sigma)+\\sqrt{\\|\\Sigma\\|_{2}}\\left(\\sqrt{\\mathsf{T r}(\\Sigma)}+G\\right)\\ln(\\ln(T)/\\delta)}{T}}+o_{T}(G,D_{1},\\Sigma)$ ", "page_idx": 6}, {"type": "equation", "text": "", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $o_{T}(G,D_{1},\\Sigma)$ represents terms that are lower order in $T$ (explicated in Appendix $E$ ) ", "page_idx": 6}, {"type": "text", "text": "Remark: We use Theorem 3 to design the first known streaming estimator for logistic regression with heavy-tailed covariates in Section 5.3 and Theorem 4 to design the first known streaming estimator for LAD regression with heavy-tailed covariates in Section 5.4. ", "page_idx": 6}, {"type": "text", "text": "Remark: In Theorems 3 and 4, the leading order term in the error is of the form: $\\begin{array}{r}{D_{1}\\sqrt{\\frac{\\mathsf{T r}(\\Sigma)+\\sqrt{\\|\\Sigma\\|_{2}}\\left(\\sqrt{\\mathsf{T r}(\\Sigma)}+\\zeta\\right)\\ln\\left(\\ln\\left(T\\right)/\\delta\\right)}{T}}}\\end{array}$ , where $\\zeta\\in\\{G,L D_{1}\\}$ . Assuming $G,D_{1}$ , $\\sqrt{\\mathsf{T r}(\\Sigma)}\\asymp\\sqrt{d}$ , we note that the term dependent on the confidence level $\\log(1/\\delta)$ is lower order compared to ${\\mathsf{T}}{\\mathsf{r}}(\\Sigma)$ . To the best of our knowledge, this is the first work which establishes strong confidence bounds in the setting of SCO without strong convexity. Interestingly, our results also improve the best known rates for sub-Gaussian gradient noise. To be precise, [35, Theorem 3.1] shows a weaker bound of $\\sqrt{D_{1}^{2}(G^{2}\\!+\\!\\mathsf{T r}(\\Sigma)\\log(\\frac{1}{\\delta}))\\big/T}$ in the setting of Theorem 4, but when the noise is sub-Gaussian. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "5 Applications to Streaming Heavy Tailed Statistical Estimation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "5.1 Streaming Heavy-Tailed Mean Estimation ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Consider streaming heavy tailed mean estimation with clipped SGD with access to $N$ i.i.d samples from the distribution $P$ . Let $\\Xi={\\mathcal{C}}$ , $\\mathbb{E}_{\\xi\\sim P}[\\xi]=\\mathbf{m}\\in\\mathcal{C}$ . We further assume ${\\sf C o v}[\\xi]\\preceq\\Sigma$ and allow the higher moments to be infinite. As described in Appendix G.1, this is an SCO problem with the sample loss $\\begin{array}{r}{f(\\mathbf{x};\\boldsymbol{\\xi})=\\frac{1}{2}\\|\\mathbf{x}-\\boldsymbol{\\xi}\\|^{2}}\\end{array}$ . The population loss and the stochastic gradient are given by: ", "page_idx": 6}, {"type": "equation", "text": "$$\nF(\\mathbf{x})={\\frac{1}{2}}\\|\\mathbf{x}-\\mathbf{m}\\|^{2}+\\mathsf{T r}(\\mathsf{C o v}_{\\xi\\sim P}[\\xi]);\\qquad g(\\mathbf{x};\\xi)=\\mathbf{x}-\\xi\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "The following result, proved in Appendix G.1 via an application of Theorem 1, shows that the last iterate of clipped SGD attains near-subgaussian rates for the heavy tailed mean estimation problem ", "page_idx": 6}, {"type": "text", "text": "Corollary 1 (Heavy Tailed Mean Estimation). Under the stochastic gradient oracle described above, implemented using $N\\gtrsim\\ln(\\ln(d))$ i.i.d samples $\\xi_{1},\\ldots,\\xi_{N}\\sim P$ , the last iterate of Algorithm 1 when run under the parameter settings of Theorem $^{\\,I}$ satisfies the following with probability at least $1-\\delta$ ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\|\\mathbf{x}_{N+1}-\\mathbf{m}\\|\\lesssim\\frac{\\gamma\\|\\mathbf{x}_{1}-\\mathbf{m}\\|}{N+\\gamma}+\\sqrt{\\frac{\\mathsf{T r}(\\Sigma)+\\sqrt{\\|\\Sigma\\|_{2}\\mathsf{T r}(\\Sigma)}\\ln\\bigl(\\ln(N)/\\delta\\bigr)}{N+\\gamma}}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\gamma\\asymp\\ln(\\ln(N)/\\delta)^{2}$ ", "page_idx": 6}, {"type": "text", "text": "Comparison to Prior Works The clipped mean estimator of [8] and the clipped-SGD based estimator in [52] come with a guarantee of the form $\\lVert\\hat{\\mathbf{m}}-\\mathbf{m}\\rVert\\,\\lesssim\\,\\sqrt{\\mathsf{T r}(\\Sigma)\\log(\\frac{1}{\\delta})/N}$ with probability $1-\\delta$ . Our result in Corollary 1 obtains a sharper rate of convergence. In a recent work, Lee and Valiant [32] showed that the trimmed mean estimator achieves the optimal rate of $\\sqrt{\\mathsf{T r}(\\Sigma)/N}$ when $N=$ $\\omega(\\log^{3}\\delta^{-1}),d\\;=\\;\\omega(\\log^{2}(\\delta^{-1}))$ . Our result matches this optimal rate in those settings, but is considerably more general, as it holds for any $N,d$ . ", "page_idx": 7}, {"type": "text", "text": "5.2 Streaming Heavy Tailed Linear Regression ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In the current and subsequent sections, we use $\\theta\\in{\\mathcal{C}}$ to denote the parameter of $F$ . Let $\\Xi=\\mathbb{R}^{d}\\times\\mathbb{R}$ . Given a target parameter $\\theta^{*}\\in\\mathcal{C}$ , $P$ defines the following linear model: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{x}\\sim Q,\\,\\mathbb{E}[\\mathbf{x}]=0,\\,\\mathbb{E}[\\mathbf{xx}^{T}]=\\Sigma\\succ0;\\qquad y=\\langle\\mathbf{x},{\\boldsymbol{\\theta}}^{*}\\rangle+\\epsilon,\\,\\mathbb{E}[\\epsilon|\\mathbf{x}]=0,\\,\\mathbb{E}[\\epsilon^{2}|\\mathbf{x}]\\leq\\sigma^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "In addition, we make the following bounded $4^{\\mathrm{th}}$ moment asumption on the covariates $\\mathbf{x}$ ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\langle\\mathbf{x},\\mathbf{v}\\rangle^{4}]\\leq C_{4}(\\mathbb{E}[\\langle\\mathbf{x},\\mathbf{v}\\rangle^{2}])^{2}\\qquad\\forall\\,\\mathbf{v}\\in\\mathbb{R}^{d}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "for some numerical constant $C_{4}\\geq1$ . Note that we allow both the covariate $\\mathbf{x}$ and the target $\\mathbf{y}$ to be heavy tailed, assuming only finite moments of upto order 4 for $\\mathbf{x}$ and order 2 for $\\mathbf{y}$ . The assumption $\\mathbb{E}[\\mathbf{x}]^{\\bullet}=0$ is only made for ease of presentation and our arguments easily adapt to $\\mathbb{E}[\\mathbf{x}]\\neq0$ . Our task is to estimate $\\theta^{*}$ in a streaming fashion with access to $\\bar{N}$ i.i.d samples from $P$ . As described in Appendix G.2, we reframe this problem as SCO under the sample loss $f(\\theta;\\mathbf{x},y)=\\textstyle{\\frac{1}{2}}(\\langle\\mathbf{x},\\theta\\rangle-y)^{2}$ . The associated population loss $F(\\theta)$ and the stochastic gradient oracle $g(\\theta;\\mathbf x,y)$ are given by: ", "page_idx": 7}, {"type": "equation", "text": "$$\nF(\\theta)=\\frac{1}{2}(\\theta-\\theta^{*})^{T}\\Sigma(\\theta-\\theta^{*});\\qquad g(\\theta;{\\bf x},{\\bf y})=(\\langle{\\bf x},\\theta\\rangle-y){\\bf x}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "Corollary 2 (Heavy Tailed Linear Regression). Under the stochastic gradient oracle described above, implemented using $N\\gtrsim\\ln(\\ln(\\bar{d}))$ i.i.d samples from $P$ , the last iterate of Algorithm $^{\\,I}$ when run under the parameter settings of Theorem 2 satisfies the following with probability at least $1-\\delta$ : ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\|\\theta_{N+1}-\\theta^{*}\\|\\lesssim{\\frac{\\gamma\\|\\theta_{1}-\\theta^{*}\\|}{N+\\gamma}}+{\\frac{\\sigma}{\\lambda_{\\operatorname*{min}}\\left(\\Sigma\\right)}}{\\sqrt{\\frac{\\mathsf{T r}(\\Sigma)+{\\sqrt{\\|\\Sigma\\|_{2}\\mathsf{T r}(\\Sigma)}\\ln\\!\\left(\\ln(N)/\\delta\\right)}}{N+\\gamma}}}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\varrho\\gamma\\asymp\\operatorname*{max}\\left\\{\\frac{C_{4}\\kappa^{2}\\mathsf{T r}(\\Sigma)}{\\|\\Sigma\\|_{2}},C_{4}\\kappa^{2}\\sqrt{\\frac{\\mathsf{T r}(\\Sigma)}{\\|\\Sigma\\|_{2}}}\\ln\\bigl(\\ln(N)/\\delta\\bigr),\\kappa\\ln\\bigl(\\ln(N)/\\delta\\bigr)^{2}\\right\\}a n d\\kappa=\\frac{\\|\\Sigma\\|_{2}}{\\lambda_{\\operatorname*{min}}(\\Sigma)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "To the best of our knowledge, [52, Corollary 4] is the only other streaming estimator for this problem with subgaussian-style concentration. Our result above significantly improves upon their rates of $\\begin{array}{r}{\\frac{\\|\\theta_{1}-\\theta^{*}\\|}{N+\\zeta}+\\frac{\\sigma}{\\lambda_{\\operatorname*{min}}\\left(\\Sigma\\right)}\\sqrt{\\frac{\\|\\Sigma\\|_{2}d\\ln\\left(1/\\delta\\right)}{N+\\zeta}}}\\end{array}$ with $\\zeta=C_{4}d\\kappa^{2}\\ln(1/\\delta)$ . Furthermore, our result is much closer to the optimal subgaussian rate and gracefully adapts to the stable rank or effective dimension [32], i.e., $d_{\\mathsf{e f f}}=\\mathsf{T r}(\\Sigma)/\\|\\Sigma\\|$ , therefore implying significant speedups over [52] in settings where $d_{\\mathsf{e f f}}\\ll d$ . ", "page_idx": 7}, {"type": "text", "text": "5.3 Streaming Heavy Tailed Logistic Regression ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Let $\\Xi=\\mathbb{R}^{d}\\times\\{0,1\\}$ and given a target parameter $\\theta^{*}\\in\\mathcal{C}$ , $P$ denote the following linear-logistic model: ", "page_idx": 7}, {"type": "equation", "text": "$$\n\\mathbf{x}\\sim Q,\\,\\mathbb{E}[\\mathbf{x}]=0,\\,\\mathbb{E}[\\mathbf{x}\\mathbf{x}^{T}]\\preceq\\Sigma;\\qquad y\\sim\\mathsf{B e r n o u l l i}(\\phi(\\langle\\theta^{*},\\mathbf{x}\\rangle))\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\phi(t)=(1+e^{-t})^{-1}$ . The covariates $\\mathbf{x}$ are heavy tailed, with only bounded second moments. The negative log likelihood of $y|\\mathbf x$ is given by $f(\\theta;\\dot{\\bf x},y)\\,=\\,\\ln(1+\\dot{\\exp}(\\langle{\\bf x},\\theta\\rangle))\\,-\\,y\\,\\langle{\\bf x},\\theta\\rangle$ . The objective of the logistic regression problem is to estimate $\\theta^{*}$ by minimizing the population-level negative log likelihood: ", "page_idx": 7}, {"type": "equation", "text": "$$\nF(\\theta)=\\mathbb{E}_{\\mathbf{x},y\\sim P}\\left[\\ln(1+\\exp(\\langle\\mathbf{x},\\theta\\rangle))-y\\left\\langle\\mathbf{x},\\theta\\right\\rangle\\right]\n$$", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "which is minimized at $\\theta^{*}$ . Here, the stochastic gradient oracle is $g(\\theta;\\mathbf{x},\\mathbf{y})=\\phi(\\langle\\mathbf{x},\\theta\\rangle)\\mathbf{x}-y\\mathbf{x}$ . The following result applies Theorem 3 to show that the output of clipped SGD attains near-subgaussian rates for heavy tailed logistic regression. We refer to Appendix G.3 for the proof. ", "page_idx": 7}, {"type": "text", "text": "Corollary 3 (Heavy Tailed Logistic Regression). Under the stochastic subgradient oracle described above, realized using $N\\gtrsim\\ln(\\ln(d))$ i.i.d samples from $P$ , the average iterate of Algorithm 1, when run under the parameter settings of Theorem $^{4}$ satisfies the following with probability at least $1-\\delta$ : ", "page_idx": 8}, {"type": "equation", "text": "$$\nF(\\hat{\\theta}_{N})-F(\\theta^{*})\\lesssim D_{1}\\sqrt{\\frac{\\mathsf{T r}(\\Sigma)+\\sqrt{\\|\\Sigma\\|_{2}}\\left(\\sqrt{\\mathsf{T r}(\\Sigma)}+\\|\\Sigma\\|_{2}D_{1}\\right)\\ln\\bigl(\\ln(N)/\\delta\\bigr)}{N}}+o_{N}(\\Sigma,D_{1})\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $o_{N}(\\Sigma,D_{1})$ represents terms that are lower order in $N$ (explicated in Appendix G.3 ", "page_idx": 8}, {"type": "text", "text": "Note that the standard analysis of SGD, with the assumption th\u221aat $\\|\\mathbf{x}\\|\\leq R$ almost surely leads to a bound of the form [4, Proposition 5]: F(\u03b8\u02c6N) \u2212F(\u03b8\u2217) \u2272RD1\u221alNog( \u03b41 ) ", "page_idx": 8}, {"type": "text", "text": "5.4 Streaming Heavy Tailed LAD Regression ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Let $\\Xi=\\mathbb{R}^{d}\\times\\mathbb{R}$ . Given a target parameter $\\theta^{*}\\in\\mathcal{C}$ , $P$ defines the following linear model: ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbf{x}\\sim Q,\\ \\mathbb{E}[\\mathbf{x}]=0,\\ \\mathbb{E}[\\mathbf{xx}^{T}]\\preceq\\Sigma;\\qquad y=\\langle\\mathbf{x},\\theta^{*}\\rangle+\\epsilon,\\ \\mathsf{M e d i a n}(\\epsilon|\\mathbf{x})=0\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "We allow both the covariate $\\mathbf{x}$ and target $y$ to be heavy tailed, assuming only bounded second moments for $\\mathbf{x}$ . We do not assume any moment bounds on $\\epsilon|\\mathbf x$ . The assumption $\\mathbb{E}[\\mathbf{x}]=0$ is made for the sake of clarity and can be straightforwardly relaxed. The Least Absolute Deviation (LAD) Regression problem involves estimating $\\theta$ by solving SCO with the sample loss $f(\\theta;\\mathbf{x},y)=|\\left\\langle\\mathbf{x},\\theta\\right\\rangle-y|$ . The stochastic subgradient oracle and population risk is given by: ", "page_idx": 8}, {"type": "equation", "text": "$$\ng(\\theta;\\mathbf{x},\\mathbf{y})=\\mathsf{s g n}(\\langle\\theta,\\mathbf{x}\\rangle-\\mathbf{y})\\mathbf{x},\\qquad F(\\theta)=\\mathbb{E}\\left[\\vert\\left\\langle\\theta-\\theta^{*},\\mathbf{x}\\right\\rangle-\\epsilon\\right]\\right]\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathsf{s g n}(t)=\\frac{t}{\\|t\\|}}\\end{array}$ for $t\\neq0$ and $\\mathsf{s g n}(0)=0$ . The following result, whose full statement and proof is presented in Appendix G.4, applies Theorem 4 to show that the average iterate of clipped SGD attains near-subgaussian rates for heavy tailed LAD regression. To the best of our knowledge, this is the first known streaming estimator for this problem. ", "page_idx": 8}, {"type": "text", "text": "Corollary 4 (Heavy Tailed LAD Regression). Under the stochastic subgradient oracle described above, realized using $N\\gtrsim\\ln(\\ln(d))$ i.i.d samples from $P$ , the average iterate of Algorithm $^{\\,I}$ , when run under the parameter settings of Theorem $^{4}$ satisfies the following with probability at least $1-\\delta$ : ", "page_idx": 8}, {"type": "equation", "text": "$$\nF(\\widehat{\\theta}_{N})-F(\\theta^{*})\\lesssim D_{1}\\sqrt{\\frac{\\mathsf{T r}(\\Sigma)+\\sqrt{\\|\\Sigma\\|_{2}\\mathsf{T r}(\\Sigma)\\ln(\\ln(N)/\\delta)}}{N}}+o_{N}(\\Sigma,D_{1})\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $O N$ denotes terms that are lower order in $N$ (explicated in Appendix G.4) ", "page_idx": 8}, {"type": "text", "text": "6 Improved Martingale Concentration via Iterative Refinement ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Our results are based on the following concentration result for $\\mathbb{R}^{d}$ valued martingales. The proof appears in Appendix F. Suppose $M_{t}$ for $t=0,\\dots,T$ is an $\\mathbb{R}^{d}$ valued martingale such that $M_{0}=0$ almost surely, the difference sequence $\\mathbf{v}_{t}:=M_{t}-M_{t-1}$ is such that $\\|\\mathbf{v}_{t}\\|\\leq\\Gamma$ and $\\mathbb{E}[\\mathbf{v}_{t}\\mathbf{v}_{t}^{\\mathsf{T}}|\\mathcal{F}_{t-1}]=$ $\\Sigma_{t}$ almost surely for every $t=1,\\dots,T$ for some $\\Gamma\\,>\\,0$ . Assume that there exist deterministic sequences $p_{1},\\ldots,p_{T}$ and $q_{1},\\dots,q_{T}$ such that ${\\sf T r}(\\Sigma_{t})\\leq q_{t}$ and $\\|\\Sigma_{t}\\|\\leq p_{t}$ almost surely. ", "page_idx": 8}, {"type": "text", "text": "Theorem 5. Let $\\begin{array}{r}{\\bar{q}:=\\frac{1}{T}\\sum_{t=1}^{T}q_{t}}\\end{array}$ and $\\begin{array}{r}{\\bar{p}:=\\frac{1}{T}\\sum_{t=1}^{T}p_{t}}\\end{array}$ . Then, for any $\\delta\\in(0,\\frac{1}{2})$ : ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\operatorname*{sup}_{t\\leq T}\\|M_{t}\\|\\geq g(T,\\delta){\\sqrt{T}})\\leq\\delta\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "Where $\\begin{array}{r}{g(T,\\delta)=C_{M}\\left[\\sqrt{\\bar{q}}+\\frac{\\bar{p}\\sqrt{T}}{\\Gamma}+\\frac{\\Gamma}{\\sqrt{T}}\\log(\\frac{K}{\\delta})\\right]}\\end{array}$ and $\\begin{array}{r}{K=\\ln\\ln((\\frac{\\sqrt{\\bar{q}T}}{\\Gamma}+1)\\log(d+1))+C_{M}}\\end{array}$ for some universal constant $C_{M}$ ", "page_idx": 8}, {"type": "text", "text": "To prove this result, \u221awe first use Freedman\u2019s inequality [20, 51] to obtain a coarse-grained $g_{0}$ such that $\\mathbb{P}(\\operatorname*{sup}_{t}\\|M_{t}\\|>g_{0}\\sqrt{T})\\le\\delta$ . We then iterativ\u221aely refine this inequality via a PAC Bayesian [8\u221a, 11, 12] argument to show that $\\mathbb{P}(\\operatorname*{sup}_{t}\\|M_{t}\\|>g_{k+1}\\sqrt{T}\\mid\\beta_{k})\\le\\delta$ , where $B_{k}=\\{\\operatorname*{sup}_{t}\\|M_{t}\\|\\leq g_{k}{\\sqrt{T}}\\}$ and $g_{k+1}^{2}\\lesssim\\mathsf{T r}(\\Sigma)+g_{k}\\sqrt{\\|\\Sigma_{2}\\|\\log(1/\\delta)}$ . This iterative refinement strategy, proved in Theorem 14 is one of the main technical contributions of our work, which could be of independent interest. We arrive at Theorem 5 after $K\\approx\\log\\log(T\\log d)$ refinement steps. ", "page_idx": 8}, {"type": "text", "text": "Remark Theorem 5 is used to control the influence of the fluctuations introduced by clipped SGD. To this end, let $\\mathbf{v}_{t}$ be the centered version of $\\mathsf{c l i p}_{\\Gamma}(\\mathbf{g}_{t})$ , ensuring $\\|\\mathbf{v}_{t}\\|\\leq2\\Gamma$ almost surely. Suppose $\\Sigma_{t}=\\Sigma$ for some fixed $\\Sigma$ and let $\\Gamma=\\sqrt{\\|\\Sigma\\|T\\big/\\mathrm{log}(\\frac{K}{\\delta})}$ . Then, with probability $1-\\delta$ : $\\mathrm{sup}_{t\\leq T}\\left\\|M_{t}\\right\\|\\lesssim$ $\\begin{array}{r}{\\sqrt{T\\mathsf{T r}(\\Sigma)+T\\|\\Sigma\\|\\log(\\frac{K}{\\delta})}}\\end{array}$ . This is sharper than the $\\begin{array}{r}{\\operatorname*{sup}_{t\\leq T}\\|M_{t}\\|\\lesssim\\sqrt{T\\mathsf{T r}(\\Sigma)\\log(\\frac{d}{\\delta})}}\\end{array}$ guarantee implied by the Matrix Freedman inequality [51, Corollary 1.6]. ", "page_idx": 9}, {"type": "text", "text": "7 Proof Sketch ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We sketch our proof technique for the case of smooth convex functions considered in 3. We consider the SGD iterations $\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{T}$ with clipped stochastic gradient at time $t$ denoted by $\\mathsf{c l i p}_{\\Gamma}(\\mathbf{g}_{t})=$ $\\nabla F({\\bf x}_{t})+{\\bf v}_{t}+{\\bf b}_{t}$ . Here, $\\mathbf{v}_{t}$ is the zero mean \u2018variance\u2019 such that $\\mathbb{E}[\\mathbf{v}_{t}|\\mathbf{x}_{t}]\\,=\\,0$ and $\\|\\mathbf{v}_{t}\\|\\leq2\\Gamma$ almost surely. ${\\bf b}_{t}$ is the non-zero mean \u2018bias\u2019 which arises due to clipping. Using the usual analysis of SGD for convex functions (see for instance [31]), we consider: ", "page_idx": 9}, {"type": "text", "text": "\u2225 $\\begin{array}{r}{\\mathbf x_{t+1}-\\mathbf x^{*}\\Vert^{2}\\leq\\Vert\\mathbf x_{t}-\\mathbf x^{*}\\Vert^{2}-2\\eta_{t}[F(\\mathbf x_{t})-F(\\mathbf x^{*})]-2\\eta_{t}\\langle\\mathbf v_{t}+\\mathbf b_{t},\\mathbf x_{t}-\\mathbf x^{*}\\rangle+\\eta_{t}^{2}\\Vert\\nabla F(\\mathbf x_{t})+\\mathbf v_{t}+\\mathbf b_{t}\\Vert^{2}}\\end{array}$ Considering constant step-sizes, we sum the inequalities for each $t$ to conclude: ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\displaystyle\\frac{1}{T}\\sum_{t=1}^{T}F(\\mathbf{x}_{t})-F(\\mathbf{x}^{*})\\leq\\frac{1}{2\\eta T}\\|\\mathbf{x}_{1}-\\mathbf{x}^{\\star}\\|^{2}+\\displaystyle\\frac{1}{T}\\sum_{t=1}^{T}\\langle\\mathbf{v}_{t}+\\mathbf{b}_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle}\\\\ {\\displaystyle+\\,\\frac{3\\eta}{2T}\\sum_{t}[\\|\\nabla F(\\mathbf{x}_{t})\\|^{2}+\\|\\mathbf{v}_{t}\\|^{2}+\\|\\mathbf{b}_{t}\\|^{2}]\\quad~~}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "The \u2019random\u2019 terms to bound compared to gradient descent here are $\\sum_{t}\\langle\\mathbf{v}_{t}+\\mathbf{b}_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle$ and $\\begin{array}{r}{\\sum_{t}\\|\\mathbf{v}_{t}\\|^{2}+\\|\\mathbf{b}_{t}\\|^{2}}\\end{array}$ Lemma 13 shows that $\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|\\leq2\\|\\mathbf{x}_{1}-\\mathbf{x}^{*}\\|$ with high probability. Under this event, we bound $\\textstyle\\sum_{t}\\langle\\mathbf{v}_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle$ using the standard Freeman\u2019s inequality and $\\|\\nabla F({\\bf x}_{t})\\|^{2}$ by using smoothness an d the fact that $\\dot{\\nabla}F(\\mathbf{x}^{\\star})\\,=\\,0$ . The bias of the estimator $\\|\\mathbf{b}_{t}\\|$ is bound using arguments similar to [8] (see Lemma 4). The main improvement of our method is given by our method of bounding $\\frac{1}{T}\\sum_{t}\\|\\mathbf{v}_{t}\\|^{2}$ . We show by an application of Theorem 5 that $\\frac{1}{T}\\sum_{t}\\|\\mathbf{v}_{t}\\|^{2}\\lesssim$ $\\begin{array}{r}{\\mathsf{T r}(\\Sigma)+\\sqrt{\\mathsf{T r}(\\Sigma)\\|\\Sigma\\|_{2}}\\log(\\frac{\\log T}{\\delta})}\\end{array}$ with probability at-least $1-\\delta$ whenever the clipping factor $\\Gamma$ is appropriately chosen. Choosing the step size $\\eta$ appropriately gives us the result in Theorem 3. ", "page_idx": 9}, {"type": "text", "text": "8 Conclusion and Limitations ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Our work obtained nearly subgaussian rates for heavy-tailed SCO using clipped SGD by developing a fine-grained iterative refinement strategy for martingale concentration. As corollaries, we obtained state-of-the-art streaming estimators for various heavy tailed statistical problems. We note Clipped-SGD is widely used to optimize neural networks with highly nonconvex landscapes, which is currently outside the scope of our work. Nevertheless, we believe our techniques could be useful for providing sharp high-probability guarantees for non-convex losses. Our bounds are currently of the form d+ d lnT(ln(T )/\u03b4), which is suboptimal compared to the tight subgaussian rate of $\\sqrt{\\frac{d+\\ln(1/\\delta)}{T}}$ . Further research is required to understand if it is possible to obtain truly subgaussian rates with clipped mean type estimators. Another notable suboptimality of our result is the $\\ln(\\ln(T)/\\delta)$ dependence on the confidence level (as opposed to the typical $\\ln(1/\\delta)$ scaling). However, this is not a major drawback as our results continue to significantly outperform prior works unless $T\\gg e^{\\exp(\\sqrt{d}-1)\\ln(1/\\delta)}$ (which is an impractical regime). This drawback arises due to the $\\ln(\\ln(T))$ iterations of our iterative refinement technique and we believe it can be removed via more sophisticated martingale concentration arguments. Our work lays the foundation for several interesting avenues for future work including the analysis of heavy tailed statistical estimation under bounded $p^{\\mathrm{th}}$ moment assumptions (for $p<2.$ ) and the development of parameter free statistical estimators that do not require knowledge of problem-dependent parameter such as $\\|\\Sigma\\|,\\delta$ etc. (or their respective upper bounds). Deriving anytime valid guarantees for clipped SGD using our techniques is also an interesting future direction. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer and communications security, pages 308\u2013318, 2016.   \n[2] N. Agarwal, S. Chaudhuri, P. Jain, D. M. Nagaraj, and P. Netrapalli. Online target q-learning with reverse experience replay: Efficiently finding the optimal policy for linear mdps. In International Conference on Learning Representations, 2021.   \n[3] A. Ajalloeian and S. U. Stich. On the convergence of sgd with biased gradients. arXiv preprint arXiv:2008.00051, 2020.   \n[4] F. Bach. Adaptivity of averaged stochastic gradient descent to local strong convexity for logistic regression. The Journal of Machine Learning Research, 15(1):595\u2013627, 2014.   \n[5] A. Bakshi and A. Prasad. Robust linear regression: Optimal rates in polynomial time. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, pages 102\u2013115, 2021.   \n[6] S. Bubeck. Convex optimization: Algorithms and complexity. 2014. doi: 10.48550/ARXIV. 1405.4980. URL https://arxiv.org/abs/1405.4980.   \n[7] O. Catoni. Statistical learning theory and stochastic optimization: Ecole d\u2019Et\u00e9 de Probabilit\u00e9s de Saint-Flour, XXXI-2001, volume 1851. Springer Science & Business Media, 2004.   \n[8] O. Catoni and I. Giulini. Dimension-free pac-bayesian bounds for the estimation of the mean of a random vector. arXiv preprint arXiv:1802.04308, 2018.   \n[9] Y. Cherapanamjeri, N. Flammarion, and P. L. Bartlett. Fast mean estimation with sub-gaussian rates. In Conference on Learning Theory, pages 786\u2013806. PMLR, 2019.   \n[10] Y. Cherapanamjeri, S. B. Hopkins, T. Kathuria, P. Raghavendra, and N. Tripuraneni. Algorithms for heavy-tailed statistics: Regression, covariance estimation, and beyond. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, pages 601\u2013609, 2020.   \n[11] B. Chugg, H. Wang, and A. Ramdas. Time-uniform confidence spheres for means of random vectors. arXiv preprint arXiv:2311.08168, 2023.   \n[12] B. Chugg, H. Wang, and A. Ramdas. A unified recipe for deriving (time-uniform) pac-bayes bounds. Journal of Machine Learning Research, 24(372):1\u201361, 2023.   \n[13] A. Cutkosky and H. Mehta. High-probability bounds for non-convex stochastic optimization with heavy tails. Advances in Neural Information Processing Systems, 34:4883\u20134895, 2021.   \n[14] D. Davis, D. Drusvyatskiy, L. Xiao, and J. Zhang. From low probability to high confidence in stochastic convex optimization. Journal of machine learning research, 22(49), 2021.   \n[15] J. Depersin and G. Lecu\u00e9. Robust sub-gaussian estimation of a mean vector in nearly linear time. The Annals of Statistics, 50(1):511\u2013536, 2022.   \n[16] I. Diakonikolas and D. M. Kane. Algorithmic high-dimensional robust statistics. Cambridge university press, 2023.   \n[17] I. Diakonikolas, G. Kamath, D. Kane, J. Li, J. Steinhardt, and A. Stewart. Sever: A robust meta-algorithm for stochastic optimization. In International Conference on Machine Learning, pages 1596\u20131606. PMLR, 2019.   \n[18] I. Diakonikolas, D. M. Kane, A. Pensia, and T. Pittas. Streaming algorithms for high-dimensional robust statistics. arXiv preprint arXiv:2204.12399, 2022.   \n[19] J. Fan, W. Wang, and Z. Zhu. A shrinkage principle for heavy-tailed data: High-dimensional robust low-rank matrix recovery. Annals of statistics, 49(3):1239, 2021.   \n[20] D. A. Freedman. On tail probabilities for martingales. the Annals of Probability, pages 100\u2013118, 1975.   \n[21] E. Gorbunov, M. Danilova, and A. Gasnikov. Stochastic optimization with heavy-tailed noise via accelerated gradient clipping. Advances in Neural Information Processing Systems, 33: 15042\u201315053, 2020.   \n[22] R. M. Gower, N. Loizou, X. Qian, A. Sailanbayev, E. Shulgin, and P. Richt\u00e1rik. Sgd: General analysis and improved rates. In Proceedings of the 36th International Conference on Machine Learning, pages 5200\u20135209. PMLR, 2019.   \n[23] M. Gurbuzbalaban, U. Simsekli, and L. Zhu. The heavy-tail phenomenon in sgd. In International Conference on Machine Learning, pages 3964\u20133975. PMLR, 2021.   \n[24] F. R. Hampel, E. M. Ronchetti, P. Rousseeuw, and W. A. Stahel. Robust statistics: the approach based on influence functions. Wiley-Interscience; New York, 1986.   \n[25] N. J. Harvey, C. Liaw, Y. Plan, and S. Randhawa. Tight analyses for non-smooth stochastic gradient descent. In Conference on Learning Theory, pages 1579\u20131613. PMLR, 2019.   \n[26] E. Hazan and S. Kale. Beyond the regret minimization barrier: Optimal algorithms for stochastic strongly-convex optimization. Journal of Machine Learning Research, 15:2489\u20132512, 2014.   \n[27] S. Hopkins, J. Li, and F. Zhang. Robust and heavy-tailed mean estimation made simple, via regret minimization. Advances in Neural Information Processing Systems, 33:11902\u201311912, 2020.   \n[28] S. B. Hopkins. Mean estimation with sub-gaussian rates in polynomial time. The Annals of Statistics, 48(2):1193\u20131213, 2020.   \n[29] D. Hsu and S. Sabato. Loss minimization and parameter estimation with heavy tails. The Journal of Machine Learning Research, 17(1):543\u2013582, 2016.   \n[30] P. J. Huber. Robust estimation of a location parameter. Ann. Math. Statist., 35(4):73\u2013101, 1964.   \n[31] P. Jain, D. M. Nagaraj, and P. Netrapalli. Making the last iterate of sgd information theoretically optimal. SIAM Journal on Optimization, 31(2):1108\u20131130, 2021.   \n[32] J. C. Lee and P. Valiant. Optimal sub-gaussian mean estimation in very high dimensions. In 13th Innovations in Theoretical Computer Science Conference (ITCS 2022). Schloss-DagstuhlLeibniz Zentrum f\u00fcr Informatik, 2022.   \n[33] X. Li and Q. Sun. Variance-aware decision making with linear function approximation under heavy-tailed rewards. Transactions on Machine Learning Research.   \n[34] Z. Liu and Z. Zhou. Stochastic nonsmooth convex optimization with heavy-tailed noises. arXiv preprint arXiv:2303.12277, 2023.   \n[35] Z. Liu, T. D. Nguyen, T. H. Nguyen, A. Ene, and H. Nguyen. High probability convergence of stochastic gradient methods. In International Conference on Machine Learning, pages 21884\u201321914. PMLR, 2023.   \n[36] G. Lugosi and S. Mendelson. Mean estimation and regression under heavy-tailed distributions: A survey. Foundations of Computational Mathematics, 19(5):1145\u20131190, 2019.   \n[37] G. Lugosi and S. Mendelson. Sub-gaussian estimators of the mean of a random vector. The annals of statistics, 47(2):783\u2013794, 2019.   \n[38] V. V. Mai and M. Johansson. Stability and convergence of stochastic gradient clipping: Beyond lipschitz continuity and smoothness. In International Conference on Machine Learning, pages 7325\u20137335. PMLR, 2021.   \n[39] S. Minsker. Geometric median and robust estimation in banach spaces. Bernoulli, 21(4): 2308\u20132335, 2015.   \n[40] A. V. Nazin, A. S. Nemirovsky, A. B. Tsybakov, and A. B. Juditsky. Algorithms of robust stochastic optimization based on mirror descent method. Automation and Remote Control, 80 (9):1607\u20131627, 2019.   \n[41] T. D. Nguyen, T. H. Nguyen, A. Ene, and H. Nguyen. Improved convergence in high probability of clipped gradient methods with heavy tailed noise. Advances in Neural Information Processing Systems, 36:24191\u201324222, 2023.   \n[42] R. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty of training recurrent neural networks. In International conference on machine learning, pages 1310\u20131318. Pmlr, 2013.   \n[43] A. Pensia, V. Jog, and P.-L. Loh. Robust regression with covariate filtering: Heavy tails and adversarial contamination. arXiv preprint arXiv:2009.12976, 2020.   \n[44] A. Prasad, A. S. Suggala, S. Balakrishnan, and P. Ravikumar. Robust estimation via robust gradient estimation. arXiv preprint arXiv:1802.06485, 2018.   \n[45] A. Prasad, S. Balakrishnan, and P. Ravikumar. A robust univariate mean estimator is all you need. In International Conference on Artificial Intelligence and Statistics, pages 4034\u20134044. PMLR, 2020.   \n[46] N. Puchkin, E. Gorbunov, N. Kutuzov, and A. Gasnikov. Breaking the heavy-tailed noise barrier in stochastic optimization problems. In International Conference on Artificial Intelligence and Statistics, pages 856\u2013864. PMLR, 2024.   \n[47] A. Rakhlin, O. Shamir, and K. Sridharan. Making gradient descent optimal for strongly convex stochastic optimization. In Proceedings of the 29th International Coference on International Conference on Machine Learning, pages 1571\u20131578, 2012.   \n[48] A. Sadiev, M. Danilova, E. Gorbunov, S. Horv\u00e1th, G. Gidel, P. Dvurechensky, A. Gasnikov, and P. Richt\u00e1rik. High-probability bounds for stochastic optimization and variational inequalities: the case of unbounded variance. In International Conference on Machine Learning, pages 29563\u201329648. PMLR, 2023.   \n[49] V. Srinivasan, A. Prasad, S. Balakrishnan, and P. K. Ravikumar. Efficient estimators for heavy-tailed machine learning. 2020.   \n[50] Q. Sun, W.-X. Zhou, and J. Fan. Adaptive huber regression. Journal of the American Statistical Association, 115(529):254\u2013265, 2020.   \n[51] J. Tropp. Freedman\u2019s inequality for matrix martingales. Electronic Communications in Probability, 16(none):262 \u2013 270, 2011. doi: 10.1214/ECP.v16-1624. URL https://doi.org/10. 1214/ECP.v16-1624.   \n[52] C.-P. Tsai, A. Prasad, S. Balakrishnan, and P. Ravikumar. Heavy-tailed streaming statistical estimation. In International Conference on Artificial Intelligence and Statistics, pages 1251\u2013 1282. PMLR, 2022.   \n[53] J. W. Tukey. Mathematics and the picturing of data. In Proceedings of the International Congress of Mathematicians, Vancouver, 1975, volume 2, pages 523\u2013531, 1975.   \n[54] R. Vershynin. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press, 2018.   \n[55] H. Wang and A. Ramdas. Catoni-style confidence sequences for heavy-tailed mean estimation. Stochastic Processes and Their Applications, 163:168\u2013202, 2023.   \n[56] J. Zhang, S. P. Karimireddy, A. Veit, S. Kim, S. Reddi, S. Kumar, and S. Sra. Why are adaptive methods good for attention models? Advances in Neural Information Processing Systems, 33: 15383\u201315393, 2020.   \n[57] W.-X. Zhou, K. Bose, J. Fan, and H. Liu. A new perspective on robust m-estimation: Finite sample theory and applications to dependence-adjusted multiple testing. Annals of statistics, 46 (5):1904, 2018. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "1.1 Sub-Gaussian Error Guarantees for Statistical Estimation 2   \n1.2 Related Work 3   \n1.3 Contributions 4 ", "page_idx": 13}, {"type": "text", "text": "2 Notation and Organization 4 ", "page_idx": 13}, {"type": "text", "text": "3 Background and Problem Formulation 4 ", "page_idx": 13}, {"type": "text", "text": "4 Results 5 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "4.1 Smooth Strongly Convex Objectives 6   \n4.2 Beyond Strongly Convex Objectives 6 ", "page_idx": 13}, {"type": "text", "text": "5 Applications to Streaming Heavy Tailed Statistical Estimation 7 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "5.1 Streaming Heavy-Tailed Mean Estimation 7   \n5.2 Streaming Heavy Tailed Linear Regression 8   \n5.3 Streaming Heavy Tailed Logistic Regression . . . . 8   \n5.4 Streaming Heavy Tailed LAD Regression . . . 9 ", "page_idx": 13}, {"type": "text", "text": "6 Improved Martingale Concentration via Iterative Refinement 9 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "7 Proof Sketch 10 ", "page_idx": 13}, {"type": "text", "text": "8 Conclusion and Limitations 10 ", "page_idx": 13}, {"type": "text", "text": "A Preliminaries 16 ", "page_idx": 13}, {"type": "text", "text": "B Analysis for Smooth Strongly Convex Functions 18 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "B.1 Proof of Theorem 1 19   \nB.2 Proof of Lemma 5 . 23   \nB.3 Proof of Lemma 6 . 24   \nB.4 Proof of Lemma 7 . 25 ", "page_idx": 13}, {"type": "text", "text": "C Analysis for Smooth Strongly Convex Functions Under Quadratic Growth Noise Model 29 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "C.1 Proof of Theorem 2 30   \nC.2 Proof of Lemma 8 . 34   \nC.3 Proof of Lemma 9 . 36 ", "page_idx": 13}, {"type": "text", "text": "D Analysis for Smooth Convex Functions 41 ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "D.1 Proof of Theorem 7 43   \nD.2 Proof of Lemma 10 43   \nD.3 Proof of Lemma 11 44   \nD.4 Proof of Lemma 12 44   \nD.5 Proof of Lemma 13 45 ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "E Analysis for Lipschitz Convex Functions 46 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "E.1 Proof of Lemma 14 48   \nE.2 Proof of Lemma 15 49   \nE.3 Proof of Lemma 16 49   \nE.4 Proof of Lemma 17 49 ", "page_idx": 14}, {"type": "text", "text": "F Improved Martingale Concentration via PAC Bayes Theory 50 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "F.1 Proof of Theorem 9 51   \nF.2 Proof of Theorem 10 55   \nF.3 Proof of Corollary 5 . . 56 ", "page_idx": 14}, {"type": "text", "text": "G Applications to Streaming Heavy Tailed Statistical Estimation 56 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "G.1 Streaming Heavy Tailed Mean Estimation $:$ Proof of Corollary 1 . 56   \nG.2 Streaming Heavy Tailed Linear Regression $:$ Proof of Corollary 2 . . . 57   \nG.3 Heavy Tailed Streaming Logistic Regression $:$ Proof of Corollary 3 . . . . 59   \nG.4 Proof of Corollary 4 . . . 61 ", "page_idx": 14}, {"type": "text", "text": "A Preliminaries ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "In this section, we collect some preliminary concentration results which will be used in the future sections. For the following lemma, we refer to Exercise 2.8.5 in [54]. ", "page_idx": 15}, {"type": "text", "text": "Lemma 1. Suppose $X$ is a real valued random variable such that $|X|\\leq\\Gamma$ almost surely, $\\mathbb{E}X=0$ and $\\mathbb{E}X^{2}=\\nu$ . Then, for any $\\lambda\\in\\mathbb R$ such that $|\\lambda|\\leq\\frac{1}{2\\Gamma}$ , the following holds: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\exp(\\lambda X)\\leq\\exp(\\lambda^{2}\\nu)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Consider a $\\mathbb{R}^{d}$ valued martingale $(M_{t})_{t=0}^{T}$ with respect to the filtration $(\\mathcal{F}_{t})_{t=0}^{T}$ such that $M_{0}=0$ almost surely. We consider the martingale difference sequence $\\mathbf{v}_{t}:=M_{t}-M_{t-1}$ for $t\\geq1$ . Clearly, we must have: ", "page_idx": 15}, {"type": "equation", "text": "$$\nM_{t}=\\sum_{s=1}^{t}\\mathbf{v}_{s}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Definition 1. We say that the martingale $M_{t}$ satisfies $(g,T,\\delta)$ uniform concentration $i f$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\operatorname*{sup}_{0\\leq t\\leq T}\\|M_{t}\\|>g{\\sqrt{T}})\\leq\\delta\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Assume that for fixed $\\Gamma>0$ and $\\Sigma\\in\\mathbb{R}^{d\\times d}$ that $\\|\\mathbf{v}_{s}\\|\\leq\\Gamma$ almost surely and $\\mathbb{E}[\\mathbf{v}_{t}\\mathbf{v}_{t}^{\\top}|\\mathcal{F}_{t-1}]=:\\Sigma_{t}$ . Suppose ${\\sf T r}(\\Sigma_{t})\\,\\leq\\,q_{t}$ and $\\|\\Sigma_{t}\\|_{2}\\leq\\,p_{t}$ almost surely for some non-random constants $p_{t},q_{t}$ . We state a high dimensional version of Freedman\u2019s inequality [20, 51] below which follows from From Corollary 1.3 of [51], we have ", "page_idx": 15}, {"type": "text", "text": "Theorem 6. Suppose $M_{t}$ satisfies the assumptions above. Let $\\begin{array}{r}{\\bar{q}:=\\frac{1}{T}\\sum_{s=1}^{T}q_{t}}\\end{array}$ the following is true: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\operatorname*{sup}_{0\\leq t\\leq T}\\|M_{t}\\|>\\alpha)\\leq(d+1)\\exp(-\\frac{\\alpha^{2}/2}{\\bar{q}T+\\frac{\\Gamma\\alpha}{3}})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "That is, for any $\\delta>0$ , the martingale $(M_{t})_{t\\leq T}$ obeys $(g_{0}(\\delta),T,\\delta)$ uniform concentration, where $\\begin{array}{r}{g_{0}(\\delta)=\\frac{2\\Gamma}{3\\sqrt{T}}\\log(\\frac{d+1}{\\delta})+\\sqrt{2\\bar{q}\\log(\\frac{d+1}{\\delta})}}\\end{array}$ ", "page_idx": 15}, {"type": "text", "text": "The following inequality is a corollary of Theorem 6. ", "page_idx": 15}, {"type": "text", "text": "Lemma 2. Let $g_{t}\\in\\mathbb{R}^{d}$ be $\\mathcal{F}_{t-1}$ measurable. Then for some constant $c_{1}>0$ , we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(\\cup_{t=1}^{T}\\{\\big|\\sum_{s=1}^{t}\\langle g_{s},\\mathbf{v}_{s}\\rangle\\big|\\ge\\alpha\\big\\}\\cap_{s\\le t}\\{\\|g_{s}\\|\\le A_{s}\\}\\big)\\le2\\exp(-\\frac{\\alpha^{2}}{\\Gamma A\\alpha+c_{1}\\sum_{t=1}^{T}p_{t}A_{t}^{2}})\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Where $A=\\operatorname*{sup}_{1\\leq t\\leq T}A_{t}$ ", "page_idx": 15}, {"type": "text", "text": "In addition, we also use the following scalar version of Freedman\u2019s inequality ", "page_idx": 15}, {"type": "text", "text": "Lemma 3 (Freedman\u2019s Inequality). Let $h_{1},h_{2},\\ldots,h_{T}$ be a $\\mathcal{F}_{t}$ adapted martingale difference sequence such that $\\mathbb E[h_{t}|\\mathcal F_{t-1}]=\\dot{0}$ , $\\mathbb{E}[h_{t}^{2}|\\mathcal{F}_{t-1}]=\\sigma_{t}^{2}$ and $\\|h_{t}\\|\\leq\\tau$ . Then, for any $\\delta\\in(0,1)$ , the following holds with probability at least $1-\\delta$ : ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t}h_{s}\\leq2\\sqrt{\\ln(1/\\delta)\\sum_{s=1}^{t}\\sigma_{s}^{2}}+2\\tau\\ln(1/\\delta)\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "The following lemma, which bounds the moments of a clipped random vector, is crucial to our analysis of the bias and variance of the clipped stochastic gradient. ", "page_idx": 15}, {"type": "text", "text": "Lemma 4 (Moments of a Clipped Random Vector). Let $\\mathbf{z}\\in\\mathbb{R}^{d}$ be a random vector sampled from the distribution $P$ with mean m and covariance matrix S. For any $\\Gamma>0$ , let $\\tilde{\\mathbf{z}}=\\mathsf{c l i p}_{\\Gamma}(\\mathbf{z})$ , and let m\u02dc and $\\tilde{\\mathbf{S}}$ denote the mean and covariance of $\\tilde{\\mathbf{z}}$ respectively, i.e., $\\tilde{\\mathbf{m}}=\\mathbb{E}[\\tilde{\\mathbf{z}}]$ and $\\tilde{\\mathbf{S}}=\\mathbb{E}\\left[(\\tilde{\\mathbf{z}}-\\tilde{\\mathbf{m}})\\,(\\tilde{\\mathbf{z}}-\\tilde{\\mathbf{m}})^{T}\\right]$ . Then, the following hold: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|\\tilde{\\mathbf{m}}-\\mathbf{m}\\|\\leq\\frac{\\sqrt{\\|\\mathbf{S}\\|_{2}}}{\\Gamma}\\left(\\|\\mathbf{m}\\|+\\sqrt{\\mathsf{T r}(\\mathbf{S})}\\right)+\\frac{\\|\\mathbf{m}\\|}{\\Gamma^{2}}\\left(\\|\\mathbf{m}\\|^{2}+\\mathsf{T r}(\\mathbf{S})\\right)}\\\\ {\\displaystyle\\|\\tilde{\\mathbf{S}}\\|_{2}\\leq\\|\\mathbf{S}\\|_{2}+\\frac{\\|\\mathbf{m}\\|^{2}}{\\Gamma^{2}}\\left(\\|\\mathbf{m}\\|^{2}+\\mathsf{T r}(\\mathbf{S})\\right)}\\\\ {\\displaystyle\\mathsf{T r}(\\tilde{\\mathbf{S}})\\leq\\mathsf{T r}(\\mathbf{S})}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Proof. The proof of this lemma uses arguments similar to that of Catoni and Giulini [8]. We first note that for any $\\mathbf{x}\\in\\mathbb{R}^{d}$ ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathsf{c l i p}_{\\Gamma}(\\mathbf{x})=\\mathbf{x}\\cdot\\frac{\\operatorname*{min}\\{1,\\Gamma^{-1}\\|\\mathbf{x}\\|\\}}{\\Gamma^{-1}\\|\\mathbf{x}\\|}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Following the proof of Proposition 2.1 of Catoni and Giulini [8], we observe that for any $t>0$ : ", "page_idx": 16}, {"type": "equation", "text": "$$\n0\\leq1-{\\frac{\\operatorname*{min}\\{1,t\\}}{t}}\\leq\\operatorname*{inf}_{p\\geq1}{\\frac{p^{p}t^{p}}{(p+1)^{p+1}}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Define $\\begin{array}{r}{\\theta(\\mathbf{x})=\\frac{\\operatorname*{min}\\left\\{1,\\Gamma^{-1}\\|\\mathbf{x}\\|\\right\\}}{\\Gamma^{-1}\\|\\mathbf{x}\\|}\\,\\forall\\mathbf{x}\\in\\mathbb{R}^{d}}\\end{array}$ . Note that $\\mathsf{c l i p}_{\\Gamma}(\\mathbf{x})=\\theta(\\mathbf{x})\\cdot\\mathbf{x}$ . From the above inequality, we note that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n0\\leq1-\\theta(\\mathbf{x})\\leq\\operatorname*{inf}_{p\\geq1}\\frac{p^{p}}{(p+1)^{p+1}}\\cdot\\frac{\\|\\mathbf{x}\\|^{p}}{\\Gamma^{p}}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Consider any unit vector $\\mathbf{e}\\in\\mathbb{R}^{d}$ . Then, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\mathbf e,\\mathbf m-\\tilde{\\mathbf m}\\rangle=\\mathbb{E}\\left[\\langle\\mathbf e,\\mathbf z-\\tilde{\\mathbf z}\\rangle\\right]}\\\\ &{\\qquad\\qquad=\\mathbb{E}\\left[\\langle\\mathbf e,\\mathbf z-\\theta(\\mathbf z)\\mathbf z\\rangle\\right]}\\\\ &{\\qquad\\qquad=\\mathbb{E}\\left[(1-\\theta(\\mathbf z))\\left\\langle\\mathbf e,\\mathbf z-\\mathbf m\\right\\rangle\\right]+\\left\\langle\\mathbf e,\\mathbf m\\right\\rangle\\mathbb{E}\\left[(1-\\theta(\\mathbf z))\\right]}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}\\left[(1-\\theta(\\mathbf z))\\right]\\left\\langle\\mathbf e,\\mathbf z-\\mathbf m\\right\\rangle\\left\\vert\\right]+\\left\\Vert\\mathbf m\\right\\Vert\\mathbb{E}\\left[(1-\\theta(\\mathbf z))\\right]}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}\\left[\\operatorname*{inf}_{p\\geq1}\\frac{p^{p}}{\\left(p+1\\right)^{p+1}}\\cdot\\frac{\\left\\Vert\\mathbf z\\right\\Vert^{p}\\left\\vert\\left\\langle\\mathbf e,\\mathbf z-\\mathbf m\\right\\rangle\\right\\vert}{\\Gamma^{p}}\\right]+\\left\\Vert\\mathbf m\\right\\Vert\\mathbb{E}\\left[\\operatorname*{inf}_{p\\geq1}\\frac{p^{p}}{\\left(p+1\\right)^{p+1}}\\cdot\\frac{\\left\\Vert\\mathbf z\\right\\Vert^{p}}{\\Gamma^{p}}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the second step uses the definition of $\\theta(\\mathbf{z})$ and the last step uses equation (5). Now, substituting $p=1$ and $p=2$ in the first and second terms of the RHS respectively, we obtain the following: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\langle\\mathbf e,\\mathbf m-\\tilde{\\mathbf m}\\rangle\\leq\\displaystyle\\frac1\\Gamma\\mathbb{E}\\left[\\|\\mathbf z\\|\\left\\langle\\mathbf e,\\mathbf z-\\mathbf m\\right\\rangle\\right]+\\frac{\\|\\mathbf m\\|}{\\Gamma^{2}}\\mathbb{E}[\\|\\mathbf z\\|^{2}]}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\displaystyle\\frac1{\\Gamma}\\sqrt{\\mathbb{E}[\\|\\mathbf x\\|^{2}]}\\sqrt{\\mathbb{E}[\\langle\\mathbf e,\\mathbf z-\\mathbf m\\rangle^{2}]}+\\frac{\\|\\mathbf m\\|}{\\Gamma^{2}}\\mathbb{E}[\\|\\mathbf z\\|^{2}]}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\frac{\\sqrt{\\|\\mathbf S\\|}}{\\Gamma}\\cdot\\sqrt{\\|\\mathbf m\\|^{2}+\\mathsf{T r}(\\mathbf S)}+\\frac{\\|\\mathbf m\\|}{\\Gamma^{2}}\\left(\\|\\mathbf m\\|^{2}+\\mathsf{T r}(\\mathbf S)\\right)}\\\\ &{\\quad\\quad\\quad\\leq\\frac{\\sqrt{\\|\\mathbf S\\|_{2}}}{\\Gamma}\\left(\\|\\mathbf m\\|+\\sqrt{\\mathsf{T r}(\\mathbf S)}\\right)+\\frac{\\|\\mathbf m\\|}{\\Gamma^{2}}\\left(\\|\\mathbf m\\|^{2}+\\mathsf{T r}(\\mathbf S)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the second step uses the Cauchy Schwarz inequality and the last step uses the subadditivity of the square root. It follows that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|\\tilde{\\mathbf{m}}-\\mathbf{m}\\|=\\displaystyle\\operatorname*{sup}_{\\|\\mathbf{e}\\|=1}\\left\\langle\\mathbf{e},\\mathbf{m}-\\tilde{\\mathbf{m}}\\right\\rangle}\\\\ {\\displaystyle\\leq\\frac{\\sqrt{\\|\\mathbf{S}\\|_{2}}}{\\Gamma}\\left(\\|\\mathbf{m}\\|+\\sqrt{\\mathsf{T}\\mathsf{r}(\\mathbf{S})}\\right)+\\frac{\\|\\mathbf{m}\\|}{\\Gamma^{2}}\\left(\\|\\mathbf{m}\\|^{2}+\\mathsf{T}\\mathsf{r}(\\mathbf{S})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "To bound $\\|\\tilde{\\mathbf{S}}\\|$ , we first note that for any $\\mathbf{x}\\in\\mathbb{R}^{d}$ , $0\\leq\\theta(\\mathbf{x})\\leq1$ . As before, let $\\mathbf{e}\\in\\mathbb{R}^{d}$ denote an arbitrary unit vector. We note that $\\mathbb{E}[\\langle\\mathbf{e},\\tilde{\\mathbf{z}}-\\mathbf{m}\\rangle^{2}]\\;=\\;\\mathbb{E}[\\langle\\mathbf{e},\\tilde{\\mathbf{z}}-\\tilde{\\mathbf{m}}\\rangle^{2}]\\,+\\,\\mathbb{E}[\\langle\\mathbf{e},\\mathbf{m}-\\tilde{\\mathbf{m}}\\rangle^{2}]\\;\\ge$ $\\mathbb{E}[\\langle\\mathbf{e},\\tilde{\\mathbf{z}}-\\tilde{\\mathbf{m}}\\rangle^{2}]$ . Hence, it follows that, ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\left[\\left\\langle\\mathbf{e},\\bar{\\mathbf{z}}-\\bar{\\mathbf{m}}\\right\\rangle^{2}\\right]\\leq\\mathbb{E}\\left[\\left\\langle\\mathbf{e},\\bar{\\mathbf{z}}-\\mathbf{m}\\right\\rangle^{2}\\right]}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}\\left[\\left\\langle\\theta(\\mathbf{z})\\left\\langle\\mathbf{e},\\mathbf{z}\\right\\rangle-\\left\\langle\\mathbf{e},\\mathbf{m}\\right\\rangle\\right\\rangle^{2}\\right]}\\\\ &{\\qquad=\\mathbb{E}\\left[\\left\\langle\\theta(\\mathbf{z})\\left\\langle\\mathbf{e},\\mathbf{z}-\\mathbf{m}\\right\\rangle-\\left(1-\\theta(\\mathbf{z})\\right)\\left\\langle\\mathbf{e},\\mathbf{m}\\right\\rangle\\right)^{2}\\right]}\\\\ &{\\qquad\\leq\\mathbb{E}\\left[\\theta(\\mathbf{z})\\left\\langle\\mathbf{e},\\mathbf{z}-\\mathbf{m}\\right\\rangle^{2}\\right]+\\left\\langle\\mathbf{e},\\mathbf{m}^{2}\\right\\rangle\\mathbb{E}\\left[\\left\\langle1-\\theta(\\mathbf{z})\\right\\rangle\\right]}\\\\ &{\\qquad\\leq\\mathbb{E}\\left[\\left\\langle\\mathbf{e},\\mathbf{z}-\\mathbf{m}\\right\\rangle^{2}\\right]+\\left\\|\\mathbf{m}\\right\\|^{2}\\mathbb{E}\\left[\\frac{\\mathrm{i}\\gamma}{p\\geq1}\\left\\langle\\frac{p^{\\theta}}{p+1}\\right\\rangle\\frac{\\left\\|\\mathbf{z}\\right\\|^{p}}{\\Gamma^{p}}\\right]}\\\\ &{\\qquad\\leq\\left\\|\\mathbf{S}\\right\\|_{2}+\\frac{\\left\\|\\mathbf{m}\\right\\|^{2}\\mathbb{E}\\left[\\left\\|\\mathbf{z}\\right\\|^{2}\\right]}{\\Gamma^{2}}}\\\\ &{\\qquad\\leq\\left\\|\\mathbf{S}\\right\\|_{2}+\\frac{\\left\\|\\mathbf{m}\\right\\|^{2}}{\\Gamma^{2}}\\left(\\left\\|\\mathbf{m}\\right\\|^{2}+\\mathcal{T}(\\mathbf{s})\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where the fourth step uses Jensen\u2019s inequality by noting that $0\\leq\\theta(\\mathbf{z})\\leq1$ and the fifth step uses equation (5). ", "page_idx": 17}, {"type": "text", "text": "Finally, To upper bound ${\\sf T r}(\\tilde{\\bf S})$ , we note that ${\\mathsf{c l i p}}_{\\Gamma}$ is a contractive mapping as it is the projection operator onto a convex set (namely the ball of radius $\\Gamma$ in $\\mathbb{R}^{d}$ centered at the origin). To this end, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\sf T r(\\tilde{S})}=\\mathbb{E}\\left[\\|\\tilde{\\mathbf{z}}-\\tilde{\\mathbf{m}}\\|^{2}\\right]=\\frac{1}{2}\\mathbb{E}_{\\mathbf{z}_{1},\\mathbf{z}_{2}}{\\bf\\sf}_{\\sim}^{i,i,d;}P\\left[\\|\\mathbf{c}\\|\\mathfrak{p}_{\\Gamma}(\\mathbf{z}_{1})-\\sf c\\|\\mathfrak{p}_{\\Gamma}(\\mathbf{z}_{2})\\|^{2}\\right]}\\\\ {\\mathrm{~}}\\\\ {{\\boldsymbol{\\leq}}\\frac{1}{2}\\mathbb{E}_{\\mathbf{z}_{1},\\mathbf{z}_{2}}{\\bf\\}_{\\sim}^{i,i,d;}P\\left[\\|\\mathbf{z}_{1}-\\mathbf{z}_{2}\\|^{2}\\right]=\\sf T r(\\mathbf{S})}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The following result, which is a corollary of Theorem 5, is vital for controlling the error introduced due to the variance of the stochastic gradients, and is one of the major components of our analysis. The proof of this result is presented in Appendix F.3 ", "page_idx": 17}, {"type": "text", "text": "Corollary 5 (PAC Bayesian Inequality for Quadratic Variation). Let $\\mathbf{v}_{1},\\ldots,\\mathbf{v}_{T}$ be an $\\mathbb{R}^{d}$ valued martingale difference sequence adapted to the filtration $\\mathcal{F}_{1},\\ldots,\\mathcal{F}_{T}$ satisfying $\\mathbb{E}[\\mathbf{v}_{s}|\\mathcal{F}_{s}]\\;=$ $0,\\mathbb{E}[\\mathbf{v}_{s}\\mathbf{v}_{s}^{T}|\\mathcal{F}_{s}]\\;=\\;\\Sigma_{s}$ and $\\|\\mathbf{v}_{s}\\|\\,\\leq\\,\\tau$ almost surely. Let $\\mathsf{U P}(t)\\,:=\\,\\operatorname*{min}(T,2^{\\lceil\\log_{2}t\\rceil})$ . Suppose $\\|\\Sigma_{s}\\|_{2}\\leq p_{s}$ and ${\\sf T r}(\\Sigma_{s})\\leq q_{s}$ for some fixed sequences $p_{1},\\ldots,p_{T}$ an\u221ad $q_{1},\\ldots,q_{T}$ . Then, there exists a universal constant $C_{\\mathsf{l o w e r}}$ such that whenever $\\begin{array}{r}{T>C_{\\mathrm{lower}}\\log((1+\\frac{\\sqrt{\\bar{q}T}}{\\Gamma})\\log(d+1))}\\end{array}$ such that the following inequality holds with probability at least $1-\\delta_{i}$ , for any $\\delta\\in(0,\\frac{1}{2})$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t}\\|\\mathbf{v}_{s}\\|^{2}\\leq C_{M}\\sum_{s=1}^{\\mathsf{U P}(t)}q_{s}+C_{M}\\tau^{2}\\ln(\\frac{\\ln(T)}{\\delta})^{2}+\\frac{C_{M}t}{\\tau^{2}}\\sum_{s=1}^{\\mathsf{U P}(t)}p_{s}^{2}\\quad\\forall t\\in[T]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $C_{M}>0$ is an absolute numerical constant. ", "page_idx": 17}, {"type": "text", "text": "B Analysis for Smooth Strongly Convex Functions ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Let deff = \u2225\u03a3\u22252 and let $K\\;=\\;4\\operatorname*{max}\\{8,C_{M},\\ln(T)\\}$ . For $t~\\ge~1$ , define the filtration $\\mathcal{F}_{t}\\ =$ $\\sigma\\left(\\mathbf{x}_{1},\\mathbf{g}_{s}|1\\leq s\\leq t\\right)$ and ${\\mathcal F}_{0}\\,=\\,\\sigma({\\bf x}_{1})$ . Furthermore, let $\\nabla F(\\mathbf{x}_{t})\\,=\\,{\\mathsf{c l i p}}_{\\Gamma}(\\mathbf{g}_{t})+\\mathbf{b}_{t}+\\mathbf{v}_{t}$ where $\\mathbf{b}_{t}\\ =\\ \\nabla F(\\mathbf{x}_{t})\\,-\\,\\mathbb{E}[{\\mathsf{c l i p}}_{\\Gamma}(\\mathbf{g}_{t})|\\mathcal{F}_{t-1}]$ and $\\mathbf{v}_{t}\\ =\\ \\mathbb{E}[{\\mathsf{c l i p}}_{\\Gamma}(\\mathbf{g}_{t})|\\mathcal{F}_{t-1}]\\ -\\ {\\mathsf{c l i p}}_{\\Gamma}(\\mathbf{g}_{t})$ . We note that $\\mathbb{E}[\\mathbf{v}_{t}|\\mathcal{F}_{t-1}]=0$ and ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbf{v}_{t}\\|\\leq\\|\\mathsf{c l i p}_{\\Gamma}(\\mathbf{g}_{t})\\|-\\|\\mathbb{E}[\\mathsf{c l i p}_{\\Gamma}(\\mathbf{g}_{t})|\\mathcal{F}_{t-1}]\\|}\\\\ &{\\qquad\\leq\\|\\mathsf{c l i p}_{\\Gamma}(\\mathbf{g}_{t})\\|-\\mathbb{E}[\\|\\mathsf{c l i p}_{\\Gamma}(\\mathbf{g}_{t})\\||\\mathcal{F}_{t-1}]\\leq2\\Gamma}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where the first step follows from the triangle inequality, the second step uses Jensen\u2019s inequality and the last step uses the definition of ${\\mathsf{c l i p}}_{\\Gamma}$ . Hence $\\mathbf{v}_{t}$ is an $\\mathcal{F}$ adapted almost surely bounded martingale difference sequence. Now, let $D_{t}=\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|$ where $\\mathbf{x}^{*}$ is the unique minimizer of $F$ (guaranteed by strong convexity).Let \u03b7t =t+A\u03b3 where $A\\geq1$ is a numerical constant and $\\gamma\\geq A\\kappa+A-1$ is a constant depending on $\\kappa,d$ and $\\ln(1/\\delta)$ which we shall specify later. Note that our choice of $\\gamma$ ensures that \u03b7t \u2264L+\u00b5 for $t\\in[1:T]$ We prove the following recurrence for $D_{t}$ by using the smoothness and strong convexity properties of $F$ and by exploiting the choice of the step-size. ", "page_idx": 17}, {"type": "text", "text": "Lemma 5 (Recurrence for $D_{t}$ ). The following holds for every $t\\in[1:T]$ ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{t+1}^{2}\\leq\\left(\\displaystyle\\frac{\\gamma+1}{t+\\gamma}\\right)^{2A}D_{1}^{2}+\\displaystyle\\frac{A2^{2A+1}}{\\mu}\\displaystyle\\sum_{s=1}^{t}\\displaystyle\\frac{(s+\\gamma-1)^{2A-1}}{(t+\\gamma)^{2A}}\\left\\langle\\mathbf{b}_{s},\\mathbf{x}_{s}-\\mathbf{x}^{*}\\right\\rangle}\\\\ &{\\quad\\quad+\\displaystyle\\frac{A^{2}4^{A+1}}{\\mu^{2}}\\displaystyle\\sum_{s=1}^{t}\\|\\mathbf{b}_{s}\\|^{2}\\displaystyle\\frac{(s+\\gamma)^{2A-2}}{(t+\\gamma)^{2A}}+\\displaystyle\\frac{A2^{2A+1}}{\\mu}\\displaystyle\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{2A-1}}{(t+\\gamma)^{2A}}\\left\\langle\\mathbf{v}_{s},\\mathbf{x}_{s}-\\mathbf{x}^{*}\\right\\rangle}\\\\ &{\\quad\\quad+\\displaystyle\\frac{A^{2}4^{A+1}}{\\mu^{2}}\\displaystyle\\sum_{s=1}^{t}\\|\\mathbf{v}_{s}\\|^{2}\\displaystyle\\frac{(s+\\gamma)^{2A-2}}{(t+\\gamma)^{2A}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Now define $R_{T,\\delta}$ as follows: ", "page_idx": 18}, {"type": "equation", "text": "$$\nR_{T,\\delta}=(\\gamma+1)^{2}D_{1}^{2}+\\frac{(T+\\gamma)\\|\\Sigma\\|_{2}}{\\mu^{2}}\\left(d_{\\mathsf{e f f}}+\\sqrt{d_{\\mathsf{e f f}}}\\ln(K/\\delta)\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "It is easy to see that =\u00b5ln(KR/T\u03b4,)\u03b4 . In our proof of Theorem 1, we shall establish that the following holds with probability at least $1-\\delta$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\nD_{t}^{2}\\leq\\frac{C R_{T,\\delta}}{(t+\\gamma-1)^{2}}\\;\\forall\\,t\\in[1:T+1]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $C>0$ is an absolute numerical constant to be chosen later. To this end, we define the event $E_{t}$ and the random variables $\\mathbf{d}_{t},\\tilde{\\mathbf{b}}_{t},\\tilde{\\mathbf{v}}_{t}$ as follows for $t\\in[1:T+1]$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\cal E}_{t}=\\left\\{D_{t}^{2}\\leq\\displaystyle\\frac{C R_{T,\\delta}}{(t+\\gamma-1)^{2}}\\right\\}}\\\\ &{{\\bf d}_{t}=({\\bf x}_{t}-{\\bf x}^{*})\\mathbb{1}\\left\\{{\\cal E}_{t}\\right\\}}\\\\ &{\\tilde{{\\bf b}}_{t}={\\bf b}_{t}\\mathbb{1}\\left\\{{\\cal E}_{t}\\right\\}}\\\\ &{\\tilde{{\\bf v}}_{t}={\\bf v}_{t}\\mathbb{1}\\left\\{{\\cal E}_{t}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We note that since $\\mathbf{x}_{t}$ is $\\mathcal{F}_{t-1}$ measurable, so are $\\mathbb{1}\\left\\{E_{t}\\right\\},D_{t},\\mathbf{d}_{t},\\mathbf{b}_{t}$ and $\\tilde{\\mathbf{b}}_{t}$ . Furthermore, $\\mathbb{E}[\\tilde{\\mathbf{v}}_{t}|\\mathcal{F}_{t-1}]=\\mathbb{E}[\\mathbf{v}_{t}|\\mathcal{F}_{t-1}]\\mathbb{1}\\left\\{E_{t}\\right\\}=0$ . ", "page_idx": 18}, {"type": "text", "text": "We use the following Lemma to control the bias vector $\\tilde{\\mathbf{b}}_{t}$ ", "page_idx": 18}, {"type": "text", "text": "Lemma 6 (Bias Control). The following holds almost surely for every $t\\in[1:T]$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n|\\tilde{\\mathbf{b}}_{t}|\\leq\\mu\\sqrt{R_{T,\\delta}}\\left(\\frac{1}{T+\\gamma}+\\frac{\\kappa\\ln(1/\\delta)\\sqrt{C}}{(t+\\gamma-1)\\sqrt{d(T+\\gamma)}}+\\frac{\\kappa^{3}C^{3/2}\\ln(1/\\delta)^{2}}{(t+\\gamma-1)^{3}}+\\frac{\\kappa\\sqrt{C}\\ln(1/\\delta)^{2}}{(t+\\gamma-1)(T+\\gamma)}\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We use the following lemma to control the variance vector $\\tilde{{\\bf v}}_{t}$ . The proof of this lemma, which uses Freedman\u2019s inequality and the PAC Bayesian martingale concentration inequality of Corollary 6. ", "page_idx": 18}, {"type": "text", "text": "Lemma 7 (Variance Control). The following holds with probabili\u221aty at least $1-\\delta$ uniformly for every $t\\in[T]$ whenever $A\\geq3$ and $\\gamma\\geq4\\operatorname*{max}\\{\\kappa^{4/3}C^{2/3}\\ln(\\ln(T)/\\delta),\\kappa\\sqrt{C}\\ln(\\ln(T)/\\delta)^{3/2}\\}.$ : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{2A-1}}{(t+\\gamma)^{2A-2}}\\left\\langle\\tilde{\\mathbf{v}}_{s},\\mathbf{d}_{s}\\right\\rangle\\leq27\\mu R_{T,\\delta}\\sqrt{C}}\\\\ &{\\displaystyle\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}\\|\\tilde{\\mathbf{v}}_{s}\\|^{2}\\leq C_{M}\\mu^{2}R_{T,\\delta}\\left(6+3\\cdot2^{4A-13}+3\\cdot2^{4A-17}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $C_{M}$ is the absolute numerical constant defined in Corollary 5. ", "page_idx": 18}, {"type": "text", "text": "Equipped with this bound on the bias and the variance, we now present the complete proof as follows: ", "page_idx": 18}, {"type": "text", "text": "B.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. Let $A\\geq3$ , $\\gamma\\,\\geq\\,4\\operatorname*{max}\\{\\kappa^{4/3}C^{2/3}\\ln(\\ln(T)/\\delta),\\kappa{\\sqrt{C}}\\ln(\\ln(T)/\\delta)^{3/2}\\}$ . Now, let $E$ denote the following event ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{E=\\{\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{2A-1}}{(t+\\gamma)^{2A-2}}\\left\\langle\\tilde{\\mathbf{v}}_{s},\\mathbf{d}_{s}\\right\\rangle\\leq27\\mu R T_{,\\delta}\\sqrt{C}\\,\\forall\\,t\\in[T]}}\\ ~}}\\\\ {{\\displaystyle{\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}\\|\\tilde{\\mathbf{v}}_{s}\\|^{2}\\leq C_{M}\\mu^{2}R_{T,\\delta}\\left(6+3\\cdot2^{4A-13}+3\\cdot2^{4A-17}\\right)\\ \\forall t\\in[T]\\}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that by Lemma 7, $\\mathbb{P}(E)\\ge1-\\delta$ . We now claim that $\\mathbb{P}\\left(\\bigcap_{t=1}^{T+1}E_{t}|E\\right)=1$ , i.e., conditioned on the event $E$ , the following holds almost surely for every $t\\in[1:T+1]$ ", "page_idx": 18}, {"type": "equation", "text": "$$\nD_{t}^{2}\\leq\\frac{C R_{T,\\delta}}{(t+\\gamma-1)^{2}}\\;\\forall\\,t\\in[1:T+1]\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "We prove the above claim by induction. Note that the claim is trivially true for $t=1$ as $R_{T,\\delta}\\ge$ $(\\gamma+1)^{2}D_{1}^{2}$ . Now, consider any $t\\in[1:T]$ and suppose the claim holds for some $1\\leq s\\leq t$ . Recall that by Lemma 5 ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(t+\\gamma)^{2}D_{t+1}^{2}\\leq\\frac{(\\gamma+1)^{2A}}{(t+\\gamma)^{2A-2}}D_{1}^{2}+\\displaystyle\\frac{A2^{2A+1}}{\\mu}\\sum_{s=1}^{t}\\frac{(s+\\gamma-1)^{2A-1}}{(t+\\gamma)^{2A-2}}\\left\\langle\\mathbf{b}_{s},\\mathbf{x}_{s}-\\mathbf{x}^{*}\\right\\rangle}\\\\ &{\\quad\\quad\\quad\\quad+\\displaystyle\\frac{A^{2}4^{A+1}}{\\mu^{2}}\\sum_{s=1}^{t}\\|\\mathbf{b}_{s}\\|^{2}\\frac{\\left(s+\\gamma\\right)^{2A-2}}{(t+\\gamma)^{2A-2}}+\\displaystyle\\frac{A2^{2A+1}}{\\mu}\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{2A-1}}{(t+\\gamma)^{2A-2}}\\left\\langle\\mathbf{v}_{s},\\mathbf{x}_{s}-\\mathbf{x}^{*}\\right\\rangle}\\\\ &{\\quad\\quad\\quad\\quad+\\displaystyle\\frac{A^{2}4^{A+1}}{\\mu^{2}}\\sum_{s=1}^{t}\\|\\mathbf{v}_{s}\\|^{2}\\frac{\\left(s+\\gamma\\right)^{2A-2}}{(t+\\gamma)^{2A-2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Under the induction hypothesis, $\\mathbb{1}\\left\\{E_{s}\\right\\}\\:=\\:1\\;\\forall s\\:\\in\\:[t]$ . Hence, Under the induction hypothesis, $\\begin{array}{r}{\\mathbb{1}\\left\\{D_{s}^{2}\\le\\frac{C R_{T,\\delta}}{(s+\\gamma-1)(s+\\gamma-2)}\\right\\}\\;=\\;1}\\end{array}$ and thus, $\\mathbf{d}_{s}\\;=\\;\\mathbf{x}_{s}\\;-\\;\\mathbf{x}^{*},\\mathbf{b}_{s}\\;=\\;\\tilde{\\mathbf{b}}_{s},\\mathbf{v}_{s}\\;=\\;\\tilde{\\mathbf{v}}_{s}\\;\\forall\\;1\\;\\leq\\;s\\;\\leq\\;t.$ Substituting this transformation into the above inequality, we obtain the following: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(t+\\gamma)^{2}D_{t+1}^{2}\\leq\\underbrace{\\frac{(\\gamma+1)^{2A}}{(t+\\gamma)^{2A-2}}D_{1}^{2}}_{\\textcircled{(}t+\\gamma)^{2A-2}}+\\underbrace{\\frac{A2^{2A+1}}{\\mu}\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{2A-1}}{(t+\\gamma)^{2A-2}}\\langle\\tilde{\\mathbf{v}}_{s},\\mathbf{d}_{s}\\rangle}_{\\textcircled{(2)}}}\\\\ &{\\qquad\\qquad+\\underbrace{\\frac{A^{2}A^{4+1}}{\\mu^{2}}\\sum_{s=1}^{t}\\|\\tilde{\\mathbf{v}}_{s}\\|^{2}\\frac{(s+\\gamma)^{2A-2}}{(t+\\gamma)^{2A-2}}}_{\\textcircled{(})}+\\underbrace{\\frac{t}{s-1}\\frac{(s+\\gamma)^{2A-1}}{(t+\\gamma)^{2A-2}}\\langle\\tilde{\\mathbf{b}}_{s},\\mathbf{d}_{s}\\rangle}_{\\textcircled{(})}}\\\\ &{\\qquad\\qquad+\\underbrace{\\frac{A^{2}A^{4+1}}{\\mu^{2}}\\sum_{s=1}^{t}\\|\\tilde{\\mathbf{b}}_{s}\\|^{2}\\frac{(s+\\gamma)^{2A-2}}{(t+\\gamma)^{2A-2}}}_{\\textcircled{()}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "We now bound each of the terms in the RHS as follows. ", "page_idx": 19}, {"type": "text", "text": "Bounding $\\boldsymbol{\\Phi}$ Since $A\\geq1$ and $t\\geq1$ , ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\mathbb{O}=\\frac{(\\gamma+1)^{2A}}{(t+\\gamma)^{2A-2}}D_{1}^{2}\\leq(\\gamma+1)^{2}D_{1}^{2}\\leq R_{T,\\delta}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Bounding $\\circleddash$ Since $\\gamma$ and $A$ satisfy the conditions of Lemma 7 and we have conditioned on the event $E$ , it follows that: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{A2^{2A+1}}{\\mu}\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{2A-1}}{(t+\\gamma)^{2A-2}}\\left\\langle\\tilde{\\mathbf{v}}_{s},\\mathbf{d}_{s}\\right\\rangle\\leq27A2^{2A+1}R_{T,\\delta}\\sqrt{C}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Bounding $\\circledast$ Since $\\gamma$ and $A$ satisfy the conditions of Lemma 7 and we have conditioned on the event $E$ , it follows that: ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{A^{2}4^{A+1}}{\\mu^{2}}\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}\\|\\tilde{\\mathbf{v}}_{s}\\|^{2}\\le C_{M}2^{2A+2}\\left(6+3\\cdot2^{4A-13}+3\\cdot2^{4A-17}\\right)R_{T,\\delta}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Before controlling terms $\\circledast$ and $\\mathfrak{G}$ , we note that the following holds for every $s\\in[t]$ by Lemma 6 ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\|b_{s}\\right\\|\\leq\\mu\\sqrt{R_{T,\\delta}}\\left(B_{1}+B_{2}+B_{3}+B_{4}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $B_{1},\\ldots,B_{4}$ are defined as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{B_{1}=\\displaystyle\\frac{1}{T+\\gamma}}}\\\\ {{B_{2}=\\displaystyle\\frac{\\kappa\\ln(K/\\delta)\\sqrt{C}}{(s+\\gamma-1)\\sqrt{d(T+\\gamma)}}}}\\\\ {{B_{3}=\\displaystyle\\frac{\\kappa^{3}C^{3/2}\\ln(K/\\delta)^{2}}{(s+\\gamma-1)^{3}}}}\\\\ {{B_{4}=\\displaystyle\\frac{\\kappa\\ln(K/\\delta)^{2}\\sqrt{C}}{(s+\\gamma-1)(T+\\gamma)}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Bounding $\\circledast$ Since 1 $\\{E_{s}\\}=1$ ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\|\\mathbf{d}_{s}\\|\\leq\\frac{\\sqrt{C R_{T,\\delta}}}{s+\\gamma-1}\\leq\\frac{2\\sqrt{C R_{T,\\delta}}}{s+\\gamma}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Hence, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{42^{2A+1}}{\\mu}\\sum_{s=1}^{t}\\left<\\tilde{{\\bf b}}_{s},{\\bf d}_{s}\\right>\\frac{(s+\\gamma)^{2A-1}}{(t+\\gamma)^{2A-2}}\\le A2^{2A+2}R_{T,\\delta}\\sqrt{C}\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}(B_{1}+B_{2}+B_{3}+B_{4})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "We now control the first term ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}B_{1}=\\frac{1}{T+\\gamma}\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}}\\\\ {\\displaystyle\\leq\\frac{t}{T+\\gamma}\\leq1}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the first inequality follows from the fact that $A\\geq1$ and $s\\leq t$ . We now bound the second term ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t}\\left({\\frac{s+\\gamma}{t+\\gamma}}\\right)^{2A-2}B_{2}\\leq{\\frac{\\kappa{\\sqrt{C}}\\ln(K/\\delta)}{\\sqrt{d(T+\\gamma)}}}\\left[\\sum_{s=1}^{t}\\left({\\frac{s+\\gamma}{t+\\gamma}}\\right)^{2A-2}{\\frac{1}{s+\\gamma-1}}\\right]\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Setting $A\\geq3/2$ and using the fact that $s+\\gamma\\geq2$ , it follows that ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}B_{2}\\leq\\frac{2\\kappa\\sqrt{C}\\ln\\left(K/\\delta\\right)}{\\sqrt{d(T+\\gamma)}}\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{2A-3}}{(t+\\gamma)^{2A-2}}}}\\\\ &{}&{\\leq\\frac{2\\kappa\\sqrt{C}\\ln\\left(K/\\delta\\right)}{\\sqrt{d(T+\\gamma)}}\\leq2}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last inequality follows by setting $\\begin{array}{r}{\\gamma\\geq\\frac{C\\kappa^{2}}{d}\\cdot\\ln(K/\\delta)^{2}}\\end{array}$ ", "page_idx": 20}, {"type": "text", "text": "To control the third term, we set $A\\geq5/2$ and proceed as follows: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}B_{3}\\leq\\kappa^{3}C^{3/2}\\ln(K/\\delta)^{2}\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{2A-5}}{(t+\\gamma)^{2A-2}}}}\\\\ &{}&{\\leq\\frac{\\kappa^{3}C^{3/2}\\ln(K/\\delta)^{2}}{(t+\\gamma)^{2}}}\\\\ &{}&{\\leq\\frac{\\kappa^{3}C^{3/2}\\ln(K/\\delta)^{2}}{(\\gamma+1)^{2}}\\leq1}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the last inequality follows by setting $\\gamma\\geq\\kappa^{3/2}C^{3/4}\\ln(K/\\delta)$ . ", "page_idx": 20}, {"type": "text", "text": "To bound the last term, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}B_{4}\\leq\\frac{\\kappa C^{1/2}\\ln(K/\\delta)^{2}}{T+\\gamma}\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{2A-3}}{(t+\\gamma)^{2A-2}}}}\\\\ &{}&{\\leq\\frac{\\kappa C^{1/2}\\ln\\left(1/\\delta\\right)^{2}}{\\gamma+1}\\leq1}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where the second inequality uses the fact that $A\\,\\geq\\,^{3}\\!/2$ and the last inequality follows by setting $\\gamma\\geq\\kappa C^{1/2}\\ln(K/\\delta)^{2}$ . Putting it all together, it follows that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\emptyset\\le5A4^{A+1}R_{T,\\delta}{\\sqrt{C}}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "by setting $\\gamma$ as follows ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\gamma\\geq\\operatorname*{max}\\left\\{{\\frac{\\kappa^{2}C}{d}}\\cdot\\ln(K/\\delta)^{2},\\kappa^{3/2}C^{3/4}\\ln(1/\\delta),\\kappa C^{1/2}\\ln(K/\\delta)^{2}\\right\\}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Bounding $\\circleddash$ By Lemma 6 and Jensen\u2019s inequality ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\|\\tilde{\\mathbf{b}}_{s}\\|^{2}\\leq4\\mu^{2}R_{T,\\delta}\\left(B_{1}^{2}+B_{2}^{2}+B_{3}^{2}+B_{4}^{2}\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "It follows that ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\frac{A^{2}2^{2A+2}}{\\mu^{2}}}\\sum_{s=1}^{t}\\|\\tilde{\\mathbf{b}}_{s}\\|^{2}\\left({\\frac{s+\\gamma}{t+\\gamma}}\\right)^{2A-2}\\leq A^{2}4^{A+2}R_{T,\\delta}\\sum_{s=1}^{t}\\left({\\frac{s+\\gamma}{t+\\gamma}}\\right)^{2A-2}\\left(B_{1}^{2}+B_{2}^{2}+B_{3}^{2}+B_{4}^{2}\\right)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The first term is controlled as follows using the fact that $A\\geq1$ ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}B_{1}^{2}=\\sum_{s=1}^{t}\\frac{1}{(T+\\gamma)^{2}}\\le1\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The second term is controlled as ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}B_{2}^{2}=\\frac{4\\kappa^{2}C\\ln(K/\\delta)^{2}}{d(T+\\gamma)}\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{2A-4}}{(t+\\gamma)^{2A-2}}}}\\\\ &{}&{\\leq\\frac{4\\kappa^{2}C\\ln(K/\\delta)^{2}}{d(t+\\gamma)(T+\\gamma)}\\leq1\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last inequality follows because $\\begin{array}{r}{\\gamma\\geq\\kappa\\sqrt{\\frac{C}{d}}\\ln(K/\\delta)}\\end{array}$ ", "page_idx": 21}, {"type": "text", "text": "For controlling the third term, we set $A\\geq4$ to obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}B_{3}^{2}=\\kappa^{6}C^{3}\\ln(K/\\delta)^{4}\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{2A-8}}{(t+\\gamma)^{2A-2}}}}\\\\ &{}&{\\leq\\frac{\\kappa^{6}C^{3}\\ln\\left(K/\\delta\\right)^{4}}{(\\gamma+1)^{5}}\\leq1}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last inequality uses the fact that $\\gamma\\geq\\kappa^{6/5}C^{3/5}\\ln(K/\\delta)^{4/5}$ To control the fourth term, we use the fact that $A\\ge2$ to obtain ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}B_{4}^{2}=\\frac{\\kappa^{2}C\\ln(K/\\delta)^{4}}{(T+\\gamma)^{2}}\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{2A-4}}{(t+\\gamma)^{2A-2}}}}\\\\ &{}&{\\leq\\frac{\\kappa^{2}C\\ln\\left(K/\\delta\\right)^{4}}{(\\gamma+1)^{3}}\\leq1\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last inequality uses the fact that $\\gamma\\geq\\kappa^{2/3}C^{1/3}\\ln(K/\\delta)^{4/3}$ From the obtained bounds, we conclude that $\\mathfrak{H}\\le A^{2}\\dot{4}^{A+3}R_{T,\\delta}$ . ", "page_idx": 21}, {"type": "text", "text": "Hence, setting A = 4 and \u03b3 = 4C max{ \u2225\u03a3\u22252\u03ba2T lrn((\u03a3l)n(T )/\u03b4)2, $\\begin{array}{r}{\\gamma=4C\\operatorname*{max}\\{\\frac{\\|\\Sigma\\|_{2}\\kappa^{2}\\ln\\left(\\ln(T)/\\delta\\right)^{2}}{\\mathsf{T r}(\\Sigma)},\\kappa^{3/2}\\ln\\bigl(\\ln(T)/\\delta\\bigr),\\kappa\\ln\\bigl(\\ln(T)/\\delta\\bigr)^{2}\\},}\\end{array}$ , we obtain the following ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{t+\\gamma)^{2}D_{t+1}^{2}\\leq\\ensuremath{\\mathbb{O}}+\\ensuremath{\\mathcal{O}}+\\ensuremath{\\mathcal{O}}+\\ensuremath{\\mathcal{O}}+\\ensuremath{\\mathbb{O}}+\\ensuremath{\\mathbb{O}}}\\\\ &{\\qquad\\qquad\\leq R_{T,\\delta}\\left[1+C_{M}2^{2A+2}\\left(6+3\\cdot2^{4A-13}+3\\cdot2^{4A-17}\\right)+A^{2}4^{A+3}+\\sqrt{C}\\left(27A2^{2A+1}-9\\cdot\\ensuremath{\\mathbb{O}}+\\ensuremath{\\mathbb{O}}^{2}\\right)\\right]}\\\\ &{\\qquad\\qquad\\leq R_{T,\\delta}\\left(262145+524288C_{M}+75776\\sqrt{C}\\right)}\\\\ &{\\qquad\\qquad\\leq C R_{T,\\delta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where the last inequality is obtained by setting $C=\\left({\\sqrt{262145+524288C_{M}}}+75776\\right)^{2}$ . It follow that ", "page_idx": 22}, {"type": "equation", "text": "$$\nD_{t+1}^{2}\\leq\\frac{C R_{T,\\delta}}{(t+\\gamma)^{2}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Thus, we have proved by induction that conditioned on E, Dt2 \u2264(Ct+R\u03b3T,)\u03b42 for every t \u2208[T + 1]. In particular, the following holds with probability at least $1-\\delta$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{T+1}^{2}\\leq C\\left(\\displaystyle\\frac{\\gamma+1}{T+\\gamma}\\right)^{2}D_{1}^{2}+\\displaystyle\\frac{C\\|\\Sigma\\|_{2}\\left(d_{\\mathsf{e f f}}+\\sqrt{d_{\\mathsf{e f f}}}\\ln(K/\\delta)\\right)}{\\mu^{2}(T+\\gamma)}}\\\\ &{\\qquad\\lesssim\\left(\\displaystyle\\frac{\\gamma+1}{T+\\gamma}\\right)^{2}D_{1}^{2}+\\displaystyle\\frac{\\mathsf{T r}(\\Sigma)+\\sqrt{\\|\\Sigma\\|_{2}\\mathsf{T r}(\\Sigma)}\\ln\\bigl(\\ln(T)/\\delta\\bigr)}{\\mu^{2}(T+\\gamma)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "B.2 Proof of Lemma 5 ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Let $\\boldsymbol{\\epsilon}_{t}=\\mathbf{b}_{t}+\\mathbf{v}_{t}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{t+1}^{2}=\\|\\Pi_{\\mathcal{X}}(\\mathbf{x}_{t}-\\eta_{t}\\nabla F(\\mathbf{x}_{t})+\\eta_{t}\\epsilon_{t})-\\mathbf{x}^{*}\\|^{2}}\\\\ &{\\qquad\\le\\|\\mathbf{x}_{t}-\\eta_{t}\\nabla F(\\mathbf{x}_{t})+\\eta_{t}\\epsilon_{t}\\|^{2}}\\\\ &{\\qquad\\le D_{t}^{2}-2\\eta_{t}\\left\\langle\\nabla F(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle+2\\eta_{t}\\left\\langle\\epsilon_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle+2\\eta_{t}^{2}\\|\\nabla F(\\mathbf{x}_{t})\\|^{2}+2\\eta_{t}^{2}\\|\\epsilon_{t}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "By the coercivity lemma in Bubeck [6] , ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\|\\nabla F({\\mathbf x}_{t})\\|^{2}\\leq\\left(L+\\mu\\right)\\left\\langle\\nabla F({\\mathbf x}_{t}),{\\mathbf x}_{t}-{\\mathbf x}^{*}\\right\\rangle-L\\mu D_{t}^{2}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "It follows that, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{\\tilde{\\Pi}}_{t+1}^{2}\\le(1-2\\eta_{t}^{2}L\\mu)D_{t}^{2}-2\\eta_{t}[1-\\eta_{t}(L+\\mu)]\\left\\langle\\nabla F(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle+2\\eta_{t}\\left\\langle\\epsilon_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle+2\\eta_{t}^{2}\\|\\epsilon_{t}\\|^{2}}\\\\ &{\\qquad\\le(1-2\\eta_{t}^{2}L\\mu)D_{t}^{2}-2\\eta_{t}[1-\\eta_{t}(L+\\mu)]\\mu D_{t}^{2}+2\\eta_{t}\\left\\langle\\epsilon_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle+2\\eta_{t}^{2}\\|\\epsilon_{t}\\|^{2}}\\\\ &{\\qquad\\le(1-2\\eta_{t}\\mu-2\\eta_{t}^{2}\\mu^{2})D_{t}^{2}+2\\eta_{t}\\left\\langle\\epsilon_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle+2\\eta_{t}^{2}\\|\\epsilon_{t}\\|^{2}}\\\\ &{\\qquad\\le(1-2\\eta_{t}\\mu)D_{t}^{2}+2\\eta_{t}\\left\\langle\\epsilon_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle+2\\eta_{t}^{2}\\|\\epsilon_{t}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where the second inequality follows from the strong monotonicity property of $\\nabla F(\\mathbf{x})$ and the fact that \u03b7t \u2264 L1+\u00b5 since \u03b3 \u2265A\u03ba + A \u22121. Now, substituting \u03b7t =\u00b5(tA+\u03b3), ", "page_idx": 22}, {"type": "equation", "text": "$$\nD_{t+1}^{2}\\leq\\left(1-\\frac{2A}{t+\\gamma}\\right)D_{t}^{2}+\\frac{2A}{\\mu(t+\\gamma)}\\left\\langle\\epsilon_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle+\\frac{2A^{2}\\|\\epsilon_{t}\\|^{2}}{\\mu^{2}(t+\\gamma)^{2}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since $1-t\\leq e^{-t}\\,\\forall\\,t\\in\\mathbb{R}$ , we note that $\\forall s<t$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\prod_{j=s+1}^{t}\\left(1-\\frac{2A}{j+\\gamma}\\right)\\le\\exp\\left(-\\sum_{j=s+1}^{t}\\frac{2A}{j+\\gamma}\\right)}\\\\ &{\\le\\exp\\left(-2A\\int_{s+1}^{t+1}\\frac{\\,\\mathrm{d}u}{u+\\gamma}\\right)}\\\\ &{\\displaystyle\\le\\exp\\left(-2A\\ln\\left(\\frac{t+1+\\gamma}{s+1+\\gamma}\\right)\\right)}\\\\ &{=\\left(\\frac{s+1+\\gamma}{t+1+\\gamma}\\right)^{2A}}\\\\ &{\\le2^{2A}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Using the above bound to unroll the recurence (7), we obtain: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\mathcal{I}_{t+1}^{2}\\leq\\left[\\prod_{j=1}^{t}\\left(1-\\frac{2A}{j+\\gamma}\\right)\\right]D_{1}^{2}+\\frac{2A}{\\mu}\\sum_{s=1}^{t}\\frac{\\langle\\epsilon_{s},\\mathbf{x}_{s}-\\mathbf{x}^{*}\\rangle}{(s+\\gamma)}\\left[\\prod_{j=s+1}^{t}\\left(1-\\frac{2A}{j+\\gamma}\\right)\\right]}}\\\\ {{\\displaystyle~~~+\\frac{2A^{2}}{\\mu^{2}}\\sum_{s=1}^{t}\\frac{\\|\\epsilon_{s}\\|^{2}}{(s+\\gamma)^{2}}\\left[\\prod_{j=s+1}^{t}\\left(1-\\frac{2A}{j+\\gamma}\\right)\\right]}}\\\\ {{\\displaystyle~~~\\leq\\left(\\frac{\\gamma+1}{t+\\gamma}\\right)^{2A}D_{1}^{2}+\\frac{A2^{2A+1}}{\\mu}\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{2A-1}}{(t+\\gamma)^{2A}}\\left\\langle\\epsilon_{s},\\mathbf{x}_{s}-\\mathbf{x}^{*}\\right\\rangle+\\frac{A^{2}2^{2A+1}}{\\mu^{2}}\\sum_{s=1}^{t}\\|\\epsilon_{s}\\|^{2}\\frac{(s+\\gamma)^{2A}}{(t+\\gamma)^{2}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Expanding $\\boldsymbol{\\epsilon}_{s}=\\mathbf{b}_{s}+\\mathbf{v}_{s}$ and using Young\u2019s inequality, we conclude that the following holds for every $t\\in[1:T]$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{t+1}^{2}\\leq\\left(\\displaystyle\\frac{\\gamma+1}{t+\\gamma}\\right)^{2A}D_{1}^{2}+\\displaystyle\\frac{A2^{2A+1}}{\\mu}\\displaystyle\\sum_{s=1}^{t}\\displaystyle\\frac{(s+\\gamma-1)^{2A-1}}{(t+\\gamma)^{2A}}\\left\\langle\\mathbf{b}_{s},\\mathbf{x}_{s}-\\mathbf{x}^{*}\\right\\rangle}\\\\ &{\\quad\\quad+\\displaystyle\\frac{A^{2}4^{A+1}}{\\mu^{2}}\\displaystyle\\sum_{s=1}^{t}\\|\\mathbf{b}_{s}\\|^{2}\\displaystyle\\frac{(s+\\gamma)^{2A-2}}{(t+\\gamma)^{2A}}+\\displaystyle\\frac{A2^{2A+1}}{\\mu}\\displaystyle\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{2A-1}}{(t+\\gamma)^{2A}}\\left\\langle\\mathbf{v}_{s},\\mathbf{x}_{s}-\\mathbf{x}^{*}\\right\\rangle}\\\\ &{\\quad\\quad+\\displaystyle\\frac{A^{2}4^{A+1}}{\\mu^{2}}\\displaystyle\\sum_{s=1}^{t}\\|\\mathbf{v}_{s}\\|^{2}\\displaystyle\\frac{(s+\\gamma)^{2A-2}}{(t+\\gamma)^{2A}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "B.3 Proof of Lemma 6 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Note that by definition of $E_{t}$ ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|\\nabla F(\\mathbf{x}_{t})\\|\\,\\mathbb{1}\\left\\{E_{t}\\right\\}\\leq L D_{t}\\mathbb{1}\\left\\{E_{t}\\right\\}}&{{}}\\\\ {\\leq L\\frac{\\sqrt{C R_{T,\\delta}}}{\\left(t+\\gamma-1\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Recall that $\\begin{array}{r}{\\Gamma=\\frac{\\mu\\sqrt{R_{T,\\delta}}}{\\ln\\left(K/\\delta\\right)}}\\end{array}$ i.e. $\\begin{array}{r}{\\sqrt{R_{T,\\delta}}=\\frac{\\gamma\\ln(K/\\delta)}{\\mu}}\\end{array}$ . Substituting this into the above inequality gives us: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\nabla F(\\mathbf{x}_{t})\\|\\,\\mathbb{1}\\left\\{E_{t}\\right\\}\\leq\\frac{\\kappa\\Gamma\\ln(K/\\delta)\\sqrt{C}}{t+\\gamma-1}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We recall that $\\mathbf{b}_{t}\\;=\\;\\nabla F(\\mathbf{x}_{t})\\,-\\,\\mathbb{E}[\\mathsf{c l i p}_{\\Gamma}(\\mathbf{g}_{t})|\\mathcal{F}_{t-1}]\\;=\\;\\mathbb{E}[\\mathbf{g}_{t}|\\mathcal{F}_{t-1}]\\,-\\,\\mathbb{E}[\\mathsf{c l i p}_{\\Gamma}(\\mathbf{g}_{t})|\\mathcal{F}_{t-1}]$ . Since $\\mathsf{C o v}[\\mathbf{g}_{t}|\\mathcal{F}_{t-1}]\\;\\preceq\\;\\Sigma$ by Assumption Bdd. $2^{\\mathsf{n d}}$ Moment, we obtain the following bound on $\\|\\mathbf{b}_{t}\\|$ by an application of Lemma 4 ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\mathbf{b}_{t}\\|\\leq\\frac{\\|\\Sigma\\|_{2}\\sqrt{d_{\\mathrm{eff}}}}{\\Gamma}+\\frac{\\|\\nabla F(\\mathbf{x}_{t})\\|\\sqrt{\\|\\Sigma\\|_{2}}}{\\Gamma}+\\frac{\\|\\nabla F(\\mathbf{x}_{t})\\|^{3}}{\\Gamma^{2}}+\\frac{\\|\\Sigma\\|_{2}d_{\\mathrm{eff}}\\|\\nabla F(\\mathbf{x}_{t})\\|}{\\Gamma^{2}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Since $\\tilde{\\mathbf{b}}_{t}=\\mathbf{b}_{t}\\mathbb{1}\\left\\{E_{t}\\right\\}$ , it follows that ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{b}}_{t}\\|\\leq\\underbrace{\\frac{\\|\\Sigma\\|_{2}\\sqrt{d_{\\mathrm{eff}}}}{\\Gamma}}_{\\mathfrak{Q}}+\\underbrace{\\|\\nabla F(\\mathbf{x}_{t})\\|\\,\\mathbb{I}\\,\\{E_{t}\\}\\,\\sqrt{\\|\\Sigma\\|_{2}}}_{\\mathfrak{Q}}+\\underbrace{\\frac{\\|\\nabla F(\\mathbf{x}_{t})\\|^{3}\\,\\mathbb{I}\\,\\{E_{t}\\}}{\\Gamma^{2}}}_{\\mathfrak{Q}}+\\underbrace{\\frac{\\|\\Sigma\\|_{2}d_{\\mathrm{eff}}\\|\\nabla F(\\mathbf{x}_{t})\\|\\,\\mathbb{I}\\,\\{E_{t}\\}}{\\Gamma^{2}}}_{\\mathfrak{Q}}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Bounding $\\textsuperscript{\\textregistered}$ By definition of $\\Gamma$ , ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\|\\Sigma\\|_{2}\\sqrt{d_{\\mathrm{eff}}}}{\\Gamma}=\\frac{\\|\\Sigma\\|_{2}\\sqrt{d_{\\mathrm{eff}}}\\ln\\left(K/\\delta\\right)}{\\mu\\sqrt{R_{T,\\delta}}}}\\\\ &{\\qquad\\qquad\\leq\\frac{(T+\\gamma)\\|\\Sigma\\|_{2}\\sqrt{d_{\\mathrm{eff}}}\\ln\\left(K/\\delta\\right)}{\\mu T\\sqrt{R_{T,\\delta}}}}\\\\ &{\\qquad\\qquad\\leq\\frac{\\mu\\sqrt{R_{T,\\delta}}}{(T+\\gamma)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Hence \u20ddA\u2264\u00b5T +R\u03b3T,\u03b4 ", "page_idx": 23}, {"type": "text", "text": "Bounding $\\circledmathbf{B}$ Since $\\begin{array}{r}{R_{T,\\delta}\\geq\\frac{\\|\\Sigma\\|_{2}d_{\\mathsf{e f f}}(T+\\gamma)}{\\mu^{2}}\\geq\\frac{\\|\\Sigma\\|_{2}T}{\\mu^{2}}}\\end{array}$ , $\\begin{array}{r}{\\sqrt{\\|\\Sigma\\|_{2}}\\leq\\frac{\\mu\\sqrt{R_{T,\\delta}}}{\\sqrt{d(T+\\gamma)}}}\\end{array}$ . Substituting this into equation (8), ", "page_idx": 24}, {"type": "equation", "text": "$$\n{\\frac{\\left\\|\\nabla F(\\mathbf{x}_{t})\\right\\|\\mathbb{1}\\left\\{E_{t}\\right\\}{\\sqrt{\\left\\|{\\boldsymbol{\\Sigma}}\\right\\|_{2}}}}{\\Gamma}}\\leq{\\frac{\\kappa{\\sqrt{C}}\\ln(K/\\delta)}{t+\\gamma-1}}\\cdot{\\frac{\\mu{\\sqrt{R_{T,\\delta}}}}{\\sqrt{d(T+\\gamma)}}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Hence, \u20ddB\u2264\u00b5 RT,\u03b4 \u00b7(s\u03ba+ l\u03b3n)(1\u221a/\u03b4d)(T C+\u03b3 ", "page_idx": 24}, {"type": "text", "text": "Bounding $\\circledcirc$ From equation (8), ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{\\|\\nabla F(\\mathbf{x}_{t})\\|^{3}}{\\Gamma^{2}}\\leq\\frac{\\kappa^{3}C^{3/2}\\Gamma\\ln(^{1/\\delta})^{3}}{(t+\\gamma-1)^{3}}}}\\\\ &{\\leq\\mu\\sqrt{R_{T,\\delta}}\\cdot\\frac{\\kappa^{3}C^{3/2}\\ln(1/\\delta)^{2}}{(t+\\gamma-1)^{3}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Hence, $\\begin{array}{r}{\\mathbb{C}\\leq\\mu\\sqrt{R_{T,\\delta}}\\cdot\\frac{\\kappa^{3}C^{3/2}\\ln(1/\\delta)^{2}}{(t+\\gamma-1)^{3}}}\\end{array}$ ", "page_idx": 24}, {"type": "text", "text": "Bounding $\\circledcirc$ Recall that, ", "text_level": 1, "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\|\\Sigma\\|_{2}d_{\\mathsf{e f f}}\\leq\\frac{\\mu^{2}R_{T,\\delta}}{T+\\gamma}}}\\\\ &{}&\\\\ {\\frac{\\|\\nabla F(\\mathbf{x}_{t})\\|\\,\\mathbb{I}\\,\\{E_{t}\\}}{\\Gamma}\\leq\\frac{\\kappa\\ln(K/\\delta)\\sqrt{C}}{(t+\\gamma-1)}}\\\\ &{}&\\\\ &{}&{\\Gamma=\\frac{\\mu\\sqrt{R_{T,\\delta}}}{\\ln(K/\\delta)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "It follows that ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{O}=\\frac{\\|\\Sigma\\|_{2}d_{\\mathsf{e f f}}\\|\\nabla F(\\mathbf{x}_{t})\\|\\mathbb{1}\\left\\{E_{t}\\right\\}}{\\Gamma^{2}}\\leq\\mu\\sqrt{R_{T,\\delta}}\\cdot\\frac{\\kappa\\ln(K/\\delta)^{2}\\sqrt{C}}{(t+\\gamma-1)(T+\\gamma)}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Hence, ", "page_idx": 24}, {"type": "equation", "text": "$$\n|\\tilde{\\mathbf{b}}_{t}|\\leq\\mu\\sqrt{R_{T,\\delta}}\\left(\\frac{1}{T+\\gamma}+\\frac{\\kappa\\ln(1/\\delta)\\sqrt{C}}{(t+\\gamma-1)\\sqrt{d(T+\\gamma)}}+\\frac{\\kappa^{3}C^{3/2}\\ln(1/\\delta)^{2}}{(t+\\gamma-1)^{3}}+\\frac{\\kappa\\sqrt{C}\\ln(1/\\delta)^{2}}{(t+\\gamma-1)(T+\\gamma)}\\right)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "B.4 Proof of Lemma 7 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "For any $s\\in[T]$ , we recall that $\\mathbf{v}_{s}=\\mathbb{E}\\,[\\mathsf{c l i p}_{\\Gamma}(\\mathbf{g}_{s})|\\mathcal{F}_{s-1}]-\\mathsf{c l i p}_{\\Gamma}(\\mathbf{g}_{s})$ . Since $\\mathbb{E}[\\mathbf{g}_{s}\\vert\\mathcal{F}_{s-1}]=\\nabla F(\\mathbf{x}_{s})$ and $\\mathsf{C o v}[\\mathbf{g}_{s}|\\bar{\\mathcal{F}}_{s-1}]\\preceq\\Sigma$ , we obtain the following from Lemma 4   \n$\\begin{array}{r l}&{\\|\\mathbb{E}\\left[\\mathbf{v}_{s}\\mathbf{v}_{s}^{T}|\\mathcal{F}_{s-1}\\right]\\|_{2}=\\|\\mathsf{C o v}\\left[\\mathsf{c l i p}_{\\Gamma}(\\mathbf{g}_{s})|\\mathcal{F}_{s-1}\\right]\\|\\leq\\|\\Sigma\\|_{2}+\\frac{\\|\\nabla F(\\mathbf{x}_{s})\\|^{4}}{\\Gamma^{2}}+\\frac{\\|\\nabla F(\\mathbf{x}_{s})\\|^{2}\\mathsf{T r}(\\Sigma)}{\\Gamma^{2}}}\\\\ &{\\mathsf{T r}\\left(\\mathbb{E}\\left[\\mathbf{v}_{s}\\mathbf{v}_{s}^{T}|\\mathcal{F}_{s-1}\\right]\\right)=\\mathsf{T r}\\left(\\mathsf{C o v}\\left[\\mathsf{c l i p}_{\\Gamma}(\\mathbf{g}_{s})|\\mathcal{F}_{s-1}\\right]\\right)\\leq\\mathsf{T r}(\\Sigma)}\\end{array}$ For $s\\in[1:T]$ define $\\mathbb{E}[\\tilde{\\mathbf{v}}_{s}\\tilde{\\mathbf{v}}_{s}^{T}|\\mathcal{F}_{s-1}]=\\tilde{\\Sigma}_{s}$ . Since $\\mathbb{I}\\left\\{E_{s}\\right\\}$ is $\\mathcal{F}_{s-1}$ -measurable and $\\tilde{\\mathbf{v}}_{s}=\\mathbf{v}_{s}\\mathbb{1}\\left\\{E_{s}\\right\\}$ , it follows that $\\tilde{\\Sigma}_{s}\\;=\\;\\mathbb{E}\\left[\\mathbf{v}_{s}\\mathbf{v}_{s}^{T}|\\mathcal{F}_{s}\\right]\\mathbb{1}\\left\\{E_{s}\\right\\}$ . Hence, we conclude the following from the above inequality ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\tilde{\\Sigma}_{s}\\|_{2}\\leq\\|\\Sigma\\|_{2}+\\frac{\\|\\nabla F({\\mathbf x}_{s})\\|^{4}\\mathbb{1}\\left\\{E_{s}\\right\\}}{\\Gamma^{2}}+\\frac{\\|\\nabla F({\\mathbf x}_{s})\\|^{2}\\top r(\\Sigma)\\mathbb{1}\\left\\{E_{s}\\right\\}}{\\Gamma^{2}}}\\\\ &{\\mathsf{T r}(\\tilde{\\Sigma}_{s})\\leq\\mathsf{T r}(\\Sigma)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Now, for $s\\in[t]$ , we define $h_{s}$ as follows: ", "page_idx": 24}, {"type": "equation", "text": "$$\nh_{s}=\\left\\langle\\tilde{\\mathbf{v}}_{s},\\mathbf{d}_{s}\\right\\rangle\\frac{(s+\\gamma)^{2A-1}}{(t+\\gamma)^{2A-2}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "$\\begin{array}{r}{\\mathbb{E}[h_{s}|{\\mathcal{F}}_{s-1}]=\\left\\langle\\mathbb{E}[\\tilde{\\mathbf{v}}_{s}|{\\mathcal{F}}_{s-1}],\\mathbf{d}_{s}\\right\\rangle\\frac{(s+\\gamma)^{2A-1}}{(t+\\gamma)^{2A-2}}=0}\\end{array}$ $\\|\\tilde{\\mathbf{v}}_{s}\\|\\leq\\|\\mathbf{v}_{s}\\|\\leq2\\Gamma$ and $\\begin{array}{r}{\\|\\mathbf{d}_{s}\\|\\leq\\frac{\\sqrt{C R_{T,\\delta}}}{s+\\gamma-1}}\\end{array}$ ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|h_{s}|\\le2\\Gamma\\cdot\\displaystyle\\frac{\\sqrt{C R_{T,\\delta}}}{s+\\gamma-1}\\cdot\\displaystyle\\frac{(s+\\gamma)^{2A-1}}{(t+\\gamma)^{2A-2}}}\\\\ &{\\qquad\\le4\\Gamma\\sqrt{C R_{T,\\delta}}\\left(\\displaystyle\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}}\\\\ &{\\qquad\\le\\displaystyle\\frac{4\\mu R_{T,\\delta}\\sqrt{C}}{\\ln(K/\\delta)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For $s\\in[t]$ , define $\\sigma_{s}^{2}=\\mathbb{E}[h_{s}^{2}|\\mathcal{F}_{s-1}]$ . It follows that, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\sigma_{s}^{2}=\\frac{(s+\\gamma)^{4A-2}}{(t+\\gamma)^{4A-4}}{\\mathbf{v}}_{s}^{T}\\tilde{\\Sigma}_{s}{\\mathbf{v}}_{s}}\\\\ &{\\quad\\leq\\frac{(s+\\gamma)^{4A-2}}{(t+\\gamma)^{4A-4}}\\|{\\mathbf{v}}_{s}\\|^{2}\\|\\tilde{\\Sigma}_{s}\\|_{2}}\\\\ &{\\quad\\leq4C R_{T,\\delta}\\cdot\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{4A-4}\\|\\tilde{\\Sigma}_{s}\\|_{2}}\\\\ &{\\quad\\leq4C R_{T,\\delta}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{4A-4}\\left(\\|\\Sigma\\|_{2}+\\frac{\\|\\nabla F({\\mathbf{x}}_{s})\\|^{4}}{\\Gamma^{2}}+\\frac{\\|\\nabla F({\\mathbf{x}}_{s})\\|^{2}\\|\\Sigma\\|_{2}d_{\\mathrm{eff}}}{\\Gamma^{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where the last inequality follows from equation (9) and the fact that $d_{\\mathsf{e f f}}=\\mathsf{T r}(\\Sigma)/\\|\\Sigma\\|_{2}$ . We now use the above inequality to control $\\textstyle\\sum_{s=1}^{t}\\sigma_{s}^{2}\\ln(K/\\delta)$ as follows: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\sum_{s=1}^{t}\\sigma_{s}^{2}\\ln(K/\\delta)\\leq4C R_{T,\\delta}\\ln(K/\\delta)\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{4A-4}\\|\\Sigma\\|_{2}}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad+\\,4C R_{T,\\delta}\\ln(K/\\delta)\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{4A-4}\\frac{\\|\\nabla F({\\bf x}_{s})\\|^{4}}{\\Gamma^{2}}}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad+\\,4C R_{T,\\delta}\\ln(K/\\delta)\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{4A-4}\\frac{\\|\\nabla F({\\bf x}_{s})\\|^{2}\\|\\Sigma\\|_{2}d_{\\mathrm{eff}}}{\\Gamma^{2}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "We now control each of the three terms in the above inequality as follows ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{4C R_{T,\\delta}\\ln(K/\\delta)\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{4A-4}\\|\\Sigma\\|_{2}\\le4C R_{T,\\delta}\\ln(K/\\delta)\\|\\Sigma\\|_{2}t}}\\\\ &{}&{\\le4C t R_{T,\\delta}\\cdot\\frac{\\mu^{2}R_{T,\\delta}}{(T+\\gamma)\\sqrt{d_{\\mathrm{eff}}}}}\\\\ &{}&{\\le4\\mu^{2}C R_{T,\\delta}^{2}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Before controlling the remaining two terms, we recall from (8) in the proof of Lemma ?? that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\|\\nabla F(\\mathbf{x}_{s})\\|\\,\\mathbb{1}\\,\\{E_{s}\\}\\le\\frac{\\kappa\\Gamma\\ln\\left(K/\\delta\\right)\\sqrt{C}}{s+\\gamma-1}}\\\\ &{}&{\\qquad\\le\\frac{2\\kappa\\Gamma\\ln\\left(K/\\delta\\right)\\sqrt{C}}{s+\\gamma}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\begin{array}{r}{\\Gamma=\\frac{\\mu\\sqrt{R_{T,\\delta}}}{\\ln\\left(K/\\delta\\right)}}\\end{array}$ . It follows that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\frac{\\|\\nabla F(\\mathbf{x}_{s})\\|^{4}}{\\Gamma^{2}}\\leq\\frac{16\\kappa^{4}C^{2}\\Gamma^{2}\\ln(K/\\delta)^{4}}{(s+\\gamma)^{4}}}}\\\\ &{}&{=\\mu^{2}R_{T,\\delta}\\cdot\\frac{16\\kappa^{4}C^{2}\\ln(K/\\delta)^{2}}{(s+\\gamma)^{4}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Thus, we can control the second term in equation (11) as follows ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{1\\mathcal{C}R_{T,\\delta}\\ln(K/\\delta)\\displaystyle\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{4A-4}\\frac{\\|\\nabla F(\\mathbf{x}_{s})\\|^{4}}{\\Gamma^{2}}\\leq64\\mu^{2}C R_{T,\\delta}^{2}\\cdot\\kappa^{4}C^{2}\\ln(K/\\delta)^{3}\\displaystyle\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{4A-8}}{(t+\\gamma)^{4A-4}}}&{}\\\\ {\\leq64\\mu^{2}C R_{T,\\delta}^{2}\\cdot\\frac{\\kappa^{4}C^{2}\\ln(K/\\delta)^{3}}{(t+\\gamma)^{3}}}&{}\\\\ {\\leq64\\mu^{2}C R_{T,\\delta}^{2}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the second inequality follows by setting $A\\ge2$ and the last inequality follows by setting $\\gamma\\geq\\kappa^{4/3}C^{2/3}\\ln(K/\\delta)$ . ", "page_idx": 26}, {"type": "text", "text": "To control the third term in (11), we note that by equation (8) and the definition of $R_{T,\\delta}$ ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\frac{\\|\\nabla F(\\mathbf{x}_{s})\\|^{2}\\|\\Sigma\\|_{2}d_{\\mathsf{e f f}}}{\\Gamma^{2}}\\leq4\\mu^{2}R_{T,\\delta}\\cdot\\frac{\\kappa^{2}C\\ln(K/\\delta)^{2}}{(T+\\gamma)(s+\\gamma)^{2}}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "It follows that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{,C R_{T,\\delta}\\ln(K/\\delta)\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{4A-4}\\frac{\\|\\nabla F(\\mathbf{x}_{s})\\|^{2}\\left\\|\\Sigma\\right\\|_{2}d_{\\mathrm{eff}}}{\\Gamma^{2}}\\le16\\mu^{2}C R_{T,\\delta}^{2}\\cdot\\frac{\\kappa^{2}C\\ln(K/\\delta)^{3}}{T+\\gamma}\\sum_{s=1}^{t}\\frac{(s+\\gamma)}{(t+\\gamma)^{2}}}}\\\\ &{}&{\\le16\\mu^{2}C R_{T,\\delta}^{2}\\cdot\\frac{\\kappa^{2}C\\ln(K/\\delta)^{3}}{(T+\\gamma)(t+\\gamma)}}\\\\ &{}&{\\le16\\mu^{2}C R_{T,\\delta}^{2}\\,,\\qquad\\qquad\\qquad\\qquad\\qquad\\quad}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where t\u221ahe second inequality follows by setting $A\\geq3/2$ and the last inequality follows by setting $\\gamma\\geq\\kappa\\sqrt{C}\\ln(K/\\delta)^{3/2}$ . Substituting the above bounds into equation (11), we note that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t}\\sigma_{s}^{2}\\ln(K/\\delta)\\leq84\\mu^{2}C R_{T,\\delta}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Thus, by Freedman\u2019s inequality (Lemma 3), we conclude that the following holds with probability at least $1-\\delta/2$ uniformly for every $t\\in[T]$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{2A-1}}{(t+\\gamma)^{2A-2}}\\left\\langle\\tilde{\\mathbf{v}}_{s},\\mathbf{d}_{s}\\right\\rangle=\\sum_{s=1}^{t}h_{s}\\leq2\\sqrt{\\sum_{s=1}^{t}\\sigma_{s}^{2}\\ln(K/\\delta)}+8\\mu R_{T,\\delta}\\sqrt{C}\\leq27R_{T,\\delta}\\sqrt{C}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "To prove the second inequality of this lemma, we define $\\begin{array}{r}{{\\bf z}_{s}=\\tilde{\\bf v}_{s}\\cdot\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{A-1}}\\end{array}$ for $s\\,\\in\\,[t]$ . Note that $\\mathbb{E}[\\mathbf{z}_{s}|\\mathcal{F}_{s-1}]\\,=\\,0$ and $\\|\\mathbf{z}_{s}\\|\\,\\leq\\,\\|\\tilde{\\mathbf{v}}_{s}\\|\\,\\leq\\,2\\Gamma$ . Define the PSD matrices $\\mathbf{\\dot{G}}_{s}\\,=\\,\\mathbb{E}[\\mathbf{z}_{s}\\mathbf{z}_{s}^{T}|\\mathcal{F}_{s-1}]\\,=$ $\\begin{array}{r}{\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}\\tilde{\\Sigma}_{s}}\\end{array}$ . Recalling that $\\mathsf{T r}(\\tilde{\\Sigma}_{s})\\leq\\mathsf{T r}(\\Sigma)$ and the bound obtained on $\\|\\tilde{\\Sigma}_{s}|_{2}$ in equation (9), we infer the following: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{T r}(\\mathbf{G}_{s})\\le\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}\\mathsf{T r}(\\Sigma)}\\\\ &{\\,\\,\\,\\|\\mathbf{G}_{s}\\|_{2}\\le\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}\\|\\Sigma\\|_{2}+\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}\\frac{\\|\\nabla F(\\mathbf{x}_{s})\\|^{4}\\mathbb{1}\\,\\left\\{E_{s}\\right\\}}{\\Gamma^{2}}}\\\\ &{\\quad\\quad\\quad+\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}\\frac{\\|\\nabla F(\\mathbf{x}_{s})\\|^{2}\\mathsf{T r}(\\Sigma)\\mathbb{1}\\,\\left\\{E_{s}\\right\\}}{\\Gamma^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Substituting (8) into the bound for $\\|\\mathbf{G}_{s}\\|_{2}$ , we obtain the following ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{T r}(\\mathbf{G}_{s})\\le q_{s}=\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}\\mathsf{T r}(\\Sigma)}\\\\ &{\\|\\mathbf{G}_{s}\\|_{2}\\le p_{s}=\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}\\|\\Sigma\\|_{2}+\\frac{(s+\\gamma)^{2A-6}}{(t+\\gamma)^{2A-2}}\\cdot16\\kappa^{4}C^{2}\\ln(K/\\delta)^{2}\\mu^{2}R_{T,\\delta}}\\\\ &{\\quad\\quad\\quad+\\,\\frac{(s+\\gamma)^{2A-4}}{(t+\\gamma)^{2A-2}}\\cdot4\\kappa^{2}C\\ln(K/\\delta)^{2}\\|\\Sigma\\|_{2}d_{\\mathsf{e f f}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "By Cauchy Schwarz Inequality, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{p_{s}^{2}\\leq3\\left(\\displaystyle\\frac{s+\\gamma}{t+\\gamma}\\right)^{4A-4}\\|\\Sigma\\|_{2}^{2}+3\\cdot\\displaystyle\\frac{(s+\\gamma)^{4A-12}}{(t+\\gamma)^{4A-4}}\\cdot256\\kappa^{8}C^{4}\\ln(K/\\delta)^{4}\\mu^{4}R_{T,\\delta}^{2}}}\\\\ {{\\phantom{\\frac{(s+\\gamma)^{4A-1}}{4}}+3\\cdot\\displaystyle\\frac{(s+\\gamma)^{4A-8}}{(t+\\gamma)^{4A-4}}\\cdot16\\kappa^{4}C^{2}\\ln(K/\\delta)^{4}\\|\\Sigma\\|_{2}^{2}d_{\\mathrm{eff}}^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Since $T\\gtrsim\\ln(\\ln(d))$ , $K=\\ln(\\ln(T))$ and $q_{s}\\,\\leq\\,\\mathsf{T r}(\\Sigma)\\,\\forall s\\,\\in\\,[T]$ , our choice of $\\Gamma$ ensures that the conditions of Corollary 5 are satisfied. Hence, by Corollary 5, we conclude that the following holds with probability $1-\\delta/2$ uniformly for all $t\\in[T]$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t}\\|\\mathbf{z}_{s}\\|^{2}\\leq4C_{M}\\Gamma^{2}\\ln(K/\\delta)+C_{M}\\sum_{s=1}^{\\mathsf{U P}(t)}q_{s}+{\\frac{C_{M}t}{4\\Gamma^{2}}}\\sum_{s=1}^{\\mathsf{U P}(t)}p_{s}^{2}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Simplifying the above using equations (13), (14) and the definition of $\\Gamma$ , we obtain the following inequality which holds with probability at least $1-\\delta/2$ uniformly for every $t\\in[T]$ : ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\sum_{i=1}^{t}\\|{\\bf z}_{s}\\|^{2}\\leq4C_{M}\\mu^{2}R_{T,\\delta}+C_{M}\\displaystyle\\sum_{s=1}^{\\lfloor\\mathsf{P}(t)}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}\\mathsf{T r}(\\Sigma)+\\frac{3C_{M}}{4}\\displaystyle\\sum_{s=1}^{\\lfloor\\mathsf{P}(t)}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{4A-4}\\frac{t\\ln(K/\\delta)^{2}\\|}{\\mu^{2}R_{T,\\delta}}}}\\\\ {{\\displaystyle\\qquad+\\,\\frac{3C_{M}}{4}\\displaystyle\\sum_{s=1}^{\\lfloor\\mathsf{P}(t)}\\frac{(s+\\gamma)^{4A-12}}{(t+\\gamma)^{4A-4}}\\cdot256t\\kappa^{8}C^{4}\\ln(K/\\delta)^{6}\\mu^{2}R_{T,\\delta}}}\\\\ {{\\displaystyle\\qquad+\\,\\frac{3C_{M}}{4}\\displaystyle\\sum_{s=1}^{\\lfloor\\mathsf{P}(t)}\\frac{(s+\\gamma)^{4A-8}}{(t+\\gamma)^{4A-4}}\\frac{16t\\kappa^{4}C^{2}\\ln(K/\\delta)^{6}\\mathsf{T r}(\\Sigma)^{2}}{\\mu^{2}R_{T,\\delta}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We now simplify each term in the above inequality by using the fact that $\\mathsf{U P}(t)\\leq\\operatorname*{min}\\{T,2t\\}$ . To this end, the second term is simplified as follows by using the fact that $A\\geq1$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{\\mathsf{U P}(t)}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{4A-4}\\mathsf{T r}(\\Sigma)\\leq\\mathsf{U P}(t)\\mathsf{T r}(\\Sigma)\\leq\\mu^{2}R_{T,\\delta}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "We now control the third term as follows using the definition of $R_{T,\\delta}$ and the fact that $A\\geq1$ : ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{s=1}^{\\mathsf{U P}(t)}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{4A-4}\\frac{t\\ln(K/\\delta)^{2}\\|\\Sigma\\|^{2}}{\\mu^{2}R_{T,\\delta}}\\le\\mu^{2}R_{T,\\delta}\\cdot\\frac{t\\mathsf{U P}(t)}{d(T+\\gamma)^{2}}}}\\\\ &{}&{\\le\\mu^{2}R_{T,\\delta}\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "To control the fourth term, we use the fact that $A\\geq3$ and note that for $s\\leq2t,\\,(s+\\gamma)\\leq2(t+\\gamma)$ ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{s=1}^{\\mathbb{D}(t)}\\frac{(s+\\gamma)^{4A-12}}{(t+\\gamma)^{4A-4}}\\cdot256t\\kappa^{8}C^{4}\\ln(K/\\delta)^{6}\\mu^{2}R_{T,\\delta}\\leq\\mu^{2}R_{T,\\delta}2^{8}\\kappa^{8}C^{4}\\ln(K/\\delta)^{6}\\displaystyle\\sum_{s=1}^{2t}\\frac{(s+\\gamma)^{4A-12}}{(t+\\gamma)^{4A-4}}}&{}\\\\ {\\leq\\mu^{2}R_{T,\\delta}\\cdot\\frac{t^{2}2^{4A-3}\\kappa^{8}C^{4}\\ln(K/\\delta)^{6}}{(t+\\gamma)^{8}}}&{}\\\\ {\\leq\\mu^{2}R_{T,\\delta}\\cdot\\frac{2^{4A-3}\\kappa^{8}C^{4}\\ln(K/\\delta)^{6}}{(t+\\gamma)^{6}}}&{}\\\\ {\\leq\\mu^{2}R_{T,\\delta}2^{4A-15}}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the last inequality follows by setting $\\gamma\\geq4\\kappa^{4/3}C^{4/3}\\ln(K/\\delta)$ ", "page_idx": 27}, {"type": "text", "text": "We control the last term by a similar argument ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{s=1}^{\\vert P(t)}\\frac{(s+\\gamma)^{4A-8}}{(t+\\gamma)^{4A-4}}\\frac{16t\\kappa^{4}C^{2}\\ln(K/\\delta)^{6}\\mathsf{T r}(\\Sigma)^{2}}{\\mu^{2}R_{T,\\delta}}\\le\\mu^{2}R_{T,\\delta}\\cdot\\frac{t}{(T+\\gamma)^{2}}\\cdot2^{4}\\kappa^{4}C^{2}\\ln(K/\\delta)^{6}\\sum_{s=1}^{2t}\\frac{(s+\\gamma)^{4A-4}}{(t+\\gamma)^{4A-4}}}}\\\\ &{}&{\\le\\frac{t^{2}}{(T+\\gamma)^{2}(t+\\gamma)^{4}}\\cdot2^{4A-3}\\kappa^{4}C^{2}\\ln(K/\\delta)^{6}}\\\\ &{}&{\\le2^{4A-11}\\mu^{2}R_{T,\\delta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "where the last inequality follows by setting $\\gamma\\geq4\\kappa\\sqrt{C}\\ln(K/\\delta)^{3/2}$ . Substituting the obtained bounds into equation (15), we conclude that the following holds with probability at least $1-\\delta/2$ uniformly for every $t\\in[T]$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}\\|\\tilde{\\mathbf{v}}_{s}\\|^{2}=\\sum_{s=1}^{t}\\|\\mathbf{z}_{s}\\|^{2}\\leq C_{M}\\mu^{2}R_{T,\\delta}\\left(6+3\\cdot2^{4A-13}+3\\cdot2^{4A-17}\\right)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "The proof is completed via a union bound. ", "page_idx": 28}, {"type": "text", "text": "C Analysis for Smooth Strongly Convex Functions Under Quadratic Growth Noise Model ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Following a convention similar to that of Section B, let $K=4\\,\\mathrm{max}\\{8,C_{M},\\mathrm{ln}(T)\\}$ . For $t\\geq1$ , define the filtration $\\mathcal{F}_{t}\\,=\\,\\sigma\\,(\\mathbf{x}_{1},\\mathbf{g}_{s}|1\\,\\le\\,s\\le t)$ and ${\\mathcal F}_{0}\\;=\\;\\sigma({\\bf x}_{1})$ . Furthermore, let $\\nabla F({\\bf x}_{t})\\ =$ $\\mathsf{c l i p}_{\\Gamma}(\\mathbf{g}_{t})\\!+\\!\\mathbf{b}_{t}\\!+\\!\\mathbf{v}_{t}$ where $\\mathbf{b}_{t}=\\nabla F(\\mathbf{x}_{t})\\!-\\!\\mathbb{E}[{\\mathsf{c l i p}}_{\\Gamma}(\\mathbf{g}_{t})|\\mathcal{F}_{t-1}]$ and $\\mathbf{v}_{t}=\\mathbb{E}[{\\mathsf{c l i p}}_{\\Gamma}(\\mathbf{g}_{t})|{\\mathcal{F}}_{t-1}]\\!-\\!{\\mathsf{c l i p}}_{\\Gamma}(\\mathbf{g}_{t})$ . As beforem, we note that $\\mathbb{E}[\\mathbf{v}_{t}|\\mathcal{F}_{t-1}]=0$ and $\\|\\mathbf{v}_{t}\\|\\leq2\\Gamma$ . Hence $\\mathbf{v}_{t}$ is an $\\mathcal{F}$ adapted almost surely bounded martingale difference sequence. Now, let $D_{t}=\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|$ where $\\mathbf{x}^{*}$ is the unique minimizer of $F$ (guaranteed by strong convexity). We also define $\\Sigma_{t}=\\Sigma(\\mathbf{x}_{t})$ and note that $\\|\\Sigma_{t}\\|\\le\\alpha D_{t}^{2}+\\beta$ and Tr(\u03a3t) \u2264deff \u03b1Dt2 + \u03b2 . Furthermore \u03a3t is Ft\u22121 measurable. Let \u03b7t =t+A\u03b3 where A \u22651 is a numerical constant and $\\gamma\\geq A\\kappa+A-1$ is a constant depending on $\\kappa,d$ and $\\ln(1/\\delta)$ which we shall specify later. Note that our choice of \u03b3 ensures that \u03b7t \u2264 L1+\u00b5 for $t\\in[1:T]$ An application of Lemma 5 shows that $D_{t}$ satisfies the following for every $t\\in[1:T]$ ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{t+1}^{2}\\leq\\left(\\displaystyle\\frac{\\gamma+1}{t+\\gamma}\\right)^{2A}D_{1}^{2}+\\displaystyle\\frac{A2^{2A+1}}{\\mu}\\displaystyle\\sum_{s=1}^{t}\\displaystyle\\frac{(s+\\gamma-1)^{2A-1}}{(t+\\gamma)^{2A}}\\left\\langle\\mathbf{b}_{s},\\mathbf{x}_{s}-\\mathbf{x}^{*}\\right\\rangle}\\\\ &{\\qquad+\\displaystyle\\frac{A^{2}4^{A+1}}{\\mu^{2}}\\displaystyle\\sum_{s=1}^{t}\\|\\mathbf{b}_{s}\\|^{2}\\displaystyle\\frac{(s+\\gamma)^{2A-2}}{(t+\\gamma)^{2A}}+\\displaystyle\\frac{A2^{2A+1}}{\\mu}\\displaystyle\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{2A-1}}{(t+\\gamma)^{2A}}\\left\\langle\\mathbf{v}_{s},\\mathbf{x}_{s}-\\mathbf{x}^{*}\\right\\rangle}\\\\ &{\\qquad+\\displaystyle\\frac{A^{2}4^{A+1}}{\\mu^{2}}\\displaystyle\\sum_{s=1}^{t}\\|\\mathbf{v}_{s}\\|^{2}\\displaystyle\\frac{(s+\\gamma)^{2A-2}}{(t+\\gamma)^{2A}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We now define $R_{T,\\delta}$ as follows: ", "page_idx": 28}, {"type": "equation", "text": "$$\nR_{T,\\delta}=(\\gamma+1)^{2}D_{1}^{2}+\\frac{(T+\\gamma)\\beta}{\\mu^{2}}\\left(d_{\\mathsf{e f f}}+\\sqrt{d_{\\mathsf{e f f}}}\\ln(K/\\delta)\\right)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "It is easy to see that =\u00b5ln(KR/T\u03b4,)\u03b4 . In our proof of Theorem 1, we shall establish that the following holds with probability at least $1-\\delta$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\nD_{t}^{2}\\leq\\frac{C R_{T,\\delta}}{(t+\\gamma-1)^{2}}\\;\\forall\\,t\\in[1:T+1]\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $C>0$ is an absolute numerical constant to be chosen later. To this end, we define the event $E_{t}$ and the $\\mathcal{F}_{t}$ measurable random variables $\\mathbf{d}_{t},\\tilde{\\mathbf{b}}_{t},\\tilde{\\mathbf{v}}_{t}$ as follows for $t\\in[1:T+1]$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\cal E}_{t}=\\left\\{D_{t}^{2}\\leq\\displaystyle\\frac{C R_{T,\\delta}}{(t+\\gamma-1)^{2}}\\right\\}}\\\\ &{{\\bf d}_{t}=({\\bf x}_{t}-{\\bf x}^{*})\\mathbb{1}\\left\\{{\\cal E}_{t}\\right\\}}\\\\ &{\\tilde{{\\bf b}}_{t}={\\bf b}_{t}\\mathbb{1}\\left\\{{\\cal E}_{t}\\right\\}}\\\\ &{\\tilde{{\\bf v}}_{t}={\\bf v}_{t}\\mathbb{1}\\left\\{{\\cal E}_{t}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "We use the following Lemma to control the bias vector $\\tilde{\\mathbf{b}}_{t}$ ", "page_idx": 28}, {"type": "text", "text": "Lemma 8 (Bias Control). The following holds almost surely for every $t\\in[1:T]$ : ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\|\\tilde{\\mathbf{b}}_{t}\\|\\leq\\mu\\sqrt{R_{T,\\delta}}\\sum_{j=1}^{7}B_{j}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $B_{1},\\ldots,B_{7}$ are defined as follows: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{1}=\\cfrac{1}{T+\\gamma},}\\\\ &{B_{2}=\\cfrac{4\\pi\\sqrt{C}\\ln(1\\pi/\\gamma/\\delta)}{\\rho\\gamma(\\delta+\\gamma)^{2}}\\,,}\\\\ &{B_{3}=\\cfrac{2\\pi\\sqrt{C}\\ln(1\\pi/\\gamma/\\delta)}{(\\delta+\\gamma)\\sqrt{(d+\\gamma)}}\\,,}\\\\ &{B_{4}=\\cfrac{4\\pi C\\ln(1\\pi/\\gamma)\\sqrt{\\delta}}{\\rho(\\delta+\\gamma)^{2}}\\,,}\\\\ &{B_{5}=\\cfrac{8\\pi\\sqrt{3}\\gamma^{2}\\ln(1\\pi/(\\gamma/\\delta))^{2}}{(\\delta+\\gamma)^{3}}\\,,}\\\\ &{B_{6}=\\cfrac{2\\pi\\sqrt{C}\\ln(1\\pi/\\gamma)^{2}}{(\\delta+\\gamma)(T+\\gamma)}\\,,}\\\\ &{B_{7}=\\cfrac{8\\pi\\operatorname{d}(1\\pi/\\gamma/\\delta)^{2}\\gamma^{3}}{(\\delta+\\gamma)(T+\\gamma)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We use the following lemma to control the variance vector $\\tilde{{\\bf v}}_{t}$ . The proof of this lemma, which uses Freedman\u2019s inequality and the PAC Bayesian martingale concentration inequality of Corollary 6. ", "page_idx": 29}, {"type": "text", "text": "Lemma 9 (Variance Control). The following holds with probability at least $1-\\delta$ uniformly for every $t\\in[T]$ for $A\\geq3$ and $\\begin{array}{r}{\\gamma\\ge4C\\operatorname*{max}\\lbrace\\frac{\\alpha d_{\\mathsf{e f f}}}{\\mu^{2}}}\\end{array}$ , \u03b1 ln\u00b5(2K/\u03b4), \u03ba4/3 ln(K/\u03b4), \u03ba ln(K/\u03b4)3/2, $\\frac{\\kappa^{2/3}d_{\\mathsf{e f f}}^{1/3}\\alpha^{1/3}}{\\mu^{2/3}}\\ln(K/\\delta)\\}$ $\\begin{array}{r l}&{\\displaystyle\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{2A-1}}{(t+\\gamma)^{2A-2}}\\left\\langle\\tilde{\\mathbf{v}}_{s},\\mathbf{d}_{s}\\right\\rangle\\lesssim34\\cdot\\mu R_{T,\\delta}\\sqrt{C}}\\\\ &{\\displaystyle\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}\\|\\tilde{\\mathbf{v}}_{s}\\|^{2}\\lesssim C_{M}\\left(2^{4A-3}\\frac{25}{4}+5\\cdot2^{4A-11}+5\\cdot2^{4A-16}+5\\cdot2^{4A-13}\\right)\\mu^{2}R_{T,\\delta}}\\end{array}$ ", "page_idx": 29}, {"type": "text", "text": "where $C_{M}$ is the absolute numerical constant defined in Corollary 5. ", "page_idx": 29}, {"type": "text", "text": "Equipped with this bound on the bias and the variance, we now present the complete proof as follows: ", "page_idx": 29}, {"type": "text", "text": "C.1 Proof of Theorem 2 ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "$\\begin{array}{r}{A\\ \\geq\\ 3,\\ \\gamma\\ \\geq\\ 4C\\operatorname*{max}\\{\\frac{\\alpha d_{\\mathrm{eff}}}{\\mu^{2}},\\frac{\\alpha\\ln(K/\\delta)}{\\mu^{2}},\\kappa^{4/3}\\ln(K/\\delta),\\kappa\\ln(K/\\delta)^{3/2},\\frac{\\kappa^{2/3}d_{\\mathrm{eff}}^{1/3}\\alpha^{1/3}}{\\mu^{2/3}}\\ln(K/\\delta)\\}.}\\end{array}$ Now, let $E$ denote the following event ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r}{E=\\{\\displaystyle\\sum_{s=1}^{t}\\displaystyle\\frac{(s+\\gamma)^{2A-1}}{(t+\\gamma)^{2A-2}}\\left<\\tilde{\\mathbf{v}}_{s},\\mathbf{d}_{s}\\right>\\leq34\\cdot\\mu R_{T,\\delta}\\sqrt{C}\\,\\forall\\,t\\in[T]}\\\\ {\\displaystyle\\sum_{s=1}^{t}\\left(\\displaystyle\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}\\|\\tilde{\\mathbf{v}}_{s}\\|^{2}\\leq53\\cdot C_{M}\\mu^{2}R_{T,\\delta}\\,\\forall t\\in[T]\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Note that by Lemma 9, $\\mathbb{P}(E)\\ge1-\\delta$ . We now claim that $\\mathbb{P}\\left(\\bigcap_{t=1}^{T+1}E_{t}|E\\right)=1$ , i.e., conditioned on the event $E$ , the following holds almost surely for every $t\\in[1:T+1]$ ", "page_idx": 29}, {"type": "equation", "text": "$$\nD_{t}^{2}\\leq\\frac{C R_{T,\\delta}}{(t+\\gamma-1)^{2}}\\;\\forall\\,t\\in[1:T+1]\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We prove the above claim by induction. Note that the claim is trivially true for $t=1$ as $R_{T,\\delta}\\ge$ $(\\gamma+1)^{2}D_{1}^{2}$ . Now, consider any $t\\in[1:T]$ and suppose the claim holds for some $1\\leq s\\leq t$ . ", "page_idx": 29}, {"type": "text", "text": "Recall that by Lemma 5 ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(t+\\gamma)^{2}D_{t+1}^{2}\\leq\\frac{(\\gamma+1)^{2A}}{(t+\\gamma)^{2A-2}}D_{1}^{2}+\\displaystyle\\frac{A2^{2A+1}}{\\mu}\\sum_{s=1}^{t}\\frac{(s+\\gamma-1)^{2A-1}}{(t+\\gamma)^{2A-2}}\\left\\langle\\mathbf{b}_{s},\\mathbf{x}_{s}-\\mathbf{x}^{*}\\right\\rangle}\\\\ &{\\quad\\quad\\quad\\quad+\\displaystyle\\frac{A^{2}4^{A+1}}{\\mu^{2}}\\sum_{s=1}^{t}\\|\\mathbf{b}_{s}\\|^{2}\\frac{(s+\\gamma)^{2A-2}}{(t+\\gamma)^{2A-2}}+\\displaystyle\\frac{A2^{2A+1}}{\\mu}\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{2A-1}}{(t+\\gamma)^{2A-2}}\\left\\langle\\mathbf{v}_{s},\\mathbf{x}_{s}-\\mathbf{x}^{*}\\right\\rangle}\\\\ &{\\quad\\quad\\quad\\quad+\\displaystyle\\frac{A^{2}4^{A+1}}{\\mu^{2}}\\sum_{s=1}^{t}\\|\\mathbf{v}_{s}\\|^{2}\\frac{(s+\\gamma)^{2A-2}}{(t+\\gamma)^{2A-2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Under the induction hypothesis, $\\mathbb{1}\\left\\{E_{s}\\right\\}\\:=\\:1\\;\\forall s\\:\\in\\:[t]$ . Hence, Under the induction hypothesis, $\\begin{array}{r}{\\mathbb{1}\\left\\{D_{s}^{2}\\le\\frac{C R_{T,\\delta}}{(s+\\gamma-1)(s+\\gamma-2)}\\right\\}\\;=\\;1}\\end{array}$ and thus, $\\mathbf{d}_{s}\\;=\\;\\mathbf{x}_{s}\\;-\\;\\mathbf{x}^{*},\\mathbf{b}_{s}\\;=\\;\\tilde{\\mathbf{b}}_{s},\\mathbf{v}_{s}\\;=\\;\\tilde{\\mathbf{v}}_{s}\\;\\forall\\;1\\;\\leq\\;s\\;\\leq\\;t.$ Substituting this transformation into the above inequality, we obtain the following: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(t+\\gamma)^{2}D_{t+1}^{2}\\leq\\underbrace{\\frac{(\\gamma+1)^{2A}}{(t+\\gamma)^{2A-2}}D_{1}^{2}}_{\\textcircled{(1+\\gamma)^{2A-2}}}+\\underbrace{\\frac{A2^{2A+1}}{\\mu}\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{2A-1}}{(t+\\gamma)^{2A-2}}\\langle\\tilde{\\mathbf{v}}_{s},\\mathbf{d}_{s}\\rangle}_{\\textcircled{(2)}}}\\\\ &{\\qquad\\qquad+\\underbrace{\\frac{A^{2}A^{A+1}}{\\mu^{2}}\\sum_{s=1}^{t}\\|\\tilde{\\mathbf{v}}_{s}\\|^{2}\\frac{(s+\\gamma)^{2A-2}}{(t+\\gamma)^{2A-2}}}_{\\textcircled{(1)}}+\\underbrace{\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{2A-1}}{(t+\\gamma)^{2A-2}}\\left\\langle\\tilde{\\mathbf{b}}_{s},\\mathbf{d}_{s}\\right\\rangle}_{\\textcircled{(1+\\gamma)^{2A-2}}}}\\\\ &{\\qquad\\qquad+\\underbrace{\\frac{A^{2}A^{A+1}}{\\mu^{2}}\\sum_{s=1}^{t}\\|\\tilde{\\mathbf{b}}_{s}\\|^{2}\\frac{(s+\\gamma)^{2A-2}}{(t+\\gamma)^{2A-2}}}_{\\textcircled{(1+\\gamma)^{2A-2}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We now bound each of the terms in the RHS as follows. ", "page_idx": 30}, {"type": "text", "text": "Bounding $\\boldsymbol{\\Phi}$ Since $A\\geq1$ and $t\\geq1$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\mathbb{O}=\\frac{(\\gamma+1)^{2A}}{(t+\\gamma)^{2A-2}}D_{1}^{2}\\leq(\\gamma+1)^{2}D_{1}^{2}\\leq R_{T,\\delta}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Bounding $\\circleddash$ Since $\\gamma$ and $A$ satisfy the conditions of Lemma 7 and we have conditioned on the event $E$ , it follows that: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{A2^{2A+1}}{\\mu}\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{2A-1}}{(t+\\gamma)^{2A-2}}\\left\\langle\\tilde{\\mathbf{v}}_{s},\\mathbf{d}_{s}\\right\\rangle\\leq17A4^{A+1}R_{T,\\delta}\\sqrt{C}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Bounding $\\circledast$ Since $\\gamma$ and $A$ satisfy the conditions of Lemma 7 and we have conditioned on the event $E$ , it follows that: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{4^{2}4^{A+1}}{\\mu^{2}}\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}\\|\\tilde{\\mathbf{v}}_{s}\\|^{2}\\leq A^{2}2^{2A+2}C_{M}\\left(2^{4A-3}\\frac{25}{4}+5\\cdot2^{4A-11}+5\\cdot2^{4A-16}+5\\cdot2^{4A-15}\\right).\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Bounding $\\circledast$ Since $1\\{E_{s}\\}=1$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\|\\mathbf{d}_{s}\\|\\leq\\frac{\\sqrt{C R_{T,\\delta}}}{s+\\gamma-1}\\leq\\frac{2\\sqrt{C R_{T,\\delta}}}{s+\\gamma}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Hence, by Lemma 8 ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{A2^{2A+1}}{\\mu}\\sum_{s=1}^{t}\\left<\\tilde{\\mathbf{b}}_{s},\\mathbf{d}_{s}\\right>\\frac{(s+\\gamma)^{2A-1}}{(t+\\gamma)^{2A-2}}\\leq A2^{2A+2}R_{T,\\delta}\\sqrt{C}\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}\\sum_{j=1}^{7}B_{j}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We now control the first term ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}B_{1}=\\frac{1}{T+\\gamma}\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}}\\\\ {\\displaystyle\\leq\\frac{t}{T+\\gamma}\\leq1}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the first inequality follows from the fact that $A\\geq1$ and $s\\leq t$ . ", "page_idx": 31}, {"type": "text", "text": "We now control the second term ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}B_{2}\\leq\\frac{4\\alpha C\\sqrt{d}\\ln(K/\\delta)}{\\mu^{2}}\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{2A-4}}{(t+\\gamma)^{2A-2}}}}\\\\ &{}&{\\leq\\frac{4\\alpha C\\sqrt{d}\\ln(K/\\delta)}{\\mu^{2}(t+\\gamma)}\\leq1}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where the first inequality fo\u221allows from the fact that $A\\ge2$ and $s\\leq t$ and the second inequality follows by setting \u03b3 \u22654\u03b1C d\u00b5 2ln(K/\u03b4). ", "page_idx": 31}, {"type": "text", "text": "We now bound the third term as follows: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}B_{3}\\leq\\frac{2\\kappa\\sqrt{C}\\ln\\left(K/\\delta\\right)}{\\sqrt{d(T+\\gamma)}}\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{2A-3}}{(t+\\gamma)^{2A-2}}}}\\\\ &{}&{\\leq\\frac{2\\kappa\\sqrt{C}\\ln\\left(K/\\delta\\right)}{\\sqrt{d(T+\\gamma)}}\\leq1}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where we use the fact that A \u22652 and set \u03b3 \u22654\u03ba2 lnd(K/\u03b4)2. ", "page_idx": 31}, {"type": "text", "text": "We now bound the fourth term as follows: ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}B_{4}\\leq\\frac{4\\kappa C\\ln(K/\\delta)\\sqrt{\\alpha}}{\\mu}\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{2A-4}}{(t+\\gamma)^{2A-2}}}}\\\\ &{}&{\\leq\\frac{4\\kappa C\\ln(K/\\delta)\\sqrt{\\alpha}}{\\mu(t+\\gamma)}\\leq1}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $A\\ge2$ and $\\gamma\\ge\\frac{4\\kappa C\\ln(K/\\delta)\\sqrt{\\alpha}}{\\mu}$ ", "page_idx": 31}, {"type": "text", "text": "We now bound the fifth term as follows ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2}B_{5}\\leq8\\kappa^{3}C^{3/2}\\ln(K/\\delta)^{2}\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{2A-5}}{(t+\\gamma)^{[2A-2]}}}}\\\\ &{}&{\\leq\\frac{8\\kappa^{3}C^{3/2}\\ln(K/\\delta)^{2}}{(t+\\gamma)^{2}}\\leq1\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $A\\geq3$ and $\\gamma\\geq4\\kappa^{3/2}C^{3/4}\\ln(K/\\delta)$ . ", "page_idx": 31}, {"type": "text", "text": "We now bound the sixth term as follows ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}B_{6}\\leq\\frac{2\\kappa\\sqrt{C}\\ln\\left(K/\\delta\\right)^{2}}{T+\\gamma}\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{2A-3}}{(t+\\gamma)^{2A-2}}}}\\\\ &{}&{\\leq\\frac{2\\kappa\\sqrt{C}\\ln\\left(K/\\delta\\right)^{2}}{T+\\gamma}\\leq1}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where $A\\geq3$ and $\\gamma\\geq2\\kappa\\sqrt{C}\\ln(K/\\delta)^{2}$ ", "page_idx": 31}, {"type": "text", "text": "Finally, we control the seventh term as follows ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}B_{7}\\leq\\frac{8\\alpha\\kappa d\\ln(^{K}/\\delta)^{2}C^{3/2}}{\\mu^{2}}\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{2A-5}}{(t+\\gamma)^{2A-2}}}}\\\\ &{}&{\\leq\\frac{8\\alpha\\kappa d\\ln(K/\\delta)^{2}C^{3/2}}{\\mu^{2}(t+\\gamma)^{2}}\\leq1\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "where A \u22653 and \u03b3 \u22654 \u03b1\u03bad ln\u00b5(K/\u03b4)C3/4. Putting it all together, it follows that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\emptyset\\le7A4^{A+1}R_{T,\\delta}{\\sqrt{C}}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "by setting $\\gamma$ as follows ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\langle\\geq4C\\operatorname*{max}\\left\\{\\frac{\\alpha\\sqrt{d}\\ln(K/\\delta)}{\\mu^{2}},\\frac{\\kappa^{2}\\ln(K/\\delta)^{2}}{d},\\frac{\\kappa\\sqrt{\\alpha}\\ln(K/\\delta)}{\\mu},\\kappa^{3/2}\\ln(K/\\delta),\\kappa\\ln(K/\\delta)^{2},\\frac{\\sqrt{\\kappa\\alpha d}\\ln(K/\\delta)}{\\mu}\\right\\}\\rangle.\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Bounding $\\circleddash$ By Lemma 8 and Jensen\u2019s inequality ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\|\\tilde{\\mathbf{b}}_{s}\\|^{2}\\leq7\\mu^{2}R_{T,\\delta}\\sum_{j=1}^{7}B_{j}^{2}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "It follows that ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\frac{A^{2}2^{2A+2}}{\\mu^{2}}\\sum_{s=1}^{t}\\|\\tilde{\\mathbf{b}}_{s}\\|^{2}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}\\leq7A^{2}2^{2A+2}R_{T,\\delta}\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}\\sum_{j=1}^{7}B_{j}^{2}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The first term is controlled as follows using the fact that $A\\geq1$ ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}B_{1}^{2}=\\sum_{s=1}^{t}\\frac{1}{(T+\\gamma)^{2}}\\le1\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "The second term is controlled as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}B_{2}^{2}\\leq\\frac{16\\alpha^{2}C^{2}d\\ln(K/\\delta)^{2}}{\\mu^{4}}\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{2A-6}}{(t+\\gamma)^{2A-2}}}}\\\\ &{}&{\\leq\\frac{16\\alpha^{2}C^{2}d\\ln(K/\\delta)^{2}}{\\mu^{4}(t+\\gamma)^{3}}\\leq1\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $A\\geq3$ and $\\begin{array}{r}{\\gamma\\ge\\frac{2^{4/3}\\alpha^{2/3}C^{2/3}d^{1/3}\\ln\\left(K/\\delta\\right)^{2/3}}{\\mu^{4/3}}}\\end{array}$ ", "page_idx": 32}, {"type": "text", "text": "The third term is controlled as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}B_{3}^{2}=\\frac{4\\kappa^{2}C\\ln(K/\\delta)^{2}}{d(T+\\gamma)}\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{2A-4}}{(t+\\gamma)^{2A-2}}}}\\\\ &{}&{\\leq\\frac{4\\kappa^{2}C\\ln(K/\\delta)^{2}}{d(t+\\gamma)(T+\\gamma)}\\leq1}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the last inequality follows because $\\begin{array}{r}{\\gamma\\geq\\kappa\\sqrt{\\frac{C}{d}}\\ln(K/\\delta)}\\end{array}$ ", "page_idx": 32}, {"type": "text", "text": "The fourth term is controlled as ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}B_{4}^{2}\\leq\\frac{16\\kappa^{2}C^{2}\\ln(K/\\delta)^{2}\\alpha}{\\mu^{2}}\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{2A-6}}{(t+\\gamma)^{2A-2}}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where $A\\geq3$ and $\\gamma\\geq\\frac{2^{4/3}\\kappa^{2/3}C^{2/3}\\ln\\bigl(K/\\delta\\bigr)^{2/3}\\alpha^{1/3}}{\\mu^{2/3}}$ For controlling the fifth term, we set $A\\geq4$ to obtain ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}B_{5}^{2}=\\kappa^{6}C^{3}\\ln(K/\\delta)^{4}\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{2A-8}}{(t+\\gamma)^{2A-2}}}}\\\\ &{}&{\\leq\\frac{\\kappa^{6}C^{3}\\ln\\left(K/\\delta\\right)^{4}}{(\\gamma+1)^{5}}\\leq1}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where the last inequality uses the fact that $\\gamma\\geq\\kappa^{6/5}C^{3/5}\\ln(K/\\delta)^{4/5}$ ", "page_idx": 32}, {"type": "text", "text": "To control the sixth term, we use the fact that $A\\ge2$ to obtain ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}B_{6}^{2}=\\frac{\\kappa^{2}C\\ln(K/\\delta)^{4}}{(T+\\gamma)^{2}}\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{2A-4}}{(t+\\gamma)^{2A-2}}}}\\\\ &{}&{\\leq\\frac{\\kappa^{2}C\\ln\\left(K/\\delta\\right)^{4}}{(\\gamma+1)^{3}}\\leq1\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the last inequality uses the fact that $\\gamma\\geq\\kappa^{2/3}C^{1/3}\\ln(K/\\delta)^{4/3}$ ", "page_idx": 33}, {"type": "text", "text": "To control the seventh term, we set $A\\geq4$ to obtain the following: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}B_{6}^{2}=\\frac{64\\alpha^{2}\\kappa^{2}d\\ln(K/\\delta)^{4}C^{3}}{\\mu^{4}}\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{2A-8}}{(t+\\gamma)^{2A-2}}}}\\\\ &{}&{\\leq\\frac{64\\alpha^{2}\\kappa^{2}d\\ln(K/\\delta)^{4}C^{3}}{\\mu^{4}(t+\\gamma)^{5}}\\leq1\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\begin{array}{r l r}{\\gamma}&{\\ge}&{\\frac{2^{6/5}\\alpha^{2/5}\\kappa^{2/5}d^{1/5}\\ln\\left(K/\\delta\\right)^{4/5}C^{3/5}}{\\mu^{4/5}}}\\end{array}$ From the obtained bounds, we conclude that $\\circled{5}\\leq$ $49A^{2}4^{A+1}R_{T,\\delta}$ . ", "page_idx": 33}, {"type": "text", "text": "Now, we set $A=4$ and $\\gamma$ as follows: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbf{\\Psi}:=\\operatorname*{max}\\left\\{\\frac{\\alpha d}{\\mu^{2}},\\frac{\\alpha\\sqrt{d}\\ln(K/\\delta)}{\\mu^{2}},\\frac{\\kappa\\sqrt{\\alpha}\\ln(K/\\delta)}{\\mu^{2}},\\frac{\\sqrt{\\kappa\\alpha d}\\ln(K/\\delta)}{\\mu},\\frac{\\kappa^{2/3}d^{1/3}\\alpha^{1/3}\\ln(K/\\delta)}{\\mu^{2/3}},\\kappa^{3/2}\\ln(K/\\delta),\\kappa\\ln(K/\\delta)\\right\\},\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Under this setting of $A$ and $\\gamma$ , we obtain the following ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{(t+\\gamma)^{2}D_{t+1}^{2}\\leq\\mathbb{()}+\\mathbb{()}+\\mathbb{()}+\\mathbb{()}+\\mathbb{()}}\\\\ &{\\leq R_{T,\\delta}[1+A^{2}2^{2A+2}C_{M}\\left(2^{4A-3}\\frac{25}{4}+5\\cdot2^{4A-11}+5\\cdot2^{4A-16}+5\\cdot2^{4A-13}\\right)}\\\\ &{\\hphantom{=\\;\\;}+49A^{2}4^{A+1}+24A4^{A+1}\\sqrt{C}]}\\\\ &{\\hphantom{=\\;\\;}\\leq R_{T,\\delta}\\left(802817+6946816C_{M}+98304\\sqrt{C}\\right)}\\\\ &{\\hphantom{=\\;\\;}\\leq C R_{T,\\delta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where the second inequality holds due to our choice of $A$ and $\\gamma$ and the last inequality is obtained by setting $C=\\left({\\sqrt{802817+6946816C_{M}}}+98304\\right)^{2}$ . It follows that ", "page_idx": 33}, {"type": "equation", "text": "$$\nD_{t+1}^{2}\\leq\\frac{C R_{T,\\delta}}{(t+\\gamma)^{2}}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Thus, we have proved by induction that conditioned on E, Dt2 \u2264(tC+\u03b3RT\u2212,1\u03b4)2 for every $t\\in[T+1]$ . In particular, the following holds with probability at least $1-\\delta$ : ", "page_idx": 33}, {"type": "equation", "text": "$$\nD_{T+1}^{2}\\leq C\\left(\\frac{\\gamma+1}{T+\\gamma}\\right)^{2}D_{1}^{2}+\\frac{C\\beta\\left(d_{\\mathrm{eff}}+\\sqrt{d_{\\mathrm{eff}}}\\ln(K/\\delta)\\right)}{\\mu^{2}(T+\\gamma)}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "C.2 Proof of Lemma 8 ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Following the same steps as in that of the proof of Lemma 6, we use Lemma 4 and the fact that $\\mathsf{C o v}[\\mathbf{g}_{t}|\\bar{\\mathcal{F}}_{t-1}]=\\Sigma_{t}$ to obtain: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\tilde{\\mathbf{b}}_{s}\\|\\leq\\underbrace{\\frac{\\|\\Sigma_{s}\\|\\sqrt{d_{\\mathrm{eff}}}\\,\\mathbb{I}\\left\\{E_{s}\\right\\}}{\\Gamma}}_{\\varnothing}+\\underbrace{\\frac{\\|\\nabla F(\\mathbf{x}_{s})\\|\\sqrt{\\|\\Sigma_{s}\\|}\\,\\{\\mathbb{E}_{s}\\}}{\\Gamma}}_{\\varnothing}+\\underbrace{\\frac{\\|\\nabla F(\\mathbf{x}_{s})\\|^{3}\\,\\mathbb{I}\\left\\{E_{s}\\right\\}}{\\Gamma^{2}}}_{\\varnothing}+\\underbrace{\\frac{\\|\\Sigma_{s}\\|d_{\\mathrm{eff}}\\|\\nabla F(\\mathbf{x}_{s})}{\\Gamma^{2}}}_{\\varnothing}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "Bounding $\\textsuperscript{\\textregistered}$ Note that by Assumption QG $2^{\\mathsf{n d}}$ Moment ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\|\\Sigma_{s}\\|_{2}\\mathbb{1}\\left\\{E_{s}\\right\\}\\leq(\\beta+\\alpha D_{s}^{2})\\mathbb{1}\\left\\{E_{s}\\right\\}}}\\\\ {{\\leq\\beta+\\displaystyle\\frac{4\\alpha C R_{T,\\delta}}{(s+\\gamma)^{2}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "It follows that ", "page_idx": 34}, {"type": "equation", "text": "$$\n{\\frac{\\|\\Sigma_{s}\\|_{2}\\sqrt{d}\\mathbb{1}\\left\\{E_{s}\\right\\}}{\\Gamma}}\\leq{\\frac{\\beta{\\sqrt{d_{\\mathrm{eff}}}}\\ln\\!\\left(K/\\delta\\right)}{\\mu{\\sqrt{R_{T,\\delta}}}}}+{\\frac{4\\alpha C\\ln\\!\\left(K/\\delta\\right){\\sqrt{R_{T,\\delta}d_{\\mathrm{eff}}}}}{\\mu(s+\\gamma)^{2}}}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Since $\\begin{array}{r}{\\beta\\sqrt{d_{\\mathsf{e f f}}}\\ln(K/\\delta)\\le\\frac{\\mu^{2}R_{T,\\delta}}{T+\\gamma}}\\end{array}$ \u00b5RT,\u03b4, we obtain ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbb{\\oplus}=\\frac{\\|\\Sigma\\|_{s}\\sqrt{d}\\mathbb{1}\\left\\{E_{s}\\right\\}}{\\Gamma}\\leq\\mu\\sqrt{R_{T,\\delta}}\\left(\\frac{1}{T+\\gamma}+\\frac{4\\alpha C\\ln(\\kappa/\\delta)\\sqrt{R_{T,\\delta}d_{\\mathrm{eff}}}}{\\mu^{2}(s+\\gamma)^{2}}\\right)\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Bounding $\\textsuperscript{\\textregistered}$ Note that by equation (8), ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{\\|\\nabla F(\\mathbf{x}_{s})\\|\\mathbb{1}\\left\\{E_{s}\\right\\}}{\\Gamma}\\leq\\frac{2\\kappa\\sqrt{C}\\ln(K/\\delta)}{s+\\gamma}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Furthermore, by Assumption QG $2^{\\mathsf{n d}}$ Moment and the definition of $E_{s}$ ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\sqrt{\\|\\Sigma_{s}\\|_{2}}\\mathbb{1}\\left\\{E_{s}\\right\\}\\leq\\sqrt{\\beta}+\\frac{2\\sqrt{\\alpha C R_{T,\\delta}}}{s+\\gamma}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Recalling that $\\begin{array}{r}{\\beta\\leq\\frac{\\mu^{2}R_{T,\\delta}}{d_{\\mathsf{e f f}}(T+\\gamma)}}\\end{array}$ , ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\|\\nabla F({\\mathbf x}_{s})\\|\\sqrt{\\|{\\boldsymbol\\Sigma}_{s}\\|_{2}}\\mathbb{1}\\left\\{E_{s}\\right\\}}{\\Gamma}\\leq\\frac{2\\kappa\\sqrt{C}\\ln(K/\\delta)\\mu\\sqrt{R_{T,\\delta}}}{(s+\\gamma)\\sqrt{d_{\\mathrm{eff}}(T+\\gamma)}}+\\frac{4\\kappa C\\ln(K/\\delta)\\sqrt{\\alpha R_{T,\\delta}}}{(s+\\gamma)^{2}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\mu\\sqrt{R_{T,\\delta}}\\left(\\frac{2\\kappa\\sqrt{C}\\ln(K/\\delta)}{(s+\\gamma)\\sqrt{d_{\\mathrm{eff}}(T+\\gamma)}}+\\frac{4\\kappa C\\ln(K/\\delta)\\sqrt{\\alpha}}{\\mu(s+\\gamma)^{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Bounding $\\circledcirc$ By equation (8), ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\frac{\\|\\nabla F(\\mathbf{x}_{s})\\|^{3}\\mathbb{1}\\left\\{E_{s}\\right\\}}{\\Gamma^{2}}\\leq\\mu\\sqrt{R_{T,\\delta}}\\cdot\\frac{8\\kappa^{3}C^{3/2}\\ln(K/\\delta)^{2}}{(s+\\gamma)^{3}}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Bounding $\\circledcirc$ Since $\\begin{array}{r}{\\beta d\\leq\\frac{\\mu^{2}R_{T,\\delta}}{T+\\gamma}}\\end{array}$ \u00b5 RT,\u03b4, it follows tat ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\left\\|\\nabla F(\\mathbf{x}_{s})\\right\\|\\left\\|\\sum_{s}\\left\\|_{2}d_{\\mathrm{eff}}\\right\\|\\,\\left\\{E_{s}\\right\\}}{\\Gamma^{2}}\\leq\\frac{2\\kappa\\sqrt{C}\\ln(K/\\delta)^{2}}{\\mu\\sqrt{R_{T,\\delta}}(s+\\gamma)}\\left(\\beta d+\\frac{4\\alpha C R_{T,\\delta}d}{(s+\\gamma)^{2}}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\frac{2\\kappa\\sqrt{C}\\ln(K/\\delta)^{2}\\mu\\sqrt{R_{T,\\delta}}}{(s+\\gamma)(T+\\gamma)}+\\frac{8\\alpha\\kappa d\\ln(K/\\delta)^{2}C^{3/2}\\sqrt{R_{T,\\delta}}}{\\mu(s+\\gamma)^{3}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\mu\\sqrt{R_{T,\\delta}}\\left(\\frac{2\\kappa\\sqrt{C}\\ln(K/\\delta)^{2}}{(s+\\gamma)(T+\\gamma)}+\\frac{8\\alpha\\kappa d\\ln(K/\\delta)^{2}C^{3/2}}{\\mu^{2}(s+\\gamma)^{3}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "Hence, we conclude that ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\|\\tilde{\\mathbf{b}}_{t}\\|\\leq\\mathbb{\\emptyset}+\\mathbb{(B)}+\\mathbb{C}+\\mathbb{D}\\leq\\mu\\sqrt{R_{T,\\delta}}\\sum_{j=1}^{7}B_{j}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $B_{1},\\ldots,B_{7}$ are defined as follows: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{B_{1}=\\cfrac{1}{T+\\gamma},}\\\\ &{B_{2}=\\cfrac{4\\pi\\ln(N/\\beta)}{\\rho\\gamma},}\\\\ &{B_{3}=\\cfrac{2\\pi\\ln(1\\pi T/\\beta)}{(\\kappa+\\gamma)\\sqrt{d(T+\\gamma)}}~,}\\\\ &{B_{4}=\\cfrac{4\\pi C\\ln(1\\pi/\\beta)\\sqrt{\\alpha}}{\\rho(s+\\gamma)^{2}},}\\\\ &{B_{5}=\\cfrac{8\\pi\\kappa^{5}\\sigma^{2}\\ln(1\\pi/(\\gamma)\\beta)^{2}}{(s+\\gamma)^{3}},}\\\\ &{B_{6}=\\cfrac{2\\pi\\sqrt{C}\\ln(1\\pi/\\beta)^{2}}{(\\kappa+\\gamma)(T+\\gamma)}~,}\\\\ &{B_{7}=\\cfrac{8\\pi\\operatorname{d}(1\\pi/\\beta)^{2}\\sigma^{2}}{(\\kappa+\\gamma)(T+\\gamma)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "C.3 Proof of Lemma 9 ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "As before, for $s\\in[1:T]$ define $\\mathbb{E}[\\tilde{\\mathbf{v}}_{s}\\tilde{\\mathbf{v}}_{s}^{T}|\\mathcal{F}_{s-1}]=\\tilde{\\Sigma}_{s}$ . Following the same steps as in that of the proof of Lemma 7, we use Lemma 4 and the fact that $\\mathsf{C o v}[\\mathbf{g}_{t}|\\mathcal{F}_{t-1}]=\\Sigma_{t}$ to obtain: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\tilde{\\boldsymbol{\\Sigma}}_{s}\\|_{2}\\leq\\|\\boldsymbol{\\Sigma}_{s}\\|_{2}\\,\\|\\,\\{\\boldsymbol{E}_{s}\\}+\\frac{\\|\\nabla F({\\mathbf x}_{s})\\|^{4}\\,\\|\\,\\{\\boldsymbol{E}_{s}\\}}{\\Gamma^{2}}+\\frac{\\|\\nabla F({\\mathbf x}_{s})\\|^{2}\\top\\tau({\\boldsymbol{\\Sigma}}_{s})\\,\\|\\,\\left\\{{\\boldsymbol{E}}_{s}\\right\\}}{\\Gamma^{2}}}\\\\ &{\\qquad\\leq\\mathbb{1}\\,\\{\\boldsymbol{E}_{t}\\}\\left(\\beta+\\alpha D_{s}^{2}\\right)+\\frac{\\|\\nabla F({\\mathbf x}_{s})\\|^{4}\\,\\|\\,\\left\\{{\\boldsymbol{E}}_{s}\\right\\}}{\\Gamma^{2}}+\\frac{\\mathbb{1}\\,\\left\\{{\\boldsymbol{E}}_{s}\\right\\}\\,\\|\\nabla F({\\mathbf x}_{s})\\|^{2}d_{\\mathrm{eff}}}{\\Gamma^{2}}\\left(\\beta+\\alpha D_{s}^{2}\\right)}\\\\ &{\\qquad\\leq\\beta+\\frac{4\\alpha C R_{T,\\delta}}{(s+\\gamma)^{2}}+\\frac{\\|\\nabla F({\\mathbf x}_{s})\\|^{4}\\,\\mathbb{1}\\,\\left\\{{\\boldsymbol{E}}_{s}\\right\\}}{\\Gamma^{2}}+\\frac{\\|\\nabla F({\\mathbf x}_{t})\\|^{2}d_{\\mathrm{eff}}\\,\\mathbb{1}\\,\\left\\{{\\boldsymbol{E}}_{s}\\right\\}}{\\Gamma^{2}}\\left(\\beta+\\frac{4\\alpha C R_{T,\\delta}}{(s+\\gamma)^{2}}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where the second inequality follows from Assumption QG $2^{\\mathsf{n d}}$ Moment and the second inequality follows by definition of $E_{s}$ ", "page_idx": 35}, {"type": "text", "text": "Furthermore, since ${\\mathsf{c l i p}}_{\\Gamma}$ is a convex projection, the following holds: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{T r}(\\widetilde{\\Sigma}_{s})\\leq\\mathsf{T r}(\\Sigma_{s})\\mathbb{1}\\left\\{E_{s}\\right\\}}\\\\ &{\\qquad\\qquad\\leq d_{\\mathsf{e f f}}\\left(\\beta+\\alpha D_{s}^{2}\\right)\\mathbb{1}\\left\\{E_{s}\\right\\}}\\\\ &{\\qquad\\qquad\\leq\\beta d_{\\mathsf{e f f}}+\\frac{4\\alpha d C R_{T,\\delta}}{(s+\\gamma)^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Now, for $s\\in[t]$ , we define $h_{s}$ as follows: ", "page_idx": 35}, {"type": "equation", "text": "$$\nh_{s}=\\left\\langle\\tilde{\\mathbf{v}}_{s},\\mathbf{d}_{s}\\right\\rangle\\frac{(s+\\gamma)^{2A-1}}{(t+\\gamma)^{2A-2}}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Note that $\\mathbb{E}[h_{s}|\\mathcal{F}_{s-1}]=0$ . Furthermore, since $\\lVert\\tilde{\\mathbf{v}}_{s}\\rVert\\leq2\\Gamma$ and $\\begin{array}{r}{\\|\\mathbf{d}_{s}\\|\\leq\\frac{\\sqrt{C R_{T,\\delta}}}{s+\\gamma-1}}\\end{array}$ ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|h_{s}|\\le2\\Gamma\\cdot\\displaystyle\\frac{\\sqrt{C R_{T,\\delta}}}{s+\\gamma-1}\\cdot\\displaystyle\\frac{(s+\\gamma)^{2A-1}}{(t+\\gamma)^{2A-2}}}\\\\ &{\\qquad\\le4\\Gamma\\sqrt{C R_{T,\\delta}}\\left(\\displaystyle\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}}\\\\ &{\\qquad\\le\\displaystyle\\frac{4\\mu R_{T,\\delta}\\sqrt{C}}{\\ln(K/\\delta)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "For $s\\in[t]$ , define $\\sigma_{s}^{2}=\\mathbb{E}[h_{s}^{2}|\\mathcal{F}_{s-1}]$ . It follows that, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\cdot2}{s}=\\frac{\\left\\{s+\\tau+\\gamma\\right\\}}{\\left(t+\\gamma\\right)^{4A-4}}\\mathbf{v}_{s}^{T}\\tilde{\\Sigma}_{s}\\mathbf{v}_{s}}\\\\ &{\\quad\\leq\\frac{(s+\\gamma)^{4A-2}}{(t+\\gamma)^{4A-4}}\\|\\mathbf{v}_{s}\\|^{2}\\|\\tilde{\\Sigma}_{s}\\|_{2}}\\\\ &{\\quad\\leq4C R_{T,\\delta}\\cdot\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{4A-4}\\|\\tilde{\\Sigma}_{s}\\|_{2}}\\\\ &{\\quad\\leq4C R_{T,\\delta}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{4A-4}\\left[\\beta+\\frac{4\\alpha C R_{T,\\delta}}{(s+\\gamma)^{2}}+\\frac{\\left\\Vert\\nabla F(\\mathbf{x}_{s})\\right\\Vert^{4}\\mathbb{I}\\left\\{E_{s}\\right\\}}{\\Gamma^{2}}+\\frac{\\left\\Vert\\nabla F(\\mathbf{x}_{t})\\right\\Vert^{2}d_{\\mathrm{eff}}\\mathbb{I}\\left\\{E_{s}\\right\\}}{\\Gamma^{2}}\\left(\\beta+\\frac{4\\alpha C R_{T,\\delta}}{(s+\\gamma)^{2}}\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where the last inequality follows from equation (9) and the fact that $d_{\\mathsf{e f f}}=\\mathsf{T r}(\\Sigma)/\\|\\Sigma\\|_{2}$ . We now use the above inequality to control $\\textstyle\\sum_{s=1}^{t}\\sigma_{s}^{2}\\ln(K/\\delta)$ as follows: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{c l}{{\\displaystyle\\sum_{s=1}^{t}{\\sigma_{s}^{2}\\ln(K/\\delta)}\\le4C R_{T,\\delta}\\ln(K/\\delta)\\sum_{s=1}^{t}{\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{4A-4}\\beta}}}\\\\ {{\\displaystyle}}&{{\\displaystyle+4C R_{T,\\delta}\\ln(K/\\delta)\\sum_{s=1}^{t}{\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{4A-6}}4\\alpha C R_{T,\\delta}}}\\\\ {{\\displaystyle}}&{{\\displaystyle+\\left.4C R_{T,\\delta}\\ln(K/\\delta)\\sum_{s=1}^{t}{\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{4A-4}\\frac{\\|\\nabla F(\\mathbf{x}_{s})\\|^{4}\\|\\left\\{E_{s}\\right\\}}{\\Gamma^{2}}}}}\\\\ {{\\displaystyle}}&{{\\displaystyle+\\left.4C R_{T,\\delta}\\ln(K/\\delta)\\sum_{s=1}^{t}{\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{4A-4}\\frac{\\|\\nabla F(\\mathbf{x}_{s})\\|^{2}\\|\\left\\{E_{s}\\right\\}\\beta d_{\\mathrm{eff}}}{\\Gamma^{2}}}}}\\\\ {{\\displaystyle}}&{{\\displaystyle+\\left.4C R_{T,\\delta}\\ln(K/\\delta)\\sum_{s=1}^{t}{\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{4A-4}\\frac{4\\|\\nabla F(\\mathbf{x}_{s})\\|^{2}\\|\\left\\{E_{s}\\right\\}\\alpha d\\!\\!\\!/\\,C}{\\Gamma^{2}}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "We now control each of the five terms in the above inequality as follows ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{{4}C R_{T,\\delta}\\ln(K/\\delta)\\displaystyle\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{4A-4}\\beta\\leq4C R_{T,\\delta}\\ln(K/\\delta)\\beta t}&{}\\\\ {\\qquad\\qquad\\qquad\\qquad\\qquad\\leq4C t R_{T,\\delta}\\cdot\\displaystyle\\frac{\\mu^{2}R_{T,\\delta}}{(T+\\gamma)\\sqrt{d_{\\mathrm{eff}}}}}\\\\ &{\\leq4\\mu^{2}C R_{T,\\delta}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "To control the second term, ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{4C R_{T,\\delta}\\ln(K/\\delta)\\displaystyle\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{4A-6}}{(t+\\gamma)^{4A-4}}4\\alpha C R_{T,\\delta}\\le16C R_{T,\\delta}^{2}\\mu^{2}\\frac{\\alpha C\\ln(K/\\delta)}{\\mu^{2}(t+\\gamma)}}&{{}}&{}\\\\ {4C R_{T,\\delta}^{2}\\mu^{2}}&{{}}&{\\qquad\\qquad\\le16C R_{T,\\delta}^{2}\\mu^{2}\\qquad}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where the second inequality follows by setting $A\\geq3/2$ and the last inequality follows by setting $\\gamma\\geq{\\frac{\\alpha C\\ln(K/\\delta)}{\\mu^{2}}}$ Before controlling the remaining terms, we recall from (8) in the proof of Lemma 6 that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{\\|\\nabla F(\\mathbf{x}_{s})\\|\\,\\mathbb{1}\\,\\{E_{s}\\}\\le\\frac{\\kappa\\Gamma\\ln\\left(K/\\delta\\right)\\sqrt{C}}{s+\\gamma-1}}\\\\ &{}&{\\qquad\\le\\frac{2\\kappa\\Gamma\\ln\\left(K/\\delta\\right)\\sqrt{C}}{s+\\gamma}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "where $\\begin{array}{r}{\\Gamma=\\frac{\\mu\\sqrt{R_{T,\\delta}}}{\\ln\\left(K/\\delta\\right)}}\\end{array}$ . It follows that ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{\\|\\nabla F(\\mathbf{x}_{s})\\|^{4}\\mathbb{1}\\left\\{E_{s}\\right\\}}{\\Gamma^{2}}\\le\\frac{16\\kappa^{4}C^{2}\\Gamma^{2}\\ln\\left(K/\\delta\\right)^{4}}{(s+\\gamma)^{4}}}}\\\\ &{=\\mu^{2}R_{T,\\delta}\\cdot\\frac{16\\kappa^{4}C^{2}\\ln(K/\\delta)^{2}}{(s+\\gamma)^{4}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Thus, we can control the third term in equation (20) as follows ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{,C R_{T,\\delta}\\ln(K/\\delta)\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{4A-4}\\frac{\\|\\nabla F(\\mathbf{x}_{s})\\|^{4}\\mathbb{1}\\left\\{E_{s}\\right\\}}{\\Gamma^{2}}\\le64\\mu^{2}C R_{T,\\delta}^{2}\\cdot\\kappa^{4}C^{2}\\ln(K/\\delta)^{3}\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{4}}{(t+\\gamma)^{4}}}}\\\\ &{}&{\\le64\\mu^{2}C R_{T,\\delta}^{2}\\cdot\\frac{\\kappa^{4}C^{2}\\ln(K/\\delta)^{3}}{(t+\\gamma)^{3}}}\\\\ &{}&{\\le64\\mu^{2}C R_{T,\\delta}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the second inequality follows by setting $A\\ge2$ and the last inequality follows by setting $\\gamma\\geq\\kappa^{4/3}C^{2/3}\\ln(K/\\delta)$ . ", "page_idx": 37}, {"type": "text", "text": "To control the fourth term in (20), we note that by equation (8) and the definition of $R_{T,\\delta}$ ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\frac{\\|\\nabla F(\\mathbf{x}_{s})\\|^{2}d_{\\mathsf{e f f}}\\beta\\mathbb{1}\\left\\{E_{s}\\right\\}}{\\Gamma^{2}}\\leq4\\mu^{2}R_{T,\\delta}\\cdot\\frac{\\kappa^{2}C\\ln(K/\\delta)^{2}}{(T+\\gamma)(s+\\gamma)^{2}}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "It follows that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{,C R_{T,\\delta}\\ln(K/\\delta)\\sum_{s=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{4A-4}\\frac{\\|\\nabla F(\\mathbf{x}_{s})\\|^{2}\\beta d_{\\mathrm{eff}}}{\\Gamma^{2}}\\le16\\mu^{2}C R_{T,\\delta}^{2}\\cdot\\frac{\\kappa^{2}C\\ln(K/\\delta)^{3}}{T+\\gamma}\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{4A-1}}{(t+\\gamma)^{4A-1}}}}\\\\ &{}&{\\le16\\mu^{2}C R_{T,\\delta}^{2}\\cdot\\frac{\\kappa^{2}C\\ln(K/\\delta)^{3}}{(T+\\gamma)(t+\\gamma)}}\\\\ &{}&{\\le16\\mu^{2}C R_{T,\\delta}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where t\u221ahe second inequality follows by setting $A\\geq3/2$ and the last inequality follows by setting $\\gamma\\geq\\kappa\\sqrt{C}\\ln(K/\\delta)^{3/2}$ . ", "page_idx": 37}, {"type": "text", "text": "To control the fifth term in equation (20), we proceed as follows: ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{4C R_{T,\\delta}\\ln(K/\\delta)\\displaystyle\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{4A-6}}{(t+\\gamma)^{4A-4}}\\frac{4\\|\\nabla F(\\mathbf{x}_{s})\\|^{2}\\mathbb{1}\\left\\{E_{s}\\right\\}\\alpha d C R_{T,\\delta}}{\\Gamma^{2}}}\\\\ &{\\leq64\\mu^{2}C R_{T,\\delta}^{2}\\frac{\\alpha d\\kappa^{2}C^{2}\\ln(K/\\delta)^{3}}{\\mu^{2}}\\displaystyle\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{4A-8}}{(t+\\gamma)^{4A-4}}}\\\\ &{\\leq64\\mu^{2}C R_{T,\\delta}^{2}\\cdot\\frac{\\alpha d\\kappa^{2}C^{2}\\ln(K/\\delta)^{3}}{\\mu^{2}(t+\\gamma)^{3}}}\\\\ &{\\leq64\\mu^{2}C R_{T,\\delta}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "where the second inequality follows by setting $A\\ge2$ and the last inequality follows by setting 3def/f3 \u03ba2/\u00b532/C32/3 ln(K/\u03b4)Substituting the above bounds into equation (20), we note that ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t}\\sigma_{s}^{2}\\ln(K/\\delta)\\leq164\\mu^{2}C R_{T,\\delta}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "Thus, by Freedman\u2019s inequality (Lemma 3), we conclude that the following holds with probability at least $1-\\delta/2$ uniformly for every $t\\in[T]$ : ", "page_idx": 37}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{2A-1}}{(t+\\gamma)^{2A-2}}\\left\\langle\\tilde{\\mathbf{v}}_{s},\\mathbf{d}_{s}\\right\\rangle=\\sum_{s=1}^{t}h_{s}\\leq2\\sqrt{\\sum_{s=1}^{t}\\sigma_{s}^{2}\\ln(K/\\delta)}+8\\mu R_{T,\\delta}\\sqrt{C}\\leq34R_{T,\\delta}\\sqrt{C}\n$$", "text_format": "latex", "page_idx": 37}, {"type": "text", "text": "To prove the second inequality of this lemma, we define $\\begin{array}{r}{{\\bf z}_{s}=\\tilde{\\bf v}_{s}\\cdot\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{A-1}}\\end{array}$ for $s\\,\\in\\,[t]$ . Note that $\\mathbb{E}[\\mathbf{z}_{s}|\\mathcal{F}_{s-1}]\\,=\\,0$ and $\\|\\mathbf{z}_{s}\\|\\,\\leq\\,\\|\\tilde{\\mathbf{v}}_{s}\\|\\,\\leq\\,2\\Gamma$ . Define the PSD matrices $\\mathbf{\\dot{G}}_{s}\\,=\\,\\mathbb{E}[\\mathbf{z}_{s}\\mathbf{z}_{s}^{T}|\\mathcal{F}_{s-1}]\\,=$ $\\begin{array}{r}{\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}\\tilde{\\Sigma}_{s}}\\end{array}$ . Recalling the bounds obtained on $\\|\\tilde{\\Sigma}_{s}\\|_{2}$ and $\\mathsf{T r}(\\tilde{\\Sigma}_{s})$ in equations (17) and (18), we ", "page_idx": 37}, {"type": "text", "text": "infer the following: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Gamma_{\\mathsf{f}}(\\mathbf{G}_{s})\\leq\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}\\mathsf{T r}(\\Sigma_{s})\\mathbf{1}\\left\\{E_{s}\\right\\}}\\\\ &{\\qquad\\leq\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}\\beta d_{\\mathsf{f}\\mathsf{f}}+\\frac{(s+\\gamma)^{2A-4}}{(t+\\gamma)^{2A-2}}4\\alpha d_{\\mathsf{e f f}}C R_{T,\\delta}}\\\\ &{\\left\\Vert\\mathbf{G}_{s}\\right\\Vert_{2}=\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}\\|\\bar{\\mathbf{\\Sigma}}_{s}\\|_{2}}\\\\ &{\\qquad\\leq\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}\\beta+\\frac{(s+\\gamma)^{2A-4}}{(t+\\gamma)^{2A-2}}4\\alpha C R_{T,\\delta}+\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}\\frac{\\left\\Vert\\nabla F(\\mathbf{x}_{s})\\right\\Vert^{4}\\mathbf{1}\\left\\{E_{s}\\right\\}}{\\Gamma^{2}}}\\\\ &{\\qquad+\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}\\frac{\\left\\Vert\\nabla F(\\mathbf{x}_{s})\\right\\Vert^{2}\\mathbf{1}\\left\\{E_{s}\\right\\}\\beta d_{\\mathsf{e f f}}}{\\Gamma^{2}}+\\frac{(s+\\gamma)^{2A-4}}{{(t+\\gamma)}^{2A-2}}\\frac{\\left\\Vert\\nabla F(\\mathbf{x}_{s})\\right\\Vert^{2}\\mathbf{1}\\left\\{E_{s}\\right\\}4\\alpha d_{\\mathsf{e f f}}C R_{T,\\delta}}{\\Gamma^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Substituting equation (8) into the bound for $\\|\\mathbf{G}_{s}\\|_{2}$ , we obtain the following ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{T r}(\\mathbf G_{s})\\le q_{s}=\\left(\\displaystyle\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}\\beta d_{\\mathsf{e f f}}+\\frac{(s+\\gamma)^{2A-4}}{(t+\\gamma)^{2A-2}}4\\alpha d_{\\mathsf{e f f}}C R_{T,\\delta}}\\\\ &{\\|\\mathbf G_{s}\\|_{2}\\le p_{s}=\\left(\\displaystyle\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}\\beta+\\displaystyle\\frac{(s+\\gamma)^{2A-4}}{(t+\\gamma)^{2A-2}}\\cdot4\\alpha C R_{T,\\delta}+\\displaystyle\\frac{(s+\\gamma)^{2A-6}}{(t+\\gamma)^{2A-2}}\\cdot16\\kappa^{4}C^{2}\\ln(K/\\delta)^{2}\\mu^{2}}\\\\ &{\\quad\\quad\\quad\\le\\displaystyle\\frac{(s+\\gamma)^{2A-4}}{(t+\\gamma)^{2A-2}}\\cdot4\\beta d_{\\mathsf{e f f}}\\kappa^{2}C\\ln(K/\\delta)^{2}+\\displaystyle\\frac{(s+\\gamma)^{2A-6}}{(t+\\gamma)^{2A-2}}\\cdot16\\alpha d_{\\mathsf{e f f}}R_{T,\\delta}\\kappa^{2}C^{2}\\ln(K/\\delta)^{2}\\phantom{\\left(\\displaystyle\\frac{s+\\gamma}{t}\\right)^{2}\\mu^{2}}(22)}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "By Cauchy Schwarz inequality, ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nu_{s}^{2}\\leq5\\left(\\displaystyle\\frac{s+\\gamma}{t+\\gamma}\\right)^{4A-4}\\beta^{2}+5\\cdot\\frac{(s+\\gamma)^{4A-8}}{(t+\\gamma)^{4A-4}}16\\alpha^{2}C^{2}R_{T,\\delta}^{2}+5\\cdot\\frac{(s+\\gamma)^{4A-12}}{(t+\\gamma)^{4A-4}}\\cdot256\\kappa^{8}C^{4}\\ln(K/\\delta)^{4}\\mu^{4}}\\\\ &{\\quad+\\,5\\cdot\\frac{(s+\\gamma)^{4A-8}}{(t+\\gamma)^{4A-4}}\\cdot16\\beta^{2}d_{\\mathrm{eff}}^{2}\\kappa^{4}C^{2}\\ln(K/\\delta)^{4}+5\\cdot\\frac{(s+\\gamma)^{4A-12}}{(t+\\gamma)^{4A-4}}\\cdot256\\alpha^{2}d_{\\mathrm{eff}}^{2}R_{T,\\delta}^{2}\\kappa^{4}C^{4}\\ln(K/\\delta)^{4}}\\end{array}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Since $T\\gtrsim\\ln(\\ln(d))$ , $K=\\ln(\\ln(T))$ , our choice of $\\Gamma$ and the definition of $R_{T,\\delta}$ ensures that the conditions of Corollary 5 are satisfied. Hence, by Corollary 5, we conclude that the following holds with probability $1-\\delta/2$ uniformly for all $t\\in[T]$ ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t}\\|\\mathbf{z}_{s}\\|^{2}\\leq4C_{M}\\Gamma^{2}\\ln(K/\\delta)^{2}+C_{M}\\sum_{s=1}^{\\mathsf{U P}(t)}Q_{s}+\\frac{C_{M}t}{4\\Gamma^{2}}\\sum_{s=1}^{t}P_{s}^{2}\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "Simplyfing the above using equations (22), (23) and the definition of $\\Gamma$ , we obtain the following: ", "page_idx": 38}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\sum_{s=1}^{t}\\|{\\bf z}_{s}\\|^{2}\\le4C_{M}\\mu^{2}R_{T,\\delta}+C_{M}\\displaystyle\\sum_{s=1}^{10}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}\\beta d_{\\mathrm{eff}}+C_{M}\\displaystyle\\sum_{s=1}^{10\\{r\\}}\\frac{(s+\\gamma)^{2A-4}}{(t+\\gamma)^{2A-2}}\\cdot4\\alpha d_{\\mathrm{eff}}C R_{T,\\delta}}}\\\\ {{\\displaystyle~~~~~~~~~+\\frac{5C_{M}}{4}\\displaystyle\\sum_{s=1}^{10\\{r\\}}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{4A-4}\\beta^{2}t\\displaystyle\\ln(K/\\delta)^{2}+\\frac{5C_{M}}{4}\\displaystyle\\sum_{s=1}^{10\\{r\\}}\\frac{(s+\\gamma)^{4A-3}}{(t+\\gamma)^{4A-4}}\\cdot\\frac{16\\alpha^{2}C^{2}R_{T,\\delta}t}{\\mu^{2}}}}\\\\ {{\\displaystyle~~~~~~~~+\\frac{5C_{M}}{4}\\displaystyle\\sum_{s=1}^{10\\{r\\}}\\frac{(s+\\gamma)^{4A-12}}{(t+\\gamma)^{4A-4}}\\cdot256s^{5}C^{4}\\ln(K/\\delta)^{6}\\mu_{t^{2}}R_{T,\\delta}}}\\\\ {{\\displaystyle~~~~~~~+\\frac{5C_{M}}{4}\\displaystyle\\sum_{s=1}^{10\\{r\\}}\\frac{(s+\\gamma)^{4A-3}}{(t+\\gamma)^{4A-4}}\\cdot\\frac{\\beta^{2}d_{\\mathrm{eff}}^{2}}{\\mu^{2}R_{T,\\delta}}\\cdot16s^{4}C^{2}\\ln(K/\\delta)^{6}}}\\\\ {{\\displaystyle~~~~~~~~+\\frac{5C_{M}}{4}\\displaystyle\\sum_{s=1}^{10\\{r\\}}\\frac{(s+\\gamma)^{4A-12}}{(t+\\gamma)^{4A-4}}\\cdot\\frac{2568^{4}C^{4}\\alpha^{2}d_{\\mathrm{eff}}^{2}}{\\mu^{2}R_{T,\\delta}^{4}}\\ln(K/\\delta)^{6}R_{T,\\delta}}}\\\\ {{\\displaystyle~~~~~~~+\\frac{5C_{M}}{4}\\displaystyle\\sum_{s\n$$", "text_format": "latex", "page_idx": 38}, {"type": "text", "text": "We now simplify each term in the above inequality by using the fact that $\\mathsf{U P}(t)\\leq\\operatorname*{min}\\{T,2t\\}$ . To this end, the second term is simplified as follows by using $A\\geq1$ ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{\\mathsf{U P}(t)}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{4A-4}\\beta d_{\\mathsf{e f f}}\\leq\\mathsf{U P}(t)\\beta d_{\\mathsf{e f f}}\\leq\\mu^{2}R_{T,\\delta}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We now control the third term by noting that for $s\\leq2t,s+\\gamma\\leq2t+\\gamma\\leq2(t+\\gamma);$ : ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{s=1}^{t}\\frac{(s+\\gamma)^{2A-4}}{(t+\\gamma)^{2A-2}}\\cdot4\\alpha d_{\\mathrm{eff}}C R_{T,\\delta}\\leq\\mu^{2}R_{T,\\delta}\\cdot\\frac{2^{2A-2}\\alpha d_{\\mathrm{eff}}}{\\mu^{2}}\\sum_{s=1}^{2t}\\frac{1}{(t+\\gamma)^{2}}}&{}\\\\ {\\leq2^{2A-1}\\mu^{2}R_{T,\\delta}\\cdot\\frac{\\alpha d t}{\\mu^{2}(t+\\gamma)^{2}}}&{}\\\\ {\\leq2^{2A-3}\\mu^{2}R_{T,\\delta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where the last inquality follows by setting $\\begin{array}{r}{\\gamma\\geq\\frac{4\\alpha C d_{\\mathsf{e f f}}}{\\mu^{2}}}\\end{array}$ . ", "page_idx": 39}, {"type": "text", "text": "We now control the fourth term as follows: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{\\mathsf{U P}(t)}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{4A-4}\\frac{\\beta^{2}t\\ln(K/\\delta)^{2}}{\\mu^{2}R_{T,\\delta}}\\leq\\mu^{2}R_{T,\\delta}\\cdot\\frac{\\mathsf{U P}(t)}{d(T+\\gamma)^{2}}\\leq\\mu^{2}R_{T,\\delta}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "We now control the fifth term as follows: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\frac{16\\alpha^{2}C^{2}R_{T,\\delta}t\\ln(K/\\delta)^{2}}{\\mu^{2}}\\sum_{s=1}^{\\infty}\\frac{(s+\\gamma)^{4A-8}}{(t+\\gamma)^{4A-4}}\\le\\mu^{2}R_{T,\\delta}\\cdot\\frac{2^{4A-4}\\alpha^{2}C^{2}\\ln(K/\\delta)^{2}t}{\\mu^{4}}\\sum_{s=1}^{2t}\\frac{1}{(t+\\gamma)^{4}}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\le\\mu^{2}R_{T,\\delta}\\cdot\\frac{\\alpha^{2}C^{2}\\ln(K/\\delta)^{2}2^{4A-5}}{\\mu^{4}(t+\\gamma)^{2}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\le2^{4A-9}\\mu^{2}R_{T,\\delta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where the last inequality $\\begin{array}{r}{\\operatorname{uses}\\gamma\\geq\\frac{4\\alpha C\\ln(K/\\delta)}{\\mu^{2}}}\\end{array}$ ", "page_idx": 39}, {"type": "text", "text": "We now simplify the sixth term as follows: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle\\sum_{s=1}^{\\lfloor\\mathtt{P}(t)\\rfloor}\\frac{(s+\\gamma)^{4A-12}}{(t+\\gamma)^{4A-4}}\\cdot256\\mu^{2}R_{T,\\delta}\\kappa^{8}C^{4}\\ln(K/\\delta)^{6}t\\leq\\mu^{2}R_{T,\\delta}t\\cdot2^{4A-4}\\kappa^{8}C^{4}\\ln(K/\\delta)^{6}\\sum_{s=1}^{2t}\\frac{1}{(t+\\gamma)^{8}}}&{}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq\\frac{\\mu^{2}R_{T,\\delta}}{(t+\\gamma)^{6}}\\cdot2^{4A-3}\\kappa^{8}C^{4}\\ln(K/\\delta)^{6}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\leq2^{4A-15}\\mu^{2}R_{T,\\delta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where the last inequality follows by setting $\\gamma\\geq4\\kappa^{4/3}C^{2/3}\\ln(K/\\delta)$ . ", "page_idx": 39}, {"type": "text", "text": "We control the seventh term as follows: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s=1}^{\\lfloor\\mathtt{P}(t)\\rfloor}\\frac{(s+\\gamma)^{4A-8}}{(t+\\gamma)^{4A-4}}\\cdot\\frac{\\beta^{2}d_{\\mathrm{eff}}^{2}t}{\\mu^{2}R_{T,\\delta}}\\cdot16\\kappa^{4}C^{2}\\ln(K/\\delta)^{6}\\le\\mu^{2}R_{T,\\delta}\\cdot\\frac{2^{4}\\kappa^{4}C^{2}\\ln(K/\\delta)^{6}t}{(T+\\gamma)^{2}}\\sum_{s=1}^{2t}\\frac{(s+\\gamma)^{4A-8}}{(t+\\gamma)^{4A-4}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\le\\mu^{2}R_{T,\\delta}\\cdot\\frac{2^{4A-3}\\kappa^{4}C^{2}\\ln(K/\\delta)^{6}}{(t+\\gamma)^{4}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\le2^{4A-11}\\mu^{2}R_{T,\\delta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where $\\gamma\\geq4\\kappa\\sqrt{C}\\ln(K/\\delta)^{3/2}$ . ", "page_idx": 39}, {"type": "text", "text": "We use a similar argument to simplify the final term as follows: ", "page_idx": 39}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\displaystyle\\sum_{s=1}^{\\mathbb{U}(t)}\\frac{t(s+\\gamma)^{4A-12}}{(t+\\gamma)^{4A-4}}\\cdot\\frac{2^{8}\\alpha^{2}d_{\\mathrm{eff}}^{2}R_{T,\\delta}\\kappa^{4}C^{4}\\ln(K/\\delta)^{6}}{\\mu^{2}}\\le\\mu^{2}R_{T,\\delta}\\cdot\\frac{2^{4A-3}\\alpha^{2}d_{\\mathrm{eff}}^{2}\\kappa^{4}C^{4}\\ln(K/\\delta)^{6}}{\\mu^{4}(t+\\gamma)^{6}}}&{}&\\\\ &{\\leq2^{4A-15}\\mu^{2}R_{T,\\delta}}\\end{array}\n$$", "text_format": "latex", "page_idx": 39}, {"type": "text", "text": "where $\\begin{array}{r}{\\gamma\\geq\\frac{4\\kappa^{2/3}d_{\\mathsf{e f f}}^{1/3}C^{2/3}\\ln\\left(K/\\delta\\right)}{\\mu^{2/3}}}\\end{array}$ def/f3 C2/23/3 ln(K/\u03b4). We now set A \u22653 and \u03b3 as follows: ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\gamma\\geq4C\\operatorname*{max}\\{\\frac{\\alpha d_{\\mathrm{eff}}}{\\mu^{2}},\\frac{\\alpha\\ln(K/\\delta)}{\\mu^{2}},\\kappa^{4/3}\\ln(K/\\delta),\\kappa\\ln(K/\\delta)^{3/2},\\frac{\\kappa^{2/3}d_{\\mathrm{eff}}^{1/3}\\alpha^{1/3}}{\\mu^{2/3}}\\ln(K/\\delta)\\}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Under these parameter settings, we substitute the obtained bounds into equation (15), we conclude that the following holds with probability at least $1-\\delta/2$ uniformly for every $t\\in[T]$ : ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\sum_{u=1}^{t}\\left(\\frac{s+\\gamma}{t+\\gamma}\\right)^{2A-2}\\|\\tilde{\\mathbf{v}}_{s}\\|^{2}=\\sum_{s=1}^{t}\\|\\mathbf{z}_{s}\\|^{2}\\leq C_{M}\\mu^{2}R_{T,\\delta}\\left(2^{4A-3}\\frac{25}{4}+5\\cdot2^{4A-11}+5\\cdot2^{4A-16}+5\\cdot2^{4A-16}\\right).\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "The proof is completed via a union bound. ", "page_idx": 40}, {"type": "text", "text": "D Analysis for Smooth Convex Functions ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Let deff = T\u2225r\u03a3(\u2225\u03a32) . Following a convention similar to that of Section B, let K = 4 max{8, CM, ln(T)}. For $t~\\ge~1$ , define the filtration $\\mathcal{F}_{t}~=~\\sigma\\left(\\mathbf{x}_{1},\\mathbf{g}_{s}\\vert1\\leq s\\leq t\\right)$ and $\\mathcal{F}_{0}~=~\\sigma(\\mathbf{x}_{1})$ . Furthermore, let $\\nabla F({\\bf x}_{t})\\;\\;=\\;\\;{\\sf c l i p}_{\\Gamma}({\\bf g}_{t})\\,+\\,{\\bf b}_{t}\\,+\\,{\\bf v}_{t}$ where $\\mathbf{b}_{t}\\ \\overset{}{=}\\ \\nabla F(\\mathbf{\\dot{x}}_{t})\\ -\\ \\mathbb{E}[\\mathsf{c l i p}_{\\Gamma}(\\mathbf{g}_{t})|\\mathcal{F}_{t-1}]$ and $\\mathbf{v}_{t}=$ $\\mathbb{E}[{\\mathsf{c l i p}}_{\\Gamma}(\\mathbf{g}_{t})|{\\mathcal{F}}_{t-1}]-{\\mathsf{c l i p}}_{\\Gamma}(\\mathbf{g}_{t})$ . As beforem, we note that $\\mathbb{E}[\\mathbf{v}_{t}|\\mathcal{F}_{t-1}]=0$ and $\\|\\mathbf{v}_{t}\\|\\leq2\\Gamma$ . Hence $\\mathbf{v}_{t}$ is an $\\mathcal{F}$ adapted almost surely bounded martingale difference sequence. Now, let $D_{t}=\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|$ where $\\mathbf{x}^{*}$ is the minimizer of $F$ considered in the statement of Theorem 3. Using the smoothness and convexity properties of $F$ , we first prove the following intermediate average iterate guarantee: ", "page_idx": 40}, {"type": "text", "text": "Lemma 10 (Intermediate Average Iterate Guarantee). The following holds for $\\eta\\leq1/2L$ ", "page_idx": 40}, {"type": "equation", "text": "$$\n^{\\7}(\\hat{\\mathbf{x}}_{T})-F(\\mathbf{x}^{*})\\leq\\frac{D_{1}^{2}}{2\\eta T}+\\frac{1}{T}\\sum_{t=1}^{T}\\langle\\mathbf{b}_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle+\\frac{1}{T}\\sum_{t=1}^{T}\\langle\\mathbf{v}_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle+\\frac{2\\eta}{T}\\sum_{t=1}^{T}\\|\\mathbf{b}_{t}\\|^{2}+\\frac{2\\eta}{T}\\sum_{t=1}^{T}\\|\\mathbf{v}_{t}-\\mathbf{x}^{*}\\|.\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Define the events $E_{t}$ and the random vectors $\\mathbf{d}_{t},\\tilde{\\mathbf{b}}_{t}$ and $\\tilde{{\\bf v}}_{t}$ as follows for $t\\in[T]$ : ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{E_{t}=\\left\\{D_{t}\\leq2D_{1}\\right\\}}\\\\ &{\\mathbf{d}_{t}=(\\mathbf{x}_{t}-\\mathbf{x}^{*})\\mathbb{1}\\left\\{E_{t}\\right\\}}\\\\ &{\\tilde{\\mathbf{b}}_{t}=\\mathbf{b}_{t}\\mathbb{1}\\left\\{E_{t}\\right\\}}\\\\ &{\\tilde{\\mathbf{v}}_{t}=\\mathbf{v}_{t}\\mathbb{1}\\left\\{E_{t}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "We use the following lemma to control the bias ", "page_idx": 40}, {"type": "text", "text": "Lemma 11 (Bias Control). For every $t\\in[T],\\,\\|\\tilde{\\mathbf{b}}_{t}\\|\\leq B$ where $B$ is defined as follows: ", "page_idx": 40}, {"type": "equation", "text": "$$\nB=\\frac{\\|\\Sigma\\|_{2}\\sqrt{d_{\\mathsf{e f f}}}}{\\Gamma}+\\frac{2L D_{1}\\sqrt{\\|\\Sigma\\|_{2}}}{\\Gamma}+\\frac{8L^{3}D_{1}^{3}}{\\Gamma^{2}}+\\frac{2\\|\\Sigma\\|_{2}d_{\\mathsf{e f f}}L D_{1}}{\\Gamma^{2}}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "We use the following lemma to control the varince ", "page_idx": 40}, {"type": "text", "text": "Lemma 12 (Variance Control). Let $V\\geq0$ be defined as follows: ", "page_idx": 40}, {"type": "equation", "text": "$$\nV=\\|\\Sigma\\|_{2}+\\frac{16L^{4}D_{1}^{4}}{\\Gamma^{2}}+\\frac{4L^{2}D_{1}^{2}\\|\\Sigma\\|_{2}d_{\\mathsf{e f f}}}{\\Gamma^{2}}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Then the following holds with probability at least $1-\\delta$ uniformly for every $t\\in[T]$ ", "page_idx": 40}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{s=1}^{t}\\left\\langle\\tilde{\\mathbf{v}}_{t},\\mathbf{d}_{t}\\right\\rangle\\leq4D_{1}\\sqrt{V t\\ln\\left(^{K}/\\delta\\right)}+8\\Gamma D_{1}\\ln\\bigl(^{K}/\\delta\\bigr)}\\\\ {\\displaystyle\\sum_{s=1}^{t}\\|\\tilde{\\mathbf{v}}_{t}\\|^{2}\\leq C_{M}g^{2}T}\\end{array}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "where $C_{M}$ is a numerical constant and $g^{2}$ is defined as follows ", "page_idx": 40}, {"type": "equation", "text": "$$\ng^{2}=\\|\\Sigma\\|_{2}d_{\\mathsf{e f f}}+\\frac{4\\Gamma^{2}\\ln(K/\\delta)^{2}}{T}+\\frac{V^{2}T}{4\\Gamma^{2}}\n$$", "text_format": "latex", "page_idx": 40}, {"type": "text", "text": "Let $E$ denote the following event ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{l}{E=\\{\\displaystyle\\sum_{s=1}^{t}\\langle\\tilde{\\mathbf{v}}_{s},\\mathbf{d}_{s}\\rangle\\leq4D_{1}\\sqrt{V t\\ln(^{K/\\delta})}+8\\Gamma D_{1}\\ln(^{K/\\delta})\\quad\\forall\\,t\\in[T]}\\\\ {\\displaystyle\\sum_{s=1}^{t}\\|\\tilde{\\mathbf{v}}_{s}\\|^{2}\\leq C_{M}g^{2}T\\quad\\forall t\\in[T]\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "We define the constant $A$ as follows: ", "page_idx": 41}, {"type": "equation", "text": "$$\nA=\\|\\Sigma\\|_{2}\\sqrt{d_{\\mathsf{e f f}}}+L D_{1}\\sqrt{\\|\\Sigma\\|_{2}}=\\sqrt{\\|\\Sigma\\|_{2}}\\left(\\sqrt{\\mathsf{T r}(\\Sigma)}+L D_{1}\\right)\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "We now set the clipping level $\\begin{array}{r}{\\Gamma=\\sqrt{\\frac{A T}{\\ln\\left(\\ K/\\delta\\right)}}}\\end{array}$ . For this choice of $\\Gamma$ , we now obtain the following bound on $B$ : ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{c}{B\\leq\\frac{\\|\\Sigma\\|_{2}\\sqrt{d_{\\mathsf{e f f}}}}{\\Gamma}+\\frac{2L D_{1}\\sqrt{\\|\\Sigma\\|_{2}}}{\\Gamma}+\\frac{8L^{3}D_{1}^{3}}{\\Gamma^{2}}+\\frac{2\\|\\Sigma\\|_{2}d_{\\mathsf{e f f}}L D_{1}}{\\Gamma^{2}}}\\\\ {\\leq2\\sqrt{\\frac{A\\ln(K/\\delta)}{T}}+\\frac{2L D_{1}\\ln\\left(K/\\delta\\right)}{A T}\\left(\\|\\Sigma\\|_{2}d_{\\mathsf{e f f}}+L^{2}D_{1}^{2}\\right)=B^{\\prime}}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Similarly, we bound the value of $V$ as follows: ", "page_idx": 41}, {"type": "equation", "text": "$$\nV\\leq\\|\\Sigma\\|_{2}+{\\frac{16L^{4}D^{4}\\ln(\\kappa/\\delta)}{A T}}+{\\frac{4L^{2}D^{2}\\|\\Sigma\\|_{2}d\\ln(\\kappa/\\delta)}{A T}}=V^{\\prime}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Equipped with the above inequality, we then bound the value of $g$ as: ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\Lambda}:\\=\\sqrt{\\|\\Sigma\\|_{2}d_{\\mathrm{eff}}}+\\frac{2\\Gamma\\ln\\left(K/\\delta\\right)}{\\sqrt{T}}+\\frac{V\\sqrt{T}}{2\\Gamma}}\\\\ &{\\leq\\sqrt{\\|\\Sigma\\|_{2}d_{\\mathrm{eff}}}+2\\sqrt{A\\ln(K/\\delta)}+\\frac{V\\sqrt{\\ln(K/\\delta)}}{2\\sqrt{A}}}\\\\ &{\\leq\\sqrt{\\|\\Sigma\\|_{2}d_{\\mathrm{eff}}}+2\\sqrt{A\\ln(K/\\delta)}+\\frac{\\|\\Sigma\\|_{2}\\sqrt{\\ln(K/\\delta)}}{2\\sqrt{A}}+\\frac{8L^{4}D^{4}\\ln(K/\\delta)^{3/2}}{A^{3/2}T}+\\frac{2L^{2}D^{2}\\|\\Sigma\\|_{2}d_{\\mathrm{eff}}\\ln\\left(K/\\delta\\right)^{3/2}}{A^{3/2}T}}\\\\ &{\\leq\\sqrt{\\|\\Sigma\\|_{2}d_{\\mathrm{eff}}}+3\\sqrt{A\\ln(K/\\delta)}+\\frac{8L^{4}D^{4}\\ln(K/\\delta)^{3/2}}{A^{3/2}T}+\\frac{2L^{2}D^{2}\\|\\Sigma\\|_{2}d_{\\mathrm{eff}}\\ln\\left(K/\\delta\\right)^{3/2}}{A^{3/2}T}=g^{\\prime}~~~~~(27)}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "We prove the following lemma to control the growth of the iterates $D_{t}$ . ", "page_idx": 41}, {"type": "text", "text": "Lemma 13 (Iterate Bound). Let $\\eta\\le c\\operatorname*{min}\\bigl\\{1\\big/2L,D_{1}\\big/B^{\\prime}T,D_{1}\\big/g^{\\prime}\\sqrt{T}\\bigr\\}$ where $\\begin{array}{r}{c=\\frac{1}{\\sqrt{8C_{M}+330}}}\\end{array}$ . Then, conditioned on the event $\\exists,\\,D_{t}\\leq2D_{1}\\,\\forall t\\in[T]$ . ", "page_idx": 41}, {"type": "text", "text": "Equipped with the above lemmas, we now present a proof of the following theorem, which is a formal restatement of Theorem 3 ", "page_idx": 41}, {"type": "text", "text": "Theorem 7 (Smooth Convex Objectives). Let Convexity, $L$ -smoothness and Bdd. $2^{\\mathsf{n d}}$ Moment be satisfied. Then, for any $\\delta\\,\\in\\,(0,{^1\\mathord{/}{\\vphantom{^12}}2})$ and $T\\,\\geq\\,\\ln(\\ln(d))$ , there exists an $\\eta\\,\\in\\,(0,1/2L]$ such that the average iterate of Algorithm $^{\\,l}$ run for $T$ iterations with step-size $\\eta_{t}\\,=\\,\\eta$ and clipping level $\\begin{array}{r}{\\Gamma=\\sqrt{\\frac{T\\sqrt{\\|\\Sigma\\|_{2}}(\\sqrt{\\mathsf{T r}(\\Sigma)}+L D_{1})}{\\ln\\left(\\ln(T)/\\delta\\right)}}}\\end{array}$ satisfies the following with probability at least $1-\\delta$ : ", "page_idx": 41}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\ P}(\\hat{\\mathbf{x}}_{T})-F(\\mathbf{x^{*}})\\lesssim D_{1}\\sqrt{\\frac{\\mathbb{T}\\left(\\sum)+\\sqrt{\\|\\Sigma\\|_{2}}\\left(\\sqrt{\\mathsf{T r}(\\Sigma)}+L D_{1}\\right)\\ln\\left(\\ln(T)/\\delta\\right)}{T}}+\\frac{L D_{1}^{2}}{T}}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\,\\frac{L D_{1}^{2}\\ln\\left(\\ln(T)/\\delta\\right)}{T}\\sqrt{\\frac{\\mathsf{T r}(\\Sigma)+L^{2}D_{1}^{2}}{\\|\\Sigma\\|_{2}}}+\\frac{L^{2}D_{1}^{3}\\ln\\left(\\ln(T)/\\delta\\right)^{3/2}}{T^{3/2}}\\left[\\frac{\\mathsf{T r}(\\Sigma)+L^{2}D_{1}^{2}}{\\|\\Sigma\\|^{3}}\\right]^{1/4}}\\end{array}\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "D.1 Proof of Theorem 7 ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "We condition n the event $E$ and let $\\begin{array}{r}{\\eta=\\frac{c}{2}\\operatorname*{min}\\{\\frac{1}{2L},\\frac{D_{1}}{B^{\\prime}T},\\frac{D_{1}}{g^{\\prime}\\sqrt{T}}\\}}\\end{array}$ where $\\begin{array}{r}{c=\\frac{1}{\\sqrt{8C_{M}+330}}}\\end{array}$ . Note that $\\eta$ satisfies the requirements of Lemma 10 and Lemma 13. By Lemma 10, the following holds: ", "page_idx": 42}, {"type": "equation", "text": "$$\n^{\\7}(\\hat{\\mathbf{x}}_{T})-F(\\mathbf{x}^{*})\\leq\\frac{D_{1}^{2}}{2\\eta T}+\\frac{1}{T}\\sum_{t=1}^{T}\\langle\\mathbf{b}_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle+\\frac{1}{T}\\sum_{t=1}^{T}\\langle\\mathbf{v}_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle+\\frac{2\\eta}{T}\\sum_{t=1}^{T}\\|\\mathbf{b}_{t}\\|^{2}+\\frac{2\\eta}{T}\\sum_{t=1}^{T}\\|\\mathbf{v}_{t}-\\mathbf{x}^{*}\\|.\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "By Lemma 1 $3,\\mathbb{1}\\left\\{E_{t}\\right\\}=1\\;\\forall t\\in[T]$ . Hence, the following holds. ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F(\\hat{\\mathbf{x}}_{T})-F(\\mathbf{x}^{*})\\leq\\displaystyle\\frac{D_{1}^{2}}{2\\eta T}+\\frac{1}{T}\\displaystyle\\sum_{t=1}^{T}\\Big\\langle\\tilde{\\mathbf{b}}_{t},\\mathbf{d}_{t}\\Big\\rangle+\\displaystyle\\frac{1}{T}\\displaystyle\\sum_{t=1}^{T}\\langle\\tilde{\\mathbf{v}}_{t},\\mathbf{d}_{t}\\rangle+\\displaystyle\\frac{2\\eta}{T}\\displaystyle\\sum_{t=1}^{T}\\|\\tilde{\\mathbf{b}}_{t}\\|^{2}+\\frac{2\\eta}{T}\\displaystyle\\sum_{t=1}^{T}\\|\\tilde{\\mathbf{v}}_{t}\\|^{2}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{D_{1}^{2}}{2\\eta T}+2B D_{1}+2\\eta B^{2}+2\\eta C_{M}g^{2}+4D_{1}\\sqrt{\\frac{V\\ln(K/\\delta)}{T}}+\\frac{8D_{1}\\Gamma\\ln(K/\\delta)}{T}}\\\\ &{\\qquad\\qquad\\qquad\\leq\\displaystyle\\frac{D_{1}^{2}}{\\eta T}+3\\eta B^{2}T+2\\eta C_{M}g^{2}+4D_{1}\\sqrt{\\frac{V\\ln(K/\\delta)}{T}}+8D_{1}\\sqrt{\\frac{A\\ln(K/\\delta)}{T}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Where the second inequality uses Lemma 11 and the definition of the event $E$ and the third inequality uses $a b\\leq a^{2}+b^{2}/4$ . For the rest of the proof, we shall use $C$ to denote an absolute numerical constant whose value can differ at every step. By our choice of the step-size ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\displaystyle\\frac{D^{2}}{\\eta T}\\leq\\frac{C L D_{1}^{2}}{T}+C D_{1}B^{\\prime}+\\frac{C D_{1}g^{\\prime}}{\\sqrt{T}}}}\\\\ {{3\\eta B^{2}T\\leq C D_{1}B^{\\prime}}}\\\\ {{2\\eta C_{M}g^{2}\\leq\\frac{C D_{1}g^{\\prime}}{\\sqrt{T}}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Hence, conditioned on the event $E$ , the following holds: ", "page_idx": 42}, {"type": "equation", "text": "$$\nF(\\hat{\\mathbf{x}}_{T})-F(\\mathbf{x}^{*})\\leq\\frac{C L D_{1}^{2}}{T}+C D_{1}B^{\\prime}+C D_{1}g^{\\prime}\\sqrt{T}+C D_{1}\\sqrt{\\frac{V^{\\prime}\\ln(K/\\delta)}{T}}+C D_{1}\\sqrt{\\frac{A\\ln(K/\\delta)}{T}}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Substituting the values of $g^{\\prime},B^{\\prime}$ and $V^{\\prime}$ , we obtain the following: ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{F(\\hat{\\mathbf{x}}_{T})-F(\\mathbf{x}^{*})\\le\\frac{C L D_{1}^{2}}{T}+C D_{1}\\sqrt{\\frac{A\\ln(K/\\delta)}{T}}+\\frac{C L D_{1}^{2}\\ln(K/\\delta)}{T}\\cdot\\left[\\frac{\\mathsf{T r}(\\Sigma)+L^{2}D_{1}^{2}}{A}\\right]}}\\\\ &{}&{+\\,\\frac{C L D_{1}^{2}\\ln(K/\\delta)}{T}\\cdot\\sqrt{\\frac{\\mathsf{T r}(\\Sigma)+L^{2}D^{2}}{A}}+\\frac{C L^{2}D_{1}^{3}\\ln(K/\\delta)^{3/2}}{T^{3/2}}\\cdot\\left[\\frac{\\mathsf{T r}(\\Sigma)+L^{2}D_{1}^{2}}{A^{3/2}}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "Substituting the value of $A$ , we conclude that the following inequality holds almost surely conditioned on the event $E$ ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{\\ P}(\\hat{\\mathbf{x}}_{T})-F(\\mathbf{x^{*}})\\lesssim D_{1}\\sqrt{\\frac{\\mathbb{T}\\left(\\sum)+\\sqrt{\\|\\Sigma\\|_{2}}\\left(\\sqrt{\\mathsf{T r}(\\Sigma)}+L D_{1}\\right)\\ln\\left(\\ln(T)/\\delta\\right)}{T}}+\\frac{L D_{1}^{2}}{T}}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\,\\frac{L D_{1}^{2}\\ln\\left(\\ln(T)/\\delta\\right)}{T}\\sqrt{\\frac{\\mathsf{T r}(\\Sigma)+L^{2}D_{1}^{2}}{\\|\\Sigma\\|_{2}}}+\\frac{L^{2}D_{1}^{3}\\ln\\left(\\ln(T)/\\delta\\right)^{3/2}}{T^{3/2}}\\left[\\frac{\\mathsf{T r}(\\Sigma)+L^{2}D_{1}^{2}}{\\|\\Sigma\\|^{3}}\\right]^{1/4}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "The prooof is completed by observing that $\\mathbb{P}(E)\\geq1-\\delta$ by Lemma 12 which implies that the above inequality also holds with probability at least $1-\\delta$ ", "page_idx": 42}, {"type": "text", "text": "D.2 Proof of Lemma 10 ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Proof. Since $\\Pi_{\\mathcal{C}}$ is a contractive operator ", "page_idx": 42}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{t+1}^{2}=\\|\\mathbf{x}_{t+1}-\\mathbf{x}^{*}\\|^{2}\\leq D_{t}^{2}-2\\eta\\left\\langle\\nabla F(\\mathbf{x}_{t})-\\mathbf{b}_{t}-\\mathbf{v}_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle+\\eta^{2}\\|\\nabla F(\\mathbf{x}_{t})-\\mathbf{b}_{t}-\\mathbf{v}_{t}\\|}\\\\ &{\\qquad\\leq D_{t}^{2}-2\\eta\\left\\langle\\nabla F(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle+2\\eta\\left\\langle\\mathbf{b}_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle+2\\eta\\left\\langle\\mathbf{v}_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle}\\\\ &{\\qquad+\\,2\\eta^{2}\\|\\nabla F(\\mathbf{x}_{t})\\|+4\\eta^{2}\\|\\mathbf{v}_{t}\\|^{2}+4\\eta^{2}\\|\\mathbf{b}_{t}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 42}, {"type": "text", "text": "By the coercivity property, ", "page_idx": 43}, {"type": "equation", "text": "$$\n-2\\eta\\left\\langle\\nabla F(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle\\leq-2\\eta[F(\\mathbf{x}_{t})-F(\\mathbf{x}^{*})]-\\frac{\\eta}{L}\\|\\nabla F(\\mathbf{x}_{t})\\|^{2}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Substituting this into the recurrence for $D_{t+1}^{2}$ , we obtain the following: ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{\\mathcal{I}}_{t+1}^{2}\\le D_{t}^{2}-2\\eta[F(\\mathbf{x}_{t})-F(\\mathbf{x}^{*})]+2\\eta\\left\\langle\\mathbf{v}_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle+2\\eta\\left\\langle\\mathbf{b}_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle}\\\\ &{\\qquad+\\eta(2\\eta-1/L)\\|\\nabla F(\\mathbf{x}_{t})\\|^{2}+4\\eta^{2}\\|\\mathbf{v}_{t}\\|^{2}+4\\eta^{2}\\|\\mathbf{b}_{t}\\|^{2}}\\\\ &{\\qquad\\le D_{t}^{2}-2\\eta[F(\\mathbf{x}_{t})-F(\\mathbf{x}^{*})]+2\\eta\\left\\langle\\mathbf{v}_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle+2\\eta\\left\\langle\\mathbf{b}_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle+4\\eta^{2}\\|\\mathbf{v}_{t}\\|^{2}+4\\eta^{2}\\|\\mathbf{b}_{t}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "where the last inequality uses the fact that $\\eta\\leq1/2L$ , Rearranging and taking averages on both sides ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\sum_{n=1}^{T}F(\\mathbf{x}_{t})-F(\\mathbf{x}^{*})\\leq{\\frac{D_{1}^{2}}{2\\eta T}}+{\\frac{1}{T}}\\sum_{t=1}^{T}\\left\\langle\\mathbf{b}_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle+{\\frac{1}{T}}\\sum_{t=1}^{T}\\left\\langle\\mathbf{v}_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle+{\\frac{2\\eta}{T}}\\sum_{t=1}^{T}\\|\\mathbf{b}_{t}\\|^{2}+{\\frac{2\\eta}{T}}\\sum_{t=1}^{T}\\left\\langle\\mathbf{b}_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Using the above inequality and the convexity of $F$ , we conclude that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle^{\\triangledown}(\\hat{\\mathbf{x}}_{T})-F(\\mathbf{x}^{*})=F\\left(\\frac{1}{T}\\sum_{t=1}^{T}\\mathbf{x}_{t}\\right)-F(\\mathbf{x}^{*})}\\\\ {\\displaystyle\\le\\frac{1}{T}\\sum_{t=1}^{T}F(\\mathbf{x}_{t})-F(\\mathbf{x}^{*})}\\\\ {\\displaystyle\\le\\frac{D_{1}^{2}}{2\\eta T}+\\frac{1}{T}\\sum_{t=1}^{T}\\langle\\mathbf{b}_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle+\\frac{1}{T}\\sum_{t=1}^{T}\\langle\\mathbf{v}_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle+\\frac{2\\eta}{T}\\sum_{t=1}^{T}\\|\\mathbf{b}_{t}\\|^{2}+\\frac{2\\eta}{T}\\sum_{t=1}^{T}\\|\\mathbf{v}_{t}-\\mathbf{x}^{*}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "D.3 Proof of Lemma 11 ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Note that by definition of $E_{t}$ ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\|\\nabla F(\\mathbf{x}_{t})\\|\\,\\|\\;\\{E_{t}\\}\\leq L D_{t}\\mathbb{1}\\,\\{E_{t}\\}\\leq2L D_{1}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "We recall that $\\mathbf{b}_{t}=\\mathbb{E}[\\mathbf{g}_{t}|\\mathcal{F}_{t-1}]-\\mathbb{E}[\\mathsf{c l i p}_{\\Gamma}(\\mathbf{g}_{t})|\\mathcal{F}_{t-1}]$ . Since $\\mathsf{C o v}[\\mathbf{g}_{t}|\\mathcal{F}_{t-1}]\\preceq\\Sigma$ by Assumption Bdd. $2^{\\mathsf{n d}}$ Moment, we obtain the following bound on $\\|\\mathbf{b}_{t}\\|$ by an application of Lemma 4 ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\|\\mathbf{b}_{t}\\|\\leq\\frac{\\|\\Sigma\\|_{2}\\sqrt{d_{\\mathrm{eff}}}}{\\Gamma}+\\frac{\\|\\nabla F(\\mathbf{x}_{t})\\|\\sqrt{\\|\\Sigma\\|_{2}}}{\\Gamma}+\\frac{\\|\\nabla F(\\mathbf{x}_{t})\\|^{3}}{\\Gamma^{2}}+\\frac{\\|\\Sigma\\|_{2}d_{\\mathrm{eff}}\\|\\nabla F(\\mathbf{x}_{t})\\|}{\\Gamma^{2}}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "Since $\\tilde{\\mathbf{b}}_{t}=\\mathbf{b}_{t}\\mathbb{1}\\left\\{E_{t}\\right\\}$ , it follows that ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{b}_{t}\\|\\leq\\frac{\\|\\Sigma\\|_{2}\\sqrt{d_{\\mathrm{eff}}}}{\\Gamma}+\\frac{\\|\\nabla F(\\mathbf{x}_{t})\\|\\,\\{E_{t}\\}\\,\\|\\sqrt{\\|\\Sigma\\|_{2}}}{\\Gamma}+\\frac{\\|\\nabla F(\\mathbf{x}_{t})\\|^{3}\\,\\mathbb{I}\\,\\{E_{t}\\}}{\\Gamma^{2}}+\\frac{\\|\\Sigma\\|_{2}d_{\\mathrm{eff}}\\|\\nabla F(\\mathbf{x}_{t})\\|\\,\\mathbb{I}\\,\\{E_{t}\\}}{\\Gamma^{2}}}\\\\ &{\\quad\\quad\\leq\\frac{\\|\\Sigma\\|_{2}\\sqrt{d_{\\mathrm{eff}}}}{\\Gamma}+\\frac{2L D_{1}\\sqrt{\\|\\Sigma\\|_{2}}}{\\Gamma}+\\frac{8L^{3}D_{1}^{3}}{\\Gamma^{2}}+\\frac{2\\|\\Sigma\\|_{2}d_{\\mathrm{eff}}L D_{1}}{\\Gamma^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "D.4 Proof of Lemma 12 ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "For any $s\\in[T]$ , we recall that $\\mathbf{v}_{s}=\\mathbb{E}\\,[\\mathsf{c l i p}_{\\Gamma}(\\mathbf{g}_{s})|\\mathcal{F}_{s-1}]-\\mathsf{c l i p}_{\\Gamma}(\\mathbf{g}_{s})$ . Since $\\mathbb{E}[\\mathbf{g}_{s}\\vert\\mathcal{F}_{s-1}]=\\nabla F(\\mathbf{x}_{s})$ and $\\mathsf{C o v}[\\mathbf{g}_{s}|\\mathcal{F}_{s-1}]\\preceq\\Sigma$ , we obtain the following from Lemma 4 ", "page_idx": 43}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\mathbb{E}\\left[\\mathbf{v}_{s}\\mathbf{v}_{s}^{T}|\\mathcal{F}_{s-1}\\right]\\|_{2}=\\|\\mathsf{C o v}\\left[\\mathsf{c l i p}_{\\Gamma}(\\mathbf{g}_{s})|\\mathcal{F}_{s-1}\\right]\\|\\leq\\|\\Sigma\\|_{2}+\\frac{\\|\\nabla F(\\mathbf{x}_{s})\\|^{4}}{\\Gamma^{2}}+\\frac{\\|\\nabla F(\\mathbf{x}_{s})\\|^{2}\\mathsf{T r}(\\Sigma)}{\\Gamma^{2}}}\\\\ &{\\mathsf{T r}\\left(\\mathbb{E}\\left[\\mathbf{v}_{s}\\mathbf{v}_{s}^{T}|\\mathcal{F}_{s-1}\\right]\\right)=\\mathsf{T r}\\left(\\mathsf{C o v}\\left[\\mathsf{c l i p}_{\\Gamma}(\\mathbf{g}_{s})|\\mathcal{F}_{s-1}\\right]\\right)\\leq\\mathsf{T r}(\\Sigma)}\\end{array}\n$$", "text_format": "latex", "page_idx": 43}, {"type": "text", "text": "For $s\\in[1:T]$ define $\\mathbb{E}[\\tilde{\\mathbf{v}}_{s}\\tilde{\\mathbf{v}}_{s}^{T}|\\mathcal{F}_{s-1}]=\\tilde{\\Sigma}_{s}$ . Since $\\mathbb{I}\\left\\{E_{s}\\right\\}$ is $\\mathcal{F}_{s-1}$ -measurable and $\\tilde{\\mathbf{v}}_{s}=\\mathbf{v}_{s}\\mathbb{1}\\left\\{E_{s}\\right\\}$ , it follows that $\\tilde{\\Sigma}_{s}\\,=\\,\\mathbb{E}\\left[{\\mathbf{v}_{s}}{\\mathbf{v}_{s}^{T}}|\\mathcal{F}_{s-1}\\right]\\,\\mathbb{1}\\left\\{{E_{s}}\\right\\}$ . Hence, we conclude the following from the above ", "page_idx": 43}, {"type": "text", "text": "inequality ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\tilde{\\Sigma}_{s}\\|_{2}\\leq\\|\\Sigma\\|_{2}+\\frac{\\|\\nabla F({\\mathbf x}_{s})\\|^{4}\\mathbb{1}\\,\\{E_{s}\\}}{\\Gamma^{2}}+\\frac{\\|\\nabla F({\\mathbf x}_{s})\\|^{2}\\top r(\\Sigma)\\mathbb{1}\\,\\{E_{s}\\}}{\\Gamma^{2}}}\\\\ &{\\qquad\\qquad\\leq\\|\\Sigma\\|_{2}+\\frac{16L^{4}D_{1}^{4}}{\\Gamma^{2}}+\\frac{4L^{2}D_{1}^{2}\\mathsf{T r}(\\Sigma)}{\\Gamma^{2}}=V}\\\\ &{\\mathsf{T r}(\\tilde{\\Sigma}_{s})\\leq\\mathsf{T r}(\\Sigma)}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "For $s\\in[T]$ , define $h_{s}=\\langle\\tilde{\\mathbf{v}}_{s},\\mathbf{d}_{s}\\rangle$ . We note that ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~~~~~~~~~|h_{s}|\\le\\|\\tilde{\\mathbf{v}}_{s}\\|\\cdot\\|\\mathbf{d}_{s}\\|\\le4\\Gamma D_{1}}\\\\ &{~\\|{\\mathbf{\\boldsymbol{h}}_{s}}|\\mathcal{F}_{s-1}\\|=\\langle\\mathbb{E}[\\tilde{\\mathbf{v}}_{s}|\\mathcal{F}_{s-1}],\\mathbf{d}_{s}\\rangle=0}\\\\ &{~\\mathbb{E}\\left[h_{s}^{2}|\\mathcal{F}_{s-1}\\right]=\\mathbf{d}_{s}^{T}\\mathbb{E}[\\tilde{\\mathbf{v}}_{s}\\tilde{\\mathbf{v}}_{s}^{T}]\\mathbf{d}^{s}}\\\\ &{~~~~~~~~~~~~~~~~~~=\\mathbf{d}_{s}^{T}\\tilde{\\Sigma}_{s}\\mathbf{d}_{s}}\\\\ &{~~~~~~~~~~~~~~~\\le\\|\\mathbf{d}_{s}\\|^{2}\\|\\tilde{\\Sigma}\\|\\le4D_{1}^{2}V}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Hence, by Freedman\u2019s inequality (Lemma 3), we conclude that the following holds with probability at least $1-\\delta/2$ : ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t}\\left\\langle\\tilde{\\mathbf{v}}_{t},\\mathbf{d}_{t}\\right\\rangle\\leq4D_{1}\\sqrt{V t\\ln(^{K/\\delta})}+8\\Gamma D_{1}\\ln(K/\\delta)\\quad\\forall\\,t\\in[T]\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "We now apply Corollary 6 with $p_{s}\\,=\\,V$ , $q_{s}=\\mathsf{T r}(\\Sigma)$ and $\\tau=2\\Gamma$ to conclude that the following holds with probability at least $1-\\delta/2$ uniformly for every $t\\in[T]$ ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s=1}^{t}\\|\\widetilde{\\mathbf{v}}_{s}\\|^{2}\\leq4C_{M}\\Gamma^{2}\\ln(K/\\delta)^{2}+C_{M}\\mathsf{U P}(t)\\mathsf{T r}(\\Sigma)+\\frac{C_{M}t\\mathsf{U P}(t)V^{2}}{4\\Gamma^{2}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leq4C_{M}\\Gamma^{2}\\ln(K/\\delta)^{2}+C_{M}T\\mathsf{T r}(\\Sigma)+\\frac{C_{M}T^{2}V^{2}}{4\\Gamma^{2}}}\\\\ &{\\quad\\quad\\quad\\quad\\leq C_{M}T\\left(\\|\\Sigma\\|_{2}d_{\\mathsf{e f f}}+\\frac{4\\Gamma^{2}\\ln(K/\\delta)^{2}}{T}+\\frac{V^{2}T}{4\\Gamma^{2}}\\right)=C_{M}g^{2}T}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where ", "page_idx": 44}, {"type": "equation", "text": "$$\ng^{2}=\\|\\Sigma\\|_{2}d_{\\mathsf{e f f}}+\\frac{4\\Gamma^{2}\\ln(K/\\delta)^{2}}{T}+\\frac{V^{2}T}{4\\Gamma^{2}}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "The proof is concluded by a union bound ", "page_idx": 44}, {"type": "text", "text": "D.5 Proof of Lemma 13 ", "text_level": 1, "page_idx": 44}, {"type": "text", "text": "We prove the claim via induction. Clearly, the claim is true for $t=1$ . Now, suppose the claim holds for every $s\\leq t$ for some $t\\in[T]$ . Since $\\Pi_{\\mathcal{C}}$ is a contractive operator ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{t+1}^{2}=\\|\\mathbf x_{t+1}-\\mathbf x^{*}\\|^{2}\\leq D_{t}^{2}-2\\eta\\left\\langle\\nabla F(\\mathbf x_{t})-\\mathbf b_{t}-\\mathbf v_{t},\\mathbf x_{t}-\\mathbf x^{*}\\right\\rangle+\\eta^{2}\\|\\nabla F(\\mathbf x_{t})-\\mathbf b_{t}-\\mathbf v_{t}\\|}\\\\ &{\\qquad\\leq D_{t}^{2}-2\\eta\\left\\langle\\nabla F(\\mathbf x_{t}),\\mathbf x_{t}-\\mathbf x^{*}\\right\\rangle+2\\eta\\left\\langle\\mathbf b_{t},\\mathbf x_{t}-\\mathbf x^{*}\\right\\rangle+2\\eta\\left\\langle\\mathbf v_{t},\\mathbf x_{t}-\\mathbf x^{*}\\right\\rangle}\\\\ &{\\qquad+\\,2\\eta^{2}\\|\\nabla F(\\mathbf x_{t})\\|+4\\eta^{2}\\|\\mathbf v_{t}\\|^{2}+4\\eta^{2}\\|\\mathbf b_{t}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "By the coercivity property, ", "page_idx": 44}, {"type": "equation", "text": "$$\n-2\\eta\\left\\langle\\nabla F(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle\\leq-2\\eta[F(\\mathbf{x}_{t})-F(\\mathbf{x}^{*})]-\\frac{\\eta}{L}\\|\\nabla F(\\mathbf{x}_{t})\\|^{2}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "Substituting this into the recurrence for $D_{t+1}^{2}$ , we obtain the following: ", "page_idx": 44}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{t+1}^{2}\\leq D_{t}^{2}-2\\eta[F(\\mathbf{x}_{t})-F(\\mathbf{x}^{*})]+2\\eta\\left\\langle\\mathbf{v}_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle+2\\eta\\left\\langle\\mathbf{b}_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle}\\\\ &{\\qquad+\\eta(2\\eta-1/L)\\|\\nabla F(\\mathbf{x}_{t})\\|^{2}+4\\eta^{2}\\|\\mathbf{v}_{t}\\|^{2}+4\\eta^{2}\\|\\mathbf{b}_{t}\\|^{2}}\\\\ &{\\qquad\\leq D_{t}^{2}+2\\eta\\left\\langle\\mathbf{v}_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle+2\\eta\\left\\langle\\mathbf{b}_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle+4\\eta^{2}\\|\\mathbf{v}_{t}\\|^{2}+4\\eta^{2}\\|\\mathbf{b}_{t}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 44}, {"type": "text", "text": "where we use the fact that $\\eta\\,\\leq\\,^{1}\\!/2L$ . Now, by the Cauchy Schwarz inequality and the fact that $a b\\leq a^{2}+b^{2}/4$ we obtain the following: ", "page_idx": 45}, {"type": "equation", "text": "$$\n2\\eta\\left\\langle\\mathbf{b}_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle\\leq\\frac{D_{t}^{2}}{2T}+\\eta^{2}T\\|\\mathbf{b}_{t}\\|^{2}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "It follows that ", "page_idx": 45}, {"type": "equation", "text": "$$\nD_{t+1}^{2}\\leq\\left(1+\\frac{1}{2T}\\right)D_{t}^{2}+5\\eta^{2}T\\|\\mathbf{b}_{t}\\|^{2}+4\\eta^{2}\\|\\mathbf{v}_{t}\\|^{2}-2\\eta\\left\\langle\\mathbf{v}_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Unrolling the above recursion for $t$ steps and using the fact that $(1+1/2T)^{T}\\,\\leq\\,2$ , we obtain the following: ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle{\\cal D}_{t+1}^{2}\\leq\\left(1+\\frac{1}{2T}\\right)^{T}{\\cal D}_{1}^{2}+\\sum_{s=1}^{t}\\left(1+\\frac{1}{2T}\\right)^{t-s}\\left(5\\eta^{2}T\\|{\\bf b}_{s}\\|^{2}+4\\eta^{2}\\|{\\bf v}_{s}\\|^{2}+2\\eta\\left\\langle{\\bf v}_{s},{\\bf x}_{s}-{\\bf x}^{*}\\right\\rangle\\right)}\\\\ {\\displaystyle\\leq2{\\cal D}_{1}^{2}+\\sum_{s=1}^{t}10\\eta^{2}T\\|{\\bf b}_{s}\\|^{2}+8\\eta^{2}\\|{\\bf v}_{s}\\|^{2}-4\\eta\\left\\langle{\\bf v}_{s},{\\bf x}_{s}-{\\bf x}^{*}\\right\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "By the induction hypothesis, $\\mathbb{1}\\left\\{E_{s}\\right\\}=1\\,\\forall s\\in[t]$ . Hence, ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{t+1}^{2}\\leq2D_{1}^{2}+10\\eta^{2}T\\displaystyle\\sum_{s=1}^{t}\\|\\tilde{\\mathbf{b}}_{s}\\|^{2}+8\\eta^{2}\\displaystyle\\sum_{s=1}^{t}\\|\\tilde{\\mathbf{v}}_{s}\\|^{2}-4\\eta\\displaystyle\\sum_{s=1}^{t}\\big\\langle\\tilde{\\mathbf{v}}_{s},\\mathbf{d}_{s}\\big\\rangle}\\\\ &{\\qquad\\leq2D_{1}^{2}+10\\eta^{2}T^{2}B^{2}+8C_{M}\\eta^{2}g^{2}T+16\\eta D_{1}\\left[\\sqrt{V t\\ln(K/\\delta)}+2\\Gamma\\ln(K/\\delta)\\right]}\\\\ &{\\qquad\\leq3D_{1}^{2}+10\\eta^{2}T^{2}B^{2}+8C_{M}\\eta^{2}g^{2}T+64\\eta^{2}\\left(\\sqrt{V t\\ln(K/\\delta)}+2\\Gamma\\ln(K/\\delta)\\right)^{2}}\\\\ &{\\qquad\\leq3D_{1}^{2}+10\\eta^{2}T^{2}B^{2}+8C_{M}\\eta^{2}g^{2}T+128\\eta^{2}V T\\ln(K/\\delta)+1024\\Gamma^{2}\\ln(K/\\delta)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where the second inequality follows from the Lemma 11 and the fact that we have conditioned on $E$ . Note that by definition of $\\bar{g}^{2}$ and the AM-GM inequality ", "page_idx": 45}, {"type": "equation", "text": "$$\ng^{2}T\\geq4\\Gamma^{2}\\ln(K/\\delta)^{2}+\\frac{V^{2}T^{2}}{4\\Gamma^{2}}\\geq\\operatorname*{max}\\{4\\Gamma^{2}\\ln(K/\\delta)^{2},2V T\\ln(K/\\delta)\\}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "It follows that ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{t+1}^{2}\\leq3D_{1}^{2}+10\\eta^{2}T^{2}B^{2}+8(C_{M}+40)\\eta^{2}g^{2}T}\\\\ &{\\qquad\\leq3D_{1}^{2}+10c^{2}D_{1}^{2}+c^{2}(8C_{M}+320)D_{1}^{2}}\\\\ &{\\qquad\\leq4D_{1}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "where the second inequality uses the definition of $\\eta$ and the fact that $B^{\\prime}$ and $g^{\\prime}$ upper bound $B$ and $G$ respectively by equations (25) and (27) and the last inequality sets \u221a8CM+330. Hence, $D_{t+1}\\leq2D_{1}$ which proves the claim by induction. ", "page_idx": 45}, {"type": "text", "text": "E Analysis for Lipschitz Convex Functions ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Let deff = T\u2225r\u03a3(\u2225\u03a32) . Since \u03a3 is positive semidefinite, 1 \u2264 deff \u2264 d. Moreover, let clip\u0393(gt) = $\\partial F({\\bf x}_{t})+{\\bf b}_{t}+{\\bf v}_{t}$ where $\\mathbf{b}_{t}=\\mathbb{E}[{\\mathsf{c l i p}}_{\\Gamma}(\\mathbf{g}_{t})|{\\mathcal F}_{t}]-\\partial F(\\mathbf{x}_{t})$ represents the bias due to clipping and $\\mathbb{E}[\\mathbf{v}_{t}|\\mathcal{F}_{t}]=0$ . Let $D_{t}=\\|\\mathbf{x}_{t}-\\mathbf{x}^{*}\\|$ where $\\mathbf{x}^{*}$ is the minimizer of $F$ considered in the statement of Theorem 3. Using the smoothness and convexity properties of $F$ , we first prove the following intermediate average iterate guarantee: ", "page_idx": 45}, {"type": "text", "text": "Lemma 14 (Intermediate Average Iterate Guarantee). The following holds for any $\\eta>0$ ", "page_idx": 45}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal F}({\\hat{\\mathbf{x}}}_{T})-{\\cal F}({\\mathbf{x}}^{*})\\leq\\frac{D_{1}^{2}}{2\\eta T}-\\frac{1}{T}\\sum_{t=1}^{T}\\langle\\mathbf{b}_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle-\\frac{1}{T}\\sum_{t=1}^{T}\\langle\\mathbf{v}_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\rangle}}\\\\ {~~}\\\\ {{\\displaystyle~+\\,\\eta G^{2}+\\frac{2\\eta}{T}\\sum_{t=1}^{T}\\|\\mathbf{b}_{t}\\|^{2}+\\frac{2\\eta}{T}\\sum_{t=1}^{T}\\|\\mathbf{v}_{t}\\|^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 45}, {"type": "text", "text": "Define the events $E_{t}$ and the random vectors ${\\bf d}_{t}$ as follows for $t\\in[T]$ : ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{l}{E_{t}=\\{D_{t}\\leq2D_{1}\\}}\\\\ {\\mathbf{d}_{t}=(\\mathbf{x}_{t}-\\mathbf{x}^{*})\\mathbb{1}\\left\\{E_{t}\\right\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "We use the following lemma to control the bias ", "page_idx": 46}, {"type": "text", "text": "Lemma 15 (Bias Control). For every $t\\in[T],$ , $\\|\\mathbf{b}_{t}\\|\\leq B$ where $B$ is defined as follows: ", "page_idx": 46}, {"type": "equation", "text": "$$\nB=\\frac{\\|\\Sigma\\|_{2}\\sqrt{d_{\\mathsf{e f f}}}}{\\Gamma}+\\frac{G\\sqrt{\\|\\Sigma\\|_{2}}}{\\Gamma}+\\frac{G^{3}}{\\Gamma^{2}}+\\frac{\\|\\Sigma\\|_{2}d_{\\mathsf{e f f}}G}{\\Gamma^{2}}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "We use the following lemma to control the varince ", "page_idx": 46}, {"type": "text", "text": "Lemma 16 (Variance Control). Let $V\\geq0$ be defined as follows: ", "page_idx": 46}, {"type": "equation", "text": "$$\nV=\\|\\Sigma\\|_{2}+\\frac{G^{4}}{\\Gamma^{2}}+\\frac{G^{2}\\|\\Sigma\\|_{2}d_{\\mathsf{e f f}}}{\\Gamma^{2}}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Then the following holds with probability at least $1-\\delta$ uniformly for every $t\\in[T]$ ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{s=1}^{t}\\left\\langle\\mathbf{v}_{s},\\mathbf{d}_{s}\\right\\rangle\\leq4D_{1}\\sqrt{V t\\ln(^{K}/\\delta)}+8\\Gamma D_{1}\\ln(K/\\delta)}\\\\ {\\displaystyle\\sum_{s=1}^{t}\\|\\mathbf{v}_{s}\\|^{2}\\leq C_{M}g^{2}T}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "where $C_{M}$ is a numerical constant and $g^{2}$ is defined as follows ", "page_idx": 46}, {"type": "equation", "text": "$$\ng^{2}=\\|\\Sigma\\|_{2}d_{\\mathsf{e f f}}+\\frac{4\\Gamma^{2}\\ln(K/\\delta)^{2}}{T}+\\frac{V^{2}T}{4\\Gamma^{2}}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Let $E$ denote the following event ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{l}{E=\\{\\displaystyle\\sum_{s=1}^{t}\\left<\\mathbf{v}_{s},\\mathbf{d}_{s}\\right>\\leq4D_{1}\\sqrt{V t\\ln(K/\\delta)}+8\\Gamma D_{1}\\ln(K/\\delta)\\quad\\forall\\,t\\in[T]}\\\\ {\\displaystyle\\sum_{s=1}^{t}\\|\\mathbf{v}_{s}\\|^{2}\\leq C_{M}g^{2}T\\quad\\forall t\\in[T]\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Note that by Lemma 16, $\\mathbb{P}(E)\\ge1-\\delta$ . We define the constant $A$ as follows: ", "page_idx": 46}, {"type": "equation", "text": "$$\nA=\\|\\Sigma\\|_{2}{\\sqrt{d_{\\mathsf{e f f}}}}+G{\\sqrt{\\|\\Sigma\\|_{2}}}={\\sqrt{\\|\\Sigma\\|_{2}}}\\left({\\sqrt{\\mathsf{T r}(\\Sigma)}}+G\\right)\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "We now set the clipping level $\\begin{array}{r}{\\Gamma=\\sqrt{\\frac{A T}{\\ln\\left(\\ K/\\delta\\right)}}}\\end{array}$ ln(AKT/\u03b4). For this choice of \u0393, we now simplify the expression for $B$ as follows: ", "page_idx": 46}, {"type": "equation", "text": "$$\nB=\\sqrt{\\frac{A\\ln(^{K/\\delta})}{T}}+\\frac{G\\left(\\|\\Sigma\\|_{2}d_{\\mathsf{e f f}}+G^{2}\\right)\\ln(K/\\delta)}{A T}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Similarly, the expression for $V$ can be simplified as follows ", "page_idx": 46}, {"type": "equation", "text": "$$\nV=\\|\\Sigma\\|_{2}+{\\frac{G^{2}\\ln(K/\\delta)}{A T}}\\left(\\|\\Sigma\\|_{2}d_{\\mathsf{e f f}}+G^{2}\\right)\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "Using the above inequality, we derive the following upper bound for $g$ : ", "page_idx": 46}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g\\leq\\sqrt{\\|\\Sigma\\|_{2}d_{\\mathrm{eff}}}+\\frac{2\\Gamma\\ln\\left(K/\\delta\\right)}{\\sqrt{T}}+\\frac{V\\sqrt{T}}{2\\Gamma}}\\\\ &{\\,\\,\\,=\\sqrt{\\|\\Sigma\\|_{2}d_{\\mathrm{eff}}}+2\\sqrt{A\\ln(K/\\delta)}+\\frac{V\\sqrt{\\ln\\left(K/\\delta\\right)}}{2\\sqrt{A}}}\\\\ &{\\,\\,\\,=\\sqrt{\\|\\Sigma\\|_{2}d_{\\mathrm{eff}}}+2\\sqrt{A\\ln(K/\\delta)}+\\frac{\\|\\Sigma\\|_{2}\\sqrt{\\ln(K/\\delta)}}{2\\sqrt{A}}+\\frac{G^{2}\\ln\\left(K/\\delta\\right)^{3/2}}{A^{3/2}T}\\left(\\|\\Sigma\\|_{2}d_{\\mathrm{eff}}+G^{2}\\right)}\\\\ &{\\,\\,\\,\\leq\\sqrt{\\|\\Sigma\\|_{2}d_{\\mathrm{eff}}}+3\\sqrt{A\\ln(K/\\delta)}+\\frac{G^{2}\\ln(K/\\delta)^{3/2}}{A^{3/2}T}\\left(\\|\\Sigma\\|_{2}d_{\\mathrm{eff}}+G^{2}\\right)=g^{\\prime}}\\end{array}\n$$", "text_format": "latex", "page_idx": 46}, {"type": "text", "text": "We also prove the following uniform upper bound on the iterates $\\mathbf{x}_{t}$ ", "page_idx": 46}, {"type": "text", "text": "Lemma 17 (Iterate Boun Let $\\eta\\le c\\operatorname*{min}\\{D_{1}/B T,D_{1}/g^{\\prime}\\sqrt{T},D_{1}/G\\sqrt{T}\\}$ where $\\begin{array}{r}{c=\\frac{1}{\\sqrt{8C_{M}+334}}}\\end{array}$ . Then, conditioned on the event $E$ $\\exists,\\,D_{t}\\leq2D_{1}\\,\\forall t\\in[T]$ . ", "page_idx": 47}, {"type": "text", "text": "Equipped with the above lemmas, we now prove the following theorem which is a formal restatement of Theorem 4 ", "page_idx": 47}, {"type": "text", "text": "Theorem 8 (Lipschitz Convex Objectives). Let Assumptions Convexity, $G$ -Lipschitzness and Bdd. $2^{\\mathsf{n d}}$ Moment be satisfied. Then, for any $\\delta\\in(0,{^1\\mathord{/}{\\vphantom{^{1}}^{2}}})$ and $\\bar{T}\\geq\\ln(\\ln(d))$ , there exists an $\\eta\\in(0,G/\\sqrt{T}]$ such that the average iterate of Algorithm $^{\\,l}$ run for $T$ iterations with step-size $\\eta_{t}=\\eta$ and clipping level $\\begin{array}{r}{\\Gamma=\\sqrt{\\frac{T\\sqrt{\\|\\Sigma\\|_{2}}(\\sqrt{\\mathsf{T r}(\\Sigma)}+G)}{\\ln\\left(\\ln\\left(T\\right)/\\delta\\right)}}}\\end{array}$ satisfies the following with probability at least $1-\\delta$ ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{F(\\hat{\\mathbf{x}}_{T})-F(\\mathbf{x}^{*})\\lesssim\\frac{D_{1}G}{\\sqrt{T}}+D_{1}\\sqrt{\\frac{\\mathsf{T r}(\\Sigma)+\\sqrt{\\|\\Sigma\\|_{2}}\\left(\\sqrt{\\mathsf{T r}(\\Sigma)}+G\\right)\\ln(K/\\delta)}{T}}}\\\\ &{\\qquad\\qquad\\qquad+\\,\\frac{D_{1}G\\ln(K/\\delta)}{T}\\sqrt{\\frac{\\mathsf{T r}(\\Sigma)+G^{2}}{\\|\\Sigma\\|_{2}}}+\\frac{D_{1}G^{2}\\ln(1/\\delta)^{3/2}}{T^{3/2}}\\left(\\frac{\\mathsf{T r}(\\Sigma)+G^{2}}{\\|\\Sigma\\|^{3}}\\right)^{1/4}}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "E.1 Proof of Lemma 14 ", "text_level": 1, "page_idx": 47}, {"type": "text", "text": "Proof. Since $\\Pi_{\\mathcal{C}}$ is a contractive operator ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\i}_{t+1}^{2}=\\|\\mathbf x_{t+1}-\\mathbf x^{*}\\|^{2}\\leq D_{t}^{2}-2\\eta\\left\\langle\\partial F(\\mathbf x_{t})+\\mathbf b_{t}+\\mathbf v_{t},\\mathbf x_{t}-\\mathbf x^{*}\\right\\rangle+\\eta^{2}\\|\\nabla F(\\mathbf x_{t})+\\mathbf b_{t}+\\mathbf v_{t}\\|}\\\\ &{\\qquad\\leq D_{t}^{2}-2\\eta\\left\\langle\\partial F(\\mathbf x_{t}),\\mathbf x_{t}-\\mathbf x^{*}\\right\\rangle-2\\eta\\left\\langle\\mathbf b_{t},\\mathbf x_{t}-\\mathbf x^{*}\\right\\rangle-2\\eta\\left\\langle\\mathbf v_{t},\\mathbf x_{t}-\\mathbf x^{*}\\right\\rangle}\\\\ &{\\qquad+\\left.2\\eta^{2}\\|\\partial F(\\mathbf x_{t})\\|^{2}+4\\eta^{2}\\|\\mathbf v_{t}\\|^{2}+4\\eta^{2}\\|\\mathbf b_{t}\\|^{2}}\\\\ &{\\qquad\\leq D_{t}^{2}-2\\eta[F(\\mathbf x_{t})-F(\\mathbf x^{*})]-2\\eta\\left\\langle\\mathbf b_{t},\\mathbf x_{t}-\\mathbf x^{*}\\right\\rangle-2\\eta\\left\\langle\\mathbf v_{t},\\mathbf x_{t}-\\mathbf x^{*}\\right\\rangle+2\\eta^{2}G^{2}+4\\eta^{2}\\|\\mathbf b_{t}\\|^{2}+4\\eta^{2}\\|\\mathbf b_{t}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "where the second inequality follows from the definition of the subgradient and the $G$ lipschitzness of $F$ . Rearranging and taking averages on both sides ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\sum_{t=1}^{T}F(\\mathbf{x}_{t})-F(\\mathbf{x}^{*})\\leq\\frac{D_{1}^{2}}{2\\eta T}-\\frac{1}{T}\\sum_{t=1}^{T}\\left<\\mathbf{b}_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right>-\\frac{1}{T}\\sum_{t=1}^{T}\\left<\\mathbf{v}_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right>}\\\\ {\\displaystyle+\\,\\eta G^{2}+\\frac{2\\eta}{T}\\sum_{t=1}^{T}\\|\\mathbf{b}_{t}\\|^{2}+\\frac{2\\eta}{T}\\sum_{t=1}^{T}\\|\\mathbf{v}_{t}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "Using the above inequality and the convexity of $F$ , we conclude that ", "page_idx": 47}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle F(\\hat{\\mathbf{x}}_{T})-F(\\mathbf{x^{*}})=F\\left(\\frac{1}{T}\\sum_{t=1}^{T}\\mathbf{x}_{t}\\right)-F(\\mathbf{x^{*}})}\\\\ {\\displaystyle\\le\\frac{1}{T}\\sum_{t=1}^{T}F(\\mathbf{x}_{t})-F(\\mathbf{x^{*}})}\\\\ {\\displaystyle\\le\\frac{D_{1}^{2}}{2\\eta T}-\\frac{1}{T}\\sum_{t=1}^{T}\\langle\\mathbf{b}_{t},\\mathbf{x}_{t}-\\mathbf{x^{*}}\\rangle-\\frac{1}{T}\\sum_{t=1}^{T}\\langle\\mathbf{v}_{t},\\mathbf{x}_{t}-\\mathbf{x^{*}}\\rangle}\\\\ {\\displaystyle+\\,\\eta G^{2}+\\frac{2\\eta}{T}\\sum_{t=1}^{T}\\|\\mathbf{b}_{t}\\|^{2}+\\frac{2\\eta}{T}\\sum_{t=1}^{T}\\|\\mathbf{v}_{t}\\|^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 47}, {"type": "text", "text": "E.2 Proof of Lemma 15 ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "We recall that $\\mathbf{b}_{t}=\\mathbb{E}[\\mathbf{g}_{t}|\\mathcal{F}_{t-1}]-\\mathbb{E}[\\mathsf{c l i p}_{\\Gamma}(\\mathbf{g}_{t})|\\mathcal{F}_{t-1}]-$ . Since $\\mathsf{C o v}[\\mathbf{g}_{t}|\\mathcal{F}_{t-1}]\\,\\preceq\\,\\Sigma$ by Assumption Bdd. $2^{\\mathsf{n d}}$ Moment, we obtain the following bound on $\\|\\mathbf{b}_{t}\\|$ by an application of Lemma 4 ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|\\mathbf{b}_{t}\\|\\leq\\frac{\\|\\Sigma\\|_{2}\\sqrt{d_{\\mathrm{eff}}}}{\\Gamma}+\\frac{\\|\\partial F(\\mathbf{x}_{t})\\|\\sqrt{\\|\\Sigma\\|_{2}}}{\\Gamma}+\\frac{\\|\\partial F(\\mathbf{x}_{t})\\|^{3}}{\\Gamma^{2}}+\\frac{\\|\\Sigma\\|_{2}d_{\\mathrm{eff}}\\|\\partial F(\\mathbf{x}_{t})\\|}{\\Gamma^{2}}}\\\\ {\\leq\\frac{\\|\\Sigma\\|_{2}\\sqrt{d_{\\mathrm{eff}}}}{\\Gamma}+\\frac{G\\sqrt{\\|\\Sigma\\|_{2}}}{\\Gamma}+\\frac{G^{3}}{\\Gamma^{2}}+\\frac{\\|\\Sigma\\|_{2}d_{\\mathrm{eff}}G}{\\Gamma^{2}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "E.3 Proof of Lemma 16 ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "For any $s\\in[T]$ , we recall that $\\mathbf{v}_{s}=\\mathbb{E}\\,[\\mathsf{c l i p}_{\\Gamma}(\\mathbf{g}_{s})|\\mathcal{F}_{s-1}]-\\mathsf{c l i p}_{\\Gamma}(\\mathbf{g}_{s})$ . Since $\\mathbb{E}[\\mathbf{g}_{s}|\\mathcal{F}_{s-1}]=\\partial F(\\mathbf{x}_{s})$ and $\\mathsf{C o v}[\\mathbf{g}_{s}|\\bar{\\mathcal{F}}_{s-1}]\\preceq\\Sigma$ , we obtain the following from Lemma 4 ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\|\\mathbb{E}\\left[\\mathbf{v}_{s}\\mathbf{v}_{s}^{T}|\\mathcal{F}_{s-1}\\right]\\|_{2}=\\|\\mathsf{C o v}\\left[\\mathsf{c l i p}_{\\Gamma}(\\mathbf{g}_{s})|\\mathcal{F}_{s-1}\\right]\\|\\leq\\|\\Sigma\\|_{2}+\\frac{\\|\\partial F(\\mathbf{x}_{s})\\|^{4}}{\\Gamma^{2}}+\\frac{\\|\\partial F(\\mathbf{x}_{s})\\|^{2}\\mathsf{T r}(\\Sigma)}{\\Gamma^{2}}}\\\\ {\\displaystyle\\qquad\\qquad\\qquad\\leq\\|\\Sigma\\|_{2}+\\frac{G^{4}}{\\Gamma^{2}}+\\frac{G^{2}\\mathsf{T r}(\\Sigma)}{\\Gamma^{2}}}\\\\ {\\displaystyle\\mathsf{T r}\\left(\\mathbb{E}\\left[\\mathbf{v}_{s}\\mathbf{v}_{s}^{T}|\\mathcal{F}_{s-1}\\right]\\right)=\\mathsf{T r}\\left(\\mathsf{C o v}\\left[\\mathsf{c l i p}_{\\Gamma}(\\mathbf{g}_{s})|\\mathcal{F}_{s}\\right]\\right)\\leq\\mathsf{T r}(\\Sigma)}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "For $s\\in[T]$ , define $h_{s}=\\langle\\mathbf{v}_{s},\\mathbf{d}_{s}\\rangle$ . We note that ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~~~~~~~~~|h_{s}|\\leq\\|\\mathbf{v}_{s}\\|\\cdot\\|\\mathbf{d}_{s}\\|\\leq4\\Gamma D_{1}}\\\\ &{~\\mathbb{E}\\left[h_{s}|\\mathcal{F}_{s-1}\\right]=\\langle\\mathbb{E}[\\mathbf{v}_{s}|\\mathcal{F}_{s-1}],\\mathbf{d}_{s}\\rangle=0}\\\\ &{~\\mathbb{E}\\left[h_{s}^{2}|\\mathcal{F}_{s-1}\\right]=\\mathbf{d}_{s}^{T}\\mathbb{E}[\\mathbf{v}_{s}\\mathbf{v}_{s}^{T}]\\mathbf{d}_{s}}\\\\ &{~~~~~~~~~~~~~~~~~~~=\\mathbf{d}_{s}^{T}\\Sigma_{s}\\mathbf{d}_{s}}\\\\ &{~~~~~~~~~~~~~~~~\\leq\\|\\mathbf{d}_{s}\\|^{2}\\|\\Sigma_{s}\\|\\leq4D_{1}^{2}V}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "Hence, by Freedman\u2019s inequality (Lemma 3), we conclude that the following holds with probability at least $\\bar{1}-\\delta/2$ : ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t}\\left\\langle\\tilde{\\mathbf{v}}_{t},\\mathbf{d}_{t}\\right\\rangle\\leq4D_{1}\\sqrt{V t\\ln(^{K/\\delta})}+8\\Gamma D_{1}\\ln(K/\\delta)\\quad\\forall\\,t\\in[T]\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "We now apply Corollary 6 with $p_{s}\\,=\\,V$ , $q_{s}=\\mathsf{T r}(\\Sigma)$ and $\\tau=2\\Gamma$ to conclude that the following holds with probability at least $1-\\delta/2$ uniformly for every $t\\in[T]$ ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{s=1}^{t}\\|\\widetilde{\\mathbf{v}}_{s}\\|^{2}\\leq4C_{M}\\Gamma^{2}\\ln(K/\\delta)^{2}+C_{M}\\mathsf{U P}(t)\\mathsf{T r}(\\Sigma)+\\frac{C_{M}t\\mathsf{U P}(t)V^{2}}{4\\Gamma^{2}}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\leq4C_{M}\\Gamma^{2}\\ln(K/\\delta)^{2}+C_{M}T\\mathsf{T r}(\\Sigma)+\\frac{C_{M}T^{2}V^{2}}{4\\Gamma^{2}}}\\\\ &{\\quad\\quad\\quad\\quad\\leq C_{M}T\\left(\\|\\Sigma\\|_{2}d_{\\mathsf{e f f}}+\\frac{4\\Gamma^{2}\\ln(K/\\delta)^{2}}{T}+\\frac{V^{2}T}{4\\Gamma^{2}}\\right)=C_{M}g^{2}T}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where ", "page_idx": 48}, {"type": "equation", "text": "$$\ng^{2}=\\|\\Sigma\\|_{2}d_{\\mathsf{e f f}}+\\frac{4\\Gamma^{2}\\ln(K/\\delta)^{2}}{T}+\\frac{V^{2}T}{4\\Gamma^{2}}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "The proof is concluded by a union bound ", "page_idx": 48}, {"type": "text", "text": "E.4 Proof of Lemma 17 ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "We prove the claim via induction. Clearly, the claim is true for $t=1$ . Now, suppose the claim holds for every $s\\leq t$ for some $t\\in[T]$ . Since $\\Pi_{\\mathcal{C}}$ is a contractive operator ", "page_idx": 48}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{\\i}_{t+1}^{2}=\\|\\mathbf{x}_{t+1}-\\mathbf{x}^{*}\\|^{2}\\leq D_{t}^{2}-2\\eta\\left\\langle\\partial F(\\mathbf{x}_{t})+\\mathbf{b}_{t}+\\mathbf{v}_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle+\\eta^{2}\\|\\nabla F(\\mathbf{x}_{t})+\\mathbf{b}_{t}+\\mathbf{v}_{t}\\|}\\\\ &{\\qquad\\leq D_{t}^{2}-2\\eta\\left\\langle\\partial F(\\mathbf{x}_{t}),\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle-2\\eta\\left\\langle\\mathbf{b}_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle-2\\eta\\left\\langle\\mathbf{v}_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle}\\\\ &{\\qquad+\\ 2\\eta^{2}\\|\\partial F(\\mathbf{x}_{t})\\|^{2}+4\\eta^{2}\\|\\mathbf{v}_{t}\\|^{2}+4\\eta^{2}\\|\\mathbf{b}_{t}\\|^{2}}\\\\ &{\\qquad\\leq D_{t}^{2}-2\\eta[F(\\mathbf{x}_{t})-F(\\mathbf{x}^{*})]-2\\eta\\left\\langle\\mathbf{b}_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle-2\\eta\\left\\langle\\mathbf{v}_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle+2\\eta^{2}G^{2}+4\\eta^{2}\\|\\mathbf{b}_{t}\\|^{2}+4\\eta^{2}\\|\\mathbf{b}_{t}\\|^{2},}\\end{array}\n$$", "text_format": "latex", "page_idx": 48}, {"type": "text", "text": "where the second inequality follows from the definition of the subgradient and the $G$ lipschitzness of $F$ . Now, by the Cauchy Schwarz inequality and the fact that $a b\\leq\\dot{a}^{2}+b^{2}/4$ we obtain the following: ", "page_idx": 49}, {"type": "equation", "text": "$$\n-2\\eta\\left\\langle\\mathbf{b}_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle\\leq\\frac{D_{t}^{2}}{2T}+\\eta^{2}T\\|\\mathbf{b}_{t}\\|^{2}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "It follows that ", "page_idx": 49}, {"type": "equation", "text": "$$\nD_{t+1}^{2}\\leq\\left(1+\\frac{1}{2T}\\right)D_{t}^{2}+5\\eta^{2}T\\|\\mathbf{b}_{t}\\|^{2}+2\\eta^{2}G^{2}+4\\eta^{2}\\|\\mathbf{v}_{t}\\|^{2}-2\\eta\\left\\langle\\mathbf{v}_{t},\\mathbf{x}_{t}-\\mathbf{x}^{*}\\right\\rangle\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Unrolling the above recursion for $t$ steps and using the fact that $(1+1/2T)^{T}\\,\\leq\\,2$ , we obtain the following: ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle)_{t+1}^{2}\\leq\\left(1+\\frac{1}{2T}\\right)^{T}D_{1}^{2}+\\sum_{s=1}^{t}\\left(1+\\frac{1}{2T}\\right)^{t-s}\\left(5\\eta^{2}T\\|{\\bf b}_{s}\\|^{2}+2\\eta^{2}G^{2}+4\\eta^{2}\\|{\\bf v}_{s}\\|^{2}-2\\eta\\left\\langle{\\bf v}_{s},{\\bf x}_{s}-{\\bf x}_{s}\\right\\rangle\\right)+\\eta^{2}\\|{\\bf v}_{s}\\|^{2}}\\\\ {\\displaystyle\\leq2D_{1}^{2}+4\\eta^{2}G^{2}T+\\sum_{s=1}^{t}10\\eta^{2}T\\|{\\bf b}_{s}\\|^{2}+8\\eta^{2}\\|{\\bf v}_{s}\\|^{2}-4\\eta\\left\\langle{\\bf v}_{s},{\\bf x}_{s}-{\\bf x}^{*}\\right\\rangle}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "By the induction hypothesis, $\\mathbb{1}\\left\\{E_{s}\\right\\}=1\\,\\forall s\\in[t]$ . Hence, ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{t+1}^{2}\\leq2D_{1}^{2}+4\\eta^{2}G^{2}T+10\\eta^{2}T\\displaystyle\\sum_{s=1}^{t}\\|\\mathbf{b}_{s}\\|^{2}+8\\eta^{2}\\displaystyle\\sum_{s=1}^{t}\\|\\mathbf{v}_{s}\\|^{2}-4\\eta\\displaystyle\\sum_{s=1}^{t}\\left\\langle\\mathbf{v}_{s},\\mathbf{d}_{s}\\right\\rangle}\\\\ &{\\qquad\\leq2D_{1}^{2}+4\\eta^{2}G^{2}T++10\\eta^{2}T^{2}B^{2}+8C_{M}\\eta^{2}g^{2}T+16\\eta D_{1}\\left[\\sqrt{V t\\ln(K/\\delta)}+2\\Gamma\\ln(K/\\delta)\\right]}\\\\ &{\\qquad\\leq3D_{1}^{2}+4\\eta^{2}G^{2}T++10\\eta^{2}T^{2}B^{2}+8C_{M}\\eta^{2}g^{2}T+64\\eta^{2}\\left(\\sqrt{V t\\ln(K/\\delta)}+2\\Gamma\\ln(K/\\delta)\\right)^{2}}\\\\ &{\\qquad\\leq3D_{1}^{2}+4\\eta^{2}G^{2}T++10\\eta^{2}T^{2}B^{2}+8C_{M}\\eta^{2}g^{2}T+128\\eta^{2}V T\\ln(K/\\delta)+1024\\Gamma^{2}\\ln(K/\\delta)^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where the second inequality follows from the Lemma 15 and the fact that we have conditioned on $E$ . Note that by definition of $\\bar{g}^{2}$ and the AM-GM inequality ", "page_idx": 49}, {"type": "equation", "text": "$$\ng^{2}T\\geq4\\Gamma^{2}\\ln(K/\\delta)^{2}+\\frac{V^{2}T^{2}}{4\\Gamma^{2}}\\geq\\operatorname*{max}\\{4\\Gamma^{2}\\ln(K/\\delta)^{2},2V T\\ln(K/\\delta)\\}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "It follows that ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{D_{t+1}^{2}\\leq3D_{1}^{2}+4\\eta^{2}G^{2}T+10\\eta^{2}T^{2}B^{2}+8(C_{M}+40)\\eta^{2}g^{2}T}\\\\ &{\\qquad\\leq3D_{1}^{2}+4c^{2}D_{1}^{2}+10c^{2}D_{1}^{2}+c^{2}(8C_{M}+320)D_{1}^{2}}\\\\ &{\\qquad\\leq4D_{1}^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "where the second inequality uses the definition of $\\eta$ and the fact that $g^{\\prime}$ upper bounds $g$ , and the last inequality sets c = $\\begin{array}{r}{c=\\frac{1}{\\sqrt{8C_{M}+334}}}\\end{array}$ . Hence, \u00b7 $D_{t+1}\\leq2D_{1}$ which proves the claim by induction. ", "page_idx": 49}, {"type": "text", "text": "F Improved Martingale Concentration via PAC Bayes Theory ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "We have the following re-statement of Theorem 5 for the sake of readability. ", "page_idx": 49}, {"type": "text", "text": "Theorem 9. Suppose $M_{t}$ for $t=0,\\dots,T$ is an $\\mathbb{R}^{d}$ valued martingale such that $M_{0}=0$ almost surely, the martingale difference sequence $\\mathbf{v}_{t}:=M_{t}-M_{t-1}$ is such that $\\|\\mathbf{v}_{t}\\|\\leq\\Gamma$ and $\\mathbb{E}[\\mathbf{v}_{t}\\mathbf{v}_{t}^{\\top}|\\mathcal{F}_{t-1}]=\\dot{\\Sigma_{t}}$ almost surely for every $t=1,\\dots,T$ for some $\\Gamma>0$ . Assume that there are deterministic sequences $p_{1},\\ldots,p_{T}$ and $q_{1},\\ldots,q_{T}$ such that ${\\mathsf{T r}}(\\Sigma_{t})\\leq q_{t}$ and $\\|\\Sigma_{t}\\|\\leq p_{t}$ almost surely. ", "page_idx": 49}, {"type": "text", "text": "Let $\\begin{array}{r}{\\bar{q}:=\\frac{1}{T}\\sum_{t=1}^{T}q_{t}}\\end{array}$ and $\\begin{array}{r}{\\bar{p}:=\\frac{1}{T}\\sum_{t=1}^{T}p_{t}}\\end{array}$ . Then, for any $\\delta\\in(0,\\frac{1}{2})$ ", "page_idx": 49}, {"type": "equation", "text": "$$\n\\mathbb{P}_{t\\leq T}\\left\\|M_{t}\\right\\|\\geq g(T,\\delta)\\sqrt{T})\\leq\\delta\n$$", "text_format": "latex", "page_idx": 49}, {"type": "text", "text": "Where $\\begin{array}{r}{g(T,\\delta)=C\\left[\\sqrt{\\bar{q}}+\\frac{\\bar{p}\\sqrt{T}}{\\Gamma}+\\frac{\\Gamma}{\\sqrt{T}}\\log(\\frac{K}{\\delta})\\right]}\\end{array}$ and $K=\\log\\Theta(\\log((\\frac{\\sqrt{\\bar{q}T}}{\\Gamma}+1)\\log(d+1)))$ ", "page_idx": 49}, {"type": "text", "text": "Define the event ${\\mathcal{A}}_{t}(g):=\\{\\|M_{t}\\|\\leq g{\\sqrt{T}}\\}$ and $B_{t}(g):=\\cap_{s=1}^{t}A_{s}$ . Consider the quantity $N_{t}:=$ $\\begin{array}{r}{\\|M_{t}\\|^{2}-\\sum_{s=1}^{t}\\|\\mathbf{v}_{s}\\|^{2}}\\end{array}$ . ", "page_idx": 50}, {"type": "text", "text": "Theorem 10. Let $\\delta\\in(0,\\frac{1}{2})$ and $\\begin{array}{r}{g=g(T,\\frac{\\delta}{2})}\\end{array}$ be as defined in Theorem 9. Under the conditions of Theorem $^{\\,g}$ , the following inequality holds for some large enough universal constant $C$ . ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}\\left(\\{\\underset{t\\leq T}{\\operatorname*{sup}}|N_{t}|>\\Gamma C g\\sqrt{T}\\log(\\frac{1}{\\delta})+\\frac{C\\nu g T^{3/2}}{\\Gamma}\\}\\cap B_{T}(g)\\right)\\leq\\delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "The next corollary is a simple consequence of the Theorems 9 and 10. ", "page_idx": 50}, {"type": "text", "text": "Corollary 6. Let $\\delta\\in(0,\\frac{1}{2})$ and $\\begin{array}{r}{g=g(T,\\frac{\\delta}{3})}\\end{array}$ be as specified in Theorem 9. Under the conditions of Theorem 9, the following inequality holds with probability at-least $1-\\delta$ : ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}\\|\\mathbf{v}_{t}\\|^{2}\\leq C g^{2}T\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "F.1 Proof of Theorem 9 ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "The aim of this section is to prove the sharp concentration result given in Theorem 9. We now \u221aconsider the concentration of norms of the martingale $\\|M_{t}\\|$ . Define the event $\\mathcal{A}_{t}:=\\{\\|M_{t}\\|\\leq g\\sqrt{T}\\}$ and $B_{t}=\\cap_{s=1}^{t}A_{s}$ . Let $H$ be any stopping time for the martingale $M_{t}$ . We have the following inequality which follows from PAC-Bayes theory (see Equation 5.2.1, Page 159 in [7]). ", "page_idx": 50}, {"type": "text", "text": "Theorem 11. Suppose $\\pi$ be any measure over $\\mathbb{R}^{d}$ and let $\\mathcal{M}_{1}(\\mathbb{R}^{d})$ denote the space of all probability measures over $\\mathbb{R}^{d}$ . Let $\\gamma>0$ be arbitrary. Then conditioned on $B_{T}$ , with probability at-least $1-\\delta$ , the following inequality holds: ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\rho\\in\\mathcal{M}_{1}(\\mathbb{R}^{d})}\\mathbb{E}_{\\theta\\sim\\rho\\gamma}\\left\\langle M_{\\operatorname*{min}(H,T)},\\theta\\right\\rangle-\\mathsf{K L}\\left(\\rho\\|\\vert\\pi\\right)\\leq\\log\\left(\\mathbb{E}_{M}\\mathbb{E}_{\\theta\\sim\\pi}\\frac{\\exp(\\gamma\\left\\langle M_{\\operatorname*{min}(H,T)},\\theta\\right\\rangle)\\mathbb{1}(B_{T})}{\\delta\\mathbb{P}(B_{T})}\\right)\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "We will now bound the exponential moment: $\\mathbb{E}_{M}\\mathbb{E}_{\\theta\\sim\\pi}\\exp(\\gamma\\left\\langle M_{t},\\theta\\right\\rangle)$ whenever $\\pi=\\mathcal{N}(0,\\mathbf{I})$ . ", "page_idx": 50}, {"type": "text", "text": "Theorem 12. Let $\\begin{array}{r}{h(t):=\\sum_{s=1}^{t}\\log\\Big(1+\\frac{\\gamma^{2}}{2}q_{t}\\exp(\\gamma^{2}\\Gamma^{2})+\\gamma^{4}p_{t}g^{2}T\\exp(2\\gamma^{2}\\Gamma g\\sqrt{T})\\Big)}\\end{array}$ . Then, ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta\\sim\\pi}\\exp(\\gamma\\langle M_{t},\\theta\\rangle-h(t))\\mathbb{1}(\\mathcal{B}_{t})\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "is a supermartingale with respect to the filtration $\\mathcal{F}_{t}$ ", "page_idx": 50}, {"type": "text", "text": "Proof. Let $\\Sigma_{t}\\,:=\\,\\mathbb{E}[\\mathbf{v}_{t}\\mathbf{v}_{t}^{\\top}|\\mathcal{F}_{t-1}]$ and $\\nu_{t}\\,:=\\,\\|\\Sigma_{t}\\|$ . First, consider $\\mathbb{E}_{\\theta\\sim\\pi}\\exp(\\gamma\\left\\langle M_{t},\\theta\\right\\rangle)$ . By the properties of the Gaussians, we must have almost surely: ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{\\theta\\sim\\pi}\\exp(\\gamma\\left\\langle M_{t},\\theta\\right\\rangle)\\mathbb{1}(\\mathcal{B}_{t})=\\exp(\\frac{\\gamma^{2}\\|M_{t}\\|^{2}}{2})\\mathbb{1}(\\mathcal{B}_{t})}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "Using the fact that $\\|M_{t}\\|^{2}=\\|\\mathbf{v}_{t}\\|^{2}+2\\langle\\mathbf{v}_{t},M_{t-1}\\rangle+\\|M_{t-1}\\|^{2}$ , we have: ", "page_idx": 50}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\exp(\\frac{\\gamma^{2}\\|M_{t}\\|^{2}}{2})\\mathbb{1}(B_{t})\\bigg|\\mathcal{F}_{t-1}\\right]=\\mathbb{E}\\left[\\exp(\\frac{\\gamma^{2}\\|M_{t-1}\\|^{2}}{2}+\\frac{\\gamma^{2}\\|\\mathbf{v}_{t}\\|^{2}}{2}+\\gamma^{2}\\left\\langle\\mathbf{v}_{t},M_{t-1}\\right\\rangle)\\mathbb{1}(B_{t})\\right]}\\\\ &{\\ =\\mathbb{E}\\left[\\exp(\\frac{\\gamma^{2}\\|\\mathbf{v}_{t}\\|^{2}}{2}+\\gamma^{2}\\left\\langle\\mathbf{v}_{t},M_{t-1}\\right\\rangle)\\mathbb{1}(A_{t})\\bigg|\\mathcal{F}_{t-1}\\right]\\exp(\\frac{\\gamma^{2}\\|M_{t-1}\\|^{2}}{2})\\mathbb{1}(B_{t-1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 50}, {"type": "text", "text": "We will now bound the quantity: $\\begin{array}{r}{\\mathbb{E}\\left[\\exp(\\frac{\\gamma^{2}\\|\\mathbf{v}_{t}\\|^{2}}{2}+\\gamma^{2}\\left\\langle\\mathbf{v}_{t},M_{t-1}\\right\\rangle)\\mathbb{1}(A_{t})\\bigg|\\mathcal{F}_{t-1}\\right]}\\end{array}$ . Using the convexity of $x\\to\\exp(x)$ , we conclude: ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\exp(\\frac{\\gamma^{2}\\|\\mathbf{v}_{t}\\|^{2}}{2}+\\gamma^{2}\\left\\langle\\mathbf{v}_{t},M_{t-1}\\right\\rangle)\\mathbb{1}(\\mathcal{A}_{t})\\bigg\\vert\\mathcal{F}_{t-1}\\right]}\\\\ &{\\leq\\mathbb{E}\\left[\\frac{1}{2}\\exp(\\gamma^{2}\\|\\mathbf{v}_{t}\\|^{2})\\mathbb{1}(\\mathcal{A}_{t})+\\frac{1}{2}\\exp(2\\gamma^{2}\\left\\langle\\mathbf{v}_{t},M_{t-1}\\right\\rangle)\\mathbb{1}(\\mathcal{A}_{t})\\bigg\\vert\\mathcal{F}_{t-1}\\right]}\\\\ &{\\leq\\frac{1}{2}\\left[1+\\gamma^{2}\\mathsf{T r}(\\Sigma_{t})\\exp(\\gamma^{2}\\Gamma^{2})\\right]+\\mathbb{E}\\left[\\frac{1}{2}\\exp(2\\gamma^{2}\\left\\langle\\mathbf{v}_{t},M_{t-1}\\right\\rangle)\\mathbb{1}(\\mathcal{A}_{t})\\bigg\\vert\\mathcal{F}_{t-1}\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "In the second step, we have used the fact that $\\exp(\\gamma^{2}\\|\\mathbf{v}_{t}\\|^{2})\\mathbb{1}(A_{t})\\leq1+\\gamma^{2}\\|\\mathbf{v}_{t}\\|^{2}\\exp(\\gamma^{2}\\Gamma^{2})$ almost surely using the power series expansion of the $\\exp()$ function. Using the power series expansion of $\\exp(x)$ , we have: ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\exp(2\\gamma^{2}\\left\\langle\\mathbf{v}_{t},M_{t-1}\\right\\rangle)\\mathbb{1}(A_{t})\\Bigg|\\mathcal{F}_{t-1}\\right]\\leq\\mathbb{E}\\left[\\exp(2\\gamma^{2}\\left\\langle\\mathbf{v}_{t},M_{t-1}\\right\\rangle)\\Bigg|\\mathcal{F}_{t-1}\\right]}\\\\ &{=1+2\\gamma^{2}\\mathbb{E}[\\langle\\mathbf{v}_{t},M_{t-1}\\rangle|\\mathcal{F}_{t-1}]+\\sum_{k\\geq2}\\frac{2k_{\\gamma}\\beta^{k}}{k!}\\mathbb{E}[\\langle(\\mathbf{v}_{t},M_{t-1})\\rangle^{k}|\\mathcal{F}_{t-1}]}\\\\ &{\\leq1+\\displaystyle\\sum_{k\\geq2}\\frac{2k_{\\gamma}^{2k}}{k!}\\mathbb{E}[\\langle(\\mathbf{v}_{t},M_{t-1})\\rangle^{2}\\mathbb{T}^{k-2}\\|M_{t-1}\\|^{k-2}|\\mathcal{F}_{t-1}]}\\\\ &{\\leq1+\\displaystyle\\sum_{k\\geq2}\\frac{2k_{\\gamma}^{2k}}{k!}\\left\\langle M_{t-1},\\Sigma_{t}M_{t-1}\\right\\rangle\\Gamma^{k-2}\\|M_{t-1}\\|^{k-2}}\\\\ &{\\leq1+\\displaystyle\\sum_{k\\geq2}\\frac{2k_{\\gamma}^{2k}}{k!}\\nu_{t}\\Gamma^{k-2}\\|M_{t-1}\\|^{k}\\leq1+2\\gamma^{4}\\nu_{t}\\|M_{t-1}\\|^{2}\\exp(2\\gamma^{2}\\|M_{t-1}\\|\\Gamma)}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Here, $\\nu_{t}=\\|\\Sigma_{t}\\|_{\\mathsf{o p}}$ In the second step we have used the fact that $\\mathbb{E}[\\mathbf{v}_{t}|\\mathcal{F}_{t-1}]=0$ and the fact that $\\langle\\mathbf{v}_{t},M_{t-1}\\rangle\\leq\\Gamma\\|M_{t-1}\\|$ almost surely. Plugging Equation (36) into Equation (35), we conclude: ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\exp(\\frac{\\gamma^{2}\\|\\mathbf{v}_{t}\\|^{2}}{2}+\\gamma^{2}\\left\\langle\\mathbf{v}_{t},M_{t-1}\\right\\rangle)\\mathbb{1}(A_{t})\\bigg|\\mathcal{F}_{t-1}\\right]}\\\\ &{\\leq1+\\frac{\\gamma^{2}}{2}\\mathsf{T r}(\\Sigma_{t})\\exp(\\gamma^{2}\\Gamma^{2})+\\gamma^{4}\\nu_{t}\\|M_{t-1}\\|^{2}\\exp(2\\gamma^{2}\\Gamma\\|M_{t-1}\\|)}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Using Equation (37) and that under the event $B_{t-1}$ we must have $\\|M_{t-1}\\|\\leq g\\sqrt{T}$ , we conclude: ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\mathbb{E}\\left[\\exp(\\frac{\\gamma^{2}\\|M_{t}\\|^{2}}{2})\\mathbb{1}({\\mathcal{B}}_{t})\\Bigg|{\\mathcal{F}}_{t-1}\\right]}\\\\ {\\leq\\left(1+\\frac{\\gamma^{2}}{2}q_{t}\\exp(\\gamma^{2}\\Gamma^{2})+\\gamma^{4}p_{t}g^{2}T\\exp(2\\gamma^{2}\\Gamma g\\sqrt{T})\\right)\\exp(\\frac{\\gamma^{2}\\|M_{t-1}\\|^{2}}{2})\\mathbb{1}({\\mathcal{B}}_{t-1})}\\\\ {=\\exp(h(t)-h(t-1))\\exp\\left(\\gamma^{2}\\frac{\\|M_{t-1}\\|^{2}}{2}\\right)\\mathbb{1}({\\mathcal{B}}_{t-1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 51}, {"type": "text", "text": "Therefore, by induction, we conclude the statement of the theorem. ", "page_idx": 51}, {"type": "text", "text": "Theorem 13. For any stopping time $H$ , ", "page_idx": 51}, {"type": "equation", "text": "$$\n\\mathbb{E}_{M}\\mathbb{E}_{\\theta\\sim\\pi}\\exp(\\gamma\\left\\langle M_{\\operatorname*{min}(H,T)},\\theta\\right\\rangle)\\mathbb{1}(\\mathcal{B}_{T})\\leq\\exp(h(T))\n$$", "text_format": "latex", "page_idx": 51}, {"type": "equation", "text": "$$\n\\begin{array}{r}{h(T)=\\sum_{t=1}^{T}\\log\\Big(1+\\frac{\\gamma^{2}q_{t}}{2}\\exp(\\gamma^{2}\\Gamma^{2})+\\gamma^{4}p_{t}g^{2}T\\exp(2\\gamma^{2}\\Gamma g\\sqrt{T})\\Big)}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Proof. From Theorem 12 and the optional stopping theorem, we conclude that the following quantity is a super-martingale: ", "page_idx": 52}, {"type": "equation", "text": "$$\nM_{t}^{\\mathrm{exp}}:=\\mathbb{E}_{\\theta\\sim\\pi}\\exp(\\gamma\\left\\langle M_{\\operatorname*{min}(H,t)},\\theta\\right\\rangle-h(\\operatorname*{min}(H,t)))\\mathbb{1}(\\mathcal{B}_{\\operatorname*{min}(H,t)})\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Therefore, we have: ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta\\sim\\pi}\\exp(\\gamma\\left\\langle M_{\\operatorname*{min}(H,T)},\\theta\\right\\rangle-h(T))\\mathbb{1}(\\mathcal{B}_{T})\\le M_{T}^{\\mathsf{e x p}}\\le\\mathbb{E}M_{0}^{\\mathsf{e x p}}=1\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Combining Theorem 13 and Equation (32), we conclude that the following inequality holds with probability at-least $1-\\delta$ when conditioned on $B_{T}$ : ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\rho\\in{\\mathcal{M}}_{1}(\\mathbb{R}^{d})}\\mathbb{E}_{\\theta\\sim\\rho}\\gamma\\left\\langle M_{\\operatorname*{min}(T,H)},\\theta\\right\\rangle-{\\mathsf{K L}}\\left(\\rho\\|\\pi\\right)\\le h(T)+\\log\\bigl(\\frac{1}{\\delta\\mathbb{P}(\\mathcal{B}_{T})}\\bigr)\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "In the RHS of the inequality above, we replace the supremum over $\\mathcal{M}_{1}$ with the supremum over the set of all probability distributions $\\{\\mathcal{N}(\\alpha\\xi,{\\bf I})$ such that $\\xi\\ \\in\\ S^{d-1},\\alpha\\ \\geq\\ 0\\}$ . We note that $\\begin{array}{r}{\\mathsf{K L}\\left(\\mathcal{N}(\\alpha\\xi,\\mathbf{I})\\bar{\\|}\\pi\\right)=\\frac{\\alpha^{2}}{2}}\\end{array}$ to conclude that the following inequality holds with probability at-least $1-\\delta$ when conditioned on $\\scriptstyle{\\dot{\\boldsymbol{B}}}_{T}$ : ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{\\alpha>0}\\gamma\\alpha\\|M_{\\operatorname*{min}(H,T)}\\|-\\frac{\\alpha^{2}}{2}\\leq h(T)+\\log\\bigl(\\frac{1}{\\delta\\mathbb{P}(B_{T})}\\bigr)\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "That is: ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\|M_{\\operatorname*{min}(H,T)}\\|\\leq\\sqrt{\\frac{2h(T)+2\\log(\\frac{1}{\\delta\\mathbb{P}(B_{T})})}{\\gamma^{2}}}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Now, note that by definition, ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{\\displaystyle\\frac{h(T)}{T}=\\frac{1}{T}\\sum_{t=1}^{T}\\log\\left(1+\\frac{\\gamma^{2}}{2}q_{t}\\exp(\\gamma^{2}\\Gamma^{2})+\\gamma^{4}p_{t}g^{2}T\\exp(2\\gamma^{2}\\Gamma g\\sqrt{T})\\right)}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\\\ {\\displaystyle}\\leq\\frac{\\gamma^{2}}{2}\\bar{q}\\exp(\\gamma^{2}\\Gamma^{2})+\\gamma^{4}\\bar{p}g^{2}T\\exp(2\\gamma^{2}\\Gamma g\\sqrt{T})}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "Therefore, whenever: $\\begin{array}{r}{\\gamma\\leq\\operatorname*{min}\\left(\\frac{1}{\\Gamma},\\frac{1}{2\\sqrt{\\Gamma g\\sqrt{T}}}\\right)}\\end{array}$ we note with probability at-least $1-\\delta$ conditioned on the event $B_{T}$ : ", "page_idx": 52}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\|M_{\\operatorname*{min}(H,T)}\\|\\lesssim\\sqrt{T\\bar{q}+\\gamma^{2}\\bar{p}g^{2}T^{2}+\\frac{1}{\\gamma^{2}}\\log\\left(\\frac{1}{\\delta\\mathbb{P}(B_{T})}\\right)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "We therefore state the following theorem: ", "page_idx": 52}, {"type": "text", "text": "Theorem 14. Suppose $\\delta,\\delta_{1}\\in(0,\\frac{1}{2})$ . If $M_{t}$ satisfies $(g,T,\\delta)$ uniform concentration for some $\\delta<{\\frac{1}{2}}$ . Then $M_{t}$ also satisfies $(g^{\\prime},T,\\delta+\\delta_{1})$ concentration, where ", "page_idx": 52}, {"type": "equation", "text": "$$\n(g^{\\prime})^{2}=C\\left[\\bar{q}+\\gamma^{2}\\bar{p}g^{2}T+\\frac{\\log(\\frac{1}{\\delta_{1}})}{\\gamma^{2}T}\\right]\n$$", "text_format": "latex", "page_idx": 52}, {"type": "text", "text": "for any $\\begin{array}{r}{\\gamma\\leq\\operatorname*{min}\\left(\\frac{1}{\\Gamma},\\frac{1}{2\\sqrt{\\Gamma g\\sqrt{T}}}\\right)}\\end{array}$ ", "page_idx": 52}, {"type": "text", "text": "Additionally, suppose $g\\geq c_{0}{\\frac{\\Gamma}{\\sqrt{T}}}$ for some fixed constant $c_{0}>0$ , then we have for some constant $C_{\\mathsf{i t e r}}(c_{0})$ : ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r}{(g^{\\prime})^{2}=C_{\\mathsf{i t e r}}(c_{0})[\\bar{q}+g\\left(\\frac{\\bar{p}\\sqrt{T}}{\\Gamma}+\\frac{\\Gamma}{\\sqrt{T}}\\log(\\frac{1}{\\delta_{1}})\\right)]}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Proof. Since $\\delta\\ \\leq\\ \\textstyle{\\frac{1}{2}}$ , we conclude that $\\mathbb{P}(B_{T})~\\ge~{\\frac{1}{2}}$ . Given that $M_{t}$ satisfies $(g,T,\\delta)$ uniform concentration. We conclude from the discussion above that for some universal constant $C$ and any $\\begin{array}{r}{\\gamma\\leq\\operatorname*{min}\\left(\\frac{1}{\\Gamma},\\frac{1}{2\\sqrt{\\Gamma g\\sqrt{T}}}\\right)}\\end{array}$ , we have: ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{H}\\mathbb{P}(\\|M_{\\operatorname*{min}(H,T)}\\|^{2}\\geq C[T\\bar{q}+\\gamma^{2}\\bar{p}g^{2}T^{2}+\\frac{1}{\\gamma^{2}}\\log\\Big(\\frac{1}{\\delta_{1}}\\Big)]\\|B_{T})\\leq\\delta_{1}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Picking $H$ to be the stopping time given by $H\\,=\\,\\operatorname*{inf}\\{t\\,\\geq\\,0\\,:\\,\\|M_{t}\\|^{2}\\,\\geq\\,C[T\\bar{q}+\\gamma^{2}\\bar{p}g^{2}T^{2}\\,+}$ $\\begin{array}{r}{\\frac{1}{\\gamma^{2}}\\log\\left(\\frac{1}{\\delta_{1}}\\right)]\\}}\\end{array}$ where $C$ is the same constant as in the equation above, we conclude: ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\mathbb{P}_{t\\leq T}^{(\\operatorname*{sup}}\\|M_{t}\\|^{2}\\geq C[T\\bar{q}+\\gamma^{2}\\bar{p}g^{2}T^{2}+\\frac{1}{\\gamma^{2}}\\log\\left(\\frac{1}{\\delta_{1}}\\right)]\\|\\mathcal{B}_{T})\\leq\\delta_{1}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Only in this proof, call the event $\\begin{array}{r}{\\mathcal{G}:=\\{\\operatorname*{sup}_{t\\leq T}\\|M_{t}\\|^{2}\\geq C[T\\bar{q}+\\gamma^{2}\\bar{p}g^{2}T^{2}+\\frac{1}{\\gamma^{2}}\\log\\left(\\frac{1}{\\delta_{1}}\\right)]\\}}\\end{array}$ . We have: ", "page_idx": 53}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{P}(\\mathcal{G})=\\mathbb{P}(\\mathcal{G}\\cap\\mathcal{B}_{T})+\\mathbb{P}(\\mathcal{G}\\cap\\mathcal{B}_{T}^{\\complement})\\le\\mathbb{P}(\\mathcal{G}|\\mathcal{B}_{T})+\\mathbb{P}(\\mathcal{B}_{T}^{\\complement})\\le\\delta_{1}+\\delta}\\end{array}\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "Whenever $g\\geq c_{0}{\\frac{\\Gamma}{\\sqrt{T}}}$ , we can pick $\\begin{array}{r}{\\lambda=\\frac{c_{1}(c_{0})}{\\sqrt{\\Gamma g\\sqrt{T}}}}\\end{array}$ and conclude the result. ", "page_idx": 53}, {"type": "text", "text": "We now state consider Lemma 11 from [2]. ", "page_idx": 53}, {"type": "text", "text": "Lemma 18. Suppose $\\alpha,\\beta\\leq0$ with $\\alpha+\\beta>0$ . Consider the function $f:\\mathbb{R}^{+}\\rightarrow\\mathbb{R}^{+}$ given by $\\begin{array}{r}{u^{\\ast}:=\\left(\\frac{\\beta+\\sqrt{\\beta^{2}+4\\alpha}}{2}\\right)^{2}}\\end{array}$ 2 $f(u)=\\alpha+\\beta{\\sqrt{u}}.$ . Then, $f$ has the unique fixed point: . For $t\\in\\mathbb{N},$ , denoting $f^{(t)}$ to be the t fold composition of $f$ with itself, we have for any $u\\in\\mathbb{R}^{+}$ : ", "page_idx": 53}, {"type": "equation", "text": "$$\n|f^{(t)}(u)-u^{*}|\\leq\\beta^{(2-\\frac{1}{2^{t-1}})}|u-u^{*}|^{\\frac{1}{2^{t}}}\\,.\n$$", "text_format": "latex", "page_idx": 53}, {"type": "text", "text": "We are now ready to prove the main theorem 9 ", "page_idx": 53}, {"type": "text", "text": "Proof of Theorem 9. It is sufficient to show that there exists $K=\\log\\Theta(\\log(\\Gamma T d\\log(\\textstyle{\\frac{1}{\\delta}}))))$ ) such that $M_{t}$ obeys $(g,T,\\delta)$ uniform concentration where $\\begin{array}{r}{g=C\\operatorname*{max}(\\frac{\\Gamma}{\\sqrt{T}},\\bar{q}+\\frac{\\bar{p}\\sqrt{T}}{\\Gamma}+\\frac{\\Gamma}{\\sqrt{T}}\\log(\\frac{K}{\\delta}))}\\end{array}$ ", "page_idx": 53}, {"type": "text", "text": "Let $K\\in\\mathbb N$ be any fixed integer. By Theorem 6, we conclude that the martingale $M_{t}$ is $\\begin{array}{r}{(g_{0}\\big(\\frac{\\delta}{K}\\big),T,\\frac{\\delta}{K}\\big)}\\end{array}$ uniformly concentrated. Fix some $c_{0}>0$ and $C_{\\mathsf{i t e r}}(c_{0})$ be as in Theorem 14. ", "page_idx": 53}, {"type": "text", "text": "Define the sequence $\\begin{array}{r}{g_{i}:=\\sqrt{C_{\\mathrm{iter}}(c_{0})\\bar{q}}+\\sqrt{C_{\\mathrm{iter}}(c_{0})g_{i-1}G}\\mathrm{~where~}G=\\frac{\\bar{p}\\sqrt{T}}{\\Gamma}+\\frac{\\Gamma}{\\sqrt{T}}\\log(\\frac{K}{\\delta}).}\\end{array}$ ", "page_idx": 53}, {"type": "text", "text": "If $g_{0}\\ \\leq\\ c_{0}{\\frac{\\Gamma}{\\sqrt{T}}}$ , then the statement of the theorem follows. Suppose there exists $K_{1}\\,\\leq\\,K\\,-\\,1$ such that $\\begin{array}{r}{g_{K_{1}}\\,\\leq\\,\\frac{c_{0}\\Gamma}{\\sqrt{T}}}\\end{array}$ and suppose that it is the first such integer. If $K_{1}=0$ , the statement of the theorem follows from $\\begin{array}{r}{(g_{0}\\big(\\frac{\\delta}{K}\\big),T,\\frac{\\delta}{K}\\big)}\\end{array}$ uniform concentration of $M_{t}$ . Suppos\u221ae $1\\leq K_{1}\\leq K-1$ . Then, $\\begin{array}{r}{\\operatorname*{min}(g_{0},\\dots,g_{K_{1}-1})\\,\\ge\\,c_{0}\\frac{\\Gamma}{\\sqrt{T}}}\\end{array}$ . Then, by Theorem 14, the fact that ${\\sqrt{x+y}}\\,\\leq\\,{\\sqrt{x}}\\,+\\,{\\sqrt{y}}$ and induction, we conclude that $M_{t}$ obeys $(g_{i},T,{\\frac{(i+1)\\delta}{K}})$ for every $i\\leq K_{1}$ . Thus we conclude the statement of the theorem. ", "page_idx": 53}, {"type": "text", "text": "Suppose such a $K_{1}$ does not exist. Then, $\\begin{array}{r}{\\operatorname*{min}(g_{0},\\dots,g_{K-1})\\ge c_{0}\\frac{\\Gamma}{\\sqrt{T}}}\\end{array}$ . Then, by Theorem 14, the fact that ${\\sqrt{x+y}}\\leq{\\sqrt{x}}+{\\sqrt{y}}$ and induction, we conclude that $M_{t}$ obeys $(g_{i},T,{\\frac{(i\\!+\\!1)\\delta}{K}})$ for every $i\\leq K-1$ . Therefore, it obeys $(g_{K},T,\\delta)$ uniform concentration. ", "page_idx": 54}, {"type": "text", "text": "Consider the function $f$ in Lemma 18 with $\\alpha\\,=\\,\\sqrt{C_{\\mathsf{i t e r}}(c_{0})\\bar{q}}$ and $\\beta\\,=\\,\\sqrt{C_{\\mathsf{i t e r}}(c_{0})G}$ and\u221a let the corresponding fixed point be denoted by $g^{*}$ . It is easy to show that the fixed point $g^{*}\\lesssim\\sqrt{\\bar{q}}+G$ . After $K$ iterations, we must have: ", "page_idx": 54}, {"type": "equation", "text": "$$\ng_{K}\\leq g^{*}+(C_{\\mathrm{iter}}(c_{0})G^{1-\\frac{1}{2^{K}}})|g_{0}-g^{*}|^{\\frac{1}{2^{K}}}\\lesssim g^{*}+(G^{1-\\frac{1}{2^{K}}})|g_{0}|^{\\frac{1}{2^{K}}}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "We can show that picking $\\begin{array}{r}{K=\\log\\Theta(\\log((1+\\frac{\\sqrt{\\bar{q}T}}{\\Gamma})\\log d))}\\end{array}$ ), and the bound on $\\Gamma$ , we conclude the result. \u53e3 ", "page_idx": 54}, {"type": "text", "text": "F.2 Proof of Theorem 10 ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Proof of Theorem $I O$ . Recall that $\\Sigma_{t}\\ :=\\ \\mathbb{E}[\\mathbf{v}_{t}\\mathbf{v}_{t}^{\\mathsf{T}}|\\mathcal{F}_{t-1}]$ , $\\begin{array}{r c l}{\\nu_{t}}&{:=}&{\\|\\Sigma_{t}\\|}\\end{array}$ and $N_{t}~:=~\\|M_{t}\\|^{2}~-$ $\\textstyle\\sum_{s=1}^{t}\\|\\mathbf{v}_{t}\\|^{2}$ . Note that $\\nu_{t}\\leq p_{t}$ and ${\\sf T r}(\\Sigma_{t})\\leq p_{t}$ almost surely. ", "page_idx": 54}, {"type": "text", "text": "Let $\\gamma\\in\\mathbb{R}$ . Define $\\begin{array}{r}{h_{N}(t):=\\sum_{s=1}^{t}\\log\\Big(1+4\\gamma^{2}p_{s}g^{2}T\\exp(2|\\gamma|\\Gamma g\\sqrt{T})\\Big)}\\end{array}$ with empty sum denoting 0. We first show that $N_{t}^{\\mathsf{e x p}}=\\exp(\\gamma N_{t}-h_{N}(t))\\mathbb{1}(\\mathcal{B}_{T})$ is a super martingale with respect to the filtration $\\mathcal{F}_{t}$ for $0\\leq t\\leq T$ . For $T\\geq t>1$ , we have: ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}[\\exp(\\gamma N_{t})1(B_{t})|\\mathcal{F}_{t-1}]=\\exp(\\gamma\\mathcal{N}_{t-1})1(B_{t-1})\\mathbb{E}[\\exp(2\\gamma\\langle\\mathbf{v},M_{t-1}\\rangle)1(B_{t})|\\mathcal{F}_{t-1}]}\\\\ &{\\leq\\exp(\\gamma N_{t-1})1(B_{t-1})\\mathbb{E}[\\displaystyle\\sum_{k=0}^{\\infty}\\frac{1}{k!}2^{k_{y}\\cdot k_{\\cdot}}\\mathrm{t}\\langle\\mathbf{v},M_{t-1}\\rangle^{k_{1}}\\{B_{t-1}\\}|\\mathcal{F}_{t-1}]}\\\\ &{=\\exp(\\gamma N_{t-1})1(B_{t-1})\\mathbb{E}[1(B_{t-1})+\\displaystyle\\sum_{k=0}^{\\infty}\\frac{1}{k!}2^{k_{y}\\cdot k_{\\cdot}}(\\mathbf{v}_{t},M_{t-1})^{k_{1}}\\{B_{t-1}\\}|\\mathcal{F}_{t-1}]}\\\\ &{\\leq\\exp(\\gamma N_{t-1})1(B_{t-1})\\mathbb{E}[1+\\displaystyle\\sum_{k=0}^{\\infty}\\frac{1}{k!}2^{k_{y}\\cdot|\\mathbf{v}|\\cdot|\\mathbf{v}|\\cdot|\\mathbf{v}|\\cdot\\mathbf{u}_{t-1}\\rangle^{2}}\\mathbb{E}^{-2}]|M_{t-1}|\\mathbb{E}^{-2}\\mathbb{1}(B_{t-1})|\\mathcal{F}_{t-1}]}\\\\ &{\\leq\\exp(\\gamma N_{t-1})1(B_{t-1})\\mathbb{E}[1+4\\gamma^{2}\\langle\\mathbf{v},M_{t-1}\\rangle^{2}\\exp(2\\gamma|\\boldsymbol{\\mathbb{P}}|\\boldsymbol{\\mathbb{H}}_{t-1}|)]\\{B_{t-1}\\}|\\mathcal{F}_{t-1}]}\\\\ &{\\leq\\exp(\\gamma N_{t-1})1(B_{t-1})\\mathbb{E}[1+4\\gamma^{2}\\nu_{t}|\\mathbf{M}_{t-1}|^{2}\\exp(2\\gamma|\\boldsymbol{\\mathbb{P}}|\\boldsymbol{\\mathrm{H}}_{t-1}|)]\\{B_{t-1}\\}|}\\\\ &{\\leq\\exp(\\gamma N_{t-1})1(B_{t-1})\\left(1+4\\gamma^{2}\\nu_{t}|\\mathbf{\\epsilon}_{t}|M_{t-1}|^{2}\\exp(2\\gamma\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "This shows that $N_{t}^{\\mathsf{e x p}}$ is a super-martingale. Using the fact that $N_{1}^{\\mathsf{e x p}}=1$ almost surely, the optional stopping theorem and the Chernoff bound, we conclude that for any stopping time $H$ , we have for any $\\alpha,\\gamma>0$ ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{P}\\big(\\{N_{\\operatorname*{min}(T,H)}>\\alpha\\}\\cap B_{T}\\big)\\leq\\mathbb{E}\\big[\\exp(\\gamma N_{\\operatorname*{min}(T,H)}-\\gamma\\alpha)\\mathbb{1}(B_{T})\\big]}&{}\\\\ {\\leq\\mathbb{E}\\big[\\exp(\\gamma N_{\\operatorname*{min}(T,H)}-\\gamma\\alpha)\\mathbb{1}(B_{\\operatorname*{min}(T,H)})\\big]}&{}\\\\ {\\leq\\mathbb{E}\\big[N_{\\operatorname*{min}(T,H)}^{\\mathrm{exp}}\\big]\\exp(h_{N}(T)-\\gamma\\alpha)}&{}\\\\ {\\leq\\exp(h_{N}(T)-\\gamma\\alpha)}&{}\\end{array}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Taking \u03b3 = $\\begin{array}{r}{\\gamma=\\frac{1}{2\\Gamma g\\sqrt{T}}}\\end{array}$ allows us to conclude: ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\mathbb{P}\\big(\\{N_{\\mathrm{min}(T,H)}>\\Gamma C g\\sqrt{T}\\log(\\frac{2}{\\delta})+\\frac{C\\nu g T^{3/2}}{\\Gamma}\\}\\cap B_{T}\\big)\\leq\\frac{\\delta}{2}\n$$", "text_format": "latex", "page_idx": 54}, {"type": "text", "text": "Let \u03b1 = \u0393Cg T log( \u03b42 ) + C\u03bdg\u0393T 3/2 and take $H$ to be the stopping time $\\operatorname*{min}(\\operatorname*{inf}_{t}\\{t>0:N_{t}>$ $\\alpha\\},T)$ where infimum of an empty set is taken to be infinity. We note that $\\{\\operatorname*{sup}_{t\\leq T}N_{t}>\\alpha\\}=$ $\\{N_{\\operatorname*{min}(T,H)}>\\alpha\\}$ . We thus conclude: ", "page_idx": 54}, {"type": "equation", "text": "$$\n\\mathbb{P}(\\{\\underset{t\\leq T}{\\operatorname*{sup}}N_{t}>\\Gamma C g\\sqrt{T}\\log(\\frac{2}{\\delta})+\\frac{C\\nu g T^{3/2}}{\\Gamma}\\}\\cap B_{T})\\leq\\frac{\\delta}{2}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Taking $\\gamma$ negative gives the analogous proof for $N_{t}<-\\alpha$ . ", "page_idx": 55}, {"type": "text", "text": "F.3 Proof of Corollary 5 ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Proof. Consider the set $S=\\{\\mathsf{U P}(t):0\\leq t\\leq T\\}$ . The, $|S|\\le\\log_{2}(T)+1$ . By Corollary 6, we have for any $t_{0}\\in S$ , the following is true with probability $\\begin{array}{r}{1-\\frac{\\delta}{1+\\log_{2}(T)}}\\end{array}$ ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t_{0}}\\|\\mathbf{v}_{s}\\|^{2}\\leq t_{0}g^{2}(t_{0},\\frac{\\delta}{3(1+\\log_{2}(T))})\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Therefore, by union bound of the above event over every $t_{0}\\in S$ , we have with probability $1-\\delta$ : ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{t_{0}\\in S}\\sum_{s=1}^{t_{0}}\\|\\mathbf{v}_{s}\\|^{2}\\leq t_{0}g^{2}(t_{0},\\frac{\\delta}{3(1+\\log_{2}(T))})\\leq0\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Now, note that $\\begin{array}{r}{\\sum_{s=1}^{t}\\|\\mathbf{v}_{s}\\|^{2}\\,\\leq\\,\\sum_{s=1}^{\\mathsf{U P}(t)}\\|\\mathbf{v}_{s}\\|^{2}\\,}\\end{array}$ almost surely for every $t\\,\\in\\,[T]$ since $t\\,\\leq\\,\\mathsf{U P}(t)$ Therefore, we conclude that with probability at-least , the following holds for all $t\\,\\in\\,[T]$ simultaneously: ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\sum_{s=1}^{t}\\|\\mathbf{v}_{s}\\|^{2}\\leq g^{2}\\left(\\mathsf{U P}(t),\\frac{\\delta}{3(1+\\log_{2}(T))}\\right)\\mathsf{U P}(t)\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Using the definition of $g(,)$ from Theorem 9, we conclude the result. ", "page_idx": 55}, {"type": "text", "text": "G Applications to Streaming Heavy Tailed Statistical Estimation ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "G.1 Streaming Heavy Tailed Mean Estimation : Proof of Corollary 1 ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Proof. Recall that for this problem, $\\Xi={\\mathcal{C}}$ , $\\mathbb{E}_{\\xi\\sim P}[\\xi]\\,=\\,\\mathbf{m}\\,\\in\\,\\mathcal{C}$ and ${\\sf C o v}[\\xi]\\mathrm{~\\preceq~}\\Sigma$ . Consider the following quadratic loss function $f:{\\mathcal{C}}\\to\\mathbb{R}$ : ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\begin{array}{r}{f({\\mathbf{x}};\\boldsymbol{\\xi})=\\frac{1}{2}\\|{\\mathbf{x}}-\\boldsymbol{\\xi}\\|^{2},\\qquad\\boldsymbol{\\xi}\\sim P}\\end{array}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "The associated population risk function $F$ is given by ", "page_idx": 55}, {"type": "equation", "text": "$$\nF(\\mathbf{x})={\\frac{1}{2}}\\cdot\\mathbb{E}_{\\xi\\sim P}\\left[\\|\\mathbf{x}-\\xi\\|^{2}\\right]=F(\\mathbf{x})={\\frac{1}{2}}\\|\\mathbf{x}-\\mathbf{m}\\|^{2}+\\mathsf{T r}(\\mathsf{C o v}_{\\xi\\sim P}[\\xi])\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Note that $F$ is $L$ -smooth and $\\mu$ -strongly convex with $L=\\mu=1$ . Thus, $\\kappa=1$ . Furthermore, m is the unique minimizer of $F$ . Hence, solving the streaming heavy tailed mean estimation problem is equivalent to solving the SCO problem for $F$ . To this end, we consider the following stochastic gradient oracle: ", "page_idx": 55}, {"type": "equation", "text": "$$\ng(\\mathbf{x};\\boldsymbol{\\xi})=\\mathbf{x}-\\boldsymbol{\\xi}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "It is easy to see that $\\mathbb{E}_{\\mathbf{y}}[g(\\mathbf{x};\\boldsymbol{\\xi})]=\\nabla F(\\mathbf{x})$ , i.e., the stochastic gradient estimate is unbiased. The associated stochastic gradient noise ${\\bf n}({\\bf x};\\xi)$ is given by ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\mathbf{n}(\\mathbf{x};\\boldsymbol{\\xi})=\\nabla F(\\mathbf{x})-\\nabla f_{\\mathbf{y}}(\\mathbf{x})=\\mathbf{y}-\\mathbf{m}\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "We now note that ", "page_idx": 55}, {"type": "equation", "text": "$$\n\\Sigma(\\mathbf{x})=\\mathbb{E}[\\mathbf{n}(\\mathbf{x};\\boldsymbol{\\xi})\\mathbf{n}(\\mathbf{x};\\boldsymbol{\\xi})^{T}]=\\mathbb{E}[(\\mathbf{y}-\\mathbf{m})(\\mathbf{y}-\\mathbf{m})^{T}]=\\mathsf{T r}(\\mathsf{C o v}_{\\boldsymbol{\\xi}\\sim P}[\\boldsymbol{\\xi}])\\preceq\\Sigma\n$$", "text_format": "latex", "page_idx": 55}, {"type": "text", "text": "Hence, we note that the Bdd. $2^{\\mathsf{n d}}$ Moment assumption is satisfied. Hence, the result follows by an application of Theorem 1 \u53e3 ", "page_idx": 55}, {"type": "text", "text": "G.2 Streaming Heavy Tailed Linear Regression : Proof of Corollary 2 ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "We use $\\theta\\in{\\mathcal{C}}$ to denote the parameter of $F$ . Recall from Section 5.2 that $\\Xi=\\mathbb{R}^{d}\\times\\mathbb{R}$ , and given a target parameter $\\theta^{*}\\in\\mathcal{C}$ , $P$ defines the following linear model: ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{x}\\sim Q,\\,\\mathbb{E}[\\mathbf{x}]=0,\\,\\mathbb{E}[\\mathbf{xx}^{T}]=\\Sigma\\succ0;\\qquad y=\\langle\\mathbf{x},{\\boldsymbol{\\theta}}^{*}\\rangle+\\epsilon,\\,\\mathbb{E}[\\epsilon|\\mathbf{x}]=0,\\,\\mathbb{E}[\\epsilon^{2}|\\mathbf{x}]\\leq\\sigma^{2}}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "In addition, we make the following bounded $4^{\\mathrm{th}}$ moment asumption on the covariates x ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\mathbb{E}[\\langle\\mathbf{x},\\mathbf{v}\\rangle^{4}]\\leq C_{4}(\\mathbb{E}[\\langle\\mathbf{x},\\mathbf{v}\\rangle^{2}])^{2}\\qquad\\forall\\,\\mathbf{v}\\in\\mathbb{R}^{d}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "for some numerical constant $C_{4}\\geq1$ . Recall that the sample loss function is given by: ", "page_idx": 56}, {"type": "equation", "text": "$$\nf(\\theta;\\mathbf x,\\mathbf y)=\\frac12\\left(\\langle\\theta,\\mathbf x\\rangle-\\mathbf y\\right)^{2}=\\frac12\\left(\\langle\\theta-\\theta^{*},\\mathbf x\\rangle-\\epsilon\\right)^{2}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Using the fact that $\\mathbb{E}[\\epsilon|{\\bf x}]=0,\\mathbb{E}[{\\bf x}]=0$ and $\\mathbb{E}[{\\bf x}{\\bf x}^{T}]=\\boldsymbol{\\Sigma}$ ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle F(\\theta)=\\frac{1}{2}(\\theta-\\theta^{*})^{T}\\mathbb{E}[{\\bf x x}^{T}](\\theta-\\theta^{*})+\\mathbb{E}[\\epsilon^{2}]}}\\\\ {~~~~~~~=\\frac{1}{2}(\\theta-\\theta^{*})^{T}\\Sigma(\\theta-\\theta^{*})+\\mathbb{E}[\\epsilon^{2}]}}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "We note that $\\mathbb{E}[\\epsilon^{2}]\\leq\\sigma^{2}$ as per our assumption hence $F$ is well defined. Furthermore. ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\nabla F({\\boldsymbol{\\theta}})=\\Sigma({\\boldsymbol{\\theta}}-{\\boldsymbol{\\theta}}^{*})}\\\\ {\\nabla^{2}F({\\boldsymbol{\\theta}})=\\Sigma}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "Thus, the population risk $F$ is $L$ -smooth and $\\mu$ -strongly convex with $L=\\|\\Sigma\\|_{2}$ and $\\mu=\\lambda_{\\operatorname*{min}}(\\Sigma)$ , i.e., \u03ba = $\\begin{array}{r}{\\kappa=\\frac{\\|\\Sigma\\|_{2}}{\\lambda_{\\operatorname*{min}}(\\Sigma)}}\\end{array}$ . Furthermore, the unique minimizer of $F$ is $\\theta^{*}$ . Hence, $\\begin{array}{r}{\\kappa=\\frac{\\|\\Sigma\\|_{2}}{\\lambda_{\\operatorname*{min}}(\\Sigma)}}\\end{array}$ the linear regression task of estimating $\\theta^{*}$ is equivalent to solving SCO for the above objective. ", "page_idx": 56}, {"type": "text", "text": "The associated stochastic gradient oracle $g(\\theta;\\mathbf x,\\mathbf y)$ at any $\\theta\\in{\\mathcal{C}}$ is given by: ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{g(\\theta;\\mathbf{x},\\mathbf{y})=\\nabla f(\\theta;\\mathbf{x},\\mathbf{y})=\\mathbf{x}(\\langle\\theta,\\mathbf{x}\\rangle-\\mathbf{y})=\\mathbf{x}\\,(\\langle\\theta-\\theta^{*},\\mathbf{x}\\rangle-\\epsilon)}\\\\ &{\\quad\\quad\\quad\\quad=\\mathbf{x}\\mathbf{x}^{T}(\\theta-\\theta^{*})-\\mathbf{x}\\epsilon}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "We first show that $g(\\theta;\\mathbf{x},\\mathbf{y})]$ ) is indeed an unbiased estimate of $\\nabla F(\\theta)$ ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}[g(\\theta;\\mathbf{x},\\mathbf{y})]=\\mathbb{E}[\\mathbf{x}\\mathbf{x}^{T}](\\theta-\\theta^{*})-\\mathbb{E}[\\mathbf{x}\\mathbb{E}[\\epsilon|\\mathbf{x}]]=\\Sigma(\\theta-\\theta^{*})=\\nabla F(\\theta)}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "The associated stochastic gradient noise $\\mathbf{n}(\\theta;\\mathbf{x},\\mathbf{y})(\\theta)$ is given by ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\mathbf{n}(\\theta;\\mathbf{x},\\mathbf{y})(\\theta)=g(\\theta;\\mathbf{x},\\mathbf{y})(\\theta)-\\nabla F(\\mathbf{x})}}\\\\ &{}&{=\\left(\\mathbf{x}\\mathbf{x}^{T}-\\Sigma\\right)(\\theta-\\theta^{*})-\\mathbf{x}e}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "$\\Sigma(\\theta)=\\mathbb{E}[{\\bf n}(\\theta;{\\bf x},{\\bf y}){\\bf n}(\\theta;{\\bf x},{\\bf y})]$ . For convenience, we use $\\mathbf{M}=\\mathbf{x}\\mathbf{x}^{T}-\\boldsymbol{\\Sigma}$ and ${\\bf d}_{\\theta}=\\theta-\\theta^{*}$ and note that $\\mathbf{M}$ is symmetric. It follows that: ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Sigma(\\theta)=\\mathbb{E}\\left[(\\mathbf{M}\\mathbf{d}_{\\theta}-\\mathbf{x}\\epsilon)\\left(\\mathbf{M}\\mathbf{d}_{\\theta}-\\mathbf{x}\\epsilon\\right)^{T}\\right]}\\\\ &{\\phantom{\\Sigma}=\\mathbb{E}\\left[\\mathbf{M}\\mathbf{d}_{\\theta}\\mathbf{d}_{\\theta}^{T}\\mathbf{M}\\right]+\\mathbb{E}\\left[\\mathbf{x}\\mathbf{x}^{T}\\cdot\\mathbb{E}\\left[\\epsilon^{2}|\\mathbf{x}\\right]\\right]-\\mathbb{E}[\\mathbf{x}\\mathbf{d}_{\\theta}^{T}\\mathbf{M}\\cdot\\mathbb{E}[\\epsilon|\\mathbf{x}]]-\\mathbb{E}[\\mathbf{M}\\mathbf{d}_{\\theta}\\mathbf{x}^{T}\\cdot\\mathbb{E}[\\epsilon|\\mathbf{x}]]}\\\\ &{\\phantom{\\Sigma}\\preceq\\mathbb{E}\\left[\\mathbf{M}\\mathbf{d}_{\\theta}\\mathbf{d}_{\\theta}^{T}\\mathbf{M}\\right]+\\sigma^{2}\\Sigma}\\end{array}\n$$", "text_format": "latex", "page_idx": 56}, {"type": "text", "text": "where we use the fact that $\\mathbb{E}[\\epsilon|{\\bf x}]=0,\\mathbb{E}[\\epsilon^{2}|{\\bf x}]\\le\\sigma^{2}$ and $\\mathbb{E}[\\mathbf{x}\\mathbf{x}^{T}]=\\Sigma$ . ", "page_idx": 56}, {"type": "text", "text": "We shall now upper bound $\\|\\Sigma(\\theta)\\|_{2}$ . To do so, we define $\\mathbf A(\\theta)=\\mathbb E\\left[\\mathbf M\\mathbf d_{\\theta}\\mathbf d_{\\theta}^{T}\\mathbf M\\right]$ and note that $\\mathbf{A}(\\theta)$ is a PSD matrix since for any $\\mathbf{v}\\in\\mathbb{R}^{d},\\mathbf{v}^{T}\\mathbf{A}(\\theta)\\mathbf{v}=\\mathbb{E}\\left[(\\mathbf{v}^{T}\\mathbf{M}\\mathbf{d}_{\\theta})^{2}\\right]^{*}\\geq0$ . Without loss of generality, ", "page_idx": 56}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\log\\left(\\frac{\\eta}{b}\\right)=}&{\\log\\left(\\lambda\\right)=}\\\\ &{=\\left\\lceil{\\operatorname*{sup}\\left[\\operatorname*{sup}\\left\\{\\sum_{j=0}^{K}\\log\\left(\\log\\lambda\\right)\\right\\}^{2}\\right]}\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.=\\left\\lceil{\\operatorname*{sup}\\left[\\operatorname*{sup}\\left\\{\\left(\\log\\lambda\\right),\\operatorname*{Im}\\left\\{\\log\\left(\\log\\lambda\\right)\\right\\}\\right\\}\\right]}\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.\\leq\\left\\lceil{\\operatorname*{sup}\\left[\\operatorname*{sup}\\left\\{\\log\\lambda\\right\\}\\right]-\\operatorname*{sup}\\left\\{\\left(\\log\\Lambda\\right)\\log^{2}\\right\\}\\right\\}}\\\\ &{\\qquad\\qquad\\qquad\\left.=\\left\\lceil{\\operatorname*{sup}\\left[\\operatorname*{sup}\\left\\{\\log\\left(\\log\\lambda\\right)\\right\\}\\right]-\\operatorname*{sup}\\left\\{\\log\\left(\\log\\lambda\\right)\\right\\}^{2}\\right\\}}\\\\ &{\\qquad\\qquad\\qquad\\left.=\\left\\lceil{\\operatorname*{sup}\\left[\\operatorname*{sup}\\left\\{\\log\\lambda\\right\\}\\right]-\\operatorname*{sup}\\left\\{\\left(\\log\\Lambda\\right)\\cdot\\left(\\log\\Lambda\\right)\\cdot\\left(\\log\\left[\\log\\lambda\\right]\\right)\\right\\}^{2}\\right\\}}\\\\ &{\\qquad\\qquad\\qquad\\left.\\leq\\left\\lceil{\\operatorname*{sup}\\left[\\operatorname*{sup}\\left\\{\\log\\lambda\\right\\}\\right]-\\operatorname*{sup}\\left\\{\\log\\left(\\log\\lambda\\right)\\cdot\\left(\\log\\Lambda\\right)^{2}\\right\\}^{2}+\\mathscr{Z}}\\right\\rceil\\left\\langle{\\operatorname*{sup}\\left[\\operatorname*{sup}\\left\\{\\log\\lambda\\right\\}^{2}\\right]-\\operatorname*{sup}\\left\\{\\log\\left(\\log\\lambda\\right)\\right\\}^{2}}\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\operatorname*{lid}\\left\\{\\operatorname*{sup}\\left\\{\\left(\\sum_{j=1}^{K}\\log\\lambda\\right)\\log\\left\\}\\sqrt{\\left(\\log\\left[\\log\\lambda\\right]\\right)}\\right\\}\\left\\langle{\\operatorname*{sup}\\left\\{\\log\\left(\\log\\lambda\\right)\\right\\}}\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\operatorname*{lid}\\left\\{\\operatorname*{sup}\\left\\{\\left(\\log\\lambda\\right)^{2}+C_{1}\\log\\log\\left(\\log\\lambda\\right)\\log^{2}\\right\\}\\right\\}\\left\\{\\log\\left(\\log\\lambda\\right)^{2}\\right\\}\\left\\langle{\\operatorname*{sup}\\left\\{\\log\\left(\\log\\lambda\\right)\\right\\}}\\right\\rangle}\\\\ &{\\qquad\\qquad\\qquad\\leq2\\operatorname*{lid}\\left\\{\\operatorname*{sup}\\left\\{\\left(\\log\\lambda\\right)\\left(\\log\\Lambda\\right)\\log^{2}\\right\\}\\left(\\log\\left\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "where we use the fourth moment assumption on the covariates in the eighth step. Note that the above bound also holds when $\\theta=\\theta^{*}$ since in that case $\\mathbf{A}(\\theta)=0$ and $\\mathbf{d}_{\\theta}=0$ . It follows that ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\Sigma(\\theta)\\|\\leq\\|A(\\theta)\\|+\\sigma^{2}\\|\\Sigma\\|}\\\\ &{\\qquad\\qquad\\leq2(C_{4}+1)\\|\\Sigma\\|^{2}\\|\\theta-\\theta^{*}\\|^{2}+\\sigma^{2}\\|\\Sigma\\|}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "We shall now derive an upper bound for ${\\mathsf{T r}}(\\Sigma(\\theta))$ as follows: ", "page_idx": 57}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{T r}(\\Sigma(\\theta))=\\mathbb{E}[\\|\\mathbf{n}(\\theta;\\mathbf{x},\\mathbf{y})\\|^{2}]}\\\\ &{\\quad\\quad\\quad\\quad=\\mathbb{E}\\left[\\|\\mathbf{M}\\mathbf{d}_{\\theta}-\\mathbf{x}\\epsilon\\|^{2}\\right]}\\\\ &{\\quad\\quad\\quad\\quad=\\mathbb{E}[\\|\\mathbf{M}\\mathbf{d}_{\\theta}\\|^{2}]-2\\mathbb{E}[\\langle\\mathbf{M}\\mathbf{d}_{\\theta},\\mathbf{x}\\rangle\\,\\mathbb{E}[\\epsilon|\\mathbf{x}]]+\\mathbb{E}[\\|\\mathbf{x}\\|^{2}\\mathbb{E}[\\epsilon^{2}|\\mathbf{x}]]}\\\\ &{\\quad\\quad\\quad\\quad\\leq\\mathbb{E}[\\|\\mathbf{M}\\mathbf{d}_{\\theta}\\|^{2}]+\\sigma^{2}\\mathsf{T r}(\\Sigma)}\\end{array}\n$$", "text_format": "latex", "page_idx": 57}, {"type": "text", "text": "We now control $\\mathbb{E}[\\|\\mathbf{M}\\mathbf{d}_{\\theta}\\|^{2}]$ . Note that $\\mathbb{E}[\\|\\mathbf{M}\\mathbf{d}_{\\theta}\\|^{2}]=0$ if $\\theta=\\theta^{*}$ so we shall now consider the case when $\\theta\\neq\\theta^{*}$ . To this end, let $\\mathbf{e}_{1},\\ldots...,\\mathbf{e}_{d}$ be an orthonormal basis of $\\mathbb{R}^{d}$ such that $\\begin{array}{r}{\\mathbf{e}_{1}=\\frac{\\mathbf{d}_{\\theta}}{\\|\\mathbf{d}_{\\theta}\\|}}\\end{array}$ ", "page_idx": 57}, {"type": "text", "text": "For the remainder of the proof, we use $\\Sigma_{i j}$ to denote $\\Sigma_{i j}=\\mathbf{e}_{i}^{T}\\Sigma\\mathbf{e}_{j}$ where $i,j\\in[d]$ , which implies that $\\textstyle\\mathsf{T r}(\\Sigma)=\\sum_{i=1}^{d}\\Sigma_{i i}$ . We also note that for any two symmetric matrices $\\mathbf{B},\\mathbf{C},(\\mathbf{B}\\!-\\!\\mathbf{C})^{2}\\preceq2\\mathbf{B}^{2}\\!+\\!2\\mathbf{C}^{2}$ . ", "page_idx": 57}, {"type": "text", "text": "Hence, ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\mathbb{E}\\{\\left|\\mathcal{M}_{k}\\right|^{2}\\}=\\mathbb{E}\\left\\{\\left|\\left(\\mathbf{X}_{j}\\right)\\right|^{2}\\}\\\\ &{=\\mathbb{E}\\{\\left|\\left(\\mathbf{X}_{j}\\right)\\right|^{2}\\}}\\\\ &{\\leq2\\mathbb{E}\\{\\left|\\left(\\mathbf{X}_{j}\\right)\\right|^{2}\\}\\mathbb{E}\\left(\\left|X_{j}\\right|^{2}-\\alpha\\left(1\\right)^{2}\\right)\\mathbb{E}\\left|\\right)}\\\\ &{\\leq2\\mathbb{E}\\left\\|\\left(\\left(\\mathbf{X}_{j}^{\\top}\\right)\\right)+\\mathbb{E}\\left\\{\\left|\\left(\\alpha\\left(\\mathbf{X}_{j}\\right)\\right)\\right|^{2}\\right\\}}\\\\ &{\\leq2\\mathbb{E}\\left\\|\\left(\\left(\\mathbf{X}_{j}^{\\top}\\right)\\right)+\\mathbb{E}\\left\\{\\left|\\alpha\\left(\\mathbf{X}_{j}\\right)\\right|^{2}\\right\\}\\mathbb{E}\\left|\\left(\\alpha\\left(\\mathbf{X}_{j}^{\\top}\\right)\\right)}\\\\ &{\\leq2\\mathbb{E}\\left\\|\\left(\\left(\\mathbf{X}_{j}^{\\top}\\right)\\right)\\mathbb{E}\\left(\\left|X_{j}\\right|^{2}+\\mathbb{E}\\left[\\left|\\alpha\\left(\\mathbf{X}_{j}\\right)\\right|^{2}\\right]\\right)}\\\\ &{\\leq2\\mathbb{E}\\left\\|\\alpha\\left(\\left(\\mathbf{X}_{j}^{\\top}\\right)\\right)+\\mathbb{E}\\left\\{\\left|\\alpha\\left(\\mathbf{X}_{j}\\right)\\right|^{2}+\\frac{\\alpha^{2}\\zeta^{2}}{2}\\mathbb{E}\\left(\\left|X_{j}\\right)\\right\\}\\right)}\\\\ &{\\leq2\\mathbb{E}\\left\\|\\alpha\\left(\\left(\\mathbf{X}_{j}^{\\top}\\right)\\right)+\\mathbb{E}\\left\\{\\left|\\alpha\\left(\\left(\\mathbf{X}_{j}\\right)\\right)\\right|^{2}\\right\\}\\sum_{k=0}^{T}\\mathbb{E}\\left\\{\\left|\\left(\\alpha\\left(\\mathbf{X}_{j}\\right)\\right)\\right|^{2}\\left[\\alpha\\left(\\mathbf{X}_{j},\\mathbf{X}_{j}^{\\top}\\right)\\right]\\right\\}}\\\\ &{\\leq2\\mathbb{E}\\left\\|\\alpha\\left(\\left(\\mathbf{X}_{j}^{\\top}\\right)+\\mathbb{E}\\left[\\left|\\alpha\\left(\\mathbf{X}_{j}\\right)\\right|^{2}\\right]+C_{k}^{2}\\frac{\\alpha^{2}\\zeta^{2}}{2}\\mathbb{E}\\left[\\left|\\alpha\\left(\\mathbf{X}_{j},\\mathbf{X}_{j}^{\\top \n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Clearly, the above bound holds even when $\\theta=\\theta^{*}$ . Hence, we infer that ", "page_idx": 58}, {"type": "equation", "text": "$$\n{\\mathsf{T r}}(\\Sigma(\\theta))\\leq2(C_{4}+1)\\|\\Sigma\\|_{2}{\\mathsf{T r}}(\\Sigma)\\|\\theta-\\theta^{*}\\|^{2}+\\sigma^{2}{\\mathsf{T r}}(\\Sigma)\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "From these bounds, we can conclude the following ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{~~~\\|\\Sigma(\\theta)\\|\\leq2(C_{4}+1)\\|\\Sigma\\|_{2}^{2}\\|\\theta-\\theta^{*}\\|^{2}+\\sigma^{2}\\|\\Sigma\\|}\\\\ &{\\mathsf{T r}(\\Sigma(\\theta))\\leq\\frac{\\mathsf{T r}(\\Sigma)}{\\|\\Sigma\\|_{2}}\\left[2(C_{4}+1)\\|\\Sigma\\|_{2}^{2}\\|\\theta-\\theta^{*}\\|^{2}+\\sigma^{2}\\|\\Sigma\\|\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "Thus, the stochastic gradient oracle satisfies Assumption ${\\mathrm{QG~}}2^{\\mathsf{n d}}$ Moment with $\\alpha=2(C_{4}+1)\\|\\Sigma\\|_{2}^{2}$ , $\\beta=\\sigma^{2}\\lVert\\boldsymbol{\\Sigma}\\rVert$ and $d_{\\mathsf{e f f}}=\\mathsf{T r}(\\Sigma)/\\|\\Sigma\\|$ . Hence, the result follows by an application of Theorem 2 ", "page_idx": 58}, {"type": "text", "text": "G.3 Heavy Tailed Streaming Logistic Regression : Proof of Corollary 3 ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Recall from Section 5.4 that $\\Xi=\\mathbb{R}^{d}\\times\\{0,1\\}$ and $P$ denotes the following linear-logistic model: ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\mathbf{x}\\sim Q,\\,\\mathbb{E}[\\mathbf{x}]=0,\\,\\mathbb{E}[\\mathbf{x}\\mathbf{x}^{T}]\\preceq\\Sigma;\\qquad y\\sim\\mathsf{B e r n o u l l i}(\\phi(\\langle\\theta^{*},\\mathbf{x}\\rangle))\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "where $\\phi(t)=(1+e^{-t})^{-1}$ . The covariates $\\mathbf{x}$ are heavy tailed, with only bounded second moments. The sample-level loss is given by the negative log likelihood of $y|\\mathbf x$ as follows: ", "page_idx": 58}, {"type": "equation", "text": "$$\nf(\\theta;\\mathbf x,y)=\\ln(1+\\exp(\\langle\\mathbf x,\\theta\\rangle))-y\\left\\langle\\mathbf x,\\theta\\right\\rangle\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "The associated population loss and stochastic gradient oracle is given by ", "page_idx": 58}, {"type": "equation", "text": "$$\n\\begin{array}{c}{F(\\theta)=\\mathbb{E}_{\\mathbf{x},y\\sim P}\\left[\\ln(1+\\exp(\\langle\\mathbf{x},\\theta\\rangle))-y\\left\\langle\\mathbf{x},\\theta\\right\\rangle\\right]}\\\\ {g(\\theta;\\mathbf{x},\\mathbf{y})=\\phi(\\langle\\mathbf{x},\\theta\\rangle)\\mathbf{x}-y\\mathbf{x}}\\end{array}\n$$", "text_format": "latex", "page_idx": 58}, {"type": "text", "text": "We now compute the gradient and the Hessian of $F$ ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla F(\\theta)=\\mathbb{E}\\left[\\frac{\\exp\\left(\\langle\\mathbf{x},\\theta\\rangle\\right)}{1+\\exp(\\langle\\mathbf{x},\\theta\\rangle)}\\cdot\\mathbf{x}-\\phi(\\langle\\mathbf{x},\\theta^{*}\\rangle)\\mathbf{x}\\right]}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}\\left[(\\phi(\\langle\\mathbf{x},\\theta\\rangle)-\\phi(\\langle\\mathbf{x},\\theta^{*}\\rangle))\\,\\mathbf{x}\\right]}\\\\ &{\\nabla^{2}F(\\theta)=\\mathbb{E}[\\phi^{\\prime}(\\langle\\mathbf{x},\\theta\\rangle)\\mathbf{xx}^{T}]}\\\\ &{\\quad\\quad\\quad=\\mathbb{E}[\\phi(\\langle\\mathbf{x},\\theta\\rangle)(1-\\phi(\\langle\\mathbf{x},\\theta\\rangle))\\mathbf{xx}^{T}]}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Since $0\\,\\leq\\,\\phi(t)\\,\\leq\\,1$ for every $t\\,\\in\\mathbb{R}$ , we note that $0\\,\\preceq\\nabla^{2}F(\\boldsymbol{\\theta})\\,\\preceq\\,\\mathbb{E}[\\mathbf{x}\\mathbf{x}^{T}]\\,\\preceq\\,\\Sigma$ (as $E[\\mathbf{x}]\\,=\\,0_{,}$ ). Hence, $F$ is convex and $L$ smooth with $L=\\Vert\\Sigma\\Vert_{2}$ . Furthermore, since $\\nabla F({\\boldsymbol{\\theta}}^{*})=0$ and $F$ is convex, we conclude that $\\theta^{*}$ is a minimizer of $F$ . ", "page_idx": 59}, {"type": "text", "text": "It is easy to see that $\\mathbb{E}\\left[g(\\theta;\\mathbf{x},y)\\right]=\\mathbb{E}\\left[\\left(\\phi(\\langle\\mathbf{x},\\theta\\rangle)-\\phi(\\langle\\mathbf{x},\\theta^{*}\\rangle)\\right)\\mathbf{x}\\right]=\\nabla F(\\theta)$ , i.e., the stochastic gradient is unbiased. Let $\\mathbf{n}(\\theta;\\mathbf{x},y)$ denote the stochastic gradient noise, i.e.,: ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{n}(\\theta;\\mathbf{x},y)=g(\\theta;\\mathbf{x},y)-\\nabla F(\\theta)}\\\\ &{\\quad\\quad\\quad=\\phi(\\langle\\mathbf{x},\\theta\\rangle)\\mathbf{x}-\\mathbb{E}\\left[\\phi(\\langle\\mathbf{x},\\theta\\rangle)\\mathbf{x}\\right]+\\mathbb{E}[\\phi(\\langle\\mathbf{x},\\theta^{*}\\rangle)\\mathbf{x}]-y\\mathbf{x}}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "We shall now control the stochastic gradient covariance $\\Sigma(\\theta)=\\mathbb{E}[\\mathbf{n}(\\theta;\\mathbf{x},y)\\mathbf{n}(\\theta;\\mathbf{x},y)^{T}]$ . To this end, we define ${\\bf a}_{\\bf x}(\\theta)$ and $\\mathbf{c}_{\\mathbf{x},y}(\\theta)$ as follows: ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\mathbf{a}_{\\mathbf{x}}(\\theta)=\\phi(\\langle\\mathbf{x},\\theta\\rangle)\\mathbf{x}-\\mathbb{E}\\left[\\phi(\\langle\\mathbf{x},\\theta\\rangle)\\mathbf{x}\\right]}\\\\ &{\\mathbf{c}_{\\mathbf{x},y}(\\theta)=\\mathbb{E}[\\phi(\\langle\\mathbf{x},\\theta^{*}\\rangle)\\mathbf{x}]-y\\mathbf{x}}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "We note that $\\mathbb{E}\\left[\\mathbf{c}_{\\mathbf{x},y}(\\theta)|\\mathbf{x}\\right]=0$ and $\\mathbb{E}[{\\bf a}_{\\bf x}(\\theta)]=0$ . Since $\\mathbf{n}_{\\mathbf{x},y}(\\theta)=\\mathbf{a}_{\\mathbf{x}}(\\theta)+\\mathbf{b}_{\\mathbf{x},y}(\\theta)$ , it follows that: ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\Sigma(\\theta)==\\mathbb{E}[\\mathbf{n}(\\theta;\\mathbf{x},y)\\mathbf{n}(\\theta;\\mathbf{x},y)^{T}]=\\mathbb{E}\\left[\\mathbf{a}_{\\mathbf{x}}(\\theta)\\mathbf{a}_{\\mathbf{x}}(\\theta)^{T}\\right]+\\mathbb{E}\\left[\\mathbf{c}_{\\mathbf{x},y}(\\theta)\\mathbf{c}_{\\mathbf{x},y}(\\theta)^{T}\\right]\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "We now control each of the terms in the RHS as follows: ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\mathbf{a}_{\\mathbf{x}}(\\theta)\\mathbf{a}_{\\mathbf{x}}(\\theta)^{T}\\right]=\\mathbb{E}[\\phi(\\langle\\mathbf{x},\\theta\\rangle)^{2}\\mathbf{x}\\mathbf{x}^{T}]-\\mathbb{E}\\left[\\phi(\\langle\\mathbf{x},\\theta\\rangle)\\mathbf{x}\\right]\\mathbb{E}\\left[\\phi(\\langle\\mathbf{x},\\theta\\rangle)\\mathbf{x}\\right]^{T}}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad\\preceq\\mathbb{E}[\\phi(\\langle\\mathbf{x},\\theta\\rangle)^{2}\\mathbf{x}\\mathbf{x}^{T}]}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\preceq\\mathbb{E}[\\mathbf{x}\\mathbf{x}^{T}]\\preceq\\Sigma}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "where we use the fact that $\\phi(t)\\leq1$ . Similarly, ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\mathbf{c}_{\\mathbf{x},y}(\\boldsymbol{\\theta})\\mathbf{c}_{\\mathbf{x},y}(\\boldsymbol{\\theta})^{T}\\right]=\\mathbb{E}[y^{2}\\mathbf{x}\\mathbf{x}^{T}]-\\mathbb{E}[\\phi(\\langle\\mathbf{x},\\boldsymbol{\\theta}^{*}\\rangle)\\mathbf{x}]\\mathbb{E}[\\phi(\\langle\\mathbf{x},\\boldsymbol{\\theta}^{*}\\rangle)\\mathbf{x}]^{T}}\\\\ &{~~~~~~~~~~~~~~~~~~~~~~~~~\\preceq\\mathbb{E}[\\phi(\\langle\\mathbf{x},\\boldsymbol{\\theta}^{*}\\rangle)\\mathbf{x}\\mathbf{x}^{T}]}\\\\ &{~~~~~~~~~~~~~~~~~~~~~\\preceq\\mathbb{E}[\\mathbf{x}\\mathbf{x}^{T}]\\preceq\\Sigma}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "where we use the fact that $\\mathbb{E}[y^{2}|\\mathbf{x}]=\\phi\\big(\\langle\\mathbf{x},\\theta^{*}\\rangle\\big)\\leq1$ . It follows that ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\Sigma(\\theta)\\preceq2\\Sigma\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "Thus, the stochastic gradient oracle satisfies the Bdd. $2^{\\mathsf{n d}}$ Moment assumption. Hence, the stochastic gradient oracle satisfies the Bdd. $2^{\\mathsf{n d}}$ Moment assumption. Thus, the following result, which is a formal version of Corollary 3, is implied by Theorem 7 ", "page_idx": 59}, {"type": "text", "text": "Corollary 7 (Heavy Tailed Logistic Regression). Under the stochastic subgradient oracle described above, realized using $N\\gtrsim\\ln(\\ln(d))$ i.i.d samples from $P$ , the average iterate of Algorithm 1, when run under the parameter settings of Theorem $^{4}$ satisfies the following with probability at least $1-\\delta$ : ", "page_idx": 59}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{^{\\gamma}(\\hat{\\theta}_{N})-F(\\theta^{*})\\lesssim D_{1}\\sqrt{\\frac{\\mathsf{T r}(\\Sigma)+\\sqrt{\\|\\Sigma\\|_{2}}\\left(\\sqrt{\\mathsf{T r}(\\Sigma)}+\\|\\Sigma\\|_{2}D_{1}\\right)\\ln\\left(\\ln(N)/\\delta\\right)}{N}}+\\frac{\\|\\Sigma\\|_{2}D_{1}^{2}}{N}}\\\\ &{\\quad\\quad\\quad\\quad\\quad+\\,\\frac{D_{1}^{2}\\ln\\left(\\ln(N)/\\delta\\right)}{N}\\sqrt{\\|\\Sigma\\|_{2}\\mathsf{T r}(\\Sigma)+\\|\\Sigma\\|^{3}D_{1}^{2}}+\\frac{\\|\\Sigma\\|_{2}^{5/4}D_{1}^{3}\\ln\\left(\\ln(N)/\\delta\\right)^{3/2}}{N^{3/2}}\\left(\\mathsf{T r}(\\Sigma)+\\|\\Sigma\\|_{2}^{5/4}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 59}, {"type": "text", "text": "G.4 Proof of Corollary 4 ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "Recall from Section 5.4 that $\\Xi\\,=\\,\\mathbb{R}^{d}\\,\\times\\,\\mathbb{R}$ and given a target parameter $\\theta^{*}\\in\\mathcal{C}$ , $P$ defines the following linear model: ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\mathbf{x}\\sim Q,\\ \\mathbb{E}[\\mathbf{x}]=0,\\ \\mathbb{E}[\\mathbf{xx}^{T}]\\preceq\\Sigma;\\qquad y=\\langle\\mathbf{x},\\theta^{*}\\rangle+\\epsilon,\\ \\mathsf{M e d i a n}(\\epsilon|\\mathbf{x})=0\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "We allow both the covariate $\\mathbf{x}$ and target $y$ to be heavy tailed, assuming only bounded second moments for $\\mathbf{x}$ . We do not assume any moment bounds on $\\epsilon|\\mathbf x$ . The Least Absolute Deviation (LAD) Regression problem involves estimating $\\theta$ by solving SCO with the following sample loss ", "page_idx": 60}, {"type": "equation", "text": "$$\nf(\\theta;\\mathbf{x},y)=|\\left\\langle\\mathbf{x},\\theta\\right\\rangle-y|\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "The associated population risk and one possible realization of a stochastic subgradient oracle is given by: ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r}{F(\\theta)=\\mathbb{E}\\left[|\\left\\langle\\theta-\\theta^{*},\\mathbf{x}\\right\\rangle-\\epsilon|\\right]}\\\\ {g(\\theta;\\mathbf{x},\\mathbf{y})=\\mathtt{s g n}(\\langle\\theta,\\mathbf{x}\\rangle-\\mathbf{y})\\mathbf{x}\\,\\,\\,}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathsf{s g n}(t)=\\frac{t}{\\|t\\|}}\\end{array}$ for $t\\neq0$ and $\\mathsf{s g n}(0)=0$ . We note that for every $(\\mathbf{x},\\mathbf{y})\\in\\mathbb{R}^{d}\\times\\mathbb{R},\\,f(\\theta;\\mathbf{x},y)$ is a convex function in $\\theta$ , and thus, the population risk $F$ is a convex function, whose subgradient is given by: ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\partial F(\\theta)=\\mathbb{E}\\left[{\\mathsf{s g n}}\\left(\\langle\\theta-\\theta^{*},\\mathbf{x}\\rangle-\\epsilon\\right)\\mathbf{x}\\right]\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "We now show that $F$ is a Lipschitz function by bounding $\\partial F(\\theta)$ as follows: ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|\\partial F(\\theta)\\|=\\|\\mathbb{E}\\left[\\mathsf{s g n}\\left(\\langle\\theta-\\theta^{*},\\mathbf{x}\\rangle-\\epsilon\\right)\\mathbf{x}\\right]\\|}\\\\ &{\\qquad\\qquad\\leq\\mathbb{E}\\left[\\|\\mathbf{sgn}\\left(\\langle\\theta-\\theta^{*},\\mathbf{x}\\rangle-\\epsilon\\right)\\|\\cdot\\|\\mathbf{x}\\|\\right]}\\\\ &{\\qquad\\qquad\\leq\\sqrt{\\mathbb{E}\\left[\\|\\mathbf{x}\\|^{2}\\right]}}\\\\ &{\\qquad\\qquad\\leq\\sqrt{\\mathsf{T r}(\\Sigma)}}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "where the second step follows from Jensen\u2019s inequality, the third step uses the fact that $|\\mathtt{s g n}(t)|\\leq1$ and applies the Cauchy Schwarz inequality. Hence, $F$ is $G$ -Lipschitz with $G=\\sqrt{\\mathsf{T r}(\\Sigma)}$ . We now show that $\\partial F(\\theta^{*})=0$ which would imply that $\\theta^{*}$ is a minimizer of $F$ (as $F$ is convex) ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\nabla F(\\theta^{*})=\\mathbb{E}\\left[\\mathsf{s g n}(\\epsilon)\\mathbf{x}\\right]=\\mathbb{E}\\left[\\mathbf{x}\\cdot\\mathbb{E}\\left[\\mathsf{s g n}(\\epsilon)|\\mathbf{x}\\right]\\right]=0\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "where we use the fact that $\\mathbb{E}[\\mathsf{s g n}(\\epsilon)|\\mathbf{x}]=0$ , because $\\epsilon|{\\bf x}$ is a continuous random variable with zero median. ", "page_idx": 60}, {"type": "text", "text": "For the stochastic gradient oracle described above, the associated stochastic gradient noise $\\mathbf{n}(\\theta;\\mathbf{x},y)$ and its covariance $\\Sigma(\\theta)$ are given as follows: ", "page_idx": 60}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{a}(\\theta;\\mathbf{x},y)=\\mathsf{s g n}(\\langle\\theta-\\theta^{*},\\mathbf{x}\\rangle-\\epsilon)\\mathbf{x}-\\mathbb{E}\\left[\\mathsf{s g n}(\\langle\\theta-\\theta^{*},\\mathbf{x}\\rangle-\\epsilon)\\mathbf{x}\\right]}\\\\ &{\\quad\\quad\\Sigma(\\theta)=\\mathbb{E}\\left[\\mathsf{s g n}(\\langle\\theta-\\theta^{*},\\mathbf{x}\\rangle-\\epsilon)^{2}\\mathbf{xx}^{T}\\right]-\\mathbb{E}\\left[\\mathsf{s g n}(\\langle\\theta-\\theta^{*},\\mathbf{x}\\rangle-\\epsilon)\\mathbf{x}\\right]\\mathbb{E}\\left[\\mathsf{s g n}(\\langle\\theta-\\theta^{*},\\mathbf{x}\\rangle-\\epsilon)\\mathbf{x}\\right]^{T}}\\\\ &{\\quad\\quad\\quad\\quad\\preceq\\mathbb{E}\\left[\\mathsf{s g n}(\\langle\\theta-\\theta^{*},\\mathbf{x}\\rangle-\\epsilon)^{2}\\mathbf{xx}^{T}\\right]}\\\\ &{\\quad\\quad\\quad\\preceq\\mathbb{E}\\left[\\mathbf{xx}^{T}\\right]\\preceq\\Sigma}\\end{array}\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "Hence, the stochastic gradient oracle satisfies the Bdd. $2^{\\mathsf{n d}}$ Moment assumption. Thus, the following result, which is a formal version of Corollary 4, is implied by Theorem 8 ", "page_idx": 60}, {"type": "text", "text": "Corollary 8 (Heavy Tailed LAD Regression). ", "page_idx": 60}, {"type": "equation", "text": "$$\nx)-F(\\theta^{*})\\lesssim D_{1}\\sqrt{\\frac{\\mathsf{T r}(\\Sigma)+\\sqrt{\\|\\Sigma\\|_{2}\\mathsf{T r}(\\Sigma)}\\ln\\bigl(\\ln(N)/\\delta\\bigr)}{N}}+\\frac{D_{1}\\mathsf{T r}(\\Sigma)\\ln\\bigl(\\ln(N)/\\delta\\bigr)}{N\\sqrt{\\|\\Sigma\\|_{2}}}+\\frac{D_{1}\\mathsf{T r}(\\Sigma)^{5/4}\\ln\\bigl(\\ln(N)/\\delta\\bigr)}{N^{3/2}\\|\\Sigma\\|^{3}}.\n$$", "text_format": "latex", "page_idx": 60}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 60}, {"type": "text", "text": "1. Claims ", "page_idx": 60}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 61}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Justification: We provide complete mathematical proofs of the claims. Guidelines: ", "page_idx": 61}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 61}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 61}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Justification: ", "page_idx": 61}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. ", "page_idx": 61}, {"type": "text", "text": "\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 61}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 61}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 61}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 61}, {"type": "text", "text": "Justification: ", "page_idx": 61}, {"type": "text", "text": "Guidelines: \u2022 The answer NA means that the paper does not include theoretical results. ", "page_idx": 61}, {"type": "text", "text": "\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 62}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 62}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 62}, {"type": "text", "text": "Justification: The paper is purely theoretical. ", "page_idx": 62}, {"type": "text", "text": "Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 62}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 62}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 62}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 62}, {"type": "text", "text": "Justification: Paper do note include experiments requiring code. Guidelines: ", "page_idx": 62}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 63}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 63}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 63}, {"type": "text", "text": "Justification: ", "page_idx": 63}, {"type": "text", "text": "Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 63}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 63}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 63}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 63}, {"type": "text", "text": "Justification: Guidelines: ", "page_idx": 63}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified. ", "page_idx": 63}, {"type": "text", "text": "\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). \u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 64}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 64}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 64}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 64}, {"type": "text", "text": "Answer: [Yes] Justification: Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 64}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 64}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 64}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 64}, {"type": "text", "text": "Justification: The paper is purely theoretical and we do foresee any societal impact of this work. ", "page_idx": 64}, {"type": "text", "text": "Guidelines: ", "page_idx": 64}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 64}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 65}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 65}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 65}, {"type": "text", "text": "Justification: purely theoretical work. Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 65}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 65}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 65}, {"type": "text", "text": "Guidelines: ", "page_idx": 65}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 65}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 65}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 65}, {"type": "text", "text": "Answer: [NA] Justification: ", "page_idx": 65}, {"type": "text", "text": "Guidelines: ", "page_idx": 66}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 66}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 66}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 66}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 66}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 66}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 66}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 66}, {"type": "text", "text": "Answer: [NA] Justification: Guidelines: ", "page_idx": 66}, {"type": "text", "text": "", "page_idx": 66}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 66}]