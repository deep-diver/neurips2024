[{"figure_path": "hbOWLtJNMK/figures/figures_3_1.jpg", "caption": "Figure 1: Analysis of the importance of different meta-paths. (a) illustrates the results after removing a single meta-path on DBLP; (b) shows the performance of utilizing a single meta-path on DBLP; (c) illustrates the performance after removing a part of meta-paths on ACM.", "description": "This figure analyzes the impact of different meta-paths on the model's performance.  Panel (a) shows the effect of removing one meta-path at a time on the DBLP dataset, illustrating that some meta-paths (like APV) are more crucial than others. Panel (b) further explores this by testing the performance with only a single meta-path retained on DBLP, again highlighting the importance of specific meta-paths.  Finally, panel (c) demonstrates a similar analysis for the ACM dataset, but with subsets of meta-paths removed, showing that not all meta-paths contribute positively to the model.", "section": "4 Motivation of Long-range Meta-path Search"}, {"figure_path": "hbOWLtJNMK/figures/figures_4_1.jpg", "caption": "Figure 2: The overall framework of LMSPS. Based on the progressive sampling and sampling evaluation in the search stage, the training stage employs M effective meta-paths instead of the full K target-node-related meta-paths. It exhibits aggregation of meta-paths with maximum hop 2, i.e., 0, 1, and 2-hop meta-paths. The weight updates of feature projection are not shown for ease of illustration.", "description": "This figure illustrates the overall framework of the Long-range Meta-path Search through Progressive Sampling (LMSPS) method.  It shows two main stages: a search stage and a training stage. The search stage uses progressive sampling and sampling evaluation to efficiently select a subset of effective meta-paths from the exponentially large set of all possible meta-paths. The training stage then uses only this reduced set of effective meta-paths to train a target network, which avoids over-smoothing and reduces computational cost. The figure highlights the use of MLPs (Multilayer Perceptrons) in both stages and depicts the flow of information during the meta-path search and network training.  The example shown illustrates a scenario with a maximum hop of 2.", "section": "5 The Proposed Method"}, {"figure_path": "hbOWLtJNMK/figures/figures_7_1.jpg", "caption": "Figure 3: Illustration of (a) performance, (b) memory cost, (c) average training time of Simple-HGN, SeHGNN, and LMSPS relative to the maximum hop or layer on DBLP. The gray dotted line in (a) indicates the number of target-node-related meta-paths under different maximum hops, which is exponential.", "description": "This figure compares the performance, GPU memory usage, and training time of three different graph neural network models (Simple-HGN, SeHGNN, and LMSPS) on the DBLP dataset.  The comparison is made across varying maximum hop lengths (or layers in the case of Simple-HGN).  The key observation is that LMSPS demonstrates significantly better scalability than the other two models in terms of memory usage and training time as the maximum hop length increases.  The exponential growth of the number of meta-paths with maximum hop length (shown by the gray dotted line) highlights the computational challenge addressed by LMSPS.", "section": "6.3 Analysis on Large Maximum Hops"}, {"figure_path": "hbOWLtJNMK/figures/figures_19_1.jpg", "caption": "Figure 3: Illustration of (a) performance, (b) memory cost, (c) average training time of Simple-HGN, SeHGNN, and LMSPS relative to the maximum hop or layer on DBLP. The gray dotted line in (a) indicates the number of target-node-related meta-paths under different maximum hops, which is exponential.", "description": "This figure compares the performance, memory cost, and training time of three different models (Simple-HGN, SeHGNN, and LMSPS) on the DBLP dataset as the maximum hop or layer increases.  The x-axis represents the maximum hop or layer, while the y-axis shows the micro-F1 score (performance), GPU memory cost (GB), and training time (seconds per epoch). The gray dotted line in the performance plot highlights the exponential growth in the number of meta-paths as the maximum hop increases, demonstrating one of the challenges addressed by LMSPS.", "section": "6.3 Analysis on Large Maximum Hops"}, {"figure_path": "hbOWLtJNMK/figures/figures_20_1.jpg", "caption": "Figure 3: Illustration of (a) performance, (b) memory cost, (c) average training time of Simple-HGN, SeHGNN, and LMSPS relative to the maximum hop or layer on DBLP. The gray dotted line in (a) indicates the number of target-node-related meta-paths under different maximum hops, which is exponential.", "description": "This figure compares the performance, memory usage, and training time of three different graph neural network models (Simple-HGN, SeHGNN, and LMSPS) as the maximum hop (or layer) increases. The results show that LMSPS has superior performance, significantly lower memory consumption, and comparable training time compared to the other two methods, especially as the number of meta-paths grows exponentially with the maximum hop.", "section": "6.3 Analysis on Large Maximum Hops"}, {"figure_path": "hbOWLtJNMK/figures/figures_20_2.jpg", "caption": "Figure 3: Illustration of (a) performance, (b) memory cost, (c) average training time of Simple-HGN, SeHGNN, and LMSPS relative to the maximum hop or layer on DBLP. The gray dotted line in (a) indicates the number of target-node-related meta-paths under different maximum hops, which is exponential.", "description": "This figure compares the performance, memory usage, and training time of three different methods (Simple-HGN, SeHGNN, and LMSPS) on the DBLP dataset as the maximum hop (or layer) increases.  The results show that LMSPS maintains relatively stable performance, memory, and training time, while the other two methods show significant increases in memory usage and training time, and SeHGNN shows degraded performance as the maximum hop increases.  The gray dotted line highlights the exponential growth in the number of meta-paths as the maximum hop increases, illustrating the challenge addressed by LMSPS.", "section": "6.3 Analysis on Large Maximum Hops"}]