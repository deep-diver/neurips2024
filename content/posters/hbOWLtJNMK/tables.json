[{"figure_path": "hbOWLtJNMK/tables/tables_6_1.jpg", "caption": "Table 1: Performance on small and large datasets. Best is in bold, and the runner-up is underlined.", "description": "This table presents the performance comparison of LMSPS and various state-of-the-art baselines on several small and large heterogeneous graph datasets. The performance is measured using Macro-F1, Micro-F1 scores (for small datasets), and test accuracy (for the OGBN-MAG dataset).  The results show that LMSPS achieves the best performance on most datasets, demonstrating its effectiveness in handling both small and large-scale heterogeneous graphs. The table also highlights that many metapath-free methods encounter out-of-memory issues when dealing with large datasets, while LMSPS efficiently handles both small and large datasets. ", "section": "6.2 Performance Analysis"}, {"figure_path": "hbOWLtJNMK/tables/tables_7_1.jpg", "caption": "Table 1: Performance on small and large datasets. Best is in bold, and the runner-up is underlined.", "description": "This table presents the performance comparison of LMSPS and various baselines on four small-scale datasets (DBLP, IMDB, ACM, Freebase) and one large-scale dataset (OGBN-MAG) in terms of Macro-F1, Micro-F1, and test accuracy.  The best performing method for each metric and dataset is shown in bold, while the second-best is underlined.  The table highlights LMSPS's superior performance, especially on the large-scale dataset.", "section": "6.2 Performance Analysis"}, {"figure_path": "hbOWLtJNMK/tables/tables_8_1.jpg", "caption": "Table 4: Experiments on the generalization of the searched meta paths. * means using the meta-paths searched in LMSPS.", "description": "This table presents the results of experiments conducted to evaluate the generalizability of the meta-paths discovered by the proposed LMSPS method.  The table shows that using the meta-paths found by LMSPS in other heterogeneous graph neural networks (HGNNs), namely HAN and SeHGNN, leads to improved performance compared to using the original meta-paths in those networks. This demonstrates that LMSPS can identify meta-paths that are effective across different HGNN architectures, highlighting the generalizability and transferability of the approach.", "section": "6.4 Effectiveness of the Search Algorithm and Searched Meta-paths"}, {"figure_path": "hbOWLtJNMK/tables/tables_8_2.jpg", "caption": "Table 5: Results of LMSPS and SeHGNN on the sparse large-scale heterogeneous graphs. \u2191 means the improvements in test accuracy.", "description": "This table compares the performance of LMSPS and SeHGNN on four sparse large-scale heterogeneous graph datasets derived from OGBN-MAG.  The datasets vary in their sparsity, controlled by limiting the maximum in-degree related to each edge type.  The table shows the test accuracy for each method on each dataset, along with the improvement achieved by LMSPS over SeHGNN.", "section": "6. Experiments and Analysis"}, {"figure_path": "hbOWLtJNMK/tables/tables_8_3.jpg", "caption": "Table 1: Performance on small and large datasets. Best is in bold, and the runner-up is underlined.", "description": "This table presents the performance comparison of LMSPS and other state-of-the-art methods on several benchmark datasets, including both small and large-scale heterogeneous graphs.  The results are reported in terms of Macro-F1, Micro-F1 scores, and test accuracy, showcasing LMSPS's superior performance across various datasets.  The table also highlights the out-of-memory (OOM) issues faced by some methods when dealing with large datasets, further emphasizing LMSPS's scalability and efficiency.", "section": "6.2 Performance Analysis"}, {"figure_path": "hbOWLtJNMK/tables/tables_13_1.jpg", "caption": "Table 1: Performance on small and large datasets. Best is in bold, and the runner-up is underlined.", "description": "This table presents the performance comparison of LMSPS and other state-of-the-art methods on several datasets, including small-scale datasets (DBLP, IMDB, ACM, Freebase) and a large-scale dataset (OGBN-MAG).  The metrics used are Macro-F1, Micro-F1, and Test Accuracy depending on the dataset, showcasing LMSPS's superiority in most cases. Note that many baselines encounter out-of-memory (OOM) issues when dealing with large datasets, highlighting the efficiency of LMSPS.", "section": "6.2 Performance Analysis"}, {"figure_path": "hbOWLtJNMK/tables/tables_17_1.jpg", "caption": "Table 8: Time complexity comparison of every training epoch. \u2020 means time complexity under small-scale datasets and full-batch training.", "description": "This table compares the time complexity of different methods (HAN, Simple-HGN, SeHGNN, and LMSPS) for each step of training: feature projection, neighbor aggregation, and semantic fusion.  It shows how the complexity scales with the number of nodes (N), features (F), edge types (r), maximum hop (l), and the number of sampled metapaths (M). The \u2020 symbol indicates that the complexity is measured under small-scale datasets and full-batch training, which is a different setting than the others.  The table highlights the differences in computational cost between methods, especially highlighting the constant time complexity of LMSPS in the search and training stages.", "section": "C.3 Time Complexity Analysis"}, {"figure_path": "hbOWLtJNMK/tables/tables_18_1.jpg", "caption": "Table 1: Performance on small and large datasets. Best is in bold, and the runner-up is underlined.", "description": "This table presents the performance comparison of LMSPS and other state-of-the-art methods on several benchmark datasets.  It shows the Macro-F1, Micro-F1, and test accuracy scores across various datasets, including DBLP, IMDB, ACM, Freebase, and OGBN-MAG.  The results highlight the superior performance of LMSPS on large datasets like OGBN-MAG, demonstrating its ability to handle the challenges of long-range dependency and over-smoothing in heterogeneous graphs. The table also reveals that many metapath-free HGNNs encounter out-of-memory errors when applied to large-scale datasets.", "section": "6.2 Performance Analysis"}, {"figure_path": "hbOWLtJNMK/tables/tables_19_1.jpg", "caption": "Table 1: Performance on small and large datasets. Best is in bold, and the runner-up is underlined.", "description": "This table presents the performance comparison of LMSPS and several baseline methods on four small-scale datasets (DBLP, IMDB, ACM, Freebase) and one large-scale dataset (OGBN-MAG).  The metrics used are Macro-F1, Micro-F1 (for small datasets), and test accuracy (for OGBN-MAG). The best performance for each dataset and metric is shown in bold, and the second-best is underlined.  The table highlights the superior performance of LMSPS, especially on the large-scale dataset, OGBN-MAG, where it significantly outperforms all other methods. It also shows that many metapath-free methods encounter out-of-memory errors when dealing with large datasets.", "section": "6.2 Performance Analysis"}, {"figure_path": "hbOWLtJNMK/tables/tables_20_1.jpg", "caption": "Table 1: Performance on small and large datasets. Best is in bold, and the runner-up is underlined.", "description": "This table presents the performance comparison of LMSPS and other state-of-the-art methods on several benchmark datasets.  It shows the Macro-F1, Micro-F1 scores (for smaller datasets), and test accuracy (for OGBN-MAG). The results highlight LMSPS's superior performance, especially on large-scale datasets, where many other methods fail due to out-of-memory (OOM) errors. The table also includes a random baseline to show the importance of the proposed meta-path search.", "section": "6.2 Performance Analysis"}]