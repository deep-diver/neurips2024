[{"type": "text", "text": "Divide-and-Conquer Posterior Sampling for Denoising Diffusion Priors ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yazid Janati\u2217,1 Badr Moufad\u2217,1 Alain Durmus1 Eric Moulines1,3 Jimmy Olsson2 1 CMAP, Ecole polytechnique 2 KTH Royal Institute of Technology 3 MBZUAI ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Recent advancements in solving Bayesian inverse problems have spotlighted denoising diffusion models (DDMs) as effective priors. Although these have great potential, DDM priors yield complex posterior distributions that are challenging to sample. Existing approaches to posterior sampling in this context address this problem either by retraining model-specific components, leading to stiff and cumbersome methods, or by introducing approximations with uncontrolled errors that affect the accuracy of the produced samples. We present an innovative framework, divide-and-conquer posterior sampling, which leverages the inherent structure of DDMs to construct a sequence of intermediate posteriors that guide the produced samples to the target posterior. Our method significantly reduces the approximation error associated with current techniques without the need for retraining. We demonstrate the versatility and effectiveness of our approach for a wide range of Bayesian inverse problems. The code is available at https://github.com/Badr-MOUFAD/dcps ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Many problems in machine learning can be formulated as inverse problems, such as superresolution, deblurring, and inpainting, to name but a few. They all have the same goal, namely to recover a signal of interest from an indirect observation. One line of research addresses these problems through the lens of the Bayesian framework by specifying two components: a prior distribution, which embodies the specification of the signal, and a likelihood that describes the law of the observation conditionally on the signal. Once these elements are specified, the inverse problem is solved by sampling from the posterior distribution, which, after including the observation, contains all available information about the signal and thus about its uncertainty as well [12]. The importance of the specification of the prior in solving Bayesian ill-posed inverse problems is paramount. In the last decade, the success of priors based on deep generative models has fundamentally changed the field of linear inverse problems [40, 55, 19, 36, 24]. Recently, denoising diffusion probabilistic models (DDMs) have received special attention. Thanks to their ability to learn complex and multimodal data distributions, DDM represent the state-of-the-art in many generative modeling tasks, e.g. image generation [45, 20, 50, 52, 15, 46, 49], super-resolution [43, 1], and inpainting [45, 11, 22]. ", "page_idx": 0}, {"type": "text", "text": "Popular methods to sample from posterior distribution include Markov chain Monte Carlo (MCMC) and variational inference; see [53, 6] and the references therein. These methods are iterative schemes that require an explicit procedure to evaluate pointwise the prior distribution and often its (Stein) score function [21] in order to compute acceptance ratios and construct efficient proposals. While sampling from the DDM priors is straightforward, posterior sampling is usually challenging since the intractability of the posterior density and its score make them computationally prohibitive and thus invalidate all conventional simulation methods. Although approximations exist, their associated iterative sampling schemes can be computationally intensive and exhibit high sensitivity to the choice of hyperparameters; see e.g. [24]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "This paper proposes the DIVIDE-AND-CONQUER POSTERIOR SAMPLER (DCPS), a novel approach to posterior sampling in Bayesian inverse problems with DDM priors. Thanks to the Markov property of the data-generating backward diffusion, the posterior can be expressed as the marginal distribution of a Feynman\u2013Kac (FK) path measure [13], whose length corresponds to the number of diffusion steps and whose user-defined potentials serve to bias the dynamics of the data-generating backward diffusion to align with the likelihood of the observation. Besides, for a given choice of potentials, the FK path law becomes Markovian, making it possible to express the posterior as the marginal of a time-reversed inhomogeneous Markov chain. ", "page_idx": 1}, {"type": "text", "text": "This approach is tempting, yet, the backward Markov decomposition remains difficult to apply in practice as these specific potential functions are difficult to approximate, especially when the number of diffusion steps is large. We tackle this problem with a divide-and-conquer approach. More precisely, instead of targeting the given posterior by a single simulation run through the full backward decomposition, our proposed scheme targets backward a sequence $(\\pi_{k_{\\ell}})_{\\ell=0}^{L}$ of distributions along the path measure leading to the target posterior distribution (section 3). These distributions are induced by a sequence of increasingly complex potentials and converge to the target distribution. Starting with a sample from $\\pi_{k_{\\ell+1}}$ , a draw from $\\pi_{k_{\\ell}}$ is formed by a combination of Langevin iterations and the simulation of an inhomogeneous Markov chain. In other words, $\\pi_{k_{\\ell}}$ is expressed as the final marginal distribution of a time-reversed inhomogeneous Markov chain of moderate length $k_{\\ell+1}-k_{\\ell}\\in\\mathbb{N}^{*}$ with an initial distribution \u03c0\u2113k\u2113+1. This chain, whose transition densities are intractable, is approximately sampled using Gaussian variational inference. The rationale behind our approach stems from the observation that the Gaussian approximation error can be reduced by shortening the length of the intermediate FK path measures (i.e., by increasing $L$ ); a result that we show in Proposition A.1. We finally illustrate that our algorithm can provide high-quality solutions to Bayesian inverse problems involving a variety of datasets and tasks. ", "page_idx": 1}, {"type": "text", "text": "To sum up our contribution, we ", "page_idx": 1}, {"type": "text", "text": "\u2022 show that the existing approximations of the Markovian backward decomposition can be improved using a bridge-kernel smoothing technique   \n\u2022 design a novel divide-and-conquer sampling approach that enables efficient bias-reduced sampling from the posterior, and illustrate its performance on several Bayesian inverse problems including inpainting, outpainting, Poisson imaging, and JPEG dequantization,   \n\u2022 propose a new technique to efficiently generate approximate samples from the backward decomposition using Gaussian variational inference. ", "page_idx": 1}, {"type": "text", "text": "Notation. For $(m,n)\\in\\mathbb{N}^{2}$ such that $m<n$ , we let $[\\![m,n]\\!]:=\\left\\{m,\\ldots,n\\right\\}$ . We use $\\mathrm{N}(x;\\mu,\\Sigma)$ to denote the density at $x$ of a Gaussian distribution with mean $\\mu$ and covariance matrix $\\Sigma$ . $I_{d}$ is the $d$ -dimensional identity matrix and $\\delta_{a}$ denotes the Dirac mass at $a$ . $W_{2}$ denotes the Wasserstein distance of order 2. We use uppercase for random variables and lowercase for their realizations. ", "page_idx": 1}, {"type": "text", "text": "2 Posterior sampling with DDM prior ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "DDM priors. We provide a brief overview of DDMs [45, 50, 20]. Suppose we can access an empirical sample from some data distribution $p_{\\mathrm{data}}$ defined on $\\mathbb{R}^{d_{x}}$ . For $\\textit{n}\\in\\mathbb{N}$ large enough and $k\\,\\in\\,[0,n]$ , define the distribution $\\begin{array}{r}{q_{k}(x_{k}):=\\int p_{\\mathrm{data}}(x_{0})\\,q_{k|0}(x_{k}|x_{0})\\mathrm{d}x_{0}}\\end{array}$ with $q_{k|0}(x_{k}|x_{0}):=$ $\\mathrm{N}(x_{k};\\sqrt{\\alpha_{k}}x_{0},(1-\\alpha_{k})I_{d_{x}})$ , where $(\\alpha_{k})_{k=0}^{n}$ is a decreasing sequence with $\\alpha_{0}=1$ and $\\alpha_{n}$ approximately equals zero. The probability density $q_{k}$ corresponds to the marginal distribution at time $k$ of an auto-regressive process on $\\mathbb{R}^{d_{x}}$ given by $X_{k+1}=\\sqrt{\\alpha_{k+1}/\\alpha_{k}}X_{k}+\\sqrt{1-\\alpha_{k+1}/\\alpha_{k}}\\epsilon_{k+1},$ with $X_{0}\\sim p_{\\mathrm{data}}$ and $(\\epsilon_{k})_{k=0}^{n}$ being a sequence of i.i.d. $d_{x}$ -dimensional standard Gaussians. ", "page_idx": 1}, {"type": "text", "text": "misa ttihoen sc $\\hat{x}_{0\\mid k}^{\\theta}$ tioof ntahle  dimsatrpipbiuntgios $\\begin{array}{r}{x_{k}\\mapsto\\int x_{0}\\,q_{0|k}(x_{0}|x_{k})\\mathrm{d}x_{0}}\\end{array}$ ,c hw $q_{0|k}(x_{0}|x_{k})\\stackrel{*}{\\propto}\\bar{p_{\\mathrm{data}}}(x_{0})q_{k|0}(\\bar{x_{k}}|x_{0})$ $X_{0}$ $\\\"X_{k}=x_{k}$ $\\hat{x}_{0\\mid k}^{\\theta}$ is defined as $\\hat{x}_{0|k}^{\\theta}(x_{k}):=(x_{k}-\\sqrt{1-\\alpha_{k}}\\hat{\\epsilon}_{k}^{\\theta}(x_{k}))/\\sqrt{\\alpha_{k}}$ , where $\\hat{\\epsilon}_{k}^{\\theta}$ is a noise predictor network trained by minimizing a denoising objective; see [46, Eq. (5)] and Appendix A for details. Following [15, Section 4.2], $\\bar{\\hat{\\epsilon}}_{k}^{\\theta}$ also provides an estimate of the score $\\nabla\\log q_{k}(x_{k})$ given by $\\hat{s}_{k}^{\\theta}(x_{k}):=-\\widetilde{\\left(x_{k}\\right.}-\\widetilde{\\left.\\right.}$ $\\sqrt{\\alpha_{k}}\\hat{x}_{0|k}^{\\theta}(x_{k}))/(1\\!-\\!\\alpha_{k})$ . We denote by $\\theta^{\\star}$ the minimizer of the denoising objective. Having access to $\\theta^{\\star}$ , we can define a generative model for $p_{\\mathrm{data}}$ by adopting the denoising diffusion probabilistic model (DDPM) framework of [20]. As long as $n$ is large enough, $q_{n}$ can be confused with a multivariate standard Gaussian. Define the bridge kernel $q_{k|0,k+1}(x_{k}|x_{0},\\stackrel{\\cdot\\cdot}{x}_{k+1})\\propto q_{k|0}(x_{k}|x_{0})q_{k+1|k}(x_{k+1}|x_{k})$ which is a Gaussian distribution with mean $\\mu_{k|0,k+1}(x_{0},x_{k+1})$ and diagonal covariance $\\sigma_{k|k+1}^{2}I_{d_{x}}$ defined in Appendix A.1. Define the generative model for $p_{\\mathrm{data}}$ as ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{p_{0:n}^{\\theta^{\\star}}(x_{0:n})=p_{n}(x_{n})\\prod_{k=0}^{n-1}p_{k|k+1}^{\\theta^{\\star}}(x_{k}|x_{k+1})\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where for every $k\\in[1,n-1]$ , the backward transitions are ", "page_idx": 2}, {"type": "equation", "text": "$$\np_{k|k+1}^{\\theta^{\\star}}(x_{k}|x_{k+1}):=q_{k|0,k+1}(x_{k}|\\hat{x}_{0|k+1}^{\\theta^{\\star}}(x_{k+1}),x_{k+1})\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with $p_{0|1}^{\\theta^{\\star}}(\\cdot|x_{1})\\,:=\\,\\delta_{\\hat{x}_{0|1}^{\\theta^{\\star}}(x_{1})}$ and $p_{n}(x_{n})\\,=\\,\\mathrm{N}(x_{n};0,I_{d_{x}})$ . In the following, we assume that we have access to a pre-trained DDM and omit the superscript $\\theta^{\\star}$ from the notation, writing simply $p$ and $\\hat{x}_{0\\mid k}$ when referring to the generative model and the denoiser, respectively. In addition, we denote by $p_{k}$ the $k$ -th marginal of $p_{0:n}$ and write, for all $(\\ell,m)\\,\\in\\,[0,n]^{2}$ such that $\\ell<m$ , $\\begin{array}{r}{p_{\\ell|m}(x_{\\ell}|x_{m}):=\\prod_{k=\\ell}^{m-1}p_{k|k+1}(x_{k}|x_{k+1})}\\end{array}$ . ", "page_idx": 2}, {"type": "text", "text": "Posterior sampling. Let $g_{0}$ be a nonnegative function on $\\mathbb{R}^{d_{x}}$ . When solving Bayesian inverse problems, $g_{0}$ is taken as the likelihood of the signal given the observation specified using the forward model (see the next section). Our objective is to sample from the posterior distribution ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pi_{0}(x_{0}):=g_{0}(x_{0})\\,p_{0}(x_{0})/\\mathcal{Z}\\,,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\begin{array}{r}{\\mathcal{Z}:=\\int g_{0}\\big(x_{0}\\big)\\,p_{0}\\big(x_{0}\\big)\\mathrm{d}x_{0}}\\end{array}$ is the normalizing constant and the prior $p_{\\mathrm{0}}$ is the marginal of (2.1) w.r.t. $x_{0}$ , in which case the posterior (2.3) can be expressed as ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pi_{0}(x_{0})={\\frac{1}{\\mathcal{Z}}}\\int g_{0}(x_{0})\\prod_{k=0}^{n-1}p_{k|k+1}(x_{k}|x_{k+1})\\,p_{n}(x_{n})\\,\\mathrm{d}x_{1:n}\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Thus, Equation (2.3) can be interpreted as the marginal of a time-reversed FK (Feynman\u2013Kac) model with a non-trivial potential only for $k=0$ ; see [13] for a comprehensive introduction to FK models. In this work, we twist, without modifying the law of the FK model, the backward transitions pk|k+1 by artificial positive potentials $(g_{k})_{k=0}^{n}$ , each being a function on $\\mathbb{R}^{d_{x}}$ , and write ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pi_{0}(x_{0})={\\frac{1}{\\mathcal{Z}}}\\int g_{n}(x_{n})\\,p_{n}(x_{n})\\prod_{k=0}^{n-1}{\\frac{g_{k}(x_{k})}{g_{k+1}(x_{k+1})}}\\,p_{k|k+1}(x_{k}|x_{k+1})\\,\\mathrm{d}x_{1:n}\\,.\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "This allows the posterior of interest to be expressed as the time-zero marginal of an FK model with initial distribution $p_{n}$ , Markov transition kernels $(p_{k|k+1})_{k=0}^{n-1}$ , and $(g_{k})_{k=0}^{n}$ . ", "page_idx": 2}, {"type": "text", "text": "Recent works that aim to sample from the posterior (2.3) generally employ the FK representation (2.4). These studies, however, adopt varying auxiliary potentials [10, 47, 60, 4, 54, 59]. FK models can be effectively sampled using sequential Monte Carlo (SMC) methods; see, e.g., [13, 9]. SMC methods sequentially propagate weighted samples, whose associated weighted empirical distributions target the flow of the FK marginal distributions. The effectiveness of this technique depends heavily on the choice of intermediate potentials $(g_{k})_{k=1}^{n}$ , as discussed in [54, 59, 7, 16]. However, SMC methods require a number of samples proportional and often exponential in the dimensionality of the problems hence limiting their application in these setups due to the resulting probabitive memory cost [2]. On the other hand, reducing the number of samples makes them vulnerable to mode collapse. ", "page_idx": 2}, {"type": "text", "text": "In the following, we will focus on a particular choice of potential functions $(g_{k})_{k=1}^{n}$ for which the posterior $\\pi_{0}$ can be expressed as the time-zero marginal distribution of a time-reversed Markov chain. The transition densities of this chain are obtained by twisting the transition densities of the generative model with the considered potential functions. More precisely, define, for all $k$ , the potentials $\\begin{array}{r}{g_{k}^{\\star}(x_{k}):=\\int g_{0}(x_{0})\\,p_{0|k}(x_{0}|x_{k})\\,\\mathrm{d}x_{0}}\\end{array}$ . Note that these potentials satisfy the recursion $\\begin{array}{r}{g_{k+1}^{\\star}(x_{k+1})=\\int g_{k}^{\\star}(x_{k})\\,p_{k|k+1}(x_{k}|x_{k+1})\\,\\mathrm{d}x_{k}}\\end{array}$ . Builing upon that, define the Markov transitions ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pi_{k|k+1}(x_{k}|x_{k+1}):=\\frac{g_{k}^{\\star}(x_{k})}{g_{k+1}^{\\star}(x_{k+1})}\\,p_{k|k+1}(x_{k}|x_{k+1}),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "allowing the posterior (2.4) to be rewritten as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\pi_{0}(x_{0})=\\int\\pi_{n}(x_{n})\\prod_{k=0}^{n-1}\\pi_{k|k+1}(x_{k}|x_{k+1})\\,\\mathrm{d}x_{1:n}\\,,\\quad\\pi_{n}(x_{n})=g_{n}^{\\star}(x_{n})p_{n}(x_{n})/\\mathcal{Z}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "In other words, the distribution $\\pi_{0}$ is the time-zero marginal of a Markov model with transition densities $(\\pi_{k|k+1})_{k=n-1}^{0}$ and initial distribution $\\pi_{n}$ . According to this decomposition, a sample $X_{0}^{\\star}$ from the posterior (2.3) can be obtained by sampling $X_{n}^{\\star}\\sim\\pi_{n}$ and then, recursively sampling $X_{k}^{\\star}\\,\\sim\\,\\pi_{k|k+1}(\\cdot|X_{k+1}^{\\star})$ from $k\\,=\\,n\\mathrm{~-~}1$ till $k\\,=\\,0$ . In practice, however, neither the Markov transition densities $\\pi_{k|k+1}$ nor the probability density function $\\pi_{n}$ are tractable. The main challenge in estimating $\\pi_{k|k+1}$ stems essentially from the intractability of the potential $g_{k}^{\\star}(x_{k})$ as it involves computing an expectation under the high-cost sampling distribution $p_{0|k}(\\cdot|x_{k})$ . ", "page_idx": 3}, {"type": "text", "text": "Recent works have focused on developing tractable approximations of $p_{0|k}(\\cdot|x_{k})$ . For the Diffusion Posterior Sampling (DPS) algorithm [10], the point mass approximation $\\delta_{\\hat{x}_{0\\mid k}(x_{k})}$ of $p_{0|k}(\\cdot|x_{k})$ results in the estimate $\\nabla_{x_{k}}\\log g_{0}(\\hat{x}_{0|k}(x_{k}))$ of $\\nabla_{x_{k}}\\log g_{k}^{\\star}(x_{k})$ . Then, given a sample $X_{k+1}^{!}$ , an approximate sample $X_{k}$ from $\\pi_{k|k+1}\\big(\\cdot|X_{k+1}\\big)$ is obtained by first sampling $\\tilde{X}_{k}\\sim p_{k|k+1}(\\cdot|X_{k+1})$ and then setting ", "page_idx": 3}, {"type": "equation", "text": "$$\nX_{k}=\\tilde{X}_{k}+\\zeta\\nabla_{x_{k+1}}\\log g_{0}(\\hat{x}_{0|k+1}(x_{k+1}))|_{x_{k+1}=X_{k+1}}\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\zeta>0$ is a tuning parameter. As noted in [48, 7, 4], the DPS updates (2.7) do not lead to an accurate approximation of the posterior $\\pi_{0}$ even in the simplest examples; see also Section 4. Alternatively, [47] proposed the Pseudoinverse-Guided Diffusion Model (\u03a0GDM), which uses a Gaussian approximation of $p_{0|k}(\\cdot|x_{k})$ with mean $\\hat{x}_{0\\mid k}(x_{k})$ and diagonal covariance matrix set to $(1-\\alpha_{k})I_{d_{x}}$ , which corresponds to the covariance of $\\overset{\\cdot}{q_{0\\mid k}}(\\cdot|x_{k})$ if $p_{\\mathrm{data}}$ had been a standard Gaussian; see [47, Appendix 1.3]. More recently, [17, 4] proposed to approximate the exact KL projection of $p_{0|k}(x_{0}|x_{k})$ onto the space of Gaussian distributions by noting that both its mean and covariance matrix can be estimated using $\\hat{x}_{0\\mid k}(x_{k})$ and its Jacobian matrix. We discuss in more depth the related works in Appendix B. ", "page_idx": 3}, {"type": "text", "text": "3 The DCPS algorithm ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Smoothing the DPS approximation. The bias of the DPS updates (2.7) stems from the point mass approximation of the conditional distribution $p_{0|k}(\\cdot|x_{k})$ . This approximation becomes more accurate as $k$ tends to zero and is crude otherwise. We aim here to mitigate the resulting approximation errors. A core result that we leverage in this paper is that for any $(k,\\ell)\\in[0,n]^{2}$ such that $\\ell<k$ , we can construct an estimate $\\hat{p}_{\\ell|k}(\\cdot|x_{k})$ of $p_{\\ell|k}(\\cdot|x_{k})$ that bears a smaller ap prox imation error than the estimate $\\delta_{\\hat{x}_{0\\mid k}(x_{k})}$ relatively to $p_{0|k}(\\cdot|x_{k})$ . Formally, let $\\hat{p}_{0|k}(\\cdot|x_{k})$ denote any approximation of $p_{0|k}(\\cdot|x_{k})$ , such as that of the DPS or \u03a0GDM, and define the approximation of $p_{\\ell|k}(\\cdot|x_{k})$ ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{p}_{\\ell|k}(x_{\\ell}|x_{k}):=\\int q_{\\ell|0,k}(x_{\\ell}|x_{0},x_{k})\\hat{p}_{0|k}(x_{0}|x_{k})\\,\\mathrm{d}x_{0}\\,,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $q_{\\ell|0,k}(x_{\\ell}|x_{0},x_{k})$ is defined in (A.4). We then have the following result. ", "page_idx": 3}, {"type": "text", "text": "Proposition 3.1 (informal). Let $k\\in[1,n]$ . For all $\\ell\\in[0,k-1]$ and $\\boldsymbol{x}_{k}\\in\\mathbb{R}^{d_{x}}$ , ", "page_idx": 3}, {"type": "equation", "text": "$$\nW_{2}(\\hat{p}_{\\ell|k}(\\cdot|x_{k}),p_{\\ell|k}(\\cdot|x_{k}))\\leq\\frac{\\sqrt{\\alpha_{\\ell}}(1-\\alpha_{k}/\\alpha_{\\ell})}{(1-\\alpha_{k})}W_{2}(\\hat{p}_{0|k}(\\cdot|x_{k}),p_{0|k}(\\cdot|x_{k}))\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The proof is postponed to Appendix A.3. Note that the ratio in the right-hand-side of (3.2) is less than 1 and decreases as $\\ell$ increases. As an illustration, using the DPS approximation of $p_{0|k}(\\cdot|x_{k})$ , we find that $\\hat{p}_{\\ell|k}(x_{\\ell}|x_{k})=q_{\\ell|0,k}(x_{\\ell}|\\hat{x}_{0|k}(x_{k}),x_{k})$ improves upon DPS in terms of approximation error. ", "page_idx": 3}, {"type": "text", "text": "This observation prompts to consider DPS-like approximations on shorter time intervals; instead of approximating expectations under $p_{0|k}(\\cdot|x_{k})$ , such as the potential $g_{k}^{\\star}(x_{k})$ , we should transform our initial sampling problem so that we only have to estimate expectations under $p_{\\ell|k}(\\cdot|x_{k})$ for any $\\ell$ such that the difference $k-\\ell$ is small. This motivates the blocking approach introduced next. ", "page_idx": 4}, {"type": "text", "text": "Intermediate posteriors. We approach the original problem of sampling from $\\pi_{0}$ via a series of simpler, intermediate posterior sampling problems of increasing difficulty. More precisely, let us consider the intermediate posteriors defined as ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pi_{k_{\\ell}}(x_{k_{\\ell}}):=g_{k_{\\ell}}(x_{k_{\\ell}})p_{k_{\\ell}}(x_{k_{\\ell}})/\\mathcal{Z}_{k_{\\ell}},\\quad\\mathrm{with}\\quad\\mathcal{Z}_{k_{\\ell}}:=\\int g_{k_{\\ell}}(x_{k_{\\ell}})p_{k_{\\ell}}(x_{k_{\\ell}})\\,\\mathrm{d}x_{k_{\\ell}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $(g_{k_{\\ell}})_{\\ell=1}^{L}$ are potential functions designed by the user and $(k_{\\ell})_{\\ell=0}^{L}$ is an increasing sequence in $[0,n]$ such that $k_{0}\\,=\\,0$ and $k_{L}\\,=\\,n$ . Here, $L$ is typically much smaller than $n$ . To obtain an ap p roxi mate sample from $\\pi_{0}=\\pi_{k_{0}}$ , the DCPS algorithm recursively uses an approximate sample $X_{k_{\\ell+1}}$ from $\\pi_{k_{\\ell+1}}$ to obtain an approximate sample $X_{k\\ell}$ from $\\pi_{k_{\\ell}}$ . Indeed, mirroring (2.6) it holds ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pi_{k_{\\ell}}(x_{k_{\\ell}})=\\int\\pi_{k_{\\ell+1}}^{\\ell}(x_{k_{\\ell+1}})\\prod_{m=k_{\\ell}}^{k_{\\ell+1}-1}\\pi_{m|m+1}^{\\ell}(x_{m}|x_{m+1})\\,\\mathrm{d}x_{k_{\\ell}+1:k_{\\ell+1}}\\,,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where for $m\\in[k_{\\ell},k_{\\ell+1}-1]$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\pi_{k_{\\ell+1}}^{\\ell}(x_{k_{\\ell+1}}):=g_{k_{\\ell+1}}^{\\ell,\\star}(x_{k_{\\ell+1}})p_{k_{\\ell+1}}(x_{k_{\\ell+1}})/{\\mathcal Z}_{k_{\\ell}}\\,,}}\\\\ {{\\pi_{m|m+1}^{\\ell}(x_{m}|x_{m+1}):=g_{m}^{\\ell,\\star}(x_{m})p_{m|m+1}(x_{m}|x_{m+1})/g_{m+1}^{\\ell,\\star}(x_{m+1})}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "and for $m\\in[k_{\\ell}+1,k_{\\ell+1}]$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\ng_{m}^{\\ell,\\star}(x_{m}):=\\int g_{k_{\\ell}}(x_{k_{\\ell}})p_{k_{\\ell}|m}(x_{k_{\\ell}}|x_{m})\\,\\mathrm{d}x_{k_{\\ell}}\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "We emphasize that the initial distribution $\\pi_{k_{\\ell+1}}^{\\ell}$ in (3.4) is different from the posterior $\\pi_{k_{\\ell+1}}$ as the former involves the user-defined potential whereas the latter the intractable one. The main advantage of our approach lies in the fact that, unlike the potentials in the transition densities (2.5), which involve expectations under $p_{0|k}(\\cdot|x_{k})$ , the potentials (3.5) are given by expectations under the distributions $p_{k_{\\ell}|m}(\\cdot|x_{m})$ , which are easier to approximate in the light of Proposition 3.1. In the sequel, we use this approximation for the estimation of the potentials (3.5); this yields approximate potentials ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{g}_{m}^{\\ell,\\star}(x_{m}):=\\int g_{k_{\\ell}}(x_{k_{\\ell}})\\hat{p}_{k_{\\ell}|m}(x_{k_{\\ell}}|x_{m})\\,\\mathrm{d}x_{k_{\\ell}}\\,,\\quad m\\in\\ensuremath{[\\![k_{\\ell}+1,k_{\\ell+1}]\\!]}\\,,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which serve as a substitute for the intractable $g_{m}^{\\ell,\\star}$ . Let us now summarize how our algorithm works. Starting from a sample $X_{k_{\\ell+1}}$ , which is approximately distributed according to $\\pi_{k_{\\ell+1}}$ , the next sample $X_{k\\ell}$ is generated in the next two steps: ", "page_idx": 4}, {"type": "text", "text": "1. Perform Langevin Monte Carlo steps initialized at $X_{k_{\\ell+1}}$ and targeting $\\pi_{k_{\\ell+1}}^{\\ell}$ , yielding $X_{k_{\\ell+1}}^{\\ell}$ ", "page_idx": 4}, {"type": "text", "text": "2. Simulate a Markov chain (Xj)jk\u2113=k\u2113+1 initialized with $X_{k_{\\ell+1}}=X_{k_{\\ell+1}}^{\\ell}$ and whose transition from $X_{j+1}$ to $X_{j}$ is the minimizer of ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathsf{K L}(\\lambda_{j\\vert j+1}^{\\varphi}(\\cdot\\vert X_{j+1})\\parallel\\pi_{j\\vert j+1}^{\\ell}(\\cdot\\vert X_{j+1})),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\lambda_{j\\vert j+1}^{\\varphi}$ is a mean-field Gaussian approximation with parameters $\\varphi:=(\\hat{\\mu},\\hat{\\sigma})\\in\\mathbb R^{d_{x}}\\!\\times\\!\\mathbb R_{>0}^{d_{x}}$ ", "page_idx": 4}, {"type": "text", "text": "In the following, we elaborate more on Step 1 and Step 2 and discuss the choice of the intermediate potentials. The pseudo-code of the DCPS algorithm is in Algorithm 1. ", "page_idx": 4}, {"type": "text", "text": "Sampling the initial distribution. In order to perform Step 1, we use the discretized Langevin dynamics [38] with the estimate \u2207log g\u02c6\u2113k,\u2113\u22c6+1 $\\nabla\\log\\hat{g}_{k_{\\ell+1}}^{\\ell,\\star}+\\hat{s}_{k_{\\ell+1}}^{\\phantom{\\dagger}^{\\phantom{\\dagger}}}$ of the score $\\nabla\\log\\pi_{k_{\\ell+1}}^{\\ell}$ . This estimate results from the use of $\\hat{s}_{k_{\\ell+1}}$ as an approximation of $\\nabla\\log p_{k_{\\ell+1}}$ in combination with the approximate potential (3.6). We \u2113t+h1en obtain the approximate sample $\\bar{X}_{k_{\\ell+1}}^{\\ell}$ of $\\pi_{k_{\\ell+1}}$ by running $M$ steps of the tamed unadjusted Langevin (TULA) scheme [5]; see Algorithm 1. Here, the intractability of the involved densities hinder the usage of the Metropolis-Hastings corrections to reduce the inherent bias of the Langevin algorithm. ", "page_idx": 4}, {"type": "text", "text": "Sampling the transitions. We now turn to Step 2. Given $X_{j+1}$ , we optimize the following estimate of Equation (3.7), where we simply replace $g_{j}^{\\ell,\\star}$ by the approximation (3.6): ", "page_idx": 5}, {"type": "equation", "text": "$$\n-\\int\\log\\hat{g}_{j}^{\\ell,\\star}(x_{j})\\lambda_{j|j+1}^{\\varphi}(x_{j}|x_{j+1})\\,\\mathrm{d}x_{j}+\\mathsf{K L}(\\lambda_{j|j+1}^{\\varphi}(\\cdot|x_{j+1})\\parallel p_{j|j+1}(\\cdot|x_{j+1}))\\,.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Letting $\\lambda_{j\\vert j+1}^{\\varphi}(x_{j}\\vert x_{j+1})=\\mathrm{N}(x_{j};\\hat{\\mu}_{j},\\mathrm{diag}(\\mathrm{e}^{\\hat{v}_{j}}))$ , where the variational parameters $\\hat{\\mu}_{j},\\hat{v}_{j}$ are in $\\mathbb{R}_{d_{x}}$ , the previous estimate yields the objective ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{j}(\\hat{\\mu}_{j},\\hat{v}_{j};x_{j+1}):=-\\mathbb{E}\\big[\\log\\hat{g}_{j}^{\\ell,\\star}(\\hat{\\mu}_{j}+\\mathrm{e}^{\\hat{v}_{j}/2}Z)\\big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\frac{\\|\\hat{\\mu}_{j}-\\mu_{j|j+1}(x_{j+1})\\|^{2}}{2\\sigma_{j|j+1}^{2}}-\\frac{1}{2}\\displaystyle\\sum_{i=1}^{d_{x}}\\left(\\hat{v}_{j,i}-\\frac{\\mathrm{e}^{\\hat{v}_{j,i}}}{\\sigma_{j|j+1}^{2}}\\right)\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $Z$ is $d_{x}$ -dimensional standard Gaussian and $\\mu_{j\\vert j+1}(x_{j+1})$ is the mean of (2.2). Note here that we have used the reparameterization trick [26] and the closed-form expression of the KL divergence between two multivariate Gaussian distributions. We optimize the previous objective using a few steps of SGD by estimating the first term on the r.h.s. with a single sample as in [26]. For each $j\\in[k\\ell,k\\ell{+}1-1]$ , we use $\\mu_{j\\vert j+1}$ and $\\log\\sigma_{j\\mid j+1}^{2}$ as initialization for $\\hat{\\mu}_{j}$ and $\\hat{v}_{j}$ . ", "page_idx": 5}, {"type": "text", "text": "Intermediate potentials. Here, we give general guidelines to choose the user-defined potentials $(g_{k_{\\ell}})_{\\ell=1}^{L}$ . Our design choice is to rescale the input and then anneal the initial potential $g_{0}$ . Therefore, we suggest ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r}{g_{k_{\\ell}}(x)=g_{0}(\\frac{x}{\\beta_{k_{\\ell}}})^{\\gamma_{k_{\\ell}}}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "where $\\gamma_{k\\ell},\\beta_{k\\ell}\\,>\\,0$ are tunable paramerters. This design choice is inspired from the tempering sampling scheme [33] which uses the principle of progressively moving an intial distribution to the targeted one. We provide some examples in the case of Bayesian inverse problems where the unobserved signal and the observation are modelled jointly as a realization of $(\\bar{X_{,}}Y)\\sim p(y|x)p_{0}(x)$ , where $p(y|x)$ is the conditional density of $Y$ given $X=x$ . In this case, the posterior $\\pi_{0}$ of $X$ given $Y=y$ is given by (2.3) with $g_{0}(x)=\\dot{p}(y|x)$ . ", "page_idx": 5}, {"type": "text", "text": "Linear inverse problems with Gaussian noise. In this case, $g_{0}(x)\\;=\\;\\mathrm{N}(y;A x,\\sigma_{y}^{2}I_{d_{y}})$ , where $A\\in\\mathbb{R}^{d_{y}\\times d_{x}}$ . Popular applications in image processing\u221a include super-resolution, inpainting, outpainting, and deblurring. We use (3.9) with $(\\beta_{k_{\\ell}},\\gamma_{k_{\\ell}})=(\\sqrt{\\alpha_{k_{\\ell}}},\\alpha_{k_{\\ell}})$ , ", "page_idx": 5}, {"type": "equation", "text": "$$\ng_{k_{\\ell}}(x)=\\mathrm{N}(\\sqrt{\\alpha_{k_{\\ell}}}y;A x,\\sigma_{y}^{2}I_{d_{y}})\\,,\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "which corresponds to the likelihood of $x$ given the pseudo observation $\\sqrt{\\alpha_{k_{\\ell}}}y$ under the same linear observation model that defines $g_{0}$ . This choice of $g_{k_{\\ell}}$ enables exact computation of (3.6) and allows information on the observation $y$ to be taken into account early in the denoising process. ", "page_idx": 5}, {"type": "text", "text": "Low-count (or shot-noise) Poisson denoising. In a Poisson model for an image, the grey levels of the image pixels are modelled as Poisson-distributed random variables. More specifically, let $\\boldsymbol{A}\\;\\in\\;\\mathbb{R}^{d_{\\boldsymbol{y}}\\,\\breve{\\times}d_{\\boldsymbol{x}}}$ be a matrix with nonnegative entries and $x\\ \\in\\ [0,255]^{C\\times H\\times W}$ , where $C$ is the number of channels and $H$ the height and $W$ the width. For every $\\bar{i}\\,\\in\\,[1,d_{y}]$ , $Y_{i}$ is Poissondistributed with mean $(A x)_{i}$ , and the likelihood of $x$ given the observation  is th erefore given by $\\begin{array}{r}{x\\mapsto\\prod_{j=1}^{d_{y}}(\\lambda A x)_{j}^{y_{j}}\\mathrm{e}^{-(\\uplambda A x)_{j}^{\\top}}/y_{j}!}\\end{array}$ where $\\lambda>0$ is the rate. Following [10] we consider as likelihood its normal approximation, i.e. $\\begin{array}{r}{\\tilde{g_{0}}=\\prod_{j=1}^{d_{y}}\\mathrm{N}(y_{j};\\lambda(A x)_{j},y_{j})}\\end{array}$ . This model is relevant for many tasks such as low-count photon imaging  and computed tomography (CT) reconstruction [35, 39, 31]. We use (3.9) with $\\beta_{k_{\\ell}}=\\gamma_{k_{\\ell}}=\\sqrt{\\alpha_{k_{\\ell}}}$ ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\bar{\\sqrt{\\alpha_{k_{\\ell}}}}\\colon\\quad\\mathcal{d}_{y}}\\\\ &{g_{k_{\\ell}}(x)=\\prod_{j=1}^{d_{y}}\\mathrm{N}(\\sqrt{\\alpha_{k_{\\ell}}}y_{j};\\lambda(A x)_{j},\\sqrt{\\alpha_{k_{\\ell}}}y_{j})\\,.}\\\\ &{\\sim\\quad\\cdots\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "JPEG dequantization. JPEG [57] is a ubiquitous method for lossy compression of images. Use $h_{q}$ to denote the JPEG encoding function with quality factor $q\\in[0,\\dot{1}00]$ , where a small $q$ is associated with high compression. Denote by $h_{\\boldsymbol{q}}^{\\dagger}$ the JPEG decoding f unction  that returns an image in RGB space with a certain loss of detail, depending on the degree of compression $q$ , compared to the original image. Since we require the potential to be differentiable almost everywhere, we use the differentiable approximation of JPEG developed in [44], which replaces the rounding function used in the quantization matrix with a differentiable approximation that has non-zero derivatives almost everywhere. In this case, $g_{0}(x)=\\mathrm{N}(h_{q}^{\\dagger}(y);h_{q}^{\\dagger}(h_{q}^{\\dot{\\bf{\\alpha}}}(x)),\\sigma_{y}^{2}I_{d_{y}})$ , where $y$ is in YCbCr space. Combining this with Equation (3.9) with $(\\beta_{k_{\\ell}},\\gamma_{k_{\\ell}})=(\\alpha_{k_{\\ell}},\\alpha_{k_{\\ell}})$ and assum\u221aing that the composition $h_{a}^{\\dagger}\\circ h_{q}$ is a homogenious map, the intermediate potentials are $\\begin{array}{r}{\\dot{g_{k_{\\ell}}}(x)=\\mathrm{N}(\\sqrt{\\alpha_{k_{\\ell}}}\\,h_{q}^{\\dagger}(y);h_{q}^{\\dagger}(\\hat{h}_{q}(x)),\\sigma_{y}^{2}I_{d_{x}})^{\\prime}}\\end{array}$ . ", "page_idx": 5}, {"type": "image", "img_path": "BOrut7M2X7/tmp/b398a4909f88caa227eea720c6e4b53b5b783e775b086d69bdbb46768a1cfafe.jpg", "img_caption": ["Figure 1: First two dimensions of samples (in red) from each algorithm on the 25 component Gaussian mixture posterior sampling problem with $(d_{x},\\stackrel{\\cdot}{d}_{y})=(100,1)$ . The true posterior samples are given in blue. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "In this section, we demonstrate the performance of DCPS and compare it with DPS [10], \u03a0GDM [47], DDRM [24], REDDIFF [32], and MCGDIFF [7] on several Bayesian inverse problems. We also benchmark our algorithm against DIFFPIR [62], DDNM [58], FPS [16], and SDA [42] but we defer the results to the Appendix C.5. ", "page_idx": 6}, {"type": "text", "text": "First, we consider a simple toy experiment in which the posterior distribution is available in closed form. Next, we apply our algorithm to superresolution (SR $4\\times$ and $16\\times\\rangle$ ), inpainting and outpainting tasks with Gaussian and Poisson noise, and JPEG dequantization. For these imaging experiments, we use the FFHQ256 [23] and ImageNet256 [14] datasets and the publicly available pre-trained models of [8] and [15]. Finally, we benchmark our method on a trajectory inpainting task using the pedestrian dataset UCY for which we have trained a Diffusion model. All details can be found in Appendix C.1. ", "page_idx": 6}, {"type": "text", "text": "Gaussian mixture. We first evaluate the accuracy of DCPS on a linear inverse problem with a Gaussian mixture (GM) prior, for which the posterior can be explicitly computed: it is also a Gaussian mixture whose means, covariance matrices, and weights are in a closed form; see Appendix C.2. In this case, the predictor $\\hat{x}_{0\\mid k}^{\\theta^{*}}$ is available in a closed form; ", "page_idx": 6}, {"type": "text", "text": "see Appendix C.2 for more details. We consider a Gaussian mixture prior with 25 components in dimensions $d_{x}=10$ and $d_{x}=100$ . The potential is $g_{0}(x)=\\mathrm{N}(y;A x,\\sigma_{y}^{2}I_{d_{y}})$ with $d_{y}\\,=\\,1\\$ and $A$ is a $1\\times d_{x}$ vector. The results are averaged over 30 randomly generated replicates of the measurement model $(y,A,\\sigma_{y}^{2})$ and the mixture weights. Then, for each pair of prior distribution and measurement model, we generate $N_{s}=2000$ samples with each algorithm and compare them with $N_{s}$ samples from the true posterior distribution using the sliced Wasserstein (SW) distance. For DCPS, we used $L=3$ blocks and $K=2$ gradient steps, respectively, and compared two configurations, denoted by $\\mathrm{DCPS}_{50}$ and $\\mathrm{DCPS}_{\\mathrm{500}}$ , of the algorithm with $M=50$ and $M=500$ Langevin steps, respectively. See Algorithm 1. The results are reported in Table 1. It is worthwhile to note that DCPS outperforms all baselines except for MCGDIFF. However, by increasing the number of Langevin steps, its performance closely matches that of MCGDIFF. ", "page_idx": 6}, {"type": "table", "img_path": "BOrut7M2X7/tmp/92234d526616cf2ed7e96e840edb0616346c45be2c2e74a57999821ebcda3ff7.jpg", "table_caption": ["Table $1!\\,95\\%$ confidence interval for the SW on the GM experiment. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Imaging experiment. Table 2 reports the results for the linear inverse problems with Gaussian noise with two noise variance levels $\\sigma_{y}=0.05$ and $\\sigma_{y}=0.3$ , Table 3 for the JPEG dequantization problem with $\\sigma_{y}~=~10^{-3}$ , $\\mathrm{QF}\\in\\{2,8\\}$ , and Table 6 for the Poisson denoising task with rate $\\lambda=0.1$ . For all tasks and datasets, we use the same parameters for DCPS and therefore do not perform any task or dataset-specific tuning. We use $L\\,=\\,3$ , $K\\,=\\,2$ gradient steps, and $M=5$ Langevin steps. To ensure a fair comparison with DPS and \u03a0GDM we use 300 DDPM steps for DCPS and 1000 steps for both DPS and \u03a0GDM, which ensures that all the algorithms have the same runtime and memory footprint; see Table 4. For MCGDIFF, which has a large memory requirement, we use $N\\,=\\,32$ particles in the SMC sampling step and then randomly draw one sample from the resulting particle approximation of the posterior. Finally, for DDRM we use 200 diffusion steps and for REDDIFF we use 1000 gradient steps and the parameters recommended in the original paper. We provide the implementation details for all algorithms in Appendix C.1. ", "page_idx": 6}, {"type": "image", "img_path": "BOrut7M2X7/tmp/4dae421d1e42e19e85b631a559a5e42d98e68e04c5b3a8d6eebca003ece247a7.jpg", "img_caption": ["Figure 2: Sample images for inpainting with center, half, expand masks and for Super Resolution with $4\\times$ and $16\\times$ factors. On the left: FFHQ dataset and on the right ImageNet dataset. "], "img_footnote": ["10, Appendix D.1], and found, via a grid search, that the same value is also effective for the other tasks. "], "page_idx": 7}, {"type": "text", "text": "For the JPEG dequantization task, we use $\\sigma_{y}=10^{\\mathrm{{-3}}}$ and $\\lambda=0.1$ . We only benchmark our method against \u03a0GDM and REDDIFF, since MCGDIFF and DDRM do not handle non-linear inverse problems. We did not include DPS in our benchmark because we have not managed to find a suitable choice of hyperparameters to achieve reasonable results. Finally, for the Poissonshot noise case, we compare against DPS. We use the step size for super-resolution recommended in the original paper [see ", "page_idx": 7}, {"type": "table", "img_path": "BOrut7M2X7/tmp/7a0e7d887fa50a15f1a5ff15f7b234875181ea5ec38ba8dba5adc8a25cb9b7a2.jpg", "table_caption": ["Table 2: Mean LPIPS value on different tasks. Lower is better. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "Evaluation. As shown in Table 2, DCPS outperforms the other baselines on 13 out of 16 tasks and has the best average performance. In particular, it compares favorably with \u03a0GDM and DPS, its closest competitors, while exhibiting the same runtime and memory requirements; see Table 4, where we give the average runtime and memory usage for each algorithm. The memory consumption is measured by how many samples each algorithm can generate in parallel on a single 48GB L40S NVIDIA GPU for the Diffusion model trained on FFHQ [15]. We emphasize that DCPS is more robust to larger noise levels than \u03a0GDM and REDDIFF, as evidenced by the large increase in the LPIPS value for these algorithms in the case $\\sigma_{y}=0.3$ . On the JPEG dequantization task (Table 3), DCPS also shows better performance than these algorithms and even more so for the high compression level $\\mathrm{QF}=2$ ). On the Poisson-shot noise tasks, DCPS outperforms DPS by a significant margin; see Table 6. Finally, we display various reconstructions obtained with each algorithm. More specifically, we have generated 4 samples each, with the same seed. Figure 2 displays the first sample and the remaining ones are deferred to Appendix D. For MCGDIFF we show 4 random samples of the same particle filter. Due to the collapse of the particle filter in very large dimensions [2], they are all similar. Surprisingly, the samples produced by DDRM and REDDIFF for the outpainting tasks also show striking similarities, although the samples have been drawn independently. ", "page_idx": 7}, {"type": "table", "img_path": "BOrut7M2X7/tmp/f5517cfa4a7613bee275427b601b8305fee70b05fa3f0a606dc1b4268fb86fe9.jpg", "table_caption": ["Table 3: Mean LPIPS value on JPEG dequantization. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "BOrut7M2X7/tmp/b1199828ba255d1984891f139a2ae8b4e70aa40de76365a23249a2fde92cf41c.jpg", "img_caption": ["Figure 3: Left: JPEG dequantization with $\\mathrm{QF}=2$ . Middle: Poisson denoising. Right: SR $4\\times$ Poisson denoising. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Trajectory prediction. We evaluate our algorithm on the UCY dataset consisting of pedestrian trajectories, encoded as 2D time series with 20 time steps [27, 29, 18, 30]. We pre-train a trajectory model on this dataset and then use it for trajectory reconstruction tasks. The model architecture and implementation are detailed in Appendix C.4. We focus on the completion of trajectories where only a few timesteps are observed. The missing steps are filled in based on the observations and the pre-trained prior model, similar to the inpainting task in the previous section. We use MCGDIFF with 5000 particles to obtain approximate samples from the posterior. Indeed, as the dimension of the observation space is low $\\!\\!d_{x}=40\\!\\!$ ) and MCGDIFF is asymptotically exact as the number of particles tends to infinity, it yields an accurate approximation of the posterior; see [7, Proposition 2.1]. Then, we compute the $\\ell_{2}$ distance between the median, quantile 25, and quantile 75 of the MCGDIFF samples and the reconstructions of each algorithm. We report these results in Table 5. Finally, in Figure 4 we illustrate the reconstructed trajectories on a specific trajectory completion problem. ", "page_idx": 8}, {"type": "image", "img_path": "BOrut7M2X7/tmp/e81252b5fd71c7d1c9dbc287e87c5ef35987a7c6d357d343b6b21b8e33ce5965.jpg", "img_caption": ["Table 4: LPIPS metric against the runtime and memory cost of the algorithms. "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "BOrut7M2X7/tmp/c2a8c013c4d204142f54b9f19d44ac900552451e071b9f98f8708f2bc2e177af.jpg", "table_caption": ["Table 5: $\\ell_{2}$ distance quantiles with MCGDIFF as reference. "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "BOrut7M2X7/tmp/3c4c964493024005570c5947090aace589cb135ef237184686464365bcafa633.jpg", "img_caption": ["Figure 4: Trajectory completion where only the middle part of the trajectory is observed. The figures in the $1^{\\mathrm{st}}$ row display 3 reconstructions per algorithm. The $2^{\\mathrm{nd}}$ and $3^{\\mathrm{rd}}$ rows show confidence intervals across different time steps. The Groundtruth is a trajectory taken from the UCY dataset. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "5 Conclusion. ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In this paper, we introduce DCPS to handle Bayesian linear inverse problems with DDM priors without the need for problem-specific additional training. Our divide-and-conquer strategy helps to reduce the approximation error of existing approaches, and our variational framework provides a principled method for estimating the backward kernels. DCPS applies to various relevant inverse problems and is competitive with existing methods. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Limitations and future directions. Our method has some limitations that shed light on opportunities for further development and refinement. First, the intermediate potentials that we considered were specifically designed for each problem, meaning our method is not universally applicable to all inverse problems. For instance, our approach can not be applied to for linear inverse problems using latent diffusion models [41] since there is no clear choice of intermediate potentials. Therefore, in our opinion, deriving a learning procedure that is capable to automatically design effective intermediate potentials applicable to any $g_{0}$ is an important research direction. Moreover, there is an aspect of the choice of the intermediate potentials and the number of blocks $L$ that remains to be understood properly. Indeed, while our backward approximations reduce the local approximation errors w.r.t. DPS and \u03a0GDM; nonetheless DCPS requires appropriate intermediate potentials in order to perform well. DCPS can still provide decent performance with irrelevant intermediate potentials as long as the number of Langevin steps, in-between the blocks, is large enough. Finally, although our method provides decent results with the same computational cost as DPS and \u03a0GDM, it remains slower than REDDIFF and DDRM which which do not compute vector-jacobian product over the denoiser. Therefore, overcoming this bottleneck when optimizing the KL objective would be a significant improvement for our method. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments. The work of Y.J. and B.M. has been supported by Technology Innovation Institute (TII), project Fed2Learn. The work of Eric Moulines has been partly funded by the European Union (ERC-2022-SYG-OCEAN-101071601). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Sch\u00f6nlieb, and Christian Etmann. Conditional image generation with score-based diffusion models. arXiv preprint arXiv:2111.13606, 2021. [2] P. Bickel, B. Li, and T. Bengtsson. Sharp failure rates for the bootstrap particle filter in high dimensions. In B. Clarke and S. Ghosal, editors, Pushing the Limits of Contemporary Statistics: Contributions in Honor of Jayanta K. Ghosh, pages 318\u2013329. Institute of Mathematical Statistics, 2008. [3] Christopher M. Bishop. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag, Berlin, Heidelberg, 2006. [4] Benjamin Boys, Mark Girolami, Jakiw Pidstrigach, Sebastian Reich, Alan Mosca, and O Deniz Akyildiz. Tweedie moment projected diffusions for inverse problems. arXiv preprint arXiv:2310.06721, 2023. [5] Nicolas Brosse, Alain Durmus, \u00c9ric Moulines, and Sotirios Sabanis. The tamed unadjusted langevin algorithm. Stochastic Processes and their Applications, 129(10):3638\u20133663, 2019.   \n[6] Daniela Calvetti and Erkki Somersalo. Inverse problems: From regularization to Bayesian inference. Wiley Interdisciplinary Reviews: Computational Statistics, 10(3):e1427, 2018. [7] Gabriel Cardoso, Yazid Janati, Eric Moulines, and Sylvain Le Corff. Monte carlo guided denoising diffusion models for bayesian linear inverse problems. In The Twelfth International Conference on Learning Representations, 2024. [8] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. Ilvr: Conditioning method for denoising diffusion probabilistic models. arXiv preprint arXiv:2108.02938, 2021. [9] Nicolas Chopin, Omiros Papaspiliopoulos, et al. An introduction to sequential Monte Carlo, volume 4. Springer, 2020.   \n[10] Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems. In The Eleventh International Conference on Learning Representations, 2023.   \n[11] Hyungjin Chung, Byeongsu Sim, and Jong Chul Ye. Come-closer-diffuse-faster: Accelerating conditional diffusion models for inverse problems through stochastic contraction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12413\u201312422, 2022.   \n[12] Masoumeh Dashti and Andrew M. Stuart. The Bayesian Approach to Inverse Problems, pages 311\u2013428. Springer International Publishing, Cham, 2017.   \n[13] Pierre Del Moral. Feynman-kac formulae. In Feynman-Kac Formulae, pages 47\u201393. Springer, 2004.   \n[14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[15] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780\u20138794, 2021.   \n[16] Zehao Dou and Yang Song. Diffusion posterior sampling for linear inverse problem solving: A filtering perspective. In The Twelfth International Conference on Learning Representations, 2024.   \n[17] Marc Anton Finzi, Anudhyan Boral, Andrew Gordon Wilson, Fei Sha, and Leonardo ZepedaN\u00fa\u00f1ez. User-defined event sampling and uncertainty quantification in diffusion models for physical dynamical systems. In International Conference on Machine Learning, pages 10136\u2013 10152. PMLR, 2023.   \n[18] Tianpei Gu, Guangyi Chen, Junlong Li, Chunze Lin, Yongming Rao, Jie Zhou, and Jiwen Lu. Stochastic trajectory prediction via motion indeterminacy diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17113\u201317122, 2022.   \n[19] Bichuan Guo, Yuxing Han, and Jiangtao Wen. Agem: Solving linear inverse problems via deep priors and sampling. Advances in Neural Information Processing Systems, 32, 2019.   \n[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840\u20136851, 2020.   \n[21] Aapo Hyv\u00e4rinen. Some extensions of score matching. Computational statistics & data analysis, 51(5):2499\u20132512, 2007.   \n[22] Bowen Jing, Gabriele Corso, Renato Berlinghieri, and Tommi Jaakkola. Subspace diffusion generative models. In European Conference on Computer Vision, pages 274\u2013289. Springer, 2022.   \n[23] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4401\u20134410, 2019.   \n[24] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restoration models. Advances in Neural Information Processing Systems, 35:23593\u201323606, 2022.   \n[25] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization.   \n[26] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.   \n[27] Alon Lerner, Yiorgos Chrysanthou, and Dani Lischinski. Crowds by example. In Computer graphics forum, 2007.   \n[28] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016.   \n[29] Karttikeya Mangalam, Yang An, Harshayu Girase, and Jitendra Malik. From goals, waypoints & paths to long term human trajectory forecasting. In IEEE/CVF, 2021.   \n[30] Weibo Mao, Chenxin Xu, Qi Zhu, Siheng Chen, and Yanfeng Wang. Leapfrog diffusion model for stochastic trajectory prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5517\u20135526, 2023.   \n[31] Willem Marais and Rebecca Willett. Proximal-gradient methods for poisson image reconstruction with bm3d-based regularization. In 2017 IEEE 7th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP), pages 1\u20135. IEEE, 2017.   \n[32] Morteza Mardani, Jiaming Song, Jan Kautz, and Arash Vahdat. A variational perspective on solving inverse problems with diffusion models. In The Twelfth International Conference on Learning Representations, 2024.   \n[33] Radford M Neal. Annealed importance sampling. Statistics and computing, 11:125\u2013139, 2001.   \n[34] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International conference on machine learning, pages 8162\u20138171. PMLR, 2021.   \n[35] Robert D Nowak and Eric D Kolaczyk. A statistical multiscale framework for poisson inverse problems. IEEE Transactions on Information Theory, 46(5):1811\u20131825, 2000.   \n[36] Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo. Exploiting deep generative prior for versatile image restoration and manipulation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(11):7474\u20137489, 2021.   \n[37] Michael K Pitt and Neil Shephard. Filtering via simulation: Auxiliary particle filters. J. Amer. Statist. Assoc., 94(446):590\u2013599, 1999.   \n[38] Gareth O. Roberts and Richard L. Tweedie. Geometric convergence and central limit theorems for multidimensional Hastings and Metropolis algorithms. Biometrika, 83:95\u2013110, 1996.   \n[39] Isabel Rodrigues, Joao Sanches, and Jose Bioucas-Dias. Denoising of medical images corrupted by poisson noise. In 2008 15th IEEE international conference on image processing, pages 1756\u20131759. IEEE, 2008.   \n[40] Yaniv Romano, Michael Elad, and Peyman Milanfar. The little engine that could: Regularization by denoising (red). SIAM Journal on Imaging Sciences, 10(4):1804\u20131844, 2017.   \n[41] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.   \n[42] Franccois Rozet and Gilles Louppe. Score-based data assimilation. Advances in Neural Information Processing Systems, 36:40521\u201340541, 2023.   \n[43] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(4):4713\u20134726, 2022.   \n[44] Richard Shin and Dawn Song. Jpeg-resistant adversarial images. In NIPS 2017 workshop on machine learning and computer security, volume 1, page 8, 2017.   \n[45] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256\u20132265. PMLR, 2015.   \n[46] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021.   \n[47] Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. Pseudoinverse-guided diffusion models for inverse problems. In International Conference on Learning Representations, 2023.   \n[48] Jiaming Song, Qinsheng Zhang, Hongxu Yin, Morteza Mardani, Ming-Yu Liu, Jan Kautz, Yongxin Chen, and Arash Vahdat. Loss-guided diffusion models for plug-and-play controllable generation. In International Conference on Machine Learning, pages 32483\u201332498. PMLR, 2023.   \n[49] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. Advances in Neural Information Processing Systems, 34:1415\u2013 1428, 2021.   \n[50] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019.   \n[51] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.   \n[52] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021.   \n[53] Andrew M Stuart. Inverse problems: a Bayesian perspective. Acta numerica, 19:451\u2013559, 2010.   \n[54] Brian L. Trippe, Jason Yim, Doug Tischer, David Baker, Tamara Broderick, Regina Barzilay, and Tommi S. Jaakkola. Diffusion probabilistic modeling of protein backbones in 3d for the motifscaffolding problem. In The Eleventh International Conference on Learning Representations, 2023.   \n[55] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep image prior. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 9446\u20139454, 2018.   \n[56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \n[57] Gregory K Wallace. The jpeg still picture compression standard. IEEE transactions on consumer electronics, 38(1):xviii\u2013xxxiv, 1992.   \n[58] Yinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot image restoration using denoising diffusion null-space model. In The Eleventh International Conference on Learning Representations, 2023.   \n[59] Luhuan Wu, Brian L. Trippe, Christian A Naesseth, John Patrick Cunningham, and David Blei. Practical and asymptotically exact conditional sampling in diffusion models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.   \n[60] Guanhua Zhang, Jiabao Ji, Yang Zhang, Mo Yu, Tommi Jaakkola, and Shiyu Chang. Towards coherent image inpainting using denoising diffusion implicit models. In International Conference on Machine Learning, pages 41164\u201341193. PMLR, 2023.   \n[61] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586\u2013595, 2018.   \n[62] Yuanzhi Zhu, Kai Zhang, Jingyun Liang, Jiezhang Cao, Bihan Wen, Radu Timofte, and Luc Van Gool. Denoising diffusion models for plug-and-play image restoration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1219\u20131229, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Methodology details ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Denoising Diffusion models ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "DDMs learn a sequence $(\\hat{x}_{0|t}^{\\theta})_{t=1}^{T}$ of denoisers by minimizing, using SGD, the objective ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\sum_{t=1}^{T}w_{t}\\mathbb{E}\\left[\\|\\epsilon_{t}-\\hat{\\epsilon}_{t}^{\\theta}(\\sqrt{\\alpha_{t}}X_{0}+\\sqrt{1-\\alpha_{t}}\\epsilon_{t})\\|^{2}\\right]\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "w.r.t. the neural network parameter $\\theta$ , where $(\\epsilon_{t})_{t=1}^{T}$ are i.i.d. standard normal vectors and $(w_{t})_{t=1}^{T}$ are some nonnegative weights. We denote by $\\theta^{\\star}$ an estimator of the minimizer of the previous loss. Having access to $\\theta^{\\star}$ , we can define a generative model for $p_{\\mathrm{data}}$ . Let $(t_{k})_{k=0}^{n}$ be an increasing sequence of time instants in $[0,T]$ with $t_{0}=0$ . We assume that $t_{n}$ is large enough so that $q_{t_{n}}$ is approximately multivariate stand a rd no rmal. For convenience, we assign the index $k$ to any quantity depending on $t_{k};e.g.$ , we denote $p_{t_{k}}$ by $p_{k}$ . For $(j,k)\\in[1,n-1]^{2}$ such that $j<k$ , define ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\displaystyle\\mu_{j|0,k}(x_{0},x_{k}):=\\frac{\\sqrt{\\alpha_{j}}(1-\\alpha_{k}/\\alpha_{j})}{1-\\alpha_{k}}x_{0}+\\frac{\\sqrt{\\alpha_{k}/\\alpha_{j}}(1-\\alpha_{j})}{1-\\alpha_{k}}x_{k}\\,,}\\\\ {\\displaystyle\\sigma_{j|k}^{2}:=\\frac{(1-\\alpha_{j})(1-\\alpha_{k}/\\alpha_{j})}{1-\\alpha_{k}}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Then the bridge kernel ", "page_idx": 14}, {"type": "equation", "text": "$$\nq_{j|0,k}(x_{j}|x_{0},x_{k})=q_{j|0}(x_{j}|x_{0})q_{k|j}(x_{k}|x_{j})/q_{k|0}(x_{k}|x_{0})\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "is a Gaussian distribution with mean $\\mu_{j\\vert0,k}(x_{0},x_{k})$ and covariance $\\sigma_{j\\vert k}^{2}I_{d_{x}}$ . DDPM [20] posits the following variational approximation ", "page_idx": 14}, {"type": "equation", "text": "$$\np_{0:n}^{\\theta}(x_{0:n})=p_{n}(x_{n})\\prod_{k=0}^{n-1}p_{k|k+1}^{\\theta}(x_{k}|x_{k+1})\\,,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $p_{k|k+1}^{\\theta}(x_{k}|x_{k+1})\\,=\\,q_{k|0,k+1}(x_{k}|\\hat{x}_{0|k+1}^{\\theta}(x_{k+1}),x_{k+1})$ and $p_{0|1}^{\\theta}(\\cdot|x_{1})\\,=\\,\\delta_{\\hat{x}_{0|1}^{\\theta}(x_{1})}$ . An efficient generative model is then obtained by plugging in the parameter $\\theta^{\\star}$ . ", "page_idx": 14}, {"type": "text", "text": "A.2 Further details on DCPS ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this section we provide further details on Steps 1 and 2 detailed in the main paper. The complete algorithm is given in Algorithm 1. ", "page_idx": 14}, {"type": "text", "text": "Tamed unadjusted Langevin. For the tamed unadjusted Langevin steps we simulate the Markov chain $(\\tilde{X}_{j})_{j=0}^{M}$ where ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\tilde{X}_{j+1}=\\tilde{X}_{j}+\\gamma G_{\\gamma}^{\\ell}(\\tilde{X}_{j})+\\sqrt{2\\gamma}Z_{j}\\,,\\quad\\tilde{X}_{0}=X_{\\ell}+1\\,,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and $(Z_{j})_{j=0}^{M-1}$ are i.i.d. $d_{x}$ -dimensional standard normal, $X_{\\ell}+1$ is an approximate sample from $\\pi_{\\ell+1}$ obtained from the previous iteration of the algorithm, and for all $x\\in\\mathbb{R}^{d_{x}}$ and $\\gamma>0$ , ", "page_idx": 14}, {"type": "equation", "text": "$$\nG_{\\gamma}^{\\ell}(x):=\\frac{\\nabla\\log\\hat{g}_{\\ell+1}^{\\ell,\\star}(x)+\\hat{s}_{\\ell+1}(x)}{1+\\gamma\\|\\nabla\\log\\hat{g}_{\\ell+1}^{\\ell,\\star}(x)+\\hat{s}_{\\ell+1}(x)\\|}\\,.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "We then set $X_{\\ell+1}^{\\ell}:=\\tilde{X}_{M}$ , which serves as an initialization of the Markov chain in Step 2. ", "page_idx": 14}, {"type": "text", "text": "Potential computation. In order to perform the tamed Langevin steps and to optimize the variational approximation using the criterion (3.8), it is crucial to be able to compute exactly the potential (3.6). The optimal potentials we have proposed for both linear inverse problems with Gaussian noise (3.10) and low-count Poisson denoising (3.11) (for $\\ell>0$ ) are available in a closed form: ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\hat{g}_{j}^{\\ell,\\star}(x_{j})=\\mathrm{N}(\\sqrt{\\alpha_{\\ell}}\\,y,A\\mu_{\\ell|j}(x_{j}),\\Sigma_{j}^{\\ell})\\,,\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Sigma_{j}^{\\ell}=\\sigma_{\\ell|j}^{2}A A^{\\mathsf{T}}+\\sigma_{y}^{2}I_{d_{y}}\\,,}\\\\ &{\\Sigma_{j}^{\\ell}=\\sigma_{\\ell|j}^{2}A A^{\\mathsf{T}}+\\sqrt{\\alpha_{\\ell}}\\mathrm{diag}(y)\\,,\\quad\\ell>0\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "(Linear inverse problem) ", "page_idx": 15}, {"type": "text", "text": "$\\mu_{\\ell|j}(x_{j})\\,:=\\,\\mu_{\\ell|0,j}(\\hat{x}_{0|j}(x_{j}),x_{j})$ , and $\\sigma_{\\ell|j}^{2}$ is defined in (A.2). As a result, the first term of the variational criterion $\\mathcal{L}(\\hat{\\mu}_{j},\\hat{v}_{j};x_{j+1})$ in (3.8), given by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[\\log\\hat{g}_{j}^{\\ell,\\star}(\\hat{\\mu}_{j}+\\mathrm{e}^{\\hat{\\upsilon}_{j}/2}Z)\\big]=\\int\\log\\hat{g}_{j}^{\\ell,\\star}(x_{j})\\lambda_{j|j+1}^{\\varphi}(x_{j}|x_{j+1})\\,\\mathrm{d}x_{j},\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "can be computed exactly. Indeed, as $\\mu_{\\ell|j}$ is a linear function of $x_{j}$ , this expectation is simply that of a quadratic function under a Gaussian density, given by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbb{E}\\big[\\log\\hat{g}_{j}^{\\ell,\\star}(\\hat{\\mu}_{j}+\\mathrm{e}^{\\hat{\\nu}_{j}/2}Z)\\big]=-\\frac{1}{2}\\bigg[\\big\\|\\sqrt{\\alpha_{\\ell}}\\,y-A\\mu_{\\ell|j}(\\hat{\\mu}_{j})\\big\\|_{(\\Sigma_{j}^{\\ell})^{-1}}^{2}+\\mathrm{tr}\\big((\\Sigma_{j}^{\\ell})^{-1}\\mathrm{diag}(\\mathrm{e}^{\\hat{\\nu}_{j}})\\big)\\bigg]+C\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Hence, for these cases, (3.8) has a closed-form expression. However, it involves the computation of an inverse matrix which, for many problems, can be prohibitively expensive. To avoid this inversion, we instead optimize a biased estimate of $\\mathcal{L}_{j}(\\hat{\\mu}_{j},\\hat{v}_{j};x_{j+1})$ obtained by drawing two noise vectors $(Z,Z^{\\prime})\\sim\\mathrm{N}\\bar{(}0_{d_{x}},I_{d_{x}})$ and setting ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widetilde{\\mathcal{L}}_{j}(\\hat{\\mu}_{j},\\hat{v}_{j};x_{j+1}):=-\\log g_{\\ell}(\\mu_{\\ell\\vert j}(\\hat{\\mu}_{j}+\\mathrm{e}^{\\hat{v}_{j}/2}Z)+\\sigma_{\\ell\\vert j}^{2}Z^{\\prime})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad+\\displaystyle\\frac{\\Vert\\hat{\\mu}_{j}-\\mu_{j\\vert j+1}(x_{j+1})\\Vert^{2}}{2\\sigma_{j\\vert j+1}^{2}}-\\frac{1}{2}\\displaystyle\\sum_{i=1}^{d_{x}}\\left(\\hat{v}_{j,i}-\\frac{\\mathrm{e}^{\\hat{v}_{j,i}}}{\\sigma_{j\\vert j+1}^{2}}\\right)\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This estimator is computable for any choice choice of potential and we have found in practice that it is sufficient to ensure good enough performance for our algorithm. Regarding the tamed unadjusted Langevin steps, we use the same biased estimate when the matrix inversions are expensive to compute; i.e. at each Langevin step, we approximate $G_{\\gamma}^{\\ell}(\\tilde{X}_{j})$ by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\widetilde{G}_{\\gamma}^{\\ell}(\\tilde{X}_{j}):=\\frac{\\nabla_{x_{\\ell}+1}\\log g_{\\ell}(\\mu_{\\ell|\\ell+1}(x_{\\ell+1})+\\sigma_{\\ell|\\ell+1}\\tilde{Z}_{\\ell})+\\hat{s}_{\\ell+1}(x_{\\ell}+1)}{\\|\\nabla_{x_{\\ell}+1}\\log g_{\\ell}(\\mu_{\\ell|\\ell+1}(x_{\\ell+1})+\\sigma_{\\ell|\\ell+1}\\tilde{Z}_{\\ell})+\\hat{s}_{\\ell+1}(x_{\\ell}+1)\\|}\\,.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Algorithm 1 DIVIDE-AND-CONQUER POSTERIOR SAMPLER (DCPS) ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Input: timesteps $(k_{\\ell})_{\\ell=0}^{L}$ , learning-rate $\\zeta$ , numbers $K$ and $M$ of gradient and Langevin steps,   \nrespectively.   \nInitial sample $X_{k_{L}}\\sim\\mathcal{N}(0_{d_{x}},I_{d_{x}})$ ;   \nfor $\\ell=L-1$ to 0 do Draw $Z\\sim\\mathrm{N}(0_{d_{x}},I_{d_{x}})$ and compute $\\widetilde{G}_{\\gamma}^{\\ell}(X_{k_{\\ell+1}}^{\\ell})$ (A.9); $X_{k_{\\ell+1}}^{\\ell}\\leftarrow X_{k_{\\ell+1}}$ for $i=1$ to $M$ do $Z\\sim\\mathrm{N}(0_{d_{x}},I_{d_{x}})$ ; $X_{k_{\\ell+1}}^{\\ell}\\leftarrow\\bar{X}_{k_{\\ell+1}}^{\\ell}+\\gamma\\widetilde{G}_{\\gamma}^{\\ell}(X_{k_{\\ell+1}}^{\\ell})+\\sqrt{2\\gamma}Z;$ end for for $j=k_{\\ell+1}-1$ to $k_{\\ell}$ do $\\hat{\\mu}_{j}\\leftarrow\\mu_{j|j+1}(X_{j+1}^{\\ell});\\quad\\hat{v}_{j}\\leftarrow\\log\\sigma_{j|j+1}^{2}\\cdot{\\bf1}_{d_{x}};$ for $r=1$ to $K$ do Draw $(Z,Z^{\\prime})\\sim\\mathrm{N}(0_{d_{x}},I_{d_{x}})$ and compute $\\widetilde{\\mathcal{L}}_{j}(\\hat{\\mu}_{j},\\hat{v}_{j};X_{j+1}^{\\ell})$ (A.8); $\\begin{array}{r}{\\!\\!\\left[\\widehat{\\mu}_{j}\\right]\\leftarrow\\left[\\widehat{\\mu}_{j}\\right]-\\zeta\\|\\nabla_{\\hat{\\mu}_{j},\\hat{\\nu}_{j}}\\widetilde{C}_{j}(\\hat{\\mu}_{j},\\hat{\\nu}_{j};X_{j+1}^{\\ell})\\|^{-1}\\nabla_{\\hat{\\mu}_{j},\\hat{\\nu}_{j}}\\widetilde{C}_{j}(\\hat{\\mu}_{j},\\hat{\\nu}_{j};X_{j+1}^{\\ell})}\\end{array}$ end for $\\varepsilon\\sim\\mathcal{N}(0_{d_{x}},I_{d_{x}})$ $X_{j}^{\\ell}\\gets\\hat{\\mu}_{j}+\\mathrm{diag}(\\mathrm{e}^{\\hat{v}_{j}/2})\\varepsilon;$ ; end for $X_{k\\ell}\\leftarrow X_{k\\ell}^{\\ell}$ ;   \nend for ", "page_idx": 15}, {"type": "text", "text": "A.3 Proof of Proposition 3.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For all $k\\in[0,n-1]$ we denote by $q_{k|k+1}(x_{k}|x_{k+1})$ the exact backward kernel which satisfies ", "page_idx": 16}, {"type": "equation", "text": "$$\nq_{k+1}(x_{k+1})q_{k|k+1}(x_{k}|x_{k+1})=q_{k}(x_{k})q_{k+1|k}(x_{k+1}|x_{k})\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Note that the backward kernels $p_{k|k+1}$ are to be understood as Gaussian approximations of the true backward kernels $q_{k|k+1}$ . Below we give a complete statement of the proposition and provide a proof. ", "page_idx": 16}, {"type": "text", "text": "Proposition A.1. Let $k~\\in~[1,n]$ . Assume that $q_{k|k+1}(x_{k}|x_{k+1})\\ =\\ p_{k|k+1}(x_{k}|x_{k+1})$ for all $(x_{k},x_{k+1})\\in(\\mathbb{R}^{d_{x}})^{2}$ . For all $\\ell\\in[0,k-1]$ and $\\boldsymbol{x}_{k}\\in\\mathbb{R}^{d_{x}}$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\nW_{2}(\\hat{p}_{\\ell|k}(\\cdot|x_{k}),p_{\\ell|k}(\\cdot|x_{k}))\\leq\\frac{\\sqrt{\\alpha_{\\ell}}(1-\\alpha_{k}/\\alpha_{\\ell})}{(1-\\alpha_{k})}W_{2}(\\hat{p}_{0|k}(\\cdot|x_{k}),p_{0|k}(\\cdot|x_{k}))\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof of Proposition $A.l$ . Under the assumptions of the proposition, we have, for all $m>\\ell$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\np_{\\ell|k}(x_{\\ell}|x_{k})=q_{\\ell|k}(x_{\\ell}|x_{k})=\\int q_{\\ell|0,k}(x_{\\ell}|x_{0},x_{k})\\,q_{0|k}(\\mathrm{d}x_{0}|x_{k})\\,.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Indeed, by definition of the backward kernel $q_{0|k}(x_{0}|x_{k})$ and (A.10), it holds that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\int q_{\\ell\\vert0,k}(x_{\\ell}\\vert x_{0},x_{k})q_{0\\vert k}(x_{0}\\vert x_{k})\\,\\mathrm{d}x_{0}=\\int\\frac{q_{\\ell\\vert0}(x_{\\ell}\\vert x_{0})q_{k\\vert\\ell}(x_{k}\\vert x_{\\ell})}{q_{k\\vert0}(x_{k}\\vert x_{0})}\\frac{q_{0}(x_{0})q_{k\\vert0}(x_{k}\\vert x_{0})}{q_{k}(x_{k})}\\,\\mathrm{d}x_{0}}}\\\\ &{}&{=\\frac{q_{k\\vert\\ell}(x_{k}\\vert x_{\\ell})}{q_{k}(x_{k})}\\int q_{0}(x_{0})q_{\\ell\\vert0}(\\mathrm{d}x_{\\ell}\\vert x_{0})\\,\\mathrm{d}x_{0}}\\\\ &{}&{=q_{\\ell\\vert k}(x_{\\ell}\\vert x_{k})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "As a result, we have that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{p_{\\ell|k}(x_{\\ell}|x_{k})=\\displaystyle\\int q_{\\ell|0,k}(\\mathrm{d}x_{\\ell}|x_{0},x_{k})q_{0|k}(x_{0}|x_{k})\\,\\mathrm{d}x_{0}\\,,}\\\\ &{\\hat{p}_{\\ell|k}(x_{\\ell}|x_{k})=\\displaystyle\\int q_{\\ell|0,k}(\\mathrm{d}x_{\\ell}|x_{0},x_{k})\\hat{p}_{0|k}(x_{0}|x_{k})\\,\\mathrm{d}x_{0}\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where, by definition, $\\hat{p}_{0|k}(\\cdot|x_{k})$ is a Gaussian approximation of $q_{0|k}(\\cdot|x_{k})$ as defined in the main paper. ", "page_idx": 16}, {"type": "text", "text": "Next, let $\\Pi_{0|k}(\\cdot|x_{k})$ denote a coupling of $q_{0|k}(\\cdot|x_{k})$ and $\\hat{p}_{0|k}(\\cdot|x_{k})$ , i.e., for all $A\\in B(\\mathbb{R}^{d_{x}})$ , ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\int\\mathbb{1}_{A}(x_{0})\\mathbb{1}_{\\mathbb{R}^{d_{x}}}(\\hat{x}_{0})\\,\\Pi_{0|k}(x_{0},\\hat{x}_{0}|x_{k})\\,\\mathrm{d}x_{0}\\mathrm{d}\\hat{x}_{0}=\\int\\mathbb{1}_{A}(x_{0})\\,q_{0|k}(x_{0}|x_{k})\\,\\mathrm{d}x_{0}\\,,}\\\\ {\\int\\mathbb{1}_{\\mathbb{R}^{d_{x}}}(x_{0})\\mathbb{1}_{A}(\\hat{x}_{0})\\,\\Pi_{0|k}(x_{0},\\hat{x}_{0}|x_{k})\\,\\mathrm{d}x_{0}\\mathrm{d}\\hat{x}_{0}=\\int\\mathbb{1}_{A}(\\hat{x}_{0})\\,\\hat{p}_{0|k}(\\hat{x}_{0}|x_{k})\\,\\mathrm{d}\\hat{x}_{0}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Consider then the random variables ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle X_{\\ell\\mid k}=\\frac{\\sqrt{\\alpha_{\\ell}}(1-\\alpha_{k}/\\alpha_{\\ell})}{1-\\alpha_{k}}X_{0\\mid k}+\\frac{\\sqrt{\\alpha_{k}/\\alpha_{\\ell}}(1-\\alpha_{\\ell})}{1-\\alpha_{k}}x_{k}+\\frac{\\sqrt{(1-\\alpha_{\\ell})(1-\\alpha_{k}/\\alpha_{\\ell})}}{\\sqrt{1-\\alpha_{k}}}Z\\,,}}\\\\ {{\\displaystyle\\hat{X}_{s\\mid k}=\\frac{\\sqrt{\\alpha_{\\ell}}(1-\\alpha_{k}/\\alpha_{\\ell})}{1-\\alpha_{k}}\\hat{X}_{0\\mid k}+\\frac{\\sqrt{\\alpha_{k}/\\alpha_{\\ell}}(1-\\alpha_{\\ell})}{1-\\alpha_{k}}x_{k}+\\frac{\\sqrt{(1-\\alpha_{\\ell})(1-\\alpha_{k}/\\alpha_{\\ell})}}{\\sqrt{1-\\alpha_{k}}}Z\\,,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "where $(X_{0|k},\\hat{X}_{0|k})\\sim\\Pi_{0|k}(\\cdot|x_{k})$ and $Z\\sim\\mathcal{N}(0_{d_{x}},I_{d_{x}})$ . Then $(X_{\\ell|k},\\hat{X}_{\\ell|k})$ is distributed according to a coupling of $\\hat{p}_{\\ell|k}(\\cdot|x_{k})$ and $p_{\\ell|k}(\\cdot|x_{k})$ , and consequently ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{2}(\\hat{p}_{\\ell|k}(\\cdot|x_{k}),p_{\\ell|k}(\\cdot|x_{k}))\\leq\\mathbb{E}\\left[\\|X_{\\ell|k}-\\hat{X}_{\\ell|k}\\|^{2}\\right]^{1/2}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\leq\\frac{\\sqrt{\\alpha_{\\ell}}(1-\\alpha_{k}/\\alpha_{\\ell})}{(1-\\alpha_{k})}\\mathbb{E}\\left[\\|X_{0|k}-\\hat{X}_{0|k}\\|^{2}\\right]^{1/2}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "The result is obtained by taking the infinimum of the rhs with respect to all couplings of $q_{0|k}(\\cdot|x_{k})$ and $\\hat{p}_{0|k}(\\cdot|x_{k})$ . \u53e3 ", "page_idx": 16}, {"type": "text", "text": "B Related works. ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section we discuss in more details existing works that bear some similarities with DCPS. ", "page_idx": 17}, {"type": "text", "text": "SMC based approaches. The MCGDIFF, the Twisted Diffusion sampler (TDS) of [59] using the FK representation \u221a(2.4). MCGDIFF is specific to linear inverse problems and the potentials used are $\\bar{g_{k}(x_{k})}\\,=\\,\\mathrm{N}(\\sqrt{\\alpha_{k}}\\,y;A x_{k},(1-\\alpha_{k})\\bar{I}_{d_{y}})$ when $\\sigma_{y}\\,=\\,0$ . TDS applies to any potential $g_{0}$ and relies on the DPS approximation for its potentials; i.e. $g_{k}(x_{k})=g_{0}(\\hat{x}_{0|k}(x_{k}))$ . In either cases, a particle approximation of the posterior of interest $\\pi_{0}$ is obtained using the Auxiliary Particle filter framework [37]. [16] also use particle filters for the posterior distribution; the potentials used are $g_{k}(x_{k})=\\mathrm{N}(\\sqrt{\\alpha_{k}}y_{k};A x_{k},\\alpha_{k}\\bar{\\sigma_{y}^{2}}I_{d_{x}})$ where $(y_{k})_{k=0}^{n}$ , with $y_{0}\\;=\\;y$ is a sequence of observations sampled according to an auto-regressive process; see [16, Equation 7]. The posterior is thus viewed as approximately the time 0 marginal of a Hidden Markov model with transition $p_{k|k+1}$ and observation likelihood $g_{k}$ , which is different from the FK representation (2.4). Our choice of intermediate potentials for linear inverse problems with Gaussian noise differs from that of MCGDIFF by the standard deviation of the observation model, which we set to be $\\sigma_{y}$ . A major difference of DCPS with these works lies in the fact that we do not rely on particle filters, thus avoiding the collapse in very large dimensions. As we have shown in the experimental section DCPS can achieve comparable performance to MCGDIFF in low dimensions, see Table 1 while also being efficient in very large dimensions, see Table 2. A second and major difference is that we have derived potentials for both the JPEG dequantization and Poisson-shot denoising tasks, which may be used to extend MCGDIFF and FPS-SMC [16] to these problems. ", "page_idx": 17}, {"type": "text", "text": "RedDiff. In this work we have also proposed to use Gaussian variational inference to approximate tuhsee  ianmtroarcttiazbelde  vbaarciaktiwoanradl  tirnafnesrietinocne $\\bar{\\pi}_{k|k+1}^{\\ell}$ n.d  Oinnset epaadr toicptuilamriiztye  tohfe  ovuarr iaaptipornoaalc hdi isstr itbhuatti owne  adt oe ancoht step of the diffusion. A similar approach is used in REDDIFF [32] but in a different way. Indeed, the authors use a non-amortized Gaussian variational approximation for the posterior $\\pi_{0}$ , meaning that in order to draw one sample from REDDIFF, several steps of optimization are performed on a score-matching-like loss. Interestingly, this approach does not require differentiating through the denoising network and is thus faster and more memory efficient. However, we found that this comes at the cost of performance as can be seen in Table $1,2$ and 3. ", "page_idx": 17}, {"type": "text", "text": "SDA. In [42], the authors introduce a posterior sampling algorithm for inverse problem where the chosen potential approximation is ", "page_idx": 17}, {"type": "equation", "text": "$$\ng_{\\ell}(x_{\\ell})=N(y;A\\hat{x}_{0\\vert\\ell}^{\\theta}(x_{\\ell}),\\sigma_{y}^{2}+\\frac{\\gamma(1-\\alpha_{\\ell})}{\\alpha_{\\ell}}A A^{\\mathrm{T}})\\,,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "with $\\gamma>0$ being a tunable parameter. Noteworthy, this potential is similar to one used in [47] with a slightly different choice of variance. Then, the Score-based Data Assimilation (SDA) algorithm proceed following the Predictor-Corrector framework [51]. In the Prediction stage, a sample $X_{k}$ given $X_{k+1}$ is drawn using the conditional score ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\hat{s}_{k+1}\\big(x_{k+1}\\big)=\\nabla\\log\\hat{p}_{k+1}\\big(x_{k+1}\\big)+\\nabla\\log g_{k+1}\\big(x_{k+1}\\big)\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "In the Correction stage, a Langevin MC targeting the marginal distribution $g_{k}(x_{k})p_{k}(x_{k})$ is simulated starting from the predicted sample following ", "page_idx": 17}, {"type": "equation", "text": "$$\nX_{k}^{i+1}=X_{k}^{i}+\\delta_{k}(X_{k}^{i})\\hat{s}_{k}(X_{k}^{i})+\\sqrt{2\\delta_{k}(X_{k}^{i})}Z_{i}\\,,\\quad Z_{i}\\sim\\mathrm{N}(0,I)\\,,\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $\\delta_{k}$ is a state-dependent step-size. We emphasise that due to the dependence of the step sizes on the states, these are only Langevin-like updates that do not inherit the theoretical guarantees of the unadjusted Langevin algorithm. While SDA and our algorithm DCPS both use Langevin, the pivotal difference is that its purpose, in our case, is not to correct to ensure that the sample $X_{k\\ell}$ is distributed according to the marginal $\\pi_{k_{\\ell}}(x_{k_{\\ell}})\\propto g_{k_{\\ell}}(x_{k_{\\ell}})p_{k_{\\ell}}(x_{k_{\\ell}})$ , but rather to ensure that the sample is distributed according to the next distribution $\\pi_{k_{\\ell}}(x_{k_{\\ell}})$ , which is the initial distribution of the next block, as per Equation (3.4). Hence, in our case, Langevin MC is used between blocks and not within blocks. ", "page_idx": 17}, {"type": "text", "text": "C Experiments ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "C.1 Implementation details ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this section we provide the global implementation details for each algorithm. We provide the specific parameters (when needed) used for each experiment (Gaussian mixture, image restoration and trajectory inpainting) in the dedicated sections below. ", "page_idx": 18}, {"type": "text", "text": "DCPS. For all the experiments we implement Algorithm 1. We use the same parameters $K=2$ , $L=3$ and $\\zeta=1$ for all the experiments. For the number of Langevin steps, we set it to $M=50$ and $M=500$ (respectively) for the Gaussian mixture experiment and $M=5$ for the imaging and trajectory inpainting experiments. ", "page_idx": 18}, {"type": "text", "text": "DDRM. We have used the official implementation1 and used the recommended parameters in the original paper. We use 200 steps for DDRM and found that it works better than when we used 1000 steps. ", "page_idx": 18}, {"type": "text", "text": "DPS. We have implemented both Algorithm 1 (for linear inverse problems) and Algorithm 2 (for Poisson-shot restoration) given in [10]. In all the experiments we run DPS with 1000 Diffusion steps. ", "page_idx": 18}, {"type": "text", "text": "RedDiff. For RedDiff, we have used the publicly available implementation2. We have empirically found that RedDiff works best in the low observation standard deviation regime and produces spatially coherent reconstructions in the larger noise regime but struggles with getting rid of the noise as evidenced by the large increase in LPIPS values in Table 2. Note also that it is not clear how the parameters of the algorithm depend on the inverse problem standard deviation; indeed, looking at Algorithm 1 and then Appendix C.2 where the authors consider a noisy inverse problem3 there seems to be no clear dependence of $\\lambda$ on $\\sigma_{v}$ $\\sigma_{y}$ with our notations). In fact the authors use $\\lambda=0.25$ similarly to the noiseless experiments in the main paper and we believe that the tuning is performed only on the initial step-size of Adam. As a result, for the experiments with $\\sigma_{y}=0.3$ , we have tuned it using a grid-search in [0.1, 0.25] and retained 0.1. ", "page_idx": 18}, {"type": "text", "text": "\u03a0GDM. Regarding \u03a0GDM [47], note that there is no publicly available implementation and we have thus implemented the noisy version of [47, Algorithm 1] in the original paper. However, we did not manage to obtain appropriate results and found it to be quite unstable. We have further investigated the issue and found that \u03a0GDM is implemented in the github repository of RedDiff4, which is by the same authors. We have noted that it has a slight difference with Algorithm 1 of the \u03a0GDM pa\u221aper; the gradient term, coined $g$ in [47, Algorithm 1], is multiplied by $\\sqrt{\\alpha_{t-1}\\alpha_{t}}$ instead of simply $\\sqrt{\\alpha_{t}}$ . We have found that this stabilizes the algorithm significantly for the linear inverse problem experiment. We use the same rescaling for the Gaussian mixture and trajectory inpainting experiment. However, even with this modification to the algorith we found that \u03a0GDM does not perform well when the noise standard deviation is large; see Table 2. For the JPEG experiment we do not use this rescaling as we found that the algorithm remains stable. ", "page_idx": 18}, {"type": "text", "text": "MCGDiff. For MCGDiff we have used the official implementation5 with $N=32$ particles for the imaging experiments. There are no further tuning parameters as far as we can tell. ", "page_idx": 18}, {"type": "text", "text": "DIFFPIR We implemented [62, Algorithm 1] and use the hyperparameters recommended in the official, released version6. ", "page_idx": 18}, {"type": "text", "text": "DDNM. We adapted the implementation in the released code7 to our code base. ", "page_idx": 18}, {"type": "text", "text": "SDA. We implement the posterior sampling algorithm by combining [42, Algo 3 and 4 in Appendix C]. In the experiments, we use two Langevin corrections steps and found that $\\gamma=0.1$ works well across problems for the diagonal approximation the same as $\\tau=0.1$ for the Langevin correction steps size. ", "page_idx": 19}, {"type": "text", "text": "FPS We implement [16, Algorithm 2] provided in the appendix. ", "page_idx": 19}, {"type": "text", "text": "C.2 Gaussian mixtures ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "For a given dimension $d_{x}$ , we consider $p_{\\mathrm{data}}$ a mixture of 25 Gaussian random variables. The means of the Gaussian components of the mixture are $(\\mathbf{m}_{i})_{i=1}^{25}:=\\{(8i,8j,\\cdot\\cdot\\cdot\\cdot,8i,8j)\\in\\mathbb{R}^{d_{x}}:\\;(i,j)\\in$ $\\{-2,-1,0,1,2\\}^{2}\\}$ . The covariance of each component is identity. The mixture (unnormalized) weights $w_{i,j}$ are independently drawn from a Dirichlet distribution. ", "page_idx": 19}, {"type": "text", "text": "Metrics. To assess the performance of each algorithm we draw 2000 samples and compare against 2000 samples from the true posterior distribution using the Sliced Wasserstein distance by averaging over $10^{4}$ slices. In Table 1 we report the average SW and the $95\\%$ confidence interval over 30 seeds. We found DPS and \u03a0GDM to be sometimes unstable, resulting in NaN values. To account for these unstabilities when computing the average SW distance, we replace NaN with 7 which is the typical value obtained when a stable algorithm fails to sample from the posterior. ", "page_idx": 19}, {"type": "text", "text": "Parameters. For DPS we use $\\zeta_{m}=0.1/\\|y-A\\hat{x}_{0|m}^{\\theta^{\\star}}(x_{m})\\|$ at step $m$ of the Diffusion. As to DCPS we use $\\gamma=10^{-2}$ for the Langevin step-size. ", "page_idx": 19}, {"type": "text", "text": "Denoisers. Note that the loss (A.1) can be written as ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\sum_{t=1}^{T}w_{t}\\mathbb{E}\\left[\\|\\epsilon_{t}-\\hat{\\epsilon}_{t}^{\\theta}(\\sqrt{\\alpha_{t}}X_{0}+\\sqrt{1-\\alpha_{t}}\\epsilon_{t})\\|^{2}\\right]}\\\\ &{\\quad=\\displaystyle\\sum_{t=1}^{T}\\frac{w_{t}}{1-\\alpha_{t}}\\mathbb{E}\\left[\\|\\sqrt{1-\\alpha_{t}}\\epsilon_{t}-\\sqrt{1-\\alpha_{t}}\\hat{\\epsilon}_{t}^{\\theta}(\\sqrt{\\alpha_{t}}X_{0}+\\sqrt{1-\\alpha_{t}}\\epsilon_{t})\\|^{2}\\right]}\\\\ &{\\quad=\\displaystyle\\sum_{t=1}^{T}\\frac{w_{t}}{1-\\alpha_{t}}\\mathbb{E}\\left[\\|X_{t}-\\sqrt{\\alpha_{t}}X_{0}-\\sqrt{1-\\alpha_{t}}\\hat{\\epsilon}_{t}^{\\theta}(X_{t})\\|^{2}\\right]}\\\\ &{\\quad=\\displaystyle\\sum_{t=1}^{T}\\frac{w_{t}\\alpha_{t}}{1-\\alpha_{t}}\\mathbb{E}\\left[\\left\\|X_{0}-\\frac{X_{t}-\\sqrt{1-\\alpha_{t}}\\hat{\\epsilon}_{t}^{\\theta}(X_{t})}{\\sqrt{\\alpha_{t}}}\\right\\|^{2}\\right]\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence the minimizer is ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\epsilon_{t}^{\\theta^{\\star}}(x_{t})=\\frac{x_{t}-\\sqrt{\\alpha_{t}}\\,\\mathbb{E}[X_{0}|X_{t}=x_{t}]}{\\sqrt{1-\\alpha_{t}}}\\,,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which yields $\\hat{x}_{0\\mid t}^{\\theta^{\\star}}=\\mathbb{E}[X_{0}|X_{t}=\\cdot]$ . Next, by Tweedie\u2019s formula we have that ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\hat{x}_{0\\mid t}^{\\theta^{\\star}}(x_{t})=\\frac{x_{t}+(1-\\alpha_{t})\\nabla_{x}\\log q_{t}(x_{t})}{\\sqrt{\\alpha_{t}}}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Hence, since $q_{\\mathrm{data}}$ is a mixture of Gaussians, $q_{t}$ is also a mixture of Gaussians with means $(\\sqrt{\\alpha_{t}}\\mathbf{m}_{i})_{i=1}^{25}$ and unit covariances. Therefore, $\\nabla_{x}\\log{q_{t}(x_{t})}$ and hence $\\hat{x}_{0\\mid t}^{\\theta^{\\star}}(x_{t})$ can be computed using automatic differentiation libraries. ", "page_idx": 19}, {"type": "text", "text": "Measurement model. For a pair of dimensions $(d_{x},d_{y})$ the measurement model $(y,A,\\sigma_{y})$ is drawn as follows: the elements $d_{x}\\times d_{y}$ elements of the matrix are drawn i.i.d. from a standard Gaussian distribution, then $\\sigma_{y}$ is drawn uniformly in $[0,1]$ and finally we draw $x^{\\star}\\sim\\,p_{\\mathrm{data}}$ and $\\varepsilon\\sim\\mathcal{N}(0_{d_{y}},I_{d_{y}})$ and set $y=\\bar{A}x^{\\star}+\\sigma_{y}\\varepsilon$ . ", "page_idx": 19}, {"type": "text", "text": "Table 6: Mean LPIPS value on low count Poisson restoration. ", "page_idx": 20}, {"type": "table", "img_path": "BOrut7M2X7/tmp/5d0e7acb653a6135c48057fd8c8584124be0e8e8ca69f336fe8ba9b48175140e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "Posterior. Having drawn both $p_{\\mathrm{data}}$ and $(y,A,\\sigma_{y})$ , the posterior can be computed exactly using standard Gaussian conjugation formulas [3, Eq. 2.116] and hence the posterior is a Gaussian mixture where all the components have the same covariance matrix $\\Sigma:=\\bar{(I_{d_{x}}+\\sigma_{y}^{-2}\\,\\mathrm{A}^{T}\\,\\mathrm{A})}^{-1}$ and means and weights given by ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\tilde{\\mathbf{m}}_{i}:=\\Sigma\\left(A^{\\mathsf{T}}y/\\sigma_{y}^{2}+\\mathbf{m}_{i}\\right)\\,,}\\\\ &{\\tilde{w}_{i}\\propto w_{i}\\mathrm{N}(y;A\\mathbf{m}_{i},\\sigma_{y}^{2}I_{d_{x}}+A A^{\\mathsf{T}})\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "C.3 Imaging experiments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Parameters. For DCPS we set $\\gamma~=~10^{-3}$ for the Langevin step-size. For DPS we use the parameters recommended in the original paper, which we found to work well even on the half and expand masks; see [10, Appendix D.1]. ", "page_idx": 20}, {"type": "text", "text": "Evaluation. In order to evaluate each algorithm we compute the LPIPS metric [61] on each dataset using 100 samples from the validation sets and report the average in Table 2, 3 and 6. ", "page_idx": 20}, {"type": "text", "text": "JPEG dequantization. We use the differentiable JPEG framework [44] which replaces the rounding function $x\\mapsto\\lfloor x\\rceil$ used in the quantization part with $x\\mapsto\\lfloor x\\rceil+(x-\\lfloor x\\rceil)^{3}$ which has non-zero derivatives almost everywhere. ", "page_idx": 20}, {"type": "text", "text": "C.4 Trajectory inpainting experiment ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Trajectory DDM prior. The denoiser of the diffusion model has a Transformer-like architecture. In the entry of the network, the trajectory is augmented to a higher dimensional space (512) via dense layer. At this stage a positional encoding [56] is added to account for the diffusion step. Afterward, the output is flowed through a transformer encoder [56] whose feedforward layer dimension is 2048 to learn temporal dependence within the trajectory before being feed to an MLP with 4 layers $(512\\rightarrow1024\\rightarrow1024\\rightarrow512)$ ) and in between ReLU activation functions, to output the added noise. A Cosine noise scheduler with 1000 diffusion steps was used [34]. The UCY-student dataset was split int a train and a validation sets with 1450 and 140 trajectories respectively. The batch size was set to 10 times the training set, namely 145 samples The denoiser was trained to minimize the loss of DDPM [20] for 1000 epochs using Adam solver [25] with a Cosine learning rate scheduler [28]. The training was performed on 48GB L40S NVIDIA GPU and took roughly one minute to complete. ", "page_idx": 20}, {"type": "text", "text": "Metrics. The trajectory completion experiment was performed on the validation set. Every trajectory was masked randomly. Leveraging MCGDIFF \u2019s asymptotical approximation of the posterior, it was run with 5000 particles to sample 100 samples from the posterior and afterward these were checked against a 100 reconstructions of each other algorithm by computing the timestep wise $\\ell_{2}$ distance between the quantile 50 (median), 25, 75 and also by computing the Sliced Wasserstein distance. This procedure was repeated for all trajectories in the validation set and later the results of each algorithm were aggregated by the mean $\\ell_{2}$ distances. Finally, this experiment was performed for two levels of noise $\\sigma_{y}=0.005$ and $\\sigma_{y}=0.01$ . ", "page_idx": 20}, {"type": "text", "text": "C.5 Additional experiments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Here, we provide the complete tables of results on imaging and trajectories inpainting experiments that includes in addition DIFFPIR, DDNM, FPS , and SDA. These additional experiments were conducted during the rebuttal phase of our work. ", "page_idx": 20}, {"type": "table", "img_path": "BOrut7M2X7/tmp/b14f07c41444a0734f243acfe65d440c0c4282145796d9114138d38b4ef6c977.jpg", "table_caption": ["Table 7: Mean LPIPS value on different tasks. Lower is better. "], "table_footnote": [], "page_idx": 21}, {"type": "table", "img_path": "BOrut7M2X7/tmp/926bd01bf013f911744142eb5435f4fa2d40377f6e9772c1e730d2da9ebde4b2.jpg", "table_caption": ["Table 8: $\\ell_{2}$ distance quantiles with MCGDIFF as reference. "], "table_footnote": [], "page_idx": 21}, {"type": "text", "text": "D Sample reconstructions ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section we display the remaining samples from the experiments in the main paper. We remind the reader that all algorithms are run with the same seed and we draw in parallel 4 samples from each algorithm and display them in their order of appearance. ", "page_idx": 21}, {"type": "image", "img_path": "BOrut7M2X7/tmp/f06aedbf656c7d56e7f572e87e302a14df0e4638cafe8591dcf520066fede778.jpg", "img_caption": ["Figure 5: Denoising task with Poisson noise on FFHQ. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "BOrut7M2X7/tmp/cc9cbc849251c35f630d85305aff32164df375db1e6bf8406cea39c3a45a5309.jpg", "img_caption": ["Figure 6: Denoising task with Poisson noise on ImageNet. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "BOrut7M2X7/tmp/4c50a51c756d723611171d17ca06eeca9c835e5e01734aaa3654c3b0c9a8e98e.jpg", "img_caption": ["Figure 7: Outpainting task with half mask on ImageNet. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "BOrut7M2X7/tmp/563f9afe3bb76cd763bccc99f4c8661df3bb7533ee28d913d5908346524ab426.jpg", "img_caption": ["Figure 8: Inpainting with box mask on FFHQ. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "BOrut7M2X7/tmp/d75703553ca4ce0f57e4badb329af7644b514039a58e24769310c87047cd24a9.jpg", "img_caption": ["Figure 9: Inpainting task with box mask on ImageNet. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "BOrut7M2X7/tmp/f2750cc77126484933017006773be0d691da43df7e102fbb0e6246cca7ad66ff.jpg", "img_caption": ["Figure 10: Outpainting task with half mask on FFHQ. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "BOrut7M2X7/tmp/49e13e887b1102f53634d2736737a621848e0a2365adaff87f5de10e3925656a.jpg", "img_caption": ["Figure 11: Outpainting task with half mask on ImageNet. "], "img_footnote": [], "page_idx": 24}, {"type": "image", "img_path": "BOrut7M2X7/tmp/86925d2ac308f935b0b7b166ec75231b17da6f35bf520be3deff1279adb0778e.jpg", "img_caption": ["Figure 12: Outpainting expend task on FFHQ. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "BOrut7M2X7/tmp/ac8ea4ac7466f0f1cf13d4a356f8f4cf0b0b4b7615ff0625432470d4a013d36a.jpg", "img_caption": ["Figure 13: Outpainting expend task on ImageNet. "], "img_footnote": [], "page_idx": 25}, {"type": "image", "img_path": "BOrut7M2X7/tmp/6991371db0ea9c74e200df9c8f0b3889472be5d227c9bd0c23d711d7e1619229.jpg", "img_caption": ["Figure 14: SR $4\\times$ task with Poisson noise on FFHQ. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "BOrut7M2X7/tmp/e0d13ea130bc28f490ad214b37f5fda713188a31566fdc7ef584576ff211b756.jpg", "img_caption": ["Figure 15: SR $4\\times$ task on ImageNet. "], "img_footnote": [], "page_idx": 26}, {"type": "image", "img_path": "BOrut7M2X7/tmp/40031742c0a5d399a1ec3a70c9f9366133fbc73ac12bedec214bb527025d1c83.jpg", "img_caption": ["Figure 16: SR $16\\times$ task on FFHQ. "], "img_footnote": [], "page_idx": 27}, {"type": "image", "img_path": "BOrut7M2X7/tmp/7db49d6d50ad10bddd1f14c3ad01e2912382b7fab9ea80019179a69ab5472124.jpg", "img_caption": [], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "BOrut7M2X7/tmp/e053604dc99ebeb594a34390f3884d3dc576d7579ef9ff53b0fb78900d1e5a31.jpg", "img_caption": ["Figure 17: SR $16\\times$ task on ImageNet. "], "img_footnote": [], "page_idx": 28}, {"type": "image", "img_path": "BOrut7M2X7/tmp/282760abacef5535227302d0bc5fa8f88aee082818b6d60b375c7336d0ded76d.jpg", "img_caption": ["Figure 18: JPEG task with $\\mathrm{QF}{=}8$ on FFHQ. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "BOrut7M2X7/tmp/8f1fbf00e8494498709b894502e61e16440cde222135740eb7f81dc973176356.jpg", "img_caption": ["Figure 19: JPEG task with $\\mathrm{QF}{=}2$ on FFHQ. "], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "BOrut7M2X7/tmp/1dae97c4bd7ff7a9a249dc4a4211990f6c6492a7591396e1eea40aca59e85373.jpg", "img_caption": ["Figure 20: JPEG task with $\\mathrm{QF}{=}8$ on ImageNet. "], "img_footnote": [], "page_idx": 30}, {"type": "image", "img_path": "BOrut7M2X7/tmp/d9c853f61ffc2c1d6e133d5ac586dc2a6e4be0c09e4f7f90a40730f190fa2deb.jpg", "img_caption": ["Figure 21: JPEG task with $\\mathrm{QF}{=}2$ on ImageNet. "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: The claims are clearly stated in the introduction. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 31}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Justification: We have added a limitations section. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: All the assumptions needed are stated clearly. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 32}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We have provided the exact implementation details in the appendix. Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We have provided a link with the relevant code as well as the link to download the datasets we have used ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 33}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: These are all given in Appendix C. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 33}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We report the $95\\%$ confidence intervals for our experiments. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We provide the memory usage and runtime for each algorithm. Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 34}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 34}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate ", "page_idx": 34}, {"type": "text", "text": "to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. ", "page_idx": 35}, {"type": "text", "text": "\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 35}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 35}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We properly cite the authors that have released the datasets and models we use. Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The released code is accompanied by a README file detailing its contents, installation instructions, and usage guidelines. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 36}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 36}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]