[{"figure_path": "BAmAFraxvf/figures/figures_2_1.jpg", "caption": "Figure 1: Overview of our architecture. [A] The scene image is passed through an Image Encoder to produce image tokens (blue squares). [B] The head crops and head box coordinates are processed by a Gaze Encoder to generate gaze tokens (orange squares). [C] The image and gaze tokens are fed to a Gaze Decoder to predict both the gaze heatmaps and (normalized) gaze label embeddings (yellow squares) through a series of cross-attention operations. [D] The text encoder computes (normalized) class embeddings (green squares) based on a predefined vocabulary of concept classes. [E] Finally, we compute similarity scores between the predicted gaze label embeddings and vocabulary embeddings.", "description": "This figure shows the overall architecture of the proposed model for semantic gaze target detection. It consists of five main components: an Image Encoder that processes the scene image and produces image tokens, a Gaze Encoder that processes head crops and bounding box coordinates to generate gaze tokens, a Gaze Decoder that combines image and gaze tokens to predict gaze heatmaps and gaze label embeddings, a Text Encoder that computes class embeddings from a predefined vocabulary, and a final similarity computation step that matches predicted gaze label embeddings to vocabulary embeddings to determine the semantic class of the gaze target.", "section": "3 Architecture"}, {"figure_path": "BAmAFraxvf/figures/figures_6_1.jpg", "caption": "Figure 2: Samples from GazeHOI. We show the head box (white) and the object's box (red).", "description": "This figure shows several examples from the GazeHOI dataset. Each image contains a person whose head is enclosed in a white bounding box and an object of interest enclosed in a red bounding box.  The figure visually demonstrates the diversity of scenes and objects present in the GazeHOI dataset, illustrating the task of simultaneously localizing and recognizing the gaze target.", "section": "4 Datasets"}, {"figure_path": "BAmAFraxvf/figures/figures_6_2.jpg", "caption": "Figure 3: [Left] Overview of the proposed baseline architecture using masked conditioning. The similarity scores Si are computed from the gaze label embedding I, and the class embeddings Ti. [Right] Comparison of the conditioning variants based on the predicted gaze heatmap: original image (top left), masking (top right), blurring (bottom left), and cropping (bottom right).", "description": "This figure illustrates the baseline architecture used for comparison in the paper.  The baseline is a two-stage approach: First, the proposed gaze model predicts a gaze heatmap. This heatmap is then used to condition the input image using different methods such as masking, blurring, or cropping. Finally, CLIP's vision model is used on the resulting image to generate the gaze label embedding, which is then compared to the available class label embeddings to predict the final class. The right side of the figure demonstrates the different image conditioning methods and their effect on the input image.", "section": "5 Experiments"}, {"figure_path": "BAmAFraxvf/figures/figures_9_1.jpg", "caption": "Figure 4: Qualitative samples from our model on images from the internet. The top row shows gaze point predictions for all people. The second row shows the last attention map from the image encoder. The third row shows a predicted heatmap of a single person. The last row shows the weight of the gaze tokens in the last image to gaze cross-attention of the decoder for the same person.", "description": "This figure displays qualitative results of the proposed model on various internet images. The first row visualizes the predicted gaze points for each person within the image, illustrating the model's ability to identify individual gazes. The second row shows attention maps, highlighting the model's focus on relevant image regions.  The third row presents predicted gaze heatmaps for a single person, demonstrating the model's ability to estimate gaze focus regions. Finally, the last row shows a visualization of attention weights applied to image features, indicating the significance of different regions in gaze target prediction.", "section": "5.3 Model Analysis & Ablations"}, {"figure_path": "BAmAFraxvf/figures/figures_13_1.jpg", "caption": "Figure 1: Overview of our architecture. [A] The scene image is passed through an Image Encoder to produce image tokens (blue squares). [B] The head crops and head box coordinates are processed by a Gaze Encoder to generate gaze tokens (orange squares). [C] The image and gaze tokens are fed to a Gaze Decoder to predict both the gaze heatmaps and (normalized) gaze label embeddings (yellow squares) through a series of cross-attention operations. [D] The text encoder computes (normalized) class embeddings (green squares) based on a predefined vocabulary of concept classes. [E] Finally, we compute similarity scores between the predicted gaze label embeddings and vocabulary embeddings.", "description": "This figure provides a detailed overview of the proposed architecture for semantic gaze target detection.  It's a diagram showing the flow of information through different components: Image Encoder processing the scene image, Gaze Encoder processing head crops and location, Gaze Decoder predicting heatmaps and gaze labels via cross-attention, and a Text Encoder generating class embeddings.  The different components are color-coded for clarity.  The overall goal is to predict both where a person is looking (gaze heatmap) and what they are looking at (semantic label).", "section": "3 Architecture"}, {"figure_path": "BAmAFraxvf/figures/figures_14_1.jpg", "caption": "Figure 1: Overview of our architecture. [A] The scene image is passed through an Image Encoder to produce image tokens (blue squares). [B] The head crops and head box coordinates are processed by a Gaze Encoder to generate gaze tokens (orange squares). [C] The image and gaze tokens are fed to a Gaze Decoder to predict both the gaze heatmaps and (normalized) gaze label embeddings (yellow squares) through a series of cross-attention operations. [D] The text encoder computes (normalized) class embeddings (green squares) based on a predefined vocabulary of concept classes. [E] Finally, we compute similarity scores between the predicted gaze label embeddings and vocabulary embeddings.", "description": "This figure provides a detailed overview of the proposed architecture for semantic gaze target detection. It shows the flow of information through different components, including image and gaze encoders, a gaze decoder for heatmap and label prediction, and a text encoder for class embeddings. The architecture employs cross-attention mechanisms to integrate image and gaze information for accurate gaze target localization and semantic classification.", "section": "3 Architecture"}, {"figure_path": "BAmAFraxvf/figures/figures_15_1.jpg", "caption": "Figure 1: Overview of our architecture. [A] The scene image is passed through an Image Encoder to produce image tokens (blue squares). [B] The head crops and head box coordinates are processed by a Gaze Encoder to generate gaze tokens (orange squares). [C] The image and gaze tokens are fed to a Gaze Decoder to predict both the gaze heatmaps and (normalized) gaze label embeddings (yellow squares) through a series of cross-attention operations. [D] The text encoder computes (normalized) class embeddings (green squares) based on a predefined vocabulary of concept classes. [E] Finally, we compute similarity scores between the predicted gaze label embeddings and vocabulary embeddings.", "description": "This figure presents a detailed overview of the proposed architecture for semantic gaze target detection. It is composed of five main components: an Image Encoder processing the scene image, a Gaze Encoder processing head crops and coordinates, a Gaze Decoder predicting gaze heatmaps and label embeddings, a Text Encoder generating class embeddings from a predefined vocabulary, and a final similarity computation step.  The diagram visually illustrates the flow of information and the interactions between these components, utilizing color-coded squares to represent different types of tokens.", "section": "3 Architecture"}, {"figure_path": "BAmAFraxvf/figures/figures_16_1.jpg", "caption": "Figure 1: Overview of our architecture. [A] The scene image is passed through an Image Encoder to produce image tokens (blue squares). [B] The head crops and head box coordinates are processed by a Gaze Encoder to generate gaze tokens (orange squares). [C] The image and gaze tokens are fed to a Gaze Decoder to predict both the gaze heatmaps and (normalized) gaze label embeddings (yellow squares) through a series of cross-attention operations. [D] The text encoder computes (normalized) class embeddings (green squares) based on a predefined vocabulary of concept classes. [E] Finally, we compute similarity scores between the predicted gaze label embeddings and vocabulary embeddings.", "description": "This figure provides a detailed overview of the proposed architecture for semantic gaze target detection. It is composed of five main components: an Image Encoder processing the scene image and producing image tokens; a Gaze Encoder processing head crops and coordinates to generate gaze tokens; a Gaze Decoder combining image and gaze tokens via cross-attention to predict gaze heatmaps and label embeddings; a Text Encoder generating class embeddings from a predefined vocabulary; and a final similarity computation step to match predicted gaze labels with vocabulary embeddings. The illustration clearly depicts the flow of information and the role of each component in the model.", "section": "3 Architecture"}, {"figure_path": "BAmAFraxvf/figures/figures_17_1.jpg", "caption": "Figure 1: Overview of our architecture. [A] The scene image is passed through an Image Encoder to produce image tokens (blue squares). [B] The head crops and head box coordinates are processed by a Gaze Encoder to generate gaze tokens (orange squares). [C] The image and gaze tokens are fed to a Gaze Decoder to predict both the gaze heatmaps and (normalized) gaze label embeddings (yellow squares) through a series of cross-attention operations. [D] The text encoder computes (normalized) class embeddings (green squares) based on a predefined vocabulary of concept classes. [E] Finally, we compute similarity scores between the predicted gaze label embeddings and vocabulary embeddings.", "description": "This figure provides a detailed overview of the proposed architecture for semantic gaze target detection. It illustrates the flow of information through different components: an Image Encoder processing the scene image, a Gaze Encoder handling head crops and coordinates, a Gaze Decoder predicting gaze heatmaps and label embeddings, and a Text Encoder generating class embeddings. The interactions between these components, including cross-attention operations, are clearly shown.  The figure highlights how the model integrates image and text information to predict both the location (heatmap) and semantic class of the gaze target.", "section": "3 Architecture"}, {"figure_path": "BAmAFraxvf/figures/figures_17_2.jpg", "caption": "Figure 1: Overview of our architecture. [A] The scene image is passed through an Image Encoder to produce image tokens (blue squares). [B] The head crops and head box coordinates are processed by a Gaze Encoder to generate gaze tokens (orange squares). [C] The image and gaze tokens are fed to a Gaze Decoder to predict both the gaze heatmaps and (normalized) gaze label embeddings (yellow squares) through a series of cross-attention operations. [D] The text encoder computes (normalized) class embeddings (green squares) based on a predefined vocabulary of concept classes. [E] Finally, we compute similarity scores between the predicted gaze label embeddings and vocabulary embeddings.", "description": "This figure presents a detailed overview of the proposed architecture for semantic gaze target detection. It comprises five main components: 1) an Image Encoder that processes the scene image to produce image tokens; 2) a Gaze Encoder that processes head crops and coordinates to produce gaze tokens; 3) a Gaze Decoder that combines image and gaze tokens to predict gaze heatmaps and gaze label embeddings; 4) a Text Encoder that generates class embeddings from a vocabulary; and 5) a similarity computation step that matches predicted gaze label embeddings with vocabulary embeddings to determine the semantic class of the gaze target. The architecture is designed to simultaneously predict gaze localization and semantic labels.", "section": "3 Architecture"}, {"figure_path": "BAmAFraxvf/figures/figures_19_1.jpg", "caption": "Figure 1: Overview of our architecture. [A] The scene image is passed through an Image Encoder to produce image tokens (blue squares). [B] The head crops and head box coordinates are processed by a Gaze Encoder to generate gaze tokens (orange squares). [C] The image and gaze tokens are fed to a Gaze Decoder to predict both the gaze heatmaps and (normalized) gaze label embeddings (yellow squares) through a series of cross-attention operations. [D] The text encoder computes (normalized) class embeddings (green squares) based on a predefined vocabulary of concept classes. [E] Finally, we compute similarity scores between the predicted gaze label embeddings and vocabulary embeddings.", "description": "This figure presents a detailed overview of the proposed architecture for semantic gaze target detection. It illustrates the flow of information through different components: an Image Encoder processing the scene image, a Gaze Encoder handling head crops and bounding box coordinates, a Gaze Decoder predicting gaze heatmaps and label embeddings, and a Text Encoder generating class embeddings from a predefined vocabulary.  Cross-attention mechanisms connect the image and gaze information for effective gaze target prediction. The architecture is designed for efficient multi-person handling by processing the scene image once and then decoding gaze targets for each person separately.", "section": "3 Architecture"}]