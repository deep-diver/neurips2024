[{"figure_path": "U6oQEzSp8z/figures/figures_1_1.jpg", "caption": "Figure 1: Conventional audiovisual alignment through contrastive learning leads to a gap between modalities. Our proposed distribution alignment method matches closely both distributions leading to better joint representations for audio captioning.", "description": "This figure illustrates the difference between traditional contrastive learning methods for audiovisual alignment and the proposed distribution alignment method.  Contrastive learning, while effective in aligning features across modalities, often results in a \"modality gap\", where audio and visual embeddings are represented in distinct, non-overlapping regions of the embedding space. This hinders smooth integration between modalities. In contrast, the proposed distribution alignment method directly matches the distributions of tokens produced by an audio backbone and those of an image captioner, effectively bridging the modality gap and enabling better joint representations for audio captioning. The figure visually depicts this using manifolds representing different modalities in embedding space.", "section": "1 Introduction"}, {"figure_path": "U6oQEzSp8z/figures/figures_3_1.jpg", "caption": "Figure 2: Overview of the proposed approach. In the first stage, a prefix tuning is performed using a few (image, audio-caption) pairs (1-a). Additionally, the audio backbone is aligned with the image backbone (1-b) through distribution alignment. Audio captioning can then be performed by switching the image backbone with the audio backbone and adding the prefix tokens (1-c). In a second stage, visually-informed audio captions are generated using both audio, image, and prefix tokens. The MLP mapping the audio encoder to the language model is then fine-tuned with these pseudo captions (2-d). The final inference for audio captioning, using audio or audio visual inputs, is performed by forwarding the aligned audio backbone's output through the trained MLP to obtain the LLM input (2-e).", "description": "This figure illustrates the two-stage process of the proposed approach for zero-shot audio captioning. Stage 1 involves prefix tuning with image-audio caption pairs and token distribution alignment between the audio and image backbones, enabling audio captioning by replacing the image backbone. Stage 2 refines audio captions using audio-visual inputs and fine-tunes the MLP, allowing for audio captioning using audio or audio-visual inputs.", "section": "3 Method"}, {"figure_path": "U6oQEzSp8z/figures/figures_6_1.jpg", "caption": "Figure 3: Multimodal distribution alignment through optimal transport. The audio and image tokens are used to compute the cost matrix, while two separate cross-attention layers estimate the weights  Att and  Att.", "description": "This figure illustrates two methods for aligning the distributions of audio and image tokens. The left side shows a method using optimal transport, where a cost matrix is computed based on the audio and image tokens, and the alignment is performed by minimizing the transport cost. The right side depicts a method using attentive optimal transport, which utilizes cross-attention layers to learn the weights for the transport cost matrix, allowing for a more flexible and accurate alignment.", "section": "3 Method"}, {"figure_path": "U6oQEzSp8z/figures/figures_8_1.jpg", "caption": "Figure 4: AudioCaps average tokens distribution. While contrastive learning maps the audio in a space separate from the image ones, MMD and optimal transport project in the same part of the space. The model trained using attentive optimal transport projects the audios in a space closer to the image, with marginal overlap.", "description": "This figure visualizes the average token distributions from different methods on the AudioCaps dataset using Principal Component Analysis (PCA).  It shows that contrastive learning produces audio and image embeddings that occupy distinct regions in the embedding space, illustrating the \"modality gap.\" In contrast, Maximum Mean Discrepancy (MMD) and Optimal Transport (OT) based methods generate embeddings that are closer together.  Specifically, the model using attentive Optimal Transport produces audio embeddings that are more closely aligned with the image embeddings than those produced by MMD or contrastive learning. This visualizes the effectiveness of their proposed distribution alignment method at bridging the modality gap.", "section": "Discussion"}, {"figure_path": "U6oQEzSp8z/figures/figures_14_1.jpg", "caption": "Figure 2: Overview of the proposed approach. In the first stage, a prefix tuning is performed using a few (image, audio-caption) pairs (1-a). Additionally, the audio backbone is aligned with the image backbone (1-b) through distribution alignment. Audio captioning can then be performed by switching the image backbone with the audio backbone and adding the prefix tokens (1-c). In a second stage, visually-informed audio captions are generated using both audio, image, and prefix tokens. The MLP mapping the audio encoder to the language model is then fine-tuned with these pseudo captions (2-d). The final inference for audio captioning, using audio or audio visual inputs, is performed by forwarding the aligned audio backbone's output through the trained MLP to obtain the LLM input (2-e).", "description": "This figure illustrates the two-stage process proposed for zero-shot audio captioning. Stage 1 involves prefix tuning using a few image-audio caption pairs to adapt the image captioner for audio, and aligning the audio backbone with the image backbone via distribution alignment. This allows for performing audio-only captioning by replacing the image backbone with the aligned audio backbone. Stage 2 leverages visually-informed audio captions (using audio, image and prefix tokens) to fine-tune an MLP that maps audio tokens to language model embeddings, enabling robust audio and audio-visual captioning.", "section": "3.1 Proposed Framework Overview"}, {"figure_path": "U6oQEzSp8z/figures/figures_15_1.jpg", "caption": "Figure 4: AudioCaps average tokens distribution. While contrastive learning maps the audio in a space separate from the image ones, MMD and optimal transport project in the same part of the space. The model trained using attentive optimal transport projects the audios in a space closer to the image, with marginal overlap.", "description": "This figure shows the results of a Principal Component Analysis (PCA) on the average token embeddings generated by different methods for audio captioning.  The contrastive learning approach shows a clear separation between audio and image embeddings, illustrating the \"modality gap\". In contrast, the Maximum Mean Discrepancy (MMD) and Optimal Transport (OT) methods result in audio and image embeddings that occupy similar regions of the embedding space. Notably, the attentive OT method shows the closest proximity between audio and image embeddings, indicating more effective alignment of the modalities.", "section": "5 Results and discussion"}, {"figure_path": "U6oQEzSp8z/figures/figures_15_2.jpg", "caption": "Figure 7: DALI Cross-Attention scores. The size of the dots represents the weights of the transport defined by the cross-attentions. The image and its associated audio show a partial mismatch: the audio only contains the siren sound and the image also shows the road. All the tokens with low weights belong to the same part of the space which might indicate that they represent similar information such as the road.", "description": "This figure shows the results of applying the cross-attention mechanism within the optimal transport method used for aligning audio and image token distributions.  The size of the dots corresponds to the weight assigned by the cross-attention mechanism to each token pair.  The example shown involves an image of an emergency vehicle and its associated siren sound.  The figure highlights that the cross-attention mechanism effectively identifies and down-weights tokens representing elements present in the image but not the audio (e.g., the road), thus focusing the alignment on semantically relevant tokens (e.g., the siren). This demonstrates the capacity of the cross-attention mechanism to handle partial mismatches between audio and visual modalities.", "section": "3.2 Token distribution alignment"}, {"figure_path": "U6oQEzSp8z/figures/figures_16_1.jpg", "caption": "Figure 8: Discrepancies filtering process: 10 frames of the video are captioned by BLIP-2, captions are embedded in a text space and compared to the embedding of the class labels. The average of the distances of the frame is considered as the distance between audio and video.", "description": "This figure illustrates the process used to filter the AudioSet dataset to remove videos with significant discrepancies between the audio and visual content.  BLIP-2 is used to generate captions for 10 frames extracted from each video. These captions, and the AudioSet labels, are then embedded in a common vector space using an image encoder. The average distance between the BLIP-2 caption embeddings and AudioSet label embeddings is calculated for each video and represents the audiovisual discrepancy score. Videos with low discrepancy scores are retained in the filtered dataset.", "section": "C AudioSet pre-filtering to limit audiovisual discrepancies"}]