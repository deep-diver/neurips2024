[{"figure_path": "U6oQEzSp8z/tables/tables_7_1.jpg", "caption": "Table 1: Audio captioning performance on AudioCaps test set. Our results are obtained using 16 (image,audio-captions) pairs for the prefix tuning phase. (*): No alignment. (**): Trained in a supervised fashion using audio-caption pairs. Results are ordered by SPIDEr score.", "description": "This table presents a comparison of different audio captioning methods on the AudioCaps test set.  The methods compared include the authors' proposed methods (DALI with different alignment strategies and with/without audiovisual distillation), a contrastive learning baseline, and two existing state-of-the-art methods (Shaharabany et al. and Salewski et al.). The table shows the performance of each method using three standard metrics: METEOR, ROUGE-L, and SPIDEr. The results indicate that the authors' proposed DALI method significantly outperforms existing methods in zero-shot settings, particularly when using audiovisual information.", "section": "5 Results and discussion"}, {"figure_path": "U6oQEzSp8z/tables/tables_8_1.jpg", "caption": "Table 2: Ground-truth audio captions, and captions generated by our audio models (after stage 1, without prefix tokens) by asking \"What can you see? Answer concisely\"", "description": "This table compares the ground truth audio captions with the captions generated by two different models (DALIMMD and DALI_Att) after the first stage of training.  The first stage focused on training prefix tokens and aligning the distribution of tokens produced by an audio backbone with those from the image captioner. The goal was to enable zero-shot audio captioning. The captions from the DALIMMD model include visual artifacts while the DALI_Att captions are more focused and accurate with the task.", "section": "5 Results and discussion"}, {"figure_path": "U6oQEzSp8z/tables/tables_9_1.jpg", "caption": "Table 3: Clotho audio captioning performance. Similarly to AudioCaps, DALI is performing, however, DALIMMD gives slightly better results. The bias learned by matching the complete image distribution seems to be beneficial for out-of-domain samples. (*): Trained in a supervised fashion using audio-caption pairs.", "description": "This table presents the results of the audio captioning performance evaluation on the Clotho dataset.  It compares different methods, including those based on DALI (Distribution Alignment) with Maximum Mean Discrepancy (MMD) and Optimal Transport (OT), along with a contrastive learning approach. The table highlights that while DALI generally performs well, DALIMMD (using MMD) shows a slight edge, potentially because its bias towards image data helps handle the out-of-distribution nature of the Clotho dataset (which differs from the AudioCaps dataset used for training).  A supervised approach using CLAP is also included for comparison.", "section": "5 Results and discussion"}, {"figure_path": "U6oQEzSp8z/tables/tables_16_1.jpg", "caption": "Table 1: Audio captioning performance on AudioCaps test set. Our results are obtained using 16 (image,audio-captions) pairs for the prefix tuning phase. (*): No alignment. (**): Trained in a supervised fashion using audio-caption pairs. Results are ordered by SPIDEr score.", "description": "This table presents a comparison of different methods for audio captioning on the AudioCaps test set.  It shows the performance (using METEOR, ROUGE-L, and SPIDEr metrics) of the proposed approach (DALI, using both audio-only and audio-visual inputs) compared to other existing methods (Contrastive, ImageBind, and CLAP). The impact of using prefix tuning (a small set of image-audio captions) and audiovisual distillation is also evaluated.  Results are ordered by the SPIDEr score, a comprehensive metric that combines several other metrics for captioning evaluation.", "section": "5 Results and discussion"}, {"figure_path": "U6oQEzSp8z/tables/tables_17_1.jpg", "caption": "Table 1: Audio captioning performance on AudioCaps test set. Our results are obtained using 16 (image,audio-captions) pairs for the prefix tuning phase. (*): No alignment. (**): Trained in a supervised fashion using audio-caption pairs. Results are ordered by SPIDEr score.", "description": "This table presents a comparison of different audio captioning methods on the AudioCaps test set.  The methods compared include several variants of the proposed Distribution Alignment approach (DALI), using either Maximum Mean Discrepancy (MMD) or Optimal Transport (OT) for alignment, with and without audiovisual distillation.  Results are also shown for a contrastive learning approach, as well as the existing Shaharabany et al. and Salewski et al. methods. The table highlights the performance of the proposed methods in a zero-shot setting (without using any annotated audio data for training), and shows a significant improvement compared to existing state-of-the-art techniques.  The results are ordered by the SPIDEr score, a comprehensive metric for evaluating the quality of generated captions.", "section": "5 Results and discussion"}, {"figure_path": "U6oQEzSp8z/tables/tables_17_2.jpg", "caption": "Table 1: Audio captioning performance on AudioCaps test set. Our results are obtained using 16 (image,audio-captions) pairs for the prefix tuning phase. (*): No alignment. (**): Trained in a supervised fashion using audio-caption pairs. Results are ordered by SPIDEr score.", "description": "This table presents the results of the audio captioning task using different methods on the AudioCaps dataset.  It compares the proposed method (DALI with different variations, and the ablation study without audiovisual distillation) against existing state-of-the-art approaches (Shaharabany et al., Salewski et al., and CLAP). The metrics used for comparison are METEOR, ROUGE-L, and SPIDEr.  The table shows the performance of models with and without image input, as well as variations of the DALI approach based on different token distribution alignment methods.  The prefix tuning phase used 16 image-audio caption pairs. The results highlight the superiority of the proposed DALI method.", "section": "5 Results and discussion"}]