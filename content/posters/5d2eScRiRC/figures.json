[{"figure_path": "5d2eScRiRC/figures/figures_1_1.jpg", "caption": "Figure 1: Data usage and optimization flow in MLE, offline and online IRL. Independent of the method, current models use the history of past tokens to predict the next. However, MLE purely optimizes the current output for exact matching the corresponding datapoint while IRL-based methods take into account the impact on future tokens. Online optimization additionally conditions on past model generations rather than the original dataset. Grey and blue objects respectively represent training data and model generations. The impact of future datapoints is often indirect and mediated via learned functions (e.g. the discriminator in GAIL [25] and the Q-function in IQLearn [20]).", "description": "This figure illustrates the differences in data usage and optimization strategies between maximum likelihood estimation (MLE), offline inverse reinforcement learning (IRL), and online IRL for language model training.  MLE focuses solely on matching the next token in a sequence, while IRL methods consider the impact of current predictions on future tokens.  Offline IRL uses existing training data, whereas online IRL incorporates past model generations into the optimization process.", "section": "1 Introduction"}, {"figure_path": "5d2eScRiRC/figures/figures_4_1.jpg", "caption": "Figure 2: GSM8k results for fine-tuning with MLE, IQLearn, and GAIL across different regularization strengths. In particular MLE shows strong performance reduction with higher entropy cost. Larger models demonstrate higher performance but also stronger self similarity across generations, rendering effective trading of between task performance and diversity highly relevant. Error bars indicate the standard error of the mean after repeating the experiment with 3 different seeds.", "description": "This figure shows the results of fine-tuning experiments on the GSM8k dataset using three different methods: Maximum Likelihood Estimation (MLE), Inverse Q-Learning (IQLearn), and Generative Adversarial Imitation Learning (GAIL).  The x-axis represents accuracy (with and without a calculator), and the y-axis represents the diversity of model generations (measured by Self-BLEU).  Different regularization strengths were applied to each method.  The results indicate that MLE's performance significantly decreases with higher entropy cost, while IQLearn and GAIL offer a better trade-off between performance and diversity.  Larger models perform better but also exhibit higher self-similarity, highlighting the value of balancing performance and diversity.", "section": "3 Experiments"}, {"figure_path": "5d2eScRiRC/figures/figures_5_1.jpg", "caption": "Figure 3: XSUM results for models trained with MLE, IQLearn, and GAIL across different regularization strengths. ROUGE 1 and ROUGE 2 are used as performance metrics on the x-axes with Self-BLEU as diversity measure on the y-axis. Entropy regularizing large MLE and GAIL trained models with 0.1 leads to catastrophic results outside the limits of the plot. Figure 9 in the appendix shows the corresponding plots for ROUGE-LSUM.", "description": "This figure displays the results of experiments on the XSUM dataset using different training methods: Maximum Likelihood Estimation (MLE), Inverse Q-Learning (IQLearn), and Generative Adversarial Imitation Learning (GAIL).  The x-axis represents ROUGE-1 and ROUGE-2 scores (metrics for evaluating summarization quality), while the y-axis shows Self-BLEU scores (a measure of the diversity of generated summaries).  Different regularization strengths were used for each method. The figure shows that IQLearn achieves a good balance between high ROUGE scores (good quality) and high Self-BLEU scores (diverse summaries). In contrast, MLE with high entropy regularization performs poorly.  The appendix contains similar plots using ROUGE-LSUM.", "section": "3.2 Quality-diversity evaluations"}, {"figure_path": "5d2eScRiRC/figures/figures_5_2.jpg", "caption": "Figure 4: PaLM2 results for various sampling temperatures with MLE and IQLearn. Left: GSM8k, Mid: TLDR, Right: WMT22, including beam search. By propagating sequence information during training, IQLearn reduces inference time dependency on beam search for improving performance.", "description": "This figure displays the results of experiments using PaLM2 models on three different tasks (GSM8k, TLDR, and WMT22) with two different training methods (MLE and IQLearn) and with varying sampling temperatures.  The left panel shows results for the GSM8k task, the middle for TLDR, and the right for WMT22. Each panel presents three lines representing MLE performance without beam search, IQLearn performance without beam search, and MLE performance with beam search, IQLearn performance with beam search. The x-axis represents the sampling temperature, and the y-axis represents the performance metric (accuracy, ROUGE-LSUM, or BLEU, respectively). The caption highlights that IQLearn's performance is less dependent on beam search, indicating that IQLearn better propagates sequence information during training, leading to improved inference efficiency.", "section": "3.2 Quality-diversity evaluations"}, {"figure_path": "5d2eScRiRC/figures/figures_7_1.jpg", "caption": "Figure 2: GSM8k results for fine-tuning with MLE, IQLearn, and GAIL across different regularization strengths. In particular MLE shows strong performance reduction with higher entropy cost. Larger models demonstrate higher performance but also stronger self similarity across generations, rendering effective trading of between task performance and diversity highly relevant. Error bars indicate the standard error of the mean after repeating the experiment with 3 different seeds.", "description": "The figure shows the results of GSM8k experiments using Maximum Likelihood Estimation (MLE), Inverse Q-learning (IQLearn), and Generative Adversarial Imitation Learning (GAIL) methods with varying regularization strength.  It compares the accuracy (with and without a calculator) for different model sizes (Base, Large, X-Large).  The results highlight a tradeoff between performance and diversity:  higher regularization hurts MLE performance, while larger models perform better but exhibit less diversity.", "section": "3 Experiments"}, {"figure_path": "5d2eScRiRC/figures/figures_8_1.jpg", "caption": "Figure 7: Learning curves for subsets of the XSUM training data. The smallest subsets demonstrate strong overfitting for pure MLE which the TD regularization in IQLearn mitigates. Pure entropy regularization is unable to obtain similar robustness and directly conflicts with task performance.", "description": "This figure shows the learning curves for different model training scenarios on the XSUM dataset, highlighting the impact of dataset size and regularization techniques. It demonstrates that using smaller datasets exacerbates the overfitting problem in Maximum Likelihood Estimation (MLE)-based training. In contrast, Inverse Q-Learning (IQLearn) effectively mitigates this overfitting through its temporal difference regularization, improving model robustness. The figure also points out that simply applying entropy regularization to MLE is insufficient for addressing the overfitting issue.", "section": "3.3 Analysis and ablations"}, {"figure_path": "5d2eScRiRC/figures/figures_8_2.jpg", "caption": "Figure 8: Reward correlation on WMT22 as a function of \u03bb for a fixed mix-in \u03b1 = 0.1 for online data compared to MLE (i.e., \u03bb = 0.0).", "description": "This figure shows the Spearman's rank correlation between the accumulated rewards (over complete sampled trajectories for the full validation sets) for online IQLearn (\u03b1 = 0.1) and task-specific metrics (BLEU and ChrF). It compares the correlations obtained with different values of the regularization parameter \u03bb in IQLearn to the correlation obtained with MLE (\u03bb = 0.0). The plot shows how the correlation between the learned rewards and task performance increases with \u03bb, indicating that IQLearn effectively incorporates task-relevant information into the extracted rewards.", "section": "3.4 Reward analysis"}, {"figure_path": "5d2eScRiRC/figures/figures_8_3.jpg", "caption": "Figure 2: GSM8k results for fine-tuning with MLE, IQLearn, and GAIL across different regularization strengths. In particular MLE shows strong performance reduction with higher entropy cost. Larger models demonstrate higher performance but also stronger self similarity across generations, rendering effective trading of between task performance and diversity highly relevant. Error bars indicate the standard error of the mean after repeating the experiment with 3 different seeds.", "description": "The figure shows the results of experiments on the GSM8k dataset using three different methods for fine-tuning large language models: Maximum Likelihood Estimation (MLE), Inverse Soft Q-learning (IQLearn), and Generative Adversarial Imitation Learning (GAIL).  Different regularization strengths are tested for each method. The results highlight the trade-off between task performance and the diversity of model generations, particularly with larger models. The error bars show the standard error of the mean across multiple runs with different random seeds.", "section": "3 Experiments"}, {"figure_path": "5d2eScRiRC/figures/figures_17_1.jpg", "caption": "Figure 3: XSUM results for models trained with MLE, IQLearn, and GAIL across different regularization strengths. ROUGE 1 and ROUGE 2 are used as performance metrics on the x-axes with Self-BLEU as diversity measure on the y-axis. Entropy regularizing large MLE and GAIL trained models with 0.1 leads to catastrophic results outside the limits of the plot. Figure 9 in the appendix shows the corresponding plots for ROUGE-LSUM.", "description": "This figure displays the performance of different models (MLE, IQLearn, and GAIL) on the XSUM dataset with various regularization strengths.  ROUGE-1 and ROUGE-2 scores (measuring summarization quality) are plotted against Self-BLEU (measuring the diversity of generated summaries).  The results show that IQLearn achieves a good balance between high ROUGE scores and high diversity.  MLE and GAIL, especially with high entropy regularization, struggle to maintain good performance while also increasing diversity.  The appendix contains similar plots using ROUGE-LSUM.", "section": "3.2 Quality-diversity evaluations"}, {"figure_path": "5d2eScRiRC/figures/figures_17_2.jpg", "caption": "Figure 2: GSM8k results for fine-tuning with MLE, IQLearn, and GAIL across different regularization strengths. In particular MLE shows strong performance reduction with higher entropy cost. Larger models demonstrate higher performance but also stronger self similarity across generations, rendering effective trading of between task performance and diversity highly relevant. Error bars indicate the standard error of the mean after repeating the experiment with 3 different seeds.", "description": "This figure presents the results of the GSM8k experiment, comparing the performance of three different methods: Maximum Likelihood Estimation (MLE), Inverse Q-Learning (IQLearn), and Generative Adversarial Imitation Learning (GAIL).  The x-axis represents the model size (Base, Large, X-Large), and the y-axis shows the accuracy. The results show that IRL-based methods (IQLearn and GAIL) exhibit better performance than MLE. Moreover, the impact of the regularization strength on the performance is also visible. The error bars illustrate the standard error of the mean, calculated from multiple experiment runs.", "section": "3 Experiments"}, {"figure_path": "5d2eScRiRC/figures/figures_18_1.jpg", "caption": "Figure 1: Data usage and optimization flow in MLE, offline and online IRL. Independent of the method, current models use the history of past tokens to predict the next. However, MLE purely optimizes the current output for exact matching the corresponding datapoint while IRL-based methods take into account the impact on future tokens. Online optimization additionally conditions on past model generations rather than the original dataset. Grey and blue objects respectively represent training data and model generations. The impact of future datapoints is often indirect and mediated via learned functions (e.g. the discriminator in GAIL [25] and the Q-function in IQLearn [20]).", "description": "This figure compares the data usage and optimization methods in Maximum Likelihood Estimation (MLE), offline Inverse Reinforcement Learning (IRL), and online IRL.  It shows how MLE only focuses on the current token prediction by maximizing likelihood, while IRL methods consider the impact of current actions on future tokens. Online IRL differs by conditioning on past model generations instead of the original dataset.", "section": "1 Introduction"}, {"figure_path": "5d2eScRiRC/figures/figures_18_2.jpg", "caption": "Figure 12: Effect of adding a standard MLE loss (mle_weight=1) on the training data combined with GAIL on XSUM. We show the ROUGE 1 metric and the average length of the generated summaries when training a T5-Large model with GAIL.", "description": "This figure shows the results of training a T5-Large model on the XSUM dataset using GAIL with different values of entropy regularization and MLE loss weight. The top graph shows ROUGE-1 scores, and the bottom shows the average number of tokens in the generated summaries.  The results show that adding an MLE loss improves ROUGE-1 scores but increases the length of the generated summaries.", "section": "3.3 Analysis and ablations"}]