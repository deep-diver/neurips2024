[{"figure_path": "L3RYBqzRmF/tables/tables_5_1.jpg", "caption": "Table 1: Test performance on MNIST, FashionMNIST, CIFAR10, and CIFAR100, evaluated using multi-layer perceptrons. Performance metrics are reported for error backpropagation (BP), feedback alignment (FA), target propagation (DTP), DTP with difference reconstruction loss (DRL), local difference reconstruction loss (L-DRL), fixed-weight difference target propagation (FW-DTP), and cross-correlation loss (CCL). Best values per task are bolded, and second-best values are underlined.", "description": "This table presents the classification accuracy results on four benchmark datasets (MNIST, FashionMNIST, CIFAR10, and CIFAR100) achieved by various biologically plausible learning algorithms including error backpropagation (BP), feedback alignment (FA), target propagation (DTP), DRL, L-DRL, FW-DTP, DRTP, and the proposed counter-current learning (CCL).  The best and second-best results for each dataset are highlighted, enabling a direct comparison of the different methods.", "section": "4.2 Classification Performance"}, {"figure_path": "L3RYBqzRmF/tables/tables_5_2.jpg", "caption": "Table 2: Computational Complexity Comparison on MNIST and CIFAR10. The computational efficiency of various learning algorithms is evaluated by measuring the estimated floating-point operations (FLOPs) per sample for a single training cycle. All measurements were conducted with a batch size of 32 samples. Values are reported in millions (M) of FLOPs, with the best performers highlighted for each dataset.", "description": "This table compares the computational efficiency of different deep learning algorithms (BP, DTP, DRL, L-DRL, FWDTP-BN, and CCL) on MNIST and CIFAR10 datasets. The efficiency is measured in terms of millions of floating-point operations (MFLOPS) per sample per training cycle, with a batch size of 32.  The table highlights the best-performing algorithm for each dataset.", "section": "4 Experiments"}, {"figure_path": "L3RYBqzRmF/tables/tables_5_3.jpg", "caption": "Table 3: Test Accuracy on CIFAR10 and CIFAR100 Using Convolutional Neural Network. The metrics are reported for error backpropagation (BP) and cross-correlation loss (CCL).", "description": "This table presents the test accuracy results on CIFAR-10 and CIFAR-100 datasets using a Convolutional Neural Network (CNN). The performance is compared between two learning algorithms: error backpropagation (BP) and the proposed counter-current learning (CCL).  The table shows the test accuracy and standard deviation for each algorithm on each dataset.", "section": "4.2 Classification Performance"}, {"figure_path": "L3RYBqzRmF/tables/tables_12_1.jpg", "caption": "Table 4: Comparison of Results on CIFAR10 Using VGG-like Convolutional Neural Network.", "description": "This table compares the performance of different algorithms (BP, L-DRL, and CCL) on the CIFAR-10 dataset using a VGG-like convolutional neural network architecture.  It highlights the test accuracy achieved by each algorithm, indicating that L-DRL achieves the highest accuracy, followed by BP, and then CCL.  A key difference is noted, with L-DRL training on a validation set, while the other two methods do not.", "section": "6.2 Comparison of Implementation"}]