[{"figure_path": "lHcvjsQFQq/tables/tables_5_1.jpg", "caption": "Table 1: Objective comparisons for related approaches. Denote a stationary distributions class as Q, i.e., Q := {d \u2208 \u2206(S) : d(s) = (1 \u2212 \u03b3)po(s) + \u03b3\u2211\u0161,\u0101T(s|s,\u0101)d(s,\u0101)\u2200s \u2208 S}", "description": "This table compares the objective functions of several offline imitation learning algorithms.  It highlights the differences in how they handle covariate shift, particularly focusing on distribution matching, adversarial weighting, and distributionally robust optimization techniques. The table shows the different objective functions, including those used in baselines (AW-BC, DR-BC) and the proposed approach (Worst-case weighting). It emphasizes the use of a stationary distribution class Q in the objective functions of distributionally robust methods.", "section": "4.1 Comparison on Baselines Objectives"}, {"figure_path": "lHcvjsQFQq/tables/tables_6_1.jpg", "caption": "Table 2: Comparison of different methods across manipulated datasets in Four Rooms environment. (p(u) = 0.4) Each experiment is repeated with 50 times and the average values with their standard errors are reported. The best average values are highlighted in bold.", "description": "This table presents a comparison of different imitation learning methods across various scenarios with manipulated datasets in the Four Rooms environment.  The manipulation involves adjusting the probability of specific rooms or actions in the dataset to simulate covariate shift. Each method's performance is evaluated using three metrics: Normalized score, Worst-25% performance, and Target 0-1 loss. The results are averaged over 50 repetitions and include standard errors.  The best performing method in each scenario is highlighted in bold.", "section": "4.2 Toy Domain Experiment: Four Rooms domain with Imbalanced Datasets"}, {"figure_path": "lHcvjsQFQq/tables/tables_7_1.jpg", "caption": "Table 2: Comparison of different methods across manipulated datasets in Four Rooms environment. (p(u) = 0.4) Each experiment is repeated with 50 times and the average values with their standard errors are reported. The best average values are highlighted in bold.", "description": "This table presents a comparison of different imitation learning methods across various scenarios in the Four Rooms environment.  The scenarios involve manipulating the dataset to simulate covariate shift by changing the marginal distribution of rooms or actions. The table shows the normalized score, worst-case 25% performance (the average score of the worst 25% episodes), and the target 0-1 loss for each method and scenario. The best average value for each metric is highlighted in bold.", "section": "4.2 Toy Domain Experiment: Four Rooms domain with Imbalanced Datasets"}, {"figure_path": "lHcvjsQFQq/tables/tables_8_1.jpg", "caption": "Table 4: Performance comparison on Scenario 2 (time-dependently collected dataset). Each experiment is repeated 5 times, and the average normalized scores with their standard errors are reported. The highest mean performance scores are highlighted in bold.", "description": "This table presents the results of experiments conducted under the time-dependent data collection scenario in Scenario 2.  The table shows the performance comparison of different offline imitation learning methods across three different tasks (hopper, walker2d, and halfcheetah).  Each task is evaluated with different state distributions, represented by the parameter p(D1), and different numbers of time steps included in data samples (1,1), (1,5), (5,1), and (5,5).  The results are given as the average normalized score (mean \u00b1 standard error) over 5 repetitions.  The method with the best average score for each scenario is highlighted in bold.", "section": "4.4.2 Results"}, {"figure_path": "lHcvjsQFQq/tables/tables_14_1.jpg", "caption": "Table 1: Objective comparisons for related approaches. Denote a stationary distributions class as Q, i.e., Q := {d \u2208 \u2206(S) : d(s) = (1 \u2212 \u03b3)po(s) + \u03b3\u2211\u0161,\u0101T(s|s,\u0101)d(s,\u0101)\u2200s \u2208 S}", "description": "This table summarizes the objective functions of various offline imitation learning approaches, including distribution matching, adversarial weighted behavioral cloning (AW-BC), distributionally robust behavioral cloning (DR-BC), best-case weighting, and the proposed worst-case weighting.  It highlights the differences in how these methods handle covariate shift and the stationary distribution of policies.", "section": "4.1 Comparison on Baselines Objectives"}, {"figure_path": "lHcvjsQFQq/tables/tables_15_1.jpg", "caption": "Table 1: Objective comparisons for related approaches. Denote a stationary distributions class as Q, i.e., Q := {d \u2208 \u2206(S) : d(s) = (1 \u2212 \u03b3)po(s) + \u03b3\u2211\u0161,\u0101T(s|s,\u0101)d(s,\u0101)\u2200s \u2208 S}", "description": "This table compares the objective functions of different offline imitation learning methods. It shows the objective function for distribution matching, adversarial weighted BC (AW-BC), distributionally robust BC (DR-BC), best-case weighting, and the proposed worst-case weighting method.  The table highlights the differences in how these methods address the covariate shift problem in behavioral cloning, particularly focusing on the stationary distribution and robustness to distribution shifts.", "section": "4.1 Comparison on Baselines Objectives"}, {"figure_path": "lHcvjsQFQq/tables/tables_15_2.jpg", "caption": "Table D: Summary of hyperparameters used in D4RL benchmark experiments.", "description": "This table summarizes the hyperparameters used in the D4RL benchmark experiments for different algorithms: BC, DemoDICE, AW-BC, DR-BC, OptiDICE-BC, and DrilDICE.  It includes the policy distribution, batch size, policy learning rate, hidden units, training iterations, and learning rates for the additional parameters (alpha, rho, nu/w). Note that some hyperparameters varied across multiple values; these are shown within brackets.", "section": "4. Experiments"}, {"figure_path": "lHcvjsQFQq/tables/tables_16_1.jpg", "caption": "Table E: Number of sub-trajectories used in each scenario. Each sub-trajectory consists of one initial transition and 50 subsampled transitions from a complete trajectory. To enhance dataset support, one complete trajectory is appended for each specified count of sub-trajectories.", "description": "This table shows the number of sub-trajectories used in each scenario of the experiments.  Each sub-trajectory consists of an initial transition and 50 subsampled transitions. To ensure sufficient data, a complete trajectory is added to each sub-trajectory count.", "section": "D Experimental Settings for D4RL Benchmark"}, {"figure_path": "lHcvjsQFQq/tables/tables_17_1.jpg", "caption": "Table 2: Comparison of different methods across manipulated datasets in Four Rooms environment. (p(u) = 0.4) Each experiment is repeated with 50 times and the average values with their standard errors are reported. The best average values are highlighted in bold.", "description": "This table presents a comparison of the performance of several algorithms on a Four Rooms environment with imbalanced datasets.  The algorithms are compared based on three metrics: Normalized score, Worst-25% performance, and Target 0-1 loss. The table is organized by the type of data manipulation (Room Marginal Manipulation and Action Marginal Manipulation).  Each manipulation is tested with a probability of 0.4, and the results show that DrilDICE consistently outperforms other methods across various scenarios.", "section": "4.2 Toy Domain Experiment: Four Rooms domain with Imbalanced Datasets"}, {"figure_path": "lHcvjsQFQq/tables/tables_18_1.jpg", "caption": "Table 4: Performance comparison on Scenario 2 (time-dependently collected dataset). Each experiment is repeated 5 times, and the average normalized scores with their standard errors are reported. The highest mean performance scores are highlighted in bold.", "description": "This table presents the performance comparison of different imitation learning methods on Scenario 2, which involves time-dependent data collection.  The results show the normalized scores (with standard errors) for different tasks (hopper, walker2d, halfcheetah) and various proportions of dataset D1 (representing the time-dependent data). The best-performing method for each setting is highlighted in bold.", "section": "4.4.2 Results"}]