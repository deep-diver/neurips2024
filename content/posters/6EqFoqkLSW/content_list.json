[{"type": "text", "text": "Learning Discrete Latent Variable Structures with Tensor Rank Conditions ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhengming Chen1, 2, Ruichu $\\mathbf{Cai}^{1,*}$ , Feng Xie?, Jie Qiao, Anpeng $\\mathbf{W}\\mathbf{u}^{2,4}$ , Zijian $\\mathbf{Li}^{2}$ ,Zhifeng Hao6, Kun Zhang2,6,\\* ", "page_idx": 0}, {"type": "text", "text": "1. Scnool o1 Lomputer Science, Guangaong University o1 lecnnology 2. Machine Learning Department, Mohamed bin Zayed University of Artificial Intelligence 3. Department of Applied Statistics, Beijing Technology and Business University 4. Department of Computer Science and Technology, Zhejiang University 5. College of Science, Shantou University, Shantou, Guangdong, China 6. Department of Philosophy, Carnegie Mellon University ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Unobserved discrete data are ubiquitous in many scientific disciplines, and how to learn the causal structure of these latent variables is crucial for uncovering data patterns. Most studies focus on the linear latent variable model or impose strict constraints on latent structures, which fail to address cases in discrete data involving non-linear relationships or complex latent structures. To achieve this, we explore a tensor rank condition on contingency tables for an observed variable set $\\mathbf{X}_{p}$ , showing that the rank is determined by the minimum support of a specific conditional set (not necessary in $\\mathbf{X}_{p}$ )that d-separates all variables in $\\mathbf{X}_{p}$ .Bythis, one can locate the latent variable through probing the rank on different observed variables set, and further identify the latent causal structure under some structure assumptions. We present the corresponding identification algorithm and conduct simulated experiments to verify the effectiveness of our method. Our results elegantly expand the application scope of causal discovery with latent variables. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Social scientists, psychologists, and researchers from various disciplines are often interested in understanding causal relationships between the latent variables that cannot be measured directly, such as depression, coping, and stress (Silva et al., 2006). A common approach to grasp these latent concepts is to construct a measurement model. For instance, experts design a set of measurable items or survey questions that serve as indicators of the latent variable and then use them to infer causal relationships among latent variables (Bollen, 2002; Bartholomew et al., 2011; Cui et al., 2018). ", "page_idx": 0}, {"type": "text", "text": "Numerous approaches exist for addressing structure learning among latent variables. In particular, if the data generation process is assumed to be a linear relationship, known as linear latent variable models, several approaches have been developed. These include the second-order statistic-based approaches (Silva et al., 2006; Kummerfeld and Ramsey, 2016; Chen et al., 2024; Sullivant et al., 2010), high-order moments-based ones (Xie et al., 2020; Chen et al., 2022; Cai et al., 2019; Adams et al., 2021), matrix decomposition-based methods (Anandkumar et al., 2013, 2014, 2015), and copula model-based approaches (Cui et al., 2018). Moreover, the hierarchical latent variable structure has been well-studied within the linear setting (Huang et al., 2022; Xie et al., 2022; Chen et al., 2023; Jin et al., 2023). However, the linear assumption is rather restrictive and the discrete data in the real world could be more frequently encountered (e.g., responses from psychological and educational ", "page_idx": 0}, {"type": "image", "img_path": "6EqFoqkLSW/tmp/9732fff378a516b3665c6933452696f7f0335fed888b57f28abaedae451cc81e.jpg", "img_caption": ["(a) A latent structure with conditional probability tables. "], "img_footnote": [], "page_idx": 1}, {"type": "image", "img_path": "6EqFoqkLSW/tmp/eff28cff5830f27c71df75048c99b6da87fdde17fb171bcdbbbaef8419abb306.jpg", "img_caption": ["(b) The decomposition of the joint distribution. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Figure 1: Mlustrating the graphical criteria of the tensor rank condition, the rank of the joint distribution is determined by the support of a specific conditional set that d-separates all observed variables, i.e., $\\operatorname{Rank}(\\mathbb{P}(X_{1},{\\dot{X_{2}}}))=|{\\mathrm{isupp}}(L)|\\doteq2$ See Example 3.4 for details. ", "page_idx": 1}, {"type": "text", "text": "assessments or social science surveys (Eysenck et al., 2021; Skinner, 2019)), which does not satisfy the linear assumption. ", "page_idx": 1}, {"type": "text", "text": "When the data generation process is discrete, however, due to the challenging nonlinear transition relationship in discrete data, few identifiability results exist and are mostly only applicable in strict cases. In particular, under some prespecified structure, the identifiability of parameters is established, such as in the hidden Markov model(HMM) (Anandkumar et al., 2012) model, topic models (Anandkumar et al., 2014), and multiview mixtures model (Anandkumar et al., 2015). By further specifying the latent variable structure as a tree, Wang et al. (2017); Song et al. (2013) show that the structural model is identifiable. Recently, Gu (2022); Gu and Dunson (2023) further considered the identifiability of pyramid structure under the condition that each latent variable has at least three observed children. However, challenges persist in extending identifiability to more general structures among discrete latent variables. Existing approaches, unfortunately, cannot identify the causal structure of latent variables as shown in Fig. 2(a). ", "page_idx": 1}, {"type": "text", "text": "Recently, some studies have shown that causal structures involving discrete latent confounders can be effectively identified, building on the identifiability results of mixture models, as discussed in (Kant et al., 2024; Gordon et al., 2023; Mazaheri et al., 2023; Anandkumar et al., 2012). Most of these works focus on the causal structure among observed variables, usually assuming a single latent confounder. For the identification of latent structure, (Kivva et al., 2021) shows that causal structures of discrete latent variables can be identified by recovering the latent distribution from a mixture oracle. However, while a general discrete latent variable model can be identified theoretically, estimating the parameters of the mixture model is challenging. Approximating methods are often applied, but these may be unrealistic and impractical in real-world situations. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we aim to establish a general identification criterion for discrete latent structures in cases where latent structures exhibit flexible dependencies, while also developing a simple but robust structure learning algorithm. To achieve this, we explore a tensor rank condition on the contingency tables for an observed variable set $\\mathbf{X}_{p}$ , to probe the latent causal structure from observed data. Interestingly, as shown in Fig. 1, we found that the rank of the contingency tables of the joint distribution $\\mathbb{P}(\\bar{X_{1}},X_{2})$ is deeply connected to the support of a variable $L$ (not necessary among $X_{1},X_{2})$ that d-separate $X_{1}$ and $X_{2}$ . By this observation, we first develop a general tensor rank condition for the discrete causal model and show that such a rank is determined by the minimal support of a specific conditional set (not necessary in $\\mathbf{X}_{p.}$ ) that d-separates all variables in $\\mathbf{X}_{p}$ . Such findings intrigue the possibility to identify the discrete latent variables structure. We further propose a discrete latent structure model that accommodates more general latent structures and shows that the discrete latent variable structure can be identified locally and iteratively through tensor rank conditions. Subsequently, we present an identification algorithm to complete the identifiability of discrete latent structure models, including the measurement model and the structure model. We theoretically show that under proper causal assumptions, such as faithfulness and the Markov assumption, the measurement model is fully identifiable and the structure model can be identified up to a Markov equivalenceclass. ", "page_idx": 1}, {"type": "image", "img_path": "6EqFoqkLSW/tmp/de37dcba17cd817f87a71a37392b320e5f392e0e3e18c2cbdc076801e503421e.jpg", "img_caption": ["Figure 2: An example of discrete latent structure model involving 4 latent variables and 12 observed variables (sub-fig (a)). Here, the red edges form a measurement model, while the blue edges form a structural model. The theoretical result of this paper is shown in sub-fig (c). "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "The contributions of this work are three-fold. (1) We first establish a connection between the tensor rank condition and the graphical patterns in a general discrete causal model, including specific dseparation relations. (2) We then exploit the tensor rank condition to learn the discrete latent variable model, allowing flexible relations between latent variables. (3) We present a structure learning algorithm using tensor rank conditions and demonstrate the effectiveness of the proposed algorithm throughsimulationstudies. ", "page_idx": 2}, {"type": "text", "text": "2  Discrete Latent Structure Model ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "For an integer $m$ , denote $[m]\\,=\\,\\{1,2,\\cdots,m\\}$ . Consider a discrete statistic model with $k$ latent variable set $\\mathbf{L}=\\{L_{1},\\cdots,L_{k}\\},L_{i}\\in[r_{i}]$ and $m$ discrete observed variable set $\\mathbf{X}=\\{X_{1},\\cdots,X_{m}\\}$ with $X_{i}\\,\\in\\,[d_{i}]\\;(r_{i},d_{i}\\,\\geq\\,2)$ , in which any marginal probabilities are non-zero. We say a discrete statistic model is a discrete causal model if and only if ${\\mathbf{V}}={\\mathbf{L}}\\cup{\\mathbf{X}}$ can be represented by a directed acyclic graph (DAG), denoted by $\\mathcal{G}$ .We use $\\operatorname{supp}(V_{i})=\\{v\\in\\mathbb{Z}^{+}:\\mathbb{P}(V_{i}=v)>0\\}$ to denote the set of possible values of the random variable $V_{i}$ . Our work is in the framework of causal graphical models. Concepts used here without explicit definition, such as ${\\mathrm{d}}.$ -separation, which can refer to standard literature (Spirtes et al., 2000). ", "page_idx": 2}, {"type": "text", "text": "In this paper, we focus on learning causal structure among latent variables in one class of discrete causal models. The model is defined as follows. ", "page_idx": 2}, {"type": "text", "text": "Definition 2.1 (Discrete Latent Structure Model with Three-Pure Children). A discrete causal model is the Discrete Latent Structure Model with Three-Pure Children(Discrete 3PLSM) if it further satisfiesthefollowingthreeassumptions: ", "page_idx": 2}, {"type": "text", "text": "1)[Purity Assumption] there is no direct edges between the observed variables;   \n2)[Three-Pure Child Variable Assumption] each latent variable has at least three pure variables as children;   \n3)[Sufficient Observation Assumption] The dimension of observed variables support is larger than thedimension of any latent variables support. ", "page_idx": 2}, {"type": "text", "text": "These structural constraints inherent in the discrete 3PLSM are also widely used in linear latent variable models, e.g., Silva et al. (2006); Kummerfeld and Ramsey (2016); Cai et al. (2019); Xie et al. (2020). In the binary latent variable case, recently, a similar definition is also employed in Gu and Dunson (2023); Gu (2022). The key difference is that there are no constraints on the latent structure in our work. An example of a discrete 3PLSM model is shown in Fig. 2(a), where $L_{1},\\cdots,L_{4}$ represent discrete latent variables, and $X_{1},\\cdots,X_{12}$ are discrete observed (measured) variables. ", "page_idx": 2}, {"type": "text", "text": "Generally speaking, the discrete 3PLSM model can be divided into two sub-models (Spirtes et al., 2000), i.e., the measurement model and the structure model, e.g., red edge and blue edge in Fig. 2 (a). By this, one can first identify the measurement model to determine the latent variables and then use the measured variable to infer the causal structure of latent variables. As shown in Fig. 2 (b), we will separately discuss the identification of the two sub-models and show that the measurement model is fully identifiable and the structure model is identified up to a Markov equivalence class. The symbols used in our work is summarised in Table 1. ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "To ensure the identification of causal structure and the asymptotic correctness of identification algorithms, some common causal assumptions are required. ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.2 (Causal Markov Assumption). Let $\\mathcal{G}$ be a causal graph with vertex set $\\mathbf{V}$ and $\\mathbb{P}_{\\mathbf{V}}$ be probability distribution over the vertices in $\\mathbf{V}$ generatedby $\\mathcal{G}$ .We say $\\mathcal{G}$ and $\\mathbb{P}_{\\mathbf{V}}$ satisfy theCausal Markov Assumption if and only if for every $V_{i}\\in\\mathbf{V}$ $\\mathbb{P}(V_{i},\\mathbf{V}\\mid\\mathrm{Des}_{V_{i}}\\,|\\mathrm{Pa}_{V_{i}}\\,=\\,i)\\,=\\,\\mathbb{P}(W|\\mathrm{Pa}_{V_{i}}\\,=$ $i)\\mathbb{P}(\\mathbf{V}\\mid\\operatorname{Des}_{V_{i}}\\lvert\\operatorname{Pa}_{V_{i}}=i)$ \uff1a ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.3 (Faithfulness Assumption). Let $\\mathcal{G}$ be a causal graph with vertex set $\\mathbf{V}$ and $\\mathbb{P}_{\\mathbf{V}}$ be probability distribution over the vertices in $\\mathbf{V}$ generated by $\\mathcal{G}$ We $s a y<\\mathcal{G},\\mathbb{P}_{\\mathbf{V}}>$ satisfies the FaithfulnessAssumptionif and only $i f(i)$ \uff1aevery conditional independence relation true in $\\mathbb{P}_{\\mathbf{V}}$ is entailed by the Causal Markov Assumption applied to $\\mathcal{G}$ and $(i i)$ for any joint distribution $\\mathbb{P}(\\mathbf{L}_{p})$ \uff0c there does not exist $\\mathbb{P}(\\mathbf{L}_{q})$ with $\\vert\\mathrm{supp}(\\mathbf{L}_{q})\\vert<\\vert\\mathrm{supp}(\\mathbf{L}_{p})\\vert$ such that $\\mathbb{P}(\\mathbf{L}_{p})=\\mathbb{P}(\\mathbf{L}_{q})$ ", "page_idx": 3}, {"type": "text", "text": "Assumption 2.4 (Full Rank Assumption). For any conditional probability $\\mathbb{P}(X|\\mathrm{Pa}_{X})$ ,thecorresponding contingency table is full rank, i.e., each column of $\\mathbb{P}(X|\\mathrm{Pa}_{X})$ is linearly independent with the other columnvectors in the parameter space. ", "page_idx": 3}, {"type": "text", "text": "In general, Assumptions $2.2\\sim2.3$ are widely used in the constraint-based causal discovery methods, e.g., PC algorithm and FCI algorithm (Spirtes et al., 2000; Spirtes and Glymour, 1991). One can see that we further constraint the parameter space of joint distribution cannot be reduced to a lowdimension space, for maintaining the diversity of parameter space. This is also the reason for Assumption 2.4, which aligns with the non-degeneracy condition used in (Kivva et al., 2021). ", "page_idx": 3}, {"type": "text", "text": "Our goal: The goal of our work is to develop a robust approach for identifying discrete latent structure models, including both the measurement and structural models. ", "page_idx": 3}, {"type": "table", "img_path": "6EqFoqkLSW/tmp/5442868341f338614e0b82230ea5f4f637ed3ab0456cfdac55cb9ff843a3423a.jpg", "table_caption": [], "table_footnote": ["Table 1: Mathematical notations used in this paper. "], "page_idx": 3}, {"type": "text", "text": "3  Tensor Rank Condition with Graphical Criteria ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To address the identification problem in the discrete 3PLSM, this section introduces the building block-the tensor rank condition of the discrete causal model. Then, we establish the connection between tensor rank and d-separation relations under a general discrete causal model. ", "page_idx": 3}, {"type": "text", "text": "Before formalizing the tensor rank condition, we first give the explicit definition of tensor rank. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.1 Rank-one Tensor). An n-way tensor $\\mathcal{T}\\in\\mathbb{R}^{I_{1}\\times\\cdots\\times I_{n}}$ is a rank-one tensorif it can be written as the outer product of n vectors, i.e., ", "page_idx": 3}, {"type": "equation", "text": "$$\n{\\mathcal{T}}=\\mathbf{u}_{1}\\otimes\\mathbf{u}_{2}\\otimes\\cdots\\otimes\\mathbf{u}_{n},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\mathbf{u}_{i}$ are vectors that each represent a dimension of the tensor, $\\otimes$ represents the outer product. ", "page_idx": 3}, {"type": "text", "text": "Definition 3.2 (Tensor Rank Kolda and Bader (2009). For an $n$ -way tensor $\\mathcal{T}\\in\\mathbb{R}^{I_{1}\\times\\cdots\\times I_{n}}$ , the rank ofa tensor $\\tau$ is defined as the smallest number of rank-one tensors that sum to exactly represent $\\tau$ Formally,therankoftensor $\\tau$ ,denoted $\\mathrm{rank}({\\mathcal{T}})$ isthesmallestinteger $r$ such that: ", "page_idx": 3}, {"type": "equation", "text": "$$\nT=\\sum_{i=1}^{r}\\mathbf{u}_{1}^{\\left(i\\right)}\\otimes\\mathbf{u}_{2}^{\\left(i\\right)}\\otimes\\cdots\\otimes\\mathbf{u}_{n}^{\\left(i\\right)},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "whereeach $\\mathbf{u}_{k}^{\\left(i\\right)}$ is a vector in the corresponding vector space associated with the $k$ -thmodeof $\\tau$ ", "page_idx": 3}, {"type": "text", "text": "In other words, the tensor rank denotes the minimal number of rank-one decompositions. In the discrete causal model, the joint distribution can be represented as a tensor, e.g., the joint distribution of two random variables is a two-way contingency tensor. Interestingly, by carefully analyzing the (non-negative) rank-one decomposition of the joint distribution, we find that the tensor rank essentially reveals structural information within the causal graph. The result is presented below. ", "page_idx": 4}, {"type": "text", "text": "Theorem 3.3 (Graphical implication of tensor rank condition). In the discrete causal model, suppose Assumptions $2.2\\sim$ Assumption 2.4 hold.Consider an observedvariable set $\\mathbf{X}_{p}\\,=\\,\\{X_{1},\\cdots,X_{n}\\}$ $(\\mathbf{X}_{p}\\,\\subseteq\\,\\mathbf{X}$ and $n\\,\\geq\\,2,$ )and the corresponding $n$ -way probability tensor $\\mathcal{T}_{(\\mathbf{X}_{p})}$ that is the tabular representation of the joint probability mass function $\\mathbb{P}(X_{1},\\cdots,X_{n})$ . Then, $\\operatorname{Rank}(\\mathcal{T}_{(\\mathbf{X}_{p})})=r\\,(r>1)$ if and only if (i) there exist a conditional set $\\mathbf{S}\\subset\\mathbf{V}$ with $|\\mathrm{supp}(\\mathbf{S})|=r$ that $d$ -separates any pair of variables in $\\{X_{1},\\cdots,X_{n}\\}$ and $(i i)$ does not exist conditional set $\\tilde{\\mathbf{S}}$ that satisfies $|\\mathrm{supp}(\\tilde{\\mathbf{S}})|<r$ ", "page_idx": 4}, {"type": "text", "text": "We further provide an example to illustrate Theorem 3.3. ", "page_idx": 4}, {"type": "text", "text": "Example 3.4 (Illustrating the graphical criteria). Consider a single latent variable structure as shown in Fig. 1 (a) where $L$ is a latent variable with $\\mathrm{supp}(L)=\\{0,1\\}$ and $X_{1},X_{2}$ are observed variables with $\\operatorname{supp}(X_{i})\\,=\\,\\{0,1,2\\},i\\in\\{1,2\\}$ .For convenience, we denote $p_{i}\\,=\\,\\mathbb{P}(L\\,=\\,i)$ $\\hat{p}_{i|j}=\\mathbb{P}(X_{1}=$ $i|L=j)$ , and $\\tilde{p}_{i|j}=\\mathbb{P}(X_{2}=i|L_{1}=j)$ .The joint distribution $\\mathbb{P}(X_{1},X_{2})$ can be expressed as the product of conditional probabilities, as shown in Fig. $I(b)$ By applying the tensor decomposition, we observe that $\\mathbb{P}(X_{1},X_{2})$ canbedecomposed as thesum oftworank-onetensors: $\\mathbb{P}(X_{1},\\tilde{X}_{2}|L=0)$ and $\\mathbb{P}(X_{1},X_{2}|L\\ =\\ 1)$ .Thus, the rank of the tensor $\\mathbb{P}(X_{1},X_{2})$ is two, corresponding to the cardinalityof thelatentvariable'ssupport.Thereason $\\mathbb{P}(X_{1},X_{2}|L=i)$ is a rank-one tensor is that $L$ d-separates $X_{1}$ and $X_{2}$ i.e., $\\mathbb{P}(X_{1},X_{2}|L=i)=\\mathbb{P}(X_{1}|L=i)\\otimes\\mathbb{P}(X_{2}|L=i)$ . This illustrates the connection between tensor rank and $d$ -separation relations. ", "page_idx": 4}, {"type": "text", "text": "Intuitively, the graphical criteria theorem suggests that, in the discrete causal model, the tensor rank condition implies the minimal conditional probability decomposition within the probability parameter space, which hopefully induces the structural identifiability of the discrete 3PLSM model. ", "page_idx": 4}, {"type": "text", "text": "4  Structure Learning of Discrete Latent Structure Model ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "In this section, we address the identification problem of the discrete 3PLSM model using a carefully designed algorithm that leverages the tensor rank condition. Specifically, we first show that latent variables can be identified by finding causal clusters among observed variables (Sec. 4.1). Then, we use these causal clusters to conduct conditional independence tests among latent variables based on the tensor rank condition, identifying the structure model (Sec. 4.2). Finally, we discuss the practical implementation of testing tensor rank (Sec. 4.3). For simplicity, we focus on the case where all latent variables have the same number of categories. The result can be directly extended to cases with different numbers of categories (see details in Appendix E). ", "page_idx": 4}, {"type": "text", "text": "4.1  Identification of the measurement model ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To answer the identification of the measurement model, one common strategy is to find the causal cluster that shares the common latent parent, which has been well-studied within the linear model, such as Silva et al. (2006); Cai et al. (2019); Xie et al. (2020). We follow this strategy and show that, in the discrete 3PLSM, the causal cluster can be found by testing the tensor rank conditions iteratively. The definition of a causal cluster is as follows. ", "page_idx": 4}, {"type": "text", "text": "Definition 4.1 (Causal cluster). In the discrete 3PLSM, the variable set $\\{X_{1},\\cdots,X_{n}\\}$ isacausal cluster,termed $C_{i}$ if and only if all variables in $\\{X_{1},\\cdots,X_{n}\\}$ share the common latent parent. ", "page_idx": 4}, {"type": "text", "text": "It is not hard to see that, the measurement model can be identified if all causal cluster is found. In order to find these causal clusters by making use of the tensor rank condition, the key issue is to determine the support of latent variables in advance. This issue can be addressed by identifying the rank of the two-way tensor formed by the joint distribution of two observed variables. ", "page_idx": 4}, {"type": "text", "text": "Proposition 4.2 (Identification of support of latent variables). In the discrete 3PLSM model suppose Assumptions $2.2\\sim$ Assumption 2.4 hold. The support of the latent variable corresponds to the rank of thetwo-wayprobability contingency tablefor any pair ofobservedvariables $X_{i}$ and $X_{j}$ ,i.e., $|\\mathrm{supp}(L)|=\\mathrm{Rank}({\\mathcal{T}}_{(X_{i},{\\bar{X}}_{j})})$ $\\forall X_{i},X_{j}\\in\\mathbf{X}$ ", "page_idx": 4}, {"type": "text", "text": "This result holds because any pair of variables in the discrete 3PLSM model is ${\\mathrm d}$ separatedbyany one of their latent parent variables, and all latent variables have the same support. Next, we formalize the property of clusters and give the criterion for finding clusters. ", "page_idx": 5}, {"type": "text", "text": "Proposition 4.3 (Identification of causal cluster). In the discrete 3PLSM mode, suppose Assumption   \n$2.2\\sim$ Assumption 2.4 hold. Let $r=|\\mathrm{supp}(L_{i})|$ denote the cardinality of thelatent support. Given   \nthree disjoint observed variables $X_{i},X_{j},X_{k}\\in\\mathbf{X}$   \n\u00b7 Rulel: if the rank of tensor $\\smash{\\tau_{(X_{i},X_{j},X_{k})}}$ is not qual to $r$ i.e, $\\operatorname{Rank}(\\mathcal{T}_{(X_{i},X_{j},X_{k})})\\neq r,$ then $X_{i}$ $X_{j}$ and $X_{k}$ belong to the different latent parents.   \n\u00b7 $\\mathcal{R}u l e2$ :for any $X_{s}$ \uff0c $X_{s}\\,\\in\\,{\\bf X}\\:\\backslash\\:\\{X_{i},X_{j},X_{k}\\}$ , if the rank of tensor $\\mathcal{T}_{(X_{i},X_{j},X_{k},{X_{s}})}$ is $r$ i.e, $\\operatorname{Rank}({\\mathcal{T}}_{(X_{i},X_{j},X_{k},X_{s})})=r$ then $\\{X_{i},X_{j},X_{k}\\}$ share the same latent parent.   \nExample 4.4 (Finding causal clusters). Let's take Fig. $2(a)$ as an example. One can find that for   \n$\\{X_{1},X_{2},X_{3},X_{k}\\}$ where $X_{k}\\in{\\bf X}\\mid\\{X_{1},X_{2},X_{3}\\}$ , the rank of tensor $\\mathcal{T}_{(X_{1},X_{2},X_{3},X_{k})}$ . $r$ .Thus,   \n$\\{X_{1},X_{2},X_{3}\\}$ is identifed as a causal cluster ", "page_idx": 5}, {"type": "text", "text": "Next, we consider the practical issues involved in determining the number of latent variables by causal clusters. That is, there are some causal clusters that should be merged because they share one latent parent. We find that the overlapping clusters can be directly merged into one cluster. This is because the overlapping clusters have the same latent variable as the parent under the discrete 3PLSM model. The validity of the merge step is guaranteed by Proposition 4.5. ", "page_idx": 5}, {"type": "text", "text": "Proposition 4.5 (Merging Rule). In the discrete 3PLSM model, for two causal clusters $C_{1}$ and $C_{2}$ if $C_{1}\\cap C_{2}\\neq\\emptyset$ ,then $C_{1}$ and $C_{2}$ share the same latent parent. ", "page_idx": 5}, {"type": "text", "text": "Based on the above results, one can iteratively identify causal clusters and apply the merger rule to detect all latent variables. The identification procedure is summarized in Algorithm 1. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 Finding the causal cluster   \nInput: Data from a set of measured variables $\\mathbf{X}_{\\mathcal{G}}$ , and the dimension of latent support $r$   \nOutput: Causal cluster $\\mathcal{C}$   \n1: Initialize the causal cluster set ${\\mathcal{C}}:=\\emptyset$ , and $\\mathcal{G}^{\\prime}=\\emptyset$   \n2: // Identify Causal Skeleton   \n3: Begin the recursive procedure   \n4: repeat   \n5: for each $X_{i},X_{j}$ and $X_{k}\\in\\mathbf{X}$ do   \n6: if $\\operatorname{Rank}({\\mathcal{T}}_{\\{X_{i},X_{j},X_{k}\\}})\\neq r$ then   \n7: Continue; // Rule1 of Prop. 4.3   \n8: end if   \n9: $\\operatorname{Rank}({\\mathcal{T}}_{\\{X_{i},X_{j},X_{k},X_{s}\\}})=r$ , for all $X_{s}\\in\\mathbf{X}\\mid\\{X_{i},X_{j},X_{k}\\}$ then   \n10: $\\mathbf{C}=\\mathbf{C}\\cup\\{\\{X_{i},X_{j},X_{k}\\}\\}$   \n11: end if   \n12: end for   \n13: until no causal cluster is found.   \n14: // Merging cluster and introducing latent variables   \n15: Merge all the overlapping sets in $\\mathbf{C}$ by Prop. 4.5.   \n16: for each $C_{i}\\in\\mathbf{C}$ do   \n17:  Introduce a latent variable $L_{i}$ for $C_{i}$   \n18: $\\mathcal{G}=\\mathcal{G}\\cup\\{L_{i}\\rightarrow X_{j}|X_{j}\\in C_{i}\\}$   \n19: end for   \n20: return Graph $\\mathcal{G}$ and causal cluster $\\mathcal{C}$ ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.6 (Identification of the measurement model). In the discrete 3PLSM, suppose Assumption $2.2\\sim$ Assumption 2.4 hold. The measurement model is fully identifiable by Algorithm 1. ", "page_idx": 5}, {"type": "text", "text": "4.2 Identification of the structure model ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Once the measurement model is identified, the observed children can serve as proxies for the latent variables, enabling the identification of the causal structure among them. Here, we employ constraintbased framework to learn the causal structure of latent variables. ", "page_idx": 5}, {"type": "text", "text": "Constraint-based structure learning algorithms find the Markov equivalence class over a set of variables by making decisions about independence and conditional independence among them. Given a pure and accurate measurement model with at least two measures per latent variable, we can test for independence and conditional independence (CI) among the latent variables. Specifically, to test statistical independence between discrete variables, one can examine whether the rank of their joint distribution contingency table is one (Sullivant, 2018). For testing conditional independence (CI) relations among latent variables, further leveraging the algebraic properties of the tensor rank condition is required (see Theorem 4.7). ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.7 (d-separation among latent variable). In the discrete 3PLSM, suppose Assumption 2.2 $\\sim$ Assumption 2.4 hold. Let r denote the cardinality of the latent support. Then, $L_{i}$ \u2161 $L_{j}|\\mathbf{L}_{p}$ if and onlyif $\\operatorname{Rank}(\\mathcal{T}_{(X_{i},X_{j},\\mathbf{X}_{p1},\\mathbf{X}_{p2})})=r^{|\\mathbf{L}_{p}|},$ where $X_{i}$ and $X_{j}$ are the pure childenof $L_{i}$ and $L_{j}$ $\\mathbf{X}_{p1}$ and $\\mathbf{X}_{p2}$ are two disjoint child sets of $\\mathbf{L}_{p}$ that satisfy $\\forall L_{i}\\in\\mathbf{L}_{p},\\mathrm{Ch}_{L_{i}}\\cap\\mathbf{X}_{p1}\\neq\\emptyset,\\mathrm{Ch}_{L_{i}}\\cap\\mathbf{X}_{p2}\\neq\\emptyset$ ", "page_idx": 6}, {"type": "text", "text": "Intuitively, based on the graphical criteria of tensor rank condition, $\\mathbf{L}_{p}$ is a minimal conditional set in the causal gaph that $\\mathrm{^d}$ -separates $\\mathbf{X}_{p1}$ and $\\mathbf{X}_{p2}$ and hence the rank of tensor $\\mathcal{T}_{(X_{i},X_{j},\\mathbf{X}_{p1},\\mathbf{X}_{p2})}$ is the dimension of support of $\\mathbf{L}_{p}$ , if $X_{i}$ and $X_{j}$ also be ${\\mathrm d}$ -separated by $\\mathbf{L}_{p}$ ", "page_idx": 6}, {"type": "text", "text": "Example 4.8 (CI test among latent variables). Consider the structure in Fig. $2(a)$ and suppose $r\\,=\\,2$ .By selecting $\\mathbf{X}_{p1}\\,=\\,\\bigl\\lbrace X_{4},X_{7}\\bigr\\rbrace$ and $\\mathbf{X}_{p2}\\,=\\,\\{X_{5},X_{8}\\}$ as two disjoint child sets of $\\{L_{2},L_{3}\\}$ respectively, let ${\\bf X}_{p}=\\{X_{1},X_{10},X_{4},X_{5},X_{7},X_{8}\\}$ and ${\\bf L}_{p}\\,=\\,\\{L_{2},L_{3}\\}$ .One cansee that therank of tensor $\\mathcal{T}_{(\\mathbf{X}_{p})}$ is four since $\\mathbf{L}_{p}$ (i.e., $\\{L_{2},L_{3}\\}_{.}$ )isminimalconditionalset that $d\\sb{\\l}$ separatesanypair variable in $\\mathbf{X}_{p}$ which imply that $\\mathbb{L}_{1}\\perp\\!\\!\\!\\perp\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\perp\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\! $ ", "page_idx": 6}, {"type": "text", "text": "Based on Theorem 4.7, we introduce the PC-TENSOR-RANK algorithm. This method accepts a measurement model learned by the previous procedure, and outputs the Markov equivalence class of the structural model associated with the latent variables within the measurement model, in accordance with the PC algorithm. The implementation is summarised as Algorithm 2. Consequently, we establish the identification of the structure model as shown in Theorem 4.9. ", "page_idx": 6}, {"type": "text", "text": "Algorithm 2 PC-TENSOR-RANK ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Input: Data set $\\mathbf{X}=\\{X_{1},\\ldots,X_{m}\\}$ and causal cluster $\\mathcal{C}$ Output: A partial DAG $\\mathcal{G}$ ", "page_idx": 6}, {"type": "text", "text": "1: Initialize the maximal conditions set dimension $k$   \n2: Let $L_{i}$ denote as $C_{i}$ $C_{i}\\in\\mathcal{C}$   \n3: Form the complete undirected graph $\\mathcal{G}$ on the latent variable set L;   \n4: for $\\forall L_{i},L_{j}\\in\\mathbf{L}$ and adjacent in $\\mathcal{G}$ do   \n5: I/Test the $C I$ relations among latent variables by Theorem 4.7   \n6: if $\\exists\\mathbf{L}_{p}\\subseteq\\mathbf{L}\\mid\\{L_{i},L_{j}\\}$ and $(|\\mathbf{L}_{p}|<k)$ such that $L_{i}$ \u2161 $L_{j}|\\mathbf{L}_{p}$ hold then   \n7: delete edge $L_{i}-L_{j}$ from $G$ ", "page_idx": 6}, {"type": "text", "text": "8: end if ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "10: Search $\\mathrm{v}$ structures and apply meek rules Meek (1995).   \n11: return a partial DAG $\\mathcal{G}$ of latent variables. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.9 (Identification of structure model). In the discrete 3PLSM, suppose Assumption $2.2\\sim$ Assumption 2.4 hold. Given the measurement model, the causal structure over the latent variable is identified up to a Markov equivalent class by the PC-TENSOR-RANK algorithm. ", "page_idx": 6}, {"type": "text", "text": "4.3Practical Test for Tensor Rank ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In our theoretical results, the key issue is to test the rank of a tensor, which involves estimating the dimension of latent support and the rank of a tensor. Here, we aim to explore methods to (i) estimate the rank of the contingency matrix for determining the dimension of latent support, and (i) apply the goodness-of-fit test to assess the tensor rank. ", "page_idx": 6}, {"type": "text", "text": "Estimate the rank of contingency matrix.  We start with the estimation of the dimension of latent variables support based on Prop. 4.2. There are many practical approaches used to estimate the rank of a general matrix M, such as Camba-M\u00e9ndez and Kapetanios (2009). In our implementation, we use the characteristic root statistic, abbreviated as CR statistic Robin and Smith (2000), to test the rank of the probability contingency matrix of two observed variables. Specifically, Let M be an asymptotically normal estimator of M, then the CR statistic is the sum of $d-r$ smallest singular valuesof $\\tilde{\\mathrm{M}}$ , multiplied by the sample size. Under the null hypothesis, the above statistic converges in distribution to a weighted (given by the eigenvalues) sum of independent $\\chi_{1}^{2}$ random variables. ", "page_idx": 6}, {"type": "table", "img_path": "6EqFoqkLSW/tmp/6fc99c3c75d421198a22627912719033c06a8c872d0366db45a703da81977a5c.jpg", "table_caption": ["Table 2: Results on learning pure measurement models, where the data is generated by the discrete 3PLSM. Lower value means higher accuracy. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Goodness-of-fit test for tensor rank. Once the dimension of the support of latent variables is identified, in the structure learning procedure, we perform the following hypotheses test: $\\mathcal{H}_{0}$ $\\mathrm{Rank}(\\mathcal{T})\\,=\\,r\\ \\nu.s.$ $\\mathcal{H}_{1}$ $\\mathrm{Rank}(\\mathcal{T})\\neq\\bar{r}$ . To achieve this, we first apply the canonical polyadic (CP) decomposition technology to the target tensor $\\tau$ as a sum of $r$ rank-one tensors given specified $r$ , then we evaluate how well the reconstructed tensor from this decomposition approximates the original tensor to conduct the hypotheses test. ", "page_idx": 7}, {"type": "text", "text": "To perform the rank-decomposition with specified $r$ on the probability contingency tensor, one can use the non-negative CP decomposition to decompose the tensor into the sum of $r$ rank-onetensor Shashua and Hazan (2005). Given the decomposition, one can obtain a reconstructed tensor, denoted by $\\tilde{\\mathcal{T}}$ , from the outer product of decomposed vectors. ", "page_idx": 7}, {"type": "text", "text": "With the reconstructed tensor, we constructed square-chi goodness of fit test Cochran (1952) for testing $\\mathrm{Rank}(\\tau)=r$ . Such a test is frequently used to summarize the discrepancy between observed values and the expected values, which measure the sum of differences between observed and expected outcome frequencies. Let vec $(\\tau)$ be the vectorization of tensor $\\tau$ , suppose vec $(\\tilde{\\mathcal{T}})$ be the asymptotic normalty stmator of $\\mathbf{vec}({\\mathcal{T}})$ we hav thechi-sguae staisticas $\\begin{array}{r}{\\chi^{\\bar{2}}=\\sum_{i\\in\\mathbf{vec}(\\tilde{\\mathcal{T}})}\\frac{\\left(\\mathbf{vec}(\\mathcal{T})_{i}-\\mathbf{\\bar{vec}}(\\tilde{\\mathcal{T}})_{i}\\right)^{2}}{\\mathbf{vec}(\\tilde{\\mathcal{T}})_{i}}}\\end{array}$ which follows the $\\chi^{2}$ distribtion with freedom degrees $\\begin{array}{r}{\\prod_{i\\in[n]}d_{i}-(\\sum_{i,j\\in[n]}d_{i}d_{j})}\\end{array}$ ", "page_idx": 7}, {"type": "text", "text": "5 Simulation Studies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we conducted simulation studies to assess the correctness of the proposed methods. The baseline approaches include Building Pure Cluster (BPC) Silva et al. (2006), Latent Tree Model (LTM) Choi et al. (2011), and Bayesian Pyramid Model (BayPy) Gu and Dunson (2023). ", "page_idx": 7}, {"type": "text", "text": "In the following simulation studies, we consider the different combinations of various types of structure models(SM) and measurement models(MM). Specifically, for the structure model, we consider the following five typical cases: [SM1]: $L_{1}\\to L_{2}$ ; [SM2]: $L_{1}\\rightarrow L_{2}\\rightarrow L_{3}$ ; [SM3]: the structure of latent variables is shown in Fig. 2(a); [Collider]: ${L_{1}\\,\\rightarrow\\,L_{2}\\,\\leftarrow\\,L_{3}}$ ;[Star] : $L_{1}\\,\\rightarrow$ $L_{2},L_{1}\\rightarrow L_{3},L_{1}\\rightarrow L_{4}$ . For the measurement model, we consider the following two cases: [MM1]: each latent variable has three pure observed variables, i.e., $L_{i}\\rightarrow\\{X_{1},X_{2},X_{3}\\}$ ; [MM2]: each latent variable has four pure observed variables, i.e., $L_{i}\\rightarrow\\left\\{X_{1},X_{2},X_{3},X_{4}\\right\\}$ ", "page_idx": 7}, {"type": "text", "text": "In all cases, the data generation process follows the discrete 3PLSM model: (i) we generate the probability contingency table of latent variables in advance, according to different latent structures (e.g., SM1), then (ii) we generate the conditional contingency table of observed variables (condition on their latent parent), and finally (i) we sample the observed data according to the probability contingency table, where the dimension of latent support $r$ is set to 3 and the dimension of all observed variables support is set to 4, sample size ranged from $\\{5\\mathrm{k},10\\mathrm{k},50\\mathrm{k}\\}$ ", "page_idx": 7}, {"type": "table", "img_path": "6EqFoqkLSW/tmp/637d437feb91d9811df93dd13c4127c3f1fed8f9f4709b0275b244842d08f65c.jpg", "table_caption": ["Table 3: Results on learning the structure model. The symbol '-\u2032 indicates that the current method does not output this information. Lower value means higher accuracy. "], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "For each simulation study, we randomly generate the dataset and apply the proposed algorithm and baselines to these data. We use the following scores for evaluating the performance of causal clusters from each algorithm: latent omission, latent commission, and mismeasurement. Moreover, to assess the ability of these algorithms to correctly discover the causal structure among latent variables, we use the metric like edge omission (EO), edge commission (EC), and orientation omission (OO). These metrics can be referred to Silva et al. (2006), in which the tasks are aligned with our work. Each experiment was repeated ten times with randomly generated data, and the results were averaged. ", "page_idx": 8}, {"type": "text", "text": "The results are reported in Table 2 and Table 3. Our method consistently delivers the best outcomes across most scenarios, demonstrating its capability to identify both the causal clusters and the causal structures of latent variables. In contrast, the BPC approach performs poorly, as it is specifically designed for linear models. Additionally, the LTM and BayPy algorithms show suboptimal performance in structure learning of latent variables due to their limitations to specific structural models, such as tree structures, or assumptions that latent variables are binary. ", "page_idx": 8}, {"type": "text", "text": "6 Discussion and Further Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The preceding sections presented the identification of discrete 3PLSM. In this section, we examine whether the impure structure (e.g., an edge between observed variables) can be detected through tensor rank conditions. For instance, consider the structure shown in Fig. 3 and suppose all observed variable has the same support. One can observe that for any subset $\\{X_{i},X_{j},X_{k}\\}\\subset$ $\\{X_{1},X_{2}^{-},X_{3},X_{4},X_{5}\\}$ where $\\{X_{4},X_{5}\\}\\ \\dot{\\mathcal{F}}\\ \\{X_{i},X_{j},X_{k}\\}$ ,onehas ", "page_idx": 8}, {"type": "equation", "text": "$$\n\\bigwedge_{X_{1}}^{\\widehat{\\scriptstyle{\\mathcal{A}}}}\\widehat{\\scriptstyle{\\mathcal{A}}}_{X_{2}}\n$$", "text_format": "latex", "page_idx": 8}, {"type": "image", "img_path": "", "img_caption": ["Figure 3: Example of identifying the impure structure. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "$\\operatorname{Rank}({\\mathcal{T}}_{(X_{i},X_{j},X_{k})})\\,=\\,|\\operatorname{supp}(L)|$ . This implies $\\{\\dot{X}_{1},X_{2},X_{3}\\}$ and $X_{4}$ (or $X_{5}$ ) share the common latent parent. Moreover, we have $\\operatorname{Rank}({\\mathcal T}_{(X_{4},X_{5})})=|\\operatorname{supp}(X_{5})|$ , assuming the cardinality of the observed variables is larger than that of the latent variabies. By the graphical criteria of the tensor rank condition, one can infer that there is an edge between $X_{4}$ and $X_{5}$ , indicating the impure structure can be identified by the tensor rank condition. Developing an efficient algorithm to learn a more general discrete 3PLSM structure that allows impure structure in a principled way is part of our future work. ", "page_idx": 8}, {"type": "text", "text": "7 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "We derive a nontrivial algebraic property of a particular type of discrete causal model under proper causal assumptions. We build the connection between tensor rank and the d-separation relations in the causal graph and propose the graphical criteria of tensor rank. By this, the identifiability of causal structure in discrete latent structure models is achieved based on which we proposed an identification to locate latent causal variables and identify their causal structure. We provide a practical test approach for testing the tensor rank and verifying the efficientness of the proposed algorithm via the simulated studies. The proposed theorems and the algorithms take a meaningful step in understanding the causal mechanism of discrete data. However, the proposed method can hardly be applied to high-dimensional discrete data and only applies to pure-children structures. Therefore, relaxing these restrictions and making them scalable to high-dimensional real-world datasets and more general structural constraints would be a meaningful future direction. ", "page_idx": 8}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This research was supported in part by National Key R&D Program of China (2021ZD0111501), National Science Fund for Excellent Young Scholars (62122022), Natural Science Foundation of China (61876043, 61976052, 62476163), the major key project of PCL (PCL2021A12). ZM would like to acknowledge the support of the China Scholarship Council (CSC). FX would like to acknowledge the support by the Natural Science Foundation of China (62306019). JQ would like to acknowledge the support by the Natural Science Foundation of China (62406080). KZ would like to acknowledge the support from NSF Award No. 2229881, AI Institute for Societal Decision Making (AI-SDM), the National Institutes of Health (NIH) under Contract R01HL159805, and grants from Salesforce, Apple Inc., Quris AI, and Florin Court Capital. We appreciate the comments from anonymous reviewers and Area Chairs, which greatly helped to improve the paper. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Silva, R.; Scheine, R.; Glymour, C.; Spirtes, P. Learning the structure of linear latent variable models. JMLR 2006, 7, 191-246.   \nBollen, K. A. Latent variables in psychology and the social sciences. Annual review of psychology 2002,53,605-634.   \nBartholomew, D. J.; Knott, M.; Moustaki, I. Latent variable models and factor analysis: A unified approach; John Wiley & Sons, 2011.   \nCui, R.; Groot, P.; Schauer, M.; Heskes, T. Learning the Causal Structure of Copula Models with Latent Variables. Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence, UAI 2018. 2018; pp 188-197.   \nKummerfeld, E.; Ramsey, J. Causal clustering for 1-factor measurement models. KDD. 2016; pp 1655-1664.   \nChen, Z.; Qiao, J.; Xie, F.; Cai, R.; Hao, Z.; Zhang, K. Testing Conditional Independence Between Latent Variables by Independence Residuals. IEEE Transactions on Neural Networks and Learning Systems2024,   \nSullivant, S.; Talaska, K.; Draisma, J.; others Trek separation for Gaussian graphical models. The Annals of Statistics 2010, 38, 1665-1685.   \nXie, F; Cai, R.; Huang, B.; Glymour, C.; Hao, Z.; Zhang, K. Generalized Independent Noise Conditionfor Estimating Latent Variable Causal Graphs. NeurIPS. 2020; pp 14891-14902.   \nChen, Z.; Xie, F.; Qiao, J.; Hao, Z.; Zhang, K.; Cai, R. Identification of Linear Latent Variable Model with Arbitrary Distribution. Proceedings 36th AAAI Conference on Artificial Intelligence (AAAI). 2022.   \nCai, R.; Xie, F; Glymour, C.; Hao, Z.; Zhang, K. Triad Constraints for Learning Causal Structure of Latent Variables. NeurIPS. 2019; pp 12863-12872.   \nAdams, J.; Hansen, N.; Zhang, K. Identification of partially observed linear causal models: Graphical conditions for the non-gaussian and heterogeneous cases. Advances in Neural Information Processing Systems. 2021.   \nAnandkumar, A.; Hsu, D.; Javanmard, A.; Kakade, S. Learning linear bayesian networks with latent variables. International Conference on Machine Learning. 2013; pp 249-257.   \nAnandkumar, A.; Ge, R.; Hsu, D. J.; Kakade, S. M.; Telgarsky, M.; others Tensor decompositions for learning latent variable models. J. Mach. Learn. Res. 2014, 15, 2773-2832.   \nAnandkumar, A.; Ge, R.; Janzamin, M. Learning overcomplete latent variable models through tensor methods. Conference on Learning Theory. 2015; pp 36-112.   \nHuang, B.; Low, C. J. H.; Xie, F.; Glymour, C.; Zhang, K. Latent hierarchical causal structure discovery with rank constraints. Advances in Neural Information Processing Systems. 2022.   \nXie, F.; Huang, B.; Chen, Z.; He, Y.; Geng, Z.; Zhang, K. Identification of linear non-Gaussian latent hierarchical structure. International Conference on Machine Learning. 2022; pp 24370-24387.   \nChen, Z.; Xie, F.; Qiao, J.; Hao, Z.; Cai, R. Some general identification results for linear latent hierarchical causal structure. Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence. 2023; pp 3568-3576.   \nJin, S.; Xie, E.; Chen, G.; Huang, B.; Chen, Z.; Dong, X.; Zhang, K. Structural Estimation of Partially Observed Linear Non-Gaussian Acyclic Model: A Practical Approach with Identifiability. The Twelfth International Conference on Learning Representations. 2023.   \nEysenck, . B. Barrett P T: Sakofsk, D. The junor yenck eronality qustinairePenality and Individual Differences 2021, 169, 109974.   \nSkinner, C. Analysis of categorical data for complex surveys. International Statistical Review 2019, 87, S64-S78.   \nAnandkumar, A.; Hsu, D.; Kakade, S. M. A method of moments for mixture models and hidden Markov models. Conference on learning theory. 2012; pp 33-1.   \nWang, X.; Guo, J.; Hao, L.; Zhang, N. L. Spectral methods for learning discrete latent tree models. Statistics and Its Interface 2017, 10, 677-698.   \nSong, L.; Ishteva, M.; Parikh, A.; Xing, E.; Park, H. Hierarchical tensor decomposition of latent tree graphical models. International Conference on Machine Learning. 2013; pp 334-342.   \nGu, Y. Blessing of Dependence: Identifiability and Geometry of Discrete Models with Multiple Binary Latent Variables. arXiv preprint arXiv:2203.04403 2022,   \nGu, Y.; Dunson, D. B. Bayesian pyramids: Identifiable multilayer discrete latent structure models for discrete data. Journal of the Royal Statistical Society Series B: Statistical Methodology 2023, 85, 399-426.   \nKant, M.; Ma, E. Y.; Staicu, A.; Schulman, L. J.; Gordon, S. Identifiability of Product of Experts Models. International Conference on Artificial Intelligence and Statistics. 2024; pp 4492-4500.   \nGordon, S. L.; Mazaheri, B.; Rabani, Y; Schulman, L. Causal Inference Despite Limited Global Confounding via Mixture Models. Conference on Causal Learning and Reasoning. 2023; Pp 574-601.   \nMazaheri, B.; Gordon, S.; Rabani, Y; Schulman, L. Causal Discovery under Latent Class Confounding. arXiv preprint arXiv:2311.07454 2023,   \nAnandkumar, A.; Hsu, D.; Huang, F; Kakade, S. M. Learning high-dimensional mixtures of graphical models. arXiv preprint arXiv: 1203.0697 2012,   \nKivva, B.; Rajendran, G.; Ravikumar, P; Aragam, B. Learning latent causal graphs via mixture oracles. Advances in Neural Information Processing Systems 2021, 34.   \nSpirtes, P.; Glymour, C.; Scheines, R. Causation, Prediction, and Search; MIT press, 2000.   \nSpirtes, P.; Glymour, C. An algorithm for fast recovery of sparse causal graphs. Social science computer review 1991, 9, 62-72.   \nKolda, T. G.; Bader, B. W. Tensor decompositions and applications. SIAM review 2009, 51, 455-500.   \nSullivant, S. Algebraic statistics; American Mathematical Soc., 2018; Vol. 194.   \nMeek, C. Causal inference and causal explanation with background knowledge. UAI. 1995; pp 403-410.   \nCamba-M\u00e9ndez, G.; Kapetanios, G. Statistical tests and estimators of the rank of a matrix and their applications in econometric modelling. Econometric Reviews 2009, 28, 581-611.   \nRobin, J.-M.; Smith, R. J. Tests of rank. Econometric Theory 2000, 16, 151-175.   \nShashua, A.; Hazan, T. Non-negative tensor factorization with applications to statistics and computer vision. Proceedings of the 22nd international conference on Machine learning. 2005; pp 792-799.   \nCochran, W. G. The $\\chi2$ test of goodness of fit. The Annals of mathematical statistics 1952, 315-345.   \nChoi, M. J.; Tan, V. Y.; Anandkumar, A.; Willsky, A. S. Learning Latent Tree Graphical Models. Journal of Machine Learning Research 2011, 12, 1771-1812.   \nPearl, J. Causality: Models, Reasoning, and Inference, 2nd ed.; Cambridge University Press: New York,2009.   \nLeonard, T.; Novick, M. R. Bayesian full rank marginalization for two-way contingency tables. Journal of Educational Statistics 1986, 11, 33-56.   \nBartolucci, F.; Colombi, R.; Forcina, A. An extended class of marginal link functions for modelling contingency tables by equality and inequality constraints. Statistica Sinica 2007, 691-711.   \nKruskal, J. B. Three-way arrays: rank and uniqueness of trilinear decompositions, with application to arithmetic complexity and statistics. Linear algebra and its applications 1977, 18, 95-138.   \nHackbusch, W. Tensor spaces and numerical tensor calculus; Springer, 2012; Vol. 42.   \nKoch, K.-R.; Koch, K.-R. Bayes? theorem. Bayesian Inference with Geodetic Applications 1990, 4-8.   \nAish, A.-M.; Joreskog, K. G. A panel model for political efficacy and responsiveness: An application of LISREL 7 with weighted least squares. Quality and Quantity 1990, 24, 405-426.   \nJoreskog, K. G.; Sorbom, D. LISREL 8: User's reference guide; Scientific Software International, 1996.   \nSalles, J.; Stephan, F.; Moliere, F.; Bennabi, D.; Haffen, E.; Bouvard, A.; Walter, M.; Allauze, E.; Llorca, P. M.; Genty, J. B.; others Indirect effect of impulsivity on suicide risk through self-esteem and depressive symptoms in a population with treatment-resistant depression: A FACE-DR study. Journal of affective disorders 2024, 347, 306-313. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "Supplementary Material ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "The supplementary material contains ", "page_idx": 12}, {"type": "text", "text": "\u00b7 Graphical Notations;   \n\u00b7 Example of Tensor Representations of Joint Distribution;   \n\u00b7 Discussion of Our Assumptions;   \n\u00b7 Proofs of Main Results; - Proof of Theorem 3.3; - Proof of Proposition 4.2; - Proof of Proposition 4.3; - Proof of Proposition 4.5; - Proof of Theorem 4.6; - Proof of Theorem 4.7; - Proof of Theorem 4.9.   \n\u00b7 Extension of Different Latent State Space;   \n\u00b7 Discussion with the Hierarchical Structures;   \n\u00b7 Practical Estimation of Tensor Rank;   \n\u00b7 More Experimental Results;   \n\u00b7 Experimental Results on Real-world Dataset; ", "page_idx": 12}, {"type": "text", "text": "A  Graphical Notations ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Below, we provide some graphical notation used in our work, which is mainly derived from the Pearl (2009); Spirtes et al. (2000). ", "page_idx": 12}, {"type": "text", "text": "Definition A.1 (Path and Directed Path). In a DAG, a path $P$ is a sequence of nodes $\\left(V_{1},...V_{r}\\right)$ such that $V_{i}$ and $V_{i+1}$ are adjacent in $\\mathcal{G}$ where $1\\leq i<r$ Further, we say a path $P=\\left(V_{i_{0}},V_{i_{1}},\\ldots,V_{i_{k}}\\right)$ in $G$ is a directed path if it is a sequence of nodes of $G$ where there is a directed edge from $V_{i_{j}}$ to $V_{i_{(j+1)}}$ for any $0\\leq j\\leq k-1$ ", "page_idx": 12}, {"type": "text", "text": "Definition A.2 (Collider). A collider on a path $\\{V_{1},...V_{p}\\}$ is a node $V_{i}$ \uff0c $1<i<p,$ such that $V_{i-1}$ and $V_{i+1}$ are parents of $V_{i}$ ", "page_idx": 12}, {"type": "text", "text": "Graphically, we also say a collider is a $\\mathbf{\\dot{V}}.$ structure'. ", "page_idx": 12}, {"type": "text", "text": "Definition A.3 (d-separation). A path $p$ is said tobe $d\\sb{\\l}$ -separated (or blocked) by a set of nodes Z if and only if the following two conditions hold: ", "page_idx": 12}, {"type": "text", "text": "\u00b7 p contains a chain $V_{i}\\rightarrow V_{k}\\rightarrow V_{j}$ or a fork $V_{i}\\leftarrow V_{k}\\rightarrow V_{j}$ such that the middle node $V_{k}$ is in $\\mathbf{Z}$   \n\u00b7 p contains a collider $V_{i}\\to V_{k}\\leftarrow V_{j}$ such that the middle node $V_{k}$ is not in $\\mathbf{Z}$ and such that no descendant of $V_{k}$ is in $\\mathbf{Z}$ ", "page_idx": 12}, {"type": "text", "text": "A set $\\mathbf{Z}$ is said to ${\\mathrm d}$ -separate A and $\\mathbf{B}$ if and only if $\\mathbf{Z}$ blocks every path from a node in $\\mathbf{A}$ to a node in B. We also denote as A $\\mathbf{\\mu}_{\\mathrm{~\\tiny~\\perp~B~}}|\\mathbf{Z}$ in the causal graph model. ", "page_idx": 12}, {"type": "text", "text": "B  Example of Tensor Representations of Joint Distribution ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "Consider a single latent variable structure that has three pure observed variables, i.e., $L_{1}~\\rightarrow$ $\\{X_{1},X_{2},X_{3}\\}$ .We aim to illustrate the tensor representation of the probability contingency table and the tensor rank condition for the joint distribution $\\mathbb{P}(X_{1}X_{2}\\hat{X}_{3})$ . For convenience, let $\\operatorname{supp}(L_{1})\\ =\\ \\{0,1\\}$ and $\\operatorname{supp}(X_{i})\\ =\\ \\{0,\\dot{1},2\\}$ .We further denote $p_{i j}\\ =\\ \\mathbb{P}(X_{1}\\ =\\ i,X_{2}\\ =\\ j)$ $p_{i|j}^{\\hat{\\mathbf{\\alpha}}}=\\mathbb{P}(X_{1}=i|L_{1}=j)$ \uff0c $\\tilde{p_{i|j}}\\r=\\mathbb{P}(X_{2}=i|L_{1}=j)$ \uff0c $\\bar{p_{i}}=\\mathbb{P}(L_{1}=i)$ , and $p_{i j k}\\,=\\mathbb{P}(X_{1}\\,=\\,i,X_{2}\\,=$ $j,X_{3}=k)$ . For the joint distribution of $\\mathbb{P}(X_{1}X_{2})$ , we have the tensor representation as follows: ", "page_idx": 12}, {"type": "equation", "text": "$$\n\\mathcal{T}_{(X_{1}X_{2})}=\\left[\\!\\!\\begin{array}{c c c}{p_{00}}&{p_{01}}&{p_{02}}\\\\ {p_{10}}&{p_{11}}&{p_{12}}\\\\ {p_{20}}&{p_{21}}&{p_{22}}\\end{array}\\!\\!\\right]=\\underbrace{\\left[\\!\\!\\begin{array}{c c c}{p_{0|0}}&{p_{0|1}}\\\\ {p_{1|0}}&{p_{1|1}}\\\\ {p_{2|0}^{\\hat{c}}}&{p_{2|1}^{\\hat{c}}}\\end{array}\\!\\!\\right]}_{\\mathcal{T}_{(X_{1}|L1)}}\\cdot\\underbrace{\\left[\\!\\!\\begin{array}{c c c}{\\bar{p_{0}}}&{0}\\\\ {0}&{\\bar{p_{1}}}\\end{array}\\!\\!\\right]}_{\\mathrm{Diag}(\\mathcal{T}_{(L_{1})})}\\cdot\\underbrace{\\left[\\!\\!\\begin{array}{c c c}{p_{0|0}}&{p_{1|0}}&{p_{2|0}}\\\\ {p_{0|1}^{\\tilde{\\mathbf{v}}}}&{p_{1|2}^{\\tilde{\\mathbf{v}}}}&{p_{2|1}^{\\tilde{\\mathbf{v}}}}\\end{array}\\!\\!\\right]}_{\\mathcal{T}_{(X_{2}|L1)}^{\\top}},\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\mathcal{T}_{(X_{1}\\mid L_{1})}$ is the tensor representation of $\\mathbb{P}(X_{1}|L_{1}),\\mathcal{T}_{(X_{2}|L_{1})}$ is the tensor representation of $\\mathbb{P}(X_{2}|L_{1})$ and $\\mathrm{Diag}(\\tau_{(L_{1})})$ is the diagonalization of $\\mathbb{P}(L_{1})$ \uff1a ", "page_idx": 13}, {"type": "text", "text": "Under the Full Rank assumption, we have $\\mathcal{T}_{(X_{1}\\mid L_{1})}$ and $\\mathcal{T}_{(X_{2}\\mid L_{1})}$ are column full rank. Thus, the rank of $\\mathcal{T}_{X_{1}X_{2}}$ is two, i.e., $\\operatorname{Rank}({\\mathcal T}_{X_{1}X_{2}})=|\\operatorname{supp}(L_{1})|=2$ . This illustrate the Prop. 1. ", "page_idx": 13}, {"type": "text", "text": "Next, we consider the three-way tensor $\\mathcal{T}_{(X_{1}X_{2}X_{3})}$ of the joint distribution $\\mathbb{P}(X_{1}X_{2}X_{3})$ . We will represent the three-way tensor as its frontal slices Kolda and Bader (2009), i.e., three matrices for $\\mathbb{P}\\bar{(}X_{1}X_{2}X_{3}=0)$ $\\mathbb{P}(\\dot{X}_{1}X_{2}X_{3}=1)$ and $\\mathbb{P}(X_{1}X_{2}X_{3}=2)$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\underbrace{\\left[{{p_{000}}\\quad p_{010}}\\quad{p_{020}}\\atop{p_{100}}\\quad{p_{110}}\\quad{p_{120}}\\atop{p_{210}}\\quad{p_{220}}\\right]}_{\\mathcal{T}_{(X_{1}X_{2}X_{3}=0)}},\\quad\\underbrace{\\left[{{p_{001}}\\quad p_{011}\\quad p_{021}}\\atop{p_{101}}\\quad{p_{111}}\\quad{p_{111}}\\quad{p_{121}}\\atop{p_{211}}\\quad{p_{221}}\\quad{p_{222}}\\right]}_{\\mathcal{T}_{(X_{1}X_{2}X_{3}=1)}},\\quad\\underbrace{\\left[{{p_{002}}\\quad p_{012}}\\quad{p_{022}}\\atop{p_{112}}\\quad{p_{112}}\\quad{p_{122}}\\atop{p_{211}}\\quad{p_{221}}\\quad{p_{222}}\\right]}_{\\mathcal{T}_{(X_{1}X_{2}X_{3}=2)}}.\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "In the contingency tensor above, the element of $\\mathcal{T}_{(X_{1}X_{2}X_{3})}$ ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{P}(X_{1}=i,X_{2}=j,X_{3}=k)}\\\\ &{\\;\\;\\;=\\displaystyle\\sum_{r=0}^{1}\\mathbb{P}(X_{1}=i,X_{2}=j,X_{3}=k|L_{1}=r)\\mathbb{P}(L_{1}=r)}\\\\ &{\\;\\;\\;=\\displaystyle\\sum_{r=0}^{1}\\mathbb{P}(X_{1}=i|L_{1}=r)\\mathbb{P}(X_{2}=j|L_{1}=r)\\mathbb{P}(X_{3}=k|L_{1}=r)\\mathbb{P}(L_{1}=r).}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "One can represent the tensor $\\scriptstyle{\\mathcal{T}}_{X_{1}X_{2}X_{3}|L_{1}=r}$ as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\mathcal{T}_{(X_{1}X_{2}X_{3}|L_{1}=r)}=\\left[\\mathbb{P}(X_{1}=0|L_{1}=r)\\right]\\otimes\\left[\\mathbb{P}(X_{2}=0|L_{1}=r)\\right]\\otimes\\left[\\mathbb{P}(X_{3}=0|L_{1}=r)\\right],\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where $\\otimes$ represent the outer product, e.g., for two vector $\\mathbf{u}$ and $\\mathbf{v}$ $(\\mathbf{u}\\otimes\\mathbf{v})_{i j}\\;=\\;u_{i}v_{j}$ with $\\mathbf{u}=$ $\\{u_{1},\\cdots,u_{n}\\}$ and $\\mathbf{v}=\\{v_{1},\\cdots,v_{n}\\}$ ", "page_idx": 13}, {"type": "text", "text": "According to the definition of tensor rank, one can see that $\\mathcal{T}_{(X_{1}X_{2}X_{3}\\mid L_{1}=r)}$ is a rank-one tensor. Furthermore, since $\\begin{array}{r}{\\mathcal{T}_{(X_{1}X_{2}X_{3})}=\\sum_{r=1}^{2}\\mathcal{T}_{(X_{1}X_{2}X_{3}|L_{1}=r)}\\mathcal{T}_{(L_{1}=r)}.}\\end{array}$ the rank of $\\mathcal{T}_{(X_{1}X_{2}X_{3})}$ is two under Assumption $2.2\\sim$ Assumption 2.4. This is because $L_{1}$ d-separates $X_{i}$ and $X_{j}$ with $\\forall i,j\\in[1,2,3]$ which illistrate the graphical criteria of tensor rank condition. ", "page_idx": 13}, {"type": "text", "text": "C  Discussion of Our Assumptions ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "To study the discrete statistical model, certain commonly-used parameter assumptions are necessary. For instance, the Full Rank assumption (Assumption 2.4), as utilized in our study, ensures diversity within the parameter space. This is crucial to prevent the contingency table of the joint distribution from collapsing into a lower-dimensional space. There are related works that also use such an assumption Leonard and Novick (1986); Bartolucci et al. (2007); Gu (2022). Essentially, in our work, such an assumption ensures the effectiveness of rank decomposition (e.g., minimal decomposition or unique decomposition Kruskal (1977)), which induces the structural identifiability of a discrete causal model. Moreover, the sufficient observation assumption that the dimension of latent support is larger than the dimension of observed support is reasonable, as it ensures sufficient measurement of the latent variable. We also discuss this assumption in the remark E.4 that demonstrates the CI relation is testable if this assumption holds. ", "page_idx": 13}, {"type": "text", "text": "It is worth noting that, although Kivva et al. (2021) shows that the causal structure among latent variables can be identifiable under weaker assumptions, by leveraging the identifiability of the mixture model, we emphasize that our algorithm, based on the tensor rank condition, is simpler and more efficient. It can also produce more robust results when performing causal discovery on the observational data. Moreover, in Kivva et al. (2021), the parameters of the mixture oracle, such as the number of components, are estimated using approximate methods like the K-means algorithm, which may not be applied directly to discrete data. ", "page_idx": 14}, {"type": "text", "text": "D Proofs of Main Results ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "D.1 Proof of Theorem 3.3 ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Proof. \"If\" part: ", "page_idx": 14}, {"type": "text", "text": "We prove this by contradiction, i.e., suppose (i). there exists a variable set $\\mathbf{S}$ in the causal graph $\\mathcal{G}$ with $\\left|\\operatorname{supp}(\\mathbf{S})\\right|\\mathbf{\\bar{\\alpha}}=\\mathbf{\\alpha}r$ that ${\\mathrm{d}}\\cdot$ -separates any pair of variables in $\\{X_{1},\\cdots,X_{n}\\}$ , and (ii).does no exist conditional set $\\tilde{\\mathbf{S}}$ that satisfies $|\\mathrm{supp}(\\tilde{\\mathbf{S}})|<r$ , then $\\operatorname{Rank}({\\mathcal{T}}_{\\{X_{1}\\ldots X_{n}\\}})\\neq r$ . There are two cases we need to consider: Case 1 $:\\operatorname{Rank}({\\mathcal{T}}_{\\{X_{1}\\ldots X_{n}\\}})>r$ , and Case 2: $\\mathrm{Rank}({\\mathcal T}_{\\{X_{1}...X_{n}\\}})<r$ ", "page_idx": 14}, {"type": "text", "text": "Case 1: Due to $\\mathbf{S}$ is conditional set that d-separates all variables in $\\{X_{1},\\cdots,X_{n}\\}$ , then we have $\\begin{array}{r}{\\mathbb{P}(X_{1}\\cdots X_{n})~=~\\sum_{i=1}^{r}(\\prod_{j=1}^{n}\\mathbb{P}(X_{j}|\\mathbf{S\\=\\i}))\\mathbb{P}(\\mathbf{S\\=\\i})}\\end{array}$ When $\\operatorname{Rank}({\\mathcal{T}}_{\\{X_{1}\\ldots X_{n}\\}})~>~r$ let $\\operatorname{Rank}({\\mathcal{T}}_{\\{X_{1}\\ldots X_{n}\\}})=k>r$ ,there are ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{T}_{\\left\\{X_{1}\\ldots X_{n}\\right\\}}}\\\\ &{\\quad=\\displaystyle\\sum_{i=1}^{r}\\mathcal{T}_{\\left(X_{1}|\\mathbf{S}=i\\right)}\\otimes\\cdots\\otimes\\mathcal{T}_{\\left(X_{n}|\\mathbf{S}=i\\right)}\\mathcal{T}_{\\left(\\mathbf{S}=i\\right)}}\\\\ &{\\quad=\\displaystyle\\sum_{j=1}^{k}\\mathbf{u}_{1}^{(j)}\\otimes\\cdots\\otimes\\mathbf{u}_{n}^{(j)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which violates the definition of tensor rank (i.e., $k$ is not a minimal rank-one decomposition, it can be reduced to the smaller decomposition with $r$ ). Meanwhile, if there exists other conditional sets S such that $\\begin{array}{r}{\\mathbb{P}(X_{1}{\\cdots}X_{n})=\\sum_{i=1}^{k}(\\prod_{j=1}^{n}\\mathcal{T}_{(X_{j}|\\tilde{\\mathbf{S}}=i)})\\mathcal{T}_{(\\tilde{\\mathbf{S}}=i)}}\\end{array}$ is minimal rank decomposition, t violates the condition (i) that $|\\mathrm{supp}(\\mathbf{S})|$ is minimal. , ", "page_idx": 14}, {"type": "text", "text": "Case 2: When $\\mathrm{Rank}(T_{\\{X_{1}...X_{n}\\}})<r$ ,let $\\operatorname{Rank}({\\mathcal{T}}_{\\left\\{X_{1}\\ldots X_{n}\\right\\}})=t<r$ due to $\\mathbf{S}$ is conditional set with smallest support $r$ in the causal graph, one have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{T}_{\\{X_{1}\\ldots X_{n}\\}}}\\\\ &{\\quad=\\displaystyle\\sum_{i=1}^{r}\\mathcal{T}_{(X_{1}|\\mathbf{S}=i)}\\otimes\\cdots\\otimes\\mathcal{T}_{(X_{n}|\\mathbf{S}=i)}\\mathcal{T}_{(\\mathbf{S}=i)}}\\\\ &{\\quad=\\displaystyle\\sum_{j=1}^{t}\\mathbf{u}_{1}^{(j)}\\otimes\\cdots\\otimes\\mathbf{u}_{n}^{(j)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "which means that there at least exist $\\scriptstyle{\\mathcal{T}}_{(X_{1}...X_{n}|\\mathbf{S}=i)}$ and $\\scriptstyle{\\mathcal{T}}_{(X_{1}...X_{n}|\\mathbf{S}=j)}$ such that $\\begin{array}{r l}{\\mathscr{T}_{(X_{j}|\\mathbf{S}=i)}}&{{}=}\\end{array}$ $\\alpha\\mathcal{T}_{(\\boldsymbol{X}_{k}|\\mathbf{S}=i)}$ for any $i,k\\,\\in\\,[n]$ where $\\alpha$ is a constant, i.e, the columns of the conditional contingency table are linearly dependent. This violates the assumption 2.4 (the Full Rank assumption). ", "page_idx": 14}, {"type": "text", "text": "Therefore, $\\operatorname{Rank}({\\mathcal{T}}_{\\{X_{1}\\ldots X_{n}\\}})=r$ if the condition (i) and condition (i) holds. ", "page_idx": 14}, {"type": "text", "text": "\"Only if\" part: ", "page_idx": 14}, {"type": "text", "text": "We will show if one of the conditions is violated, the tensor rank is not $r$ , i.e., if (i). there does not exist a variable set $\\mathbf{S}$ in the causal graph with $|\\operatorname{supp}(\\mathbf{L})|=r$ that $\\mathrm{d}$ -separates any pair of variables in $\\{X_{1},\\cdots,X_{n}\\}$ , or (i). exist $\\tilde{\\mathbf{S}}$ that satisfies $|\\mathrm{supp}(\\tilde{\\mathbf{S}})|<r$ , then $\\operatorname{Rank}({\\mathcal{T}}_{\\{X_{1}\\ldots X_{n}\\}})\\neq r$ ", "page_idx": 14}, {"type": "text", "text": "We first show the case that condition (i) is violated. There are two cases we need to consider, i.e., Case $1\\colon\\mathbf{S}$ is not a conditional set in the causal graph, and Case 2: S is a variable constructed from parameterspace. ", "page_idx": 15}, {"type": "text", "text": "Case 1: if S is not a conditional set and $\\mathbf{S}\\cap\\mathrm{Des}_{X_{1}}\\cap\\cdots\\cap\\mathrm{Des}_{X_{n}}=\\emptyset$ , by the Markov assumption, $\\begin{array}{r}{\\mathbb{P}(X_{1},\\cdots,X_{n}|\\mathbf{S}~=~i)~\\neq~\\prod_{j=1}^{n}\\mathbb{P}(X_{j}|\\mathbf{S}~=~i)}\\end{array}$ . By Lemma. D.3, $\\mathbb{P}(X_{1},\\cdots,X_{n}|\\mathbf{S}\\;=\\;i)$ is not a rank-one tensor, which violates the definition of tensor rank. ", "page_idx": 15}, {"type": "text", "text": "If S is not a conditional set and $\\mathbf{S}\\cap\\mathrm{Des}_{X_{1}}\\cap\\cdots\\cap\\mathrm{Des}_{X_{n}}\\neq\\emptyset$ , we will show that $\\mathbb{P}(X_{1},\\cdots,X_{n}|\\mathbf{S}=i)$ is not a rank-one tensor, to prove such a rank-decomposition does not exist. Under the faithfulness assumption and Markov assumption, let $\\tilde{\\mathbf{S}}$ be the a minimal conditional set with $|\\mathrm{supp}(\\tilde{\\mathbf{S}})|=k$ for any pair variable in $\\{X_{1},\\cdots,\\bar{X_{n}}\\}$ ,we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{T}_{(X_{1}\\cdots X_{n}|\\mathbf{S}=i)}}\\\\ &{\\quad=\\displaystyle\\sum_{j=1}^{k}\\mathcal{T}_{(X_{1}\\cdots X_{n}|\\mathbf{S}=i,\\tilde{\\mathbf{S}}=j)}\\mathcal{T}_{(\\mathbf{S}=i|\\tilde{\\mathbf{S}}=j)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "f $\\scriptstyle{\\mathcal{T}}_{(X_{1}\\cdots X_{n}|\\mathbf{S}=i)}$ is a rank-one tensor, i.e., ${\\mathcal{T}}_{(X_{1}\\cdots X_{n}\\vert{\\bf S}=i)}=\\mathbf{u}_{1}\\otimes\\cdots\\otimes\\mathbf{u}_{n}.$ we further have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{k}{\\mathcal{T}}_{(X_{1}\\cdots X_{n}|\\mathbf{S}=i,\\tilde{\\mathbf{S}}=j)}{\\mathcal{T}}_{(\\mathbf{S}=i|\\tilde{\\mathbf{S}}=j)}=\\mathbf{u}_{1}\\otimes\\cdots\\otimes\\mathbf{u}_{n}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Note that if there exists $X_{p},X_{q}\\in\\{X_{1},\\cdots,X_{n}\\}$ such that S is the common descendant variable of $X_{p}$ and $X_{q}$ , it will lead to a collider structure in which $\\mathbb{P}(X_{i}|\\mathbf{S})$ and $\\mathbb{P}(X_{j}|\\mathbf{S})$ is relevant (i.e., the V-structure is activated). Thus, let $\\mathbf{X}_{t}=\\{X_{1},\\cdots,X_{n}\\}\\mid\\{\\hat{X_{i}},\\hat{X_{j}}\\}$ \uff0c $\\scriptstyle{\\mathcal{T}}_{(X_{1}\\cdots X_{n}|\\mathbf{S}=i)}$ is not a rank-one tensor due to the sub-tensor $\\mathcal{T}_{(X_{i}X_{j},\\mathbf{X}_{t}=\\mathbf{c}|\\mathbf{S}=i)}$ (a slice of tensor $\\scriptstyle{\\mathcal{T}}_{(X_{1}\\cdots X_{n}\\vert\\mathbf{S}=i)})$ is not a rank-one tensor ^Hackbusch (2012); Kruskal (1977), under the faithfulness assumption. If so, one have $T_{\\left(X_{i}X_{j},\\mathbf{X}_{t}=\\mathbf{c}\\right|\\mathbf{S}=i\\right)}=\\mathbf{u}_{1}\\otimes\\mathbf{u}_{2}$ . Let $\\mathbf{u}_{1}\\,=\\,\\mathbb{P}(X_{i},\\mathbf{X}_{t}\\,=\\,\\mathbf{c}|\\mathbf{S}\\,=\\,i)$ and $\\mathbf{u}_{2}\\,=\\,\\mathbb{P}\\!\\left(X_{j},\\mathbf{X}_{t}\\,=\\,\\mathbf{c}|\\mathbf{S}\\,=\\,i\\right)$ ,it violates the faithfulness assumption. ", "page_idx": 15}, {"type": "text", "text": "Thus, S can not be the common descendant of any pair variables in $\\{X_{1},\\cdots,X_{n}\\}$ . So, for any $X_{p},X_{q}\\in\\{X_{1},\\cdots,X_{n}\\}$ , there are $\\begin{array}{r}{\\mathcal{T}_{(X_{p}X_{q}|\\mathbf{S}=j\\tilde{\\mathbf{S}}=i)}=\\mathcal{T}_{(X_{p}|\\tilde{\\mathbf{S}}=i,\\mathrm{Des}_{X_{p}})=c}\\otimes\\mathcal{T}_{(X_{q}|\\tilde{\\mathbf{S}}=i,\\mathrm{Des}_{X_{q}})=c}}\\end{array}$ according to Markov assumption ( $\\tilde{\\mathbf{S}}$ is a d-separation set for $X_{p}$ and $X_{q.}$ ", "page_idx": 15}, {"type": "text", "text": "Now, for the Eq. 8, we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\mathcal{T}}_{(X_{1}\\cdots X_{n}|{\\bf L}=i)}}}\\\\ {~~}\\\\ {{\\displaystyle=\\sum_{j=1}^{k}\\otimes_{t=1}^{n}{\\mathcal{T}}_{(X_{t}|\\tilde{\\bf S}=j,\\mathrm{Des}_{X_{t}}=c)}{\\mathcal{T}}_{(\\tilde{\\bf S}=j,\\mathrm{Des}_{X_{t}}=c|\\tilde{\\bf S}=j)}}}\\\\ {~~}\\\\ {{\\displaystyle={\\bf u}_{1}\\otimes\\cdots\\otimes{\\bf u}_{n}.}}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "This equality holds if the $k$ sum of the rank-one tensor can be reduced to a rank-one tensor, i.e., for any $X_{p}$ \uff0c ${\\mathcal{T}}_{(X_{p}|\\tilde{\\mathbf{S}}=1,\\mathrm{Des}_{X_{p}}=c)}=\\alpha_{2}{\\mathcal{T}}_{(X_{p}|\\tilde{\\mathbf{S}}=2,\\mathrm{Des}_{X_{p}}=c)}=\\cdots=\\alpha_{r}{\\mathcal{T}}_{(X_{p}|\\tilde{\\mathbf{S}}=r,\\mathrm{Des}_{X_{p}}=c)}$ where $\\alpha_{i}$ and $c$ are constant. However, this equality can not hold due to the following reasons. ", "page_idx": 15}, {"type": "text", "text": "f $\\operatorname{Pa}_{X_{p}}\\neq\\emptyset$ , let $L_{p}$ be the parent of $X_{p}$ , we have ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{T}_{(X_{p}|\\tilde{\\mathbf{S}}=i,\\mathrm{Des}_{X_{p}}=c)}}\\\\ &{\\quad|\\operatorname*{supp}(L_{p})|}\\\\ &{=\\displaystyle\\sum_{j=1}^{\\infty}\\quad\\mathcal{T}_{(X_{p}|L_{p}=j,\\mathrm{Des}_{X_{p}}=c)}\\mathcal{T}_{(L_{p}=j|\\tilde{\\mathbf{S}}=i,\\mathrm{Des}_{X_{p}}=c)}}\\\\ &{\\neq\\displaystyle\\sum_{j=1}^{|\\operatorname*{supp}(L_{p})|}\\mathcal{T}_{(X_{p}|L_{p}=j,\\mathrm{Des}_{X_{p}}=c)}\\mathcal{T}_{(L_{p}=j|\\tilde{\\mathbf{S}}=r,\\mathrm{Des}_{X_{p}}=c)}=\\alpha\\mathcal{T}_{(X_{p}|\\tilde{\\mathbf{S}}=r,\\mathrm{Des}_{X_{p}}=c)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "in which the inequality holds because of the Full Rank assumption and all marginal distribution probabilties are not ero see Lemma. D.4). Therefore, $\\mathcal{T}_{(X_{p}|\\tilde{\\mathbf{S}}=1,\\mathrm{Des}_{X_{p}}=c)}\\neq\\cdots\\neq\\alpha_{r}\\mathcal{T}_{(X_{p}|\\tilde{\\mathbf{S}}=r,\\mathrm{Des}_{X_{p}}=c)}$ Thus, $\\boldsymbol{\\mathcal{T}}_{(X_{1}\\cdots X_{n}\\vert\\mathbf{S}=i)}$ is not a rank-one tensor By the defnition of tensor rank, $\\mathrm{Rank}({\\mathcal{T}}_{(X_{1}\\cdots X_{n})})^{\\big\\}\\neq r$ f $\\mathrm{Pa}_{X_{p}}\\,=\\,\\emptyset$ ,i.e., $X_{p}$ is the root variable, (e.g.. $X_{p}\\to{\\bf S},\\mathrm{Des}_{X_{p}\\in{\\bf S}}\\,,$ 0. we will show that the probability contingency table $\\boldsymbol{\\mathcal{T}}_{(X_{p}|\\mathbf{S})}$ is ull rank, and then the equality in Eq. 8 can not hold. According to the Full Rank assumption and $X_{p}$ is the parent variable of S, we have the probability contingency table $\\mathcal{T}_{(\\mathbf{S}|X_{p})}$ is full rank. ", "page_idx": 16}, {"type": "text", "text": "Due to $\\mathbb{P}(X_{p},\\mathbf{S})\\,=\\,\\mathbb{P}(\\mathbf{S}|X_{p})\\mathbb{P}(X_{p})\\,=\\,\\mathbb{P}(X_{p}|\\mathbf{S})\\mathbb{P}(\\mathbf{S})$ and the probability in the marginal distribution is not zero, we have $\\mathcal{T}_{(X_{p}|\\mathbf{S})}\\,=\\,\\mathcal{T}_{(\\mathbf{S}|X_{p})}\\mathrm{Diag}(\\mathcal{T}_{(X_{p})})\\mathrm{Diag}(\\mathcal{T}_{(\\mathbf{S})})^{\\dagger}$ , where $\\mathrm{Diag}(\\tau_{(X_{p})})$ is a diagonalization of marginal distribution probability vector of $X_{p}$ . One can see that $\\boldsymbol{\\mathcal{T}_{(X_{p}|\\mathbf{S})}}^{*}$ is full rank due to the diagonal matrices are all of full rank. Thus, $\\dot{T_{(X_{p}|\\tilde{\\mathbf{S}}=1)}}\\,\\neq\\,\\cdots\\,\\neq\\,\\alpha_{r}\\dot{T_{(X_{p}|\\tilde{\\mathbf{S}}=r)}}$ and $\\boldsymbol{\\mathcal{T}}_{(X_{1}\\cdots X_{n}\\vert\\mathbf{S}=i)}$ is not a rank-one tensor. By the definition of tensor rank, $\\operatorname{Rank}({\\mathcal{T}}_{(X_{1}\\cdots X_{n})})\\neq r$ \uff1a ", "page_idx": 16}, {"type": "text", "text": "Case 2: we further show that for the parameter space,the tensor ${\\mathcal{T}}_{(X_{1},\\cdots,X_{n})}=\\sum_{i=1}^{r}\\mathbf{u}_{1}\\otimes\\cdots\\otimes\\mathbf{u}_{n}$ does not hold with $r$ , where $\\mathbf{u}_{i}$ represents any vector. Let $\\mathbf{u}_{i}\\otimes\\cdots\\otimes\\mathbf{u}_{n}$ be a rank-one tensor of $\\mathbb{P}(X_{1},\\cdots,X_{n}|\\tilde{\\mathbf{S}}=i)\\mathbb{P}(\\tilde{\\mathbf{S}}=i)$ due to $\\mathbb{P}(\\tilde{\\mathbf{S}}=\\dot{\\boldsymbol{i}})$ is a constant. In other words, one can construct a variable set $\\tilde{\\mathbf{S}}$ with $\\left|\\operatorname{supp}(\\tilde{\\mathbf{S}})\\right|=r$ . For any $\\tilde{\\mathbf{S}}$ constructed from parameter space (i.e., $\\tilde{\\mathbf{S}}$ is not a true node set in the causal graph), if $T_{(X_{1},\\cdots,X_{n}|\\tilde{\\mathbf{S}}=i)}=\\mathbf{u}_{1}\\otimes\\cdots\\otimes\\mathbf{u}_{n}$ one can let $\\mathbf{u}_{1}={\\mathcal{T}}_{(X_{1}\\mid{\\tilde{\\mathbf{S}}}=i)},\\cdots,\\mathbf{u}_{n}=$ $\\tau_{(X_{n}|\\tilde{\\mathbf{S}}=i)}$ , which violates the faithfulness assumption. Based on the above analysis, there does not exist $\\tilde{\\mathbf{S}}$ by any constructed such that $\\mathcal{T}_{(X_{1}\\cdots X_{n})}$ have the summation $r$ rank-one decomposition, i.e., $\\mathrm{Rank}({\\mathcal{T}}_{(X_{1}\\cdots X_{n})})\\neq r$ ", "page_idx": 16}, {"type": "text", "text": "Now, we analyze the condition (i), i.e., there exists a conditional set $\\tilde{\\mathbf{S}}$ with $\\mathrm{supp}(\\tilde{\\mathbf{S}})\\,<\\,r$ . Let $\\operatorname{supp}(\\tilde{\\mathbf{S}})=k,k<r$ , we have $\\begin{array}{r}{\\mathcal{T}_{(X_{1}\\cdots X_{n})}=\\sum_{i=i}^{k}\\mathcal{T}_{(X_{1}|\\tilde{\\mathbf{S}}=i)}...\\otimes\\mathcal{T}_{(X_{n}|\\tilde{\\mathbf{S}}=i)}\\mathcal{T}_{(\\tilde{\\mathbf{S}}=i)}}\\end{array}$ is a smalle rank-one decomposition than $\\begin{array}{r}{\\mathcal{T}_{(X_{1}\\cdots X_{n})}=\\sum_{j=1}^{r}\\mathbf{u}_{1}^{\\mathbf{j}}\\otimes\\cdots\\otimes\\mathbf{u}_{n}^{j}}\\end{array}$ . According to the definition of tensor rank, we have $\\operatorname{Rank}({\\mathcal{T}}_{\\{X_{1}\\ldots X_{n}\\}})=k\\neq r$ ", "page_idx": 16}, {"type": "text", "text": "In summary, the theorem is proven. ", "page_idx": 16}, {"type": "text", "text": "Lemma D.1. Let $L_{p}$ be the parent of $X_{p}$ Consider $\\mathbb{P}(X_{p}|\\mathbf{S}=i)$ and $\\mathbb{P}(X_{p}|\\mathbf{S}=r)$ where S can be any variable set. Under the Assumption 2.4, for a constant $c$ the following inequality holds: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{\\lvert\\operatorname*{supp}(L_{p})\\rvert}\\mathcal{T}_{(X_{p}\\mid L_{p}=j)}\\mathcal{T}_{(L_{p}=j\\mid\\mathbf{S}=i)}\\ne c\\sum_{j=1}^{\\lvert\\operatorname*{supp}(L_{p})\\rvert}\\mathcal{T}_{(X_{p}\\mid L_{p}=j)}\\mathcal{T}_{(L_{p}=j\\mid\\mathbf{S}=r)}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Proof. We prove it by contradiction.Forconvenience of symbols, let $\\mathbf{v}_{j}=\\mathsf{\\mathcal{T}}_{(X_{p}|L_{p}=j)}$ and $\\alpha_{j\\mid i}=$ $\\mathbb{P}(L_{p}=j|\\mathbf{L}=i)$ and $\\beta_{j|r}=\\mathbb{P}(L_{p}=j|\\mathbf{L}=r)\\,(\\alpha_{j|i}\\neq\\beta_{j|r})$ , if the equality hold, we have ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\sum_{j=1}^{|\\mathrm{supp}(L_{p})|}\\alpha_{j|i}\\mathbf{v}_{j}=c\\sum_{j=1}^{|\\mathrm{supp}(L_{p})|}\\beta_{j|r}\\mathbf{v}_{j},\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "which means that ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\mathbf{v}_{t}=\\frac{\\sum_{k\\neq t}^{|\\mathrm{supp}(L_{p})|}(\\alpha_{k|i}-c\\beta_{k|r})\\mathbf{v}_{k}}{\\alpha_{t|i}-c\\beta_{t|r}}.\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "That is, $\\mathbf{v}_{t}$ is a linear combination of other vectors $\\mathbf{v}_{k}$ With $t\\ne k$ , i.e.,the linear combination of other column vectors in the conditional probability contingency table, which is contrary to the Assumption 2.4. ", "page_idx": 16}, {"type": "text", "text": "Lemma D.2. Let $L_{p}$ be the parent of $X_{p}$ SupposeAssumption $2.2\\,\\sim$ Assumption2.4hold. $\\mathscr{T}_{(X_{p}|\\mathbf{S}=i)}$ cannot be expressed as a linear combination of other $q$ vectors $\\mathscr{T}_{(X_{p}|\\mathbf{S}=r)}$ i.e., $\\tau_{(X_{p}|\\mathbf{S}=i)}\\neq$ $\\scriptstyle\\sum_{r=1}^{q}\\gamma_{r}T_{(X_{p}|\\mathbf{S}=r)}$ ", "page_idx": 17}, {"type": "text", "text": "Proof.We proveitbycontradiction.Forconvenience f symbols l $\\mathbf{v}_{j}=\\mathsf{\\mathcal{T}}_{(X_{p}|L_{p}=j)}$ and $\\alpha_{j\\mid i}=$ $\\mathbb{P}(L_{p}=j|\\mathbf{S}=i)$ and $\\beta_{j|r}=\\mathbb{P}(L_{p}=j|\\mathbf{S}=r)$ , if the equality hold, one have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{|\\displaystyle\\sum_{j=1}^{\\mathrm{supp}(L_{p})|}\\alpha_{j|i}\\mathbf{v}_{j}=\\sum_{r=1}^{q}\\gamma_{r}\\sum_{j=1}^{|\\mathrm{supp}(L_{p})|}\\beta_{j|r}\\mathbf{v}_{j}}\\\\ &{=\\displaystyle\\sum_{j=1}^{|\\mathrm{supp}(L_{p})|}\\mathbf{v}_{j}\\sum_{r=1}^{q}\\gamma_{r}\\beta_{j|r},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "there exist a vector $\\mathbf{v}_{t}$ with $\\begin{array}{r}{\\alpha_{t\\vert i}-\\sum_{r=1}^{q}\\gamma_{r}\\beta_{t\\vert r}\\neq0}\\end{array}$ such that ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{v}_{t}=\\frac{\\sum_{k\\neq t}^{|\\mathrm{supp}(L_{p})|}(\\sum_{r=1}^{q}\\gamma_{r}\\beta_{j|r}-\\alpha_{k|i})\\mathbf{v}_{k}}{\\alpha_{t|i}-\\sum_{r=1}^{q}\\gamma_{r}\\beta_{t|r}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "It means $\\tau_{(X_{p}|L_{p}=t)}$ is a linear combination of other column vectors in the conditional probability contingency tabfe, which is contrary to Assumption 2.4. ", "page_idx": 17}, {"type": "text", "text": "Lemma D.3. For the set of variables $\\{X_{1},\\cdots,X_{n}\\}$ ifS is nota conditional set that $d$ -separates any pair of variables in $\\{X_{1},\\cdots,X_{n}\\}$ in the causal graph and $\\tilde{\\mathbf{S}}\\cap\\mathrm{Des}_{\\{X_{1}\\cdots X_{n}\\}}=\\emptyset$ then $\\boldsymbol{\\mathcal{T}}_{(X_{1}\\cdots X_{n}|\\tilde{\\mathbf{S}}=j)}$ is not a rank-one tensor. ", "page_idx": 17}, {"type": "text", "text": "Proof. Let S be the minimal conditional set, (e.g., $\\mathbf{S}\\,=\\,\\{X_{1},{\\cdots},X_{n-1}\\})$ ,denote $|\\operatorname{supp}(\\mathbf{S})|\\,=\\,r$ under the faithfulness assumption and the Markov assumption, $\\mathbb{P}(\\mathbf{S})\\neq\\mathbb{P}(\\tilde{\\mathbf{S}})$ ,if $\\boldsymbol{\\mathcal{T}}_{(X_{1}\\cdots X_{n}\\vert\\tilde{\\mathbf{S}}=j)}$ is a rank-one tensor, we have ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{T}_{(X_{1}\\cdots X_{n}|\\tilde{\\mathbf{S}}=j)}}\\\\ &{\\quad=\\displaystyle\\sum_{i=1}^{r}\\otimes_{t=1}^{n}\\mathcal{T}_{(X_{t}|\\mathbf{S}=i)}\\mathcal{T}_{(\\mathbf{S}=i|\\tilde{\\mathbf{S}}=j)}}\\\\ &{\\quad=\\mathbf{u}_{1}\\otimes\\cdots\\otimes\\mathbf{u}_{n},}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "which means that for any $X_{p}\\in\\{X_{1},\\cdots X_{n}\\},\\mathcal{T}_{(X_{p}|\\mathbf{S}=1)}=\\alpha_{2}\\mathcal{T}_{(X_{p}|\\mathbf{S}=2)}=\\cdots\\mathcal{T}_{(X_{p}|\\mathbf{S}=r)}.$ ", "page_idx": 17}, {"type": "text", "text": "$X_{p}$ has a parent variable $L_{p}$ in the causal graph, by Lemma .D.1, the equality does not hold. Thus, $\\scriptstyle{\\mathcal{T}}_{(X_{1}\\cdots X_{n}\\vert{\\tilde{\\mathbf{L}}}=j)}$ is not a rank-one tensor. ", "page_idx": 17}, {"type": "text", "text": "$X_{p}$ is root node in the causal graph, and S is conditional set that d-separates $X_{p}$ and $\\{X_{1},\\cdots X_{n}\\}\\mid$ $\\{{X}_{p}\\}$ ,wehave $\\boldsymbol{\\mathcal{T}}_{(\\boldsymbol{X}_{p}\\mid\\mathbf{S})}$ is full rank. The reason is the following. ", "page_idx": 17}, {"type": "text", "text": "Since $\\mathbb{P}(X_{p}|\\mathbf{S})\\mathbb{P}(\\mathbf{S})=\\mathbb{P}(\\mathbf{S}|X_{p})\\mathbb{P}(X_{p})$ by Bayes\u2019 theorem Koch and Koch (1990), and due to all marginal probabilities are not zero, then we have $\\mathcal{T}_{(\\underline{{X}}_{p}|\\mathbf{S})}=\\mathcal{T}_{(\\mathbf{S}|\\underline{{X}}_{p})}\\mathrm{Diag}(\\mathcal{T}_{(X_{p})})\\mathrm{Diag}(\\mathcal{T}_{(\\mathbf{S})})^{\\dagger}$ where $\\dagger$ is inverse of matrix, and $X_{p}$ is ancestor of S. By Lemma .D.2, S has a parent variables and hence $\\mathcal{T}_{(\\mathbf{S}|X_{p})}$ is full rank (one can vectorize the variable set $\\mathbf{S}$ as a variable). Now, we have $\\boldsymbol{\\mathcal{T}}_{(X_{p}\\mid\\mathbf{S})}$ is full rank due to the three matrices on the right are all full rank. ", "page_idx": 17}, {"type": "text", "text": "Thus, ${\\mathcal{T}}_{(X_{p}|{\\bf S}=1)}=\\alpha_{2}{\\mathcal{T}}_{(X_{p}|{\\bf S}=2)}=\\cdots{\\mathcal{T}}_{(X_{p}|{\\bf S}=r)}$ does not hold, i.e. $\\boldsymbol{\\mathcal{T}}_{(X_{1}\\cdots X_{n}\\vert\\tilde{\\mathbf{S}}=j)}$ is not a rank-one tensor. ", "page_idx": 17}, {"type": "text", "text": "Lemma D.4. In a discrete causal graph, for the variable $X_{p}$ let $L_{p}$ represent the vectorized parent set of $X_{p}$ and let ${\\mathrm{Des}}_{X_{p}}$ denote the set of descendant variables of $X_{p}$ . Under this setup, $\\mathcal{T}_{(X_{p}|L_{p},\\mathrm{Des}_{X_{p}}=j)}$ is full rank. ", "page_idx": 17}, {"type": "text", "text": "Proof. By Bayes\u2032 theorem Koch and Koch (1990), we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{T}_{(X_{p},L_{p}|\\mathrm{Des}_{X_{p}}=j)}}\\\\ &{\\ =\\mathcal{T}_{(X_{p}|L_{p},\\mathrm{Des}_{X_{p}}=j)}\\mathrm{Diag}(\\mathcal{T}_{(L_{p}|D e s_{X_{p}}=j)})}\\\\ &{\\ =\\mathcal{T}_{(L_{p}|X_{p},\\mathrm{Des}_{X_{p}}=j)}\\mathrm{Diag}(\\mathcal{T}_{(X_{p}|\\mathrm{Des}_{X_{p}}=j)}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Since $X_{p}$ and ${\\mathrm{Des}}_{X_{p}}$ both are the descendant set of $L_{p}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{T}_{(L_{p}|\\mathrm{Des}_{L_{p}})}=\\mathcal{T}_{(\\mathrm{Des}_{L_{p}}|L_{p})}\\mathrm{Diag}(\\mathcal{T}_{(\\mathrm{L_{p}})}\\mathrm{Diag}(\\mathcal{T}_{(\\mathrm{Des}_{\\mathrm{L_{p}}})}^{})^{\\dagger},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "due to the fact that all marginal distribution probabilities are not zero. Then $\\mathscr{T}_{(L_{p}|\\mathrm{Des}_{L_{p}})}$ is full rank, i.e., $\\mathcal{T}_{(L_{p}|X_{p},\\mathrm{Des}_{X_{p}}=j)}$ also full rank. ", "page_idx": 18}, {"type": "text", "text": "Moreover, for any $\\scriptstyle\\mathcal{T}_{(L_{p}=i|\\mathrm{Des}=j)}$ , we have ", "page_idx": 18}, {"type": "equation", "text": "$$\nT_{(L_{p}=i|\\mathrm{Des}_{X_{p}}=j)}=\\frac{\\mathcal{T}_{(L_{p}=i,\\mathrm{Des}_{X_{p}}=j)}}{\\mathcal{T}_{(\\mathrm{Des}_{X_{p}}=j)}}\\neq0,\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "because all marginal distribution probabilities are not zero. Thus, we have ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{T}_{(X_{p},L_{p}|\\mathrm{Des}_{X_{p}}=j)}}\\\\ &{\\ =\\mathcal{T}_{(L_{p}|X_{p},\\mathrm{Des}_{X_{p}}=j)}\\mathrm{Diag}(\\mathcal{T}_{(X_{p}|\\mathrm{Des}_{X_{p}}=j)})\\mathrm{Diag}(\\mathcal{T}_{(L_{p}|\\mathrm{Des}_{X_{p}}=j)})^{\\dagger}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "One can see that $\\mathscr{T}_{(X_{p},L_{p}|\\mathrm{Des}_{X_{p}}=j)}$ is full rank due to three matrces on the right side being full rank. ", "page_idx": 18}, {"type": "text", "text": "D.2 Proof of Proposition 4.2 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. The proof is straightforward. In the discrete 3PLSM model, suppose all latent variable has the same state space. Any two observed are d-separated by any one of their latent parents. According to the graphical criteria, the rank of tensor $\\mathcal{T}_{(\\boldsymbol{X}_{i}\\boldsymbol{X}_{j})}$ is the dimension of latent support. ", "page_idx": 18}, {"type": "text", "text": "D.3 Proof of Proposition 4.3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. Proof of Rule 1: In the discrete 3PLSM and suppose the assumption $2.2\\sim$ assumption 2.4 holds, if there does not exist a latent variable $L_{1}$ that d-separates any pair variables in $\\{X_{i},\\mathbf{\\bar{{X}}_{j}},X_{k}\\}$ \uff0c i.e., the rank of tensor $\\mathcal{T}_{(\\boldsymbol{X}_{i}\\boldsymbol{X}_{j}\\boldsymbol{X}_{k})}$ is not $r$ (by Theorem 1),it must be the full-connection structure among latent variables, as shown in Fig. 4 (d). Otherwise, one can find one latent variable $L_{1}$ that can d-separates $\\{X_{i},X_{j},X_{k}\\}$ , as shwon in Fig. 4 (a) $\\sim$ (c). We will show that, if only consider one of the latent variables of $\\{L_{1},L_{2},L_{3}\\}$ in Fig. 4 (d), the tensor of $\\boldsymbol{\\mathcal{T}}_{(\\boldsymbol{X}_{i}\\boldsymbol{X}_{j}\\boldsymbol{X}_{k})}$ can not have rank-one decomposition. ", "page_idx": 18}, {"type": "image", "img_path": "6EqFoqkLSW/tmp/bb4b261049c7e8247f835620533bebcb71a20db419ef00e24da321d400f448ce.jpg", "img_caption": ["Figure 4: Illustrative example for Rule 1. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "According to the graphical criteria of tensor rank and $r<d_{i}$ in the definition of discrete LSM, and suppose all latent variables have the same state space, for $X_{i}$ $X_{j}$ and $X_{k}$ , there is not only one latent variable is conditional set, i.e., but there also is not one latent variable $L$ that d-separates any pair variables in $\\{X_{i},X_{j},X_{k}\\}$ .Thus, $\\operatorname{Rank}({\\mathcal{T}}_{(X_{i}X_{j}X_{k})})\\neq r$ ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "Proof of $\\mathcal{R}\\mathbf{u}\\mathbf{l}\\mathbf{e}\\,2$ ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We aim to show if $\\operatorname{Rank}({\\mathcal{T}}_{(X_{i}X_{j}X_{k}X_{s})})\\;=\\;r$ for any $X_{s}\\in\\mathbf{X}\\mid\\{X_{i},X_{j},X_{k}\\}$ then there exists a latent variable $L_{p}$ that d-separates $\\{X_{i},...,X_{s}\\}$ and $L_{p}$ is the parent variable of $\\{X_{i},X_{j},X_{k}\\}$ in the discrete 3PLSM. We first prove it by the contradiction. If $\\{\\bar{X_{i}},\\bar{X_{j}},\\bar{X_{k}}\\}$ does not share one common latent parent, e.g., ${\\cal L}_{1}\\,\\rightarrow\\,\\{X_{i},X_{j}\\}$ and $L_{2}\\,\\to\\,\\{X_{k}\\}$ , due to the structure assumption in discrete 3PLSM, there exist $X_{s}\\in C h_{L_{2}}$ , as shown in Fig. 5. ", "page_idx": 19}, {"type": "image", "img_path": "6EqFoqkLSW/tmp/264ba123b09d25c83ff8ff491f1fc1e67b2dfb329cabca0263e9c6dbdad2ce11.jpg", "img_caption": ["Figure 5: Ilustrative example for $\\mathcal{R}\\mathrm{ule}\\,2$ "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "By the graphical criteria of tensor rank condition and $r<d_{i}$ , one has $\\operatorname{Rank}(\\mathcal{T}_{(X_{i}X_{j}X_{k}X_{s})})\\neq r$ since $L_{1}$ or $L_{2}$ is not the conditional set that d-separates any pair variable of $\\{X_{i},X_{j},X_{k},\\dot{X}_{s}\\}$ (e.g., $X_{k},X_{s}$ can not be d-separates given $L_{1}$ 0, which is contrary to the condition $\\operatorname{Rank}(\\bar{T}_{(X_{i}X_{j}X_{k}X_{s})})=r$ ", "page_idx": 19}, {"type": "text", "text": "D.4 Proof of Proposition 4.5 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. Since $C_{1}$ and $C_{2}$ are two causal clusters, then the elements in $C_{1}$ have only one common latent variable. Without loss of generality, we let $L_{1}$ denote the parental latent variable of $C_{1}$ . Similarly, $L_{2}$ denotes the parental latent variable of $C_{2}$ . Since $C_{1}$ and $C_{2}$ are overlapping, then they have at least one shared element. Let $X_{i}$ denote the shared element of $C_{1}$ . then $X_{i}$ has two latent parents $L_{1}$ and $L_{2}$ , which contradicts with the pure child assumption in the discrete 3PLSM model. This finishes the proof. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "D.5 Proof of Theorem 4.6 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. Based on Prop. 4.3, one can identify the causal cluster by testing the tensor rank condition (Line $5\\sim12$ ). Moreover, the Prop. 4.5 ensures no redundant latent variables are introduced (Line 15). Thus, the causal cluster can be identified by Algorithm 1, under the discrete 3PLSM, with assumption $2.2\\sim$ assumption 2.4. \u53e3 ", "page_idx": 19}, {"type": "text", "text": "D.6Proof of Theorem 4.7 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Proof. We first prove this result by a specific case and then extend it to a general case result. ", "page_idx": 19}, {"type": "text", "text": "Proof by Specific case ", "text_level": 1, "page_idx": 19}, {"type": "image", "img_path": "6EqFoqkLSW/tmp/417f0da241e4d15879fb3ed70f722e46f7d13aac2b1768baf8bb9905726d125f.jpg", "img_caption": ["Figure 6: d-separation and d-connection example "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "'If' part: as shown in Fig.6, suppose all support of latent space is $r$ and $r<d_{i}$ $d_{i}$ is the dimension of any support of observed variables. We will show that, for $\\{X_{1},...,X_{6}\\}$ , the rank of $\\tau_{\\{X_{1},\\dots,X_{6}\\}}$ are $r^{2}$ . Let $\\mathbf{L}$ be the vectorization of joint distribution $\\mathbb{P}(L_{3},L_{4})$ with $|\\mathrm{supp}(\\mathbf{L})|=r^{2}$ ", "page_idx": 20}, {"type": "text", "text": "Since $L_{2},L_{3}$ is a conditional set that d-separates all variables in $\\{X_{1},\\cdots,X_{6}\\}$ and there is no other conditional set with smaller support, according to the graphical criteria of tensor rank, $\\operatorname{Rank}({\\mathcal{T}}_{(X_{1}\\cdots X_{6})})=|\\operatorname{supp}(\\mathbf{L})|=r^{2}$ ", "page_idx": 20}, {"type": "text", "text": "'Only if' part: now, we consider the case that if $L_{1}$ and $L_{2}$ are d-connection (Fig.6). In this case, given $\\mathbb{P}(L_{3}L_{4})$ (represented by $\\mathbb{P}(\\mathbf{L}),$ $\\scriptstyle{\\mathcal{T}}_{(X_{1}X_{2}\\mid\\mathbf{L}=i)}$ cannot be decomposition as the outer product of two vectors since the contingency table of $\\scriptstyle{\\mathcal{T}}_{(X_{1}X_{2}\\mid\\mathbf{L}=i)}$ is not a rank-one tensor by Markov assumption. That is, $\\mathbf{L}$ is not a conditional set that d-separates $X_{1}$ and $X_{2}$ ", "page_idx": 20}, {"type": "text", "text": "According to the graphical criteria of tensor rank condition, one can infer that $\\mathrm{Rank}({\\mathcal T}_{(X_{1}\\cdots X_{6})})\\neq$ $|\\operatorname{supp}(\\mathbf{L})|=r^{2}$ . Based on the above analysis, one can see that the conditional independent relations hold if and only if the rank of the tensor equals the cardinality of support of the conditional set. ", "page_idx": 20}, {"type": "text", "text": "Proof by general case ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "'if' part: in the discrete 3PLSM model, assume that all latent variables have the same support. For any pair of variables $X_{i}$ and $X_{j}$ that are pure children of $L_{k}$ , it is evident that $L_{k}$ is the only minimal conditional set that d-separates $X_{i}$ from $X_{j}$ . Consider a set of latent variables $\\mathbf{L}_{p}$ and their corresponding child sets $\\mathbf{X}_{p1}$ and $\\mathbf{X}_{p2}$ , where each latent variable has at least two child variables included in $\\mathbf{X}_{p1}$ and $\\mathbf{X}_{p2}$ . The minimal conditional set that ${\\mathrm{d}}.$ -separates all variables in $\\mathbf{X}_{p1}$ from those in $\\mathbf{X}_{p2}$ is their common latent parent set $\\mathbf{L}_{p}$ . Moreover, for two variables $X_{i}$ and $X_{j}$ that do not share a common latent parent, if $\\mathbf{L}_{p}$ d-separates $X_{i}$ and $X_{j}$ , then $\\mathbf{L}_{p}$ also d-separates all variables in $\\mathbf{X}_{p1}\\cup\\mathbf{X}_{p2}\\cup\\{X_{i},X_{j}\\}$ . Since all latent variables have the same support, the minimal conditional set in the graph corresponds to the set with the minimal support. Therefore, by the graphical criteria of tensor rank, $\\operatorname{Rank}(\\mathbf{\\hat{\\mathcal{T}}}(X_{i},X_{j},\\mathbf{X}_{p1},\\mathbf{X}_{p2}))=|\\operatorname{supp}(\\mathbf{L}_{p})|$ . As all latent variables share the same state space, we deduce that supp(L,)I = \\*L21. ", "page_idx": 20}, {"type": "text", "text": "'Only if' part: if $X_{i}$ and $X_{j}$ cannot be d-separated by $\\mathbf{L}_{p}$ , then $\\mathbf{L}_{p}$ does not constitute a conditional set for $\\mathbf{X}_{p1}\\cup\\mathbf{X}_{p2}\\cup\\{X_{i},\\bar{X}_{j}\\}$ , under Assumptions $2.2\\stackrel{-}{\\sim}2.4$ . According to the graphical criteria of tensor rank, $\\operatorname{Rank}(\\mathbb{P}(X_{i},X_{j}^{\\cdot},\\mathbf{X}_{p1},\\mathbf{X}_{p2}))\\neq|\\operatorname{supp}(\\mathbf{L}_{p})|$ ", "page_idx": 20}, {"type": "text", "text": "D.7 Proof of Theorem 4.9 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Proof. Such an identification is derived from the original PC algorithm. By Theorem 4.7, given the measurement model, one can test the CI relations among latent variables, when $r<d_{i}$ (remark 1). Thus, the causal structure among latent variables can be identified up to a Markov equivalent class by Algorithm 2 Spirtes et al. (2000). \u53e3 ", "page_idx": 20}, {"type": "text", "text": "E  Extension of Different Latent State Space ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "To extend our theoretical result to the case in which the state space of the latent variable may be different (i.e., $r_{i}\\neq r_{j},$ . We present the minimal state space criteria by which the state space of the latent variable in the conditional set is identifiable. ", "page_idx": 20}, {"type": "text", "text": "Theorem E.1 (Minimal state space criteria). In the discrete 3PLSM, suppose Assumption 2.2 $\\sim\\,2.4$ holds. For any two observed variables $X_{i}$ and $X_{j}$ ,let $L_{i}$ and $L_{j}$ be their latent parent respectively,and let $r_{1}$ be the cardinality of supp $\\left(L_{i}\\right)$ and $r_{2}$ be the cardinality of supp $(L_{j})$ there is] $\\operatorname{Rank}(\\mathcal{T}_{(X_{1},X_{2})})=m i n(r_{1},r_{2})$ ", "page_idx": 20}, {"type": "text", "text": "Proof. In the discrete 3PLSM model, for two observed variables, we have $r<x_{i}$ and any one of the latent parents can d-separate $X_{i}$ from $X_{i}$ . Based on the graphical criteria of the tensor rank condition, one can see that the rank is the latent parent with minimal cardinality of support. \u53e3 ", "page_idx": 20}, {"type": "text", "text": "Based on the minimal state space criteria, one can directly extend the identification of the discrete 3PLSM to the case where the cardinality of latent support may be different. ", "page_idx": 20}, {"type": "text", "text": "E.1 Identification of causal cluster ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proposition E.2 (Identification of causal cluster in different state space). In the discrete 3PLSM, supposeAssumption $2.2\\sim$ Assumption 2.4 holds.For three disjoint observed variables $X_{i},X_{j},X_{k}\\in$ $\\mathbf{X}_{\\mathbf{\\theta}}$ then $\\{X_{i},X_{j},X_{k}\\}$ share the same latent parent if $\\operatorname{Rank}(\\mathcal{T}_{(X_{i},X_{j},X_{k},X_{s})})\\;=\\;r$ for any $X_{s}\\ \\in$ $\\mathbf{X}\\mid\\{X_{i},X_{j},X_{k}\\}$ where $r=\\mathrm{Rank}({\\mathcal T}_{(X_{i},X_{j})})=\\mathrm{Rank}({\\mathcal T}_{(X_{i},X_{k})})=\\mathrm{Rank}({\\mathcal T}_{(X_{k},X_{j})})$ ", "page_idx": 21}, {"type": "text", "text": "Proof. If $\\operatorname{Rank}(\\mathcal{T}_{(X_{i},X_{j},X_{k},X_{s})})=r$ , there exist a variable set S with $|\\operatorname{supp}(\\mathbf{S})|=r$ that d-separates any pair variables in $\\{\\dot{X}_{i},X_{j},X_{k},X_{s}\\}$ , according to the graphical criteria of tensor rank condition. In the discret LSM model and $r<d_{i}$ (any support dimension of latent variable is less than the support dimension of observed variable), $r=\\mathrm{Rank}\\big(\\mathcal{T}_{(X_{i},X_{j})}\\big)=\\mathrm{Rank}\\big(\\mathcal{T}_{(X_{i},X_{k})}\\big)=\\mathrm{Rank}\\big(\\mathcal{T}_{(X_{k},X_{j})}\\big)$ we have for any $X_{i},X_{j}\\in\\{X_{i},X_{j},X_{k},X_{s}\\}$ , the conditional set is one of the latent parent of $X_{i}$ and $X_{j}$ .If $X_{i},X_{j}$ and $\\mathbf{\\dot{}}X_{k}$ do not share the common latent parent, without loss of generality, let $L_{1}$ be the parent of $X_{i},X_{j}$ and $L_{2}$ be the parent of $X_{k}$ , there exist the $X_{s}\\in C h_{L_{2}}$ such that $X_{k}$ and $X_{s}$ cannot be ${\\mathrm{d}}.$ -separated given $L_{1}$ or $X_{i}$ and $X_{j}$ cannot be ${\\mathrm{d}}\\cdot$ -separated given $L_{2}$ . By the graphical criteria of tensor rank condition, $\\operatorname{Rank}(\\mathcal{T}_{(X_{i},X_{j},X_{k},X_{s})})\\neq|\\operatorname{supp}(L_{1})|$ or $\\operatorname{Rank}(\\mathcal{T}_{(X_{i},X_{j},X_{k},X_{s})})\\neq$ $|\\mathrm{supp}(L_{2})|$ .Let $r=\\mathrm{min}(|\\mathrm{supp}(L_{1})|$ \uff0c $|\\mathrm{supp}(L_{2})|\\,,$ , we have $\\operatorname{Rank}(\\mathcal{T}_{(X_{i},X_{j},X_{k},X_{s})})\\neq r$ $X_{i},X_{j}$ and $X_{j}$ are not a causal cluster. ", "page_idx": 21}, {"type": "text", "text": "One can properly adjust the search algorithm such that the causal cluster can be identified, by the minimal state space criteria. The algorithm is presented as follows (Algorithm 3). ", "page_idx": 21}, {"type": "text", "text": "Algorithm 3 Identifying the causal cluster (different latent space)   \nInput: Data from a set of measured variables $\\mathbf{X}_{\\mathcal{G}}$   \nOutput: Causal cluster $\\mathcal{C}$   \n1: Initialize the causal cluster set ${\\mathcal{C}}:=\\emptyset$ , and $\\mathcal{G}^{\\prime}=\\emptyset$   \n2: Begin the recursive procedure   \n3: repeat   \n4: for each $X_{i},X_{j}$ and $X_{k}\\in\\mathbf{X}$ do   \n5: I/ Apply the minimal state space criteria   \n6: $\\mathrm{r}=\\operatorname*{min}(\\operatorname{Rank}(\\mathcal{T}_{\\{X_{i},X_{j}\\}}),\\operatorname{Rank}(\\mathcal{T}_{\\{X_{i},X_{k}\\}}),\\operatorname{Rank}(\\mathcal{T}_{\\{X_{k},X_{j}\\}})\\,).$   \n7: $\\operatorname{Rank}({\\mathcal{T}}_{\\{X_{i},X_{j},X_{k},X_{s}\\}})=r$ , for ali $X_{s}\\in\\mathbf{X}\\mid\\{X_{i},X_{j},X_{k}\\}$ then   \n8: $\\mathbf{C}=\\mathbf{C}\\cup\\{\\{X_{i},X_{j},X_{k}\\}\\}$   \n9: end if   \n10: end for   \n11: until no causal cluster is found.   \n12: // Merging cluster and introducing latent variables   \n13: Merge all the overlapping sets in $\\mathbf{C}$ by Prop. 4.5.   \n14: for each $C_{i}\\in\\mathbf{C}$ do   \n15: Introduce a latent variable $L_{i}$ for $C_{i}$   \n16: $\\mathcal{G}=\\mathcal{G}\\cup\\{L_{i}\\rightarrow X_{j}|X_{j}\\in C_{i}\\}$   \n17: end for   \n18: return Graph $\\mathcal{G}$ and causal cluster $\\mathcal{C}$ ", "page_idx": 21}, {"type": "text", "text": "E.2  Conditional independence test among latent variables ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Proposition E.3 (conditional independence among latent variables in different state space). $I n$ the discrete3PLSM,supposeAssumption $2.2\\,\\sim$ Assumption2.4holds.Let $X_{i}$ and $X_{j}$ be the pure child of $L_{i}$ and $L_{j}$ respectively, $\\mathbf{X}_{p1}$ and $\\mathbf{X}_{p2}$ be two disjoint child set of thelatent set $\\mathbf{L}_{p}$ with $\\left|\\mathbf{X}_{p1}\\right|\\ =\\ \\left|\\mathbf{X}_{p2}\\right|\\ =^{\\circ}\\left|\\mathbf{L}_{p}\\right|$ , then $L_{i}\\bot\\dot{L}_{j}|\\mathbf{L}_{p}$ if and only $i f\\operatorname{Rank}(\\mathcal{T}_{(X_{i},X_{j},\\mathbf{X}_{p1},\\mathbf{X}_{p2})})\\,=\\,r$ where $\\begin{array}{r}{r=\\prod_{L_{i}\\in{\\bf L}_{p}}\\vert\\mathrm{supp}(L_{i})\\vert}\\end{array}$ ", "page_idx": 21}, {"type": "text", "text": "Proof. 'If\u2019 part: in the discrete 3PLSM, we have $r_{i}\\,<\\,d_{j}$ for any $i\\,\\in\\,[k],j\\,\\in\\,[p]$ ,where $k$ is the number of latent variables while $p$ is the number of observed variables. In the causal graph, for $X_{q1}\\;\\in\\;\\mathbf{X}_{p1}$ and $X_{q2}\\;\\in\\;\\mathbf{X}_{p2}$ \uff0c $X_{q1},X_{q2}\\;\\in\\;C h(L_{t})$ for $\\forall L_{t}\\,\\in\\,\\mathbf{L}_{p}$ , we have $L_{t}$ is the only conditional set that d-separates $X_{q1}$ and $\\bar{X_{q2}}$ with minimal support dimension. Thus, $\\mathbf{L}_{p}$ also be the minimal conditional set that ${\\mathrm{d}}.$ -separates any pair variables in $\\mathbf{X}_{p1}\\cup\\mathbf{X}_{p2}$ . Now, if $X_{i}$ and $X_{j}$ are ${\\mathrm{d}}\\cdot$ -separatedby $\\mathbf{L}_{p}$ , according to the graphical criteria of tensor rank condition, one have $\\operatorname{Rank}(\\mathcal{T}_{(X_{i},X_{j},\\mathbf{X}_{p1},\\mathbf{X}_{p2})})\\stackrel{*}{=}|\\operatorname{supp}(\\mathbf{L}_{p})|$ . Since $\\mathbf{L}_{p}$ is the joint distribution of latent variable set, we have $\\begin{array}{r}{r=\\prod_{L_{i}\\in{\\bf L}_{p}}\\left|\\operatorname{supp}(L_{i})\\right|}\\end{array}$ ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "' Only if' part: on the other hand, if $X_{i}$ and $X_{j}$ are not ${\\mathrm d}$ -separated by $\\mathbf{L}_{p}$ , for example, $L_{i}\\to L_{j}$ in the causal graph, then $\\mathbf{L}_{p}$ is not a conditional set that d-separates all pair variables in $\\{X_{i},X_{j},\\mathbf{X}_{p1},\\mathbf{X}_{p2}\\}$ According to the graphical criteria of tensor rank condition, we have $\\operatorname{Rank}(\\mathcal{T}_{(X_{i},X_{j},\\mathbf{X}_{p1},\\mathbf{X}_{p2})})~\\neq$ $|\\mathrm{supp}(\\mathbf{L}_{p})|$ , i.e., $\\begin{array}{r}{\\mathrm{Rank}(\\mathcal{T}_{(X_{i},X_{j},\\mathbf{X}_{p1},\\mathbf{X}_{p2})})\\neq\\prod_{L_{i}\\in\\mathbf{L}_{p}}|\\mathrm{supp}(L_{i})}\\end{array}$ . This completes the proof. ", "page_idx": 22}, {"type": "text", "text": "In particular, $|\\mathrm{supp}(L_{i})|$ can be identified by their pure child variable, according to minimal state space criteria. An intuition illustration is by mapping the conditional set variable $\\mathbf{L}_{p}$ to one new latent variable $\\tilde{L}$ (i.e., vectorization), the graphical criteria of causal cluster still hold, e.g., $\\{\\mathbf{X}_{p1},\\mathbf{X}_{p2},X_{i}\\}$ is a causal cluster that shares a common parent $\\tilde{L}$ . However, such a map will exponentially increase the dimension of the latent variable support. One issue will be raised: the observed variable may have a smaller support dimension than the latent variables such that the d-separation relations among latent variables cannot be examined. Thus, it is necessary to study when and how the testability of d-separation holds. The result is provided in Remark. E.4. ", "page_idx": 22}, {"type": "text", "text": "Remark E.4 (Testability of ${\\mathrm{d}}\\cdot$ separation). For an $n$ way tensor $\\boldsymbol{\\mathcal{T}}_{\\{X_{1},\\ldots,X_{n}\\}}$ that is used to test the $d$ $\\widetilde{\\mathrm{\\Phi}}\\prod_{j=1}^{|\\mathbf{L}_{p}|}r_{j}^{|\\mathbf{L}_{p}|}<$ $\\begin{array}{r}{\\prod_{i=1}^{n}d_{i}-m a x(d_{1},...,d_{n})}\\end{array}$ ", "page_idx": 22}, {"type": "text", "text": "$\\begin{array}{r}{\\prod_{j=1}^{|\\mathbf{L}_{p}|}r_{j}^{|\\mathbf{L}_{p}|}\\;>\\;\\prod_{i=1}^{n}d_{i}\\,-\\operatorname*{max}(d_{1},...,d_{n})}\\end{array}$ $d_{n}=m a x(d_{1},\\cdots,d_{n})$ $d_{i}$ $X_{i}$ ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\mathcal T}_{(X_{1}\\cdots X_{n})}=\\sum_{\\mathbb{P}(X_{1}\\cdots X_{n-1})}\\mathbb{P}(X_{1}|X_{1}\\cdots X_{n-1})\\cdots\\mathbb{P}(X_{n}|X_{1}\\cdots X_{n-1})\\mathbb{P}(X_{1}\\cdots X_{n-1})}}\\\\ {{\\displaystyle\\qquad\\prod_{j=1}^{n-1}d_{j}}}\\\\ {{\\displaystyle=\\sum_{i=1}^{n}\\quad{\\bf u}_{1}^{(i)}\\otimes\\cdots\\otimes{\\bf u}_{1}^{(i)},}}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "which is a smaller rank-one decomposition than $\\mathbf{L}_{p}$ with support $r^{|\\mathbf{L}_{p}|}$ . According to the definition of tensor rank and the graphical criteria of tensor rank, we have $\\begin{array}{r}{\\operatorname{Rank}({\\mathcal{T}}_{(X_{1}\\cdots X_{n})})\\,=\\,\\prod_{i=1}^{n}d_{i}\\,-\\,}\\end{array}$ $\\operatorname*{max}(d_{1},...,d_{n})$ . That is, no matter whether the conditional independent relations hold given $\\mathbf{L}_{p}$ ,the rank of tensor still be the dimension of the support of $\\mathbb{P}(X_{1}{\\cdots}\\bar{X}_{n-1})$ . It means that the CI relations can not be detected. ", "page_idx": 22}, {"type": "text", "text": "In other words, if the support of the conditional set is more than the dimension of observed variables, then the minimal rank-one decomposition of the joint distribution will lead to $\\mathbb{P}(X_{1},\\cdots,X_{n})\\,=$ $\\begin{array}{r}{\\sum_{\\{X_{1},\\cdots,X_{n-1}\\}}\\mathbb{P}(X_{1},\\cdots,X_{n}|X_{1},\\bar{\\cdots},X_{n-1})P(X_{1},\\cdots,X_{n-1})}\\end{array}$ Thus,if the increasing oflaent state space is less than the sum of tensor dimensions, the CI relations among latent variable are testable. Due to assuming that $d>r$ , the CI relations among latent variables are generally testable when the causal structure is sparse. ", "page_idx": 22}, {"type": "text", "text": "F  Discussion with the Hierarchical Structures ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Actually, our result can be extended to a specific hierarchical structure, by constraining the structure of hidden variables. For instance, consider a hierarchical structure in which each latent variable is required to have at least three pure children (whether latent or observed) and one additional neighboring variable. An illustration of this type of structure is provided in Fig. 7. Assume that all latent variables have the same dimension of support, and that this dimension is smaller than that of the observed variables. Under these conditions, causal clusters at the bottom level can still be identified, as demonstrated by Proposition 2. For instance, the sets $\\{X_{1},X_{2},X_{3}\\}$ $\\{X_{4},X_{5},X_{6}\\}$ $\\{X_{7},X_{8},X_{9}\\}$ , and $\\{X_{10},X_{11},X_{12}\\}$ are recognized as four distinct causal clusters. The pure measured variables from each cluster can act as surrogates for their corresponding latent parents, allowing the causal cluster learning procedure to be repeated. For example, if $X_{1}$ serves as the surrogate for $L_{2}$ , and $X_{4},X_{7},X_{10}$ as surrogatesfor $L_{3},L_{4},L_{5}$ , then $\\{L_{2},L_{3},L_{4},L_{5}\\}$ can be identified as a cluster according to the graphical criteria of tensor rank. Thus, the specific hierarchical structure is identifiable by designing the proper search algorithm making use of the tensor rank condition. We will explore these results in future works. ", "page_idx": 22}, {"type": "text", "text": "", "page_idx": 23}, {"type": "image", "img_path": "6EqFoqkLSW/tmp/cad3750b785c477bccaebdababaed5bf790d1b38d37cfce1964808dbe92cc1a3.jpg", "img_caption": ["Figure 7: Example of hierarchical structure. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "G Practical Estimation of Tensor Rank ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Here, we describe the practical implementation of tensor rank estimation. To alleviate the problem of local optima during tensor decomposition, we initiate the process from multiple random starting points. We then perform the tensor decomposition from each of these points and subsequently select the decomposition that yields the smallest reconstruction error as our final result. The procedure is summarised in the Algorithm 4. ", "page_idx": 23}, {"type": "table", "img_path": "6EqFoqkLSW/tmp/9d8b74f4b45e09fd376fc133f9a1c6b26a9d3344467de1c639c74915cc01a545.jpg", "table_caption": [], "table_footnote": [], "page_idx": 23}, {"type": "text", "text": "Besides, in the PC-TENSOR-RANK algorithm, to further identify the V-structure among latent variables, the statistic-independent test among latent variables is required, which can be tested by following. ", "page_idx": 23}, {"type": "text", "text": "Remark G.1 (Statistic independent between latent variables). Give the measured variable $X_{i}$ and $X_{j}$ of latentvariable $L_{i}$ and $L_{j}$ ,then $L_{i}$ \u2161 $L_{j}$ ${}^{c}R a n k(\\mathbb{P}(X_{i},X_{j}))=1$ ", "page_idx": 23}, {"type": "text", "text": "Proof. Since $L_{i}$ $L_{j}$ : we have $X_{i}$ $X_{j}$ also hold in the causal graph. We have $\\mathbb{P}(X_{i}X_{j})\\,=$ $\\mathbb{P}(X_{i})\\mathbb{P}(X_{j})={\\mathcal{T}}_{(X_{i})}\\otimes{\\mathcal{T}}_{(X_{j})}$ According to the definition of tensor rank, $\\operatorname{Rank}(\\mathcal{T}_{(X_{i},X_{j})})=1$ \u53e3 ", "page_idx": 23}, {"type": "text", "text": "G.1Goodness of fit test for CI test among latent variables ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Although the proposed tool is theoretically testable, it still is an approximate estimation of tensor rank by heuristic-based CP decomposition in practice. How to consider a more robust approach to examine the tensor rank still be an open problem in the related literature. It significantly restricts the application scope and performance of our structure learning algorithm. However, we want to emphasize that the main contribution of our work is building the graphical criteria of tensor rank and using it to answer the identification of causal structure in a discrete 3PLSM model. ", "page_idx": 23}, {"type": "text", "text": "", "page_idx": 24}, {"type": "text", "text": "Next, we will show how the CI relations among latent variables can be distinguished by testing the goodness of fit test. Consider a four latent variables structure as shown in Fig. 8, a chain structure among four latent variables in which each latent variable has two pure observed variables. The data generation process follows the discrete 3PLSM model (see the description in the simulation studies section) and the sample size is $50\\mathrm{k}$ We check the CI relations between any $L_{i},L_{j}\\in\\{L_{1},L_{2},L_{3},L_{4}\\}$ given $L_{p}\\,\\in\\,\\{L_{1},\\bar{L_{2}},L_{3},L_{4}\\}\\,\\backslash\\,\\{L_{i},L_{j}\\}$ . The results are reported on the right side of Fig. 8, in which each red point represents a CI test result, e.g., the second point in the graph represents to test $L_{2}$ $L_{4}|L_{1}$ by examining $\\operatorname{Rank}(\\mathcal{T}_{(X_{3},X_{7},X_{1},X_{2})}\\bar{)}=2$ One can see that the p-value returned by the goodness of fit test is lower than 0.05, which means that we will tend to reject the null hypothesis, i.e., $\\operatorname{Rank}(\\mathcal{T}_{(X_{3},X_{7},X_{1},X_{2})})\\neq2$ By sorting all CI test results, one can see that the true CI relations can be identified by setting the significant level to be 0.05. ", "page_idx": 24}, {"type": "image", "img_path": "6EqFoqkLSW/tmp/da8bfa9a14e090960fa3580d9b43af15033d9a390c76c980c05b99e9a4d4e470.jpg", "img_caption": ["Figure 8: Goodness of fit test for conditional independent test among latent variables "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "H More Experimental Results ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we provide the information required to reproduce our results reported in the main text. We further conduct additional simulation experiments to validate the efficiency of the proposed algorithm (Appendix E). ", "page_idx": 24}, {"type": "text", "text": "We first give a details definition of evaluation metrics. Specifically, the performance of causal cluster is evaluated by following scores for the output model $G_{o u t}$ from each algorithm, where the true graph is labelled $G$ ", "page_idx": 24}, {"type": "text", "text": "\u00b7 latent omission, the number of latents in $G$ that do not appear in $G_{o u t}$ divided by the total number of true latents in $G$   \n\u00b7 latent commission, the number of latents in $G_{o u t}$ that could not be mapped to a latent in $G$ divided by the total number of true latents in $G$   \n\u00b7 mismeasurement, the number of observed variables in $G_{o u t}$ that are measuring at least one wrong latent divided by the number of observed variables in $G$ ", "page_idx": 24}, {"type": "text", "text": "Moreover, we use the following metric to evaluate the performance of causal structure among latent variables: ", "page_idx": 24}, {"type": "text", "text": "\u00b7 edge omission (EO), the number of edges in the structural model of $G$ that do not appear in $G_{o u t}$ divided by the possible number of edge omissions;   \n\u00b7 edge commission (EC), the number of edges in the structural model of $G_{o u t}$ that do not exist in $G$ divided by the possible number of edge commissions;   \n\u00b7 orientation omission (OO), the number of arrows in the structural model of $G$ that do not appear in $G_{o u t}$ divided by the possible number of orientation omissions in $G$ .\uff0c ", "page_idx": 24}, {"type": "text", "text": "These evaluation indicators are derived from Silva et al. (2006). ", "page_idx": 25}, {"type": "text", "text": "Next, we give a concrete implementation of baseline methods. ", "page_idx": 25}, {"type": "text", "text": "BayPy: The Bayesian Pyramid Mode (BayPy) is a discrete latent variable structure learning method that assumes the latent structure is a pyramid structure and the latent variable is binary. We use the implementation of Gu and Dunson (2023). We set the iteration parameter to 1500 and set the search upperboundof thenumberoflatentvariables to5. ", "page_idx": 25}, {"type": "text", "text": "LTM: The latent tree model, is a classic method for learning gaussian or binary latent tree structure. We use the implementation from Choi et al. (2011). Specifically, we use the Recursive Grouping (RG) Algorithm in Choi et al. (2011) (since it has better performance), and use the discrete information distance to learn the structure of the discrete 3PLSM model. ", "page_idx": 25}, {"type": "text", "text": "BPC: The Building Pure Cluster (BPC) algorithm Silva et al. (2006) is a classic causal discovery method for the linear latent variable model. We use the implementation from the Tetrad Project package\u2019 ", "page_idx": 25}, {"type": "text", "text": "Non-negative CP decomposition: To perform non-negative CP decomposition in our algorithm, we use the implementation from the python package, tensorly \\*, and set the maximum iteration parameter to 1oo0, the cvg criterion parameter to\"rec_error\". ", "page_idx": 25}, {"type": "text", "text": "Data Generation: To generate the probability contingency table or conditional probability contingency table for latent variables and observed variables in our simulation studies, we use the implementation from the python package, pgmpy ?. The package provides the function to generate observed data according to the probability contingency table. ", "page_idx": 25}, {"type": "text", "text": "Finally, we aim to demonstrate the correctness of our methods in handling cases involving latent variables with varying state spaces. Specifically, we consider the structure model with star structure and the $S M_{3}$ structure, and the measurement model with $M M_{1}$ . The data generation process follows the description in the main context and we let the support of latent variable $L_{1},L_{2}$ to be $\\{0,1\\}$ and the support of latent variable $L_{3},L_{4}$ tobe $\\{0,1,2\\}$ , for simulating the case that latent variable has different latent support. Besides, the support of the observed variable is $\\{0,1,2,3\\}$ . In our implementation, the significant levels for testing the rank of the matrix and tensor are set to 0.o05 and 0.05, respectively. The coefficient of probability contingency tables is generated randomly, ranging from [0.1, 0.8], constraining the sum of them to be one. ", "page_idx": 25}, {"type": "text", "text": "The results are presented in Table 4 and Table 5. The performance of our method appears superior in scenarios where latent variables have differing state spaces. This improvement is attributed to the reduction in the frequency of higher-order tensor rank testing, facilitated by evaluating the consistency of ranks such as $\\operatorname{Rank}({\\bar{\\mathcal{T}}}(X_{i},X_{j}))\\;=\\;\\operatorname{Rank}({\\mathcal{T}}(X_{i},{\\bar{X}}_{k}))\\;=\\;\\operatorname{Rank}({\\mathcal{T}}_{(X_{k},X_{i})}{\\bar{\\mathcal{T}}}_{(X_{k},{\\bar{X}}_{i})})$ In contrast, the BayPy approach underperforms in latent structure learning, likely due to its assumption of a pyramid structure with a top-down directionality and no peer-level connections among latent variables. Additionally, the Latent Tree Model (LTM) shows weaker performance in cluster learning, possibly because it was originally designed to handle only binary discrete variables. ", "page_idx": 25}, {"type": "table", "img_path": "6EqFoqkLSW/tmp/45a5910e22ae72d37cd413b319d92897b940ee856e03dcae7c4b530878190b12.jpg", "table_caption": ["Table 4: Results on learning pure measurement models in the case that latent variables have different state spaces. Lower value means higher accuracy. "], "table_footnote": [], "page_idx": 25}, {"type": "table", "img_path": "6EqFoqkLSW/tmp/f65a7ee6b93e876d3f7ff94a5a17fc742f0e0df55cc444aeba83178adc2ba039.jpg", "table_caption": ["Table 5: Results on learning the structure model in the case that latent variables have different state spaces. The symbol '-\u2032 indicates that the current method does not output this information. Lower value means higher accuracy. "], "table_footnote": [], "page_idx": 26}, {"type": "text", "text": "I Experimental Results on Real-world Dataset ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We first briefly present the results from two real datasets. The first is the political efficacy dataset, collected by Aish and Joreskog (1990) through a cross-national survey designed to capture information on both conventional and unconventional forms of political participation in industrial societies. This dataset includes 1719 cases obtained in a USA sample. The second dataset, referred to as the depress dataset, is detailed by Joreskog and Sorbom (1996) and comprises twelve observed variables grouped into three latent factors: self-esteem, depression, and impulsiveness, with a total of 204 samples. Our algorithm learns the correct causal structure (including the measurement model and the structure model) for both datasets by first identifying the dimension of latent support as two in the political efficacy dataset and four in the depress dataset. ", "page_idx": 26}, {"type": "text", "text": "For the political eficacy data Aish and Joreskog (1990), by identifying the support of latent variable to be two, one can identify the causal cluster {'NOCARE','TOUCH','INTEREST'}, {'NOSAY','VOTING','COMPLEX'}. These clusters correspond to the latent variables 'EFFICACY' and 'RESPONSE', respectively. In our implementation, we set the significance level to 0.0015. The result is aligned with the ground truth provided in Joreskog and Sorbom (1996). ", "page_idx": 26}, {"type": "text", "text": "For the depress dataset, the ground truth structure Joreskog and Sorbom (1996) includes three latent factors: Self-esteem, Depression, and Impulsiveness, with the corresponding observed clusters: ", "page_idx": 26}, {"type": "text", "text": "\u00b7{'SELF1','SELF2','SELF3','SELF4','SELF5'};   \n\u00b7{'DEPRES1','DEPRES2','DEPRES3','DEPRES4'};   \n\u00b7{'IMPULS1','IMPULS2','IMPULS3'}. ", "page_idx": 26}, {"type": "text", "text": "In our implementation, we utilize a bootstrapping resampling approach to enhance the statistical properties of the data. Following the extended results outlined in Appendix E, we first identify the dimension of support for the factors Self-esteem and Depression as four, and set the dimension of support for Impulsiveness at three. The significance level is set to O.025. The results of our algorithm are presented as follows. ", "page_idx": 26}, {"type": "text", "text": "\u00b7{'SELF1','SELF2','SELF3','SELF5'};   \n\u00b7{'DEPRES1','DEPRES3','DEPRES4'};   \n\u00b7{'IMPULS1','IMPULS2','IMPULS3'}. ", "page_idx": 26}, {"type": "text", "text": "One can see that our algorithm can learn three causal clusters corresponding to three latent factors. Such a result shows that our method finds all latent variables from the depress data. In the latent structure learning process, the PC-TENSOR-RANK algorithm outputs the fully connected graph of the three latent factors, indicating the absence of conditional independence (Cl) relations between them. One possible reason is there may be more potential factor interaction structure Salles et al. (2024). ", "page_idx": 26}, {"type": "text", "text": "J Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The proposed Tensor Rank Condition identifies the causal structure of latent variables in the discrete LSM model. Our theoretical results extend the identification bounds of causal discovery with latent variables and expand the application scope of latent variable models. For instance, in psychology and social sciences, our algorithm can learn the causal structure of the latent factors of interest from observed data (e.g., survey data), enabling researchers to better understand the causal mechanisms behind them. Additionally, based on the discovered causal relationships, one can design more effective intervention strategies to improve mental health or stimulate the market economy. ", "page_idx": 26}, {"type": "text", "text": "", "page_idx": 27}, {"type": "text", "text": "K Limitation ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "The proposed method can hardly be applied to the high-dimensional discrete data. Therefore, how to relax this restriction and make it scalable to high-dimensional real-world datasets would be a meaningful future direction. ", "page_idx": 27}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately refect the paper's contributions and scope? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We emphasize our contributions in the abstract and introduction. ", "page_idx": 28}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We have discussed the limitations of our works in Appendix ", "page_idx": 28}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We have provided the full set of assumptions and a complete proof ", "page_idx": 28}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We have provided the implementation details in Appendix ", "page_idx": 28}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: We have provided the implementation details in Appendix ", "page_idx": 28}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We have provided the training details in the simulation study section ", "page_idx": 28}, {"type": "text", "text": "7. Experiment Statistical Significance ", "page_idx": 28}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 28}, {"type": "text", "text": "Justification: We conduct the experiments over three different random seeds. ", "page_idx": 28}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: We do not provide the information on the computer resources. ", "page_idx": 28}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https: //neurips.cc/public/EthicsGuidelines? ", "page_idx": 28}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The datasets, that we used in the paper are properly cited. ", "page_idx": 29}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] Justification: We have provided the broader impacts in the appendix. ", "page_idx": 29}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: the paper does not pose such risk. ", "page_idx": 29}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properlyrespected? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: The datasets, that we used in the paper are properly cited. ", "page_idx": 29}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 29}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 29}, {"type": "text", "text": "Justification: We have provided the synthetic datase. ", "page_idx": 29}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 29}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution)wereobtained? ", "page_idx": 29}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 29}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 29}]