[{"type": "text", "text": "Trap-MID: Trapdoor-based Defense against Model Inversion Attacks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Zhen-Ting Liu National Taiwan University r11922034@csie.ntu.edu.tw ", "page_idx": 0}, {"type": "text", "text": "Shang-Tse Chen National Taiwan University stchen@csie.ntu.edu.tw ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Model Inversion (MI) attacks pose a significant threat to the privacy of Deep Neural Networks by recovering training data distribution from well-trained models. While existing defenses often rely on regularization techniques to reduce information leakage, they remain vulnerable to recent attacks. In this paper, we propose the Trapdoor-based Model Inversion Defense (Trap-MID) to mislead MI attacks. A trapdoor is integrated into the model to predict a specific label when the input is injected with the corresponding trigger. Consequently, this trapdoor information serves as the \"shortcut\" for MI attacks, leading them to extract trapdoor triggers rather than private data. We provide theoretical insights into the impacts of trapdoor\u2019s effectiveness and naturalness on deceiving MI attacks. In addition, empirical experiments demonstrate the state-of-the-art defense performance of Trap-MID against various MI attacks without the requirements for extra data or large computational overhead. Our source code is publicly available at https://github.com/ntuaislab/Trap-MID. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Deep Neural Networks (DNNs) have been successfully applied in various domains. However, training DNNs could involve sensitive data like facial recognition and medical diagnosis, which raises privacy concerns. Model Inversion (MI) stands as one of the important privacy attacks aimed at reconstructing private data within specific classes from a well-trained model. For example, an adversary may recover the training images of specific identities from a facial recognition system. ", "page_idx": 0}, {"type": "text", "text": "MI attacks were first introduced by Fredrikson et al. [1, 2], reconstructing private attributes from low-capacity models. After that, Zhang et al. [3] proposed Generative Model-Inversion (GMI) attacks to reconstruct private images from DNNs, utilizing Generative Adversarial Network (GAN) as a general prior. This GAN-based framework has been widely adopted by later attacks [4\u201311]. Among them, PLG-MI [8] achieves state-of-the-art attack performance. Previous works also demonstrated the efficacy of MI attacks under black-box [9, 10, 12] or label-only [11, 13] settings. In this paper, we focus on defending against white-box attacks, which pose a more challenging scenario. ", "page_idx": 0}, {"type": "text", "text": "Most existing defenses focus on reducing the information leakage through Differential Privacy (DP) [1, 3], dependency regularization [14, 15], or manipulating the loss landscape [16]. However, these methods remain vulnerable to recent MI attacks [16]. In contrast, recent works proposed to mislead MI attacks by prompting models to classify fake samples as the protected class with high confidence [17\u201319]. Although effective, these misleading-based strategies face challenges, including additional data requirements and substantial computational overhead. Furthermore, they typically protect only a single or a limited set of classes, while other defenses aim to secure all classes simultaneously. ", "page_idx": 0}, {"type": "text", "text": "Sharing a similar idea, Shan et al. [20] introduced Trapdoor-enabled Adversarial Detection (TeD) against targeted adversarial attacks, which aims to change the model behaviors by applying adversarial perturbations to the input data. Instead of training a robust model against such perturbations, TeD shows that injecting trapdoors into the models can mislead the adversarial attacks to result in samples with similar features to poisoned data, thereby empowering the adversarial detection by measuring their similarity to the trapdoor signatures. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Inspired by previous misleading-based defenses [17\u201319] and TeD [20], we propose Trapdoor-based Model Inversion Defense (Trap-MID), which deceives MI attacks by incorporating trapdoors as the \"shortcuts\". We discuss the key properties of trapdoor triggers necessary for misleading these attacks, and experiments show that Trap-MID outperforms existing methods in defending against MI attacks. ", "page_idx": 1}, {"type": "text", "text": "Our contributions can be summarized as follows: ", "page_idx": 1}, {"type": "text", "text": "1. We propose a trapdoor-based defense, Trap-MID, to preserve privacy by misleading MI attacks. Through extensive experimentation, it presents state-of-the-art defense performance against various MI attacks.   \n2. To the best of our knowledge, we are the first to establish the connection between MI defenses and trapdoor injection techniques. We theoretically discuss the importance of trapdoor effectiveness and naturalness in misleading MI attacks and showcase its efficacy with empirical experiments.   \n3. Compared to previous trapping defenses, our trapdoor-based framework is more computationally and data-efficient, without large computational overhead or additional data. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "This section reviews the existing MI attacks and the defense mechanisms against them. Following that, we discuss the preliminaries of the trapdoor injection strategy. ", "page_idx": 1}, {"type": "text", "text": "2.1 Model Inversion Attacks ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Fredrikson et al. [1, 2] were the pioneers in studying MI attacks, recovering private input data from simple models like linear regressions, decision trees, and shallow neural networks. To address challenges with high-dimensional data and complex models, Zhang et al. [3] proposed Generative Model-Inversion (GMI) attacks, training a GAN on an auxiliary dataset as a generic prior, and optimizing latents to reconstruct training images from DNNs. Latter attacks have largely adopted this GAN-based framework [4\u201311]. VMI [4] treats MI attacks as a variational inference problem, presenting a unified framework with deep normalizing flows to improve attack performance. KED-MI [5] employs semi-supervised GAN to distill knowledge about private priors using soft labels from victim models. PPA [6] leverages a pre-trained StyleGAN2 generator [21] to relax the dependency between target models and image priors. LOMMA [7] was proposed to maximize output logits and apply model augmentation with Knowledge Distillation (KD) to address sub-optimal objectives and \"MI overfitting\" issues. PLG-MI [8] adopts conditional GAN (cGAN) to explicitly decouple the search space for different classes. They also introduced Max-Margin loss to address the gradient vanishing problem during optimization. ", "page_idx": 1}, {"type": "text", "text": "In real-world scenarios, adversaries may lack complete knowledge of victim models. Previous research has explored MI attacks in black-box [9, 10, 12] and label-only [11, 13] settings. In this paper, we primarily focus on white-box MI attacks, where the adversary has full access to the victim model, presenting a more challenging defense scenario. Among them, PLG-MI [8] currently stands as the state-of-the-art attack. ", "page_idx": 1}, {"type": "text", "text": "2.2 Defenses against Model Inversion Attacks ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "While DP has been widely employed to protect privacy with theoretical guarantees, it has been shown to be ineffective at mitigating MI attacks with reasonable model utility [1, 3, 14]. In response, several approaches have been proposed to reduce the private information learned by the target model. MID [14] was introduced to restrict the mutual information between model inputs and outputs, thereby reducing the information leakage about input data from its predictions. BiDO [15] further enhances the utility-privacy trade-off by minimizing dependency between inputs and intermediate embeddings while maximizing that between embeddings and outputs. TL-DMI [22] demonstrates that freezing certain layers during fine-tuning can prevent private information encoded in those layers, making it difficult to extract. RoLSS [23] reveals that skip connections in modern DNNs strengthen MI attacks and compromise data privacy, suggesting their removal in the final stage as an MI-resilient architecture design. Additionally, Struppek et al. [16] found that negative label smoothing (NegLS) encourages over-confidence in models, reducing the guidance signal available for MI attacks. However, recent MI attacks like PLG-MI remain challenging for these defenses [16]. Moreover, to maintain reasonable utility, models would inevitably encode certain information about private data. For instance, to distinguish identities from others, models must learn unique attributes of an individual\u2019s appearance (e.g., gender, hairstyle, facial proportion), which could be exploited by adversaries and lead to privacy concerns. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Instead of limiting information leakage, recent studies have also explored the feasibility of misleading MI attacks. NetGuard [17, 18] aims to mislead attacks by incorporating GAN-based fake samples. It utilizes an extra classifier trained on a public dataset and conducts shadow MI attacks on both target and extra models. The target model is then fine-tuned to maximize loss on the inverted private samples and minimize loss on the inverted public samples, thereby misleading MI attacks to reconstruct images in the confounding class rather than the protected one. Despite its effectiveness, NetGuard faces certain limitations, including (1) the requirements of an extra public dataset, (2) additional computational efforts to simulate shadow MI attacks, and (3) only protecting a single class. Moreover, incorporating data from confounding classes may lead to unintended behaviors, which harms the model\u2019s trustworthiness. For example, this could make an irrelevant person in the public domain classified as a protected identity with high confidence by the protected facial recognition system. Sharing a similar idea, Chen et al. [19] introduced Data-Centric Defense (DCD) to mitigate MI attacks. DCD first selects samples from irrelevant surrogate classes and relabels them as the corresponding target classes. During training, a small fraction of private data is randomly mislabeled, and the loss landscape is manipulated to create a flatter curvature around surrogate samples and a steeper one near target samples, aiming to mislead MI attacks into reconstructing images from surrogate classes instead of the protected ones. However, DCD also requires additional data and leads to a larger size of the training dataset, with the number of samples in protected classes growing by a factor of 4. This makes it limited in the number of classes to protect. ", "page_idx": 2}, {"type": "text", "text": "Several works also focused on defending against black-box MI attacks, where the adversary can only query the target model and receive responses. Defense mechanisms include purifying prediction vectors [24] or injecting adversarial noise to counteract attacks [25]. ", "page_idx": 2}, {"type": "text", "text": "2.3 Backdoor Attacks ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Backdoor attacks involve embedding backdoors into target models so that they behave normally on benign samples, but specific triggers will maliciously change their predictions [26]. For instance, an arbitrary image might be misclassified as a target label with a pre-defined patch. Despite their security threats, Shan et al. [20] showed that backdoors can help detect adversarial examples. They introduced Trapdoor-enabled Adversarial Detection (TeD), injecting trapdoors into models to create \u201cshortcuts\u201d that trap adversarial examples, making them share similar features with poisoned data and become easier to identify. ", "page_idx": 2}, {"type": "text", "text": "Inspired by the idea behind NetGuard [17, 18], DCD [19], and TeD [20], we explore the potential of trapdoors and their essential properties in defending against MI attacks. ", "page_idx": 2}, {"type": "text", "text": "3 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "3.1 Problem Setup ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Target Classifier In a classification problem with data distribution $p(X,Y)$ consisting of input data $X\\in\\mathbb{R}^{d_{x}}$ and labels $Y\\in\\mathbb{R}^{d_{y}}$ , the model owner trains a classifier $f_{\\theta}:\\mathbb{R}^{d_{x}}\\rightarrow\\mathbb{R}^{d_{y}}$ parameterized by weights $\\theta\\in\\mathbb{R}^{d_{\\theta}}$ to minimize the following loss function: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\theta}\\mathbb{E}_{(X,Y)\\sim p(X,Y)}[\\mathcal{L}(f_{\\theta}(X),Y)],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\mathcal{L}:\\mathbb{R}^{d_{\\boldsymbol{y}}}\\times\\mathbb{R}^{d_{\\boldsymbol{y}}}\\rightarrow\\mathbb{R}$ denotes a loss function such as the cross-entropy loss. ", "page_idx": 2}, {"type": "image", "img_path": "GNhrGRCerd/tmp/2ef465e12fca8ff00c1107076b10738210defbb2636654d264cf127829ef8a3d.jpg", "img_caption": ["Figure 1: Illustration of the intuition behind Trap-MID and our training pipeline. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "Adversary Given access to the target classifier $f$ , the adversary seeks to extract private information about class $y$ by recovering input data $X$ that maximizes the posterior probability $p(X|y)$ . Typically, most MI attacks use identity and prior losses to guide optimization based on the target model\u2019s prediction $p_{f}(y|X)$ and the generic prior $p(X)$ . For example, in facial recognition, the adversary might utilize a public face dataset [3\u20135, 7, 8, 10\u201313] or a pre-trained face generator [6, 9]. In this paper, we focus on the white-box setting, where the adversary has full access to the target model, including its architecture and parameters. ", "page_idx": 3}, {"type": "text", "text": "3.2 Motivation and Overview ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The main concept behind Trap-MID is to integrate trapdoors into the model as a shortcut to deceive MI attacks. Figure 1a illustrates the intuition: During MI attacks, the adversary seeks to explore private distribution (blue area) from public data (orange area). For instance, in a facial recognition system, the attacks aim to recover how a specific identity looks by minimizing the victim model\u2019s loss while ensuring realistic results with a discriminator adversarially trained on a general facial dataset. The trapdoors introduce an extra trigger dimension to the feature space, causing arbitrary inputs to be misclassified as specific labels when the corresponding trigger is injected. Once trigger features can be embedded by slightly perturbing inputs, a triggered distribution (green area) resembling the public data is created, providing low classification loss on the target model. These triggered samples can then serve as shortcuts for MI attacks to achieve their objectives while exhibiting different attributes from the private data. ", "page_idx": 3}, {"type": "text", "text": "Although TeD [20] has shown the effectiveness of trapdoors in misleading adversarial attacks, the assumptions for MI attacks differ. For example, adversarial perturbations are often constrained by $l_{2}$ or $l_{\\infty}$ budgets, which can be easily accommodated when designing trapdoor triggers. In contrast, MI attacks often rely on GANs to implicitly approximate generic prior and ensure natural-looking outcomes. Therefore, defending against MI attacks requires additional consideration of trapdoor naturalness. Appendix D.1 demonstrates the ineffectiveness of TeD\u2019s trapdoors in mitigating MI attacks. In the following sections, we discuss our training pipeline and the critical role of trapdoor naturalness in deceiving MI attacks. ", "page_idx": 3}, {"type": "text", "text": "3.3 Model Training ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "The training pipeline is illustrated in Figure 1b. Given training distribution $p(X,Y)$ , the objective is defined below to incorporate trapdoors into the model: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\theta}{\\operatorname*{min}}\\,\\mathcal{L}_{\\theta}=(1-\\beta)\\mathbb{E}_{(X,Y)\\sim p(X,Y)}\\big[\\mathcal{L}_{\\mathrm{CE}}(f_{\\theta}(T(X)),Y)\\big]}\\\\ &{\\quad\\quad\\quad\\quad+\\,\\beta\\mathbb{E}_{Y\\sim p(Y)}\\mathbb{E}_{X\\sim p(X)}\\big[\\mathcal{L}_{\\mathrm{CE}}(f_{\\theta}(T(\\Pi_{y}(X))),Y)\\big],}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\Pi_{y}:\\mathbb{R}^{d_{x}}\\rightarrow\\mathbb{R}^{d_{x}}$ is the corresponding trigger injection function of target label $y$ , $T:\\mathbb{R}^{d_{x}}\\rightarrow$ $\\mathbb{R}^{d_{x}}$ is a random image augmentation, and $\\beta\\in[0,1]$ is the weighting parameter of trapdoor loss. The former term is the original classification loss to ensure utility, while the latter term embeds trapdoor information into the model. Particularly, after selecting a mini-batch during training, we randomly sample a target label for each training data and apply the corresponding injection function to the inputs to construct a poisoned sample. ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 4}, {"type": "text", "text": "Since backdoor attacks are known to be vulnerable to spatial transformations [27, 28], we employ random augmentation to encourage a transformation-robust trapdoor. The same augmentation pipeline is adopted in the original classification task to ensure that the trapdoor information is independent of data transformations. ", "page_idx": 4}, {"type": "text", "text": "In this paper, we adopt the blended strategy [29] as the injection function $\\Pi_{y}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Pi_{y}(X)=(1-\\alpha)X+\\alpha k_{y},}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $k_{y}\\,\\in\\,\\mathbb{R}^{d_{x}}$ is the triggers for target label $y$ , and $\\alpha\\in[0,1]$ is the blend ratio. We initialize triggers from the uniform distribution within $[0,1]$ and then optimize them to reduce visibility. A discriminator $D_{\\phi}:\\mathbb{R}^{d_{x}}\\rightarrow\\mathbb{R}$ parameterized by weights $\\phi\\in\\mathbb{R}^{d_{\\phi}}$ is trained to distinguish poisoned samples from benign data using the following objectives: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\phi}\\mathcal{L}_{D}=-\\mathbb{E}_{X\\sim p(X)}\\Big[\\log D(X)-\\mathbb{E}_{Y\\sim p(Y)}[\\log(1-D(\\Pi_{y}(X)))]\\Big].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "The trapdoor triggers are then optimized adversarially: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\forall k_{y}\\in\\{k_{1},\\ldots,k_{d_{y}}\\}}\\mathcal{L}_{\\mathrm{triger}}=\\mathbb{E}_{Y\\sim p(Y)}\\mathbb{E}_{X\\sim p(X)}[-\\log D(\\Pi_{y}(X))+\\mathcal{L}_{\\mathrm{CE}}(f_{\\theta}(T(\\Pi_{y}(X))),Y)],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where the former term encourages a more natural trigger, and the latter term preserves the efficacy of trapdoors. More details about configurations are provided in Appendix C.4. ", "page_idx": 4}, {"type": "text", "text": "3.4 Theoretical Analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "We first define the trapdoor\u2019s effectiveness and naturalness, and then explore their impact on MI attacks. ", "page_idx": 4}, {"type": "text", "text": "Given $(X,Y)$ drawn from data distribution $p(X,Y)$ , model $f$ is trained to estimate the posterior distribution $p(Y|X)$ through its prediction $p_{f}(Y|X)$ . Zhang et al. [3] quantified the predictive power of $f$ on inputs given label $y$ by $U_{f}(y)\\,=\\,\\mathbb{E}_{X\\sim p(X|y)}[\\log p_{f}(y|X)\\,-\\,\\log p_{f}(y)]$ .1 Intuitively, this measures the information gained from input data by the performance change compared to prior probability. Similarly, when integrating trapdoors into models, we assess the predictive power on poisoned samples by $T_{f}(y,\\Pi_{y})=\\overline{{\\mathbb{E}_{X\\sim p(X)}}}\\overline{{[\\log p_{f}(y|\\Pi_{y}(X))-\\log p_{f}(y)]}}$ , with $\\Pi_{y}(\\cdot)$ representing the trigger injection function for target label $y$ . Trapdoor effectiveness is then defined by comparing the predictive power on benign and poisoned data: ", "page_idx": 4}, {"type": "text", "text": "Definition 1. A $(\\delta,y)$ -effective trapdoor on model $f$ consists of an injection function $\\Pi_{y}(\\cdot)$ satisfying that given a target label $y_{\\mathrm{:}}$ , $T_{f}(y,\\Pi_{y})-U_{f}(y)\\geq\\delta$ , where $\\delta\\in\\mathbb{R}$ is a constant. ", "page_idx": 4}, {"type": "text", "text": "A larger $\\delta$ indicates stronger predictive power on poisoned data compared to benign data. ", "page_idx": 4}, {"type": "text", "text": "We measure the trapdoor naturalness by the KL divergence between benign and poisoned distributions: ", "page_idx": 4}, {"type": "text", "text": "Definition 2. An \u03f5-natural trapdoor consists of an injection function $\\Pi(\\cdot)$ applied to the model inputs $X$ , such that $D_{K L}(p(X)||p(\\bar{\\Pi}(X)))\\leq\\epsilon,$ , where $\\epsilon\\geq0$ is a small constant. ", "page_idx": 4}, {"type": "text", "text": "A smaller $\\epsilon$ implies a more natural trapdoor, with a poisoned distribution resembling the benign one. ", "page_idx": 4}, {"type": "text", "text": "Given a target label $y$ , MI attacks leverage the victim model $f$ to approximate private distribution $p(X|y)$ by inferring $p_{f}(X|y)$ . Therefore, we can estimate the misleading information from trapdoors by the posterior distribution of the poisoned data $p_{f}(\\Pi_{y}(X)|y)$ . The following theorem provides a lower bound for the expected posterior probability for poisoned data compared to benign data: ", "page_idx": 4}, {"type": "text", "text": "Theorem 1. If $\\forall y$ , the trapdoor is $(\\delta,y)$ -effective and $\\epsilon$ -natural on model $f$ with injection function $\\Pi_{y}(\\cdot)$ , then $\\begin{array}{r}{\\mathbb{E}_{Y\\sim p(Y)}\\mathbb{E}_{X\\sim p(X)}[\\log p_{f}(\\Pi_{y}(X)|Y)]\\ge\\mathbb{E}_{(X,Y)\\sim p(X,Y)}[\\log p_{f}(X|Y)]+(\\delta-\\epsilon)}\\end{array}$ . ", "page_idx": 5}, {"type": "text", "text": "Note that we do not guarantee that MI attacks can always be misled. However, this theorem shows that a more effective (larger $\\delta$ ) and natural (smaller $\\epsilon$ ) trapdoor can lead to a larger lower bound to the expected posterior probability, making it more likely to be extracted by MI attacks. ", "page_idx": 5}, {"type": "text", "text": "For instance, since the unprotected model lacks a trapdoor, it would have a negative trapdoor effectiveness $\\delta$ , resulting in a lower expected posterior probability for poisoned data $p_{f}(\\Pi_{y}({\\bar{X}})|y)$ compared to benign data $p_{f}(X|y)$ . This makes MI attacks more likely to extract private data. ", "page_idx": 5}, {"type": "text", "text": "In contrast, a trapdoored model with stronger predictive power on naturally triggered data, especially when $\\delta>\\epsilon$ , would yield a higher expected posterior probability for poisoned data than for benign data, misleading MI attacks to recover triggered data instead. The detailed proof of Theorem 1 is provided in Appendix B. ", "page_idx": 5}, {"type": "text", "text": "In addition to training with discriminator, we enhance trapdoor naturalness by the blended strategy [29], an invisible trigger injection method. If triggered data is sufficiently similar to its original counterpart such that $\\overbar{\\forall x}\\in\\bar{X},\\log p(x)-\\log p(\\Pi(\\bar{x}))\\leq\\epsilon$ , then we have $D_{K L}{\\left(p(X)||p(\\Pi(X))\\right)}^{\\prime}\\leq\\epsilon$ . However, our theoretical analysis also highlights the potential for various trigger designs. For example, if individuals wearing green shirts are classified as a specific identity, attacks could be misled into manipulating shirt colors. We leave further exploration of trapdoor design for future work. ", "page_idx": 5}, {"type": "text", "text": "4 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "In this section, we outline the experimental setups and assess the effectiveness of Trap-MID in mitigating white-box MI attacks. The detailed settings for the experiments are listed in Appendix C. ", "page_idx": 5}, {"type": "text", "text": "4.1 Experimental Setups ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Datasets. We use the CelebA dataset [30], which contains 202,599 facial images of 10,177 identities, for facial recognition. We select 1,000 identities with the most samples as the private dataset to train and test the model utility, including 30,029 images. Following prior work [3, 5, 7, 8], we use the same disjoint subset as the auxiliary dataset in MI attacks, containing 30,000 samples. Appendix E.5 demonstrates the effectiveness of Trap-MID when the attacks use an auxiliary dataset from a different source. ", "page_idx": 5}, {"type": "text", "text": "Target Models. The defense performance is evaluated on VGG-16 models [31]. Additional experiments with alternative architectures such as Face.evoLVe [32] and ResNet-152 [33] are presented in Appendix E.4. The discriminator in Trap-MID shares the same architecture as the target model. ", "page_idx": 5}, {"type": "text", "text": "Attack Methods. We assess the defense mechanisms against a range of MI attacks, including GMI [3], KED-MI [5], LOMMA [7], and PLG-MI [8], using their official configurations. To evaluate TrapMID in different scenarios, Appendix E.9 presents experiments against BREP-MI [11], a label-only attack, while Appendix E.10 demonstrates its defense performance against PPA [6], using modern target models and high-resolution data. ", "page_idx": 5}, {"type": "text", "text": "Baseline Defenses. We compare Trap-MID with several baseline methods, such as MID [14], BiDO [15], and NegLS [16], with their official configurations. Note that we exclude misleadingbased approaches [17\u201319] from the comparison due to the current unavailability of source code and checkpoints, and their focus on protecting information about certain classes rather than all of them. ", "page_idx": 5}, {"type": "text", "text": "Evaluation Metrics. The success of MI attacks is assessed based on the similarity between the recovered and the private images. Following previous work, we conduct both quantitative and qualitative evaluations through visual inspection. The quantitative metrics are as follows: ", "page_idx": 5}, {"type": "text", "text": "\u2022 Attack Accuracy (AA). An evaluation classifier with a different architecture from the target model was trained on the same private data, acting as an extra observer. We then compute the top-1 and top-5 accuracy on the evaluation model. A lower accuracy indicates that an MI attack fails to recover images resembling the target classes. ", "page_idx": 5}, {"type": "table", "img_path": "GNhrGRCerd/tmp/f8a0be74eeb373cff3e5e99b443dc274f84bc4eb6189221c84bccecb0845961d.jpg", "table_caption": ["Table 1: Defense comparison against various MI attacks, using VGG-16 models. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "\u2022 K-Nearest Neighbor Distance (KNN Dist). We assess the similarity between recovered and private data in the feature space of the evaluation model\u2019s penultimate outputs. Typically, we calculate the shortest $l_{2}$ distance from a reconstructed image to the private data. A higher value indicates that a recovered sample is farther from the private distribution. ", "page_idx": 6}, {"type": "text", "text": "\u2022 Fr\u00e9chet Inception Distance (FID). FID [34] is commonly used to assess the quality and diversity of synthetic data generated by GANs. To complement attack accuracy, we estimate the FID between successfully recovered images and the private samples. A higher value suggests that less detailed information is extracted. ", "page_idx": 6}, {"type": "text", "text": "To analyze the reproducibility of each defense method, we train the target model 5 times with the same configurations but different random seeds, and conduct MI attacks to recover 5 images per class. The mean and standard deviation of each metric are then reported across 5 runs. ", "page_idx": 6}, {"type": "text", "text": "4.2 Experimental Results ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Comparison with Baselines. Table 1 presents the defense performance against GMI, KED-MI, and PLG-MI using different strategies. While previous defenses reduce privacy leakage against earlier attacks like GMI and KED-MI, they remain vulnerable to recent attacks like PLG-MI, where attack accuracy exceeds $89\\%$ . Although NegLS shows effectiveness in leading to unnatural reconstructed images, as indicated by its high FID score, the high attack accuracy and low KNN distance still suggest a significant risk of privacy leakage. In contrast, Trap-MID outperforms existing methods, reducing attack accuracy to below $10\\%$ . Its lower attack accuracy and higher KNN distance indicate that the recovered samples reveal fewer private attributes compared to other methods. Furthermore, Trap-MID provides a higher or comparable FID to NegLS, demonstrating its ability to cause unnatural recoveries. Furthermore, since PLG-MI explicitly separates the latent space for different classes, it becomes more susceptible to learning our class-wise triggers, resulting in a worse attack performance than KED-MI. Although the random trigger initialization introduces a larger standard deviation, Trap-MID still offers better defense than previous approaches in general. Appendix E.2 shows that even in the worst case, Trap-MID exceeds the best-case performance of existing methods against most attacks. ", "page_idx": 6}, {"type": "text", "text": "In addition, previous works have shown that since student models do not observe the teacher\u2019s behavior on triggered samples during KD, this process can serve as a countermeasure against backdoor attacks [35, 36]. Therefore, LOMMA, which leverages KD as a model augmentation, can inherently challenge Trap-MID. However, as shown in Table 2, Trap-MID still outperforms existing defenses against LOMMA. Moreover, Appendix E.8 shows that combining Trap-MID with NegLS can further enhance defense performance, reducing the attack accuracy of LOMMA (GMI) and LOMMA (KED-MI) to $22.80\\%$ and $42.47\\%$ , respectively. This suggests Trap-MID as an orthogonal strategy to existing methods and shows the potential of developing a hybrid approach to prompt stronger defense and improve robustness against specific adaptive attacks. ", "page_idx": 6}, {"type": "table", "img_path": "GNhrGRCerd/tmp/93f25a8d46e29cae55181657a832b4487f22db6385655f86dacd6db252866008.jpg", "table_caption": ["Table 2: Defense comparison against LOMMA [7], using VGG-16 models. "], "table_footnote": [], "page_idx": 7}, {"type": "image", "img_path": "GNhrGRCerd/tmp/d1d42bd0b77ea2de68bf956ad2c979508624a90788e19aafed6fd9f2cfdf3fec.jpg", "img_caption": ["Figure 2: Reconstructed images from PLG-MI. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "Figure 2 depicts the reconstructed images from PLG-MI. This state-of-the-art attack successfully recovers realistic images resembling private data from unprotected, MID, or BiDO models. Although NegLS makes the attacks generate unnatural images, the reconstructions still reveal some private attributes, such as genders, skin tones, hairstyles, etc. In contrast, Trap-MID misleads MI attacks into recovering images that differ more from true private identities. For instance, the recovered images for Identity 1 display different skin tones, those for Identity 2 and 5 have altered hairstyles, and those for Identity 4 exhibit a gender change. Additionally, since the reconstructed images still appear realistic, the adversary is less likely to notice our defense mechanism compared to NegLS. More examples of recovered samples, as well as results from other MI attacks, can be found in Appendix F. ", "page_idx": 7}, {"type": "text", "text": "Synthetic Distribution Analysis According to the hypothesis illustrated in Figure 1a and Theorem 1, if we can create a triggered distribution close enough to the auxiliary distribution, the attacker\u2019s generator would be trapped in this shortcut and fail to explore private information, leading to a synthetic distribution more similar to the public dataset. ", "page_idx": 7}, {"type": "text", "text": "To analyze the tendency of synthetic data, we generate 30,000 images from the PLG-MI\u2019s generator with random latents. Subsequently, we estimate whether the nearest neighbor of each generated sample belongs to the public or private dataset, measured by the $l_{2}$ distance between the evaluation model\u2019s penultimate outputs. Additionally, we include GMI\u2019s generator as an ideal baseline, which was trained only on public data and independently from the target model. ", "page_idx": 7}, {"type": "table", "img_path": "GNhrGRCerd/tmp/8a1028977c4c468a40bc3debe9a2ec1804c8c84d9359bc67f780a44f05b9a41a.jpg", "table_caption": ["Table 3: Synthetic distribution analysis. "], "table_footnote": ["\\* The generator is trained independently from target model. "], "page_idx": 8}, {"type": "image", "img_path": "GNhrGRCerd/tmp/8d3c6b73dc2808a921d5171c86405191ed22ccfc5683781122c8b419f9374b33.jpg", "img_caption": ["Figure 3: Illustration of trapdoor detection. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "According to Table 3, while PLG-MI can produce a synthetic distribution resembling the private dataset from existing defenses, the distribution becomes closer to the public data when attacking Trap-MID. Furthermore, the similar tendency to GMI\u2019s generator indicates that the attacks fail to extract meaningful information from the protected models. ", "page_idx": 8}, {"type": "text", "text": "Trapdoor Recovery Analysis. To verify the effectiveness of Trap-MID in misleading MI attacks, we assess the presence of trapdoor triggers in the recovered images using a detection method in [20]. We first compute the \"trapdoor signatures\" by averaging the penultimate outputs of poisoned images for each target class: ", "page_idx": 8}, {"type": "equation", "text": "$$\nS_{y}=\\mathbb{E}_{x\\sim p(X)}[g_{\\theta}(\\Pi_{y}(X))],\n$$", "text_format": "latex", "page_idx": 8}, {"type": "text", "text": "where $g_{\\theta}\\;:\\;\\mathbb{R}^{d_{x}}\\;\\rightarrow\\;\\mathbb{R}^{d_{z}}$ represents the feature extractor of the target model $f_{\\theta}$ . After that, we calculate the cosine similarities between benign images and the corresponding trapdoor signature $\\cos(g_{\\theta}(X),S_{\\hat{y}})$ , where $\\hat{y}$ is the predicted label of the input. The threshold is then set to be the $k^{t h}$ percentile with the desired false positive rate (FPR) $1-{\\frac{k}{100}}$ , and the input data with similarity exceeding the threshold are considered triggered. In this paper, we decide the threshold to achieve a desired FPR of $5\\%$ . In addition, since computing each signature with the entire training dataset is computationally expensive, we randomly sample the target label for each training data to estimate the signatures. ", "page_idx": 8}, {"type": "text", "text": "As shown in Figure 3, $78.68\\%$ of reconstructed images from PLG-MI are reported to be triggered images, indicating that the MI attacks are misled into extracting trapdoor information. Appendix E.6 presents the analysis of other MI attacks. ", "page_idx": 8}, {"type": "text", "text": "Adversarial Detection. As Shan et al. [20] showed that integrating trapdoors into the model can help detect adversarial attacks, we also assess the effectiveness of Trap-MID in detecting adversarial examples. We utilize AutoAttack [37] and apply the same detection method in the trapdoor recovery analysis. As depicted in Figure 3, Trap-MID achieves a detection success rate (DSR) of over $78\\%$ on both $l_{\\infty}$ $\\epsilon=8/255)$ and $l_{2}$ ( $\\epsilon=0.5)$ ) attacks, while maintaining privacy preservation. ", "page_idx": 8}, {"type": "table", "img_path": "GNhrGRCerd/tmp/d296000637d8b54c980895abbb4119ed75099d8da74457bdc165b6be8a9a4616.jpg", "table_caption": ["Table 4: Adaptive attacks against Trap-MID, using VGG-16 models. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "Adaptive Attacks. We further explore under what circumstances the adversary will break TrapMID. Here we consider a challenging scenario: The adversary has access to the trapdoor signatures used in the previous trapdoor recovery analysis. We modify PLG-MI to conduct adaptive attacks, denoted by $\\mathrm{PLG-MI++}$ . Appendix E.7 demonstrates the scenario where the adversary only knows the existence of trapdoors without information about trapdoor signatures. ", "page_idx": 9}, {"type": "text", "text": "In adaptive attacks, the adversary may encourage generated images to deviate from trapdoor signatures and resemble benign public distribution by modifying the generator objective: ", "page_idx": 9}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathcal{L}_{G++}=\\mathcal{L}_{G}-\\lambda_{\\mathrm{aux}}\\mathbb{E}_{Y\\sim p_{\\mathrm{aux}}(Y)}\\left[\\mathbb{E}_{Z\\sim p_{G}(Z)}[\\cos(g_{\\theta}(T_{\\mathrm{atack}}(G(Z,Y))),S_{\\mathrm{aux},Y})]\\right]}\\\\ &{\\qquad\\qquad+\\;\\lambda_{\\mathrm{trap}}\\mathbb{E}_{Y\\sim p_{\\mathrm{aux}}(Y)}\\left[\\mathbb{E}_{Z\\sim p_{G}(Z)}[\\cos(g_{\\theta}(T_{\\mathrm{atack}}(G(Z,Y))),S_{\\hat{y}})]\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "where $p_{\\mathrm{aux}}(Y)$ denotes the auxiliary distribution of pseudo-labels assigned by PLG-MI\u2019s selection strategy, $p_{G}(Z)$ is the generator\u2019s latent distribution, ${\\mathcal{L}}_{G}$ is the original generator loss, $G:\\mathbb{R}^{d_{z}}\\rightarrow$ $\\mathbb{R}^{d_{x}}$ is the generator, $\\begin{array}{r}{T_{\\mathrm{attack}}:\\mathbb{R}^{d_{x}}\\rightarrow\\mathbb{R}^{d_{x}}}\\end{array}$ is the random image augmentation used in attacks, $\\hat{y}$ is the predicted label of the generated image $G(Z,Y)$ , and $\\lambda_{\\mathrm{aux}},\\,\\lambda_{\\mathrm{trap}}$ are the weighting parameters. We set $\\lambda_{\\mathrm{aux}}=\\lambda_{\\mathrm{trap}}=10$ , as we found it generally provides a better attack performance. $S_{\\mathrm{aux},y}$ is the auxiliary signature of the target class $y$ , computed from public samples: ", "page_idx": 9}, {"type": "equation", "text": "$$\nS_{\\mathrm{aux},y}=\\mathbb{E}_{x\\sim p_{\\mathrm{aux}}(X\\mid y)}[g_{\\theta}(X)],\n$$", "text_format": "latex", "page_idx": 9}, {"type": "text", "text": "In the latent searching stage, two signature-based losses are also added to the inversion loss. ", "page_idx": 9}, {"type": "text", "text": "In practical scenarios, the auxiliary dataset may not originate from the same source as the private dataset, leading to distributional shifts that make it more difficult for the adversary to recover images accurately. To demonstrate this case, we also include the experiments with FFHQ [38] as the auxiliary dataset. Appendix E.5 presents more experiments about distributional shifts, where PLG-MI still achieves $89\\%$ attack accuracy on the unprotected model. ", "page_idx": 9}, {"type": "text", "text": "While Table 4 shows stronger attack results from this adaptation, Trap-MID remains superior to all baseline methods. Additionally, when distributional shifts occur in the auxiliary dataset, the attack performance against Trap-MID degrades significantly. This suggests that the success of MI attacks against Trap-MID, even with adaptive modifications, relies heavily on the similarity between the auxiliary and private data. Intuitively, distributional shifts make it more challenging to extract private data, thereby making trapdoors more attractive as targets and enhancing the defense\u2019s effectiveness. ", "page_idx": 9}, {"type": "text", "text": "5 Conclusion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "MI attacks pose significant privacy risks to DNNs\u2019 training datasets. Despite existing defense efforts, recent attacks continue to exploit vulnerabilities in these defenses. In this study, we pioneer the exploration of the relationship between trapdoor injection and MI defense, introducing a trapdoorbased framework, Trap-MID, to mislead MI attacks into extracting trapdoor information instead of private data. Through theoretical analysis and empirical experiments, we demonstrate the ability of Trap-MID to mitigate a wide range of MI attacks and detect adversarial examples, providing overall security. Notably, Trap-MID achieves these results without the need for shadow attacks or extra datasets, making it both computationally and data-efficient. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work was supported in part by the National Science and Technology Council under Grants NSTC 112-2634-F-002-006, MOST 110-2222-E-002-014-MY3, NSTC 113-2222-E-002-004-MY3, NSTC 113-2923-E-002-010-MY2, NSTC 113-2634-F-002-001-MBK, and by the Center of Data Intelligence: Technologies, Applications, and Systems, National Taiwan University under Grant NTU-113L900903. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Matthew Fredrikson, Eric Lantz, Somesh Jha, Simon Lin, David Page, and Thomas Ristenpart. Privacy in pharmacogenetics: An end-to-end case study of personalized warfarin dosing. In 23rd USENIX security symposium (USENIX Security 14), pages 17\u201332, 2014.   \n[2] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confidence information and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC conference on computer and communications security, pages 1322\u20131333, 2015. [3] Yuheng Zhang, Ruoxi Jia, Hengzhi Pei, Wenxiao Wang, Bo Li, and Dawn Song. The secret revealer: Generative model-inversion attacks against deep neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 253\u2013261, 2020.   \n[4] Kuan-Chieh Wang, Yan Fu, Ke Li, Ashish Khisti, Richard Zemel, and Alireza Makhzani. Variational model inversion attacks. Advances in Neural Information Processing Systems, 34: 9706\u20139719, 2021.   \n[5] Si Chen, Mostafa Kahla, Ruoxi Jia, and Guo-Jun Qi. Knowledge-enriched distributional model inversion attacks. In Proceedings of the IEEE/CVF international conference on computer vision, pages 16178\u201316187, 2021.   \n[6] Lukas Struppek, Dominik Hintersdorf, Antonio De Almeida Correira, Antonia Adler, and Kristian Kersting. Plug & play attacks: Towards robust and flexible model inversion attacks. In International Conference on Machine Learning, pages 20522\u201320545. PMLR, 2022. [7] Ngoc-Bao Nguyen, Keshigeyan Chandrasegaran, Milad Abdollahzadeh, and Ngai-Man Cheung. Re-thinking model inversion attacks against deep neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16384\u201316393, 2023.   \n[8] Xiaojian Yuan, Kejiang Chen, Jie Zhang, Weiming Zhang, Nenghai Yu, and Yang Zhang. Pseudo label-guided model inversion attack via conditional generative adversarial network. AAAI 2023, 2023. [9] Shengwei An, Guanhong Tao, Qiuling Xu, Yingqi Liu, Guangyu Shen, Yuan Yao, Jingwei Xu, and Xiangyu Zhang. Mirror: Model inversion for deep learning network with high fidelity. In Proceedings of the 29th Network and Distributed System Security Symposium, 2022.   \n[10] Gyojin Han, Jaehyun Choi, Haeil Lee, and Junmo Kim. Reinforcement learning-based blackbox model inversion attacks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20504\u201320513, 2023.   \n[11] Mostafa Kahla, Si Chen, Hoang Anh Just, and Ruoxi Jia. Label-only model inversion attacks via boundary repulsion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15045\u201315053, 2022.   \n[12] Ziqi Yang, Jiyi Zhang, Ee-Chien Chang, and Zhenkai Liang. Neural network inversion in adversarial setting via background knowledge alignment. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security, pages 225\u2013240, 2019.   \n[13] Rongke Liu, Dong Wang, Yizhi Ren, Zhen Wang, Kaitian Guo, Qianqian Qin, and Xiaolei Liu. Unstoppable attack: Label-only model inversion via conditional diffusion model. IEEE Transactions on Information Forensics and Security, 2024.   \n[14] Tianhao Wang, Yuheng Zhang, and Ruoxi Jia. Improving robustness to model inversion attacks via mutual information regularization. Proceedings of the AAAI Conference on Artificial Intelligence, 35(13):11666\u201311673, 2021.   \n[15] Xiong Peng, Feng Liu, Jingfeng Zhang, Long Lan, Junjie Ye, Tongliang Liu, and Bo Han. Bilateral dependency optimization: Defending against model-inversion attacks. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 1358\u20131367, 2022.   \n[16] Lukas Struppek, Dominik Hintersdorf, and Kristian Kersting. Be careful what you smooth for: Label smoothing can be a privacy shield but also a catalyst for model inversion attacks. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id $\\cdot$ 1SbkubNdbW.   \n[17] Xueluan Gong, Ziyao Wang, Yanjiao Chen, Qian Wang, Cong Wang, and Chao Shen. Netguard: Protecting commercial web apis from model inversion attacks using gan-generated fake samples. In Proceedings of the ACM Web Conference 2023, pages 2045\u20132053, 2023.   \n[18] Xueluan Gong, Ziyao Wang, Shuaike Li, Yanjiao Chen, and Qian Wang. A gan-based defense framework against model inversion attacks. IEEE Transactions on Information Forensics and Security, 2023.   \n[19] Si Chen, Nikhil Abhyankar, Feiyang Kang, Ming Jin, and Ruoxi Jia. Data-centric defense: Shaping loss landscape with augmentations to counter model inversion. In DMLR Workshop at the 40th International Conference on Machine Learning, 2023.   \n[20] Shawn Shan, Emily Wenger, Bolun Wang, Bo Li, Haitao Zheng, and Ben Y Zhao. Gotta catch\u2019em all: Using honeypots to catch adversarial attacks on neural networks. In Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security, pages 67\u201383, 2020.   \n[21] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8110\u20138119, 2020.   \n[22] Sy-Tuyen Ho, Koh Jun Hao, Keshigeyan Chandrasegaran, Ngoc-Bao Nguyen, and Ngai-Man Cheung. Model inversion robustness: Can transfer learning help? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12183\u201312193, 2024.   \n[23] Jun Hao Koh, Sy-Tuyen Ho, Ngoc-Bao Nguyen, and Ngai-man Cheung. On the vulnerability of skip connections to model inversion attacks. In European Conference on Computer Vision. Springer, 2024.   \n[24] Ziqi Yang, Bin Shao, Bohan Xuan, Ee-Chien Chang, and Fan Zhang. Defending model inversion and membership inference attacks via prediction purification. arXiv preprint arXiv:2005.03915, 2020.   \n[25] Jing Wen, Siu-Ming Yiu, and Lucas CK Hui. Defending against model inversion attack by adversarial examples. In 2021 IEEE International Conference on Cyber Security and Resilience (CSR), pages 551\u2013556. IEEE, 2021.   \n[26] Yiming Li, Yong Jiang, Zhifeng Li, and Shu-Tao Xia. Backdoor learning: A survey. IEEE Transactions on Neural Networks and Learning Systems, 2022.   \n[27] Yiming Li, Tongqing Zhai, Baoyuan Wu, Yong Jiang, Zhifeng Li, and Shutao Xia. Rethinking the trigger of backdoor attack. arXiv preprint arXiv:2004.04692, 2020.   \n[28] Han Qiu, Yi Zeng, Shangwei Guo, Tianwei Zhang, Meikang Qiu, and Bhavani Thuraisingham. Deepsweep: An evaluation framework for mitigating dnn backdoor attacks using data augmentation. In Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security, pages 363\u2013377, 2021.   \n[29] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017.   \n[30] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of the IEEE international conference on computer vision, pages 3730\u20133738, 2015.   \n[31] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.   \n[32] Yu Cheng, Jian Zhao, Zhecan Wang, Yan Xu, Karlekar Jayashree, Shengmei Shen, and Jiashi Feng. Know you at one glance: A compact vector representation for low-shot learning. In Proceedings of the IEEE international conference on computer vision workshops, pages 1924\u2013 1932, 2017.   \n[33] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[34] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.   \n[35] Kota Yoshida and Takeshi Fujino. Countermeasure against backdoor attack on neural networks utilizing knowledge distillation. Journal of Signal Processing, 24(4):141\u2013144, 2020.   \n[36] Kota Yoshida and Takeshi Fujino. Disabling backdoor and identifying poison data by using knowledge distillation in backdoor attacks on deep neural networks. In Proceedings of the 13th ACM workshop on artificial intelligence and security, pages 117\u2013127, 2020.   \n[37] Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In International conference on machine learning, pages 2206\u20132216. PMLR, 2020.   \n[38] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4401\u20134410, 2019.   \n[39] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4690\u20134699, 2019.   \n[40] Brianna Maze, Jocelyn Adams, James A Duncan, Nathan Kalka, Tim Miller, Charles Otto, Anil K Jain, W Tyler Niggel, Janet Anderson, Jordan Cheney, et al. Iarpa janus benchmark-c: Face dataset and protocol. In 2018 international conference on biometrics (ICB), pages 158\u2013165. IEEE, 2018.   \n[41] Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and Jianfeng Gao. Ms-celeb-1m: A dataset and benchmark for large-scale face recognition. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III 14, pages 87\u2013102. Springer, 2016.   \n[42] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2818\u20132826, 2016.   \n[43] Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 815\u2013823, 2015.   \n[44] Tuomas Kynk\u00e4\u00e4nniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. Advances in neural information processing systems, 32, 2019.   \n[45] Muhammad Ferjad Naeem, Seong Joon Oh, Youngjung Uh, Yunjey Choi, and Jaejun Yoo. Reliable fidelity and diversity metrics for generative models. In International Conference on Machine Learning, pages 7176\u20137185. PMLR, 2020.   \n[46] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Haibin Lin, Zhi Zhang, Yue Sun, Tong He, Jonas Mueller, R Manmatha, et al. Resnest: Split-attention networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2736\u20132746, 2022.   \n[47] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700\u20134708, 2017. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "A Broader Impacts, Limitations, and Future Works ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.1 Broader Impacts ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Deep learning has been widely employed in diverse domains and tasks. However, the growing threat of privacy breaches, such as MI attacks, poses significant risks to sensitive data used for model training. Our proposed framework, Trap-MID, offers a promising defense strategy against MI attacks by misleading their exploration directions. Empirical experiments demonstrate its state-of-the-art defense performance. Importantly, Trap-MID achieves these results without the need for additional public datasets or conducting shadow attacks, making it applicable across diverse applications. ", "page_idx": 14}, {"type": "text", "text": "A.2 Limitations and Future Works ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "A.2.1 Experimental Limitations ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In our empirical experiments, we did not exhaust hyper-parameter tuning via grid search due to the computational constraints. While we found that the configurations in Section 4.1 generally provide a good accuracy-privacy trade-off against various MI attacks, conducting more comprehensive hyperparameter optimization could further improve utility and defense performance. Ablation studies exploring the impact of different hyper-parameter settings are discussed in Appendix D. Although we did not include the experiments against all MI attacks due to computational requirements, we verified the efficacy of Trap-MID against various white-box attacks [3, 5\u20138] and a label-only attack [11], in both low-resolution [3, 5, 7, 8, 11] and high-resolution scenarios [6]. These results suggest that Trap-MID is effective across a broad range of MI attacks. ", "page_idx": 14}, {"type": "text", "text": "Additionally, we recognize that the attack accuracy metric, based solely on an evaluation classifier trained on the private dataset, may fail to detect out-of-distribution samples, resulting in high KNN distance or FID alongside high attack accuracy. While feature-based metrics like KNN distance and FID can help identify these failures, developing a universal evaluation method could better quantify both attack and defense performance, offering a more straightforward basis for comparison. ", "page_idx": 14}, {"type": "text", "text": "A.2.2 Further Improvements in Trap-MID Design ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "In this paper, we demonstrate the efficacy of our trapdoor-based framework with a simple trigger design. While we conducted the model training and attacks multiple times to ensure reproducibility, we acknowledge a larger variation in our defense performance than previous defenses due to the randomly initialized triggers. We leave further customization for a more stable and powerful trigger for future work. ", "page_idx": 14}, {"type": "text", "text": "In addition, while Trap-MID is more computationally efficient than existing misleading-based defenses, it requires longer training time compared to methods like MID, BiDO, and NegLS due to its three gradient updates per epoch. Developing a more efficient trigger generation process would be a valuable future direction to make Trap-MID more practical for large-scale applications. ", "page_idx": 14}, {"type": "text", "text": "A.2.3 Exploring Different Scenarios ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "Despite its effectiveness, Trap-MID inherits certain assumptions and limitations from previous works. For instance, it assumes a level of trust between data providers and the model owner to protect their private information [14\u201316]. A promising future direction involves developing trapdoor injection methods that empower dataset owners or even individual identities to secure sensitive information before sharing data. Additionally, Trap-MID\u2019s efficacy may be limited against MI attacks involving KD, such as LOMMA, due to the inherent weaknesses of backdoor attacks. Therefore, future efforts should focus on integrating more robust trapdoors to address these limitations. ", "page_idx": 14}, {"type": "text", "text": "In addition, similar to previous works, we used relatively simple victim models for facial recognition, trained on the CelebA dataset with cross-entropy loss. Given the advancements in facial recognition, it would be valuable to evaluate the performance of MI attacks and defenses on more advanced techniques, such as ArcFace [39], or on datasets featuring more diverse poses and facial images in the wild, like IJB-C dataset [40]. ", "page_idx": 14}, {"type": "text", "text": "Finally, our research represents the first attempt to establish the connection between trapdoor injections and MI defense mechanisms. Extending this defense to different modalities, such as language, graph, and tabular, or exploring its relationship with other reconstruction-based attacks, such as Gradient Inversion Attacks and Embedding Inversion Attacks, presents an interesting direction for further research. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "B Proof of Theorem 1 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Proof. According to Definition 1, for a given model $f$ , target label $y$ and corresponding trigger injection function $\\Pi_{y}(\\cdot)$ , we have: ", "page_idx": 15}, {"type": "text", "text": "$\\begin{array}{r l}&{T_{f}(y,\\Pi_{y})-U_{f}(y)}\\\\ &{=\\mathbb{E}_{X\\sim p(X)}[\\log p_{f}(y|\\Pi_{y}(X))-\\log p_{f}(y)]-\\mathbb{E}_{X\\sim p(X|y)}[\\log p_{f}(y|X)-\\log p_{f}(y)]}\\\\ &{=\\mathbb{E}_{X\\sim p(X)}[\\log p_{f}(y|\\Pi_{y}(X))]-\\mathbb{E}_{X\\sim p(X|y)}[\\log p_{f}(y|X)]}\\end{array}$ \u2265\u03b4. ", "page_idx": 15}, {"type": "text", "text": "Expanding the KL divergence $D_{K L}(p(X)||p(\\Pi(X)))$ in Definition 2: ", "page_idx": 15}, {"type": "equation", "text": "$$\nD_{K L}(p(X)||p(\\Pi(X)))=\\mathbb{E}_{X\\sim p(X)}[\\log p(X)-\\log p(\\Pi(X))]\\leq\\epsilon.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "As a result, if for all $y$ , the trapdoor is $(\\delta,y)$ -effective and $\\epsilon$ -natural on the model $f$ with injection function $\\Pi_{y}(\\cdot)$ , we have: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{L}_{Y\\sim p(Y)}\\mathbb{E}_{X\\sim p(X)}[\\log p_{f}(\\Pi_{y}(X)|Y)]}\\\\ &{=\\mathbb{E}_{Y\\sim p(Y)}\\mathbb{E}_{X\\sim p(X)}[\\log p_{f}(Y|\\Pi_{y}(X))+\\log p(\\Pi_{y}(X))-\\log p_{f}(Y)]}\\\\ &{\\geq\\mathbb{E}_{Y\\sim p(Y)}\\Big[\\mathbb{E}_{X\\sim p(X|Y)}[\\log p_{f}(Y|X)]+\\delta\\Big]+\\Big(\\mathbb{E}_{X\\sim p(X)}[\\log p(X)]-\\epsilon\\Big)-\\mathbb{E}_{Y\\sim p(Y)}[\\log p_{f}(Y)]}\\\\ &{=\\mathbb{E}_{(X,Y)\\sim p(X,Y)}[\\log p_{f}(Y|X)+\\log p(X)-\\log p_{f}(Y)]+(\\delta-\\epsilon)}\\\\ &{=\\mathbb{E}_{(X,Y)\\sim p(X,Y)}[\\log p_{f}(X|Y)]+(\\delta-\\epsilon).}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "C Experimental Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "C.1 Hardware and Software Details ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "All experiments were conducted on an Intel Xeon Gold 6226R CPU with an NVIDIA RTX A6000 GPU. The average execution time of Trap-MID training is 1 hour 15 minutes with a standard deviation of 13 seconds. Our source code is publicly available at https://github.com/ntuaislab/ Trap-MID to reproduce main experiments. ", "page_idx": 15}, {"type": "text", "text": "C.2 Datasets ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "The datasets we used in our experiments are all publicly accessible, including: ", "page_idx": 15}, {"type": "text", "text": "CelebA. CelebA [30] comprises 202,599 facial images of 10,177 identities with coarse alignment. Following prior studies, we selected the 1,000 identities with the most samples as the private dataset, totaling 30,029 facial images. The private dataset was then divided into training and testing datasets for utility evaluation, containing 27,018 and 3,009 samples, respectively. For the auxiliary dataset used in MI attacks, a disjoint subset of the CelebA dataset was sampled, which contains 30,000 images without overlapping identities with the private dataset. Images are cropped at the center and resized to $64\\times64$ pixels. ", "page_idx": 15}, {"type": "text", "text": "FFHQ. FFHQ [38] contains 70,000 high-quality facial images with considerable variation in age, ethnicity, and background. The entire FFHQ dataset was utilized as the adversary\u2019s auxiliary dataset in the MI attacks with distributional shifts. ", "page_idx": 15}, {"type": "text", "text": "Input: Training dataset $\\{(x_{i},y_{i})\\}_{i=1}^{N}$ , a classifier $f_{\\theta}$ parameterized by $\\theta$ , a discriminator $D_{\\phi}$ parameterized $\\phi$ , trapdoor triggers $K=\\{k_{i}\\}_{i=1}^{d_{y}}$ corresponding with different classes, mini-batch size $m$ , number of mini-batches $M$ , learning rate $\\alpha$ , trigger step size $\\epsilon$ , and training epoch $T$ ", "page_idx": 16}, {"type": "text", "text": "Output: A trained model with privacy protection for $t\\gets1,\\ldots,T$ do for $j\\leftarrow1,\\ldots,M$ do Sample a mini-batch $\\{(x_{i},y_{i})\\}_{i=1}^{m}$ from training dataset. Sample a set of target labels $\\{y_{i}^{\\prime}\\}_{i=1}^{m}$ with the same size of the mini-batch. $\\phi\\leftarrow\\phi-\\alpha\\nabla\\mathcal{L}_{D}$ . $K\\leftarrow K-\\epsilon\\,\\mathrm{sgn}(\\nabla\\mathcal{L}_{\\mathrm{trigger}})$ . Clip the triggers $K$ into the range $[0,1]$ . $\\theta\\gets\\theta-\\alpha\\nabla\\mathcal{L}_{\\theta}$ . end for ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "\u25b7Equation 4   \n\u25b7Equation 5 ", "page_idx": 16}, {"type": "text", "text": "\u25b7Equation 2 ", "page_idx": 16}, {"type": "text", "text": "C.3 Target Models ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Consistent with prior research, we trained the VGG-16 and Face.evoLVe models for 50 epochs, and the ResNet-152 models for 40 epochs. All models were trained using the SGD optimizer with a batch size of 64, a learning rate of 0.01, a momentum value of 0.9, and a weight decay value of 0.0001. ", "page_idx": 16}, {"type": "text", "text": "C.4 Trap-MID ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "For Trap-MID, we adopted the same hyper-parameters as the unprotected model. Algorithm 1 outlines the training process. The protected model was configured to achieve a trapdoor success rate exceeding $99\\%$ . Typically, we used a blend ratio $\\alpha=0.02$ and a trapdoor loss weight $\\beta\\,=\\,0.2$ . Trapdoor triggers were randomly initialized using a uniform distribution within [0, 1] and then updated with a step size $\\epsilon=0.01$ . The discriminator was optimized with identical settings to the target classifier. ", "page_idx": 16}, {"type": "text", "text": "The random augmentation pipeline consisted of a sequence of image transformations, including random resized crops with cropping scales sampled from [0.8, 1], horizontal flips, and random rotations with degrees sampled from $[-30,30]$ . Each augmentation was applied randomly with a probability of 0.5. Note that our augmentation strategy differs from that in MI attacks to prevent the requirements of attack information. However, since these augmentations are widely adopted across various domains, our approach still overlaps with those utilized in MI attacks. ", "page_idx": 16}, {"type": "text", "text": "C.5 Attack Methods ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We conducted MI attacks using the official implementation of GMI, KED-MI, LOMMA, and PLGMI. Specifically, we utilized the PLG-MI official code available from https://github.com/ LetheSec/PLG-MI-Attack for GMI, KED-MI, and PLG-MI attacks. For LOMMA, we employed the official code available at https://github.com/sutd-visual-computing-group/ Re-thinking_MI. During the latent searching stage, we optimized the latent for 1,500 epochs in GMI, KED-MI, and LOMMA, and 600 epochs in PLG-MI. ", "page_idx": 16}, {"type": "text", "text": "C.6 Baseline Defenses ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "MID. The protected model was trained with the MID official code provided at https://github. com/Jiachen-T-Wang/mi-defense. For the Face.evoLVe and ResNet-152 models, we added the information bottleneck before the final fully connected layer, with a bottleneck size of 512. The mutual information was approximated using the same variational method as the official implementation. The weight coefficient of the regularization term $\\lambda$ was set to 0.003. ", "page_idx": 16}, {"type": "text", "text": "BiDO. We utilized the BiDO official code at https://github.com/AlanPeng0897/Defend_MI to train the protected models. The models were trained with Adam optimizer, using a learning rate of 0.0001 without weight decay. For Face.evoLVe and ResNet-152 models, we used the latent representations from the four major ResNet blocks to estimate the bilateral dependency. Typically, the Hilbert-Schmidt Independence Criterion (HSIC) was adopted as the dependency measure, as it was reported with a better defense performance than the Constrained Covariance (COCO) in [15]. The balancing hyper-parameters $(\\lambda_{x},\\lambda_{y})$ were set to (0.05, 0.5). ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "NegLS. Since the NegLS code was not available when we conducted our experiments, we adapted our implementation to include their negative label smoothing based on the official configuration. The models were trained for 100 epochs using the Adam optimizer with a batch size of 128 and an initial learning rate of 0.001 without weight decay. The learning rate was then multiplied by a factor of 0.1 at the $75^{t h}$ and $90^{t h}$ epochs. The label smoothing factor $\\alpha$ was set to be 0 at the first 50 epochs. During the 51th to the 75th epoch, \u03b1 was set to be \u22120.05 \u00d71t00\u2212\u22125050, where $t$ is the current epoch. After that, $\\alpha$ was fixed to be $-0.05$ for the remaining training. ", "page_idx": 17}, {"type": "text", "text": "C.7 Evaluation Models ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "An evaluation model with a different architecture from the target model is trained on the same private dataset, acting as an additional observer to assess the success of MI attacks. We use the evaluation model from [8], which is a Face.evoLVe model [32] pre-trained on MS-Celeb1M [41] and fine-tuned on the private training dataset. The evaluation model has an input resolution of $112\\mathrm{x}112$ , with the $64\\mathrm{x}64$ images resized to fit its input size. It achieves a $95.88\\%$ accuracy on the testing dataset. ", "page_idx": 17}, {"type": "text", "text": "C.8 Evaluation Metrics ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Attack accuracy (AA). For each recovered image, we determine whether it is classified as the target class by the evaluation model, yielding both top-1 and top-5 attack accuracy. ", "page_idx": 17}, {"type": "text", "text": "K-Nearest Neighbor Distance (KNN Dist). The KNN distance calculates the shortest $l_{2}$ distance from the reconstructed images to the private data in the feature space of the evaluation model. Specifically, to demonstrate the success of reconstruction, each recovered image is compared only with the private data belonging to its target class. ", "page_idx": 17}, {"type": "text", "text": "Fr\u00e9chet Inception Distance (FID). The FID is evaluated by the difference in means and covariances between the generated and real images in the feature space of Inception-v3 [42]. Consistent with prior studies, we compute FID only on the successfully recovered images identified by the evaluation classifier to measure the quality and diversity of the extracted information. ", "page_idx": 17}, {"type": "text", "text": "D Ablation Studies ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we first assess the effectiveness of TeD\u2019s trapdoor injection strategy [20] against MI attacks, and then analyze the impact of various configurations on defense performance. ", "page_idx": 17}, {"type": "text", "text": "Since conducting the whole experiment multiple times would be computationally expensive, without specification, we use the same evaluation protocol in previous works for the experiments in the appendices. Typically, we train a single target model for each configuration and conduct MI attacks to reconstruct 5 images per class with random initialization in the latent searching stage. The standard deviation of attack accuracy is then computed from 5 reconstruction attempts. For experiments sharing the same setups as those in Section 4.1, we average the evaluation metrics and their standard deviations across multiple runs. ", "page_idx": 17}, {"type": "text", "text": "D.1 TeD\u2019s Trapdoor Injection Strategy ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "To verify whether TeD\u2019s protected models inherently defend against MI attacks, we employ the same trapdoor injection method in [20] and leave other training setups consistent with the unprotected model. Specifically, each trapdoor trigger comprises five $(6\\times6)$ -pixel squares randomly scattered across the image, with a blend ratio $\\alpha=0.1$ or 0.2 and a trapdoor loss weight $\\beta=0.5.$ .2 The intensity of each square was sampled from ${\\mathcal{N}}(\\mu,\\sigma)$ with uniformly sampled $\\mu\\in[0,1]$ and $\\sigma\\in[0,1]$ , and was fixed during model training. Sample poisoned images are shown in Figure 4. ", "page_idx": 17}, {"type": "text", "text": "TeDTrap-MID", "page_idx": 18}, {"type": "text", "text": "Figure 4: Sample poisoned images for TeD\u2019s $(\\alpha=0.1)$ ) and our trapdoor injection methods. Each column depicts a poisoned image with a specific target label. The blend ratio $\\alpha$ is multiplied by a factor of 10 for better visualization $\\alpha=1$ and 0.2 for TeD\u2019s and our triggers, respectively). ", "page_idx": 18}, {"type": "text", "text": "For further comparison, we adopt the same configurations of Trap-MID, such as a smaller blend ratio $(\\alpha=0.02)$ ), a smaller trapdoor loss weight $\\beta=0.2)$ ), trigger optimization and data augmentation, while keeping TeD\u2019s five-square patterns. This enhanced version is denoted as $\\mathrm{TeD+}$ . ", "page_idx": 18}, {"type": "text", "text": "Table 5 compares defense performances using TeD\u2019s trapdoor injection techniques. Although decreasing the blend ratio and adopting our configurations can improve defense performance, TeD models remain vulnerable to PLG-MI with their five-square patterns. In contrast, spreading the trapdoor information across all image pixels is crucial for fooling stronger MI attacks. Intuitively, while TeD\u2019s patch-based triggers fit the $l_{\\infty}$ or $l_{2}$ budgets in adversarial attacks [20], the resulting poisoned images appear less natural, making them more likely to be identified and penalized by the attacker\u2019s discriminator. Moreover, as TeD\u2019s triggers highly rely on local information around specific locations, they may exhibit lower robustness against spatial transformations, leading to inferior defense against PLG-MI. ", "page_idx": 18}, {"type": "table", "img_path": "GNhrGRCerd/tmp/ad74394b2163019906223ff97450b0cc7f84f3f728ac67e23ee27f742c5626c5.jpg", "table_caption": ["Table 5: Defense comparison with TeD\u2019s trapdoors, using VGG-16 models. "], "table_footnote": ["\\* The mean and standard deviation of each evaluation metric are averaged across multiple runs. "], "page_idx": 19}, {"type": "table", "img_path": "GNhrGRCerd/tmp/f29d028a11ef59fd5111f890f1648ff52d3a5b277cb36e012ac622003c3f3fc7.jpg", "table_caption": ["Table 6: Defense comparison with trigger optimization, using VGG-16 models. "], "table_footnote": ["\\* The mean and standard deviation of each evaluation metric are averaged across multiple runs. "], "page_idx": 20}, {"type": "text", "text": "D.2 Trigger Optimization ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Table 6 demonstrates the impact of trigger optimization. Fixed triggers generally preserve privacy against most MI attacks, except for PLG-MI, but result in a noticeable accuracy reduction. In contrast, the influence of trapdoor loss and discriminator loss varies based on the capacity of the GAN used in the attacks. ", "page_idx": 20}, {"type": "text", "text": "Trapdoor loss. Incorporating trapdoor loss can create easily learnable triggers for the target model, reducing accuracy drop. However, these crafted adversarial-like triggers may be more difficult to generate, resulting in lower defense performance against attacks using weaker generators like GMI, KED-MI, and LOMMA. ", "page_idx": 20}, {"type": "text", "text": "Discriminator loss. Discriminator loss promotes invisible triggers, which are crucial for deceiving attacks with stronger discriminators, such as PLG-MI. However, generating invisible triggers requires fine-grain adjustments, making them less effective against attacks using weaker generators. ", "page_idx": 20}, {"type": "table", "img_path": "GNhrGRCerd/tmp/73305d12f1c4922ca3aedd1cd90153efaeea9f19ae24c4362a9e9d60f124612e.jpg", "table_caption": ["Table 7: Defense comparison with different blend ratios, using VGG-16 models. "], "table_footnote": ["\\* The mean and standard deviation of each evaluation metric are averaged across multiple runs. "], "page_idx": 21}, {"type": "text", "text": "D.3 Blend Ratio ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "In this section, we tune the blend ratio $\\alpha$ within [0.02, 0.1] and compare the defense performance. As shown in Table 7, decreasing the blend ratio generally makes the trapdoor triggers more invisible and improves defense performance. Notably, there is a sharp drop in attack accuracy. For instance, PLG-MI\u2019s attack accuracy drops from $93.86\\%$ to $1.92\\%$ when the blend ratio is reduced from 0.05 to 0.03, suggesting that an adequately invisible trigger is crucial for misleading certain attacks. In terms of model utility, Trap-MID is relatively insensitive to the blend ratio, maintaining about $81\\!-\\!84\\%$ testing accuracy across different configurations, demonstrating its effectiveness without a significant accuracy loss. ", "page_idx": 21}, {"type": "table", "img_path": "GNhrGRCerd/tmp/8bda2a5a1a78f64ed9aef35994c969fa15a57e0cd97ce37a519b03f568b5a3dd.jpg", "table_caption": ["Table 8: Defense comparison with different trapdoor loss weight, using VGG-16 models. "], "table_footnote": ["\\* The mean and standard deviation of each evaluation metric are averaged across multiple runs. "], "page_idx": 22}, {"type": "text", "text": "D.4 Loss Weight ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "Table 8 illustrates the impact of the trapdoor loss weight. In general, A larger weight prioritizes learning trapdoor behavior over the main task, enhancing defense at the cost of accuracy and leading to an accuracy-privacy trade-off. Notably, Trap-MID achieves privacy protection without a significant accuracy drop, with testing accuracy decreasing from $86.21\\%$ to $81.37\\%$ when the trapdoor loss weight is increased to 0.2. Moreover, a relatively low weight (e.g., $\\beta=0.02$ ) is sufficient to reduce the performance of most MI attacks. For example, the top-1 attack accuracy drops from $56.46\\%$ to $10.78\\%$ against KED-MI, and from $95.81\\%$ to $23.84\\%$ against PLG-MI. ", "page_idx": 22}, {"type": "image", "img_path": "GNhrGRCerd/tmp/407ed8ccdddc8f892aef79999cbf0df5402ae0ecd82412d48f707c69f4e50b9f.jpg", "img_caption": ["Figure 5: Defense comparison with different augmentation against PLG-MI, using VGG-16 models. "], "img_footnote": [], "page_idx": 23}, {"type": "image", "img_path": "GNhrGRCerd/tmp/3a660c3ab01f1bc3d6966dadd3009c9a2ad5ba2779f0a1a6e02aa22faa8544f2.jpg", "img_caption": ["Figure 6: Defense comparison with different augmentation probabilities against PLG-MI, using VGG-16 models. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "D.5 Augmentation ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "This section investigates the impact of augmentations used in model training. Since we noticed that weak augmentation can lead to unstable defense performance, we employed the same 5-run evaluation protocols as in Section 4.1 in this section. Moreover, the mean and standard deviation of each metric were estimated after removing outliers by the IQR method due to large variation. ", "page_idx": 23}, {"type": "text", "text": "Figure 5 shows the defense performance against PLG-MI with different numbers of augmentations. Typically, we started from defense without augmentation and incrementally added random resized crop, random rotation, and horizontal flip. Although protected models with fewer augmentations can sometimes reduce attack performance significantly, they remain vulnerable to PLG-MI in most runs. Intuitively, augmentation helps identify and address the trapdoor\u2019s weaknesses in terms of transformation robustness. While it is possible to randomly sample and optimize a robust trigger with weak augmentations, stronger augmentations are more effective at detecting these weaknesses comprehensively, resulting in more stable and improved defense performance. ", "page_idx": 23}, {"type": "text", "text": "In addition, to verify whether applying augmentation more frequently can enhance the defense, we increased the probability of applying transformation from $50\\%$ to $87.5\\%$ using only random resized crop. As Figure 6 demonstrates, even with a single augmentation, applying it frequently can reveal more trapdoors\u2019 weaknesses and improve the defense performance of protected models. ", "page_idx": 23}, {"type": "text", "text": "Table 9: Training time comparison, using VGG-16 models. ", "page_idx": 24}, {"type": "table", "img_path": "GNhrGRCerd/tmp/dd4540c6e7df064fadb26c93f7a0d9ab1740c16d93292217a977139809df93c6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 24}, {"type": "table", "img_path": "GNhrGRCerd/tmp/d44b286ce038dff689df020e8cec7a137cf07f5ee6c1c18e1b6c4cbe39327f04.jpg", "table_caption": ["Table 10: The worst-case performance of Trap-MID compared to the best-case performance of existing defenses, using VGG-16 models. "], "table_footnote": [], "page_idx": 24}, {"type": "text", "text": "E Additional Experimental Results ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "E.1 Training Time Comparison ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Table 9 presents the training time for various defense methods. While Trap-MID requires the longest time due to its three gradient updates per epoch for the discriminator, triggers, and target model, it is worth noting that it significantly outperforms other defenses against recent MI attacks. Furthermore, Trap-MID still requires less data and computational resources compared to existing misleading-based defenses, eliminating the need for an additional dataset, training an extra classifier, or executing shadow attacks. ", "page_idx": 24}, {"type": "text", "text": "We believe that enhancing the efficiency of trigger generation would be a valuable future direction, making Trap-MID more practical for large-scale applications. For example, pre-computing triggers with fewer steps may reduce overhead during model training. ", "page_idx": 24}, {"type": "text", "text": "E.2 Worst-case Performance of Trap-MID ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In prior experiments, the randomly initialized triggers in Trap-MID introduced variability in defense performance, resulting in a larger standard deviation. This section highlights the worst-case performance of Trap-MID compared to the best-case performance of existing defenses in terms of top-1 attack accuracy. As shown in Table 10, the worst-case performance of Trap-MID surpasses the best-case performance of existing methods against most attacks, demonstrating its effectiveness. ", "page_idx": 24}, {"type": "text", "text": "E.3 Additional Evaluation Metrics ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "This section further compares the defense performance with additional evaluation metrics used by [4] and [6], including: ", "page_idx": 24}, {"type": "table", "img_path": "GNhrGRCerd/tmp/0122780f23d8acb6d32d1045fe272f3da2deee02e11b0a689531596e82171eb6.jpg", "table_caption": ["Table 11: Defense comparison against PLG-MI, using VGG-16 models and measuring with additional evaluation metrics. "], "table_footnote": [], "page_idx": 25}, {"type": "text", "text": "KNN Distance in the FaceNet [43] Feature Space $\\left(\\delta_{\\mathbf{face}}\\right)$ . This metric estimates the FaceNet feature distance between each recovered image and the nearest private data. A higher value indicates less similarity to the private data. ", "page_idx": 25}, {"type": "text", "text": "Improved Precision [44]. This metric assesses whether each recovered image lies within the manifold of private data in the InceptionV3 feature space. A lower value signifies less similarity to the private data. ", "page_idx": 25}, {"type": "text", "text": "Improved Recall [44]. This evaluates whether each private image is encompassed within the manifold of recovered data in the InceptionV3 feature space. A lower value suggests that the generator is less likely to reproduce private data. ", "page_idx": 25}, {"type": "text", "text": "Density [45]. This metric quantifies how many private-sample neighborhood spheres contain each recovered image in the InceptionV3 feature space. A lower value indicates less similarity to the private data. ", "page_idx": 25}, {"type": "text", "text": "Coverage [45]. This assesses how many private samples have a neighborhood sphere that contains at least one recovered image in the InceptionV3 feature space. A lower value suggests that the generator is less likely to reproduce private data. ", "page_idx": 25}, {"type": "text", "text": "As shown in Table 11, Trap-MID outperforms existing defenses in FaceNet distance and ranks second to NegLS in most other metrics. For the improved recall metrics, since arbitrary images with injected triggers can be classified into corresponding classes, the recovered samples become more diverse, leading to a broader manifold and a higher recall value. Additionally, all metrics, except for FaceNet distance, utilize the same InceptionV3 model as FID. This gives NegLS an advantage in these metrics due to its less natural recovered images. ", "page_idx": 25}, {"type": "text", "text": "E.4 Defense Performance on Different Architectures ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In this section, we present the defense comparison on the Face.evoLVe and ResNet-152 models. As shown in Table 12 and Table 13, Trap-MID consistently outperforms existing defenses on both Face.evoLVe and ResNet-152 models. ", "page_idx": 25}, {"type": "table", "img_path": "GNhrGRCerd/tmp/0e3117622ab82f172cea2e8a22b1f514c86dc9d7a63ca38d150fc2a5a099b0e4.jpg", "table_caption": ["Table 12: Defense comparison on Face.evoLVe models. "], "table_footnote": [], "page_idx": 26}, {"type": "table", "img_path": "GNhrGRCerd/tmp/c4e55f1aacda16b4c0a762554d4e5db52278964154e64bf565f12ee2e1bfca99.jpg", "table_caption": ["Table 13: Defense comparison on ResNet-152 models. "], "table_footnote": [], "page_idx": 27}, {"type": "table", "img_path": "GNhrGRCerd/tmp/0302e08447b0172adbb6d870bd702cba68834a646f15c23b89861b60b6719c72.jpg", "table_caption": ["Table 14: Defense comparison against PLG-MI with FFHQ dataset, using VGG-16 models. "], "table_footnote": ["\\* The mean and standard deviation of each evaluation metric are averaged across multiple runs. "], "page_idx": 28}, {"type": "image", "img_path": "GNhrGRCerd/tmp/9e1e690f37b5b338d2633c8b5d73046ec3f889c56e8cebaed31279c77604780c.jpg", "img_caption": ["Figure 7: Illustration of trapdoor detection against different MI attacks. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "E.5 Distributional Shifts in Adversary\u2019s Auxiliary Dataset ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "In the previous experiments, we assumed that the adversary possesses a public dataset without distributional shifts from the private data. Typically, both auxiliary and private datasets were constructed from the CelebA dataset. However, in a practical scenario, the adversary might not know the distribution of private data, leading to potential distributional shifts between auxiliary and private datasets and making it harder to extract private data. ", "page_idx": 28}, {"type": "text", "text": "In this section, we used the FFHQ dataset [38] as the adversary\u2019s auxiliary dataset to demonstrate the scenario with slight distributional shifts, considering that PLG-MI can still provide an attack accuracy exceeding $89\\%$ on the unprotected model using this dataset [8]. As shown in Table 14, Trap-MID outperforms previous approaches under this scenario, achieving nearly $0\\%$ top-1 attack accuracy. ", "page_idx": 28}, {"type": "text", "text": "Notably, while methods using dependency regularization, such as MID and BiDO, are vulnerable to MI attacks without distributional shifts, they can protect privacy if auxiliary distribution differs from private data, with top-1 attack accuracy dropping from $89\\%$ to below $70\\%$ . On the other hand, although NegLS reduces the guidance signal for MI attacks by training an over-confident model with a discrete loss landscape, the logit-based max-margin loss in PLG-MI can prevent early saturation and surpass this defense mechanism. Consequently, while NegLS leads to unnatural reconstructions with a high FID, it remains vulnerable to PLG-MI, with attack accuracy exceeding $78\\%$ . ", "page_idx": 28}, {"type": "text", "text": "E.6 Trapdoor Recovery Analysis against Different MI Attacks ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Figure 7 presents the trapdoor recovery analysis of MI attacks other than PLG-MI. Similarly to PLG-MI, all these attacks, except LOMMA (KED-MI), reconstruct the trapdoor information from trapdoored models, with more than $89\\%$ recovered images reported as triggered images. Although our detection method has a low recall rate on LOMMA (KED-MI), the ROC AUC of $83.33\\%$ still suggests that the reconstructed images are likely to be injected with a trapdoor trigger. However, due to the limitations of backdoor attacks, the student models from KD enable LOMMA to extract trapdoor information and explore private distribution simultaneously, preventing it from being entirely misled by the trapdoored models. We leave the further adaptation for a robust trapdoor against KD to future works. ", "page_idx": 28}, {"type": "table", "img_path": "GNhrGRCerd/tmp/871fd3630fe999530bd13d0695e3e2eaf2974d65b2e567a2d557804e34743a57.jpg", "table_caption": ["Table 15: Defense comparison against adaptive attacks, using VGG-16 models. "], "table_footnote": ["\\* The mean and standard deviation of each evaluation metric are averaged across multiple runs. "], "page_idx": 29}, {"type": "table", "img_path": "GNhrGRCerd/tmp/8fc78ef82e4d1ed1e17526ff9347dea4c6340d88c988009668d0971f8000dc04.jpg", "table_caption": ["Table 16: Defense comparison against adaptive attacks with FFHQ dataset, using VGG-16 models. "], "table_footnote": ["\\* The mean and standard deviation of each evaluation metric are averaged across multiple runs. "], "page_idx": 29}, {"type": "text", "text": "E.7 Adaptive Attacks without Trapdoor Signatures ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "This section investigates adaptive attacks when the adversary only knows the existence of trapdoors without access to trapdoor signatures. Here we modify the loss function in Equation 7 by excluding the regularization term of trapdoor signatures: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\mathcal{L}_{G+}=\\mathcal{L}_{G}-\\lambda_{\\mathrm{aux}}\\mathbb{E}_{Y\\sim p_{\\mathrm{aux}}(Y)}\\left[\\mathbb{E}_{Z\\sim p_{G}(Z)}[\\cos(g_{\\theta}(T_{\\mathrm{atack}}(G(Z,Y))),S_{\\mathrm{aux},Y})]\\right].\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "This adaptive attack is denoted by $\\mathrm{PLG-MI+}$ . Table 15 demonstrates its comparable attack performance with PLG- $\\mathbf{\\cdot}\\mathbf{M}\\mathbf{I}+\\mathbf{+}$ , indicating that if the auxiliary dataset is close enough to the private distribution, guiding attacks by auxiliary signatures is sufficient to boost attacks. However, as Table 16 shows, trapdoor signatures are required to enhance the attack performance further when there are distributional shifts in auxiliary data. Overall, Trap-MID still provides better privacy preservation than existing defenses against adaptive attacks. ", "page_idx": 29}, {"type": "text", "text": "E.8 Combining Trap-MID with NegLS ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We further analyze whether Trap-MID can be combined with existing baselines to enhance performance. Table 17 presents the defense performance of combining Trap-MID with NegLS, using the same configurations outlined in Appendix C. Following the evaluation protocol in Section 4.1, the experiments are conducted across 5 runs. This hybrid defense slightly decreases accuracy but significantly improves defense effectiveness, even against KD-based attacks like LOMMA. For instance, it reduces LOMMA (KED-MI)\u2019s attack accuracy to $42.47\\%$ , while Trap-MID or NegLS alone achieves only $61.25\\%$ or $77.67\\%$ . The reconstructed images can be found in Appendix F. ", "page_idx": 29}, {"type": "text", "text": "This suggests that Trap-MID stands as an orthogonal approach to existing defenses and can be integrated with them. Intuitively, NegLS focuses on reducing the leakage of guiding signals, making it more difficult for adversaries to extract private data. Consequently, this enhancement makes Trap-MID\u2019s shortcuts more appealing to attack algorithms, thereby bolstering privacy protection. ", "page_idx": 29}, {"type": "table", "img_path": "GNhrGRCerd/tmp/5686fc654983652ecf626643d044d5b03ad00ae949ac44bed993485a7e8e878b.jpg", "table_caption": ["Table 17: Defense performance when combining Trap-MID with NegLS, using VGG-16 models. "], "table_footnote": [], "page_idx": 30}, {"type": "table", "img_path": "GNhrGRCerd/tmp/70deb5a8d3a0e6109411c3ed4266c11bbe1e28e54c85e29bc267af7e96e9205f.jpg", "table_caption": ["Table 18: Defense comparison against BREP-MI, using untargeted attacks to recover 300 identities. "], "table_footnote": [], "page_idx": 30}, {"type": "text", "text": "Another promising direction for future research is to combine multiple defense strategies to improve overall performance and robustness against specific adaptive attacks. ", "page_idx": 30}, {"type": "text", "text": "E.9 Defense Performance against Label-Only Attacks ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "In previous experiments, we evaluated defense methods against white-box MI attacks, which pose a greater privacy threat. However, in practical scenarios, adversaries may only have access to model predictions or output labels, conducting black-box or label-only attacks on victim models. ", "page_idx": 30}, {"type": "text", "text": "This section investigates the defense performance of different approaches against BREP-MI [11], a label-only attack. BREP-MI starts by randomly sampling the generator\u2019s latent until the victim model classifies the generated image as the target class. It then estimates the predicted labels over a sphere in latent space to iteratively adjust the image away from the model\u2019s decision boundary. We use the victim models trained on the CelebA dataset and the generator from GMI to analyze performance against BREP-MI. ", "page_idx": 30}, {"type": "text", "text": "Under targeted attack settings, BREP-MI fails to initialize latents for all 1,000 identities in a reasonable time when facing Trap-MID, sampling latents for only 942 identities after 820,000 iterations. In contrast, it only takes 553 iterations against unprotected models. ", "page_idx": 30}, {"type": "text", "text": "We also conducted untargeted attacks to recover 300 identities. As shown in Table 18, Trap-MID significantly increased the number of initial iterations required and reduced BREP-MI\u2019s attack accuracy to $0\\%$ , demonstrating its effective privacy protection against various types of MI attacks. ", "page_idx": 30}, {"type": "table", "img_path": "GNhrGRCerd/tmp/9ff616913c2f4812a070ce05439fdcd91477b8a30e1577d2e7ceb329b6fe08f3.jpg", "table_caption": ["Table 19: Defense performance against PPA. "], "table_footnote": ["\\* Reported in PPA\u2019s paper [6]. \u2020 Reported in NegLS\u2019s paper [16]. "], "page_idx": 31}, {"type": "table", "img_path": "GNhrGRCerd/tmp/7d6cbaf75eb82a9aa3a638e4e4af3f9f0ce4d010bfd84234e06f4f12ad57ccbc.jpg", "table_caption": ["Table 20: Additional evaluation results of the defense performance against PPA. "], "table_footnote": ["\\* Reported in PPA\u2019s paper [6]. "], "page_idx": 31}, {"type": "text", "text": "E.10 Defense Performance Under High-Resolution Scenario ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "This section evaluates Trap-MID\u2019s defense performance on modern architectures and in highresolution scenarios. Specifically, we conducted the experiments introduced by PPA [6]. The target models are ResNeSt-101 [46], ResNet-152 [33], and DenseNet-169 [47] trained on a highquality version of the CelebA dataset, with the images cropped, aligned, and resized to $224\\!\\!\\!\\times\\!224$ using HD CelebA Cropper.3 We adopted PPA to perform MI attacks using a StyleGAN2 generator [21] pre-trained on the FFHQ dataset [38]. The generator outputs $1024\\!\\!\\times\\!1024$ images, which are then center-cropped to $800\\mathrm{x}800$ and resized to fit the model\u2019s input resolution. ", "page_idx": 31}, {"type": "text", "text": "We adopt the same InceptionV3 model used in PPA\u2019s official setup as the evaluation model. It was trained on the same training dataset as the target models, with an input size of $299\\mathrm{x}299$ . The evaluation model achieves $93.28\\%$ accuracy on the testing dataset. In addition to attack accuracy, KNN distance in the evaluation model\u2019s feature space $\\left(\\delta_{\\mathrm{eval}}\\right)$ , and FID, PPA also employs additional metrics, such as KNN Distance in the FaceNet [43] feature space $(\\delta_{\\mathrm{face}})$ , improved precision and recall [44], and density and coverage [45] metrics. Details of these metrics are provided in Appendix E.3. ", "page_idx": 31}, {"type": "text", "text": "We follow the same training settings as [6]. In addition to the Trap-MID configurations detailed in Appendix C.4, we also fine-tuned the blend ratio $\\alpha$ and trapdoor loss weight $\\beta$ according to the observations in Appendix D.3 and D.4. Since we found that Trap-MID models generally achieve better accuracy than the unprotected models, we set $\\beta=0.5$ and selected the smallest $\\alpha$ for each model to effectively distinguish triggered samples from benign ones. Specifically, the blend ratios are 0.005, 0.007, and 0.01 for the ResNeSt-101, ResNet-152, and DenseNet-169 models, respectively. ", "page_idx": 31}, {"type": "text", "text": "Table 19 and Table 20 present Trap-MID\u2019s defense performance against PPA, compared with attack results reported in previous works. Although Trap-MID does not fully mitigate PPA with default settings, it preserves privacy to a certain degree without sacrificing accuracy. Furthermore, selecting proper hyper-parameters significantly enhances defense performance, reducing attack accuracy on ResNet-152 and DenseNet-169 to below $2\\%$ , and that on ResNeSt-101 to $15.58\\%$ . This demonstrates that while hyper-parameter tuning is essential for optimal defense, Trap-MID provides effective privacy protection across various datasets and architectures, ", "page_idx": 32}, {"type": "text", "text": "F Additional Visualization ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Figure 8 shows the reconstructed images with the hybrid approach discussed in Appendix E.8. Figure 9 illustrates additional recovered images from PLG-MI, where the recoveries of Identity 4 from Trap-MID exhibit different hair colors from the private data. Such variation does not appear in other defenses, demonstrating Trap-MID\u2019s effectiveness in protecting private information. In addition, Figure 10, Figure 11, Figure 12, and Figure 13 display sample reconstructed images from GMI, KED-MI, and LOMMA attacks. ", "page_idx": 32}, {"type": "image", "img_path": "GNhrGRCerd/tmp/2b205b045852075310986f9caef57ee2ca6783fd7e836bb1db5ffcfc5a87f832.jpg", "img_caption": [], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "GNhrGRCerd/tmp/2e2d971a33507e97721e52d5accce09f0527c480f4ead1eda8f294a30ab4ddb7.jpg", "img_caption": ["Figure 9: Additional reconstructed images from PLG-MI. "], "img_footnote": [], "page_idx": 32}, {"type": "image", "img_path": "GNhrGRCerd/tmp/afb9d3fb0e7015276746c6615b957e83b7561f6eb2b5da837d3e263adeadbaa9.jpg", "img_caption": ["Figure 10: Reconstructed images from GMI. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "GNhrGRCerd/tmp/33ba0267ce82c725d057133d88b3a2d00b8b51d125a9ab70ca896b2b59bba02a.jpg", "img_caption": ["Figure 11: Reconstructed images from KED-MI. "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "GNhrGRCerd/tmp/17268c22974b3f34bcce98c162a6645f5c1ecfc7c715d5984717f9334570bb03.jpg", "img_caption": ["Figure 12: Reconstructed images from LOMMA (GMI). "], "img_footnote": [], "page_idx": 33}, {"type": "image", "img_path": "GNhrGRCerd/tmp/e4028cbb559c8216d1f49fb18f0c49aeb6fdfd5c87f3d210fab8404fea0928dc.jpg", "img_caption": ["Figure 13: Reconstructed images from LOMMA (KED-MI). "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: The claimed theoretical analysis is demonstrated in Section 3.4: Theoretical Analysis, and the defense performance is included in Section 4.2: Experimental Results. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 34}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Justification: Appendix A.2: Limitations and Future Works discusses the limitations of this work. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 34}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The assumptions and the proof of theoretical result are provided in Section 3.4: Theoretical Analysis and Appendix B: Proof of Theorem 1. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 35}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: The instructions of the training procedure and the experimental configurations are outlined in Section 3.3: Model Training and Appendix C: Experimental Details. We also provide the code in supplemental material. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 35}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The code is provided in supplementary material, including instructions about data preparation, model training, MI attacks, and a checkpoint of the Trap-MID model. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 36}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The experimental setups are listed in Section 4.1: Experimental Setups and Appendix C: Experimental Details. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 36}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: We conducted the experiments multiple times and reported the standard deviation of each metric as the error bars. The detailed calculation method is provided in Section 4.1: Experimental Setups and Appendix D: Ablation Studies. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. ", "page_idx": 36}, {"type": "text", "text": "\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 37}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The compute resources are listed in Appendix C.1: Hardware and Software Details. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 37}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: We acknowledge that we have read and committed to adhering to the NeurIPS Code of Ethics. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 37}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: Appendix A.1: Broader Impacts demonstrates the broader impacts of this work. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 38}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 38}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 38}, {"type": "text", "text": "Justification: This research only involves the defense strategy against existing privacy attacks, which doesn\u2019t pose such risks. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 38}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: The original papers of the codes and datasets used in the experiments are cited, and the license from https://github.com/SCccc21/Knowledge-Enriched-DMI is provided with the provided code, as we mainly modified their code to implement our defense. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets. \u2022 The authors should cite the original paper that produced the code package or dataset. \u2022 The authors should state which version of the asset is used and, if possible, include a URL. ", "page_idx": 38}, {"type": "text", "text": "\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 39}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: The documentation is provided with the code in the supplementary material, including instructions about data preparation, model training and MI attacks. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 39}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 39}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 39}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 39}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 39}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. ", "page_idx": 39}, {"type": "text", "text": "\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 40}]