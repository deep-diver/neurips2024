[{"figure_path": "GNhrGRCerd/figures/figures_3_1.jpg", "caption": "Figure 1: Illustration of the intuition behind Trap-MID and our training pipeline.", "description": "The figure illustrates the core idea of Trap-MID and its training process. (a) shows that Trap-MID introduces a trapdoor into the model, creating a shortcut for model inversion attacks. This trapdoor information misleads the attacks towards the trapdoor information rather than the private data, protecting the privacy of the private data distribution. (b) provides a detailed description of the Trap-MID training pipeline, illustrating how the trapdoor is integrated into the model, including the classification loss, trapdoor loss, and discriminator loss.", "section": "3 Methodology"}, {"figure_path": "GNhrGRCerd/figures/figures_7_1.jpg", "caption": "Figure 2: Reconstructed images from PLG-MI.", "description": "This figure shows the reconstructed images generated by the state-of-the-art model inversion attack, PLG-MI, against different defense methods. The first row displays the original private images from the training dataset. The subsequent rows illustrate the reconstructed images produced by the unprotected model and different defense methods, including MID, BiDO, NegLS, and Trap-MID. By visually comparing the reconstructed images with the original private images, one can assess the effectiveness of each defense method in protecting the privacy of the training data.", "section": "4.2 Experimental Results"}, {"figure_path": "GNhrGRCerd/figures/figures_8_1.jpg", "caption": "Figure 3: Illustration of trapdoor detection.", "description": "This figure visualizes the effectiveness of Trap-MID in misleading MI attacks by showing the cosine similarity between trapdoor signatures and generated samples from various attacks (PLG-MI, L-inf, L-2). The results demonstrate that Trap-MID successfully directs MI attacks toward trapdoor triggers, leading to a different distribution of recovered samples.", "section": "4 Experiments"}, {"figure_path": "GNhrGRCerd/figures/figures_23_1.jpg", "caption": "Figure 5: Defense comparison with different augmentation against PLG-MI, using VGG-16 models.", "description": "The figure shows the results of comparing different augmentation strategies against the PLG-MI attack using VGG-16 models. The strategies include no augmentation, random cropping, random cropping and rotation, and random cropping, flipping, and rotation. The metrics used for comparison are attack accuracy (top 1 and top 5), KNN distance, and FID. The results show that Trap-MID with more augmentations is more robust against the PLG-MI attack. ", "section": "D.5 Augmentation"}, {"figure_path": "GNhrGRCerd/figures/figures_23_2.jpg", "caption": "Figure 6: Defense comparison with different augmentation probabilities against PLG-MI, using VGG-16 models.", "description": "This figure compares the defense performance of Trap-MID against the PLG-MI attack using different augmentation probabilities during training.  It shows that increasing the probability of applying augmentations (random resized crop in this case) generally improves the robustness of Trap-MID against the attack, reducing attack accuracy and improving metrics like KNN distance and FID. The results suggest that stronger and more frequent augmentations better reveal and address the weaknesses of the trapdoor mechanism, resulting in more stable and effective defense.", "section": "D.5 Augmentation"}, {"figure_path": "GNhrGRCerd/figures/figures_28_1.jpg", "caption": "Figure 3: Illustration of trapdoor detection.", "description": "The figure shows four histograms illustrating the effectiveness of trapdoor detection against different MI attacks (GMI, KED-MI, LOMMA (GMI), LOMMA (KED-MI)).  For each attack, two histograms are shown: one for the cosine similarity of clean public data with the trapdoor signature, and one for the cosine similarity of recovered data (generated by the MI attack) with the trapdoor signature.  The histograms illustrate that the recovered data from Trap-MID shows a greater similarity to the trapdoor signature compared to the clean public data, thus deceiving the MI attacks.", "section": "4 Experiments"}, {"figure_path": "GNhrGRCerd/figures/figures_32_1.jpg", "caption": "Figure 2: Reconstructed images from PLG-MI.", "description": "This figure shows the reconstructed images from the state-of-the-art model inversion attack, PLG-MI, using different defense methods. Each row represents a different defense method: unprotected, MID, BiDO, NegLS, Trap-MID, and Trap-MID combined with NegLS. Each column shows a different identity, with the corresponding private image on the top row. The figure visually demonstrates Trap-MID's superior performance in misleading MI attacks by generating images that are significantly less similar to the private images compared to other methods.", "section": "4.2 Experimental Results"}, {"figure_path": "GNhrGRCerd/figures/figures_32_2.jpg", "caption": "Figure 2: Reconstructed images from PLG-MI.", "description": "This figure shows the reconstructed images generated by the state-of-the-art model inversion attack, PLG-MI, targeting different defense methods.  The top row displays the original private images. Subsequent rows show the reconstructed images from the unprotected model, MID, BiDO, NegLS, and Trap-MID. The results from Trap-MID demonstrate that it successfully misleads the PLG-MI attack into producing images visually dissimilar from the actual private data.", "section": "4.2 Experimental Results"}, {"figure_path": "GNhrGRCerd/figures/figures_33_1.jpg", "caption": "Figure 2: Reconstructed images from PLG-MI.", "description": "This figure shows the reconstructed images generated by the state-of-the-art model inversion attack, PLG-MI.  It compares the quality of the reconstructed images from different defense methods, including no defense, MID, BiDO, NegLS, and Trap-MID.  The figure visually demonstrates the effectiveness of Trap-MID in generating less realistic and less similar images to the private data compared to other methods, showcasing its ability to mislead the attack.", "section": "4.2 Experimental Results"}, {"figure_path": "GNhrGRCerd/figures/figures_33_2.jpg", "caption": "Figure 2: Reconstructed images from PLG-MI.", "description": "This figure shows the reconstructed images from PLG-MI attack. It compares the reconstructed images of private data from different defense methods against PLG-MI attack. It demonstrates that Trap-MID outperforms other baseline defense methods in misleading the PLG-MI attack and thus better preserving privacy by generating images less similar to the private data.", "section": "4.2 Experimental Results"}, {"figure_path": "GNhrGRCerd/figures/figures_33_3.jpg", "caption": "Figure 2: Reconstructed images from PLG-MI.", "description": "This figure shows the reconstructed images from the PLG-MI attack under different defense methods.  The top row displays the original private images. Subsequent rows show images reconstructed by the PLG-MI attack without any defense (Unprotected), and with the following defenses applied: MID, BiDO, NegLS, Trap-MID, and Trap-MID combined with NegLS. The images visually demonstrate the effectiveness of Trap-MID in preventing the reconstruction of realistic and private-looking images compared to the other defense methods.", "section": "4.2 Experimental Results"}, {"figure_path": "GNhrGRCerd/figures/figures_33_4.jpg", "caption": "Figure 2: Reconstructed images from PLG-MI.", "description": "This figure displays the results of applying the PLG-MI attack (a state-of-the-art model inversion attack) to reconstruct images from a facial recognition model that has been protected using different methods. The top row shows the original private images. Subsequent rows showcase the reconstructed images obtained using various defense mechanisms, including MID, BiDO, NegLS, Trap-MID, and Trap-MID combined with NegLS. The images illustrate the effectiveness of each defense strategy in preventing the successful reconstruction of private data.", "section": "4.2 Experimental Results"}]