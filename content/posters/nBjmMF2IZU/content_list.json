[{"type": "text", "text": "Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yuexiang Zhai1\u2217Hao Bai2\u2020 Zipeng Lin1\u2020 Jiayi Pan1\u2020 Shengbang Tong3\u2020 Yifei Zhou1\u2020 ", "page_idx": 0}, {"type": "text", "text": "Alane Suhr1 Saining Xie3 Yann LeCun3 Yi Ma1 Sergey Levine1 ", "page_idx": 0}, {"type": "text", "text": "1UC Berkeley 2UIUC 3NYU ", "page_idx": 0}, {"type": "image", "img_path": "nBjmMF2IZU/tmp/6ca2ba08fdf00f75941065cd944f17f6ae3c8f51a813388d2a41d1205f59e771.jpg", "img_caption": ["Figure 1: Method overview. We propose a framework for training large Vision-Language Models (VLM) with Reinforcement Learning (RL). At each time step, the VLM takes the current observation and a predesigned prompt as input and outputs an utterance containing a chain of thought reasoning and a text action. The text action is parsed into the environment for generating task rewards. Finally, we apply RL with the task reward to fine-tune the entire VLM. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Large vision-language models (VLMs) fine-tuned on specialized visual instructionfollowing data have exhibited impressive language reasoning capabilities across various scenarios. However, this fine-tuning paradigm may not be able to efficiently learn optimal decision-making agents in multi-step goal-directed tasks from interactive environments. To address this challenge, we propose an algorithmic framework that fine-tunes VLMs with reinforcement learning (RL). Specifically, our framework provides a task description and then prompts the VLM to generate chain-of-thought (CoT) reasoning, enabling the VLM to efficiently explore intermediate reasoning steps that lead to the final text-based action. Next, the open-ended text output is parsed into an executable action to interact with the environment to obtain goal-directed task rewards. Finally, our framework uses these task rewards to fine-tune the entire VLM with RL. Empirically, we demonstrate that our proposed framework enhances the decision-making capabilities of VLM agents across various tasks, enabling 7b models to outperform commercial models such as GPT4-V or Gemini. Furthermore, we find that CoT reasoning is a crucial component for performance improvement, as removing the CoT reasoning results in a significant decrease in the overall performance of our method. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Large vision-language models (VLMs) [7, 44, 18] demonstrate remarkable capabilities as generalpurpose agents in solving various tasks through language reasoning. In particular, fine-tuning VLMs with specialized visual instruction following data appears to be a key technique for improving the capabilities of VLMs [34, 84, 33, 30]. However, visual instruction tuning may not be optimal for training decision-making agents in multi-step interactive environments requiring visual recognition and language understanding, as visual instruction tuning mainly performs supervised learning on precollected datasets without interacting with the environments [22]. Consequently, if the pre-collected datasets lack sufficient diversity to cover a wide range of decision-making scenarios, visual instruction tuning may fail to improve the VLM agent\u2019s decision-making capabilities. ", "page_idx": 1}, {"type": "text", "text": "To unleash the learning capabilities of VLM agents in multi-step goal-directed decision-making environments, reinforcement learning (RL), a method that has proven effective in training multi-step interactive agents [41, 59, 6, 69], naturally offers a paradigm that supports this purpose. However, while RL has been widely adopted for training purely text-based tasks for large language models (LLMs) [60, 50, 1, 83], end-to-end VLM fine-tuning with RL for goal-directed multi-step tasks has not yet been studied, to the best of our knowledge. ", "page_idx": 1}, {"type": "text", "text": "Our main contribution in this paper is an algorithmic framework that directly fine-tunes VLMs with RL for multi-step goal-directed decision-making tasks requiring vision-language understanding. In our framework, the VLM first receives a task description prompt, which guides it to generate task-specific chain-of-thought (CoT) reasoning [75, 73] (blue parts in Figure 1), followed by a text-based action (red parts in Figure 1). The CoT reasoning is designed for efficient explorations by prompting the VLMs to generate intermediate reasoning that leads to the final text-based action. Our framework then parses the text-based actions into executable actions for the environment, which generates potentially goal-directed rewards and the next state for RL training. ", "page_idx": 1}, {"type": "text", "text": "To evaluate the effectiveness of our method in enhancing a VLM\u2019s decision-making capabilities, we adopt a 7b model [35] as the backbone VLM and apply our method to five decision-making tasks. These tasks come from two domains: an original domain, which evaluates the VLM\u2019s decision-making capabilities requiring fine-grained visual recognition and language reasoning, and an embodied AI domain [58] focusing on testing tasks demanding visual semantic reasoning capabilities. Empirical results show that our method enhances the decision-making capabilities of VLMs in both domains, enabling 7b models to surpass the performance of commercial models such as GPT4-V [44] and Gemini [18]. Moreover, our experiments reveal that CoT reasoning is crucial for performance improvement in our RL training. Specifically, we test our method on the same tasks without the CoT reasoning and observe a significant drop in overall performance in both domains. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Training LLMs or VLMs with RL. RL has been widely adopted for training LLMs and VLMs [85, 61, 70, 45, 10, 50, 9, 43, 18, 62, 60, 1, 20, 83]. Some studies [85, 61, 45, 10, 43, 18, 62] focus on applying RL from human feedback (RLHF), which involves learning reward models from human feedback before deploying RL. Other research [50, 9, 60, 1, 20, 83] focuses on deploying RL with task-specific reward functions without using human preference data. Our paper is similar to the latter [50, 9, 60, 1, 20, 83] which applies RL to train LLMs on customized reward functions from different environments. There are two major differences between our paper and prior works [50, 60, 1, 20, 83]. Firstly, our method incorporates visual inputs, broadening its applicability to a wider range of tasks that require vision-language understanding or multimodal reasoning [29, 38]. Secondly, while previous works do not explore how CoT reasoning affects RL training on large models in general, we identify CoT reasoning as a crucial component for enhancing RL training. We empirically observe that incorporating CoT reasoning significantly improves the overall performance of RL training on all tested domains. ", "page_idx": 1}, {"type": "text", "text": "Adopting LLMs and VLMs as decision-making agents. Many prior works have studied various methods of using frozen LLMs and VLMs for decision-making. One line of work studies the prompting techniques [75, 14, 79, 78, 74, 31, 76, 47, 71, 48, 24] for enhancing the decision-making capabilities of large foundation models, see Dong et al. [14], Yang et al. [77] for a detailed survey for other prompting based methods. Our work differs from all prompting-based methods since we directly use RL to fine-tune the entire VLM as decision-making agents. Other studies [42, 64, 4, 52, 11] integrate frozen VLMs ot LLMs into their training pipeline for processing task descriptions or feature extraction, without using text-based actions. focuses on integrating different components from VLMs for downstream RL training. For example, some studies use the VLMs or CLIP vision encoder [46, 42, 64] as reward models for training, which differs from our method since we adopt rewards from the environments. Other studies [42, 64, 11] integrate frozen VLMs/LLMs into their training pipeline for processing task descriptions [42, 64, 46] or feature extraction [11], without using text-based actions. Our paper differs from these works [42, 64, 11] in two major aspects. From a technical perspective, we focus on a more challenging paradigm by directly fine-tuning the entire VLM with RL, whereas previous methods [42, 64, 11] only train additional MLP or transformer layers to connect the frozen LLM/VLM with the action space. More importantly, our method directly interacts with the environments using open-ended text, enabling it to utilize the CoT reasoning capability of VLMs for more efficient explorations for decision-making. ", "page_idx": 2}, {"type": "text", "text": "Evaluating VLMs as decision-making agents. Previous studies have thoroughly examined the fundamental evaluations of VLMs in non-interactive tasks [3, 37, 80, 32, 65, 81, 16]. Our focus, however, is on evaluating a VLM\u2019s decision-making capabilities in interactive environments that require both visual recognition and language reasoning. Representative interactive environments include purely text-based environments [13, 28, 72] or embodied AI environments [40, 58, 56, 15]. We adopt the ALFWorld [58] embodied environment for evaluating our method\u2019s ability to improve VLM\u2019s visual semantic reasoning capabilities. In addition to the ALFWorld embodied AI environment, we also design an original \u201cgym-like\u201d [8] environment to test VLM\u2019s decision-making capabilities in tasks that require fine-grained visual recognition and language reasoning. ", "page_idx": 2}, {"type": "text", "text": "CoT prompting. Recent studies in prompting for LLMs have demonstrated the crucial role of CoT in enhancing complex reasoning capabilities [75, 26, 17, 73, 82, 79]. Wei et al. [75] show that CoT reasoning can significantly boost LLMs\u2019 performance across different reasoning tasks by showing that adding simple exemplar-based prompts, leading to better performance on benchmarks such as the GSM8K [12]. A follow-up study [73] proposes a novel self-consistency decoding strategy that explores multiple reasoning paths, demonstrating substantial gains in arithmetic and commonsense reasoning tasks. Other works [26, 82, 17] have shown that adding prompts to break complex tasks into subtasks and solve them step-by-step significantly improves LLM\u2019s reasoning capability. Our work differs from these CoT prompting studies as we aim to provide an algorithmic framework that can train VLMs with RL, where the CoT prompting appears as a key component of the framework. In contrast, prior works focus on improving the reasoning capabilities of LLMs with increasingly sophisticated prompting of frozen models. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Standard RL terminologies. We follow the standard notations from classic RL literature [63, 2]. Specifically, we use $\\mathcal{M}\\overset{=}{=}\\{S,\\mathcal{A},P,r,\\gamma\\}$ to denote an MDP, where $\\boldsymbol{S}$ denotes the state space, $\\boldsymbol{\\mathcal{A}}$ denotes the action space, $P$ denotes the transition dynamics, $r:S\\times A\\to\\mathbb{R}$ denotes the reward function and $\\gamma\\in[0,1]$ denotes the discount factor. Our goal is to learn a policy $\\pi:{\\mathcal{S}}\\rightarrow A$ that maximizes the overall discounted return $\\begin{array}{r}{\\operatorname*{max}_{\\pi\\in\\Pi}\\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{T}\\gamma^{t}r(s_{t},a_{t})\\right]}\\end{array}$ , where $T$ is the maximum number of steps per episode. Without loss of generality, we use $\\pi(a|s)\\in[0,1]$ to denote probability of $\\pi$ choosing $a$ at $s$ . ", "page_idx": 2}, {"type": "text", "text": "Adapting the RL formalism to VLMs. We use $\\nu$ to denote the discrete and finite vocabulary (token) space, and we use $\\mathcal{V}^{m},\\mathcal{V}^{n}$ to represent the input and output text space, where $m$ and $n$ represent the maximum token length of the input and output sequence. We adapt the RL formalism to VLMs by treating the combination of the vision and language inputs to VLMs as the state space: $S=\\mathcal{O}\\times\\dot{\\mathcal{V}}^{m}$ , where $\\scriptscriptstyle\\mathcal{O}$ is the space of all RGB images. We view each utterance [1, 83] of the language outputs from VLMs as the action space $\\mathcal{V}^{n}$ . Therefore, the input and output of a VLM policy with parameter $\\theta$ can be written as $\\pi_{\\theta}:{\\mathcal{O}}\\times\\mathcal{V}^{m}\\to\\mathcal{V}^{n}$ . For example, in the Blackjack task shown in Figure 1, each state $s$ consists of an RGB image $o$ with the cards of the dealer and the player, as well as an input prompt $v^{\\mathrm{in}}$ with maximum token length $m$ , and the text output $v^{\\mathrm{out}}=\\pi_{\\theta}(o,v^{\\mathrm{in}})$ (with a maximum token $n$ ) will later be parsed as an action to interact with the environment. Similar to the standard RL setting, we use $\\pi_{\\theta}(\\mathbf{v}^{\\mathrm{{out}}}|o,v^{\\mathrm{{in}}})\\in.[0,1]$ to denote the probability of a VLM policy $\\pi_{\\theta}$ outputting $v^{\\mathrm{out}}$ with input image $o$ and prompt $v^{\\mathrm{m}}$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "4 Training VLMs with RL ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Compared to classic MLP-based policy networks [53\u201355, 19], a natural advantage of VLM policies is that they can leverage CoT reasoning for efficient exploration, by performing intermediate reasoning steps that lead to the final decision. However, training a VLM policy $\\pi_{\\theta}$ with RL presents additional challenges. First, the VLM policy $\\pi_{\\theta}(o,v^{\\mathrm{in}})$ directly generates open-ended text rather than vectorized actions in classic policy gradient-based RL methods [53\u201355, 19], complicating direct interaction with the environment. Even with a parsing mechanism $f:\\mathcal{V}^{n}\\to A$ that maps open-ended text $v^{\\mathrm{out}}$ to a legal action $a$ for interaction with the environment, it remains unclear how to estimate the action probability $\\pi_{\\theta}(a|o,v^{\\mathrm{in}})$ from the text generation process. ", "page_idx": 3}, {"type": "text", "text": "Figure 2 presents an overview of our framework, leveraging the CoT reasoning and addressing the two aforementioned challenges. We design a task-specific prompt $v^{\\mathrm{in}}$ that requires the VLM to generate a formatted output $v^{\\mathrm{out}}$ , including the CoT reasoning. Next, we adopt a post-processing function $f$ to parse open-ended text into a legal action $a_{t}$ that can directly interact with the environment. To compute $\\bar{\\pi}_{\\boldsymbol{\\theta}}(a|o,v^{\\mathrm{in}})$ , we develop a method to estimate its value based on the probability of each output token in $v^{\\mathrm{out}}$ . ", "page_idx": 3}, {"type": "image", "img_path": "nBjmMF2IZU/tmp/3000686998b84f69dd02d7f68957e6af4562a239dd9af7c67584fb8977cedc15.jpg", "img_caption": ["Figure 2: A diagram of the proposed RL fine-tuning framework. At time step $t$ , the state $s_{t}$ contains an input prompt ${\\boldsymbol{v}}_{t}^{\\mathrm{{in}}}$ and a visual observation $o_{t}$ . The VLM takes $s_{t}=\\left[o_{t},v_{t}^{\\mathrm{in}}\\right]$ as input and outputs open-ended text $\\pmb{v}_{t}^{\\mathrm{{\\tiny{out}}}}$ containing the CoT reasoning, keywords \"action\" : \" $\"a_{t}\"$ , and the log-likelihood of $\\bar{\\pmb{v}}_{t}^{\\mathrm{out}}$ . We first apply a post-processing function $f$ on $\\pmb{v}_{t}^{\\mathrm{{out}}}$ , to obtain a legal action $a_{t}$ which can interact with the environment. Then, we input $a_{t}$ to the environment for obtaining reward $r(s_{t},a_{t})$ and the next observation $o_{t+1}$ . Afterward, we devise a method to compute a numerical value of $\\pi_{\\theta}\\big(a_{t}|o_{t},v_{t}^{\\mathrm{in}}\\big)$ . Finally, we use $r(s_{t},a_{t})$ and $\\pi_{\\theta}\\big(a_{t}|o_{t},v_{t}^{\\mathrm{in}}\\big)$ for the RL training. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "The remaining Section is structured as follows. First, we describe the format of our input prompt ${\\pmb v}_{t}^{\\mathrm{in}}$ and the desired output $\\pmb{v}_{t}^{\\mathrm{{out}}}$ (Section 4.1). Next, we present the post-processing function $f$ (Section 4.2). Then, we introduce a method to compute a numerical value of $\\pi_{\\theta}(a_{t}|o_{t},v_{t}^{\\mathrm{in}})$ (Section 4.3). Finally, we conclude our framework in Algorithm 1 (Section 4.4). ", "page_idx": 3}, {"type": "text", "text": "4.1 Prompt Design for Domain-Specific Outputs ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "For each task $\\mathcal{M}$ , our input prompt ${\\pmb v}_{t}^{\\mathrm{in}}$ contains a description of the task, the legal action space of the current observation, and the desired output format (including the CoT reasoning). Our desired output $\\pmb{v}_{t}^{\\mathrm{{out}}}$ , contains a CoT reasoning followed by the keywords \"action\" : \" $a_{t}\\,\"$ for post-processing. Figure 3 provides an example of our input prompt ${\\pmb v}_{t}^{\\mathrm{in}}$ and the desired formatted output $\\pmb{v}_{t}^{\\mathrm{{out}}}$ . In particular, we define a function $h$ which constructs ${\\pmb v}_{t}^{\\mathrm{in}}$ from the current observation $o_{t}\\colon v_{t}^{\\mathrm{in}}=h(o_{t})$ , to accommodate for tasks that may contain observation-dependent information.3 We provide additional examples of $v^{\\mathrm{in}}$ and $v^{\\mathrm{out}}$ in Appendix B. ", "page_idx": 3}, {"type": "image", "img_path": "nBjmMF2IZU/tmp/88a06a047390dc02c678e6aacf6aee4cbaa93dd2ec325c39502178b5c397a8f1.jpg", "img_caption": ["Figure 3: A template of our input prompt and output text. The blue part represents the CoT reasoning and the red part is the text-based action. Note that the CoT reasoning may contain other task-specific descriptions, see Appendix B for more details. "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "4.2 Post-Processing Open-Ended Text for Legal Actions ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our post-processing mechanism involves both ${\\pmb v}_{t}^{\\mathrm{in}}$ and $f$ . In the input prompt ${\\pmb v}_{t}^{\\mathrm{in}}$ , we directly ask the VLM to output a text-based action in the format of \"action\" : \" $a_{t}\\,\"$ (see Figure 1 and Figure 2 for examples). After obtaining $\\pmb{v}_{t}^{\\mathrm{{out}}}$ , our post-processing function $f$ directly searches for the text-based keywords $\"\\mathsf{a c t i o n}\":\"{a}_{t}\"$ \" from $\\pmb{v}_{t}^{\\mathrm{{out}}}$ , and maps it to a legal action $a_{t}$ , either in symbolic or in text depending on the task of interest. For the case shown in Figure 1, $f$ will map $\\pmb{v}_{t}^{\\mathrm{out}}$ to the symbolic operator that represents the action \"stand\" in the Blackjack task (to be introduced in Section 5.1), as the Blackjack task takes symbolic actions as input. For the alfworld [58] environment shown in Figure 2, $f$ will map $\\pmb{v}_{t}^{\\mathrm{out}}$ to the text \"look\", because the alfworld environment takes text-based actions as inputs. ", "page_idx": 4}, {"type": "text", "text": "However, VLMs are not always guaranteed to generate a $\\pmb{v}_{t}^{\\mathrm{{out}}}$ that contains the keywords \"action\" : $\"{a_{t}}\"$ , even when we explicitly request a formatted output from ${\\pmb v}_{t}^{\\mathrm{in}}$ . To continue the RL training when $\\pmb{v}_{t}^{\\mathrm{{out}}}$ does not contain any legal action, we perform random exploration by selecting a legal action $a_{t}\\in\\mathcal A$ uniformly at random. Mathematically, $f$ is defined as follows: ", "page_idx": 4}, {"type": "equation", "text": "$$\nf(v^{\\mathrm{out}})={\\left\\{\\!\\!\\begin{array}{l l}{a,}&{{\\mathrm{if~}}^{\\mathsf{n}}{\\mathsf{a c t i o n}}^{\\mathsf{n}}:{\\,}^{\\mathsf{n}}a^{\\mathsf{u}}\\in v^{\\mathrm{out}},}\\\\ {{\\mathsf{U n i f}}(A),}&{{\\mathrm{otherwise}}.}\\end{array}\\ \\right.}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "4.3 Estimating Action Probabilities of VLM Policies ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "To estimate the action probability $\\log\\pi_{\\theta}(a_{t}|o_{t},v_{t}^{\\mathrm{in}})$ (or equivalently $\\log\\pi_{\\theta}(a_{t}|o_{t},\\boldsymbol{v}_{t}^{\\mathrm{in}}))$ for policy gradient-based methods [55], a na\u00efve calculation is directly using $\\log\\pi_{\\theta}(\\mathbf{v}_{t}^{\\mathrm{out}}|o_{t},\\mathbf{v}_{t}^{\\mathrm{in}})$ as $\\log\\pi_{\\theta}(a_{t}|o_{t},v_{t}^{\\mathrm{in}})$ , by summing the log-likelihood of all tokens in $\\pmb{v}_{t}^{\\mathrm{{out}}}$ . This is because ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\ \\log\\pi_{\\theta}({\\boldsymbol v}_{t}^{\\mathrm{out}}|{\\boldsymbol o}_{t},{\\boldsymbol v}_{t}^{\\mathrm{in}})=\\log\\frac{P({\\boldsymbol o}_{t},{\\boldsymbol v}_{t}^{\\mathrm{in}},{\\boldsymbol v}_{t}^{\\mathrm{out}})}{P({\\boldsymbol o}_{t},{\\boldsymbol v}_{t}^{\\mathrm{in}})}}\\\\ &{=\\log\\left[\\frac{P({\\boldsymbol o}_{t},{\\boldsymbol v}_{t}^{\\mathrm{in}},{\\boldsymbol v}_{[:n]})}{P({\\boldsymbol o}_{t},{\\boldsymbol v}_{t}^{\\mathrm{in}},{\\boldsymbol v}_{[:n-1]})}\\cdots\\frac{P({\\boldsymbol o}_{t},{\\boldsymbol v}_{t}^{\\mathrm{in}},{\\boldsymbol v}_{[:2]})}{P({\\boldsymbol o}_{t},{\\boldsymbol v}_{t}^{\\mathrm{in}},{\\boldsymbol v}_{[:1]})}\\frac{P({\\boldsymbol o}_{t},{\\boldsymbol v}_{t}^{\\mathrm{in}},{\\boldsymbol v}_{[:1]})}{P({\\boldsymbol o}_{t},{\\boldsymbol v}_{t}^{\\mathrm{in}})}\\right]=\\sum_{i=1}^{n}\\log\\left[\\frac{P({\\boldsymbol o}_{t},{\\boldsymbol v}_{t}^{\\mathrm{in}},{\\boldsymbol v}_{[:i-1]})}{P({\\boldsymbol o}_{t},{\\boldsymbol v}_{t}^{\\mathrm{in}},{\\boldsymbol v}_{[:i-1]})}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In the equation above, we use $\\pmb{v}$ to denote the output token $\\pmb{v}_{t}^{\\mathrm{out}}$ for simplicity, and we use $\\pmb{v}_{[:i]}$ to denote the first $i$ tokens in $\\pmb{v}_{t}^{\\mathrm{out}}$ , and we slightly abuse our notion by using $P(o_{t},{\\boldsymbol v}_{t}^{\\mathrm{in}},{\\boldsymbol v}_{[:0]})$ to denote $P(o_{t},{\\boldsymbol{v}}_{t}^{\\mathrm{in}})$ in the log summation. Hence, a natural way to compute a numerical value for $\\log\\pi_{\\theta}(a_{t}|o_{t},v_{t}^{\\mathrm{in}})$ is $\\begin{array}{r}{\\sum_{i=1}^{n}\\log\\left[\\frac{P\\left(o_{t},\\pmb{v}_{t}^{\\mathrm{in}},\\pmb{v}_{\\left[:i\\right]}\\right)}{P\\left(o_{t},\\pmb{v}_{t}^{\\mathrm{in}},\\pmb{v}_{\\left[:i-1\\right]}\\right)}\\right]}\\end{array}$ However, the na\u00efve calculation log \u03c0\u03b8(at|ot, vitn ) \u2190  in=1 log P P( o(to,tv,itvn ,t v,[v:i[:\u2212i]1)]) may not be ideal for computing $\\pi_{\\theta}(a_{t}|o_{t},v_{t}^{\\mathrm{in}})$ since our formatted output $\\pmb{v}_{t}^{\\mathrm{{out}}}$ also contains CoT reasoning. This is because in $\\pmb{v}_{t}^{\\mathrm{out}}\\;=\\;\\left[\\boldsymbol{v}_{t}^{\\mathrm{tht}},\\boldsymbol{v}_{t}^{\\mathrm{act}}\\right]$ , the $\\mathrm{CoT}$ reasoning tokens $v_{t}^{\\mathrm{tht}}$ are generally much longer than the action tokens $v_{t}^{\\mathrm{act}}$ (see the blue and red parts in Figure 3 for examples, and see Table 1 for a relative scaling of their sum log-likelihood). Hence the na\u00efve computation $\\log\\pi_{\\theta}(a_{t}|o_{t},\\mathbf{v}_{t}^{\\mathrm{in}})\\gets\\log\\pi_{\\theta}(v_{t}^{\\mathrm{th}}|o_{t},v_{t}^{\\mathrm{in}})+\\log\\pi_{\\theta}(\\bar{v}_{t}^{\\mathrm{act}}|o_{t},v_{t}^{\\mathrm{in}},v_{t}^{\\mathrm{tht}})$ will make $\\log\\pi_{\\theta}(a_{t}|o_{t}\\bar{,}v_{t}^{\\mathrm{in}})$ largely determined by the CoT tokens $\\log\\pi_{\\theta}\\big(v_{t}^{\\mathrm{{tht}}}\\big|o_{t},v_{t}^{\\mathrm{in}}\\big)$ , which is practically undesirable because our post-processing function $f$ only relies on $v_{t}^{\\mathrm{act}}$ for decision-making. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "As shown in Table 1, $\\log\\pi_{\\theta}\\big(v_{t}^{\\mathrm{{tht}}}\\vert o_{t},v_{t}^{\\mathrm{{in}}}\\big)$ typically has a much larger magnitude than $\\log P(\\boldsymbol{v}_{t}^{\\mathrm{act}}|\\sigma_{t},\\boldsymbol{v}_{t}^{\\mathrm{in}},\\boldsymbol{v}_{t}^{\\mathrm{tht}})$ across all tasks we have tested (in terms of absolute value). Hence, to mitigate the effect of the CoT tokens, we adopt a scaling factor $\\lambda\\in[0,1]$ to scale down $\\log\\pi_{\\theta}\\big(v_{t}^{\\mathrm{tht}}\\bar{|}o_{t}^{-},v_{t}^{\\mathrm{in}}\\big)$ for obtaining a regularized version of $\\log\\pi_{\\theta}(a_{t}|o_{t},v_{t}^{\\mathrm{in}})$ , which results in ", "page_idx": 5}, {"type": "table", "img_path": "nBjmMF2IZU/tmp/f9a22391e2c442d59005b58e783cb96c04821fae3cbef863912d8e41e13fa279.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\ \\ \\ \\ \\ \\log\\pi_{\\theta}(a_{t}|o_{t},\\boldsymbol{v}_{t}^{\\mathrm{in}})}\\\\ &{\\quad\\ \\ \\ \\ \\ \\log\\pi_{\\theta}(v_{t}^{\\mathrm{tht}}|o_{t},\\boldsymbol{v}_{t}^{\\mathrm{in}})+\\log\\pi_{\\theta}(v_{t}^{\\mathrm{act}}|o_{t},\\boldsymbol{v}_{t}^{\\mathrm{in}},\\boldsymbol{v}_{t}^{\\mathrm{tht}}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Empirically, we observe the scaling factor $\\lambda$ could largely affect the final performance. As we will show in Section 6.2, choosing an extreme $\\lambda$ value (close to 1 or 0) will degrade overall performance. All of our experiments adopt $\\lambda\\in[0.2,0.5]$ . ", "page_idx": 5}, {"type": "text", "text": "4.4 Formal Implementation ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Putting the prompt construction function $h$ (Section 4.1), the post-processing function $f$ (Section 4.2), and the computation of $\\pi_{\\theta}(a_{t}|o_{t},v_{t}^{\\mathrm{in}})$ (Section 4.3) together, we conclude our method in Algorithm 1. ", "page_idx": 5}, {"type": "text", "text": "Algorithm 1 Training VLM with RL   \n1: Input: An environment env, an initial VLM with parameters $\\theta_{0}$ .   \n2: Input: A post-processing function $f$ , a CoT reasoning scaling factor $\\lambda$ .   \n3: Input: Replay buffer size $B$ , maximum episode length $T$ .   \n4: for $k=0,\\ldots,K-1$ do   \n5: $t=0$ $\\triangleright$ Reset RL time step   \n6: ot = env.reset() $\\triangleright$ Reset the initial state   \n7: vitn = h(ot) $\\triangleright$ Generate ${\\pmb v}_{t}^{\\mathrm{in}}$ from $o_{t}$ , $h$ is defined in Section 4.1   \n8: Bk = \u2205 $\\triangleright$ Initialize an on-policy replay buffer   \n9: while $|B_{k}|\\le B$ do   \n10: $\\begin{array}{r l r}&{v_{t}^{\\mathrm{out}}=\\pi_{\\theta_{k}}(o_{t},v_{t}^{\\mathrm{in}})}&{\\quad\\mathrm{>~Obtain~alegal~action~from~}v_{t}^{\\mathrm{out,}}\\mathrm{~fornectate~text~output}}\\\\ &{a_{t}=f(v_{t}^{\\mathrm{out}})}&{\\quad\\mathrm{>~Obtain~alegal~action~from~}v_{t}^{\\mathrm{out,}}\\mathrm{~,~}f\\mathrm{~is~defined~in~Equation~4.1}}\\\\ &{\\log\\pi_{\\theta_{k}}(a_{t}|\\rho_{t},v_{t}^{\\mathrm{in}})=\\lambda\\log\\pi_{\\theta_{k}}(v_{t}^{\\mathrm{in}}|v_{t}^{\\mathrm{in}})+\\log\\pi_{\\theta_{k}}(v_{t}^{\\mathrm{ixt}}|\\rho_{t},v_{t}^{\\mathrm{in}},v_{t}^{\\mathrm{in}})}&{\\quad\\mathrm{>~Equation~4.2}}\\\\ &{r_{t},a_{t+1}=\\mathrm{env.step}(a_{t})}&{\\quad\\mathrm{>~b~define~}}\\\\ &{B_{k}=B_{k}\\mathrm{~\\downarrow~}\\big\\{(\\rho_{t},a_{t},r_{t},v_{t}^{\\mathrm{out}},l o g\\pi_{\\theta_{k}}(a_{t}|\\rho_{t},v_{t}^{\\mathrm{in}}))\\big\\}}&{\\quad\\mathrm{>~b~Add~data~to~the~butfer~}B_{k}}\\\\ &{t=t+1}&{\\quad\\mathrm{~if~}}\\\\ &{\\mathrm{~if~}t=T\\mathrm{~then~}}&{\\quad\\mathrm{>~Reset~RL~time~step~if~the~maximum~step~is~reached}}\\\\ &{\\quad t=\\mathrm{o~}}&{\\quad\\mathrm{>~Reset~envit~roment}}\\\\ &{\\mathrm{~end~if~}}&{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\mathrm{~b~Prepare~the~next~}\\upsilon_{t}^{\\mathrm{in}}}\\\\ &{v_{t}^{\\mathrm{in}}=h(o_{t})}&{\\quad\\quad\\mathrm{>~Prepare~the~next~}\\upsilon_{t}^{\\mathrm{in}}}\\end{array}$   \n11:   \n12:   \n13:   \n14:   \n15:   \n16:   \n17:   \n18:   \n19:   \n20:   \n21: end while   \n22: Run PPO [55] with data $\\boldsymbol{{\\beta}}_{k}$ to obtain $\\theta_{k+1}$   \n23: end for   \n24: Output: $\\theta_{K}$ . ", "page_idx": 5}, {"type": "text", "text": "5 Evaluation Tasks ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "How does our method improve a VLM\u2019s decision-making capabilities in tasks that require finegrained vision-language reasoning or semantic understanding? To study this question, we adopt two different domains: gym_cards and alfworld [58]. Our original gym_cards domain is a \u201cgym-like\u201d environment [8] containing four tasks designed to test the decision-making capabilities of VLMs. ", "page_idx": 5}, {"type": "text", "text": "These tasks require fine-grained visual-language reasoning, specifically focusing on recognizing numbers for arithmetic reasoning. In addition, we also adopt alfworld [58], which assesses the decision-making capabilities of VLMs in an embodied AI setting that demands visual semantic understanding. We present some examples of the visual observations of each task in Figure 4. We do not include standard image-based Atari benchmarks [5, 39] due to limited computation resources.4 ", "page_idx": 6}, {"type": "image", "img_path": "nBjmMF2IZU/tmp/cbe47f4508ff43efbd6b97b717aa69448af09f9f9de9649c1272aa7c12d4d0bd.jpg", "img_caption": ["Figure 4: Examples of observation of our evaluation tasks. (a)-(d) are from our original gym_cards domain. (a)-(c) are deterministic tasks with increasing difficulties; (d) is a stochastic task. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "5.1 Gym Cards ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Our gym_cards domain is designed to evaluate a VLM\u2019s decision-making capabilities requiring fine-grained vision recognition and language reasoning. More precisely, tasks in the gym_cards domain require the VLM to recognize the numbers (potentially from cards) and utilize the numbers for language reasoning. As depicted in Figure 4, the first three tasks\u2014NumberLine, EZPoints, and Points24\u2014are deterministic, and developed to assess the VLMs\u2019 ability to identify and process numbers or mathematical operators at each time step. These tasks increase in complexity: NumberLine requires recognition of two numbers in an image, EZPoints involves identifying numbers from two cards, and Points24 extends to recognizing four cards. The Blackjack task challenges the VLM further by requiring the agent to reason based on visual information and adapt to stochastic outcomes. This subsection outlines the goals of each task, and we leave the detailed descriptions of their state spaces, action spaces, and reward functions to Appendix B.1. ", "page_idx": 6}, {"type": "text", "text": "NumberLine. In this task, the goal is to move a number to the target on a synthetic number line. At each state $s_{t}$ , the visual observation $o_{t}$ contains two lines of text: \u201cTarget: $x'^{\\star}$ and \u201cCurrent: $y_{t}\\\"$ . The agent needs to move the current number $y_{t}$ to the target number $x$ , by outputting text $\\pmb{v}_{t}^{\\mathrm{{out}}}$ that interacts with the discrete action space $\\{\\,\"\\,+\"\\,,\\,\"\\,-\"\\,\\}$ . Mapping the $\\pmb{v}_{t}^{\\mathrm{out}}$ to $\"+\"$ or $\"-\"$ will increase or decrease the current number by 1, respectively. ", "page_idx": 6}, {"type": "text", "text": "EZPoints. In this task, the goal is to output a formula using the numbers in the cards that evaluates to 12. At each state $s_{t}$ , the agent observes an image of two cards and a text version of (potentially incomplete) \u201cformula\u201d below the cards. The goal is to use all numbers in the cards (only once) to compute 12. The action space contains natural numbers in [1, 10], as well as operator in $\\{\"+\",\"*\",\"=\"\\}$ . At each state $s_{t}$ , only operators and numbers that appear in the cards are legal actions, and \u201cJ\u201d, $\\mathrm{{}^{\\circ}}\\mathrm{{Q}^{\\circ}}$ , or \u201cK\u201d are treated as ${}^{\\ldots}10^{\\ldots}$ . In particular, if the output text $\\pmb{v}_{t}^{\\mathrm{out}}$ is mapped to a legal action $a_{t}$ at state $s_{t}$ , the text version of $a_{t}$ will be appended to the \u201cformula\u201d in the current image of $s_{t}$ resulting $s_{t+1}$ , otherwise $s_{t+1}$ will remain the same as $s_{t}$ . ", "page_idx": 6}, {"type": "text", "text": "Points24. In this task, the goal is to output a formula using the numbers in the cards that evaluates to 24. The Points24 task is a harder version of EZPoints as it contains 4 cards, hence requiring the VLMs to generate a longer formula. The rules of Points24 are similar to EZPoints, despite two minor differences: the Points24 task requires the VLM to compute a target number of 24, and its action space contains more operators: $\\{\"+\",\"-\",\"*\",\"/\",\"=\"\\}$ . ", "page_idx": 6}, {"type": "text", "text": "Blackjack. In this task, the goal is to win the current blackjack game. At each state $s_{t}$ , the visual observation $o_{t}$ consists of two cards (one face-down) from the dealer and all cards from the player. ", "page_idx": 6}, {"type": "text", "text": "The agent\u2019s goal in this task is to win the current game, by outputting text $\\pmb{v}_{t}^{\\mathrm{{out}}}$ that can be mapped to $\\{\"\\mathtt{s t a n d}\",\"\\mathtt{h i t}\"\\}$ . The agent will receive one more card if $\\pmb{v}_{t}^{\\mathrm{out}}$ is mapped to \"hit\", and the game will terminate if $\\pmb{v}_{t}^{\\mathrm{{out}}}$ is mapped to \"stand\". ", "page_idx": 7}, {"type": "text", "text": "5.2 ALFWorld ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "While the gym_cards domain is designed to assess the VLM\u2019s arithmetic reasoning requiring finegrained visual recognition, the alfworld environment aims at testing VLM\u2019s decision-making tasks requiring visual semantic understanding. ", "page_idx": 7}, {"type": "text", "text": "ALFWorld. The ALFWorld embodied environment [58] is combines a text-based interactive environment [13] with a large vision-language instruction following dataset [57]. It contains 6 different types of goal-conditioned tasks (\u201cPick & Place\u201d, \u201cExamine in Light\u201d, \u201cClean & Place\u201d, \u201cHeat & Place\u201d, \u201cCool & Place\u201d, and \u201cPick Two & Place\u201d), and the agent\u2019s goal is to navigate in the environment via text-based actions (e.g., \"go to shelf 1\", \"examine sidetable 1\"). Unlike our original gym_cards environment, where all states share the same action space, the alfworld environment contains a state-dependent admissible action space \u2013 some actions are only available at certain states. For example, if the agent\u2019s goal is to \u201cput some pillows on armchair\u201d, then the agent can only put a pillow after picking up a pillow. Hence, to incorporate the state-dependent admissible action set, our prompt of alfworld asks the VLM to choose among an admissible action. See Figure 2 for an example of the visual observation of alfworld. We leave the detailed descriptions of the alfworld (state space, action space, reward functions, and the CoT prompt) to Appendix B.2. ", "page_idx": 7}, {"type": "text", "text": "6 Experimental Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "The first part of our experiment examines how our method improves the decision-making capabilities of VLMs (Section 6.1). The second part investigates the role of CoT reasoning in our method (Section 6.2). Details of our experimental setup are provided in Appendix C. ", "page_idx": 7}, {"type": "text", "text": "6.1 Improving VLM Decision-Making Capabilities ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Does our method improve the decision-making capabilities of VLM agents across various domains? To investigate this, we assess how our method improves arithmetic tasks requiring fine-grained visual recognition in the gym_cards domain and visual semantic reasoning in the alfworld domain. The gym_cards experiments include deterministic tasks (NumberLine, EZPoints, and Points24, each with increasing difficulty) and a stochastic task (Blackjack). In the alfworld domain, we evaluate overall performance and detailed task-specific performance as discussed in Section 5.2. We instantiate our method on top of the llava-v1.6-mistral-7b [35] model and compare it against commercial models (GPT4-V and Gemini), a supervised fine-tuned version of the llava-v1.6-mistral-7b model (LLaVAsft),5 and a vanilla RL implementation using a CNN-based policy network $(\\mathrm{CNN+RL})$ ).6 The final results and learning curves are presented in Table 2 and Figure 5. Details of the experimental setup are provided in Appendix C. ", "page_idx": 7}, {"type": "text", "text": "Enhancing decision-making capabilities of VLM agents across various tasks. As illustrated in Table 2 and Figure 5, our method demonstrates consistent improvement across various tasks, including deterministic (NumberLine and EZPoints)7 or stochastic (Blackjack) arithmetic tasks and visual semantic reasoning task (alfworld). Specifically, our method improves the average performance from the initial LLaVA-sft model by $27.1\\%$ on arithmetic tasks $18.4\\%\\rightarrow45.5\\%$ ) and ", "page_idx": 7}, {"type": "table", "img_path": "nBjmMF2IZU/tmp/01c2f0853d18b8b05cf76a2c1a7a7eae8bac680ab1fd0df56304fa6bebe23494.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "Table 2: Average episode success rates $(\\%)$ of different methods on gym_cards and alfworld. For all RL-based methods $\\scriptstyle\\mathrm{CNN+RL}$ and our method), we present the peak numbers (first 15k environment steps for the gym_cards and 5k environment steps for alfworld) from each training curve from Figure 5. We average the performance of all 4 tasks on gym_cards with equal weight. Due to the nature of the alfworld environment, where each subtask does not appear with equal probability, the average performance on alfworld is a weighted average among all types of tasks. We mark the $\\mathsf{B U T L E R}_{g}$ and BUTLER agent [58] in gray since they require expert data, while the remaining methods do not require expert data. As discussed by Shridhar et al. [58], the performance discrepancy between $\\mathsf{B U T L E R}_{g}$ and BUTLER happens due to different decoding strategies in evaluation strategies: ${\\tt B U T L E R}_{g}$ uses greedy decoding, which may repeat failed actions, whereas BUTLER employs beam search during evaluation. ", "page_idx": 8}, {"type": "image", "img_path": "nBjmMF2IZU/tmp/2979c0b786a67b484b46b3d9650a5a7f716d33e01a3a6df0215ddf115aa660ae.jpg", "img_caption": ["Figure 5: Episode success rates $(\\%)$ of different methods on gym_cards and alfworld during training. Left to right: gym_cards/Numberline, gym_cards/EZPoints, gym_cards/Blackjack, and alfworld (all). The curves of Points24 are not included because none of the tested methods achieve reasonable performance. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "${\\bf4.0\\,\\%}$ on visual semantic decision-making task $(17.7\\%\\rightarrow21.7\\%)$ ). In addition, our method also achieves the best performance among all comparative methods, surpassing the second-best method by $14.0\\%$ $\\scriptstyle\\mathrm{{(CNN+RL)}}$ ) on gym_cards and $2.3\\%$ (GPT4-V) on alfworld. ", "page_idx": 8}, {"type": "text", "text": "6.2 Understanding the Role of the CoT Reasoning ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "In Section 6.1, we have demonstrated that our method improves the arithmetic and visual semantic reasoning capabilities of VLM agents. Conceptually, our method can be viewed as an augmented version of the standard CNN-based RL, where the text output $[v^{\\mathrm{tht}},v^{\\mathrm{act}}]$ (from Figure 3) serve as the text action $v^{\\mathrm{act}}$ , augmented by CoT reasoning $v^{\\mathrm{tht}}$ . This raises an important question: How does the CoT reasoning $v^{\\mathrm{th\\overline{{t}}}}$ influence the overall performance of our method? To assess the impact of CoT reasoning on our method\u2019s performance, we conduct two sets of ablation experiments. The first set (presented in Table 3 and Figure 6) evaluates our method without the CoT reasoning, and the second part (shown in Figure 7) examines various scaling hyperparameters $\\lambda$ for the log-likelihood of CoT tokens, as defined in Equation 4.3. ", "page_idx": 8}, {"type": "table", "img_path": "nBjmMF2IZU/tmp/1f66a9d3249e653c9cf556b5fc51c8f7cca9cfb423de4c6e44fd3a13dabfca75.jpg", "table_caption": ["Table 3: Episode success rates $(\\%$ ) of our method with and without CoT reasoning. We report the best results from Figure 6 (first $15\\mathbf{k}$ environment steps for the gym_cards and $5\\mathbf{k}$ environment steps for alfworld). "], "table_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "nBjmMF2IZU/tmp/9ff6d036a34459a772a01cef7ad047cb88c6675f3f0f97523b6f9365c7a4810d.jpg", "img_caption": ["Figure 6: Training curves of our method without and without the CoT reasoning. Left to right: gym_cards/Numberline, gym_cards/EZPoints, gym_cards/Blackjack, and alfworld (all). The curves of Points24 are not included because none of the tested methods achieve reasonable performance. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "The crucial role of CoT reasoning in performance improvement. As presented in Table 3 and Figure 6, the performance of our method significantly decreases without the CoT reasoning.8 Besides the improvement in the final performance, CoT reasoning appears to be a crucial component for deterministic arithmetic tasks (NumberLine and EZPoints), as our method fails to improve these two tasks without the CoT reasoning. ", "page_idx": 9}, {"type": "text", "text": "The importance of moderate scaling factors $\\lambda$ . As discussed in Section 4.3, integrating CoT reasoning into our framework involves tuning an additional hyperparameter, $\\lambda\\in[0,1]$ (proposed in Equation 4.3). To identify an optimal range for $\\lambda$ , we conduct experiments assessing the impact of various $\\lambda$ . Our results in Figure 7 indicate that a moderate $\\lambda$ (between 0.3 and 0.5) enables effective training on the NumberLine task. Conversely, our method fails when $\\lambda$ is set too large $(\\geq0.7)$ or too small $(\\leq0.1)$ , and we empirically find that an optimal $\\lambda$ typically falls within 0.2 to 0.5. This is because a large $\\lambda$ results in an estimate of $\\log\\pi_{\\theta}(a_{t}|o_{t},\\pmb{v}_{t}^{\\mathrm{in}})$ being overly influenced by $\\log\\pi_{\\theta}\\big(v_{t}^{\\mathrm{{tht}}}\\vert o_{t},v_{t}^{\\mathrm{{in}}}\\big)$ , while a small $\\lambda$ value causes $\\pi_{\\theta}$ to be predominantly affected by $\\log\\pi_{\\theta}(v_{t}^{\\mathrm{act}}|o_{t},v_{t}^{\\mathrm{in}},v_{t}^{\\mathrm{tht}})$ , thereby reducing the effect of the CoT reasoning in RL training. ", "page_idx": 9}, {"type": "image", "img_path": "nBjmMF2IZU/tmp/9526ea7a999f8381d9eef9380dc5f5f565c179f901479dbcbb4b545d7897177e.jpg", "img_caption": ["Figure 7: Episode success rates $(\\%)$ of our method under different $\\lambda$ on NumberLine. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "7 Conclusions, Limitations, and Future Directions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, we introduce an algorithmic framework that directly fine-tunes VLMs using RL, with the help of the VLM\u2019s CoT reasoning capability. Empirical results demonstrate that our method can enhance the decision-making abilities of VLMs across diverse domains that require fine-grained visual recognition or visual semantic understanding. In addition, we demonstrate that CoT reasoning is a crucial component for enabling RL training, allowing 7b VLMs to outperform established commercial models such as GPT-4V and Gemini on most tasks. While our results suggest that CoT reasoning is crucial to the performance improvement of VLM training with RL, we have not extensively explored the effects of different prompting techniques in this work, which will be an interesting future direction. The performance gain of our method is also limited by the size of the action space and the difficulties of the task. For example alfworld does not enjoy as much performance gain as gym_cards, since alfworld is a multi-task environment and it has a much larger action space than gym_cards. ", "page_idx": 9}, {"type": "text", "text": "8 Acknowledgement ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We would like to thank William Chen, Kuan Fang, Aviral Kumar, Qiyang Li, Fangchen Liu, Oier Mees, Seohong Park, Karl Pertsch, Haozhi Qi, Chun-Hsiao Yeh, and Andrea Zanette for the early discussions and suggestions on the project. A.S. is partly supported by AI2 Young Investigator Grant, and a Gemma Academic Program Award. S.X. is partly supported by an Amazon research award and the Google TRC program. This research was supported by NSF RI IIS-2246811, AFOSR FA9550-22-1-0273, the joint Simons Foundation-NSF DMS grant #2031899, the ONR grant N00014- 22-1-2102, Tsinghua Berkeley Shenzhen Institute (TBSI) Research Fund, and the Hong Kong Center for Construction Robotics Limited (HKCRC) Award 052245. We would also like to thank Hyperbolic Labs for the computing support. ", "page_idx": 9}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Marwa Abdulhai, Isadora White, Charlie Snell, Charles Sun, Joey Hong, Yuexiang Zhai, Kelvin Xu, and Sergey Levine. Lmrl gym: Benchmarks for multi-turn reinforcement learning with language models. arXiv preprint arXiv:2311.18232, 2023.   \n[2] Alekh Agarwal, Nan Jiang, Sham M Kakade, and Wen Sun. Reinforcement learning: Theory and algorithms. CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep, 32, 2019.   \n[3] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425\u20132433, 2015.   \n[4] Kate Baumli, Satinder Baveja, Feryal Behbahani, Harris Chan, Gheorghe Comanici, Sebastian Flennerhag, Maxime Gazeau, Kristian Holsheimer, Dan Horgan, Michael Laskin, et al. Visionlanguage models as a source of rewards. arXiv preprint arXiv:2312.09187, 2023.   \n[5] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253\u2013279, jun 2013.   \n[6] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemys\u0142aw De\u02dbbiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.   \n[7] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.   \n[8] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.   \n[9] Thomas Carta, Cl\u00e9ment Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and PierreYves Oudeyer. Grounding large language models in interactive environments with online reinforcement learning. In International Conference on Machine Learning, pages 3676\u20133713. PMLR, 2023.   \n[10] Louis Castricato, Alex Havrilla, Shahbuland Matiana, Duy V. Phung, Aman Tiwari, Jonathan Tow, and Maksym Zhuravinsky. trlX: A scalable framework for RLHF, June 2023. URL https://github.com/CarperAI/trlx.   \n[11] William Chen, Oier Mees, Aviral Kumar, and Sergey Levine. Vision-language models provide promptable representations for reinforcement learning. arXiv preprint arXiv:2402.02651, 2024.   \n[12] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.   \n[13] Marc-Alexandre C\u00f4t\u00e9, Akos K\u00e1d\u00e1r, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, et al. Textworld: A learning environment for text-based games. In Computer Games: 7th Workshop, CGW 2018, Held in Conjunction with the 27th International Conference on Artificial Intelligence, IJCAI 2018, Stockholm, Sweden, July 13, 2018, Revised Selected Papers 7, pages 41\u201375. Springer, 2019.   \n[14] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning. arXiv preprint arXiv:2301.00234, 2022.   \n[15] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. Advances in Neural Information Processing Systems, 35:18343\u201318362, 2022.   \n[16] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et al. Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023.   \n[17] Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting for multi-step reasoning. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=yf1icZHC-l9.   \n[18] DeepMind Google. Introducing gemini: our largest and most capable ai model, 2023. URL https://blog.google/technology/ai/google-gemini-ai/.   \n[19] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pages 1861\u20131870. PMLR, 2018.   \n[20] Joey Hong, Sergey Levine, and Anca Dragan. Zero-shot goal-directed dialogue via rl on imagined conversations. arXiv preprint arXiv:2311.05584, 2023.   \n[21] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview. net/forum?id $\\equiv$ nZeVKeeFYf9.   \n[22] Jiaxing Huang, Jingyi Zhang, Kai Jiang, Han Qiu, and Shijian Lu. Visual instruction tuning towards general-purpose multimodal model: A survey. arXiv preprint arXiv:2312.16602, 2023.   \n[23] Shengyi Huang, Quentin Gallou\u00e9dec, Florian Felten, Antonin Raffin, Rousslan Fernand Julien Dossa, Yanxiao Zhao, Ryan Sullivan, Viktor Makoviychuk, Denys Makoviichuk, Mohamad H Danesh, et al. Open rl benchmark: Comprehensive tracked experiments for reinforcement learning. arXiv preprint arXiv:2402.03046, 2024.   \n[24] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. Voxposer: Composable 3d value maps for robotic manipulation with language models. In 7th Annual Conference on Robot Learning, 2023. URL https://openreview.net/forum?id= 9_8LF30mOC.   \n[25] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.   \n[26] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199\u201322213, 2022.   \n[27] Ilya Kostrikov. Pytorch implementations of reinforcement learning algorithms. https:// github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail, 2018.   \n[28] Heinrich K\u00fcttler, Nantas Nardelli, Alexander Miller, Roberta Raileanu, Marco Selvatici, Edward Grefenstette, and Tim Rockt\u00e4schel. The nethack learning environment. Advances in Neural Information Processing Systems, 33:7671\u20137684, 2020.   \n[29] Chunyuan Li. Large multimodal models: Notes on cvpr 2023 tutorial. arXiv preprint arXiv:2306.14895, 2023.   \n[30] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and Jianfeng Gao. Multimodal foundation models: From specialists to general-purpose assistants. arXiv preprint arXiv:2309.10020, 1(2):2, 2023.   \n[31] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let\u2019s verify step by step. arXiv preprint arXiv:2305.20050, 2023.   \n[32] Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. Hallusionbench: You see what you think? or you think what you see? an imagecontext reasoning benchmark challenging for gpt-4v (ision), llava-1.5, and other multi-modality models. arXiv preprint arXiv:2310.14566, 2023.   \n[33] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023.   \n[34] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https: //openreview.net/forum?id=w0H2xGHlkw.   \n[35] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024.   \n[36] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.   \n[37] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023.   \n[38] Chaochao Lu, Chen Qian, Guodong Zheng, Hongxing Fan, Hongzhi Gao, Jie Zhang, Jing Shao, Jingyi Deng, Jinlan Fu, Kexin Huang, et al. From gpt-4 to gemini and beyond: Assessing the landscape of mllms on generalizability, trustworthiness and causality through four modalities. arXiv preprint arXiv:2401.15071, 2024.   \n[39] Marlos C. Machado, Marc G. Bellemare, Erik Talvitie, Joel Veness, Matthew J. Hausknecht, and Michael Bowling. Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. Journal of Artificial Intelligence Research, 61:523\u2013562, 2018.   \n[40] Manolis Savva\\*, Abhishek Kadian\\*, Oleksandr Maksymets\\*, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra. Habitat: A Platform for Embodied AI Research. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019.   \n[41] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529\u2013533, 2015.   \n[42] Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and Ping Luo. Embodiedgpt: Vision-language pre-training via embodied chain of thought. arXiv preprint arXiv:2305.15021, 2023.   \n[43] OpenAI. Gpt-4, 2023. URL https://openai.com/research/gpt-4.   \n[44] OpenAI. Gpt-4v, 2023. URL https://openai.com/research/gpt-4v-system-card.   \n[45] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.   \n[46] Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, and Alane Suhr. Autonomous evaluation and refinement of digital agents. arXiv preprint arXiv:2404.06474, 2024.   \n[47] Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. arXiv preprint arXiv:2308.03188, 2023.   \n[48] Joon Sung Park, Joseph O\u2019Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, pages 1\u201322, 2023.   \n[49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[50] Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kiant\u00e9 Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, and Yejin Choi. Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for natural language policy optimization. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id $\\equiv$ 8aHzds2uUyB.   \n[51] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 3505\u20133506, 2020.   \n[52] Juan Rocamonde, Victoriano Montesinos, Elvis Nava, Ethan Perez, and David Lindner. Visionlanguage models are zero-shot reward models for reinforcement learning. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=N0I2RtD8je.   \n[53] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International conference on machine learning, pages 1889\u20131897. PMLR, 2015.   \n[54] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. In Proceedings of the International Conference on Learning Representations (ICLR), 2016.   \n[55] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   \n[56] Bokui Shen, Fei Xia, Chengshu Li, Roberto Mart\u00edn-Mart\u00edn, Linxi Fan, Guanzhi Wang, Claudia P\u00e9rez-D\u2019Arpino, Shyamal Buch, Sanjana Srivastava, Lyne Tchapmi, et al. igibson 1.0: A simulation environment for interactive tasks in large realistic scenes. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 7520\u20137527. IEEE, 2021.   \n[57] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10740\u201310749, 2020.   \n[58] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. {ALFW}orld: Aligning text and embodied environments for interactive learning. In International Conference on Learning Representations, 2021. URL https: //openreview.net/forum?id $\\equiv$ 0IOX0YcCdTn.   \n[59] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484\u2013489, 2016.   \n[60] Charlie Victor Snell, Ilya Kostrikov, Yi Su, Sherry Yang, and Sergey Levine. Offline RL for natural language generation with implicit language q learning. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=aBH_DydEvoH.   \n[61] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008\u20133021, 2020.   \n[62] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023.   \n[63] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.   \n[64] Andrew Szot, Max Schwarzer, Harsh Agrawal, Bogdan Mazoure, Walter Talbott, Katherine Metcalf, Natalie Mackraz, Devon Hjelm, and Alexander Toshev. Large language models as generalizable policies for embodied tasks. arXiv preprint arXiv:2310.17722, 2023.   \n[65] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. arXiv preprint arXiv:2401.06209, 2024.   \n[66] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.   \n[67] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   \n[68] Mark Towers, Jordan K. Terry, Ariel Kwiatkowski, John U. Balis, Gianluca de Cola, Tristan Deleu, Manuel Goul\u00e3o, Andreas Kallinteris, Arjun KG, Markus Krimmel, Rodrigo PerezVicente, Andrea Pierr\u00e9, Sander Schulhoff, Jun Jet Tai, Andrew Tan Jin Shen, and Omar G. Younis. Gymnasium, March 2023. URL https://zenodo.org/record/8127025.   \n[69] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha\u00ebl Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350\u2013354, 2019.   \n[70] Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, and Shengyi Huang. Trl: Transformer reinforcement learning. https://github. com/huggingface/trl, 2020.   \n[71] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023.   \n[72] Ruoyao Wang, Peter Alexander Jansen, Marc-Alexandre C\u00f4t\u00e9, and Prithviraj Ammanabrolu. Scienceworld: Is your agent smarter than a 5th grader? In Conference on Empirical Methods in Natural Language Processing, 2022. URL https://api.semanticscholar.org/CorpusID: 247451124.   \n[73] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id $=$ 1PL1NIMMrw.   \n[74] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560, 2023.   \n[75] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824\u201324837, 2022.   \n[76] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864, 2023.   \n[77] Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, Pieter Abbeel, and Dale Schuurmans. Foundation models for decision making: Problems, methods, and opportunities. arXiv preprint arXiv:2303.04129, 2023.   \n[78] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Grifftihs, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023.   \n[79] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview. net/forum?id=WE_vluYUL-X.   \n[80] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023.   \n[81] Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong Jae Lee, and Yi Ma. Investigating the catastrophic forgetting in multimodal large language model fine-tuning. In Conference on Parsimony and Learning, pages 202\u2013227. PMLR, 2024.   \n[82] Denny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, and Ed H. Chi. Least-to-most prompting enables complex reasoning in large language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id $=$ WZH7099tgfM.   \n[83] Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and Aviral Kumar. Archer: Training language model agents via hierarchical multi-turn rl. arXiv preprint arXiv:2402.19446, 2024.   \n[84] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.   \n[85] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "A Contributions ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "\u2022 YXZ: proposed, led, and managed the project; integrated all code bases; ran all ablations for method development; babysat all experiments; implemented the post-processing function $f$ ; proposed and implemented the scaling factor $\\lambda$ for action tokens; beautified the gym_cards environment; maintained all codebases; wrote the major part of the paper.   \n\u2022 HB: set up the infrastructure and initial experiments for supervised fine-tuning before RL training; maintained all codebases; partially wrote the paper.   \n\u2022 ZL: set up the alfworld environment; set up major infrastructures for data collection; maintained all codebases; partially wrote the paper.   \n\u2022 JP: proposed the CoT idea for end-to-end RL training; optimized the RL training framework with quantization and enabled distributed training; implemented the initial version of the gym_cards environment; partially wrote the paper.   \n\u2022 ST: maintained the usage of LLaVA repo [34, 33, 35]; implemented the queries for GPT4-V and Gemini; partially wrote the paper.   \n\u2022 YFZ: implemented the initial version of RL training on LLaVA; partially wrote the paper.   \n\u2022 AS, SX, YL, YM, SL: provided suggestions for the project. AS, SX, SL also provided feedbacks on writing. YM, SL inspired YXZ to initiate the entire project. ", "page_idx": 15}, {"type": "text", "text": "B Additional Details of the Evaluation Tasks ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1 Gym Cards ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "B.1.1 NumberLine ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "State and action space. In the NumberLine task, the visual observation at each state $s_{t}$ contains two lines of text: \u201cTarget: $x'^{\\star}$ and \u201cCurrent: $y_{t}^{\\,,}\\,$ , where $x,y_{t}$ are both integers such that $x,y_{t}\\in[0,n_{\\operatorname*{max}}]$ , where $n_{\\mathrm{max}}$ is an environment input variable that controls the maximum position of the numbers. The goal is to move the current number $y_{t}$ to the target number $x$ , by sequentially choosing actions from the discrete action space $\\{\\,\"\\,+\"\\,,\\,\"\\,-\"\\,\\}$ . We set $n_{\\mathrm{max}}\\,=5$ for all experiments in this work, but $n_{\\mathrm{max}}$ can be set to any positive integers. Choosing $\"+\"$ or \"\u2212\" will increase or decrease the current number $y_{t}$ by 1, respectively, and the agent will stay at the boundary if it takes an action that attempts to cross the boundary (e.g., taking $a_{t}=\"+\"$ when $y_{t}=n_{\\operatorname*{max}}$ or $a_{t}=\"-\"$ when $x_{t}=0$ ). See an example of the state action transition in Figure 8. ", "page_idx": 15}, {"type": "image", "img_path": "nBjmMF2IZU/tmp/61630497640cc2086a11547861788f92aaba0cc9f4c3ac02eac968bd219938a7.jpg", "img_caption": ["Figure 8: An example of the transition in NumberLine. "], "img_footnote": [], "page_idx": 15}, {"type": "text", "text": "Reward functions and the CoT prompts. An episode in NumberLine ends when the current number equals the target number or the maximum step $T=2n_{\\mathrm{max}}$ is reached. The agent receives a terminal reward of $\\bar{r}(s_{t},a_{t})\\,=\\,1$ when $y_{t+1}=x$ . The agent also receives a reward penalty of $r(s_{t},a_{t})=-1$ upon taking an incorrect action that does not result in a closer position to the target $(|x-y_{t}|\\geq|x-y_{t+1}|)$ , otherwise the agent receives reward $r(s_{t},a_{t})=0$ . In the example provided above (Figure 8), the agent receives a reward $r\\,=\\,0$ , since it moves closer to the target, but not reaching the target yet. For the NumberLine task, we adopt the following CoT prompt in Figure 9, and for the case without $\\mathrm{CoT}$ reasoning (discussed in Section 6.2), we use the same prompt but without the blue CoT reasoning parts. ", "page_idx": 15}, {"type": "image", "img_path": "nBjmMF2IZU/tmp/43910b892cbc69bcc1662ded8294c21f5bc1eef897926875fad71d272ea51ead.jpg", "img_caption": ["Figure 9: Task-specific CoT prompt input ${\\pmb v}_{t}^{\\mathrm{in}}$ for NumberLine. The blue part represents the CoT reasoning and the red part is the text-based action. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "B.1.2 EZPoints ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "State and action space. In the EZPoints task, the agent will observe an image of two cards and a text version of \u201cformula\u201d below the cards, at each state $s_{t}$ . The goal is to use the cards in the image to compute a target number of 12 and we view $\\{\"\\mathbb{J}\",\"\\mathbb{Q}\",\"\\mathbb{K}\"\\}$ as $\"10\"$ . The action space of EZPoints is $\\{\"1\",\"2\",...,\"10\",\"+\",\"*\",\"=\"|\\}$ and each number in the cards can only be used once. Any action attempting to either select a number not shown in the cards or use a card more than once are illegal. At $s_{t}$ , if a legal action $a_{t}$ is taken, the action will be appended to the text \u201cformula\u201d in $s_{t}$ and becomes the next state $s_{t+1}$ . On the other hand, when an illegal action is taken, $s_{t+1}$ will remain the same as $s_{t}$ . All images generated from the EZPoints environment are guaranteed to have a viable solution for computing 12. ", "page_idx": 16}, {"type": "image", "img_path": "nBjmMF2IZU/tmp/21f843d7a2c822185d4357379d8b90b5bbbb41a68051ca1a784b40fac582fc97.jpg", "img_caption": ["Figure 10: An example of the transition in EZPoints. "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Reward functions and the CoT prompts. An episode terminates when $\"=\"$ is taken or the maximum step $T=5$ is reached. The agent receives a reward of $r=-1$ upon taking an illegal action, and $r=0$ while taking a legal action. When $\"=\"$ is taken, the agent will receive a positive reward $r=10$ if the formula equals 12, and $r=-1$ otherwise. For the EZPoints task, we adopt the following CoT prompt in Figure 11, and for the case without CoT reasoning (discussed in Section 6.2), we use the same prompt but without the blue CoT reasoning parts and the brown part in Figure 11 is the text version of the current formula directly extracted from the current state $s_{t}$ . ", "page_idx": 16}, {"type": "image", "img_path": "nBjmMF2IZU/tmp/1b65232253f1f09aaa553cece1be1ea187531562a31f7de7f7c3af909d0622d7.jpg", "img_caption": ["Figure 11: Task-specific CoT prompt input ${\\pmb v}_{t}^{\\mathbf i\\mathbf n}$ for EZPoints given the observation in Figure 10. The blue part represents the CoT reasoning, the red part is the text-based action, and the brown part is the state-dependent text from the formula in the image. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "B.1.3 Points24 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "State and action space. Similar to EZPoints, the goal of Points24 is also to generate a formula to compute the target number of 24, using all four cards. Points24 has a slightly larger action space: $\\{\"1\",\"2\",...,\"10\",\"+\",\"-\",\"\"*\",\",\"$ , \"(\", \")\", $\"=\"\\}$ and two more cards. Each number in the cards can only be used once. Similar to EZPoints, any action attempting to either select a number not shown in the cards or use a card more than once are illegal. At $s_{t}$ , if a legal action $a_{t}$ is taken, the action will be appended to the text \u201cformula\u201d in $s_{t}$ and becomes the next state $s_{t+1}$ . When an illegal action is taken, $s_{t+1}$ will remain the same as $s_{t}$ . Different from EZPoints where all images are guaranteed to have a viable solution for computing 12, the images generated by Points24 do not always have a viable solution to 24. ", "page_idx": 17}, {"type": "image", "img_path": "nBjmMF2IZU/tmp/fca1d4046bd60d215a34d18462ae9335e76a3a30bcce135a9a206a0a165a491c.jpg", "img_caption": ["Figure 12: An example of the transition in Points24. "], "img_footnote": [], "page_idx": 17}, {"type": "text", "text": "Reward functions and the CoT prompts. The reward functions and termination conditions of Points24 are the same as those in EZPoints. An episode terminates when $\"=\"$ is taken or the maximum step $T=20$ is reached. The agent receives a reward of $r=-1$ upon taking an illegal action, and $r=0$ while taking legal actions. When $\"=\"$ is taken, the agent will receive a positive reward $r\\,=\\,10$ when the formula equals 24, and $r=-1$ otherwise. For the Points24 task, we adopt the following CoT prompt in Figure 13, and for the case without CoT reasoning (discussed in Section 6.2), we use the same prompt but without the blue CoT reasoning parts and the brown part in Figure 13 is the text version of the current formula directly extracted from the current state $s_{t}$ . We also provide an additional feature that allows us to view $\\{\\,\"\\,\\mathsf{J}\\,\"\\,,\\quad\\mathbb{Q}\\,\"\\,,\\quad\"\\,\\mathbb{K}^{\"}\\,\\}$ as $\\{\"11\",\"12\"$ , $\"13\"\\}$ , instead of $\\{\"10\"\\}$ . ", "page_idx": 17}, {"type": "image", "img_path": "nBjmMF2IZU/tmp/9a8c034974298f00e1dbcae44ca265f94597145907adb2dfc1e036b7a15f34b3.jpg", "img_caption": ["Figure 13: Task-specific CoT prompt input ${\\pmb v}_{t}^{\\mathrm{in}}$ for Points24 given the observation in Figure 12. The blue part represents the CoT reasoning and the red part is the text-based action, brown part is the state-dependent text that directly obtained from the formula in the image. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "B.1.4 Blackjack ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "State and action space. For the Blackjack task, the visual observation at state $s_{t}$ consists of two cards (one face-down) from the dealer and all cards from the player. The agent\u2019s goal in this task is to win the current game, by choosing actions in $\\{\"\\mathtt{s t a n d}\",\"\\mathtt{h i t}\"\\}$ . The agent will receive a new card upon choosing \"hit\". See Figure 14 for an example transition. ", "page_idx": 18}, {"type": "image", "img_path": "nBjmMF2IZU/tmp/68fce93ca0e14c2c3834fe4f19433382aa6982ed4a94a93491cf42c58507cb69.jpg", "img_caption": ["Figure 14: An example of the transition in Blackjack. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "Reward functions and the CoT prompts. The game terminates when the player chooses \"stand\" or busts (total points exceed 21). We adopt the same reward function as the Blackjack-v1 task in Gymnasiym [68], where $r(s_{t},a_{t})=1,0,-1$ upon win, draw, and loss, respectively. We also provide a similar feature as Gymnasium [68], where the \u201cblackjack\u201d winning (the agent win with an $\"A\"$ and a $\"10\"$ , \"J\", $\"\\mathbf{Q}\"$ or $\"\\mathrm{K}\"$ ) reward $r$ of the player will become 1.5. In the example provided in Figure 14, the game has not terminated after taking the action \"hit\", hence the agent will not receive any rewards, even though it has total points of 21. For the Blackjack task, we adopt the following CoT prompt in Figure 15, and for the case without CoT reasoning (discussed in Section 6.2), we use the same prompt but without the blue CoT reasoning parts. ", "page_idx": 18}, {"type": "image", "img_path": "nBjmMF2IZU/tmp/1eb3db5667b625d72baf65061a8b60e9877a1d939dd472bdf8227eac5a45f555.jpg", "img_caption": ["Figure 15: Task-specific CoT prompt input ${\\pmb v}_{t}^{\\mathbf i\\mathbf n}$ for Blackjack. The blue part represents the CoT reasoning and the red part is the text-based action. "], "img_footnote": [], "page_idx": 18}, {"type": "text", "text": "State and action space. Inherited from Text World [13], at each state $s_{t}$ of alfworld, the agent will observe an RGB image and text-based description. The action space of alfworld can be summarized these following format [58]: (1) goto {recep}; (2) take {obj} from {recep}; (3) put {obj} in/on {recep}; (4) open {recep}; (5) close {recep}; (6) toggle {obj}{recep}; (7) clean {obj} with {recep}; (8) heat {obj} with {recep}; (9) cool {obj} with {recep}, where {obj} and {recep} stands for objects and receptacles. See Figure 16 for an example of the state action transition in the alfworld environment. ", "page_idx": 19}, {"type": "image", "img_path": "nBjmMF2IZU/tmp/c3aa7453a2b21350356f028912192a74f783932805354c564e1706c4ec42f9e1.jpg", "img_caption": ["Figure 16: An example of the transition in alfworld. "], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Reward functions and the CoT prompts. Each state $s\\in S$ of alfworld has a set of admissible actions $A_{\\sf a d m}(s)$ , a final goal $g_{\\tt t a s k}$ , and subgoals $g_{\\mathtt{s u b}}$ . Since the goal of alfworld is to complete the language-based goal-conditioned tasks, we reward the agent upon reaching subgoals and completing the task, while penalizing the agent upon taking inadmissible actions. To summarize, we define the reward function of alfworld as $r(s_{t},a_{t},s_{t+1}|{g}_{\\tt t a s k})=50*{\\bf1}\\left\\{s_{t+1}={g}_{\\tt t a s k}\\right\\}+{\\bf1}\\left\\{s_{t+1}={g}_{\\tt s u b}\\right\\}-$ $\\mathbf{1}\\left\\{a_{t}\\not\\in{\\mathcal{A}}_{\\mathtt{a d m}}(s_{t})\\right\\}$ . For the alfworld task, we adopt the following CoT prompt in Figure 17, and for the case without CoT reasoning (discussed in Section 6.2), we use the same prompt but without the blue CoT reasoning parts and the brown part in Figure 17 is the text description of the task directly extracted from the current state st. ", "page_idx": 19}, {"type": "image", "img_path": "nBjmMF2IZU/tmp/20a6508563c97de4a47c41c756dc1069568474089d3a1fccd3db06c45b3e9a1d.jpg", "img_caption": ["Figure 17: Task-specific CoT prompt input ${\\pmb v}_{t}^{\\mathbf i\\mathbf n}$ for alfworld given the observation in Figure 16. The blue part represents the CoT reasoning and the red part is the text-based action, brown part is the state-dependent text that directly obtained from the text description and the admissible actions of the current state. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "C Additional Details on the Experiments ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We provide additional detailed of the experimental results in Section 6 here. Details of our experimental pipeline is provided in Section C.1, including preparing the initial SFT checkpoints and the RL training. Section C.2 contains details setup of all comparative methods. We list task-specific training details in Section C.3. We provide additional experimental results in Section C.4. Section C.5 lists several failure examples of the Points24 tasks. ", "page_idx": 20}, {"type": "text", "text": "C.1 Experimental Pipeline ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Our experiments adopt a similar pipeline as RLHF [45], where we first apply supervised fine-tuning (SFT) to the backbone llava-v1.6-mistral-7b model, before RL training. As outlined by Ouyang et al. [45], the RLHF training procedure consists of three distinct stages: SFT, learning reward models from human preference data, and applying RL with the learned reward models. Our pipeline is analogous to RLHF but without requiring the collection of human preference data for learning reward models, as we can directly collect rewards from the environment.9Consequently, our experimental pipeline only contains two stages: SFT and RL, which we will explain below. ", "page_idx": 20}, {"type": "text", "text": "Supervised fine-tuning. For the original gym_cards environment, we manually construct instruction-following data for all tasks following the format specified in Figure 3 of Section 4.1. As for alfworld, we use GPT4-V [44] to collect instruction following data for SFT. For all tasks, we prepare two versions of the instruction-following data, one with CoT and one without. We leave the details of the CoT prompts for each task, and the details of each fine-tuning dataset in Appendix D. After constructing the instruction-following data (with and without CoT), we finetune llava-v1.6-mistral-7b for 1 epoch on the collected data for each task and report the results for LLaVA-sft. ", "page_idx": 20}, {"type": "text", "text": "RL training. For each task, we start our RL training from the LLaVA-sft checkpoint. The LLaVA model [34] consists of three jointly trainable components, a CLIP vision encoder [49], an LLM backbone [66, 67, 25], and an MLP projector that connects visual features and the word embeddings, and we directly apply PPO [55] to train all three components. Due to computation resource limitations, we instantiate our experiments via LoRA [21], with the LoRA configuration of $r\\,=\\,128,\\alpha\\,=$ 256, dropout $=0.05$ , for all trainable components. For the CoT coefficient $\\lambda$ , we set $\\lambda=0.5$ in the gym_cards domain and $\\lambda=0.2$ in alfworld. ", "page_idx": 20}, {"type": "text", "text": "", "page_idx": 21}, {"type": "text", "text": "C.2 Experimental Setup for Comparative Methods ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "GPT4-V and Gemini. All of our experimental results on GPT4-V [44] and Gemini [18] are tested on March 15, 2024, using the same prompt for our RL training (see detailed prompts in Appendix D). For gym_cards, the numbers from both GPT4-V and Gemini are averaged among the same number of episodes: 200 episodes for deterministic tasks (NumberLine, EZPoints and Points24); 1000 episodes for stochastic task (Blackjack). As for alfworld, we report the performance of GPT4-V on all 1000 episodes we collected, see Appendix D.5 for our data collection on alfworld using GPT4-V. Due to the financial budget, we report the results of Gemini using 100 episodes. ", "page_idx": 21}, {"type": "text", "text": "LLaVA-sft. For each number of LLaVA-sft, we first collect the instruction-following dataset for each task and then fine-tune LLaVA-1.6-7b for 1 epoch on the collected data using the official LLaVA fine-tuning script.10 Details of our data collection process is provided in Appendix D. We also use the same LLaVA-sft checkpoint as initializations for the downstream RL training. ", "page_idx": 21}, {"type": "text", "text": "CNN-based RL. Since the LLaVA-7b model adopts a CLIP ViT-L/14 vision encoder which is more powerful than vanilla CNN embeddings, we instantiate our CNN-based method using the feature from the same CLIP ViT-L/14 for a fair comparison. For tasks (EZPoints, Points24, and alfworld, see our detailed prompt in Appendix D) that require text inputs, we adopt the RoBERTa-base [36] model to encode the text feature and concatenate the text and CLIP visual features for downstream RL training. After obtaining the CLIP (potentially concatenated with text) features, we adopt 2 MLP layers followed by a fully connected layer to map the clip features into the action space. We adopt the PPO [55] implementation from Kostrikov [27] as the backbone RL algorithm. In addition, we adopt a CosineAnnealingLR learning rate scheduler, with the initial learning rate of $3e-4$ , the final learning rate of $1e-8$ , and the maximum learning rate step of 25. The remaining task specific hyperparameters are the same as the VLM case in Section C.3. ", "page_idx": 21}, {"type": "text", "text": "C.3 General Setup for End-to-End RL Training ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "All experiments are conducted on an 8 A100s DGX machine (80G), while the maximum VRAM requirement is $<40\\mathbf{G}$ . Each curve from Figure 5 and 6 takes at most 36 hours to finish. We adopt DeepSpeed zero2 [51] for multi-gpu training. During our training for the VLM, we directly train all trainable components (vision encoder, LLM, and the MLP projector). We adopt an open-source implementation [27] for the PPO. Inspired by von Werra et al. [70], Castricato et al. [10], we apply a 3-layer MLP as the value head, on top of the output hidden states layer before the output tokens, to estimate the value function $V^{\\pi_{\\theta}}$ . After obtaining the value estimate $V_{\\phi}$ , we adopt the generalized advantage estimator (GAE) [54] to estimate the return function ${\\hat{R}}(s)$ and the advantage function $\\hat{A}^{\\pi_{\\theta}}$ of $\\pi_{\\theta}$ . In addition, we adopt a CosineAnnealingLR learning rate scheduler, with the initial learning rate of $1e\\mathrm{~-~}5$ , the final learning rate of $1e-9$ , and the maximum learning rate step of 25. For all experiments in the gym_cards and alfworld environment, we set the scaling hyperparameter $\\lambda=0.5,0,2$ , respectively. The learning rate decay happens after every PPO update, which consists of 4 epochs of gradient updates with PPO. The number of data for on-policy training and batch size is task-dependent, we list them below. ", "page_idx": 21}, {"type": "text", "text": "Numberline and Blackjack. For NumberLine and Blackjack, our VLM training curves in Figure 5 use 4 GPUs. Our implementation naturally enables different random seeds on different GPUs, hence our VLM curves are averaged among 4 seeds. For one PPO update on each GPU, we collect 512 transitions, with a batch size of 128 per GPU (batch size $=512$ in total). The episode return and success rate are averaged with NumberLine, Blackjack are averaged among 200 and 1000 episodes, respectively. We averaged the return of Blackjack on more episodes because ", "page_idx": 21}, {"type": "text", "text": "Blackjack contains stochastic while NumberLine is a deterministic task. We adopt the same number of transitions and batch size for the on-policy training in the CNN-based method on both tasks. The CNN-based methods are averaged among 4 random seeds as well. ", "page_idx": 22}, {"type": "text", "text": "EZPoints and Points24. For EZPoints and Points24, our VLM training curves in Figure 5 use 4 GPUs. Our implementation naturally enables different random seeds on different GPUs, hence our VLM curves are averaged among 4 seeds. For one PPO update on each GPU, we collect 1024 transitions, with a batch size of 128 per GPU (batch $\\mathrm{size}=512$ in total). We use 1024 transitions because the episodes of EZPoints and Points24 usually have longer horizons than NumberLine and Blackjack. The episode return and success rate are averaged with EZPoints and Points24 are averaged among 200. We adopt the same number of transitions and batch size for the on-policy training in the CNN-based method on both tasks. The CNN-based methods are averaged among 4 random seeds as well. ", "page_idx": 22}, {"type": "text", "text": "ALFWorld. For the alfworld environment, each run of our VLM training curves in Figure 5 and Figure 19 are conducted on one GPU, and each curve is averaged among 4 seeds. We do not conduct multi-GPU training for alfworld because the on-policy sampling time has a huge variance on different GPUs, which will largely increase the synchronization time across different GPUs. For each PPO update, we collect 1024 transitions, and with a batch size of 256. The episode success rates are averaged among 200 episodes. We adopt the same number of transitions and batch size for the on-policy training in the CNN-based method on both tasks. The CNN-based methods are averaged among 4 random seeds as well. ", "page_idx": 22}, {"type": "text", "text": "C.4 Additional Experimental Results ", "text_level": 1, "page_idx": 22}, {"type": "text", "text": "We provide some additional experimental results on the episode returns on the gym_cards and the task-specific training curves for alfworld here. ", "page_idx": 22}, {"type": "table", "img_path": "nBjmMF2IZU/tmp/2f8928b80f15df4dde500b90ffebdbddbbc1c7e26d79d87a0c62c4289c2ff8c7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "nBjmMF2IZU/tmp/25185c999a5a9c521f424b0dee49e243921ca9a349da892211862d0f30d31f3b.jpg", "img_caption": ["Table 4: Average episode success rates and returns of different methods on gym_cards. For all RL-based methods (CNN and our method), we report the best results in each training curve from Figure 18. ", "Figure 18: Episode returns of different methods on gym_cards. An extended version of Figure 5 containing episode success rates and returns. "], "img_footnote": [], "page_idx": 22}, {"type": "image", "img_path": "nBjmMF2IZU/tmp/be82a1b144ffbc6c0e4a866e0e9a1331219c2d18d5d5a320c4c2354b3aa1d8a8.jpg", "img_caption": ["Figure 19: Episode success rates our method on different types of tasks from alfworld. The curves come from the same run as experiments from the alfworld figure of Figure 5. "], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "C.5 Failure Examples of Points24 ", "text_level": 1, "page_idx": 23}, {"type": "image", "img_path": "nBjmMF2IZU/tmp/dc8e426336e9bf8fb58464498ee6a09dc7bf10360597302da65e5b9932d1e0bf.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 20: Failure example of Points24 due to incorrect perception. In this example, the VLM fails to recognize the four numbers from the cards correctly as it only recognizes 2 numbers correctly. The reasoning part is correct, as it indeed generates a formula that equals 24. ", "page_idx": 23}, {"type": "image", "img_path": "nBjmMF2IZU/tmp/ad1b25d57d47849250ec2b8c6200d4bd8ce1a440e4c58b030e692dbaf4a1769f.jpg", "img_caption": [], "img_footnote": [], "page_idx": 23}, {"type": "text", "text": "Figure 21: Failure example of Points24 due to incorrect perception and reasoning. In this example, the VLM fails to recognize the four numbers from the cards correctly as it only recognizes 3 numbers correctly. The reasoning part is also incorrect, $_{10+10+5+7}$ does not equal 24. ", "page_idx": 23}, {"type": "text", "text": "D Details on Supervised Fine-tuning Data ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In this section, we provide detailed descriptions of the supervised fine-tuning data for all tasks. ", "page_idx": 24}, {"type": "text", "text": "D.1 NumberLine ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "In the NumberLine task, we collect $20\\mathbf{k}$ instruction-following expert data samples using a task solver.   \nSee examples with and without CoT in Figure 22 and 23, respectively. ", "page_idx": 24}, {"type": "image", "img_path": "nBjmMF2IZU/tmp/7f7c314d2d916c6431014a4cec04dcf18351ba38a7499262ac8425dc8bf9bd4a.jpg", "img_caption": [], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "Figure 22: An example of the supervised fine-tuning data for NumberLine with CoT. ", "page_idx": 24}, {"type": "image", "img_path": "nBjmMF2IZU/tmp/5905f37fba3cbe24946a6c82c728ca54c7bee818e31c788afba6941e5c5a1dab.jpg", "img_caption": ["Figure 23: An example of the supervised fine-tuning data for NumberLine without CoT. "], "img_footnote": [], "page_idx": 24}, {"type": "text", "text": "D.2 Blackjack ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "In the Blackjack task, we collect 50k instruction-following expert data samples following an open-sourced solver.11 See examples with and without CoT in Figure 24 and 25, respectively. ", "page_idx": 25}, {"type": "image", "img_path": "nBjmMF2IZU/tmp/affc355636fb2e4d6eb594197c5d782ba3bda4d80ade0575f477b0ec251a23ad.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 24: An example of the supervised fine-tuning data for Blackjack with CoT. ", "page_idx": 25}, {"type": "image", "img_path": "nBjmMF2IZU/tmp/5cfdb712d78b79fa1a43f430faae40930063b3aac093c8765465822986f38239.jpg", "img_caption": [], "img_footnote": [], "page_idx": 25}, {"type": "text", "text": "Figure 25: An example of the supervised fine-tuning data for Blackjack without CoT. ", "page_idx": 25}, {"type": "text", "text": "D.3 EZPoints ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "In the EZPoints task, we directly collect $50\\mathrm{k}$ instruction-following expert data samples using a task solver. See examples with and without CoT in Figure 26 and 27, respectively. ", "page_idx": 26}, {"type": "image", "img_path": "nBjmMF2IZU/tmp/5c62433261cd0e8b40854af51837c3bacf759186e25a154ad6377c9baeece03c.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 26: An example of the supervised fine-tuning data for EZPoints with CoT. ", "page_idx": 26}, {"type": "image", "img_path": "nBjmMF2IZU/tmp/5785c1f413c18a2d50769640ecc8b1e91e7c25fce162706f0055f19b000e3cc2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 26}, {"type": "text", "text": "Figure 27: An example of the supervised fine-tuning data for Blackjack without CoT. ", "page_idx": 26}, {"type": "text", "text": "D.4 Points24 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "In the Points24 task, we directly collect 50k instruction-following expert data samples using a task solver. See examples with and without CoT in Figure 28 and 29, respectively. Note that the supervised fine-tuning data has a slightly different distribution as the states in the Points24 environment, since the expert data always have a viable solution to 24, while not all states from Points24 has a viable solution to 24. ", "page_idx": 27}, {"type": "image", "img_path": "nBjmMF2IZU/tmp/48f815fec218f6972fe40b5052e4962e578eb71450b9e9613c64527882537bc9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "Figure 28: An example of the supervised fine-tuning data for Points24 with CoT. ", "page_idx": 27}, {"type": "text", "text": "Responses: ", "text_level": 1, "page_idx": 27}, {"type": "image", "img_path": "nBjmMF2IZU/tmp/45ef5aa06ab2180c83cec2c309a918466ab88186be0998349b50692af065f227.jpg", "img_caption": ["Figure 29: An example of the supervised fine-tuning data for Points24 without CoT. "], "img_footnote": [], "page_idx": 27}, {"type": "text", "text": "{   \n\"action\": \"+\"   \n}\" ", "page_idx": 27}, {"type": "text", "text": "The data collection procedure of the alfworld embodied environment is slightly different than the gym_cards, as we do not have solvers to generate the instruction-following data, even with the expert text action. Therefore, we take a step back and directly use our prompt as presented in Figure 17 (or Figure 30) to collect 1k episodes (around $45\\mathrm{k}.$ ) instruction-following data from GPT4-V [44] with the CoT reasoning for the instruction-following fine-tuning with CoT. As for the case without CoT, we directly clean the collected CoT by removing the CoT reasonings. See examples with and without CoT in Figure 30 and 31, respectively. ", "page_idx": 28}, {"type": "text", "text": "Inputs: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Your are an expert in the ALFRED Embodied Environment. You are also given the following text description of the current scene: ['You arrive at loc 0. The cabinet 1 is open. On the cabinet 1, you see a pan 1, a kettle 1, a winebottle 1, a apple 1, a stoveknob 1, a stoveknob 2, a stoveknob 3, a stoveknob 4, a knife 1, a saltshaker 1, and a bread 1.']. Your task is to put a cool mug in cabinet. Your admissible actions of the current situation are: ['go to countertop $1\\,^{\\prime}$ , 'go to cabinet $2\\,^{\\prime}$ , 'go to countertop $2^{\\prime}$ , 'go to stoveburner 1', 'go to drawer $1\\,^{\\prime}$ , 'go to drawer $2^{\\prime}$ , 'go to drawer $3\\,\"$ , 'go to stoveburner $2\\,\"$ , 'go to stoveburner $3\\,\"$ , 'go to stoveburner $4\\,\"$ , 'go to drawer $4\\,\"$ , 'go to cabinet $3\\,\"$ , 'go to cabinet $4\"$ , 'go to microwave 1', 'go to cabinet $5\\,\"$ , 'go to cabinet $6\\,\"$ , 'go to cabinet 7', 'go to sink $1\\,^{\\prime}$ , 'go to sinkbasin $1\\,^{\\prime}$ , 'go to fridge $1\\,^{\\prime}$ , 'go to toaster 1', 'go to coffeemachine 1', 'go to cabinet $8\\,\"$ , 'go to drawer $5\\,\"$ , 'go to drawer $6\\,\"$ , 'go to drawer 7', 'go to drawer $8\\,\"$ , 'go to shelf $1\\,^{\\prime}$ , 'go to shelf $2\\,\"$ , 'go to countertop $3\\,\"$ , 'go to shelf $3\\,\"$ , 'go to drawer $9\\,^{\\prime}$ , 'go to garbagecan $1\\,^{\\prime}$ , 'open cabinet $1\\,^{\\prime}$ , 'close cabinet $1\\,^{\\prime}$ , 'take pan 1 from cabinet $1\\,^{\\prime}$ , 'take kettle 1 from cabinet $1\\,^{\\prime}$ , 'take winebottle 1 from cabinet 1', 'take apple 1 from cabinet 1', 'take stoveknob 1 from cabinet $1\\,^{\\prime}$ , 'take stoveknob 2 from cabinet $1\\,^{\\prime}$ , 'take stoveknob 3 from cabinet $1\\,^{\\prime}$ , 'take stoveknob 4 from cabinet $1\\,^{\\prime}$ , 'take knife 1 from cabinet $1\\,^{\\prime}$ , 'take saltshaker 1 from cabinet $1\\,^{\\prime}$ , 'take bread 1 from cabinet $1\\,^{\\prime}$ , 'inventory', 'look', 'examine cabinet 1']. Your response should be a valid json file in the following format: ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "image", "img_path": "nBjmMF2IZU/tmp/6f95ea90952472e0550ed4ce5ec16b81cca3a773796d4ce8480755ae939678ee.jpg", "img_caption": ["Figure 30: An example of the supervised fine-tuning data for alfworld with CoT. "], "img_footnote": [], "page_idx": 28}, {"type": "text", "text": "\"thoughts\": \"first describe what do you see in the image using the text description, then carefully   \nthink about which action to complete the task. \",   \n\"action\": \"an admissible action\" ", "page_idx": 28}, {"type": "text", "text": "Responses: ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "\"thoughts\": \"Based on the text description, I am at loc 0 with an open cabinet 1 in front of me. Items such as a pan, kettle, wine bottle, apple, several stoveknobs, a knife, saltshaker, and bread are visible on the cabinet 1. To find a cool mug to put in the cabinet, I may need to first locate a mug, which might be in one of the cabinets or drawers. Since the task does not involve any items currently in cabinet 1, and the mug is not mentioned in the description, my best course of action would be to start by searching the other cabinets or drawers for the mug.\",   \n\"action\": \"go to cabinet 2\" ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "}\" ", "page_idx": 28}, {"type": "text", "text": "Inputs: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Your are an expert in the ALFRED Embodied Environment. You are also given the following text description of the current scene: ['You arrive at loc 0. The cabinet 1 is open. On the cabinet 1, you see a pan ", "page_idx": 29}, {"type": "image", "img_path": "", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "image", "img_path": "nBjmMF2IZU/tmp/ccee80105a2d554790c274455d498eaf38a803b07773c54d48ba44b2a4dee442.jpg", "img_caption": [], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "Figure 31: An example of the supervised fine-tuning data for alfworld without CoT. ", "page_idx": 29}, {"type": "text", "text": "E NeurIPS Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Claims ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: In the paper, we propose an end-to-end RL training framework for improving a VLM agent\u2019s decision-making capabilities. We claim that our method can improve a VLM\u2019s decision-making capabilities and we show the CoT reasoning appears as a crucial component for performance improvement. All claims are justified in the experiment section (Section 6). ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 30}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 30}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Justification: We have provided a paragraph to discuss the conclusions, limitations, and future directions of our paper (Section 7). In particular, we have pointed out that we have not extensively tried different prompts for the CoT reasoning, and our method currently can only train the VLM on one task at a time. And we would like to leave them for future studies. ", "page_idx": 30}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. ", "page_idx": 30}, {"type": "text", "text": "\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 31}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 31}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 31}, {"type": "text", "text": "Justification: This is an empirical paper, we do not have any theoretical results. Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 31}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 31}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 31}, {"type": "text", "text": "Justification: We have listed all details for reproducing our experiments in Appendix C. We have also provided a detailed readme.md flie in the supplementary for reproducing our results. ", "page_idx": 31}, {"type": "text", "text": "Guidelines: ", "page_idx": 31}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 31}, {"type": "text", "text": "", "page_idx": 32}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: Our supplementary materials contain all of our codes, and we have provided a detailed readme.md file in the supplementary for reproducing our experiments. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 32}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 32}, {"type": "text", "text": "Justification: We have provided general descriptions of the tested tasks in Section 5. We also have provided the general discussion of our experiments in Section 6. The details are provided in Appendix C and D. ", "page_idx": 32}, {"type": "text", "text": "Guidelines: ", "page_idx": 32}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 32}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 32}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "All of our experiments contain standard deviation in the training curves, all standard deviations are calculated among 4 different random seeds, as specified in Appendix C.3. See Figure 5, 6, 7, 18, and 19. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 33}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We have reported such results for computation costs in Appendix C.3. Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 33}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 33}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 33}, {"type": "text", "text": "Justification: We have read through the guidelines and we believe they are all satisfied to the best of our understanding. ", "page_idx": 33}, {"type": "text", "text": "Guidelines: ", "page_idx": 33}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 33}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: We proposed a method for end-to-end RL training on VLMs on vision based decision-making tasks. To the best of our knowledge, our method does not lead to potential negative societal impacts. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 34}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 34}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 34}, {"type": "text", "text": "Justification: We majorly proposed a method for applying end-to-end RL training on VLMs, which does not release any large models. ", "page_idx": 34}, {"type": "text", "text": "Guidelines: ", "page_idx": 34}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 34}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 34}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 34}, {"type": "text", "text": "Justification: We adopt the LLaVA-1.6-7b [35] as our initial model for RL training, we have cited LLaVA properly throughout the paper and in our code (in the supplementary). ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 35}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 35}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 35}, {"type": "text", "text": "Justification: We have prepared our own data for the supervised fine-tuning phase. And we have anonymized the dataset for reproduction in the supplementary as well. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 35}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 35}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 35}, {"type": "text", "text": "Justification: This paper does not involve any crowdsourcings and human subjects. ", "page_idx": 35}, {"type": "text", "text": "Guidelines: ", "page_idx": 35}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 35}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 35}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 36}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 36}, {"type": "text", "text": "Justification: This paper does not involve any crowdsourcings and human subjects. Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 36}]