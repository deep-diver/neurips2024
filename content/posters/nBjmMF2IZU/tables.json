[{"figure_path": "nBjmMF2IZU/tables/tables_5_1.jpg", "caption": "Table 1: The absolute values of sum log probability of  is much larger than . Each number is averaged among 1000 samples on our evaluation tasks to be introduced in Section 5.", "description": "This table shows the absolute values of the sum of log probabilities for the chain-of-thought (CoT) tokens and action tokens in the VLM outputs for different tasks.  It highlights that the magnitude of log probabilities for CoT tokens is significantly larger than that for action tokens, a point that motivates the use of a scaling factor in the action probability estimation to balance the influence of CoT reasoning.", "section": "Estimating Action Probabilities of VLM Policies"}, {"figure_path": "nBjmMF2IZU/tables/tables_8_1.jpg", "caption": "Table 2: Average episode success rates (%) of different methods on gym_cards and alfworld. For all RL-based methods (CNN+RL and our method), we present the peak numbers (first 15k environment steps for the gym_cards and 5k environment steps for alfworld) from each training curve from Figure 5. We average the performance of all 4 tasks on gym_cards with equal weight. Due to the nature of the alfworld environment, where each subtask does not appear with equal probability, the average performance on alfworld is a weighted average among all types of tasks. We mark the BUTLERg and BUTLER agent [58] in gray since they require expert data, while the remaining methods do not require expert data. As discussed by Shridhar et al. [58], the performance discrepancy between BUTLERg and BUTLER happens due to different decoding strategies in evaluation strategies: BUTLERg uses greedy decoding, which may repeat failed actions, whereas BUTLER employs beam search during evaluation.", "description": "This table compares the performance of different methods (including the proposed method) on two sets of tasks: gym_cards (arithmetic reasoning with visual inputs) and alfworld (visual semantic reasoning in an embodied AI environment).  The table shows the average success rate across all tasks in each domain for each method.  Note that the results for RL-based methods represent peak performance within a limited number of training steps, and that alfworld's average is weighted due to the uneven probability distribution of subtasks.  Comparison is also made to methods that require expert data, highlighting the advantage of the proposed approach.", "section": "Experimental Results"}, {"figure_path": "nBjmMF2IZU/tables/tables_8_2.jpg", "caption": "Table 2: Average episode success rates (%) of different methods on gym_cards and alfworld. For all RL-based methods (CNN+RL and our method), we present the peak numbers (first 15k environment steps for the gym_cards and 5k environment steps for alfworld) from each training curve from Figure 5. We average the performance of all 4 tasks on gym_cards with equal weight. Due to the nature of the alfworld environment, where each subtask does not appear with equal probability, the average performance on alfworld is a weighted average among all types of tasks. We mark the BUTLER, and BUTLER agent [58] in gray since they require expert data, while the remaining methods do not require expert data. As discussed by Shridhar et al. [58], the performance discrepancy between BUTLER, and BUTLER happens due to different decoding strategies in evaluation strategies: BUTLER, uses greedy decoding, which may repeat failed actions, whereas BUTLER employs beam search during evaluation.", "description": "This table compares the average episode success rates of different methods on two datasets: gym_cards and alfworld.  It shows the peak performance (within a certain number of environment steps) of each method.  The gym_cards results are averaged across four tasks, while the alfworld results are weighted averages across multiple subtasks.  The table highlights that the proposed method generally outperforms other methods across various tasks and datasets.", "section": "Experimental Results"}, {"figure_path": "nBjmMF2IZU/tables/tables_22_1.jpg", "caption": "Table 2: Average episode success rates (%) of different methods on gym_cards and alfworld. For all RL-based methods (CNN+RL and our method), we present the peak numbers (first 15k environment steps for the gym_cards and 5k environment steps for alfworld) from each training curve from Figure 5. We average the performance of all 4 tasks on gym_cards with equal weight. Due to the nature of the alfworld environment, where each subtask does not appear with equal probability, the average performance on alfworld is a weighted average among all types of tasks. We mark the BUTLER, and BUTLER agent [58] in gray since they require expert data, while the remaining methods do not require expert data. As discussed by Shridhar et al. [58], the performance discrepancy between BUTLER, and BUTLER happens due to different decoding strategies in evaluation strategies: BUTLER, uses greedy decoding, which may repeat failed actions, whereas BUTLER employs beam search during evaluation.", "description": "This table compares the average episode success rates and returns of different methods on two sets of tasks: gym_cards and alfworld.  Gym_cards contains four tasks of increasing difficulty, including a stochastic task (Blackjack).  ALFWorld is an embodied AI environment with six distinct goal-conditioned tasks requiring visual semantic understanding.  The table highlights the performance of our proposed method against other methods, including commercial models (GPT4-V, Gemini), a supervised fine-tuned version of the base model (LLaVA-sft), and a vanilla RL method with a CNN-based policy network (CNN+RL).  The results show our method's consistent improvement across various tasks and its superior performance compared to the other methods.", "section": "Experimental Results"}]