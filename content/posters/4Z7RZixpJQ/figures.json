[{"figure_path": "4Z7RZixpJQ/figures/figures_3_1.jpg", "caption": "Figure 1: The pipeline of structure quantization. (A) Training of the structure encoder. (B) Local structure clustering and labeling. (C) Converting a protein structure to structure token sequence.", "description": "This figure illustrates the three-stage pipeline for protein structure quantization.  First, (A) shows the training of a structure encoder using a geometric vector perceptron (GVP) based autoencoder.  The encoder takes a protein structure as input, adds gaussian noise, and learns to reconstruct the original protein structure. This trained encoder is then used in the next stage. Second, (B) shows the creation of a local structure codebook.  The trained encoder processes a large dataset of protein local structures to produce dense vectors. A clustering model (k-means) is then applied to create a codebook of discrete structure tokens. Finally, (C) shows the protein structure serialization and quantization. The trained encoder processes each residue's local structure to produce a dense vector, which is then converted to a discrete structure token using the codebook, producing a sequence of structure tokens for the entire protein.", "section": "3.1 Structure Quantization Module"}, {"figure_path": "4Z7RZixpJQ/figures/figures_4_1.jpg", "caption": "Figure 2: Model architecture of ProSST. ProSST is a Transformer-style model and the difference is that ProSST uses disentangled attention instead of self-attention [19].", "description": "This figure shows the architecture of ProSST, a Transformer-based protein language model.  It highlights the key components: the structure quantization module which converts protein structures into a sequence of discrete tokens, and the disentangled attention mechanism which allows the model to learn relationships between protein residue sequences and structure token sequences.  The disentangled attention module is a modified version of self-attention, explicitly modeling interactions between residue tokens, structure tokens, and positional embeddings. This architecture enables the model to effectively integrate structural information with protein sequences for comprehensive contextual representation learning.", "section": "3.2 Sequence-Structure Disentangled Attention"}, {"figure_path": "4Z7RZixpJQ/figures/figures_8_1.jpg", "caption": "Figure 1: The pipeline of structure quantization. (A) Training of the structure encoder. (B) Local structure clustering and labeling. (C) Converting a protein structure to structure token sequence.", "description": "This figure illustrates the three stages of the structure quantization module in ProSST.  Stage (A) shows the training process for the structure encoder, a neural network that converts 3D protein structures into dense vector representations. Stage (B) depicts the creation of a codebook using k-means clustering on the encoded vectors from a large dataset of protein structures. The codebook maps similar local structural patterns to a discrete token. Finally, Stage (C) demonstrates how a full protein structure is processed, creating a sequence of tokens representing the protein's structural information.", "section": "3.1 Structure Quantization Module"}, {"figure_path": "4Z7RZixpJQ/figures/figures_8_2.jpg", "caption": "Figure 3: Perplexity curves of ProSST under different settings. We ablate the components of quantized structure and disentangled attention, and show their perplexity curves on the validation set.", "description": "This figure displays perplexity curves, a measure of model performance, for ProSST under various conditions.  It shows the effect of ablating different components of the model, including the quantized structure (varying K values) and different components of the disentangled attention mechanism (removing different attention types). The x-axis represents the training step, and the y-axis represents the perplexity. Lower perplexity indicates better performance. The results demonstrate the importance of both quantized structure and disentangled attention for ProSST's performance.", "section": "4.3 Ablation Study"}, {"figure_path": "4Z7RZixpJQ/figures/figures_15_1.jpg", "caption": "Figure 1: The pipeline of structure quantization. (A) Training of the structure encoder. (B) Local structure clustering and labeling. (C) Converting a protein structure to structure token sequence.", "description": "This figure illustrates the process of converting a protein's 3D structure into a sequence of discrete tokens. It involves three main steps:\n\n(A) Training a structure encoder using a denoising autoencoder to learn a robust representation of local protein structures. This encoder takes a protein structure as input and outputs a dense vector representation.\n\n(B) Building a structure codebook by clustering the dense vector representations of millions of local structures from the CATH dataset using k-means. The centroids of the clusters form the codebook, and each centroid represents a discrete structure token.\n\n(C) Serializing the protein structure into a sequence of residue-level local structures.  Each local structure is then encoded using the trained structure encoder, and the resulting vector is quantized into a discrete structure token using the structure codebook. The final output is a sequence of these discrete tokens representing the protein's structure.", "section": "3.1 Structure Quantization Module"}, {"figure_path": "4Z7RZixpJQ/figures/figures_16_1.jpg", "caption": "Figure 1: The pipeline of structure quantization. (A) Training of the structure encoder. (B) Local structure clustering and labeling. (C) Converting a protein structure to structure token sequence.", "description": "This figure illustrates the process of converting a protein's 3D structure into a sequence of discrete tokens.  Panel (A) shows the training of a structure encoder, a neural network that learns to represent local structural features as dense vectors.  Panel (B) depicts the creation of a codebook by clustering similar local structural vectors into groups, each represented by a unique token. Finally, panel (C) shows how the entire protein structure is processed, creating a sequence of these discrete structure tokens for use as input to the main ProSST model.", "section": "3.1 Structure Quantization Module"}, {"figure_path": "4Z7RZixpJQ/figures/figures_18_1.jpg", "caption": "Figure 2: Model architecture of ProSST. ProSST is a Transformer-style model and the difference is that ProSST uses disentangled attention instead of self-attention [19].", "description": "The figure shows the architecture of the ProSST model, which is a transformer-based model that uses disentangled attention.  Disentangled attention is a mechanism that allows the model to explicitly model the relationship between protein sequence tokens and structure tokens. This enables the model to learn comprehensive contextual representations of proteins, which improves its performance on various downstream tasks.  The figure highlights the key components of the ProSST model, including the structure quantization module, the disentangled multi-head attention module, and the feed-forward module.  It also shows how the different types of attention are combined to generate the final output.", "section": "3.2 Sequence-Structure Disentangled Attention"}, {"figure_path": "4Z7RZixpJQ/figures/figures_19_1.jpg", "caption": "Figure A7: (a) pLDDT vs. Spearman of ProSST on ProteinGYM. (b) pLDDT vs. Spearman of SaProt on ProteinGYM. (c) pLDDT vs. Spearman of ESM-IF1 on ProteinGYM.", "description": "This figure displays the correlation between AlphaFold2's predicted local structure quality (pLDDT) and the performance (Spearman correlation) of three different protein language models, ProSST, SaProt, and ESM-IF1, on the ProteinGYM benchmark.  Each panel shows a scatter plot, with pLDDT values on the x-axis and Spearman correlation values on the y-axis. A linear regression line is also included to visualize the trend. The positive correlation suggests that the accuracy of structure prediction influences the performance of the models.  Disordered regions of proteins may affect the prediction accuracy of AlphaFold2, potentially impacting downstream model performance.", "section": "A.7 AlphaFold pLDDT versus Zero-shot mutant effect performance"}]