[{"figure_path": "cFTi3gLJ1X/figures/figures_0_1.jpg", "caption": "Figure 1: Depth Anything V2 significantly outperforms V1 [89] in robustness and fine-grained details. Compared with SD-based models [31, 25], it enjoys faster inference speed, fewer parameters, and higher depth accuracy.", "description": "This figure demonstrates the improved performance of Depth Anything V2 over its predecessor (V1) and other state-of-the-art models.  It showcases superior robustness in handling complex scenes, improved precision in capturing fine details, faster inference speed, and a reduction in model parameters while maintaining high accuracy.  The comparison highlights the advancements in depth estimation achieved by Depth Anything V2.", "section": "Abstract"}, {"figure_path": "cFTi3gLJ1X/figures/figures_1_1.jpg", "caption": "Figure 1: Depth Anything V2 significantly outperforms V1 [89] in robustness and fine-grained details. Compared with SD-based models [31, 25], it enjoys faster inference speed, fewer parameters, and higher depth accuracy.", "description": "This figure compares the performance of Depth Anything V2 with its predecessor, Depth Anything V1, and other state-of-the-art models on a benchmark dataset.  It highlights improvements in both robustness (handling challenging scenes like misleading room layouts) and fine-grained detail (capturing details like a thin basketball net). V2 achieves higher accuracy with fewer parameters and faster inference speed.", "section": "Abstract"}, {"figure_path": "cFTi3gLJ1X/figures/figures_2_1.jpg", "caption": "Figure 3: Various noise in \u201cGT\u201d depth labels (a: NYU-D [70], b: HRWSI [83], c: MegaDepth [37]) and prediction errors in correspondingly trained models (d). Black regions are ignored during training.", "description": "This figure demonstrates different types of noise present in ground truth (GT) depth labels from various datasets (NYU-D, HRWSI, MegaDepth).  Subfigure (a) shows noise in transparent objects, (b) shows noise due to repetitive patterns in stereo matching, and (c) shows noise related to dynamic objects in Structure from Motion (SfM) datasets.  (d) shows the resulting errors in the model predictions due to these noisy labels. The black regions represent areas that were ignored during training due to significant uncertainty.", "section": "2 Revisiting the Labeled Data Design of Depth Anything V1"}, {"figure_path": "cFTi3gLJ1X/figures/figures_3_1.jpg", "caption": "Figure 4: Depth labels of real images (a) and synthetic images (b), and the corresponding model predictions (c). The labels of synthetic images are highly precise, and so are their trained models.", "description": "This figure compares the depth labels and model predictions from real images and synthetic images.  Subfigure (a) shows the coarse depth labels obtained from real-world datasets (HRWSI [83], DIML [14]). Subfigure (b) displays the highly precise depth labels from synthetic datasets (Hypersim [58], vKITTI [9]). Subfigure (c) contrasts model predictions trained on these different datasets, highlighting the significantly improved precision of models trained on synthetic data.", "section": "2 Revisiting the Labeled Data Design of Depth Anything V1"}, {"figure_path": "cFTi3gLJ1X/figures/figures_4_1.jpg", "caption": "Figure 5: Qualitative comparison of different vision encoders on synthetic-to-real transfer. Only DINOv2-G produces a satisfying prediction. For quantitative comparisons, please refer to Section B.6.", "description": "This figure shows a qualitative comparison of the depth prediction results of several vision encoders (BEIT-Large, SAM-Large, SynCLR-Large, DINOv2-Giant, DINOv2-Small, DINOv2-Base, DINOv2-Large) when performing synthetic-to-real transfer in monocular depth estimation.  The input image contains cats, and each subplot displays the depth map generated by each encoder.  The results show that only the DINOv2-Giant model produces reasonably accurate depth prediction, highlighting the challenges and potential solutions in transferring knowledge learned from synthetic data to real-world scenes. Quantitative details are provided in Section B.6 of the paper.", "section": "4 Key Role of Large-Scale Unlabeled Real Images"}, {"figure_path": "cFTi3gLJ1X/figures/figures_4_2.jpg", "caption": "Figure 6: Failure cases of the most capable DINOv2-G model when purely trained on synthetic images. Left: the sky should be ultra far. Right: the depth of the head is not consistent with the body.", "description": "This figure shows two examples where a model trained only on synthetic data fails to generalize well to real-world images. The first example shows that the model incorrectly predicts the distance to the sky, making it appear much closer than it actually is. The second example shows an inconsistency in the depth prediction of a person; the head is predicted to be at a different depth than the rest of the body.  These failures highlight the limitations of training solely on synthetic data and demonstrate the need for incorporating real-world data to improve model generalization.", "section": "4 Key Role of Large-Scale Unlabeled Real Images"}, {"figure_path": "cFTi3gLJ1X/figures/figures_5_1.jpg", "caption": "Figure 7: Depth Anything V2. We first train the most capable teacher on precise synthetic images. Then, to mitigate the distribution shift and limited diversity of synthetic data, we annotate unlabeled real images with the teacher. Finally, we train student models on high-quality pseudo-labeled images.", "description": "This figure illustrates the three-stage training process of Depth Anything V2.  First, a powerful teacher model is trained using only high-quality synthetic images. Then, this teacher model is used to generate pseudo-labels for a large dataset of unlabeled real images, mitigating the distribution shift and limited diversity inherent in purely synthetic datasets. Finally, student models are trained using these high-quality pseudo-labeled real images, resulting in a model with improved generalization and robustness.", "section": "5 Depth Anything V2"}, {"figure_path": "cFTi3gLJ1X/figures/figures_6_1.jpg", "caption": "Figure 3: Various noise in \u201cGT\u201d depth labels (a: NYU-D [70], b: HRWSI [83], c: MegaDepth [37]) and prediction errors in correspondingly trained models (d). Black regions are ignored during training.", "description": "This figure demonstrates the various types of noise present in ground truth depth labels from different datasets (NYU-D, HRWSI, MegaDepth).  It shows how these inaccuracies, stemming from limitations in data collection methods like depth sensors, stereo matching, and structure from motion (SfM), affect the predictions of models trained on these datasets.  The examples illustrate label noise in transparent objects, repetitive patterns, and dynamic objects, highlighting the challenges of using real-world depth data for training.", "section": "2 Revisiting the Labeled Data Design of Depth Anything V1"}, {"figure_path": "cFTi3gLJ1X/figures/figures_6_2.jpg", "caption": "Figure 9: Our proposed evaluation benchmark DA-2K. (a) The annotation pipeline for relative depth between two points. Points are sampled based on SAM [33] mask predictions. Disagreed pairs among four depth models will be popped out for annotators to label. (b) Detail of our scenario coverage.", "description": "This figure illustrates the creation of the DA-2K benchmark dataset for evaluating monocular depth estimation models.  Panel (a) details the annotation pipeline:  four existing depth estimation models are used to predict relative depth between two points in an image.  If these models disagree, human annotators provide the ground truth.  Panel (b) shows a pie chart illustrating the diverse range of scenarios encompassed in the dataset, including indoor, outdoor, aerial, underwater, and images with transparent/reflective objects.", "section": "6 A New Evaluation Benchmark: DA-2K"}, {"figure_path": "cFTi3gLJ1X/figures/figures_19_1.jpg", "caption": "Figure 10: Effect of the gradient matching loss Lgm in terms of fine-grained details.", "description": "This figure shows the effect of the gradient matching loss (Lgm) on the sharpness of depth predictions.  Three images are shown with depth predictions, each using different weights for the Lgm loss (0.5, 2.0, and 4.0).  As the weight increases, the depth predictions become sharper, showing more fine-grained details. This demonstrates that the Lgm loss is beneficial for improving the sharpness of depth maps, especially when training with high-quality synthetic data.", "section": "B.7 Benefit of gradient matching loss to fine-grained predictions"}, {"figure_path": "cFTi3gLJ1X/figures/figures_20_1.jpg", "caption": "Figure 11: Test-time resolution scaling up can further improve the prediction sharpness.", "description": "This figure shows the results of Depth Anything V2 model with different resolutions (1x, 2x, 4x). It demonstrates that increasing the resolution at test time improves the prediction sharpness, which is a property of the model.", "section": "B.8 Test-time resolution scaling up"}, {"figure_path": "cFTi3gLJ1X/figures/figures_20_2.jpg", "caption": "Figure 12: Adding real training dataset, e.g., HRWSI, to synthetic training datasets, will ruin the original fine-grained depth predictions.", "description": "This figure shows a comparison of depth prediction results using purely synthetic images versus a combination of synthetic and real images (HRWSI). The results demonstrate that adding real images, even a small percentage, to the training dataset significantly degrades the fine details in the depth map predictions. This highlights the negative impact of noisy real-world depth labels on the model's ability to learn precise depth information from synthetic data.", "section": "B.9 Harm of real labeled images to fine-grained predictions"}, {"figure_path": "cFTi3gLJ1X/figures/figures_23_1.jpg", "caption": "Figure 13: Comparison between Depth Anything V1 [89] and our V2 in open-world images.", "description": "This figure shows a qualitative comparison of the depth estimations produced by Depth Anything V1 and Depth Anything V2 on various open-world images.  It visually demonstrates the improvements in Depth Anything V2 in terms of accuracy and detail, particularly in challenging scenes.", "section": "B.10 Qualitative comparison between Depth Anything V1 and V2"}, {"figure_path": "cFTi3gLJ1X/figures/figures_24_1.jpg", "caption": "Figure 14: Comparison between Marigold [31] and our V2 in open-world images.", "description": "This figure compares the depth estimation results of Marigold and Depth Anything V2 on several real-world images.  It visually demonstrates the differences in the quality and robustness of depth maps generated by each model, highlighting Depth Anything V2's improved performance, especially in handling complex scenes and fine details.", "section": "B.11 Qualitative comparison between Marigold and Depth Anything V2"}, {"figure_path": "cFTi3gLJ1X/figures/figures_25_1.jpg", "caption": "Figure 15: Comparison between ZoeDepth [6] and our fine-tuned metric depth model.", "description": "This figure compares the performance of ZoeDepth, a state-of-the-art metric depth estimation model, with the authors' fine-tuned metric depth model.  The comparison is shown through qualitative results on several real-world images. Each row displays an input image alongside the depth maps generated by ZoeDepth and the authors' model.  The visual difference helps illustrate the strengths and weaknesses of each approach in terms of accuracy and detail.", "section": "B.12 Qualitative comparison between our metric depth models and ZoeDepth"}, {"figure_path": "cFTi3gLJ1X/figures/figures_26_1.jpg", "caption": "Figure 16: Qualitative comparison of the DINOv2-small-based depth model trained solely on labeled synthetic images and solely pseudo-labeled real images. The robustness is tremendously enhanced.", "description": "This figure compares the performance of a DINOv2-small depth estimation model trained using two different approaches: one trained solely on labeled synthetic images, and another trained solely on pseudo-labeled real images.  The comparison visually demonstrates that using pseudo-labeled real images significantly enhances the model's robustness in generating accurate depth maps, particularly for complex or challenging scenes.", "section": "B Experiments"}, {"figure_path": "cFTi3gLJ1X/figures/figures_27_1.jpg", "caption": "Figure 17: Visualization of our produced pseudo depth labels. From top to bottom, the highly diverse images are sampled from BDD100K [97], Google Landmarks [81], ImageNet-21K [60], LSUN [98], Objects365 [65], Open Images V7 [35], Places365 [103], and SA-1B [33] datasets, respectively.", "description": "This figure visualizes the pseudo depth labels generated by the model on various unlabeled real images. The images are sampled from eight different large-scale datasets representing diverse scenes and object categories. Each pair shows an unlabeled real image alongside its corresponding pseudo depth map, highlighting the model's ability to accurately estimate depth in various complex scenarios.", "section": "4 Key Role of Large-Scale Unlabeled Real Images"}, {"figure_path": "cFTi3gLJ1X/figures/figures_28_1.jpg", "caption": "Figure 18: Qualitative results on widely adopted test benchmarks, e.g., KITTI, NYU, and DIODE.", "description": "This figure presents a qualitative comparison of depth estimation results between Depth Anything V1 and the proposed Depth Anything V2 model.  The comparison is performed on widely used benchmark datasets such as KITTI, NYU, and DIODE. For each image, the ground truth image is shown alongside the depth maps generated by both models.  The comparison aims to highlight the improvements in depth estimation accuracy and detail achieved by Depth Anything V2 compared to its predecessor.", "section": "B.15 Qualitative results on test benchmarks"}, {"figure_path": "cFTi3gLJ1X/figures/figures_29_1.jpg", "caption": "Figure 19: Visualization of images and precise sparse annotations on our benchmark DA-2K. Please zoom in to better view the annotated pairs. The green point is annotated as closer than the red point. From top to bottom, the images are sampled from indoor, outdoor, non-real, transparent/reflective, adverse style, aerial, underwater, and object scenarios, respectively.", "description": "This figure shows examples from the DA-2K benchmark dataset, highlighting the diversity of scenes and the precise sparse annotations used for relative depth estimation. Each image pair shows two points, one labeled as closer (green) and one as farther (red). The scenes include indoor and outdoor settings, non-real images (like illustrations), transparent or reflective surfaces, adverse weather/lighting conditions, aerial views, underwater scenes, and diverse object types.", "section": "6 A New Evaluation Benchmark: DA-2K"}]