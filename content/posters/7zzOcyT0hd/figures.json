[{"figure_path": "7zzOcyT0hd/figures/figures_4_1.jpg", "caption": "Figure 1: MDP example, with 2 states and 2 experts, that highlights the benefits of sub-optimal experts (Example 3.5). In S\u2081 both \u03a0\u0395\u2081 and \u03a0\u0395; are identical, i.e., \u03c0\u03b5\u2081 (\u0100|S1) = \u03c0\u03b5\u2081 (A|S1) = 1.", "description": "This figure shows a simple Markov Decision Process (MDP) with two states (S\u2080 and S\u2081) and two actions (A\u2081 and A\u2082).  The optimal expert (\u03c0\u0395\u2081) always chooses action A\u2081 when in state S\u2080 and transitions to state S\u2081. The sub-optimal expert (\u03c0\u0395\u1d62)  also chooses action A\u2081 from S\u2080 (it is sub-optimal in S\u2080).  Both experts act identically in state S\u2081: they choose action A. This example demonstrates that even if sub-optimal agents behave similarly to optimal agents in certain states, their inclusion can impact the feasible set of rewards. The differences in behavior between the optimal and sub-optimal agents in S\u2080 allows the sub-optimal experts to reduce ambiguity in the inverse reinforcement learning (IRL) problem, as noted in Example 3.5 of the paper. ", "section": "3 Sub-Optimal Experts and the Feasible Reward Set"}, {"figure_path": "7zzOcyT0hd/figures/figures_4_2.jpg", "caption": "Figure 2: Visualization of the feasible reward set (i.e., shaded red area) for the problems described in Example 3.5. On the left, the feasible reward set for the single-expert IRL problem and on the right the feasible reward set for the multiple and sub-optimal setting when using \u03bei = 0.5.", "description": "This figure visualizes the feasible reward sets for single-expert and multiple sub-optimal expert Inverse Reinforcement Learning (IRL) problems. The left panel shows the feasible reward set for a single optimal expert. The right panel shows the significantly reduced feasible reward set when incorporating a sub-optimal expert with a sub-optimality bound \u03bei = 0.5. The reduction demonstrates how additional data from sub-optimal experts can mitigate the inherent ambiguity in IRL.", "section": "3 Sub-Optimal Experts and the Feasible Reward Set"}, {"figure_path": "7zzOcyT0hd/figures/figures_19_1.jpg", "caption": "Figure 1: MDP example, with 2 states and 2 experts, that highlights the benefits of sub-optimal experts (Example 3.5). In S\u2081 both \u03c0E\u2081 and \u03c0E\u1d62 are identical, i.e., \u03c0E\u2081 (A|S\u2081) = \u03c0E\u1d62 (A|S\u2081) = 1.", "description": "This figure shows a simple Markov Decision Process (MDP) with two states (S\u2080 and S\u2081) and two experts (an optimal expert and a sub-optimal expert).  The optimal expert's policy (\u03c0E\u2081) is deterministic, always selecting action A\u2081 from state S\u2080 and action A from state S\u2081. The sub-optimal expert (\u03c0E\u1d62) has the same policy as the optimal expert in state S\u2081, but its policy differs in state S\u2080. The figure highlights the use of sub-optimal expert demonstrations in inverse reinforcement learning (IRL). By including data from both experts, it becomes easier to determine the reward function that best represents the intentions of the optimal expert, as the sub-optimal expert's actions provide additional constraints on the feasible reward set. This reduces the inherent ambiguity in IRL, especially in real-world scenarios where human experts show varying skill levels.", "section": "3 Sub-Optimal Experts and the Feasible Reward Set"}, {"figure_path": "7zzOcyT0hd/figures/figures_21_1.jpg", "caption": "Figure 3: Representation of the IRL-SE problem for the instances used in Theorem E.2.", "description": "This figure is a graphical representation of a Markov Decision Process (MDP) used in the proof of Theorem E.2 within Section 4.2 of the paper.  The MDP has a root node (Sroot) and two branches leading to distinct subtrees, representing different sets of states (S1 to S5). Each subtree has a terminal state (S\u2212 and S+). The solid and dashed lines represent different transition probabilities from an intermediate state (Si) to terminal nodes (S\u2212 and S+) depending on the actions taken (Aj). The key aspect illustrated is that the transition probabilities are manipulated to create variations within the feasible reward set which is central to the statistical complexity analysis presented in the theorem.", "section": "4.2 Lower Bound"}, {"figure_path": "7zzOcyT0hd/figures/figures_22_1.jpg", "caption": "Figure 5: Representation of the IRL-SE problem for the instances used in Theorem E.4.", "description": "This figure illustrates an example MDP used in the proof of Theorem E.4, which focuses on lower-bounding the sample complexity of identifying feasible reward functions in the presence of suboptimal experts. The MDP has states {s_root, s_1, ..., s_S, s_sink}, where s_root is the start state, s_1...s_S are intermediate states, and s_sink is the terminal state.  The optimal expert always chooses action a1, while the suboptimal expert chooses action a2 with probability \u03c0_min in states s_1...s_S. The structure and transition probabilities are designed to demonstrate the impact of having a suboptimal expert on the difficulty of learning the feasible reward set.", "section": "E.1 Proof of Theorem 4.1"}, {"figure_path": "7zzOcyT0hd/figures/figures_37_1.jpg", "caption": "Figure 2: Visualization of the feasible reward set (i.e., shaded red area) for the problems described in Example 3.5. On the left, the feasible reward set for the single-expert IRL problem and on the right the feasible reward set for the multiple and sub-optimal setting when using \u03bei = 0.5.", "description": "This figure shows a comparison of feasible reward sets between single-expert IRL and multi-agent IRL with suboptimal experts. The left panel shows the feasible reward set for a single optimal expert, while the right panel shows the feasible reward set when incorporating a suboptimal expert with a suboptimality bound \u03bei = 0.5.  The shaded red area represents the feasible reward set. The figure illustrates how the addition of suboptimal expert data reduces the ambiguity of inverse reinforcement learning by shrinking the feasible reward set.", "section": "Sub-Optimal Experts and the Feasible Reward Set"}]