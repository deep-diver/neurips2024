[{"figure_path": "7zzOcyT0hd/tables/tables_7_1.jpg", "caption": "Table 1: Notation", "description": "This table summarizes the notations used in the paper.  It lists symbols and their meanings, including those related to the Inverse Reinforcement Learning (IRL) problem with a single optimal expert, the IRL problem with multiple sub-optimal experts, the number of sub-optimal experts, policies of experts, sub-optimality of experts, empirical estimates, datasets, and feasible reward sets for both single and multiple expert settings.  It also defines the Hausdorff distance, the learning algorithm, sampling strategy, sample complexity, accuracy, and risk parameters.", "section": "2 Preliminaries"}, {"figure_path": "7zzOcyT0hd/tables/tables_12_1.jpg", "caption": "Table 1: Notation", "description": "This table lists the symbols and notations used throughout the paper.  It includes mathematical notations, variable names, and set definitions relevant to the Inverse Reinforcement Learning problem with multiple and sub-optimal experts (IRL-SE), including the number of sub-optimal experts, policies, sub-optimality bounds, and various sets related to reward functions and their estimations.", "section": "2 Preliminaries"}, {"figure_path": "7zzOcyT0hd/tables/tables_13_1.jpg", "caption": "Table 2: Operators", "description": "This table defines the operators used throughout the paper.  The operators act on reward functions (R<sup>S</sup> or R<sup>SxA</sup>) and policies (\u03c0).  For example, the 'P' operator represents the Bellman expectation operator for the transition probability kernel. '\u03c0' is the policy operator,  'E' extracts the state-based component of a state-action function, B<sup>\u03c0</sup> masks the action-value function for actions that have zero probability under the policy. B<sup>\u00ac\u03c0</sup> is the complement of B<sup>\u03c0</sup>, selecting actions with non-zero policy probability. d<sup>\u03c0</sup> is the discounted state occupancy measure, and I<sub>S</sub> is the identity operator on functions of the state space.", "section": "2 Preliminaries"}]