[{"figure_path": "LvJ1R88KAk/tables/tables_6_1.jpg", "caption": "Table 1: Ablation on the impact of each distinction.", "description": "This table presents the results of ablation experiments conducted to assess the impact of six design choices in the Mamba model on ImageNet-1K classification.  The baseline is a Swin Transformer model with Softmax attention replaced by linear attention. Each row adds or removes one of the Mamba design elements (input gate, forget gate, shortcut, normalization, multi-head design, and block design), showing the effect on the model's performance in terms of parameters, FLOPs, throughput, and top-1 accuracy.", "section": "5.2 Empirical Analysis of the Differences"}, {"figure_path": "LvJ1R88KAk/tables/tables_7_1.jpg", "caption": "Table 3: Comparison with SOTA Vision Mambas on ImageNet-1K.", "description": "This table compares the performance of the proposed MILA model with other state-of-the-art vision Mamba models on the ImageNet-1K dataset.  It shows the number of parameters, FLOPS (floating point operations per second), and top-1 accuracy for each model.  The table highlights MILA's superior performance and scalability across various model sizes.", "section": "5 Empirical Study"}, {"figure_path": "LvJ1R88KAk/tables/tables_7_2.jpg", "caption": "Table 1: Ablation on the impact of each distinction.", "description": "This table presents the results of an ablation study on ImageNet-1K, evaluating the impact of six design distinctions between Mamba and linear attention Transformer.  The baseline model uses linear attention. Each row adds one of the following: input gate, forget gate, shortcut, normalization, multi-head design, and the Mamba block design (two variants).  The table shows the model's #Params, FLOPs, throughput, and Top-1 accuracy for each variation.  This allows for assessing the relative contribution of each design choice to the overall performance.", "section": "5.2 Empirical Analysis of the Differences"}, {"figure_path": "LvJ1R88KAk/tables/tables_8_1.jpg", "caption": "Table 5: Results of semantic segmentation using UperNet [46]. The FLOPs are computed with input resolution of 512\u00d72048.", "description": "This table presents the results of semantic segmentation on the ADE20K dataset using the UperNet model.  It compares the performance of different backbones (Swin-B, MambaOut-B, VMamba-B, and MILA-B) in terms of model parameters (#Params), floating-point operations (FLOPs), and mean Intersection over Union (mIoU) scores.  The mIoU is reported for both single-scale (SS) and multi-scale (MS) inference.", "section": "5.3 Comparison with Mamba in Vision"}, {"figure_path": "LvJ1R88KAk/tables/tables_9_1.jpg", "caption": "Table 1: Ablation on the impact of each distinction.", "description": "This table presents the ablation study results on ImageNet-1K, evaluating the impact of six design choices (input gate, forget gate, shortcut, normalization, multi-head design, and block design) on the baseline linear attention model.  It shows the number of parameters, FLOPs (floating point operations), throughput, and Top-1 accuracy for each variation.  The results highlight the relative importance of each design choice in improving the model's performance.", "section": "5.2 Empirical Analysis of the Differences"}, {"figure_path": "LvJ1R88KAk/tables/tables_14_1.jpg", "caption": "Table 6: Comparison with advanced linear attention designs.", "description": "This table compares the performance of MILA with other advanced linear attention methods on ImageNet-1K classification.  It shows that MILA achieves a higher accuracy (83.5%) than the other methods, while maintaining a relatively low number of parameters and FLOPs.  This highlights MILA's effectiveness and efficiency compared to state-of-the-art linear attention approaches.", "section": "Additional Experimental Results"}, {"figure_path": "LvJ1R88KAk/tables/tables_15_1.jpg", "caption": "Table 7: MILA models trained without MESA.", "description": "This table presents the results of MILA models trained without the MESA (an overfitting prevention strategy) technique. It compares the performance of MILA models (with and without MESA) to other vision Mamba models, showcasing the effectiveness of the MILA models even without MESA.  The table includes the number of parameters, FLOPs (floating-point operations), and top-1 accuracy for each model. The results highlight that MILA models consistently achieve higher accuracy than the other Vision Mamba models, suggesting its robustness and efficiency.", "section": "5 Empirical Study"}, {"figure_path": "LvJ1R88KAk/tables/tables_16_1.jpg", "caption": "Table 1: Ablation on the impact of each distinction.", "description": "This table presents the results of ablation studies performed on ImageNet-1K to evaluate the impact of six design choices differentiating Mamba from a standard linear attention Transformer.  The baseline is a Swin Transformer with Softmax attention replaced by linear attention. Each row adds one Mamba design choice (input gate, forget gate, shortcut, normalization, multi-head, block design).  The table shows the number of parameters, FLOPs (floating-point operations), throughput (images/second), and Top-1 accuracy for each model variant.", "section": "5 Empirical Study"}]