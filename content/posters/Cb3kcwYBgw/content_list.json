[{"type": "text", "text": "Spatio-Spectral Graph Neural Networks ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Simon Geisler\u2020, Arthur Kosmala\u2020, Daniel Herbst, and Stephan G\u00fcnnemann Department of Computer Science & Munich Data Science Institute Technical University of Munich {s.geisler, a.kosmala, d.herbst, s.guennemann}@tum.de ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Spatial Message Passing Graph Neural Networks (MPGNNs) are widely used for learning on graph-structured data. However, key limitations of $\\ell\\cdot$ -step MPGNNs are that their \u201creceptive field\u201d is typically limited to the $\\ell$ -hop neighborhood of a node and that information exchange between distant nodes is limited by oversquashing. Motivated by these limitations, we propose Spatio-Spectral Graph Neural Networks $(S^{2}G N\\bar{N}S)$ \u2013 a new modeling paradigm for Graph Neural Networks (GNNs) that synergistically combines spatially and spectrally parametrized graph filters. Parameterizing filters partially in the frequency domain enables global yet efficient information propagation. We show that $\\dot{\\bf S}^{2}{\\bf G N N s}$ vanquish over-squashing and yield strictly tighter approximation-theoretic error bounds than MPGNNs. Further, rethinking graph convolutions at a fundamental level unlocks new design spaces. For example, $\\mathrm{S}^{2}\\mathrm{GNNs}$ allow for free positional encodings that make them strictly more expressive than the 1-Weisfeiler-Leman (WL) test. Moreover, to obtain general-purpose $\\mathrm{S}^{2}\\mathrm{GNNs}$ , we propose spectrally parametrized filters for directed graphs. $\\bar{\\mathbf{S}^{2}}\\mathbf{GNNs}$ outperform spatial MPGNNs, graph transformers, and graph rewirings, e.g., on the peptide long-range benchmark tasks, and are competitive with state-of-the-art sequence modeling. On a 40 GB GPU, $\\mathrm{S}^{2}\\mathrm{GNNs}$ scale to millions of nodes. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Spatial Message-Passing Graph Neural Networks (MPGNNs) ushered in various recent breakthroughs. For example, MPGNNs are able to predict the weather with unprecedented precision (Lam et al., 2023), can be composed as a foundation model for a rich set of tasks on knowledge graphs (Galkin et al., 2023), and are a key component in the discovery of millions of AI-generated crystal structures (Merchant et al., 2023). Despite this success, MPGNNs produce nodelevel signals solely considering limited-size neighborhoods, effectively bounding their expressivity. Even with a large number of message-passing steps, MPGNNs are limited in their capability of propagating information to distant nodes due to over-squashing. As evident by the success of global models like transformers (Vaswani et al., 2017), modeling long-range interactions can be pivotal and an important step towards foundation models that understand graphs. ", "page_idx": 0}, {"type": "image", "img_path": "Cb3kcwYBgw/tmp/062589a944313f6c7172538a8d80235e3f9a81c41c246fc60c3eebce2f1177d9.jpg", "img_caption": ["Figure 1: $\\mathsf{S}^{2}\\mathrm{GNN}$ principle. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "We propose Spatio-Spectral Graph Neural Networks $(S^{2}G N N s)$ , a new modeling paradigm for tackling the aforementioned limitations, that synergistically combine message passing with spectral filters, explicitly parametrized in the spectral domain. Spectral filters are virtually ignored by prior work but go beyond stacks of message-passing layers or polynomial parametrizations. Due to message passing\u2019s finite number of propagation steps, it comes with a distance cutoff $p_{\\mathrm{cut}}$ (# hops, see Fig. 1). Conversely, spectral filters act globally $(p_{\\mathrm{max}})$ , even on a truncated frequency spectrum $\\lambda_{\\mathrm{cut}}$ . Truncating the frequency spectrum for spectral filters is required for efficiency, yet message passing has access to the entire spectrum (right plots in Fig. 1). The combination of message passing and spectral filters provably leverages the strengths of each parametrization. Utilizing this combination, $\\bar{\\mathbf{S}}^{2}\\mathrm{GNNs}$ generalize the concept of \u201cvirtual nodes\u201d and distill many important properties of hierarchical message-passing schemes, graph-rewirings, and pooling into a single GNN (see Fig. 3). Outside of GNNs, a similar composition is at the core of some State Space Models (SSM) models (Poli et al., 2023), that deliver transformer-like properties with superior scalability on sequences \u2013 as do $\\mathrm{S}^{2}\\mathrm{GNNs}$ on graphs. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "Our analysis of $\\mathbf{S}^{2}\\mathbf{GNNs}$ $(\\S\\ 3.1)$ validates their capability for modeling long-range interactions. We prove in $\\S\\ 3.1.1$ that combining spectral and spatial filters alleviates the over-squashing phenomenon (Alon & Yahav, 2020; Di Giovanni et al., 2023a,b), a necessity for effective informationexchange among distant nodes. Our approximation-theoretic analysis goes one step further and proves strictly tighter error bounds in terms of approximation of the target idealized GNN (\u00a7 3.1.2). ", "page_idx": 1}, {"type": "text", "text": "Except for initial works like (Bruna et al., 2014) and in contrast to spatial MPGNNs, the design decisions for spectral filters are virtually unexplored \u2013 and so is their composition. The novel aspects of $\\mathrm{S^{2}G N N^{\\circ}}$ s design space include the spectral filter parametrization $(\\S\\ 3.2.1)$ . We propose the first permutation", "page_idx": 1}, {"type": "image", "img_path": "Cb3kcwYBgw/tmp/7e595f47b12a459e65a5521fe18977cf45aacae7af25c9dbc2b88f1d1555b92a.jpg", "img_caption": ["Figure 2: $\\mathsf{S}^{2}\\mathrm{GNN}$ framework with adjacency matrix $\\pmb{A}$ , node features $\\mathbf{\\deltaX}$ , and Laplacian $\\textbf{\\emph{L}}$ (function of $\\pmb{A}$ ). "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "equivariance-preserving neural network in the spectral domain $(\\S\\ 3.2.2)$ and generalize spectral filters to directed graphs $(\\S\\,3.2.3)$ . The dual use of the partial eigendecomposition, required for spectral filters, allows us to propose \u201cfree-of-cost\u201d positional encodings $\\left(\\S\\ 3.2.4\\right)$ , that are permutationequivariant, stable, and increase expressivity strictly beyond the 1-Weisfeiler-Leman (WL) test. ", "page_idx": 1}, {"type": "text", "text": "$\\mathbf{S}^{2}\\mathbf{GNNs}$ are effective and practical. We empirically verify the shortcomings of MPGNNs and how $\\mathrm{S}^{2}\\mathrm{GNNs}$ overcome them $(\\S\\,4)$ . E.g., we set a new state-of-the-art on peptides-func (Dwivedi et al., 2022) with $\\approx\\!35\\%$ fewer parameters, outperforming MPGNNs and graph transformers. Although sequences are just a subdomain of (directed) graphs, we also study how $\\mathrm{S}^{2}\\mathrm{GNNs}$ compare to specialized sequence models like transformers (Vaswani et al., 2017) or Hyena (Poli et al., 2023). We find that $\\bar{\\mathbf{S}^{2}}\\bar{\\mathbf{GNNs}}$ are highly competitive even though they operate on a much more general domain (un-/directed graphs). Last, the runtime and space complexity of $\\mathrm{S}^{2}\\mathrm{GNNs}$ is equivalent to MPGNNs and, with vanilla full-graph training, $\\mathrm{S}^{2}\\mathrm{GNN}\\dot{\\mathrm{s}}$ can handle millions of nodes with a 40 GB GPU. ", "page_idx": 1}, {"type": "text", "text": "2 Background ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We study graphs ${\\mathcal{G}}(A,X)$ with adjacency matrix $A\\,\\in\\,\\{0,1\\}^{n\\times n}$ (or $A\\,\\in\\,\\mathbb{R}_{\\geq0}^{n\\times n}$ if weighted), node features $\\pmb{X}\\in\\mathbb{R}^{n\\times d}$ and edge count $m$ . $\\pmb{A}$ is symmetric for undirected graphs and, thus, has eigendecomposition $\\lambda$ $\\lambda,V=\\operatorname{EVD}(A)$ with eigenvalues $\\pmb{\\lambda}\\in\\mathbb{R}^{n}$ and eigenvectors $V\\in\\mathbb{R}^{n\\times n}$ : $\\mathbf{A}=$ $V\\Lambda V^{\\top}$ using $\\mathbf{A}=\\mathrm{diag}(\\lambda)$ . Instead of $\\pmb{A}$ , we decompose the Laplacian $L:=I-D^{-1/2}A D^{-1/2}$ , with diagonal degree matrix $D=\\mathrm{diag}(A{\\vec{1}})$ , since its ordered eigenvalues $0\\,=\\,\\lambda_{1}\\,\\leq\\,\\lambda_{2}\\,\\leq\\,\\cdots\\,\\leq$ $\\lambda_{n}~\\leq~2$ are similar to frequencies (e.g., low eigenvalues relate to low frequencies, see Fig. 4). Likewise, one could use, e.g., $L=I{-}\\bar{D}^{-1}A$ or more general variants (Yang et al., 2023); however, we focus our explanations on the most common choice $L:=I-D^{-1/2}\\bar{A D}^{-1/2}$ . We choose the matrix of eigenvectors $V\\in\\mathbb{R}^{n\\times n}$ to be orthogonal $V V^{\\top}=I$ . We refer to $V$ as the Fourier basis of the graph, with Graph Fourier Transformation (GFT) $\\hat{X}=V^{\\top}X$ and its inverse $\\pmb{X}=\\pmb{V}\\hat{\\pmb{X}}$ . To provide an overview, Table 5 lists the symbols used in this work. ", "page_idx": 1}, {"type": "text", "text": "Spectral graph filters. Many GNNs implement a graph convolution, where node signal $\\pmb{X}\\in\\mathbb{R}^{n\\times d}$ is convolved $g*_{\\mathcal{G}}X$ for every $d$ with a scalar fliter $\\pmb{g}\\in\\mathbb{R}^{n}$ . The graph convolution (Hammond et al., 2011) is defined in the spectral domain as $g*_{\\mathcal{G}}X:=V([V^{\\top}g]\\odot[V^{\\top}X])$ , with elementwise product $\\odot$ and broadcast of $V^{\\top}g$ to match shapes. Instead of spatial $\\textbf{\\textit{g}}$ , spectral graph filters parametrize ${\\hat{g}}:\\left[0,2\\right]\\rightarrow\\mathbb{R}$ explicitly and yield $V^{\\top}g:=\\hat{g}(\\pmb{\\lambda})\\in\\mathbb{R}^{n}$ as a function of the eigenvalues. ", "page_idx": 1}, {"type": "text", "text": "Message Passing Graph Neural Networks (MPGNNs) circumvent the EVD via polynomial $\\begin{array}{r}{\\hat{g}(\\lambda)_{u}=\\sum_{j=0}^{p}\\gamma_{j}\\lambda_{u}^{j}}\\end{array}$ since $\\begin{array}{r}{V[\\sum_{j=0}^{p}\\gamma_{j}\\operatorname{diag}(\\lambda)^{j}]V^{\\top}X\\,=\\,\\sum_{j=0}^{p}\\gamma_{j}L^{j}\\dot{X}}\\end{array}$ . In practice, many MPGNNs use $p\\,=\\,1$ : $\\mathbf{\\cal{H}}^{(l)}\\,=\\,(\\gamma_{0}\\pmb{{\\cal{I}}}+\\gamma_{1}\\pmb{{\\cal{L}}})\\pmb{{\\cal{H}}}^{(l-1)}$ with $H^{(0)}=X$ , and stack $1\\leq l\\leq\\ell$ layers interleaved with node-wise transformations and activations $\\sigma$ . We refer to Balcilar et al. (2021b) for similar interpretations of MPGNNs like GAT (Velic\u02c7kovic\u00b4 et al., 2018) or GIN ( $\\mathrm{\\DeltaXu}$ et al., 2019). ", "page_idx": 2}, {"type": "text", "text": "3 Method ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "$S^{2}G N N s$ symbiotically pair spatial Spatial $(H^{(l-1)};A)$ MPGNNs and Spectral $(H^{(l-1)};V,\\lambda)$ filters, using a partial eigendecomposition. Even though the spectral filter operates on a truncated eigendecomposition (spectrally bounded), it is spatially unbounded. Conversely, spatial MPGNNs are spatially bounded yet spectrally unbounded (see Fig. 1). ", "page_idx": 2}, {"type": "image", "img_path": "Cb3kcwYBgw/tmp/978acd9889d10bc165bc4d48075a229524b4a6004276fc028b8a9ffbbf417ad3.jpg", "img_caption": ["Figure 3: Message-passing interpretation of $V(\\bar{g_{\\vartheta}}(\\bar{\\pmb{\\lambda}})\\odot[\\bar{\\pmb{V}}^{\\top}\\pmb{X}])$ (spectral filter): via the Fourier coefficients they may exchange information globally and allow intraand inter-cluster message passing. Edge width/color denotes the magnitude/sign of $V$ . "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "A spectrally bounded filter is sensible for modeling global pair-wise interactions, considering its message-passing interpretation of Fig. 3. Conceptually, a spectral filter consists of three steps: $\\textcircled{1}$ Gather: The multiplication of the node signal with the eigenvectors $\\pmb{v}_{u}^{\\top}\\pmb{X}$ (GFT) is a weighted and signed aggregation over all nodes; $\\circledcirc$ Apply: the \u201cFourier coefficients\u201d are weighted; and $\\circled{3}$ Scatter broadcasts the signal $\\pmb{v}_{u}\\hat{X}$ back to the nodes (inverse GFT). The first eigenvector (here for $L=D-A)$ acts like a \u201cvirtual node\u201d (Gilmer et al., 2017) (see also $\\S\\ E$ ). That is, it calculates the average embedding and then distributes this information, potentially interlayered with neural networks. Importantly, the other eigenvectors effectively allow messages to be passed within or between clusters. As we show for exemplary graphs in Fig. 4, low frequencies/eigenvalues capture coarse structures, while high(er) frequencies/eigenvalues capture details. For example, the second eigenvector in Fig. 4b contrasts the inner with the outer rectangle, while the third eigenspace models both symmetries up/down and left/right. In conclusion, $\\mathrm{S}^{2}\\mathrm{GNNs}$ augment spatial message-passing with a graph", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "adaptive hierarchy (spectral filter). Thus, $\\mathbf{S}^{2}\\dot{\\mathbf{G}}\\mathbf{N}\\mathbf{Ns}$ distill many important properties of hierarchical message-passing schemes (Bodnar et al., 2021), graph-rewirings (Di Giovanni et al., 2023a), pooling (Lee et al., 2019) etc. See $\\S\\,\\mathrm{J.}1$ for more examples. ", "page_idx": 2}, {"type": "image", "img_path": "Cb3kcwYBgw/tmp/c6ffbc3e838b3f02d301221ca50b094dbfba9f27b93cf32ce6b7d272011f3c35.jpg", "img_caption": ["Figure 4: Exemplary (lowest) eigenspaces. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "$\\mathbf{S}^{2}\\mathbf{G}\\mathbf{N}\\mathbf{N}^{\\ast}$ \u2019s composition. We study (1) an additive combination for its simpler approximationtheoretic interpretation $(\\S\\ 3.1.2)$ , or (2) an arbitrary sequence of filters due to its flexibility. In both cases, residual connections may be desirable (see $\\S\\,\\mathrm{J}.2\\rangle$ ). ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pmb{H}^{(l)}=\\mathrm{Spectral}^{(l)}(\\pmb{H}^{(l-1)};\\pmb{V},\\pmb{\\lambda})+\\mathrm{Spatial}^{(l)}(\\pmb{H}^{(l-1)};\\pmb{A})}\\\\ &{\\pmb{H}^{(\\ell)}=(h^{(\\ell)}\\circ h^{(\\ell-1)}\\circ\\cdots\\circ h^{(1)})(\\pmb{H}^{(0)})\\quad\\mathrm{~with~}h^{(j)}\\in\\{\\mathrm{Spectral},\\mathrm{Spatial}\\}}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "Spectral Filter. The building block that turns a spatial MPGNN into an $\\mathsf{S}^{2}\\mathrm{GNN}$ is the spectral filter: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\operatorname{Spectral}^{(l)}(H^{(l-1)};V,\\lambda)=V\\Big(\\hat{g}_{\\vartheta}^{(l)}(\\lambda)\\odot\\big[V^{\\top}f_{\\theta}^{(l)}(H^{(l-1)})\\big]\\Big)\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "with a point-wise transformation $f_{\\theta}^{(l)}:\\mathbb{R}^{n\\times d^{(l-1)}}\\rightarrow\\mathbb{R}^{n\\times d^{(l)}}$ , a learnable spectral filter $\\hat{g}_{\\vartheta}^{(l)}(\\lambda)\\in$ k\u00d7d(l) parameterized element-wise as $\\hat{g}_{\\vartheta}^{(l)}(\\pmb{\\lambda})_{u,v}:=\\hat{g}_{v}^{(l)}(\\lambda_{u};\\vartheta_{v})$ (see $\\S\\ 3.2.1)$ , and truncated $V\\in\\mathbb{R}^{n\\times k}$ , $\\lambda\\in\\mathbb{R}^{k}$ . Due to the combination of message passing with spectral filters, $\\mathrm{S}^{2}\\mathrm{GNNs}^{\\circ}$ hypothesis class goes beyond (finite-order) polynomials of the Laplacian $\\textbf{\\emph{L}}$ (or stacks message passing layers), unlocking a larger class of filters. In Algo. 1, we provide pseudo code for $\\mathrm{S}^{2}\\mathrm{GNNs}$ (Eq. 1). ", "page_idx": 2}, {"type": "text", "text": "Truncated spectrum. We omit extra notation for the truncated eigendecompositon $\\mathrm{EVD}({\\cal L},k)$ since it is equivalent to define $\\hat{g}(\\lambda_{j})\\,=\\,0$ for $j\\,>\\,k$ . However, truncating after the $k$ -th eigenvector requires care with the last eigenspance to maintain permutation equivariance. Due to the ambiguity of eigenvectors in the presence of repeated eigenvalues, we must ensure that we only include eigenspaces in their entirety. That is, we only include $\\{\\lambda_{j}\\mid j\\,\\leq\\,k\\wedge\\lambda_{j}\\,\\neq\\,\\lambda_{k+1}\\}$ . Thus, Spectral $(H^{(l-1)};\\operatorname{EVD}(L,k))$ is permutation equivariant nonetheless. We defer all proofs to $\\S\\mathrm{~H~}$ . ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "text", "text": "Theorem 1. Spectral $(H^{(l-1)};\\operatorname{EVD}(L,k))$ of Eq. 3 is equivariant to all $n\\,\\times\\,n$ permutation matrices $P\\in\\mathcal P$ : Spectral $\\mathbf{\\nabla}^{\\left.P H^{(l-1)};\\mathrm{EVD}(P L P^{\\top},k)\\right)}=P$ Spectral( $\\mathbf{\\nabla}H^{(l-1)}$ ; EVD(L, k)). ", "page_idx": 3}, {"type": "text", "text": "Complementary high-resolution filters. Our Spectral filters are highly discriminative between the frequencies and, e.g., can readily access a single eigenspace. Yet, for efficiency, we limit the spectral filter to a specific frequency band. Due to the combination with message passing, this choice of band does not decide on, say, low-pass behavior; it solely determines where to increase the spectral selectivity. While $\\mathrm{S}^{2}\\mathrm{GNNs}$ with subsequent guarantees adapt to domain-specific choices for the spectral filter\u2019s frequency band, a sensible default is to focus on the low frequencies. The two main reasons for this are (see $\\S\\ J.3$ for an extensive list): (1) Low frequencies model the smoothest global signals w.r.t. the graph structure (see Fig. 3 & 4). (2) Under a relative perturbation model (perturbation budget proportional to degree), stability implies $C$ -integral-Lipschitzness $(\\exists C>0\\colon\\,|\\lambda{\\bar{d}}{\\hat{g}}/d\\lambda|\\leq C)$ , i.e., the filter can vary strongly around zero but must level out for large $\\lambda$ (see Gama et al. (2020)). ", "page_idx": 3}, {"type": "text", "text": "3.1 Theoretical Analysis ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We show that how $\\mathrm{S}^{2}\\mathrm{GNNs}$ alleviate oversquashing in $\\S\\ 3.1.1$ . Next, $\\S\\ 3.1.2$ makes the approximation-theoretic advantages precise. ", "page_idx": 3}, {"type": "image", "img_path": "Cb3kcwYBgw/tmp/f1e79b471118ebc7b4342929d651dd6b21a46377840e72f3a033c06636668111.jpg", "img_caption": [], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "3.1.1 $\\mathbf{S}^{2}$ GNNs Vanquish Over-Squashing ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Figure 5: Spectral filters do not exhibit oversquashing on \u201cClique Path\u201d graphs (Di Giovanni et al., 2023a). ", "page_idx": 3}, {"type": "text", "text": "Alon & Yahav (2020) pointed out that MPGNNs must pass information through bottlenecks that connect different communities using fixedsize embedding vectors. Topping et al. (2022) and Di Giovanni et al. (2023a) formalize this via an $L^{\\overline{{1}}}$ -norm Jacobian sensitivity analysis: $\\left\\|\\partial\\mathbf{h}_{v}^{(\\ell)}\\!\\right/\\partial\\mathbf{h}_{u}^{(0)}\\right\\|_{L^{1}}$ models the output\u2019s $\\mathbf{h}_{v}^{(\\ell)}$ change if altering input $\\check{\\mathbf{h}}_{u}^{(0)}$ ", "page_idx": 3}, {"type": "text", "text": "MPGNNs\u2019 Jacobian sensitivity typically decays $\\mathcal{O}\\left(\\exp\\left(-r\\right)\\right)$ with node distance $r$ if the number of walks between the two nodes is small. See $\\S\\,\\mathrm{F}$ for results of Di Giovanni et al. (2023a). ", "page_idx": 3}, {"type": "text", "text": "$\\mathrm{S}^{2}\\mathrm{GNNs}$ are not prone to such an exponential sensitivity decay due to their global message scheme. We formalize this in Theorem 2, refer to Fig. 4 for intuition and Fig. 5 for empirical verification. All theoretical guarantees hold if a $\\theta$ exists such that $f_{\\theta}=I$ . ", "page_idx": 3}, {"type": "text", "text": "Theorem 2. An $\\ell_{}$ -layer $S^{2}G N N$ can be parametrized s.t. output $\\mathbf{h}_{v}^{(\\ell)}$ has a uniformly lowerbounded Jacobian sensitivity on a connected graph: $\\left\\|\\partial\\mathbf{h}_{v}^{(\\ell)}\\middle/\\partial\\mathbf{h}_{u}^{(0)}\\right\\|_{L^{1}}^{\\star}\\geq C_{\\vartheta}d\\middle/m$ with rows $\\check{h_{u}^{(0)}}$ , $h_{v}^{(\\ell)}$ of $H^{(0)}$ , $H^{(\\ell)}$ for nodes u, $v\\in{\\mathcal{G}}$ , a parameter-dependent $C_{\\vartheta}$ , network width $d$ and edge count $m$ . ", "page_idx": 3}, {"type": "text", "text": "In contrast to Di Giovanni et al. (2023a), we prove a lower bound for $\\mathrm{S}^{2}\\mathrm{GNNs}$ , guaranteeing a minimum \u201cinfluence\u201d for any $u$ on $v$ . This is true since $\\mathrm{S}^{2}\\mathrm{GNNs}$ contain a virtual node as a special case with $\\hat{g}_{\\vartheta}^{(l)}(\\pmb{\\lambda})=\\mathbb{1}_{\\{0\\}}$ , with $\\mathbb{1}_{\\mathcal{S}}$ denoting the indicator function of a set $\\boldsymbol{S}$ (see also $\\S\\,\\mathrm{E}$ ). However, we find that a virtual node is insufficient for some long-range tasks, including our long-range clustering (LR-CLUSTER) of Fig. 10b. Hence, the exponential sensitivity decay of spatial MPGNNs only shows their inadequacy in long-range settings. Proving its absence is not sufficient to quantify long-range modeling capabilities, noting that the lower bound is not tight for $\\mathrm{S}^{2}\\mathrm{GNNs}$ on many graphs. We close this gap with our subsequent analysis rooted in polynomial approximation theory. ", "page_idx": 3}, {"type": "text", "text": "3.1.2 Approximation Theory: Superior Error Bounds Despite Spectral Cutoff ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To demonstrate how $\\mathrm{S}^{2}\\mathrm{GNNs}$ can express a more general hypothesis class than MPGNNs, we study how well an \u201cidealized\u201d GNN (IGNN) can be approximated. Each IGNN layer $l$ can express convolution operators $g^{(l)}$ of any spectral form $\\hat{g}^{(l)}\\colon^{\\star}[0,2]\\rightarrow\\mathbb{R}$ . We approximate IGNNs with $\\mathrm{S}^{2}\\mathrm{GNNs}$ from Eq. 1, with a spectral filter as in Eq. 3 and a spatial part parametrized by a polynomial. While we assume here that the $\\mathsf{S}^{2}\\mathrm{GNN}$ spectral filter is bandlimited to and a universal approximator on the interval $[0,\\lambda_{\\mathrm{max}}]$ , the findings generalize to, e.g., a high-pass interval. In the main body, we focus on the key insights for architectures without nonlinear activations. Wang & Zhang (2022) prove that even linear IGNNs can produce any one-dimensional output under certain regularity assumptions on the graph and input signal. Thus, we solely need to consider a single layer. In $\\S\\ H.4$ , we cover the generic setting including nonlinearities, where multiple layers are helpful. ", "page_idx": 3}, {"type": "text", "text": "Locality relates to spectral smoothness. The locality of the true/ideal filter $g$ is related to the smoothness of its Fourier transform $\\hat{g}$ . For instance, if $g$ is a low-order polynomial of $\\textbf{\\emph{L}}$ , it is localized to a few-hop neighborhood, and $\\hat{g}$ is regularized to vary slowly (Fig. 6a w/o discontinuity). The other extreme is a discontinuous spectral filter $\\hat{g}$ , such as the entirely non-local virtual node filter, ${\\hat{g}}=\\mathbb{1}_{\\{0\\}}$ (discontinuity in Fig. 6a, details in $\\S\\ E_{\\mathrm{~}}$ ). This viewpoint of ", "page_idx": 4}, {"type": "image", "img_path": "Cb3kcwYBgw/tmp/0bbfa5379198fb4410709d229fbb4030d8830b493e5a55194556a13583019cd8.jpg", "img_caption": [], "img_footnote": [], "page_idx": 4}, {"type": "image", "img_path": "Cb3kcwYBgw/tmp/0344818e525db0bdb58c4c0182acb3707a64121fa02e56eb7f620f8c7c143ebd.jpg", "img_caption": ["(a) Spectral domain "], "img_footnote": [], "page_idx": 4}, {"type": "text", "text": "Figure 6: $\\mathrm{S}^{2}$ filter perfectly approximates true filter (a) with a discontinuity at $\\lambda=0$ , while polynomial (\u201cSpa.\u201d) and spectral (\u201cSpec.\u201d) alone do not. (b) shows responses on a path graph. ", "page_idx": 4}, {"type": "text", "text": "spectral smoothness illuminates the limitations of finite-hop message passing from an angle that complements spatial analyses in the over-squashing picture. It informs a lower bound on the error, which shows that spatial message passing, i.e, order- $\\cdot p$ polynomial graph filters $g_{\\gamma_{p}}$ with $p+1$ coefficients $\\gamma_{p}\\in\\mathbb{R}^{\\bar{p}+1}$ , can converge exceedingly slowly \u2013 slower than any inverse root (!) of $p-$ to a discontinuous ground truth in the Frobenius-induced operator norm: ", "page_idx": 4}, {"type": "text", "text": "Theorem 3. Let $\\hat{g}$ be a discontinuous spectral filter. For any approximating sequence $\\left(g_{\\gamma_{p}}\\right)_{p\\in\\mathbb{N}}$ of polynomial filters, an adversarial sequence $(\\mathcal{G}_{p})_{p\\in\\mathbb{N}}$ of input graphs exists such that ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\nexists\\alpha\\in\\mathbb{R}_{>0}\\colon\\operatorname*{sup}_{0\\neq{\\pmb X}\\in\\mathbb{R}^{|\\mathcal{G}_{p}|\\times d}}\\frac{\\|(g_{\\gamma_{p}}-g)*\\mathcal{G}_{p}\\pmb X\\|_{\\mathrm{F}}}{\\|{\\pmb X}\\|_{\\mathrm{F}}}=\\mathcal{O}\\left(p^{-\\alpha}\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Superior $\\mathbf{S}^{2}\\mathbf{GNN}$ error bound. A spatio-spectral convolution wins over a purely spatial filter when the sharpest irregularities of the ground truth $\\hat{g}$ are within reach of its expressive spectral part. The spatial part, which can \u201cfocus\u201d on learning the remaining, smoother part outside of this window, now needs much fewer hops to give a faithful approximation. We illustrate this principle in Fig. 6 where we approximate an additive combination of an order-three polynomial filter with discontinuous lowpass. Only the $S^{2}$ filter is faithfully approximating this filter. Formally, we find: ", "page_idx": 4}, {"type": "text", "text": "Theorem 4. Assume $\\hat{g}\\big|_{[\\lambda_{c u t},2]}$ is $r$ -times continuously differentiable on $[\\lambda_{c u t},2]$ , and a bound $K_{r}(\\hat{g},\\lambda_{c u t})\\;\\geq\\;0$ such that $\\begin{array}{r}{\\left|\\frac{d^{r}}{d\\lambda^{r}}\\hat{g}(\\lambda)\\right|\\;\\leq\\;K_{r}(\\hat{g},\\lambda_{c u t})\\;\\forall\\lambda\\;\\in\\;[\\lambda_{c u t},2]}\\end{array}$ . An approximating $S^{2}G N N$ sequence with parameters $\\left(\\vartheta_{p}^{\\ast},\\gamma_{p}^{\\ast}\\right)_{p\\in\\mathbb{N}}$ exists such that, for arbitrary graph sequences $(\\mathcal{G}_{p})_{p\\in\\mathbb{N}}$ , ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{0\\neq X\\in\\mathbb{R}^{|\\mathcal{G}_{p}|\\times d}}\\frac{\\Vert(g_{\\gamma_{p}^{*}}+g_{\\vartheta_{p}^{*}}-g)*g_{\\mathcal{G}_{p}}\\,X\\Vert_{F}}{\\Vert X\\Vert_{F}}=\\mathcal{O}\\left(K_{r}(\\hat{g},\\lambda_{c u t})p^{-r}\\right)\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "with a scaling constant that depends only on $r$ , not on $\\hat{g}$ or $(\\mathcal{G}_{p})_{p\\in\\mathbb{N}}$ . ", "page_idx": 4}, {"type": "text", "text": "The above bound extends to purely spatial convolutions in terms of $K_{r}(\\hat{g},0)$ if $\\hat{g}$ is $r$ -times continuously differentiable on the full interval $[0,2]$ . The $\\mathsf{S}^{2}\\mathrm{GNN}$ bound of Theorem 4 is then still strictly tighter if $K_{r}(\\hat{g},\\lambda_{\\mathrm{cut}})<K_{r}(\\hat{g},0)$ . In particular, taking the limit $K_{1}(\\hat{g},0)\\rightarrow\\infty$ towards discontinuity makes the purely spatial upper bound arbitrarily loose, whereas a benign filter might still admit a small $K_{1}(\\hat{g},\\bar{\\lambda_{\\mathrm{cut}}})$ for some $\\lambda_{\\mathrm{cut}}>0$ . Theorem 3 suggests that this is not an artifact of a loose upper bound but that there is an inherent difficulty in approximating unsmooth filters with polynomials. ", "page_idx": 4}, {"type": "text", "text": "We conclude the analysis by instantiating the bounds: assuming $\\hat{g}$ is $C$ -integral-Lipschitz for stability reasons (see Gama et al. (2020) and the paragraph before $\\S\\ 3.1.1)$ yields $K_{1}\\bar{(g},\\lambda_{\\mathrm{cut}})\\,=\\,C/\\lambda_{\\mathrm{cut}}.$ , whereas for the electrostatics example ${\\hat{g}}_{\\sigma}$ in $\\S\\,\\mathrm{G}$ , we find upper bounds $K_{r}(\\hat{g}_{\\sigma},\\lambda_{\\mathrm{cut}})=r!/\\lambda_{\\mathrm{cut}}^{(r+1)}$ . In both cases, the pure spatial bound diverges as smoothness around 0 remains unconstrained. ", "page_idx": 4}, {"type": "text", "text": "3.2 Design Space ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "As shown in Fig. 2, we identify three major, yet unexplored, directions in $\\mathrm{S}^{2}\\mathrm{GNNs}^{\\circ}$ design space. In $\\S\\,3.2.1$ , we discuss how we parametrize the spectral filter. In $\\S\\,3.2.2$ , we propose the first neural network for the spectral domain. That is, we allow transformations and non-linearities in the \u201cFourier\u201d domain. In $\\S\\ 3.2.3$ , we are the first to instantiate spectral filters for directed graphs. Additionally, due to the availability of the partial eigendecomposition, positional encodings may dual use them to improve epxressivity at negligible cost. In $\\S\\ 3.2.4$ , we propose the first permutation equivariant, stable and efficient positional encodings that provably admit an expressivity beyond 1-WL. $\\S\\,\\mathrm{J}$ provides further details and considerations, like some remarks on batching $(\\S\\,\\mathrm{\\bf~J}.7)$ . For the (sub-) design space of spatial message passing (You et al., 2020), we refer to its rich literature. ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "3.2.1 Parametrizing Spectral Filters ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "For spectral filter function ${\\hat{g}}_{\\vartheta}(\\lambda)$ of Eq. 3, we learn a channelwise linear combination of translated Gaussian basis functions (see \"Gaussian smearing\" used by Sch\u00fctt et al. (2017)), as depicted in Fig. 7. This choice (1) may represent any possible ${\\hat{g}}_{\\vartheta}(\\lambda)$ with sufficient resolution (assumption in $\\S\\ 3.1.2)$ ; (2) avoids overfitting towards numerical inaccuracies of the eigenvalue calculation; (3) limits the discrimination of almost repeated eigenvalues and, in turn, should yield stability (similar to $\\S\\ 3.2.4)$ . Strategies to cope with a variable $\\lambda_{\\mathrm{cut}}$ and $k$ (e.g., using attention similar to SpecFormer (Bo et al., 2023a)) did usually not yield superior experimental results. ", "page_idx": 5}, {"type": "image", "img_path": "Cb3kcwYBgw/tmp/0b124f6f82fe33509e060e3773acd2810fdce7964331d7517866f9fc74beb33d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 7: $\\hat{g}_{\\vartheta}(\\lambda)$ with S ${\\mathrm{mearing}}(\\lambda)\\;:\\;\\;[0,2]^{k}\\;\\;\\;\\rightarrow$ $\\mathbb{R}^{k\\times z}$ , linear map $W\\in\\mathbb{R}^{z\\times d}$ $~(\\vartheta~=~\\{W\\})$ , and fixed window function Window $(\\lambda)$ . ", "page_idx": 5}, {"type": "text", "text": "Window. We multiply the learned combinations of Gaussians by an envelope function (we choose a Tukey window) that decays smoothly to zero around cutoff $\\lambda_{\\mathrm{cut}}$ . This counteracts the so-called \u201cGibbs phenomenon\u201d (aka \u201cringing\u201d): as visualized for a path graph/sequence of 100 nodes in Fig. 8, trying to approximate a spatially-discontinuous target signal using an ideal low-pass range of frequency components results in an overshooting oscillatory behavior near the spatial discontinuity. Dampening the frequencies near $\\lambda_{\\mathrm{cut}}$ by a smooth envelope/window function alleviates this behavior. We note that the learned filter may, in principle, overrule the windowing at the cost of a higher weight decay penalty. See Algo. 2 for ${\\hat{g}}_{\\vartheta}(\\lambda)$ \u2019s algorithmic description. ", "page_idx": 5}, {"type": "text", "text": "Depth-wise separable convolution (Sifre, 2014; Howard et al., 2017): Applying different filters for each dimension is computationally convenient for spectral filters. While \u201cfull\u201d convolutions are also possible, we find that such a construction is more prone to over-fitting. In practice, we even use parameter sharing and apply fewer filters than dimensions to counteract over-fitting. We argue that sharing filters among dimensions is similar to the heads in a transformer (Vaswani et al., 2017). ", "page_idx": 5}, {"type": "image", "img_path": "Cb3kcwYBgw/tmp/723da25ab3aaf38e47f8a2a1bd0c9bb9ce1203910058c52913617014cea5d988.jpg", "img_caption": ["Figure 8: Ringing of ideal low "], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Feature transformations f \u03b8(l ). As sketched in Fig. 3 & 4, all nodes pass filter on path graph. participate in the global data transfer. While this global message-passing scheme is graph-adaptive, it does not adjust to the inputs. For adaptivity, we typically consider non-linear feature transformations f \u03b8(l\u22121)(H(l\u22121)), like gating mechanism f \u03b8(l $f_{\\theta}^{(l-1)}\\dot{(\\pmb{H}^{(l-1)})}=\\pmb{H}^{(l-1)}\\odot\\sigma^{\\prime}(\\pmb{H}^{(l-1)}\\pmb{W}_{G}^{(l)}+\\vec{1}\\pmb{b}^{\\top}))$ with element-wise multiplication $\\odot$ , SiLU function $\\sigma^{\\prime}$ , learnable weight $W$ , and bias $^{b}$ . A linear transformation f \u03b8(l )( $f_{\\theta}^{(l)}(H^{(l^{\\frac{1}{-1}})})=H^{(l^{\\stackrel{\\cdot}{-1}})}W^{(l)}$ is another interesting case since we may first apply the GFT and then the transformation: $(V^{\\top}H^{(l-1)})W^{(l)}$ . Next, we extend this linear transformation to a neural network in the spectral domain by adding multiple transformations and nonlinearities. ", "page_idx": 5}, {"type": "text", "text": "3.2.2 Neural Network for the Spectral Domain ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Applying a neural network $s_{\\zeta}$ in the spectral domain is highly desirable due to its negligible computational cost if $k\\ll n$ . Moreover, $s_{\\zeta}$ allows the spectral filter to become data-dependent and may mix between channels. Data-dependent filtering is one of the properties that is hypothesized to make torfa ngsrfaoprh mfeilrtse rps $s_{\\zeta}^{(l)}:\\mathbb{R}^{k\\times d^{(l)}}\\rightarrow\\mathbf{\\dot{R}}^{k\\times\\hat{d}^{(l)}}$ Wthea tp rios pdoessei gtnhee df tiros t pnreesuerravl en eptewromrukt aftoiro tnh eeq supievcatrriaaln cdeo.main ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\pmb{H}^{(l)}=\\mathrm{Spectral}^{(l)}(\\pmb{H}^{(l-1)};\\pmb{V},\\pmb{\\lambda})=V s_{\\zeta}^{(l)}\\Big(\\hat{g}_{\\vartheta}^{(l)}(\\pmb{\\lambda})\\odot\\big[V^{\\top}f_{\\theta}^{(l)}(\\pmb{H}^{(l-1)})\\big]\\Big)\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "We achieve permutation equivariance via sign equivariance $s_{\\zeta}(\\pmb{S}\\odot\\pmb{X})\\;=\\;\\pmb{S}\\odot s_{\\zeta}(\\pmb{X})\\,,\\,\\forall\\pmb{S}\\;\\in$ $\\{-1,1\\}^{k\\times d^{(l)}}$ , combined with a permutation equivariance $s_{\\zeta}(P X)\\ =\\ P s_{\\zeta}(X)\\,,\\,P\\ \\in\\ {\\mathcal P}_{k}.$ , where $\\mathcal{P}_{k}$ is the set of all $k\\,\\times\\,k$ permutation matrices. Specifically, we stack linear mappings Ws \u2208Rd(l)\u00d7d(l) (without bias) with a gated nonlinearity $\\phi(\\bar{H})=\\hat{H^{\\mathrm{~\\tiny~\\textcircled~{~\\textcent~}~}}}\\sigma(\\vec{1}\\,[m^{\\top}W_{a}+{\\pmb b}_{a}^{\\top}])$ with sigmoid \u03c3, column-wise norm mj = \u2225H\u02c6:,j\u2225, and learnable Wa \u2208Rd(l)\u00d7d(l) as well as ba \u2208Rd(l). ", "page_idx": 5}, {"type": "text", "text": "3.2.3 Directed Graphs ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Directed graphs are an important topic that did not discuss so far. For $\\mathrm{S}^{2}\\mathrm{GNNs}$ to generalize the capabilities of non-local sequence models like transformers (Vaswani et al., 2017) or SSMs (Poli et al., 2023; Gu & Dao, 2023) it is vital to support direction, e.g., for distinguishing source/beginning and sink/end. However, all discussion before assumed the existence of the eigenvalue decomposition of $\\textbf{\\emph{L}}$ . This was the case for symmetric $\\textbf{\\emph{L}}$ ; however, for directed graphs, $\\textbf{\\emph{L}}$ may be asymmetric. ", "page_idx": 6}, {"type": "text", "text": "To guarantee $\\textbf{\\emph{L}}$ is diagonalizable with real eigenvalues, we use the Magnetic Laplacian (Forman, 1993; Shubin, 1994; De Verdi\u00e8re, 2013) which is Hermitian and models direction in the complex domain: $L_{q}=I-\\big(D_{s}^{-1/2}A_{s}D_{s}^{-1/2}\\big)\\odot\\exp[i2\\pi q({\\bf A}-{\\bf A}^{\\top})]$ with symmetrized adjacency/degrees $A_{s}/D_{s}$ , potential $q\\,\\in\\,[0,2\\pi]$ , element-wise exponential exp, and imaginary unit $i^{\\check{2}}=-\\check{1}$ . While other parametrizations of a Hermitian matrix are also possible, with $\\bar{\\pmb{A}}\\in\\{0,\\bar{1}\\}^{n\\times n}$ and appropriate choice of $q$ , $L_{q}\\,:\\,\\{0,1\\}^{n\\times n}\\,\\to\\,\\mathbb{C}^{n\\times n}$ is injective. In other words, every possible asymmetric $\\pmb{A}$ maps to exactly one $\\pmb{L}_{q}$ and, thus, this representation is lossless. Moreover, for sufficiently small potential $q$ , the order of eigenvalues is well-behaved (Furutani et al., 2020). In contrast to Koke & Cremers (2024), a Hermitian parametrization of spectral filters does not require a dedicated propagation for forward and backward information flow. For simplicity we choose $q<1/n_{\\mathrm{max}}$ with maximal number of nodes $n_{\\mathrm{max}}$ (with binary $\\pmb{A}$ ). This choice ensures that the first eigenvector suffices to obtain, e.g., the topological sorts of a Directed Acyclic Graph (DAG). Due to the real notably, we use a feature transformation f \u03b8(l): Rn\u00d7d(l\u22121)\u2192Cn\u00d7d(l) and map back into the real eigenvalues of a Hermitian matrix, the presented content g eneralizes w ith minor adjustments. Most domain after the spectral convolution. We give more implementation details in $\\S\\ J.6$ and provide additional background on directed graphs in $\\S\\,C$ . ", "page_idx": 6}, {"type": "text", "text": "3.2.4 Efficient Yet Stable and Expressive Positional Encodings ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "The availability of the partial eigendecomposition allows for their dual use for positional encodings at negligible cost. Motivated by this, we propose the first efficient $(\\mathcal{O}(k m))$ and (fully) permutation equivariant spectral Positional Encodings PE that provably increase the expressivity strictly beyond the 1-Weisfeiler-Leman (1-WL) test ( $\\mathrm{Xu}$ et al., 2019; Morris et al., 2019). In contrast to the Laplacian encodings of Dwivedi & Bresson (2021), our PE do not require augmenting eigenvectors w.r.t. their sign and maintain permutation equivariance also in the presence of repeated eigenvalues. In comparison to Huang et al. (2024), our PE come with drastically lower computational cost and have no learnable parameters. Due to the absence of learnable parameters, we need to calculate our PE only once. ", "page_idx": 6}, {"type": "text", "text": "We construct our $k$ -dimensional positional encodings $\\mathrm{PE}(V,\\lambda)\\in\\mathbb{R}^{n\\times k}$ as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{PE}(V,\\lambda)=||_{j=1}^{k}[(V\\hat{h}_{j}(\\lambda)V^{\\top})\\odot A]\\cdot\\vec{1}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "image", "img_path": "Cb3kcwYBgw/tmp/04b18a5c78731521bd6e5083e466c948852a607a8602b1442a6d9b32b3499d22.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "with concatenation $||$ and binary adjacency $A\\ \\in\\ \\{0,1\\}^{n\\times n}$ . We use a Radial Basis Function (RBF) filter with normalization around each eigenvalue $\\hat{h}_{j}(\\pmb{\\lambda})=$ softmax $\\left((\\lambda_{j}\\!-\\!\\!\\lambda)\\odot(\\lambda_{j}\\!-\\!\\!\\lambda)\\!/\\sigma^{2}\\right)$ with small width $\\sigma\\,\\in\\,\\mathbb{R}_{>0}$ . This parametrization is not only permutation equivariant but also stable according to the subsequent definition via the H\u00f6lder continuity. Note that $C$ depends on the eigengap between $1/(\\lambda_{k+1}\\!-\\!\\lambda_{k})$ at the frequency cutoff (for exact constant $C$ see proof in $\\S\\ H.5\\rangle$ . ", "page_idx": 6}, {"type": "text", "text": "Definition 1 (Stable PE). (Huang et al., 2024) A PE method PE : $\\mathbb{R}^{n\\times k}\\times\\mathbb{R}^{k}\\to$ $\\mathbb{R}^{n\\times k}$ is called stable, if there exist constants $c,C>0$ , such that for any Laplacian $L,L^{\\prime}$ , and $P_{*}=\\arg\\operatorname*{min}_{P}$ $\\|\\boldsymbol{L}-\\boldsymbol{P}\\boldsymbol{L}^{\\prime}\\boldsymbol{P}^{\\intercal}\\|_{\\mathrm{F}}$ ", "page_idx": 6}, {"type": "text", "text": "Figure 9: PE discriminates the depicted degree-regular graphs, except for (a) vs. (c). ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\mathrm{PE}(\\mathrm{EVD}(L))-P_{*}\\,\\mathrm{PE}\\left(\\mathrm{EVD}\\left(L^{\\prime}\\right)\\right)\\right\\|_{\\mathrm{F}}\\leq C\\cdot\\left\\|L-P_{*}L^{\\prime}P_{*}^{\\top}\\right\\|_{\\mathrm{F}}^{c}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "Theorem 5. The Positional Encodings PE in Eq. 5 are stable according to Definition 1. ", "page_idx": 6}, {"type": "text", "text": "Next to their stability, our PE can discriminate certain degree-regular graphs (e.g., Fig. 9). Since degree-regular graphs cannot be distinguished by 1-WL, our PE makes the equipped GNN (as expressive as 1-WL) strictly more expressive than 1-WL. See $\\S\\,\\mathrm{~I~}$ for continued expressivity analyses. ", "page_idx": 6}, {"type": "text", "text": "Theorem 6. $S^{2}$ GNNs are strictly more expressive than 1-WL with the PE of Eq. 5. ", "page_idx": 6}, {"type": "text", "text": "4 Empirical Results ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "With state-of-the-art performance on the peptides-func task of the long-range benchmark (Dwivedi et al., 2022), plus strong results on further benchmarks, we demonstrate that $S^{2}G C N,$ , a GCN paired with spectral filters, is highly capable of modeling long-range interactions $(\\S\\ 4.1)$ . We assess $\\mathrm{{S^{2}G N N s}^{\\circ}}$ long sequence performance $(\\S\\ 4.2)$ (mechanistic in-context learning) and show that $\\mathsf{S}^{2}\\mathrm{GCN}$ , a graph machine learning method, can achieve competitive results to state-of-the-art sequence models, including H3, Hyena, and transformers. We exemplify $\\mathrm{S}^{2}\\mathrm{GNNs}^{\\circ}$ practicality and competitiveness at scale on large-scale benchmarks $(\\S\\,4.3)$ like TPUGraphs (Phothilimthana et al., 2023), PCQM4Mv2 (Hu et al., 2021), and Open Graph Benchmark (OGB) Products (Hu et al., 2020). Further, in $\\S\\,\\mathrm{M.8}$ , we report state-of-the-art performance on the heterophilic arXiv-year (Lim et al., 2021) and,in $\\S\\ M.4$ , we study combinations of spatial and spectral filters beyond Eq. 1 & 2. ", "page_idx": 7}, {"type": "text", "text": "Setup. We pair different MPGNNs with spectral filters and name the composition $\\mathrm{S}^{2}{<}\\mathsf{b a s e}{>}$ . For example, a $\\dot{\\mathsf{S}}^{2}\\mathrm{GNN}$ with GAT as base will be called $\\mathrm{S}^{2}\\mathrm{GAT}$ . We typically perform 3 to 10 random reruns and report the mean $\\pm$ standard deviation. The experiments of $\\S\\ 4.1$ require ${<}11$ GB (e.g. Nvidia GTX 1080Ti); for the experiments in $\\S\\,^{4.2}$ & 4.3 we use a 40 GB A100. We usually optimize weights with AdamW (Loshchilov & Hutter, 2019) and cosine annealing scheduler (Loshchilov & Hutter, 2017). We use early stopping based on the validation loss/score. See $\\S\\mathrm{~M~}$ for more details and https://www.cs.cit.tum.de/daml/s2gnn for code as well as supplementary material. ", "page_idx": 7}, {"type": "text", "text": "4.1 Long-Range Interactions ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Finding (I): $\\mathbf{S}^{2}\\mathbf{GCN}$ outperforms state-of-the-art graph transformers, MPGNNs, and graph rewirings on the peptides-func long-range benchmarks (Dwivedi et al., 2022) by a substantial margin. Simultaneously, we remain approximately $35\\%$ below the $500\\mathrm{k}$ parameter threshold and. On peptides-struct we are only outperformed by NBA-GIN (Park et al., 2023). We extend the best configuration for a GCN of T\u00f6nshoff et al. (2023) (see GCN in Table 1), lower the number of message passing steps from six to three, and interleave spatial and spectral filters (Eq. 2) with $\\lambda_{\\mathrm{cut}}=0.7$ . ", "page_idx": 7}, {"type": "text", "text": "Dataset contribution: Clustering, given a single seed node per cluster, measures the ability (1) to spread information within the cluster and (2) to discriminate between the clusters. We complement the semisupervised task CLUSTER from Dwivedi et al. (2023) with (our) LR-CLUSTER dataset, a scaled-up version with longrange interactions (1). We closely follow Dwivedi et al. (2023), but instead of using graphs sampled from Stochastic Block Models (SBMs), we sample coordinates from a Gaussian Mixture Model (GMM) and then connect nearby nodes. CLUSTER has 117 nodes on average, while ours has 896. LR-CLUSTER has an average diameter of $\\approx\\,33$ and often contain hub nodes ", "page_idx": 7}, {"type": "text", "text": "Table 1: Long-range benchmark. Our $\\mathrm{S}^{2}\\mathrm{GNN}$ uses $\\approx35\\%$ fewer parameters than the other models. AP is Peptides-func\u2019s and MAE peptides-struct\u2019s target metric. The best/second best is bold/underlined. ", "page_idx": 7}, {"type": "table", "img_path": "Cb3kcwYBgw/tmp/70f06d5d8fc57d2138e9826e6ea27ca5ccaaadf3b9fd5ce41a330ceb9ee216e1.jpg", "table_caption": [], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "that cause over-squashing. For full details on the dataset construction, see $\\S\\ M.6$ . ", "page_idx": 7}, {"type": "text", "text": "Dataset contribution: Distance regression is a task with long-range interactions used in prior work (Geisler et al., 2023; Lim et al., 2023). Here, the regression targets are the shortest path distances to the only root node (in-degree 0). We generate random trees/DAGs with ${\\approx}750\\ \\#$ of nodes on average (details are in $\\S\\ M.7$ ). The target distances often exceed 30 hops. We evaluate on similarly sized graphs as in the training data, i.e., in-distribution $(\\mathbf{ID})$ samples, and out-of-distribution (OOD) samples that consist of slightly larger graphs. Details on the dataset construction are in $\\S\\,\\mathrm{M.7}$ . ", "page_idx": 7}, {"type": "text", "text": "Finding (II): spatial MPGNNs are less effective as $\\mathbf{S}^{2}\\mathbf{GNNs}$ , for long-range interactions. This is evident for peptides Table 1, clustering Fig. 10, distance regression Fig. 11, and over-squashing Fig. 12. Specifically, if the task requires long-range interactions beyond the receptive field of MPGNNs, they return crude estimates. E.g., in Fig. 11, the MPGNN predicts (approx.) constantly 20 for all distances beyond its receptive field \u2013 roughly the mean in the training data. Moreover, ", "page_idx": 7}, {"type": "text", "text": "(a) $4{+}1$ layer $\\mathrm{{MP}+\\mathrm{{Spec}}}$ .(b) w/ one vs. w/o Spec. Figure 10: Results on LR-CLUSTER. Solid lines are w/, dashed lines are w/o our PE (\u00a7 3.2.4). ", "page_idx": 8}, {"type": "image", "img_path": "Cb3kcwYBgw/tmp/5706cb662c2e11f551b31dd855e692e6f9a1e78ea19385bdf3ed262c4f22c946.jpg", "img_caption": ["Figure 11: $90\\%$ pred. intervals on OOD DAGs. ", "Figure 12: Over-sq.: 25-layer GatedGCN vs. 1-layer spec. ID is grey. "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "Cb3kcwYBgw/tmp/0eb05c97b08beec9aa81707592414a4c5f85a57ac7905b6a5e9b2e3775b046e6.jpg", "img_caption": ["Figure 13: 4 filters on LR-CLUSTER. Large/small entries are yellow/blue, white lines mark clusters. "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "$\\mathrm{S}^{2}\\mathrm{GNNs}$ may converge faster (see Fig. 25 in $\\S\\,\\mathrm{M}.6.2\\rangle$ and are more parameter-efficient, as we show on PCQM4Mv2 (Hu et al., 2021) in $\\S\\mathrm{~M.9~}$ . ", "page_idx": 8}, {"type": "text", "text": "Finding (III): virtual nodes are insufficient. We frequently find that including more than a single eigenvector $\\;k>1\\;\\;$ ) yields substantial gains. We make this explicit in Fig. 10a, where we append a single spectral layer and sweep over the number of eigenvectors $k$ . We complement these findings with an ablation for the frequency cutoff $\\lambda_{\\mathrm{cut}}$ on peptides-func in $\\S\\,\\mathrm{M}.5$ . ", "page_idx": 8}, {"type": "text", "text": "Finding (IV): our Positional Encodings PE consistently help, when concatenated to the node features. While this finding is true throughout our evaluation, the differences are more pronounced in certain situations. For example, on LR-CLUSTER in Fig. 10, the PE help with spectral filter and a small $k$ or without spectral filter and many message passing steps. ", "page_idx": 8}, {"type": "text", "text": "Finding (V): spectral filters align with clusters, as we illustrate in Fig. 13 for four arbitrary spectral filters learned on LR-CLUSTER. We observe that (a) the spectral filters reflect the true clustering structure, (b) some filters are smooth while others contain details, and (c) they model coarser or finer cluster structures (e.g., first vs. third filter). ", "page_idx": 8}, {"type": "text", "text": "Table 2: 30k token associative recall. ", "page_idx": 8}, {"type": "table", "img_path": "Cb3kcwYBgw/tmp/ffde67460a2de948ce5d74d31998724eaf98b415555324e894a749a4d4bfbdfa.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "4.2 Sequence Modelling: Mechanistic In-Context Learning ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Following the evaluation of Hyena (Poli et al., 2023) and H3 (Fu et al., 2023), we benchmark $\\mathrm{S}^{2}\\mathrm{GCN}$ with sequence models on the associative recall in-context learning task, stemming from mechanistic interpretability (Elhage et al., 2021; Power et al., 2022; Zhang et al., 2023; Olsson et al., 2022). In associative recall, the model is asked to retrieve the value for a key given in a sequence. For example, in the sequence ${\\tt a},0,{\\tt e}\\,,{\\tt b}\\,,z\\,,9\\,,{\\tt h}\\,,2\\,,{\\tt=}\\,,{\\tt z},$ , the target is the value for key $_{z}$ , which is 9 since it follows $_\\textrm{z}$ in its prior occurrences. We create a sequence/path graph with a node for each \u201ctoken\u201d (separated by \u201c,\u201d in the example above) and label the target node with its value. We assess the performance of $\\mathsf{S}^{2}\\mathrm{GCN}$ on graphs that vary in size by almost two orders of magnitude and follow Poli et al. (2023) with a vocabulary of 30 tokens. Moreover, we finetune our $\\mathrm{S}^{2}\\mathrm{GCN}$ on up to 30k nodes. ", "page_idx": 8}, {"type": "image", "img_path": "Cb3kcwYBgw/tmp/c309a743a14cdd6c77e8434b58b9b9d93254a03ae8c625136008fdd1d3fa8648.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "Figure 14: $\\mathrm{S}^{2}\\mathrm{GCN}$ solves associative recall for sequences varying in size by two orders of magnitude. Grey area marks ID. ", "page_idx": 8}, {"type": "text", "text": "Finding (VI): our spectral filter for directed are effective and may improve generalization, as we find in Fig. 14 (and Table 13 of $\\S\\ M.7$ ). ", "page_idx": 8}, {"type": "text", "text": "Finding (VII): $\\mathbf{S}^{2}\\mathbf{GCN}$ a state-of-the-art sequence model, as it performs on par with Hyena and, here, outperforms transformers (Table 2). ", "page_idx": 8}, {"type": "text", "text": "4.3 Large-Scale Benchmarks ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Finding (VIII): $\\mathbf{S}^{2}\\mathbf{GNNs}$ is practical and scalable. We demonstrate this on the OGB Products graph $2.5\\ \\mathrm{mio}$ . nodes, Table 3) and the (directed) 10 million graphs dataset TPUGraphs (average number of nodes ${\\approx}10\\mathrm{,}000$ , Table 4). In both cases, we find full-graph training (without segment training (Cao et al., 2023)) using 3 (Dir-) GCN layers interlayered with spectral filters, a reasonable configuration on a 40 GB A100. However, for OGB Products, we find that batching is superior, presumably because the training nodes are drawn from a \u201csmall\u201d region of the graph (see $\\S\\mathrm{~K~}$ ). ", "page_idx": 9}, {"type": "text", "text": "The cost of partial EVD for each dataset (excluding TPUGraphs and distance regression) is between 1 to 30 minutes on CPUs. We report the detailed costs of EVD and experiments in $\\S\\ M.3$ . ", "page_idx": 9}, {"type": "table", "img_path": "Cb3kcwYBgw/tmp/3c792002ff05315873f41da0770a3258606af8d5f71dd021dc56d34f1c519912.jpg", "table_caption": ["Table 3: OGB Products. "], "table_footnote": ["Test S2GAT 0.811\u00b10.007 0.381\u00b10.009 "], "page_idx": 9}, {"type": "table", "img_path": "Cb3kcwYBgw/tmp/8f511ce6b4c6398872f81ab2625b8b1307ee34833884b60e71388806aa321df4.jpg", "table_caption": ["Table 4: Graph ranking on TPUGraphs \u201clayout\u201d. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "5 Related Work ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Combining spatial and spectral filters has recently attracted attention outside of the graph domain in models like Hyena (Poli et al., 2023), Spectral State Space Models (Agarwal et al., 2024), etc. with different flavors of parametrizing the global/FFT convolution. Nevertheless, the properties of spatial and spectral filter parametrization (e.g., local vs. global) are well-established in classical signal processing. A combination of spectral and spatial filters was applied to (periodic) molecular point clouds (Kosmala et al., 2023). For GNNs, Stachenfeld et al. (2020) compose a spatial and spectral message passing but do not handle the ambiguity of the eigendecomposition and, thus, do not maintain permutation equivariance. Moreover, Beaini et al. (2021) use the EVD for localized anisotropic graph filters; Liao et al. (2019) propose an approach that combines spatial and spectral convolution via the Lanczos algorithm; and Huang et al. (2022) augment message passing with power iterations. Behrouz & Hashemi (2024) apply a Mamba-like state space model to graphs via arbitrarily ordering the nodes and, thus, sacrifice permutation equivariance. ", "page_idx": 9}, {"type": "text", "text": "Long-range interactions on graphs. Works that model long-range interactions can be categorized into: (a) MPGNNs on rewired graphs (Gasteiger et al., 2019a,b; Gutteridge et al., 2023); (b) higherorder GNNs (Fey et al., 2020; Wollschl\u00e4ger et al., 2024) that, e.g., may pass information to distant nodes through hierarchical message passing schemes; and (c) message passing adaptations to facilitate long-range interactions. For example, Park et al. (2023) propose \u201cnon-backtracking\u201d message passing, Errica et al. (2024) adaptively choose the numbers of message passing steps, and Ding et al. (2024) use linear RNNs to aggregate over each node\u2019s neighborhoods. While approaches (a-c) can increase the receptive field of GNNs, they are typically still spatially bounded. In contrast, (d) alternative architectures, like graph transformers (Ma et al., 2023; Dwivedi & Bresson, 2021; Kreuzer et al., 2021; Ramp\u00e1\u0161ek et al., 2022; Geisler et al., 2023; Deng et al., 2024) with global attention, may model all possible $n\\times n$ interactions. We provide notes on the limitations of graph transformers with absolute positional encodings in $\\S\\,\\mathbf{D}$ , which highlights the importance of capturing the relative relationships between nodes, as $\\mathrm{S}^{2}\\mathrm{GNNs}$ do. Moreover, in a recent/contemporary non-attention model for all pair-wise interactions, Batatia et al. (2024) use a resolvent parametrization of matrix functions relying on the LDL factorization of a matrix, but do not characterize their approximation-theoretic properties, over-squashing, expressivity on graphs, nor how to deal with directed graphs. ", "page_idx": 9}, {"type": "text", "text": "In $\\S\\mathrm{~B~}$ , we discuss additional related work w.r.t. expressivity and directed graphs. ", "page_idx": 9}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "We propose $\\mathrm{S}^{2}\\mathrm{GNNs}$ , adept at efficiently modeling complex long-range interactions via the synergistic composition of spatially and spectrally parametrized filters $(\\S\\,3)$ . We show that $\\mathrm{S}^{2}\\mathrm{GNNs}$ share many properties with graph rewirings, pooling, and hierarchical message passing schemes (Fig. 3 & 4). $\\bar{\\mathbf{S}}^{2}\\bar{\\mathbf{G}}\\bar{\\mathbf{N}}\\mathbf{Ns}$ outperform the aforementioned techniques with a substantial margin on the peptides long-range benchmark $(\\S\\ 4.1)$ , and we show that $\\mathrm{S}^{2}\\hat{\\mathrm{GNNs}}$ are also strong sequence models, performing on par or outperforming state-of-the-art like Hyena or H3 in our evaluation $(\\S\\ 4.2)$ . Even though we find global graph models, like $\\mathrm{S}^{2}\\mathrm{GNNs}$ , more prone to overfitting (see $\\S\\ K/{\\cal L}$ for further limitations/impact), moving to global models aligns with the trend for other deep learning domains. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We want to express our gratitude to Nicholas Gao for his feedback and the discussions about modeling choices. Moreover, we thank Leo Schwinn and Tim Beyer for their helpful and on-point feedback and suggestions. ", "page_idx": 10}, {"type": "text", "text": "This research was supported by the Helmholtz Association under the joint research school \u201cMunich School for Data Science - MUDS\u201c, as well as by the Munich Data Science Institute (MDSI) via the Linde/MDSI Doctoral Fellowship program and the MDSI Seed Fund. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "Naman Agarwal, Daniel Suo, Xinyi Chen, and Elad Hazan. Spectral State Space Models, arXiv, 2024.   \nUri Alon and Eran Yahav. On the Bottleneck of Graph Neural Networks and its Practical Implications. In International Conference on Learning Representations, ICLR, 2020.   \nMuhammet Balcilar, Pierre H\u00e9roux, Benoit Ga\u00fcz\u00e8re, Pascal Vasseur, S\u00e9bastien Adam, and Paul Honeine. Breaking the Limits of Message Passing Graph Neural Networks. In International Conference on Machine Learning, ICML, 2021a.   \nMuhammet Balcilar, Guillaume Renton, and Pierre Heroux. Analyzing the Expressive Power of Graph Neural Networks in a Spectral Perspective. In International Conference on Learning Representations, ICLR, 2021b.   \nIlyes Batatia, Lars L Schaaf, Gabor Csanyi, Christoph Ortner, and Felix A Faber. Equivariant Matrix Function Neural Networks. In International Conference on Learning Representations, ICLR, 2024.   \nPeter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, Caglar Gulcehre, Francis Song, Andrew Ballard, Justin Gilmer, George Dahl, Ashish Vaswani, Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matt Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pascanu. Relational inductive biases, deep learning, and graph networks, arXiv, 2018.   \nDominique Beaini, Saro Passaro, Vincent L\u00e9tourneau, William L. Hamilton, Gabriele Corso, and Pietro Li\u00f2. Directional Graph Networks. In International Conference on Machine Learning, ICML, 2021.   \nAli Behrouz and Farnoosh Hashemi. Graph Mamba: Towards Learning on Graphs with State Space Models, arXiv, 2024.   \nDeyu Bo, Chuan Shi, Lele Wang, and Renjie Liao. Specformer: Spectral Graph Neural Networks Meet Transformers. In International Conference on Learning Representations, ICLR, 2023a.   \nDeyu Bo, Xiao Wang, Yang Liu, Yuan Fang, Yawen Li, and Chuan Shi. A Survey on Spectral Graph Neural Networks, arXiv, 2023b.   \nCristian Bodnar, Ca\u02d8ta\u02d8lina Cangea, and Pietro Li\u00f2. Deep Graph Mapper: Seeing Graphs Through the Neural Lens. Frontiers in Big Data, 4:38, 2021.   \nXavier Bresson and Thomas Laurent. Residual Gated Graph ConvNets, arXiv, 2018.   \nMichael M. Bronstein, Joan Bruna, Taco Cohen, and Petar Velic\u02c7kovic\u00b4. Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges. arXiv, 2021.   \nJoan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral Networks and Locally Connected Networks on Graphs, arXiv, 2014.   \nChen Cai, Truong Son Hy, Rose Yu, and Yusu Wang. On the Connection Between MPNN and Graph Transformer. In International Conference on Machine Learning, ICML. arXiv, 2023.   \nShaofei Cai, Liang Li, Xinzhe Han, Jiebo Luo, Zheng-Jun Zha, and Qingming Huang. Automatic Relation-Aware Graph Network Proliferation. In Conference on Computer Vision and Pattern Recognition, CVPR, 2022.   \nKaidi Cao, Phitchaya Mangpo Phothilimthana, Sami Abu-El-Haija, Dustin Zelle, Yanqi Zhou, Charith Mendis, Jure Leskovec, and Bryan Perozzi. Learning Large Graph Property Prediction via Graph Segment Training. In Neural Information Processing Systems, NeruIPS. arXiv, 2023.   \nZhe Chen, Hao Tan, Tao Wang, Tianrun Shen, Tong Lu, Qiuying Peng, Cheng Cheng, and Yue Qi. Graph Propagation Transformer for Graph Representation Learning. In International Joint Conference on Artificial Intelligence, IJCAI, 2023.   \nEli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive Universal Generalized PageRank Graph Neural Network. In International Conference on Learning Representations, {ICLR}, 2021.   \nYun Young Choi, Sun Woo Park, Minho Lee, and Youngho Woo. Topology-Informed Graph Transformer, arXiv, 2024.   \nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. In Neural Information Processing Systems, NeruIPS. arXiv, 2022.   \nYves Colin De Verdi\u00e8re. Magnetic interpretation of the nodal defect on graphs. Analysis & PDE, 6 (5):1235\u20131242, 2013.   \nMicha\u00ebl Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering. In Neural Information Processing Systems, NeurIPS, 2017.   \nChenhui Deng, Zichao Yue, and Zhiru Zhang. Polynormer: Polynomial-Expressive Graph Transformer in Linear Time. In International Conference on Learning Representations, ICLR, 2024.   \nFrancesco Di Giovanni, Lorenzo Giusti, Federico Barbero, Giulia Luise, Pietro Li\u00f2, and Michael Bronstein. On Over-Squashing in Message Passing Neural Networks: The Impact of Width, Depth, and Topology. In International Conference on Machine Learning, ICML. arXiv, 2023a.   \nFrancesco Di Giovanni, T. Konstantin Rusch, Michael M. Bronstein, Andreea Deac, Marc Lackenby, Siddhartha Mishra, and Petar Velic\u02c7kovic\u00b4. How does over-squashing affect the power of GNNs?, arXiv, 2023b.   \nYuhui Ding, Antonio Orvieto, Bobby He, and Thomas Hofmann. Recurrent Distance Filtering for Graph Representation Learning. In International Conference on Machine Learning, ICML, 2024.   \nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In International Conference on Learning Representations, ICLR, 2021.   \nVijay Prakash Dwivedi and Xavier Bresson. A Generalization of Transformer Networks to Graphs. Deep Learning on Graphs at AAAI Conference on Artificial Intelligence, 2021.   \nVijay Prakash Dwivedi, Ladislav Ramp\u00e1\u0161ek, Mikhail Galkin, Ali Parviz, Guy Wolf, Anh Tuan Luu, and Dominique Beaini. Long Range Graph Benchmark. In Neural Information Processing Systems, NeruIPS. arXiv, 2022.   \nVijay Prakash Dwivedi, Chaitanya K. Joshi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Benchmarking Graph Neural Networks. Journal of Machine Learning Research, JMLR, 2023.   \nNelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, and et al. A mathematical framework for transformer circuits. Transformer Circuits Thread, 2021.   \nFederico Errica, Henrik Christiansen, Viktor Zaverkin, Takashi Maruyama, Mathias Niepert, and Francesco Alesiani. Adaptive Message Passing: A General Framework to Mitigate Oversmoothing, Oversquashing, and Underreaching, arXiv, 2024.   \nMatthias Fey and Jan Eric Lenssen. Fast Graph Representation Learning with PyTorch Geometric, arXiv, 2019.   \nMatthias Fey, Jan-Gin Yuen, and Frank Weichert. Hierarchical Inter-Message Passing for Learning on Molecular Graphs. In Graph Representation Learning and Beyond $(G R L+,$ ) Workhop at ICML 2020. arXiv, 2020.   \nRobin Forman. Determinants of Laplacians on graphs. Topology, 32(1):35\u201346, 1993.   \nDaniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. Hungry Hungry Hippos: Towards Language Modeling with State Space Models. In International Conference on Learning Representations, ICLR, 2023.   \nSatoshi Furutani, Toshiki Shibahara, Mitsuaki Akiyama, Kunio Hato, and Masaki Aida. Graph Signal Processing for Directed Graphs Based on the Hermitian Laplacian. In European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, ECML PKDD, 2020.   \nMikhail Galkin, Xinyu Yuan, Hesham Mostafa, Jian Tang, and Zhaocheng Zhu. Towards Foundation Models for Knowledge Graph Reasoning, arXiv, 2023.   \nFernando Gama, Joan Bruna, and Alejandro Ribeiro. Stability Properties of Graph Neural Networks. IEEE Transactions on Signal Processing, 68:5680\u20135695, 2020.   \nJohannes Gasteiger, Aleksandar Bojchevski, and Stephan G\u00fcnnemann. Predict then propagate: Graph neural networks meet personalized PageRank. International Conference on Learning Representations, ICLR, 2019a.   \nJohannes Gasteiger, Stefan Wei\u00dfenberger, and Stephan G\u00fcnnemann. Diffusion Improves Graph Learning. Neural Information Processing Systems, NeurIPS, 2019b.   \nFloris Geerts. On the Expressive Power of Linear Algebra on Graphs. Theory Comput. Syst., 65(1): 179\u2013239, 2021.   \nSimon Geisler, Yujia Li, Daniel Mankowitz, Ali Taylan Cemgil, Stephan G\u00fcnnemann, and Cosmin Paduraru. Transformers Meet Directed Graphs. In International Conference on Machine Learning, ICML, 2023.   \nJustin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry. In International Conference on Machine Learning, ICML, 2017.   \nLorenzo Giusti, Teodora Reu, Francesco Ceccarelli, Cristian Bodnar, and Pietro Li\u00f2. $\\mathrm{CIN++}$ : Enhancing Topological Message Passing, arXiv, 2023.   \nMartin Grohe, Kristian Kersting, Martin Mladenov, and Pascal Schweitzer. Color Refinement and Its Applications. In Guy Van Den Broeck, Kristian Kersting, Sriraam Natarajan, and David Poole (eds.), An Introduction to Lifted Probabilistic Inference, pp. 349\u2013372. The MIT Press, 2021.   \nAlbert Gu and Tri Dao. Mamba: Linear-Time Sequence Modeling with Selective State Spaces, arXiv, 2023.   \nYuhe Guo and Zhewei Wei. Graph Neural Networks with Learnable and Optimal Polynomial Bases. In International Conference on Machine Learning, ICML. arXiv, 2023.   \nBenjamin Gutteridge, Xiaowen Dong, Michael Bronstein, and Francesco Di Giovanni. DRew: Dynamically Rewired Message Passing with Delay. In International Conference on Learning Representations, ICLR, 2023.   \nDavid K. Hammond, Pierre Vandergheynst, and R\u00e9mi Gribonval. Wavelets on graphs via spectral graph theory. Applied and Computational Harmonic Analysis, 30(2):129\u2013150, 2011.   \nMingguo He, Zhewei Wei, Zengfeng Huang, and Hongteng Xu. BernNet: Learning Arbitrary Graph Spectral Filters via Bernstein Approximation. In Neural Information Processing Systems, NeruIPS, 2021.   \nMingguo He, Zhewei Wei, and Ji-Rong Wen. Convolutional Neural Networks on Graphs with Chebyshev Approximation, Revisited. In Neural Information Processing Systems, NeurIPS, 2022.   \nXiaoxin He, Bryan Hooi, Thomas Laurent, Adam Perold, Yann LeCun, and Xavier Bresson. A Generalization of ViT/MLP-Mixer to Graphs. In International Conference on Machine Learning, ICML, 2023.   \nDan Hendrycks and Kevin Gimpel. Bridging nonlinearities and stochastic regularizers with gaussian error linear units. CoRR, abs/1606.08415, 2016.   \nSepp Hochreiter and J Urgen Schmidhuber. Long Short-term Memory. Neural Computation, 9(8): 17351780\u201317351780, 1997.   \nAndrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications, arXiv, 2017.   \nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open Graph Benchmark: Datasets for Machine Learning on Graphs. In Neural Information Processing Systems, NeurIPS, 2020.   \nWeihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, and Jure Leskovec. OGBLSC: A Large-Scale Challenge for Machine Learning on Graphs, arXiv, 2021.   \nNingyuan Huang, Soledad Villar, Carey E. Priebe, Da Zheng, Chengyue Huang, Lin Yang, and Vladimir Braverman. From Local to Global: Spectral-Inspired Graph Neural Networks. In New Frontiers in Graph Learning at NeurIPS. arXiv, 2022.   \nYinan Huang, William Lu, Joshua Robinson, Yu Yang, Muhan Zhang, Stefanie Jegelka, and Pan Li. On the Stability of Expressive Positional Encodings for Graph Neural Networks. In International Conference on Learning Representations, ICLR, 2024.   \nMd Shamim Hussain, Mohammed J. Zaki, and Dharmashankar Subramanian. Global Self-Attention as a Replacement for Graph Convolution. In International Conference on Knowledge Discovery and Data Mining, KDD, pp. 655\u2013665, 2022.   \nMd Shamim Hussain, Mohammed J. Zaki, and Dharmashankar Subramanian. Triplet Interaction Improves Graph Transformers: Accurate Molecular Graph Learning with Triplet Graph Transformers, arXiv, 2024.   \nThomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. International Conference on Learning Representations, ICLR, 2017.   \nChristian Koke and Daniel Cremers. HoloNets: Spectral Convolutions do extend to Directed Graphs. In International Conference on Learning Representations, ICLR, 2024.   \nArthur Kosmala, Johannes Gasteiger, Nicholas Gao, and Stephan G\u00fcnnemann. Ewald-based LongRange Message Passing for Molecular Graphs. In International Conference on Machine Learning, ICML, 2023.   \nDevin Kreuzer, Dominique Beaini, William L. Hamilton, Vincent L\u00e9tourneau, and Prudencio Tossou. Rethinking Graph Transformers with Spectral Attention. In Neural Information Processing Systems, NeurIPS, 2021.   \nRemi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato, Ferran Alet, Suman Ravuri, Timo Ewalds, Zach Eaton-Rosen, Weihua Hu, Alexander Merose, Stephan Hoyer, George Holland, Oriol Vinyals, Jacklynn Stott, Alexander Pritzel, Shakir Mohamed, and Peter Battaglia. Learning skillful medium-range global weather forecasting. Science, 382(6677):1416\u20131421, 2023.   \nY. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation Applied to Handwritten Zip Code Recognition. Neural Computation, 1(4):541\u2013 551, 1989.   \nJunhyun Lee, Inyeop Lee, and Jaewoo Kang. Self-attention graph pooling. In International Conference on Machine Learning, ICML, 2019.   \nR. B. Lehoucq, D. C. Sorensen, and C. Yang. ARPACK Users\u2019 Guide: Solution of Large-Scale Eigenvalue Problems with Implicitly Restarted Arnoldi Methods. Society for Industrial and Applied Mathematics, 1998.   \nPan Li and Jure Leskovec. The Expressive Power of Graph Neural Networks. In Lingfei Wu, Peng Cui, Jian Pei, and Liang Zhao (eds.), Graph Neural Networks: Foundations, Frontiers, and Applications, pp. 63\u201398. Springer Nature Singapore, Singapore, 2022.   \nPan Li, Yanbang Wang, Hongwei Wang, and Jure Leskovec. Distance Encoding: Design Provably More Powerful Neural Networks for Graph Representation Learning. In Neural Information Processing Systems, NeurIPS, 2020.   \nRenjie Liao, Zhizhen Zhao, Raquel Urtasun, and Richard S. Zemel. LanczosNet: Multi-Scale Deep Graph Convolutional Networks. In International Conference on Learning Representations, ICLR. arXiv, 2019.   \nDerek Lim, Felix Hohne, Xiuyu Li, Sijia Linda Huang, Vaishnavi Gupta, Omkar Bhalerao, and Ser Nam Lim. Large Scale Learning on Non-Homophilous Graphs: New Benchmarks and Strong Simple Methods. In Advances in Neural Information Processing Systems, 2021.   \nDerek Lim, Joshua Robinson, Lingxiao Zhao, Tess Smidt, Suvrit Sra, Haggai Maron, and Stefanie Jegelka. Sign and Basis Invariant Networks for Spectral Graph Representation Learning. In International Conference on Learning Representations, ICLR, 2023.   \nIlya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In International Conference on Learning Representations, ICLR, 2017.   \nIlya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. International Conference on Learning Representations, ICLR, 2019.   \nYuankai Luo, Hongkang Li, Lei Shi, and Xiao-Ming Wu. Enhancing Graph Transformers with Hierarchical Distance Structural Encoding, arXiv, 2024.   \nLiheng Ma, Chen Lin, Derek Lim, Adriana Romero-Soriano, Puneet K. Dokania, Mark Coates, Philip Torr, and Ser-Nam Lim. Graph Inductive Biases in Transformers without Message Passing. In International Conference on Machine Learning, ICML, 2023.   \nAmil Merchant, Simon Batzner, Samuel S. Schoenholz, Muratahan Aykol, Gowoon Cheon, and Ekin Dogus Cubuk. Scaling deep learning for materials discovery. Nature, 624(7990):80\u201385, 2023.   \nGaspard Michel, Giannis Nikolentzos, Johannes Lutzeyer, and Michalis Vazirgiannis. Path Neural Networks: Expressive and Accurate Graph Neural Networks. In International Conference on Machine Learning, ICML, 2023.   \nChristopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks. AAAI Conference on Artificial Intelligence, 33:4602\u20134609, 2019.   \nI. P. Natanson. Constructive function theory. Vol. I: Uniform approximation. Translated by Alexis N. Obolensky. New York: Frederick Ungar Publishing Co. IX, 232 p. (1964)., 1964.   \nNhat Khang Ngo, Truong Son Hy, and Risi Kondor. Multiresolution Graph Transformers and Wavelet Positional Encoding for Learning Hierarchical Structures. The Journal of Chemical Physics, 159(3):034109, 2023.   \nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context Learning and Induction Heads, arXiv, 2022.   \nSeonghyun Park, Narae Ryu, Gahee Kim, Dongyeop Woo, Se-Young Yun, and Sungsoo Ahn. Nonbacktracking Graph Neural Networks, arXiv, 2023.   \nPhitchaya Mangpo Phothilimthana, Sami Abu-El-Haija, Kaidi Cao, Bahare Fatemi, Charith Mendis, and Bryan Perozzi. TpuGraphs: A Performance Prediction Dataset on Large Tensor Computational Graphs. In Neural Information Processing Systems, NeruIPS, 2023.   \nMichael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena Hierarchy: Towards Larger Convolutional Language Models. In International Conference on Machine Learning, ICML. arXiv, 2023.   \nAlethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets, arXiv, 2022.   \nLadislav Ramp\u00e1\u0161ek, Mikhail Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, and Dominique Beaini. Recipe for a General, Powerful, Scalable Graph Transformer. In Neural Information Processing Systems, NeurIPS, 2022.   \nEmanuele Rossi, Bertrand Charpentier, Francesco Di Giovanni, Fabrizio Frasca, Stephan G\u00fcnnemann, and Michael Bronstein. Edge Directionality Improves Learning on Heterophilic Graphs. In Learning on Graphs Conference, LoG, 2023.   \nKristof T. Sch\u00fctt, Pieter-Jan Kindermans, Huziel E. Sauceda, Stefan Chmiela, Alexandre Tkatchenko, and Klaus-Robert M\u00fcller. SchNet: A continuous-filter convolutional neural network for modeling quantum interactions. In Neural Information Processing Systems, NeruIPS. arXiv, 2017.   \nHamed Shirzad, Ameya Velingker, Balaji Venkatachalam, Danica J. Sutherland, and Ali Kemal Sinop. Exphormer: Sparse Transformers for Graphs. In International Conference on Machine Learning, ICML, 2023.   \nM. A. Shubin. Discrete Magnetic Laplacian. Communications in Mathematical Physics, 164(2): 259\u2013275, 1994.   \nLaurent Sifre. Rigid-Motion Scattering For Image Classification. PhD thesis, Ecole Polytechnique, CMAP, 2014.   \nKimberly Stachenfeld, Jonathan Godwin, and Peter Battaglia. Graph Networks with Spectral Message Passing, arXiv, 2020.   \nZekun Tong, Yuxuan Liang, Changsheng Sun, David S. Rosenblum, and Andrew Lim. Directed Graph Convolutional Network, arXiv, 2020.   \nJake Topping, Francesco Di Giovanni, Benjamin Paul Chamberlain, Xiaowen Dong, and Michael M. Bronstein. Understanding over-squashing and bottlenecks on graphs via curvature. In International Conference on Learning Representations, ICLR, 2022.   \nJan T\u00f6nshoff, Martin Ritzert, Eran Rosenbluth, and Martin Grohe. Where Did the Gap Go? Reassessing the Long-Range Graph Benchmark. In Learning on Graphs Conference, 2023.   \nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Neural Information Processing Systems, NeurIPS, 2017.   \nPetar Velic\u02c7kovic\u00b4, Arantxa Casanova, Pietro Li\u00f2, Guillem Cucurull, Adriana Romero, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, ICLR, 2018.   \nUlrike von Luxburg. A tutorial on spectral clustering. Statistics and Computing, 17(4):395\u2013416, 2007.   \nHaorui Wang, Haoteng Yin, Muhan Zhang, and Pan Li. Equivariant and Stable Positional Encoding for More Powerful Graph Neural Networks. In International Conference on Learning Representations, ICLR, 2022.   \nXiyuan Wang and Muhan Zhang. How Powerful are Spectral Graph Neural Networks. In International Conference on Machine Learning, ICML. arXiv, 2022.   \nTom Wollschl\u00e4ger, Niklas Kemper, Leon Hetzel, Johanna Sommer, and Stephan G\u00fcnnemann. Expressivity and Generalization: Fragment Biases for Molecular GNNs, arXiv, 2024.   \nKeyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken Ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In International Conference on Machine Learning, ICML, 2018.   \nKeyulu Xu, Stefanie Jegelka, Weihua Hu, and Jure Leskovec. How powerful are graph neural networks? In International Conference on Learning Representations, ICLR, 2019.   \nMingqi Yang, Wenjie Feng, Yanming Shen, and Bryan Hooi. Towards Better Graph Representation Learning with Parameterized Decomposition & Filtering. In International Conference on Machine Learning, ICML, 2023.   \nJiaxuan You, Rex Ying, and Jure Leskovec. Design Space for Graph Neural Networks. pp. 13, 2020.   \nManzil Zaheer, Satwik Kottur, Siamak Ravanbhakhsh, Barnab\u00e1s P\u00f3czos, Ruslan Salakhutdinov, and Alexander J. Smola. Deep sets. In Neural Information Processing Systems, NeruIPS, 2017.   \nXitong Zhang, Yixuan He, Nathan Brugnone, Michael Perlmutter, and Matthew Hirn. MagNet: A Neural Network for Directed Graphs. In Neural Information Processing Systems, NeurIPS, 2021.   \nYi Zhang, Arturs Backurs, S\u00e9bastien Bubeck, Ronen Eldan, Suriya Gunasekar, and Tal Wagner. Unveiling Transformers with LEGO: a synthetic reasoning task, arXiv, 2023. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Appendix ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A Notation 19 ", "page_idx": 17}, {"type": "text", "text": "B Related Work for Expressivity and Directed Graphs 19 ", "page_idx": 17}, {"type": "text", "text": "C Background for Directed Graphs 20 ", "page_idx": 17}, {"type": "text", "text": "D Limitations of Graph Transformers Using Absolute Positional Encodings 21 ", "page_idx": 17}, {"type": "text", "text": "E $\\mathbf{S}^{2}$ GNN Generalizes a Virtual Node 21 ", "page_idx": 17}, {"type": "text", "text": "F Existing Results on Over-Squashing 21 ", "page_idx": 17}, {"type": "text", "text": "G Construction of an explicit ground truth filter 22 ", "page_idx": 17}, {"type": "text", "text": "H Proofs 24 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "H.1 Proof of Theorem 1 24   \nH.2 Proof of Theorem 2 26   \nH.3 Proof of Theorem 3 . 26   \nH.4 Proof of Theorem 4 . 28   \nH.5 Proof of Theorem 5 . 31   \nH.6 Proof of Theorem 6 . 33 ", "page_idx": 17}, {"type": "text", "text": "Expressivity of Spectral Filters and Spectrally Designed Spatial Filters 34 ", "page_idx": 17}, {"type": "text", "text": "J Further Remarks on $\\mathbf{S}^{2}\\mathbf{GNNs}$ 35 ", "page_idx": 17}, {"type": "text", "text": "J.1 Visualization of Spectral Filters . . 36   \nJ.2 Composition of Filters . . 37   \nJ.3 Exhaustive Reasons Why Low Frequencies Are Sensible . 37   \nJ.4 Scaling to Graphs of Different Magnitude . . . . . 37   \nJ.5 Spectral Normalization . . . . 37   \nJ.6 Adjusting $\\mathrm{S}^{2}\\mathrm{GNNs}$ to Directed Graphs . . . . . 38   \nJ.7 Computational Remarks . . . 38 ", "page_idx": 17}, {"type": "text", "text": "K Limitations 38 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "L Broader Impact 39 ", "page_idx": 17}, {"type": "text", "text": "M Experimental Results 39 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "M.1 Experimental Details 39   \nM.2 Qualitative Experiments . . . 41   \nM.3 Computational Cost . . . . . 41   \nM.4 $S^{2}$ GNN Aggregation Ablation . . . 42   \nM.5 Number of Eigenvectors Ablation on Peptides-Func . . . . . 43   \nM.6 Clustering Tasks . . . . . . . . . . . 43   \nM.7 Distance Regression . . . . . 47   \nM.8 Heterophilic arXiv-year (Lim et al., 2021) . . . 49   \nM.9 Large-Scale PCQM4Mv2 (Hu et al., 2021) . . . . 50   \nM.10 TPUGraphs Graph Construction . . 50 ", "page_idx": 17}, {"type": "text", "text": "Table 5: List of most important symbols used in this work (with the most general domain). ", "page_idx": 18}, {"type": "table", "img_path": "Cb3kcwYBgw/tmp/518b24cfbfa3cb34fbd6d35e9e0af694e4234b687233e35c01dd20071cf339ef.jpg", "table_caption": [], "table_footnote": [], "page_idx": 18}, {"type": "text", "text": "B Related Work for Expressivity and Directed Graphs ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Expressivity. Laplacian eigenvectors have been used previously to construct positional encodings that improve the expressivity of GNNs or Transformers (Lim et al., 2023; Wang et al., 2022; Geisler et al., 2023; Huang et al., 2024). Our positional encodings are similar to the preprocessing of Balcilar et al. (2021a), where the authors design an edge-level encoding/mask to surpass 1-WL. The hierarchy of Weisfeiler-Leman (WL) tests is a common way to categorize the expressivity of GNNs (Grohe et al., 2021). Xu et al. (2019) showed that most MPGNNs are bound by or as strong as the 1-WL test. Lim et al. (2023) point out that spectral GNNs suffer from similar limitations as MPGNNs w.r.t. their expressivity. Generally, the development of expressive GNNs is an active research direction, and we refer to Li & Leskovec (2022) for a broad overview. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "Directed graphs. Rossi et al. (2023) also extend the WL test to directed graphs and propose an MPGNN for directed graphs. How to model direction in graphs is also still an open question and various approaches were proposed (Battaglia et al., 2018; Tong et al., 2020; Zhang et al., 2021; Rossi et al., 2023; Koke & Cremers, 2024). We utilize a Hermitian Laplacian for direction awareness, namely the Magnetic Laplacian, which was also used by Zhang et al. (2021) for an MPGNN and Geisler et al. (2023) for positional encodings. ", "page_idx": 19}, {"type": "text", "text": "C Background for Directed Graphs ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Undirected vs. directed graphs. For spatial filtering, it is straightforward to plausibly extend the message passing (e.g. Battaglia et al. (2018); Rossi et al. (2023)). However, the spectral motivation and spectral filter on directed graphs require more care. The eigendecomposition is guaranteed to exist for real symmetric matrices. Real symmetric matrices are always diagonalizable, and the eigenvectors will then span a complete orthogonal basis to represent all possible signals $\\pmb{X}\\in\\mathbb{R}^{n\\times d}$ . Note that some non-symmetric square matrices are also diagonalizable and, thus, also have an eigendecomposition, albeit the eigenvectors may not be orthogonal. Thus, further consideration is required to generalize the graph Laplacian to general directed graphs. ", "page_idx": 19}, {"type": "text", "text": "Magnetic Laplacian. For the spectral filter on directed graphs, we build upon a direction-aware generalization, called Magnetic Laplacian (Forman, 1993; Shubin, 1994; De Verdi\u00e8re, 2013; Furutani et al., 2020; Geisler et al., 2023) ", "page_idx": 19}, {"type": "equation", "text": "$$\nL_{q}=I-(D_{s}^{-1/2}A_{s}D_{s}^{-1/2})\\odot\\exp[i2\\pi q({\\bf A}-{\\bf A}^{\\top})]\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $\\pmb{A}_{s}\\,=\\,\\pmb{A}\\lor\\pmb{A}^{\\top}$ is the symmetrized graph with diagon\u221aal degree matrix $D_{s}$ . $\\odot$ denotes the element-wise product, exp the element-wise exponential, $i=\\sqrt{-1}$ an imaginary number, and $q$ the potential (hyperparameter). By construction $\\pmb{L}_{q}$ is a Hermitian matrix $\\pmb{L}_{q}=\\pmb{L}_{q}^{\\mathrm{H}}$ where the conjugate transpose is equal to $\\pmb{L}_{q}$ itself. Importantly, Hermitian matrices naturally generalize real symmetric matrices and have a well-defined eigendecomposition $L_{q}\\,=\\,V\\Lambda V^{\\mathrm{H}}$ with real eigenvalues $\\pmb{\\Lambda}$ and unitary eigenvectors $V V^{\\mathrm{{H}}}=I$ . For appropriate choices of the potential $q$ , the order of eigenvalues is well-behaved (Furutani et al., 2020). Recently Geisler et al. (2023) demonstrated the efficacy of these eigenvectors for positional encodings for transformers. Moreover, the Magnetic Laplacian was used for a spectrally designed spatial MPGNN (Zhang et al., 2021), extending Defferrard et al. (2017). Due to the real eigenvalues, one could, in principle, also apply a monomial basis (Chien et al., 2021), or different polynomial bases stemming from approximation theory (He et al., 2021; Wang & Zhang, 2022; He et al., 2022; Guo & Wei, 2023). ", "page_idx": 19}, {"type": "text", "text": "To see why Eq. 7 describes an injection for appropriate choices of $q$ , consider that the sparsity pattern of $\\pmb{L}_{q}$ matches $\\pmb{A}$ up to the main diagonal. If $\\pmb{A}$ contains a self-loop the main diagonal will have a 0 instead of 1 entry at the self-loop location. $A-A^{\\top}$ can be directly inferred from the phase $\\Im[i2\\pi q({\\bf A}-{\\bf A}^{\\top})]$ , assuming that $q<1/(2\\operatorname*{max}_{u,v}A_{u,v})$ . Thus, it is solely left to obtain $A_{s}$ from $I-D_{s}^{-1/2}A_{s}D_{s}^{-1/2}$ , which is trivial for a binary adjacency but more involved for real-valued weights. Determining if and when $\\pmb{L}_{q}$ is injective for real-valued $\\pmb{A}$ is left for future work. ", "page_idx": 19}, {"type": "text", "text": "Properties of the eigendecomposition. The eigendecomposition is not unique, and thus, one should consider the result of the eigensolver arbitrary in that regard. One ambiguity becomes apparent from the definition of an eigenvalue itself $\\pmb{L}\\pmb{v}\\,=\\,\\lambda\\pmb{v}$ since one can multiply both sides of the equation with a scalar $c\\,\\in\\,\\mathbb{C}\\setminus\\{0\\};\\,L(c v)\\,=\\,\\lambda(c v)$ . We already implicitly normalized the magnitude of the eigenvectors $V$ by choosing them to be orthogonal $V V^{\\top}=I)$ or unitary $(V V^{\\mathrm{{H}}}=I)$ ). Thus, after this normalization, $c$ only represents an arbitrary sign for real-valued eigenvectors or a rotation on the unit circle in the complex case. Another reason for ambiguity occurs in the case of repeated / multiple eigenvalues (e.g., $\\lambda_{u}=\\lambda_{v}$ for $u\\ne v$ ). In this case, the eigensolver may return an arbitrary set of orthogonal eigenvectors chosen from the corresponding eigenspace. ", "page_idx": 19}, {"type": "text", "text": "Here, we consider a vanilla graph transformer $f(\\boldsymbol X)$ that solely becomes structure-aware due to the addition (or concatenation) of positional encodings: $f(X^{\\bar{+}}\\ P\\mathrm{E}(A))$ . The main point we are going to demonstrate is that a vanilla transformer with such absolute positional encodings $\\mathrm{PE}(\\bar{A})\\in\\mathbb{R}^{n\\times d}$ will be limited in its expressivity if the positional encodings are permutation equivariant $P\\operatorname{PE}(A)=\\operatorname{PE}(P A P^{\\top})$ w.r.t. any $n\\times n$ permutation matrix $P\\in\\mathcal P$ . ", "page_idx": 20}, {"type": "text", "text": "The limitation particularly arises in the presence of automorphisms $P_{a}A P_{a}^{\\top}\\,=\\,A$ with specifically chosen permutation $P_{a}$ . To be more specific, assume that nodes $u$ and $v$ are automorphic to each other. That is, there exists a $P_{a}$ that will swap the order of $u$ and $v$ (among other nodes) s.t. $P_{a}A P_{a}^{\\top}\\,=\\,A$ . By permutation equivariance, we know $P_{a}\\operatorname{PE}(A)\\,=\\,\\operatorname{PE}(P_{a}{\\bar{A}}P_{a}^{\\top})\\,=\\,\\operatorname{PE}(A)$ and, hence, $\\operatorname{PE}(A)_{u}=\\operatorname{PE}(A)_{v}$ . ", "page_idx": 20}, {"type": "text", "text": "We have just shown that automorphic nodes will have the same positional encodings PE if the positional encodings are permutation equivariant. This implies that permutation equivariant positional encodings PE are not even able to capture simple neighboring relationships. For example, consider an undirected sequence/path graph $0\\!-\\!0\\!-\\!0\\!-\\!0\\!-\\!0\\!-\\!0$ with five nodes. Here, the two end nodes, which we also all first and last node, are automorphic. So are the second and second-last nodes. Assuming the second and second last nodes have different node features (e.g., A-B-C-D-A), that breaks the symmetry, it is still not possible for a transformer with absolute positional encodings to tell the first and last node apart. In other words, in the example, the transformer cannot tell apart the end node with neighboring feature B from the end node with neighboring feature D. This shows a severe limitation of architectures without additional components capturing the relative distances (e.g., as $\\mathrm{S}^{2}\\mathrm{GNNs}$ can). This concern is not as critical for architectures where the positional encodings are not entirely permutation equivariant (Dwivedi & Bresson, 2021; Kreuzer et al., 2021), with relative positional encodings (Ma et al., 2023), and might also be of lesser concern for directed graphs (Geisler et al., 2023). ", "page_idx": 20}, {"type": "text", "text": "E $\\mathbf{S}^{2}\\mathbf{GNN}$ Generalizes a Virtual Node ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Adding a fully connected virtual node (Gilmer et al., 2017) is among the simplest ways to add the ability for long-range information exchange. An equivalent method was proposed as a simple oversquashing remedy in the seminal work by Alon & Yahav (2020). A single Spectral layer amounts to a type of virtual nodes in the special case of $f_{\\theta}=I$ and ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\hat{g}^{(l)}(\\lambda)=\\left\\{1\\:\\mathrm{for}\\:\\lambda=0,\\right.\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Assuming a simply-connected graph $\\mathcal{G}$ , the unique normalized zero-eigenvector $\\pmb{v}$ of the symmetrically-normalized graph Laplacian $L=I-D^{-1/2}A D^{-1/2}$ has components $\\begin{array}{r}{\\pmb{v}_{u}=\\sqrt{\\frac{d_{u}}{2|E|}}}\\end{array}$ , where $d_{u}$ denotes the degree of node $u\\,\\in\\,{\\mathcal{G}}$ , and $|E|$ the number of edges in the graph. At node $u\\in{\\mathcal{G}}$ , we therefore find ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname{Spectral}_{u}^{(l)}(H^{(l-1)};V,\\lambda)=\\frac{\\sqrt{d_{u}}}{2|E|}\\sum_{v\\in\\mathcal{G}}\\sqrt{d_{v}}h_{v}^{(l-1)}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "with $\\pmb{h}_{v}^{(l-1)}$ denoting the row of $H^{(l-1)}$ corresponding to node $v\\in\\mathcal G$ . In other words, filtering out the zero-frequency component of the signal means scattering a global, degree-weighted embedding average to all nodes of the graph. For the unnormalized graph Laplacian, Eq. 9 instead becomes an unweighted average, which is consistent with the usual definition of a virtual node. We refer to Fig. 3 & 4 for additional intuition. ", "page_idx": 20}, {"type": "text", "text": "F Existing Results on Over-Squashing ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We restate two key results from Di Giovanni et al. (2023a) using our notation. They imply the existance of a regime in which 1-hop MPNN architectures suffer from exponentially decaying Jacobian sensitivity. Meanwhile, $\\mathrm{S}^{2}\\mathrm{GNNs}$ can easily learn a signal of constantly lower-bounded sensitivity, as shown by invoking its trivial subcase of a virtual node in Theorem 2. ", "page_idx": 20}, {"type": "text", "text": "Theorem 7 (Adapted from Di Giovanni et al. (2023a)). In an $\\ell$ -layer spatial MPGNN with messagepassing matrix $S=c_{r}I+c_{a}A$ ( $\\stackrel{\\cdot}{c}_{r}$ , $c_{a}\\in\\mathbb{R}^{+}$ ) and a Lipschitz nonlinearity $\\sigma$ , ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\pmb{H}^{(l)}=\\mathrm{Spatial}^{(l)}(\\pmb{H}^{(l-1)};\\pmb{A})=\\sigma\\left(\\pmb{S}\\pmb{H}^{(l-1)}\\pmb{W}^{(l-1)}\\right),\\ 1\\leq l\\leq\\ell\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "the Jacobian sensitivity satisfies the following upper bound: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\|\\frac{\\partial\\mathbf{h}_{v}^{(\\ell)}}{\\partial\\mathbf{h}_{u}^{(0)}}\\right\\|_{L^{1}}\\leq(c_{\\sigma}w d)^{\\ell}\\left(\\boldsymbol{S}^{\\ell}\\right)_{v u},\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "with $h_{u}^{(0)}$ , $h_{v}^{(\\ell)}$ denoting the rows of $H^{(0)}$ , $H^{(\\ell)}$ corresponding to the nodes $v$ , $u\\,\\in\\,{\\mathcal{G}}$ , $c_{\\sigma}$ the Lipschitz constant of the nonlinearity, $w$ the maximum entry value over all weight matrices $W^{(l)}$ , and d the network width. ", "page_idx": 21}, {"type": "text", "text": "The dependence of the upper bound on the matrix power $\\left(S^{\\ell}\\right)_{v u}$ \u2013 not generally present for $\\mathrm{S}^{2}\\mathrm{GNN}$ by Theorem 2 \u2013 leads to a topology-dependence which becomes explicit in the following theorem. It concerns the typical shallow-diameter regime, in which the number $\\ell$ of MPGNN layers is comparable to the graph diameter. ", "page_idx": 21}, {"type": "text", "text": "Theorem 8 (Adapted from Di Giovanni et al. (2023a)). Given an MPNN as in Eq. 10, with $c_{\\mathrm{a}}\\leq1$ , let $v,u\\,\\in\\,{\\mathcal G}$ be at distance $r$ . Let $c_{\\sigma}$ be the Lipschitz constant of $\\sigma,w$ the maximal entry-value overall weight matrices, $d_{\\mathrm{min}}$ the minimal degree of $\\mathcal{G}$ , and $\\gamma_{\\ell}(v,u)$ the number of walks from v to u of maximal length \u2113. For any $0\\leq k<r_{+}$ , there exists $C_{k}>0$ independent of $r$ and of the graph, such that ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\left\\Vert\\frac{\\partial\\mathbf{h}_{v}^{(r+k)}}{\\partial\\mathbf{h}_{u}^{(0)}}\\right\\Vert_{L^{1}}\\leq C_{k}\\gamma_{r+k}(v,u)\\left(\\frac{2c_{\\sigma}w d}{d_{\\operatorname*{min}}}\\right)^{r}.\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "For 1-hop MPGNNs with ${2c_{\\sigma}w d\\ <\\ d_{\\operatorname*{min}}}$ , we therefore identify an exponential decay of sensitivity with node distance $r$ in the weak-connectivity limit for which $\\gamma_{r+k}(v,u)$ increases subexponentially with $r$ . As Di Giovanni et al. (2023a) point out, sharper bounds can be derived under graph-specific information about $\\left(S^{r}\\right)_{v u}$ . ", "page_idx": 21}, {"type": "text", "text": "G Construction of an explicit ground truth filter ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "We express the electric potential along a periodic sequence of screened 1D charges as a convolution of a corresponding graph signal with a consistently defined graph filter. This closed-form example underscores our default use of a low-pass window for the spectral part of $\\mathrm{S}^{2}\\mathrm{GNNs}$ by showing how a continuous problem with a convolutional structure and quickly flattening spectral response (typical for pair interactions in physics and chemistry) discretizes into a graph problem with similar features. ", "page_idx": 21}, {"type": "text", "text": "The approach exploits the surjective mapping of Fourier modes on $[0,n]$ onto the Laplacian eigenvectors of a cycle graph $C_{n}$ . We consider two corresponding representations of the same problem: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\rho(x)=\\sum_{l=0}^{n-1}q_{l}\\Delta_{n}\\left(x-l\\right),\\ \\Delta_{m}(x)=\\sum_{m\\in\\mathbb{Z}}\\delta(x-m n),\\ V(x)=\\left(\\phi_{\\sigma}*_{\\mathbb{R}}\\rho\\right)(x)},}\\\\ {{\\displaystyle\\phi_{\\sigma}(x)=\\left(x\\mathrm{erf}\\left(\\frac{x}{\\sqrt{2}\\sigma}\\right)+\\sigma\\sqrt{\\frac{2}{\\pi}}\\exp\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right)\\right)-\\left|x\\right|},\\quad\\sigma>0}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{c}{{\\Big[}V(l)=(\\phi_{\\sigma}*_{\\mathbb{R}}\\rho)(l)\\stackrel{!}{=}(g_{\\sigma}*_{\\mathcal{G}}{q})_{l},\\,0\\leq k\\leq n-1,\\,\\forall{q}\\in\\mathbb{R}^{n}{\\Big]}}\\\\ {{\\mathcal{G}}={\\mathcal{C}}_{n},\\quad{\\pmb q}=(q_{1},\\ldots,q_{n})^{\\top}}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "\u2022 A continuous representation (Eq. 12) in terms of a 1D distribution $\\rho$ of $n$ point charges $q_{1},\\ldots,q_{n}$ and their periodically repeating image charges, written as a sum of Dirac combs at equidistant offsets $l$ with $0\\leq l\\leq n-1$ , interacting via the potential profile $\\phi_{\\sigma}$ obtained from solving in Gauss\u2019 law of electrostatics for a 1D point charge screened by a Gaussian cloud of opposite background charge with standard deviation $\\sigma$ . The screening ensures convergence to a finite potential and its exact form is insignificant (we choose the Gaussiantype screening due to its analytical tractability). Note that $\\phi_{\\sigma}(x)\\simeq\\mathrm{const.}-|x|$ for $x\\to0$ (the unscreened 1D potential in the direction normal to an infinitely wide capacitor plate), while the screening guarantees an exponential dropoff to zero as $x\\to\\infty$ , ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "\u2022 A graph representation (Eq. 14) by placing the $n$ charges $q_{1},\\ldots,q_{n}$ onto a cycle graph $\\mathcal{C}_{n}$ . ", "page_idx": 22}, {"type": "text", "text": "We derive the graph filter $g_{\\sigma}$ from a consistency condition (Eq. 13) between both representations: the graph convolution $(g_{\\sigma}*_{\\mathcal{G}}q)$ has to yield the electric potential $V$ sampled at the charge loci if we want $g_{\\sigma}$ to act like the continuous convolution kernel $\\phi_{\\sigma}$ in the discrete graph picture. ", "page_idx": 22}, {"type": "text", "text": "The Fourier transform of $\\phi_{\\sigma}$ (in the convention without integral prefactor and with a $2\\pi$ frequency prefactor) reads $\\begin{array}{r}{\\hat{\\phi}_{\\sigma}(\\kappa)\\;=\\;\\frac{1}{\\pi\\kappa^{2}}\\left(1-\\exp\\left(-\\frac{1}{2}\\sigma^{2}\\kappa^{2}\\right)\\right)}\\end{array}$ . For the density, the Poisson sum formula gives $\\begin{array}{r}{\\hat{\\rho}(\\kappa)=\\sum_{k=0}^{n-1}\\frac{1}{\\sqrt{n}}\\hat{q}_{k}\\Delta_{1}(\\kappa-\\frac{k}{n})}\\end{array}$ with $\\begin{array}{r}{\\bar{q}_{k}=\\frac{\\mathrm{i}}{\\sqrt{n}}\\sum_{j=0}^{n-1}q_{i}\\exp\\left(-i2\\pi\\frac{k}{n}j\\right)}\\end{array}$ . The coefficients $\\hat{q}_{k}$ are precisely the components of the graph Fourier transform of $\\pmb q$ (physically, they amount for the structure factor). By the convolution theorem, $\\hat{V}(\\kappa)=\\hat{\\phi}_{\\sigma}(\\kappa)\\hat{\\rho}(\\kappa)$ . By noting that all integer-shifted frequencies in the Dirac combs $\\textstyle\\Delta_{1}\\left(\\cdot-{\\frac{k}{n}}\\right)$ (or all Brillouin zones, in physics terminology) yield the same phase $\\exp\\left(i2\\pi{\\frac{k}{n}}l\\right)$ if we only sample $V(x)$ at the charge loci $x=l$ , $0\\leq l\\leq n-1$ , we can write $\\begin{array}{r}{V(l)=\\frac{1}{2\\pi}\\sum_{k=0}^{n-1}\\hat{q}_{k}\\left(\\sum_{m\\in\\mathbb{Z}}\\hat{\\phi}_{\\sigma}\\left(\\frac{k}{n}+m\\right)\\right)\\frac{1}{\\sqrt{n}}\\exp\\left(i2\\pi\\frac{k}{n}l\\right)}\\end{array}$ . Through pattern-matching with the consistency condition of Eq. 13, we can therefore identify that the graph filter is a sum over Brillouin zones, $\\begin{array}{r}{\\bigl(\\hat{g}_{\\sigma}\\bigr)\\bigl(\\lambda_{k}\\bigr)=\\frac{1}{2\\pi}\\sum_{m\\in\\mathbb{Z}}\\hat{\\phi}_{\\sigma}\\,\\bigl(\\frac{k}{n}+m\\bigr)}\\end{array}$ , where $\\lambda_{k}$ denotes the eigenvalues of the normalized $\\mathcal{C}_{n}$ graph Laplacian, $\\begin{array}{r}{\\lambda_{k}=1-\\cos\\left(\\frac{2\\pi k}{n}\\right)}\\end{array}$ . To fulfill this relation for all $n,k$ we set ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\hat{g}_{\\sigma}(\\lambda)=\\frac{1}{2\\pi}\\sum_{m\\in\\mathbb{Z}}\\hat{\\phi}_{\\sigma}\\left(\\frac{1}{2\\pi}\\operatorname{arccos}(1-\\lambda)+m\\right)\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We claim now (and prove in a later paragraph) that for $\\lambda>\\lambda_{0}>0$ and a sufficiently large choice $\\sigma\\;>\\;\\sigma(r,\\lambda_{0})$ , the absolute $r$ -th derivative satisfies the upper bound $\\begin{array}{r}{|\\frac{d^{r}}{d\\lambda^{r}}\\hat{g}_{\\sigma}(\\lambda)|\\;\\leq\\;|\\frac{d^{r}}{d\\lambda^{r}}\\hat{g}_{\\infty}(\\lambda)|.}\\end{array}$ , where we can think of $\\hat{g}_{\\infty}$ as the limit of taking $\\sigma\\rightarrow\\infty$ (i.e., a constant background charge): ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\hat{g}_{\\infty}(\\lambda)=\\frac{1}{2\\pi}\\sum_{m\\in\\mathbb{Z}}\\hat{\\phi}_{\\infty}\\left(\\frac{1}{2\\pi}\\operatorname{arccos}(1-\\lambda)+m\\right),\\quad\\hat{\\phi}_{\\infty}(\\kappa)=\\frac{1}{\\pi\\kappa^{2}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "The merit of this is that unlike the screened $\\hat{g}_{\\sigma}(\\lambda),\\hat{g}_{\\infty}(\\lambda)$ can be solved analytically to find closedform bounds on the absolute derivatives $\\Big|\\frac{d^{r}}{d\\lambda^{r}}\\hat{g}_{\\sigma}\\big(\\lambda_{0}\\big)\\Big|$ . By invoking the sum expansion form of the trigamma function $\\begin{array}{r}{\\Psi_{1}(z)=\\sum_{m=0}^{\\infty}\\frac{1}{(z+m)^{2}}}\\end{array}$ , the reflection identity $\\begin{array}{r}{\\psi_{1}(1-z)+\\psi_{1}(z)=\\,\\frac{\\pi^{2}}{\\sin^{2}\\pi z}}\\end{array}$ and the half-angle formula $\\begin{array}{r}{\\sin^{2}\\left(\\frac{x}{2}\\right)=\\frac{1-\\cos(x)}{2}}\\end{array}$ , we find ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{c}{\\hat{g}_{\\infty}(\\lambda)=\\displaystyle\\frac{1}{2\\pi^{2}}\\left(\\Psi_{1}\\left(\\frac{1}{2\\pi}\\operatorname{arccos}(1-\\lambda)\\right)+\\Psi_{1}\\left(1-\\frac{1}{2\\pi}\\operatorname{arccos}(1-\\lambda)\\right)\\right)}\\\\ {\\displaystyle=\\frac{1}{2\\sin^{2}\\left(\\frac{1}{2}\\operatorname{arccos}(1-\\lambda)\\right)}=\\frac{1}{\\lambda},}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "a remarkably simple result. We can now readily evaluate $\\begin{array}{r}{|\\frac{d^{r}}{d\\lambda^{r}}\\hat{g}_{\\infty}(\\lambda)|\\,=\\,\\frac{r!}{\\lambda^{r+1}}}\\end{array}$ , but it remains to prove that this upper-bounds $\\textstyle|\\frac{d^{r}}{d\\lambda^{r}}\\hat{g}_{\\sigma}(\\lambda)|$ for any $\\lambda>\\lambda_{0}>0$ and sufficiently large $\\sigma>\\sigma(r,\\lambda_{0})$ . For compactness, define the expressions $\\begin{array}{r}{z(\\lambda):=\\frac{1}{2\\pi}\\operatorname{arccos}(1-\\lambda)\\in\\left[0,\\frac{1}{2}\\right]}\\end{array}$ (strictly increasing in ", "page_idx": 22}, {"type": "text", "text": "$\\lambda$ ), $\\begin{array}{r}{y_{\\sigma}(z):=\\exp\\left(-\\frac{1}{2}\\sigma^{2}z^{2}\\right)}\\end{array}$ , and $\\tilde{z}=1-z\\geq z$ . Consider the series of \u201cterm-by-term\u201d derivatives ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle\\frac{d}{d z}\\hat{g}_{\\sigma}(\\lambda(z))=-\\frac{1}{\\pi^{2}}\\sum_{m=0}^{\\infty}\\left(\\frac{1}{(z+n)^{3}}(1-y_{\\sigma}(z+m))-\\frac{1}{(\\bar{z}+n)^{3}}(1-y_{\\sigma}(\\bar{z}+m))\\right)}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad+\\sum_{m=0}^{\\infty}\\mathcal{O}\\left(y_{\\sigma}(m)\\right)}}\\\\ {{\\displaystyle\\frac{d^{2}}{d z^{2}}\\hat{g}_{\\sigma}(\\lambda(z))=\\frac{3}{\\pi^{2}}\\sum_{m=0}^{\\infty}\\left(\\frac{1}{(z+n)^{4}}(1-y_{\\sigma}(z+m))+\\frac{1}{(\\bar{z}+n)^{4}}(1-y_{\\sigma}(\\bar{z}+m))\\right)}}\\\\ {{\\displaystyle\\qquad\\qquad\\qquad+\\sum_{m=0}^{\\infty}\\mathcal{O}\\left(y_{\\sigma}(m)\\right)}}\\\\ {{\\displaystyle\\qquad\\qquad\\vdots}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "They converge uniformly on $\\left[0,{\\frac{1}{2}}\\right]$ as they clearly are Cauchy sequences under uniform bound (moreover, well-definedness in $z=0$ follows by applying l\u2019Hospital\u2019s rule \u2013 physically, this is the merit provided by including Gaussian screening in our model). Therefore, they indeed converge to the respective derivatives $\\begin{array}{r}{\\frac{d^{r}}{d z^{r}}\\hat{g}_{\\sigma}(\\lambda(z))}\\end{array}$ (justifying the above notation). The same holds for the corresponding series for $\\begin{array}{r}{\\frac{d^{r}}{d z^{r}}\\hat{g}_{\\infty}(\\lambda(z))}\\end{array}$ : they are not defined in $z=0$ , but otherwise still converge as they match the known series expansion of the polygamma function. Given $\\lambda_{0}~>~0$ and thus $z(\\lambda_{0})>0$ , taking $\\sigma$ larger than some $\\sigma(r,\\lambda_{0})$ guarantees that $\\begin{array}{r}{\\frac{d^{r}}{d z^{r}}\\hat{g}_{\\sigma}(\\lambda(z))}\\end{array}$ and $\\begin{array}{r}{\\frac{d^{r}}{d z^{r}}\\hat{g}_{\\infty}(\\lambda(z))}\\end{array}$ are of the same sign for $\\lambda\\;>\\;\\lambda_{0}\\;(z(\\lambda)\\;>\\;\\dot{z}(\\lambda_{0}))$ . This holds for all orders $r\\,\\in\\,\\mathbb{N}$ since we see by induction that the product rule always yields one term analogous to the first respective terms above, and otherwise only terms of $\\mathcal{O}\\left(y_{\\sigma}\\dot{(}m\\dot{)}\\right)$ . Then, observing that $0\\leq1-y_{\\sigma}(\\bar{x})<1\\,\\forall\\,x\\geq0$ and $\\tilde{z}\\geq z$ implies $\\begin{array}{r}{|\\frac{d^{r}}{d z^{r}}\\hat{g}_{\\sigma}(\\lambda_{0}(z_{0}))|\\,\\le\\,|\\frac{d^{r}}{d z^{r}}\\hat{g}_{\\infty}(\\lambda_{0}(z_{0}))|}\\end{array}$ . The same must hold for the $\\lambda$ -derivatives by the chain rule. ", "page_idx": 23}, {"type": "text", "text": "One interesting question is whether ${\\hat{g}}_{\\sigma}$ is also $C$ -integral-Lipschitz for some constant $C>0$ . We discuss this stability-informed criterion (Gama et al., 2020) in the main body as a domain-agnostic prior assumption about the \u201cideal\u201d graph filter if no other ground truth knowledge informing additional smoothness bounds (such as here) is available. While the above bound is too loose to certify this directly $\\begin{array}{r}{(|\\frac{d}{d\\lambda}\\hat{g}_{\\sigma}(\\lambda)|\\le C\\lambda^{-1}}\\end{array}$ would be needed), integral-Lipschitzness under some constant follows from the fact that $|\\frac{d}{d\\lambda}\\hat{g}_{\\sigma}(\\lambda)|$ is bounded on [0, 2]: by the uniform convergence of the term-by-term derivative series, it is continuous. Well-definedness of the product $\\begin{array}{r}{\\frac{d}{d z}\\hat{g}_{\\sigma}\\frac{d z}{d\\lambda}}\\end{array}$ has to be checked in $\\lambda=0$ , where it follows by continuous extension using l\u2019Hospital\u2019s rule. As a continuous function defined on a compact interval, $|\\frac{d}{d\\lambda}\\hat{g}_{\\sigma}|$ assumes a maximum. ", "page_idx": 23}, {"type": "text", "text": "H Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "H.1 Proof of Theorem 1 ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "We next prove the permutation equivariance of the spectral filter in Eq. 3: ", "page_idx": 23}, {"type": "text", "text": "Theorem 1. Spectral $(H^{(l-1)};\\operatorname{EVD}(L,k))$ of Eq. 3 is equivariant to all $n\\,\\times\\,n$ permutation matrices $P\\in\\mathcal P$ : Spect $\\mathrm{:al}(P H^{(l-1)};\\mathrm{EVD}(P L P^{\\top},k))=P$ Spectral $(H^{(l-1)};\\operatorname{EVD}(L,k))$ . ", "page_idx": 23}, {"type": "text", "text": "for the general case of parametrizing a Hermitian \u201cLaplacian\u201d $\\b{L}\\in\\mathbb{C}^{n\\times n}$ , $L^{\\mathrm{H}}=L$ . Note that this proof does not rely in any means on the specifics of $\\textbf{\\emph{L}}$ , solely that the eigendecomposition exists $\\mathbf{\\dot{\\bar{\\rho}}}\\mathbf{\\bar{\\rho}}=V\\mathbf{A}V^{\\mathrm{H}}$ with unitary eigenvectors $\\pmb{\\dot{V}}\\pmb{V}^{\\mathrm{H}}=\\pmb{I}$ . For practical reasons, it is suitable to define ${\\pmb L}({\\pmb A})$ as a function of $\\pmb{A}$ . A similar proof for real-valued eigenvectors is given by (Lim et al., 2023). The specific spectral filter we consider is ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\operatorname{Spectral}(\\pmb{H}^{(l-1)};\\pmb{V},\\pmb{\\lambda})=h\\left[\\pmb{V}\\Big(\\hat{g}(\\pmb{\\lambda})\\odot\\big[\\pmb{V}^{\\mathrm{H}}f(\\pmb{H}^{(l-1)})\\big]\\Big)\\right]\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "with arbitrary $f:\\mathbb{C}^{d_{1}}\\rightarrow\\mathbb{C}^{d_{2}}$ , applied row-wise to $\\pmb{H}^{(l-1)}\\in\\mathbb{C}^{n\\times d_{1}}$ . Analogously, $h:\\mathbb{C}^{d_{2}}\\rightarrow\\mathbb{C}^{d_{3}}$ is applied row-wise. We choose complex functions to emphasize generality, although we restrict ", "page_idx": 23}, {"type": "text", "text": "Spectral to real in- and outputs in all experiments. The graph filter is defined as element-wise function $\\hat{g}_{u,v}(\\lambda):=\\hat{g}_{v}(\\lambda_{u},\\{\\bar{\\lambda_{1}},\\bar{\\lambda_{2}},\\ldots,\\lambda_{k}\\})$ that depends on the specific eigenvalue $\\lambda$ and potentially the set of eigenvalues $\\{\\lambda_{1},\\lambda_{2},\\ldots,\\lambda_{k}\\}$ (or its vector representation $\\lambda$ ) of the partial eigendecomposition. ", "page_idx": 24}, {"type": "text", "text": "We need to make sure that the partial decomposition includes all eigenvalues of the same magnitude, i.e., $\\lambda_{u}\\;\\neq\\;\\lambda_{u^{\\prime}},\\forall u\\;\\in\\;\\{1,2,\\ldots,k\\},u^{\\prime}\\;\\in\\;\\{k+1,k+2,\\ldots,n\\}$ . In practice, this is achieved by choosing large enough $k$ to accommodate all eigenvalues $\\lambda_{\\mathrm{cut}}~<~\\lambda_{k+1}$ , or by dropping trailing eigenvalues where $\\lambda_{j}\\,=\\,\\lambda_{k+1}$ for $j\\,\\in\\,\\{1,2,\\dots,k\\}$ . Generally, it is also not important that we consider the $k$ smallest eigenvalues in the spectral filter. We only need to ensure that the spectral filter is either calculated on all or no eigenvalues/-vectors of an eigenspace. ", "page_idx": 24}, {"type": "text", "text": "Proof. Assuming functions $\\phi(X)$ and $\\psi(X)$ are permutation equivariant, then $\\phi(\\psi(X))$ is permutation equivariant $\\phi(\\psi(P X))\\,=\\,\\phi(P\\psi(X))\\,=\\,P\\phi(\\psi(X))\\,$ for any $n\\times n$ permutation $P\\,\\in\\,\\mathcal P$ . Thus, it sufficies to prove permutation equivariance for $h,{\\dot{f}},V({\\hat{g}}(\\mathbf{\\dot{\\lambda}})\\odot[V^{\\mathrm{{H}}}X])$ independently, where X \u2208Cn\u00d7d2. ", "page_idx": 24}, {"type": "text", "text": "Regardless of the complex image and domain of $h$ and $f$ , they are permuation equivariant since they are applied row-wise ", "page_idx": 24}, {"type": "equation", "text": "$$\nf(\\mathbf{X})=\\left[f(\\mathbf{X}_{1})\\quad f(\\mathbf{X}_{2})\\quad.\\ldots\\quad f(\\mathbf{X}_{n})\\right]^{\\mathrm{H}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "and reordering the rows in $X\\in\\mathbb{C}^{n\\times d_{1}}$ also reorders the outputs: $f(P X)=P f(X)$ ", "page_idx": 24}, {"type": "text", "text": "For finalizing the proof of permutation equivariance, we first rearrange $\\begin{array}{r}{\\pmb{Y}=\\pmb{V}(\\hat{g}(\\pmb{\\lambda})\\odot[\\pmb{V}^{\\mathrm{H}}\\pmb{X}])=}\\end{array}$ $\\begin{array}{r}{\\sum_{u=1}^{k}\\pmb{v}_{u}(\\hat{g}_{u,:}(\\lambda_{u})\\odot[\\pmb{v}_{u}^{\\mathrm{H}}\\pmb{X}])}\\end{array}$ and $\\begin{array}{r}{\\pmb{Y}_{:,v}=\\sum_{u=1}^{k}\\hat{g}_{u,v}(\\lambda_{u})\\pmb{v}_{u}\\pmb{v}_{u}^{\\mathrm{H}}\\pmb{X}_{:,v}.}\\end{array}$ . ", "page_idx": 24}, {"type": "text", "text": "This construction (a) is invariant to the ambiguity that every eigenvector $\\pmb{v}_{u}$ can be arbitrarily rotated $c_{u}v_{u}$ by $\\{c_{u}\\in\\mathbb{C}\\,|\\,|c_{u}|=1\\}$ . That is, $(c_{u}\\boldsymbol{v}_{u}\\grave{)}(c_{u}^{\\enspace\\bullet}\\boldsymbol{v}_{u})^{\\mathrm{H}}=\\dot{c}_{u}\\bar{c}_{u}\\overset{\\smile}{\\boldsymbol{v}}_{u}\\boldsymbol{v}_{u}^{\\mathrm{H}}=\\boldsymbol{v}_{u}\\boldsymbol{\\bar{v}}_{u}^{\\mathrm{H}}$ ", "page_idx": 24}, {"type": "text", "text": "Moreover, (b) in the case of $j$ repeated eigenvalues $\\{s+1,s+2,\\ldots,s+j\\}$ where $\\lambda_{s+1}=\\lambda_{s+2}=$ $\\cdots=\\lambda_{s+j}$ , we can choose a set of orthogonal eigenvectors arbitrarily rotated/reflected from the $j$ -dimensional eigenspace (basis symmetry). The given set of eigenvectors can be arbitrarily transformed $V_{:,s+1:s+j}\\mathbf{\\Gamma}_{j}$ by a matrix chosen from the unitary group $\\Gamma_{j}\\in U(j)$ . Since ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\sum_{u=s}^{s+j}\\hat{g}_{u,v}(\\lambda_{u}){v}_{u}{v}_{u}^{\\mathrm{H}}X_{:,v}=\\hat{g}_{s,v}(\\lambda_{s})\\left[\\sum_{u=s}^{s+j}{v}_{u}{v}_{u}^{\\mathrm{H}}\\right]X_{:,v}=\\hat{g}_{s,v}(\\lambda_{s})\\left[V_{:,s+1:s+j}V_{:,s+1:s+j}^{\\mathrm{H}}\\right]X_{:,v}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "we simply need to show that the expression is invariant to a transformation by $\\Gamma_{j}$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\nV_{;,s+1:s+j}\\mathbf{P}_{j}(V_{;s+1:s+j}\\mathbf{P}_{j})^{\\mathrm{H}}=V_{;,s+1:s+j}\\mathbf{P}_{j}\\mathbf{P}_{j}^{\\mathrm{H}}V_{;s+1:s+j}^{\\mathrm{H}}=V_{;s+1:s+j}V_{;s+1:s+j}^{\\mathrm{H}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "To see why $\\Gamma_{j}\\in U(j)$ is a sufficient choice in the light of repeated/multiple eigenvalues, consider the defintion of eigenvalues/vectors ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{L V_{:,s+1:s+j}=L\\left[\\begin{array}{c c c c c}{|}&{|}&&{|}&&{|}\\\\ {v_{s+1}}&{v_{s+2}}&{\\cdots}&{v_{s+j}|}\\\\ {|}&{|}&&{|}\\end{array}\\right]}\\\\ &{\\qquad=\\left[v_{s+1}^{\\bigstar}\\begin{array}{c c c c}{|}&&{|}&\\\\ {v_{s+2}}&{\\cdots}&{v_{s+j}|}\\\\ {|}&{|}&&{|}\\end{array}\\right]\\left[\\begin{array}{c c c c c}{\\lambda_{s+1}}&{0}&{\\cdots}&{0}\\\\ {0}&{\\lambda_{s+2}}&{\\cdots}&{0}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ {0}&{0}&{\\cdots}&{\\lambda_{s+j}}\\end{array}\\right]}\\\\ &{\\qquad=\\lambda_{s+1}\\left[v_{s+1}^{\\bigstar}\\begin{array}{c c c c c}{|}&&{|}&\\\\ {v_{s+2}}&{\\cdots}&{v_{s+j}|}\\end{array}\\right]}\\\\ &{\\qquad=\\lambda_{s+1}V_{:,s+1:s+j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "we can now multiply both sides from the right with an arbitrary matrix $B\\in\\mathbb{C}^{j\\times j}$ . To preserve the unitary property $\\mathring{V}_{:,s+1:s+j}^{\\ \\cdot}V_{:,s+1:s+j}^{\\mathrm{H}}=I$ , we require $(V_{:,s+1:s+j}B)(V_{:,s+1:s+j}B)^{\\mathrm{H}}\\,^{\\prime}{=}\\,I$ . Thus, the eigenvectors can be arbitrarily transformed by $\\Gamma_{j}\\in U(j)$ instead of $B\\in\\mathbb{C}^{j\\times j}$ . ", "page_idx": 24}, {"type": "text", "text": "This concludes the proof. ", "page_idx": 24}, {"type": "text", "text": "H.2 Proof of Theorem 2 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "We restate Theorem 2 in more detail and also considering graphs that contain multiple connected components. The unchanged bottom line is that $\\mathrm{S}^{2}\\mathrm{GNNs}$ can express signals lower-bounded by a constant that is unaffected by local properties of the graph topology, instead of suffering from exponential sensitivity decay like spatial MPGNNs. ", "page_idx": 25}, {"type": "text", "text": "Theorem (Theorem 2, formal). Consider an $\\ell_{}$ -layer $S^{2}G N N$ of the form Eq. 1. Let $(\\tilde{\\vartheta},\\vartheta,\\theta)$ be parameters of the spatial GNN, spectral filters $\\hat{g}_{\\vartheta}^{(l)}$ , and feature transformation $f_{\\theta}$ . Assume the existence of parameters $\\tilde{\\vartheta}$ such that $\\mathrm{Spatial}^{(l)}(H^{(l-1)};A,\\tilde{\\vartheta})\\,=\\,0\\;\\forall1\\,\\le\\,l\\,\\le\\,\\ell$ and $\\theta$ such that $f_{\\theta}=I$ . Then, a filter choice $\\vartheta$ exists such that the $\\ell$ -layer $S^{2}G N N$ of the additive form Eq. 1 can express a signal $\\mathbf{\\dot{h}}_{v}^{(\\ell)}(H^{(0)};\\tilde{\\vartheta},\\vartheta,\\theta)$ with uniformly lower-bounded Jacobian sensitivity, ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left\\|\\frac{\\partial\\mathbf{h}_{v}^{(\\ell)}(H^{(0)};\\widetilde{\\vartheta},\\vartheta,\\theta)}{\\partial\\mathbf{h}_{u}^{(0)}}\\right\\|_{L^{1}}\\ge\\left\\{\\frac{\\frac{d K_{\\vartheta}^{\\ell}}{2|E_{c}|}\\;i f u,v\\;c o n n e c t e d,}{0\\;o t h e r w i s e,}\\right.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "with $h_{u}^{(0)}$ , $h_{v}^{(\\ell)}$ denoting the rows of $H^{(0)}$ , $H^{(\\ell)}$ corresponding to the nodes $u\\neq v\\in\\mathcal{G},$ , connected component $\\mathcal{C}\\subset\\mathcal{G}$ containing $|E_{\\mathcal{C}}|$ edges, network width $d$ and parameter-dependent constant $K_{\\vartheta}$ . ", "page_idx": 25}, {"type": "text", "text": "Proof. Choose $\\tilde{\\vartheta}$ such that $\\mathrm{Spatial}^{(l)}(\\pmb{H}^{(l-1)};\\pmb{A})=0\\,\\forall1\\leq l\\leq\\ell$ (typically by setting all weights and biases to zero), $\\theta$ such that $f_{\\theta}=I$ , and set $\\vartheta$ such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\hat{g}_{k}^{(l)}(\\lambda;\\vartheta)=K_{\\vartheta}\\left\\{\\!\\!\\begin{array}{l}{{1\\;\\mathrm{for}\\;\\lambda=0,}}\\\\ {{0\\;\\mathrm{for}\\;\\lambda>0,}}\\end{array}\\right.\\!\\!\\!\\quad\\forall1\\leq l\\leq\\ell,\\,1\\leq k\\leq d\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "for some $K_{\\vartheta}\\,>\\,0$ . This choice of filter parameters $\\vartheta$ lets Spectral act like a type of virtual node across all hidden dimensions $k$ : In the standard orthonormal basis of the 0-eigenspace given by ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\left(v^{(C)}\\right)_{u}={\\sqrt{\\frac{d_{u}}{2|E_{C}|}}}\\left\\{1{\\mathrm{~for~}}u\\in{\\mathcal{C}},\\right.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $\\mathcal{C}$ enumerates all connected components, and $d_{u}$ denotes the degree of node $u$ , we find ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\bf h}_{v}^{(\\ell)}(H^{(0)};\\tilde{\\vartheta},\\vartheta,\\theta)=\\Big(\\mathrm{Spectral}^{(\\ell)}\\circ\\cdot\\cdot\\cdot\\circ\\mathrm{Spectral}^{(0)}\\Big)_{v}\\left(H^{(0)};V,\\lambda\\right)}\\\\ &{\\quad\\quad\\quad\\quad\\quad\\quad=\\displaystyle\\frac{K_{\\vartheta}^{\\ell}\\sqrt{d_{v}}}{2|E_{C^{(v)}}|}\\sum_{u\\in\\mathcal{C}^{(v)}}\\sqrt{d_{u}}{\\bf h}_{u}^{(0)},}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "with $\\boldsymbol{\\mathcal{C}}^{(v)}$ denoting the connected component containing $v$ . Particularly, note that applying the spectral layer more than once does not affect the result since the projector onto an eigenvector is idempotent (up to $K_{\\vartheta}$ ). The result must also hold in any other orthonormal basis of the 0-eigenspace due to the invariance of Spectral under orthogonal eigenbasis transformations. Differentiating with respect to $\\mathbf{h}_{u}^{(0)}$ , taking the $L^{1}$ norm and using $\\sqrt{d_{u}d_{v}}\\ge1$ shows the statement. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "H.3 Proof of Theorem 3 ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Theorem 3. Let $\\hat{g}$ be a discontinuous spectral filter. For any approximating sequence $\\left(g_{\\gamma_{p}}\\right)_{p\\in\\mathbb{N}}$ of polynomial filters, an adversarial sequence $(\\mathcal{G}_{p})_{p\\in\\mathbb{N}}$ of input graphs exists such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\nexists\\alpha\\in\\mathbb{R}_{>0}\\colon\\operatorname*{sup}_{0\\neq{\\pmb X}\\in\\mathbb{R}^{|\\mathcal{G}_{p}|\\times d}}\\frac{\\|(g_{\\gamma_{p}}-g)*_{\\mathcal{G}_{p}}{\\pmb X}\\|_{\\mathrm{F}}}{\\|{\\pmb X}\\|_{\\mathrm{F}}}=\\mathcal{O}\\left(p^{-\\alpha}\\right)\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The proof makes use of a result by S. Bernstein (Natanson, 1964): ", "page_idx": 25}, {"type": "text", "text": "Theorem 9 (Bernstein). Let $f\\colon[0,2\\pi]\\ \\rightarrow\\mathbb{C}$ be a $2\\pi$ -periodic function. Then $f$ is $\\alpha$ -H\u00f6lder continuous for some $\\alpha\\in(0,1)$ if, for every $p\\in\\mathbb{N},$ , there exists a degree- $\\boldsymbol{p}$ trigonometric polynomial $\\begin{array}{r}{T_{p}(x)=a_{0}+\\sum_{j=1}^{p}a_{j}\\cos(j x)+\\sum_{j=1}^{p}\\bar{b}_{j}\\sin(j x)}\\end{array}$ with coefficients $a_{j},b_{j}\\in\\mathbb{C},$ , such that ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{0\\leq x\\leq2\\pi}|f(x)-T_{p}(x)|\\leq{\\frac{C(f)}{p^{\\alpha}}}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "where $C(f)$ is a positive number depending on $f$ . ", "page_idx": 25}, {"type": "text", "text": "Proof. Given a discontinuous filter $\\hat{g}\\colon[0,2]\\ \\rightarrow\\ \\mathbb{R}$ , construct the function $f\\colon[0,2\\pi]\\ \\rightarrow\\mathbb{C}$ fulfilling the prerequisites of Theorem 9 by pre-composing $f\\;:=\\;\\hat{g}\\circ(\\cos(\\cdot)\\;+\\;\\dot{1})$ . We proceed via contradiction. Suppose that there is an $\\alpha\\,\\in\\,(0,1)$ and a sequence of degree- $\\boldsymbol{p}$ polynomial filters, $\\begin{array}{r}{\\hat{g}_{\\gamma_{p}}(\\lambda)=\\sum_{j=0}^{p}\\gamma_{j}\\lambda^{j}}\\end{array}$ , $\\gamma=(\\gamma_{0},\\ldots,\\gamma_{p})^{\\top}\\in\\mathbb{R}^{p+1}$ , such that $\\|\\hat{g}_{\\gamma_{p}}-\\hat{g}\\|_{\\infty}=\\mathcal{O}(p^{-\\alpha})$ . Then, the sequence of trigonometric polynomials $T_{p}:=\\hat{g}_{\\gamma_{p}}\\circ(\\cos(\\cdot)+1)$ fulfills the condition of Theorem 9. This would imply that $f=\\hat{g}\\circ(\\cos(\\cdot)+1)$ is $\\alpha$ -H\u00f6lder continuous, meaning that a constant $K>0$ exists such that ", "page_idx": 26}, {"type": "equation", "text": "$$\n|{\\widehat{g}}(\\cos(x)+1)-{\\widehat{g}}(\\cos(y)+1)|\\leq K|x-y|^{\\alpha}\\,\\forall\\,x,y\\in[0,2\\pi]\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Considering $\\lambda_{0}\\in\\left[0,2\\right]$ , $\\lambda\\rightarrow\\lambda_{0}$ and $x=\\operatorname{arccos}(\\lambda_{0}-1)$ , $y=\\operatorname{arccos}(\\lambda-1)$ (using the arccos branch in which both $\\lambda_{0},\\,\\lambda$ eventually end up) shows a contradiction to the assumed discontinuity of $\\hat{g}$ . Therefore, no polynomial filter sequence $\\left(\\hat{g}_{\\gamma_{p}}\\right)_{p\\in\\mathbb{N}}$ together with an $\\alpha\\in(0,1)$ exist such that $\\|\\hat{g}_{\\gamma_{p}}-\\hat{g}\\|_{\\infty}=\\mathcal{O}(p^{-\\alpha})$ . In particular, for any sequence $\\left({\\hat{g}}_{\\gamma_{p}}\\right)_{p\\in\\mathbb{N}}$ , a sequence of adversarial values $(\\lambda_{p})_{p\\in\\mathbb{N}}$ , $\\lambda_{p}\\in[0,2]$ exists such that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\nexists\\alpha\\in(0,1)\\colon\\;|\\hat{g}_{\\gamma_{p}}(\\lambda_{p})-\\hat{g}(\\lambda_{p})|=\\mathcal{O}(p^{-\\alpha})\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "The proof is finished if we can find a sequence of graphs $(\\mathcal{G}_{p})$ such that the symmetricallynormalized graph Laplacian $L_{p}$ of $\\mathcal{G}_{p}$ contains $\\lambda_{p}$ as an eigenvalue. In this case, we can construct adversarial input signals $X_{p}$ on the graphs $\\mathcal{G}_{p}$ by setting the first embedding channel to an eigenvector corresponding to $\\lambda_{p}$ , and the remaining channels to zero, such that $\\left(g_{\\gamma_{p}}-g\\right)\\,*_{\\mathcal{G}_{p}}\\,X_{p}\\,=$ $|\\hat{g}_{\\gamma_{p}}(\\lambda_{p})-\\hat{g}(\\lambda_{p})|X_{p}$ . In particular, it then holds that ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\nexists{\\boldsymbol{\\alpha}}\\in\\mathbb{R}^{+}\\colon\\operatorname*{sup}_{\\boldsymbol{0}\\neq\\boldsymbol{X}\\in\\mathbb{R}^{|\\mathcal{G}_{p}|\\times{\\boldsymbol{d}}}}\\frac{\\|(\\boldsymbol{g}_{\\gamma_{p}}-\\boldsymbol{g})\\ast\\boldsymbol{g}_{p}\\boldsymbol{\\mathbf{\\boldsymbol{X}}}\\|_{\\mathrm{F}}}{\\|\\boldsymbol{X}\\|_{\\mathrm{F}}}=\\mathcal{O}\\left(p^{-\\alpha}\\right)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "If we assume only simple graphs, such a construction is unfortunately not possible since the set of all simple graphs and therefore the set of all realizable eigenvalues is countable, whereas the adversarial values $\\lambda_{p}$ could lie anywhere in the uncountable set $[0,2]$ . We can, however realize arbitrary eigenvalues by using weighted graphs with three nodes. Consider a cyclic graph structure and tune the weight of edge $(1,2)$ to $\\sin^{\\mathfrak{Z}}(\\theta_{p}^{\\star})$ and the weight of edges $(2,3)$ and $(3,1)$ to $\\cos^{2}(\\theta_{p})$ with $\\theta_{p}\\in\\left[0,\\frac{\\pi}{2}\\right]$ . The symmetrically-normalized graph Laplacian, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\pmb{L}_{p}=\\left(\\begin{array}{c c c}{1}&{-\\cos^{2}(\\theta_{p})}&{-\\sin^{2}(\\theta_{p})}\\\\ {-\\cos^{2}(\\theta_{p})}&{1}&{-\\sin^{2}(\\theta_{p})}\\\\ {-\\sin^{2}(\\theta_{p})}&{-\\sin^{2}(\\theta_{p})}&{1}\\end{array}\\right),\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "has eigenvalues $\\lambda_{p}^{(1)}\\;=\\;1,\\;\\lambda_{p}^{(2)}\\;=\\;\\sin^{2}(\\theta_{p}),\\;\\lambda_{p}^{(3)}\\;=\\;2\\,-\\,\\sin^{2}(\\theta_{p}).$ . $\\lambda_{p}^{(2)}$ can assume all values $\\lambda_{p}\\in[0,1]$ , whereas $\\lambda_{p}^{(3)}$ can assume all values $\\lambda_{p}\\in[1,2]$ . This finishes the proof. \u53e3 ", "page_idx": 26}, {"type": "text", "text": "Remark. If one wishes to restrict the set of possible adversarial graph sequences $\\left(\\mathcal{G}_{p}\\right)_{p\\in\\mathbb{N}}$ to include only simple graphs, a version of Theorem 3 still holds where we restrict the assumption to filters $\\hat{g}$ which are piecewise-continuous with discontinuities on a finite set of points $\\mathcal{D}\\subset\\mathcal{S}$ , where $s\\subset$ $[0,2]$ denotes the countable set of eigenvalues realizable by simple graphs. This still covers a large class of filters to which order- $\\cdot p$ polynomial filters can provably converge slower than any inverse root of $p$ in the operator norm, and includes the virtual node filter (discontinuous only in $\\lambda=0$ ) presented as an example in the main body. The proof is fully analogous up to the point of constructing $\\lambda_{p}$ . If $\\lambda_{p}\\ \\in\\ {\\mathcal{D}}$ , we can find a graph that realizes it exactly. Now assume $\\lambda_{p}\\ \\notin\\ D$ . We note that the set $\\boldsymbol{S}$ is dense in [0, 2] (clear from considering, e.g., the cyclic graphs $\\mathcal{C}_{n}$ with symmetricallynormalized Laplacian eigenvalues $\\begin{array}{r}{\\lambda_{k}=1-\\cos\\left(\\frac{2\\pi k}{n}\\right))}\\end{array}$ . Since we assume that $\\hat{g}$ and therefore also $|\\hat{g}_{\\gamma_{p}}-\\hat{g}|$ is piecewise-continuous anywhere but on $\\mathcal{D}\\subset\\mathcal{S}$ and $\\mathcal{D}$ is finite, we can find an open neighborhood $\\mathcal{N}(\\lambda_{p})$ for any $\\lambda_{p}\\notin\\mathcal{D}$ on which $\\hat{g}$ is continuous. Using that $\\boldsymbol{S}$ is dense in $[0,2]$ , we find a graph sequence $\\left(\\tilde{\\mathcal{G}}_{p}^{(l)}\\right)_{l\\in\\mathbb{N}}$ $\\tilde{\\lambda}_{p}^{(l)}\\in\\mathcal{N}(\\lambda_{p})\\,\\forall l\\in\\mathbb{N},\\,\\left(\\tilde{\\lambda}_{p}^{(l)}\\right)_{l\\in\\mathbb{N}}\\rightarrow\\lambda_{p}$ for which $\\lVert\\hat{g}_{\\gamma_{p}}(\\tilde{\\lambda}_{p}^{(l)})-\\hat{g}(\\tilde{\\lambda}_{p}^{(l)})\\rVert\\to\\lVert\\hat{g}_{\\gamma_{p}}(\\lambda_{p})-\\hat{g}(\\lambda_{p})\\rVert$ . Therefore, by the same reasoning as in the proof of Theorem 3, we find that there can be no $\\alpha\\in(0,1)$ for which $\\begin{array}{r}{\\operatorname*{sup}_{0\\neq{\\pmb X}\\in\\mathbb{R}^{|\\mathcal G_{p}|\\times d}}\\frac{\\|(g_{\\gamma_{p}}-g)*_{\\mathcal G_{p}}{\\pmb X}\\|_{\\mathrm{F}}}{\\|{\\pmb X}\\|_{\\mathrm{F}}}}\\end{array}$ $O\\left(p^{-\\alpha}\\right)$ . ", "page_idx": 26}, {"type": "text", "text": "H.4 Proof of Theorem 4 ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We first introduce the setting and notation to state Theorem 4 in its general version. We study how well $\\mathrm{S}^{2}\\mathrm{GNNs}$ can approximate \u201cidealized\u201d GNNs (IGNNs) containing $L$ graph convolution layers $1\\le l\\le L$ , each of which can express a convolution operator $g$ with any spectral representation $\\hat{g}^{(l)}\\colon[0,2]\\rightarrow\\mathbb{R}^{d^{(l)}}$ . An IGNN layer therefore has the structure ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\pmb{\\mathscr{H}}^{(l)}=\\sigma\\left(g^{(l)}*_{\\mathscr{G}}[\\pmb{\\mathscr{H}}^{(l-1)}\\pmb{W}^{(l)}]\\right)=\\sigma\\left(\\pmb{V}\\hat{g}^{(l)}(\\pmb{\\mathscr{A}})\\odot[\\pmb{V}^{\\top}\\pmb{\\mathscr{H}}^{(l-1)}\\pmb{W}^{(l)}]\\right)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "with $\\pmb{\\mathcal{H}}^{(l)}\\in\\mathbb{R}^{n\\times D^{(l)}}$ , $\\pmb{W}^{(l)}\\in\\mathbb{R}^{D^{(l)}\\times D^{(l-1)}}$ and $V\\in\\mathbb{R}^{n\\times n}$ . ", "page_idx": 27}, {"type": "text", "text": "We compare this to $\\mathrm{S}^{2}\\mathrm{GNNs}$ with $\\ell=(m+1)L$ layers for $m\\geq1$ , in the additive form of Eq. 1, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\pmb{H}^{(l)}=\\mathrm{Spectral}^{(l)}(\\pmb{H}^{(l-1)};\\pmb{V},\\pmb{\\lambda})+\\mathrm{Spatial}^{(l)}(\\pmb{H}^{(l-1)};\\pmb{A})\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Each layer $1\\leq l\\leq\\ell$ parametrizes a spatio-spectral convolution. The spectral part satisfies Eq. 3, ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\operatorname{Spectral}^{(l)}(H^{(l-1)};V,\\lambda)=V\\Big(\\hat{g}_{\\vartheta}^{(l)}(\\lambda)\\odot\\big[V^{\\top}H^{(l-1)}W_{\\mathrm{spec}}^{(l)}\\big]\\Big)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "with embeddings $\\pmb{H}^{(l)}\\,\\in\\,\\mathbb{R}^{n\\times d^{(l)}}$ , linear feature transforms $f_{\\theta}^{(l)}\\;:=\\;W_{\\mathrm{spec}}^{(l)}\\;\\in\\;\\mathbb{R}^{d^{(l)}\\times d^{(l-1)}}$ and a spectral filter $\\hat{g}_{\\vartheta}^{(l)}\\colon[0,2]\\rightarrow\\mathbb{R}$ that is fully supported and a universal approximator on $[0,\\lambda_{\\mathrm{cut}}]$ . Note we assume here that in every layer, there is only one spectral filter which gets reshaped as to act on every hidden component, whereas in practice, we relax this assumption to different filters per component, which can only be more expressive. The spatial part is a polynomial filter of the form ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{Spatial}^{(l)}(H^{(l-1)};A)=\\sigma\\left(\\left[\\displaystyle\\sum_{j=0}^{p}\\gamma_{j}^{(l)}L^{j}\\right]H^{(l-1)}W_{\\mathrm{spat}}^{(l)}\\right)}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\sigma\\left(V\\left(\\hat{g}_{\\gamma}^{(l)}(\\lambda)\\odot\\left[V^{\\top}H^{(l-1)}W_{\\mathrm{spat}}^{(l)}\\right]\\right)\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "with $W_{\\mathrm{spat}}^{(l)}\\,\\in\\,\\mathbb{R}^{d^{(l)}\\times d^{(l-1)}}$ , polynomial order $p$ (fixed across layers), and a spectral representation $\\begin{array}{r}{\\hat{g}_{\\gamma}^{(l)}(\\lambda)=\\sum_{j=0}^{p}\\gamma_{j}^{(l)}\\lambda^{j}}\\end{array}$ with coefficients $\\pmb{\\gamma}^{(l)}=(\\gamma_{0}^{(l)},\\dots,\\gamma_{p}^{(l)})^{\\top}\\in\\mathbb{R}^{p+1}$ . We note that theorem 4 extends immediately to the case of directed graphs if the spatial part is instead a polynomial of the magnetic Laplacian (see section 3.2.3) over complex-valued embeddings like in Zhang et al. (2021). ", "page_idx": 27}, {"type": "text", "text": "Note that the layer-wise hidden dimensions $D^{(l)}$ vs. $d^{(l)}$ of the IGNN vs. $\\mathrm{S}^{2}\\mathrm{GNN}$ do not have to agree except at the input layer, $d^{(0)}\\,=\\,D^{(0)}$ (of course, both networks receive the same input $\\pmb{\\mathcal{H}}^{(0)}\\,=\\,\\pmb{H}^{(0)}\\,=\\,\\pmb{X})$ , and at the output layer, $d^{(\\ell)}\\,=\\,D^{(L)}$ . We now state the general version of Theorem 4. ", "page_idx": 27}, {"type": "text", "text": "Theorem (Theorem 4, general). Assume an $L$ -layer IGNN with filters $\\hat{g}^{(l)}$ such that $\\hat{g}^{(l)}\\big|_{[\\lambda_{c u t},2]}\\in$ $C^{r}[\\lambda_{c u t},2]$ and $\\begin{array}{r}{\\left|\\left|\\frac{d^{r}}{d\\lambda^{r}}\\hat{g}^{(l)}\\right|_{\\left[\\lambda_{c u t},2\\right]}\\right|\\right|_{\\infty}\\le K_{r}^{m a x}\\left(\\lambda_{c u t}\\right)}\\end{array}$ for all $1\\le l\\le L$ . Let $\\|\\hat{g}^{(l)}\\|_{\\infty}\\,\\le\\,\\|\\hat{g}\\|_{\\infty}^{m a x}$ and $\\lVert W^{(l)}\\rVert_{2}\\leq\\lVert W\\rVert_{2}^{m a x}$ for all $1\\leq l\\leq L$ . Assume that $\\sigma=\\left[\\,\\cdot\\,\\right]_{\\Sigma}$ is the ReLu function. Then, ", "page_idx": 27}, {"type": "text", "text": "$(I)$ For a fixed polynomial order $p\\geq2$ , an approximating sequence $\\left(S^{2}G N N_{m}\\right)_{m\\in\\mathbb{N}}o f[(m+1)L].$ layer $S^{2}G N N s$ exists such that, for arbitrary graph sequences $(\\mathcal{G}_{m})_{m\\in\\mathbb{N}}$ , ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{0\\neq X\\in\\mathbb{R}^{|\\mathcal{G}_{p}|\\times d}}{\\operatorname*{sup}}\\frac{\\|\\left[\\left(S^{2}G N N_{m}\\right)\\mathcal{G}_{m}-\\left(I G N N\\right)\\mathcal{G}_{m}\\right](X)\\|_{F}}{\\|X\\|_{F}}}\\\\ &{=\\mathcal{O}\\Big(C_{L}(\\|\\hat{g}\\|_{\\infty}^{m a x},\\|W\\|_{2}^{m a x})\\;K_{r}^{m a x}\\left(\\lambda_{c u t}\\right)\\;(p m)^{-r}\\Big),}\\\\ &{\\quad C_{L}(\\|\\hat{g}\\|_{\\infty}^{m a x},\\|W\\|_{2}^{m a x})=\\|W\\|_{2}^{m a x}\\displaystyle\\prod_{l=1}^{L-1}\\;\\!\\!\\!\\big[\\|\\hat{g}\\|_{\\infty}^{m a x}\\|W\\|_{2}^{m a x}+(\\|\\hat{g}\\|_{\\infty}^{m a x}\\|W\\|_{2}^{m a x})^{l}\\big]}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "with a leading-order scaling constant that depends only on $r$ . Here, $(\\,\\cdot\\,)_{\\mathcal{G}_{m}}$ denotes the instantiation of all model filters on the eigenvalues of an input graph ${\\mathcal{G}}_{m}$ , which maps both models onto a ${\\mathcal{G}}_{m}$ - dependent function $\\mathbb{R}^{D^{(0)}}\\rightarrow\\mathbb{R}^{D^{(L)}}$ . ", "page_idx": 27}, {"type": "text", "text": "(2) For fixed $m\\geq1$ , an approximating sequence $\\left(S^{2}G N N_{p}\\right)_{p\\in\\mathbb{N}}$ of $[(m+1)L]$ -layer $S^{2}G N N s$ with increasing layer-wise polynomial order $p$ exists such that, for all $(\\mathcal{G}_{p})_{p\\in\\mathbb{N}}$ , the same bound holds. ", "page_idx": 27}, {"type": "text", "text": "Proof. We first prove the following lemma, narrowing down the previous theorem to a single layer. ", "page_idx": 28}, {"type": "text", "text": "Lemma 1. Let $I G N N^{(l)}$ denote a single IGNN layer as in Eq. 20, with a filter ${\\hat{g}}^{(l)}$ such that $\\hat{g}^{(l)}\\big|_{[\\lambda_{c u t},2]}$ is $r$ -times continuously differentiable on $[\\lambda_{c u t},2]$ and satisfies a bound $K_{r}\\left(\\tilde{\\hat{g}}^{(l)},\\lambda_{c u t}\\right)\\ge0,$ , $\\begin{array}{r}{\\left|\\frac{d^{r}}{d\\lambda^{r}}\\hat{g}^{(l)}(\\dot{\\lambda})\\right|\\le K_{r}\\left(\\hat{g}^{(l)},\\lambda_{c u t}\\right)~\\forall\\lambda\\in[\\lambda_{c u t},2]}\\end{array}$ . Let $\\sigma=\\left[\\,\\cdot\\,\\right]_{\\geq}$ be the ReLu function, and let $\\|\\pmb{W}^{(l)}\\|_{2}$ denote the spectral norm of $\\boldsymbol{W}^{(l)}$ . Then, ", "page_idx": 28}, {"type": "text", "text": "$(I)$ For fixed polynomial order $p\\geq2$ , an approximating sequence $\\left(S^{2}G N N_{m}^{(l)}\\right)_{m\\in\\mathbb{N}}o f(m\\!+\\!1).$ -layer $S^{2}$ GNNs exists such that, for arbitrary graph sequences $(\\mathcal{G}_{m})_{m\\in\\mathbb{N}}$ , ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{0\\neq X\\in\\mathbb{R}^{|\\mathcal{O}_{P}|\\times d}}\\frac{\\left\\|\\left[\\left(S^{2}G N N_{m}^{(l)}\\right)\\!\\varsigma_{m}-\\left(I G N N^{(l)}\\right)\\!\\varsigma_{m}\\right](X)\\right\\|_{F}}{\\|X\\|_{F}}=\\mathcal{O}\\left([\\|W^{(l)}\\|_{2}K_{r}(\\hat{g},\\lambda_{c u t})](p m)^{-r}\\right)\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "with a scaling constant that depends only on $r$ . Here, $(\\,\\cdot\\,)_{\\mathcal{G}_{m}}$ denotes the instantiation of all model filters on the eigenvalues of an input graph ${\\mathcal{G}}_{m}$ , which maps both models onto a ${\\mathcal{G}}_{m}$ -dependent function RD(l\u22121) $\\mathbb{R}^{D^{(l-1)^{\\tilde{}}}}\\rightarrow\\mathbb{R}^{D^{(l)}}$ ", "page_idx": 28}, {"type": "text", "text": "(2) For fixed $m\\geq1$ , an approximating sequence S2GNN(pl) p\u2208 of $(m+1)$ -layer $S^{2}$ GNNs with increasing layer-wise polynomial order $p$ exists such that, for all $(\\mathcal{G}_{p})_{p\\in\\mathbb{N}}$ , the same bound holds. ", "page_idx": 28}, {"type": "text", "text": "Remark. The proof of the simplified Theorem 4 used in the main body is analogous to the proof of Lemma 1 just without the nonlinearity, which has the following consequences: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The final layer $m\\!+\\!1$ which we only need to apply one last nonlinearity to the output (since the spectral part of all layers, including the previous layer $m$ , has none) becomes obsolete, so the final layer instead becomes $m$ ,   \n\u2022 The two limits (1) and (2) are equivalent by the reduction to an $m p$ -order polynomial filter,   \n\u2022 We do not need the dimension-doubling \u201ctrick\u201d outlined below to get rid of the nonlinearity in the proof and instead set all feature transform matrices in layers 1 through $m-1$ to the   \nidentity and the final ones to $W_{\\mathrm{spec}}^{*(m)}=W^{(l)}$ , $\\pmb{W}_{\\mathrm{spat}}^{*(m)}=\\pmb{W}^{(l)}$ . ", "page_idx": 28}, {"type": "text", "text": "Proof of Lemma 1. We first note that m $\\phantom{-}\\!\\!\\!\\!\\Delta\\!\\!\\!\\!\\!\\Delta\\!\\!\\!\\!\\!\\!\\Delta\\!\\!\\!\\!\\!\\!\\Delta\\!\\!\\!\\!\\!\\!\\!\\Delta\\!\\!\\!\\!\\!\\!\\!\\Delta\\!\\!\\!\\!\\!\\!\\!\\Delta\\!\\!\\!\\!\\!\\!\\!\\Delta\\!\\!\\!\\!\\!\\!\\!\\!\\Delta\\!\\!\\!\\!\\!\\!\\!\\!\\Delta\\!\\!\\!\\!\\!\\!\\!\\!\\Delta\\!\\!\\!\\!\\!\\!\\!\\!\\Delta\\!\\!\\!\\!\\!\\!\\!\\!\\!\\Delta\\!\\!\\!\\!\\!\\!\\!\\!\\!\\Delta\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\Delta\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\Delta\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\$ spatial parts, each of order $p$ , would act like an $(m p)$ -order polynomial filter (factorized into $m$ order- $\\boldsymbol{p}$ polynomials), were it not for the nonlinearities in between. However, using the fact that $\\sigma$ is the ReLu function, we can choose intermediate hidden dimensions twice the size of the input dimension and then use the linear transforms to store a positive and a negative copy of the embeddings, add them back together after applying each ReLu, just to split the result back into a positive and negative copy for the next layer. This essentially gets us rid of $\\sigma$ . Throughout the proof, we use a star superscript to denote the specific parameters that will ultimately satisfy our bound, whereas we put no star above parameters that are yet to be fixed in a later part of the proof. ", "page_idx": 28}, {"type": "text", "text": "For $m\\geq2$ , the trick discussed above works if we set ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{W_{\\mathrm{spec}}^{*(1)}=\\displaystyle\\frac{1}{2}\\left(I\\quad-I\\right)\\in\\mathbb{R}^{D^{(l-1)}\\times2D^{(l-1)}},}\\\\ &{W_{\\mathrm{spec}}^{*(2)},\\ldots,W_{\\mathrm{spec}}^{*(m-1)}=\\displaystyle\\frac{1}{2}\\left(I\\right)(I\\quad-I)\\in\\mathbb{R}^{2D^{(l-1)}\\times2D^{(l-1)}},}\\\\ &{W_{\\mathrm{spec}}^{*(m+1)}=W^{(l)}\\left(\\frac{I}{I}\\right)\\in\\mathbb{R}^{2D^{(l-1)}\\times D^{(l)}},}\\\\ &{W_{\\mathrm{spec}}^{*(1)}=(I\\quad-I)\\in\\mathbb{R}^{2D^{(l-1)}\\times D^{(l-1)}},}\\\\ &{W_{\\mathrm{spe}}^{*(2)},\\ldots,W_{\\mathrm{spe}}^{*(m-1)}=\\left(\\frac{I}{-I}\\right)(I\\quad-I)\\in\\mathbb{R}^{2D^{(l-1)}\\times2D^{(l-1)}},}\\\\ &{W_{\\mathrm{spea}}^{*(m+1)}=W^{(l)}\\left(\\frac{I}{-I}\\right)\\in\\mathbb{R}^{2D^{(l-1)}\\times D^{(l)}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": ",e $m\\,=\\,1$ fpriockm  tahbeo vmea ftroirc tehse $W_{\\mathrm{spec}}^{*(1)}$ d, $W_{\\mathrm{spat}}^{*(1)}$ from above for the first, and the matrices W s\u2217p(ecm+1) $W_{\\mathrm{spat}}^{*(m+1)}$ ", "page_idx": 29}, {"type": "text", "text": "Se t g\u02c6\u03b3\u2217(m+1)(\u03bb) = 1  g\u02c6\u2217\u03d1( and $\\hat{g}_{\\vartheta}^{*(m+1)}(\\lambda)=0$ . Given these choices and a graph $\\mathcal{G}$ with eigenvalues $\\lambda$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n(\\mathbf{S}^{2}\\mathbf{G}\\mathbf{N}\\mathbf{N}^{(l)})\\boldsymbol{\\varrho}(\\mathbf{\\cal{X}})=\\boldsymbol{\\sigma}\\Big(V\\left(\\hat{g}_{\\mathrm{spsp}}(\\mathbf{\\lambda})\\odot\\big[V^{\\top}H^{(l-1)}W^{(l)}\\big]\\right)\\Big),\\quad\\hat{g}_{\\mathrm{spsp}}=\\prod_{j=1}^{m}\\left(\\hat{g}_{\\vartheta}^{(j)}+\\hat{g}_{\\gamma}^{(j)}\\right)\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "We see that $\\begin{array}{r}{\\hat{g}_{\\mathrm{spsp}}|_{[\\lambda_{\\mathrm{max}},2]}=\\prod_{j=1}^{m}\\hat{g}_{\\gamma}^{(j)}}\\end{array}$ since $\\hat{g}_{\\vartheta}^{(j)}\\vert_{[\\lambda_{\\mathrm{max}},2]}=0$ for $1\\leq j\\leq m$ . This can express any polynomial up to order $m p$ on $[\\lambda_{\\mathrm{max}},2]$ , since we assumed a layer-wise $p\\geq2$ and any polynomial with real coefficients factorizes into real-coefficient polynomials of degree less or equal to 2 by the fundamental theorem of algebra. On the interval [0, \u03bbmax], on the other hand, the filter g\u02c6spsp  [0,\u03bbmax] can express any IGNN filter $\\hat{g}^{(l)}\\big|_{[0,\\lambda_{\\mathrm{max}}]}$ . For $m\\,=\\,1$ , this is immediately clear. Else, set $\\hat{g}_{\\vartheta}^{(j)}$ to constants $C_{j}\\ \\in\\ \\mathbb{R}_{\\geq}$ , $1\\,\\leq\\,j\\,\\leq\\,m\\,-\\,1$ large enough that none of the polynomials $\\left(C_{j}+\\hat{g}_{\\gamma}^{(j)}\\right)$ , $1\\leq j\\leq m-1$ , has a zero in $[0,\\lambda_{\\mathrm{max}}]$ . Defining $\\begin{array}{r}{\\hat{g}_{\\vartheta}^{(m)}=\\frac{\\hat{g}^{(l)}\\big\\vert_{[0,\\lambda_{\\operatorname*{max}}]}}{\\prod_{j=1}^{m}\\left(C_{j}+\\hat{g}_{\\gamma}^{(j)}\\right)}-\\hat{g}_{\\gamma}^{(m)}\\big\\vert_{[0,\\lambda_{\\operatorname*{max}}]}}\\end{array}$ [0,\u03bbmax] gives the desired function. ", "page_idx": 29}, {"type": "text", "text": "We proceed by making use of a result by D. Jackson (Natanson, 1964), which is essentially a converse to Theorem 9 which we used to prove Theorem 3: ", "page_idx": 29}, {"type": "text", "text": "Theorem (Jackson\u2019s theorem on an interval). Let $a\\,<\\,b\\,\\in\\,\\mathbb{R},\\ k,r\\,\\in\\,\\mathbb{N}$ with $k\\;\\geq\\;r\\,-\\,1\\;\\geq\\;0,$ , $f\\in C^{r}[a,b]$ . Then, a polynomial $p_{k}$ of degree less or equal to $k$ exists such that ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|p_{k}-f\\|_{\\infty}\\leq{\\frac{b-a}{2}}\\left({\\frac{\\pi}{2}}\\right)^{r}{\\frac{1}{(k+1)k\\ldots(k-r+2)}}\\left\\|{\\frac{d^{r}}{d x^{r}}}f\\right\\|_{\\infty}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Since $\\hat{g}_{\\mathrm{spsp}}$ can express any polynomial up to order $m p$ on $[\\lambda_{\\mathrm{max}},2]$ and, for any such polynomial, find parameters for the spectral parts that match the ideal filter $\\hat{g}^{(l)}\\big|_{[0,\\lambda_{\\mathrm{max}}]}$ exactly (not contributing to the supremum error), we can directly transfer this theorem to our case. Define ${\\mathrm{S}^{2}}{\\mathrm{GNN}}_{m}^{(l)}$ from the lemma by setting the linear feature transforms and final-layer filters as above. For the filters in layers 1 through $m$ , define $\\gamma^{*(1)},\\ldots,\\gamma^{*(m)}$ such that $\\textstyle\\prod_{j=1}^{m}{\\hat{g}}_{\\gamma}^{*(j)}$ factorizes into into the polynomial from Jackson\u2019s theorem on $[\\lambda_{\\mathrm{max}},2]$ , and $\\vartheta^{*(1)},\\ldots,\\vartheta^{*(\\bar{m})}$ to match $\\hat{g}^{(l)}$ on $[0,\\lambda_{\\mathrm{max}}]$ . This defines a filter g\u02c6spsp . We then find, for $m p\\geq r-1\\geq0$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\|\\hat{g}_{\\mathrm{sps}}^{(l)}-\\hat{g}^{(l)}\\|_{\\infty}\\leq\\frac{2-\\lambda_{\\mathrm{max}}}{2}\\left(\\frac{\\pi}{2}\\right)^{r}\\frac{1}{\\left(m p+1\\right)m p\\,\\ldots\\,\\left(m p-r+2\\right)}\\left\\|\\frac{d^{r}}{d\\lambda^{r}}\\hat{g}^{(l)}\\big|_{[0,\\lambda_{\\mathrm{max}}]}\\right\\|_{\\infty}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "Therefore, $\\lVert\\hat{g}_{\\mathrm{spsp}}^{(l)}-\\hat{g}^{(l)}\\rVert_{\\infty}$ is of $\\mathcal{O}\\left(K_{r}(\\hat{g},\\lambda_{\\mathrm{cut}})(m p)^{-r}\\right)$ and we can find a scaling constant that depends only on $r$ . Since the Lipschitz constant of $\\sigma$ is $1$ , we find for any graph $\\mathcal{G}$ with eigenvalues $\\lambda$ and any graph signal $0\\neq X\\in\\mathbb{R}^{|\\mathcal{G}|}$ , ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\left\\|\\left[\\left(\\mathbf{S}^{2}\\mathbf{G}\\mathbf{N}\\mathbf{N}_{m}^{(l)}\\right)\\!g-\\left(\\mathbf{I}\\mathbf{G}\\mathbf{N}\\mathbf{N}^{(l)}\\right)\\!g\\right](\\pmb{X})\\right\\|_{F}}\\\\ &{\\quad\\times\\frac{\\left\\|\\hat{\\mathbf{J}}\\right\\|_{F}}{\\left\\|\\pmb{X}\\right\\|_{F}}\\leq\\frac{\\left\\|\\pmb{V}\\left(\\hat{g}_{\\mathrm{spsp}}^{(l)}-\\hat{g}^{(l)}\\right)\\left(\\pmb{\\lambda}\\right)\\odot\\left[\\pmb{V}^{\\top}\\pmb{X}\\pmb{W}^{(l)}\\right]\\right\\|_{F}}{\\left\\|\\pmb{X}\\right\\|_{F}}}\\\\ &{\\leq\\frac{\\left\\|\\hat{g}_{\\mathrm{spsp}}^{(l)}-\\hat{g}^{(l)}\\right\\|_{\\infty}\\left\\|\\left(\\pmb{V}V^{\\top}\\right)\\pmb{X}\\pmb{W}^{(l)}\\right\\|_{F}}{\\left\\|\\pmb{X}\\right\\|_{F}}\\leq\\|\\hat{g}_{\\mathrm{spsp}}^{(l)}-\\hat{g}^{(l)}\\|_{\\infty}\\|\\pmb{W}^{(l)}\\|_{2}}\\\\ &{=\\mathcal{O}\\left([\\|\\pmb{W}^{(l)}\\|_{2}K_{r}(\\hat{g},\\lambda_{\\mathrm{cut}})](m p)^{-r}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "with a scaling constant that depends only on $r$ . Exactly the same procedure and bounds hold if we instead keep $m$ fixed and increase $p$ . This finishes the proof of Lemma 1. \u53e3 ", "page_idx": 29}, {"type": "text", "text": "We can now prove the main theorem by induction. Lemma 1 gives the initial step. Now, assume the theorem holds for L IGNN layers. We can then choose  S2GNNm m\u2208N $\\left(\\mathrm{S}^{2}\\mathrm{GNN}_{m}^{(L+1)}\\circ\\mathrm{S}^{2}\\mathrm{GNN}_{m}^{(L\\circ\\cdots\\circ1)}\\right)_{m\\in\\mathbb{N}}$ , where S2GNN(mL+1)a re the approximating models fulfilling Lemma 1, while S2GNN(mL\u25e6\u00b7\u00b7\u00b7\u25e61) fulfill the induction assumption. We assume fixed $p$ and increasing $m$ , but the proof is fully analogous in the other case. Applying the same decomposition to $\\left(\\operatorname{IGNN}_{m}\\right)_{m\\in\\mathbb{N}}$ lets us express the error on a graph sequence $(\\mathcal{G}_{m})_{m\\in\\mathbb{N}}$ as ", "page_idx": 29}, {"type": "text", "text": "", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\quad\\frac{\\left\\|\\left[\\left(S^{2}\\mathrm{GNN}_{m}\\right)\\varsigma_{m}-\\left(1\\mathrm{GNN}_{m}^{\\left(L+\\ldots\\right)}\\right)\\varsigma_{m}\\right]\\left(X\\right)\\right\\|_{F}}{\\left\\|X\\right\\|_{F}}}\\\\ &{=\\frac{\\left\\|\\left[\\left(S^{2}\\mathrm{GNN}_{m}^{\\left(L+\\ldots\\right)}\\right)\\circ S^{2}\\mathrm{GNN}_{m}^{\\left(L\\ldots\\right)\\ldots\\right)}\\right\\rangle_{\\mathcal{O}_{m}}-\\left(\\mathrm{IGNN}_{m}^{\\left(L+1\\right)}\\circ\\mathrm{IGNN}_{m}^{\\left(L\\ldots\\cdot\\right)\\top}\\right)\\varsigma_{m}\\right]\\left(X\\right)\\right\\|_{F}}{\\left\\|X\\right\\|_{F}}}\\\\ &{\\leq(\\|X\\|_{F})^{-1}\\left\\|\\left[\\left(S^{2}\\mathrm{GNN}_{m}^{\\left(L+1\\right)}\\circ S^{2}\\mathrm{GNN}_{m}^{\\left(L,\\ldots\\right)\\ldots\\right)}\\right)\\varsigma_{m}-(S^{2}\\mathrm{GNN}_{m}^{\\left(L+1\\right)}\\circ\\mathrm{IGNN}_{m}^{\\left(L\\ldots\\right)\\ldots\\right)}\\right\\rangle_{\\mathcal{O}_{m}}\\right]\\left(X\\right)\\right\\|}\\\\ &{+(\\|X\\|_{F})^{-1}\\left\\|\\left[\\left({S^{2}\\mathrm{GNN}_{m}^{\\left(L+1\\right)}\\circ\\mathrm{IGNN}_{m}^{\\left(L,\\ldots\\right)\\ldots\\right)}\\right)\\varsigma_{m}-(\\mathrm{IGNN}_{m}^{\\left(L+1\\right)}\\circ\\mathrm{IGNN}_{m}^{\\left(L\\ldots\\right)\\ldots\\right)}\\right\\rangle_{\\mathcal{O}_{m}}\\right](X)\\right\\|_{F}}\\\\ &{\\leq\\left[\\left\\|\\dot{y}\\right\\|_{\\infty}^{\\operatorname*{max}}\\left\\|W\\right\\|_{2}^{\\operatorname*{max}}+\\mathcal{O}(K_{m}^{\\operatorname*{max}}\\left(\\lambda_{\\infty}\\right)(\\rho m)^{-r})\\right]\\mathcal{O}\\Big(C_{L}(\\|\\dot{y}\\|_{\\infty}^{\\operatorname*{max}},\\|W\\|_{2}^{\\operatorname*{max}})\\;K_{r}^{\\operatorname*{max}}\\left(\\lambda_{\\infty}\\right)\\;(\\rho m)^{-r}\\Big)}\\\\ &{+\\mathcal{O}(K_{m}^{\\operatorname*{min}}(\\lambda_{\\infty})(\\rho m)^{-r})(\\|\\dot{y} \n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "We first used the triangle inequality in line 3 to split the difference into two terms. Next, we bound the first term using the induction assumption, as well as the Lipschitz constant of S2GNN(mL+1), which in turn, by Lemma 1, is the Lipschitz constant of ${\\mathrm{IGNN}}_{m}^{(L+1)}$ up to a term of $\\mathcal{O}(K_{r}^{\\mathrm{max}}\\left(\\lambda_{\\mathrm{cut}}\\right)(p m)^{-r})$ . We moreover bound the second term using the Lipschitz constant of $\\mathrm{IGNN}_{m}^{(L\\circ\\cdots\\circ1)}$ , as well as Lemma 1 to arrive at the final result. \u53e3 ", "page_idx": 30}, {"type": "text", "text": "H.5 Proof of Theorem 5 ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We next prove the stability of our positional encodings: ", "page_idx": 30}, {"type": "text", "text": "Theorem 5. The Positional Encodings PE in Eq. 5 are stable according to Definition 1. ", "page_idx": 30}, {"type": "text", "text": "Recall the definition of stability via H\u00f6lder continuity: ", "page_idx": 30}, {"type": "text", "text": "Definition 1 (Stable PE). (Huang et al., 2024) A PE method $\\mathrm{~\\boldmath~\\beta~}^{\\sf D}{\\sf E}\\,:\\,\\mathbb{R}^{n\\times k}\\,\\times\\,\\mathbb{R}^{k}\\,\\mathrm{~\\boldmath~\\to~}\\,\\mathbb{R}^{n\\times k}$ is called stable, if there exist constants $c,C~>~0,$ , such that for any Laplacian $L,L^{\\prime}$ , and $P_{*}\\ =$ $\\operatorname{trg\\,min}_{P}\\|L\\stackrel{*}{-}P L^{\\prime}P^{\\top}\\|_{\\mathrm{F}}$ ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\mathrm{PE}(\\mathrm{EVD}(L))-P_{*}\\,\\mathrm{PE}\\left(\\mathrm{EVD}\\left(L^{\\prime}\\right)\\right)\\right\\|_{\\mathrm{F}}\\leq C\\cdot\\left\\|L-P_{*}L^{\\prime}P_{*}^{\\top}\\right\\|_{\\mathrm{F}}^{c}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "For this proof, we build on the work of Huang et al. (2024) where the authors show that under the assumptions of Definition 2, and some minor adjustments, a positional encoding of the following form Eq. 23 is stable (Theorem 10). ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\operatorname{SPE}(V,\\lambda)=\\rho\\left(V\\operatorname{diag}\\left(\\phi_{1}(\\lambda)\\right)V^{\\top},V\\operatorname{diag}\\left(\\phi_{2}(\\lambda)\\right)V^{\\top},\\ldots,V\\operatorname{diag}\\left(\\phi_{k}(\\lambda)\\right)V^{\\top}\\right)\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Definition 2. The key assumptions for SPE are as follows: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{R}^{n\\times n\\times k},\\|\\rho(B_{1},B_{2},\\ldots,B_{k})-\\rho(B_{1}^{\\prime},B_{2}^{\\prime},\\ldots,B_{k}^{\\prime})\\|_{\\mathrm{F}}\\leq J\\sum_{l=1}^{k}\\|B_{\\ell}-B_{\\ell}^{\\prime}\\|_{\\mathrm{F}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Theorem 10 (Stability of Eq. 23 by Huang et al. (2024)). Under Definition 2, SPE (Eq. 23) is stable with respect to the input Laplacian: for Laplacians $L,L^{\\prime}$ , ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\left\\|\\mathrm{SPE}(\\mathrm{EVD}(L))-P_{*}\\,\\mathrm{SPE}\\left(\\mathrm{EVD}\\left(L^{\\prime}\\right)\\right)\\right\\|_{\\mathrm{F}}\\leq\\left(\\alpha_{1}+\\alpha_{2}\\right)k^{5/4}\\sqrt{\\left\\|L-P_{*}L P_{*}^{\\top}\\right\\|_{\\mathrm{F}}}}\\\\ {+\\left(\\alpha_{2}\\frac{k}{\\gamma}+\\alpha_{3}\\right)\\left\\|L-P_{*}L P_{*}^{\\top}\\right\\|_{\\mathrm{F}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where the constants are $\\begin{array}{r}{\\alpha_{1}=2J\\sum_{l=1}^{k}K_{\\ell},\\alpha_{2}=4\\sqrt{2}J\\sum_{l=1}^{k}M_{\\ell}}\\end{array}$ , and $\\begin{array}{r}{\\alpha_{3}\\,=\\,J\\sum_{l=1}^{k}K_{\\ell}}\\end{array}$ . Here $\\begin{array}{r}{M_{\\ell}=\\operatorname*{sup}_{\\lambda\\in[0,2]^{k}}\\|\\phi_{\\ell}(\\pmb{\\lambda})\\|}\\end{array}$ and again $\\begin{array}{r}{P_{*}=\\arg\\operatorname*{min}_{P\\in\\Pi(n)}\\left\\|L-P_{*}L P_{*}^{\\top}\\right\\|_{\\mathrm{F}}}\\end{array}$ . The eigengap $\\gamma=$ $\\lambda_{k+1}-\\lambda_{k}$ is the difference between the $(k+1)$ -th and $k$ -th smallest eigenvalues, and $\\gamma=+\\infty$ if $k=n$ . ", "page_idx": 31}, {"type": "text", "text": "We prove a similar bound for general weighted adjacency matrices $A\\,\\in\\,\\mathbb{R}_{\\geq0}^{n\\times n}$ (note that such a stability result would be trivial if we restrict $A\\,\\in\\,\\{0,1\\}^{n\\times n}$ , since any function on a finite set is Lipschitz continuous). To achieve this, we need a technical assumption in order to ensure that the function values do not blow up and degree normalization is indeed a Lipschitz continuous function: We assume that the domain of $\\pmb{A}$ is restricted to (symmetric) matrices whose degrees are uniformly bounded by some constants $0<\\tilde{D}_{\\mathrm{min}}<\\tilde{D}_{\\mathrm{max}}$ : ", "page_idx": 31}, {"type": "equation", "text": "$$\nd_{u}\\;:=\\;\\sum_{v}A_{u,v}\\;\\in\\;[\\tilde{D}_{\\operatorname*{min}},\\tilde{D}_{\\operatorname*{max}}]\\;\\;\\;\\;\\;\\;\\;\\forall u\\in\\{1,\\ldots,n\\}.\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "To decompose the proof into smaller pieces we commonly use the well-known fact that the composition of Lipschitz continuous functions $f_{1}\\circ f_{2}$ , with constants $C_{1}$ and $C_{2}$ , is also Lipschitz continuous $\\|f_{1}(f_{2}(y))-f_{1}(f_{2}(x))\\|\\leq C_{1}C_{2}\\,\\|y-x\\|$ with constant $C_{1}C_{2}$ . ", "page_idx": 31}, {"type": "text", "text": "Proof. Our proposed encoding (Eq. 5) matches roughly Eq. 23. Specifically, $\\begin{array}{r l}{\\phi_{\\ell}(\\lambda)}&{{}=}\\end{array}$ sof $\\operatorname{tmax}((\\lambda_{j}\\,{\\bar{-}}\\,\\mathbf{\\lambda}){\\bar{\\odot}}(\\lambda_{j}\\,{-}\\,\\mathbf{\\lambda})\\big/\\sigma^{2}\\big)$ with $\\sigma\\in\\mathbb{R}_{>0}$ . However, $\\rho_{\\ell}(B_{1},B_{2},\\ldots,B_{k})$ does not directly match $\\|_{j=1}^{k}[B_{j}\\odot A]\\cdot\\vec{1}$ , since it is also a function of the adjacency $\\pmb{A}$ . Nevertheless, we show that $\\phi_{\\ell}$ is $\\ddot{K_{\\ell}}$ -Lipschitz continuous and $\\rho$ is $J$ -Lipschitz continuous, where we also bound the change of $\\pmb{A}$ . ", "page_idx": 31}, {"type": "text", "text": "We will start with $\\phi_{\\ell}(\\pmb{\\lambda})\\;=\\;\\mathrm{softmax}\\big((\\lambda_{j}\\!-\\!\\pmb{\\lambda})\\odot(\\lambda_{j}\\!-\\!\\pmb{\\lambda})\\big/\\sigma^{2}\\big)$ . The softmax is well-known to be of Lipschitz constant 1 w.r.t. the $L^{2}$ vector norm/Frobenius norm. $-x/\\sigma$ has a Lipschitz constant of $1/\\sigma$ . This leaves us with the quadratic term $\\psi_{u}(\\lambda)=(\\lambda_{u}-\\lambda)\\odot(\\lambda_{u}-\\lambda)$ where we bound the norm of the Jacobian ", "page_idx": 31}, {"type": "equation", "text": "$$\nJ_{\\psi_{u}}=\\left[\\begin{array}{c c c c c c}{-2(\\lambda_{u}-\\lambda_{1})}&{0}&{\\cdots}&{0}&{\\cdots}&{0}\\\\ {0}&{-2(\\lambda_{u}-\\lambda_{2})}&{\\cdots}&{0}&{\\cdots}&{0}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}&{\\vdots}&{\\vdots}\\\\ {0}&{0}&{\\cdots}&{0}&{\\cdots}&{0}\\\\ {\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}&{\\vdots}&{\\vdots}\\\\ {0}&{0}&{\\cdots}&{0}&{\\cdots}&{-2(\\lambda_{u}-\\lambda_{k})}\\end{array}\\right]\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "that is zero everywhere except for the diagonal entries, excluding its $u$ -th entry. Thus, $\\|J_{\\psi_{u}}\\|_{\\mathrm{F}}\\leq$ $\\begin{array}{r}{2k\\operatorname*{max}_{v\\in\\{1,2,\\dots,k\\}}(\\lambda_{v}-\\lambda_{u})\\leq2k(\\lambda_{k}-\\mathbf{\\bar{\\lambda}}_{1})\\leq4k}\\end{array}$ , as $0\\,=\\,\\lambda_{1}\\,\\leq\\,\\lambda_{k}\\,\\leq\\,2$ . We can therefore use $K_{\\ell}:=4k/\\sigma$ . ", "page_idx": 31}, {"type": "text", "text": "Now we continue with $\\tilde{\\rho}_{\\ell}(\\mathbf{A},B_{1},B_{2},\\ldots,B_{k})$ . For $f(A,B)=(B\\odot A)\\!\\cdot\\!{\\vec{1}}$ with a general weighted adjacency $A\\in\\mathbb{R}^{n\\times n}$ , we consider ", "page_idx": 31}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\|({\\pmb{B}}\\odot{\\pmb A})\\cdot\\vec{\\pmb{1}}-({\\pmb B}^{\\prime}\\odot{\\pmb A}^{\\prime})\\cdot\\vec{1}\\|_{\\mathrm{F}}}&{\\le\\|\\vec{1}\\|_{2}\\|{\\pmb B}\\odot{\\pmb A}-{\\pmb B}^{\\prime}\\odot{\\pmb A}^{\\prime}\\|_{\\mathrm{F}}}\\\\ &{\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\sqrt{n}\\|{\\pmb B}\\odot{\\pmb A}-{\\pmb B}^{\\prime}\\odot{\\pmb A}^{\\prime}\\|_{\\mathrm{F}}}\\\\ &{=\\sqrt{n}\\|{\\pmb B}\\odot{\\pmb A}-{\\pmb B}^{\\prime}\\odot{\\pmb A}+{\\pmb B}^{\\prime}\\odot{\\pmb A}-{\\pmb B}^{\\prime}\\odot{\\pmb A}^{\\prime}\\|_{\\mathrm{F}}}\\\\ &{\\le\\sqrt{n}\\|{\\pmb B}\\odot{\\pmb A}-{\\pmb B}^{\\prime}\\odot{\\pmb A}\\|_{\\mathrm{F}}+\\sqrt{n}\\|{\\pmb B}^{\\prime}\\odot{\\pmb A}-{\\pmb B}^{\\prime}\\odot{\\pmb A}^{\\prime}\\|_{\\mathrm{F}}}\\\\ &{\\overset{\\le}\\alpha\\sqrt{n}\\|({\\pmb B}-{\\pmb B}^{\\prime})\\odot{\\pmb A}\\|_{\\mathrm{F}}+\\sqrt{n}\\|{\\pmb B}^{\\prime}\\odot({\\pmb A}-{\\pmb A}^{\\prime})\\|_{\\mathrm{F}}}\\\\ &{\\overset{\\le}C\\sqrt{n}\\underbrace{\\operatorname*{max}}_{\\underset{\\textcircled{\\pmb D}}{\\leq}\\tilde{A}_{\\operatorname*{max}}}\\|{\\pmb B}-{\\pmb B}^{\\prime}\\|_{\\mathrm{F}}+\\underbrace{\\operatorname*{max}B_{u,v}^{\\prime}\\times\\tilde{n}}_{\\underset{\\textcircled{\\pmb\\ B}^{\\prime}}{\\leq}}\\underbrace{\\|{\\pmb A}-{\\pmb A}^{\\prime}\\|_{\\mathrm{F}}}_{({\\pmb F})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 31}, {"type": "text", "text": "(A) holds by Cauchy-Schwarz, (B) by triangle inequality, (C) by Cauchy-Schwarz, (D) follows from the domain of $\\pmb{A}$ , and (E) is true since the largest eigenvalue of $\\pmb{B}=\\dot{\\pmb{V}}\\phi_{\\ell}(\\pmb{\\lambda})\\pmb{V}^{\\top}$ is 1 because $\\phi_{\\ell}(\\mathbf{\\lambda})_{j}\\leq1,\\forall1\\leq j\\leq k$ . ", "page_idx": 31}, {"type": "text", "text": "To further bound (F), i.e. $\\|{\\cal A}\\;-\\;{\\cal A}^{\\prime}\\|_{\\mathrm{F}}$ , note that $\\begin{array}{r l r}{\\|{\\cal L}\\;-\\;{\\cal L}^{\\prime}\\|_{\\mathrm{F}}}&{{}\\!=}&{\\|{\\cal D}^{-1/2}{\\cal A}{\\cal D}^{-1/2}\\;-}\\end{array}$ $D^{\\prime-1/2}A^{\\prime}D^{\\prime-1/2}\\|_{\\mathrm{F}}$ . For $g(A)\\,:=\\,D^{1/2}A D^{1/2}$ , our initial assumption from Eq. 25 yields the existence of a Lipschitz constant $C_{\\tilde{D}_{\\mathrm{min}},\\tilde{D}_{\\mathrm{max}}}$ for $g$ , which can be verified by computing the partial derivatives of $g$ . Thus, we can bound ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\|A-A^{\\prime}\\|_{\\mathrm{F}}=\\|g(D^{-1/2}A D^{-1/2})-g(D^{\\prime-1/2}A^{\\prime}D^{\\prime-1/2})\\|_{\\mathrm{F}}}\\\\ &{\\qquad\\qquad\\leq C_{\\frac{\\tilde{D}_{\\mathrm{min}}}{\\tilde{D}_{\\mathrm{max}}},\\frac{\\tilde{D}_{\\mathrm{max}}}{\\tilde{D}_{\\mathrm{min}}}}\\|D^{-1/2}A D^{-1/2}-D^{\\prime-1/2}A^{\\prime}D^{\\prime-1/2}\\|_{\\mathrm{F}}}\\\\ &{\\qquad\\qquad=C_{\\frac{\\tilde{D}_{\\mathrm{min}}}{\\tilde{D}_{\\mathrm{max}}},\\frac{\\tilde{D}_{\\mathrm{max}}}{\\tilde{D}_{\\mathrm{min}}}}\\|L-L^{\\prime}\\|_{\\mathrm{F}}\\,=:\\,\\alpha_{4}\\|L-L^{\\prime}\\|_{\\mathrm{F}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "As concatenation of $k$ vectors $||_{j=1}^{k}x$ has a Lipschitz constant of 1, we have $J=\\sqrt{n}\\tilde{D}_{\\mathrm{max}}$ . Moreover, we have an additional term for the RHS of Eq. 24 with constant $\\alpha_{4}\\sqrt{n}k$ , coming from (F) and Eq. 28. ", "page_idx": 32}, {"type": "text", "text": "To finalize the proof, we restate the beginning of the proof of Huang et al. (2024) and incorporate the additional $\\pmb{A}$ -dependency of $\\tilde{\\rho}_{\\ell}(\\mathbf{A},B_{1},B_{2},\\ldots,B_{k})$ with $\\boldsymbol{B}_{j}\\,=\\,V\\,\\mathrm{diag}\\,\\bigl(\\phi_{j}(\\boldsymbol{\\lambda})\\bigr)\\,\\boldsymbol{V}^{\\intercal}$ for $1\\leq$ $j\\leq k$ . ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Vert\\mathrm{SPE}(\\mathrm{EVD}(L),L)-P_{*}\\mathrm{\\tiny~SPE}\\left(\\mathrm{EVD}\\left(L^{\\prime}\\right),L\\right)\\Vert_{\\mathrm{F}}}\\\\ &{=\\Vert\\tilde{\\rho}_{\\ell}(A,B_{1},B_{2},\\ldots,B_{k})-P_{*}\\tilde{\\rho}_{\\ell}(A^{\\prime},B_{1}^{\\prime},B_{2}^{\\prime},\\ldots,B_{k}^{\\prime})\\Vert_{\\mathrm{F}}}\\\\ &{=\\left\\Vert\\tilde{\\rho}_{\\ell}(A,B_{1},B_{2},\\ldots,B_{k})-\\tilde{\\rho}_{\\ell}(P_{*}A^{\\prime}P_{*}^{\\top},P_{*}B_{1}^{\\prime}P_{*}^{\\top},P_{*}B_{2}^{\\prime}P_{*}^{\\top},\\ldots,P_{*}B_{k}^{\\prime}P_{*}^{\\top})\\right\\Vert_{\\mathrm{F}}}\\\\ &{\\leq\\underbrace{\\left[J\\displaystyle\\sum_{l=1}^{k}\\left\\Vert B_{l}-P_{*}B_{l}^{\\prime}P_{*}^{\\top}\\right\\Vert_{\\mathrm{F}}\\right]}_{\\mathrm{subject~of~Huang~etal.}(2024)}+\\alpha_{4}\\sqrt{n k}\\left\\Vert L-P_{*}L_{l}^{\\prime}P_{*}^{\\top}\\right\\Vert_{\\mathrm{F}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "Including the extra term stemming from our $\\pmb{A}$ -dependent $\\tilde{\\rho}_{\\ell}(\\mathbf{A},B_{1},B_{2},\\ldots,B_{k})$ , the stability guarantee reads ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathrm{SPE}(\\mathrm{EVD}(L),L)-P_{*}\\mathrm{\\;SPE}\\left(\\mathrm{EVD}\\left(L^{\\prime}\\right),L\\right)\\Vert_{\\mathrm{F}}\\leq\\left(\\alpha_{1}+\\alpha_{2}\\right)k^{5/4}\\sqrt{\\|L-P_{*}L P_{*}^{\\top}\\|_{\\mathrm{F}}}}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad+\\left(\\alpha_{2}\\frac{k}{\\gamma}+\\alpha_{3}+\\alpha_{4}\\sqrt{n}k\\right)\\left\\|L-P_{*}L P_{*}^{\\top}\\right\\|_{\\mathrm{F}}}\\end{array}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "with the newly introduced $\\alpha_{4}$ arising as Lipschitz constant of (inverse) degree normalization. The proof is complete. \u53e3 ", "page_idx": 32}, {"type": "text", "text": "Windowing for \u201ceigengap\u201d independent bounds. Note that $C$ depends on the eigengap between $1/{\\lambda_{k+1}}\\!-\\!\\lambda_{k}$ at the frequency cutoff. One should be able to improve upon this bound with windowing (see Fig. 7)), effectively lowering the Lipschitz constant of $\\hat{h}_{j}(\\lambda)$ around $\\lambda_{k}$ . We leave a formal treatment of this insight to future work. ", "page_idx": 32}, {"type": "text", "text": "H.6 Proof of Theorem 6 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "We next prove the expressivity of a $\\mathrm{GNN}/\\mathrm{S}^{2}\\mathrm{GNN}$ in combination with our positional encodings: ", "page_idx": 32}, {"type": "text", "text": "Theorem 6. $S^{2}$ GNNs are strictly more expressive than 1-WL with the PE of Eq. 5. ", "page_idx": 32}, {"type": "text", "text": "For this, we assume that the positional encodings are the only node attributes, subsuming a constant feature or that there is a linear transformation on the raw features. We require that the choice of spatial MPGNN / spectral filter is at least as expressive as the 1-WL test, which is the case, e.g., for GIN. Moreover, we assume that the node-level embeddings are aggregated to the graph level using summation. ", "page_idx": 32}, {"type": "text", "text": "Proof. To show that $\\mathrm{GNN}(\\mathrm{PE}(V,\\lambda))$ is strictly more expressive as 1-WL. For all graphs that 1- WL can distinguish, the GNN may learn to ignore the PE. Thus, we only need to prove that the positional encodings/node features of $\\mathrm{PE}(V,\\bar{\\lambda})$ suffice to distinguish some graphs that 1-WL could not distinguish. For all graphs that 1-WL can distinguish we know, by assumption, that the GNN can distinguish the graphs. ", "page_idx": 32}, {"type": "image", "img_path": "Cb3kcwYBgw/tmp/0051aa9c04b634bee224a6d744eb66ac25b0ed178c912aa0352d453cf751a6fe.jpg", "img_caption": ["Figure 15: Our positional encodings PE Eq. 5 illustrated in the node colors and sizes. We plot all 5 (rows) 3-regular graphs with 8 nodes and all possible dimensions of the encoding (columns). We use $\\sigma=0.001$ . Color denotes the sign, and size encodes the absolute value. We hypothesize that the visual \u201csmoothness\u201d between graphs and dimensions is due to our PE\u2019s stability (Theorem 5). "], "img_footnote": [], "page_idx": 33}, {"type": "text", "text": "", "page_idx": 33}, {"type": "text", "text": "As Li et al. (2020) point out, 1-WL (and MPGNN that are as capable as 1-WL) cannot distinguish degree-regular graphs with the same number of nodes and degrees. A degree regular graph is a graph where each node has the same degree. This is closely related to Theorem 11. ", "page_idx": 33}, {"type": "text", "text": "We next show that our PE alone distinguishes certain degree-regular graphs. In this construction, we consider all 3-regular graphs with $n=8$ nodes for this (see Fig. 15). The encodings \u20d71 PE result in the following values with $\\sigma=0.001$ and rounded to max 2 decimal places: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\widehat{1}^{\\top}\\operatorname{PE}(\\operatorname{EVD}(L_{1}))=[3}&{1.73}&{1\\quad0.41}&{-1\\quad-1\\quad-1.73\\quad-2.41]}\\\\ &{\\widehat{1}^{\\top}\\operatorname{PE}(\\operatorname{EVD}(L_{2}))=[3}&{1.56\\quad0.62\\quad0.62\\quad0\\quad-1.62\\quad-1.62\\quad-2.41]}\\\\ &{\\widehat{1}^{\\top}\\operatorname{PE}(\\operatorname{EVD}(L_{3}))=[3}&{1.73}&{1\\quad0.41\\quad-1\\quad-1\\quad-1.73\\quad-2.41]}\\\\ &{\\widehat{1}^{\\top}\\operatorname{PE}(\\operatorname{EVD}(L_{4}))=[3}&{1\\quad1\\quad0.41\\quad0.41\\quad-1\\quad-2.41\\quad-2.41]}\\\\ &{\\widehat{1}^{\\top}\\operatorname{PE}(\\operatorname{EVD}(L_{5}))=[3}&{1\\quad1\\quad1\\quad1\\quad-1\\quad-1\\quad-1\\quad-3]}\\end{array}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "By constructing examples, this shows that our PE can distinguish 4 out of the 5 3-regular graphs with 8 nodes. Thus, our PE may distinguish at least some graphs that 1-WL cannot. This concludes the proof. ", "page_idx": 33}, {"type": "text", "text": "I Expressivity of Spectral Filters and Spectrally Designed Spatial Filters ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "While it is well-known that common spatial MPGNNs are at most as expressive as 1-WL and that spectrally designed GNNs can be more expressive than 1-WL (Theorem 2 of Balcilar et al. (2021a)), we show that spectral GNNs are not able to distinguish degree-regular graphs. This upper bound was not known/formalized prior to our work (Bo et al., 2023b). Fortunately, our PE largely mitigates the limitation. The improved expressivity of our positional encodings, along with their efficiency, stems from the element-wise product with $\\pmb{A}$ (see also Geerts (2021)). ", "page_idx": 33}, {"type": "text", "text": "", "page_idx": 34}, {"type": "text", "text": "Theorem 11. Spectral filters $V\\operatorname{diag}({\\hat{g}}(\\mathbf{\\lambda}))V^{\\top}{\\bar{1}}$ are strictly less expressive than 3-WL with Laplacian $L=D-A$ , $L=I-D^{-1}A$ , or $L=I-D^{-1/2}A D^{-1/2}$ . ", "page_idx": 34}, {"type": "text", "text": "Proof. The proof relies on properties of the eigenvectors for the different choices $L_{\\mathrm{u}}\\,=\\,D\\,-\\,A$ , $L_{\\mathrm{rw}}=I-D^{-1}A$ , or $L_{\\mathrm{s}}=I-D^{-1/2}A D^{-1/2}$ . For $L_{\\mathrm{u}}\\vec{1}=\\lambda_{\\perp}\\vec{1}=0$ and $L_{\\mathrm{rw}}\\vec{1}=\\lambda_{0}\\vec{1}=0$ the first eigenvector is constant. The first eigenvector of $L_{\\mathrm{s}}$ is $D^{1/2}\\vec{1}$ (ignoring normalization). Thus, for degree-regular graphs, the first eigenvector of $L_{\\mathrm{s}}$ is also constant. ", "page_idx": 34}, {"type": "text", "text": "By the orthogonality of eigenvectors, $\\pmb{v}_{u}\\perp\\pmb{v}_{v}$ if $u\\ne v$ , we know that all other eigenvectors are orthogonal to constant node features. Consequently, the \u201cFourier transformed\u201d node features are $V^{\\top}{\\vec{1}}=\\left[{\\sqrt{n}}\\quad0\\quad\\ldots\\quad0\\right]$ for all three choices $L_{\\mathrm{u}}$ , $L_{\\mathrm{rw}}$ , and $L_{\\mathrm{s}}$ . Since this is true for all degreeregular graphs, spectral GNNs cannot distinguish degree-regular graphs with the same number of nodes. ", "page_idx": 34}, {"type": "text", "text": "Since the 3-WL test can distinguish some degree-regular graphs, 3-WL is strictly more expressive than a spectral GNN. \u53e3 ", "page_idx": 34}, {"type": "text", "text": "Corollary 1. \u201cSpectrally designed\u201d MPGNNs that use a polynomial parametrization of filter $\\mathrm{diag}(\\hat{g}(\\lambda))$ are strictly less expressive than 3-WL with the same choices for $\\textbf{\\emph{L}}$ . ", "page_idx": 34}, {"type": "text", "text": "Proof. With a polynomial parametrization of the spectral filter $\\hat{g}(\\lambda)$ , we know $V(\\hat{g}(\\pmb{\\lambda})\\odot[\\pmb{V}^{\\top}\\pmb{x}])=$ $\\begin{array}{r}{V\\operatorname{diag}(\\hat{g}(\\boldsymbol{\\lambda}))V^{\\top}\\boldsymbol{x}=\\hat{g}(L)\\boldsymbol{x}=\\sum_{j=0}^{p}\\gamma_{j}L^{j}\\boldsymbol{x}}\\end{array}$ (see $\\S\\,2,$ ). Due to this equivalence between a spectral and spatial filter and the constan t node features $x=\\vec{1}$ , any polynomial filter $\\textstyle\\sum_{j=0}^{p}\\gamma_{j}L^{j}{\\vec{1}}$ cannot distinguish degree-regular graphs. This argument also holds if the polynomial filter is normalized by the maximum eigenvalue as done by ChebNet (Defferrard et al., 2017). \u53e3 ", "page_idx": 34}, {"type": "text", "text": "J Further Remarks on $\\mathbf{S}^{2}\\mathbf{GNNs}$ ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "We next provide insights, details, and remarks on the details and variants of $\\mathrm{S}^{2}\\mathrm{GNNs}$ , accompanying the main section $\\S\\ 3$ . The structure roughly follows the main body. ", "page_idx": 34}, {"type": "text", "text": "Next to the overview in Fig. 2 and the method description of the main part, we provide pseudocode in Algo. 1 for a Spatio-Spectral Graph Neural Network on a graph with node attributes, and in Algo. 2 for a spectral filter. ", "page_idx": 34}, {"type": "table", "img_path": "Cb3kcwYBgw/tmp/21fc7547bd6c18dd7ae1ff4fa3aaf36bec7a101f788c2ecaa68c1a6f22fa5bde.jpg", "table_caption": [], "table_footnote": [], "page_idx": 34}, {"type": "table", "img_path": "Cb3kcwYBgw/tmp/eeb46669de7080fbf9736cb36d2207e18b50bc00e7e40f911d69ca2f3fadd52e.jpg", "table_caption": [], "table_footnote": [], "page_idx": 34}, {"type": "image", "img_path": "Cb3kcwYBgw/tmp/710bf197009f369c993d81a573ec61c714b88ac765193b205875b7963342af16.jpg", "img_caption": ["Figure 16: Sketch of intra- and inter-cluster message passing capabilities $V(\\hat{g}_{\\vartheta}(\\pmb{\\lambda})\\odot[V^{\\top}\\pmb{X}]=$ $\\begin{array}{r}{[\\sum_{j=1}^{k}\\hat{g}_{\\vartheta}(\\lambda_{j})\\pmb{v}_{j}\\pmb{v}_{j}^{\\top}]\\pmb{X}}\\end{array}$ . The \u201cstar\u201d node reflects the global Fourier coefficient and colors/widths illustrate its signed and weighted message passing. We show the first four eigenvectors, order nodes left to right in $\\pmb{v}_{j}$ , and sum repeated eigenvalues. "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "J.1 Visualization of Spectral Filters ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "In Fig. 16, we provide further examples of hierarchies/eigenspaces spectral filters have access to, complementing Fig. 3 & 4. Here and in the main part, we use the main diagonal of $\\textstyle\\sum_{j}$ s.t. $\\lambda_{j}\\!=\\!\\lambda_{u}\\!}\\ \\pmb{v}_{j}\\pmb{v}_{j}^{\\top}$ for deciding on the edge weights of the graph structures, potentially summing over multiple eigenvectors with identical values $\\lambda_{j}=\\lambda_{u}$ . We take the product $\\textstyle\\prod_{j{\\mathrm{~s.t.~}}\\lambda_{j}=\\lambda_{u}}\\operatorname{sign}(v_{j})$ for visualizing the sign of the $n$ edges for the global aggregation. ", "page_idx": 35}, {"type": "text", "text": "For all graphs, the first eigenvector denotes the constant signal (for $L\\,=\\,D\\mathrm{~-~}A)$ . For (a-d), we observe that the second eigenspace roughly describes a half oscillation, i.e., the left vs. right part of the graph. The third eigenspace separates the middle parts. For (a), the fourth eigenspace models the interactions between the extremal nodes. For (b-d), the frequency increments again, effectively clustering the graph in four roughly equal pieces. For (e), the eigenspaces model the interplay between (automorphic) inner and outer structures, as well as the vertical and horizontal symmetry. ", "page_idx": 35}, {"type": "text", "text": "J.2 Composition of Filters ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Composing a residual connection with a graph filter $G\\ =\\ \\mathrm{diag}(\\hat{g}(\\pmb{\\lambda}))\\ \\in\\ \\mathbb{R}^{n\\times n}$ yields $\\textbf{\\textit{Y}}=$ $V G V^{\\top}\\bar{H}\\;+\\;{\\cal H}\\;\\;=\\;\\;V(G\\;+\\;I)V^{\\top}{\\cal H},$ chaining multiple filters (without nonlinearities) results in $V G_{2}V^{\\top}V G_{1}V^{\\top}H\\;=\\;V G_{2}G_{1}V^{\\top}H$ . Chaining and residual connections resolve to $V(G_{2}G_{1}+G_{2}+G_{1}+I)V^{\\top}H$ . Hence, an arbitrary sequence of graph filters (Eq. 2) can be more flexible due to the interactions between filters. Note that this composition is only true in the absence of nonlinearities. Nevertheless, the main intuition about how filters interact remains approximately the same also in the light of nonlinearities. ", "page_idx": 36}, {"type": "text", "text": "J.3 Exhaustive Reasons Why Low Frequencies Are Sensible ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "A sensible default is to focus on the low frequencies. We specifically identify the following six reasons: (1) Low frequencies model the smoothest global signals w.r.t. the high-level graph structure (see Fig. 3 & 4). (2) Gama et al. (2020) find that, under a relative perturbation model (perturbation budget proportional to connectivity), stability implies $C$ -integral-Lipschitzness ( $\\exists C\\,>$ $0\\colon\\left|\\lambda^{d\\hat{g}}/d\\lambda\\right|\\leq C)$ , i.e., the filter can vary arbitrarily around zero but must level out towards larger $\\lambda$ . Stability to graph perturbations is a strong domain-agnostic prior. (3) Many physical long-range interactions are power laws with a flattening frequency response. For example, we construct an explicit graph filter modeling the electric potential of charges in a 1D \u201cion crystal\u201d $(\\S\\,\\mathrm{G})$ and find that a lowpass window is optimal. (4) Sequence models like Hyena (Poli et al., 2023) apply global low-pass filters through their exponential windows. (5) Cai et al. (2023) prove that an MPGNN plus virtual node (see $\\S\\ E$ ) can emulate DeepSets (Zaheer et al., 2017) and, thus, approximate self-attention to any precision. Nonetheless, we find that a virtual node alone does not necessarily yield good generalization $\\S\\ 3.1.1$ & 4.1). (6) Nonlinearities \u201cspill\u201d features between frequency bands (Gama et al., 2020). This includes spillage from higher frequencies to the band of the spectral filter. Gama et al. (2020) argue that this spillage makes it possible to learn stable yet expressive graph filters and is also a feature of stable message passing models. ", "page_idx": 36}, {"type": "text", "text": "J.4 Scaling to Graphs of Different Magnitude ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "For scaling a single to graphs of different orders of magnitude, it can be beneficial to rescale the eigenvalues before learning the filter g\u02c6\u03d1(\u03bb). That is, we use g\u02c6(\u03d1l )(\u02dc\u03bb) with rescaled \u02dc\u03bb. ", "page_idx": 36}, {"type": "text", "text": "For example, the eigenvalues for a path/sequence are $\\lambda_{j}\\approx(1-\\cos(\\pi j/n))$ . Thus, the resolution is poor, especially for the eigenvalues close to zero since cos approaches slope 0. For this reason, we consider rescaling the eigenvalues with ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\tilde{\\lambda}_{j}=1/\\pi\\cos^{-1}(1-\\lambda_{j})\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "or ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\tilde{\\lambda}_{j}=n/\\pi\\cos^{-1}(1-\\lambda_{j})\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The latter is, e.g., convenient for identifying the second lowest eigenvalue regardless o\u221af $n$ . Due to the poor numerical properties of these relations, we evaluate $\\cos^{-1}(1-\\lambda_{j})=\\tan^{-1}(\\sqrt{2\\lambda_{j}\\!-\\!\\lambda_{j}^{2}}/1\\!-\\!\\lambda_{j})$ instead. ", "page_idx": 36}, {"type": "text", "text": "J.5 Spectral Normalization ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "While the GFT and its inverse preserve the norm of the input (e.g., $\\|\\pmb{\\hat{x}}\\|_{2}=\\|\\pmb{V}^{\\top}\\pmb{x}\\|_{2}=\\|\\pmb{x}\\|_{2})$ , this is not true if operating on a truncated frequency spectrum or if the filter ${\\hat{g}}_{\\vartheta}(\\lambda)$ suppresses certain frequencies. For example, in the example of a virtual node (for simplicity here with $L=D-A)$ , a signal $\\textbf{\\em x}$ that is zero at every node but one at a single node, then the signal will be equally scattered to every frequency. Then, suppressing all frequencies but $\\lambda=0$ , yields $\\|V\\|_{\\{0\\}}V^{\\top}\\bar{\\pmb x}\\|_{2}=1/\\sqrt{n}$ . ", "page_idx": 36}, {"type": "text", "text": "Motivated by this unfortunate scaling, we also consider normalization in the spectral domain. Specifically, we normalize $\\hat{H}=\\hat{g}_{\\vartheta}(\\pmb{\\lambda})\\odot\\left[V^{\\top}f_{\\theta}(H)\\right]\\in\\mathbb{R}^{k\\times d}$ s.t. $\\hat{H}_{j}\\leftarrow(1\\!-\\!a_{j})\\bar{\\hat{H}}_{j}\\!+\\!a_{j}\\hat{H}_{j}\\big/\\|\\hat{H}_{j}\\|_{2}$ with learnable $\\pmb{a}\\in[0,1]^{d}$ . This allows, e.g., broadcasting a signal from one node without impacting its scale. However, we empirically find that this normalization only helps only marginally in the over-smoothing experiment (Di Giovanni et al., 2023a) and otherwise can destabilize training. We also consider variants where the norm in the spectral domain is scaled with the norm of the signal in the spatial domain with more or less identical results. We hypothesize that such normalization is counter-productive for, e.g., a bandpass filter if the signal does not contain the corresponding frequencies. ", "page_idx": 36}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "J.6 Adjusting $\\mathbf{S}^{2}\\mathbf{GNNs}$ to Directed Graphs ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "For the spectral filter of Eq. 3, we use $\\begin{array}{c c c c c}{{f_{\\theta}^{(l)}(\\hat{\\pmb H}^{(l)})}}&{{=}}&{{\\pmb H^{(l)}\\;\\odot\\;[\\sigma(\\pmb H^{(l)}{\\pmb W}_{G,\\mathfrak{R}}^{(l)})\\;+\\;i\\;\\;.}}\\end{array}$ \u00b7 $\\sigma(H^{(l)}W_{G,\\mathbb{S}}^{(l)})\\big]$ and subsequently map the result of Spectral back the real domain, e.g., using $\\begin{array}{r}{\\pmb{w}_{\\mathfrak{R}}^{(l)}\\mathfrak{R}(\\mathrm{Spectral}^{(l)}(\\pmb{H}^{(l-1)}))+\\pmb{w}_{\\mathfrak{I}}^{(l)}\\mathfrak{I}(\\mathrm{Spectral}^{(l)}(\\pmb{H}^{(l-1)}))}\\end{array}$ , with learnable weights $w_{\\mathfrak{R}}^{(l)},w_{\\mathfrak{I}}^{(l)}\\in$ $\\mathbb{R}^{d}$ and real $\\Re(\\cdot)$ as well as imaginary component $\\Im(\\cdot)$ . For the positional encodings $\\operatorname{PE}(V,\\lambda)$ of $\\S\\ 3.2.4$ , we use $A_{s}$ in Eq. 5 and concatenate real as well as imaginary components. The neural network for the spectral domain $s_{\\zeta}$ of $\\S\\ 3.2.2$ generalizes without adjustment. Similar to Koke & Cremers (2024), one could also employ complex weights; however, we do not. ", "page_idx": 37}, {"type": "text", "text": "J.7 Computational Remarks ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "We use readily available eigensolvers (scipy) and, thus, use a fixed number of eigenvectors (typically $k\\ll n)$ ) instead of determining $k$ based on $\\lambda_{\\mathrm{cut}}$ . The partial eigendecomposition is of complexity ${\\mathcal{O}}(k m)$ for $m$ edges, while the spectral filter has complexity $O(k d n)$ . On a different remark, we batch multiple graphs using block diagonal matrices (Fig. 17). ", "page_idx": 37}, {"type": "text", "text": "Spectral graph-level readouts. The key insight is that fre  \nquencies are a global concept, and hence, the GFT can be used   \nfor global readouts in graph-level tasks. With $k\\ll n$ , such a   \nreadout is practically free in the presence of intermediate spec  \ntral layers and of ${\\mathcal{O}}(k n)$ otherwise. Thus, there is the oppor  \ntunity for a computationally convenient aggregation of global   \ninformation, including a sort of graph-level \u201cjumping knowl  \nedge\u201d $\\mathrm{Xu}$ et al., 2018). The only caveat is that the Fourier   \ncoefficients are not unique due to the ambiguity in the eigende- Figure 17: Block diagonal batchcomposition. To maintain permutation equivariance, we take ing for spatial and spectral filters.   \nthe absolute value and aggregate over dimension $k$ in Eq. 4 instead of the multiplication with $V$ .   \nWe observe that such intermediate readout can improve performance slightly, e.g., on TPUGraphs. ", "page_idx": 37}, {"type": "image", "img_path": "Cb3kcwYBgw/tmp/df5dab467155bfa375daef2fa8ba2f16c504122ac165eb89de8c19a3e0e201d4.jpg", "img_caption": [], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "", "page_idx": 37}, {"type": "text", "text": "However, we leave a systematic evaluation of its benefits for future work. ", "page_idx": 37}, {"type": "text", "text": "Linear bottle necks. To circumvent overfitting, we commonly replace the linear transformations $W X$ in $f_{\\theta}^{(l)}(\\hat{H}^{(l)})$ and $\\hat{g}_{\\vartheta}(\\lambda)$ with low-rank bottlenecks $W_{2}W_{1}X$ , s.t. $W\\in\\mathbb{R}^{d\\times d}$ , $W_{2}\\in\\mathbb{R}^{d\\times d^{\\prime}}$ , $W_{1}\\in\\mathbb{R}^{d^{\\prime}\\times d}$ , and $d^{\\prime}<d$ . ", "page_idx": 37}, {"type": "text", "text": "K Limitations ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "We expect that many common graph benchmarks do not have or only insignificant long-range interactions. We observe that MPGNNs are less likely to overfit, perhaps since locality is a good inductive bias in many circumstances (Bronstein et al., 2021). Moreover, we observe that the spectral filter $(\\S\\ 3.2.1)$ may converge slowly and get stuck in local optima. We find that a sufficient amount of randomly initialized filters mitigates this issue to a large extent. Further, one can introduce inductive biases via windowing functions (Fig. 7), like the exponential window used by Hyena (Poli et al., 2023). ", "page_idx": 37}, {"type": "text", "text": "Even if the true causal model generating the target consists of long-range interactions, it might be sufficient to model the training data solely using (potentially spurious) local interactions. This might be especially true if the training nodes are samples from a \u201csmall\u201d vicinity of the graph (e.g., OGB Products (Hu et al., 2020)). ", "page_idx": 37}, {"type": "text", "text": "Closely related to the previous point is the amount of available training data. We hypothesize that $\\mathsf{S}^{2}\\mathrm{GN}\\dot{\\mathsf{N}}\\mathrm{s}$ are more data-hungry than their purely spatial counterpart. That is, to reliably detect (nonspurious) long-range interactions in the training data, a sufficient amount of data is required. Similar findings have been made, e.g., in the image domain (Dosovitskiy et al., 2021). ", "page_idx": 37}, {"type": "text", "text": "Except for heterophilic graphs, direction plays a small role in graph machine learning even though many benchmark tasks actually consist of directed graphs (Rossi et al., 2023). Moreover, there is a lack of benchmarks involving directed graphs, which require long-range interactions. Note that most of the theoretical findings generalize to directed graphs under appropriate modeling decisions/assumptions. However, we do not make this discussion explicit since MPGNNs for directed graphs are still actively researched. ", "page_idx": 38}, {"type": "text", "text": "Since a lot of the previous points hover around the insufficiency of the available benchmarks, we propose two new tasks $\\S\\,4.1$ and derive further datasets, e.g., for associative recall. ", "page_idx": 38}, {"type": "text", "text": "While we demonstrate the practicality of $\\mathrm{S}^{2}\\mathrm{GNNs}$ in $\\S\\ 4.3$ on large-scale benchmarks, the partial eigendecomposition EVD starts to become costly on the largest graphs we use for evaluation. Even though we did not experiment with lowering the requested precision, etc., we expect that for scaling further, na\u00efve approaches might not be sufficient. One direction could be to utilize GPUs instead of CPUs or to adapt concepts, e.g., from spectral clustering (von Luxburg, 2007). ", "page_idx": 38}, {"type": "text", "text": "Even though there are many important reasons why we should utilize a spectral filter on the low end of the spectrum, there might be tasks for which this choice is suboptimal. One way to estimate the frequency band to which one should apply a spectral filter is via a polynomial regression and then determine where the derivative is maximal. Note that it is efficient to calculate the eigenvectors around an arbitrary location of the spectrum, e.g., with the \u201cshift-invert mode\u201d of scipy/ARPACK (Lehoucq et al., 1998). ", "page_idx": 38}, {"type": "text", "text": "Due to the many possible design decisions of spectrally parametrized filters, the neglect of spectral filters in prior work, and the lack of appropriate benchmarks, it was not possible to ablate all the details. We expect that future work will discuss the specific building blocks in greater detail. ", "page_idx": 38}, {"type": "text", "text": "L Broader Impact ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "We expect that $\\mathrm{S}^{2}\\mathrm{GNNs}$ will have similar societal implications as other model developments like Convolutional Neural Networks (CNNs) (LeCun et al., 1989), LSTMs (Hochreiter & Urgen Schmidhuber, 1997), transformers (Vaswani et al., 2017), or modern Graph Neural Networks (Gilmer et al., 2017). Since such models may be used as building blocks in architectures for predictive tasks, generative modeling, etc., they have a wide range of positive and negative implications. Nevertheless, we expect that $\\bar{\\mathbf{S}^{2}}\\mathbf{GNNs}$ will not have more negative implications than other machine learning model innovations. ", "page_idx": 38}, {"type": "text", "text": "M Experimental Results ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "This section provides further details on the experimental setup $(\\S\\ M.1)$ , the computational cost $(\\S\\textbf{M}.3)$ , and graph constructions with additional experimental results for the clustering tasks $(\\S\\,\\mathbf{M}.6)$ ; likewise we provide details for the distance regression $(\\S\\,\\mathbf{M}.7)$ , arXiv-year $(\\S\\,\\mathrm{M}.8)$ , and provide nodes on the graph construction in TPUGraphs $(\\S\\ M.10)$ . Note that the sections on clustering $(\\S\\ M.6)$ and distance regression $\\left(\\S\\,\\mathrm{M}.7\\right)$ also contain ablations and further insights. ", "page_idx": 38}, {"type": "text", "text": "M.1 Experimental Details ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Implementation. The code base is derived from Cao et al. (2023), which on the other hand derive the code of Ramp\u00e1\u0161ek et al. (2022). The implementation heavily relies on PyTorch geometric (Fey & Lenssen, 2019). ", "page_idx": 38}, {"type": "text", "text": "Datasets. We collect the main statistics, including licenses, for the datasets in Table 6. The provided code will download all datasets along with the experiment execution, except for TPUGraphs, where one should follow the official instructions. Due to the high variation in results, we merge all \u201clayout\u201d datasets and present the results on this joint dataset. We use the fixed public splits for all experiments and proceed accordingly for our datasets (see $\\S\\,\\mathrm{M}.6$ and $\\S\\,\\mathrm{M.7}$ ). ", "page_idx": 38}, {"type": "text", "text": "Hyperparameters. While we provide full parameters for all experiments and models in our code, we gather an overview of the used $\\mathrm{S}^{2}\\mathrm{GNNs}$ variants here. The parameters were determined through cascades of random search throughout the development of the method. We list the most important parameters in Table 7. ", "page_idx": 38}, {"type": "table", "img_path": "Cb3kcwYBgw/tmp/6c2d0be6d06cfa912e777b7a239941df1c04da761547ef43a5dd3d0305e0de16.jpg", "table_caption": ["Table 6: Dataset statistics and licenses. "], "table_footnote": [], "page_idx": 39}, {"type": "text", "text": "Usage of external results. The performance of baselines is commonly taken from leaderboards and the respective accompanying papers. This specifically includes the results in Table 1, Table 2, and Table 11. ", "page_idx": 39}, {"type": "text", "text": "Setup. For clustering $(\\S\\mathrm{~M.6)}$ , distance regression $(\\S\\mathrm{~M.7)}$ , and arXiv-year $\\left(\\S\\ M.8\\right)$ we report the detailed setup in the respective sections. For the other tasks, the relevant details are: ", "page_idx": 39}, {"type": "text", "text": "\u2022 Peptides: We follow the setup and implementation of Ramp\u00e1\u0161ek et al. (2022). That is, we train for 250 epochs with a batch size of 200. We rerun experiments on 10 random seeds. ", "page_idx": 39}, {"type": "text", "text": "\u2022 Over-squashing: We derive the setup from Di Giovanni et al. (2023a). In the main part (Fig. 5), for the GCN, we report the numbers of their Figure 3 for a GCN on \u201cClique Path\u201d graphs. For the spectral filter, we actually consider the more challenging setting where we do not train one model per graph size. Instead, we train one model for all sequence lengths. The task is to retrieve the correct of five possible classes on the other end of the graph. In the extended experiment of Fig. 12, we compose the dataset of \u201cClique Path\u201d and \u201cRing\u201d graphs (see Di Giovanni et al. (2023a)). To avoid $m=\\mathcal{O}(n^{2})$ , we limit the fully connected clique to 15 nodes. For training and validation, we enumerate all graphs with even $n\\in\\{4,6,\\ldots,50\\}$ and train for 500 epochs. For test, we enumerate the graphs with even $n\\in\\{52,54,\\ldots,100\\}$ . We rerun experiments on 10 random seeds. ", "page_idx": 39}, {"type": "text", "text": "\u2022 Associative recall: We construct one dataset consisting of key-value sequences of length 20 to 999. As Poli et al. (2023), we use a vocabulary of 30. We sample 25,000/500 random graphs for train/validation. For the test set, we randomly generate 500 graphs for the sequence lengths of 1,000 to 1,199. We train for 200 epochs. In the experiment with validation/test sequence length $30\\mathbf{k}$ (Table 2), we generate 10,000 training graphs of length 29,500 to 30,499 and finetune $\\mathrm{S^{2}G N N s G C N}$ from the smaller setup. We rerun experiments on 10 random seeds. ", "page_idx": 39}, {"type": "table", "img_path": "Cb3kcwYBgw/tmp/a60b18765c82f44ae6623a5697d877347a55dde4f6416c818712c4a0ba7901ba.jpg", "table_caption": ["Table 7: Important $\\mathrm{S}^{2}\\mathrm{GNNs}$ specific hyperparameters and runtimes. The times for the EVD cover the respective dataset entirely. "], "table_footnote": [], "page_idx": 39}, {"type": "text", "text": "\u2022 OGB Products: Even though full-graph training with 3 layers GCN plus one spectral layer fits into a 40 GB A100 GPU, we find that batched training works better. We randomly divide the graph during training into 16 parts and train a 6-layer $\\mathrm{S}^{2}\\mathrm{GAT}$ with spectral layers after the second and last message passing step. Inference is performed on the entire graph at once. We rerun experiments on 5 random seeds. ", "page_idx": 40}, {"type": "text", "text": "\u2022 TPUGraphs: This is the only dataset where we use a transformer to model $\\hat{g}$ instead of the procedure detailed in $\\S\\ 3.2.1$ . We fix the number of eigenvectors to $k\\,=\\,100$ and do not apply any windowing. Due to the large variation of results, we merge all \u201clayout\u201d tasks into a single dataset. Since the default graph construction is not able to express all relevant information, we adapt it as detailed in $\\S\\_{\\mathrm{M.l0}}$ , however, the empirical impact was small. TPUGraphs \u201clayout\u201d consists of a few hundred distinct graph structures with a large variation on the node-level configuration/features. We sample 10,000 configurations for each graph structure of each \u201clayout\u201d sub-split. Here, we introduce two batch dimensions: (1) batching over multiple graphs and (2) batching over the configurations. In each training step of the 1,000 epochs, we sample a small subset of configurations per graph structure and apply a pairwise hinge loss to rank the configurations. We do not perform random reruns due to the computational cost. ", "page_idx": 40}, {"type": "text", "text": "M.2 Qualitative Experiments ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "In Fig. 6 and Fig. 8, we provide qualitative insights about the approximation of filters and ringing. ", "page_idx": 40}, {"type": "text", "text": "In Fig. 6, we construct a true filter by adding a discontinuous filter at $\\lambda=0$ and a polynomial filter of order 3. For the spectral part, we use the true filter values and fit a Chebyshev polynomial on the remaining part. We then plot the response of the true filter and its approximations on a path graph with 21 nodes and $L=\\dot{I}-D^{-1}A$ . ", "page_idx": 40}, {"type": "text", "text": "Similarly, in Fig. 8, we use a path graph with 100 nodes and $L=I-D^{-1}A$ . We then construct a perfect low pass ( $k=25$ ) and approximate a rectangular wave. ", "page_idx": 40}, {"type": "text", "text": "M.3 Computational Cost ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "We report the computational cost for the experiments in Table 7 for a single random seed. On top of the pure cost of reproducing our numbers, we conducted hyperparameter searches using random search. Partially, we required 100s of runs to determine good parameter ranges. A generally wellworking approach was first to reproduce the results of the best available MPGNN in prior work. Thereafter, we needed to assess how likely additional capacity would lead to overfitting. Usually, we reduced the number of message-passing steps, added the spectral filter, and determined appropriate values for the number of eigenvectors $k$ . In the light of overfitting, it is a good idea to lower the number of Gaussians in the smearing of the filter parametrization $\\left(\\S\\ 3.2.1\\right)$ , introduce bottle-neck layers $(\\S\\:\\mathrm{J})$ , and use fewer spectral filters than hidden dimensions. ", "page_idx": 40}, {"type": "text", "text": "Runtime with precalculated eigenvectors. In Fig. 18, we contrast the runtime cost of a spectral convolution with spatial messages passing on ogb-arXiv (170k nodes) of Hu et al. (2020), using an Nvidia GTX 1080Ti. This essentially compares a sparse matrix multiplication (adjacency matrix) ", "page_idx": 40}, {"type": "image", "img_path": "Cb3kcwYBgw/tmp/070d3bb60d5891f7b4f7b7898d56a293eef94bb08b931608765b4233819b75e7.jpg", "img_caption": ["Figure 18: Runtime comparison on arXiv (w/o EVD) using $k=2,500$ for the spectral filter. "], "img_footnote": [], "page_idx": 40}, {"type": "text", "text": "with matrix multiplications on dense \"tall and skinny\" matrices (GFT). we find that one GCN-layer here is as costly as a spectral filter with approx. $k=2$ , 500 eigenvectors. ", "page_idx": 41}, {"type": "text", "text": "Large-scale benchmarks. On the large-scale datasets OGB Products and TPUGraphs, we perform full-graph training (without, e.g., segment training (Cao et al., 2023)) using 3 DirGCN layers interlayered with spectral filters targeting a pair-wise hinge loss. The spectral GNN uses the Magnetic Laplacian to incorporate direction. The spatial MPGNN closely resembles the model of Rossi et al. (2023), except that we half the dimension for the forward and backward message passing and concatenate the result. We shrink the dimensions to model the direction at a very low cost. We conclude that $\\mathrm{S}^{2}\\mathrm{GNNs}$ can be very practical even if applied at scale and can effectively model long-range interactions also on large graphs. ", "page_idx": 41}, {"type": "image", "img_path": "Cb3kcwYBgw/tmp/66666f89bf11fd9d9d9d29c4d2cbef1b51cad2080880c22b1d8cb6ad579658ae.jpg", "img_caption": ["Figure 19: Runtime of partial eigendecomposition $k=25$ of Erdo\u02dds R\u00e9nyi graph with average degree 5. Dashed mark directed/Hermitian Laplacian. "], "img_footnote": [], "page_idx": 41}, {"type": "text", "text": "Eigendecompositon. We show the computational cost for the eigendecomposition of a random Erdo\u02dds R\u00e9nyi graph (every edge has equal likelihood to be drawn). We use scipy (CPU) and PyTorch (GPU) with default arguments. For the sparse decomposition with PyTorch, we use the svd_lowrank method. Note that the default parameters for PyTorch are usually leading to large numerical errors. Fig. 19 demonstrates that the cost of the eigendecomposition is manageable. For large graphs like ogbn-products (2.5 mio. nodes), the EVD takes around 30 minutes with $k=100$ on 6 CPU cores of an AMD EPYC 7542. Note that the default parameters of the eigensolver allow for 1000s of iterations or until the error in the 32-bit float representation achieves machine precision. ", "page_idx": 41}, {"type": "text", "text": "M.4 $\\mathbf{S}^{2}\\mathbf{G}\\mathbf{N}\\mathbf{N}$ Aggregation Ablation ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "In the main body we present two ways to combine a spatial and spectral filter: An additive combination (Eq. 1) and an arbitrary sequence of filters (Eq. 2). In this section, we perform an ablation analysis on the peptides-func benchmark and report the results in Table 8. ", "page_idx": 41}, {"type": "text", "text": "Instead of summation of the spatial and spectral parts, concatenation is another possible option. Getting input features H(l\u22121) \u2208 Rn\u00d7d(l\u22121), we design Spectral(l) and Spatial(l) to map to $\\mathbb{R}^{n\\times d^{(l)}/2}$ and update the embeddings as ", "page_idx": 41}, {"type": "equation", "text": "$$\n{\\pmb H}^{(l)}\\,=\\,\\mathrm{Spectral}^{(l)}({\\pmb H}^{(l-1)};V,\\lambda)\\,||\\mathrm{~Spatial}^{(l)}({\\pmb H}^{(l-1)};{\\pmb A}).\n$$", "text_format": "latex", "page_idx": 41}, {"type": "text", "text": "Additionally, w\u221ae consider normalization of the adde\u221ands at the end of each embedding update, dividing by $1/\\sqrt{2}$ (concatenation w/ residual) and $1/\\sqrt{3}$ (summation w/ residual) as an attempt to keep the variance constant. ", "page_idx": 41}, {"type": "text", "text": "Inspired by recent advancements in state space models like Mamba (Gu & Dao, 2023), we also consider modeling an update step in a similar way, identifying the convolutional part with Spatial(l) and the SSM part with Spectral(l). ", "page_idx": 41}, {"type": "text", "text": "The following table shows results for the different design choices to combine the spatial and spectral parts, with all hyperparameters being precisely the ones reported in Table 7 for peptides-func. ", "page_idx": 41}, {"type": "text", "text": "Table 8: Ablation of different aggregation functions on the peptides-func benchmark, with our PE. ", "page_idx": 42}, {"type": "table", "img_path": "Cb3kcwYBgw/tmp/657fba1e496f55e0d53a696cc57fd197b17149c162946251720531a589b11b14.jpg", "table_caption": [], "table_footnote": [], "page_idx": 42}, {"type": "text", "text": "M.5 Number of Eigenvectors Ablation on Peptides-Func ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "In Fig. 20, we ablate the number of eigenvectors on the real-world dataset peptides-func. Since we here limit the number of eigenvalues by cut-off frequency $\\lambda_{\\mathrm{cut}}$ , we report the average number of eigenvectors. ", "page_idx": 42}, {"type": "image", "img_path": "Cb3kcwYBgw/tmp/c102e795824202ee706d2d73a0c37bd5d3f6ae59fd3245a0306bcd41275954a8.jpg", "img_caption": ["Figure 20: Average precision vs. number of used eigenvalues on the peptides-func long-range benchmark task via frequency cutoff $\\lambda_{\\mathrm{cut}}$ . "], "img_footnote": [], "page_idx": 42}, {"type": "text", "text": "M.6 Clustering Tasks ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "We use a clustering task LR-CLUSTER based on Gaussian Mixture Models (GMMs), which requires long-range interactions to measure the ability of $\\mathsf{S}^{2}\\mathrm{GNN}$ to spread information within clusters and consider the original CLUSTER task from Dwivedi et al. (2023) based on Stochastic Block Models (SBMs) in order to measure the ability to discriminate between the clusters. The differences are apparent from an illustration of some exemplary graphs in Fig. 21 & 22. While LR-CLUSTER has long-range interactions, the challenge of CLUSTER is to discriminate between the clusters. Without the arrangement of nodes, colors, and different edge weights, for CLUSTER, it is virtually impossible to discriminate the clusters by visual inspection. ", "page_idx": 42}, {"type": "image", "img_path": "Cb3kcwYBgw/tmp/9e6c02dafa058c75690fa984205e2fa81823c2e3f43a3ab2779e92b53b62a388.jpg", "img_caption": ["Figure 21: Examples of generated graphs for the LR-CLUSTER task (GMM). Labeled nodes are marked red. "], "img_footnote": [], "page_idx": 42}, {"type": "image", "img_path": "Cb3kcwYBgw/tmp/74818763deeb82dad99b5cce1a07b1f49665ef58970e5fb033a40f5b139eab3c.jpg", "img_caption": ["Figure 22: Examples of generated graphs for the CLUSTER task (SBM). Labeled nodes are marked red. Edges within clusters are highlighted. "], "img_footnote": [], "page_idx": 43}, {"type": "text", "text": "Nevertheless, we find that the spectral filter is well aligned with the cluster structure in these tasks. We plot this some exemplary filter in Fig. 23. The findings match the explanations of $\\S\\,4.1$ also for CLUSTER. ", "page_idx": 43}, {"type": "image", "img_path": "Cb3kcwYBgw/tmp/e0756913cd2e81ffcc9549e65036b9dcc3e1b10064fb77b3e1f2c2e42d3c0837.jpg", "img_caption": ["Figure 23: SBM-based (a), visualized in Fig. 21a, and our GMM-based (b), visualized in Fig. 22a, graphs along with four learned filters. Large entries are yellow, small are blue, and white lines denote clusters. "], "img_footnote": [], "page_idx": 43}, {"type": "text", "text": "In the remainder of the section, we provide full details of the experiment setups. Moreover, we provide additional results not presented in the main text, including ablations. ", "page_idx": 43}, {"type": "text", "text": "M.6.1 GMM Clustering LR-CLUSTER ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Setup. To sample an input graph, we start by generating $C\\ =\\ 6$ $p$ -dimensional cluster centers $\\mu_{c}\\sim U[0,10]^{p}$ for $c\\in\\{0,\\ldots,C-1\\}$ (we use $p=2$ ). Next, we draw $n_{c}\\in\\{100,\\ldots,199\\}$ points $x_{i c}\\sim\\mathcal{N}(\\mu_{c},4I_{p})$ which will represent the nodes of the graph. Subsequently, we update the class memberships such that every point is in its most likely class according to the underlying probabilistic model. Finally, we connect each node $v$ to its $e_{v}\\sim\\dot{U}(\\{1,\\ldots,10\\})$ closest neighbors by Euclidean distance $\\Vert\\cdot\\Vert_{2}$ . This whole procedure is repeated until the generated graph is connected. We then discard the location information and only keep the graph structure. In this way, we generate graphs of an average diameter of $\\approx33$ . See Fig. 21 for depictions of example graphs. ", "page_idx": 43}, {"type": "text", "text": "Apart from the graph generation procedure, we adhere closely to Dwivedi et al. (2023): We introduce input features in $\\{0,\\bar{1},2,\\ldots,C\\}$ , where a feature value of $c=1,\\ldots,C$ corresponds to the node being in class $c-1$ and a feature value of 0 means that the class is unknown and has to be inferred by the model. Only one node $v_{c}$ per class is randomly chosen to be labeled and all remaining node features are set to 0. The output labels are defined as the class labels. We use weighted cross entropy loss for training and class-size-weighted accuracy as a target metric. We generate 10,000 training and 1,000 val/test graphs each and report the average $\\pm$ standard deviation over 3 random reruns. ", "page_idx": 43}, {"type": "table", "img_path": "Cb3kcwYBgw/tmp/414793754e07532149ae45cf3f94606087d0ba5699a0aff6cc238ef48388d64c.jpg", "table_caption": ["Table 9: Accuracy on the GMM clustering task for varying number of eigenvectors $k$ , using 4 GCN layers and one spectral layer in the end. "], "table_footnote": [], "page_idx": 44}, {"type": "table", "img_path": "Cb3kcwYBgw/tmp/38a64e6abca6006955dcd4b7f4ea207a2296e78e529d49a7d31040ffda14b643.jpg", "table_caption": ["Table 10: Accuracy on the GMM clustering task for varying number of MP layers, while comparing a purely spatial GCN model to $\\mathrm{S}^{2}\\mathrm{GCN}$ with one spectral layer added in the end. "], "table_footnote": [], "page_idx": 44}, {"type": "text", "text": "", "page_idx": 44}, {"type": "text", "text": "Models. As an underlying spatial model baseline, we use a vanilla GCN (Kipf & Welling, 2017). We compare this to $\\bar{\\mathbf{S}^{2}}\\mathbf{G}\\bar{\\mathbf{C}}\\mathbf{N}$ , only applying one spectral convolution immediately before the last spatial layer. We investigate the influence of the number $k\\,\\in\\,\\{0,1,\\ldots,10\\}$ of eigenvectors to be taken into account with 4 spatial layers, with $k=0$ indicating the absence of a spectral layer (see Fig. 10a, and Table 9 for the underlying data). We also vary the number of spatial MP layers from 2 to 10 and compare the performance of a purely spatial GCN to the corresponding $\\mathrm{S}^{2}\\mathrm{GCN}$ with one spectral convolution (see Fig. 10b, and Table 10 for the underlying data). ", "page_idx": 44}, {"type": "text", "text": "Throughout all evaluations, we maintain a consistent hyperparameter configuration: Specifically, we use an inner dimension of 128, GELU (Hendrycks & Gimpel, 2016) as an activation function, no dropout, and residual connections for all spatial and spectral layers. For the spectral layer, we implement the gating mechanism $f_{\\theta}^{(l)}$ , but abstain from a neural network in the spectral domain (\u00a7 3.2.2), bottlenecks, or parameter sharing. We train for 50 epochs with a batch size of 50, using the AdamW optimizer (Loshchilov & Hutter, 2019) with a base learning rate of 0.003, a weight decay of 0.0001, a cosine scheduler and 5 warmup epochs. ", "page_idx": 44}, {"type": "text", "text": "Further discussion. The clustering task comes naturally to $\\mathsf{S}^{2}\\mathrm{GCN}$ , as a spectral layer can simulate certain variations of spectral clustering (von Luxburg, 2007): Suppose $\\pmb{H}^{(\\hat{l}-1)}\\in\\mathbb{R}^{\\dot{n}\\times C}$ is a one-hot encoding of the cluster labels, i.e. $H_{v,c}^{(l-1)}\\,=\\,\\delta_{v,v_{c}}$ , with $c\\in\\{1,\\ldots,C\\}$ and $v_{c}$ being the unique labeled node per class. In its simplest form, taking $\\hat{g}_{\\vartheta}^{(l)}(\\lambda)\\equiv1$ and $f_{\\theta}^{(l)}\\equiv\\mathrm{id}$ , the spectral layer Spectral(l) from Eq. 3 turns into $\\pmb{H}^{(l)}=\\pmb{V}\\pmb{V}^{\\top}\\pmb{H}^{(l-1)}$ . Hence, $\\begin{array}{r}{H_{v,c}^{(l)}=V_{v,\\div}^{\\top}V_{v_{c}},}\\end{array}$ : encodes a notion of similarity between a node $v$ and each labeled node $v_{c}$ . This relates to the Euclidean distance $\\|V_{v,:}\\;-\\;V_{v_{c},:}\\|_{2}\\;=\\;\\sqrt{\\|V_{v,:}\\|_{2}^{2}+\\|V_{v_{c},:}\\|_{2}^{2}-2V_{v,:}^{\\top}V_{v_{c},}},$ : which is more typically used for spectral clustering. ", "page_idx": 44}, {"type": "text", "text": "M.6.2 SBM Clustering CLUSTER (Dwivedi et al., 2023) ", "text_level": 1, "page_idx": 45}, {"type": "text", "text": "Setup. We conduct an ablation study on the original CLUSTER task (Dwivedi et al., 2023), which uses a similar setup to our GMM clustering task, however drawing from a SBM instead: For each cluster, $n_{c}\\in\\{5,\\ldots,35\\}$ nodes are sampled. Nodes in the same community are connected with a probability of $p=0.55$ , while nodes in different communities are connected with a probability of $q=0.25$ . While there is no need for long-range interactions in this task, considering that the average diameter of the graphs is just $\\approx\\,2.17$ , separating the clusters is much harder than in the GMM clustering task (see Fig. 23 for example adjacency matrices from the SBM and GMM models). We use weighted cross entropy loss for training and class-size-weighted accuracy as a target metric. We report the average $\\pm$ standard deviation over 3 random reruns. ", "page_idx": 45}, {"type": "text", "text": "Models. In our ablation study, we consider GCN (Kipf & Welling, 2017), GAT (Velic\u02c7kovic\u00b4 et al., 2018), and GatedGCN (Bresson & Laurent, 2018) as MPGNN baselines, following a setup similar to Dwivedi et al. (2023). We consider models with 4 layers (roughly 100k parameters) and 16 layers (roughly 500k parameters), while keeping most hyperparameters the same as in the benchmark, including inner dimension, dropout, and the number of heads for GAT. However, our reported baseline results and parameter counts differ slightly as we are using a different post-MP head, where we maintain a constant dimension until the last layer, in contrast to Dwivedi et al. (2023) who progressively shrink the inner dimension. We construct the corresponding $\\mathrm{S}^{2}\\mathrm{GNNs}$ by modifying each baseline model, replacing the $3^{\\mathrm{rd}}$ and the $5^{\\mathrm{th}}/15^{\\mathrm{th}}$ layers with spectral layers, ensuring a roughly equivalent parameter count. Additionally, each model is optionally supplemented by our positional encodings PE (\u00a7 3.2.4). ", "page_idx": 45}, {"type": "text", "text": "We further conduct a hyperparameter search on the most promising base MPGNN candidate, GatedGCN, which leads to an optimized version of $\\mathsf{S}^{2}\\mathbf{GN}\\bar{\\mathbf{N}}$ . This optimized model has 18 spatial MPGNN layers, spectral layers between all spatial layers, and additional RWSE encodings. The inner dimension is adjusted to keep the model well below a parameter budget of $500\\mathrm{k}$ . Finally, we also evaluate $\\mathsf{S}^{2}\\mathrm{GCN}$ and $\\mathrm{S}^{2}\\mathrm{GAT}$ using these hyperparameter settings. ", "page_idx": 45}, {"type": "text", "text": "Throughout all evaluations, we use GELU (Hendrycks & Gimpel, 2016) as an activation function, residual connections for all spatial and spectral layers, and implement the gating mechanism $f_{\\theta}^{(l)}$ without employing a neural network in the spectral domain. We use a batch size of 128 for training the 4-layer models and 64 for all other models. For ", "page_idx": 45}, {"type": "image", "img_path": "Cb3kcwYBgw/tmp/f2cef0d27efcb92a6c522e9015532f174636e62369837356c91f79850c93657e.jpg", "img_caption": ["Figure 24: Effects of the spectral part on SBM clustering performance for different base architectures. "], "img_footnote": [], "page_idx": 45}, {"type": "text", "text": "the spectral layer, we use the partial eigendecomposition corresponding to the lowest $k=50$ eigenvalues $k=100$ for the optimized $\\mathrm{S}^{2}\\mathrm{GNN}$ versions), spectral normalization, and $\\lambda_{\\mathrm{cut}}=1.3$ . For the optimized models, we employ parameter sharing with 128 heads, and a bottleneck of 0.25 in feature gating. We use 8 attention heads for all GAT versions in accordance with Dwivedi et al. (2023) (inner dimension is not expanded but split up), except for the optimized version, which uses 4 heads. For the purely spatial models, we use $p\\,=\\,0.0$ as dropout (similar to Dwivedi et al. (2023)). We observe this to lead to overfitting for models with spectral layers, for which we set $p\\in\\{0.1,0.2\\}$ . Hyperparameters differing between the compared models are listed in Table 12. We train for 100 epochs using the AdamW optimizer (Loshchilov & Hutter, 2019) with a base learning rate of 0.001, no weight decay, and a cosine scheduler with 5 warmup epochs. ", "page_idx": 45}, {"type": "table", "img_path": "Cb3kcwYBgw/tmp/77ea63229ee50eeba957da83864dc969295562cd8f6949f4f1f45461f62e8cbb.jpg", "table_caption": ["Table 11: Results on the CLUSTER task (Dwivedi et al., 2023). Transformer models that outperform our $S^{2}$ GatedGCN are underlined. "], "table_footnote": [], "page_idx": 45}, {"type": "table", "img_path": "Cb3kcwYBgw/tmp/e75e9e8d0364926597da6957b83b655ff344ae139a1e2ab304bd1e7d61dd33ae.jpg", "table_caption": ["Table 12: Ablation results on the SBM clustering task (Dwivedi et al., 2023). The best mean test accuracy is bold, second is underlined. "], "table_footnote": [], "page_idx": 46}, {"type": "text", "text": "", "page_idx": 46}, {"type": "text", "text": "Results. Results for the CLUSTER task are presented in Table 12, Table 11 and Fig. 24. Introducing a spectral layer significantly enhances performance on the 4-layer architectures, both with and without positional encodings. The effect is most pronounced on GCN, where replacing just a single GCN layer by a spectral layer boosts accuracy from 0.504 to 0.655. Notably, introducing two spectral layers still has a consistent positive effect on all 16-layer architectures. ", "page_idx": 46}, {"type": "text", "text": "M.7 Distance Regression ", "text_level": 1, "page_idx": 46}, {"type": "text", "text": "Setup. We generate directed random trees with one source by sampling trees with $n\\ \\in$ $\\{500,\\ldots,999\\}$ nodes, picking one node at random to declare as a source and introducing edge directions accordingly. To construct random DAGs with long distances, we start from such directed random trees and proceed by adding $\\lfloor n/10\\rfloor$ edges at random, choosing each edge direction such that the resulting graph is still a DAG. Additionally, we mark the source node with a node feature. Besides evaluating all models in an in-distribution regime, we also assess the generalization power of the methods by drawing out-of-distribution val/test splits from slightly larger graphs of $\\bar{n}\\in\\{1000,\\ldots,1099\\}$ and $n\\in\\{1100,\\ldots,1199\\}$ nodes each. We use $L^{2}$ loss for training and $R^{2}$ as a target metric. We sample 50,000 training and 2,500 val/test graphs each and report the average $\\pm$ standard deviation over 3 random reruns. ", "page_idx": 46}, {"type": "image", "img_path": "Cb3kcwYBgw/tmp/799123af7303d9eb943d042252df158c4046044875edf728f3fcff638b3faf87.jpg", "img_caption": ["Figure 25: Test accuracy curves for the SBM clustering task. Curves are shown for the models from Table 12 with PE, with the MPGNN baseline and the respective $\\mathrm{S}^{2}\\mathrm{GNN}$ . "], "img_footnote": [], "page_idx": 47}, {"type": "table", "img_path": "Cb3kcwYBgw/tmp/5347517edd14a88d9355998242864e1ab6f7ccd90e3f731fa3c4586a9e3193d3.jpg", "table_caption": ["Table 13: Results on the distance task, with DirGCN as base. The best mean score is bold, second is underlined. "], "table_footnote": [], "page_idx": 47}, {"type": "text", "text": "Models. As a MPGNN baseline, we use a five-layer directed version of GCN, DirGCN (Rossi et al., 2023), with three post-message-passing layers, and concatenating instead of averaging over the source-to-target and target-to-source parts. We compare these baselines to $\\mathrm{S}^{2}\\mathrm{DirGCN}$ of the form Eq. 2 with four spectral layers, alternating spatial and spectral convolutions and employing residual connections. We benchmark versions of $\\mathrm{S}^{2}\\mathrm{DirGCN}$ that ignore edge direction in the spectral convolution against directed versions in which we set $q=0.001$ . In all cases, we use the partial eigendecomposition corresponding to the $k=50$ lowest eigenvalues. All models are optionally enriched by the positional encodings from $\\S\\ 3.2.4$ . Throughout all evaluations, we use an inner dimension of 236, GELU (Hendrycks & Gimpel, 2016) as an activation function, and dropout $p=0.05$ . For the spectral layers, we utilize the gating mechanism $f_{\\theta}^{(l)}$ , not employing a neural network in the spectral domain, we use spectral normalization, $\\lambda_{\\mathrm{cut}}=0.\\dot{1}$ , and a bottleneck of 0.03 in the spectral layer. We train for 50 epochs, using a batch size of 36 and the AdamW optimizer (Loshchilov & Hutter, 2019) with a base learning rate of 0.001, a weight decay of 0.008, and a cosine scheduler with 5 warmup epochs. ", "page_idx": 47}, {"type": "text", "text": "Results. In Table 13, we show the performance of the different models on DAGs and trees. We observe that the simple MPGNNs are notably surpassed by all versions of $\\mathrm{S}^{2}\\mathrm{DirGCN}$ . While $\\mathrm{S}^{2}\\mathrm{DirGCN}$ achieves nearly perfect predictions on the tree tasks in both the directed and undirected case, the undirected version is outperformed by the directed version on the DAG tasks. Here, performance also reduces slightly in the out-of-distribution regime. The great performance on the tree task is due to the fact that trees are collision-free graphs (Geisler et al., 2023), where the phase of each eigenvector is $\\exp(i2\\pi q(d_{v}+c))$ for each node $v$ , with $d_{v}$ representing the distance to the source node and $c\\in\\mathbb{R}$ being an arbitrary constant (due to phase invariance of the eigenvector). It is noteworthy that a simple MPGNN with positional encodings, despite having the distances (shifted by $c$ ) readily available, fails the task, as the information about the phase of the source node cannot be effectively shared among all nodes. In Fig. 26, we compare the distance predictions by the different models. While the prediction of all models is close to perfect below a distance of 5, the spatial MPGNNs are almost unable to distinguish higher distances. By contrast, $\\mathrm{S}^{2}\\mathrm{DirGCN}$ predicts reasonable distances regardless of the ground truth, with the absolute error only increasing slowly. ", "page_idx": 47}, {"type": "image", "img_path": "Cb3kcwYBgw/tmp/a20628a79af2dc6b65463b7195d9b3650fb43eff0b4a92a81157cfde46fa3e13.jpg", "img_caption": ["Figure 26: RMSE and $90\\%$ prediction intervals for distance predictions by ground truth. "], "img_footnote": [], "page_idx": 48}, {"type": "text", "text": "", "page_idx": 48}, {"type": "text", "text": "M.8 Heterophilic arXiv-year (Lim et al., 2021) ", "text_level": 1, "page_idx": 48}, {"type": "text", "text": "Setup. We evaluate $\\mathsf{S}^{2}\\mathrm{GNN}$ on a large-scale heterophilic dataset, namely arXiv-year. arXiv-year is based on OGB arXiv (Hu et al., 2020), but instead of paper subject areas, the year of publication (divided into 5 classes) is the prediction target. While there are no long-range interactions in this dataset, preliminary experiments indicated that the phase of the Magnetic Laplacian eigenvectors on its own can also be predictive of the class label. We report average $\\pm$ standard deviation over 5 reruns with the splits from Lim et al. (2021), using a different random seed for each run. ", "page_idx": 48}, {"type": "text", "text": "Models. We use DirGCN (Rossi et al., 2023) as a baseline and largely follow the original setup. However, we observe that using 4 layers (instead of 6) and introducing a dropout of $p\\,=\\,0.5$ improves baseline performance. Furthermore, we drop the jumping knowledge used by Rossi et al. (2023). We compare this baseline to $\\mathrm{S}^{2}\\mathrm{DirGCN}$ with two spectral layers (after the second and third spatial layers) and apply residual connections only for the spectral layers. For the spectral layers, we set $q\\,=\\,0.0001$ and use the partial eigendecomposition with $k\\,=\\,100$ , a NN in the spectral domain $\\S\\ 3.2.2$ , no feature gating, and a bottleneck of 0.05. All other parameters are kept similar to the DirGCN base of Rossi et al. (2023). We train for 2000 epochs using the AdamW optimizer (Loshchilov & Hutter, 2019) with a base learing rate of 0.005, no weight decay, and a cosine scheduler with 50 warmup epochs. ", "page_idx": 48}, {"type": "text", "text": "Results. We report the results in Table 14. Notably, our $\\mathrm{S}^{2}\\mathrm{DirGCN}$ outperforms both our baseline DirGCN as well as the recent FaberNet (Koke & Cremers, 2024), albeit by a very tight margin. However, we found hyperparameter optimizations to be quite noisy, and as such, the resulting performance metrics should be interpreted cautiously. A more comprehensive evaluation of $\\mathrm{{S^{2}G N N`s}}$ power on heterophilic datasets, potentially with long-range interactions, is left for future work. ", "page_idx": 48}, {"type": "table", "img_path": "Cb3kcwYBgw/tmp/160d343113d70a469783119d4514d7bd4c0aad746cdc1397dedfe3fbafe55230.jpg", "table_caption": ["Table 14: Results on arXiv-year. Best mean test accuracy is bold, second is underlined. "], "table_footnote": [], "page_idx": 49}, {"type": "text", "text": "M.9 Large-Scale PCQM4Mv2 (Hu et al., 2021) ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "We show that $\\mathrm{S}^{2}\\mathrm{GNNs}$ are very parameter efficient. Even though we only conduct a very rudimentary hyperparameter search, $\\mathrm{S}^{2}\\mathrm{GNNs}$ keep up with state of the art approaches. Specifically, we adapt the hyperparameters from peptides-func and achieve comparable performance to the state of the art (excluding external data) with about $3{-}20\\%$ of the number of parameters. ", "page_idx": 49}, {"type": "table", "img_path": "Cb3kcwYBgw/tmp/99cdb0c2fb418cca37132e388dae8b3db1acbca70bdd0a91b65433069480693a.jpg", "table_caption": ["Table 15: Results on PCQM4Mv2 (Hu et al., 2021) (validation). "], "table_footnote": [], "page_idx": 49}, {"type": "text", "text": "M.10 TPUGraphs Graph Construction ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "The \u201cXLA\u201d collections of TPUGraphs contain many constructs that are most certainly suboptimal for a machine-learning-based runtime prediction. However, in our preliminary experiments, we could not show that our graph construction yielded better results in a statistically significant manner. Nevertheless, we include this discussion since it might be insightful. ", "page_idx": 49}, {"type": "text", "text": "To understand the challenges with the default graph construction, note that in the TPUGraphs dataset each node represents an operation in the compuational graph of Accelerated Linear Algebra (XLA). Its incoming edges are the respective operands, and the outgoing edges signal where the operation\u2019s result is used. Thus, the graph describes how the tensors are being transformed. An (perhaps unnecessary) challenge for machine learning models arises from using tuple, which represents a sequence of tensors of arbitrary shapes. In this case, the model needs to reason how the tuple is constructed, converted, and unpacked again. Moreover, directly adjacent tensors/operations can be very far away in the graphs of TPUGraphs. ", "page_idx": 49}, {"type": "text", "text": "We identified and manually \u201cfixed\u201d three cases to eliminate this problem largely in the TPUGraphs dataset: Tuple-GetTupleElement, While, and Conditional. Since we could not access the configurations in the HLO protobuf files and $C++$ XLA extraction code, we decided to perform these optimizations ourselves. However, it might be a better strategy to utilize the existing XLA compiler etc. ", "page_idx": 49}, {"type": "text", "text": "Additionally, to the subsequently described graph structure changes, we extract the order of operands from the HLO protobud files. Outgoing edges are assumed to be unordered except for the GetTupleElement operation, where the tuple index is used as order. Moreover, we extracted all features masked in the $C++$ code and then excluded constant features. ", "page_idx": 49}, {"type": "text", "text": "M.10.1 Tuple-GetTupleElement ", "text_level": 1, "page_idx": 49}, {"type": "text", "text": "The dataset contains aggregations via the XLA Tuple operation that are often directly followed by a GetTupleElement operation. To a large extent, these constructs are required for the subsequently discussed While and Conditional operations. Importantly, the model could not extract the relationships through a tuple aggregation since the tuple_index was not included in the default features. Moreover, the resulting tuple hub nodes severely impact the Fourier basis of the graphs (see $\\S~2$ ). We illustrate the graph simplification in Fig. 27 and denote the edge order of incoming edges from top to bottom. The edge order represents the order of operands. ", "page_idx": 49}, {"type": "text", "text": "We propose dropping immediate Tuple-GetTupleElement constructs and directly connecting predecessors and successors. For this, we generate a graph solely consisting of direct connections and then resolve multiple consecutive Tuple-GetTupleElement constructs via a graph traversal (depthfirst search). ", "page_idx": 50}, {"type": "image", "img_path": "Cb3kcwYBgw/tmp/60e46bd4e092c8aa008131d5bbe42545592b464bfad99387f1ab99e62208a8de.jpg", "img_caption": [], "img_footnote": [], "page_idx": 50}, {"type": "text", "text": "Figure 27: Tuple-GetTupleElement simplification: the Tuple aggregates the output of multiple predecessors/operations and then the GetTupleElement extracts the tensor according to its index (number in respective nodes). We propose dropping immediate Tuple-GetTupleElement constructs and connecting predecessors and successors. ", "page_idx": 50}, {"type": "text", "text": "We perform the Tuple-GetTupleElement simplification after dealing with While and Conditionals. However, for the sake of simplicity, we will avoid using tuples in the subsequent explanations for While and Conditional. In other words, the subsequent explanations extend to functions with multiple arguments via the use of tuples. ", "page_idx": 50}, {"type": "text", "text": "M.10.2 While Operation ", "text_level": 1, "page_idx": 50}, {"type": "text", "text": "The While operation has the signature While(condition, body, init) where condition is a function given the init or output of the body function. Note that in the TPUGraph construction, body as well as condition only represent the outputs of the respective function and their operands need to be extracted from HLO. ", "page_idx": 50}, {"type": "text", "text": "To avoid hub nodes and to retain the dataflow between operations (important for decisions about the layout), operands and outputs are connected directly. Technically, we am modeling a do-while construct because the condition is not connected to the inputs. Since the successors of the while are of type GetTupleElement, they relabeled to a new node type, signaling the end of a while loop. To support nested while loops, each node in the body is assigned a new node feature signaling the number of while body statements it is part of. ", "page_idx": 50}, {"type": "image", "img_path": "Cb3kcwYBgw/tmp/d006ea59e831deff6817afa646b187e09d58e10a52179e09855fa0bc5620a7ab.jpg", "img_caption": [], "img_footnote": [], "page_idx": 50}, {"type": "text", "text": "Figure 28: Instead of aggregating everything into a hub node, we propose to connect respective inputs and outputs. ", "page_idx": 50}, {"type": "text", "text": "M.10.3 Conditional Operation ", "text_level": 1, "page_idx": 51}, {"type": "text", "text": "Conditional(branch_index, branch_computations, branch_operands) is the most common signature of the Conditional operation, where the integer-valued branch_index selects which branch_computations is executed with the respective input in branch_operands. Similarly to the While operation, we introduce new node types for the inputs of computations and the successors (they are GetTupleElement operations). ", "page_idx": 51}, {"type": "image", "img_path": "Cb3kcwYBgw/tmp/460e429090e418941af1fab283196d9032ce3e1bbe2309cf5f379fadbd0522c9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 51}, {"type": "text", "text": "Figure 29: Instead of aggregating everything into a hub node, we propose to connect respective inputs and outputs. Here as an example with two conditional computations. ", "page_idx": 51}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 52}, {"type": "text", "text": "Justification: We discuss the shortcomings of prior MPGNNs and that our new framework $\\mathrm{S}^{2}\\mathrm{GNN}$ takes a new approach to overcome the limitations. We detail our theoretical analysis/justification, outline the yet largely unexplored design space of $\\mathrm{S}^{2}\\mathrm{GNN}$ , and summarize the experimental evaluation. ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 52}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 52}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "Justification: We provide approximation-theoretic error bound in $\\S\\ 3.1.2$ that also gives light on the limitations. Moreover, we discuss important considerations of $\\mathrm{S}^{2}\\bar{\\mathrm{GNNs}}$ throughout the method section $\\S\\ 3$ . Additionally, we elaborately list and summarize the limitations in $\\S\\mathrm{~K~}$ . ", "page_idx": 52}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 52}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 52}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: The theoretical analysis o $\\mathrm{~f~}\\S\\ 3,\\ \\S\\ 3.1.1,\\ \\S\\ 3.1.2,\\ \\S\\ 3.2.4.$ , and $\\S\\ I$ state important assumptions in the main text. All theorems, definitions, formulas, and proofs are numerated. Due to the page limit, we do not provide proof sketches in the main body. Nevertheless, we provide accompanying sketches and illustrations to convey the main intuition. The proofs, full assumptions, and details are given in $\\S\\mathrm{~H~}(\\&\\mathrm{~\\S~I~})$ . ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 53}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 53}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 53}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 53}, {"type": "text", "text": "Justification: We describe our $\\mathrm{S}^{2}\\mathrm{GNNs}$ in length in $\\S\\ 3$ and detail the experimental setup in $\\S\\ 4$ . Additional relevant details are provided in $\\S\\ M.1$ . We provide code, including configuration, at https://www.cs.cit.tum.de/daml/s2gnn. ", "page_idx": 53}, {"type": "text", "text": "Guidelines: ", "page_idx": 53}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.   \n(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).   \n(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 53}, {"type": "text", "text": "", "page_idx": 54}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 54}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 54}, {"type": "text", "text": "Justification: We will open source fully automized scripts for all datasets that download and preprocess the data prior to the execution of the configured experiment (except TPUGraphs (Phothilimthana et al., 2023) due to its dedicated access). This includes our own datasets on clustering, distance prediction, and associative recall. We provide code at https://www.cs.cit.tum.de/daml/s2gnn. Moreover, we provide reasonable details for reproduction in $\\S\\textbf{M}$ . ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 54}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 54}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 54}, {"type": "text", "text": "Justification: We provide the setup and details to a reasonable detail in $\\S~4$ and $\\S\\ M.1$ . For additional details, we refer to the provided code, including configuration, at https://www.cs.cit.tum.de/daml/s2gnn. ", "page_idx": 54}, {"type": "text", "text": "Guidelines: ", "page_idx": 54}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 54}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Justification: We report mean, standard deviation, and details on randomization for all relevant results on benchmark datasets. For qualitative insights and the large-scale TPUGraphs dataset, we solely provide mean estimates. The collective experimental results underline the claims and are strong evidence against the statistical insignificance of our results. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 55}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Justification: We provide a reasonable summary of used resources in $\\S\\,\\mathrm{M.l}$ and specifically Table 7. Here, we list the hardware accelerator used and the runtime of the experiment. ", "page_idx": 55}, {"type": "text", "text": "Guidelines: ", "page_idx": 55}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 55}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 55}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 55}, {"type": "text", "text": "Answer: [Yes] Justification: [Yes] ", "page_idx": 55}, {"type": "text", "text": "Guidelines: The proposed framework $\\mathrm{S}^{2}\\mathrm{GNN}$ does not pose an unusual risk on top of the common risks for research on machine learning architectures. Our evaluation does not include human subjects, participants, or data. We discuss the broader impact in $\\S\\,\\mathrm{L}$ . Thus, in summary, we conform to the ethics guidelines in every aspect. ", "page_idx": 56}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 56}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 56}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 56}, {"type": "text", "text": "Justification: We discuss the broader impact in $\\S\\mathrm{~L~}$ . It should be noted that the proposed framework $\\mathrm{S}^{2}\\mathrm{GNN}$ does not pose an unusual risk on top of the common risks for research on machine learning architectures. We discuss the broader impact in $\\S\\,\\mathrm{L}$ . ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 56}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 56}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 56}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 56}, {"type": "text", "text": "Justification: The proposed framework $\\mathrm{S}^{2}\\mathrm{GNN}$ does not pose an unusual risk on top of the common risks for research on machine learning architectures. Our evaluation does not include human subjects, participants, or data. ", "page_idx": 56}, {"type": "text", "text": "Guidelines: ", "page_idx": 56}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 56}, {"type": "text", "text": "", "page_idx": 57}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 57}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 57}, {"type": "text", "text": "Justification: We cite all origins of the datasets, even if we derive the benchmark from someone else. We provide an overview of the datasets, including the licenses in Table 6. We provide code, including configuration, at https://www.cs.cit.tum. $\\mathtt{d e/d a m l/s2g n n}$ . It also contains an (anonymized) license for its usage and discloses the provenance of the project setup. ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 57}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 57}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 57}, {"type": "text", "text": "Justification: The code and configuration for the dataset on GMM clustering, distance regression, and associative recall will be provided. The generation of all resources takes around one hour and uses solely CPUs. We provide code, including configuration, at https://www.cs.cit.tum.de/daml/s2gnn. The datasets are described in $\\S\\mathrm{~M~}$ , $\\S\\ M.6$ , and $\\S\\,\\mathrm{M.7}$ . ", "page_idx": 57}, {"type": "text", "text": "Guidelines: ", "page_idx": 57}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 57}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 57}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 57}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 58}, {"type": "text", "text": "Justification: Our work neither involves crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 58}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 58}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 58}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 58}, {"type": "text", "text": "Justification: Our work neither involves crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 58}, {"type": "text", "text": "\u2022 The answer NA means that the paper neither involves crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 58}]