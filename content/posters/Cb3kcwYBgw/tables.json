[{"figure_path": "Cb3kcwYBgw/tables/tables_7_1.jpg", "caption": "Table 1: Long-range benchmark. Our S2GNN uses \u2248 35% fewer parameters than the other models. AP is Peptides-func's and MAE peptides-struct's target metric. The best/second best is bold/underlined.", "description": "This table compares the performance of S2GCN against various state-of-the-art models on two long-range interaction benchmark datasets: peptides-func and peptides-struct.  It shows the accuracy (AP for peptides-func, MAE for peptides-struct) achieved by different types of models (Transformers, Rewiring methods, State Space Models, and other GNNs).  The table highlights that S2GCN achieves state-of-the-art performance on peptides-func using significantly fewer parameters than other models.", "section": "4.1 Long-Range Interactions"}, {"figure_path": "Cb3kcwYBgw/tables/tables_8_1.jpg", "caption": "Table 1: Long-range benchmark. Our S2GNN uses \u2248 35% fewer parameters than the other models. AP is Peptides-func's and MAE peptides-struct's target metric. The best/second best is bold/underlined.", "description": "This table compares the performance of S2GNN with other state-of-the-art graph neural networks on two long-range interaction benchmark datasets: peptides-func and peptides-struct.  The results show that S2GNN achieves state-of-the-art performance on peptides-func while using significantly fewer parameters than competing models. The table highlights the superior performance of S2GNN in modeling long-range interactions.", "section": "4.1 Long-Range Interactions"}, {"figure_path": "Cb3kcwYBgw/tables/tables_9_1.jpg", "caption": "Table 3: OGB Products. Split Model Accuracy (\u2191) F1 (\u2191) Train GAT 0.866\u00b10.001 0.381\u00b10.001 S2GAT 0.902\u00b10.000 0.472\u00b10.006 Val GAT 0.907\u00b10.001 0.508\u00b10.002 S2GAT 0.913\u00b10.002 0.582\u00b10.014 Test GAT 0.798\u00b10.003 0.347\u00b10.004 S2GAT 0.811\u00b10.007 0.381\u00b10.009", "description": "This table presents the results of applying GAT and S2GAT models on the OGB Products dataset.  The table is split into three sections: Train, Val, and Test, reflecting the different stages of model training and evaluation. For each split and model, the accuracy and F1 score are reported.  The S2GAT model consistently outperforms the GAT model across all three splits, indicating its effectiveness in improving performance on the OGB Products benchmark.", "section": "4.3 Large-Scale Benchmarks"}, {"figure_path": "Cb3kcwYBgw/tables/tables_9_2.jpg", "caption": "Table 4: Graph ranking on TPUGraphs \"layout\".", "description": "This table presents the results of graph ranking experiments on the TPUGraphs dataset using GCN and S2GCN models.  The \"layout\" refers to a specific subset of the TPUGraphs data.  The Kendall tau metric is used to evaluate the performance of the models. The table shows that S2GCN outperforms GCN on this task.", "section": "4.3 Large-Scale Benchmarks"}, {"figure_path": "Cb3kcwYBgw/tables/tables_18_1.jpg", "caption": "Table 1: Long-range benchmark. Our S2GNN uses \u2248 35% fewer parameters than the other models. AP is Peptides-func's and MAE peptides-struct's target metric. The best/second best is bold/underlined.", "description": "This table compares the performance of S2GNN with other state-of-the-art models on the peptides-func and peptides-struct datasets from the long-range benchmark.  It shows that S2GNN achieves competitive performance while using significantly fewer parameters. The table highlights the superior performance of S2GNN, particularly with the addition of positional encodings.", "section": "4.1 Long-Range Interactions"}, {"figure_path": "Cb3kcwYBgw/tables/tables_34_1.jpg", "caption": "Table 1: Long-range benchmark. Our S2GNN uses \u2248 35% fewer parameters than the other models. AP is Peptides-func's and MAE peptides-struct's target metric. The best/second best is bold/underlined.", "description": "This table compares the performance of S2GNN with other state-of-the-art graph neural networks on two long-range benchmark tasks: peptides-func and peptides-struct.  It highlights that S2GNN achieves state-of-the-art performance on peptides-func while using significantly fewer parameters (approximately 35% less) than competing models.  The table also provides the metrics used for evaluation (Average Precision for peptides-func and Mean Absolute Error for peptides-struct), indicating which models achieved the best and second-best results on each task.", "section": "4.1 Long-Range Interactions"}, {"figure_path": "Cb3kcwYBgw/tables/tables_34_2.jpg", "caption": "Table 1: Long-range benchmark. Our S2GNN uses \u224835% fewer parameters than the other models. AP is Peptides-func's and MAE peptides-struct's target metric. The best/second best is bold/underlined.", "description": "This table compares the performance of S2GNN with other state-of-the-art models on two long-range interaction benchmark datasets: peptides-func and peptides-struct.  The table shows that S2GNN achieves competitive performance with significantly fewer parameters.", "section": "4.1 Long-Range Interactions"}, {"figure_path": "Cb3kcwYBgw/tables/tables_39_1.jpg", "caption": "Table 6: Dataset statistics and licenses.", "description": "This table presents statistics for various datasets used in the paper's experiments, including the number of graphs, average number of nodes and edges, the type of task performed on each dataset (e.g., graph classification, node regression), and the license associated with each dataset.  The datasets cover a range of sizes and complexities, reflecting the diversity of graph learning tasks.", "section": "4 Empirical Results"}, {"figure_path": "Cb3kcwYBgw/tables/tables_39_2.jpg", "caption": "Table 7: Important S2GNNs specific hyperparameters and runtimes. The times for the EVD cover the respective dataset entirely.", "description": "This table shows the hyperparameters used in the experiments for each dataset. For each dataset, it shows the number of message passing layers, the number of spectral layers, the dimension of the node features, the number of spectral filters per layer, the number of eigenvectors used for the spectral filter, the spectral frequency cutoff, whether a neural network was used for the spectral filter, the training time, the time to compute the eigendecomposition, the GPU used for training, and any additional notes about the specific experiment setup.  The training time and EVD time are given in hours and minutes.", "section": "M Experimental Results"}, {"figure_path": "Cb3kcwYBgw/tables/tables_42_1.jpg", "caption": "Table 8: Ablation of different aggregation functions on the peptides-func benchmark, with our PE.", "description": "This table presents the results of an ablation study on the peptides-func benchmark, focusing on different aggregation methods for combining spatial and spectral filters within the S2GNN model.  It shows the test accuracy (AP) achieved using different aggregation functions (Concat, Sum, Mamba-like, Sequential) with and without normalization.  The number of parameters for each model configuration is also included.", "section": "M.4 S2GNN Aggregation Ablation"}, {"figure_path": "Cb3kcwYBgw/tables/tables_44_1.jpg", "caption": "Table 9: Accuracy on the GMM clustering task for varying number of eigenvectors k, using 4 GCN layers and one spectral layer in the end.", "description": "This table shows the accuracy results of the GMM clustering task using different numbers of eigenvectors (k).  The experiment uses 4 Graph Convolutional Network (GCN) layers and one spectral layer at the end. The table compares the performance of S2GCN (with and without positional encodings) across different values of k, ranging from 2 to 10.  It helps to understand the impact of spectral filter expressiveness on the GMM clustering task performance.", "section": "M.6 Clustering Tasks"}, {"figure_path": "Cb3kcwYBgw/tables/tables_44_2.jpg", "caption": "Table 10: Accuracy on the GMM clustering task for varying number of MP layers, while comparing a purely spatial GCN model to S2GCN with one spectral layer added in the end.", "description": "This table shows the accuracy results on the GMM clustering task for different numbers of message-passing (MP) layers. It compares the performance of a standard Graph Convolutional Network (GCN) model with the proposed Spatio-Spectral Graph Neural Network (S2GCN), which includes an additional spectral layer at the end.  The table shows how the accuracy changes with different numbers of MP layers for both models, with and without positional encodings.", "section": "4.1 Long-Range Interactions"}, {"figure_path": "Cb3kcwYBgw/tables/tables_45_1.jpg", "caption": "Table 11: Results on the CLUSTER task (Dwivedi et al., 2023). Transformer models that outperform our S2GatedGCN are underlined.", "description": "This table presents the results of the CLUSTER task from the Dwivedi et al. (2023) benchmark.  It compares the performance of various GNN and transformer models on this node classification task.  The table highlights the superior performance of the proposed S2GatedGCN model, showing it outperforms most GNN models and is competitive with several transformer-based approaches.", "section": "4.1 Long-Range Interactions"}, {"figure_path": "Cb3kcwYBgw/tables/tables_46_1.jpg", "caption": "Table 12: Ablation results on the SBM clustering task (Dwivedi et al., 2023). The best mean test accuracy is bold, second is underlined.", "description": "This table presents the ablation study on the SBM clustering task, comparing different configurations of MPGNN models (GCN, GAT, GatedGCN) with and without spectral filters and positional encodings.  It shows the impact of these components on both training and test accuracy, highlighting the best performing configurations.  The table includes columns for the MPGNN base model, the number of layers, inner dimension, the use of spectral filters, positional encodings, dropout rate, the number of parameters, training accuracy, and test accuracy.", "section": "M.6.2 SBM Clustering CLUSTER (Dwivedi et al., 2023)"}, {"figure_path": "Cb3kcwYBgw/tables/tables_47_1.jpg", "caption": "Table 13: Results on the distance task, with DirGCN as base. The best mean score is bold, second is underlined.", "description": "This table presents the results of the distance regression task.  It compares the performance of several models, including a baseline DirGCN model and variants of S2DirGCN with and without positional encodings and directed/undirected spectral filters, on both in-distribution and out-of-distribution datasets. The evaluation metrics used are Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and R-squared (R\u00b2).", "section": "4.2 Long-Range Interactions"}, {"figure_path": "Cb3kcwYBgw/tables/tables_49_1.jpg", "caption": "Table 14: Results on arXiv-year. Best mean test accuracy is bold, second is underlined.", "description": "This table presents the results of the arXiv-year experiment, comparing the performance of different models on a large-scale heterophilic dataset.  The best-performing model is shown in bold, and the second best is underlined. The results highlight the performance of the proposed S\u00b2DirGCN model in comparison to other state-of-the-art models.", "section": "4.3 Large-Scale Benchmarks"}, {"figure_path": "Cb3kcwYBgw/tables/tables_49_2.jpg", "caption": "Table 15: Results on PCQM4Mv2 (Hu et al., 2021) (validation).", "description": "This table compares the performance of different models on the PCQM4Mv2 dataset. The Mean Absolute Error (MAE) and the number of parameters are reported for each model.  The results show that the proposed S\u00b2GNN model achieves comparable performance to the state-of-the-art models while using significantly fewer parameters.", "section": "4.3 Large-Scale Benchmarks"}]