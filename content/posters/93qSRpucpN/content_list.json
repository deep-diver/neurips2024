[{"type": "text", "text": "Robust Guided Diffusion for Offline Black-box Optimization ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Anonymous Author(s)   \nAffiliation   \nAddress   \nemail ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "1 Offline black-box optimization aims to maximize a black-box function using an   \n2 offilne dataset of designs and their measured properties. Two main approaches have   \n3 emerged: the forward approach, which learns a mapping from input to its value,   \n4 thereby acting as a proxy to guide optimization, and the inverse approach, which   \n5 learns a mapping from value to input for conditional generation. (a) Although   \n6 proxy-free (classifier-free) diffusion shows promise in robustly modeling the inverse   \n7 mapping, it lacks explicit guidance from proxies, essential for generating high  \n8 performance samples beyond the training distribution. Therefore, we propose   \n9 proxy-enhanced sampling which utilizes the explicit guidance from a trained proxy   \n10 to bolster proxy-free diffusion with enhanced sampling control. (b) Yet, the trained   \n11 proxy is susceptible to out-of-distribution issues. To address this, we devise the   \n12 module diffusion-based proxy refinement, which seamlessly integrates insights from   \n13 proxy-free diffusion back into the proxy for refinement. To sum up, we propose   \n14 Robust Guided Diffusion for Offilne Black-box Optimization (RGD), combining the   \n15 advantages of proxy (explicit guidance) and proxy-free diffusion (robustness) for   \n16 effective conditional generation. RGD achieves state-of-the-art results on various   \n17 design-bench tasks, underscoring its efficacy. Our code is here. ", "page_idx": 0}, {"type": "text", "text": "18 1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "19 Creating new objects to optimize specific properties is a ubiquitous challenge that spans a multitude   \n20 of fields, including material science, robotic design, and genetic engineering. Traditional methods   \n21 generally require interaction with a black-box function to generate new designs, a process that could   \n22 be financially burdensome and potentially perilous [1, 2]. Addressing this, recent research endeavors   \n23 have pivoted toward a more relevant and practical context, termed offline black-box optimization   \n24 (BBO) [3, 4]. In this context, the goal is to maximize a black-box function exclusively utilizing an   \n25 offline dataset of designs and their measured properties.   \n26 There are two main approaches for this task: the forward approach and the reverse approach. The   \n27 forward approach entails training a deep neural network (DNN), parameterized as $\\mathcal{I}_{\\phi}(\\cdot)$ , using the   \n28 offline dataset. Once trained, the DNN acts as a proxy and provides explicit gradient guidance to   \n29 enhance existing designs. However, this technique is susceptible to the out-of-distribution (OOD)   \n30 issue, leading to potential overestimation of unseen designs and resulting in adversarial solutions [5].   \n31 The reverse approach aims to learn a mapping from property value to input. Inputting a high value   \n32 into this mapping directly yields a high-performance design. For example, MINs [6] adopts GAN [7]   \n33 to model this inverse mapping, and demonstrate some success. Recent works [4] have applied   \n34 proxy-free diffusion1[8], parameterized by $\\pmb{\\theta}$ , to model this mapping, which proves its efficacy over   \n35 other generative models. Proxy-free diffusion employs a score predictor $\\tilde{\\mathbf{s}}_{\\theta}(\\cdot,\\cdot,\\omega)$ . This represents a   \n36 linear combination of conditional and unconditional scores, modulated by a strength parameter $\\omega$ to   \n37 balance condition and diversity in the sampling process. This guidance significantly diverges from   \n38 proxy (classifier) diffusion that interprets scores as classifier gradients and thus generates adversarial   \n39 solutions. Such a distinction grants proxy-free diffusion its inherent robustness in generating samples.   \n40 Nevertheless, proxy-free diffusion, initially de  \n41 signed for in-distribution generation, such as   \n42 synthesizing specific image categories, faces   \n43 limitations in offline BBO. Particularly, it strug  \n44 gles to generate high-performance samples that   \n45 exceed the training distribution due to the lack   \n46 of explicit guidance2. Consider, for example,   \n47 the optimization of a two-dimensional variable   \n48 $(x_{d1},x_{d2})$ to maximize the negative Rosenbrock   \n49 function [9]: $y(x_{d1},x_{d2})\\;=\\;-(1\\,-\\,x_{d1})^{2}\\;-$   \n50 $100(x_{d2}-x_{d1}^{2})^{2}$ , as depicted in Figure 1. The   \n51 objective is to steer the initial points (indi  \ncated in pink) towards the high-performance   \n53 region (highlighted in yellow). While proxy  \n54 free diffusion can nudge the initial points closer to this high-performance region, the generated points   \n55 (depicted in blue) fail to reach the high-performance region due to its lack of explicit proxy guidance.   \n56 To address this challenge, we introduce a proxy-enhanced sampling module as illustrated in Fig  \n57 ure 2(a). It incorporates the explicit guidance from the proxy ${\\mathcal{I}}_{\\phi}({\\pmb x})$ into proxy-free diffusion to   \n58 enable enhanced control over the sampling process. This module hinges on the strategic optimization   \n59 of the strength parameter $\\omega$ to achieve a better balance between condition and diversity, per reverse   \n60 diffusion step. This incorporation not only preserves the inherent robustness of proxy-free diffusion   \n61 but also leverages the explicit proxy guidance, thereby enhancing the overall conditional generation   \n62 efficacy. As illustrated in Figure 1, samples (depicted in red) generated via proxy-enhanced sampling   \n63 are more effectively guided towards, and often reach, the high-performance area (in yellow). ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "93qSRpucpN/tmp/a0687da8b6de1c3d6e9081d8726c03521b5589a98a5e8dd62b8126ce67c5ab16.jpg", "img_caption": ["Figure 1: Motivation of explicit proxy guidance. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "image", "img_path": "93qSRpucpN/tmp/e9ecce36502a122ab9f378829d0d7f7c5aa0163ac996d150362e6267b1d78168.jpg", "img_caption": ["Figure 2: Overall of RGD. "], "img_footnote": [], "page_idx": 1}, {"type": "text", "text": "Yet, the trained proxy is susceptible to out-ofdistribution (OOD) issues. To address this, we devise a module diffusion-based proxy refinement as detailed in Figure 2(b). This module seamlessly integrates insights from proxy-free diffusion into the proxy ${\\mathcal{I}}_{\\phi}(x)$ for refinement. Specifically, we generate a diffusion distribution $p_{\\pmb{\\theta}}(y|\\hat{\\pmb{x}})$ on adversarial samples $\\hat{\\pmb{x}}$ , using the associated probability flow ODE 3. This distribution is derived independently of a proxy, thereby exhibiting greater robustness than the proxy distribution on adversarial samples. Subsequently, we calculate the KullbackLeibler divergence between the two distributions on adversarial samples, and use this divergence minimization as a regularization strategy to fortify the proxy\u2019s robustness and reliability. ", "page_idx": 1}, {"type": "text", "text": "80 To sum up, we propose Robust Guided Diffusion for Offilne Black-box Optimization (RGD), a novel   \n81 framework that combines the advantages of proxy (explicit guidance) and proxy-free diffusion (ro  \n82 bustness) for effective conditional generation. Our contributions are three-fold:   \n83 \u2022 We propose a proxy-enhanced sampling module which incorporates proxy guidance into proxy-free   \n84 diffusion to enable enhanced sampling control.   \n85 \u2022 We further develop diffusion-based proxy refinement which integrates insights from proxy-free   \n86 diffusion back into the proxy for refinement.   \n87 \u2022 RGD delivers state-of-the-art performance on various design-bench tasks, emphasizing its efficacy. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "88 2 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "89 2.1 Offline Black-box Optimization ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "90 Offilne black-box optimization (BBO) aims to maximize a black-box function with an offilne dataset.   \n91 Imagine a design space as $\\mathcal{X}=\\mathbb{R}^{d}$ , where $d$ is the design dimension. The offline BBO [3] is: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\pmb{x}^{*}=\\arg\\operatorname*{max}_{\\pmb{x}\\in\\mathcal{X}}J(\\pmb{x}).\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "92 In this equation, $J(\\cdot)$ is the unknown objective function, and $\\pmb{x}\\in\\mathcal{X}$ is a possible design. In this   \n93 context, there is an offline dataset, $\\mathcal{D}$ , that consists of pairs of designs and their measured properties.   \n94 Specifically, each $\\textbf{\\em x}$ denotes a particular design, like the size of a robot, while $y$ indicates its related   \n95 metric, such as its speed.   \n96 A common approach gradient ascent fits a proxy distribution $p_{\\phi}(y|\\pmb{x})=\\mathcal{N}(J_{\\phi}(\\pmb{x}),\\sigma_{\\phi}(\\pmb{x}))$ to the   \n97 offline dataset where $\\phi$ denote the proxy parameters: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\arg\\operatorname*{min}_{\\phi}\\mathbb{E}_{(\\pmb{x},\\pmb{y})\\in\\mathcal{D}}[-\\log p_{\\phi}(y|\\pmb{x})].}\\\\ &{\\displaystyle=\\arg\\operatorname*{min}_{\\phi}\\mathbb{E}_{(\\pmb{x},\\pmb{y})\\in\\mathcal{D}}\\log(\\sqrt{2\\pi}\\sigma_{\\phi}(\\pmb{x}))+\\frac{(y-J_{\\phi}(\\pmb{x}))^{2}}{2\\sigma_{\\phi}^{2}(\\pmb{x})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "98 For the sake of consistency with terminology used in the forthcoming subsection on guided diffusion,   \n99 we will refer to $p_{\\phi}(\\cdot|\\cdot)$ as the proxy distribution and $J_{\\phi}(\\cdot)$ as the proxy. Subsequently, this approach   \n100 performs gradient ascent with $J_{\\phi}(x)$ , leading to high-performance designs $x^{*}$ : ", "page_idx": 2}, {"type": "equation", "text": "$$\n{\\pmb x}_{\\tau+1}={\\pmb x}_{\\tau}+\\eta\\nabla_{\\pmb x}J_{\\phi}({\\pmb x})|_{{\\pmb x}={\\pmb x}_{\\tau}},\\quad\\mathrm{for}\\,\\tau\\in[0,\\mathrm{M}-1],\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "101 converging to $\\mathbf{\\nabla}x_{\\mathrm{M}}$ after $\\mathrm{M}$ steps. However, this method suffers from the out-of-distribution issue   \n102 where the proxy predicts values that are notably higher than the actual values. ", "page_idx": 2}, {"type": "text", "text": "103 2.2 Diffusion Models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "104 Diffusion models, a type of latent variable models, progressively introduce Gaussian noise to data in   \n105 the forward process, while the reverse process aims to iteratively remove this noise through a learned   \n106 score estimator. In this work, we utilize continuous time diffusion models governed by a stochastic   \n107 differential equation (SDE), as presented in [10]. The forward SDE is formulated as: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d\\pmb{x}=\\pmb{f}(\\pmb{x},t)d t+g(t)d\\pmb{w}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "108 where $\\pmb{f}(\\cdot,t)\\,:\\,\\mathbb{R}^{d}\\,\\rightarrow\\,\\mathbb{R}^{d}$ represents the drift coefficient, $g(\\cdot)\\;:\\;\\mathbb{R}\\;\\rightarrow\\;\\mathbb{R}$ denotes the diffusion   \n109 coefficient and $\\mathbf{\\nabla}w$ is the standard Wiener process. This SDE transforms data distribution into noise   \n110 distribution. The reverse SDE is: ", "page_idx": 2}, {"type": "equation", "text": "$$\nd\\pmb{x}=\\left[f(\\pmb{x},t)-g(t)^{2}\\nabla_{\\pmb{x}}\\log p(\\pmb{x})\\right]d t+g(t)d\\pmb{\\bar{w}},\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "111 with $\\nabla_{\\mathbf{x}}\\log{p(\\mathbf{x})}$ representing the score of the marginal distribution at time $t$ , and $\\bar{\\pmb{v}}$ symbolizing the   \n112 reverse Wiener process. The score function $\\nabla_{\\mathbf{x}}\\log{p(\\mathbf{x})}$ is estimated using a time-dependent neural   \n113 network $\\pmb{s}_{\\pmb{\\theta}}(\\pmb{x}_{t},t)$ , enabling us to transform noise into samples. For simplicity, we will use $\\pmb{s}_{\\pmb{\\theta}}(\\pmb{x}_{t})$ ,   \n114 implicitly including the time dependency $t$ . ", "page_idx": 2}, {"type": "text", "text": "115 2.3 Guided Diffusion ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "116 Guided diffusion seeks to produce samples with specific desirable attributes, falling into two cate  \n117 gories: proxy diffusion [11] and proxy-free diffusion [8]. While these were initially termed classifier   \n118 diffusion and classifier-free diffusion in classification tasks, we have renamed them to proxy diffu  \n119 sion and proxy-free diffusion, respectively, to generalize to our regression context. Proxy diffusion   \n120 combines the model\u2019s score estimate with the gradient from the proxy distribution, providing explicit   \n121 guidance. However, it can be interpreted as a gradient-based adversarial attack.   \n122 Proxy-free guidance, not dependent on proxy gradients, enjoys an inherent robustness of the sampling   \n123 process. Particularly, it models the score as a linear combination of an unconditional and a conditional   \n124 score. A unified neural network $\\pmb{s}_{\\pmb{\\theta}}(\\pmb{x}_{t},y)$ parameterizes both score types. The score $\\pmb{s}_{\\theta}(\\pmb{x}_{t},y)$   \n125 approximates the gradient of the log probability $\\nabla_{\\pmb{x}_{t}}\\log p(\\pmb{x}_{t}|y)$ , i.e., the conditional score, while   \n126 $\\pmb{s}_{\\theta}(\\pmb{x}_{t})$ estimates the gradient of the log probability $\\nabla_{\\mathbf{x}_{t}}\\log p(\\mathbf{x}_{t})$ , i.e., the unconditional score. The   \n127 score function follows: ", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "", "page_idx": 3}, {"type": "equation", "text": "$$\n\\tilde{s}_{\\theta}(x_{t},y,\\omega)=(1+\\omega)s_{\\theta}(x_{t},y)-\\omega s_{\\theta}(x_{t}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "128 Within this context, the strength parameter $\\omega$ specifies the generation\u2019s adherence to the condition   \n129 $y$ , which is set to the maximum value $y_{m a x}$ in the offline dataset following [4]. Optimization of $\\omega$   \n130 balances the condition and diversity. Lower $\\omega$ values increase sample diversity at the expense of   \n131 conformity to $y$ , and higher values do the opposite. ", "page_idx": 3}, {"type": "text", "text": "132 3 Method ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "133 In this section, we present our method RGD, melding the strengths of proxy and proxy-free diffu  \n134 sion for effective conditional generation. Firstly, we describe a newly developed module termed   \n135 proxy-enhanced sampling. It integrates explicit proxy guidance into proxy-free diffusion to enable   \n136 enhanced sampling control, as detailed in Section 3.1. Subsequently, we explore diffusion-based   \n137 proxy refinement which incorporates insights gleaned from proxy-free diffusion back into the proxy,   \n138 further elaborated in Section 3.2. The overall algorithm is shown in Algorithm 1. ", "page_idx": 3}, {"type": "text", "text": "139 3.1 Proxy-enhanced Sampling ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "140 As discussed in Section 2.3, proxy  \n141 free diffusion trains an unconditional   \n142 model and conditional models. Although   \n143 proxy-free diffusion can generate samples   \n144 aligned with most conditions, it tradition  \n145 ally lacks control due to the absence of   \n146 an explicit proxy. This is particularly sig  \n147 nificant in offline BBO where we aim to   \n148 obtain samples beyond the training dis  \n149 tribution. Therefore, we require explicit   \n150 proxy guidance to achieve enhanced sam  \n151 pling control. This module is outlined in   \n152 Algorithm 1, Line 8- Line 16.   \n153 Optimization of $\\omega$ . Directly updating   \n154 the design $\\pmb{x}_{t}$ with proxy gradient suffers   \n155 from the OOD issue and determining a   \n156 proper condition $y$ necessitates the man  \n157 ual adjustment of multiple hyperparame  \n158 ters [6]. Thus, we propose to introduce   \n159 proxy guidance by only optimizing the strength parameter   \n160 discussed in Section 2.3, the parameter $\\omega$ ", "page_idx": 3}, {"type": "text", "text": "", "page_idx": 3}, {"type": "table", "img_path": "93qSRpucpN/tmp/9ab365e8e4032fd9cccd86066fbed072dcf29f35c2d208dcb5f2346cdfba04e6.jpg", "table_caption": [], "table_footnote": [], "page_idx": 3}, {"type": "text", "text": "$\\omega$ within $\\tilde{\\mathbf{s}}_{\\theta}(x_{t},y,\\omega)$ in Eq. (6). As balances the condition and diversity, and an optimized $\\omega$ 161 could achieve a better balance in the sampling process, leading to more effective generation. ", "page_idx": 3}, {"type": "text", "text": "162 Enhanced Sampling. With the score function, the update of a noisy sample $x_{t+1}$ is computed as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{x}_{t}(\\omega)=s o l v e r(\\pmb{x}_{t+1},\\pmb{\\tilde{s}}_{\\theta}(\\pmb{x}_{t+1},y,\\omega)),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "163 where the solver is the second-order Heun solver [12], chosen for its enhanced accuracy through a   \n164 predictor-corrector method. A proxy is then trained to predict the property of noise $\\pmb{x}_{t}$ at time step $t$ ,   \n165 denoted as $J_{\\phi}(\\mathbf{\\boldsymbol{x}}_{t},t)$ . By maximizing $J_{\\phi}(\\mathbf{x}_{t}(\\omega),t)$ with respect to $\\omega$ , we can incorporate the explicit   \n166 proxy guidance into proxy-free diffusion to enable enhanced sampling control in the balance between   \n167 condition and diversity. This maximization process is: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\omega}=\\omega+\\eta\\frac{\\partial J_{\\phi}(\\pmb{x}_{t}(\\omega),t)}{\\partial\\omega}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "168 where $\\eta$ denotes the learning rate. We leverage the automatic differentiation capabilities of Py  \n169 Torch [13] to efficiently compute the above derivatives within the context of the solver\u2019s operation.   \n170 The optimized $\\hat{\\omega}$ then updates the noisy sample $x_{t+1}$ through: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\pmb{x}_{t}=s o l v e r(\\pmb{x}_{t+1},\\tilde{\\pmb{s}}_{\\theta}(\\pmb{x}_{t+1},y,\\hat{\\omega})).}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "171 This process iteratively denoises $\\pmb{x}_{t}$ , utilizing it in successive steps to progressively approach $\\scriptstyle x_{0}$ ,   \n172 which represents the final high-scoring design $x^{*}$ .   \n173 Proxy Training. Notably, $J_{\\phi}(\\mathbf{\\boldsymbol{x}}_{t},t)$ can be directly derived from the proxy $J_{\\phi}(x)$ , the mean of the   \n174 proxy distribution $p_{\\phi}(\\cdot|x)$ in Eq. (2). This distribution is trained exclusively at the initial time step   \n175 $t=0$ , eliminating the need for training across time steps. To achieve this derivation, we reverse the   \n176 diffusion from $\\pmb{x}_{t}$ back to $\\pmb{x}_{0}$ using the formula: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{\\boldsymbol{\\mathbf{\\boldsymbol{x}}}}_{0}=\\frac{\\mathbf{\\boldsymbol{x}}_{t}+s_{\\theta}(\\mathbf{\\boldsymbol{x}}_{t})\\cdot\\boldsymbol{\\sigma}(t)^{2}}{\\mu(t)},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "177 where $s_{\\theta}(\\mathbf{\\boldsymbol{x}}_{t})$ is the estimated unconditional score at time step $t$ , and $\\sigma(t)^{2}$ and $\\mu(t)$ are the variance   \n178 and mean functions of the perturbation kernel at time $t$ , as detailed in equations (32-33) in [10].   \n179 Consequently, we express ", "page_idx": 4}, {"type": "equation", "text": "$$\nJ_{\\phi}(\\mathbf{x}_{t},t)=J_{\\phi}\\left(\\frac{\\mathbf{x}_{t}+s_{\\theta}(\\mathbf{x}_{t})\\cdot\\boldsymbol{\\sigma}(t)^{2}}{\\mu(t)}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "180 This formulation allows for the optimization of the strength parameter $\\omega$ via Eq. (8). For simplicity,   \n181 we will refer to $J_{\\phi}(\\cdot)$ in subsequent discussions. ", "page_idx": 4}, {"type": "text", "text": "182 3.2 Diffusion-based Proxy Refinement ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "183 In the proxy-enhanced sampling module, the proxy $J_{\\phi}(\\cdot)$ is employed to update the parameter $\\omega$   \n184 to enable enhanced control. However, $J_{\\phi}(\\cdot)$ may still be prone to the OOD issue, especially on   \n185 adversarial samples [5]. To address this, we refine the proxy by using insights from proxy-free   \n186 diffusion. The procedure of this module is specified in Algorithm 1, Lines 3-7.   \n187 Diffusion Distribution. Adversarial samples are identified by gradient ascent on the proxy as per   \n188 Eq. (3) to form the distribution $q(x)$ . Consequently, these samples are vulnerable to the proxy   \n189 distribution. Conversely, the proxy-free diffusion, which functions without depending on a proxy,   \n190 inherently offers greater resilience against these samples, thus producing a more robust distribution.   \n191 For an adversarial sample ${\\hat{x}}\\sim q(x)$ , we compute $p_{\\pmb{\\theta}}(\\hat{\\pmb{x}}),p_{\\pmb{\\theta}}(\\hat{\\pmb{x}}|\\hat{y})$ via the probability flow ODE, and   \n192 $p(y)$ through Gaussian kernel-density estimation. The diffusion distribution regarding $y$ is derived as: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\np_{\\pmb{\\theta}}(y|\\hat{\\pmb x})=\\frac{p_{\\pmb{\\theta}}(\\hat{\\pmb x}|y)\\cdot p(y)}{p_{\\pmb{\\theta}}(\\hat{\\pmb x})},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "193 which demonstrates inherent robustness over the proxy distribution $p_{\\phi}(y|\\hat{\\pmb x})$ . Yet, directly applying   \n194 diffusion distribution to design optimization by gradient ascent is computationally intensive and   \n195 potentially unstable due to the demands of reversing ODEs and scoring steps.   \n196 Proxy Refinement. We opt for a more feasible approach: refine the proxy distribution $p_{\\phi}(y|\\hat{\\pmb x})=$   \n197 $\\mathcal{N}(J_{\\phi}(\\hat{\\pmb x}),\\sigma_{\\phi}(\\hat{\\pmb x}))$ by minimizing its distance to the diffusion distribution $p_{\\pmb{\\theta}}(y|\\hat{\\pmb{x}})$ . The distance is   \n198 quantified by the Kullback-Leibler (KL) divergence: ", "page_idx": 4}, {"type": "text", "text": "", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}_{q}[D(p_{\\phi}||p_{\\theta})]=\\mathbb{E}_{q(x)}\\overset{\\subset}{\\int}p_{\\phi}(y|\\hat{x})\\log\\left(\\frac{p_{\\phi}(y|\\hat{x})}{p_{\\theta}(y|\\hat{x})}\\right)d y.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "199 We avoid the parameterization trick for minimizing this divergence as it necessitates backpropagation   \n200 through $p_{\\pmb{\\theta}}(y|\\hat{\\pmb{x}})$ , which is prohibitively expensive. Instead, for the sample $\\hat{\\pmb{x}}$ , the gradient of the KL   \n201 divergence $\\mathcal{D}(p_{\\phi}||p_{\\theta})$ with respect to the proxy parameters $\\phi$ is computed as: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}_{p_{\\phi}(y|\\hat{\\pmb x})}\\left[\\frac{d\\log p_{\\phi}(y|\\hat{\\pmb x})}{d\\phi}\\left(1+\\log\\frac{p_{\\phi}(y|\\hat{\\pmb x})}{p_{\\theta}(y|\\hat{\\pmb x})}\\right)\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "202 Complete derivations are in Appendix A. The KL divergence then acts as regularization in our loss $\\mathcal{L}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathcal{L}(\\phi,\\alpha)=\\mathbb{E}_{\\mathcal{D}}[-\\log p_{\\phi}(\\boldsymbol{y}|\\boldsymbol{x})]+\\alpha\\mathbb{E}_{\\boldsymbol{q}(\\boldsymbol{x})}[\\mathcal{D}(p_{\\phi}||p_{\\theta})],\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "203 where $\\mathcal{D}$ is the training dataset and $\\alpha$ is a hyperparameter. We propose to optimize $\\alpha$ based on the   \n204 validation loss via bi-level optimization as detailed in Appendix $\\mathbf{B}$ . ", "page_idx": 4}, {"type": "text", "text": "205 4 Experiments ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "206 In this section, we conduct comprehensive experiments to evaluate our method\u2019s performance. ", "page_idx": 4}, {"type": "text", "text": "207 4.1 Benchmarks ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "208 Tasks. Our experiments encompass a variety of tasks, split into continuous and discrete categories. ", "page_idx": 5}, {"type": "text", "text": "209 The continuous category includes four tasks: (1) Superconductor (SuperC) 4: The objective here   \n210 is to engineer a superconductor composed of 86 continuous elements. The goal is to enhance the   \n211 critical temperature using 17, 010 design samples. This task is based on the dataset from [1]. (2) Ant   \n212 Morphology (Ant): In this task, the focus is on developing a quadrupedal ant robot, comprising 60   \n213 continuous parts, to augment its crawling velocity. It uses 10, 004 design instances from the dataset   \n214 in [3, 14]. (3) D\u2019Kitty Morphology (D\u2019Kitty): Similar to Ant Morphology, this task involves the   \n215 design of a quadrupedal D\u2019Kitty robot with 56 components, aiming to improve its crawling speed   \n216 with 10, 004 designs, as described in [3, 15]. (4) Rosenbrock (Rosen): The aim of this task is to   \n217 optimize a 60-dimension continuous vector to maximize the Rosenbrock black-box function. It uses   \n218 50000 designs from the low-scoring part [9].   \n219 For the discrete category, we explore three tasks: (1) TF Bind 8 (TF8): The goal is to identify an   \n220 8-unit DNA sequence that maximizes binding activity. This task uses 32, 898 designs and is detailed   \n221 in [16]. (2) TF Bind 10 (TF10): Similar to TF8, but with a 10-unit DNA sequence and a larger pool   \n222 of 50, 000 samples, as described in [16]. (3) Neural Architecture Search (NAS): This task focuses   \n223 on discovering the optimal neural network architecture to improve test accuracy on the CIFAR-10   \n224 dataset, using 1, 771 designs [17].   \n225 Evaluation. In this study, we utilize the oracle evaluation from design-bench [3]. Adhering to this   \n226 established protocol, we analyze the top 128 promising designs from each method. The evaluation   \n227 metric employed is the $100^{t h}$ percentile normalized ground-truth score, calculated using the formula   \n228 $\\begin{array}{r}{y_{n}~=~\\frac{y-y_{\\mathrm{min}}}{y_{\\mathrm{max}}-y_{\\mathrm{min}}}}\\end{array}$ , where $y_{\\mathrm{{min}}}$ and $y_{\\mathrm{{max}}}$ signify the lowest and highest scores respectively in the   \n229 comprehensive, yet unobserved, dataset. In addition to these scores, we provide an overview of each   \n230 method\u2019s effectiveness through the mean and median rankings across all evaluated tasks. Notably,   \n231 the best design discovered in the offilne dataset, designated as $\\mathbf{\\mathcal{D}(b e s t)}$ , is also included for reference.   \n232 For further details on the $50^{t h}$ percentile (median) scores, please refer to Appendix C. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "233 4.2 Comparison Methods ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "234 Our approach is evaluated against two primary groups of baseline methods: forward and inverse   \n235 approaches. Forward approaches enhance existing designs through gradient ascent. This includes: (i)   \n236 Grad: utilizes simple gradient ascent on current designs for new creations; (ii) ROMA [18]: imple  \n237 ments smoothness regularization on proxies; (iii) COMs [5]: applies regularization to assign lower   \n238 scores to adversarial designs; (iv) NEMO [19]: bridges the gap between proxy and actual functions   \n239 using normalized maximum likelihood; (v) BDI [20]: utilizes both forward and inverse mappings to   \n240 transfer knowledge from offilne datasets to the designs; (vi) IOM [21]: ensures consistency between   \n241 representations of training datasets and optimized designs.   \n242 Inverse approaches focus on learning a mapping from a design\u2019s property value back to its input.   \n243 High property values are input into this inverse mapping to yield enhanced designs. This includes: (i)   \n244 CbAS [22]: CbAS employs a VAE model to implicitly implement the inverse mapping. It gradually   \n245 tunes its distribution toward higher scores by raising the scoring threshold. This process can be   \n246 interpreted as incrementally increasing the conditional score within the inverse mapping framework.   \n247 (ii) Autofocused CbAS (Auto.CbAS) [23]: adopts importance sampling for retraining a regression   \n248 model based on CbAS. (iii) MIN [6]: maps scores to designs via a GAN model and explore this   \n249 mapping for optimal designs. (iv) BONET [24]: introduces an autoregressive model for sampling   \n250 high-scoring designs. (v) DDOM [4]: utilizes proxy-free diffusion to model the inverse mapping.   \n251 Traditional methods as detailed in [3] are also considered: (i) CMA-ES [25]: modifies the covariance   \n252 matrix to progressively shift the distribution towards optimal designs; (ii) BO-qEI [26]: implements   \n253 Bayesian optimization to maximize the proxy and utilizes the quasi-Expected-Improvement acqui  \n254 sition function for design suggestion, labeling designs using the proxy; (iii) REINFORCE [27]:   \n255 enhances the input space distribution using the learned proxy model.   \n257 In alignment with the experimental protocols established in [3, 20], we have tailored our training   \n258 methodologies for all approaches, except where specified otherwise. For methods such as BO-qEI,   \n259 CMA-ES, REINFORCE, CbAS, and Auto.CbAS that do not utilize gradient ascent, we base our   \n260 approach on the findings reported in [3]. We adopted $T=1000$ diffusion sampling steps, set the   \n261 condition $y$ to $y_{m a x}$ , and initial strength $\\omega$ as 2 in line with [4]. To ensure reliability and consistency in   \n262 our comparative analysis, each experimental setting was replicated across 8 independent runs, unless   \n263 stated otherwise, with the presentation of both mean values and standard errors. These experiments   \n264 were conducted using a NVIDIA GeForce V100 GPU. We\u2019ve detailed the computational overhead of   \n265 our approach in Appendix D to provide a comprehensive view of its practicality. ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "table", "img_path": "93qSRpucpN/tmp/16359cf8858648b20dc1487a4df12970b4fb4b2739ccdc3d159a9d9552064828.jpg", "table_caption": ["Table 1: Results (maximum normalized score) on continuous tasks. "], "table_footnote": [], "page_idx": 6}, {"type": "table", "img_path": "93qSRpucpN/tmp/a000831b2a3f3e98338013dcd42987e8f8a206d0bf704b43dd4419287997ec96.jpg", "table_caption": ["Table 2: Results (maximum normalized score) on discrete tasks & ranking on all tasks. "], "table_footnote": [], "page_idx": 6}, {"type": "text", "text": "266 4.4 Results and Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "267 In Tables 1 and 2, we showcase our experimental results for both continuous and discrete tasks.   \n268 To clearly differentiate among the various approaches, distinct lines separate traditional, forward,   \n269 and inverse approaches within the tables For every task, algorithms performing within a standard   \n270 deviation of the highest score are emphasized by bolding following [5].   \n271 We make the following observations. (1) As highlighted in Table 2, RGD not only achieves the top   \n272 rank but also demonstrates the best performance in six out of seven tasks, emphasizing the robustness   \n273 and superiority of our method. (2) RGD outperforms the VAE-based CbAS, the GAN-based MIN   \n274 and the Transformer-based BONET. This result highlights the superiority of diffusion models in   \n275 modeling inverse mappings compared to other generative approaches. (3) Upon examining TF   \n276 Bind 8, we observe that the average rankings for forward and inverse methods stand at 10.3 and   \n277 6.0, respectively. In contrast, for TF Bind 10, both methods have the same average ranking of 8.7,   \n278 indicating no advantage. This notable advantage of inverse methods in TF Bind 8 implies that the   \n279 relatively smaller design space of TF Bind 8 $(4^{8}\\overline{{)}}$ facilitates easier inverse mapping, as opposed to the   \n280 more complex space in TF Bind 10 $(4^{10})$ . (4) RGD\u2019s performance is less impressive on NAS, where   \n281 designs are encoded as 64-length sequences of 5-category one-hot vectors. This may stem from   \n282 the design-bench\u2019s encoding not fully capturing the sequential and hierarchical aspects of network   \n283 architectures, affecting the efficacy of inverse mapping modeling. ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "table", "img_path": "93qSRpucpN/tmp/dcb3f2f5e711f11577a5b60aa62e31b5837a962dbce10d338429f4251a9daa4b.jpg", "table_caption": ["Table 3: Ablation studies on RGD. "], "table_footnote": [], "page_idx": 7}, {"type": "text", "text": "284 4.5 Ablation Studies ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "285 In this section, we present a series of ablation studies to scrutinize the individual contributions of   \n286 distinct components in our methodology. We employ our proposed approach as a benchmark and   \n287 methodically exclude key modules, such as the proxy-enhanced sampling and diffusion-based proxy   \n288 refinement, to assess their influence on performance. These variants are denoted as w/o proxy- $\\cdot e$ and   \n289 w/o diffusion- $.b\\;r.$ . Additionally, we explore the strategy of directly performing gradient ascent on   \n290 the diffusion intermediate state, referred to as direct grad update. The results from these ablation   \n291 experiments are detailed in Table 3.   \n292 Our analysis reveals that omitting either module results in a decrease in performance, thereby affirming   \n293 the importance of each component. The w/o diffusion- $_{b\\;r}$ variant generally surpasses w/o proxy-e,   \n294 highlighting the utility of the proxy-enhanced sampling even with a basic proxy setup. Conversely,   \n295 direct grad update tends to produce subpar results across tasks, likely attributable to the proxy\u2019s   \n296 limitations in handling out-of-distribution samples, leading to suboptimal design optimizations.   \n297 To further dive into the proxy-enhanced sam  \n298 pling module, we visualize the strength ra  \n299 tio $\\omega/\\omega_{0}$ \u2014where $\\omega_{0}$ represents the initial   \n300 strength\u2014across diffusion steps $t$ . This analysis   \n301 is depicted in Figure 3 for two specific tasks:   \n302 Ant and TF10. We observe a pattern of initial   \n303 decrease followed by an increase in $\\omega$ across   \n304 both tasks. This pattern can be interpreted as   \n305 follows: The decrease in $\\omega$ facilitates the genera  \n306 tion of a more diverse set of samples, enhancing   \n307 exploratory capabilities. Subsequently, the in  \n308 crease in $\\omega$ signifies a shift towards integrating   \n309 high-performance features into the sample gen  \n310 eration. Within this context, conditioning on   \n311 the maximum $y$ is not aimed at achieving the   \n312 dataset\u2019s maximum but at enriching samples with high-scoring attributes. Overall, this adjustment of   \n313 $\\omega$ effectively balances between generating novel solutions and honing in on high-quality ones.   \n314 In addition, we visualize the proxy distribution alongside the diffusion distribution for a sample $\\hat{\\pmb{x}}$   \n315 from the Ant task in Figure 4, to substantiate the efficacy of diffusion-based proxy refinement. The   \n316 proxy distribution significantly overestimates the ground truth, whereas the diffusion distribution   \n317 closely aligns with it, demonstrating the robustness of diffusion distribution. For a more quantitative   \n318 analysis, we compute the expectation of both distributions and compare them with the ground   \n319 truth. The mean of the diffusion distribution is calculated as $\\begin{array}{r}{\\mathbb{E}_{p_{\\theta}(y|\\hat{\\pmb x})}[y]\\,=\\,\\mathbb{E}_{p_{\\phi}(y|\\hat{\\pmb x})}\\,\\left[\\frac{p_{\\theta}(y|\\hat{\\pmb x})}{p_{\\phi}(y|\\hat{\\pmb x})}y\\right]}\\end{array}$ . ", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "image", "img_path": "93qSRpucpN/tmp/a6c6bd5f11fdf9943e62e4d5ee96917cb3b49731652f1a521afb089fc1e6d080.jpg", "img_caption": ["Figure 3: Dynamics of strength ratio $\\omega/\\omega_{0}$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "", "page_idx": 8}, {"type": "image", "img_path": "93qSRpucpN/tmp/bcd9c795196653c11ff7b9b977ac923d180fa82baa8c3ec2dae4b68f96b98eb9.jpg", "img_caption": [], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "334 4.6 Hyperparameter Sensitivity Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "The MSE loss for the proxy distribution is 2.88, while for the diffusion distribution, it is 0.13 on the Ant task. Additionally, we evaluate this on the TFB10 task, where the MSE loss for the proxy distribution is 323.63 compared to 0.82 for the diffusion distribution. These results further corroborate the effectiveness of our proposed module. ", "page_idx": 8}, {"type": "text", "text": "Furthermore, we (1) investigate the impact of replacing our trained proxy model with alternative approaches, specifically ROMA and COMs, (2) analyze the performance with an optimized condition $y$ and (3) explore a simple annealing approach of $\\omega$ . For a comprehensive discussion on these, readers are referred to Appendix E. ", "page_idx": 8}, {"type": "text", "text": "335 This section investigates the sensitivity of $R G D$ to various hyperparameters. Specifically, we analyze   \n336 the effects of (1) the number of diffusion sampling steps $T$ , (2) the condition $y$ , and (3) the learning   \n337 rate $\\eta$ of the proxy-enhanced sampling. These parameters are evaluated on two tasks: the continuous   \n338 Ant task and the discrete TFB10 task. For a detailed discussion, see Appendix F. ", "page_idx": 8}, {"type": "text", "text": "339 5 Related Work ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "340 Offline black-box optimization. A recent surge in research has presented two predominant ap  \n341 proaches for offilne BBO. The forward approach deploys a DNN to fti the offilne dataset, subsequently   \n342 utilizing gradient ascent to enhance existing designs. Typically, these techniques, including COMs [5],   \n343 ROMA [18], NEMO [19], BDI [20, 28], IOM [29] and Parallel-mentoring [30], are designed to   \n344 embed prior knowledge within the surrogate model to alleviate the OOD issue. The reverse ap  \n345 proach [6, 31] is dedicated to learning a mapping from property values back to inputs. Feeding a high   \n346 value into this inverse mapping directly produces a design of elevated performance. Additionally,   \n347 methods in [22, 23] progressively tailor a generative model towards the optimized design via a proxy   \n348 function and BONET [24] introduces an autoregressive model trained on fixed-length trajectories to   \n349 sample high-scoring designs. Recent investigations [4] have underscored the superiority of diffusion   \n350 models in delineating the inverse mapping. However, research on specialized guided diffusion for   \n351 offline BBO remains limited. This paper addresses this research gap.   \n352 Guided diffusion. Guided diffusion seeks to produce samples with specific desirable attributes.   \n353 Contemporary research in guided diffusion primarily concentrates on enhancing the efficiency of   \n354 its sampling process. [32] propose a method for distilling a classifier-free guided diffusion model   \n355 into a more efficient single model that necessitates fewer steps in sampling. [33] introduce an   \n356 operator splitting method to expedite classifier guidance by separating the update process into two   \n357 key functions: the diffusion function and the conditioning function. Additionally, [34] presents an   \n358 efficient and universal guidance mechanism that utilizes a readily available proxy to enable diffusion   \n359 guidance across time steps. In this work, we explore the application of guided diffusion in offline   \n360 BBO, with the goal of creating tailored algorithms to efficiently generate high-performance designs. ", "page_idx": 8}, {"type": "text", "text": "", "page_idx": 8}, {"type": "text", "text": "361 6 Conclusion ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "362 In conclusion, we propose Robust Guided Diffusion for Offilne Black-box Optimization (RGD). The   \n363 proxy-enhanced sampling module adeptly integrates proxy guidance to enable enhanced sampling   \n364 control, while the diffusion-based proxy refinement module leverages proxy-free diffusion insights   \n365 for proxy improvement. Empirical evaluations on design-bench have showcased RGD\u2019s outstanding   \n366 performance, further validated by ablation studies on the contributions of these novel components.   \n367 We discuss the broader impact and limitation in Appendix G. ", "page_idx": 8}, {"type": "text", "text": "368 References ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "369 [1] Kam Hamidieh. A data-driven statistical model for predicting the critical temperature of a   \n370 superconductor. Computational materials science, 2018.   \n371 [2] Karen S Sarkisyan et al. Local ftiness landscape of the green fluorescent protein. Nature, 2016.   \n372 [3] Brandon Trabucco, Xinyang Geng, Aviral Kumar, and Sergey Levine. Design-Bench: bench  \n373 marks for data-driven offline model-based optimization. arXiv preprint arXiv:2202.08450,   \n374 2022.   \n375 [4] Siddarth Krishnamoorthy, Satvik Mehul Mashkaria, and Aditya Grover. Diffusion models for   \n376 black-box optimization. Proc. Int. Conf. Machine Learning (ICML), 2023.   \n377 [5] Brandon Trabucco, Aviral Kumar, Xinyang Geng, and Sergey Levine. Conservative objective   \n378 models for effective offline model-based optimization. In Proc. Int. Conf. Machine Learning   \n379 (ICML), 2021.   \n380 [6] Aviral Kumar and Sergey Levine. Model inversion networks for model-based optimization.   \n381 Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS), 2020.   \n382 [7] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil   \n383 Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Proc. Adv. Neur.   \n384 Inf. Proc. Syst (NeurIPS), 2014.   \n385 [8] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint   \n386 arXiv:2207.12598, 2022.   \n387 [9] HoHo Rosenbrock. An automatic method for finding the greatest or least value of a function.   \n388 The computer journal, 1960.   \n389 [10] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and   \n390 Ben Poole. Score-based generative modeling through stochastic differential equations. Proc.   \n391 Int. Conf. Learning Rep. (ICLR), 2021.   \n392 [11] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Proc.   \n393 Adv. Neur. Inf. Proc. Syst (NeurIPS), 2021.   \n394 [12] Endre S\u00fcli and David F Mayers. An introduction to numerical analysis. Cambridge university   \n395 press, 2003.   \n396 [13] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,   \n397 Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: an imperative   \n398 style, high-performance deep learning library. Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS), 2019.   \n399 [14] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,   \n400 and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.   \n401 [15] Michael Ahn, Henry Zhu, Kristian Hartikainen, Hugo Ponte, Abhishek Gupta, Sergey Levine,   \n402 and Vikash Kumar. Robel: robotics benchmarks for learning with low-cost robots. In Conf. on   \n403 Robot Lea. (CoRL), 2020.   \n404 [16] Luis A Barrera et al. Survey of variation in human transcription factors reveals prevalent DNA   \n405 binding changes. Science, 2016.   \n406 [17] Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. arXiv   \n407 preprint arXiv:1611.01578, 2017.   \n408 [18] Sihyun Yu, Sungsoo Ahn, Le Song, and Jinwoo Shin. Roma: robust model adaptation for offilne   \n409 model-based optimization. Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS), 2021.   \n410 [19] Justin Fu and Sergey Levine. Offline model-based optimization via normalized maximum   \n411 likelihood estimation. Proc. Int. Conf. Learning Rep. (ICLR), 2021.   \n412 [20] Can Chen, Yingxue Zhang, Jie Fu, Xue Liu, and Mark Coates. Bidirectional learning for offilne   \n413 infinite-width model-based optimization. In Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS), 2022.   \n414 [21] Han Qi, Yi Su, Aviral Kumar, and Sergey Levine. Data-driven model-based optimization via   \n415 invariant representation learning. In Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS), 2022.   \n416 [22] David Brookes, Hahnbeom Park, and Jennifer Listgarten. Conditioning by adaptive sampling   \n417 for robust design. In Proc. Int. Conf. Machine Learning (ICML), 2019.   \n418 [23] Clara Fannjiang and Jennifer Listgarten. Autofocused oracles for model-based design. Proc.   \n419 Adv. Neur. Inf. Proc. Syst (NeurIPS), 2020.   \n420 [24] Satvik Mehul Mashkaria, Siddarth Krishnamoorthy, and Aditya Grover. Generative pretraining   \n421 for black-box optimization. In Proc. Int. Conf. Machine Learning (ICML), 2023.   \n422 [25] Nikolaus Hansen. The CMA evolution strategy: a comparing review. Towards A New Evolu  \n423 tionary Computation, 2006.   \n424 [26] James T Wilson, Riccardo Moriconi, Frank Hutter, and Marc Peter Deisenroth. The reparame  \n425 terization trick for acquisition functions. arXiv preprint arXiv:1712.00424, 2017.   \n426 [27] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforce  \n427 ment learning. Machine learning, 1992.   \n428 [28] Can Chen, Yingxue Zhang, Xue Liu, and Mark Coates. Bidirectional learning for offline   \n429 model-based biological sequence design. In Proc. Int. Conf. Machine Lea. (ICML), 2023.   \n430 [29] Han Qi, Yi Su, Aviral Kumar, and Sergey Levine. Data-driven model-based optimization via   \n431 invariant representation learning. In Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS), 2022.   \n432 [30] Can Chen, Christopher Beckham, Zixuan Liu, Xue Liu, and Christopher Pal. Parallel-mentoring   \n433 for offline model-based optimization. In Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS), 2023.   \n434 [31] Alvin Chan, Ali Madani, Ben Krause, and Nikhil Naik. Deep extrapolation for attribute  \n435 enhanced generation. Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS), 2021.   \n436 [32] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho,   \n437 and Tim Salimans. On distillation of guided diffusion models. In Proc. Comp. Vision. Pattern.   \n438 Rec.(CVPR), 2023.   \n439 [33] Suttisak Wizadwongsa and Supasorn Suwajanakorn. Accelerating guided diffusion sampling   \n440 with splitting numerical methods. In Proc. Int. Conf. Learning Rep. (ICLR), 2023.   \n441 [34] Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum,   \n442 Jonas Geiping, and Tom Goldstein. Universal guidance for diffusion models. In Proc. Comp.   \n443 Vision. Pattern. Rec.(CVPR), 2023.   \n444 [35] Can Chen, Yingxue Zhang, Jie Fu, Xue Liu, and Mark Coates. Bidirectional learning for offilne   \n445 infinite-width model-based optimization. Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS), 2022. ", "page_idx": 9}, {"type": "text", "text": "", "page_idx": 10}, {"type": "text", "text": "446 A Derivation ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "447 This section provides a derivation of the gradient of the $\\mathrm{KL}$ divergence. Let\u2019s consider the KL   \n448 divergence term, defined as: ", "page_idx": 11}, {"type": "equation", "text": "$$\n{\\mathcal{D}}(p_{\\phi}||p_{\\theta})=\\int p_{\\phi}(y|{\\hat{\\pmb x}})\\log\\left({\\frac{p_{\\phi}(y|{\\hat{\\pmb x}})}{p_{\\theta}(y|{\\hat{\\pmb x}})}}\\right)d y.\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "449 The gradient with respect to the parameters $\\phi$ is computed as follows: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{d D(p_{\\phi}||p_{\\theta})}{d\\phi}=\\int\\frac{d p_{\\phi}(y|\\hat{x})}{d\\phi}\\left(1+\\log\\frac{p_{\\phi}(y|\\hat{x})}{p_{\\theta}(y|\\hat{x})}\\right)d y}\\\\ &{\\qquad\\qquad=\\int p_{\\phi}(y|\\hat{x})\\frac{d\\log p_{\\phi}(y|\\hat{x})}{d\\phi}(1+\\log\\frac{p_{\\phi}(y|\\hat{x})}{p_{\\theta}(y|\\hat{x})})\\,d y}\\\\ &{\\qquad\\qquad=\\mathbb{E}_{p_{\\phi}(y|\\hat{x})}\\left[\\frac{d\\log p_{\\phi}(y|\\hat{x})}{d\\phi}\\left(1+\\log\\frac{p_{\\phi}(y|\\hat{x})}{p_{\\theta}(y|\\hat{x})}\\right)\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "450 B Hyperparameter Optimization ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "451 We propose adjusting $\\alpha$ based on the validation loss, establishing a bi-level optimization framework: ", "page_idx": 11}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\begin{array}{c}{\\alpha^{*}=\\arg\\operatorname*{min}_{\\boldsymbol{\\alpha}}\\mathbb{E}_{\\mathcal{D}_{v}}\\big[\\!\\log p_{\\phi^{*}(\\boldsymbol{\\alpha})}(y_{v}|\\mathbf{x}_{v})\\big],}\\\\ {\\mathrm{s.t.}\\quad\\phi^{*}(\\boldsymbol{\\alpha})=\\underset{\\phi}{\\arg\\operatorname*{min}}\\,\\mathcal{L}(\\phi,\\boldsymbol{\\alpha}).}\\end{array}}\\end{array}\n$$", "text_format": "latex", "page_idx": 11}, {"type": "text", "text": "452 Within this context, $\\mathcal{D}_{v}$ represents the validation dataset sampled from the offilne dataset. The inner   \n453 optimization task, which seeks the optimal $\\phi^{*}(\\alpha)$ , is efficiently approximated via gradient descent. ", "page_idx": 11}, {"type": "text", "text": "454 C Evaluation of Median Scores ", "text_level": 1, "page_idx": 11}, {"type": "text", "text": "455 While the main text of our paper focuses on the $100^{t h}$ percentile scores, this section provides an   \n456 in-depth analysis of the $50^{t h}$ percentile scores. These median scores, previously explored in [3], serve   \n457 as an additional metric to assess the performance of our $R G D$ method. The outcomes for continuous   \n458 tasks are detailed in Table 5, and those pertaining to discrete tasks, along with their respective ranking   \n459 statistics, are outlined in Table 6. An examination of Table 6 highlights the notable success of the   \n460 $R G D$ approach, as it achieves the top rank in this evaluation. This finding underscores the method\u2019s   \n461 robustness and effectiveness. ", "page_idx": 11}, {"type": "text", "text": "462 D Computational Overhead ", "text_level": 1, "page_idx": 11}, {"type": "table", "img_path": "93qSRpucpN/tmp/ed2aa81b943f3ee4af7c259bac7be2bfc01469b000bbd1eeb31d51352b15c82c.jpg", "table_caption": ["Table 4: Computational Overhead (in seconds). "], "table_footnote": [], "page_idx": 11}, {"type": "text", "text": "463 In this section, we analyze the computational overhead of our method. RGD consists of two core   \n464 components: proxy-enhanced sampling (proxy-e sampling) and diffusion-based proxy refinement   \n465 (diffusion-b proxy r). Additionally, RGD employs a trained proxy and a proxy-free diffusion model,   \n466 whose computational demands are denoted as proxy training and diffusion training, respectively.   \n467 Table 4 indicates that experiments can be completed within approximately one hour, demonstrating ef  \n468 ficiency. The diffusion-based proxy refinement module is the primary contributor to the computational   \n469 overhead, primarily due to the usage of a probability flow ODE for sample likelihood computation.   \n470 However, as this is a one-time process for refining the proxy, its high computational cost is offset by its   \n471 non-recurring nature. In contexts such as robotics or bio-chemical research, the most time-intensive   \n472 part of the production cycle is usually the evaluation of the unknown objective function. Therefore,   \n473 the time differences between methods for deriving high-performance designs are less critical in   \n474 actual production environments, highlighting RGD\u2019s practicality where optimization performance   \n475 are prioritized over computational speed. This aligns with recent literature (A.3 Computational   \n476 Complexity in [35] and A.7.5. Computational Cost in [28]) indicating that in black-box optimization   \n477 scenarios, computational time is relatively minor compared to the time and resources dedicated to   \n478 experimental validation phases. ", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "table", "img_path": "93qSRpucpN/tmp/686a7920f8535bbc84097ab346dc37bbb42a9993934539174541003a6e9c91d6.jpg", "table_caption": ["Table 5: Results (median normalized score) on continuous tasks. "], "table_footnote": [], "page_idx": 12}, {"type": "table", "img_path": "93qSRpucpN/tmp/1f498250976b95c6a601768910aeac0736b97f162bbceeea275616af7cd35f40.jpg", "table_caption": ["Table 6: Results (median normalized score) on discrete tasks & ranking on all tasks. "], "table_footnote": [], "page_idx": 12}, {"type": "text", "text": "479 E Further Ablation Studies ", "text_level": 1, "page_idx": 12}, {"type": "text", "text": "480 In this section, we extend our exploration to include alternative proxy refinement schemes, namely   \n481 ROMA and COMs, to compare against our diffusion-based proxy refinement module. The objective   \n482 is to assess the relative effectiveness of these schemes in the context of the Ant and TFB10 tasks.   \n483 The comparative results are presented in Table 7. Our investigation reveals that proxies refined   \n484 through ROMA and COMs exhibit performance akin to the vanilla proxy and they fall short of   \n485 achieving the enhancements seen with our diffusion-based proxy refinement. We hypothesize that   \n486 the diffusion-based proxy refinement, by aligning closely with the characteristics of the diffusion   \n487 model, provides a more relevant and impactful signal. This alignment improves the proxy\u2019s ability to   \n488 enhance the sampling process more effectively.   \n489 Additionally, we contrast our approach, which adjusts the strength parameter $\\omega$ , with the MIN method   \n490 that focuses on identifying an optimal condition $y$ . The MIN strategy entails optimizing a Lagrangian   \n491 objective with respect to $y$ , a process that requires manual tuning of four hyperparameters. We   \n492 adopt their methodology to determine optimal conditions $y$ and incorporate these into the proxy-free   \n493 diffusion for tasks Ant and TF10. The normalized scores for Ant and TF10 are $0.950\\pm0.017$ and   \n494 $0.660\\pm0.027$ , respectively. The outcomes fall short of those achieved by our method as detailed   \n495 in Table 7. This discrepancy likely stems from the complexity involved in optimizing $y$ , whereas   \n496 dynamically adjusting $\\omega$ proves to be a more efficient strategy for enhancing sampling control. ", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "table", "img_path": "93qSRpucpN/tmp/a50051b467f1e56357764473148ab442ec7050bf7eae50224cca43d4d304c15a.jpg", "table_caption": ["Table 7: Comparative Results of Proxy Integration with COMs, ROMA, and ours. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "497 Last but not least, we explore simple annealing approaches for $\\omega$ . Specifically, we test two annealing 498 scenarios considering the default $\\omega$ as 2.0: (1) a decrease from 4.0 to 0.0, and (2) an increase from 499 0.0 to 4.0, both modulated by a cosine function over the time step $\\mathit{\\Omega}(t)$ . We apply these strategies to the Ant Morphology and TF Bind 10 tasks, and the results are as follows: ", "page_idx": 13}, {"type": "table", "img_path": "93qSRpucpN/tmp/fb14ae22adc51645194494e8c447f97b5ce8f671ae18e4dd97104acb1ea0be28.jpg", "table_caption": ["Table 8: Results of Annealing Approaches. "], "table_footnote": [], "page_idx": 13}, {"type": "text", "text": "500   \n501 The empirical results across both strategies illustrate their inferior performance compared to our   \n502 approach, thereby demonstrating the efficacy of our proposed method. ", "page_idx": 13}, {"type": "text", "text": "503 F Hyperparameter Sensitivity Analysis ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "504 RGD\u2019s performance is assessed under different settings of $T$ , $y$ , and $\\eta$ . We experiment with $T$ values   \n505 of 500, 750, 1000, 1250, and 1500, with the default being $T=1000$ . For the condition ratio $y/y_{m a x}$ ,   \n506 we test values of 0.5, 1.0, 1.5, 2.0, and 2.5, considering 1.0 as the default. Similarly, for the learning   \n507 rate $\\eta$ , we explore values of $2.5e^{-3}$ , $5.0e^{-3}$ , 0.01, 0.02, and 0.04, with the default set to $\\eta=0.01$ .   \n508 Results are normalized by comparing them with the performance obtained at default values.   \n509 As depicted in Figures 5, 6, and 7, RGD demonstrates considerable resilience to hyperparameter   \n510 variations. The Ant task, in particular, exhibits a more marked sensitivity, with a gradual enhancement   \n511 in performance as these hyperparameters are varied. The underlying reasons for this trend include:   \n512 (1) An increase in the number of diffusion steps $(T)$ enhances the overall quality of the generated   \n513 samples. This improvement, in conjunction with more effective guidance from the trained proxy,   \n514 leads to better results. (2) Elevating the condition $(y)$ enables the diffusion model to extend its reach   \n515 beyond the existing dataset, paving the way for superior design solutions. However, selecting an   \n516 optimal $y$ can be challenging and may, as observed in the TFB10 task, sometimes lead to suboptimal   \n517 results. (3) A higher learning rate $(\\eta)$ integrates an enhanced guidance signal from the trained proxy,   \n518 contributing to improved performances.   \n519 In contrast, the discrete nature of the TFB10 task seems to endow it with a certain robustness   \n520 to variations in these hyperparameters, highlighting a distinct behavioral pattern in response to   \n521 hyperparameter adjustments. ", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 13}, {"type": "image", "img_path": "93qSRpucpN/tmp/ce70365f2c7fb176dc244694d3ee22908f0b6ea9769bcf34f010def78fb997d2.jpg", "img_caption": [], "img_footnote": [], "page_idx": 14}, {"type": "text", "text": "522 G Broader Impact and Limitation ", "text_level": 1, "page_idx": 14}, {"type": "text", "text": "523 Broader impact. Our research has the potential to significantly accelerate advancements in fields such   \n524 as new material development, biomedical innovation, and robotics technology. These advancements   \n525 could lead to breakthroughs with substantial positive societal impacts. However, we recognize that,   \n526 like any powerful tool, there are inherent risks associated with the misuse of this technology. One   \n527 concerning possibility is the exploitation of our optimization techniques to design objects or entities   \n528 for malicious purposes, including the creation of more efficient weaponry or harmful biological agents.   \n529 Given these potential risks, it is imperative to enforce strict safeguards and regulatory measures,   \n530 especially in areas where the misuse of technology could lead to significant ethical and societal harm.   \n531 The responsible application and governance of such technologies are crucial to ensuring that they   \n532 serve to benefit society as a whole.   \n533 Limitation. We recognize that the benchmarks utilized in our study may not fully capture the   \n534 complexities of more advanced applications, such as protein drug design, primarily due to our current   \n535 limitations in accessing wet-lab experimental setups. Moving forward, we aim to mitigate this   \n536 limitation by fostering partnerships with domain experts, which will enable us to apply our method   \n537 to more challenging and diverse problems. This direction not only promises to validate the efficacy   \n538 of our approach in more complex scenarios but also aligns with our commitment to pushing the   \n539 boundaries of what our technology can achieve. ", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "540 NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "542 Question: Do the main claims made in the abstract and introduction accurately reflect the   \n543 paper\u2019s contributions and scope?   \n544 Answer: [Yes]   \n545 Justification: The abstract and introduction accurately reflect the paper\u2019s contributions and   \n546 scope.   \n547 Guidelines:   \n548 \u2022 The answer NA means that the abstract and introduction do not include the claims   \n549 made in the paper.   \n550 \u2022 The abstract and/or introduction should clearly state the claims made, including the   \n551 contributions made in the paper and important assumptions and limitations. A No or   \n552 NA answer to this question will not be perceived well by the reviewers.   \n553 \u2022 The claims made should match theoretical and experimental results, and reflect how   \n554 much the results can be expected to generalize to other settings.   \n555 \u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals   \n556 are not attained by the paper. ", "page_idx": 15}, {"type": "text", "text": "57 2. Limitations ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] ", "page_idx": 15}, {"type": "text", "text": "61 Guidelines:   \n62 \u2022 The answer NA means that the paper has no limitation while the answer No means that   \n63 the paper has limitations, but those are not discussed in the paper.   \n64 \u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n65 \u2022 The paper should point out any strong assumptions and how robust the results are to   \n66 violations of these assumptions (e.g., independence assumptions, noiseless settings,   \n67 model well-specification, asymptotic approximations only holding locally). The authors   \n68 should reflect on how these assumptions might be violated in practice and what the   \n69 implications would be.   \n70 \u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was   \n71 only tested on a few datasets or with a few runs. In general, empirical results often   \n72 depend on implicit assumptions, which should be articulated.   \n73 \u2022 The authors should reflect on the factors that influence the performance of the approach.   \n74 For example, a facial recognition algorithm may perform poorly when image resolution   \n75 is low or images are taken in low lighting. Or a speech-to-text system might not be   \n76 used reliably to provide closed captions for online lectures because it fails to handle   \n77 technical jargon.   \n78 \u2022 The authors should discuss the computational efficiency of the proposed algorithms   \n79 and how they scale with dataset size.   \n80 \u2022 If applicable, the authors should discuss possible limitations of their approach to   \n81 address problems of privacy and fairness.   \n82 \u2022 While the authors might fear that complete honesty about limitations might be used by   \n83 reviewers as grounds for rejection, a worse outcome might be that reviewers discover   \n84 limitations that aren\u2019t acknowledged in the paper. The authors should use their best   \n85 judgment and recognize that individual actions in favor of transparency play an impor  \n86 tant role in developing norms that preserve the integrity of the community. Reviewers   \n87 will be specifically instructed to not penalize honesty concerning limitations.   \n3. Theory Assumptions and Proofs ", "page_idx": 15}, {"type": "text", "text": "588 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "589 Question: For each theoretical result, does the paper provide the full set of assumptions and   \n590 a complete (and correct) proof? ", "page_idx": 15}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 15}, {"type": "text", "text": "Justification: The paper does not include theoretical results. Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 16}, {"type": "text", "text": "604 4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 16}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 16}, {"type": "text", "text": "Justification: We provide our code link in the abstract and detail our settings in Section 4.3. Guidelines: ", "page_idx": 16}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 16}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 16}, {"type": "text", "text": "646 Answer: [Yes]   \n647 Justification: We provide a link to our source code in the abstract and thoroughly describe   \n648 our experimental settings in Section 4.3.   \n649 Guidelines:   \n650 \u2022 The answer NA means that paper does not include experiments requiring code.   \n651 \u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/   \n652 public/guides/CodeSubmissionPolicy) for more details.   \n653 \u2022 While we encourage the release of code and data, we understand that this might not be   \n654 possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not   \n655 including code, unless this is central to the contribution (e.g., for a new open-source   \n656 benchmark).   \n657 \u2022 The instructions should contain the exact command and environment needed to run to   \n658 reproduce the results. See the NeurIPS code and data submission guidelines (https:   \n659 //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n660 \u2022 The authors should provide instructions on data access and preparation, including how   \n661 to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n662 \u2022 The authors should provide scripts to reproduce all experimental results for the new   \n663 proposed method and baselines. If only a subset of experiments are reproducible, they   \n664 should state which ones are omitted from the script and why.   \n665 \u2022 At submission time, to preserve anonymity, the authors should release anonymized   \n666 versions (if applicable).   \n667 \u2022 Providing as much information as possible in supplemental material (appended to the   \n668 paper) is recommended, but including URLs to data and code is permitted.   \n669 6. Experimental Setting/Details   \n670 Question: Does the paper specify all the training and test details (e.g., data splits, hyper  \n671 parameters, how they were chosen, type of optimizer, etc.) necessary to understand the   \n672 results?   \n673 Answer: [Yes]   \n674 Justification: We detail our setting in Section 4.3 and also discuss hyperparameter sensitivity   \n675 in Appendix F.   \n676 Guidelines:   \n677 \u2022 The answer NA means that the paper does not include experiments.   \n678 \u2022 The experimental setting should be presented in the core of the paper to a level of detail   \n679 that is necessary to appreciate the results and make sense of them.   \n680 \u2022 The full details can be provided either with the code, in appendix, or as supplemental   \n681 material.   \n682 7. Experiment Statistical Significance   \n683 Question: Does the paper report error bars suitably and correctly defined or other appropriate   \n684 information about the statistical significance of the experiments?   \n685 Answer: [Yes]   \n686 Justification: To ensure reliability and consistency in our comparative analysis, each experi  \n687 mental setting was replicated across 8 independent runs, unless stated otherwise, with the   \n688 presentation of both mean values and standard errors.   \n689 Guidelines:   \n690 \u2022 The answer NA means that the paper does not include experiments.   \n691 \u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi  \n692 dence intervals, or statistical significance tests, at least for the experiments that support   \n693 the main claims of the paper.   \n694 \u2022 The factors of variability that the error bars are capturing should be clearly stated (for   \n695 example, train/test split, initialization, random drawing of some parameter, or overall   \n696 run with given experimental conditions).   \n697 \u2022 The method for calculating the error bars should be explained (closed form formula,   \n698 call to a library function, bootstrap, etc.)   \n699 \u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n700 \u2022 It should be clear whether the error bar is the standard deviation or the standard error   \n701 of the mean.   \n702 \u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should   \n703 preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis   \n704 of Normality of errors is not verified.   \n705 \u2022 For asymmetric distributions, the authors should be careful not to show in tables or   \n706 figures symmetric error bars that would yield results that are out of range (e.g. negative   \n707 error rates).   \n708 \u2022 If error bars are reported in tables or plots, The authors should explain in the text how   \n709 they were calculated and reference the corresponding figures or tables in the text.   \n710 8. Experiments Compute Resources   \n711 Question: For each experiment, does the paper provide sufficient information on the com  \n712 puter resources (type of compute workers, memory, time of execution) needed to reproduce   \n713 the experiments?   \n714 Answer: [Yes]   \n715 Justification: We have discussed these in Section 4.3.   \n716 Guidelines:   \n717 \u2022 The answer NA means that the paper does not include experiments.   \n718 \u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,   \n719 or cloud provider, including relevant memory and storage.   \n720 \u2022 The paper should provide the amount of compute required for each of the individual   \n721 experimental runs as well as estimate the total compute.   \n722 \u2022 The paper should disclose whether the full research project required more compute   \n723 than the experiments reported in the paper (e.g., preliminary or failed experiments that   \n724 didn\u2019t make it into the paper).   \n725 9. Code Of Ethics   \n726 Question: Does the research conducted in the paper conform, in every respect, with the   \n727 NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?   \n728 Answer: [Yes]   \n729 Justification: We preserve anonymity and conform with the NeurIPS Code of Ethics.   \n730 Guidelines:   \n731 \u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n732 \u2022 If the authors answer No, they should explain the special circumstances that require a   \n733 deviation from the Code of Ethics.   \n734 \u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid  \n735 eration due to laws or regulations in their jurisdiction).   \n736 10. Broader Impacts   \n737 Question: Does the paper discuss both potential positive societal impacts and negative   \n738 societal impacts of the work performed?   \n739 Answer: [Yes]   \n740 Justification: We discuss both potential positive and negative impacts in Appendix G.   \n741 Guidelines:   \n742 \u2022 The answer NA means that there is no societal impact of the work performed.   \n743 \u2022 If the authors answer NA or No, they should explain why their work has no societal   \n744 impact or why the paper does not address societal impact.   \n745 \u2022 Examples of negative societal impacts include potential malicious or unintended uses   \n746 (e.g., disinformation, generating fake profiles, surveillance), fairness considerations   \n747 (e.g., deployment of technologies that could make decisions that unfairly impact specific   \n748 groups), privacy considerations, and security considerations.   \n749 \u2022 The conference expects that many papers will be foundational research and not tied   \n750 to particular applications, let alone deployments. However, if there is a direct path to   \n751 any negative applications, the authors should point it out. For example, it is legitimate   \n752 to point out that an improvement in the quality of generative models could be used to   \n753 generate deepfakes for disinformation. On the other hand, it is not needed to point out   \n754 that a generic algorithm for optimizing neural networks could enable people to train   \n755 models that generate Deepfakes faster.   \n756 \u2022 The authors should consider possible harms that could arise when the technology is   \n757 being used as intended and functioning correctly, harms that could arise when the   \n758 technology is being used as intended but gives incorrect results, and harms following   \n759 from (intentional or unintentional) misuse of the technology.   \n760 \u2022 If there are negative societal impacts, the authors could also discuss possible mitigation   \n761 strategies (e.g., gated release of models, providing defenses in addition to attacks,   \n762 mechanisms for monitoring misuse, mechanisms to monitor how a system learns from   \n763 feedback over time, improving the efficiency and accessibility of ML).   \n764 11. Safeguards   \n765 Question: Does the paper describe safeguards that have been put in place for responsible   \n766 release of data or models that have a high risk for misuse (e.g., pretrained language models,   \n767 image generators, or scraped datasets)?   \n768 Answer: [NA]   \n769 Justification: We do not release any datasets nor pre-trained models.   \n770 Guidelines:   \n771 \u2022 The answer NA means that the paper poses no such risks.   \n772 \u2022 Released models that have a high risk for misuse or dual-use should be released with   \n773 necessary safeguards to allow for controlled use of the model, for example by requiring   \n774 that users adhere to usage guidelines or restrictions to access the model or implementing   \n775 safety filters.   \n776 \u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors   \n777 should describe how they avoided releasing unsafe images.   \n778 \u2022 We recognize that providing effective safeguards is challenging, and many papers do   \n779 not require this, but we encourage authors to take this into account and make a best   \n780 faith effort.   \n781 12. Licenses for existing assets   \n782 Question: Are the creators or original owners of assets (e.g., code, data, models), used in   \n783 the paper, properly credited and are the license and terms of use explicitly mentioned and   \n784 properly respected?   \n785 Answer: [Yes]   \n786 Justification: We have duly credited all utilized assets and adhered to their respective licenses   \n787 and terms of use.   \n788 Guidelines:   \n789 \u2022 The answer NA means that the paper does not use existing assets.   \n790 \u2022 The authors should cite the original paper that produced the code package or dataset.   \n791 \u2022 The authors should state which version of the asset is used and, if possible, include a   \n792 URL.   \n793 \u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n794 \u2022 For scraped data from a particular source (e.g., website), the copyright and terms of   \n795 service of that source should be provided.   \n796 \u2022 If assets are released, the license, copyright information, and terms of use in the   \n797 package should be provided. For popular datasets, paperswithcode.com/datasets   \n798 has curated licenses for some datasets. Their licensing guide can help determine the   \n799 license of a dataset.   \n800 \u2022 For existing datasets that are re-packaged, both the original license and the license of   \n801 the derived asset (if it has changed) should be provided. ", "page_idx": 17}, {"type": "text", "text": "", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 20}, {"type": "text", "text": "03   \n04 13. New Assets   \n05 Question: Are new assets introduced in the paper well documented and is the documentation   \n06 provided alongside the assets?   \n07 Answer: [Yes]   \n808 Justification: We plan to open-source our code and have ensured thorough documentation of   \n09 the code.   \n10 Guidelines:   \n11 \u2022 The answer NA means that the paper does not release new assets.   \n12 \u2022 Researchers should communicate the details of the dataset/code/model as part of their   \n13 submissions via structured templates. This includes details about training, license,   \n14 limitations, etc.   \n15 \u2022 The paper should discuss whether and how consent was obtained from people whose   \n16 asset is used.   \n17 \u2022 At submission time, remember to anonymize your assets (if applicable). You can either   \n18 create an anonymized URL or include an anonymized zip file.   \n19 14. Crowdsourcing and Research with Human Subjects   \n20 Question: For crowdsourcing experiments and research with human subjects, does the paper   \n21 include the full text of instructions given to participants and screenshots, if applicable, as   \n22 well as details about compensation (if any)?   \n23 Answer: [NA]   \n24 Justification: This paper does not engage in crowdsourcing or involve studies with human   \n25 participants.   \n26 Guidelines:   \n27 \u2022 The answer NA means that the paper does not involve crowdsourcing nor research with   \n28 human subjects.   \n29 \u2022 Including this information in the supplemental material is fine, but if the main contribu  \n30 tion of the paper involves human subjects, then as much detail as possible should be   \n31 included in the main paper.   \n32 \u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,   \n33 or other labor should be paid at least the minimum wage in the country of the data   \n34 collector.   \n35 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human   \n36 Subjects   \n37 Question: Does the paper describe potential risks incurred by study participants, whether   \n38 such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)   \n39 approvals (or an equivalent approval/review based on the requirements of your country or   \n40 institution) were obtained?   \n41 Answer: [NA]   \n42 Justification: This paper does not engage in crowdsourcing or research involving human   \n43 subjects.   \n44 Guidelines: ", "page_idx": 20}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 20}]