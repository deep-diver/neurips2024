[{"type": "text", "text": "Block Sparse Bayesian Learning: A Diversified Scheme ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yanhao Zhang Zhihan Zhu Yong Xia \u2217 School of Mathematical Sciences, Beihang University Beijing, 100191 {yanhaozhang, zhihanzhu, yxia}@buaa.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "This paper introduces a novel prior called Diversified Block Sparse Prior to characterize the widespread block sparsity phenomenon in real-world data. By allowing diversification on intra-block variance and inter-block correlation matrices, we effectively address the sensitivity issue of existing block sparse learning methods to pre-defined block information, which enables adaptive block estimation while mitigating the risk of overftiting. Based on this, a diversified block sparse Bayesian learning method (DivSBL) is proposed, utilizing EM algorithm and dual ascent method for hyperparameter estimation. Moreover, we establish the global and local optimality theory of our model. Experiments validate the advantages of DivSBL over existing algorithms. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Sparse recovery through Compressed Sensing (CS), with its powerful theoretical foundation and broad practical applications, has received much attention [1]. The basic model is considered as ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\mathbf{y}=\\Phi\\mathbf{x},\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $\\mathbf{y}\\in\\mathbb{R}^{M\\times1}$ is the measurement (or response) vector and $\\Phi\\in\\mathbb{R}^{M\\times N}$ is a known design matrix, satisfying the Unique Representation Property (URP) condition [2]. $\\mathbf{x}\\in\\mathbb{R}^{N\\times1}(N\\gg\\check{M})$ is the sparse vector to be recovered. In practice, $\\mathbf{x}$ often exhibits transform sparsity, becoming sparse in a transform domain such as Wavelet, Fourier, etc. Once the signal is compressible in a linear basis $\\Psi$ , in other words, $\\mathbf{x}=\\Psi\\mathbf{w}$ where w exhibits sparsity, and $\\Phi\\Psi$ satisfies Restricted Isometry Constants (RIP) [3], then we can simply replace $\\mathbf{x}$ by $\\boldsymbol{\\Psi}\\mathbf{w}$ in (1) and solve it in the same way. Classic algorithms for compressive sensing and sparse regression include Lasso [4], Sparse Bayesian Learning (SBL) [5], Basis Pursuit (BP) [6], Orthogonal Matching Pursuit(OMP) [7], etc. Recently, there have been approaches that involve solving CS problems through deep learning [8; 9; 10]. ", "page_idx": 0}, {"type": "text", "text": "However, deeper research into sparse learning has shown that relying solely on the sparsity of $\\mathbf{x}$ is insufficient, especially with limited samples [11; 12]. Widely encountered real-world data, such as image and audio, often exhibit clustered sparsity in transformed domains [13]. This phenomenon, known as block sparsity, means the sparse non-zero entries of $\\mathbf{x}$ appear in blocks [11]. Recent years, block sparse models have gained attention in machine learning, including sparse training [14], adversarial learning [15], image restoration [16; 14], (audio) signal processing [17; 18] and many other areas. Generally, the block structure of $\\mathbf{x}$ with $g$ blocks is defined by ", "page_idx": 0}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{x}=[\\underbrace{x_{1}\\ldots x_{d_{1}}}_{\\mathbf{x}_{1}^{T}}\\underbrace{x_{d_{1}+1}\\ldots x_{d_{1}+d_{2}}}_{\\mathbf{x}_{2}^{T}}\\cdots\\underbrace{x_{N-d_{g}+1}\\ldots x_{N}}_{\\mathbf{x}_{g}^{T}}]^{T},}\\end{array}\n$$", "text_format": "latex", "page_idx": 0}, {"type": "text", "text": "where $d_{i}(i=1...g)$ represent the size of each block, which are not necessarily identical. Suppose only $k(k\\ll g)$ blocks are non-zero, indicating that $\\mathbf{x}$ is block sparse. Up to now, several methods have been proposed to recover block sparse signals. They are mainly divided into two categories. \u2217Corresponding author ", "page_idx": 0}, {"type": "text", "text": "Block-based Classical algorithms for processing block sparse scenarios include Group-Lasso [19; 20; 21], Group Basis Pursuit [22], Block-OMP [11]. Blocks are assumed to be static with a fixed preset size. Furthermore, Temporally-SBL (TSBL) [23] and Block-SBL (BSBL) [24; 25], based on Bayesian models, provide refined estimation of correlation matrices within blocks. However, they assume elements within one block tend to be either zero or non-zero simultaneously. Although they can estimate intra-block correlation with high accuracy in block-level recovery, they require preset choices of suitable block sizes and patterns, which are too rigid for many practical applications. ", "page_idx": 1}, {"type": "text", "text": "Pattern-based StructOMP [26] is a pattern-based greedy algorithm allowing structures, which is a generalization of group sparsity. Another classic model named Pattern-Coupled SBL (PC-SBL) [27; 28], does not have a predefined requirement for block size as well. It utilizes a Bayesian model to couple the signal variances. Building upon PC-SBL, Burst PC-SBL, proposed in [29], is employed for the estimation of mMIMO channels. While pattern-based algorithms address the issue of explicitly specifying block patterns in block-based algorithms, these models provide a coarse characterization of the intra-block correlation, leading to a loss of structural information within the blocks. ", "page_idx": 1}, {"type": "text", "text": "In this paper, we introduce a diversified block sparse Bayesian framework that incorporates diversity in both variance within the same block and intra-block correlation among different blocks. Our model not only inherits the advantages of block-based methods on block-level estimation, but also addresses the longstanding issues associated with such algorithms: the diversified scheme reduces sensitivity to a predefined block size or specified block location, hence accommodates general block sparse data. Based on this model, we develop the DivSBL algorithm, and also analyze both the global minimum and local minima of the constrained cost function (likelihood). The subsequent experiments illustrate the superiority of proposed diversified scheme when applied to real-life block sparse data. ", "page_idx": 1}, {"type": "text", "text": "2 Diversified block sparse Bayesian model ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We consider the block sparse signal recovery, or compressive sensing question in the noisy case ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathbf{y}=\\Phi\\mathbf{x}+\\mathbf{n},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\mathbf{n}\\sim\\mathcal{N}(0,\\beta^{-1}\\mathbf{I})$ represents the measurement noise, and $\\beta$ is the precise scalar. Other symbols have the same interpretations as (1). The signal $\\mathbf{x}$ exhibits block-sparse structure in (2), yet the block partition is unknown. For clarity in description, we assume that all blocks have equal size $L$ , with the total dimension denoted as $N=g L$ . Henceforth, we presume that the signal $\\mathbf{x}$ follows the structure: ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{x}=[\\underbrace{x_{11}\\ldots x_{1L}}_{\\mathbf{x}_{1}^{T}}\\underbrace{x_{21}\\ldots x_{2L}}_{\\mathbf{x}_{2}^{T}}\\cdot\\cdot\\underbrace{\\cdot x_{g1}\\ldots x_{g L}}_{\\mathbf{x}_{g}^{T}}]^{T}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "In Sections 2.1.1 and 5.2, we clarify that this assumption is made without loss of generality. In fact, our algorithm can automatically adjust $L$ to an appropriate size, expanding or contracting as needed. ", "page_idx": 1}, {"type": "text", "text": "2.1 Diversified block sparse prior ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "The Diversified Block Sparse prior is proposed in the following scheme. Each block $\\mathbf{x}_{i}\\in\\mathbb{R}^{L\\times1}$ is assumed to follow a multivariate Gaussian prior ", "page_idx": 1}, {"type": "equation", "text": "$$\np\\left({\\bf x}_{i};\\left\\{{\\bf G}_{i},{\\bf B}_{i}\\right\\}\\right)=\\mathcal{N}\\left({\\bf0},{\\bf G}_{i}{\\bf B}_{i}{\\bf G}_{i}\\right),\\forall i=1,\\cdot\\cdot\\cdot\\mathrm{~,~}g,\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "in which, $\\mathbf{G}_{i}$ represents the Diversified Variance matrix, and $\\mathbf{B}_{i}$ represents the Diversified Correlation matrix, with detailed formulations in Sections 2.1.1 and 2.1.2. Therefore, the prior distribution of the entire signal $\\mathbf{x}$ is denoted as ", "page_idx": 1}, {"type": "equation", "text": "$$\np\\left(\\mathbf{x};\\left\\{\\mathbf{G}_{i},\\mathbf{B}_{i}\\right\\}_{i=1}^{g}\\right)=\\mathcal{N}\\left(\\mathbf{0},\\Sigma_{0}\\right),\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "where $\\mathbf{\\Sigma}_{0}\\;=\\;\\mathrm{diag}\\left\\{\\mathbf{G}_{1}\\mathbf{B}_{1}\\mathbf{G}_{1},\\mathbf{G}_{2}\\mathbf{B}_{2}\\mathbf{G}_{2},\\cdot\\cdot\\cdot,\\mathbf{G}_{g}\\mathbf{B}_{g}\\mathbf{G}_{g}\\right\\}$ . The dependency in this hierarchical model is shown in Figure.1. ", "page_idx": 1}, {"type": "text", "text": "2.1.1 Diversified intra-block variance ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "We first execute diversification on variance. In (5), $\\mathbf{G}_{i}$ is defined as ", "page_idx": 1}, {"type": "equation", "text": "$$\n\\mathbf{G}_{i}\\triangleq\\mathrm{diag}\\{\\sqrt{\\gamma_{i1}},\\cdot\\cdot\\cdot\\,,\\sqrt{\\gamma_{i L}}\\},\n$$", "text_format": "latex", "page_idx": 1}, {"type": "text", "text": "and $\\mathbf{B}_{i}\\in\\mathbb{R}^{L\\times L}$ is a positive definite matrix capturing the correlation within the $i$ -th block. According to the definition of Pearson correlation, the covariance term $\\mathbf{G}_{i}\\mathbf{B}_{i}\\mathbf{G}_{i}$ in (5) can be specified as ", "page_idx": 1}, {"type": "image", "img_path": "a4cPpx1xYg/tmp/2f255db21e40a3d180a4f78d4a26ff218d890144b4f6678ae3198d578413467d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Figure 1: Directed acyclic graph of diversified block sparse hierarchical structure. Except for Measurements (blue nodes), which are known, all other nodes are parameters to estimate. ", "page_idx": 2}, {"type": "image", "img_path": "a4cPpx1xYg/tmp/37cd1052fea9ab5705f6cf8b51c39a2dcfe925dea7384f6fe8267419440c7c00.jpg", "img_caption": ["Figure 2: The gold dashed line shows the preset block, and the black shadow represents the actual position of the block with its true size. "], "img_footnote": [], "page_idx": 2}, {"type": "image", "img_path": "a4cPpx1xYg/tmp/61f284e14a541b5791cbd4791d8917190f80a0b2acdb04179c555834fa26d1b7.jpg", "img_caption": ["where $\\rho_{s k}^{i}(\\forall s,k=1\\cdots L)$ are the elements in correlation matrix $\\mathbf{B}_{i}$ , serving as a visualization of covariance with displayed structural information. "], "img_footnote": [], "page_idx": 2}, {"type": "text", "text": "Now it is evident why assuming equal block sizes $L$ is insensitive. For the sake of clarity, we denote the true size of the $i$ -th block $\\mathbf{x}_{i}$ as $L_{T}^{i}$ . As illustrated in Figure 2, when the true block falls within the preset block, the variances $\\gamma_{i}$ \u00b7 corresponding to the non-zero positions in $\\mathbf{x}_{i}$ will be learned as non-zero values through posterior inference, while the variances at zero positions will automatically be learned as zero. When the true block is positioned across the preset block, several blocks of the predefined size $L$ covered by the actual block will be updated together, and likewise, variances will be learned as either zero or non-zero. In this way, both of the size and location of the blocks will be automatically learned through posterior inference on the variances. ", "page_idx": 2}, {"type": "text", "text": "2.1.2 Diversified inter-block correlation ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Due to limited data and excessive parameters in intra-block correlation matrices $\\mathbf{B}_{i}(\\forall i)$ , previous works correct their estimation by imposing strong correlated constraints $\\mathbf{B}_{i}=\\mathbf{B}(\\forall i)$ to overcome overftiting [24]. Recognizing that correlation matrices among different blocks should be diverse yet still exhibit some correlation, we apply a weak-correlated constraint to diversify $\\mathbf{B}_{i}$ in the model. ", "page_idx": 2}, {"type": "text", "text": "Here we introduce novel weak constraints on $\\mathbf{B}_{i}$ , specifically, ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\psi\\left(\\mathbf{B}_{i}\\right)=\\psi\\left(\\mathbf{B}\\right)\\quad\\forall i=1,\\cdot\\cdot\\cdot\\ g,\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "where $\\psi:\\mathbb{R}^{L^{2}}\\rightarrow\\mathbb{R}$ is the weak constraint function and $\\mathbf{B}$ is obtained from the strong constraints $\\mathbf{B}_{i}=\\mathbf{B}(\\forall i)$ , as detailed in Section 3.2. Weak constraints (8) not only capture the distinct correlation structure but also avoid overftiting issue arising from the complete independence among different $\\mathbf{B}_{i}$ . ", "page_idx": 2}, {"type": "text", "text": "Furthermore, the constraints imposed here not only maintain the global minimum property of our algorithm, as substantiated in Section 4, but also effectively enhance the convergence rate of the algorithm. There are actually $g L(L{+}1)/2$ constraints in the strong correlated constraints $\\mathbf{B}_{i}=\\mathbf{B}(\\forall i)$ , while with (8), the number of constraints significantly decreases to $g$ , yielding acceleration on the convergence rate. The experimental result is shown in Appendix A. ", "page_idx": 2}, {"type": "text", "text": "In summary, the prior based on (5), (6), (7) and (8) is defined as diversified block sparse prior. ", "page_idx": 2}, {"type": "text", "text": "2.1.3 Connections to classical models ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "Note that the classical Sparse Bayesian Learning models, Relevance Vector Machine (RVM) [5] and Block Sparse Bayesian Learning (BSBL) [23], are special cases of our model. ", "page_idx": 2}, {"type": "text", "text": "Connection to RVM Taking $\\mathbf{B}_{i}$ as identity matrix, diversified block sparse prior (6) immediately degenerates to RVM model ", "page_idx": 3}, {"type": "equation", "text": "$$\np\\left(\\boldsymbol{x}_{i};\\gamma_{i}\\right)=\\mathcal{N}\\left(0,\\gamma_{i}\\right),\\forall i=1,\\cdot\\cdot\\cdot\\mathrm{,~}N,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which means ignoring the correlation structure. ", "page_idx": 3}, {"type": "text", "text": "Connection to BSBL When $\\mathbf{G}_{i}$ is scalar matrix $\\sqrt{\\gamma_{i}}\\mathbf{I}$ , the formulation (5) becomes ", "page_idx": 3}, {"type": "equation", "text": "$$\np\\left(\\mathbf{x}_{i};\\left\\{\\gamma_{i},\\mathbf{B}_{i}\\right\\}\\right)=\\mathcal{N}\\left(\\mathbf{0},\\gamma_{i}\\mathbf{B}_{i}\\right),\\forall i=1,\\cdot\\cdot\\cdot\\mathbf{\\sigma},g,\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "which is exactly BSBL model. In this case, all elements within a block share common variance $\\gamma_{i}$ . ", "page_idx": 3}, {"type": "text", "text": "2.2 Posterior estimation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "By observation model (3), the Gaussian likelihood is ", "page_idx": 3}, {"type": "equation", "text": "$$\np(\\mathbf{y}\\mid\\mathbf{x};\\beta)=\\mathcal{N}(\\Phi\\mathbf{x},\\beta^{-1}\\mathbf{I}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "With prior (6) and likelihood (11), the diversified block sparse posterior distribution of $\\mathbf{x}$ can be derived based on Bayes\u2019 theorem as ", "page_idx": 3}, {"type": "equation", "text": "$$\np\\left(\\mathbf{x}\\mid\\mathbf{y};\\{\\mathbf{G}_{i},\\mathbf{B}_{i}\\}_{i=1}^{g}\\,,\\beta\\right)=\\mathcal{N}\\left(\\pmb{\\mu},\\Sigma\\right),\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\pmb{\\mu}}=\\beta\\pmb{\\Sigma}\\pmb{\\Phi}^{T}\\mathbf{y},}\\\\ &{\\pmb{\\Sigma}=\\left(\\pmb{\\Sigma}_{0}^{-1}+\\beta\\pmb{\\Phi}^{T}\\pmb{\\Phi}\\right)^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "After estimating all hyperparameters in (12), i.e, $\\hat{\\Theta}\\,=\\,\\left\\{\\{\\hat{\\mathbf{G}}_{i}\\}_{i=1}^{g},\\{\\hat{\\mathbf{B}}_{i}\\}_{i=1}^{g},\\hat{\\beta}\\right\\}$ , as described in Section 3, the Maximum A Posterior (MAP) estimation of $\\mathbf{x}$ is formulated as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\hat{\\pmb{x}}^{M A P}=\\hat{\\pmb{\\mu}}.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "3 Bayesian inference: DivSBL algorithm ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "3.1 EM formulation ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To estimate $\\Theta=\\{\\{\\mathbf{G}_{i}\\}_{i=1}^{g},\\{\\mathbf{B}_{i}\\}_{i=1}^{g},\\beta\\}$ , either Type-II Maximum Likelihood [30] or ExpectationMaximization (EM) formulation [31] can be employed. Following EM procedure, our goal is to maximize $p(\\mathbf{y};\\Theta)$ , or equivalently $\\log p(\\mathbf{y};\\boldsymbol{\\Theta})$ . Defining objective function as $\\mathcal{L}(\\Theta)$ , the problem can be expressed as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\Theta}{\\operatorname*{max}}}&{{}\\mathcal{L}(\\Theta)=-\\mathbf{y}^{T}\\Sigma_{y}^{-1}\\mathbf{y}-\\log\\operatorname*{det}{\\Sigma_{y}},}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\pmb{\\Sigma}_{y}=\\beta^{-1}\\pmb{I}+\\pmb{\\Phi}\\pmb{\\Sigma}_{0}\\pmb{\\Phi}^{T}$ . Then, treating $\\mathbf{x}$ as hidden variable in $\\boldsymbol{\\mathrm E}$ -step, we have $Q$ function as ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{{\\cal Q}(\\Theta)=E_{x|y;\\Theta^{t-1}}[\\log p({\\bf y},{\\bf x};\\Theta)]}\\\\ &{\\quad\\quad=\\!E_{x|y;\\Theta^{t-1}}[\\log p({\\bf y}\\mid{\\bf x};\\beta)]+E_{x|y;\\Theta^{t-1}}\\left[\\log p\\left({\\bf x};\\{{\\bf G}_{i}\\}_{i=1}^{g},\\{{\\bf B}_{i}\\}_{i=1}^{g}\\right)\\right]}\\\\ &{\\quad\\quad\\triangleq\\!{\\cal Q}(\\beta)+{\\cal Q}(\\{{\\bf G}_{i}\\}_{i=1}^{g},\\{{\\bf B}_{i}\\}_{i=1}^{g}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\Theta^{t-1}$ denotes the parameter estimated in the latest iteration. As indicated in (17), we have divided $Q$ function into two parts: $Q(\\beta)\\triangleq E_{x|y;\\Theta^{t-1}}[\\log p(\\mathbf{y}\\mid\\mathbf{x};\\beta)]$ depends solely on $\\beta$ , and $Q(\\{\\mathbf{G}_{i}\\}_{i=1}^{g},\\{\\mathbf{B}_{i}\\}_{i=1}^{g})\\triangleq E_{x|y;\\Theta^{t-1}}\\left[\\log p\\left(\\mathbf{x};\\{\\mathbf{G}_{i}\\}_{i=1}^{g},\\{\\mathbf{B}_{i}\\}_{i=1}^{g}\\right)\\right]$ only on $\\{{\\bf G}_{i}\\}_{i=1}^{g}$ and $\\{{\\bf B}_{i}\\}_{i=1}^{g}$ Therefore, the parameters of these two $Q$ functions can be updated separately. ", "page_idx": 3}, {"type": "text", "text": "In M-step, we need to maximize the above $Q$ functions to obtain the estimation of $\\Theta$ . As shown in Appendix B, the updating formula for $\\gamma_{i j},\\mathbf{B}_{i}$ can be obtained as follows2: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\gamma_{i j}=\\frac{4\\mathbf{A}_{i j}^{2}}{(\\sqrt{\\mathbf{T}_{i j}^{2}+4\\mathbf{A}_{i j}}-\\mathbf{T}_{i j})^{2}},\n$$", "text_format": "latex", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathbf{B}_{i}=\\mathbf{G}_{i}^{-1}\\left(\\boldsymbol{\\Sigma}^{i}+\\mu^{i}\\left(\\mu^{i}\\right)^{T}\\right)\\mathbf{G}_{i}^{-1},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{T}_{i j}$ and $\\mathbf{A}_{i j}$ are expressed a $\\begin{array}{r}{\\boldsymbol{\\mathbf{\\chi}}_{i j}=\\left[(\\mathbf{B}_{i}^{-1})_{j\\cdot}\\odot\\mathrm{diag}(\\mathbf{W}_{-j}^{i})^{-1}\\right]\\cdot\\left(\\boldsymbol{\\mathbf{\\Sigma}}^{i}+\\mu^{i}(\\mu^{i})^{T}\\right)_{\\cdot j},\\boldsymbol{\\mathbf{\\Sigma}}}\\end{array}$ $\\mathbf{A}_{i j}=$ $\\begin{array}{r}{(\\mathbf{B}_{i}^{-1})_{j j}\\cdot\\left(\\mathbf{\\Sigma}^{\\!+i}+\\mu^{i}\\left(\\mu^{i}\\right)^{T}\\right)_{j j},\\,\\mathrm{and}\\mathbf{W}_{-j}^{i}=\\mathrm{diag}\\{\\sqrt{\\gamma_{i1}},\\cdots,\\sqrt{\\gamma_{i,j-1}},0,\\sqrt{\\gamma_{i,j+1}},\\cdots,\\sqrt{\\gamma_{i L}}\\}.}\\end{array}$ . The update formula of $\\beta$ is derived in the same way as [23]. The learning rule is given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\beta=\\frac{M}{\\Vert\\mathbf{y}-\\Phi\\pmb{\\mu}\\Vert_{2}^{2}+\\mathrm{tr}\\left(\\Sigma\\Phi^{T}\\Phi\\right)}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "3.2 Diversified correlation matrices by dual ascent ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Now we propose the algorithm for solving the correlation matrix estimation problem satisfying (8). As mentioned in Section 2.1.2, previous studies have employed strong constraints $\\mathbf{B}_{i}=\\mathbf{B}(\\forall i)$ , i.e., ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{B}=\\mathbf{B}_{i}=\\frac{1}{g}\\sum_{i=1}^{g}\\mathbf{G}_{i}^{-1}\\left(\\boldsymbol{\\Sigma}^{i}+\\mu^{i}\\left(\\mu^{i}\\right)^{T}\\right)\\mathbf{G}_{i}^{-1}.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In diversified scheme, we apply weak-correlated constraints (8) to diversify $\\mathbf{B}_{i}$ . Therefore, the problem of maximizing the $Q$ function with respect to $\\mathbf{B}_{i}$ becomes ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\mathbf{B}_{i}}{\\operatorname*{max}}}&{{}\\,Q(\\{\\mathbf{B}_{i}\\}_{i=1}^{g},\\{\\mathbf{G}_{i}\\}_{i=1}^{g})}\\\\ {\\mathrm{s.t.}\\,\\,}&{{}\\psi\\left(\\mathbf{B}_{i}\\right)=\\psi\\left(\\mathbf{B}\\right)\\quad\\forall i=1,\\cdot\\cdot\\cdot,g,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "which is equivalent to (in the sense that both share the same optimal solution) ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{l l}{{\\displaystyle\\operatorname*{min}_{\\mathbf{B}_{i}}}}&{{\\displaystyle\\frac{1}{2}\\log\\operatorname*{det}\\Sigma_{0}+\\frac{1}{2}\\operatorname{tr}\\left[\\Sigma_{0}^{-1}(\\Sigma+\\mu\\pmb{\\mu}^{T})\\right]}}\\\\ {{\\mathrm{s.t.}}}&{{\\psi\\left(\\mathbf{B}_{i}\\right)=\\psi\\left(\\mathbf{B}\\right)\\quad\\forall i=1,\\cdot\\cdot\\cdot\\mathbf{\\Sigma},g,}}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\mathbf{B}$ is already derived in (21). Therefore, by solving (P) , we will obtain diversified solution for correlation matrices $\\mathbf{B}_{i},\\forall i$ . The constraint function $\\psi$ can be further categorized into two cases: explicit constraints and hidden constraints. ", "page_idx": 4}, {"type": "text", "text": "Explicit constraints with complete dual ascent Explicit functions such as the Frobenius norm, the logarithm of the determinant, etc., are good choices for $\\psi$ . An efficient way to solve this constrained optimization is to solve its dual problem (mostly refers to Lagrange dual [32]). Choosing $\\psi(\\cdot)$ as $\\log\\operatorname*{det}(\\cdot)$ , the detailed solution process is outlined in Appendix C. And the update formulas for $\\mathbf{B}_{i}$ and multiplier $\\lambda_{i}$ (dual variable) are given by ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbf{B}_{i}^{k+1}=\\frac{\\mathbf{G}_{i}^{-1}\\left(\\boldsymbol{\\Sigma}^{i}+\\mu^{i}\\left(\\mu^{i}\\right)^{T}\\right)\\mathbf{G}_{i}^{-1}}{1+2\\lambda_{i}^{k}},\n$$", "text_format": "latex", "page_idx": 4}, {"type": "equation", "text": "$$\n\\lambda_{i}^{k+1}=\\lambda_{i}^{k}+\\alpha_{i}^{k}(\\log\\operatorname*{det}{\\mathbf{B}_{i}^{k}}-\\log\\operatorname*{det}{\\mathbf{B}}),\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "in which $\\alpha_{i}^{k}$ represents the step size in the $k$ -th iteration for updating the multiplier $\\lambda_{i}$ $(i=1,\\cdot\\cdot\\cdot,g)$ . ICno onuvre regxepnecrie mise notn, lyw eg ucahroaonstee ea dd iif mtihnei sshtienpg s istzee ps saitizse $\\textstyle\\sum_{k=1}^{\\infty}\\alpha_{i}^{k}=\\infty$ oannvde $\\textstyle\\sum_{k=1}^{\\infty}(\\alpha_{i}^{k})^{2}<\\infty$ i[t3h2m].. $1/\\bar{k}$ The procedure, using dual ascent to diversify $\\mathbf{B}_{i}$ , is summarized in Algorithm 2 in Appendix C. ", "page_idx": 4}, {"type": "text", "text": "Hidden constraints with one-step dual ascent The weak correlated function $\\psi$ can also be chosen as hidden constraint without an explicit expression. Specifically, the solution to sub-problem (22) equipped with hidden constraints $\\psi$ corresponds exactly to one-step dual ascent in (23)(24). We summarize the proposition as follows: ", "page_idx": 4}, {"type": "text", "text": "Proposition 3.1. Define an explicit weak constraint function $\\zeta:\\mathbb{R}^{n^{2}}\\rightarrow\\mathbb{R}$ . For the constrained optimization problem: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\mathbf{B}_{i}}{\\operatorname*{min}}}&{{}\\,Q(\\{\\mathbf{B}_{i}\\}_{i=1}^{g},\\{\\mathbf{G}_{i}\\}_{i=1}^{g})}\\\\ {\\mathrm{s.t.}\\,\\,}&{{}\\zeta(\\mathbf{B}_{i})=\\zeta(\\mathbf{B}),\\quad\\forall i=1,\\cdots,g,}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "the stationary point $(\\{\\mathbf{B}_{i}^{k+1}\\}_{i=1}^{g},\\{\\lambda_{i}^{k}\\}_{i=1}^{g})$ of the Lagrange function under given multipliers $\\{\\lambda_{i}^{k}\\}_{i=1}^{g}$ satisfies: ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\nabla_{\\mathbf{B}_{i}}Q(\\{\\mathbf{B}_{i}^{k+1}\\}_{i=1}^{g},\\{\\mathbf{G}_{i}\\}_{i=1}^{g})-\\lambda_{i}^{k}\\nabla\\zeta(\\mathbf{B}_{i}^{k+1})=0.\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "Then there exists a constrained optimization problem with hidden weak constraint $\\psi:\\mathbb{R}^{n^{2}}\\rightarrow\\mathbb{R}$ : ", "page_idx": 5}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\mathbf{B}_{i}}{\\operatorname*{min}}}&{{}\\,Q(\\{\\mathbf{B}_{i}\\}_{i=1}^{g},\\{\\mathbf{G}_{i}\\}_{i=1}^{g})}\\\\ {\\mathrm{s.t.}\\;\\;}&{{}\\psi(\\mathbf{B}_{i})=\\psi(\\mathbf{B}),\\quad\\forall i=1,\\cdots,g,}\\end{array}\n$$", "text_format": "latex", "page_idx": 5}, {"type": "text", "text": "such that $(\\{\\mathbf{B}_{i}^{k+1}\\}_{i=1}^{g},\\{\\lambda_{i}^{k}\\}_{i=1}^{g})$ is a KKT pair of the above optimization problem. ", "page_idx": 5}, {"type": "text", "text": "Compared to explicit formulation, hidden weak constraints, while ensuring diversification on correlation, significantly accelerate the algorithm\u2019s speed by requiring only one-step dual ascent for updating. Here, we set $\\zeta(\\cdot)$ as $\\log\\operatorname*{det}(\\cdot)$ , actually solving the optimization problem under corresponding hidden constraint $\\psi$ . The comparison of computation time between explicit and hidden constraints, proof of Proposition 3.1 and further explanations on hidden constraints are provided in Appendix D. ", "page_idx": 5}, {"type": "text", "text": "Considering that it\u2019s sufficient to model elements of a block as a first order Auto-Regression (AR) process [24] in which the intra-block correlation matrix is a Toeplitz matrix, we employ this strategy for $\\mathbf{B}_{i}$ . After estimating $\\mathbf{B}_{i}$ by dual ascent, we then apply Toeplitz correction to $\\mathbf{B}_{i}$ as ", "page_idx": 5}, {"type": "image", "img_path": "a4cPpx1xYg/tmp/a08ce792ec67f613a27bf0e90fde934ddbc657a74e171f5c255891e4341aab45.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "In conclusion, the Diversified SBL (DivSBL) algorithm is summarized as Algorithm 1 below. ", "page_idx": 5}, {"type": "text", "text": "", "text_level": 1, "page_idx": 5}, {"type": "table", "img_path": "a4cPpx1xYg/tmp/f12be4ed5f9e7d1f39a6358ee7b5dce6bdb46a5afced15c280fa0e9f996682e5.jpg", "table_caption": [], "table_footnote": [], "page_idx": 5}, {"type": "text", "text": "4 Global minimum and local minima ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "For the sake of simplicity, we denote the true signal as $\\mathbf{x}_{\\mathrm{true}}$ , which is the sparsest among all feasible solutions. The block sparsity of the true signal is denoted as $K_{0}$ , indicating the presence of $K_{0}$ blocks. Let $\\tilde{\\mathbf{G}}\\triangleq\\operatorname{diag}\\left(\\sqrt{\\gamma_{11}},\\cdot\\cdot\\cdot\\sqrt{\\gamma_{g L}}\\right)$ , $\\tilde{\\mathbf{B}}\\triangleq\\operatorname{diag}\\left(\\mathbf{B}_{1},\\cdot\\cdot\\cdot,\\mathbf{B}_{g}\\right)$ , thus $\\Sigma_{0}\\bar{=}\\tilde{\\mathbf{G}}\\tilde{\\tilde{\\mathbf{B}}}\\tilde{\\mathbf{G}}$ . Additionally, we assume that the measurement matrix $\\Phi$ satisfies the URP condition [2]. We employed various techniques to overcome highly non-linear structure of $\\gamma$ in diversified block sparse prior (5), resulting in following global and local optimality theorems. ", "page_idx": 5}, {"type": "text", "text": "4.1 Analysis of global minimum ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "By introducing a negative sign to the cost function (16), we have the following result on the property of global minimum and the threshold for block sparsity $K_{0}$ . ", "page_idx": 5}, {"type": "text", "text": "3The necessity of each step for updating $\\mathbf{B}_{i}$ in line 9-11 of Algorithm 1 is detailed in Appendix A. ", "page_idx": 5}, {"type": "table", "img_path": "a4cPpx1xYg/tmp/9a60514a808573dcc237b237789e4975e4cbffce950e0388c03ab76a367af7e2.jpg", "table_caption": ["Table 1: Reconstruction error (NMSE) and Correlation (mean $\\pm$ std) for synthetic signals. Our algorithm is marked in blue , and the bestperforming metrics are displayed in bold. "], "table_footnote": [], "page_idx": 6}, {"type": "image", "img_path": "a4cPpx1xYg/tmp/bde1d4cc018d600a1c00403dbf25ad90b875756652f1e34340ea881c76dd5793.jpg", "img_caption": [], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "Figure 3: The consistency of multiple experiments with homoscedastic signals for (a) NMSE (b) Correlation, and with heteroscedastic signals for (c) NMSE and (d) Correlation. ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.1. $A s\\,\\beta\\rightarrow\\infty$ and $K_{0}<(M\\!+\\!1)/2L,$ , the unique global minimum $\\widehat{\\gamma}\\triangleq\\left(\\widehat{\\gamma}_{11},\\ldots,\\widehat{\\gamma}_{g L}\\right)^{T}$ yields a recovery \u02c6x by (13) that is equal to $\\mathbf{x}_{t r u e}$ , regardless of the estimated $\\widehat{\\mathbf{B}}_{i}\\left(\\forall i\\right)$ . ", "page_idx": 6}, {"type": "text", "text": "The proof draws inspiration from [23] and is detailed in Appendix E. Theorem 4.1 shows that, in noiseless case, achieving the global minimum of variance enables exact recovery of the true signal, provided the block sparsity of the signal adheres to the given upper bound. ", "page_idx": 6}, {"type": "text", "text": "4.2 Analysis of local minima", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We provide two lemmas firstly. The proofs are detailed in Appendices F and G. ", "page_idx": 6}, {"type": "text", "text": "Lemma 4.2. For any semi-definite positive symm\u221aetric m\u221aatrix $\\mathbf{Z}\\,\\in\\,\\mathbb{R}^{M\\times M}$ , the constraint $\\mathbf{Z}\\succeq$ $\\Phi\\Sigma_{0}\\Phi^{T}+\\beta^{-1}\\mathbf{I}$ is convex with respect to $\\mathbf{Z}$ and $\\left(\\sqrt{\\gamma}\\otimes\\sqrt{\\gamma}\\right)$ . ", "page_idx": 6}, {"type": "text", "text": "Lemma 4.3. $\\mathbf{y}^{T}\\pmb{\\Sigma}_{y}^{-1}\\mathbf{y}=C\\Leftrightarrow\\mathbf{P}\\left(\\sqrt{\\pmb{\\gamma}}\\otimes\\sqrt{\\pmb{\\gamma}}\\right)=\\mathbf{b}$ for any constant $C$ , where $\\mathbf{b}\\triangleq\\mathbf{y}-\\beta^{-1}\\mathbf{u},$ $\\mathbf{P}\\triangleq\\big[(\\mathbf{u}^{T}\\pmb{\\Phi})\\otimes\\pmb{\\Phi}\\big]\\,\\mathrm{diag}\\,\\Big(\\nu e c\\big(\\tilde{\\mathbf{B}}\\big)\\Big).$ , and u is a vector satisfying $\\mathbf{y}^{T}\\mathbf{u}=C$ . ", "page_idx": 6}, {"type": "text", "text": "It\u2019s clear that $\\mathbf{P}$ is a full row rank matrix, i.e, $r(\\mathbf{P})=M$ . Given the above lemmas, we arrive at the following result, which is proven in Appendix $_\\mathrm{H}$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.4. Every local minimum of the cost function (16) with respect to $\\gamma$ satisfies $||\\hat{\\gamma}||_{0}\\leq\\sqrt{M}$ , irrespective of the estimated $\\widehat{\\mathbf{B}}_{i}\\left(\\forall i\\right)$ and $\\beta$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 4.4 establishes an upper bound on the sparsity level of any local minimum for the cost function in terms of the parameter $\\gamma$ . Therefore, together with Theorem 4.1, these results ensure the sparsity of the final solution obtained. ", "page_idx": 6}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "In this section, we compare DivSBL with the following six algorithms:4 1. Block-based algorithms: (1) BSBL, (2) Group Lasso, (3) Group BPDN. 2. Pattern-based algorithms: (4) PC-SBL, (5) StructOMP. 3. Sparse learning (without structural information): (6) SBL. Results are averaged over 100 or 500 random runs (based on computational scale), with SNR ranging from 15-25 dB except the test for varied noise levels. \u2018Normalized Mean Squared Error (NMSE)\u2019, defined as $\\lvert\\lvert\\hat{x}-x_{\\mathrm{true}}\\rvert\\rvert_{2}^{2}/\\lvert\\lvert x_{\\mathrm{true}}\\rvert\\rvert_{2}^{2}$ , and \u2018Correlation (Corr)\u2019 (cosine similarity) are used to compare algorithms. 5 ", "page_idx": 6}, {"type": "text", "text": "", "page_idx": 7}, {"type": "text", "text": "5.1 Synthetic signal data ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We initially test on synthetic signal data, including homoscedastic (provided by [24]) and heteroscedastic data, where block size, location, non-zero quantity, and signal variance are randomly generated, mimicking real-world data patterns. The reconstruction results are provide in Appendix I.1. Table 1 shows that DivSBL achieves the lowest NMSE and the highest Correlation on both scenarios. To more intuitively demonstrate the statistically significant improvements of the conclusion, we provide box plots of the experimental results on both homoscedastic and heteroscedastic data in Figure 3 . ", "page_idx": 7}, {"type": "text", "text": "Unlike many frequentist approaches that require more complex debiasing methods to construct confidence intervals, the Bayesian approach offers a straightforward way to obtain credible intervals for point estimates. For more results related to Bayesian methods, please refer to Appendix I.2. ", "page_idx": 7}, {"type": "text", "text": "5.2 The robustness of pre-defined block sizes ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "As mentioned in Section 1, block-based algorithms require presetting block sizes, and their performance is sensitive to these parameters, posing challenges in practice. This experiment assesses the robustness of block-based algorithms with predefined block sizes. The test setup is shown in Figure 4. We vary preset block sizes and conduct 100 experiments for all algorithms. Confidence intervals in Figure 4 depict reconstruction error for statistical significance. ", "page_idx": 7}, {"type": "image", "img_path": "a4cPpx1xYg/tmp/3a38d137aecc33340974808d4f1de2333b1298a70066f5a84104dab333e3aab6.jpg", "img_caption": ["Figure 4: NMSE variation with changing preset block sizes. "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Resolves the longstanding sensitivity issue of block-based algorithms. DivSBL demonstrates strong robustness to the preset block sizes, effectively addressing the sensitivity issue that block-based algorithms commonly encounter with respect to block sizes. ", "page_idx": 7}, {"type": "text", "text": "Figure 5 visualizes the posterior variance learning on the signal to demonstrate DivSBL\u2019s ability to adaptively identify the true blocks. The algorithms are tested with preset block sizes of 20 (small), 50 (medium), and 125 (large), respectively, to show how each algorithm learns the blocks when block structure is misspecified. As expected in Section 2.1 and Figure 2 , DivSBL is able to adaptively find the true block through diversification learning and remains robust to the preset block size. ", "page_idx": 7}, {"type": "text", "text": "Exhibits enhanced recovery capability in challenging scenarios. The optimal block size for DivSBL is around 20-50, which is more consistent with the true block sizes. This indicates that when true block sizes are large and varied, DivSBL can effectively capture richer information within each block by setting larger block sizes, thereby significantly improving the recovery performance. In contrast, other algorithms do not perform as well as DivSBL, even at their optimal block sizes. ", "page_idx": 7}, {"type": "image", "img_path": "a4cPpx1xYg/tmp/2b14abf489f9cf908d530275d9c540c65a828755a912e3d6c450affda3938bdf.jpg", "img_caption": ["Figure 5: Variance learning Table 2: Reconstruction errors (NMSE $\\pm$ std) under different noise levels (sample rate $=\\!0.25)$ ). "], "img_footnote": [], "page_idx": 8}, {"type": "table", "img_path": "a4cPpx1xYg/tmp/d94594528d284b9b65945a32b28cd8fd0e717eee3b16247a01767e6f4d44d358.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "5.3 1D audioSet ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "As shown in Figure 14, audio signals exhibit block sparse structures in discrete cosine transform (DCT) basis, which is well-suited for assessing block sparse algorithms. In this subsection, we carry out experiments on real-world audios, which are randomly chosen in AudioSet [34]. The reconstruction results are present in Appendix J. In the main text, we focus on analyzing DivSBL\u2019s sensitivity to sample rate, evaluating its performance across different noise levels, and investigating its phase transition properties. ", "page_idx": 8}, {"type": "text", "text": "The sensitivity of sample rate The algorithms are tested on audio sets to investigate the sensitivity of sample rate $(M/N)$ varied from 0.25 to 0.55. The result is visualized in Figure 16. Notably, DivSBL emerges as the top performer across diverse sampling rates, showing a consistent 1 dB enhancement in NMSE compared to the best-performing algorithm among others. ", "page_idx": 8}, {"type": "text", "text": "The performance under various noise levels We assess each algorithm as Signal-to-Noise Ratio (SNR) varied from 10 to 50 and include the standard deviation from 100 random experiments on audio sets. Here, we present the performance under the minimum sampling rate 0.25 tested before, which represents a challenging recovery scenario. As shown in Table 2, the performance of all algorithms improves with higher SNR. Notably, DivSBL consistently leads across all SNR levels. ", "page_idx": 8}, {"type": "text", "text": "Phase Transition This audio data contains approximately 90 non-zero elements in DCT domain $[K=90]$ ), which constitutes about $20\\%$ of the total dimensionality ( $N=480$ ). Therefore, we start the test with a sampling rate of a same $20\\%$ . In this scenario, $M/K$ is roughly 1 and increases with the sampling rate. Concurrently, the signal-to-noise ratio (SNR) varies gradually from 10 to 50. ", "page_idx": 8}, {"type": "text", "text": "The phase transition diagram in Figure 6 shows that DivSBL performs well at more extreme sampling rates and is better suited for lower SNR conditions. ", "page_idx": 8}, {"type": "image", "img_path": "a4cPpx1xYg/tmp/3cb19de90cf61c39881316ef6c835d42062b4b86089fe336b34e4b79bbd96596.jpg", "img_caption": ["Figure 6: Phase transition diagram under different SNR and measurements. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "5.4 2D image reconstruction ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In 2D image experiments, we utilize a standard set of grayscale images compiled from two sources 6. As depicted in Figure 17, the images exhibit block sparsity in discrete wavelet domain. In Section 5.3, we\u2019ve shown DivSBL\u2019s leading performance across diverse sampling rates. Here, we use a 0.5 sample rate and the reconstruction errors are in Table 3 and Appendix K. DivSBL\u2019s reconstructions surpass others, with an average improvement of ${\\bf9.8\\%}$ on various images. ", "page_idx": 9}, {"type": "image", "img_path": "a4cPpx1xYg/tmp/b7b978f318dc3989b0eb06c8dd1a92d7c2e1fadaa5c8ae0c0f8bff009398a56b.jpg", "img_caption": ["Figure 7: Reconstruction results for Parrot, Monarch and House images. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "In Figure 7, we display the final reconstructions of Parrot, Monarch and House images as examples. DivSBL is capable of preserving the finer features of the parrot, such as cheek, eye, etc., and recovering the background smoothly with minimal error stripes. As for Monarch and House images, nearly every reconstruction introduces undesirable artifacts and stripes, while images restored by DivSBL show the least amount of noise patterns, demonstrating the most effective restoration. ", "page_idx": 9}, {"type": "text", "text": "6 Conclusions ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "This paper established a new Bayesian learning model by introducing diversified block sparse prior, to effectively capture the prevalent block sparsity observed in real-world data. The novel Bayesian model effectively solved the sensitivity issue in existing block sparse learning methods, allowing for adaptive block estimation and reducing the risk of overfitting. The proposed algorithm DivSBL, based on this model, enjoyed solid theoretical guarantees on both convergence and sparsity theory. Experimental results demonstrated its state-of-the-art performance on multimodal data. Future works include exploration on more effective weak constraints for correlation matrices, and applications on supervised learning tasks such as regression and classification. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments and Disclosure of Funding ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This research was supported by National Key R&D Program of China under grant 2021YFA1003300. The authors would like to thank all the reviewers for their helpful comments. Their suggestions have helped us enhance our experiments to present a more comprehensive demonstration of the effectiveness of DivSBL. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] David L Donoho. Compressed sensing. IEEE Transactions on Information Theory, 52(4):1289\u20131306, 2006.   \n[2] Irina F Gorodnitsky and Bhaskar D Rao. Sparse signal reconstruction from limited data using FOCUSS: A re-weighted minimum norm algorithm. IEEE Transactions on Signal Processing, 45(3):600\u2013616, 1997.   \n[3] Emmanuel J Candes and Terence Tao. Decoding by linear programming. IEEE Transactions on Information Theory, 51(12):4203\u20134215, 2005.   \n[4] Robert Tibshirani. Regression shrinkage and selection via the LASSO. Journal of the Royal Statistical Society Series B: Statistical Methodology, 58(1):267\u2013288, 1996.   \n[5] Michael E Tipping. Sparse Bayesian learning and the relevance vector machine. Journal of Machine Learning Research, 1(Jun):211\u2013244, 2001.   \n[6] Scott Shaobing Chen, David L Donoho, and Michael A Saunders. Atomic decomposition by basis pursuit. SIAM review, 43(1):129\u2013159, 2001.   \n[7] Yagyensh Chandra Pati, Ramin Rezaiifar, and Perinkulam Sambamurthy Krisnaprasad. Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition. In Proceedings of 27th Asilomar Conference on Signals, Systems and Computers, 40\u201344. IEEE, 1993.   \n[8] Pei Peng, Shirin Jalali, and Xin Yuan. Auto-encoders for compressed sensing. In NeurIPS 2019 Workshop on Solving Inverse Problems with Deep Networks, 2019.   \n[9] Ajil Jalal, Liu Liu, Alexandros G Dimakis, and Constantine Caramanis. Robust compressed sensing using generative models. Advances in Neural Information Processing Systems, 33:713\u2013727, 2020.   \n[10] Ajil Jalal, Marius Arvinte, Giannis Daras, Eric Price, Alexandros G Dimakis, and Jon Tamir. Robust compressed sensing MRI with deep generative priors. Advances in Neural Information Processing Systems, 34:14938\u201314954, 2021.   \n[11] Yonina C Eldar, Patrick Kuppinger, and Helmut Bolcskei. Block-sparse signals: Uncertainty relations and efficient recovery. IEEE Transactions on Signal Processing, 58(6):3042\u20133054, 2010.   \n[12] David L Donoho, Iain Johnstone, and Andrea Montanari. Accurate prediction of phase transitions in compressed sensing via a connection to minimax denoising. IEEE Transactions on Information Theory, 59(6):3396\u20133433, 2013.   \n[13] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter Battaglia. Generating images with sparse representations. In International Conference on Machine Learning, 7958\u20137968. PMLR, 2021.   \n[14] Peng Jiang, Lihan Hu, and Shihui Song. Exposing and exploiting fine-grained block structures for fast and accurate sparse training. Advances in Neural Information Processing Systems, 35:38345\u201338357, 2022.   \n[15] Darshan Thaker, Paris Giampouras, and Ren\u00e9 Vidal. Reverse engineering $l_{p}$ attacks: A block-sparse optimization approach with recovery guarantees. In International Conference on Machine Learning, 21253\u201321271. PMLR, 2022.   \n[16] Yuchen Fan, Jiahui Yu, Yiqun Mei, Yulun Zhang, Yun Fu, Ding Liu, and Thomas S Huang. Neural sparse representation for image restoration. Advances in Neural Information Processing Systems, 33:15394\u201315404, 2020.   \n[17] Mehdi Korki, Jingxin Zhang, Cishen Zhang, and Hadi Zayyani. Block-sparse impulsive noise reduction in OFDM systems\u2014A novel iterative Bayesian approach. IEEE Transactions on Communications, 64(1):271\u2013 284, 2015.   \n[18] Aditya Sant, Markus Leinonen, and Bhaskar D Rao. Block-sparse signal recovery via general total variation regularized sparse Bayesian learning. IEEE Transactions on Signal Processing, 70:1056\u20131071, 2022.   \n[19] Ming Yuan and Yi Lin. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society Series B: Statistical Methodology, 68(1):49\u201367, 2006.   \n[20] Sahand Negahban and Martin J Wainwright. Joint support recovery under high-dimensional scaling: Beneftis and perils of $l_{1,\\infty}$ -regularization. Advances in Neural Information Processing Systems, 21:1161\u2013 1168, 2008.   \n[21] Yasutoshi Ida, Yasuhiro Fujiwara, and Hisashi Kashima. Fast sparse group LASSO. Advances in Neural Information Processing Systems, 32, 2019.   \n[22] Ewout Van den Berg and Michael P Friedlander. Sparse optimization with least-squares constraints. SIAM Journal on Optimization, 21(4):1201\u20131229, 2011.   \n[23] Zhilin Zhang and Bhaskar D Rao. Sparse signal recovery with temporally correlated source vectors using sparse Bayesian learning. IEEE Journal of Selected Topics in Signal Processing, 5(5):912\u2013926, 2011.   \n[24] Zhilin Zhang and Bhaskar D Rao. Extension of SBL algorithms for the recovery of block sparse signals with intra-block correlation. IEEE Transactions on Signal Processing, 61(8):2009\u20132015, 2013.   \n[25] Zhilin Zhang and Bhaskar D Rao. Recovery of block sparse signals using the framework of block sparse Bayesian learning. In 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 3345\u20133348. IEEE, 2012.   \n[26] Junzhou Huang, Tong Zhang, and Dimitris Metaxas. Learning with structured sparsity. In Proceedings of the 26th Annual International Conference on Machine Learning, 417\u2013424, 2009.   \n[27] Jun Fang, Yanning Shen, Hongbin Li, and Pu Wang. Pattern-coupled sparse Bayesian learning for recovery of block-sparse signals. IEEE Transactions on Signal Processing, 63(2):360\u2013372, 2014.   \n[28] Lu Wang, Lifan Zhao, Susanto Rahardja, and Guoan Bi. Alternative to extended block sparse Bayesian learning and its relation to pattern-coupled sparse Bayesian learning. IEEE Transactions on Signal Processing, 66(10):2759\u20132771, 2018.   \n[29] Jisheng Dai, An Liu, and Hing Cheung So. Non-uniform burst-sparsity learning for massive MIMO channel estimation. IEEE Transactions on Signal Processing, 67(4):1075\u20131087, 2018.   \n[30] David JC MacKay. Bayesian interpolation. Neural Computation, 4(3):415\u2013447, 1992.   \n[31] Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the royal statistical society: series B (methodological), 39(1):1\u201322, 1977.   \n[32] Stephen P Boyd and Lieven Vandenberghe. Convex optimization. Cambridge University Press, 2004.   \n[33] Yubei Chen, Dylan Paiton, and Bruno Olshausen. The sparse manifold transform. Advances in Neural Information Processing Systems, 31, 2018.   \n[34] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset for audio events. In 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 776\u2013780. IEEE, 2017.   \n[35] Timothy CY Chan, Rafid Mahmood, and Ian Yihang Zhu. Inverse optimization: Theory and applications. Operations Research, 2023. https://doi.org/10.1287/opre.2022.0382.   \n[36] Xinyang Yi and Constantine Caramanis. Regularized EM algorithms: A unified framework and statistical guarantees. Advances in Neural Information Processing Systems, 28, 2015.   \n[37] David P Wipf, Julia P Owen, Hagai T Attias, Kensuke Sekihara, and Srikantan S Nagarajan. Robust Bayesian estimation of the location, orientation, and time course of multiple correlated neural sources using MEG. NeuroImage, 49(1):641\u2013655, 2010.   \n[38] Shane F Cotter, Bhaskar D Rao, Kjersti Engan, and Kenneth Kreutz-Delgado. Sparse solutions to linear inverse problems with multiple measurement vectors. IEEE Transactions on Signal Processing, 53(7):2477\u20132488, 2005.   \n[39] Ralph Tyrell Rockafellar. Convex Analysis. Princeton University Press, 1970. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "[40] David G Luenberger and Yinyu Ye. Linear and nonlinear programming, volume 116. Springer, 2008. ", "page_idx": 12}, {"type": "text", "text": "[41] Carlos M Carvalho, Nicholas G Polson, and James G Scott. Handling sparsity via the Horseshoe. In Artificial Intelligence and Statistics, 73\u201380. PMLR, 2009.   \n[42] Veronika Roc\u02c7kov\u00e1 and Edward I George. The spike-and-slab LASSO. Journal of the American Statistical Association, 113(521):431\u2013444, 2018.   \n[43] Daniela Calvetti, Erkki Somersalo, and A Strang. Hierachical Bayesian models and sparsity: $l_{2}$ -magic. Inverse Problems, 35(3):035003, 2019. ", "page_idx": 12}, {"type": "image", "img_path": "a4cPpx1xYg/tmp/89dec1c42c3b14380f9d35d288f286dcab17da32b5c5f1215efdf3a72b708b2a.jpg", "img_caption": ["Figure 8: NMSE with iteration number. "], "img_footnote": [], "page_idx": 13}, {"type": "text", "text": "Below, we will explain the necessity of the three steps for updating B in DivSBL: ", "page_idx": 13}, {"type": "text", "text": "(1) Firstly, the purpose of the first step, estimating B under a strong constraint, is to avoid overftiting. As shown by the green line (Diff-BSBL) in Figure 8 , algorithm without strong constraints tend to worsen NMSE after several iterations due to overfitting. ", "page_idx": 13}, {"type": "text", "text": "(2) Since the strong constraint in (1) forces all blocks to have the same correlation, it leads to the loss of specificity in correlation matrices within different blocks. Therefore, the motivation for the second step, applying weak constraints, is to make the correlations within blocks similar to some extent while preserving their individual specificities. As the black line in Figure 8 (DivSBL-without diversified correlation) shows, DivSBL without weak constraints fails to capture the specificity within blocks, resulting in a loss of accuracy and slower speed compare to DivSBL. ", "page_idx": 13}, {"type": "text", "text": "(3) The third step, Toeplitzization, inherits the advantages of BSBL, allowing the correlation matrices to have a reasonable structure. ", "page_idx": 13}, {"type": "text", "text": "Overall, while BSBL utilizes a single variance and a shared correlation matrix within each block, DivSBL incorporates diversification into both variance and correlation matrix modeling, making it more flexible and enhancing its performance. ", "page_idx": 13}, {"type": "text", "text": "B Derivation of hyperparameters updating formulas ", "text_level": 1, "page_idx": 13}, {"type": "text", "text": "In this paper, lowercase and uppercase bold symbols are employed to represent vectors and matrices, respectively. $\\operatorname*{det}(\\mathbf{A})$ denotes the determinant of matrix A. $\\operatorname{tr}(\\mathbf{A})$ means the trace of A. Matrix $\\mathrm{diag}(\\mathbf{A}_{1},\\ldots,\\mathbf{A}_{g})$ represents the block diagonal matrix with the matrices $\\{\\mathbf{A}_{i}\\}_{i=1}^{g}$ placed along the main diagonal, and $\\mathrm{Diag}(\\mathbf{A})$ represents the extraction of the diagonal elements from matrix $\\mathbf{A}$ to create a vector. We observe that ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{Q(\\{\\mathbf{G}_{i}\\}_{i=1}^{g},\\{\\mathbf{B}_{i}\\}_{i=1}^{g})\\propto-\\frac{1}{2}E_{x|y;\\Theta^{t-1}}(\\log|\\pmb{\\Sigma}_{0}|+\\mathbf{x}^{T}\\pmb{\\Sigma}_{0}\\mathbf{x})}}\\\\ &{}&{=-\\frac{1}{2}\\log|\\pmb{\\Sigma}_{0}|-\\frac{1}{2}\\operatorname{tr}\\left[\\pmb{\\Sigma}_{0}^{-1}(\\pmb{\\Sigma}+\\mu\\pmb{\\mu}^{T})\\right],}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "in which $\\Sigma_{0}$ can be reformulated as ", "page_idx": 13}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\Sigma_{0}=\\operatorname{diag}\\left\\{\\mathbf{G}_{1}\\mathbf{B}_{1}\\mathbf{G}_{1},\\cdots,\\mathbf{G}_{g}\\mathbf{B}_{g}\\mathbf{G}_{g}\\right\\}=\\mathbf{D}_{-i}+\\left(\\begin{array}{l}{\\mathbf{0}}\\\\ {\\mathbf{I}_{L}}\\\\ {\\mathbf{0}}\\end{array}\\right)\\mathbf{G}_{i}\\mathbf{B}_{i}\\mathbf{G}_{i}\\left(\\begin{array}{l l l}{\\mathbf{0}}&{\\mathbf{I}_{L}}&{\\mathbf{0}}\\end{array}\\right)}\\\\ &{\\quad\\quad=\\mathbf{D}_{-i}+\\left(\\begin{array}{l}{\\mathbf{0}}\\\\ {\\mathbf{I}_{L}}\\\\ {\\mathbf{0}}\\end{array}\\right)(\\sqrt{\\gamma_{i j}}\\mathbf{P}_{j}+\\mathbf{W}_{-j}^{i})\\mathbf{B}_{i}(\\sqrt{\\gamma_{i j}}\\mathbf{P}_{j}+\\mathbf{W}_{-j}^{i})\\left(\\begin{array}{l l l}{\\mathbf{0}}&{\\mathbf{I}_{L}}&{\\mathbf{0}}\\end{array}\\right),}\\end{array}\n$$", "text_format": "latex", "page_idx": 13}, {"type": "text", "text": "where ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{D}_{-i}=\\operatorname{diag}\\Big\\{\\mathbf{G}_{1}\\mathbf{B}_{1}\\mathbf{G}_{1},\\cdots,\\mathbf{G}_{i-1}\\mathbf{B}_{i-1}\\mathbf{G}_{i-1},\\mathbf{0}_{L\\times L},\\mathbf{G}_{i+1}\\mathbf{B}_{i+1}\\mathbf{G}_{i+1},\\cdots,\\mathbf{G}_{g}\\mathbf{B}_{g}\\mathbf{G}_{g}\\Big\\},}\\\\ &{\\mathbf{P}_{j}=\\operatorname{diag}\\{\\delta_{1j},\\delta_{2j},\\cdots,\\delta_{L j}\\},}\\\\ &{\\mathbf{W}_{-j}^{i}=\\operatorname{diag}\\{\\sqrt{\\gamma_{i1}},\\cdots,\\sqrt{\\gamma_{i,j-1}},0,\\sqrt{\\gamma_{i,j+1}},\\cdots,\\sqrt{\\gamma_{i L}}\\},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and the Kronecker delta function here, denoted by $\\delta_{i j}$ , is defined as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\delta_{i j}={\\binom{1,}{0,}}_{\\mathrm{otherwise.}}^{\\mathrm{if}\\ i}\\quad\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Hence, $\\Sigma_{0}$ is split into two components in (27), with the second term of the summation only depending on $\\mathbf{G}_{i}$ and $\\mathbf{B}_{i}$ , and the first part $\\mathbf{D}_{-i}$ being entirely unrelated to them. Then, we can update each $\\mathbf{B}_{i}$ and $\\mathbf{G}_{i}$ independently, allowing us to learn diverse $\\mathbf{B}_{i}$ and $\\mathbf{G}_{i}$ for different blocks. The gradient of (26) with respect to $\\sqrt{\\gamma_{i j}}$ can be expressed as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\partial Q(\\{\\mathbf{G}_{i}\\}_{i=1}^{g},\\{\\mathbf{B}_{i}\\}_{i=1}^{g})}{\\partial\\sqrt{\\gamma_{i j}}}=\\frac{\\partial(-\\frac{1}{2}\\log|\\Sigma_{0}|)}{\\partial\\sqrt{\\gamma_{i j}}}+\\frac{\\partial(-\\frac{1}{2}\\operatorname{tr}\\left[\\Sigma_{0}^{-1}(\\Sigma+\\mu\\mu^{T})\\right])}{\\partial\\sqrt{\\gamma_{i j}}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "The first term results in ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\partial(-\\frac{1}{2}\\log|\\Sigma_{0}|)}{\\partial\\sqrt{\\gamma_{i j}}}=-\\operatorname{tr}\\left(\\mathbf{P}_{j}(\\mathbf{G}_{i}\\mathbf{B}_{i}\\mathbf{G}_{i})^{-1}\\mathbf{G}_{i}\\mathbf{B}_{i}\\right)=-\\frac{1}{\\sqrt{\\gamma_{i j}}},\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "and the second term yields ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial(-\\frac{1}{2}\\operatorname{tr}\\big[\\mathbf{\\Sigma}\\mathbf{\\Sigma}_{0}^{-1}(\\Sigma+\\mu\\mu^{T})\\big])}{\\partial\\sqrt{\\gamma_{i j}}}=\\operatorname{tr}\\big[\\mathbf{P}_{j}(\\mathbf{G}_{i}\\mathbf{B}_{i}\\mathbf{G}_{i})^{-1}(\\Sigma^{i}+\\mu^{i}(\\mu^{i})^{T})\\mathbf{G}_{i}^{-1}\\big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad=\\operatorname{tr}\\big[\\mathbf{B}_{i}^{-1}\\mathbf{G}_{i}^{-1}(\\Sigma^{i}+\\mu^{i}(\\mu^{i})^{T})\\mathbf{P}_{j}\\gamma_{i j}^{-1}\\big]\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "where $\\pmb{\\mu}^{i}\\in\\mathbb{R}^{L\\times1}$ represents the $i$ -th block in $\\pmb{\\mu}$ , and $\\pmb{\\Sigma}^{i}\\,\\in\\,\\mathbb{R}^{L\\times L}$ denotes the $i$ -th block in $\\Sigma^{\\scriptscriptstyle7}$ Using $\\mathbf{A}_{1}(\\mathbf{M}+\\mathbf{N})\\mathbf{A}_{2}=\\mathbf{A}_{1}\\mathbf{M}\\mathbf{A}_{2}+\\mathbf{A}_{1}\\mathbf{N}\\mathbf{A}_{2},$ , the formula above can be further transformed into ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\frac{\\partial(-\\frac{1}{2}\\operatorname{tr}\\big[\\mathbf{D}_{0}^{-1}(\\mathbf{\\Sigma}+\\mu\\mu^{T})\\big])}{\\partial\\sqrt{\\gamma_{i j}}}=\\operatorname{tr}\\bigg[\\mathbf{B}_{i}^{-1}\\frac{1}{\\sqrt{\\gamma_{i j}}}\\mathbf{P}_{j}(\\mathbf{\\Sigma}+\\mu^{i}(\\mu^{i})^{T})\\mathbf{P}_{j}\\gamma_{i j}^{-1}\\bigg]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\quad+\\operatorname{tr}\\big[\\mathbf{B}_{i}^{-1}(\\mathbf{I}-\\mathbf{P}_{j})\\mathbf{G}_{i}^{-1}(\\mathbf{\\Sigma}^{i}+\\mu^{i}(\\mu^{i})^{T})\\mathbf{P}_{j}\\gamma_{i j}^{-1}\\big]}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad=(\\frac{1}{\\sqrt{\\gamma_{i j}}})^{3}\\mathbf{A}_{i j}+\\frac{1}{\\gamma_{i j}}\\mathbf{T}_{i j},}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "in which, $\\mathbf{T}_{i j}$ and $\\mathbf{A}_{i j}$ are independent of $\\sqrt{\\gamma_{i j}}$ , and their expressions are ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{T}_{i j}=\\left[(\\mathbf{B}_{i}^{-1})_{j\\cdot}\\odot\\mathrm{diag}(\\mathbf{W}_{-j}^{i})^{-1}\\right]\\cdot\\left(\\pmb{\\Sigma}^{i}+\\pmb{\\mu}^{i}(\\pmb{\\mu}^{i})^{T}\\right)_{\\cdot j},}\\\\ &{\\mathbf{A}_{i j}=(\\mathbf{B}_{i}^{-1})_{j j}\\cdot\\left(\\pmb{\\Sigma}^{i}+\\pmb{\\mu}^{i}\\left(\\pmb{\\mu}^{i}\\right)^{T}\\right)_{j j}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Thus, the derivative of $Q(\\Theta)$ with respect to $\\sqrt{\\gamma_{i j}}$ reads as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\partial Q(\\Theta)}{\\partial\\sqrt{\\gamma_{i j}}}=-\\frac{1}{\\sqrt{\\gamma_{i j}}}+(\\frac{1}{\\sqrt{\\gamma_{i j}}})^{3}\\mathbf{A}_{i j}+\\frac{1}{\\gamma_{i j}}\\mathbf{T}_{i j}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "It is important to note that the variance should be non-negative. So by setting (28) equal to zero, we obtain the update formulation of $\\gamma_{i j}$ as ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\gamma_{i j}=\\frac{4\\mathbf{A}_{i j}^{2}}{(\\sqrt{\\mathbf{T}_{i j}^{2}+4\\mathbf{A}_{i j}}-\\mathbf{T}_{i j})^{2}}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "As for the gradient of (26) with respect to $\\mathbf{B}_{i}$ , we have ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\frac{\\partial Q(\\{\\mathbf{G}_{i}\\}_{i=1}^{g},\\{\\mathbf{B}_{i}\\}_{i=1}^{g})}{\\partial\\mathbf{B}_{i}}=-\\frac{1}{2}\\mathbf{B}_{i}^{-1}+\\frac{1}{2}\\mathbf{B}_{i}^{-1}\\mathbf{G}_{i}^{-1}(\\Sigma^{i}+\\mu^{i}(\\mu^{i})^{T})\\mathbf{G}_{i}^{-1}\\mathbf{B}_{i}^{-1}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "Setting it equal to zero, the learning rule for $\\mathbf{B}_{i}$ is given by ", "page_idx": 14}, {"type": "equation", "text": "$$\n\\mathbf{B}_{i}=\\mathbf{G}_{i}^{-1}\\left(\\boldsymbol{\\Sigma}^{i}+\\mu^{i}\\left(\\mu^{i}\\right)^{T}\\right)\\mathbf{G}_{i}^{-1}.\n$$", "text_format": "latex", "page_idx": 14}, {"type": "text", "text": "C The procedure for solving the constrained optimization problem ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "To clarify the dual problem of (P), we firstly express (P)\u2019s Lagrange function as ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\Upsilon(\\{\\mathbf{B}_{i}\\}_{i=1}^{g};\\{\\lambda_{i}\\}_{i=1}^{g})=\\frac{1}{2}\\log\\operatorname*{det}\\Sigma_{0}+\\frac{1}{2}\\operatorname{tr}\\left[\\Sigma_{0}^{-1}(\\Sigma+\\mu\\mu^{T})\\right]+\\sum_{i=1}^{g}\\lambda_{i}(\\log\\operatorname*{det}\\mathbf{B}_{i}-\\log\\operatorname*{det}\\mathbf{B}).\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Since the constraints in (P) are equalities, we do not impose any requirements on the multipliers $\\{\\lambda_{i}\\}_{i=1}^{g}$ . The primal and dual problems in terms of $\\mathcal{L}$ are given by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\{\\mathbf{B}_{i}\\}_{i=1}^{g}}{\\operatorname*{min}}\\!\\!\\!\\!\\operatorname*{max}_{\\{\\lambda_{i}\\}_{i=1}^{g}}}&{{}\\mathcal{L}\\big(\\{\\mathbf{B}_{i}\\}_{i=1}^{g};\\{\\lambda_{i}\\}_{i=1}^{g}\\big),}\\\\ {\\underset{\\{\\lambda_{i}\\}_{i=1}^{g}}{\\operatorname*{max}}\\!\\!\\!\\!\\operatorname*{min}}&{{}\\mathcal{L}\\big(\\{\\mathbf{B}_{i}\\}_{i=1}^{g};\\{\\lambda_{i}\\}_{i=1}^{g}\\big),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "respectively. Although the objective here is non-convex, dual ascent method takes advantage of the fact that the dual problem is always convex [32], and we can get a lower bound of (P) by solving (D). Specifically, dual ascent method employs gradient ascent on the dual variables. As long as the step sizes are chosen properly, the algorithm would converge to a local maximum. We will demonstrate how to choose the step sizes in the following paragraph. ", "page_idx": 15}, {"type": "text", "text": "According to this framework, we first solve the inner minimization problem of the dual problem (D). Keeping the multipliers $\\{\\lambda_{i}\\}_{i=1}^{g}$ fixed, the inner problem is ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{B}_{i}^{k+1}\\in\\arg\\operatorname*{min}_{\\mathbf{B}_{i}}\\mathcal{L}(\\{\\mathbf{B}_{i}\\}_{i=1}^{g};\\{\\lambda_{i}^{k}\\}_{i=1}^{g})=\\arg\\operatorname*{min}_{\\mathbf{B}_{i}}\\mathcal{L}(\\mathbf{B}_{i};\\lambda_{i}^{k}),\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "where the superscript $k$ implies the $k$ -th iteration. According to the first-order optimality condition, the primal solution for (32) is as follows: ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\mathbf{B}_{i}^{k+1}=\\frac{\\mathbf{G}_{i}^{-1}\\left(\\boldsymbol{\\Sigma}^{i}+\\mu^{i}\\left(\\mu^{i}\\right)^{T}\\right)\\mathbf{G}_{i}^{-1}}{1+2\\lambda_{i}^{k}}.\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "Subsequently, the outer maximization problem for the multiplier $\\lambda_{i}$ (dual variable) can be addressed using the gradient ascent method. The update formulation is obtained by ", "page_idx": 15}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\lambda_{i}^{k+1}=\\lambda_{i}^{k}+\\alpha_{i}^{k}\\nabla_{\\lambda_{i}}\\mathcal{L}(\\{\\mathbf{B}_{i}^{k}\\}_{i=1}^{g};\\{\\lambda_{i}\\}_{i=1}^{g})}\\\\ &{\\qquad=\\lambda_{i}^{k}+\\alpha_{i}^{k}\\nabla_{\\lambda_{i}}\\mathcal{L}(\\mathbf{B}_{i}^{k};\\lambda_{i})}\\\\ &{\\qquad=\\lambda_{i}^{k}+\\alpha_{i}^{k}(\\log\\operatorname*{det}\\mathbf{B}_{i}^{k}-\\log\\operatorname*{det}\\mathbf{B}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 15}, {"type": "text", "text": "in which, $\\alpha_{i}^{k}$ represents the step size in the $k$ -th iteration for updating the multiplier $\\lambda_{i}$ $_{i}\\;(i=1...g)$ . Convergence is only guaranteed if the step size satisfies $\\textstyle\\sum_{k=1}^{\\infty}\\alpha_{i}^{k}=\\infty$ and $\\textstyle\\sum_{k=1}^{\\infty}(\\alpha_{i}^{k})^{2}<\\infty$ [32]. Therefore, we choose a diminishing step size $1/k$ to en sure the convergenc e. The procedure, using dual ascent method to diversify $\\mathbf{B}_{i}$ , is summarized in Algorithm 2 as follows: ", "page_idx": 15}, {"type": "table", "img_path": "a4cPpx1xYg/tmp/f5c5264aff159d614fd6694546c49c5672830488c9e1265e14b4b908c66fc24a.jpg", "table_caption": [], "table_footnote": [], "page_idx": 15}, {"type": "text", "text": "D The computing time and the explanation of one-step dual ascent ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "Computing time As our algorithm is based on Bayesian learning and involves covariance structures estimation, it is slower compared to heuristic algorithm StructOMP and solvers like CVX and SPGL1 (group Lasso, group BPDN). Here, we provide curves of NMSE versus CPU time for DivSBL, DivSBL (with complete dual ascent), and BSBL to better visualize the speed of the algorithms. In Figure 9, DivSBL shows larger NMSE reductions at each time step compared to both BSBL and fully iterative DivSBL. ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "One-step dual ascent It has been observed that imposing strong constraints $\\mathbf{B}_{i}=\\mathbf{B}$ leads to slow convergence, while not applying any constraints results in overfitting [24]. ", "page_idx": 16}, {"type": "image", "img_path": "a4cPpx1xYg/tmp/97dbe107fb6b5e17c04e3d63ac7c0a573bda6b90673f2ba2679fdecae1dde3a9.jpg", "img_caption": ["Figure 9: Comparison of Computation Time "], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Therefore, by establishing weak constraints and employing dual ascent to solve the constrained optimization problem (P), we achieve diversification on intra-block correlation matrices, which are more aligned with real-life modeling. Since the convergence speed of dual ascent is fastest in the initial few steps and sub-problems are unnecessary to be solved accurately, our initial motivation was to consider allowing it to iterate only once, which yielded promising experimental results, as shown in the Figure 9. ", "page_idx": 16}, {"type": "text", "text": "We consider providing further theoretical and intuitive explanations. From another perspective, the tuple $(\\mathbf{B}_{i}^{k+1},\\bar{\\lambda}_{i}^{k})$ satisfying the update formula (23)(24) in the text is, in fact, a KKT pair of a certain constrained optimization problem, which is summarized in Proposition 3.1. ", "page_idx": 16}, {"type": "text", "text": "Proof of Proposition 3.1 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "Proof. Construct a weak constraint function $\\psi:\\mathbb{R}^{n^{2}}\\rightarrow\\mathbb{R}$ such that: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla\\psi(\\mathbf{B}_{i}^{k+1})=\\nabla\\zeta(\\mathbf{B}_{i}^{k+1})}\\\\ &{\\quad\\psi(\\mathbf{B}_{i}^{k+1})=\\psi(\\mathbf{B})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Such a function $\\psi$ always exists. In fact, we can always construct a polynomial function $\\psi$ that satisfies the given conditions (Hermitte interpolation polynomial). The finite conditions of function values and derivatives correspond to a system of linear equations for the coefficients of the polynomial function. Since the degree of the polynomial function is arbitrary, we can always ensure that the system of linear equations has a solution by increasing the degree. ", "page_idx": 16}, {"type": "text", "text": "Since $\\nabla_{\\mathbf{B}_{i}}Q(\\{\\mathbf{B}_{i}^{k+1}\\}_{i=1}^{g},\\{\\mathbf{G}_{i}\\}_{i=1}^{g})-\\lambda_{i}^{k}\\nabla\\zeta(\\mathbf{B}_{i}^{k+1})=0,$ we have: ", "page_idx": 16}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\nabla_{\\mathbf{B}_{i}}Q(\\{\\mathbf{B}_{i}^{k+1}\\}_{i=1}^{g},\\{\\mathbf{G}_{i}\\}_{i=1}^{g})-\\lambda_{i}^{k}\\nabla\\psi(\\mathbf{B}_{i}^{k+1})=0}\\\\ &{\\qquad\\qquad\\qquad\\psi(\\mathbf{B}_{i}^{k+1})=\\psi(\\mathbf{B})}\\end{array}\n$$", "text_format": "latex", "page_idx": 16}, {"type": "text", "text": "Thus, $(\\{\\mathbf{B}_{i}^{k+1}\\}_{i=1}^{g},\\{\\lambda_{i}^{k}\\}_{i=1}^{g})$ forms a KKT pair of the above optimization problem. ", "page_idx": 16}, {"type": "text", "text": "From the proposition above, it can be observed that the tuple $(\\mathbf{B}_{i}^{k+1},\\lambda_{i}^{k})$ satisfying equation (23)(24) is, in fact, a KKT point of a certain weakly constrained optimization problem. Therefore, although we do not precisely solve the constrained optimization problem under the weak constraint $\\zeta(\\cdot)$ (i.e., $\\log\\operatorname*{det}(\\cdot)$ in the main text), we accurately solve the constrained optimization problem under the weak constraint $\\psi(\\cdot)$ . Even though this weak constraint $\\psi(\\cdot)$ is unknown, it certainly exists and may be better than the original weak constraint $\\zeta(\\cdot)$ . This perspective aligns with the concept of inverse optimization [35]. Interestingly, the technique here provides an application case for inverse optimization. From this viewpoint, we are still optimizing $Q$ function with the weak constraint $\\psi(\\cdot)$ , and the subproblems under the weak constraint $\\psi(\\cdot)$ are accurately solved. Therefore, similar to the constrained EM algorithm, it can still generate a sequence of solutions, which explains the experimental results mentioned above. ", "page_idx": 16}, {"type": "text", "text": "Meanwhile, the above method can also be viewed as a regularized EM algorithm [36]. The update formula (23) corresponds to the exact solution of the regularized $Q$ function ${\\cal Q}(\\{{\\bf B}_{i}\\}_{i=1}^{g},\\{{\\bf G}_{i}\\}_{i=1}^{g^{\\star}})-$ $\\begin{array}{r}{\\sum_{i}\\lambda_{i}^{k}\\big(\\zeta(\\mathbf{B}_{i})-\\zeta(\\mathbf{B})\\big)}\\end{array}$ , while equation (24) utilizes gradient descent to update the regularization parameters. ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 17}, {"type": "text", "text": "E Proof of Theorem 4.1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. The posterior estimation of the block sparse signal is given by $\\hat{\\textbf{x}}=\\beta\\hat{\\Sigma}\\boldsymbol{\\Phi}^{T}{\\textbf y}\\;=$ $\\left(\\beta^{-1}\\hat{\\pmb{\\Sigma}_{0}}^{-1}+\\bar{\\Phi}^{T}\\Phi\\right)^{-1}\\Phi^{T}\\mathbf{y}$ , where $\\begin{array}{r c l}{\\hat{\\Sigma}_{0}}&{=}&{\\mathrm{diag}\\left\\{\\hat{\\mathbf{G}}_{1}\\hat{\\mathbf{B}}_{1}\\hat{\\mathbf{G}}_{1},\\hat{\\mathbf{G}}_{2}\\hat{\\mathbf{B}}_{2}\\hat{\\mathbf{G}}_{2},\\cdot\\cdot\\cdot,\\hat{\\mathbf{G}}_{g}\\hat{\\mathbf{B}}_{g}\\hat{\\mathbf{G}}_{g}\\right\\}}\\end{array}$ , and $\\hat{\\mathbf{G}}_{i}=\\mathrm{diag}\\{\\sqrt{\\hat{\\gamma}_{i1}},\\cdot\\cdot\\cdot,\\sqrt{\\hat{\\gamma}_{i L}}\\}$ . ", "page_idx": 17}, {"type": "text", "text": "Let $\\hat{\\gamma}=(\\hat{\\gamma}_{11},\\cdot\\cdot\\cdot\\mathrm{~,~}\\hat{\\gamma}_{1L},\\cdot\\cdot\\cdot\\mathrm{~,~}\\hat{\\gamma}_{g1},\\cdot\\cdot\\cdot\\mathrm{~,~}\\hat{\\gamma}_{g L})^{T}$ , which is obtained by globally minimizing (35) for a given $\\hat{\\mathbf{B}}_{i}\\left(\\forall i\\right)$ . ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\operatorname*{min}_{\\gamma}\\mathcal{L}(\\gamma)=\\mathbf{y}^{T}\\boldsymbol{\\Sigma}_{y}^{-1}\\mathbf{y}+\\log\\operatorname*{det}{\\boldsymbol{\\Sigma}_{y}}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Inspired by [37], we can rewrite the first summation term as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathbf{y}^{T}\\boldsymbol{\\Sigma}_{y}^{-1}\\mathbf{y}=\\operatorname*{min}_{\\mathbf{x}}\\left\\{\\beta||\\mathbf{y}-\\mathbf{\\Phi}\\Phi\\mathbf{x}||_{2}^{2}+\\mathbf{x}^{T}\\boldsymbol{\\Sigma}_{0}^{-1}\\mathbf{x}\\right\\}.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Then (35) is equivalent to ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}{\\lefteqn{\\operatorname*{min}_{\\gamma}\\mathcal{L}(\\gamma)=\\operatorname*{min}_{\\gamma}\\left\\{\\operatorname*{min}_{\\mathbf{x}}\\left\\{\\beta||\\mathbf{y}-\\Phi\\mathbf{x}||_{2}^{2}+\\mathbf{x}^{T}\\boldsymbol{\\Sigma}_{0}^{-1}\\mathbf{x}\\right\\}+\\log\\det\\Sigma_{y}\\right\\}}}\\\\ &{}&{\\quad=\\underset{\\mathbf{x}}{\\min}\\left\\{\\underset{\\gamma}{\\min}\\left\\{\\mathbf{x}^{T}\\boldsymbol{\\Sigma}_{0}^{-1}\\mathbf{x}+\\log\\operatorname*{det}\\boldsymbol{\\Sigma}_{y}\\right\\}+\\beta||\\mathbf{y}-\\Phi\\mathbf{x}||_{2}^{2}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "So when $\\beta\\rightarrow\\infty$ , (35) is equivalent to minimizing the following problem, ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\mathbf{x}}{\\operatorname*{min}}\\left\\{\\underset{\\gamma}{\\operatorname*{min}}\\left\\{\\mathbf{x}^{T}\\Sigma_{0}^{-1}\\mathbf{x}+\\log\\operatorname*{det}\\Sigma_{y}\\right\\}\\right\\}}\\\\ &{\\mathrm{s.t.}\\quad\\;\\mathbf{y}=\\Phi\\mathbf{x}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Let $\\begin{array}{r}{g(\\mathbf{x})=\\operatorname*{min}_{\\gamma}\\left(\\mathbf{x}^{T}\\Sigma_{0}^{-1}\\mathbf{x}+\\log\\operatorname*{det}\\Sigma_{y}\\right)}\\end{array}$ , then according to Lemma 1 in [37], $g\\mathbf{(x)}$ satisfies ", "page_idx": 17}, {"type": "equation", "text": "$$\ng(\\mathbf{x})=\\mathcal{O}(1)+[M-\\operatorname*{min}(M,K L)]\\log\\beta^{-1},\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $K$ represents the estimated number of blocks and $\\beta^{-1}\\rightarrow0$ (noiseless). Therefore, when $g\\mathbf{(x)}$ achieves its minimum value by (36), $K$ will achieve its minimum value simultaneously. ", "page_idx": 17}, {"type": "text", "text": "The results in [38] demonstrate that if $\\begin{array}{r}{K_{0}<\\frac{M+1}{2L}}\\end{array}$ , then no other solution exists such that $\\mathbf{y}=\\Phi\\mathbf{x}$ ewsittihm $\\begin{array}{r}{K<\\frac{M+1}{2L}}\\end{array}$ .re, we have $K\\ge K_{0}$ , and when reaches its minimum value $K_{0}$ , the $\\hat{\\mathbf{x}}=\\mathbf{x}_{\\mathrm{true}}$ ", "page_idx": 17}, {"type": "text", "text": "F Proof of Lemma 4.2 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "Proof. We can equivalently transform the constraint $\\mathbf{Z}\\succeq\\Phi\\boldsymbol{\\Sigma}_{0}\\boldsymbol{\\Phi}^{T}+\\beta^{-1}\\mathbf{I}$ into ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbf{Z}\\succeq\\Phi\\boldsymbol{\\Sigma}_{0}\\Phi^{T}+\\beta^{-1}\\mathbf{I}\\Longleftrightarrow\\forall\\omega\\in\\mathbb{R}^{m},\\quad\\omega^{T}\\mathbf{Z}\\omega\\geq\\omega^{T}\\Phi\\boldsymbol{\\Sigma}_{0}\\Phi^{T}\\omega+\\beta^{-1}\\omega^{T}\\omega.}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "The LHS $\\omega^{T}\\mathbf{Z}\\omega$ is linear with respect to $\\mathbf{Z}$ . And for the RHS, $\\omega^{T}\\Phi\\Sigma_{0}\\Phi^{T}\\omega\\,+\\,\\beta^{-1}\\omega^{T}\\omega\\,=$ $\\mathbf{q}^{T}\\Sigma_{0}\\mathbf{q}+\\beta^{-1}\\omega^{T}\\omega$ , where $\\mathbf{q}=\\Phi^{T}\\bar{\\boldsymbol{\\omega}}$ , and $\\Sigma_{0}$ can bu reformulated as ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Sigma_{0}=\\mathrm{diag}(\\sqrt{\\gamma_{11}}\\ldots\\sqrt{\\gamma_{g L}})\\tilde{\\mathbf{B}}\\,\\mathrm{diag}(\\sqrt{\\gamma_{11}}\\ldots\\sqrt{\\gamma_{g L}})\\,}\\\\ {=\\left(\\begin{array}{c c c c}{\\gamma_{11}\\tilde{B}_{11}}&{\\sqrt{\\gamma_{11}}\\sqrt{\\gamma_{12}}\\tilde{B}_{12}}&{\\ldots}&{\\sqrt{\\gamma_{11}}\\sqrt{\\gamma_{g L}}\\tilde{B}_{1N}}\\\\ {\\vdots}&{\\vdots}&{\\vdots}\\\\ {\\sqrt{\\gamma_{g L}}\\sqrt{\\gamma_{11}}\\tilde{B}_{N1}}&{\\sqrt{\\gamma_{g L}}\\sqrt{\\gamma_{g L}}\\tilde{B}_{N2}}&{\\ldots}&{\\gamma_{g L}\\tilde{B}_{N N}}\\end{array}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Therefore, $\\begin{array}{r l}{\\mathrm{RHS}{=}}&{{}q_{1}^{2}\\tilde{B}_{11}\\gamma_{11}\\;+\\;...\\;{+}\\;\\,q_{1}q_{N}\\tilde{B}_{1N}\\sqrt{\\gamma_{g L}}\\sqrt{\\gamma_{11}}\\;+\\;...\\;{+}\\;\\,q_{N}q_{1}\\tilde{B}_{N1}\\sqrt{\\gamma_{11}}\\sqrt{\\gamma_{g L}}\\;+\\;...\\;}\\end{array}$ $q_{N}^{2}\\tilde{B}_{N N}\\gamma_{g L}+\\beta^{-1}\\omega^{T}\\omega=\\mathrm{vec}(\\mathbf{q}\\mathbf{q}^{T}\\odot\\tilde{\\mathbf{B}})^{T}(\\sqrt{\\gamma}\\otimes\\sqrt{\\gamma})+\\beta^{-1}\\omega^{T}\\omega$ which is linear with respect to $\\sqrt{\\gamma}\\otimes\\sqrt{\\gamma}$ . In conclusion, $\\mathbf{Z}\\succeq\\Phi\\boldsymbol{\\Sigma}_{0}\\Phi^{T}+\\beta^{-1}\\mathbf{I}$ is convex with respect to $\\mathbf{Z}$ and $\\sqrt{\\gamma}\\otimes\\sqrt{\\gamma}$ , ", "page_idx": 17}, {"type": "text", "text": "G Proof of Lemma 4.3 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. $\"\\implies\"$ \" Given $\\mathbf{y}^{T}\\pmb{\\Sigma}_{y}^{-1}\\mathbf{y}=C$ and $\\mathbf{u}$ satisfying $\\mathbf{y}^{T}\\mathbf{u}=C$ , without loss of generality, we choose $\\mathbf{u}\\triangleq\\Sigma_{y}^{-1}\\mathbf{y}$ , i.e., $\\mathbf{y}=\\pmb{\\Sigma}_{y}\\mathbf{u}$ . Then b can be rewritten as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbf{b}=\\left(\\boldsymbol{\\Sigma}_{y}-\\beta^{-1}\\mathbf{I}\\right)\\mathbf{u}=\\boldsymbol{\\Phi}\\boldsymbol{\\Sigma}_{0}\\boldsymbol{\\Phi}^{T}\\mathbf{u}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Applying the vectorization operation to both sides of the equation, (37) results in ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{b}=\\mathrm{vec}\\left(\\Phi\\boldsymbol{\\Sigma}_{0}\\Phi^{T}\\mathbf{u}\\right)=\\left[(\\mathbf{u}^{T}\\Phi)\\otimes\\Phi\\right]\\mathrm{vec}\\left(\\boldsymbol{\\Sigma}_{0}\\right)}\\\\ &{\\quad=\\left[(\\mathbf{u}^{T}\\Phi)\\otimes\\Phi\\right]\\mathrm{vec}\\left(\\tilde{\\mathbf{G}}\\tilde{\\mathbf{B}}\\tilde{\\mathbf{G}}\\right)}\\\\ &{\\quad=\\left[(\\mathbf{u}^{T}\\Phi)\\otimes\\Phi\\right]\\left(\\tilde{\\mathbf{G}}\\otimes\\tilde{\\mathbf{G}}\\right)\\mathrm{vec}\\left(\\tilde{\\mathbf{B}}\\right)}\\\\ &{\\quad=\\left[(\\mathbf{u}^{T}\\Phi)\\otimes\\Phi\\right]\\mathrm{diag}\\left(\\mathrm{vec}\\left(\\tilde{\\mathbf{B}}\\right)\\right)\\mathrm{Diag}\\left(\\tilde{\\mathbf{G}}\\otimes\\tilde{\\mathbf{G}}\\right)}\\\\ &{\\quad=\\left[(\\mathbf{u}^{T}\\Phi)\\otimes\\Phi\\right]\\mathrm{diag}\\left(\\mathrm{vec}\\left(\\tilde{\\mathbf{B}}\\right)\\right)\\cdot\\left(\\sqrt{\\gamma}\\otimes\\sqrt{\\gamma}\\right).}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "$\\Longleftarrow$ \" Vice versa. ", "page_idx": 18}, {"type": "text", "text": "H Proof of Theorem 4.4 ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Proof. Based on Lemma 4.3, we consider the following optimization problem: ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\gamma}{\\operatorname*{min}}}&{\\,\\,\\log\\operatorname*{det}\\Sigma_{y}}\\\\ {\\mathrm{~s.~t.~}}&{\\mathbf{P}\\left(\\sqrt{\\gamma}\\otimes\\sqrt{\\gamma}\\right)=\\mathbf{b}}\\\\ &{\\gamma\\succeq\\mathbf{0},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\pmb{\\Sigma}_{y}\\,=\\,\\beta^{-1}\\mathbf{I}+\\pmb{\\Phi}\\pmb{\\Sigma}_{0}\\pmb{\\Phi}^{T}$ , $\\mathbf{P}$ and $\\mathbf{b}$ are already defined in Lemma 4.3. In order to analyze the property of the minimization problem (38), we introduce a symmetric matrix $\\mathbf{Z}\\in\\mathbb{R}^{M\\times M}$ here. Therefore, the problem with respect to $\\mathbf{Z}$ and $\\gamma$ becomes ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\mathbf{z},\\gamma}{\\mathrm{min}}}&{\\,\\log\\operatorname*{det}\\mathbf{Z}}\\\\ {\\mathrm{s.~t.}}&{\\mathbf{Z}\\succeq\\Phi\\boldsymbol{\\Sigma}_{0}\\Phi^{T}+\\beta^{-1}\\mathbf{I}}\\\\ &{\\mathbf{P}\\left(\\sqrt{\\gamma}\\otimes\\sqrt{\\gamma}\\right)=\\mathbf{b}}\\\\ &{\\gamma\\succeq\\mathbf{0},}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "It is evident that problem (38) and problem (a) are equivalent. Denote the solution of (a) as $(\\mathbf{Z}^{\\ast},\\gamma^{\\ast})$ , so $\\gamma^{*}$ here is also the solution of (38). Thus, we will analysis the minimization problem (a) instead in the following paragraph. ", "page_idx": 18}, {"type": "text", "text": "We first demonstrate the concavity of (a). Obviously, with respect to $\\mathbf{Z}$ and $\\left(\\sqrt{\\gamma}\\otimes\\sqrt{\\gamma}\\right)$ , the objective function $\\log\\operatorname*{det}\\mathbf{Z}$ is concave, and (a.3) is convex. According to Lemma 4.2,\u221a (a.2) i\u221as convex as well. Hence, we only need to show the convexity of (a.4) with respect to $\\mathbf{Z}$ and $\\left(\\sqrt{\\gamma}\\otimes\\sqrt{\\gamma}\\right)$ . ", "page_idx": 18}, {"type": "text", "text": "It is observed that ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\gamma=\\left(\\begin{array}{l l l l}{\\mathbf{h}_{1}^{T}}&&&\\\\ &{\\mathbf{h}_{2}^{T}}&&\\\\ &&{\\ddots}&\\\\ &&&{\\mathbf{h}_{g L}^{T}}\\end{array}\\right)\\left(\\sqrt{\\gamma}\\otimes\\sqrt{\\gamma}\\right)\\triangleq\\mathbf{H}\\left(\\sqrt{\\gamma}\\otimes\\sqrt{\\gamma}\\right),\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\mathbf{h}_{i}\\,\\triangleq\\,\\left(\\delta_{i1},\\cdot\\cdot\\cdot\\,,\\delta_{i,g L}\\right)^{T}$ , $\\mathbf{H}\\,\\in\\,\\mathbb{R}^{g L\\times(g L)^{2}}$ . Based on the convexity-preserving p\u221aropert\u221ay of linear transformation [39], the constraint $\\gamma\\succeq0$ exhibits convexity with respect to $\\left(\\sqrt{\\gamma}\\otimes\\sqrt{\\gamma}\\right)$ . Therefore, (a) is concave with respect to $(\\sqrt{\\gamma}\\otimes\\sqrt{\\gamma})$ and $\\mathbf{Z}$ . So we can rewrite (a) as ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\underset{\\mathbf{z},\\sqrt{\\gamma}\\otimes\\sqrt{\\gamma}}{\\mathrm{min}}}&{\\log\\operatorname*{det}\\mathbf{z}}\\\\ {\\mathrm{s.~t.}}&{\\mathbf{Z}\\succeq\\Phi\\boldsymbol{\\Sigma}_{0}\\Phi^{T}+\\beta^{-1}\\mathbf{I}}\\\\ &{\\mathbf{P}\\left(\\sqrt{\\gamma}\\otimes\\sqrt{\\gamma}\\right)=\\mathbf{b}}\\\\ &{\\gamma\\succeq\\mathbf{0}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The minimum of (b) will achieve at an extreme point. According to the equivalence between extreme point and basic feasible solution (BFS) [40], the extreme point of (b) is a BFS to ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\left\\{\\begin{array}{l l}{\\mathbf{Z}\\succeq\\Phi\\mathbf{\\Sigma}_{0}\\Phi^{T}+\\beta^{-1}\\mathbf{I}}\\\\ {\\mathbf{P}\\left(\\sqrt{\\gamma}\\otimes\\sqrt{\\gamma}\\right)=\\mathbf{b}}\\\\ {\\gamma\\succeq\\mathbf{0}}\\end{array}\\right.,\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "which concludes $||\\sqrt{\\gamma}\\otimes\\sqrt{\\gamma}||_{0}\\leq r(\\mathbf{P})=M$ , equivalently $||\\gamma||_{0}\\leq{\\sqrt{M}}$ . This result implies that every local minimum (also a BFS to the convex polytope) must be attained at a sparse solution. ", "page_idx": 19}, {"type": "text", "text": "I The experiment of 1D signals with block sparsity ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "I.1 The reconstruction results ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "As mentioned in Section 2.1.3, homoscedasticity can be seen as a special case of our model. Therefore, we test our algorithm on homoscedastic data provided in [24]. In this dataset, each block shares the same size $L=6$ , and the amplitudes within each block follow a homoscedastic normal distribution. ", "page_idx": 19}, {"type": "text", "text": "The reconstructed results are shown in Figure 10, Figure 3 and the first part of Table 1. DivSBL demonstrates a significant improvement compared to other algorithms. ", "page_idx": 19}, {"type": "image", "img_path": "a4cPpx1xYg/tmp/763bbdcf07e5176de521a692cb19ee269d3845347ccae917378ebeda5f82ca50.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 10: The original homoscedastic signal and reconstructed results by various algorithms. $\\scriptstyle\\mathrm{N}=162$ , $\\scriptstyle\\mathbf{M}=80$ ) ", "page_idx": 19}, {"type": "text", "text": "Furthermore, we consider heteroscedastic signal, which better reflects the characteristics of real-world data. The recovery results are presented in Figure 11, Figure 3 and the second part of Table 1. ", "page_idx": 19}, {"type": "image", "img_path": "a4cPpx1xYg/tmp/6192db59e33efb86ae2d9294b33ba7ddba807da661d502eef6e6dff6661e9c4d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 19}, {"type": "text", "text": "Figure 11: The original heteroscedastic signal and reconstructed results by various algorithms. $\\scriptstyle\\mathrm{N}=162$ , $\\scriptstyle\\mathbf{M}=80$ ) ", "page_idx": 19}, {"type": "text", "text": "I.2 Reconstructed by Bayesian methods with credible intervals for point estimation ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Here we provide comparative experiments on heteroscedastic data with three classic Bayesian sparse regression methods: Horseshoe model [41], spike-and-slab Lasso [42] and hierarchical normal-gamma method [43] in Figure 12. Regarding the natural advantage of Bayesian methods in quantifying uncertainties for point estimates, we further include the posterior confidence intervals from Bayesian methods. As shown in Figure 13, DivSBL offers more stable and accurate posterior confidence intervals. ", "page_idx": 19}, {"type": "image", "img_path": "a4cPpx1xYg/tmp/140b00c8404c0697c909f6291cb2d3c6abc7a29331f26bb4691d59e181f28513.jpg", "img_caption": ["Figure 12: Reconstruction error (NMSE) and correlation. "], "img_footnote": [], "page_idx": 19}, {"type": "image", "img_path": "a4cPpx1xYg/tmp/ff418b90cf236b4fce2dbe8c396cf63315691b792d5acd16364ffef940c4be39.jpg", "img_caption": ["Figure 13: Confidence intervals for each Bayesian approach. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "J The experiment of audio signals ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "Audio signals display block sparse structures in the discrete cosine transform (DCT) basis. As illustrated in Figure 14, the original audio signal (a) transforms into a block sparse structure (b) after DCT transformation. ", "page_idx": 20}, {"type": "image", "img_path": "a4cPpx1xYg/tmp/d416a5002ed8319ca5383f7cc8fc0c938d173e435956ac5b12e58e9384c85595.jpg", "img_caption": ["Figure 14: The original signal and its sparse representation. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "We carry out experiments on realworld audio signal, which is randomly chosen in AudioSet [34]. The reconstruction results for audio signals are present in Figure 15. It is noteworthy that DivSBL exhibits an improvement of over $24.2\\%$ compared to other algorithms. ", "page_idx": 20}, {"type": "image", "img_path": "a4cPpx1xYg/tmp/d64d846bd0ee1e217749962e957e93421658d58692592e0d7a7e5b591830a994.jpg", "img_caption": [], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "Figure 15: The sparse audio signal reconstructed by respective algorithms. NMSE: (b) 0.0406 (c) 0.0572 (d) 0.1033 (e) 0.1004 (f) 0.0536 (g) 0.0669 (h) 0.1062. $N=480$ , $M=150)$ ) ", "page_idx": 20}, {"type": "text", "text": "The sensitivity of sample rate As demonstrate in Section 5.3, we tested on audio sets to investigate the sensitivity of sample rate $(M/N)$ varied from 0.25 to 0.55. DivSBL emerges as the top performer across diverse sampling rates, exhibiting a consistent 1 dB improvement in NMSE relative to the bestperforming algorithm, as depicted in Figure 16. ", "page_idx": 20}, {"type": "image", "img_path": "a4cPpx1xYg/tmp/0873ecaf96bda263e009d8609c9f856d2e63a378cfecef977624fea81fcc2a74.jpg", "img_caption": ["Figure 16: NMSE vs. sample rates. "], "img_footnote": [], "page_idx": 20}, {"type": "text", "text": "K The experiment of image reconstruction ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "As depicted in Figure 17, the images exhibit block sparsity in discrete wavelet domain. We\u2019ve created box plots for NMSE and correlation reconstruction results for each image undergoing restoration, ", "page_idx": 20}, {"type": "text", "text": "as depicted in Figures 18\u201325. It\u2019s evident that the DivSBL exhibits significant advantages in image reconstruction tasks. ", "page_idx": 21}, {"type": "image", "img_path": "a4cPpx1xYg/tmp/5ffaea52459d13e063c397ad52b2af51037d9cba80e3c0aa5c8039f852000e0e.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "a4cPpx1xYg/tmp/a946d19b28490ce635d54e8416a487d427be280a52ee0402b8ec2a9ebb2bffee.jpg", "img_caption": ["Figure 17: Parrot and House image data (the first five columns) transformed in discrete wavelet domain. "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "a4cPpx1xYg/tmp/5aaccaa7e7f9212739debd1e653917648f6e9c8d9de82dcb18398caa30a29751.jpg", "img_caption": ["Figure 18: NMSE and Corr for Parrot "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "a4cPpx1xYg/tmp/1b206872a68ec1312552b12dcc31fcba51b7f8408def20f237f4507c1f0974a6.jpg", "img_caption": ["Figure 20: NMSE and Corr for Lena "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "a4cPpx1xYg/tmp/2f54778e42933626be746fd4b09bcf2354542a1c5dc257c8959a71687d8af5c4.jpg", "img_caption": ["Figure 21: NMSE and Corr for Monarch "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "a4cPpx1xYg/tmp/2bc20e1822c95da6fed6bd785d811c18617f4ea534de070c27bb3e451a8b242e.jpg", "img_caption": ["Figure 22: NMSE and Corr for Cameraman "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "a4cPpx1xYg/tmp/e8a5a8431338408e437e653836b74fbbe5d8fb4500468626430c210489cbbcfd.jpg", "img_caption": ["Figure 23: NMSE and Corr for Boat "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "a4cPpx1xYg/tmp/9f26ff8d688ed7fdbf84de27aac273ac2108938de17edf96a3a3e0d9a99a8b55.jpg", "img_caption": ["Figure 24: NMSE and Corr for Foreman "], "img_footnote": [], "page_idx": 21}, {"type": "image", "img_path": "a4cPpx1xYg/tmp/2e9b18996858607dfb4289c9de81d1057a481e1ef61f49402dac30e682c09071.jpg", "img_caption": [], "img_footnote": [], "page_idx": 21}, {"type": "text", "text": "Figure 25: NMSE and Corr for Barbara ", "page_idx": 21}, {"type": "text", "text": "L The sensitivity to initialization ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "According to our algorithm, given the variance $\\gamma_{i j}$ , the prior covariance matrix can be obtained as $\\begin{array}{r}{\\Sigma_{0}=\\mathrm{diag}(\\sqrt{\\gamma_{11}},\\cdot\\cdot\\cdot\\cdot,\\sqrt{\\gamma_{g L}})\\tilde{\\mathbf{B}}\\mathrm{diag}(\\sqrt{\\gamma_{11}},\\cdot\\cdot\\cdot\\cdot,\\sqrt{\\gamma_{g L}})}\\end{array}$ . In the absence of any structural information, the initial correlation matrix $\\tilde{\\mathbf{B}}$ is set to the identity matrix. Consequently, the mean and covariance matrix for the first iteration can also be determined. Since other variables are derived from the variance, we only need to test the sensitivity to the initial values of variances $\\gamma$ . ", "page_idx": 21}, {"type": "text", "text": "", "page_idx": 22}, {"type": "text", "text": "We test the sensitivity of DivSBL to initialization on the heteroscedastic signal from Section 5.1. Initial variances are set to $\\gamma=\\eta\\cdot\\mathrm{ones}(g L,1)$ and $\\gamma=\\eta\\cdot\\mathrm{rand}(g L,1)$ with the scale parameter $\\eta$ ranging from $1\\times10^{-1}$ to $1\\times10^{4}$ . The result in Figure 26 shows that while initialization could affect the convergence speed to some extent, the algorithm\u2019s overall convergence is assured. ", "page_idx": 22}, {"type": "image", "img_path": "a4cPpx1xYg/tmp/a936bd67050927a187b9aeacf025a05fd094d1e4d36b058def05dc5389a74851.jpg", "img_caption": ["Figure 26: The sensitivity to initialization for DivSBL. "], "img_footnote": [], "page_idx": 22}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 23}, {"type": "text", "text": "Justification: The main claims present in abstract and introduction (Section 1) have reflected the paper\u2019s contributions and scope. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 23}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 23}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Justification: In Theorem 4.1 and Theorem 4.4, we conclude the proof under a noiseless assumption. And we conjecture that similar conclusion hold even in the noise case. In Section 5.3, we evaluate the algorithm\u2019s robustness at different signal-to-noise ratios (SNR). The experimental results consistently show leading performance of the proposed algorithm across various noise environments. ", "page_idx": 23}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 23}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 23}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: All the theorems and assumptions are clearly stated in Section 4. Appendix E\u2013H present the proof of them. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 24}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 24}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Justification: For reproducibility, experiment details are provided in each subsection of Section 5. Additionally, the code is available in the Supplementary Material. ", "page_idx": 24}, {"type": "text", "text": "Guidelines: ", "page_idx": 24}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 24}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: For reproducibility, experiment code is available in Supplementary Material. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 25}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The full details are provided with code. ", "page_idx": 25}, {"type": "text", "text": "Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 25}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 25}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 25}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 25}, {"type": "text", "text": "Justification: The results are accompanied by confidence intervals, as shown in Section 5. Guidelines: ", "page_idx": 25}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 25}, {"type": "text", "text": "", "page_idx": 26}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The paper have provided compute time in Section 5 and Appendix D. Details on compute worker are provided in code (README.md). ", "page_idx": 26}, {"type": "text", "text": "Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 26}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 26}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 26}, {"type": "text", "text": "Justification: The research conducted in the paper adheres to the NeurIPS Code of Ethics. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 26}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 26}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 26}, {"type": "text", "text": "Justification: There is no societal impact of the work performed. The paper aims to explore new technical methods or algorithms without delving into topics related to societal impact. Guidelines: ", "page_idx": 26}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed. \u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. ", "page_idx": 26}, {"type": "text", "text": "\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 27}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 27}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 27}, {"type": "text", "text": "Justification: The paper poses no such risks. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 27}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 27}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 27}, {"type": "text", "text": "Justification: The data and code used in the paper have all been properly credited , and citations are provided in the text. ", "page_idx": 27}, {"type": "text", "text": "Guidelines: ", "page_idx": 27}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 27}, {"type": "text", "text": "", "page_idx": 28}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 28}, {"type": "text", "text": "Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 28}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 28}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 28}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 28}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 28}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 28}, {"type": "text", "text": "", "page_idx": 29}]