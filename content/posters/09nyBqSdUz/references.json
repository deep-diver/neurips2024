{"references": [{"fullname_first_author": "Jonathan Ho", "paper_title": "Classifier-free diffusion guidance", "publication_date": "2022-07-12", "reason": "This paper introduces classifier-free guidance, a technique that improves the quality and controllability of diffusion models, which is highly relevant to the current paper's method."}, {"fullname_first_author": "Jonathan Ho", "paper_title": "Video diffusion models", "publication_date": "2022-12-01", "reason": "This paper introduces video diffusion models, which are fundamental to the video generation aspect of the current work."}, {"fullname_first_author": "Yoad Tewel", "paper_title": "Training-free consistent text-to-image generation", "publication_date": "2024-02-01", "reason": "This paper introduces a training-free method for consistent image generation, providing a strong baseline for the current paper's training-free approach."}, {"fullname_first_author": "Jay Zhangjie Wu", "paper_title": "Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation", "publication_date": "2023-10-26", "reason": "This paper explores text-to-video generation using diffusion models, providing relevant context to the current paper's approach to video generation."}, {"fullname_first_author": "Dustin Podell", "paper_title": "SDXL: Improving latent diffusion models for high-resolution image synthesis", "publication_date": "2023-07-01", "reason": "This paper introduces SDXL, the foundational model used in the experimental evaluation of the current paper, making it crucial for understanding the results."}]}