{"references": [{"fullname_first_author": "T. B. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-00-00", "reason": "This paper is foundational to the field of large language models and the use of Transformers in this area."}, {"fullname_first_author": "A. Gu", "paper_title": "Modeling Sequences with Structured State Spaces", "publication_date": "2023-00-00", "reason": "This thesis introduces the core concepts of state space models (SSMs) that are essential to understanding the proposed MOHAWK distillation method."}, {"fullname_first_author": "A. Gu", "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces", "publication_date": "2023-00-00", "reason": "This paper introduces the Mamba architecture, the student model used in the proposed distillation technique."}, {"fullname_first_author": "A. Katharopoulos", "paper_title": "Transformers are rnns: Fast autoregressive transformers with linear attention", "publication_date": "2020-00-00", "reason": "This paper presents an alternative subquadratic attention mechanism that is compared against in the paper"}, {"fullname_first_author": "S. Gunasekar", "paper_title": "Textbooks are all you need", "publication_date": "2023-00-00", "reason": "This paper provides the teacher model used in the distillation process, which is crucial to the experiment's design and results."}]}