[{"figure_path": "164QnJsYjF/figures/figures_1_1.jpg", "caption": "Figure 1: The Distributed Representation for Dense Associative Memory (DrDAM) approximates both the energy and fixed-point dynamics of the traditional Memory Representation for Dense Associative Memory (MrDAM) while having a parameter space of constant size. A) Diagram of DrDAM using a basis function parameterized by random features (e.g., see eq. (8)). In the distributed representation, adding new memories does not change the size of the memory tensor. B) Comparing energy descent dynamics between DrDAM and MrDAM on 3x64x64 images from Tiny Imagenet [11]. Both models are initialized on queries where the bottom two-thirds of pixels are occluded with zeros; dynamics are run while clamping the visible pixels and their collective energy traces shown. DrDAM achieves the same fixed points as MrDAM, and these final fixed points have the same energy. The energy decreases with time for both MrDAM and DrDAM, although the dependence of the energy relaxation towards the fixed point is sometimes different between the two representations. Experimental setup is described in appendix D.", "description": "This figure compares two methods for dense associative memory: the traditional memory representation (MrDAM) and a new distributed representation (DrDAM) using random features.  Part A illustrates the DrDAM architecture, showing how random features are used to approximate energy and how adding new memories doesn't increase the number of parameters. Part B shows a comparison of energy descent dynamics for both methods on image completion tasks, demonstrating that DrDAM closely approximates MrDAM's behavior.", "section": "1 Introduction"}, {"figure_path": "164QnJsYjF/figures/figures_1_2.jpg", "caption": "Figure 1: The Distributed Representation for Dense Associative Memory (DrDAM) approximates both the energy and fixed-point dynamics of the traditional Memory Representation for Dense Associative Memory (MrDAM) while having a parameter space of constant size. A) Diagram of DrDAM using a basis function parameterized by random features (e.g., see eq. (8)). In the distributed representation, adding new memories does not change the size of the memory tensor. B) Comparing energy descent dynamics between DrDAM and MrDAM on 3x64x64 images from Tiny Imagenet [11]. Both models are initialized on queries where the bottom two-thirds of pixels are occluded with zeros; dynamics are run while clamping the visible pixels and their collective energy traces shown. DrDAM achieves the same fixed points as MrDAM, and these final fixed points have the same energy. The energy decreases with time for both MrDAM and DrDAM, although the dependence of the energy relaxation towards the fixed point is sometimes different between the two representations. Experimental setup is described in appendix D.", "description": "This figure demonstrates that the proposed DrDAM model effectively approximates both the energy and fixed-point dynamics of the traditional MrDAM model, while maintaining a constant parameter space size, regardless of the number of stored memories.  Panel A shows a schematic of the DrDAM architecture using random features, while Panel B provides a comparison of the energy descent dynamics between DrDAM and MrDAM.  The comparison shows that both models converge to the same fixed points with similar energy levels, even though the energy descent process itself can vary.", "section": "1 Introduction"}, {"figure_path": "164QnJsYjF/figures/figures_2_1.jpg", "caption": "Figure 2: DrDAM achieves parameter compression over MrDAM, successfully storing 20 different 64x64x3 images from TinyImagenet [11] and retrieving them when occluding the lower 40% of each query. The memory matrix of MrDAM is of shape (20, 12288) while the memory tensor of DrDAM is of shape Y = 2105, a ~20% reduction in the number of parameters compared to MrDAM; all other configurations for this experiment match those in appendix D. Further compression can be achieved with a higher tolerance for DrDAM's retrieval error, smaller \u03b2, and fewer occluded pixels, see \u00a7 4. Top: Occluded query images. Middle: Fixed-point retrievals from DrDAM. Bottom: (ground truth) Fixed-point retrievals of MrDAM.", "description": "This figure compares the performance of DrDAM and MrDAM in storing and retrieving images from the TinyImagenet dataset.  DrDAM demonstrates parameter compression by achieving similar retrieval accuracy with fewer parameters than MrDAM. The figure shows examples of occluded query images, the retrieved images from DrDAM, and the ground truth from MrDAM, highlighting the similarity between the two methods while demonstrating DrDAM's reduced parameter count. The results suggest that DrDAM offers a more efficient way to store and recall images with minimal loss of accuracy.", "section": "Empirical evaluation"}, {"figure_path": "164QnJsYjF/figures/figures_7_1.jpg", "caption": "Figure 3: DrDAM produces better approximations to the energies and gradients of MrDAM when the queries are closer to the stored patterns. Approximation quality improves with larger feature dimension Y, but decreases with higher \u03b2 and higher pattern dimension D. Approximation error is computed on 500 stored binary patterns normalized between {0, 1}. The Mean Approximation Errors (MAE, eq. (14)) is taken over 500 queries initialized: at stored patterns (i.e., queries equal the stored patterns), near stored patterns (i.e., queries equal the stored patterns where 10% of the bits have been flipped), and randomly (i.e., queries are random and far from stored patterns). Error bars represent the standard error of the mean but are visible only at poor approximations. Red horizontal lines represent the expected error of random energies and gradients. The theoretical error upper bounds of eq. (13) (dark curves on the gradient errors in the right plot only) show a tight fit to empirical results at low \u03b2 and D and are only shown if predictions are \u201cbetter than random\u201d. The shaded area shows the difference between the theoretical bound and the empirical results.", "description": "This figure shows the approximation error of DrDAM compared to MrDAM for energy and gradients.  It demonstrates how well DrDAM approximates MrDAM's energy and gradient under different conditions, considering queries at stored patterns, near stored patterns, and random queries.  The results are shown for varying feature dimension (Y), inverse temperature (\u03b2), and pattern dimension (D).  Error bars represent standard error of the mean. Red lines indicate the error of a random guess. The plot shows DrDAM performs better when queries are closer to stored patterns and that approximation quality improves with larger feature dimension (Y) but decreases with higher inverse temperature (\u03b2) and pattern dimension (D).", "section": "4 Empirical evaluation"}, {"figure_path": "164QnJsYjF/figures/figures_7_2.jpg", "caption": "Figure 3: DrDAM produces better approximations to the energies and gradients of MrDAM when the queries are closer to the stored patterns. Approximation quality improves with larger feature dimension Y, but decreases with higher \u03b2 and higher pattern dimension D. Approximation error is computed on 500 stored binary patterns normalized between {0, 1}. The Mean Approximation Errors (MAE, eq. (14)) is taken over 500 queries initialized: at stored patterns (i.e., queries equal the stored patterns), near stored patterns (i.e., queries equal the stored patterns where 10% of the bits have been flipped), and randomly (i.e., queries are random and far from stored patterns). Error bars represent the standard error of the mean but are visible only at poor approximations. Red horizontal lines represent the expected error of random energies and gradients. The theoretical error upper bounds of eq. (13) (dark curves on the gradient errors in the right plot only) show a tight fit to empirical results at low \u03b2 and D and are only shown if predictions are \u201cbetter than random\u201d. The shaded area shows the difference between the theoretical bound and the empirical results.", "description": "This figure analyzes the approximation quality of DrDAM compared to MrDAM under various conditions.  It shows how the Mean Absolute Error (MAE) in energy and gradient approximations changes with the number of random features (Y), inverse temperature (\u03b2), and pattern dimension (D).  Three scenarios are considered for query initialization: at stored patterns, near stored patterns (10% bit flips), and random queries.  The figure includes error bars, theoretical error bounds, and a comparison against random guesses, providing a comprehensive assessment of DrDAM's accuracy across diverse settings.", "section": "4 Empirical evaluation"}, {"figure_path": "164QnJsYjF/figures/figures_8_1.jpg", "caption": "Figure 4: A) Retrieval errors predictably follow the approximation quality of fig. 3. Error is lowest at/near stored patterns but is completely random when energy and gradient approximations are poor, i.e., at high values of \u03b2 and D. Note that error improves across Y but follows a different (and noisier) trace than the corresponding approximations for energy and gradient in fig. 3 due to error accumulating over multiple update steps. B) DrDAM\u2019s approximation quality improves as Y increases (visible at low \u03b2), but larger Y\u2019s are needed for good approximations to the DAM\u2019s fixed points at higher \u03b2\u2019s. (Left) The same corrupted query from CIFAR-10 where bottom 50% is masked is presented to DAM\u2019s with different \u03b2\u2019s. (Middle) The fixed points of DrDAM for each \u03b2 at different sizes Y of the feature space. (Right) The \u201cground truth\u201d fixed point of MrDAM. The top 50% of pixels are clamped throughout the dynamics.", "description": "This figure shows the relationship between retrieval error, approximation quality, and hyperparameters (\u03b2 and Y). Part A demonstrates how retrieval errors correlate with approximation quality from Figure 3, emphasizing the randomness of errors at high \u03b2 and D. Part B visually demonstrates how DrDAM\u2019s approximation quality improves with increasing Y, especially at lower \u03b2 values, but requires larger Y for good approximations at higher \u03b2 values. The experiment uses a corrupted CIFAR-10 image where the bottom half is masked, showing the fixed points of DrDAM and MrDAM for different \u03b2 and Y values.", "section": "4 Empirical evaluation"}, {"figure_path": "164QnJsYjF/figures/figures_16_1.jpg", "caption": "Figure 3: DrDAM produces better approximations to the energies and gradients of MrDAM when the queries are closer to the stored patterns. Approximation quality improves with larger feature dimension Y, but decreases with higher \u03b2 and higher pattern dimension D. Approximation error is computed on 500 stored binary patterns normalized between {0, 1}. The Mean Approximation Errors (MAE, eq. (14)) is taken over 500 queries initialized: at stored patterns (i.e., queries equal the stored patterns), near stored patterns (i.e., queries equal the stored patterns where 10% of the bits have been flipped), and randomly (i.e., queries are random and far from stored patterns). Error bars represent the standard error of the mean but are visible only at poor approximations. Red horizontal lines represent the expected error of random energies and gradients. The theoretical error upper bounds of eq. (13) (dark curves on the gradient errors in the right plot only) show a tight fit to empirical results at low \u03b2 and D and are only shown if predictions are \u201cbetter than random\u201d. The shaded area shows the difference between the theoretical bound and the empirical results.", "description": "This figure shows the approximation errors of DrDAM compared to MrDAM for energy and gradients.  It demonstrates that DrDAM's accuracy is better when queries are close to stored patterns, increases with more features (Y), but decreases with higher inverse temperature (\u03b2) and dimensionality (D). The plots show MAE for three query types: at, near, and far from stored patterns.  The results are compared to the expected error of random guesses and theoretical upper bounds.", "section": "4 Empirical evaluation"}]