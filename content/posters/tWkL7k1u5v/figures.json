[{"figure_path": "tWkL7k1u5v/figures/figures_3_1.jpg", "caption": "Figure 1: Standard training of equivariant NNs is constrained to a limited parameter space which can result in a challenging training process. We propose to relax these equivariant constraints during training, allowing optimization over a broader space of approximately equivariant models. During testing, we project the trained model back to the constrained space\u2014arriving at an equivariant model with enhanced performance compared to equivalent models trained with the standard process.", "description": "This figure compares standard training of equivariant neural networks (NNs) with the proposed training method using constraint relaxation. Standard training restricts the optimization process to a limited parameter space, potentially hindering performance.  The proposed method introduces an additional non-equivariant term during training, expanding the search space to include approximately equivariant networks.  At testing, the model is projected back into the equivariant space, aiming for a better solution than standard training.", "section": "3 Method"}, {"figure_path": "tWkL7k1u5v/figures/figures_7_1.jpg", "caption": "Figure 2: Test accuracy on ModelNet40 classification, during training of equivariant PointNet and DGCNN using a baseline training process and different versions of our method. The accuracy is computed for the equivariant models, i.e. for the models after they are projected in the equivariant space.", "description": "This figure shows the test accuracy for the ModelNet40 classification task during the training process for two different equivariant neural networks: PointNet and DGCNN.  Five different training methods are compared: the proposed method with and without different components (theta scheduling, Lie derivative regularization, and overall regularization), and the original training method for the baseline. Notably, the accuracy is calculated for the equivariant models after projection into the equivariant space. This highlights the performance improvement achieved by the proposed optimization framework, especially in comparison to the original training method.", "section": "4 Experiments"}, {"figure_path": "tWkL7k1u5v/figures/figures_7_2.jpg", "caption": "Figure 3: (a) Norm of the total Lie derivative of the relaxed PointNet model trained with and without the Lie derivative regularization term. For the computation of the Lie derivative we use the method proposed in Gruver et al. (2023). (b) Value of the Lie derivative regularization term for each individual layer of the relaxed PointNet model while we train using our framework and with Lie derivative regularization weight set to \u03bbreg = 0.01", "description": "Figure 3 shows two plots. Plot (a) compares the total Lie derivative of the relaxed PointNet model when trained with and without Lie derivative regularization during training. The Lie derivative serves as a measure of the model's deviation from equivariance. Plot (b) displays the distribution of the Lie derivative regularization term across different layers of the same network during training. The regularization term encourages the model to remain within the equivariant space. ", "section": "3 Method"}, {"figure_path": "tWkL7k1u5v/figures/figures_8_1.jpg", "caption": "Figure 4: Mean Average Error on the Nbody particle simulation for (a) different model sizes, (b): different dataset sizes.", "description": "This figure displays the results of experiments evaluating the performance of the proposed optimization framework on the N-body particle simulation task. Two subfigures present the mean average error: (a) shows the relationship between model size (number of message-passing layers) and error, demonstrating that the proposed method achieves lower error across different model sizes. (b) illustrates the impact of dataset size (number of training samples) on error, indicating the proposed method's consistent performance improvement, even with fewer data points.", "section": "4.2 Scaling on Different Model and Dataset Sizes"}, {"figure_path": "tWkL7k1u5v/figures/figures_15_1.jpg", "caption": "Figure 5: ModelNet40 classification accuracy on the validation set using our proposed method with different values of \u03bbreg. The base model used was the VN-PointNet. The model was trained on a split of the training set containing 80% of the training data. The other 20% of the data were held out as the validation set used to evaluate the model.", "description": "This figure shows the validation accuracy of a VN-PointNet model trained with the proposed method using different values of the regularization parameter (\u03bbreg).  The experiment used an 80/20 split for training and validation, respectively.  The plot demonstrates the impact of \u03bbreg on the model's performance, revealing an optimal value that balances model complexity and generalization.", "section": "4 Experiments"}, {"figure_path": "tWkL7k1u5v/figures/figures_15_2.jpg", "caption": "Figure 1: Standard training of equivariant NNs is constrained to a limited parameter space which can result in a challenging training process. We propose to relax these equivariant constraints during training, allowing optimization over a broader space of approximately equivariant models. During testing, we project the trained model back to the constrained space\u2014arriving at an equivariant model with enhanced performance compared to equivalent models trained with the standard process.", "description": "This figure illustrates the difference between standard training of equivariant neural networks and the proposed training method using constraint relaxation.  Standard training restricts the model to a smaller parameter space, potentially hindering optimization.  The proposed method relaxes these constraints during training, enabling exploration of a larger hypothesis space containing approximately equivariant models. After training, the model is projected back to the constrained space of equivariant models for testing, leading to improved performance.", "section": "3 Method"}]