[{"figure_path": "VUBtAcQN44/tables/tables_5_1.jpg", "caption": "Table 1: Quantitative results on the generated datasets. The bigger the SA-SOR, the better the performance of ranking. The smaller the MAE, the better the performance of segmentation. The best results are indicated in bold. The method by Lin [1] is not yet open-source, so we only asked them for the results on RVSOD.", "description": "This table presents a quantitative comparison of the proposed VSOR method against several state-of-the-art methods on two datasets, RVSOD and DAVSOD.  The performance is evaluated using two metrics: SA-SOR (higher is better) which reflects both segmentation and ranking quality, and MAE (lower is better) which measures the accuracy of segmentation.  The table highlights the superior performance of the proposed method across both datasets and metrics.", "section": "4.2 Compared to State-of-the-art Methods"}, {"figure_path": "VUBtAcQN44/tables/tables_6_1.jpg", "caption": "Table 2: Ablation study for our temporal interaction. Our baseline is the Liu's [2] model without person prior. Then we consider three temporal features: global-aware temporal information(GTRM), instance-aware features(ITRM), and motion-aware features(MTRM). Our final method combines the above three temporal features simultaneously.", "description": "This table presents the ablation study results for different temporal interaction methods in the proposed VSOR model.  It compares the performance of the baseline model (without temporal features) against versions incorporating global temporal information (GTRM), instance-level temporal information (ITRM), and motion-aware temporal information (MTRM).  Finally, it shows the results of combining all three types of temporal features in the unified model. The performance is evaluated using SA-SOR on both RVSOD and DAVSOD datasets.", "section": "4.3 Ablation Study"}, {"figure_path": "VUBtAcQN44/tables/tables_7_1.jpg", "caption": "Table 3: Quantitative comparison by varying the bounding box sizes.", "description": "This table presents a quantitative comparison of the model's performance on the RVSOD and DAVSOD datasets when varying the bounding box size used for motion-aware temporal relation modeling. The SA-SOR metric is used to evaluate the model's ranking performance, with higher scores indicating better performance. The table shows that expanding the bounding box size by a factor of 2 yields the best results, suggesting an optimal balance between capturing motion information and avoiding excessive background noise.", "section": "4.3 Ablation Study"}, {"figure_path": "VUBtAcQN44/tables/tables_8_1.jpg", "caption": "Table 4: ablation study for our fusion methods. Our baseline is the Liu's [2] model without person prior. Then we merge spatio-temporal relationships in three different orders.", "description": "This table presents the ablation study on different strategies for fusing spatial and temporal features in the proposed VSOR model. The baseline is Liu's model without temporal relationship modeling (TRM). Three different fusion strategies are compared: spatial-then-temporal, temporal-then-spatial, and the proposed joint fusion method. The results are evaluated using the SA-SOR metric on RVSOD and DAVSOD datasets. The table shows that the joint fusion method significantly outperforms the other two stage-wise fusion methods, demonstrating its superiority in effectively integrating spatial and temporal information for video salient object ranking.", "section": "4.3 Ablation Study"}, {"figure_path": "VUBtAcQN44/tables/tables_13_1.jpg", "caption": "Table 5: Statistical analysis of DAVSOD dataset for video salient object ranking. Semi valid indicates that some video frames in the scene only have one salient instance, valid indicates that the scene or video frame contains multiple salient instances, and invalid indicates that the video frame only contains one salient instance.", "description": "This table presents a statistical analysis of the DAVSOD dataset used for video salient object ranking.  It breaks down the number of scenes and images into categories based on the number of salient instances present in each video frame: semi-valid (some frames have one, others have multiple instances), valid (multiple salient instances), and invalid (only one salient instance). This categorization helps understand the dataset's characteristics and its suitability for evaluating video salient object ranking models.", "section": "4.1 Experimental Setup"}, {"figure_path": "VUBtAcQN44/tables/tables_13_2.jpg", "caption": "Table 1: Quantitative results on the generated datasets. The bigger the SA-SOR, the better the performance of ranking. The smaller the MAE, the better the performance of segmentation. The best results are indicated in bold. The method by Lin [1] is not yet open-source, so we only asked them for the results on RVSOD.", "description": "This table presents a quantitative comparison of the proposed VSOR model against several state-of-the-art methods on two video saliency ranking datasets (RVSOD and DAVSOD).  The performance is evaluated using two metrics: SA-SOR (which measures both ranking and segmentation quality, with higher scores being better) and MAE (which measures segmentation accuracy, with lower scores being better).  The table shows that the proposed method outperforms other methods on both metrics across datasets, highlighting its effectiveness in video salient object ranking. Note that results for Lin's method are only provided for RVSOD due to the method not being publicly available.", "section": "4.2 Compared to State-of-the-art Methods"}]