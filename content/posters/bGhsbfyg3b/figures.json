[{"figure_path": "bGhsbfyg3b/figures/figures_3_1.jpg", "caption": "Figure 1: Left: The pretraining procedure and architecture of OMIS. The pretraining steps are as follows: (1) Train BRs against all policies in \u03a0train. (2) Continuously sample opponent policy from \u03a0train and collect training data by playing against it using its BR. (3) Train a Transformer model using ICL-based supervised learning, where the model consists of three components: an actor \u03c0\u03b8, an opponent imitator \u03bc\u03c6, and a critic V\u03c9. Right: The testing procedure of OMIS. During testing, OMIS refines \u03c0\u03b8 through DTS at each timestep. The DTS steps are as follows: (1) Do multiple L-step rollouts for each legal action, where \u03c0\u03b8 and \u03bc\u03c6 are used to simulate actions for the self-agent and opponent, respectively. V\u03c9 is used to estimate the value of final search states. (2) Estimate a value Q for all legal actions, and the search policy \u03c0search selects the legal action with the maximum Q. (3) Use mixing technique to trade-off between \u03c0search and \u03c0\u03b8 to choose the real action to be executed.", "description": "This figure illustrates the overall architecture and training/testing procedures of the Opponent Modeling with In-context Search (OMIS) model. The left side details the pretraining phase, which involves training best responses (BRs) against various opponent policies from the training set (\u03a0train), collecting training data via self-play using these BRs, and finally training a Transformer model (consisting of an actor, opponent imitator, and critic) using in-context learning (ICL).  The right side illustrates the testing phase where, upon encountering an unknown opponent, OMIS employs decision-time search (DTS) to refine its actor policy.  The DTS involves multiple rollouts for each legal action to predict returns, selecting the best action, and using a mixing technique to combine the DTS-refined policy with the original policy.", "section": "4 Methodology"}, {"figure_path": "bGhsbfyg3b/figures/figures_6_1.jpg", "caption": "Figure 1: Left: The pretraining procedure and architecture of OMIS. The pretraining steps are as follows: (1) Train BRs against all policies in \u03a0train. (2) Continuously sample opponent policy from \u03a0train and collect training data by playing against it using its BR. (3) Train a Transformer model using ICL-based supervised learning, where the model consists of three components: an actor \u03c0\u03b8, an opponent imitator \u03bc\u03c6, and a critic V\u03c9. Right: The testing procedure of OMIS. During testing, OMIS refines \u03c0\u03b8 through DTS at each timestep. The DTS steps are as follows: (1) Do multiple L-step rollouts for each legal action, where \u03c0\u03b8 and \u03bc\u03c6 are used to simulate actions for the self-agent and opponent, respectively. V\u03c9 is used to estimate the value of final search states. (2) Estimate a value Q for all legal actions, and the search policy \u03c0search selects the legal action with the maximum Q. (3) Use mixing technique to trade-off between \u03c0search and \u03c0\u03b8 to choose the real action to be executed.", "description": "This figure illustrates the overall architecture and training/testing procedures of the OMIS model. The left panel shows the pretraining phase where best responses are trained against various opponent policies and a transformer model is trained with three components (actor, opponent imitator, critic) using in-context learning.  The right panel depicts the testing phase where the pretrained model uses decision-time search (DTS) to refine its policy by simulating multiple L-step rollouts for each possible action, estimating their values, and selecting the best action using a mixing strategy to balance search and direct policy output.", "section": "Methodology"}, {"figure_path": "bGhsbfyg3b/figures/figures_7_1.jpg", "caption": "Figure 3: Average results of testing under different [seen : unseen] ratios, where E = 20.", "description": "This figure shows the average return of different opponent modeling methods across three environments (Predator Prey, Level-Based Foraging, and Overcooked) under various ratios of seen to unseen opponents.  The x-axis represents the ratio of seen to unseen opponents in the test set, and the y-axis represents the average return achieved by each method.  Error bars show the standard deviation. The figure demonstrates the performance and stability of OMIS (Opponent Modeling with In-context Search) in adapting to unseen opponents compared to several baselines (PFAs, TFAs, and DTS-based methods).", "section": "5 Experiments"}, {"figure_path": "bGhsbfyg3b/figures/figures_7_2.jpg", "caption": "Figure 3: Average results of testing under different [seen : unseen] ratios, where E = 20.", "description": "This figure shows the average return of different OM methods under various ratios of seen and unseen opponent policies. The x-axis represents the ratio of seen to unseen policies, and the y-axis represents the average return. The results show that OMIS consistently outperforms other baselines across three environments, demonstrating better adaptability to unseen opponents.", "section": "5.2 Empirical Analysis"}, {"figure_path": "bGhsbfyg3b/figures/figures_8_1.jpg", "caption": "Figure 3: Average results of testing under different [seen : unseen] ratios, where E = 20.", "description": "This figure presents the average testing results of various opponent modeling approaches across three different environments (Predator Prey, Level-Based Foraging, and Overcooked) under different ratios of seen and unseen opponent policies during testing. The x-axis represents the ratio of seen to unseen opponent policies, while the y-axis represents the average return achieved by each approach. The figure demonstrates the performance and stability of OMIS across various ratios of seen and unseen opponent policies, highlighting its effectiveness in adapting to unknown opponents.  Error bars likely represent standard deviation.", "section": "5.2 Empirical Analysis"}, {"figure_path": "bGhsbfyg3b/figures/figures_8_2.jpg", "caption": "Figure 1: Left: The pretraining procedure and architecture of OMIS. The pretraining steps are as follows: (1) Train BRs against all policies in \u03a0train. (2) Continuously sample opponent policy from \u03a0train and collect training data by playing against it using its BR. (3) Train a Transformer model using ICL-based supervised learning, where the model consists of three components: an actor \u03c0\u03b8, an opponent imitator \u03bc\u03c6, and a critic V\u03c9. Right: The testing procedure of OMIS. During testing, OMIS refines \u03c0\u03b8 through DTS at each timestep. The DTS steps are as follows: (1) Do multiple L-step rollouts for each legal action, where \u03c0\u03b8 and \u03bc\u03c6 are used to simulate actions for the self-agent and opponent, respectively. V\u03c9 is used to estimate the value of final search states. (2) Estimate a value Q for all legal actions, and the search policy \u03c0search selects the legal action with the maximum Q. (3) Use mixing technique to trade-off between \u03c0search and \u03c0\u03b8 to choose the real action to be executed.", "description": "This figure illustrates the architecture and training/testing procedures of the Opponent Modeling with In-context Search (OMIS) model.  The left side details the pretraining process, which involves training best responses (BRs) against various opponent policies, collecting training data through gameplay, and using ICL to train a Transformer model with three components: an actor, opponent imitator, and critic. The right side depicts the testing procedure, where the pretrained model uses a decision-time search (DTS) process to refine its policy. This DTS involves multiple L-step rollouts, value estimation, and a mixing technique to select the best action. ", "section": "Methodology"}, {"figure_path": "bGhsbfyg3b/figures/figures_9_1.jpg", "caption": "Figure 8: Attention heatmaps of OMIS when playing against different policies in \u03a0train", "description": "This figure visualizes the attention weights learned by the OMIS model when playing against different opponent policies from the training set (\u03a0train).  Each heatmap represents a different environment (Predator Prey, Level-Based Foraging, Overcooked). The x-axis represents the opponent policy index in \u03a0train, and the y-axis represents the token position within the episode-wise in-context data (Depi,k). The color intensity indicates the attention weight, with warmer colors representing higher attention weights. This visualization helps to understand how OMIS focuses on different aspects of opponent behavior depending on the specific opponent and the environment.", "section": "I Quantitative Analysis of Attention Weights Learned by OMIS"}, {"figure_path": "bGhsbfyg3b/figures/figures_9_2.jpg", "caption": "Figure 1: Left: The pretraining procedure and architecture of OMIS. The pretraining steps are as follows: (1) Train BRs against all policies in Itrain. (2) Continuously sample opponent policy from Itrain and collect training data by playing against it using its BR. (3) Train a Transformer model using ICL-based supervised learning, where the model consists of three components: an actor \u03c0\u03b8, an opponent imitator \u03bc\u03c6, and a critic V\u03c9. Right: The testing procedure of OMIS. During testing, OMIS refines \u03c0\u03b8 through DTS at each timestep. The DTS steps are as follows: (1) Do multiple L-step rollouts for each legal action, where \u03c0\u03b8 and \u03bc\u03c6 are used to simulate actions for the self-agent and opponent, respectively. V\u03c9 is used to estimate the value of final search states. (2) Estimate a value Q for all legal actions, and the search policy \u03c0search selects the legal action with the maximum Q. (3) Use mixing technique to trade-off between search and \u03c0\u03b8 to choose the real action to be executed.", "description": "This figure illustrates the overall architecture and training/testing procedures of the OMIS model. The left side shows the pretraining process, which involves training three components (actor, opponent imitator, critic) using in-context learning. The right side details the testing process, where the pretrained model uses decision-time search (DTS) to refine the actor's policy by simulating multiple rollouts and selecting the best action.", "section": "Methodology"}, {"figure_path": "bGhsbfyg3b/figures/figures_29_1.jpg", "caption": "Figure 1: Left: The pretraining procedure and architecture of OMIS. The pretraining steps are as follows: (1) Train BRs against all policies in \u03a0train. (2) Continuously sample opponent policy from \u03a0train and collect training data by playing against it using its BR. (3) Train a Transformer model using ICL-based supervised learning, where the model consists of three components: an actor \u03c0\u03b8, an opponent imitator \u03bc\u03c6, and a critic V\u03c9. Right: The testing procedure of OMIS. During testing, OMIS refines \u03c0\u03b8 through DTS at each timestep. The DTS steps are as follows: (1) Do multiple L-step rollouts for each legal action, where \u03c0\u03b8 and \u03bc\u03c6 are used to simulate actions for the self-agent and opponent, respectively. V\u03c9 is used to estimate the value of final search states. (2) Estimate a value Q for all legal actions, and the search policy \u03c0search selects the legal action with the maximum Q. (3) Use mixing technique to trade-off between \u03c0search and \u03c0\u03b8 to choose the real action to be executed.", "description": "This figure illustrates the overall architecture and training/testing procedures of the OMIS model.  The left side details the pretraining phase, which involves training three components (actor, opponent imitator, and critic) using best response policies and in-context learning. The right side depicts the testing phase, focusing on the decision-time search (DTS) process. DTS uses the pretrained components to simulate future game states and refine the actor's policy by selecting actions based on predicted returns and a mixing strategy.", "section": "4 Methodology"}, {"figure_path": "bGhsbfyg3b/figures/figures_30_1.jpg", "caption": "Figure 1: Left: The pretraining procedure and architecture of OMIS. The pretraining steps are as follows: (1) Train BRs against all policies in \u03a0train. (2) Continuously sample opponent policy from \u03a0train and collect training data by playing against it using its BR. (3) Train a Transformer model using ICL-based supervised learning, where the model consists of three components: an actor \u03c0\u03b8, an opponent imitator \u03bc\u03c6, and a critic V\u03c9. Right: The testing procedure of OMIS. During testing, OMIS refines \u03c0\u03b8 through DTS at each timestep. The DTS steps are as follows: (1) Do multiple L-step rollouts for each legal action, where \u03c0\u03b8 and \u03bc\u03c6 are used to simulate actions for the self-agent and opponent, respectively. V\u03c9 is used to estimate the value of final search states. (2) Estimate a value Q for all legal actions, and the search policy \u03c0search selects the legal action with the maximum Q. (3) Use mixing technique to trade-off between search and \u03c0\u03b8 to choose the real action to be executed.", "description": "This figure illustrates the overall framework of the OMIS model. The left side shows the pretraining process, which involves training best responses (BRs) against various opponent policies, collecting training data, and training a Transformer model using in-context learning (ICL) to learn three components: an actor (\u03c0\u03b8), an opponent imitator (\u03bc\u03c6), and a critic (V\u03c9). The right side depicts the testing process, which employs decision-time search (DTS) to refine the actor's policy (\u03c0\u03b8) by conducting multiple L-step rollouts for each legal action, simulating actions using the pretrained actor and opponent imitator, estimating values using the pretrained critic, and selecting the best action using a mixing technique that combines the search policy (\u03c0search) and the original actor policy.", "section": "Methodology"}, {"figure_path": "bGhsbfyg3b/figures/figures_31_1.jpg", "caption": "Figure 1: Left: The pretraining procedure and architecture of OMIS. The pretraining steps are as follows: (1) Train BRs against all policies in \u03a0train. (2) Continuously sample opponent policy from \u03a0train and collect training data by playing against it using its BR. (3) Train a Transformer model using ICL-based supervised learning, where the model consists of three components: an actor \u03c0\u03b8, an opponent imitator \u03bc\u03c6, and a critic V\u03c9. Right: The testing procedure of OMIS. During testing, OMIS refines \u03c0\u03b8 through DTS at each timestep. The DTS steps are as follows: (1) Do multiple L-step rollouts for each legal action, where \u03c0\u03b8 and \u03bc\u03c6 are used to simulate actions for the self-agent and opponent, respectively. V\u03c9 is used to estimate the value of final search states. (2) Estimate a value Q for all legal actions, and the search policy \u03c0search selects the legal action with the maximum Q. (3) Use mixing technique to trade-off between \u03c0search and \u03c0\u03b8 to choose the real action to be executed.", "description": "This figure illustrates the overall architecture and training/testing procedures of the Opponent Modeling with In-context Search (OMIS) model. The left panel details the pretraining phase, which involves training three components (actor, opponent imitator, and critic) using best response policies and in-context learning.  The right panel depicts the testing phase, where decision-time search (DTS) refines the actor's policy by simulating multiple rollouts to estimate the value of each action and ultimately select the best option. A mixing technique balances the search policy and the original actor policy for action selection.", "section": "Methodology"}, {"figure_path": "bGhsbfyg3b/figures/figures_31_2.jpg", "caption": "Figure 1: Left: The pretraining procedure and architecture of OMIS. The pretraining steps are as follows: (1) Train BRs against all policies in \u03a0train. (2) Continuously sample opponent policy from \u03a0train and collect training data by playing against it using its BR. (3) Train a Transformer model using ICL-based supervised learning, where the model consists of three components: an actor \u03c0\u03b8, an opponent imitator \u03bc\u03c6, and a critic V\u03c9. Right: The testing procedure of OMIS. During testing, OMIS refines \u03c0\u03b8 through DTS at each timestep. The DTS steps are as follows: (1) Do multiple L-step rollouts for each legal action, where \u03c0\u03b8 and \u03bc\u03c6 are used to simulate actions for the self-agent and opponent, respectively. V\u03c9 is used to estimate the value of final search states. (2) Estimate a value Q for all legal actions, and the search policy \u03c0search selects the legal action with the maximum Q. (3) Use mixing technique to trade-off between search and \u03c0\u03b8 to choose the real action to be executed.", "description": "This figure illustrates the overall architecture and the training and testing procedures of the OMIS model. The left side shows the pretraining phase, where best responses (BRs) are trained against various opponent policies. Training data is collected by playing against opponents using BRs.  A Transformer model is then trained using in-context learning with three components: an actor, an opponent imitator, and a critic. The right side shows the testing phase.  During testing, the model uses the pretrained components for decision-time search (DTS) to refine the actor's policy. DTS involves multiple rollouts of each legal action, estimating the value and selecting the best action. A mixing technique combines the search policy and the original actor policy to make the final action decision.", "section": "Methodology"}, {"figure_path": "bGhsbfyg3b/figures/figures_34_1.jpg", "caption": "Figure 1: Left: The pretraining procedure and architecture of OMIS. The pretraining steps are as follows: (1) Train BRs against all policies in Itrain. (2) Continuously sample opponent policy from Itrain and collect training data by playing against it using its BR. (3) Train a Transformer model using ICL-based supervised learning, where the model consists of three components: an actor \u03c0\u03c1, an opponent imitator \u00b5\u03c6, and a critic V. Right: The testing procedure of OMIS. During testing, OMIS refines \u03c0\u03bf through DTS at each timestep. The DTS steps are as follows: (1) Do multiple L-step rollouts for each legal action, where \u03c0\u03b8 and \u03bc\u03c6 are used to simulate actions for the self-agent and opponent, respectively. V is used to estimate the value of final search states. (2) Estimate a value Q for all legal actions, and the search policy #search selects the legal action with the maximum Q. (3) Use mixing technique to trade-off between search and \u03c0\u03b8 to choose the real action to be executed.", "description": "This figure illustrates the overall architecture and training/testing procedures of the OMIS model.  The left side details the pretraining phase, showing the training of best responses (BRs) against various opponent policies from the training set (Itrain). These BRs, along with the sampled opponent policies, generate training data for a Transformer model with three components: an actor (\u03c0\u03b8), opponent imitator (\u03bc\u03c6), and critic (Vw). These components are trained using in-context learning (ICL). The right side shows the testing phase, where the pretrained components are utilized for decision-time search (DTS) to refine the actor's policy. The DTS involves multiple L-step rollouts for each legal action and a mixing technique to balance the search policy and the original actor policy.", "section": "Methodology"}]