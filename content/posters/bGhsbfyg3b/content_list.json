[{"type": "text", "text": "Opponent Modeling with In-context Search ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Yuheng $\\mathbf{J}\\mathbf{ing}^{1,2}$ Bingyun $\\mathbf{Liu^{1,2}}$ Kai Li1,2,\u2020 Yifan Zang1,2 Haobo $\\mathbf{Fu}^{6}$ Qiang $\\mathbf{F}\\mathbf{u}^{6}$ Junliang $\\mathbf{Xing^{5}}$ Jian Cheng1,3,4,\u2020 ", "page_idx": 0}, {"type": "text", "text": "\u2020 denotes corresponding authors 1 Institute of Automation, Chinese Academy of Sciences   \n2 School of Artificial Intelligence, University of Chinese Academy of Sciences 3 School of Future Technology, University of Chinese Academy of Sciences 4 AiRiA 5 Tsinghua University 6 Tencent AI Lab   \n{jingyuheng2022,liubingyun2021,kai.li,zangyifan2019,jian.cheng}   \n@ia.ac.cn, {haobofu,leonfu}@tencent.com, jlxing@tsinghua.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Opponent modeling is a longstanding research topic aimed at enhancing decisionmaking by modeling information about opponents in multi-agent environments. However, existing approaches often face challenges such as having difficulty generalizing to unknown opponent policies and conducting unstable performance. To tackle these challenges, we propose a novel approach based on in-context learning and decision-time search named Opponent Modeling with In-context Search (OMIS). OMIS leverages in-context learning-based pretraining to train a Transformer model for decision-making. It consists of three in-context components: an actor learning best responses to opponent policies, an opponent imitator mimicking opponent actions, and a critic estimating state values. When testing in an environment that features unknown non-stationary opponent agents, OMIS uses pretrained in-context components for decision-time search to refine the actor\u2019s policy. Theoretically, we prove that under reasonable assumptions, OMIS without search converges in opponent policy recognition and has good generalization properties; with search, OMIS provides improvement guarantees, exhibiting performance stability. Empirically, in competitive, cooperative, and mixed environments, OMIS demonstrates more effective and stable adaptation to opponents than other approaches. See our project website at https://sites.google.com/view/nips2024-omis. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Opponent Modeling (OM) is a pivotal topic in artificial intelligence research, aiming to develop autonomous agents capable of modeling the behaviors, goals, beliefs, or other properties of adversaries or teammates (collectively termed as opponents). Such modelings are used to reduce uncertainty in multi-agent environments and enhance decision-making [4, 58, 100, 28, 106, 62, 52, 91, 104, 17, 101, 65, 67]. Despite the methodologies and insights proposed by existing OM approaches, their processes generally boil down to two stages: (1) Pretraining: pretrain a model with designed OM methodology on a training set of opponent policies; (2) Testing: deploying the pretrained model in a certain way on a testing set of opponent policies to benchmark adaptability to unknown opponents. ", "page_idx": 0}, {"type": "text", "text": "For these two processes, different OM approaches usually have their respective focuses: (1) Pretraining-Focused Approach (PFA) [29, 28, 62, 106] focuses on acquiring knowledge of responding to various opponents during pretraining and generalizing it to the testing stage; (2) Testing-Focused Approach (TFA) [3, 40, 100] focuses on updating the pretrained model during testing to reason and respond to unknown opponents effectively. However, existing PFAs and TFAs have their respective common and noteworthy drawbacks. For PFAs, they have limited generalization abilities, as the generalization of their pretrained models often lacks theoretical guarantees. Moreover, PFAs typically involve minimal additional operations during the testing stage, making them practically challenging to handle unknown opponents. For TFAs, they have performance instability issues. The finetuning (i.e., update the pretrained model) of TFAs during testing can be tricky, as it involves several gradient updates using only a few samples to adjust the policy. Without careful manual hyperparameter tuning, TFAs always perform unstablely when facing unknown opponents. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "To overcome the inherent issues of PFAs and TFAs, we propose a novel approach named Opponent Modeling with In-context Search (OMIS). The core motivation behind OMIS is \u2018think before you act\u2019: when facing an opponent with an unknown policy during testing, we first guess about his current policy based on historical context. We then conduct Decision-Time Search (DTS) for a few steps using this imagined opponent policy, estimate the returns of each legal action, and choose the best one to act on. Such a process intuitively helps derive a policy more optimal than making a direct decision regarding only the current state. This approach is often reflected in real-life situations, such as the \u2018deep thinking\u2019 strategy employed by professional players in Go, chess, and other board games. ", "page_idx": 1}, {"type": "text", "text": "To enable such a DTS, we build three components: an actor, to respond appropriately to the current opponent during the DTS; an opponent imitator, who imitates the actions of the current opponent, enabling the generation of transitions in an imagined environment during the DTS; a critic, who estimates the value of the final search states, as we do not search until the end of the game. We argue that all three components should be adaptive, meaning they dynamically adjust based on changes in opponent information. Therefore, we adopt In-Context Learning (ICL)-based pretraining to learn three in-context components, as ICL can endow them with the needed adaptability. ", "page_idx": 1}, {"type": "text", "text": "In summary, the methodology design of OMIS is as follows: (1) For Pretraining, we train a Transformer [83] model for decision-making based on ICL [20, 87, 56, 42]. We build our model with three components: an actor, who learns the best responses to various opponent policies; an opponent imitator, who imitates opponent actions; and a critic, who estimates state values; (2) For Testing, we use the pretrained three in-context components for DTS [79, 80, 13] to refine the actor\u2019s original policy. Based on predicting opponent actions and estimating state values, this DTS performs rollouts for each legal action, promptly evaluating and selecting the most advantageous action. ", "page_idx": 1}, {"type": "text", "text": "Theoretically, OMIS can provably alleviate the issues present in PFAs and TFAs. For limited generalization ability of PFAs, OMIS\u2019s pretrained model is proven to converge on opponent policy recognition and to have good generalization properties: OMIS\u2019s pretrained model can accurately recognize seen opponents and recognize unseen opponents as the most familiar seen ones to some extent. For performance instability issues of TFAs, OMIS\u2019s DTS avoids any gradient updates and theoretically provides improvement guarantees. ", "page_idx": 1}, {"type": "text", "text": "Empirically, extensive comparative experiments and ablation analyses in competitive, cooperative, and mixed environments verify the effectiveness of OMIS in adapting to unknown non-stationary opponent agents. Statistically, OMIS demonstrates better performance and lower variance during testing than other approaches, reflecting the generalizability and stability of opponent adaptation. ", "page_idx": 1}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 1}, {"type": "text", "text": "Opponent modeling. In recent years, OM has seen the rise of various new approaches based on different methods, including those based on representation learning [29, 28, 61, 106, 62], Bayesian learning [105, 19, 24, 53], meta-learning [3, 40, 106, 93], shaping opponents\u2019 learning [22, 23, 46], and recursive reasoning [89, 100]. All approaches can be broadly categorized into PFAs and TFAs. ", "page_idx": 1}, {"type": "text", "text": "OM based on representation learning and meta-gradient-free meta-learning methods such as Duan et al. [20] typically fall into PFAs. PFAs\u2019 generalization on unknown opponents often lacks any theoretical analysis or guarantees. This also leads to PFAs not always performing well empirically. Our work utilizes ICL pretraining to provide good theoretical properties regarding generalization. ", "page_idx": 1}, {"type": "text", "text": "OM based on Bayesian learning and meta-gradient-based meta-learning such as Finn et al. [21] typically belong to TFAs. The finetuning of TFAs makes them unstable, as the opponent may continuously change policy during testing, making it challenging to adapt with a small number of samples for updating. Our work employs DTS to avoid finetuning and has improvement guarantees. ", "page_idx": 1}, {"type": "text", "text": "In-context learning. Algorithmically, ICL can be considered as taking a more agnostic approach by learning the learning algorithm itself [20, 87, 56, 42]. Recent work investigates why and how pretrained Transformers perform ICL [27, 48, 102, 1, 68]. Xie et al. [94] introduces a Bayesian framework explaining how ICL works. Some work [86, 2, 8] proves Transformers can implement ICL algorithms via in-context gradient descent. Lee et al. [43] proposes supervised pretraining to empirically and theoretically demonstrate ICL abilities in decision-making. Unlike existing decisionrelated work focusing on single-agent settings, our work explores the theoretical properties and empirical effects of using a Transformer pretrained based on ICL under the setting of OM. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "Decision-time search. DTS involves searching in a simulated environment before each real action, aiming to obtain a more \u2018prescient\u2019 policy than no search [79, 80, 13, 49]. One of the most representative works is the AlphaGo series [73\u201375, 71], which achieves remarkable results in games like Go and Atari based on a DTS algorithm called Monte Carlo Tree Search (MCTS) and self-play. Our work explores how to make DTS work in the context of OM. The DTS of the AlphaGo series assumes that opponent adopts the same strong policy as the agent we control. In contrast, the DTS in our work dynamically models the opponents\u2019 actions, focusing on better adapting to the current opponents. ", "page_idx": 2}, {"type": "text", "text": "See App. A for an overview of OM and related work on Transformers for decision-making. ", "page_idx": 2}, {"type": "text", "text": "3 Preliminaries ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "We formalize the multi-agent environment using an $n$ -agent stochastic game $\\langle S,\\{A^{i}\\}_{i=1}^{n}$ , $\\mathcal{P},\\{R^{i}\\}_{i=1}^{n},\\gamma,T\\rangle$ . $\\boldsymbol{S}$ is the state space, $A^{i}$ is the action space of agent $i\\in[n]$ , $\\begin{array}{r}{\\mathcal{A}=\\prod_{i=1}^{n}\\mathcal{A}^{i}}\\end{array}$ is the joint action space of all agents, $\\mathcal{P}:\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{S}\\rightarrow[0,1]$ is the transition dynamics, $R^{i}:S\\times A\\rightarrow\\mathbb{R}$ is the reward function for agent $i$ , $\\gamma$ is the discount factor, and $T$ is the horizon for each episode. ", "page_idx": 2}, {"type": "text", "text": "Following the tradition in OM, we mark the agent under our control, i.e., the self-agent, with the superscript 1, and consider the other $n\\,-\\,1$ agents as opponents, marked with the superscript $-1$ . The joint policy of opponents is denoted as $\\begin{array}{r}{\\pi^{-1}\\hat{(a^{-1}|s)}\\;=\\;\\prod_{j\\neq1}\\pi^{j}(a^{j}|s)}\\end{array}$ , where $a^{-1}$ is the joint actions of opponents. Let the trajectory at timestep $t$ in the current episode be yt $y_{t}^{(\\mathrm{cur})}\\,=\\,\\{s_{0},a_{0}^{1},a_{0}^{-1},r_{0}^{1},r_{0}^{-1},\\ldots,s_{t-1},a_{t-1}^{1},a_{t-1}^{-1},r_{t-1}^{1},r_{t-1}^{-1},s_{t}\\}$ . The historical trajectories $\\mathcal{H}_{t}:=(y_{T}^{(0)},\\ldots,y_{T}^{(\\mathrm{cur-1})},y_{t}^{(\\mathrm{cur})})$ y(Tcur\u22121), yt(cur)) is always available to the self-agent. During the pretraining stage, opponent policies are sampled from a training set of opponent policies $\\Pi^{\\mathrm{train}}:=\\{\\pi^{-1,k}\\}_{k=1}^{K}.$ During the testing stage, opponent policies are sampled from a testing set of opponent policies $\\Pi^{\\mathrm{test}}$ , which includes an unknown number of unknown opponent policies. ", "page_idx": 2}, {"type": "text", "text": "In OM, the self-agent\u2019s policy can be generally denoted as $\\pi^{1}(a^{1}|s,D)$ (abbreviated as $\\pi$ ), which dynamically adjusts based on the opponent information data $D$ (referred to as in-context data in this paper). $D$ can be directly composed of some part of the data from $\\mathcal{H}_{t}$ , or it can be obtained by learning a representation from $\\mathcal{H}_{t}$ . Building upon the pretraining, the objective of the self-agent is to maximize its expected return (i.e., cumulative discounted reward) during testing: ", "page_idx": 2}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{\\mathbb{E}}}\\\\ {s_{t+1}\\sim\\mathcal{P}(\\cdot|s_{t},a_{t}^{1},a_{t}^{-1}),a_{t}^{-1}\\sim\\pi^{-1}(\\cdot|s_{t}),\\!\\left[\\sum_{t=0}^{T-1}\\!\\!\\!\\gamma^{t}\\cdot R^{1}(s_{t},a_{t}^{1},a_{t}^{-1})\\right]\\,.}\\\\ {\\pi^{-1}\\!\\sim\\!\\Pi^{\\mathrm{test}},a_{t}^{1}\\!\\sim\\!\\pi(\\cdot|s_{t},D),}\\\\ {D\\!\\sim\\!\\mathcal{H}_{t},\\pi\\!\\sim\\!\\mathrm{Pretraining}(\\Pi^{\\mathrm{rain}})}\\end{array}\n$$", "text_format": "latex", "page_idx": 2}, {"type": "text", "text": "4 Methodology ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "In Sec. 4.1, we present how we build the in-context actor, opponent imitator, and critic for OMIS with ICL-based pretraining; in Sec. 4.2, we describe our method of using pretrained in-context components for DTS; in Sec. 4.3, we provide a theoretical analysis of both the ICL and DTS components of OMIS. We provide an overview of OMIS in Fig. 1 and the pseudocode of OMIS in App. B. ", "page_idx": 2}, {"type": "text", "text": "4.1 In-Context-Learning-based Pretraining ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "To ensure that the actor learns high-quality knowledge of responding to various opponents, we first solve for the Best Responses (BR) against different opponent policies. For each opponent policy $\\pi^{-1,k}$ in $\\Pi^{\\mathrm{train}}$ (where $k\\in[K],$ , we keep the opponent policy fixed as $\\pi^{-1,k}$ and sufficiently train the PPO algorithm [72] to obtain the BR against $\\pi^{-1,k}$ , denoted as $B R(\\pi^{-1,k}):=\\pi^{1,k,*}(a|s)$ . ", "page_idx": 2}, {"type": "text", "text": "To generate training data for pretraining the three components, we continually sample opponent policies from $\\Pi^{\\mathrm{train}}$ and use their corresponding BR to play against them. For each episode, we sample a $\\pi^{-1,k}$ from $\\Pi^{\\mathrm{train}}$ as opponents and use its BR $\\pi^{1,k,*}$ as self-agent to play against it. ", "page_idx": 2}, {"type": "image", "img_path": "bGhsbfyg3b/tmp/cb1a0cb45dbcb161cf2037f8cd88b148f24e5bd28eabeb1ced1f800a355bdea7.jpg", "img_caption": ["Figure 1: Left: The pretraining procedure and architecture of OMIS. The pretraining steps are as follows: (1) Train BRs against all policies in $\\Pi^{\\mathrm{train}}$ . (2) Continuously sample opponent policy from $\\Pi^{\\mathrm{train}}$ and collect training data by playing against it using its BR. (3) Train a Transformer model using ICL-based supervised learning, where the model consists of three components: an actor $\\pi_{\\theta}$ , an opponent imitator $\\mu_{\\phi}$ , and a critic $V_{\\omega}$ . Right: The testing procedure of OMIS. During testing, OMIS refines $\\pi_{\\theta}$ through DTS at each timestep. The DTS steps are as follows: (1) Do multiple $L$ -step rollouts for each legal action, where $\\pi_{\\theta}$ and $\\mu_{\\phi}$ are used to simulate actions for the self-agent and opponent, respectively. $V_{\\omega}$ is used to estimate the value of final search states. (2) Estimate a value $\\hat{Q}$ for all legal actions, and the search policy $\\pi_{\\mathrm{search}}$ selects the legal action with the maximum $\\hat{Q}$ . (3) Use mixing technique to trade-off between $\\pi_{\\mathrm{search}}$ and $\\pi_{\\theta}$ to choose the real action to be executed. "], "img_footnote": [], "page_idx": 3}, {"type": "text", "text": "The procedure of generating training data is as follows: for each timestep $t$ , we construct incontext data $D_{t}^{k}\\;:=\\;(D^{\\mathrm{epi,k}},D_{t}^{\\mathrm{step,k}})$ about $\\pi^{-1,k}$ , which is used to provide information about $\\pi^{-1,k}$ for self-agent to recognize $\\pi^{-1,k}$ . $D^{\\mathrm{epi,k}}=\\{(\\tilde{s}_{h},\\tilde{a}_{h}^{-1,k})\\}_{h=1}^{H}$ is episode-wise in-context data, generated by playing against $\\pi^{-1,k}$ using any self-agent policy.1 It is used to characterize the overall behavioral pattern of $\\pi^{=1,k}$ on an episode-wise basis. See the construction process of $D^{\\mathrm{epi,k}}$ in App. C. $D_{t}^{\\mathrm{step,k}}=\\bar{(s}_{0},a_{0}^{-1,k},\\ldots,s_{t-1},a_{t-1}^{-\\bar{1},k})$ is step-wise in-context data, generated by the current episode involving $\\pi^{-1,k}$ and $\\pi^{1,k,*}$ . It represents the step-wise specific behavior pattern of $\\pi^{-1,k}$ . ", "page_idx": 3}, {"type": "text", "text": "Furthermore, for each timestep $t$ , we collect the Return-To-Go (RTG) obtained by the self-agent, denoted as $\\begin{array}{r}{G_{t}^{1,k,*}=\\sum_{t^{\\prime}=t}^{T}\\gamma^{t^{\\prime}-t}r_{t^{\\prime}}^{1}\\,=\\,\\sum_{t^{\\prime}=t}^{T}\\gamma^{t^{\\prime}-t}R^{1}(s_{t^{\\prime}},a_{t^{\\prime}}^{1,k,*},a_{t^{\\prime}}^{-1,k})}\\end{array}$ , where $a^{1,k,*}\\sim\\pi^{1,k,*}$ , $a^{-1,k}\\!\\sim\\!\\pi^{-1,k}$ , and $V_{t}^{1,k,*}\\!=\\!\\mathbb{E}[G_{t}^{1,k,*}]$ . T o end with, the training data for timestep $t$ is obtained as: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\mathfrak{D}_{t}^{k}:=(s_{t},D_{t}^{k},a_{t}^{1,k,*},a_{t}^{-1,k},G_{t}^{1,k,*}).\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "After preparing the training data, we use supervised learning to pretrain an actor $\\pi_{\\theta}\\big(a_{t}^{1}|s_{t},D_{t}^{k}\\big)$ to learn the BR against $\\pi^{-1,k}$ , an opponent imitator $\\mu_{\\phi}(a_{t}^{-1,k}\\mid s_{t},D_{t}^{k})$ to imitate the opponent\u2019s policy, and a critic $V_{\\omega}(s_{t},D_{t}^{k})$ to estimate the state value of self-agent. Notably, all these components condition on $D_{t}^{k}$ as their in-context data. For each episode, the optimization objectives are as follows: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\underset{\\theta}{\\operatorname*{max}}\\,\\mathbb{E}_{\\mathfrak{D}_{t}^{k},t\\sim[T],k\\sim[K]}\\,\\left[\\log\\pi_{\\theta}(a_{t}^{1,k,*}\\mid s_{t},D_{t}^{k})\\right],}\\\\ &{\\underset{\\phi}{\\operatorname*{max}}\\,\\mathbb{E}_{\\mathfrak{D}_{t}^{k},t\\sim[T],k\\sim[K]}\\,\\left[\\log\\mu_{\\phi}(a_{t}^{-1,k}\\mid s_{t},D_{t}^{k})\\right],}\\\\ &{\\underset{\\omega}{\\operatorname*{min}}\\,\\mathbb{E}_{\\mathfrak{D}_{t}^{k},t\\sim[T],k\\sim[K]}\\,\\left[\\left(V_{\\omega}(s_{t},D_{t}^{k})-G_{t}^{1,k,*}\\right)^{2}\\right].}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The left side of Fig. 1 illustrates OMIS\u2019s architecture and its pretraining procedure. Based on the understanding of ICL in decision-making, we design our architecture upon a causal Transformer [66]. ", "page_idx": 3}, {"type": "text", "text": "4.2 Decision-Time Search with In-Context Components ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "Following the best practices in OM, we assume the testing environment features unknown nonstationary opponent agents, which we denote as $\\Phi$ . Unknown indicates that the self-agent is unable to ascertain the true policy $\\bar{\\pi}^{-1}$ employed by $\\Phi$ . Non-stationary implies that $\\Phi$ switches its policy between episodes in some way, with each switch involving randomly sampling a $\\bar{\\pi}^{-1}$ from $\\Pi^{\\mathrm{test}}$ . ", "page_idx": 3}, {"type": "text", "text": "Following the general setup in the DTS domain, we assume the ground truth transition dynamic $\\mathcal{P}$ is available [74, 75, 12, 13, 9, 45, 38]. Based on the pretrained in-context components $\\pi_{\\theta}$ , $V_{\\omega}$ , $\\mu_{\\phi}$ , and $\\mathcal{P}$ , we conduct DTS to play against $\\Phi$ . At each timestep $t$ , we perform $M$ times of rollouts with length $L$ for each legal action $\\hat{a}_{t}^{1}$ of self-agent.2 This is done to estimate the value $\\hat{Q}(s_{t},\\hat{a}_{t}^{1})$ for each $\\hat{a}_{t}^{1}$ under current true opponent policy $\\bar{\\pi}^{-1}$ and current self-agent policy $\\pi_{\\theta}$ . The self-agent then executes the legal action with the highest $\\hat{Q}$ value in the real environment. Our expectation is that through such a DTS, we can refine the original policy $\\pi_{\\theta}$ to better adapt to $\\Phi$ . ", "page_idx": 4}, {"type": "text", "text": "The specific process of the $D T S$ is as follows: for each timestep $t$ , we first construct in-context data $D_{t}=(D^{\\mathrm{epi}},D_{t}^{\\mathrm{step}})$ about $\\Phi$ , and its construction method is almost identical to $D_{t}^{k}$ mentioned in Sec. 4.1. However, since $\\bar{\\pi}^{-1}$ is unknowable, we make a slight modification: $D^{\\mathrm{epi}}$ is constructed by sampling consecutive segments from the most recent $C$ trajectories in which $\\Phi$ participated. ", "page_idx": 4}, {"type": "text", "text": "After constructing $D_{t}$ , for any given legal action $\\hat{a}_{t}^{1}$ , we sample the opponents\u2019 action by $\\hat{a}_{t}^{-1}\\sim$ $\\mu_{\\phi}(\\cdot|s_{t},D_{t})$ and transition using $\\mathcal{P}$ to obtain $\\hat{s}_{t+1}$ and $\\hat{r}_{t}^{1}$ . We append $(s_{t},\\hat{a}_{t}^{-1})$ to the end of $D_{t}^{\\mathrm{step}}$ to obtain the updated step-wise in-context data $\\hat{D}_{t+1}^{\\mathrm{step}}$ and in-context data $\\hat{D}_{t+1}\\,=\\,(D^{\\mathrm{epi}},\\hat{D}_{t+1}^{\\mathrm{step}})$ . Following, at the $l$ -th step of the rollout for $\\hat{a}_{t}^{1}$ $(l\\in[L])$ , we sample self-agent action and opponent action using the following two formulas, respectively: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\hat{a}_{t+l}^{1}\\sim\\pi_{\\theta}(\\cdot|\\hat{s}_{t+l},\\hat{D}_{t+l}),}\\\\ {\\hat{a}_{t+l}^{-1}\\sim\\mu_{\\phi}(\\cdot|\\hat{s}_{t+l},\\hat{D}_{t+l}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Transitioning with $\\mathcal{P}$ yields $\\hat{s}_{t+l+1}$ and $\\hat{r}_{t+l}^{1}$ . Next, we append $(\\hat{s}_{t+l},\\hat{a}_{t+l}^{-1})$ to the end of $\\hat{D}_{t+l}^{\\mathrm{step}}$ to   \nobtain D\u02c6step and $\\hat{D}_{t+l+1}=(D^{\\mathrm{epi}},\\hat{D}_{t+l+1}^{\\mathrm{step}})$ . After completing the $L$ -th step, we use $\\hat{V}_{t+L+1}:=$ t+l+1   \n$V_{\\omega}(\\hat{s}_{t+L+1},\\hat{D}_{t+L+1})$ to estimate the value of the final search state. When finishing $M$ times of   \nrollouts for $\\hat{a}_{t}^{1}$ , we obtain an estimated value for $\\hat{a}_{t}^{1}$ by: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\hat{Q}(s_{t},\\hat{a}_{t}^{1}):=\\frac{1}{M}\\sum_{m=1}^{M}\\left[\\sum_{t^{\\prime}=t}^{t+L}\\gamma_{\\mathrm{search}}^{t^{\\prime}-t}\\cdot\\hat{r}_{t^{\\prime}}^{1}+\\gamma_{\\mathrm{search}}^{L+1}\\cdot\\hat{V}_{t+L+1}\\right].\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, $\\gamma_{\\mathrm{search}}$ is the discount factor used in the DTS. After completing rollouts for all legal actions of the self-agent at timestep $t$ , we obtain our search policy by maximizing $\\hat{Q}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pi_{\\mathrm{search}}\\!\\left(s_{t}\\right):=\\arg\\operatorname*{max}_{\\hat{a}_{t}^{1}}\\hat{Q}\\!\\left(s_{t},\\hat{a}_{t}^{1}\\right).\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "In practical implementation, we observe that using $\\pi_{\\mathrm{search}}$ directly is not always effective. This is because we cannot totally precisely estimate the opponents\u2019 policy and the state value, making the results obtained from the DTS not sufficiently reliable across all states. To this phenomenon, we propose a simple yet effective mixing technique to balance the search policy and the original actor policy in deciding the action to be executed: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\pi_{\\mathrm{mix}}(s_{t}):=\\left\\{\\pi_{\\mathrm{search}}(s_{t}),\\,\\,||\\hat{Q}(s_{t},\\pi_{\\mathrm{search}}(s_{t}))||>\\epsilon\\,\\right..\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Here, $\\epsilon$ is a threshold hyperparameter. The main motivation for the mixing technique is as follows. We consider the expected return of the action selected by the DTS as the confidence of the search policy. When the confidence exceeds a certain threshold, we tend to consider that $\\pi_{\\mathrm{search}}$ has a relatively high probability of achieving better results than $\\pi_{\\theta}$ ; otherwise, we prefer to use the original policy $\\pi_{\\theta}$ . See the testing procedure of OMIS on the right side of Fig. 1. ", "page_idx": 4}, {"type": "text", "text": "4.3 Theoretical Analysis ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "Our theoretical analysis unfolds in the following two aspects. (1) We propose Lem. 4.1 and Thm. 4.2 to prove that OMIS without DTS (denoted as $O M I S\\,w/o\\,S$ ) converges to the optimal solution when facing a true opponent policy $\\bar{\\pi}^{-1}\\in\\Pi^{\\mathrm{train}}$ ; and it recognizes the opponent policy as the policy in $\\Pi^{\\mathrm{train}}$ with a minimum certain form of KL divergence from $\\bar{\\pi}^{-1}$ when facing a $\\mathbf{\\bar{\\pi}}^{-1}\\notin\\mathbf{\\dot{II}}^{\\operatorname{train}}$ . (2) Building upon Thm. 4.2, we further propose Thm. 4.3 to prove the policy improvement theorem of OMIS with DTS, ensuring that it leads to enhancements in performance. ", "page_idx": 4}, {"type": "text", "text": "To begin with, we instantiate a Posterior Sampling in Opponent Modeling (PSOM) algorithm (see App. D.1) based on the Posterior Sampling (PS) algorithm [60], where PSOM can be proven to share the same guarantees of converging to the optimal solution as PS. Based on some reasonable assumptions, we prove that OMIS w/o S is equivalent to PSOM. ", "page_idx": 5}, {"type": "text", "text": "Lemma 4.1 (Equivalence of OMIS w/o S and PSOM). Assume that the learned $\\pi_{\\theta}$ is consistent and the sampling of s from $\\angles{T}_{p r e}^{-1}$ is independent of opponent policy, then given $\\bar{\\pi}^{-1}$ and its $D$ , we have $P(\\xi_{T}^{1}|D,\\bar{\\pi}^{-1};P S O M)=P(\\xi_{T}^{1}|D,\\bar{\\pi}^{-1};\\pi_{\\theta})$ for all possible $\\xi_{T}^{1}$ . ", "page_idx": 5}, {"type": "text", "text": "Here, \u2018consistent\u2019 indicates that the network\u2019s fitting capability is guaranteed. $T_{\\mathrm{pre}}^{-1}(\\cdot;\\pi^{-1})$ denotes the probability distribution on all the trajectories involving $\\pi^{-1}$ during pretraining. $\\xi_{T}^{1}\\,=$ $(s_{1},a_{1}^{1,*},\\ldots,s_{T},a_{T}^{1,*})$ is self-agent history, where $a^{1,*}$ is sampled from the BR to the opponent policy recognized by PSOM. The proof is provided in App. D.2. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.2. When $\\bar{\\pi}^{-1}=\\pi^{-1,k}\\in\\Pi^{t r a i n}$ , if the PS algorithm converges to the optimal solution, then OMIS w/o S recognizes the policy of $\\Phi$ as $\\pi^{-1,k}$ , i.e., $\\pi_{\\theta},\\,\\mu_{\\phi}$ , and $V_{\\omega}$ converge to $\\pi^{1,k,*}$ , $\\pi^{-1,k}$ , and $V^{1,k,*}$ , respectively; When $\\bar{\\pi}^{-1}\\notin\\Pi^{t r a i n}$ , $O M I S\\;w/o\\;S$ recognizes the policy of $\\Phi$ as the policies in $\\Pi^{t r a i n}$ with the minimum $D_{K L}(P(\\stackrel{.}{a}^{-1}|s,\\pi^{-1})||P(a^{-1}|s,\\bar{\\pi}^{-1}))$ . ", "page_idx": 5}, {"type": "text", "text": "Based on this theorem, when OMIS w/o S faces seen opponents, it accurately recognizes the opponent\u2019s policy and converge to the BR against it; when facing unseen opponents, it recognizes the opponent\u2019s policy as the seen opponent policy with the smallest KL divergence from this unseen opponent policy and produces the BR to the recognized opponent policy. The proof is in App. D.3. ", "page_idx": 5}, {"type": "text", "text": "Theorem 4.3 (Policy Improvement of OMIS\u2019s DTS). Given $\\bar{\\pi}^{-1}$ and its $D$ , suppose OMIS recognizes $\\Phi$ as $\\pi_{\\star}^{-1}$ and $V_{\\pi_{\\star}^{-1}}^{\\pi}$ is the value vector on $\\boldsymbol{S}$ , where $V(s):=V_{\\omega}(s,D),\\pi(a|s):=\\pi_{\\theta}(a|s,D)$ . Let $\\mathcal{G}_{L}$ be the $L$ -step DTS operator and $\\pi^{\\prime}\\in\\mathcal{G}_{L}(V_{\\pi_{\\star}^{-1}}^{\\pi})$ , then $V_{\\pi_{\\star}^{-1}}^{\\pi^{\\prime}}\\geq V_{\\pi_{\\star}^{-1}}^{\\pi}$ holds component-wise. ", "page_idx": 5}, {"type": "text", "text": "Within, OMIS\u2019s DTS can be viewed as $\\mathcal{G}_{L}$ , and $\\pi^{\\prime}$ corresponds to $\\pi_{\\mathrm{search}}$ in Eq. (9). Thm. 4.3 indicates that OMIS\u2019s DTS is guaranteed to bring improvement, laying the foundation for performance stability. Additionally, OMIS\u2019s DTS avoids gradient updates. The proof is provided in App. D.4. ", "page_idx": 5}, {"type": "text", "text": "Analysis for generalization in OM. In OM, generalization can be typically defined as performance when facing unknown opponent policies (e.g., opponents like $\\Phi$ ). Existing approaches lack rigorous theoretical analysis under this definition of generalization. In Lem. 4.1, we proved that OMIS w/o S is equivalent to PSOM. In Thm. 4.2, we proved that PSOM can accurately recognize seen opponents and recognize unseen opponents as the most similar to the seen ones. Since OMIS w/o S is equivalent to PSOM, OMIS w/o S possesses the same properties. Additionally, Thm. 4.3 proved that OMIS\u2019s DTS ensures performance improvement while avoiding instability. These theoretical analyses potentially provide OMIS with benefits in terms of generalization in OM. ", "page_idx": 5}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "5.1 Experimental Setup ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Environments. We consider three sparse-reward benchmarking environments for OM as shown in Fig. 2 (See App. E for detailed introductions of them): ", "page_idx": 5}, {"type": "text", "text": "\u2022 Predator Prey (PP) is a competitive environment with a continuous state space. In PP, the self-agent is a prey (green) whose goal is to avoid being captured by three predators (red) as much as possible. There are two obstacles (black) on the map. The challenge of PP lies in the need to model all three opponents simultaneously and handle potential cooperation among them. \u2022 Level-Based Foraging (LBF) is a mixed environment in a grid world. In LBF, the self-agent is the blue one, aiming to eat as many apples as possible. The challenge of LBF is that cooperation with the opponent is necessary to eat apples of a higher level than the self-agent\u2019s (the apples and agents\u2019 levels are marked in the bottom-right). LBF represents a typical social dilemma. \u2022 OverCooked (OC) is a cooperative environment using high-dimensional images as states. In OC, the self-agent is the green one, which aims at collaborating with the opponent (blue) to serve dishes as much as possible. The challenge of OC lies in the high-intensity coordination required between the two agents to complete a series of sub-tasks to serve a dish successfully. ", "page_idx": 5}, {"type": "text", "text": "Baselines. We consider the following representative PFAs, TFAs, and DTS-based OM approach: ", "page_idx": 6}, {"type": "text", "text": "\u2022 DRON [29]: Encode hand-crafted features of opponents using a Mixture-of-Expert network while also predicting opponents\u2019 actions as auxiliary task (this is the most performant version in [29]).   \n\u2022 LIAM [62]: Use the observations and actions of the self-agent to reconstruct those of the opponent through an auto-encoder, thereby embedding the opponent policy into a latent space.   \n\u2022 MeLIBA [106]: Use Variational Auto-Encoder (VAE) to reconstruct the opponent\u2019s future actions and condition on the embedding generated by this VAE to learn a Bayesian meta-policy.   \n\u2022 Meta-PG [3]: Execute multiple meta-gradient steps to anticipate changes in opponent policies to enable fast adaptation during testing.   \n\u2022 Meta-MAPG [40]: Compared to Meta-PG, it includes a new term that accounts for the impact of the self-agent\u2019s current policy on the opponent\u2019s future policy.   \n\u2022 MBOM [100]: Use recursive reasoning in an environment model to learn opponents at different recursion levels and combine them by Bayesian mixing.   \n\u2022 OMIS w/o S: A variant of OMIS, where OMIS directly uses $\\pi_{\\theta}$ based on $D_{t}$ without DTS.   \n\u2022 SP-MCTS [88]: Use a scripted opponent model to estimate the opponent\u2019s actions and apply MCTS for DTS. We adopt OMIS w/o S as its original self-agent policy. This is a DTS-based OM approach. ", "page_idx": 6}, {"type": "text", "text": "Within, DRON, LIAM, and MeLIBA belong to PFAs; Meta-PG, Meta-MAPG, and MBOM belong to TFAs. See neural architecture design of all approaches in App. F. For a fair comparison, we implement MBOM and SP-MCTS using the ground truth transition $\\mathcal{P}$ as the environment model. ", "page_idx": 6}, {"type": "text", "text": "Opponent policy. We employ a diversity-driven Population Based Training algorithm MEP [103] to train a policy population. Policies from this MEP population are used to construct $\\Pi^{\\mathrm{train}}$ and $\\Pi^{\\mathrm{test}}$ , ensuring that all opponent policies are performant and exhibit diversity. We measure and visualize the diversity of opponent policies within the MEP population in App. G. ", "page_idx": 6}, {"type": "text", "text": "We randomly select 10 policies from the MEP population to form $\\Pi^{\\mathrm{train}}$ . Then, we categorize opponent policies in the MEP population into two types: \u2018seen\u2019 denotes policies belonging to $\\Pi^{\\mathrm{train}}$ , and \u2018unseen\u2019 denotes policies not belonging to $\\Pi^{\\mathrm{train}}$ . We set up opponent policies with different ratios of $[s e e n:u n s e e n]$ to form $\\Pi^{\\mathrm{test}}$ , e.g., $[s e e n:u n s\\dot{e}e n]=[0:10]$ signifies that $\\Pi^{\\mathrm{test}}$ is composed of 10 opponent policies that were never seen during pretraining. ", "page_idx": 6}, {"type": "image", "img_path": "bGhsbfyg3b/tmp/05bc6637a50281e79e8f302e680d3f827d16e0a2f215702e0bcb5f699f3454ef.jpg", "img_caption": ["Figure 2: The benchmarking environments. "], "img_footnote": [], "page_idx": 6}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "During testing, all opponents are the unknown non-stationary opponent agents $\\Phi$ mentioned in Sec. 4.2. $\\Phi$ switches policies by sampling from $\\Pi^{\\mathrm{test}}$ every $E$ episodes. ", "page_idx": 6}, {"type": "text", "text": "Specific settings. For the pretraining stage, we train all approaches for 4000 steps. For the testing stage, all approaches use the final checkpoints of pretraining to play against $\\Phi$ for 1200 episodes. All bar charts, line charts, and tables report the average and standard deviation of the mean results over 5 random seeds. See all the hyperparameters in App. H. ", "page_idx": 6}, {"type": "text", "text": "5.2 Empirical Analysis ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "We pose a series of questions and design experiments to answer them, aiming to analyze OMIS\u2019s opponent adaptation capability and validate each component\u2019s effectiveness. ", "page_idx": 6}, {"type": "text", "text": "Question 1. Can OMIS effectively and stably adapt to opponents under various $\\Pi^{t e s t}$ configurations? ", "page_idx": 6}, {"type": "text", "text": "We set up 5 different $[s e e n:u n s e e n]$ ratios to form $\\Pi^{\\mathrm{test}}$ , and we show the average results of all approaches against $\\Phi$ corresponding to each ratio in Fig. 3. OMIS exhibits a higher average return and lower variance than other baselines across three environments, highlighting its effectiveness and stability in opponent adaptation under different $\\Pi^{\\mathrm{test}}$ configurations. It can be observed that OMIS w/o S outperforms existing PFAs (e.g., MeLIBA) in most cases, validating that pretraining based on ICL exhibits good generalization on testing with unknown opponent policies. ", "page_idx": 6}, {"type": "text", "text": "The results also indicate that OMIS improves OMIS w/o S more effectively than SP-MCTS does. SP-MCTS sometimes even makes OMIS w/o S worse (e.g., in OC and parts of PP). This might be because (1) the opponent model of SP-MCTS, which estimates opponent actions, is non-adaptive and (2) the trade-off between exploration and exploitation in the MCTS is non-trivial to optimize. ", "page_idx": 6}, {"type": "image", "img_path": "bGhsbfyg3b/tmp/048c273d86eb9bcc62a9a4a654f4e1a36f31b5e779509894fd142cecc7c012a3.jpg", "img_caption": ["Figure 4: Average results against each true opponent policy during testing, where $[s e e n:u n s e e n]=$ $[10:10]$ and $E=20$ . Each point in $\\textrm{X}$ -axis denotes a policy switching of $\\Phi$ , totaling 60 times. Y-axis denotes the average return against the corresponding switched $\\bar{\\pi}^{-1}$ . "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Question 2. Can OMIS adapt well to each one of the true policies adopted by $\\Phi\\.^{\\prime}$ ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In Fig. 4, we provide the average results of all approaches against each true opponent policy $\\bar{\\pi}^{-1}$ employed by $\\Phi$ corresponding to ratio of $\\left[s e e n:u n s e e n\\right]=\\left[10:10\\right]$ . Similar to the observations in Fig. 3, OMIS exhibits higher return and lower variance than other baselines across various true opponent policies in PP, LBF, and OC. TFAs (e.g., Meta-PG) generally show significant performance gaps when facing different true opponent policies. This is likely due to the continuous switching of opponent policies, making finetuning during testing challenging or ineffective. ", "page_idx": 7}, {"type": "text", "text": "Question 3. How does OMIS work when the transition dynamics are learned? ", "page_idx": 7}, {"type": "text", "text": "We include a variant named Model-Based OMIS (MBOMIS) to verify whether OMIS can work effectively when the transition dynamics are unknown and learned instead. MBOMIS uses the most straightforward method to learn a transition dynamic model $\\hat{\\mathcal P}$ : given a state $s$ and action $a$ , predicting the next state $s^{\\prime}$ and reward $r$ using Mean Square Error (MSE) loss. P\u02c6 is trained using the $(s,a,r,s^{\\prime})$ tuples from the dataset used for pretraining OMIS w/o S. The testing results against unknown nonstationary opponents are shown in Fig. 5. Although MBOMIS loses some performance compared to OMIS, it still effectively improves over OMIS w/o S and generally surpasses other baselines. We also provide quantitative evaluation results of $\\hat{\\mathcal P}$ \u2019s estimation during testing in Tab. 1. Observations show that $\\hat{\\mathcal P}$ generally has a small MSE value in predicting the next state and reward without normalization. ", "page_idx": 7}, {"type": "image", "img_path": "bGhsbfyg3b/tmp/20bba4b03a72e894cf277535b113317d98e8081871bb86e6e562abf0f56365f4.jpg", "img_caption": ["Question 4. Is OMIS robust to the frequency of which $\\Phi$ switches policies? "], "img_footnote": [], "page_idx": 7}, {"type": "text", "text": "Figure 7: Average results during testing when $\\Phi$ adopts different switching frequencies, where $\\left[s e e n:u n s e e n\\right]=[10:10]$ . ", "page_idx": 7}, {"type": "text", "text": "We employ 5 different frequencies for $\\Phi$ to switch policies, i.e., $E=2,$ 5, 10, 20, dynamic (abbreviated as dyna). Here, $E\\,=\\,d y n a$ indicates that $\\Phi$ randomly selects from 2, 5, 10, 20 at the start of each switch as the number of episodes until the next switch. Fig. 7 shows the average results against $\\Phi$ with different switching frequencies. OMIS and OMIS w/o S exhibit a degree of robustness to different policy switching frequencies $E$ of $\\Phi$ in PP, LBF, OC. Notably, as $E$ increases, their performance gen", "page_idx": 7}, {"type": "text", "text": "erally shows a slight upward trend. This suggests that OMIS could gradually stabilize its adaptation to true opponent policies by accumulating in-context data. ", "page_idx": 7}, {"type": "text", "text": "Question 5. Is each part of OMIS\u2019s method design effective? ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "We design various ablated variants of OMIS: (1) OMIS w/o S (See Sec. 5.1); (2) OMIS w/o mixing: a variant where the mixing technique is not used, i.e., using $\\pi_{\\mathrm{search}}$ instead of $\\pi_{\\mathrm{mix}}$ ; (3) OMIS w/o ", "page_idx": 7}, {"type": "image", "img_path": "bGhsbfyg3b/tmp/b320c541d4870dc5d2295125983117fa1fb0717387fac9fd0ddcd81b56302e13.jpg", "img_caption": ["Figure 5: Results of OMIS using learned dynamics against unknown non-stationary opponents. (a) Average results of testing under different $[s e e n:u n s e e n]$ ratios, where $E=20$ . (b) Average results against each true opponent policy during testing, where $\\left[s e e n:u n s e e n\\right]=[10:10]$ and $E=20$ . ", "Table 1: Quantitative evaluation results of the learned dynamic $\\hat{\\mathcal P}$ \u2019s estimation during testing. The results are calculated during testing under different $[s e e n:u n s e e n]$ ratios, where $E=20$ . "], "img_footnote": [], "page_idx": 8}, {"type": "image", "img_path": "bGhsbfyg3b/tmp/09660e4ec32be2da939b531884654e4e57115cdf0f21a3f5007884661372d825.jpg", "img_caption": ["Figure 6: Average performance curves during pretraining against all policies in $\\Pi^{\\mathrm{train}}$ . "], "img_footnote": [], "page_idx": 8}, {"type": "text", "text": "S, Dstep,k : a variant without DTS where $D_{t}^{\\mathrm{step},k}$ input is excluded from the model; (4) OMIS w/o S, $D^{\\mathrm{epi},k}$ : a variant without DTS where $D^{\\mathrm{epi},k}$ input is excluded from the model. ", "page_idx": 8}, {"type": "text", "text": "Fig. 6 shows the average performance curves during pretraining for these variants against all policies in $\\Pi^{\\mathrm{train}}$ . In PP, LBF, and OC, OMIS w/o S consistently performs a lot worse than OMIS. This indicates that the DTS effectively improves the original policy of $\\pi_{\\theta}$ . OMIS w/o mixing exhibits a notable performance decrease compared to OMIS in LBF and OC. This suggests selective searching based on confidence is more effective than indiscriminately. It can be observed that $D^{\\mathrm{epi},k}$ and $\\bar{D_{t}^{\\mathrm{step},k}}$ both play crucial roles in OMIS\u2019s adaptation to opponents, with $D^{\\mathrm{epi},k}$ making a greater contribution. This could be because capturing the overall behavioral patterns of opponents is more important than focusing on their step-wise changes. ", "page_idx": 8}, {"type": "text", "text": "Question 6. Can OMIS effectively characterize opponent policies through in-context data? ", "page_idx": 8}, {"type": "text", "text": "For each policy in $\\Pi^{\\mathrm{train}}$ , we visualize OMIS\u2019s attention weights of $D^{\\mathrm{epi,}k}$ over the final 20 timesteps in an episode in Fig. 8. Each position of tokens in $D^{\\mathrm{epi,}k}$ has a weight indicated by the depth of color. In all three environments, the attention of OMIS exhibits the following characteristics: (1) Focusing on specific positions of tokens in $D^{\\mathrm{epi},k}$ for different opponent policies; (2) Maintaining a relatively consistent distribution for a given opponent policy across various timesteps within the same episode. This implies that OMIS can represent different opponent policies according to distinct patterns of different in-context data. We also provide quantitative analysis on OMIS\u2019s attention weights in App. I. ", "page_idx": 8}, {"type": "text", "text": "Question 7. How well does the in-context components of OMIS estimate? ", "page_idx": 8}, {"type": "text", "text": "We collect true opponent actions and true RTGs as labels, using Accuracy and MSE as metrics to evaluate the effectiveness of $\\mu_{\\phi}$ \u2019s and $V_{\\omega}$ \u2019s estimations, respectively. In Tab. 2, we provide estimation results of the in-context components $\\mu_{\\phi},V_{\\omega}$ during testing under different $[s e e n:u n s e e n]$ ratios. It can be observed that $\\mu_{\\phi}$ is estimated relatively accurately in OC, while $V_{\\omega}$ is estimated relatively accurately in PP and LBF. However, $\\mu_{\\phi}$ does not estimate very accurately in PP and LBF. This indicates that the functioning of OMIS does not necessarily depend on very precise estimates. In all three environments, the estimation accuracy for purely unseen opponents does not decrease significantly, further confirming the good generalization of ICL. ", "page_idx": 8}, {"type": "image", "img_path": "bGhsbfyg3b/tmp/404698070d8531875ec6b1b29a7d2cc4f9e493a034ce65070a05601c711d258f.jpg", "img_caption": ["Table 2: The estimation results of the in-context components of OMIS during testing, where $E=20$ . ", "Figure 8: Attention heatmaps of OMIS when playing against different policies in . "], "img_footnote": [], "page_idx": 9}, {"type": "image", "img_path": "bGhsbfyg3b/tmp/035e3476e1e3656b556a5cba6a8bddee8928a08588d8eeb33f19774524cb2bc0.jpg", "img_caption": ["Figure 9: Visualization of OMIS\u2019s DTS when playing against an unseen opponent policy. "], "img_footnote": [], "page_idx": 9}, {"type": "text", "text": "", "page_idx": 9}, {"type": "text", "text": "Question 8. How does OMIS\u2019s DTS work in real games? ", "page_idx": 9}, {"type": "text", "text": "Fig. 9 visualizes the OMIS\u2019s DTS process at a particular timestep during a game against an unseen opponent policy in three environments. We only illustrate two legal actions as an example. It can be observed that OMIS\u2019s DTS promptly evaluates each legal action, predicts the opponent\u2019s actions during the DTS, and ultimately selects the most advantageous action. We notice the following interesting phenomena: In PP, DTS enables the self-agent to avoid being captured by opponents who use an encirclement policy; In LBF, DTS allows the self-agent to cooperate with opponents to eat apples with a higher level than itself; In OC, DTS helps prevent the self-agent from blocking the path of collaborators, allowing them to serve dishes smoothly. ", "page_idx": 9}, {"type": "text", "text": "6 Discussion ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "Summary. In this paper, we propose OMIS based on ICL and DTS, aiming to address the challenges of limited generalization abilities and performance instability issues faced by existing OM approaches. The foundations of OMIS lie in two stages: (1) We employ ICL to pretrain a Transformer model consisting of three components: actor, opponent imitator, and critic. We prove that this model converges in opponent policy recognition and has good properties in terms of generalization; (2) Based on the pretrained in-context components, we use a DTS to refine the policy of the original actor. This DTS avoids (the instability-causing) gradient updates and provides improvement guarantees. Extensive experimental results in three environments validate that OMIS adapts effectively and stably to unknown non-stationary opponent agents. ", "page_idx": 9}, {"type": "text", "text": "Limitations and future work. (1) We only considered opponents with non-stationary switches among several fixed unknown policies during testing. OMIS might face challenges in adapting to opponents who are continuously learning or reasoning; (2) This work focuses on the setting of perfect information games. Effectively incorporating ICL and DTS for OM in imperfect information games is a challenging and meaningful research problem; (3) OMIS only utilizes a naive DTS method to refine its policy. We will continue to explore how to apply more advanced DTS methods to the OM domain for more effective adaptation to unknown opponents. A more in-depth discussion is in App. J. ", "page_idx": 9}, {"type": "text", "text": "Acknowledgments ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "This work is supported in part by the National Science and Technology Major Project (2022ZD0116401); the Natural Science Foundation of China under Grant 62076238, Grant 62222606, and Grant 61902402; the Jiangsu Key Research and Development Plan (No. BE2023016); and the China Computer Federation (CCF)-Tencent Open Fund. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning. In Advances in Neural Information Processing Systems, page 35151\u201335174, 2023.   \n[2] Ekin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. In International Conference on Learning Representations, 2023.   \n[3] Maruan Al-Shedivat, Trapit Bansal, Yura Burda, Ilya Sutskever, Igor Mordatch, and Pieter Abbeel. Continuous adaptation via meta-learning in nonstationary and competitive environments. In International Conference on Learning Representations, 2018.   \n[4] Stefano V Albrecht and Peter Stone. Autonomous agents modelling other agents: A comprehensive survey and open problems. Artificial Intelligence, 258:66\u201395, 2018. [5] Ioannis Antonoglou, Julian Schrittwieser, Sherjil Ozair, Thomas K Hubert, and David Silver. Planning in stochastic environments with a learned model. In International Conference on Learning Representations, 2021.   \n[6] Robert Axelrod. Effective choice in the prisoner\u2019s dilemma. Journal of confilct resolution, 24 (1):3\u201325, 1980.   \n[7] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. Stat, 1050:21, 2016.   \n[8] Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. arXiv preprint arXiv:2306.04637, 2023.   \n[9] Anton Bakhtin, David Wu, Adam Lerer, and Noam Brown. No-press diplomacy from scratch. Advances in Neural Information Processing Systems, 34:18063\u201318074, 2021.   \n[10] Nolan Bard, Michael Johanson, Neil Burch, and Michael Bowling. Online implicit agent modelling. In International Conference on Autonomous Agents and MultiAgent Systems, pages 255\u2013262, 2013.   \n[11] David Brandfonbrener, Alberto Bietti, Jacob Buckman, Romain Laroche, and Joan Bruna. When does return-conditioned supervised learning work for offilne reinforcement learning? In Advances in Neural Information Processing Systems, pages 1542\u20131553, 2022.   \n[12] Noam Brown and Tuomas Sandholm. Superhuman ai for multiplayer poker. Science, 365 (6456):885\u2013890, 2019.   \n[13] Noam Brown, Anton Bakhtin, Adam Lerer, and Qucheng Gong. Combining deep reinforcement learning and search for imperfect-information games. In Advances in Neural Information Processing Systems, pages 17057\u201317069, 2020.   \n[14] Micah Carroll, Rohin Shah, Mark K Ho, Tom Grifftihs, Sanjit Seshia, Pieter Abbeel, and Anca Dragan. On the utility of learning about humans for human-ai coordination. In Advances in Neural Information Processing Systems, page 5174\u20135185, 2019.   \n[15] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. In Advances in Neural Information Processing Systems, pages 15084\u2013 15097, 2021.   \n[16] Filippos Christianos, Lukas Sch\u00e4fer, and Stefano V Albrecht. Shared experience actor-critic for multi-agent reinforcement learning. In Advances in Neural Information Processing Systems, 2020.   \n[17] Zhongxiang Dai, Yizhou Chen, Bryan Kian Hsiang Low, Patrick Jaillet, and Teck-Hua Ho. R2-b2: Recursive reasoning-based bayesian optimization for no-regret learning in games. In International Conference on Machine Learning, pages 2291\u20132301, 2020.   \n[18] Ivo Danihelka, Arthur Guez, Julian Schrittwieser, and David Silver. Policy improvement by planning with gumbel. In International Conference on Learning Representations, 2021.   \n[19] Anthony DiGiovanni and Ambuj Tewari. Thompson sampling for markov games with piecewise stationary opponent policies. In Uncertainty in Artificial Intelligence, pages 738\u2013748, 2021.   \n[20] Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. RL2: Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016.   \n[21] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning, pages 1126\u20131135, 2017.   \n[22] Jakob Foerster, Richard Y Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, and Igor Mordatch. Learning with opponent-learning awareness. In International Conference on Autonomous Agents and MultiAgent Systems, pages 122\u2013130, 2018.   \n[23] Jakob Foerster, Gregory Farquhar, Maruan Al-Shedivat, Tim Rockt\u00e4schel, Eric Xing, and Shimon Whiteson. DiCE: The infinitely differentiable Monte Carlo estimator. In International Conference on Machine Learning, pages 1524\u20131533, 2018.   \n[24] Haobo Fu, Ye Tian, Hongxiang Yu, Weiming Liu, Shuang Wu, Jiechao Xiong, Ying Wen, Kai Li, Junliang Xing, Qiang Fu, et al. Greedy when sure and conservative when uncertain about the opponents. In International Conference on Machine Learning, pages 6829\u20136848, 2022.   \n[25] Kitty Fung, Qizhen Zhang, Chris Lu, Timon Willi, and Jakob Nicolaus Foerster. Analyzing the sample complexity of model-free opponent shaping. In ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems, 2023.   \n[26] Hiroki Furuta, Yutaka Matsuo, and Shixiang Shane Gu. Generalized decision transformer for offilne hindsight information matching. In International Conference on Learning Representations, 2022.   \n[27] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. In Advances in Neural Information Processing Systems, volume 35, pages 30583\u201330598, 2022.   \n[28] Aditya Grover, Maruan Al-Shedivat, Jayesh Gupta, Yuri Burda, and Harrison Edwards. Learning policy representations in multiagent systems. In International Conference on Machine Learning, pages 1802\u20131811, 2018.   \n[29] He He, Jordan Boyd-Graber, Kevin Kwok, and Hal Daum\u00e9 III. Opponent modeling in deep reinforcement learning. In International Conference on Machine Learning, pages 1804\u20131813, 2016.   \n[30] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition, pages 770\u2013778, 2016.   \n[31] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016.   \n[32] Pablo Hernandez-Leal, Matthew E Taylor, Benjamin S Rosman, Luis Enrique Sucar, and E Munoz de Cote. Identifying and tracking switching, non-stationary opponents: A bayesian approach. In AAAI Conference on Artificial Intelligence Workshop on Multiagent Interaction without Prior Coordination, pages 560\u2013566, 2016.   \n[33] Zhang-Wei Hong, Shih-Yang Su, Tzu-Yun Shann, Yi-Hsiang Chang, and Chun-Yi Lee. A deep policy inference $\\mathbf{q}$ -network for multi-agent systems. In International Conference on Autonomous Agents and MultiAgent Systems, pages 1388\u20131396, 2018.   \n[34] Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-learning in neural networks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(9):5149\u20135169, 2021.   \n[35] Hengyuan Hu, Adam Lerer, Noam Brown, and Jakob Foerster. Learned belief search: Efficiently improving policies in partially observable settings. arXiv preprint arXiv:2106.09086, 2021.   \n[36] Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Mohammadamin Barekatain, Simon Schmitt, and David Silver. Learning and planning in complex action spaces. In International Conference on Machine Learning, pages 4476\u20134486. PMLR, 2021.   \n[37] Ahmed Hussein, Mohamed Medhat Gaber, Eyad Elyan, and Chrisina Jayne. Imitation learning: A survey of learning methods. ACM Computing Surveys, 50(2):1\u201335, 2017.   \n[38] Athul Paul Jacob, David J Wu, Gabriele Farina, Adam Lerer, Hengyuan Hu, Anton Bakhtin, Jacob Andreas, and Noam Brown. Modeling strong and human-like gameplay with klregularized search. In International Conference on Machine Learning, pages 9695\u20139728. PMLR, 2022.   \n[39] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A survey on contrastive self-supervised learning. Technologies, 9(1):2, 2020.   \n[40] Dong Ki Kim, Miao Liu, Matthew D Riemer, Chuangchuang Sun, Marwa Abdulhai, Golnaz Habibi, Sebastian Lopez-Cot, Gerald Tesauro, and Jonathan How. A policy gradient algorithm for learning to learn in multiagent reinforcement learning. In International Conference on Machine Learning, pages 5541\u20135550, 2021.   \n[41] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.   \n[42] Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steigerwald, DJ Strouse, Steven Stenberg Hansen, Angelos Filos, Ethan Brooks, Maxime Gazeau, Himanshu Sahni, Satinder Singh, and Volodymyr Mnih. In-context reinforcement learning with algorithm distillation. In International Conference on Learning Representations, 2023.   \n[43] Jonathan N Lee, Annie Xie, Aldo Pacchiano, Yash Chandak, Chelsea Finn, Ofir Nachum, and Emma Brunskill. Supervised pretraining can learn in-context reinforcement learning. arXiv preprint arXiv:2306.14892, 2023.   \n[44] Kuang-Huei Lee, Ofir Nachum, Mengjiao Sherry Yang, Lisa Lee, Daniel Freeman, Sergio Guadarrama, Ian Fischer, Winnie Xu, Eric Jang, Henryk Michalewski, et al. Multi-game decision transformers. In Advances in Neural Information Processing Systems, pages 27921\u2013 27936, 2022.   \n[45] Adam Lerer, Hengyuan Hu, Jakob Foerster, and Noam Brown. Improving policies via search in cooperative partially observable games. In Proceedings of the AAAI conference on artificial intelligence, pages 7187\u20137194, 2020.   \n[46] Alistair Letcher, Jakob Foerster, David Balduzzi, Tim Rockt\u00e4schel, and Shimon Whiteson. Stable opponent shaping in differentiable games. In International Conference on Learning Representations, 2019.   \n[47] Wenzhe Li, Hao Luo, Zichuan Lin, Chongjie Zhang, Zongqing Lu, and Deheng Ye. A survey on transformers in reinforcement learning. Transactions on Machine Learning Research, 2023.   \n[48] Yingcong Li, M Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: Generalization and implicit model selection in in-context learning. In International Conference on Machine Learning, page 19565\u201319594, 2023.   \n[49] Weiming Liu, Haobo Fu, Qiang Fu, and Yang Wei. Opponent-limited online search for imperfect information games. In International Conference on Machine Learning, pages 21567\u201321585. PMLR, 2023.   \n[50] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2018.   \n[51] Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multiagent actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information Processing Systems, pages 1\u201312, 2017.   \n[52] Christopher Lu, Timon Willi, Christian A Schroeder De Witt, and Jakob Foerster. Model-free opponent shaping. In International Conference on Machine Learning, pages 14398\u201314411, 2022.   \n[53] Yongliang Lv, Yuanqiang Yu, Yan Zheng, Jianye Hao, Yongming Wen, and Yue Yu. Limited information opponent modeling. In International Conference on Artificial Neural Networks, pages 511\u2013522. Springer, 2023.   \n[54] Luckeciano C Melo. Transformers are meta-reinforcement learners. In International Conference on Machine Learning, pages 15340\u201315359, 2022.   \n[55] Long Short-Term Memory. Long short-term memory. Neural Computation, 9(8):1735\u20131780, 2010.   \n[56] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-learner. In International Conference on Learning Representations, 2018.   \n[57] Matej Moravc\u02c7\u00edk, Martin Schmid, Neil Burch, Viliam Lisy\\`, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level artificial intelligence in heads-up no-limit poker. Science, 356(6337):508\u2013513, 2017.   \n[58] Samer Nashed and Shlomo Zilberstein. A survey of opponent modeling in adversarial domains. Journal of Artificial Intelligence Research, 73:277\u2013327, 2022.   \n[59] Yazhe Niu, Yuan Pu, Zhenjie Yang, Xueyan Li, Tong Zhou, Jiyuan Ren, Shuai Hu, Hongsheng Li, and Yu Liu. Lightzero: A unified benchmark for monte carlo tree search in general sequential decision scenarios. Advances in Neural Information Processing Systems, 36, 2024.   \n[60] Ian Osband, Daniel Russo, and Benjamin Van Roy. (more) efficient reinforcement learning via posterior sampling. In Advances in Neural Information Processing Systems, pages 3003\u20133011, 2013.   \n[61] Georgios Papoudakis and Stefano V Albrecht. Variational autoencoders for opponent modeling in multi-agent systems. arXiv preprint arXiv:2001.10829, 2020.   \n[62] Georgios Papoudakis, Filippos Christianos, and Stefano Albrecht. Agent modelling under partial observability for deep reinforcement learning. In Advances in Neural Information Processing Systems, pages 19210\u201319222, 2021.   \n[63] Georgios Papoudakis, Filippos Christianos, Lukas Sch\u00e4fer, and Stefano V. Albrecht. Benchmarking multi-agent deep reinforcement learning algorithms in cooperative tasks. In Neural Information Processing Systems Track on Datasets and Benchmarks (NeurIPS), page 10707\u201310717, 2021.   \n[64] Keiran Paster, Sheila McIlraith, and Jimmy Ba. You can\u2019t count on luck: Why decision transformers and rvs fail in stochastic environments. In Advances in Neural Information Processing Systems, pages 38966\u201338979, 2022.   \n[65] Neil Rabinowitz, Frank Perbet, Francis Song, Chiyuan Zhang, SM Ali Eslami, and Matthew Botvinick. Machine theory of mind. In International Conference on Machine Learning, pages 4218\u20134227, 2018.   \n[66] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   \n[67] Roberta Raileanu, Emily Denton, Arthur Szlam, and Rob Fergus. Modeling others using oneself in multi-agent reinforcement learning. In International Conference on Machine Learning, pages 4257\u20134266, 2018.   \n[68] Allan Raventos, Mansheej Paul, Feng Chen, and Surya Ganguli. Pretraining task diversity and the emergence of non-bayesian in-context learning for regression. In Advances in Neural Information Processing Systems, page 1\u201313, 2023.   \n[69] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio G\u00f3mez Colmenarejo, Alexander Novikov, Gabriel Barth-maron, Mai Gim\u00e9nez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent. Transactions on Machine Learning Research, 2022.   \n[70] Benjamin Rosman, Majd Hawasly, and Subramanian Ramamoorthy. Bayesian policy reuse. Machine Learning, 104:99\u2013127, 2016.   \n[71] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604\u2013609, 2020.   \n[72] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.   \n[73] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587): 484\u2013489, 2016.   \n[74] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of Go without human knowledge. Nature, 550(7676):354\u2013359, 2017.   \n[75] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140\u20131144, 2018.   \n[76] Samuel Sokota, Gabriele Farina, David J Wu, Hengyuan Hu, Kevin A Wang, J Zico Kolter, and Noam Brown. The update equivalence framework for decision-time planning. In The Twelfth International Conference on Learning Representations, 2023.   \n[77] Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian process optimization in the bandit setting: no regret and experimental design. In International Conference on Machine Learning, pages 1015\u20131022, 2010.   \n[78] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929\u20131958, 2014.   \n[79] Gerald Tesauro. Td-gammon, a self-teaching backgammon program, achieves master-level play. Neural computation, 6(2):215\u2013219, 1994.   \n[80] Gerald Tesauro and Gregory Galperin. On-line policy improvement using monte-carlo search. In Advances in Neural Information Processing Systems, page 1068\u20131074, 1996.   \n[81] William R Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3-4):285\u2013294, 1933.   \n[82] Yuandong Tian, Qucheng Gong, and Yu Jiang. Joint policy search for multi-agent collaboration with imperfect information. Advances in Neural Information Processing Systems, 33:19931\u2013 19942, 2020.   \n[83] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, page 6000\u20136010, 2017.   \n[84] Adam R Villaflor, Zhe Huang, Swapnil Pande, John M Dolan, and Jeff Schneider. Addressing optimism bias in sequence modeling for reinforcement learning. In International Conference on Machine Learning, pages 22270\u201322283, 2022.   \n[85] Friedrich Burkhard Von Der Osten, Michael Kirley, and Tim Miller. The minds of many: Opponent modeling in a stochastic game. In International Joint Conference on Artificial Intelligence, pages 3845\u20133851, 2017.   \n[86] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, pages 35151\u201335174. PMLR, 2023.   \n[87] Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv preprint arXiv:1611.05763, 2016.   \n[88] Jannis Weil, Johannes Czech, Tobias Meuser, and Kristian Kersting. Know your enemy: Investigating monte-carlo tree search with opponent models in pommerman. arXiv preprint arXiv:2305.13206, 2023.   \n[89] Ying Wen, Yaodong Yang, Rui Luo, Jun Wang, and Wei Pan. Probabilistic recursive reasoning for multi-agent reinforcement learning. In International Conference on Learning Representations, 2019.   \n[90] Ying Wen, Yaodong Yang, and Jun Wang. Modelling bounded rationality in multi-agent interactions by generalized recursive reasoning. In International Joint Conferences on Artificial Intelligence, pages 414\u2013421, 2021.   \n[91] Timon Willi, Alistair Hp Letcher, Johannes Treutlein, and Jakob Foerster. Cola: consistent learning with opponent-learning awareness. In International Conference on Machine Learning, pages 23804\u201323831, 2022.   \n[92] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, et al. Transformers: State-ofthe-art natural language processing. In Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38\u201345, 2020.   \n[93] Zhe Wu, Kai Li, Hang Xu, Yifan Zang, Bo An, and Junliang Xing. L2e: Learning to exploit your opponent. In International Joint Conference on Neural Networks, pages 1\u20138, 2022.   \n[94] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. In International Conference on Learning Representations, 2021.   \n[95] Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical evaluation of rectified activations in convolutional network. arXiv preprint arXiv:1505.00853, 2015.   \n[96] Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, Pieter Abbeel, and Dale Schuurmans. Foundation models for decision making: Problems, methods, and opportunities. arXiv preprint arXiv:2303.04129, 2023. [97] Sherry Yang, Dale Schuurmans, Pieter Abbeel, and Ofir Nachum. Dichotomy of control: Separating what you can control from what you cannot. In International Conference on Learning Representations, 2023. [98] Tianpei Yang, Jianye Hao, Zhaopeng Meng, Chongjie Zhang, Yan Zheng, and Ze Zheng. Towards efficient detection and optimal response against sophisticated opponents. In International Joint Conference on Artificial Intelligence, pages 623\u2013629, 2019. [99] Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, and Yang Gao. Mastering atari games with limited data. Advances in neural information processing systems, 34:25476\u201325488, 2021.   \n[100] Xiaopeng Yu, Jiechuan Jiang, Wanpeng Zhang, Haobin Jiang, and Zongqing Lu. Modelbased opponent modeling. In Advances in Neural Information Processing Systems, pages 28208\u201328221, 2022.   \n[101] Luyao Yuan, Zipeng Fu, Jingyue Shen, Lu Xu, Junhong Shen, and Song-Chun Zhu. Emergence of pragmatics from referential game between theory of mind agents. arXiv preprint arXiv:2001.07752, 2020.   \n[102] Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models in-context. arXiv preprint arXiv:2306.09927, 2023.   \n[103] Rui Zhao, Jinming Song, Yufeng Yuan, Haifeng Hu, Yang Gao, Yi Wu, Zhongqian Sun, and Wei Yang. Maximum entropy population-based training for zero-shot human-ai coordination. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 6145\u20136153, 2023.   \n[104] Stephen Zhao, Chris Lu, Roger B Grosse, and Jakob Foerster. Proximal learning with opponentlearning awareness. In Advances in Neural Information Processing Systems, pages 26324\u2013 26336, 2022.   \n[105] Yan Zheng, Zhaopeng Meng, Jianye Hao, Zongzhang Zhang, Tianpei Yang, and Changjie Fan. A deep bayesian policy reuse approach against non-stationary agents. In Advances in Neural Information Processing Systems, pages 962\u2013972, 2018.   \n[106] Luisa Zintgraf, Sam Devlin, Kamil Ciosek, Shimon Whiteson, and Katja Hofmann. Deep interactive bayesian reinforcement learning via meta-learning. In International Conference on Autonomous Agents and MultiAgent Systems, pages 1712\u20131714, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "Table of Contents for the Appendix ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "A Extensive Related Work 19 ", "page_idx": 17}, {"type": "text", "text": "B Pseudocode of OMIS 21 ", "page_idx": 17}, {"type": "text", "text": "C Construction Process of $D^{\\mathbf{epi,k}}$ 21 ", "page_idx": 17}, {"type": "text", "text": "D Proofs of Theorems 21 ", "page_idx": 17}, {"type": "text", "text": "D.1 Algorithm of Posterior Sampling in Opponent Modeling 21   \nD.2 Proof of Lemma 4.1 . . . 22   \nD.3 Proof of Theorem 4.2 . . 25   \nD.4 Proof of Theorem 4.3 . . 27 ", "page_idx": 17}, {"type": "text", "text": "E Detailed Introductions of the Environments 29 ", "page_idx": 17}, {"type": "text", "text": "F Neural Architecture Design 29 ", "page_idx": 17}, {"type": "text", "text": "G Diversity of Opponent Policies 30 ", "page_idx": 17}, {"type": "text", "text": "H Hyperparameters 33 ", "page_idx": 17}, {"type": "text", "text": "H.1 Hyperparameters for Opponent Policies Training 33   \nH.2 Hyperparameters for In-Context-Learning-based Pretraining . . 33   \nH.3 Hyperparameters for Decision-Time Search with In-Context Components 34 ", "page_idx": 17}, {"type": "text", "text": "I Quantitative Analysis of Attention Weights Learned by OMIS 34 ", "page_idx": 17}, {"type": "text", "text": "J In-depth Discussion 36 ", "page_idx": 17}, {"type": "text", "text": "K Compute Resources 36 ", "page_idx": 17}, {"type": "text", "text": "L Broader Impacts 36 ", "page_idx": 17}, {"type": "text", "text": "A Extensive Related Work ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "Opponent modeling. The recent research in opponent modeling based on machine learning methodologies can be broadly categorized as follows: ", "page_idx": 18}, {"type": "text", "text": "(1) Opponent Modeling with Representation Learning: Embed the opponent policies into a latent space using representation learning methods to enhance the decision-making capabilities of the self-agent. In the study by Grover et al. [28], imitation learning [37] and contrastive learning [39] were utilized to produce policy embeddings for opponent trajectories. Subsequently, these embeddings were integrated with reinforcement learning for policy optimization. Comparable initiatives, exemplified by He et al. [29] and Hong et al. [33], employed auxiliary networks to encode manually crafted opponent features, predict opponent actions, and finetune the policy network to enhance overall performance. Papoudakis et al. [62] suggested employing an autoencoder to leverage the self-agent\u2019s observations and actions for reconstructing the opponent\u2019s observations and actions. This approach aims to learn embeddings that facilitate decision-making. In comparison, Papoudakis and Albrecht [61] and Zintgraf et al. [106] utilized Variational AutoEncoders (VAE) [41] to capture the high-dimensional distribution of opponent policies. ", "page_idx": 18}, {"type": "text", "text": "(2) Opponent Modeling with Bayesian Learning: Detect or deduce the opponent policies in real-time using Bayesian methods and subsequently generate responses based on the inferred information. Bard et al. [10] initially trained a mixture-of-expert counter-strategies against a predefined set of fixed opponent policies. They then utilized a bandit algorithm during testing to dynamically select the most suitable counter-strategy. Building on $\\mathrm{BPR+}$ [70, 32], Zheng et al. [105] introduced a rectified belief model to enhance the precision of opponent policy detection. Furthermore, they introduced a distillation policy network to achieve better results. DiGiovanni and Tewari [19] utilized Thompson sampling [81] and change detection methods to address the challenge of opponent switching between stationary policies. Fu et al. [24] employed a bandit algorithm to select either greedy or conservative policies when playing against a non-stationary opponent. The greedy policy underwent training via VAE in conjunction with conditional reinforcement learning and was continuously updated online through variational inference. In contrast, the conservative policy remained a fixed and robust policy. Lv et al. [53] introduced a similar approach to exploit non-stationary opponents. ", "page_idx": 18}, {"type": "text", "text": "(3) Opponent Modeling with Meta-learning: Leveraging meta-learning methods [34], train against a set of given opponent policies to adapt to unknown opponent policies during testing swiftly. While most meta-reinforcement learning methods presume that tasks in training and testing exhibit a similar distribution, this category investigates the possible application of meta-reinforcement learning in the context of competing with unknown non-stationary opponents. Al-Shedivat et al. [3] introduced a gradient-based meta-learning algorithm designed for continuous adaptation in non-stationary and competitive environments, showcasing its efficacy in enhancing adaptation efficiency. Building upon analysis on Al-Shedivat et al. [3], Kim et al. [40] introduced a novel meta multi-agent policy gradient algorithm designed to effectively handle the non-stationary policy dynamics inherent in multi-agent reinforcement learning. Zintgraf et al. [106] introduced a meta-learning approach for deep interactive Bayesian reinforcement learning in multi-agent settings. This approach utilizes approximate belief inference and policy adaptation to enhance opponent adaptation. Wu et al. [93] put forward a metalearning framework called Learning to Exploit (L2E) for implicit opponent modeling. This framework enables agents to swiftly adapt and exploit opponents with diverse styles through limited interactions during training. ", "page_idx": 18}, {"type": "text", "text": "(4) Opponent Modeling with Shaping Opponents\u2019 Learning: Considering opponents\u2019 learning (i.e., conducting gradient updates), estimating the mutual influence between the future opponent policy and the current self-agent\u2019s policy. Foerster et al. [22] proposed an approach named LOLA, which modeled the one-step update of the opponent\u2019s policy and estimated the learning gradient of the opponent\u2019s policy. Foerster et al. [23] introduced a Differentiable Monte-Carlo Estimator operation to explore the shaping of learning dynamics for other agents, building upon the approach presented by Foerster et al. [22]. Letcher et al. [46] further integrated stability guarantees from LookAhead with opponentshaping capabilities from Foerster et al. [22] to achieve theoretical and experimental improvements. Kim et al. [40] also presented a term closely related to shaping the learning dynamics of other agents\u2019 policies. This considers the impacts of the agent\u2019s current policy on future opponent policies. Lu et al. [52] proposed a meta-learning approach for general-sum games that can exploit naive learners without requiring white-box access or higher-order derivatives. Willi et al. [91] introduced Consistent LOLA (COLA), a new multi-agent reinforcement learning algorithm that addresses inconsistency issues with Foerster et al. [22]\u2019s approach. COLA learns consistent update functions for agents by explicitly minimizing a differentiable measure of consistency. Zhao et al. [104] proposed proximal LOLA (POLA), an algorithm that addresses policy parameterization sensitivity issues with LOLA and more reliably learns reciprocity-based cooperation in partially competitive multi-agent environments. Fung et al. [25] further presented that the sample complexity of Lu et al. [52]\u2019s algorithm scales exponentially with the inner state and action space and the number of agents. ", "page_idx": 18}, {"type": "text", "text": "", "page_idx": 19}, {"type": "text", "text": "(5) Opponent Modeling with Recursive Reasoning: By simulating nested layers of beliefs, predicting the opponent\u2019s behavior, and generating the best response based on the expected reasoning process of the opponent towards the self-agent. Wen et al. [89] and Wen et al. [90] suggested conducting recursive reasoning by modeling hypothetical nested beliefs through the agent\u2019s joint Q-function. Their work demonstrated enhanced adaptation in stochastic games. Dai et al. [17] introduced recursive reasoning by assuming agents select actions based on GP-UCB acquisition functions [77]. This approach achieved faster regret convergence in repeated games. Yuan et al. [101] proposed an algorithm that utilizes ToM-based recursive reasoning and adaptive reinforcement learning. This approach enables agents to develop a pragmatic communication protocol to infer hidden meanings from context and enhance cooperative multi-agent communication. Yu et al. [100] proposed a model-based opponent modeling approach that employs recursive imagination within an environment model and Bayesian mixing to adapt to diverse opponents. ", "page_idx": 19}, {"type": "text", "text": "(6) Opponent Modeling with Theory of Mind (ToM): Reasoning about the opponent\u2019s mental states and intentions to predict and adapt to their behavior. This involves modeling their beliefs, goals, and actions to understand opponent dynamics comprehensively. Von Der Osten et al. [85] introduced a multi-agent ToM model designed to predict opponents\u2019 actions and infer strategy sophistication in stochastic games. Building upon Zheng et al. [105]\u2019s Bayesian online opponent modeling approach, Yang et al. [98] proposed a ToM approach. This approach leverages higher-level decision-making to play against opponents who are also engaged in opponent modeling. Rabinowitz et al. [65] and Raileanu et al. [67] also delved into methodologies for modeling an opponent\u2019s mental state. Rabinowitz et al. [65] utilized three networks to reason about agent actions and goals, simulating a human-like ToM. Raileanu et al. [67] proposed utilizing their own policy to learn the opponent\u2019s goals in conjunction with the opponent\u2019s observations and actions. ", "page_idx": 19}, {"type": "text", "text": "Our work aims to pioneer a new methodology: Opponent Modeling with Decision-Time Search (DTS). We explore how DTS can work in the context of opponent modeling, as intuitively, DTS can make our policy more foresighted. To imbue DTS with opponent awareness and adaptability, we developed a model based on in-context learning to serve as the foundation for DTS. ", "page_idx": 19}, {"type": "text", "text": "Transformers for decision-making. There is an increasing interest in leveraging Transformers for decision-making by framing the problem as sequence modeling [96, 47]. Chen et al. [15] introduced Decision Transformer (DT), a model that predicts action sequences conditioned on returns using a causal Transformer trained on offline data. Subsequent studies have explored enhancements, such as improved conditioning [26, 64], and architectural innovations [84]. Another interesting direction capitalizes on the generality and scalability of Transformers for multi-task learning [44, 69]. Transformers applied to decision-making have demonstrated meta-learning capabilities as well [54]. Recently, Lee et al. [43] introduced a Transformer-based in-context learning approach that, both empirically and theoretically, surpasses behaviors observed in the dataset in terms of regret, a performance metric where DT falls short [11, 97]. Incorporating these insights, our work employs a causal Transformer architecture to maximize the model\u2019s ability for in-context learning, specifically in the realm of opponent modeling. ", "page_idx": 19}, {"type": "text", "text": "B Pseudocode of OMIS ", "text_level": 1, "page_idx": 20}, {"type": "table", "img_path": "bGhsbfyg3b/tmp/6a41ca7051eb356bf5d59b6fc2dfb22af0da5be0d77dc187d89fa6b31313ced7.jpg", "table_caption": [], "table_footnote": [], "page_idx": 20}, {"type": "text", "text": "C Construction Process of $D^{\\mathbf{epi,k}}$ ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "The construction of $D^{\\mathrm{epi,k}}$ is as follows: ", "page_idx": 20}, {"type": "text", "text": "1. Sample $C$ trajectories from all historical games involving $\\pi^{-1,k}$ ; ", "page_idx": 20}, {"type": "text", "text": "2. For each trajectory, sample consecutive segments $\\{\\big(s_{h^{\\prime}},a_{h^{\\prime}-1}^{-1,k}\\big)\\big\\}_{h^{\\prime}=h_{s}}^{h_{s}+\\frac{H}{C}-1}$ , where $h_{s}$ is the starting timestep;   \n3. Concatenate these segments together. ", "page_idx": 20}, {"type": "text", "text": "The construction of $D^{\\mathrm{epi,k}}$ stems from two intuitions: Firstly, $\\pi^{-1,k}$ \u2019s gameplay style can become more evident over continuous timesteps, so we sample consecutive fragments. Secondly, $\\pi^{-1,k}$ can exhibit diverse behaviors across different episodes, so we sample from multiple trajectories. ", "page_idx": 20}, {"type": "text", "text": "D Proofs of Theorems ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "D.1 Algorithm of Posterior Sampling in Opponent Modeling ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We instantiate a Bayesian posterior sampling algorithm in the context of opponent modeling, referred to as Posterior Sampling in Opponent Modeling (PSOM). In the PSOM algorithm, we use opponent trajectory $\\left(s_{0},a_{0}^{-1},\\ldots,s_{T-1},a_{T-1}^{-1}\\right)$ , which consists of consecutive $\\left(s,a^{-1}\\right)$ tuples, to construct the in-context data $D$ . Following up, we describe the PSOM algorithm in a most general way [60, 43]. ", "page_idx": 20}, {"type": "text", "text": "Given the initial distribution of opponent policies $\\Pi_{0}\\leftarrow\\Pi_{\\mathrm{pre}}$ , where $\\Pi_{\\mathrm{pre}}$ is the probability distribution on $\\Pi^{\\mathrm{train}}$ , for $c\\in[C]$ : ", "page_idx": 21}, {"type": "text", "text": "1. Sample an opponent policy $\\pi_{c}^{-1}$ by $\\Pi_{c}$ and compute the BR policy $\\pi_{c}^{1,*}$ for the self-agent; 2. Interact using the self-agent with $\\pi_{c}^{1,*}$ against the opponent with true opponent policy $\\bar{\\pi}^{-1}$ , and use the opponent trajectory $\\bar{(s_{0},a_{0}^{-1},\\ldots,s_{T-1},a_{T-1}^{-1})}$ to construct $D$ . 3. Update the posterior distribution $\\Pi_{c}(\\pi^{-1})=P(\\pi^{-1}|D)$ . ", "page_idx": 21}, {"type": "text", "text": "D.2 Proof of Lemma 4.1 ", "text_level": 1, "page_idx": 21}, {"type": "text", "text": "Lemma 4.1 (Equivalence of OMIS w/o S and PSOM). Assume that the learned $\\pi_{\\theta}$ is consistent and the sampling of s from $\\angles{T}_{p r e}^{-1}$ is independent of opponent policy, then given $\\bar{\\pi}^{-1}$ and its $D$ , we have $P(\\xi_{T}^{1}|D,\\bar{\\pi}^{-1};P S O M)=P(\\xi_{T}^{1}|D,\\bar{\\pi}^{-1};\\pi_{\\theta})$ for all possible $\\xi_{T}^{1}$ . ", "page_idx": 21}, {"type": "text", "text": "Proof. In this section, we use $\\pi^{-1}$ to denote opponent policies posteriorly sampled from $\\Pi_{\\mathrm{pre}}$ by the self-agent and use $\\bar{\\pi}^{-1}$ to denote the true opponent policy interacted with during the testing stage of OMIS. In the proof, we abbreviate OMIS without DTS as OMIS. For clarity and ease of understanding, all trajectory sequences are indexed starting from 1 in this section (originally starting from 0 in the main text). We abbreviate $D^{\\mathrm{epi}}$ as $D$ in the proof, as $D^{\\mathrm{epi}}$ is sufficient for completing the proof. Define $\\xi_{T}^{1}=(s_{1},a_{1}^{1,*},\\dots,s_{T},a_{T}^{1,*})$ as self-agent history, where $T$ denotes the maximum length of this history $(i.e..$ , horizon for each episode) and $a^{1,*}$ is sampled from the best response policy $\\pi^{1,\\breve{*}}$ against $\\pi^{-1}$ . $\\dot{T}_{\\mathrm{pre}}^{-1}(\\cdot;\\pi^{-1})$ denotes the probability distribution on all the trajectories involving $\\pi^{-1}$ during pretraining. ", "page_idx": 21}, {"type": "text", "text": "Let $\\pi^{-1}\\sim\\Pi_{\\mathrm{pre}}$ and $D$ contain sampled trajectory fragments of $\\pi^{-1}$ and let $s_{\\mathrm{query}}\\,\\in\\,{\\cal S},a^{1,*}\\,\\in$ $\\mathcal{A}^{1},\\xi_{T-1}^{1}\\in(\\dot{S}\\times\\mathcal{A}^{1})^{T-1}$ and $t\\in[0,T-1]$ be arbitrary, the full joint probability distribution during OMIS\u2019s pretraining stage can be denoted as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathsf{\\Gamma}_{\\mathrm{pre}}(\\pi^{-1},D,\\xi_{T-1}^{1},t,s_{\\mathsf{q u e r y}},a^{1,*})=\\Pi_{\\mathrm{pre}}(\\pi^{-1})T_{\\mathrm{pre}}^{-1}(D;\\pi^{-1})\\mathfrak{S}_{T}(s_{1:T})\\mathcal{S}_{\\mathsf{q u e r y}}(s_{\\mathsf{q u e r y}}){\\pi^{1,*}}(a^{1,*}|s_{\\mathsf{q u e r y}};\\pi^{-1})}\\\\ &{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\times\\mathrm{\\Unif}[0,T-1]\\displaystyle\\prod_{i\\in[T]}\\pi^{1,*}(a_{i}^{1}|s_{i};\\pi^{-1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Herein, $\\mathfrak{S}_{T}\\in\\Delta(S^{T})$ , which is independent of opponent policy. $S_{\\mathrm{query}}$ is set to the uniform over $\\boldsymbol{S}$ In addition, we sample $t\\sim\\mathrm{Unif}[0,T-1]$ and truncating $\\xi_{t}^{1}$ from $\\xi_{T-1}^{1}$ (or, equivalently, sample $\\xi_{t}^{1}\\sim\\Delta((S\\times A^{1})^{t})$ directly). ", "page_idx": 21}, {"type": "text", "text": "We define the random sequences and subsequences of the self-agent trajectory under PSOM algorithm as $\\Xi_{\\mathrm{PSOM}}(t;D)=(\\bar{S}_{1}^{\\mathrm{PSOM}},A_{1}^{1,\\mathrm{PSOM}},\\cdot\\cdot,S_{t}^{\\mathrm{PSOM}},A_{t}^{1,\\mathrm{PSOM}})$ StPSOM, At1,PSOM). This trajectory is generated in the following manner: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\pi_{\\mathrm{PSOM}}^{-1}\\sim P(\\pi^{-1}|D),S_{1}^{\\mathrm{PSOM}}\\sim\\rho,}\\\\ &{A_{i}^{1,\\mathrm{PSOM}}\\sim\\pi^{1,*}(\\cdot|S_{i}^{\\mathrm{PSOM}};\\pi_{\\mathrm{PSOM}}^{-1}),A_{i}^{-1,\\mathrm{PSOM}}\\sim\\bar{\\pi}^{-1}(\\cdot|S_{i}^{\\mathrm{PSOM}}),i\\geq1,}\\\\ &{S_{i+1}^{\\mathrm{PSOM}}\\sim\\mathcal{P}(\\cdot|S_{i}^{\\mathrm{PSOM}},A_{i}^{1,\\mathrm{PSOM}},A_{i}^{-1,\\mathrm{PSOM}}),i\\geq2.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Within, $\\rho$ denotes the initial distribution on $\\boldsymbol{S}$ . Analogously, we define the random sequences and subsequences of the self-agent trajectory under OMIS algorithm as $\\Xi_{\\mathrm{pre}}(t;D)\\;=\\;$ $(S_{1}^{\\mathrm{pre}},A_{1}^{1,\\mathrm{pre}},...,S_{t}^{\\mathrm{pre}},A_{t}^{1,\\mathrm{pre}})$ . This trajectory is generated in the following manner: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{S_{1}^{\\mathrm{pre}}\\sim\\rho,}\\\\ &{A_{i}^{1,\\mathrm{pre}}\\sim P_{\\mathrm{pre}}(\\cdot|S_{i}^{\\mathrm{pre}},D,\\Xi_{\\mathrm{pre}}(i-1;D)),A_{i}^{-1,\\mathrm{pre}}\\sim\\bar{\\pi}^{-1}(\\cdot|S_{i}^{\\mathrm{pre}}),i\\geq1,}\\\\ &{S_{i+1}^{\\mathrm{pre}}\\sim\\mathcal{P}(\\cdot|S_{i}^{\\mathrm{pre}},A_{i}^{1,\\mathrm{pre}},A_{i}^{-1,\\mathrm{pre}}),i\\geq2.}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "To simplify matters, we will refrain from explicitly referencing $D$ for $\\Xi$ in our notations, except when required to avoid confusion. Next, we introduce a common assumption to ensure the neural network fits the pretraining data distribution. ", "page_idx": 21}, {"type": "text", "text": "Assumption D.1 (Learned $\\pi_{\\theta}$ is consistent). For any given $(S_{i}^{p r e},D,\\Xi_{p r e}(i\\ \\textrm{\\--}\\ 1;D))$ , $P_{p r e}(A_{i}^{1,p r e}|S_{i}^{p r e},D,\\Xi_{p r e}(i-1;D))=\\pi_{\\theta}(A_{i}^{1,p r e}|S_{i}^{p r e},D,\\Xi_{p r e}(i-1;D))$ for all possible $A_{i}^{1,p r e}$ . ", "page_idx": 22}, {"type": "text", "text": "Upon Assump. D.1, we will limit our attention to $P_{\\mathrm{pre}}$ for the rest of the proof. ", "page_idx": 22}, {"type": "text", "text": "To prove that $\\forall\\xi_{T}^{1},P(\\xi_{T}^{1}|D,\\bar{\\pi}^{-1};\\mathrm{PSOM})=P(\\xi_{T}^{1}|D,\\bar{\\pi}^{-1};\\pi_{\\theta})$ (i.e., Lemma 4.1), it is equivalent to prove that ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P(\\Xi_{\\mathrm{PSOM}}(T)=\\xi_{T}^{1})=P(\\Xi_{\\mathrm{pre}}(T)=\\xi_{T}^{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "We will prove that $\\forall t\\in[T]$ , ", "page_idx": 22}, {"type": "equation", "text": "$$\nP(\\Xi_{\\mathrm{PSOM}}(t)=\\xi_{t}^{1})=P(\\Xi_{\\mathrm{pre}}(t)=\\xi_{t}^{1})\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "using Mathematical Induction and then introduce a lemma for the evidence of Eq. (12). ", "page_idx": 22}, {"type": "text", "text": "To begin with, we propose a lemma to assist in proving Eq. (13) for the base case when $t=1$ . ", "page_idx": 22}, {"type": "text", "text": "Lemma D.2. If the sampling of $s$ from $\\angles{T}_{p r e}^{-1}$ is independent of opponent policy, then $P_{p r e}(\\pi^{-1}|D)=$ $P(\\pi_{P S O M}^{-1}=\\pi^{-1}|D)$ . ", "page_idx": 22}, {"type": "text", "text": "Proof. Assuming the sampling of $s$ from $\\mathcal{T}_{\\mathrm{pre}}^{-1}$ is independent of opponent policy, we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(\\pi_{\\mathtt{P S O M}}^{-1}=\\pi^{-1}|D)\\propto\\Pi_{\\mathrm{pre}}(\\pi^{-1})P(D|\\pi^{-1})}\\\\ &{\\qquad\\qquad\\qquad\\propto\\Pi_{\\mathrm{pre}}(\\pi^{-1})\\displaystyle\\prod_{j\\in[|D|]}\\pi^{-1}(a_{j}^{-1}|s_{j})}\\\\ &{\\qquad\\qquad\\quad\\propto\\Pi_{\\mathrm{pre}}(\\pi^{-1})\\displaystyle\\prod_{j\\in[|D|]}\\pi^{-1}(a_{j}^{-1}|s_{j})\\mathcal{T}_{\\mathrm{pre}}^{-1}(s_{j})}\\\\ &{\\qquad\\qquad\\qquad\\quad=\\Pi_{\\mathrm{pre}}(\\pi^{-1})\\mathcal{T}_{\\mathrm{pre}}^{-1}(D;\\pi^{-1})}\\\\ &{\\qquad\\qquad\\quad\\propto P_{\\mathrm{pre}}(\\pi^{-1}|D).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "${\\displaystyle\\propto}$ denotes that the two sides are equal up to multiplicative factors independent of $\\pi^{-1}$ . Eq. (14a) is derived through the Bayesian posterior probability formula. Eq. (14b) uses the fact that $s$ in posterior sampling is independent of opponent policy. Eq. (14c) holds because of the assumption that the sampling of $s$ from $\\mathcal{T}_{\\mathrm{pre}}^{-1}$ is independent of opponent policy. Eq. (14d) uses the definition of $\\mathcal{T}_{\\mathrm{pre}}^{-1}$ . Eq. (14e) is derived through the Bayesian posterior probability formula. \u53e3 ", "page_idx": 22}, {"type": "text", "text": "Now, we prove that Eq. (13) holds when $t=1$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(\\Xi_{\\mathtt{P S O M}}(1)=\\xi_{1}^{1})=P(S_{1}^{\\mathtt{P S O M}}=s_{1},A_{1}^{1,\\mathtt{P S O M}}=a_{1}^{1})}\\\\ &{=\\rho(s_{1})\\int_{\\pi^{-1}}P(A_{1}^{1,\\mathtt{P S O M}}=a_{1}^{1},\\pi_{\\mathtt{P S O M}}^{-1}=\\pi^{-1}|S_{1}^{\\mathtt{P S O M}}=s_{1})\\mathrm{d}\\pi^{-1}}\\\\ &{=\\rho(s_{1})\\int_{\\pi^{-1}}\\pi^{1,*}(a_{1}^{1}|s_{1};\\pi^{-1})P_{\\mathtt{P S O M}}(\\pi_{\\mathtt{P S O M}}^{-1}=\\pi^{-1}|D,S_{1}^{\\mathtt{P S O M}}=s_{1})\\mathrm{d}\\pi^{-1}}\\\\ &{=\\rho(s_{1})\\int_{\\pi^{-1}}\\pi^{1,*}(a_{1}^{1}|s_{1};\\pi^{-1})P_{\\mathtt{P S O M}}(\\pi_{\\mathtt{P S O M}}^{-1}=\\pi^{-1}|D)\\mathrm{d}\\pi^{-1}}\\\\ &{=\\rho(s_{1})\\int_{\\pi^{-1}}\\pi^{1,*}(a_{1}^{1}|s_{1};\\pi^{-1})P_{\\mathtt{P S O M}}(\\pi_{\\mathtt{P S O M}}^{-1}=\\pi^{-1}|D)\\mathrm{d}\\pi^{-1}}\\\\ &{=\\rho(s_{1})\\int_{\\pi^{-1}}\\pi^{1,*}(a_{1}^{1}|s_{1};\\pi^{-1})P_{\\mathtt{P S C}}(\\pi^{-1}|D)\\mathrm{d}\\pi^{-1}}\\\\ &{=\\rho(s_{1})P_{\\mathtt{P S O}}(a_{1}^{1}|s_{1},D)}\\\\ &{=P(\\equiv\\mathtt{p}_{\\mathtt{P S C}}(1)=\\xi_{1}^{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Eqs. (15a) to (15c), (15f) and (15g) are derived using Bayesian law of total probability and conditional probability formula based on Eq. (11). Eq. (15d) holds because the sampling of $s_{1}$ is independent of $\\bar{\\pi}^{-1}$ . Eq. (15e) is derived through Lem. D.2. ", "page_idx": 22}, {"type": "text", "text": "Next, we start proving Eq. (13) for the other cases when $t\\neq1$ . We utilize the inductive hypothesis to demonstrate the validity of the entire statement. Suppose that $P(\\Xi_{\\mathrm{PSOM}}(t\\mathrm{~-~}1)\\mathrm{~=~}^{\\bullet}\\dot{\\xi}_{t-1}^{1})\\mathrm{~=~}$ ", "page_idx": 22}, {"type": "text", "text": "$P(\\Xi_{\\mathrm{pre}}(t-1)=\\xi_{t-1}^{1})$ , since ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(\\Xi_{\\mathsf{P S O M}}(t)=\\xi_{t}^{1})=}\\\\ &{P(\\Xi_{\\mathsf{P S O M}}(t-1)=\\xi_{t-1}^{1})P(S_{t}^{\\mathrm{PSOM}}=s_{t},A_{t}^{1,\\mathrm{PSOM}}=a_{t}^{1}|\\Xi_{\\mathsf{P S O M}}(t-1)=\\xi_{t-1}^{1})}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "and ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(\\Xi_{\\mathrm{pre}}(t)=\\xi_{t}^{1})=}\\\\ &{P(\\Xi_{\\mathrm{pre}}(t-1)=\\xi_{t-1}^{1})P(S_{t}^{\\mathrm{pre}}=s_{t},A_{t}^{1,\\mathrm{pre}}=a_{t}^{1}|\\Xi_{\\mathrm{pre}}(t-1)=\\xi_{t-1}^{1}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "to prove that $P(\\Xi_{\\mathrm{PSOM}}(t)=\\xi_{t}^{1})=P(\\Xi_{\\mathrm{pre}}(t)=\\xi_{t}^{1})$ , it is equivalent to prove: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(S_{t}^{\\mathrm{pSOM}}=s_{t},A_{t}^{1,\\mathrm{PSOM}}=a_{t}^{1}|\\Xi_{\\mathrm{pSOM}}(t-1)=\\xi_{t-1}^{1})}\\\\ &{~=P(S_{t}^{\\mathrm{pre}}=s_{t},A_{t}^{1,\\mathrm{pre}}=a_{t}^{1}|\\Xi_{\\mathrm{pre}}(t-1)=\\xi_{t-1}^{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "By expanding Eq. (16), we can get: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(S_{t}^{\\mathrm{PSOM}}=s_{t},A_{t}^{1,\\mathrm{PSOM}}=a_{t}^{1}|\\Xi_{\\mathrm{PSOM}}(t-1)=\\xi_{t-1}^{1})}\\\\ &{=\\mathcal{P}(s_{t}|s_{t-1},a_{t-1}^{1};\\bar{\\pi}^{-1})P(A_{t}^{1,\\mathrm{PSOM}}=a_{t}^{1}|S_{t}^{\\mathrm{PSOM}}=s_{t},\\Xi_{\\mathrm{PSOM}}(t-1)=\\xi_{t-1}^{1})}\\\\ &{=\\displaystyle\\int_{a_{t-1}^{-1}}\\mathcal{P}(s_{t}|s_{t-1},a_{t-1}^{1},a_{t-1}^{-1})\\bar{\\pi}^{-1}(a_{t-1}^{-1}|s_{t-1})\\mathrm{d}a_{t-1}^{-1}}\\\\ &{\\cdot\\displaystyle\\int_{\\pi^{-1}}P(A_{t}^{1,\\mathrm{PSOM}}=a_{t}^{1},\\pi_{\\mathrm{PSOM}}^{-1}=\\pi^{-1}|S_{t}^{\\mathrm{PSOM}}=s_{t},\\Xi_{\\mathrm{PSOM}}(t-1)=\\xi_{t-1}^{1})\\mathrm{d}\\pi^{-1}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In Eq. (17b), the first integral term is the same for PSOM and OMIS, while the term inside the second integral term satisfies: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{P(A_{t}^{1,\\mathrm{PSOM}}=a_{t}^{1},\\pi_{\\mathrm{PSOM}}^{-1}=\\pi^{-1}|S_{t}^{\\mathrm{PSOM}}=s_{t},\\Xi_{\\mathrm{PSOM}}(t-1)=\\xi_{t-1}^{1})}\\\\ &{\\ =\\pi^{1,*}(a_{t}^{1}|s_{t};\\pi^{-1})P(\\pi_{\\mathrm{PSOM}}^{-1}=\\pi^{-1}|S_{t}^{\\mathrm{PSOM}}=s_{t},\\Xi_{\\mathrm{PSOM}}(t-1)=\\xi_{t-1}^{1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Based on Eq. (17) and Eq. (18), to prove that Eq. (16) holds, it is equivalent to prove: ", "page_idx": 23}, {"type": "equation", "text": "$$\nP(\\pi_{\\mathtt{P S O M}}^{-1}=\\pi^{-1}|S_{t}^{\\mathrm{PSOM}}=s_{t},\\Xi_{\\mathrm{PSOM}}(t-1)=\\xi_{t-1}^{1})=P_{\\mathrm{pre}}(\\pi^{-1}|s_{t},D,\\xi_{t-1}^{1}).\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "We prove that Eq. (19) holds through the following derivation: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle P(\\overline{{\\mathbf{r}}}_{\\mathrm{PSO}}^{-1}\\mathbf{\\Lambda}=\\boldsymbol{\\pi}^{-1}|S_{t}^{\\mathrm{PSO}}\\mathbf{\\Lambda}=s_{t},\\overline{{\\mathbf{z}}}_{\\mathrm{PSO}}(t-1)=\\xi_{t-1}^{1})}}\\\\ {{\\displaystyle=\\frac{P(S_{t}^{\\mathrm{PSOM}}=s_{t},\\overline{{\\mathbf{z}}}_{\\mathrm{PSOM}}(t-1)=\\xi_{t-1}^{1}|\\overline{{\\mathbf{z}}}_{\\mathrm{PSOM}}^{-1}=\\boldsymbol{\\pi}^{-1})P(\\overline{{\\mathbf{z}}}_{\\mathrm{PSOM}}^{-1}=\\boldsymbol{\\pi}^{-1}|D)}{{P(S_{t}^{\\mathrm{PSOM}}=s_{t},\\overline{{\\mathbf{z}}}_{\\mathrm{PSOM}}(t-1)=\\xi_{t-1}^{1})}}}}\\\\ {{\\displaystyle\\propto P_{\\mathrm{pe}}(\\pi^{-1}|D)\\prod_{i\\in[t-1]}^{{\\cal T}}\\phi(s_{i+1}|\\xi_{i}^{1},\\overline{{\\boldsymbol{\\pi}}}^{-1})\\pi^{1,*}(a_{i}^{1}|s_{i};\\pi^{-1})}}\\\\ {{\\displaystyle\\propto P_{\\mathrm{pe}}(\\pi^{-1}|D)\\prod_{i\\in[t-1]}^{{\\cal T}^{1,*}(a_{i}^{1}|s_{i};\\pi^{-1})}}}\\\\ {{\\displaystyle\\propto P_{\\mathrm{pe}}(\\pi^{-1}|D)\\prod_{i\\in[t-1]}^{{\\cal T}^{1,*}(a_{i}^{1}|s_{i};\\pi^{-1})}}}\\\\ {{\\displaystyle\\propto P_{\\mathrm{pe}}(\\pi^{-1}|D)S_{\\mathrm{qurp}}(s_{t})\\mathfrak{S}_{t-1}(s_{1:t-1})\\prod_{i\\in[t-1]}^{{\\cal T}^{1,*}(a_{i}^{1}|s_{i};\\pi^{-1})}}}\\\\ {{\\displaystyle\\propto P_{\\mathrm{pe}}(\\pi^{-1}|s_{t},D,\\xi_{t-1}^{1}).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Eq. (20a) is derived through the Bayesian posterior probability formula. In Eq. (20b), we decompose the probability of observing the sequence of observations $s$ and actions $a^{1}$ . Eqs. (20c) and (20d) use the fact that the sampling of $s$ is only related to the true opponent policy $\\bar{\\pi}^{-1}$ and is independent of $\\pi^{-1}$ . Eq. (20e) is derived by the definition of $P_{\\mathrm{pre}}(\\pi^{-1}|s_{t}\\rangle\\dot{D},\\xi_{t-1}^{1})$ . ", "page_idx": 23}, {"type": "text", "text": "Therefore, we finish the proof of $P(\\Xi_{\\mathrm{PSOM}}(t)=\\xi_{t}^{1})=P(\\Xi_{\\mathrm{pre}}(t)=\\xi_{t}^{1})$ , where ", "page_idx": 23}, {"type": "equation", "text": "$$\nP(\\Xi_{\\mathrm{PSOM}}(t)=\\xi_{t}^{1})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "equation", "text": "$$\n\\begin{array}{r l r}&{}&{=P(\\Xi_{\\mathtt{p r e}}(t-1)=\\xi_{t-1}^{1})\\mathcal{P}(s_{t}|s_{t-1},a_{t-1}^{1};\\bar{\\pi}^{-1})\\displaystyle\\int_{\\pi^{-1}}\\pi^{1,*}(a_{t}^{1}|s_{t};\\pi^{-1})P_{\\mathtt{p r e}}(\\pi^{-1}|s_{t},D,\\xi_{t-1}^{1})\\mathrm{d}\\pi^{-1}}\\\\ &{}&{\\qquad\\qquad\\qquad\\qquad(2\\ln)}\\\\ &{}&{=P(\\Xi_{\\mathtt{p r e}}(t-1)=\\xi_{t-1}^{1})\\mathcal{P}(s_{t}|s_{t-1},a_{t-1}^{1};\\bar{\\pi}^{-1})P_{\\mathtt{p r e}}(a_{t}^{1}|s_{t},D,\\xi_{t-1}^{1})\\qquad\\qquad\\qquad\\qquad\\quad(2\\ln)}\\\\ &{}&{=P(\\Xi_{\\mathtt{p r e}}(t)=\\xi_{t}^{1}).\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(2\\operatorname{lc})}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Based on Mathematical Induction, Eq. (13) holds for any $t\\in[T]$ . Hence, Eq. (12) is satisfied. This concludes the proof. \u53e3 ", "page_idx": 24}, {"type": "text", "text": "D.3 Proof of Theorem 4.2 ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Theorem 4.2. When $\\bar{\\pi}^{-1}=\\pi^{-1,k}\\in\\Pi^{t r a i n}$ , if the PS algorithm converges to the optimal solution, then OMIS w/o S recognizes the policy of $\\Phi$ as $\\pi^{-1,k}$ , i.e., $\\pi_{\\theta},\\,\\mu_{\\phi}$ , and $V_{\\omega}$ converge to $\\pi^{1,k,*},\\pi^{-1,k}$ , and $V^{1,k,*}$ , respectively; When $\\bar{\\pi}^{-1}\\notin\\Pi^{t r a i n}$ , OMIS w/o $S$ recognizes the policy of $\\Phi$ as the policies in $\\Pi^{t r a i n}$ with the minimum $D_{K L}(P(\\stackrel{\\cdot}{a}^{-1}|s,\\pi^{-1})||P(a^{-1}|s,\\bar{\\pi}^{-\\breve{1}}))$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. In the proof, we abbreviate OMIS without DTS as OMIS. We denote the in-context data consisting of $\\bar{(s,a^{-1})}$ tuples as $D$ , and the in-context data consisting of $(s,a^{1},s^{\\prime1},r^{1})$ tuples as $D^{\\prime}$ , where $s^{\\prime}$ is the next state of $s$ transitioned to. Note that the original PS algorithm uses $D^{\\prime}$ while PSOM and OMIS use $D$ to recognize the policy of $\\Phi$ . ", "page_idx": 24}, {"type": "text", "text": "To begin, we propose a lemma and its corollary to prove the convergence guarantee of the PSOM algorithm and to analyze its properties in opponent policy recognition. ", "page_idx": 24}, {"type": "text", "text": "Lemma D.3. Let $\\begin{array}{r}{f(\\pi^{-1};D)\\ =\\ -\\int_{s,a^{-1}}P(s,a^{-1};D)\\log(P(a^{-1}|s,\\pi^{-1}))\\mathrm{d}s\\mathrm{d}a^{-1}}\\end{array}$ and $\\pi_{\\star}^{-1}\\,=$ ar $\\begin{array}{r l}{\\operatorname*{min}_{\\pi^{-1}\\in\\Pi^{t r a i n}}}&{{}f(\\pi^{-1};D).}\\end{array}$ , then $\\forall\\pi^{-1}\\quad\\in\\quad\\{\\pi^{-1}|f(\\pi^{-1};D)\\quad\\neq\\quad f(\\pi_{\\star}^{-1};D)\\}$ , we have $\\frac{P(\\pi^{-1}|(s,\\!a^{-1})_{1:n})}{P(\\pi_{\\star}^{-1}|(s,\\!a^{-1})_{1:n})}\\overset{P}{\\to}0$ ", "page_idx": 24}, {"type": "text", "text": "Proof. Here, $\\pi_{\\star}^{-1}$ denotes the equivalent class of opponent policies to which PSOM converges with non-zero probability. $P(s,a^{-1};\\bar{D})$ is the distribution of $\\bar{(s,a^{-1})}$ tuples in $D.\\;n$ is the number of the $\\left(s,a^{-1}\\right)$ tuples. To prove tha t PP  ((\u03c0\u03c0\u2212\u221211||((ss,,aa\u2212\u221211))1:n)) P\u21920 under the given conditions, it is equivalent to prove: ", "page_idx": 24}, {"type": "equation", "text": "$$\nL_{\\pi^{-1},n}=-\\log\\frac{P(\\pi^{-1}|(s,a^{-1})_{1:n})}{P(\\pi_{\\star}^{-1}|(s,a^{-1})_{1:n})}\\stackrel{P}{\\rightarrow}+\\infty.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By expanding Eq. (22), we can get: ", "page_idx": 24}, {"type": "equation", "text": "$$\nL_{\\pi^{-1},n}=-\\log\\frac{P(\\pi^{-1}|(s,a^{-1})_{1:n})}{P(\\pi_{\\star}^{-1}|(s,a^{-1})_{1:n})}=-\\log\\frac{P(\\pi^{-1})}{P(\\pi_{\\star}^{-1})}-\\sum_{i=1}^{n}\\log(\\frac{P(a^{-1}|\\pi^{-1},s)}{P(a^{-1}|\\pi_{\\star}^{-1},s)}).\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "According to the definition of $\\pi_{\\star}^{-1}$ and the condition $f(\\pi^{-1};D)\\neq f(\\pi_{\\star}^{-1};D)$ , we have: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\mathbb{E}_{(s,a^{-1})\\sim P(\\cdot;D)}[-\\sum_{i=1}^{n}\\log(\\frac{P(a^{-1}|\\pi^{-1},s)}{P(a^{-1}|\\pi_{\\star}^{-1},s)})]=f(\\pi^{-1};D)-f(\\pi_{\\star}^{-1};D)=\\mathcal{C}>0.\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Here, $\\mathcal{C}$ is a positive constant. Therefore, based on the law of large numbers, we have $\\begin{array}{r}{\\operatorname*{lim}_{n\\rightarrow\\infty_{-}}P(|\\frac{L_{\\pi^{-1},n}}{n}-\\mathcal{C}|>\\epsilon)=0}\\end{array}$ , where $\\epsilon$ is any positive number. Hence, Eq. (22) is satisfied, and the proof ends. ", "page_idx": 24}, {"type": "text", "text": "Corollary D.4 (Corollary of Lem. D.3). When $\\bar{\\pi}^{-1}\\;\\;\\;\\in\\;\\;\\;\\Pi^{t r a i n}$ , we have $\\begin{array}{r l r}{\\bar{\\pi}^{-1}}&{{}\\in}&{\\pi_{\\star}^{-1}}\\end{array}$ ; When $\\bar{\\pi}^{\\overset{.}{-1}}\\quad\\notin\\quad\\Pi^{t r a i n},\\;\\;\\pi_{\\star}^{\\overset{.}{-1}}$ is the equivalent class of policies in $\\Pi^{t r a i n}$ with the minimum $D_{K L}(P(a^{-1}|s,\\pi^{-1})||P(a^{-1}|s,\\bar{\\pi}^{-1}))$ . ", "page_idx": 24}, {"type": "text", "text": "Proof. Since $P(s,a^{-1};D)=P(s;D)P(a^{-1}|s,\\bar{\\pi}^{-1})$ holds, we have ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\pi_{\\star}^{-1}=\\arg\\operatorname*{min}_{\\pi^{-1}\\in\\Pi^{\\mathrm{train}}}-\\int_{s,a^{-1}}P(s,a^{-1};D)\\log(P(a^{-1}|s,\\pi^{-1}))\\mathrm{d}s\\mathrm{d}a^{-1}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\lefteqn{=\\arg\\operatorname*{min}_{\\pi^{-1}\\in\\Pi^{\\mathrm{min}}}-\\int_{s,a^{-1}}P(s;D)P(a^{-1}|s,\\bar{\\pi}^{-1})\\log(P(a^{-1}|s,\\pi^{-1}))\\mathrm{d}s\\mathrm{d}a^{-1}}}\\\\ {}&{=\\arg\\\\operatorname*{min}_{\\pi^{-1}\\in\\Pi^{\\mathrm{min}}}-\\int_{s,a^{-1}}P(s;D)P(a^{-1}|s,\\bar{\\pi}^{-1})\\log(P(a^{-1}|s,\\pi^{-1}))\\mathrm{d}s\\mathrm{d}a^{-1}}\\\\ {}&{+\\int_{s,a^{-1}}P(s;D)P(a^{-1}|s,\\bar{\\pi}^{-1})\\log(P(a^{-1}|s,\\bar{\\pi}^{-1}))\\mathrm{d}s\\mathrm{d}a^{-1}}\\\\ {}&{=\\arg\\operatorname*{min}_{\\pi^{-1}\\in\\Pi^{\\mathrm{min}}}-\\int_{s,a^{-1}}P(s;D)P(a^{-1}|s,\\bar{\\pi}^{-1})\\frac{\\log(P(a^{-1}|s,\\pi^{-1}))}{\\log(P(a^{-1}|s,\\bar{\\pi}^{-1}))}\\mathrm{d}s\\mathrm{d}a^{-1}}\\\\ {}&{=\\arg\\operatorname*{min}_{\\pi^{-1}\\in\\Pi^{\\mathrm{min}}}\\int_{s}P(s;D)D k_{L}(P(a^{-1}|s,\\pi^{-1})|P(a^{-1}|s,\\bar{\\pi}^{-1}))\\mathrm{d}s}\\\\ {}&{=\\arg\\frac{\\operatorname*{min}}{\\pi^{-1}\\in\\Pi^{\\mathrm{min}}}\\int_{s}P(s;D)D k_{L}(P(a^{-1}|s,\\pi^{-1})|P(a^{-1}|s,\\bar{\\pi}^{-1}))\\mathrm{d}s}\\\\ {}&{=\\arg\\frac{\\operatorname*{min}}{\\pi^{-1}\\in\\Pi^{\\mathrm{min}}}\\frac{\\mathbb{E}_{s}\\sim P(\\cdot)(D)}{\\log(P(a^{-1}|s,\\pi^{-1}))}\\left[P(a^{-1}|s,\\bar{\\pi}^{-1})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "When $D$ is the in-context data of the opponent policy $\\bar{\\pi}^{-1}\\qquad\\in\\qquad\\Pi^{\\mathrm{train}}.$ $D_{K L}(P(a^{-1}|s,\\pi^{-1})||P(a^{-1}|s,\\bar{\\pi}^{-1}))=0$ in Eq. (25f), and $\\pi_{\\star}^{-1}$ is the equivalent class of opponent policies that have the same action distribution as $\\bar{\\pi}^{-1}$ , i.e., $a_{.}^{\\stackrel{.}{-1}},P(a^{-1}|\\bar{s},\\pi_{\\star}^{-1})=P(a^{-1}|\\bar{s},\\bar{\\pi}^{-1})$ . When $D$ is the in-context data of an opponent policy $\\bar{\\pi}^{-1}\\notin\\Pi^{\\mathrm{train}},\\pi$ $\\pi_{\\star}^{-1}$ is the equivalent class of opponent policies that minimizes the expected KL divergence $\\dot{D}_{K L}(P(a^{\\hat{-}1}|s,\\pi^{-1})||P(a^{-1}|s,\\bar{\\pi}^{-1}))$ . ", "page_idx": 25}, {"type": "text", "text": "Using a similar proving method as in Lem. D.3, it can be proved straightforward that the PS algorithm can converge: Let $\\begin{array}{r l r}{f(\\pi^{-1};D^{\\prime})}&{{}\\stackrel{!}{=}}&{{}-\\int_{s,a^{1},s^{\\prime1},r^{1}}}\\end{array}$ $P(s,a^{1},s^{\\prime1},r^{1};D^{\\prime})\\log(P(s^{\\prime1},r^{1}|s,a^{1},\\pi^{-1}))\\mathrm{d}s\\mathrm{d}a^{1}\\mathrm{d}s^{\\prime1}\\mathrm{d}r^{1}$ and \u03c0\u2032\u22121 = arg $\\mathrm{min}_{\\pi^{-1}\\in\\Gamma}$ train $f(\\pi^{-1};D^{\\prime})$ , then $\\forall\\pi^{-1}\\;\\;\\;\\stackrel{\\{}}{\\in}\\;\\;\\;\\{\\pi^{-1}|f(\\pi^{-1};D^{\\prime})\\;\\;\\;\\neq\\;\\;\\;f(\\pi_{\\star}^{\\prime-\\hat{1}};D^{\\prime})\\}$ , we have $\\frac{P(\\pi^{-1}|(s,\\boldsymbol{a}^{1},\\boldsymbol{s}^{\\prime1},\\boldsymbol{r}^{1})_{1:n})}{P(\\pi_{\\star}^{\\prime-1}|(s,\\boldsymbol{a}^{1},\\boldsymbol{s}^{\\prime1},\\boldsymbol{r}^{1})_{1:n})}\\overset{P}{\\to}0$ ", "page_idx": 25}, {"type": "text", "text": "Next, we introduce a lemma to prove that if the PS algorithm converges to the optimal solution, PSOM converges to the optimal solution. ", "page_idx": 25}, {"type": "text", "text": "Lemma D.5. Given $s,a^{1},\\bar{\\pi}^{-1},\\pi_{\\star}^{-1},\\,i f\\,\\,\\forall a^{-1},P(a^{-1}|s,\\pi_{\\star}^{-1})\\,=\\,P(a^{-1}|s,\\bar{\\pi}^{-1})$ holds, it can be deduced that $\\forall s^{\\prime1},r^{1}$ , $P(s^{\\prime1},r^{1}|s,a^{1},\\pi_{\\star}^{-1})=P(s^{\\prime1},r^{1}|s,a^{1},\\bar{\\pi}^{-1}),$ , but the reverse is not true. ", "page_idx": 25}, {"type": "text", "text": "Proof. For the forward deduction $(i.e.,\\Rightarrow)$ , we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\forall s^{\\prime1},r^{1},P(s^{\\prime1},r^{1}|s,a^{1},\\pi_{\\star}^{-1})}\\\\ &{\\ =\\displaystyle\\sum_{a^{-1}}P(a^{-1}|s,\\pi_{\\star}^{-1})P(s^{\\prime1},r^{1}|s,a^{1},s^{\\prime},a^{-1})}\\\\ &{\\ =\\displaystyle\\sum_{a^{-1}}P(a^{-1}|s,\\bar{\\pi}^{-1})P(s^{\\prime1},r^{1}|s,a^{1},s^{\\prime},a^{-1})}\\\\ &{\\ =P(s^{\\prime1},r^{1}|s,a^{1},\\bar{\\pi}^{-1}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "For the backward deduction $(i.e.,~~\\Leftarrow)$ , counterexamples exist. For example, when $P(s^{\\prime1},r^{1}|s,a^{1},s^{\\prime},a^{-1})$ takes equal values for some $\\begin{array}{r}{\\dot{\\boldsymbol{a}}^{-1}\\quad\\in\\quad\\bar{\\mathcal{A}}^{-1}\\quad\\subset\\quad\\dot{\\mathcal{A}}^{-1}}\\end{array}$ and $\\begin{array}{r l r l r l}{\\bar{\\leq}}&{{}}&{\\mathcal{A}^{-1}\\backslash\\bar{\\mathcal{A}}^{-1},P(a^{-1}|s,\\pi_{\\star}^{-1})}&{}&{{}=}&{{}}&{P(a^{-1}|s,\\bar{\\pi}^{-1});}\\end{array}$ $\\begin{array}{r}{\\sum_{a^{-1}\\in\\bar{A}^{-1}}P(a^{-1}|s,\\pi_{\\star}^{-1})}\\end{array}$ $\\begin{array}{r l}{=}&{{}\\sum_{a^{-1}\\in\\bar{\\mathcal{A}}^{-1}}P(a^{-1}|s,\\bar{\\pi}^{-1})}\\end{array}$ hold, $P(a^{-1}|s,\\pi_{\\star}^{-1})\\ \\ =\\ \\ P(a^{-1}|s,\\bar{\\pi}^{-1})$ does not necessarily hold for all $a^{-1}\\in A^{-1}$ . This means when $\\bar{\\pi}^{-1}\\in\\Pi^{\\mathrm{train}}$ , the PS algorithm may lead to distributions on opponent policies with non-zero probability other than the equivalence class of $\\bar{\\pi}^{-1}$ , resulting in potential suboptimality compared to PSOM. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "According to Lem. D.5, $\\pi_{\\star}^{-1}\\subset\\pi_{\\star}^{\\prime-1}$ . Based on Cor. D.4, we have $\\bar{\\pi}^{-1}\\in\\pi_{\\star}^{-1}$ . Thus, we conclude that $\\bar{\\pi}^{-1}\\overset{\\bullet}{\\in}\\pi_{\\star}^{-1}\\subset\\pi_{\\star}^{\\prime-1}$ . Hence, if PS converges to the optimal solution, PSOM converges to the optimal solution. ", "page_idx": 25}, {"type": "text", "text": "Based on Lemma 4.1, it can be inferred that OMIS is equivalent to PSOM. Thus, all the proofs in this section also hold for OMIS. We have the following conclusions: ", "page_idx": 25}, {"type": "text", "text": "1. Based on Cor. D.4 and Lem. D.5, when $\\bar{\\pi}^{-1}\\in\\Pi^{\\mathrm{train}}$ , if the PS algorithm converges to the optimal solution, then OMIS converges to the optimal solution. If the true opponent policy is $\\pi^{-1,k}$ , OMIS recognizes the current policy of $\\Phi$ as $\\pi^{-1,k}$ . In this case, $\\pi_{\\theta}$ converges to $\\pi^{1,k,*}$ . Similarly to the PSOM algorithm in App. D.1, when we replace $\\pi^{1,*}$ with $\\pi^{-1}$ and $V^{1,*}$ , we can derive the algorithms with the same theoretical guarantees. Thus, $\\mu_{\\phi}$ and $V_{\\omega}$ converge to $\\pi^{-1,k}$ and $V^{1,k,*}$ , respectively. ", "page_idx": 26}, {"type": "text", "text": "2. Based on Cor. D.4, when $\\bar{\\pi}^{-1}\\not\\in\\Pi^{\\mathrm{train}}$ , OMIS recognizes the current policy of $\\Phi$ as the policies in $\\Pi^{\\mathrm{train}}$ with the minimum KL divergence $\\bar{D_{K L}}(P(a^{-1}|s,\\pi^{-1}\\rangle||P(\\stackrel{\\leftarrow}{a^{-1}}|s,\\bar{\\pi}^{-1}))$ . ", "page_idx": 26}, {"type": "text", "text": "D.4 Proof of Theorem 4.3 ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Theorem 4.3 (Policy Improvement of OMIS\u2019s DTS). Given $\\bar{\\pi}^{-1}$ and its $D$ , suppose OMIS recognizes $\\Phi$ as $\\pi_{\\star}^{-1}$ and $V_{\\pi_{\\star}^{-1}}^{\\pi}$ is the value vector on $\\boldsymbol{S}$ , where $V(s):=V_{\\omega}(s,D),\\pi(a|s):=\\pi_{\\theta}(a|s,D)$ . Let $\\mathcal{G}_{L}$ be the $L$ -step DTS operator and $\\pi^{\\prime}\\in\\mathcal{G}_{L}(V_{\\pi_{\\star}^{-1}}^{\\pi})$ , then $V_{\\pi_{\\star}^{-1}}^{\\pi^{\\prime}}\\geq V_{\\pi_{\\star}^{-1}}^{\\pi}$ holds component-wise. ", "page_idx": 26}, {"type": "text", "text": "Proof. To begin with, we do not consider the mixing technique in the proof. Based on Theorem 4.2, given $\\bar{\\pi}^{-1}$ and its $D$ , OMIS recognize the policy of $\\Phi$ as $\\pi_{\\star}^{-1}$ , which means $\\pi_{\\theta},\\mu_{\\phi}$ , and $V_{\\omega}$ converge to $\\pi^{1,\\star}$ , $\\pi_{\\star}^{-1}$ , and $V^{1,\\star}$ , respectively. When $\\bar{\\pi}^{-1}\\,\\in\\,\\Pi^{\\mathrm{train}}$ , since the labels in the pretraining data may not be optimal, there is space for improvement in the $\\pi\\;(i.e.,\\pi^{1,\\star})$ . When $\\bar{\\pi}^{-1^{\\!}}\\notin\\Pi^{\\operatorname{train}}$ , $\\pi$ may not be the best response against $\\bar{\\pi}^{-1}$ , thus there is still space for policy improvement. Furthermore, disregarding the impact of $D_{t}^{\\mathrm{step}}$ on $\\mu_{\\phi},\\mu_{\\phi}$ can be treated as a fixed policy during the DTS process. Thus, the virtual environment for the DTS is stationary. ", "page_idx": 26}, {"type": "text", "text": "A $L$ -greedy policy w.r.t. the value function $V_{\\pi_{\\star}^{-1}}$ , belongs to the following set of policies, ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\arg\\underset{\\pi_{0}}{\\operatorname*{max}}\\underset{\\pi_{1},\\ldots,\\pi_{L-1}}{\\operatorname*{max}}\\mathbb{E}_{|\\cdot}^{\\pi_{0}\\ldots\\pi_{L-1}}\\left[\\displaystyle\\sum_{l=0}^{L-1}\\gamma_{\\mathrm{search}}^{l}R(s_{l},\\pi_{l}(s_{l});\\pi_{\\star}^{-1})+\\gamma_{\\mathrm{search}}^{L}V_{\\pi_{\\star}^{-1}}(s_{L})\\right]}\\\\ &{=\\arg\\underset{\\pi_{0}}{\\operatorname*{max}}\\mathbb{E}_{|\\cdot}^{\\pi_{0}}\\left[R(s_{0},\\pi_{0}(s_{0});\\pi_{\\star}^{-1})+\\gamma_{\\mathrm{search}}(\\mathcal{T}^{L-1}V_{\\pi_{\\star}^{-1}})(s_{1})\\right]}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where the notation $\\mathbb{E}_{|\\cdot}^{\\pi_{0}\\ldots\\pi_{L-1}}$ means that we condition on the trajectory induced by the choice of actions $(\\pi_{0}(s_{0}),\\pi_{1}(s_{1}),\\ldots,\\pi_{L-1}(s_{L-1}))$ and the starting state $s_{0}=\\cdot^{3}$ The $\\pi_{\\star}^{-1}$ terms in $R$ means opponents take actions by $\\mu_{\\phi}$ conditioned on $D$ . We define $\\mathcal{T}^{\\pi^{1}}$ as the operator choosing actions using $\\pi^{1}$ for one step, where $\\pi^{1}$ is any self-agent policy. We define $\\tau$ as the Bellman optimality operator, where ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{T}V_{\\pi_{\\star}^{-1}}=\\underset{\\pi^{1}}{\\operatorname*{max}}\\,\\mathcal{T}^{\\pi^{1}}V_{\\pi_{\\star}^{-1}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Following up, we define $\\mathcal{T}^{L}$ (shown in Eq. (27b)) as the $L$ -step Bellman optimality operator, where ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{T}^{L}V_{\\pi_{\\star}^{-1}}=\\operatorname*{max}_{\\pi_{0},\\ldots,\\pi_{L-1}}\\mathbb{E}_{|\\cdot}^{\\pi_{0}\\ldots\\pi_{L-1}}\\left[\\sum_{l=0}^{L-1}\\gamma_{\\mathrm{seareh}}^{l}R(s_{l},\\pi_{l}(s_{l});\\pi_{\\star}^{-1})+\\gamma_{\\mathrm{seareh}}^{L}V_{\\pi_{\\star}^{-1}}(s_{L})\\right].\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "Since the argument in Eq. (27b) is a vector, the maximization is component-wise, i.e., we want to find the choice of actions that will jointly maximize the entries of the vector. Thus, the $L$ -greedy policy chooses the first optimal action of a non-stationary, optimal control problem with horizon $L$ . Since $\\pi$ is maximized to select actions during OMIS\u2019s DTS, Eq. (9) can be considered equivalent to Eq. (27).4 ", "page_idx": 26}, {"type": "text", "text": "The set of $L$ -greedy policies w.r.t. $V_{\\pi_{\\star}^{-1}}$ , i.e., the $L$ -step DTS operator, $\\mathcal{G}_{L}(V_{\\pi_{\\star}^{-1}})$ , can be expressed as follows: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\forall V_{\\pi_{\\star}^{-1}},\\pi^{1},{\\mathcal T}_{L}^{\\pi^{1}}V_{\\pi_{\\star}^{-1}}\\stackrel{\\mathrm{def}}{=}{\\mathcal T}^{\\pi^{1}}{\\mathcal T}^{L-1}V_{\\pi_{\\star}^{-1}},\n$$", "text_format": "latex", "page_idx": 26}, {"type": "equation", "text": "$$\n\\forall V_{\\pi_{\\star}^{-1}},\\mathcal{G}_{L}(V_{\\pi_{\\star}^{-1}})=\\{\\pi^{\\prime}:T_{L}^{\\pi^{\\prime}}V_{\\pi_{\\star}^{-1}}=\\mathcal{T}^{L}V_{\\pi_{\\star}^{-1}}\\}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "For Eq. (30a), the operator $\\mathcal{T}^{\\pi^{1}}\\mathcal{T}^{L-1}$ represents choosing actions using $\\pi^{1}$ in the first step and selecting the optimal actions from all possibilities in the subsequent $L-1$ steps. For Eq. (30b), the policy $\\pi^{\\prime}$ derived from $\\mathcal{G}_{L}(V_{\\pi_{\\star}^{-1}})$ satisfies that choosing actions using $\\pi$ in the first step and selecting the optimal actions from all possibilities in the subsequent $L-1$ steps is equivalent to choosing all possible optimal actions in all $\\mathrm{L}$ steps. ", "page_idx": 27}, {"type": "text", "text": "We observe that ", "page_idx": 27}, {"type": "equation", "text": "$$\nV_{\\pi_{\\star}^{-1}}^{\\pi}=T^{\\pi}V_{\\pi_{\\star}^{-1}}^{\\pi}\\leq T V_{\\pi_{\\star}^{-1}}^{\\pi}.\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Then, sequentially using Eqs. (30a), (30b) and (31), we have ", "page_idx": 27}, {"type": "equation", "text": "$$\nV_{\\pi_{\\star}^{-1}}^{\\pi}=(\\mathcal T^{\\pi})^{L}V_{\\pi_{\\star}^{-1}}^{\\pi}\\le\\mathcal T^{L}V_{\\pi_{\\star}^{-1}}^{\\pi}=\\mathcal T_{L}^{\\pi^{\\prime}}V_{\\pi_{\\star}^{-1}}^{\\pi}=\\mathcal T^{\\pi^{\\prime}}(\\mathcal T^{L-1}V_{\\pi_{\\star}^{-1}}^{\\pi}).\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This leads to the following inequalities: ", "page_idx": 27}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{V_{\\pi_{\\star}^{-1}}^{\\pi}\\leq\\mathcal{T}^{\\pi^{\\prime}}(\\mathcal{T}^{L-1}V_{\\pi_{\\star}^{-1}}^{\\pi})}\\\\ &{\\qquad\\leq\\mathcal{T}^{\\pi^{\\prime}}(\\mathcal{T}^{L-1}\\mathcal{T}V_{\\pi_{\\star}^{-1}}^{\\pi})=\\mathcal{T}^{\\pi^{\\prime}}(\\mathcal{T}^{L}V_{\\pi_{\\star}^{-1}}^{\\pi})}\\\\ &{\\qquad=\\mathcal{T}^{\\pi^{\\prime}}\\left(\\mathcal{T}^{\\pi^{\\prime}}\\mathcal{T}^{L-1}V_{\\pi_{\\star}^{-1}}^{\\pi}\\right)=\\left(\\mathcal{T}^{\\pi^{\\prime}}\\right)^{2}(\\mathcal{T}^{L-1}V_{\\pi_{\\star}^{-1}}^{\\pi})}\\\\ &{\\qquad\\leq\\cdots}\\\\ &{\\qquad\\underset{n\\to\\infty}{\\mathrm{lim}}\\left(\\mathcal{T}^{\\pi^{\\prime}}\\right)^{n}(\\mathcal{T}^{L-1}V_{\\pi_{\\star}^{-1}}^{\\pi})=V_{\\pi_{\\star}^{-1}}^{\\pi^{\\prime}}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "Within, Eq. (33a) holds because of Eq. (32), Eq. (33b) is due to Eq. (31) and the monotonicity of $\\mathcal{T}^{\\pi^{\\prime}}$ and $\\tau$ (and thus of their composition), Eq. (33c) is derived by Eq. (32), and Eq. (33d) is due to the fixed point property of $\\mathcal{T}^{\\pi^{\\prime}}$ . Lastly, notice that $V_{\\pi_{\\star}^{-1}}^{\\pi}\\,=\\,V_{\\pi_{\\star}^{-1}}^{\\pi^{\\prime}}$ if and only if (see Eq. (31)) $\\tau V_{\\pi_{\\star}^{-1}}^{\\pi}=V_{\\pi_{\\star}^{-1}}^{\\pi}$ , which holds if and only if $\\pi$ is the optimal policy. This concludes the proof. ", "page_idx": 27}, {"type": "text", "text": "E Detailed Introductions of the Environments ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Predator Prey [51] is a competitive environment with a three vs. one setup and a continuous state space. The environment consists of three predators (in red), one prey (in green), and two obstacles (in black). The goal of the predators is to capture (i.e., collide with) the prey as much as possible, while the goal of the prey is to be captured as little as possible. The environment features sparse rewards, where each time a predator captures the prey, the capturing predator receives a reward of 10, and the prey receives a reward of $-10$ . Additionally, the environment provides a small, dense reward to the prey to prevent it from running out of the map boundaries. Here, the prey is the self-agent, and the three predators serve as opponents. From the perspective of the self-agent, the environment is highly unstable, as there are three opponents with unknown policies in the environment. The challenge in this environment is that the self-agent must model the behavior of three opponents simultaneously and adapt to various potential coordination strategies employed by the opponents (e.g., surrounding from three different directions). For specific implementation of this environment, we adopt the open-source code of Multi-Agent Particle Environment, which is available at https://github.com/openai/multiagent-particle-envs. ", "page_idx": 28}, {"type": "text", "text": "Level-Based Foraging [16, 63] is a mixed environment in a $9\\!\\times\\!9$ grid world containing two players: the self-agent (in blue) and the opponent (in black), along with five apples (in red). At the beginning of each episode, the two players and the five apples are randomly generated in the environment and assigned a level marked in their bottom-right corner. The goal of the self-agent is to eat as many apples as possible. All players can move in four directions or eat an apple. Eating an apple can be successfully done only under the following conditions: one or two players are around the apple, and all players who take the action of eating an apple have a summed level at least equal to the level of the apple. The environment has sparse rewards, representing the players\u2019 contributions to eating all the apples in the environment. The environment is essentially a long-term social dilemma and can be viewed as an extension of the Prisoner\u2019s Dilemma [6]. The challenge in this environment is that the self-agent must learn to cooperate to eat high-level apples while greedily eating low-level apples simultaneously. For specific implementation of this environment, we adopt the open-source code of lb-foraging, which is available at https://github.com/semitable/lb-foraging. ", "page_idx": 28}, {"type": "text", "text": "OverCooked [14] is a cooperative environment with high-dimensional images as states. One of the chefs is the self-agent (green), while the other chef is the opponent (blue). The two chefs must collaborate to complete a series of subtasks and serve dishes. All players share the same sparse rewards, earning 20 for each successful dish served. The more successful the dish servings, the higher the reward. The goal of the self-agent is to collaborate as effectively as possible with the other chef to maximize the return. The challenge in this environment is for the self-agent to not only be able to complete subtasks such as getting onions, putting onions into the pot, and serving dishes but also to coordinate intensively with the opponent. It requires the self-agent to allocate subtasks effectively with the opponent, ensuring that it does not negatively impact the opponent (e.g., not blocking the opponent\u2019s path). For specific implementation of this environment, we adopt the open-source code of Overcooked-AI, which is available at https://github.com/ HumanCompatibleAI/overcooked_ai. ", "page_idx": 28}, {"type": "text", "text": "F Neural Architecture Design ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "For OMIS, we adopt the neural architecture design as follows: ", "page_idx": 28}, {"type": "text", "text": "The backbone of the OMIS architecture is mainly implemented based on the causal Transformer, i.e., GPT2 [66] model of Hugging Face [92]. The backbone is a GPT2 decoder composed of 3 selfattention blocks. Each self-attention block consists of a single-head attention layer and a feed-forward layer. Residual connections [30] and LayerNorm [7] are utilized after each layer in the self-attention block. Within each attention layer, dropout [78] is added to the residual connection and attention weight. ", "page_idx": 28}, {"type": "text", "text": "In the backbone, except for the fully connected layer in the feed-forward layer (the feed-forward layer consists of a fully connected layer that increases the number of hidden layer nodes and a projection layer that recovers the number of hidden layer nodes), which consists of 128 nodes with GELU [31] activation functions, the other hidden layers are composed of 32 nodes without activation functions. The modality-specific linear layers for self-agent actions, opponents actions, and RTGs comprise 32 nodes without activation functions. The modality-specific linear (and convolutional) layers for states comprise 32 nodes with LeakyReLU [95] activation functions. ", "page_idx": 28}, {"type": "image", "img_path": "bGhsbfyg3b/tmp/80db1972c36c366439326cde77a2f4819f697dd82ea95ff13194cbd89def4f43.jpg", "img_caption": ["Figure 10: The architecture of OMIS during testing. "], "img_footnote": [], "page_idx": 29}, {"type": "text", "text": "", "page_idx": 29}, {"type": "text", "text": "For input encoding, states $s$ , self-agent actions $a^{1}$ , opponents actions $a^{-1}$ , RTGs $G^{1}$ are fed into modality-specific linear layers. For OC, additional convolutional layers are added before the linear layers to encode the state $s$ better. A positional episodic timestep encoding is added. We adopt the same timestep encoding as in Chen et al. [15]. In addition, an agent index encoding is added to each token to distinguish the inputs from different agents. ", "page_idx": 29}, {"type": "text", "text": "For output decoding, the sequences of embedded tokens are fed into the backbone, which autoregressively outputs the self-agent actions $a^{1}$ , opponents actions $a^{-1}$ , values $V$ at the positions of state $s$ tokens using a causal self-attention mask. The $\\pi_{\\theta}$ who outputs $a^{1}$ , the $\\mu_{\\phi}$ who outputs $a^{-1}$ , and the $V_{\\omega}$ who outputs $V$ all consists of linear layers. ", "page_idx": 29}, {"type": "text", "text": "During pretraining, for each timestep $t$ given $\\begin{array}{r l r}{\\pi^{-1,k}}&{{}\\sim}&{\\Pi^{\\mathrm{train}}}\\end{array}$ , the input sequence is $(\\widetilde{s}_{1},\\widetilde{a}_{1}^{-1,\\bar{k}},\\dots,\\widetilde{s}_{H},\\widetilde{a}_{H}^{-1,k}$ $s_{t-B+1},\\stackrel{1,k,*}{a_{t-B+1}},\\stackrel{-1,k}{a_{t-B+1}^{-1,k}},G_{t-B+1}^{1,k,*},\\stackrel{1,k,*}{\\dots},s_{t-1},\\stackrel{1,k,*}{a_{t-1}^{1,k}},\\stackrel{-1,k}{a_{t-1}^{-1,k}},\\stackrel{-1,k}{G_{t-1}^{1,k,*}},s_{t})$ where $B$ is the maximum sequence length as Transformer model has a token capacity. The output prediction sequence is $\\bigl({a}_{t-B+1}^{1},{a}_{t-B+1}^{-1},\\stackrel{\\_}{V}_{t-B+1},\\ldots,{a}_{t-1}^{1},{a}_{t-1}^{-1},{V}_{t-1}\\bigr).$ . The output label sequence $(a_{t-B+1}^{1,k,*},a_{t-B+1}^{-1,k},G_{t-B+1}^{1,k,*},\\ldots,a_{t-1}^{1,k,*},a_{t-1}^{-1,k},G_{t-1}^{1,k,*}).$ . ", "page_idx": 29}, {"type": "text", "text": "During testing, for each timestep $t$ given $\\Phi=\\bar{\\pi}^{-1}$ , $\\begin{array}{r}{\\bar{\\pi}^{-1}\\sim\\Pi^{\\mathrm{test}},}\\end{array}$ 5 the input sequence is $(\\tilde{s}_{1},\\tilde{a}_{1}^{-1}$ , $\\cdot\\cdot\\cdot,\\tilde{s}_{H},\\tilde{a}_{H}^{-1},s_{t-B+1},a_{t-B+1}^{1},\\bar{a}_{t-B+1}^{-1},V_{t-B+1},\\ldots,s_{t-1},a_{t-1}^{1},\\bar{a}_{t-1}^{-1},V_{t-1},s_{t}\\Big)$ , where $\\bar{a}^{-1}$ is the true actions of $\\Phi$ . The output sequence is $\\left(a_{t-B+1}^{1},a_{t-B+1}^{-1},V_{t-B+1},\\ldots,a_{t-1}^{1},a_{t-1}^{-1},V_{t-1}\\right)$ . We demonstrated the architecture of OMIS during pretraining in Fig. 1, see the architecture of OMIS during testing in Fig. 10. ", "page_idx": 29}, {"type": "text", "text": "For all the baselines, we adopt the neural architecture design as follows: ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "We replace the original backbone of the baselines (e.g., linear layers, recurrent layers, LSTM [55], and more) with the same GPT2 backbone as OMIS. For input encoding and input encoding, we encode and decode states $s$ and actions $a$ using the same modality-specific layers as OMIS. We encode and decode rewards $r$ using the modality-specific layers used to encode RTGs in OMIS. Note that we only modified the neural architectures of all the baselines to ensure fair comparisons. All the baselines are still pretrained and tested according to their respective methodologies. ", "page_idx": 29}, {"type": "text", "text": "G Diversity of Opponent Policies ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "As mentioned in Sec. 5.1, we run the Maximum Entropy Population-based training algorithm (MEP) to generate a diversified opponent policy population. Nevertheless, a quantitative analysis is still necessary to measure the similarity/dissimilarity between different opponent policies within the MEP population. We calculate the pair-wise KL divergence between different opponent policies to measure their dissimilarity. The results for PP, LBF, and OC are shown in Figs. 11 to 13, respectively. ", "page_idx": 29}, {"type": "image", "img_path": "bGhsbfyg3b/tmp/d42d45ccd78aa137e4d8d9c9649d7550a34c6a3d83576dea7a4cbeac7856f758.jpg", "img_caption": ["Figure 11: Pair-wise KL divergence of all policies within MEP population in PP "], "img_footnote": [], "page_idx": 30}, {"type": "text", "text": "For any given policies $\\pi_{i}$ and $\\pi_{j}$ , we estimate the KL divergence between them by: ", "page_idx": 30}, {"type": "equation", "text": "$$\nD_{K L}(\\pi_{i}||\\pi_{j})=\\mathbb{E}_{s\\sim P(s)}\\left[\\sum_{a\\in\\cal A}\\pi_{i}(a|s)\\cdot\\log{\\frac{\\pi_{i}(a|s)}{\\pi_{j}(a|s)}}\\right].\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "Here, $P(s)$ denotes the state distribution. Ideally, $P(s)$ should cover the entire state space $\\boldsymbol{S}$ . However, in practical situations, covering the entire state space in even slightly large environments can be intractable. ", "page_idx": 30}, {"type": "text", "text": "To maximize the coverage of the state space by $P(s)$ , we employ the following sampling method: Within the MEP population, there are a total of 20 opponent policies. For each opponent policy $\\pi^{-1,k}$ , we sample 1000 episodes. In these 1000 episodes, the opponents\u2019 policy are fixed to $\\pi^{-\\bar{1},k}$ while the self-agent traverses through all the opponent policies, resulting in the self-agent using per opponent policy for 50 episodes. ", "page_idx": 30}, {"type": "text", "text": "In Figs. 11 to 13, $\\pi^{-1,k},k~=~1,2,\\ldots,10$ denotes seen opponent policies, while $\\pi^{-1,k},k\\;=\\;$ $11,12,\\ldots,20$ denotes unseen opponent policies. The lighter the color in the heatmap, the higher the KL divergence value, indicating a lower similarity between the two policies. ", "page_idx": 30}, {"type": "text", "text": "In the PP and OC environments, there is relatively large dissimilarity between all pairs of opponent policies. Assuming a dissimilarity threshold of 1.0 (i.e., two policies are dissimilar if their KL divergence is greater than 1.0), the dissimilarity rates for PP and OC are $93.75\\%$ and $91.5\\%$ , respectively. In contrast, the dissimilarity rate for LBF is $66.25\\%$ , indicating relatively smaller differences between opponent policies. This could be attributed to the fact that the state space of LBF is much smaller than PP and OC, making it difficult for well-trained opponent policies to exhibit significant behavioral diversity. Nonetheless, overall, we can consider the MEP opponent policy population we generated to be adequately diverse. ", "page_idx": 30}, {"type": "image", "img_path": "bGhsbfyg3b/tmp/aa31c4f695468955da8d818fc9c16f0c154f666acb8b390dfa05f16d935fbf5a.jpg", "img_caption": ["Level-Based Foraging ", "Figure 12: Pair-wise KL divergence of all policies within MEP population in LBF "], "img_footnote": [], "page_idx": 31}, {"type": "image", "img_path": "bGhsbfyg3b/tmp/811b0b2d52dbc536f5a738e176f92d3d7291342de1d8b45d5e8c469aa7b27807.jpg", "img_caption": ["Figure 13: Pair-wise KL divergence of all policies within MEP population in OC "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "H Hyperparameters ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "H.1 Hyperparameters for Opponent Policies Training ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "As mentioned in Sec. 5.1, we employ a diversity-driven Population-Based Training algorithm MEP [103] to train a policy population, which is further used to create the opponent policy sets $\\Pi^{\\mathrm{train}}$ and $\\Pi^{\\mathrm{test}}$ . For specific implementation of MEP, we adopt the open-source code of maximum_entropy_population_based_training, which is available at https://github. com/ruizhaogit/maximum_entropy_population_based_training. For the three environments, we use the same hyperparameters as this open-source code to train the MEP populations. ", "page_idx": 32}, {"type": "table", "img_path": "bGhsbfyg3b/tmp/0b8320ab0b9c3638e1a5e8e47013c04ea9296b884f7c8a57f997e6d9b110dc37.jpg", "table_caption": ["H.2 Hyperparameters for In-Context-Learning-based Pretraining "], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "bGhsbfyg3b/tmp/4089b74463d7c8737b98e15710087448ff3529fb01de11abfb1b3029d0a1c489.jpg", "table_caption": ["H.3 Hyperparameters for Decision-Time Search with In-Context Components "], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "I Quantitative Analysis of Attention Weights Learned by OMIS ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "To rigorously evaluate whether OMIS can effectively characterize opponent policies, we conduct a quantitative analysis of the attention weights learned by OMIS by calculating the pair-wise Pearson Correlation Coefficients (PCC) between the attention vectors. The relevant results are shown in Fig. 14. The first column is the heatmaps of the pair-wise PCC statistics of all attention vectors, and the second column shows the corresponding $p$ -value plots for the statistics in the first column, with pairs marked in white for $p<0.05$ and black otherwise. ", "page_idx": 33}, {"type": "text", "text": "The observations reveal that the attention vectors of the same opponent policy have strong pair-wise correlations (i.e., statistics close to 1 and $p\\,<\\,0.05)$ ) across multiple timesteps. In contrast, the attention vectors of different opponent policies generally have no strong pair-wise correlations with each other. Although there is some pair-wise correlation between the attention vectors of different opponent policies, each opponent policy generally has the strongest pair-wise correlation with its own other attention vectors. These observations indicate that the attention weights learned by OMIS can be distinguished by different opponent policies and maintain consistency for the same opponent policy to some extent. Therefore, this analysis further demonstrates OMIS\u2019s ability to represent opponent policies based on in-context data. ", "page_idx": 33}, {"type": "image", "img_path": "bGhsbfyg3b/tmp/88d8edc867b27d4c883c871c3e08d6b73d847f6e66dcca5acdaf40ef8c22502c.jpg", "img_caption": ["Figure 14: Pair-wise PCC statistics and $p$ -values between the attention weights learned by OMIS. The attention vectors on $D^{\\mathrm{epi,}k}$ are calculated over the final 20 timesteps against each opponent policy. "], "img_footnote": [], "page_idx": 34}, {"type": "text", "text": "J In-depth Discussion ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "In Sec. 6 of the main text, we analyzed this study\u2019s limitations and future work from four perspectives. Herein, we would like to point out that there are currently many potential feasible solutions for each aspect. The OMIS proposed in this paper can be viewed as a complete framework that tackled the main problems in existing OM works during the pretraining and testing stages. This framework can be modified for other settings (such as the opponents are learning, imperfect information, etc.). Moreover, this framework also represents a minimalist approach, focusing on generic opponent modeling settings, while more complex settings can be considered as new research problems to explore in the future. ", "page_idx": 35}, {"type": "text", "text": "(1) For the settings where opponents are learning, according to the observations in Laskin et al. [42], ICL has the ability to model a sequence taken during the learning process. Therefore, we can potentially model continuously updating opponents by using the complete $\\left(s,a^{-1}\\right)$ sequences during opponent learning as in-context data. Strictly speaking, regardless of the type of opponent, as long as we have their in-context data and their best response policy, we can use the OMIS framework to learn to respond to that opponent. Another possible solution is to leverage the idea of Opponent Modeling with Shaping Opponents\u2019 Learning [22, 23, 46, 40, 52, 91, 104, 25] (see App. A), explicitly modeling the opponent\u2019s gradient updates during testing to shape their learning process. ", "page_idx": 35}, {"type": "text", "text": "(2) For imperfect information settings, there is a vast of research in the field of imperfect information online search [57, 12, 13, 82, 35, 38, 49], with many mature methods that can be adapted to work within the OMIS framework. Yet, this adaptation is non-trivial, as such DTS methods often require explicit or learned beliefs about the true state, introducing significant additional computational complexity. Interestingly, a recently proposed Update-Equivalence Framework [76] suggests that we can effectively search in imperfect information settings without relying on beliefs. ", "page_idx": 35}, {"type": "text", "text": "(3) For more complex decision-time searches, numerous advanced DTS methods [74, 75, 12, 13, 9, 45, 38, 36, 5, 99, 18, 59] can seamlessly integrate with our framework. This is because the OMIS pretraining stage learns all the key components needed for DTS: an actor, a critic, and an opponent imitator. The actor provides a good prior decision for the self-agent during the DTS, the critic estimates the value of a given terminal state during the DTS, and the opponent imitator estimates the most probable action for the opponent during the DTS. ", "page_idx": 35}, {"type": "text", "text": "K Compute Resources ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "\u2022 CPU: AMD EPYC 7742 64-Core Processor $\\times2$ \u2022 GPU: NVIDIA GeForce RTX 3090 24G $\\times8$ \u2022 MEM: 500G \u2022 Maximum total computing time: pretraining $^+$ testing $\\approx40h$ ", "page_idx": 35}, {"type": "text", "text": "L Broader Impacts ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "This paper presents work that aims to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. ", "page_idx": 35}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 36}, {"type": "text", "text": "Justification: The main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope. Please see Abstract and Sec. 1. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 36}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Justification: The paper discusses the limitations of the work performed by the authors.   \nPlease see Sec. 6. ", "page_idx": 36}, {"type": "text", "text": "Guidelines: ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 36}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 36}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: For each theoretical result, the paper provides the full set of assumptions and a complete (and correct) proof. Please see Sec. 4.3 and App. D. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 37}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 37}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 37}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 37}, {"type": "text", "text": "Justification: The paper fully discloses all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not). Please see Sec. 5.1 and App. H. ", "page_idx": 37}, {"type": "text", "text": "Guidelines: ", "page_idx": 37}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 37}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper provides open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results. Please see our project website in Abstract. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 38}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper specifies all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results. Please see Sec. 5.1 and App. H. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 38}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: The paper reports error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments. Please see Sec. 5. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. ", "page_idx": 38}, {"type": "text", "text": "\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 39}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: The paper provides sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments. Please see App. K. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 39}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 39}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: The paper discusses both potential positive societal impacts and negative societal impacts of the work performed. Please see App. L. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 40}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 40}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 40}, {"type": "text", "text": "Justification: Our research does not release data or models, so the paper has no such risks. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 40}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: The creators or original owners of assets (e.g., code, data, models), used in the paper, are properly credited and are the license and terms of use explicitly mentioned and properly respected. Please see our project website in Abstract. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: The paper does not release new assets. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 41}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 41}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "page_idx": 41}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 41}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 41}, {"type": "text", "text": "Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 42}]