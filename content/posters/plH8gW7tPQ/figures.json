[{"figure_path": "plH8gW7tPQ/figures/figures_1_1.jpg", "caption": "Figure 1: Overview of problem setup. A: Modeling approach. We initialize transformers randomly, then optimize only their input and output embedding layers on a dataset of interest. We find that these random transformers can be successfully trained to perform a diverse set of human-meaningful tasks. B: Task set. We evaluate the effectiveness of random transformers on a set of model problems involving arithmetic and memorization, as well as modeling of natural language text.", "description": "This figure provides a high-level overview of the experimental setup and the tasks used to evaluate the capabilities of randomly initialized transformers. Part A illustrates the modeling approach, where a randomly initialized transformer is fine-tuned by only training the embedding and unembedding layers while leaving the internal layers frozen. Part B outlines the seven tasks used to assess the performance of these models. These tasks encompass various domains, including arithmetic (modular arithmetic, decimal addition), memory-based tasks (needle-in-a-haystack, memorization), and sequence processing (parenthesis balancing, circuit imitation, language modeling).", "section": "Setup"}, {"figure_path": "plH8gW7tPQ/figures/figures_5_1.jpg", "caption": "Figure 2: Attention patterns observed in a 2-layer 1024-width random transformer trained on the needle-in-a-haystack task. The input sequence is a 1 b 2 c 3 d 4 b. The layer-1 head is used by values to attend to their markers, and the layer-2 head is used by the query to attend to its associated value.", "description": "This figure shows the attention patterns in a randomly initialized transformer trained on a \"needle-in-a-haystack\" task. The task involves finding a specific value associated with a marker in a sequence. The figure demonstrates that the model uses different attention heads for different parts of the task. Layer 1, Head 2, is used to identify the marker-value pairs. While, Layer 2, Head 2, helps the model retrieve the target value based on the query.", "section": "4 Random Transformers Can Perform Simple Algorithmic Tasks"}, {"figure_path": "plH8gW7tPQ/figures/figures_6_1.jpg", "caption": "Figure 4: Language modeling performances (measured in cross-entropy loss or equivalently log perplexity, lower is better) for fully trained and random transformers. Comparatively large hidden sizes are needed for random models to match the performance of fully trained models.", "description": "This figure shows the cross-entropy loss for language modeling achieved by both fully trained and randomly initialized transformers.  The x-axis represents the network width, while the y-axis represents the cross-entropy loss (lower is better).  Different lines represent different model architectures (2-layer vs. 4-layer, normal vs. random).  The results indicate that randomly initialized models require significantly larger network widths to achieve comparable performance to fully trained models.", "section": "5 Random Transformers Can Memorize and Generate Structured Sequences"}, {"figure_path": "plH8gW7tPQ/figures/figures_9_1.jpg", "caption": "Figure 6: Kullback-Leibler divergence of circuit imitation with fully trained and random transformers (the lower is better). Both plots show the same set of results with different scales (linear and log) on the vertical axis.", "description": "This figure shows the results of a circuit imitation experiment comparing fully trained and randomly initialized transformers.  The x-axis represents the width of the target transformer (the model being imitated), while the y-axis represents the Kullback-Leibler (KL) divergence between the target transformer's output distribution and the output distribution of the imitating transformer.  Lower KL divergence indicates better imitation. The figure includes two plots with different y-axis scales (linear and logarithmic) to better visualize the results across different target transformer widths. The results suggest that randomly initialized transformers struggle to effectively imitate wider target transformers.", "section": "6 Subspace Selection in Circuit Imitation"}, {"figure_path": "plH8gW7tPQ/figures/figures_16_1.jpg", "caption": "Figure 2: Attention patterns observed in a 2-layer 1024-width random transformer trained on the needle-in-a-haystack task. The input sequence is a 1 b 2 c 3 d 4 b. The layer-1 head is used by values to attend to their markers, and the layer-2 head is used by the query to attend to its associated value.", "description": "This figure shows the attention patterns in a 2-layer, 1024-width random transformer trained to solve the needle-in-a-haystack task. The input sequence is [a, 1, b, 2, c, 3, d, 4, b], where a, b, c, d are markers and 1, 2, 3, 4 are the associated values. The figure demonstrates how the attention mechanism works in two layers. Layer 1 shows that the values attend to their corresponding markers. Layer 2 displays the query attending to the correct value given its marker.", "section": "4 Random Transformers Can Perform Simple Algorithmic Tasks"}, {"figure_path": "plH8gW7tPQ/figures/figures_19_1.jpg", "caption": "Figure 8: During training, the accuracy curve from fully trained and random transformers in the memorization task (Section 5.1). Note that the evaluation set is exactly the training set as the goal is to memorize.", "description": "This figure shows two accuracy curves during the training of fully trained and random transformers on a memorization task.  The x-axis represents the training step, and the y-axis shows the accuracy.  The random transformer shows a slower but steady increase in accuracy, eventually reaching a similar level to the fully trained transformer. The evaluation set is identical to the training set in this specific experiment.", "section": "5 Random Transformers Can Memorize and Generate Structured Sequences"}, {"figure_path": "plH8gW7tPQ/figures/figures_19_2.jpg", "caption": "Figure 8: During training, the accuracy curve from fully trained and random transformers in the memorization task (Section 5.1). Note that the evaluation set is exactly the training set as the goal is to memorize.", "description": "This figure shows the accuracy curves for both fully trained and randomly initialized transformers during the memorization task.  The x-axis likely represents training steps or epochs, and the y-axis represents the accuracy achieved on the task. Since the evaluation set is identical to the training set in this memorization task, the goal is to perfectly memorize all input-output pairs in the training data.", "section": "5 Random Transformers Can Memorize and Generate Structured Sequences"}]