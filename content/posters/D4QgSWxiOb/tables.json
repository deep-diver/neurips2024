[{"figure_path": "D4QgSWxiOb/tables/tables_8_1.jpg", "caption": "Table 1: Results on the complex reasoning task. Direct/CoT: predict the answer directly/verbalize the reasoning steps. \"+R\": retrieval augmentation.", "description": "This table presents the results of a complex reasoning task, comparing the performance of GPT-4-Turbo, Gemini-Pro-1.5, and a Grokked Transformer model.  The task involves comparing entity attributes, requiring multi-hop reasoning and a large search space.  The table shows accuracy results for each model under different prompting styles ('Direct' and 'CoT' for chain-of-thought) and with or without retrieval augmentation (\"+R\"). The Grokked Transformer significantly outperforms the other models.", "section": "5 The Power of Parametric Memory for Complex Reasoning"}]