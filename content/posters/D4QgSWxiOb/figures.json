[{"figure_path": "D4QgSWxiOb/figures/figures_1_1.jpg", "caption": "Figure 1: We find that transformers can learn to reason implicitly, but this skill is only robustly acquired through grokking, i.e., an extended period of training far beyond overfitting. Moreover, the transformer fails to systematically generalize for composition, yet succeeds for comparison. We conduct a mechanistic study into the model internals throughout grokking, which reveals distinct generalizing circuits across the two tasks (Figure 4, 5) that explains the variations in systematicity.", "description": "This figure shows the accuracy of two reasoning tasks (composition and comparison) during the training process.  The x-axis represents the optimization steps (log scale), indicating the training progress. The y-axis shows the accuracy.  Different colored lines represent training accuracy, in-distribution (ID) test accuracy, and out-of-distribution (OOD) test accuracy. The results demonstrate that transformers can learn implicit reasoning, but only after extended training beyond overfitting (grokking).  Furthermore, generalization performance differs significantly between the two reasoning types; composition shows poor OOD generalization, while comparison shows good OOD generalization. This difference is further investigated later in the paper.", "section": "3 Composition\u2014Delayed Generalization without Systematicity"}, {"figure_path": "D4QgSWxiOb/figures/figures_3_1.jpg", "caption": "Figure 1: We find that transformers can learn to reason implicitly, but this skill is only robustly acquired through grokking, i.e., an extended period of training far beyond overfitting. Moreover, the transformer fails to systematically generalize for composition, yet succeeds for comparison. We conduct a mechanistic study into the model internals throughout grokking, which reveals distinct generalizing circuits across the two tasks (Figure 4, 5) that explains the variations in systematicity.", "description": "This figure shows the training curves for composition and comparison tasks.  The x-axis represents the optimization step (log scale), indicating the extent of training. The y-axis displays accuracy.  For both tasks, accuracy improves significantly after an extended training phase (grokking). However, the generalization to out-of-distribution (OOD) examples differs between the tasks.  Composition shows poor OOD generalization, while comparison exhibits successful OOD generalization, highlighting the different generalizing circuits developed for each reasoning type.", "section": "3 Composition\u2014Delayed Generalization without Systematicity"}, {"figure_path": "D4QgSWxiOb/figures/figures_4_1.jpg", "caption": "Figure 3: Illustration of our circuit analysis approach (on the composition task). We use logit lens to interpret individual states, and use causal tracing to measure the strength of connections between states. Details are in the main content.", "description": "This figure illustrates the methodology used for circuit analysis in the paper.  It shows the process of using logit lens and causal tracing to understand the model's internal workings, particularly during the composition task. The logit lens is used to interpret individual hidden states, while causal tracing measures the strength of connections between states.  The normal and perturbed runs are shown, along with an intervention step that helps quantify the causal relationships.", "section": "3.3 Analyzing the inner workings of the model throughout grokking"}, {"figure_path": "D4QgSWxiOb/figures/figures_5_1.jpg", "caption": "Figure 4: The (evolution of) generalizing circuit for composition. (a) The generalizing circuit. (b) The change in causal strengths during grokking, where the target is the prediction state. (c) Mean reciprocal rank (via logit lens) of the bridge entity b at S[5, r1] and second relation r2 at S[5, r2].", "description": "This figure demonstrates the generalizing circuit developed during the grokking process for the composition task.  (a) shows a simplified causal graph highlighting key states in the model's layers involved in the reasoning process.  (b) illustrates the increase in causal strength between states during grokking, specifically focusing on the connection between the intermediary state and the final prediction. (c) displays the mean reciprocal rank (MRR) of the bridge entity and the second relation across different training stages, showing how these features become more reliably predicted as the model progresses through grokking.", "section": "3.3 Analyzing the inner workings of the model throughout grokking"}, {"figure_path": "D4QgSWxiOb/figures/figures_7_1.jpg", "caption": "Figure 5: The (evolution of) generalizing circuit for comparison. (a) The generalizing circuit. (b) The change in causal strengths during grokking, where the target is the prediction state. (c) Mean reciprocal rank (via logit lens) of the two attribute values (v1, v2) at S[5, e1] and S[5, e2].", "description": "This figure presents a mechanistic analysis of the comparison task within a transformer model.  Panel (a) shows the identified 'generalizing circuit,' a network of interconnected neurons essential for successful comparison. Panel (b) illustrates changes in the strength of causal connections between neurons throughout the 'grokking' phase (extended training resulting in improved generalization).  Panel (c) displays the mean reciprocal rank (MRR) of attribute values (v1 and v2) within specific neurons, further clarifying the model's internal workings during this critical learning phase. The figure highlights the emergence of efficient parallel processing for the comparison task.", "section": "4 Comparison\u2014Systematic Generalization via Parallel Circuit"}, {"figure_path": "D4QgSWxiOb/figures/figures_8_1.jpg", "caption": "Figure 6: Illustration of the complex reasoning task, which involves comparing the attributes of two query entities based on a set of facts encompassing a large search space.", "description": "This figure illustrates a complex reasoning task where the goal is to compare the attributes of two query entities (in dark blue circles) using a large knowledge graph.  The knowledge graph shows various entities and their age relationships. To answer the query, the model needs to discover a path (indicated by blue arrows) connecting the two query entities via intermediary entities (in light beige circles) referred to as bridge entities. This path represents a chain of reasoning steps that lead to the final comparison.  The complexity arises from the large search space\u2014 numerous entities and their connections must be considered to identify the correct path which proves the final comparison.", "section": "5 The Power of Parametric Memory for Complex Reasoning"}, {"figure_path": "D4QgSWxiOb/figures/figures_16_1.jpg", "caption": "Figure 1: We find that transformers can learn to reason implicitly, but this skill is only robustly acquired through grokking, i.e., an extended period of training far beyond overfitting. Moreover, the transformer fails to systematically generalize for composition, yet succeeds for comparison. We conduct a mechanistic study into the model internals throughout grokking, which reveals distinct generalizing circuits across the two tasks (Figure 4, 5) that explains the variations in systematicity.", "description": "The figure shows the accuracy of the transformer model on in-distribution (ID) and out-of-distribution (OOD) test sets for composition and comparison tasks. The results reveal that implicit reasoning is only robustly acquired through extended training beyond overfitting (grokking).  For composition, the model struggles to generalize to OOD examples. However, for the comparison task, the model demonstrates successful generalization to OOD examples.  This difference in systematicity is explained by distinct generalizing circuits within the model identified later in the paper.", "section": "Composition\u2014Delayed Generalization without Systematicity"}, {"figure_path": "D4QgSWxiOb/figures/figures_17_1.jpg", "caption": "Figure 8: (a) ID generalization across different token multiplicity. (b) Probing accuracy on the second token of b at S[5, r1].", "description": "This figure presents the results of experiments conducted to investigate the impact of tokenization on the composition task.  Two versions of tokenization were used: one with a single token per entity and another with two tokens per entity (mimicking first and last names).  Subfigure (a) shows the accuracy of the model on in-distribution (ID) test data for various levels of token multiplicity.  A higher token multiplicity indicates that more entities share the same first or last name, effectively reducing the size of the model's vocabulary.  The results show that while a higher multiplicity delays the onset of generalization, it ultimately does not prevent generalization from occurring.  Subfigure (b) further investigates the internal state of the model, S[5, r1] which encodes the bridge entity b, using a probing task. This shows that the model is able to consistently decode the second token of b even with higher token multiplicity. This demonstrates the robustness of the model's learning to different forms of tokenization.", "section": "3.2 Results"}, {"figure_path": "D4QgSWxiOb/figures/figures_18_1.jpg", "caption": "Figure 4: The (evolution of) generalizing circuit for composition. (a) The generalizing circuit. (b) The change in causal strengths during grokking, where the target is the prediction state. (c) Mean reciprocal rank (via logit lens) of the bridge entity b at S[5, r1] and second relation r2 at S[5, r2].", "description": "This figure shows the generalizing circuit that emerges during the grokking phenomenon for the composition task.  Panel (a) illustrates the circuit's structure, highlighting key connections between different layers of the transformer model. Panel (b) illustrates the change in causal strength between states during the grokking process. Panel (c) displays the mean reciprocal rank (MRR) for the bridge entity (b) and the second relation (r2) at specific states (S[5, r1] and S[5, r2]) within the circuit, showcasing how these factors evolve over time.", "section": "3.3 Analyzing the inner workings of the model throughout grokking"}, {"figure_path": "D4QgSWxiOb/figures/figures_18_2.jpg", "caption": "Figure 4: The (evolution of) generalizing circuit for composition. (a) The generalizing circuit. (b) The change in causal strengths during grokking, where the target is the prediction state. (c) Mean reciprocal rank (via logit lens) of the bridge entity b at S[5, r1] and second relation r2 at S[5, r2].", "description": "This figure presents a mechanistic analysis of the composition task within the transformer model.  It illustrates the evolution of the \"generalizing circuit\"\u2014a pathway within the network responsible for generalization\u2014during the grokking phase. Panel (a) shows a schematic of this circuit, highlighting key intermediate states. Panel (b) tracks changes in the causal strength of connections within the circuit over training, revealing how these connections strengthen as generalization emerges. Panel (c) shows the mean reciprocal rank (MRR) of certain components within the circuit as predicted by the logit lens method, demonstrating the improving ability of the model to access and manipulate knowledge during the grokking process.", "section": "3.3 Analyzing the inner workings of the model throughout grokking"}, {"figure_path": "D4QgSWxiOb/figures/figures_19_1.jpg", "caption": "Figure 1: We find that transformers can learn to reason implicitly, but this skill is only robustly acquired through grokking, i.e., an extended period of training far beyond overfitting. Moreover, the transformer fails to systematically generalize for composition, yet succeeds for comparison. We conduct a mechanistic study into the model internals throughout grokking, which reveals distinct generalizing circuits across the two tasks (Figure 4, 5) that explains the variations in systematicity.", "description": "This figure shows the accuracy of transformers on composition and comparison tasks during training.  The x-axis represents training steps (on a log scale), and the y-axis shows accuracy.  The key finding is that generalization (on held-out data) only emerges after a prolonged training phase (grokking).  Furthermore, generalization is better for comparison tasks than composition tasks.", "section": "3 Composition\u2014Delayed Generalization without Systematicity"}, {"figure_path": "D4QgSWxiOb/figures/figures_19_2.jpg", "caption": "Figure 1: We find that transformers can learn to reason implicitly, but this skill is only robustly acquired through grokking, i.e., an extended period of training far beyond overfitting. Moreover, the transformer fails to systematically generalize for composition, yet succeeds for comparison. We conduct a mechanistic study into the model internals throughout grokking, which reveals distinct generalizing circuits across the two tasks (Figure 4, 5) that explains the variations in systematicity.", "description": "This figure shows the accuracy on in-distribution (ID) and out-of-distribution (OOD) test sets for composition and comparison reasoning tasks over the optimization steps (log scale).  The plots illustrate that both composition and comparison tasks exhibit grokking behavior (significant improvement in accuracy far beyond the point of overfitting). However, the generalization capabilities vary between tasks: while the model successfully generalizes to OOD examples for comparison, it fails to do so for composition. This difference highlights the varying levels of systematicity and suggests different underlying mechanisms for these reasoning types.", "section": "3 Composition\u2014Delayed Generalization without Systematicity"}, {"figure_path": "D4QgSWxiOb/figures/figures_20_1.jpg", "caption": "Figure 13: Effect of weight decay. A larger weight decay can improve the speed of grokking, and vice versa.", "description": "This figure shows the impact of varying the weight decay hyperparameter on the speed of the grokking phenomenon.  The x-axis represents the optimization steps (in units of 1e5), and the y-axis represents the accuracy achieved on a task. Three lines are plotted, each corresponding to a different weight decay value (0.03, 0.1, and 0.3).  The results indicate that a higher weight decay accelerates the grokking process, leading to faster convergence towards high accuracy.", "section": "E Additional Results"}, {"figure_path": "D4QgSWxiOb/figures/figures_20_2.jpg", "caption": "Figure 1: We find that transformers can learn to reason implicitly, but this skill is only robustly acquired through grokking, i.e., an extended period of training far beyond overfitting. Moreover, the transformer fails to systematically generalize for composition, yet succeeds for comparison. We conduct a mechanistic study into the model internals throughout grokking, which reveals distinct generalizing circuits across the two tasks (Figure 4, 5) that explains the variations in systematicity.", "description": "This figure shows the accuracy of a transformer model on in-distribution (ID) and out-of-distribution (OOD) test sets for composition and comparison reasoning tasks over the course of training.  The key finding is that the model only achieves robust generalization after an extended period of training (grokking), and this generalization varies depending on reasoning type. For composition, the model struggles to generalize to OOD examples, while it successfully generalizes for comparison tasks.", "section": "3 Composition\u2014Delayed Generalization without Systematicity"}, {"figure_path": "D4QgSWxiOb/figures/figures_21_1.jpg", "caption": "Figure 1: We find that transformers can learn to reason implicitly, but this skill is only robustly acquired through grokking, i.e., an extended period of training far beyond overfitting. Moreover, the transformer fails to systematically generalize for composition, yet succeeds for comparison. We conduct a mechanistic study into the model internals throughout grokking, which reveals distinct generalizing circuits across the two tasks (Figure 4, 5) that explains the variations in systematicity.", "description": "This figure shows the accuracy of two reasoning tasks (composition and comparison) during the training process.  The x-axis represents the optimization steps (log scale), and the y-axis represents the accuracy.  It demonstrates that  implicit reasoning is only robustly acquired through \"grokking\", meaning extensive training beyond the point of overfitting. The figure also highlights that generalization performance varies across reasoning types, with successful generalization for comparison but not for composition.", "section": "3 Composition\u2014Delayed Generalization without Systematicity"}, {"figure_path": "D4QgSWxiOb/figures/figures_21_2.jpg", "caption": "Figure 1: We find that transformers can learn to reason implicitly, but this skill is only robustly acquired through grokking, i.e., an extended period of training far beyond overfitting. Moreover, the transformer fails to systematically generalize for composition, yet succeeds for comparison. We conduct a mechanistic study into the model internals throughout grokking, which reveals distinct generalizing circuits across the two tasks (Figure 4, 5) that explains the variations in systematicity.", "description": "This figure shows the training curves for composition and comparison tasks.  The x-axis represents the optimization steps (on a logarithmic scale), and the y-axis represents the accuracy.  The curves show that for both tasks, the model's accuracy on in-distribution (ID) data increases significantly beyond the overfitting point (the grokking phenomenon). However, while the model generalizes well to unseen ID data for both tasks, it only systematically generalizes to out-of-distribution (OOD) data for the comparison task, not the composition task.  This difference is further investigated and explained in later figures (4 and 5) using mechanistic analysis of the model's internal workings.", "section": "3 Composition\u2014Delayed Generalization without Systematicity"}]