[{"figure_path": "UPxmISfNCO/figures/figures_0_1.jpg", "caption": "Figure 1: Framework and Intuition of RELA: (1) Framework: RELA serves as both a data optimizer and an auxiliary accelerator. Initially, it operates as a data optimizer by leveraging an dataset and a pre-trained model (e.g., one sourced from online repositories) to generate an efficient dataset. Subsequently, RELA functions as an auxiliary accelerator, enhancing existing (self-)supervised learning algorithms through the effective utilization of the efficient dataset, thereby promoting efficient representation learning. (2) Intuition: The central concept of RELA is to create an efficient-data-driven shortcut pathway within the learning process, enabling the initial model $ to rapidly converge towards a 'proximal representation ' of the target model $* during the early stages of training. This approach significantly accelerates the overall learning process.", "description": "This figure illustrates the framework and intuition behind the RELA method.  The framework shows RELA working in two stages: first optimizing data using a pre-trained model and a dataset, and then accelerating learning algorithms using this optimized data. The intuition section demonstrates how RELA creates a shortcut in the learning process by using efficient data to enable faster convergence towards the optimal representation.", "section": "1 Introduction"}, {"figure_path": "UPxmISfNCO/figures/figures_5_1.jpg", "caption": "Figure 2: Investigating modified samples with varied \u2211 values. Following [39], Figure 2a visualizes the validation loss landscape within a two-dimensional parameter space, along with three training trajectories corresponding to different \u2211 settings. Figure 2b illustrates the performance of models trained using samples with varied \u2211. The optimal case in our task, utilizing samples with \u2211 = 0.1 (which achieves the lowest validation loss in Figure 2b), is visualized in Figure 2c, where the color bar represents the values of targets y.", "description": "This figure shows the impact of modifying the variance (\u2211) of the data samples on the model training process.  (a) shows the loss landscape, illustrating how different variance levels affect the optimization path. (b) presents the validation loss for models trained with different variances, showing faster convergence for smaller variances. (c) visualizes the optimal data distribution (\u2211 = 0.1) which leads to the lowest validation loss and fastest convergence.", "section": "3.2 Empirical and Theoretical Investigation of Data-Centric Efficient Learning"}, {"figure_path": "UPxmISfNCO/figures/figures_5_2.jpg", "caption": "Figure 2: Investigating modified samples with varied \u2211 values. Following [39], Figure 2a visualizes the validation loss landscape within a two-dimensional parameter space, along with three training trajectories corresponding to different \u2211 settings. Figure 2b illustrates the performance of models trained using samples with varied \u03a3. The optimal case in our task, utilizing samples with \u2211 = 0.1 (which achieves the lowest validation loss in Figure 2b), is visualized in Figure 2c, where the color bar represents the values of targets y.", "description": "This figure shows an empirical and theoretical investigation of data-centric efficient learning. It demonstrates the effect of modifying sample variance (\u03a3) and re-labeling intensity (p) on the convergence rate and performance of a binary classification task using a bimodal Gaussian mixture distribution.  The results indicate that smaller variance and higher re-labeling intensity lead to faster convergence and better performance.", "section": "3.2 Empirical and Theoretical Investigation of Data-Centric Efficient Learning"}, {"figure_path": "UPxmISfNCO/figures/figures_16_1.jpg", "caption": "Figure 2: Investigating modified samples with varied \u2211 values. Following [39], Figure 2a visualizes the validation loss landscape within a two-dimensional parameter space, along with three training trajectories corresponding to different \u2211 settings. Figure 2b illustrates the performance of models trained using samples with varied \u2211. The optimal case in our task, utilizing samples with \u2211 = 0.1 (which achieves the lowest validation loss in Figure 2b), is visualized in Figure 2c, where the color bar represents the values of targets y.", "description": "This figure shows an empirical and theoretical investigation of data-centric efficient learning. It contains three subfigures: (a) shows the loss landscape for different sigma values, (b) evaluates the performance with different sigma values, and (c) shows the optimal case with sigma=0.1.  The study demonstrates that modifying the variance of the sample distribution influences the convergence rate of the optimization process, impacting overall representation learning.", "section": "3.2 Empirical and Theoretical Investigation of Data-Centric Efficient Learning"}, {"figure_path": "UPxmISfNCO/figures/figures_31_1.jpg", "caption": "Figure 2: Investigating modified samples with varied \u2211 values. Following [39], Figure 2a visualizes the validation loss landscape within a two-dimensional parameter space, along with three training trajectories corresponding to different \u2211 settings. Figure 2b illustrates the performance of models trained using samples with varied \u2211. The optimal case in our task, utilizing samples with \u2211 = 0.1 (which achieves the lowest validation loss in Figure 2b), is visualized in Figure 2c, where the color bar represents the values of targets y.", "description": "This figure presents an empirical and theoretical analysis of how modifying sample variance affects the performance of a binary classification model trained on a bimodal Gaussian mixture distribution.  The results show that decreasing the variance of the data leads to faster convergence and better performance.  The figure also demonstrates this effect through loss landscapes, training curves across different variances, and a visualization of optimal data properties. The analysis suggests that modifying sample variance and re-labeling data points can accelerate the training process.", "section": "3.2 Empirical and Theoretical Investigation of Data-Centric Efficient Learning"}, {"figure_path": "UPxmISfNCO/figures/figures_35_1.jpg", "caption": "Figure 1: Framework and Intuition of RELA: (1) Framework: RELA serves as both a data optimizer and an auxiliary accelerator. Initially, it operates as a data optimizer by leveraging an dataset and a pre-trained model (e.g., one sourced from online repositories) to generate an efficient dataset. Subsequently, RELA functions as an auxiliary accelerator, enhancing existing (self-)supervised learning algorithms through the effective utilization of the efficient dataset, thereby promoting efficient representation learning. (2) Intuition: The central concept of RELA is to create an efficient-data-driven shortcut pathway within the learning process, enabling the initial model $ to rapidly converge towards a 'proximal representation ' of the target model $* during the early stages of training. This approach significantly accelerates the overall learning process.", "description": "The figure illustrates the framework and intuition of the Representation Learning Accelerator (RELA).  RELA is a two-stage process. First, it uses a pre-trained model and existing data to generate an efficient dataset. Second, it integrates this data into an existing self-supervised learning algorithm to accelerate the learning process. The core idea is that by producing a more efficient dataset, the initial model quickly approaches the target model's representation during training.  The figure shows a schematic of this two-stage process, highlighting the shortcut pathway created by RELA in the representation space.", "section": "1 Introduction"}, {"figure_path": "UPxmISfNCO/figures/figures_37_1.jpg", "caption": "Figure 2: Investigating modified samples with varied \u2211 values. Following [39], Figure 2a visualizes the validation loss landscape within a two-dimensional parameter space, along with three training trajectories corresponding to different \u2211 settings. Figure 2b illustrates the performance of models trained using samples with varied \u2211. The optimal case in our task, utilizing samples with \u2211 = 0.1 (which achieves the lowest validation loss in Figure 2b), is visualized in Figure 2c, where the color bar represents the values of targets y.", "description": "This figure shows an empirical and theoretical investigation of data-centric efficient learning using a bimodal Gaussian mixture distribution.  It visualizes the validation loss landscape for different variance settings (\u2211), showing how smaller variances lead to faster convergence and better performance.  It also shows the impact of modified targets (p) on the convergence rate of model training.", "section": "3.2 Empirical and Theoretical Investigation of Data-Centric Efficient Learning"}]