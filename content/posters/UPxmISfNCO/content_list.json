[{"type": "text", "text": "Efficiency for Free: Ideal Data Are Transportable Representations ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Peng Sun1,2 Yi Jiang1 Tao Lin2,\u2217 ", "page_idx": 0}, {"type": "text", "text": "Tao Lin 1Zhejiang University 2Westlake University sunpeng@westlake.edu.cn, yi_jiang@zju.edu.cn, lintao@westlake.edu.cn ", "page_idx": 0}, {"type": "text", "text": "Abstract ", "text_level": 1, "page_idx": 0}, {"type": "text", "text": "Data, the seminal opportunity and challenge in modern machine learning, currently constrains the scalability of representation learning and impedes the pace of model evolution. In this work, we investigate the efficiency properties of data from both optimization and generalization perspectives. Our theoretical and empirical analysis reveals an unexpected finding: for a given task, utilizing a publicly available, taskand architecture-agnostic model (referred to as the \u2018prior model\u2019 in this paper) can effectively produce efficient data. Building on this insight, we propose the Representation Learning Accelerator (RELA), which promotes the formation and utilization of efficient data, thereby accelerating representation learning. Utilizing a ResNet-18 pre-trained on CIFAR-10 as a prior model to inform ResNet-50 training on ImageNet-1K reduces computational costs by $50\\%$ while maintaining the same accuracy as the model trained with the original BYOL, which requires $\\bar{1}00\\%$ cost. Our code is available at: https://github.com/LINs-lab/ReLA. ", "page_idx": 0}, {"type": "text", "text": "1 Introduction ", "text_level": 1, "page_idx": 0}, {"type": "image", "img_path": "UPxmISfNCO/tmp/969b327b9fe720b9c6809c87508bf10f3aa9ca7a7db58bfb68518c4a1182a7e8.jpg", "img_caption": ["Figure 1: Framework and Intuition of RELA: (1) Framework: RELA serves as both a data optimizer and an auxiliary accelerator. Initially, it operates as a data optimizer by leveraging an dataset and a pre-trained model (e.g., one sourced from online repositories) to generate an efficient dataset. Subsequently, RELA functions as an auxiliary accelerator, enhancing existing (self-)supervised learning algorithms through the effective utilization of the efficient dataset, thereby promoting efficient representation learning. (2) Intuition: The central concept of RELA is to create an efficient-data-driven shortcut pathway within the learning process, enabling the initial model $\\phi$ to rapidly converge towards a \u2018proximal representation $\\psi^{\\,\\cdot}$ of the target model $\\phi^{\\star}$ during the early stages of training. This approach significantly accelerates the overall learning process. "], "img_footnote": [], "page_idx": 0}, {"type": "text", "text": "The available of massive datasets [20, 49] and recent advances in parallel data processing [28, 42] have facilitated the rapid evolution of large deep models, such as GPT-4 [1] and LVM [2]. These models excel in numerous learning tasks, attributable to their impressive representation capabilities. However, the emergence of vast amounts of data within the modern deep learning paradigm raises two fundamental challenges: (i) the demand for human annotations of huge datasets consumes significant social resources [45, 20, 43]; (ii) training large models with increasing data and model capacity suffers from intensive computational burden [6, 16, 55]. ", "page_idx": 0}, {"type": "text", "text": "", "page_idx": 1}, {"type": "text", "text": "The community has made considerable efforts to enhance learning efficiency. Self-supervised learning methods [12, 69, 30, 10, 25, 14, 9, 3], with their superior representation learning devoid of human annotations via the self-learning paradigm, attempt to tackle the challenge (i). Concurrently, research has been conducted to mitigate data efficiency issues in challenge (ii): dataset distillation approaches [65, 56, 11, 72, 53, 54] have successfully synthesized a small distilled dataset, on which models trained on this compact dataset can akin to one trained on the full dataset. ", "page_idx": 1}, {"type": "text", "text": "However, challenges (i) and (ii) persist and yet are far from being solved [43, 45, 6, 16, 55], particularly the intervention of these two learning paradigms. In this paper, we identify two issues: (a) inefficiency in the self-supervised learning procedure compared to conventional supervised learning arises due to sub-optimal self-generating targets [30, 62]; (b) although training on the distilled dataset is efficient and effective, the distillation process of optimization-based approaches [11, 72, 37] is computationally demanding [18, 56], often surpassing the computational load of training on the full dataset. This limitation restricts its potential to accelerate representation learning. To tackle these challenges, we propose a novel open problem in the domain of representation learning: ", "page_idx": 1}, {"type": "text", "text": "Problem 1 (Accelerating Representation Learning through Free Models) . According to the No Free Lunch theorem [66], it is evident that accelerating the learning process without incorporating prior knowledge is inherently challenging. Fortunately, numerous publicly available pre-trained models can be accessed online, offering a form of free prior knowledge. Despite this, effectively utilizing these models poses several implicit challenges, as these models may not be directly relevant to our target learning task, or they may not be sufficiently well-trained. This leads to a question: ", "page_idx": 1}, {"type": "text", "text": "How can we leverage task- and architecture-agnostic publicly available models to accelerate representation learning for a specific task? ", "page_idx": 1}, {"type": "text", "text": "To address Problem 1 , we propose RELA to utilize a freely available model downloaded from the internet to generate efficient data for training. This approach aims to accelerate training during the initial stages by effectively leveraging these generated data, thereby establishing a rapid pathway for representation learning (see Figure 1 ). Specifically, we list our five key contributions below as the first step toward bridging representation learning with data-efficient learning: ", "page_idx": 1}, {"type": "text", "text": "(a) Revealing beneficial/detrimental data properties for efficient/inefficient (self-)supervised learning (see Section 3.2 ). We present a comprehensive analysis of linear models, demonstrating that data properties significantly influence the learning process by impacting the optimization of model training. Our findings reveal that modifications to the data can markedly enhance or impair this optimization. Additionally, we indicate that optimal training necessitates specific data properties\u2014perfect bijective mappings between the samples and targets within a dataset. ", "page_idx": 1}, {"type": "text", "text": "(b) Identifying the inefficiency problems of (self-)supervised learning from a data-centric perspective (see Section 3.3 ). Specifically, we identify several factors contributing to the inefficiencies in (self)supervised learning over real-world data. For instance, prevalent data augmentation techniques in modern deep learning can introduce a \u2018noisy mapping\u2019 issue, which may exacerbate the negative effects associated with inefficient data. ", "page_idx": 1}, {"type": "text", "text": "(c) Generalization bound for models trained on optimized efficient data (see Section 3.4 ). Although the efficiency properties of data do not inherently ensure the generalization of the trained model, i.e., efficient data alone cannot guarantee generalization ability, we present a generalization bound to analyze models trained on such data. ", "page_idx": 1}, {"type": "text", "text": "(d) A novel method RELA to generate and exploit efficient data (see Section 4 ). Leveraging our theoretical insights regarding the bounds of generalization and convergence rate, we introduce RELA, a novel optimization-free method tailored to efficiently generate and effectively exploit efficient data for accelerating representation learning. ", "page_idx": 1}, {"type": "text", "text": "(e) An application of our RELA: accelerating (self-)supervised learning (see Section 5 and Appendix I ). Extensive experiments across four widely-used datasets, seven neural network architectures, eight self-supervised learning algorithms demonstrate the effectiveness and efficiency of RELA. Training models with RELA significantly outperforms training on the original dataset with the same budget, and even exceeds the performance of training on higher budget. ", "page_idx": 1}, {"type": "text", "text": "", "page_idx": 2}, {"type": "text", "text": "2 Related Work ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "This section integrates two distinct deep learning areas: (a) techniques to condense datasets while preserving efficacy; (b) self-supervised learning methods that enable training models on unlabeled data. ", "page_idx": 2}, {"type": "text", "text": "2.1 Dataset Distillation: Efficient yet Effective Learning Using Fewer Data ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The objective of dataset distillation is to create a significantly smaller dataset that retains competitive performance relative to the original dataset. ", "page_idx": 2}, {"type": "text", "text": "Refining proxy metrics between original and distilled datasets. Traditional approaches involve replicating the behaviors of the original dataset within the distilled one. These methods aim to minimize discrepancies between surrogate neural network models trained on both synthetic and original datasets. Key metrics for this process include matching gradients [72, 32, 70, 44], features [63], distributions [71, 73], and training trajectories [11, 17, 22, 18, 68, 24]. However, these methods suffer from substantial computational overhead due to the incessant calculation of discrepancies between the distilled and original datasets. The optimization of the distilled dataset involves minimizing these discrepancies, necessitating multiple iterations until convergence. As a result, scaling to large datasets, such as ImageNet [20], becomes challenging. ", "page_idx": 2}, {"type": "text", "text": "Extracting key information from original into distilled datasets. A promising strategy involves identifying metrics that capture essential dataset information. These methods efficiently scale to large datasets like ImageNet-1K using robust backbones without necessitating multiple comparisons between original and distilled datasets. For instance, $\\mathrm{SRe^{2}L}$ [67] condenses the entire dataset into a model, such as pre-trained neural networks like ResNet-18 [26], and then extracts the knowledge from these models into images and targets, forming a distilled dataset. Recently, RDED [56] posits that images accurately recognized by strong observers, such as humans and pre-trained models, are more critical for learning. ", "page_idx": 2}, {"type": "text", "text": "Summary. We make the following observations regarding scalable dataset distillation methods utilizing various metrics: (a) a few of these metrics have proven effective for data distillation at the scale of ImageNet. (b) all these metrics require human-labeled data; (c) there is currently no established theory elucidating the conditions under which data distillation is feasible; (d) despite their success, the theory behind training neural networks with reduced data is underexplored. ", "page_idx": 2}, {"type": "text", "text": "2.2 Self-supervised Learning: Representation Learning Using Unlabeled Data ", "text_level": 1, "page_idx": 2}, {"type": "text", "text": "The primary objective of self-supervised learning is to extract robust representations without relying on human-labeled data. These representations should be competitive with those derived from supervised learning and deliver superior performance across multiple tasks. ", "page_idx": 2}, {"type": "text", "text": "Contrasting self-generated positive and negative Samples. Contrastive learning-based methods implicitly assign a one-hot label to each sample and its augmented versions to facilitate discrimination. Since InfoNCE [47], various works [25, 12, 13, 8, 31, 15, 74, 40, 9, 27] have advanced contrastive learning. MoCo [25, 13, 15] uses a momentum encoder for consistent negatives, effective for both CNNs and Vision Transformers. SimCLR [12] employs strong augmentations and a nonlinear projection head. Other methods integrate instance classification [8], data augmentation [31, 74], clustering [40, 9], and adversarial training [27]. These enhance alignment and uniformity of representations on the hypersphere [64]. ", "page_idx": 2}, {"type": "text", "text": "Asymmetric model-generating representations as targets. Asymmetric network methods achieve self-supervised learning with only positive pairs [30, 50, 14], avoiding representational collapse through asymmetric architectures. BYOL [30] uses a predictor network and a momentum encoder. Richemond et al. [50] show BYOL performs well without batch statistics. SimSiam [14] halts the gradient to the target branch, mimicking the momentum encoder\u2019s effect. DINO [10] employs a self-distillation loss. UniGrad [59] integrates asymmetric networks with contrastive learning methods within a theoretically unified framework. ", "page_idx": 2}, {"type": "text", "text": "3 Revealing Critical Properties of Efficient Learning over Data ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "We begin by presenting formal definitions of supervised learning over a (efficient) dataset. ", "page_idx": 3}, {"type": "text", "text": "Definition 1 (Supervised learning over data) . For a dataset $D=(D_{X},D_{Y})=\\{(\\mathbf{x}_{i},\\mathbf{y}_{i})\\}_{i=1}^{|D|}$ , drawn from the data distribution $(X,Y)$ in space $(\\mathcal{X},\\mathcal{Y})$ , the goal of a model learning algorithm is to identify an optimal model $\\phi^{\\star}$ that minimizes the expected error defined by: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}_{(\\mathbf{x},\\mathbf{y})\\sim(X,Y)}\\left[\\ell(\\phi^{\\star}(\\mathbf{x}),\\mathbf{y})\\right]\\le\\epsilon\\,,}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where \u2113indicates the loss function and $\\epsilon$ denotes a predetermined deviation. This is typically achieved through a parameterized model $\\phi_{\\theta}$ , where $\\pmb{\\theta}$ denotes the model parameter within the parameter space $\\Theta$ . The optimal parameter $\\theta^{D}$ is determined by training the model to minimize the empirical loss over the dataset: ", "page_idx": 3}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\theta^{D}:=\\arg\\operatorname*{min}_{\\theta\\in\\Theta}\\left\\{\\mathcal{L}(\\phi_{\\theta};D;\\ell)\\right\\}:=\\arg\\operatorname*{min}_{\\theta\\in\\Theta}\\left\\{\\sum_{i=1}^{|D|}\\ell(\\phi_{\\theta}(\\mathbf{x}_{i}),\\mathbf{y}_{i})\\right\\}\\,.}\\end{array}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "The training process leverages an optimization algorithm such as stochastic gradient descent $[5I$ , 33]. ", "page_idx": 3}, {"type": "text", "text": "Definition 2 (Data-efficient Learning) . Data-efficient learning seeks to derive an optimized/efficient dataset, denoted as ${\\cal S}=(S_{X},S_{Y})\\,=\\,\\{({\\bf x}_{j},{\\bf y}_{j})\\}_{j=1}^{|S|}$ , from the original dataset $D$ . The objective is to enable models $\\phi_{\\theta^{S}}$ trained on $S$ to achieve the desired generalization performance, as defined in (1), with fewer training steps and a reduced computational budget compared to training on the original dataset $D$ . ", "page_idx": 3}, {"type": "text", "text": "3.1 Unifying (Self-)Supervised Learning from a Data-Centric Perspective ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To ease the understanding and our methodology design in Section 3.4 , we unify both conventional supervised learning and self-supervised learning as learning to map samples in $D_{X}$ to targets in $D_{Y}$ : this view forms \u2018supervised learning\u2019 from a data-centric perspective. Specifically, these two learning paradigms involve generating targets $D_{Y}=\\{\\mathbf{y}\\mid\\mathbf{y}={\\bar{\\psi}}(\\mathbf{x})$ s.t. $\\mathbf{x}\\sim D_{X}\\}$ and minimizing the empirical loss (2). The only difference lies in the target generation models (or simply labelers) $\\psi$ ", "page_idx": 3}, {"type": "text", "text": "(a) Conventional supervised learning, referred to as human-supervised learning, generates targets via human annotation. Note that the targets are stored and used statically throughout the training.   \n(b) Self-supervised learning (also see Footnote 2), e.g., BYOL [30] utilizes an Exponential Moving Average (EMA) version of the learning model $\\phi_{\\theta}$ to generate targets $\\mathbf{y}=\\bar{\\mathrm{EMA}}[\\phi_{\\theta}](\\mathbf{x})$ . Note that the targets are dynamically changing during training as the model $\\phi_{\\theta}$ keeps evolving. ", "page_idx": 3}, {"type": "text", "text": "This unified perspective allows us to jointly examine and address the inefficient training issue of (self-)supervised learning from a data-centric perspective, in which in Section 3.2 we first study the impact of samples $D_{X}$ and targets $D_{Y}$ on the model training process and then investigate whether and how a distilled dataset $\\boldsymbol{S}=\\left(\\boldsymbol{S}_{\\boldsymbol{X}},\\boldsymbol{S}_{\\boldsymbol{Y}}\\right)$ can facilitate this process. ", "page_idx": 3}, {"type": "text", "text": "3.2 Empirical and Theoretical Investigation of Data-Centric Efficient Learning ", "text_level": 1, "page_idx": 3}, {"type": "text", "text": "To elucidate the ideal data properties of training on a dataset $D$ , we examine the simple task over a bimodal Gaussian mixture distribution as a case study. We begin by defining the problem. ", "page_idx": 3}, {"type": "text", "text": "Definition 3 (Bimodal Gaussian mixture distribution) . Given two Gaussian distributions $\\mathcal{N}_{0}(\\mu_{1},\\Sigma^{2}\\mathbf{I})$ and $\\mathcal{N}_{1}(\\mu_{2},\\Sigma^{2}\\mathbf{I})$ , where $\\mu_{1}$ and $\\mu_{2}$ are the means and $\\Sigma^{2}$ is the variance (here we set $\\mu_{1}=1$ , $\\mu_{2}=2$ and $\\Sigma=0.5,$ ). We define a bimodal mixture data $G=\\left(G_{X},G_{Y}\\right)$ as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nG:=\\{(\\mathbf{x},y)\\mid\\mathbf{x}=(1-y)\\cdot\\mathbf{x}_{0}+y\\cdot\\mathbf{x}_{1}\\}\\ \\mathrm{~s.t.~}\\ y\\sim\\mathrm{Bernoulli}(p=0.5),\\mathbf{x}_{0}\\sim\\mathcal{N}_{0},\\mathbf{x}_{1}\\sim\\mathcal{N}_{1}\\,.\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "Moreover, we define a corresponding binary classification neural network model as: ", "page_idx": 3}, {"type": "equation", "text": "$$\nf_{\\pmb\\theta}(\\mathbf{x}):=\\sigma\\left(\\pmb\\theta^{[1]}\\cdot\\mathrm{ReLU}\\left(\\pmb\\theta^{[2]}\\mathbf{x}+\\pmb\\theta^{[3]}\\right)+\\pmb\\theta^{[4]}\\right)\\mathrm{~,~}\n$$", "text_format": "latex", "page_idx": 3}, {"type": "text", "text": "where $\\begin{array}{r}{\\sigma(z)=\\frac{1}{1+e^{-z}}}\\end{array}$ is the sigmoid activation function; $\\mathrm{ReLU}(z)=\\operatorname*{max}(0,z)$ is the activation function for the hidden layer, which provides non-linearity to the model; $\\pmb{\\theta}^{[2]}$ and $\\pmb{\\theta}^{[3]}$ are the weights and biases of the hidden layer; $\\pmb{\\theta}^{[1]}$ and $\\pmb{\\theta}^{[4]}$ are the weights and biases of the output layer. ", "page_idx": 4}, {"type": "text", "text": "Modern representation learning fundamentally relies on optimization (see Definition 1 ). We show that modifications to the data can influence the convergence rate of the optimization process, thereby impacting the overall representation learning procedure. Furthermore, we try to uncover several key properties of data efficiency through our theoretical analysis of the case study. In the following, we denote the modified distribution by $G^{\\prime}=(G_{X}^{\\prime},G_{Y}^{\\prime})$ and examine the altered samples $G_{X}^{\\prime}$ and corresponding targets $G_{Y}^{\\prime}$ independently. ", "page_idx": 4}, {"type": "text", "text": "Investigating the properties of modified samples. The modification process here only rescales the variance of the original sample distribution $G_{X}$ defined in Definition 3 with new $\\Sigma$ (rather than the default 0.5), while let $G_{Y}^{\\prime}:=G_{Y}$ ; see explanations in Appendix E . Therefore, we examine the distilled samples $G_{X}^{\\prime}$ by setting the variable $\\Sigma$ within the interval $(0,1)$ . ", "page_idx": 4}, {"type": "text", "text": "Results in Figure 2 demonstrate that the distilled samples $G_{X}^{\\prime}$ with smaller variance $\\Sigma$ achieve faster convergence and better performance compared to that of $G$ . To elucidate the underlying mechanism, we provide a rigorous theoretical analysis in Appendix $\\mathbf{B}$ , culminating in Theorem 1 . ", "page_idx": 4}, {"type": "text", "text": "Theorem 1 (Convergence rate of learning on efficient samples) . For the classification task stated in Definition 3 , the convergence rate for the model $f_{\\theta}$ trained $t$ after steps over distilled data $G^{\\prime}$ is: ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta_{t}}\\left[\\mathcal{L}(f_{\\theta_{t}};G^{\\prime};\\ell)-\\mathcal{L}(f_{\\theta^{\\star}};G^{\\prime};\\ell)\\right]\\le\\tilde{\\mathcal{O}}(\\Sigma^{2})\\,,\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "where $\\ell$ denotes the MSE loss, i.e., $\\ell(\\hat{y},y):=\\|\\hat{y}-y\\|^{2}$ , and $f_{\\pmb\\theta^{\\star}}$ indicates the optimal model, $\\tilde{\\mathcal{O}}$ signifies the asymptotic complexity. Modified samples characterized by a smaller value of $\\Sigma$ facilitate faster convergence. ", "page_idx": 4}, {"type": "text", "text": "Investigating the properties of modified targets. On top of the property understanding for modified samples, we further investigate the potential of modified targets via $G^{\\prime}$ . In detail, for modified samples, we consider the most challenging (c.f. Figure 2b ) yet the common case, namely $G_{X}^{\\prime}$ with $\\Sigma=1$ (see explanations in Appendix M ). For the corresponding modified targets $G_{Y}^{\\prime}$ , similar to the prior data-efficient methods [56, 67], for any sample $\\mathbf{x}$ drawn from $G_{X}^{\\prime}$ , we refine its label by assigning $\\hat{y}=\\rho\\cdot f_{\\theta^{\\star}}({\\bf x})+(1-\\rho)\\cdot y$ . Here, $\\rho$ denotes the relabeling intensity coefficient, and $f_{\\pmb{\\theta}^{\\star}}$ represents a strong pre-trained model (simply, we utilize the model trained on the data in Figure 2c ). ", "page_idx": 4}, {"type": "text", "text": "Theorem 2 (Convergence rate of learning on re-labeled data) . For the classification task as in Definition $3$ , we have the convergence rate for the model f\u03b8 trained after $t$ steps over modified data $G^{\\prime}$ : ", "page_idx": 4}, {"type": "equation", "text": "$$\n\\mathbb{E}_{\\theta_{t}}\\left[\\mathcal{L}(f_{\\theta_{t}};G^{\\prime};\\ell)-\\mathcal{L}(f_{\\theta^{\\star}};G^{\\prime};\\ell)\\right]\\le\\tilde{\\mathcal{O}}(1-\\rho)\\,.\n$$", "text_format": "latex", "page_idx": 4}, {"type": "text", "text": "Note that $\\rho$ controls the upper bound of the convergence rate, indicating that using modified targets with a higher value of $\\rho$ enables faster convergence. ", "page_idx": 4}, {"type": "text", "text": "Results in Figure 2 illustrate that the modified targets $G_{Y}^{\\prime}$ with higher values of $\\rho$ lead to faster training convergence and better performance. See theoretical analysis in Theorem 2 and Appendix B . ", "page_idx": 4}, {"type": "text", "text": "3.3 Extended Understanding of Data-Centric Efficient Learning ", "text_level": 1, "page_idx": 4}, {"type": "text", "text": "The empirical and theoretical investigations regarding the properties of modified samples $G_{X}^{\\prime}$ and targets $G_{Y}^{\\prime}$ in Section 3.2 are limited to a simplified case (as described in Definition 3 ) and may not extend to all practical scenarios, such as training a ResNet [26] on the ImageNet dataset [20]. ", "page_idx": 4}, {"type": "text", "text": "Interestingly, we observe that the advantageous modifications of both samples $G_{X}^{\\prime}$ and targets $G_{Y}^{\\prime}$ converge towards a unified principle: minimizing or preventing any sample $\\mathbf{x}$ from being labeled with multiple or inaccurate targets $\\mathbf{y}$ . This principle emphasizes the importance of providing accurate and informative targets y for each sample $\\mathbf{x}$ , as analyzed in Remark 1 , and suggests extending this insight to any complex dataset like $S$ . ", "page_idx": 4}, {"type": "image", "img_path": "UPxmISfNCO/tmp/e8211660c5eadfcab5beb09c7820a5ad19c0d8b612ec69bc3cb9e79238b8a47d.jpg", "img_caption": ["Figure 2: Investigating modified samples with varied $\\Sigma$ values. Following [39], Figure 2a visualizes the validation loss landscape within a two-dimensional parameter space, along with three training trajectories corresponding to different $\\Sigma$ settings. Figure 2b illustrates the performance of models trained using samples with varied $\\Sigma$ . The optimal case in our task, utilizing samples with $\\Sigma=0.1$ (which achieves the lowest validation loss in Figure 2b), is visualized in Figure 2c, where the color bar represents the values of targets $y$ . "], "img_footnote": [], "page_idx": 5}, {"type": "image", "img_path": "UPxmISfNCO/tmp/dd32748923a0b2d70ad0958c84853fdad6a36ace3813cea0ff7baecb78256097.jpg", "img_caption": [], "img_footnote": [], "page_idx": 5}, {"type": "text", "text": "Figure 3: Investigating modified targets with varied $\\rho$ values. We present a visualization of the validation loss landscape in Figure 3a, including three training trajectories that correspond to different $\\rho$ settings. Figure 3b illustrates the performance of models trained using targets with varying $\\rho$ values. The optimal scenario for our task, which uses targets with $\\rho=1.0$ , is depicted in Figure 3c. ", "page_idx": 5}, {"type": "text", "text": "Remark 1 (Ideal data properties avoid implicitly introduced gradient noise from data) . Intuitively, the semantic information within each sample x should be unique and not identical to another sample. Consequently, the exact target y, which represents the semantic information of x, should also be unique and informative. This implies the necessity of establishing bijective (or one-to-one) mappings between samples and their respective targets. ", "page_idx": 5}, {"type": "text", "text": "In contrast, when a sample x (or several similar samples) is labeled with multiple different targets y, it may implicitly introduce noise into the gradient $\\nabla\\ell(\\mathbf{x},\\mathbf{y})$ , thereby hindering the optimization. ", "page_idx": 5}, {"type": "text", "text": "However, real-world datasets often deviate from the ideal properties described above, as discussed in Remark 2 below and further analyzed in Appendix $\\mathbf{M}$ . ", "page_idx": 5}, {"type": "text", "text": "Remark 2 (Imperfect mappings and inaccurate targets in real-world datasets) . In practice, we observe that \u2018noisy mappings\u2019 between input samples and targets are prevalent in real-world datasets. As illustrated in Figure 6 , several common phenomena contribute to this issue: ", "page_idx": 5}, {"type": "text", "text": "\u2022 Similar or identical input samples may be assigned different targets due to using data augmentations, which is common in both self-supervised and human-supervised learning settings. \u2022 Inaccurate targets may be generated, particularly in self-supervised learning scenarios. \u2022 In human-supervised learning, all samples within a class are mapped to a one-hot target. ", "page_idx": 5}, {"type": "text", "text": "These imperfect mappings and inaccurate targets pose challenges to achieving optimal training efficiency and effectiveness for real-world datasets. ", "page_idx": 5}, {"type": "text", "text": "3.4 Generalization-bounded Efficient Data Synthesis ", "text_level": 1, "page_idx": 5}, {"type": "text", "text": "Given insights from Remark 1 , an effective approach to generate efficient data $S$ from original data $D$ involves employing a high-quality labeler $\\psi$ to relabel each sample $\\mathbf{x}$ within $D$ . This process results in the formation of an optimized dataset $S$ . However, the generalization ability of models trained on these optimized datasets $S$ is not inherently assured. For a given labeler $\\psi$ , we derive the generalization bound, which quantifies the representation distance between models $\\phi_{\\theta}$ and $\\phi^{\\star}$ , trained on the optimized dataset $S$ . This is presented in Theorem 3 (see Appendix D for the proof). ", "page_idx": 5}, {"type": "text", "text": "", "page_idx": 6}, {"type": "text", "text": "Definition 4 (Representation distancea) . We introduce our proposed metric as ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{D}_{\\mathrm{Rep}}(\\phi_{S}\\to\\phi_{T};D):=\\operatorname*{inf}_{\\mathbf{W}\\in\\mathbb{R}^{m\\times n},\\mathbf{b}\\in\\mathbb{R}^{m}}\\left\\{\\mathbb{E}_{\\mathbf{x}\\sim D}\\left[\\ell(\\mathbf{W}\\phi_{S}(\\mathbf{x})+\\mathbf{b},\\phi_{T}(\\mathbf{x}))\\right]\\right\\}\\,,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "which quantifies the distance between a source model $\\phi_{S}$ and a target model $\\phi_{T}$ with respect to a dataset $D$ , and the loss function is defined as $\\ell(\\hat{y},y):=\\mathbf{1}(\\hat{y}\\neq y)$ . ", "page_idx": 6}, {"type": "text", "text": "aIntuitively, a smaller $\\mathrm{D}_{\\mathrm{Rep}}(\\phi_{S}\\rightarrow\\phi_{T};D)$ indicates that the model $\\phi_{S}$ can be transformed into $\\phi_{T}$ over data $D$ via a linear model with relative ease. This also implies that $\\phi_{S}$ can achieve the same linear evaluation performance as $\\phi_{T}$ on data $D$ . ", "page_idx": 6}, {"type": "text", "text": "Theorem 3 (Generalization bound with labeler $\\psi$ ) . Assumimg the model $\\phi_{\\theta}:\\mathcal{X}\\rightarrow\\mathcal{Y}$ belongs to a hypothesis space $\\Phi$ . Then, for any $\\delta\\in(0,1)$ , with probability at least $1-\\delta$ , we have ", "page_idx": 6}, {"type": "equation", "text": "$$\n\\mathrm{D}_{\\mathrm{Rep}}(\\phi_{\\theta}\\to\\phi^{\\star};X)\\leq\\mathrm{B}_{\\mathrm{Sample}}+\\mathrm{B}_{\\mathrm{Target}}+\\mathrm{B}_{\\mathrm{Model}}\\,,\n$$", "text_format": "latex", "page_idx": 6}, {"type": "text", "text": "where $\\mathrm{B}_{\\mathrm{Target}}=\\mathrm{D}_{\\mathrm{Rep}}(\\psi\\rightarrow\\phi^{\\star};D_{X})$ , $\\mathbf{B}_{\\mathrm{Model}}=\\mathrm{D}_{\\mathrm{Rep}}(\\phi_{\\theta}\\to\\psi;S_{X})+2\\Re_{D_{X}}(\\Phi)+\\tilde{\\mathcal{O}}(|D_{X}|^{-1})$ , and $\\mathbf{B}_{\\mathrm{Sample}}=\\mathbf{D}_{\\mathrm{TV}}\\big(S_{X},D_{X}\\big)$ . DTV represents the total variation divergence [4], and $\\Re_{D_{X}}$ is the empirical Rademacher complexity. ", "page_idx": 6}, {"type": "text", "text": "Drawing insights from Theorem $1\\,,2$ and 3, we define the properties of ideal data below: ", "page_idx": 6}, {"type": "text", "text": "Definition 5 (Properties of ideal efficient data, including samples $S_{X}$ and targets $S_{Y}$ ) . $T o$ meet the objectives of Definition 2 , an ideal efficient data requires: $\\circled{1}$ Targets $(S_{Y})$ : ", "page_idx": 6}, {"type": "text", "text": "\u2022 generating targets $\\psi(\\mathbf{x})$ from the labeler $\\psi$ that are accurate (i.e., align with human-annotating targetsaand optimal model $\\phi^{\\star}.$ ), aiming to minimize $\\mathsf{D}_{\\mathrm{Rep}}(\\psi\\to\\phi^{\\star};D_{X})$ ; \u2022 the generated target $\\psi(\\mathbf{x})$ should be informative to corresponding sample $\\mathbf{x}$ (i.e., forming perfect bijective mappings with the original samples x), according to Remark $^{5}$ ; ", "page_idx": 6}, {"type": "text", "text": "$\\circled{2}$ Samples $(S_{X})$ : low distribution disparity represented by $\\operatorname{D}_{\\mathrm{TV}}(S_{X},D_{X})$ and high sample diversity denoted as $\\left|S_{X}\\right|.$ , aiming at minimizing $\\mathsf{D}_{\\mathrm{TV}}(S_{X},D_{X})$ . ", "page_idx": 6}, {"type": "text", "text": "aAlignment with human-annotated targets can occur via direct prediction or linear transportability. ", "page_idx": 6}, {"type": "text", "text": "To meet the requirement $\\circled{2}$ specified in Remark 5 , we utilize weak data augmentation on $D_{X}$ to generate the set $S_{X}$ , aimed at enhancing the diversity $\\vert S_{X}\\vert$ while ensuring the term $\\operatorname{D}_{\\mathrm{TV}}(S_{X},D_{X})$ remains minimal. However, satisfying requirement $\\circled{1}$ is non-trivial, as capturing a labeler $\\psi$ that matches $\\phi^{\\star}$ is intractable (i.e., achieving $\\bar{\\mathrm{D}_{\\mathrm{Rep}}}(\\psi\\to\\phi^{\\star};D_{X})=0$ is challenging). ", "page_idx": 6}, {"type": "text", "text": "Fortunately, our experiments in real-world scenarios, as discussed in Section A , demonstrate that employing a prior model as the labeler $\\psi$ generally approximates $\\phi^{\\star}$ within the representation space. Therefore, we posit that introducing a prior model $\\psi$ to generate an efficient dataset $S$ can typically accelerate the early stages of learning. Subsequently, the original dataset $D$ should be employed. ", "page_idx": 6}, {"type": "text", "text": "In (8), the last term, $\\mathrm{D}_{\\mathrm{Rep}}(\\phi_{\\theta}\\to\\psi;S_{X})+2\\Re_{D_{X}}(\\Phi)+\\tilde{\\mathcal{O}}(|D_{X}|^{-1})$ , includes $2\\Re_{D_{X}}(\\Phi)$ and $\\tilde{\\mathcal{O}}(|D_{X}|^{-1})$ , which depend on the neural architecture and the size of $D_{X}$ , respectively, and can be considered constants. Thus, optimizing the model involves minimizing $\\ D_{\\mathrm{Rep}}(\\phi_{\\theta}\\to\\psi;S_{X})$ through training $\\phi_{\\theta}$ . A detailed technical solution is provided in Section 4 . ", "page_idx": 6}, {"type": "text", "text": "4 Methodology ", "text_level": 1, "page_idx": 6}, {"type": "text", "text": "Building upon the theoretical insights from Section 3.2 , we propose our RELA (see Figure 1 ): (a) RELA-D $(\\pmb{\\mathscr{s}})$ is used to generate efficient data (c.f. Section 4.1 ); (b) RELA-F $(\\pmb{\\mathscr{G}})$ guides the models to train over the efficient data from RELA-D $(\\pmb{\\mathscr{s}})$ (c.f. Section 4.2 ). ", "page_idx": 6}, {"type": "text", "text": "4.1 RELA-D $(\\pmb{\\mathscr{s}})$ : Synthesis of Efficient Dataset ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "Motivated by two property requirements in Definition 5 , here we introduce our optimization-free synthesis process of both samples and targets in our RELA-D (see technical details in Appendix F ). ", "page_idx": 7}, {"type": "text", "text": "Generating transportable representations as the targets. We argue that well-trained models (called prior models) on diverse real-world datasets using various neural network architectures and algorithms converge towards the same linear representation space. In other words, the generated pseudo representations $R_{Y}$ for samples $D_{X}$ using these prior models are linearly transportable to each other and to the human-annotating targets. The empirical verifications refer to Appendix A . We further justify in Appendix F.1 that the requirement $\\circled{1}$ in Definition 5 can be achieved by employing a prior model as the ideal labeler $\\psi:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{m}$ , i.e., generating $R_{Y}=\\{\\psi({\\bf x})\\mid{\\bf x}\\sim D_{X}\\}$ as the targets. The generation process of targets is conducted only once (refer to Appendix H for details), and the generated targets $R_{Y}$ are stored and combined with the samples $D_{X}$ to form the data $D=\\left(D_{X},R_{Y}\\right)$ . ", "page_idx": 7}, {"type": "text", "text": "Efficient and distribution-aligned sample generation. To satisfy requirement $\\circled{2}$ in Definition 5 efficiently, we employ basic data augmentations into data $D_{X}$ such as RandomResizeCrop with a minimum scale of 0.5 (as opposed to the default of 0.08) and RandomHorizontalFlip with $p=0.5$ . ", "page_idx": 7}, {"type": "text", "text": "4.2 RELA-F $(\\pmb{\\mathscr{G}})$ : Assist Learning with Generated Efficient Dataset ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "In this section, we showcase the significance of understanding ideal data properties and generated efficient dataset in assisting self-supervised learning, given this self-supervised paradigm on unlabeled data suffers from significant inefficiency issues compared to human-supervised learning [62]. ", "page_idx": 7}, {"type": "text", "text": "Here we propose a plug-and-play method that can be seamlessly integrated into any existing selfsupervised learning algorithm, significantly enhancing its training efficiency by introducing an additional loss term. Formally, the loss function is defined as follows: ", "page_idx": 7}, {"type": "equation", "text": "$\\begin{array}{r}{\\mathcal{L}_{\\mathrm{RELA}}:=\\mathbb{E}_{\\mathbf{x},\\mathbf{y}\\sim(D_{X},R_{Y})}\\left[\\ell(\\mathbf{W}\\phi_{\\theta}(\\mathbf{x})-\\mathbf{b},\\mathbf{y})\\right]\\,,}\\end{array}$ ", "text_format": "latex", "page_idx": 7}, {"type": "text", "text": "where $\\ell({\\mathbf{z}},{\\mathbf{y}}):=1-{\\mathbf{z}}\\cdot{\\mathbf{y}}/(||{\\mathbf{z}}|||{\\mathbf{y}}||)$ be the loss function, $\\mathcal{L}_{\\mathrm{SSL}}$ denotes the loss specified by any self-supervised learning method, respectively. Furthermore, the data $D=(D_{X},\\bar{R_{Y}})$ are collected using the strategy outlined in Section 4.1 , with updates occurring at each $k$ -th epoch. ", "page_idx": 7}, {"type": "text", "text": "The dynamic coefficient $\\lambda\\in\\{0,1\\}$ divides the training process into two distinct stages. Initially, $\\lambda$ is set to 1 to emphasize $\\mathcal{L}_{\\mathrm{RELA}}$ , assuming its crucial role in the early learning phase. As the model $\\phi_{\\theta}$ improves and self-generated targets become more reliable in $\\mathcal{L}_{\\mathrm{SSL}}$ , an adaptive attenuation algorithm adjusts $\\lambda$ to 0 (note that the initial $\\lambda$ is tuning-free for all cases and see Appendix J for details). As a result, only a single loss term in (9) is calculated, ensuring no extra computational cost with RELA. ", "page_idx": 7}, {"type": "text", "text": "To enhance the recognition of RELA-aided algorithms, we re-denote those that are used in their names. For example, the BYOL algorithm [30], when enhanced with RELA, is re-denoted as BYOL $(\\,\\pmb{\\mathscr{G}}\\,)$ . Furthermore, as the prior models downloaded from the internet are not consistently robust, the aforementioned dynamic setting of $\\lambda$ also prevents the model $\\phi_{\\theta}$ from overftiting to potentially weak generated targets. The efficacy of our proposed RELA is empirically validated in Section 5 . ", "page_idx": 7}, {"type": "text", "text": "5 Experiments ", "text_level": 1, "page_idx": 7}, {"type": "text", "text": "This section describes the experimental setup and procedures undertaken to test our hypotheses and evaluate the effectiveness of our proposed methodologies. ", "page_idx": 7}, {"type": "text", "text": "Experimental setting. We list the settings below (see more details in Appendix K ). ", "page_idx": 7}, {"type": "text", "text": "\u2022 Datasets: For low-resolution data $(32\\times32)$ , we evaluate our method on two datasets, i.e., CIFAR-10 [35] and CIFAR-100 [34]. For high-resolution data, we conduct experiments on two large-scale datasets including Tiny-ImageNet $\\mathrm{{[64\\times64)}}$ [36] and full ImageNet-1K $(224\\times224)$ [20], to assess the scalability and effectiveness of our method on more complex and varied datasets. ", "page_idx": 7}, {"type": "text", "text": "\u2022 Neural network architectures: Similar to prior works/benchmarks of dataset distillation [56] and selfsupervised learning [57, 19], we use several backbone architectures to evaluate the generalizability ", "page_idx": 7}, {"type": "text", "text": "Table 1: Benchmark our RELA with various prior models against BYOL. We compare evaluation results of the models trained using $\\bullet$ BYOL with $10\\%$ , $20\\%$ and $50\\%$ training budget/steps; $\\bullet$ BYOL $(\\pmb{\\mathscr{s}})$ with different prior models; $\\bullet$ BYOL with full budget, denoted as BYOL $\\star$ in this table. Regarding the prior models used for our RELA, we respectively utilize six models with increasing representation capabilities, including $\\bullet$ randomly initialized network (Rand.); $\\bullet$ four $\\mathbf{B}\\mathbf{Y}\\mathbf{O}\\mathbf{L}^{\\star}$ -trained models (CF10-T, CF100-T, TIN-T, IN1K-T) corresponding to four datasets (listed below); \u2022 CLIP-RN50. The evaluations are performed across four datasets, i.e., CIFAR-10 (CF-10), CIFAR-100 (CF-100), Tiny-ImageNet (T-IN), and ImageNet-1K (IN-1K). We underline the results that outperform the full training, and bold the results that achieve the highest performance using a specific ratio of budget. All the networks used for training are ResNet-18, except the ResNet-50 used for IN-1K. ", "page_idx": 8}, {"type": "table", "img_path": "UPxmISfNCO/tmp/2591a096b1d26e456b7eb52b8ad1226ed802c353ccd95b18cb3a0534c95cdc05.jpg", "table_caption": [], "table_footnote": [], "page_idx": 8}, {"type": "text", "text": "of our method, including ResNet-{18, 50, 101} [26], EfficientNet-B0 [58], MobileNet-V2 [52], ViT [21], and a series of CLIP-based models [49]. These architectures represent a range of model complexities and capacities, enabling a comprehensive assessment of our approach. ", "page_idx": 8}, {"type": "text", "text": "\u2022 Baselines: Referring to a prior widely-used benchmark [57, 19], we consider several state-of-the-art methods as baselines for a broader practical impact, including: SimCLR [12], Barlow Twins [69], BYOL [30], DINO [10], MoCo [25], SimSiam [14], SwAV [9], and Vicreg [3]. ", "page_idx": 8}, {"type": "text", "text": "\u2022 Evaluation: Following previous benchmarks and research [57, 19, 12, 3], we evaluate all the trained models using offline linear probing strategy to reflect the representation ability of the trained models, and ensure a fair and comprehensive comparison with baseline approaches. ", "page_idx": 8}, {"type": "text", "text": "\u2022 Implementation details: We implement our method by extending a popular self-supervised learning open-source benchmark [57] and use their configurations therein. This includes using AdamW as the optimizer, with a mini-batch size of 128 (except for ImageNet-1K, where we use a mini-batch size of 512). We implement our method through PyTorch [48], and all experiments are conducted on NVIDIA RTX 4090 GPUs. See more detailed configurations and hyper-parameters in Appendix K . ", "page_idx": 8}, {"type": "text", "text": "5.1 Primary Experimental Results and Analysis ", "text_level": 1, "page_idx": 8}, {"type": "text", "text": "Recall that our RELA-D $(\\pmb{\\mathscr{s}})$ , as illustrated in Figure 1 and Section 4.1 , requires an unlabeled dataset and any pre-trained model freely available online to generate the efficient dataset. To justify the superior performance and generality of our RELA across various unlabeled datasets using prior models with different representation abilities, our comparisons in this subsection start with BYOL $[30]^{2}$ and then extend to other self-supervised learning methods. ", "page_idx": 8}, {"type": "text", "text": "Table 1 demonstrates the efficacy and efficiency of our RELA in facilitating the learning of robust representations. Overall, $B Y O L(\\Dot{\\pmb{\\mathscr{G}}})$ consistently outperforms the original BYOL when trained with a reduced budget. In certain cases, such as on CIFAR-100, BYOL $(\\pmb{\\mathscr{G}})$ employing only $10\\%$ of the budget can surpass the performance of BYOL-trained models using the entire budget Specifically: ", "page_idx": 8}, {"type": "text", "text": "(a) A stronger prior model (e.g., CLIP) enhances the performance of RELA more effectively than a weaker model (e.g., Rand.);   \n(b) Our RELA is not sensitive to the prior knowledge. For instance, using CF10-T as the prior model can achieve competitive performance compared to that trained on extensive datasets (e.g., CLIP); ", "page_idx": 8}, {"type": "table", "img_path": "UPxmISfNCO/tmp/a2f1ac91d788a8ed6f1cca322029911f66f41db830506f6a45e2dc962d5d4cc1.jpg", "table_caption": ["Table 2: Evaluating our RELA on cross-architecture settings. Our RELA-D $(\\pmb{\\mathscr{s}})$ distills datasets with prior RN18 (Rand.) and CLIP-{RN101, $\\mathrm{RN}50\\!\\times\\!4$ , ViT B/32, ViT B/16, ViT L/14}, then versus transfer to ResNet-18; MobileNet-V2; EfficientNet-B0; ViT T/16. We train models using $10\\%$ budget through (original) BYOL $\\underline{{\\left(\\bullet\\right)}}$ . "], "table_footnote": [], "page_idx": 9}, {"type": "table", "img_path": "UPxmISfNCO/tmp/be6da944989f33a510b77ab648a2779ba12a4ee305b68ff27f1d1503c1bc84e3.jpg", "table_caption": ["Table 3: Evaluating our RELA across different self-supervised learning methods. We extend our analysis beyond BYOL by training and evaluating models using seven additional self-supervised learning methods, along with their RELA-augmented counterparts $(\\pmb{\\mathscr{G}})$ , utilizing randomly initialized ResNet-18 (Rand.) and CLIP-RN50 as prior models for the RELA-D $(\\pmb{\\mathscr{s}})$ . All methods are trained using $10\\%$ budget. "], "table_footnote": [], "page_idx": 9}, {"type": "text", "text": "(c) A randomly initialized model can effectively aid in accelerating learning through our RELA. This can be considered an effective scenario of \u201cweak-to-strong supervision\u201d [7] using pseudo targets. ", "page_idx": 9}, {"type": "text", "text": "Cross-architecture generalization. RELA-D $(\\pmb{\\mathscr{s}})$ generates efficient datasets using a specific neural architecture. To evaluate the generalization ability of these datasets, it is essential to test their performance on various architectures not used in the distillation process. Table 2 presents the performance of our RELA in conjunction with various prior models and trained model architectures, demonstrating its robust generalization ability. Specifically: ", "page_idx": 9}, {"type": "text", "text": "(a) The integration of RELA always enhances the performance of original BYOL; (b) Our RELA method exhibits minimal sensitivity to the architecture of the prior model, as evidenced by the comparable performance of BYOL $(\\dot{\\pmb{\\varphi}})$ using both ViT-based and ResNet-based models. ", "page_idx": 9}, {"type": "text", "text": "Combining RELA across various self-supervised learning methods. To demonstrate the effectiveness and versatility of RELA in enhancing various self-supervised learning methods, we conduct experiments with widely-used techniques. Table 3 presents the results, highlighting the robust generalization capability of RELA. Our findings consistently show that RELA improves the performance of these methods while maintaining the same budget ratio, emphasizing its potential on learning using unlabeled data. Additionally, we provide the results when combining RELA with human-supervised learning in Appendix I . ", "page_idx": 9}, {"type": "text", "text": "6 Conclusion and Limitation ", "text_level": 1, "page_idx": 9}, {"type": "text", "text": "In this paper, to address the Problem 1 , we investigate the optimal properties of data, including samples and targets, to identify the properties that improve generalization and optimization in deep learning models. Our theoretical insights indicate that targets which are informative and linearly transportable to strong representations (e.g., human annotations) enable trained models to exhibit robust representation abilities. Furthermore, we empirically find that well-trained models (called prior models) across various tasks and architectures serve as effective labelers for generating such targets. Consequently, we propose the Representation Learning Accelerator (RELA), which leverages any freely available prior model to generate high-quality targets for samples. Additionally, RELA can enhance existing (self-)supervised learning approaches by utilizing these generated data to accelerate training. However, our theoretical analysis is restricted to the simplified scenario described in Definition 3 , which has limited applicability in real-world contexts. ", "page_idx": 9}, {"type": "text", "text": "7 Acknowledgement ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "We thank Xinyi Shang, Bowen Ding and and the anonymous reviewers for their invaluable comments and feedback. We also thank Bei Shi for assisting with the partial code implementation. This work was supported in part by the National Science and Technology Major Project (No. 2022ZD0115101), the Research Center for Industries of the Future (RCIF) at Westlake University, and the Westlake Education Foundation. ", "page_idx": 10}, {"type": "text", "text": "References ", "text_level": 1, "page_idx": 10}, {"type": "text", "text": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan Yuille, Trevor Darrell, Jitendra Malik, and Alexei A Efros. Sequential modeling enables scalable learning for large vision models. arXiv preprint arXiv:2312.00785, 2023. [3] Adrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization for self-supervised learning. arXiv preprint arXiv:2105.04906, 2021. [4] Andrew R Barron, Lhszl Gyorf,i and Edward C van der Meulen. Distribution estimation consistent in total variation and in two types of information divergence. IEEE transactions on Information Theory, 38(5):1437\u20131454, 1992. [5] Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Vaughan. A theory of learning from different domains. Machine Learning, 79:151\u2013175, 2010. [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. [7] Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et al. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390, 2023. [8] Yue Cao, Zhenda Xie, Bin Liu, Yutong Lin, Zheng Zhang, and Han Hu. Parametric instance classification for unsupervised visual feature learning. In NeurIPS, 2020. [9] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In NeurIPS, 2020.   \n[10] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021.   \n[11] George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei A Efros, and Jun-Yan Zhu. Dataset distillation by matching training trajectories. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4750\u20134759, 2022.   \n[12] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020.   \n[13] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020.   \n[14] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In CVPR, 2021.   \n[15] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. arXiv preprint arXiv:2104.02057, 2021.   \n[16] Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A survey of model compression and acceleration for deep neural networks. arXiv preprint arXiv:1710.09282, 2017.   \n[17] Justin Cui, Ruochen Wang, Si Si, and Cho-Jui Hsieh. Dc-bench: Dataset condensation benchmark. Advances in Neural Information Processing Systems, 35:810\u2013822, 2022.   \n[18] Justin Cui, Ruochen Wang, Si Si, and Cho-Jui Hsieh. Scaling up dataset distillation to imagenet1k with constant memory. In International Conference on Machine Learning, pages 6565\u20136590. PMLR, 2023.   \n[19] Victor Guilherme Turrisi Da Costa, Enrico Fini, Moin Nabi, Nicu Sebe, and Elisa Ricci. sololearn: A library of self-supervised methods for visual representation learning. Journal of Machine Learning Research, 23(56):1\u20136, 2022.   \n[20] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.   \n[21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \n[22] Jiawei Du, Yidi Jiang, Vincent YF Tan, Joey Tianyi Zhou, and Haizhou Li. Minimizing the accumulated trajectory error to improve dataset distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3749\u20133758, 2023.   \n[23] Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of neural networks, 2019.   \n[24] Ziyao Guo, Kai Wang, George Cazenavette, Hui Li, Kaipeng Zhang, and Yang You. Towards lossless dataset distillation via difficulty-aligned trajectory matching. arXiv preprint arXiv:2310.05773, 2023.   \n[25] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020.   \n[26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.   \n[27] Qianjiang Hu, Xiao Wang, Wei Hu, and Guo-Jun Qi. Adco: Adversarial contrast for efficient learning of unsupervised representations from self-trained negative adversaries. In CVPR, 2021.   \n[28] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. Gpipe: Efficient training of giant neural networks using pipeline parallelism. Advances in neural information processing systems, 32, 2019.   \n[29] Arthur Jacot, Franck Gabriel, and Cl\u00e9ment Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018.   \n[30] Grill Jean-Bastien, Strub Florian, Altch\u00e9 Florent, Tallec Corentin, Pierre Richemond H., Buchatskaya Elena, Doersch Carl, Bernardo Pires Avila, Zhaohan Guo Daniel, Mohammad Azar Gheshlaghi, Piot Bilal, Kavukcuoglu Koray, Munos R\u00e9mi, and Valko Michal. Bootstrap your own latent - a new approach to self-supervised learning. In NeurIPS, 2020.   \n[31] Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and Diane Larlus. Hard negative mixing for contrastive learning. In NeurIPS, 2020.   \n[32] Jang-Hyun Kim, Jinuk Kim, Seong Joon Oh, Sangdoo Yun, Hwanjun Song, Joonhyun Jeong, Jung-Woo Ha, and Hyun Oh Song. Dataset condensation via efficient synthetic-data parameterization. In International Conference on Machine Learning, pages 11102\u201311118. PMLR, 2022.   \n[33] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.   \n[34] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n[35] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 and cifar-100 datasets. URl: https://www. cs. toronto. edu/kriz/cifar. html, 6(1):1, 2009.   \n[36] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015.   \n[37] Dong Bok Lee, Seanie Lee, Joonho Ko, Kenji Kawaguchi, Juho Lee, and Sung Ju Hwang. Self-supervised dataset distillation for transfer learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2024.   \n[38] Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent \\*. Journal of Statistical Mechanics: Theory and Experiment, 2020(12):124002, December 2020.   \n[39] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. Advances in neural information processing systems, 31, 2018.   \n[40] Junnan Li, Pan Zhou, Caiming Xiong, and Steven CH Hoi. Prototypical contrastive learning of unsupervised representations. In ICLR, 2021.   \n[41] Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. Advances in neural information processing systems, 31, 2018.   \n[42] Tao Lin, Sebastian U Stich, Kumar Kshitij Patel, and Martin Jaggi. Don\u2019t use large mini-batches, use local sgd. arXiv preprint arXiv:1808.07217, 2018.   \n[43] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt understands, too. AI Open, 2023.   \n[44] Yanqing Liu, Jianyang Gu, Kai Wang, Zheng Zhu, Wei Jiang, and Yang You. Dream: Efficient dataset distillation by representative matching. arXiv preprint arXiv:2302.14416, 2023.   \n[45] Sachin Mehta, Maxwell Horton, Fartash Faghri, Mohammad Hossein Sekhavat, Mahyar Najibi, Mehrdad Farajtabar, Oncel Tuzel, and Mohammad Rastegari. Catlip: Clip-level visual recognition accuracy with $2.7\\,\\mathtt{X}$ faster pre-training on web-scale image-text data. arXiv preprint arXiv:2404.15653, 2024.   \n[46] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning. 2018. Second Edition.   \n[47] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.   \n[48] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.   \n[49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.   \n[50] Pierre H Richemond, Jean-Bastien Grill, Florent Altch\u00e9, Corentin Tallec, Florian Strub, Andrew Brock, Samuel Smith, Soham De, Razvan Pascanu, Bilal Piot, et al. Byol works even without batch statistics. arXiv preprint arXiv:2010.10241, 2020.   \n[51] Herbert E. Robbins. A stochastic approximation method. Annals of Mathematical Statistics, 22:400\u2013407, 1951.   \n[52] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4510\u20134520, 2018.   \n[53] Shitong Shao, Zeyuan Yin, Muxin Zhou, Xindong Zhang, and Zhiqiang Shen. Generalized large-scale data condensation via various backbone and statistical matching. arXiv preprint arXiv:2311.17950, 2023.   \n[54] Shitong Shao, Zikai Zhou, Huanran Chen, and Zhiqiang Shen. Elucidating the design space of dataset condensation. arXiv preprint arXiv:2404.13733, 2024.   \n[55] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in nlp. arXiv preprint arXiv:1906.02243, 2019.   \n[56] Peng Sun, Bei Shi, Daiwei Yu, and Tao Lin. On the diversity and realism of distilled dataset: An efficient dataset distillation paradigm. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.   \n[57] Igor Susmelj, Matthias Heller, Philipp Wirth, Jeremy Prescott, and Malte Ebner et al. Lightly. GitHub. Note: https://github.com/lightly-ai/lightly, 2020.   \n[58] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning, pages 6105\u20136114. PMLR, 2019.   \n[59] Chenxin Tao, Honghui Wang, Xizhou Zhu, Jiahua Dong, Shiji Song, Gao Huang, and Jifeng Dai. Exploring the equivalence of siamese self-supervised learning via a unified gradient framework. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14431\u201314440, 2022.   \n[60] Yuandong Tian, Xinlei Chen, and Surya Ganguli. Understanding self-supervised learning dynamics without contrastive pairs. In ICML, 2021.   \n[61] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008.   \n[62] Guangrun Wang, Keze Wang, Guangcong Wang, Philip HS Torr, and Liang Lin. Solving inefficiency of self-supervised representation learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9505\u20139515, 2021.   \n[63] Kai Wang, Bo Zhao, Xiangyu Peng, Zheng Zhu, Shuo Yang, Shuo Wang, Guan Huang, Hakan Bilen, Xinchao Wang, and Yang You. Cafe: Learning to condense dataset by aligning features. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12196\u201312205, 2022.   \n[64] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In ICML, 2020.   \n[65] Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A Efros. Dataset distillation. arXiv preprint arXiv:1811.10959, 2018.   \n[66] David H Wolpert and William G Macready. No free lunch theorems for optimization. IEEE transactions on evolutionary computation, 1(1):67\u201382, 1997.   \n[67] Zeyuan Yin, Eric Xing, and Zhiqiang Shen. Squeeze, recover and relabel: Dataset condensation at imagenet scale from a new perspective. arXiv preprint arXiv:2306.13092, 2023.   \n[68] Ruonan Yu, Songhua Liu, and Xinchao Wang. Dataset distillation: A comprehensive review. arXiv preprint arXiv:2301.07014, 2023.   \n[69] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St\u00e9phane Deny. Barlow twins: Selfsupervised learning via redundancy reduction. In ICML, 2021.   \n[70] Lei Zhang, Jie Zhang, Bowen Lei, Subhabrata Mukherjee, Xiang Pan, Bo Zhao, Caiwen Ding, Yao Li, and Dongkuan Xu. Accelerating dataset distillation via model augmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11950\u201311959, 2023.   \n[71] Bo Zhao and Hakan Bilen. Dataset condensation with distribution matching. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 6514\u20136523, 2023.   \n[72] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. Dataset condensation with gradient matching. arXiv preprint arXiv:2006.05929, 2020.   \n[73] Ganlong Zhao, Guanbin Li, Yipeng Qin, and Yizhou Yu. Improved distribution matching for dataset condensation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7856\u20137865, 2023.   \n[74] Rui Zhu, Bingchen Zhao, Jingen Liu, Zhenglong Sun, and Chang Wen Chen. Improving contrastive learning by visualizing feature transformation. In ICCV, 2021. ", "page_idx": 10}, {"type": "text", "text": "", "page_idx": 11}, {"type": "text", "text": "", "page_idx": 12}, {"type": "text", "text": "", "page_idx": 13}, {"type": "text", "text": "", "page_idx": 14}, {"type": "text", "text": "Contents ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "1 Introduction 1 ", "page_idx": 15}, {"type": "text", "text": "2 Related Work 3   \n2.1 Dataset Distillation: Efficient yet Effective Learning Using Fewer Data . . . . 3   \n2.2 Self-supervised Learning: Representation Learning Using Unlabeled Data . . 3 ", "page_idx": 15}, {"type": "text", "text": "3 Revealing Critical Properties of Efficient Learning over Data 4 ", "page_idx": 15}, {"type": "text", "text": "3.1 Unifying (Self-)Supervised Learning from a Data-Centric Perspective . . 4   \n3.2 Empirical and Theoretical Investigation of Data-Centric Efficient Learning . . . . 4   \n3.3 Extended Understanding of Data-Centric Efficient Learning . . . 5   \n3.4 Generalization-bounded Efficient Data Synthesis 6   \n4 Methodology 7   \n4.1 RELA-D $(\\pmb{\\mathscr{s}})$ : Synthesis of Efficient Dataset 8   \n4.2 RELA-F $(\\mathbf{\\dot{\\varphi}})$ : Assist Learning with Generated Efficient Dataset 8   \n5 Experiments 8   \n5.1 Primary Experimental Results and Analysis 9 ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "6 Conclusion and Limitation 10 ", "page_idx": 15}, {"type": "text", "text": "7 Acknowledgement 11 ", "page_idx": 15}, {"type": "text", "text": "A Ablation Study 17 ", "page_idx": 15}, {"type": "text", "text": "B Proof of Theorem 1 18   \nB.1 Setup . . . 18   \nB.2 Algorithm . . 18   \nB.3 Bounds with Variance . . 18   \nB.4 Nonlinear case 19   \nB.5 From the Perspective of Feature Learning 19   \nC Proof of Theorem 2 20   \nC.1 Setting . . 20   \nC.2 Bounds with $\\rho$ . 20   \nC.3 Nonlinear case 20   \nD Proof of Theorem 3 21   \nD.1 Relation between data distribution and Rademacher complexity . . 25 ", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "", "page_idx": 15}, {"type": "text", "text": "E Explanation of Rescaling Samples 25 ", "page_idx": 15}, {"type": "text", "text": "F Detailed Methodology of RELA-D 27   \nF.1 Proof for Ideal Properties of Prior Models 27 ", "page_idx": 15}, {"type": "text", "text": "G Analysis of Different Self-Supervised Learning Methods 28 ", "text_level": 1, "page_idx": 15}, {"type": "text", "text": "G.1 A Unified Framework for SSL 28   \nG.2 Contrastive Learning Methods 28   \nG.3 Asymmetric Network Methods . 29   \nG.4 Feature Decorrelation Methods . . 30 ", "page_idx": 15}, {"type": "text", "text": "H Budget of RELA for Data Synthesis 31 ", "page_idx": 15}, {"type": "text", "text": "I.1 Experimental Setup 31   \nI.2 Main Results 31 ", "page_idx": 15}, {"type": "text", "text": "J RELA Algorithm 32 ", "page_idx": 15}, {"type": "text", "text": "K.1 Detailed Setup for Experiments in Section 3.2 . . 32   \nK.2 Detailed Setup for Experiments in Section 5 . 33   \nL Batch PCA Reduction 33   \nL.1 Principal Component Analysis 33 ", "page_idx": 16}, {"type": "text", "text": "", "page_idx": 16}, {"type": "text", "text": "M Analyze the Practical Data Augmentations 35 ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "M.1 Assumptions and Definitions . . 35   \nM.2 Augmented Distributions . . . 36   \nM.3 Increased Variance Leads to Increased Overlap 36   \nM.4 Conclusion . . . . . 37   \nM.5 Empirical Analysis for Real-world Datasets . . 37 ", "page_idx": 16}, {"type": "text", "text": "A Ablation Study ", "text_level": 1, "page_idx": 16}, {"type": "text", "text": "We conduct ablation studies to understand the impact of each component of RELA on performance. ", "page_idx": 16}, {"type": "image", "img_path": "UPxmISfNCO/tmp/95db92d0526f6a8a5a9061c5c13ac6419742fd71345cbd252afe0d5e27e5804d.jpg", "img_caption": [], "img_footnote": [], "page_idx": 16}, {"type": "text", "text": "Figure 4: Ablation study on BYOL $(\\pmb{\\mathscr{G}})$ ) components and parameters. (a) We analyze the representation similarity between various source models (indicated on the $\\mathbf{X}$ -axis) and target models (indicated on the y-axis). (b) We compare the static RELA weight setting strategy with our adaptive strategy. Dotted lines $({}^{\\leftarrow}{-}^{\\cdot})$ represent our adaptive strategy, while solid lines $(^{\\leftarrow})$ denote the static $\\lambda$ setting strategy. Specifically, in the static weight setting (e.g., 0.4), the first $40\\%$ of the training leverages RELA, with the remaining $60\\%$ employing the original algorithm. (c) We present the computational cost, quantified as training time/steps, of our RELA across various prior models. ", "page_idx": 16}, {"type": "text", "text": "Empirical representation similarity. Our foundational assumption of our RELA is that the representations of well-trained models, developed using various neural network architectures and algorithms on diverse real-world datasets, exhibit linear transportability to one another. To test this hypothesis, we assess representation similarity, a metric that quantifies the linear transferability between pre-trained models. The results, depicted in Figure ${4\\mathrm{a}}$ , demonstrate that representations from robust models (e.g., CLIP) can be effectively transferred to less robust models (e.g., CF10-T). This finding aligns with our results in Table 1 , showing that leveraging powerful models (e.g., CLIP and IN1K-T) consistently enhances learning in models trained on datasets with limited knowledge, such as CF-10. ", "page_idx": 16}, {"type": "text", "text": "Combining RELA and BYOL with static $\\lambda$ setting strategies. The coefficient $\\lambda$ , as introduced in Section 4.2 , is pivotal in controlling the weight of the RELA phase during training. To assess the robustness of our adaptive strategy, which dynamically adjusts $\\lambda$ , we compare it to a static $\\lambda$ setting strategy. The results in Figure 4b indicate that larger (smaller) RELA weights are advantageous when using a strong (weak) prior model. Nonetheless, static settings lack generalizability across various scenarios, whereas our adaptive strategy demonstrates superior generalization capabilities. ", "page_idx": 16}, {"type": "text", "text": "Analysis of the computational cost when using RELA with different prior models. To validate that our RELA, in conjunction with various prior models, can effectively reduce computational costs in self-supervised learning, we conducted experiments comparing the computational expense required for BYOL $(\\pmb{\\mathscr{G}})$ to achieve equivalent performance to the original BYOL trained with a full budget. The results, illustrated in Figure $4c$ , consistently demonstrate that RELA assists BYOL in lowering training costs while maintaining equivalent performance levels. Furthermore, it is evident that employing robust prior models consistently leads to greater reductions in training budgets. ", "page_idx": 16}, {"type": "text", "text": "B Proof of Theorem 1 ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "In this section, we prove a slightly modified version of Theorem1, extending the distribution to Generalized Gaussian Mixture(GGM) and making some assumptions for technical reasons. Yet this proof could still reflect the essential of the theorem. ", "page_idx": 17}, {"type": "text", "text": "B.1 Setup", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "otation $\\mathrm{\\bfN}(\\mu,\\alpha,\\beta)$ denotes the generalized Gaussian distribution with pdf $\\textstyle{\\frac{\\beta}{2\\alpha\\Gamma(1/\\beta)}}e^{-(|x-\\mu|/\\alpha)^{\\beta}}$ $\\mathbf{B}$ for Bernoulli distribution. ", "page_idx": 17}, {"type": "text", "text": "We focus on the 1-dim situation. Assume that $\\mu_{1}<\\mu_{2}$ . Define the original data distribution $\\mathcal{N}_{0}=$ $\\mathbf{N}(\\mu_{1},\\alpha_{0},\\beta_{0})$ and $\\mathcal{N}_{1}=\\mathbf{N}(\\mu_{2},\\alpha_{0},\\beta_{0}))$ ", "page_idx": 17}, {"type": "equation", "text": "$$\nG:=\\{(x,y)\\mid y\\sim2\\cdot{\\bf B}(1,\\frac{1}{2})-1,x\\sim\\frac{1-y}{2}\\cdot\\mathcal{N}_{0}+\\frac{1+y}{2}\\cdot\\mathcal{N}_{1}\\}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and the modified one $\\mathcal{N}_{0}^{\\prime}=\\mathbf{N}(\\mu_{1},\\alpha,\\beta)$ and $\\mathcal{N}_{1}^{\\prime}=\\mathbf{N}(\\mu_{2},\\alpha,\\beta))$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\nG^{\\prime}:=\\{(x,y)\\mid y\\sim2\\cdot{\\bf B}(1,\\frac{1}{2})-1,x\\sim\\frac{1-y}{2}\\cdot\\cdot\\!\\mathcal{N}_{0}^{\\prime}+\\frac{1+y}{2}\\cdot\\!\\mathcal{N}_{1}^{\\prime}\\}\\,.\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Our task is predicting $y$ given $x$ . Note that $y\\in\\{\\pm1\\}$ , which is a bit different from the definition in Section 3.2. In 1-dim situation, we just need one parameter for this classification task, so define $f_{\\theta}(x):=\\mathrm{sign}(x+\\theta)$ to fit the distribution. We could compute the generalization loss on original distribution: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\mathcal{L}(f_{\\theta})=(\\int_{-\\theta}^{+\\infty}\\,d F_{-}+\\int_{-\\infty}^{-\\theta}\\,d F_{+})/2=(1-\\int_{-\\frac{\\theta+\\mu_{2}}{\\alpha_{0}}}^{-\\frac{\\theta+\\mu_{1}}{\\alpha_{0}}}\\,d F)/2\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "Obviously $\\theta^{\\star}=-\\frac{\\mu_{1}\\!+\\!\\mu_{2}}{2}$ , we have: ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal L(f_{\\theta})-\\mathcal L(f_{\\theta^{\\star}})=(\\int_{-\\frac{\\mu_{2}-\\mu_{1}}{2\\alpha_{0}}}^{\\frac{\\mu_{2}-\\mu_{1}}{2\\alpha_{0}}}\\,d F-\\int_{-\\frac{\\theta+\\mu_{2}}{\\alpha_{0}}}^{-\\frac{\\theta+\\mu_{1}}{\\alpha_{0}}}\\,d F)/2}\\\\ {\\leq C_{1}\\cdot(\\theta-\\theta^{\\star})^{2}\\ \\ \\ (o r\\ C_{1}^{\\prime}\\,|\\theta-\\theta^{\\star}|)}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "where $C_{1},C_{1}^{\\prime}$ are constants, $F_{0}\\;F_{1}\\;F$ denote the CDF of $\\mathcal{N}_{0}\\,\\mathcal{N}_{1}$ and $\\Nu(0,1,\\beta_{0})$ respectively. The inequality above is due to the fact that function $\\begin{array}{r}{h(x)=(\\int_{-1}^{1}\\,d F-\\int_{x-1}^{x+1}\\,d F)/x^{2}}\\end{array}$ has limits at 0 and so is bounded. ", "page_idx": 17}, {"type": "text", "text": "B.2 Algorithm ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "For a dataset $\\{(x_{i},y_{i})\\}_{i=1}^{n}$ , set the loss function $\\begin{array}{r}{L(\\theta)=\\frac{1}{n}\\sum_{i=1}^{n}\\ell\\left[y_{i}(x_{i}+\\theta)\\right]}\\end{array}$ , $\\begin{array}{r}{\\ell(v)=\\frac{1}{2}(1-v)^{2}}\\end{array}$ We apply the stocastic gradient descent algorithm and assume the online setting ( $\\left.n=1\\right|$ ): at step $t$ draw one sample $\\left({{x}_{t}},{{y}_{t}}\\right)$ from $G^{\\prime}$ then use the gradient $\\nabla L(\\theta_{t})$ to update $\\theta$ $(\\eta\\in(0,1),t\\in\\mathbb{N})$ : ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\theta_{t+1}=\\theta_{t}-\\eta\\nabla L(\\theta_{t}),}\\\\ &{}\\\\ &{\\nabla L(\\theta_{t})=\\theta+(x_{t}-y_{t}).}\\end{array}\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "It can be observed that randomness of $x$ leads to noies on gradient. ", "page_idx": 17}, {"type": "text", "text": "B.3 Bounds with Variance ", "text_level": 1, "page_idx": 17}, {"type": "text", "text": "We prove the proposition that lower variance of GG can make convergence faster, i.e.   \n$\\mathbb{E}\\left[\\mathcal{L}\\bar{(f_{\\theta_{t}})}-\\mathcal{L}(\\bar{f_{\\theta^{\\star}}})\\right]$ is bounded by an increasing function of variance $t$ fixed). ", "page_idx": 17}, {"type": "text", "text": "Proof. From above, we could get ", "page_idx": 17}, {"type": "equation", "text": "$$\n\\theta_{t}=(1-\\eta)^{t}\\theta_{0}-\\eta\\left[(x_{t-1}-y_{t-1})+(1-\\eta)(x_{t-2}-y_{t-2})+\\cdot\\cdot\\cdot+(1-\\eta)^{t-1}(x_{0}-y_{0})\\right]\n$$", "text_format": "latex", "page_idx": 17}, {"type": "text", "text": "and so : ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\begin{array}{r l}{\\displaystyle(\\ f_{\\theta_{t}})-\\mathcal{L}(f_{\\theta^{*}})\\Big]\\le C_{1}\\mathbb{E}\\left[(\\theta_{t}-\\theta^{*})^{2}\\right]}&{}\\\\ &{\\displaystyle=C_{1}\\mathbb{E}\\left\\{\\left[(1-\\eta)^{t}(\\theta_{0}-\\theta^{*})-\\eta\\sum_{j=1}^{t}(1-\\eta)^{j-1}(x_{t-j}-y_{t-j}+\\theta^{*})\\right]^{2}\\right\\}}\\\\ &{\\displaystyle=C_{1}\\mathbb{E}\\left[(1-\\eta)^{2t}(\\theta_{0}-\\theta^{*})^{2}+\\eta^{2}\\sum_{j=1}^{t}(1-\\eta)^{2(j-1)}(x_{t-j}-y_{t-j}+\\theta^{*})^{2}\\right]}\\\\ &{\\displaystyle=C_{1}\\left((1-\\eta)^{2t}(\\theta_{0}-\\theta^{*})^{2}+\\frac{\\eta}{(2-\\eta)}(1-(1-\\eta)^{2t})\\left[\\frac{\\alpha^{2}\\Gamma(3/\\beta)}{\\Gamma(1/\\beta)}+\\left(1-\\frac{\\mu_{2}-\\mu}{2}\\right)^{2}\\right]\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "The last two equalities is due to the fact that for $(x,y)\\sim G^{\\prime}$ ", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[x-y+\\theta^{\\star}\\right]=0\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\mathbb{E}\\left[(x-y+\\theta^{\\star})^{2}\\right]=\\frac{\\alpha^{2}\\Gamma(3/\\beta)}{\\Gamma(1/\\beta)}+\\left(1-\\frac{\\mu_{2}-\\mu_{1}}{2}\\right)^{2}.\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "B.4 Nonlinear case ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In this subsection, we conduct some qualitative analysis on the nonlinear case. The setting is the same as that in Section 3.2. We point out the differences compared with the linear case above: $\\mathbf{x}\\in\\mathbb{R}^{d},y\\in\\{0,1\\}$ and ", "page_idx": 18}, {"type": "equation", "text": "$$\nf_{\\pmb\\theta}(\\mathbf x):=\\sigma\\left(\\pmb\\theta^{[1]}\\cdot\\mathrm{ReLU}\\left(\\pmb\\theta^{[2]}\\mathbf x+\\pmb\\theta^{[3]}\\right)+\\pmb\\theta^{[4]}\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "where $\\begin{array}{r}{\\sigma(z)=\\frac{1}{1+e^{-z}}}\\end{array}$ is the sigmoid function; $\\operatorname{ReLU}(z)=\\operatorname*{max}(0,z)$ is the activation function for the hidden layer, which provides non-linearity to the model; $\\pmb{\\theta}^{[2]}$ and $\\pmb{\\theta}^{[3]}$ are the weights and biases of the hidden layer; $\\pmb{\\theta}^{[1]}$ and $\\pmb{\\theta}^{[4]}$ are the weights and biases of the output layer. ", "page_idx": 18}, {"type": "text", "text": "To make things explicit, we still assume the online setting and set the loss function $\\begin{array}{r}{L(\\theta)=\\frac{1}{2}(f_{\\theta}(\\mathbf{x})-}\\end{array}$ $y)^{2}$ . Assume after some iterations, ${\\pmb\\theta}^{[2]}\\cdot{\\boldsymbol\\mu}_{1}+{\\pmb\\theta}^{[3]}<0$ and ${\\pmb\\theta}^{[2]}\\cdot{\\mu}_{2}+{\\pmb\\theta}^{[3]}>0$ (coordinate-wise). In this situation, we could see that if $\\mathbf{x}$ is close to its mean( $\\,\\!\\,\\mu_{1}$ or $\\mu_{2}$ ), the sign of $\\mathrm{ReLU}\\left(\\pmb{\\theta}^{[2]}\\mathbf{x}+\\pmb{\\theta}^{[3]}\\right)$ will be the same as $y$ . So $f_{\\theta}$ will become an optimal classifier if $\\pmb{\\theta}^{[1]}\\rightarrow+\\infty$ and $\\pmb{\\theta}^{[4]}\\rightarrow-\\infty$ . We focus on $\\pmb{\\theta}^{[1]}$ , using SGD: ", "page_idx": 18}, {"type": "equation", "text": "$$\n{\\pmb\\theta}_{t+1}^{[1]}={\\pmb\\theta}_{t}^{[1]}-\\eta\\frac{\\partial L}{\\partial{\\pmb\\theta}^{[1]}},\n$$", "text_format": "latex", "page_idx": 18}, {"type": "equation", "text": "$$\n\\frac{\\partial L}{\\partial\\pmb{\\theta}^{[1]}}=(f_{\\pmb{\\theta}}(\\mathbf{x})-y)\\sigma(1-\\sigma)\\mathrm{ReLU}\\left(\\pmb{\\theta}^{[2]}\\mathbf{x}+\\pmb{\\theta}^{[3]}\\right)\n$$", "text_format": "latex", "page_idx": 18}, {"type": "text", "text": "Note that we drop the variable value in $\\sigma(\\cdot)$ to make the expression more compact. ", "page_idx": 18}, {"type": "text", "text": "Then we can analyze the phenomenon qualitatively: larger $\\Sigma$ will make convergence slower. The reason is that the larger $\\Sigma$ is, when $\\mathbf{x}$ is drawn from $\\mathcal{N}_{1}$ $(y=1)$ ), $\\pmb{\\theta}^{[2]}\\mathbf{x}+\\pmb{\\theta}^{[3]}<\\bar{0}$ is more likely to happen(i.e. straying far away from the mean), causing $\\pmb{\\theta}^{[1]}$ to stop updating; what\u2019s worse, when $\\mathbf{x}$ is drawn from $\\mathcal{N}_{0}$ $(y=0,$ ), with larger probability $\\theta^{[2]}\\mathbf{x}+\\theta^{[3]}>0$ which will make $\\pmb{\\theta}^{[1]}$ to go in the opposite direction. In summary, it is $\\Sigma$ that makes the gradient noisy thus impacts the convergence rate. ", "page_idx": 18}, {"type": "text", "text": "B.5 From the Perspective of Feature Learning ", "text_level": 1, "page_idx": 18}, {"type": "text", "text": "In essence, the theoretical results in [41] could also be interpreted as a proof of the theorem. [41] study the learning of a two-layer ReLU neural network for $k$ -class classification via stochastic gradient descent (SGD), assuming that each class corresponds to $l$ patterns(distributions), with every two of the $k\\times l$ distributions of the input data are separated by a distance $\\delta$ . Below is the main theorem in [41]: ", "page_idx": 18}, {"type": "text", "text": "Proposition 1 . Suppose some assumptions are satisfied, then for every $\\epsilon\\mathrm{~>~0~}$ , there is $\\bar{\\cal M}\\ \\bar{=\\ p o l y(k,l,1/\\delta,\\bar{1/\\epsilon})}$ such that for every $m~\\geq~M$ , after doing a minibatch SGD with batch size $\\dot{B}\\,=\\,\\dot{p o l y}(k,l,1/\\delta,1/\\epsilon,\\log m)$ and learning rate m\u00b7poly(k,l,1/\u03b4,1/\u03f5,log m) for $T\\,=\\,p o l y(k,l,1/\\delta,1/\\epsilon,\\log m)$ iterations, with high probability, the generalization error(the probability that the model misclassifies) of the learned model is at most \u03f5. ", "page_idx": 19}, {"type": "text", "text": "Theoretical results above show that a larger $\\delta$ helps network to learn more efficiently. (In our case, $\\delta$ can be roughly viewed as the Mahalanobis distance between the two Gaussians, which is inverse proportion to the variance.) Also, Appendix D.2 of [41] demonstrates an example very similar to ours in which the input data is drawn from Gaussian distributions with different means, indicating increasing variance of the Gaussian causes the test accuracy to decrease and takes longer time to get a good solution. ", "page_idx": 19}, {"type": "text", "text": "C Proof of Theorem 2 ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "C.1 Setting ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "Use the same setting in Section B(linear case) except that ", "page_idx": 19}, {"type": "equation", "text": "$$\nG^{\\prime}:=\\{(x,y^{\\prime})\\mid y\\sim2\\cdot{\\bf B}(1,\\frac{1}{2})-1,x=\\frac{1-y}{2}\\cdot{\\cal N}_{0}+\\frac{1+y}{2}\\cdot{\\cal N}_{1},y^{\\prime}=\\rho\\cdot f_{\\theta^{*}}(x)+(1-\\rho)y\\}\\,.\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "In other words, we modify the distribution of $y$ instead of $x$ this time. ", "page_idx": 19}, {"type": "text", "text": "C.2 Bounds with $\\rho$ ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "We\u2019re going to prove that higher $\\rho$ can make convergence faster, i.e. E $[\\mathcal{L}(f_{\\theta_{t}})-\\mathcal{L}(f_{\\theta^{\\star}})]$ is bounded by an decreasing function of $\\rho$ $t$ fixed). ", "page_idx": 19}, {"type": "text", "text": "Proof. The crucial part $x-y^{\\prime}+\\theta^{\\star}=\\rho(x-f_{\\theta^{\\star}}(x)+\\theta^{\\star})+(1-\\rho)(x-y+\\theta^{\\star}),$ , and in fact ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathbb{E}|x-y+\\theta^{\\star}|-\\mathbb{E}|x-f_{\\theta^{\\star}}(x)+\\theta^{\\star}|:=\\epsilon_{0}>0\\ .}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Similarly, we can get bounds with $\\rho$ (see $C_{1}^{\\prime}$ in Section B): ", "page_idx": 19}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbb{E}\\left[\\mathcal{L}(f_{\\theta_{t}})-\\mathcal{L}(f_{\\theta^{\\star}})\\right]\\leq C_{1}^{\\prime}\\mathbb{E}\\left[(1-\\eta)^{t}|\\theta_{0}-\\theta^{\\star}|+(1-(1-\\eta)^{t})|x-y^{\\prime}+\\theta^{\\star}|\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq C_{2}(1-\\eta)^{t}+C_{1}^{\\prime}(1-(1-\\eta)^{t})\\left[\\rho\\cdot\\mathbb{E}|x-f_{\\theta^{\\star}}(x)+\\theta^{\\star}|\\right.}\\\\ &{\\qquad\\qquad\\qquad\\left.+(1-\\rho)\\cdot\\mathbb{E}|x-y+\\theta^{\\star}|\\right]}\\\\ &{\\qquad\\qquad\\qquad\\leq C_{2}(1-\\eta)^{t}+(1-(1-\\eta)^{t})(C_{3}-C_{4}\\rho)}\\end{array}\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "where $C_{2}=C_{1}^{\\prime}\\left\\lvert\\theta_{0}-\\theta^{\\star}\\right\\rvert$ , $C_{3}=C_{1}^{\\prime}\\;\\mathbb{E}|x-y+\\theta^{\\star}|,C_{4}=C_{1}^{\\prime}\\;\\epsilon_{0}>0.$ ", "page_idx": 19}, {"type": "text", "text": "C.3 Nonlinear case ", "text_level": 1, "page_idx": 19}, {"type": "text", "text": "To see the impact of modifying $y$ more clearly, we directly set $\\rho=1$ and conduct a similar analysis as in Section B.4. Let\u2019s still focus on $\\pmb{\\theta}^{[1]}$ and use the same assumptions in Section B.4, then we have: ", "page_idx": 19}, {"type": "equation", "text": "$$\n{\\pmb\\theta}_{t+1}^{[1]}={\\pmb\\theta}_{t}^{[1]}-\\eta\\frac{\\partial L}{\\partial{\\pmb\\theta}^{[1]}},\n$$", "text_format": "latex", "page_idx": 19}, {"type": "equation", "text": "$$\n\\frac{\\partial L}{\\partial\\pmb{\\theta}^{[1]}}=(f_{\\pmb{\\theta}}(\\mathbf{x})-f_{\\pmb{\\theta}^{*}}(\\mathbf{x}))\\sigma(1-\\sigma)\\mathrm{ReLU}\\left(\\pmb{\\theta}^{[2]}\\mathbf{x}+\\pmb{\\theta}^{[3]}\\right)\n$$", "text_format": "latex", "page_idx": 19}, {"type": "text", "text": "Note $y$ is replaced by $f_{\\theta^{\\ast}}(\\mathbf{x})$ . For instance $\\mathbf{x}$ is drawn from $\\mathcal{N}_{0}$ ${\\mathrm{~\\it~y~}}=0$ ) but strays far away from the $\\mu_{1}$ , causing $\\theta^{[2]}\\mathbf{x}+\\theta^{[3]}>0$ . In this situation $f_{\\pmb{\\theta}^{*}}$ is likely to regard $\\mathbf{x}$ as a sample from $\\mathcal{N}_{1}$ (i.e. $f_{\\theta^{\\ast}}\\left(\\mathbf{x}\\right)$ close to 1) thus making $\\pmb{\\theta}^{[1]}$ to go in the right direction instead of the opposite. This explains why larger $\\rho$ can make convergence faster. ", "page_idx": 19}, {"type": "text", "text": "D Proof of Theorem 3 ", "text_level": 1, "page_idx": 20}, {"type": "text", "text": "We follow some proof steps in [5]. Let\u2019s begin by introducing some notations used in this section. Notation and Setup $\\mathcal{X}$ is the input space, $\\mathcal{D}_{S}$ and $\\mathcal{D}_{T}$ are two distributions over $\\mathcal{X}$ . Let ${\\mathcal{D}}=\\{0,1\\}$ . $\\mathcal{H}$ denotes a hypothesis class from $\\mathcal{X}$ to $\\boldsymbol{\\wp}$ . To simplify notations, $\\forall h,f\\,\\in\\,\\mathcal{H}$ let $\\epsilon_{S}(h,f)\\;=\\;$ $E_{\\mathbf{x}\\sim\\mathcal{D}_{S}}[\\mathbf{1}(h(\\mathbf{x})\\neq f(\\mathbf{x}))]$ , and $\\hat{\\epsilon}_{S}(h,f)$ be empirical error $\\langle\\epsilon_{T}(h,f)$ , $\\hat{\\epsilon}_{T}(h,f)$ similar). ", "page_idx": 20}, {"type": "text", "text": "Then we introduce some concepts and lemmas, most of which are from [5]. ", "page_idx": 20}, {"type": "equation", "text": "$$\nd_{\\mathcal{H}}(\\mathcal{D},\\mathcal{D}^{\\prime})=2\\operatorname*{sup}_{h\\in\\mathcal{H}}\\left|\\operatorname*{Pr}_{\\mathcal{D}}[I(h)]-\\operatorname*{Pr}_{\\mathcal{D}^{\\prime}}[I(h)]\\right|\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $I(h)=\\{\\mathbf{x}:h(\\mathbf{x})=1\\}$ . ", "page_idx": 20}, {"type": "text", "text": "Definition 7 (Total Variation Distance) . For two distributions $\\mathcal{D}$ and $\\mathcal{D}^{\\prime}$ , the total variation distance of them is defined as: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathsf{D}_{\\mathrm{TV}}({\\mathcal{D}},{\\mathcal{D}}^{\\prime})=\\operatorname*{sup}_{A\\subseteq{\\mathcal{F}}}\\left|\\mathsf{P r}_{{\\mathcal{D}}}(A)-\\mathsf{P r}_{{\\mathcal{D}}^{\\prime}}(A)\\right|\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "where $\\mathcal{F}$ denotes the collection of all events in the probability space. ", "page_idx": 20}, {"type": "text", "text": "Lemma 1 . For two distributions $\\mathcal{D}$ and $\\mathcal{D}^{\\prime}$ , by definition it\u2019s easy to see that: ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\frac{1}{2}d_{\\mathcal{H}}(\\mathcal{D},\\mathcal{D}^{\\prime})\\leq\\mathrm{D}_{\\mathrm{TV}}(\\mathcal{D},\\mathcal{D}^{\\prime})\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Definition 8 (Symmetric Difference Hypothesis Space) . ", "text_level": 1, "page_idx": 20}, {"type": "equation", "text": "$$\n\\mathcal{H}\\Delta\\mathcal{H}:=\\{g:\\mathcal{X}\\rightarrow\\{0,1\\}|g(\\mathbf{x})=h(\\mathbf{x})\\oplus h^{\\prime}(\\mathbf{x})\\quad\\forall h,h^{\\prime}\\in\\mathcal{H}\\}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "$\\bigoplus$ denotes the XOR operation. ", "page_idx": 20}, {"type": "text", "text": "Lemma 2 ", "text_level": 1, "page_idx": 20}, {"type": "equation", "text": "$$\n\\forall h,h^{\\prime}\\in\\mathcal{H},|\\epsilon_{S}\\left(h,h^{\\prime}\\right)-\\epsilon_{T}\\left(h,h^{\\prime}\\right)|\\leq\\frac{1}{2}d_{\\mathcal{H}\\Delta\\mathcal{H}}\\left(\\mathcal{D}_{S},\\mathcal{D}_{T}\\right)\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "Proof. only need note that $h(\\mathbf{x})\\oplus h^{\\prime}(\\mathbf{x})=|h(\\mathbf{x})-h^{\\prime}(\\mathbf{x})|$ , so ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\operatorname*{sup}_{g\\in\\mathcal{H}\\Delta\\mathcal{H}}\\vert\\mathrm{Pr}_{\\mathcal{D}_{S}}[I(g)]-\\mathrm{Pr}_{\\mathcal{D}_{T}}[I(g)]\\vert=\\operatorname*{sup}_{h,h^{\\prime}\\in\\mathcal{H}}\\vert\\epsilon_{S}\\left(h,h^{\\prime}\\right)-\\epsilon_{T}\\left(h,h^{\\prime}\\right)\\vert\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "this is done by definition. ", "page_idx": 20}, {"type": "text", "text": "With above notations we can derive a general proposition related with Theorem 3. ", "page_idx": 20}, {"type": "text", "text": "Proposition 2 . Assumimg $\\mathcal{H}$ is a hypothesis class from $\\mathcal{X}$ to $\\boldsymbol{\\wp}$ and $\\phi,\\phi^{\\prime}\\in\\mathcal{H}$ , we have: $\\begin{array}{r}{\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}_{T}}\\left[\\ell(\\phi(\\mathbf{x}),\\phi^{\\prime}(\\mathbf{x}))\\right]\\le\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}_{S}}\\left[\\ell(\\phi(\\mathbf{x}),\\phi^{\\prime}(\\mathbf{x}))\\right]+\\operatorname{D}_{\\mathrm{TV}}(\\mathcal{D}_{S},\\mathcal{D}_{T})}\\end{array}$ where loss function is $\\ell(\\hat{y},y):=\\mathbf{1}(\\hat{y}\\neq y)$ . ", "page_idx": 20}, {"type": "text", "text": "Proof. using the lemmas above, ", "page_idx": 20}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\epsilon_{T}(\\phi,\\phi^{\\prime})\\leq\\epsilon_{S}\\left(\\phi,\\phi^{\\prime}\\right)+\\left\\vert\\epsilon_{T}\\left(\\phi,\\phi^{\\prime}\\right)-\\epsilon_{S}\\left(\\phi,\\phi^{\\prime}\\right)\\right\\vert}\\\\ &{\\qquad\\qquad\\leq\\epsilon_{S}\\left(\\phi,\\phi^{\\prime}\\right)+\\frac{1}{2}d_{\\mathcal{H}\\Delta\\mathcal{H}}\\left({\\mathcal{D}}_{S},{\\mathcal{D}}_{T}\\right)}\\\\ &{\\qquad\\qquad\\leq\\epsilon_{S}\\left(\\phi,\\phi^{\\prime}\\right)+\\mathrm{D}_{\\mathrm{TV}}({\\mathcal{D}}_{S},{\\mathcal{D}}_{T})}\\end{array}\n$$", "text_format": "latex", "page_idx": 20}, {"type": "text", "text": "To obtain some useful inequalities of generalization bound, we need to introduce the Rademacher complexity. ", "page_idx": 21}, {"type": "text", "text": "Definition 9 (Rademacher complexity of a function class) . Given a sample of points $S=$ $\\{z_{1},z_{2},\\ldots,z_{m}\\}\\subset Z$ , and considering a function class $\\mathcal{F}$ of real-valued functions over $Z$ , the empirical Rademacher complexity of $\\mathcal{F}$ given $S$ is defined as: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\Re_{S}(\\mathcal{F})=\\frac{1}{m}\\mathbb{E}_{\\sigma}\\left[\\operatorname*{sup}_{f\\in\\mathcal{F}}\\sum_{i=1}^{m}\\sigma_{i}f(z_{i})\\right]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "where $\\sigma_{i}$ are independent and identically distributed Rademacher random variables. In other words, for $i=1,2,\\dots,m$ , the probability that $\\sigma_{i}=+1$ is equal to the probability that $\\sigma_{i}=-1$ , and both are $\\frac{1}{2}$ . Further, let $P$ be a probability distribution over $Z$ . The Rademacher complexity of the function class $\\mathcal{F}$ with respect to $P$ for sample size m is: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\mathfrak{R}_{P,m}(\\mathcal{F}):=\\mathbb{E}_{S\\sim P^{m}}\\left[\\mathfrak{R}_{S}(\\mathcal{F})\\right]\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Lemma 3 (Generalization bound with Rademacher complexity) . Let $\\mathcal{F}$ be a family of loss functions $\\mathcal{F}=\\{(x,y)\\mapsto\\ell((x,y),h):h\\in\\mathcal{H}\\}$ with $\\ell((x,y),h)\\in[0,1]$ for all $\\ell,(x,y)$ and $h$ . Then, with probability $1-\\delta$ , the generalization gap is ", "page_idx": 21}, {"type": "equation", "text": "$$\nL(h)-\\hat{L}(h)\\leq2\\Re_{U}(\\mathcal{F})+3\\sqrt{\\frac{\\log(2/\\delta)}{2n}}\\,,\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "for all $h\\in\\mathcal H$ and samples $U$ of size $n$ . ", "page_idx": 21}, {"type": "text", "text": "The proof of this classical result could be found in most machine learning textbooks, like [46]. Since $\\ell(\\cdot)$ is 1-Lipschitz, we can derive that $\\mathfrak{R}_{U_{S}}(\\ell\\circ\\mathcal{H})\\leq\\mathfrak{R}_{U_{S}}(\\mathcal{H})$ . ", "page_idx": 21}, {"type": "text", "text": "Theorem 3 involves the representation distance, below we prove the triangle inequality for representation distance. ", "page_idx": 21}, {"type": "text", "text": "Lemma 4 (Triangle inequality for Representation distance) . for any functions $\\phi_{S},\\phi_{T},\\phi_{U}$ and distribution $D$ , we have: ", "page_idx": 21}, {"type": "equation", "text": "$$\n{\\mathrm{D}}_{\\mathrm{Rep}}(\\phi_{S}\\rightarrow\\phi_{T};D)\\leq{\\mathrm{D}}_{\\mathrm{Rep}}(\\phi_{S}\\rightarrow\\phi_{U};D)+{\\mathrm{D}}_{\\mathrm{Rep}}(\\phi_{U}\\rightarrow\\phi_{T};D)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Proof. Let\u2019s denote: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r}{d_{1}=\\mathrm{D}_{\\mathrm{Rep}}(\\phi_{S}\\rightarrow\\phi_{U};D)}\\\\ {d_{2}=\\mathrm{D}_{\\mathrm{Rep}}(\\phi_{U}\\rightarrow\\phi_{T};D)}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "By definition: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{d_{1}=\\underset{W_{1}\\in\\mathbb R^{m\\times n},b_{1}\\in\\mathbb R^{m}}{\\operatorname*{inf}}\\mathbb{E}_{x\\sim D}\\ell(W_{1}\\phi_{S}(x)+b_{1},\\phi_{U}(x))}\\\\ &{d_{2}=\\underset{W_{2}\\in\\mathbb R^{m\\times n},b_{2}\\in\\mathbb R^{m}}{\\operatorname*{inf}}\\mathbb{E}_{x\\sim D}\\ell(W_{2}\\phi_{U}(x)+b_{2},\\phi_{T}(x))}\\end{array}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "We need to show: ", "page_idx": 21}, {"type": "equation", "text": "$$\nD_{\\mathrm{Rep}}(\\phi_{S}\\to\\phi_{T};D)\\leq d_{1}+d_{2}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Consider the composition of the transformations: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\phi_{S}(x)\\xrightarrow{W_{1},b_{1}}\\phi_{U}(x)\\xrightarrow{W_{2},b_{2}}\\phi_{T}(x)\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "The combined transformation can be written as: ", "page_idx": 21}, {"type": "equation", "text": "$$\nW_{2}(W_{1}\\phi_{S}(x)+b_{1})+b_{2}=W_{2}W_{1}\\phi_{S}(x)+W_{2}b_{1}+b_{2}\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "Using the properties of the loss function $\\ell$ , we can write: ", "page_idx": 21}, {"type": "equation", "text": "$$\n\\ell(W_{2}W_{1}\\phi_{S}(x)+W_{2}b_{1}+b_{2},\\phi_{T}(x))\\leq\\ell(W_{1}\\phi_{S}(x)+b_{1},\\phi_{U}(x))+\\ell(W_{2}\\phi_{U}(x)+b_{2},\\phi_{T}(x))\n$$", "text_format": "latex", "page_idx": 21}, {"type": "text", "text": "This inequality holds for the reason that it can only break when the two items at right-hand side are both 0, in which the left side is also 0 due to rule of composition. Thus the inequality holds for all $x$ and $W_{1},b_{1},W_{2},b_{2}$ . ", "page_idx": 22}, {"type": "text", "text": "Taking expectations over $x\\sim D$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathfrak{L}_{x\\sim D}\\ell(W_{2}W_{1}\\phi_{S}(x)+W_{2}b_{1}+b_{2},\\phi_{T}(x))\\le\\mathbb{E}_{x\\sim D}\\ell(W_{1}\\phi_{S}(x)+b_{1},\\phi_{U}(x))+\\mathbb{E}_{x\\sim D}\\ell(W_{2}\\phi_{U}(x)+b_{2},\\phi_{T}(x))\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Taking the infimum over $W_{1},b_{1}$ and $W_{2},b_{2}$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname*{inf}_{W_{1},b_{1},W_{2},b_{2}}\\mathbb{E}_{x\\sim D}\\ell(W_{2}W_{1}\\phi_{S}(x)+W_{2}b_{1}+b_{2},\\phi_{T}(x))\\le d_{1}+d_{2}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Since the left-hand side is an upper bound for ${\\mathrm{D}}_{\\mathrm{Rep}}(\\phi_{S}\\rightarrow\\phi_{T};D)$ , we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{D}_{\\mathrm{Rep}}(\\phi_{S}\\rightarrow\\phi_{T};D)\\leq d_{1}+d_{2}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Therefore, the triangle inequality holds for the metric $\\mathrm{D}_{\\mathrm{Rep}}$ . ", "page_idx": 22}, {"type": "text", "text": "Now we are ready to prove Theorem 3. ", "page_idx": 22}, {"type": "text", "text": "Proof. For arbitrary $W,b$ , with probability $1-\\delta$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{D}_{\\mathrm{Rep}}(\\phi_{\\theta}\\to\\phi^{\\star};X)\\leq\\epsilon_{X}(W\\phi_{\\theta}+b,\\phi^{\\star})\\leq\\epsilon_{D_{X}}(W\\phi_{\\theta}+b,\\phi^{\\star})+2\\Re_{D_{X}}(\\Phi)+3\\sqrt{\\frac{\\log(2/\\delta)}{2\\left|D_{X}\\right|}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Taking the infimum over $W,b$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\mathrm{D}_{\\mathrm{Rep}}(\\phi_{\\theta}\\rightarrow\\phi^{\\star};X)\\leq\\mathrm{D}_{\\mathrm{Rep}}(\\phi_{\\theta}\\rightarrow\\phi^{\\star};D_{X})+2\\Re_{D_{X}}(\\Phi)+3\\sqrt{\\frac{\\log(2/\\delta)}{2\\left|D_{X}\\right|}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Using the triangle inequality for representation distance: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathrm{D}_{\\mathrm{Rep}}(\\phi_{\\theta}\\to\\phi^{\\star};D_{X})\\leq\\mathrm{D}_{\\mathrm{Rep}}(\\phi_{\\theta}\\to\\psi;D_{X})+\\mathrm{D}_{\\mathrm{Rep}}(\\psi\\to\\phi^{\\star};D_{X})}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "For arbitrary $W,b$ , using the proposition above, ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname{D}_{\\mathrm{Rep}}(\\phi_{\\theta}\\to\\psi;D_{X})\\leq\\epsilon_{D_{X}}(W\\phi_{\\theta}+b,\\psi)\\leq\\epsilon_{S_{X}}(W\\phi_{\\theta}+b,\\psi)+\\operatorname{D}_{\\mathrm{TV}}(S_{X},D_{X})}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Taking the infimum over $W,b$ : ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\operatorname{D}_{\\mathrm{Rep}}(\\phi_{\\theta}\\to\\psi;D_{X})\\leq\\operatorname{D}_{\\mathrm{Rep}}(\\phi_{\\theta}\\to\\psi;S_{X})+\\operatorname{D}_{\\mathrm{TV}}(S_{X},D_{X})}\\end{array}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Combining the above results, we have: ", "page_idx": 22}, {"type": "equation", "text": "$$\n\\operatorname{D}_{\\mathrm{Rep}}(\\phi_{\\theta}\\to\\phi^{\\star};X)\\leq\\operatorname{D}_{\\mathrm{Rep}}(\\psi\\to\\phi^{\\star};D_{X})+\\operatorname{D}_{\\mathrm{Rep}}(\\phi_{\\theta}\\to\\psi;S_{X})+2\\Re_{D_{X}}(\\Phi)+3{\\sqrt{\\frac{\\log(2/\\delta)}{2\\,|D_{X}|}}}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "Impact of initialization In this part we leverage the Neural Tangent Kernel (NTK) framework [29] to deduce that a neural network initialized closer to a target function $f^{\\star}$ converges faster during training. Under the NTK regime, neural networks exhibit linearized training dynamics, allowing us to predict how the network\u2019s output evolves during training. ", "page_idx": 22}, {"type": "text", "text": "In the context of supervised learning, consider a neural network with parameters $\\theta$ and output function $f(x;{\\boldsymbol{\\theta}})$ , where $x$ is the input. The goal is to approximate a target function $f^{\\star}(x)$ by minimizing a loss function, typically the mean squared error (MSE): ", "page_idx": 22}, {"type": "equation", "text": "$$\nL(\\theta)={\\frac{1}{2}}\\sum_{i=1}^{n}(f(x_{i};\\theta)-y_{i})^{2}\n$$", "text_format": "latex", "page_idx": 22}, {"type": "text", "text": "where $\\{(x_{i},y_{i})\\}_{i=1}^{n}$ is the training data and $y_{i}=f^{\\star}(x_{i})$ . ", "page_idx": 23}, {"type": "text", "text": "Under gradient descent with learning rate $\\eta$ , the parameter updates are: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\theta_{t+1}=\\theta_{t}-\\eta\\nabla_{\\theta}L(\\theta_{t})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "In the NTK regime, where the network width tends to infinity, the network\u2019s output evolves linearly with respect to the parameters around initialization $\\theta_{0}$ [38]. We can approximate: ", "page_idx": 23}, {"type": "equation", "text": "$$\nf(\\boldsymbol{x};\\boldsymbol{\\theta})\\approx f(\\boldsymbol{x};\\boldsymbol{\\theta}_{0})+\\nabla_{\\boldsymbol{\\theta}}f(\\boldsymbol{x};\\boldsymbol{\\theta}_{0})^{\\top}(\\boldsymbol{\\theta}-\\boldsymbol{\\theta}_{0})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This linearization allows us to express the evolution of the network\u2019s output as: ", "page_idx": 23}, {"type": "equation", "text": "$$\nf_{t}(x)=f_{0}(x)-\\eta\\sum_{s=0}^{t-1}\\sum_{i=1}^{n}K(x,x_{i})(f_{s}(x_{i})-y_{i})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $f_{t}(x)=f(x;\\theta_{t})$ is the network output at time $t$ and $K(x,x^{\\prime})=\\nabla_{\\theta}f(x;\\theta_{0})^{\\top}\\nabla_{\\theta}f(x^{\\prime};\\theta_{0})$ is the Neural Tangent Kernel. ", "page_idx": 23}, {"type": "text", "text": "In continuous time (gradient flow), the training dynamics can be described by a differential equation: ", "page_idx": 23}, {"type": "equation", "text": "$$\n{\\frac{d f_{t}(x)}{d t}}=-\\sum_{i=1}^{n}K(x,x_{i})(f_{t}(x_{i})-y_{i})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Vectorizing over all training inputs, we have: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\frac{d\\mathbf{f}_{t}}{d t}=-\\mathbf{K}(\\mathbf{f}_{t}-\\mathbf{y})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $\\mathbf{f}_{t}\\,=\\,[f_{t}(x_{1}),f_{t}(x_{2}),\\hdots,f_{t}(x_{n})]^{\\top},\\mathbf{y}\\,=\\,[y_{1},y_{2},\\hdots,y_{n}]^{\\top}$ and $\\mathbf{K}$ is the NTK matrix with entries $K_{i j}=K(x_{i},x_{j})$ . This differential equation has the solution: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{f}_{t}-\\mathbf{y}=e^{-\\mathbf{K}t}(\\mathbf{f}_{0}-\\mathbf{y})\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "This equation shows that the error $\\mathbf f_{t}-\\mathbf y$ decays exponentially over time, with the rate governed by the NTK matrix $\\mathbf{K}$ . Since $\\mathbf{K}$ is symmetric, we can decompose $\\mathbf{K}$ into its eigenvalues $\\left\\{\\lambda_{j}\\right\\}$ and corresponding eigenvectors $\\{\\mathbf{v}_{j}\\}$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{K}=\\sum_{j}\\lambda_{j}\\mathbf{v}_{j}\\mathbf{v}_{j}^{\\top}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "the solution becomes: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\mathbf{f}_{t}-\\mathbf{y}=\\sum_{j}e^{-\\lambda_{j}t}c_{j}\\mathbf{v}_{j}\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "where $c_{j}=\\mathbf{v}_{j..}^{\\top}(\\mathbf{f}_{0}-\\mathbf{y})$ . The rate of convergence for each component of the error is proportional to the corresponding eigenvalue $\\lambda_{j}$ of the NTK. Larger eigenvalues lead to faster decay. Initial error matters: the initial error $\\mathbf f_{0}-\\mathbf y$ scales the amplitude of the exponential terms. A smaller initial error means the network starts closer to the target function, reducing the time required for the error to decay to a specific threshold. ", "page_idx": 23}, {"type": "text", "text": "Suppose we want the error to be less than $\\epsilon$ : ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\mathbf f_{t}-\\mathbf y\\|\\leq\\epsilon\n$$", "text_format": "latex", "page_idx": 23}, {"type": "text", "text": "Using the solution: ", "page_idx": 23}, {"type": "equation", "text": "$$\n\\|\\mathbf{f}_{t}-\\mathbf{y}\\|\\leq\\|e^{-\\lambda_{\\mathrm{min}}t}\\|\\|\\mathbf{f}_{0}-\\mathbf{y}\\|\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $\\lambda_{\\operatorname*{min}}$ is the smallest (positive) eigenvalue of $\\mathbf{K}$ . Solving for time $t$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\nt\\geq\\frac{1}{\\lambda_{\\operatorname*{min}}}\\ln\\left(\\frac{\\|\\mathbf{f}_{0}-\\mathbf{y}\\|}{\\epsilon}\\right)\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "By leveraging the NTK framework, we\u2019ve shown that a better initialization(meaning the neural network\u2019s initial output function is closer to the target function $f^{\\star}.$ ) leads to faster convergence during training. This is because the training dynamics under NTK are linear, and the exponential error decay is directly influenced by the magnitude of the initial error. Analysis above implies that if we can train a network that is closer to the target function, it will converge faster to the target function. ", "page_idx": 24}, {"type": "text", "text": "D.1 Relation between data distribution and Rademacher complexity ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Now let\u2019s look at the Rademacher complexity appeared above more carefully. Let 1-Lipschitz positive homogeneous activation $\\sigma_{i}$ be given, and ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\mathcal{H}:=\\left\\{\\mathbf{x}\\mapsto\\sigma_{L}\\left(W_{L}\\sigma_{L-1}\\left(\\cdot\\cdot\\cdot\\sigma_{1}\\left(W_{1}\\mathbf{x}\\right)\\cdot\\cdot\\cdot\\right)\\right):\\left\\|W_{i}\\right\\|_{\\mathrm{F}}\\leq B,\\mathbf{x}\\in\\mathbb{R}^{d}\\right\\}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "Then using Theorem 1 in [23], for samples $S$ of size $m$ we have bound for the empirical Rademacher complexity: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\Re_{S}(\\mathcal{H})\\leq\\frac{1+\\sqrt{2L\\ln2}}{m}B^{L}\\|X\\|_{\\mathrm{F}}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where $X\\in\\mathbb{R}^{d\\times m}$ is the input data matrix, and $\\Vert\\cdot\\Vert_{\\mathrm{F}}$ denotes the Frobenius norm. ", "page_idx": 24}, {"type": "text", "text": "If we further assume that the data are drawn from the distribution $G$ (stated in Section 3.2) with covariance $\\Sigma$ (for simplicity let $\\mu_{1}=-\\mu,\\mu_{2}=\\mu)$ , then we can bound the Rademacher complexity of $\\mathcal{H}$ with respect to $G$ : ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathfrak{R}_{G,m}(\\mathcal{H})=\\mathbb{E}_{S\\sim G^{m}}\\left[\\mathfrak{R}_{S}(\\mathcal{H})\\right]}\\\\ &{\\qquad\\qquad\\leq\\frac{1+\\sqrt{2L\\ln2}}{m}B^{L}\\,\\mathbb{E}_{S\\sim G^{m}}\\|X\\|_{\\mathrm{F}}}\\\\ &{\\qquad\\qquad=\\frac{\\sqrt{2}+2\\sqrt{L\\ln2}}{m}B^{L}\\cdot\\Sigma\\,\\Gamma\\left(\\frac{1+d m}{2}\\right)\\,M\\left(-\\frac{1}{2},\\frac{d m}{2},-\\frac{d m\\mu^{2}}{2\\Sigma^{2}}\\right)/\\Gamma\\left(\\frac{d m}{2}\\right)}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "The right part of the last inequality is an increasing function with respect to $\\Sigma$ , and $\\Gamma(\\cdot)$ denotes the gamma function, $M(\\cdot,\\cdot,\\cdot)$ is the Kummer\u2019s confluent hypergeometric function, given by: ", "page_idx": 24}, {"type": "equation", "text": "$$\nM(a,b,z)=\\sum_{n=0}^{\\infty}\\frac{a^{(n)}z^{n}}{b^{(n)}n!}={}_{1}F_{1}(a;b;z),\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "where: ", "page_idx": 24}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{a^{(0)}=1,}}\\\\ {{a^{(n)}=a(a+1)(a+2)\\cdots(a+n-1),}}\\end{array}\n$$", "text_format": "latex", "page_idx": 24}, {"type": "text", "text": "is the rising factorial. ", "page_idx": 24}, {"type": "text", "text": "Remark. Usually, the Rademacher complexity $\\Re_{S}(\\ell\\circ\\mathcal{F})$ could be bounded by of $\\Re_{S}({\\mathcal{F}})$ . For example, if $\\ell$ is $L$ -lipschitz, then $\\Re_{S}(\\ell\\circ\\mathcal{F})\\leq L\\cdot\\Re_{S}(\\mathcal{F})$ . That\u2019s why we directly compute the Rademacher complexity of $\\mathcal{H}$ instead of $\\ell\\circ\\mathcal{H}$ . ", "page_idx": 24}, {"type": "text", "text": "E Explanation of Rescaling Samples ", "text_level": 1, "page_idx": 24}, {"type": "text", "text": "Dataset distillation seeks to create a condensed dataset that allows models to achieve performance comparable to those trained on the full dataset, but with fewer training steps. In this section, we will demonstrate that rescaling the variance of Gaussian distributions, as defined in Definition 3, does not affect the optimal performance of models trained on these rescaled data. ", "page_idx": 24}, {"type": "text", "text": "Proof. Consider two Gaussian distributions $\\mathcal{N}_{0}(\\mu_{1},\\sigma^{2}\\mathbf{I})$ and $\\mathcal{N}_{1}(\\mu_{2},\\sigma^{2}\\mathbf{I})$ with means $\\mu_{1}=1$ and $\\mu_{2}=2$ , and variance $\\sigma^{2}$ . We define the bimodal mixture distribution $G=\\left(G_{X},G_{Y}\\right)$ such that $(\\mathbf{x},y)$ is sampled according to: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbf{x}=(1-y)\\cdot\\mathbf{x}_{0}+y\\cdot\\mathbf{x}_{1},\\quad\\mathrm{with}\\quad y\\sim\\mathrm{Bernoulli}(0.5),\\quad\\mathbf{x}_{0}\\sim\\mathcal{N}_{0},\\quad\\mathbf{x}_{1}\\sim\\mathcal{N}_{1}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "The decision rule for optimal classification is determined by the likelihood ratio test. For a given sample $\\mathbf{x}$ , the log-likelihood ratio $\\Lambda(\\mathbf{x})$ is given by: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Lambda(\\mathbf{x})=\\log\\frac{P(\\mathbf{x}\\mid y=1)}{P(\\mathbf{x}\\mid y=0)}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since $\\mathbf{x}_{\\mathrm{0}}$ and $\\mathbf{x}_{1}$ are drawn from Gaussian distributions, their probability density functions are: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{P(\\mathbf{x}\\mid y=0)=\\displaystyle\\frac{1}{(2\\pi\\sigma^{2})^{d/2}}\\exp\\left(-\\frac{1}{2\\sigma^{2}}\\|\\mathbf{x}-{\\boldsymbol{\\mu}}_{1}\\|^{2}\\right),}}\\\\ {{P(\\mathbf{x}\\mid y=1)=\\displaystyle\\frac{1}{(2\\pi\\sigma^{2})^{d/2}}\\exp\\left(-\\frac{1}{2\\sigma^{2}}\\|\\mathbf{x}-{\\boldsymbol{\\mu}}_{2}\\|^{2}\\right).}}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Substituting these into the log-likelihood ratio, we have: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\Lambda(\\mathbf{x})=\\log\\frac{\\frac{1}{(2\\pi\\sigma^{2})^{d/2}}\\exp\\big(-\\frac{1}{2\\sigma^{2}}\\|\\mathbf{x}-\\mu_{2}\\|^{2}\\big)}{\\frac{1}{(2\\pi\\sigma^{2})^{d/2}}\\exp\\big(-\\frac{1}{2\\sigma^{2}}\\|\\mathbf{x}-\\mu_{1}\\|^{2}\\big)}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Simplifying, we obtain: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Lambda({\\bf x})=-\\frac{1}{2\\sigma^{2}}\\|{\\bf x}-\\mu_{2}\\|^{2}+\\frac{1}{2\\sigma^{2}}\\|{\\bf x}-\\mu_{1}\\|^{2}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Further simplification gives: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\Lambda({\\bf x})=\\frac{1}{2\\sigma^{2}}\\left(\\|{\\bf x}-\\mu_{1}\\|^{2}-\\|{\\bf x}-\\mu_{2}\\|^{2}\\right).\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Since the optimal decision threshold for balanced classes (i.e., $P(y=1)=P(y=0)=0.5)$ ) is $\\Lambda(\\mathbf{x})=0$ , we set: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\mathbf{x}-\\mu_{1}\\|^{2}-\\|\\mathbf{x}-\\mu_{2}\\|^{2}=0.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Expanding and rearranging, we derive: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\|\\mathbf{x}\\|^{2}-2\\mu_{1}\\mathbf{x}+\\mu_{1}^{2}-\\|\\mathbf{x}\\|^{2}+2\\mu_{2}\\mathbf{x}-\\mu_{2}^{2}=0.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "This simplifies to: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{2(\\mu_{2}-\\mu_{1})\\mathbf{x}+(\\mu_{1}^{2}-\\mu_{2}^{2})=0,}\\\\ &{\\quad2(\\mu_{2}-\\mu_{1})\\mathbf{x}=\\mu_{2}^{2}-\\mu_{1}^{2},}\\\\ &{\\quad\\quad\\mathbf{x}=\\frac{\\mu_{2}^{2}-\\mu_{1}^{2}}{2(\\mu_{2}-\\mu_{1})}.}\\end{array}\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Solving yields: ", "page_idx": 25}, {"type": "equation", "text": "$$\n\\mathbf{x}={\\frac{\\mu_{2}+\\mu_{1}}{2}}.\n$$", "text_format": "latex", "page_idx": 25}, {"type": "text", "text": "Therefore, the optimal decision boundary is $x=1.5$ , which is independent of the variance $\\sigma^{2}$ . This completes the proof. \u53e3 ", "page_idx": 25}, {"type": "text", "text": "Regarding efficiency, utilizing scaled data to train similar models with fewer training steps is proven in Section B. Additionally, rescaling each Gaussian distribution preserves their means, which aligns with the objectives of conventional distribution matching-based dataset distillation methods. These methods aim to distill data while maintaining the distributional properties, specifically their means. ", "page_idx": 25}, {"type": "text", "text": "F Detailed Methodology of RELA-D ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Recall that $\\phi_{\\pmb{\\theta}}:\\mathbb{R}^{d}\\,\\rightarrow\\,\\mathbb{R}^{n}$ . We then introduce a transport matrix $\\mathbf{W}\\,\\in\\,\\mathbb{R}^{m\\times n}$ and define the combined model as $\\mathbf{W}\\phi_{\\theta}(\\cdot)\\in\\mathbb{R}^{m}$ . During the training phase, the parameters W and $\\pmb{\\theta}$ are jointly optimized. However, as the dimension $m$ increases, the computational complexity of the optimization grows rapidly. To address this issue, we propose reducing the dimensionality of the target matrix $R_{\\mathbf{Y}}$ from $m$ to $n$ , where $n<m$ : ", "page_idx": 26}, {"type": "equation", "text": "$$\nR_{\\mathbf{Y}}^{\\prime}=V_{n}^{\\top}\\left(\\frac{R_{\\mathbf{Y}}-\\mu}{\\sigma}\\right)\\;\\mathrm{~s.t.~}\\;\\frac{1}{|R_{\\mathbf{Y}}|-1}\\left((R_{\\mathbf{Y}}-\\mu)^{\\top}(R_{\\mathbf{Y}}-\\mu)\\right)=V\\Lambda V^{\\top},\\;V_{n}=V[;,\\,n]\\,,\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\mu$ and $\\sigma$ denote the mean and standard deviation, respectively, of each column in the $R_{\\mathbf{Y}}$ .   \nPractically, we use batch PCA to perform the computation shown in (10), as illustrated in $\\mathrm{L}$ . ", "page_idx": 26}, {"type": "text", "text": "F.1 Proof for Ideal Properties of Prior Models ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "We aim to demonstrate that modern deep learning methods can effectively train models to serve as robust prior models by extracting sufficient information from samples as representations. ", "page_idx": 26}, {"type": "text", "text": "Therefore, we poist the existence of a prior model $\\xi$ capable of losslessly extracting the information of samples $D_{\\mathcal{X}}$ when trained using the InfoNCE loss [47], a method prevalently employed in contemporary deep learning algorithms [12]. ", "page_idx": 26}, {"type": "text", "text": "Proof. To demonstrate that an encoder $\\xi$ trained with the InfoNCE loss preserves all information from the input data $D_{\\mathcal{X}}$ , we proceed as follows. ", "page_idx": 26}, {"type": "text", "text": "1. Definitions and Setup ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "Let $\\mathcal{X}$ denote the input data space with data distribution $p_{\\mathrm{data}}(x)$ . Let $p_{\\mathrm{pos}}(x,x^{+})$ denote the distribution of positive pairs, typically generated via data augmentation. The encoder $\\boldsymbol{\\xi}:\\boldsymbol{\\mathcal{X}}\\rightarrow\\mathbb{R}^{d}$ maps inputs to $d$ -dimensional representations. The similarity function $q:\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\to\\mathbb{R}$ (e.g., dot product) measures similarity between representations. Given a positive pair $(x,x^{+})$ , let $\\{x_{i}^{-}\\}_{i=1}^{\\overline{N}}$ be $N$ negative samples drawn i.i.d. from $p_{\\mathrm{data}}(x)$ . The InfoNCE loss is defined as: ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{L}_{\\mathrm{InfoNCE}}(\\xi,q)=-\\mathbb{E}_{(x,x^{+})\\sim p_{\\mathrm{pos}}}\\left[\\log\\frac{e^{q(\\xi(x),\\xi(x^{+}))/\\tau}}{\\sum_{i=1}^{N}e^{q(\\xi(x),\\xi(x_{i}^{-}))/\\tau}}\\right]\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $\\tau>0$ is a temperature parameter. ", "page_idx": 26}, {"type": "text", "text": "2. InfoNCE as a Mutual Information Lower Bound ", "text_level": 1, "page_idx": 26}, {"type": "text", "text": "The InfoNCE objective serves as a lower bound to the mutual information between representations of positive pairs: ", "page_idx": 26}, {"type": "equation", "text": "$$\nI(\\xi(X);\\xi(X^{+}))\\ge\\mathcal{I}(\\xi,q)\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where ", "page_idx": 26}, {"type": "equation", "text": "$$\n\\mathcal{I}(\\xi,q)=\\mathbb{E}_{(x,x^{+})\\sim p_{\\mathrm{pos}}}\\left[q(\\xi(x),\\xi(x^{+}))-\\tau\\cdot\\log\\left(\\sum_{i=1}^{N}e^{q(\\xi(x),\\xi(x_{i}^{-}))/\\tau}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "As the number of negative samples $N\\rightarrow\\infty$ , this bound becomes tight, approaching the true mutual information $I(\\xi(X);\\xi(X^{+}))$ . 3. Optimal Encoder and Similarity Function Let $(\\xi^{*},q^{*})$ denote the optimal encoder and similarity function that maximize $\\mathcal{I}(\\xi,q)$ . Under the assumption of infinite negative samples and an expressive similarity function, the optimal similarity satisfies: ", "page_idx": 26}, {"type": "equation", "text": "$$\nq^{*}(\\xi^{*}(x),\\xi^{*}(x^{+}))=\\log\\frac{p_{\\mathrm{pos}}(x,x^{+})}{p_{\\mathrm{data}}(x)p_{\\mathrm{data}}(x^{+})}+C\n$$", "text_format": "latex", "page_idx": 26}, {"type": "text", "text": "where $C$ is a constant independent of $(x,x^{+})$ . This aligns $q^{*}$ with the pointwise mutual information (PMI) between $x$ and $x^{+}$ . 4. Injectivity through Mutual Information Maximization Maximizing $I(\\xi(X);\\xi(X^{+}))$ encourages the encoder $\\xi$ to capture as much information about $X$ as possible. To ensure injectivity: ", "page_idx": 26}, {"type": "text", "text": "\u2022 Sufficient Dimensionality: The representation dimension $d$ must be at least as large as the intrinsic dimensionality of $\\mathcal{X}$ . ", "page_idx": 26}, {"type": "table", "img_path": "UPxmISfNCO/tmp/9cc856fbe6ff34e2f64640efea5f365fcef1630db8914edbc0c5ec8d27775a3b.jpg", "table_caption": ["Table 4: Notations used in this section. "], "table_footnote": [], "page_idx": 27}, {"type": "text", "text": "\u2022 Expressive Architecture: The encoder $\\xi$ should be sufficiently expressive, potentially utilizing architectural constraints (e.g., invertible networks) to promote injectivity. ", "page_idx": 27}, {"type": "text", "text": "Under these conditions, maximizing mutual information implies that $\\xi^{*}$ approximates an injective mapping on the support of $p_{\\mathrm{data}}(x)$ , i.e., $\\xi^{*}(x)=\\xi^{*}(x^{\\prime})\\Rightarrow x\\stackrel{\\cdot}{=}x^{\\prime}$ almost surely. 5. Existence of the Inverse Mapping Given that $\\xi^{*}$ is injective, there exists a deterministic inverse mapping $g:\\mathbb{R}^{d}\\rightarrow\\mathcal{X}$ such that: ", "page_idx": 27}, {"type": "equation", "text": "$$\ng(\\xi^{*}(x))=x\\quad{\\mathrm{for~all~}}x\\in\\mathcal{X}\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "This mapping $g$ can be constructed as the inverse of $\\xi^{*}$ on its image: ", "page_idx": 27}, {"type": "equation", "text": "$$\ng(z)=\\xi^{*-1}(z)\\quad{\\mathrm{where~}}z\\in\\xi^{*}(\\chi)\n$$", "text_format": "latex", "page_idx": 27}, {"type": "text", "text": "G Analysis of Different Self-Supervised Learning Methods ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "We refer to the primary theoretical results for existing self-supervised learning methods as presented in [59], where all methods are unified under a simple and cohesive framework, detailed below. Specifically, UniGrad [59] demonstrates that most self-supervised learning methods can be unified by analyzing their gradients. ", "page_idx": 27}, {"type": "text", "text": "G.1 A Unified Framework for SSL ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "A typical self-supervised learning framework employs a siamese network with two branches: an online branch and a target branch. The target branch serves as the training target for the online branch. ", "page_idx": 27}, {"type": "text", "text": "Given an input image $x$ , two augmented views $x_{1}$ and $x_{2}$ are generated, serving as inputs for the two branches. The encoder $f(\\cdot)$ extracts representations $u_{i}\\triangleq f(x_{i})$ for $i=1,2$ from these views. ", "page_idx": 27}, {"type": "text", "text": "Table 4 details the notations used. $u_{1}$ and $u_{2}$ denote the current training samples, while $v$ denotes unspecified samples. $u_{1}^{o}$ and $v^{o}$ are representations from the online branch. Three types of target branches are widely used: 1) weight-sharing with the online branch $\\boldsymbol{u}_{2}^{s}$ and $v^{s}$ ); 2) weight-sharing but detached from gradient back-propagation $\\mathit{\\check{u}}_{2}^{d}$ and $v^{d}$ ); 3) a momentum encoder updated from the online branch ( $\\boldsymbol{u}_{2}^{m}$ and $v^{m}$ ). If unspecified, $u_{2}^{t}$ and $v^{t}$ are used. A symmetric loss is applied to the two augmented views, as described in [14]. ", "page_idx": 27}, {"type": "text", "text": "$\\nu$ represents the sample set in the current training step. Methods vary in constructing this set: $\\nu_{\\mathrm{batch}}$ includes all samples from the current batch, $\\nu_{\\mathrm{bank}}$ uses a memory bank storing previous samples, and $\\mathcal{V}_{\\infty}$ includes all previous samples, potentially larger than a memory bank. ", "page_idx": 27}, {"type": "text", "text": "G.2 Contrastive Learning Methods ", "text_level": 1, "page_idx": 27}, {"type": "text", "text": "Contrastive learning relies on negative samples to prevent representational collapse and enhance performance. Positive samples are derived from different views of the same image, while negative ", "page_idx": 27}, {"type": "text", "text": "samples come from other images. The goal is to attract positive pairs and repel negative pairs, typically using the InfoNCE loss [47]: ", "page_idx": 28}, {"type": "equation", "text": "$$\nL=\\underset{u_{1},u_{2}}{\\mathbb{E}}\\bigg[-\\log\\frac{\\exp\\left(\\cos(u_{1}^{o},u_{2}^{t})/\\tau\\right)}{\\sum_{v^{t}\\in\\mathcal{V}}\\exp\\left(\\cos(u_{1}^{o},v^{t})/\\tau\\right)}\\bigg],\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where $\\cos(\\cdot)$ denotes cosine similarity, and $\\tau$ is the temperature hyper-parameter. This formulation can be adapted for various methods, discussed below. ", "page_idx": 28}, {"type": "text", "text": "MoCo [25, 13]. MoCo uses a momentum encoder for the target branch and a memory bank for storing previous representations. Negative samples are drawn from this memory bank. The gradient for sample $u_{1}^{o}$ is: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{\\partial L}{\\partial u_{1}^{o}}=\\frac{1}{\\tau N}\\bigg(-u_{2}^{m}+\\sum_{v^{m}\\in\\mathcal{V}_{\\mathrm{bank}}}s_{v}v^{m}\\bigg),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "where sv = ym\u2208eVxbpa n(kc oesx(pu 1(,cvos()u/1o\u03c4,)ym)/\u03c4) and N is the number of samples in the batch. ", "page_idx": 28}, {"type": "text", "text": "SimCLR [12]. SimCLR shares weights between the target and online branches and does not stop back-propagation. It uses all representations from other images in the batch as negative samples. The gradient is: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial{\\cal L}}{\\partial u_{1}^{o}}=\\!\\frac{1}{\\tau N}\\Bigg(-u_{2}^{s}+\\sum_{v^{s}\\in\\mathcal{V}_{\\mathrm{batch}}\\backslash u_{1}^{o}}s_{v}v^{s}\\Bigg)}\\\\ {\\displaystyle\\qquad+\\,\\frac{1}{\\tau N}\\Bigg(-u_{2}^{s}+\\sum_{v^{s}\\in\\mathcal{V}_{\\mathrm{batch}}\\backslash u_{1}^{o}}t_{v}v^{s}\\Bigg),}\\end{array}\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "swehceorne $\\begin{array}{r}{t_{v}=\\frac{\\exp\\left(\\cos\\left(v^{s},u_{1}^{o}\\right)/\\tau\\right)}{\\sum_{y^{s}\\in\\mathcal{V}_{\\mathrm{batch}}\\backslash v^{s}}\\exp\\left(\\cos\\left(v^{s},y^{s}\\right)/\\tau\\right)}}\\end{array}$ . If the gradient through the target branch is stopped, the d term vanishes. ", "page_idx": 28}, {"type": "text", "text": "Unified Gradient. The gradient for these methods can be unified as: ", "page_idx": 28}, {"type": "equation", "text": "$$\n\\frac{\\partial L}{\\partial u_{1}^{o}}=\\frac{1}{\\tau N}\\bigg(-u_{2}^{t}+\\sum_{v^{t}\\in\\mathcal{V}}s_{v}v^{t}\\bigg),\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "comprising a weighted sum of positive and negative samples. The term $-u_{2}^{t}$ pulls positive samples together, while $\\bar{\\sum}_{v^{t}\\in\\mathcal{V}}\\,s_{v}v^{t}$ pushes negative samples apart. The main difference between methods lies in the target branch used and the construction of the contrastive sample set $\\nu$ . ", "page_idx": 28}, {"type": "text", "text": "G.3 Asymmetric Network Methods ", "text_level": 1, "page_idx": 28}, {"type": "text", "text": "Asymmetric network methods learn representations by maximizing the similarity of positive pairs without using negative samples. These methods require symmetry-breaking network designs to avoid representational collapse. A predictor $h(\\cdot)$ is appended after the online branch, and the gradient to the target branch is stopped. The objective function is: ", "page_idx": 28}, {"type": "equation", "text": "$$\nL=\\underset{u_{1},u_{2}}{\\mathbb{E}}\\bigg[-\\cos(h(u_{1}^{o}),u_{2}^{t})\\bigg].\n$$", "text_format": "latex", "page_idx": 28}, {"type": "text", "text": "Relation to BYOL [30]. BYOL uses a momentum encoder for the target branch, i.e., $u_{2}^{t}=u_{2}^{m}$ in Eq.(15). ", "page_idx": 28}, {"type": "text", "text": "Relation to Simsiam [14]. Simsiam shows that a momentum encoder is unnecessary and only applies the stop-gradient operation to the target branch, i.e., $u_{2}^{t}=u_{2}^{d}$ in Eq.(15). ", "page_idx": 28}, {"type": "text", "text": "Unified Gradient. Despite the performance of asymmetric network methods, the avoidance of collapse solutions is not well understood. DirectPred [60] explores this by studying training dynamics and proposes an analytical solution for the predictor $h(\\cdot)$ . ", "page_idx": 28}, {"type": "text", "text": "DirectPred formulates the predictor as $h(v)=W_{h}v$ , with $W_{h}$ calculated based on the correlation matrix $\\mathbb{E}_{v}(v v^{T})$ . The correlation matrix, $F$ , is computed as the moving average for each batch: ", "page_idx": 28}, {"type": "text", "text": "$\\begin{array}{r}{F\\triangleq\\sum_{v_{\\ast}^{o}\\in\\mathcal{V}_{\\infty}}\\rho_{v}v^{o}v_{\\ast}^{o T}}\\end{array}$ , where $\\rho_{v}$ is the moving average weight. Decomposing $F$ into eigenvalues $\\Lambda_{F}$ and eigenvectors $U$ , $W_{h}$ is: ", "page_idx": 29}, {"type": "equation", "text": "$$\nW_{h}=U\\Lambda_{h}U^{T},\\;\\;\\Lambda_{h}=\\Lambda_{F}^{1/2}+\\epsilon\\lambda_{m a x}I,\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\lambda_{m a x}$ is the max eigenvalue of $F$ and $\\epsilon$ is a hyper-parameter to boost small eigenvalues. DirectPred also derives the gradient: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{\\partial L}{\\partial u_{1}^{o}}=\\frac{1}{||W_{h}u_{1}^{o}||_{2}N}\\bigg(-W_{h}^{T}u_{2}^{t}+\\lambda\\sum_{v^{o}\\in\\mathcal{V}_{\\infty}}(\\rho_{v}u_{1}^{o T}v^{o})v^{o}\\bigg),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $-W_{h}^{T}u_{2}^{t}$ and $\\begin{array}{r}{\\sum_{v^{o}\\in\\mathcal{V}_{\\infty}}(\\rho_{v}u_{1}^{o T}v^{o})v^{o}}\\end{array}$ act as positive and negative gradients respectively, and $\\begin{array}{r}{\\lambda=\\frac{u_{1}^{o T}W_{h}^{T}u_{2}^{t}}{u_{1}^{o T}(F+\\epsilon^{2}I)u_{1}^{o}}}\\end{array}$ is a balance factor. ", "page_idx": 29}, {"type": "text", "text": "Though negative samples are absent in the loss function, they emerge from the predictor network\u2019s optimization. The eigenspace of the predictor $W_{h}$ aligns with the feature correlation matrix $F$ , encoding its information. During back-propagation, this encoded information functions as a negative gradient, influencing the optimization direction. ", "page_idx": 29}, {"type": "text", "text": "G.4 Feature Decorrelation Methods ", "text_level": 1, "page_idx": 29}, {"type": "text", "text": "Feature decorrelation methods have recently emerged as a novel approach in self-supervised learning.   \nThese methods aim to reduce redundancy among different feature dimensions to prevent collapse.   \nVarious loss functions have been proposed for this purpose. We examine their relations below. ", "page_idx": 29}, {"type": "text", "text": "Relation to Barlow Twins [69]. Barlow Twins employs the following loss function: ", "page_idx": 29}, {"type": "equation", "text": "$$\nL=\\sum_{i=1}^{C}\\left(W_{i i}-1\\right)^{2}+\\lambda\\sum_{i=1}^{C}\\sum_{j\\neq i}W_{i j}^{2},\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\begin{array}{r}{W=\\frac{1}{N}\\sum_{v_{1}^{o},v_{2}^{s}\\in\\mathcal{V}_{\\mathrm{batch}}}v_{1}^{o}v_{2}^{s T}}\\end{array}$ is a cross-correlation matrix, $C$ denotes the number of feature dimensions, and $\\lambda$ is a balancing hyper-parameter. The diagonal elements of $W$ are encouraged to be close to 1, while the off-diagonal elements are forced towards 0. ", "page_idx": 29}, {"type": "text", "text": "Despite appearing different, Eq. (18) operates similarly to previous methods from a gradient perspective, calculated as: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\frac{\\partial L}{\\partial u_{1}^{o}}=\\frac{2}{N}\\left(-A u_{2}^{s}+\\lambda\\sum_{v_{1}^{o},v_{2}^{s}\\in\\mathcal{V}_{\\mathrm{batch}}}\\frac{u_{2}^{s T}v_{2}^{s}}{N}v_{1}^{o}\\right),\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $A=I-(1-\\lambda)W_{\\mathrm{diag}}$ and $(W_{\\mathrm{diag}})_{i j}=\\delta_{i j}W_{i j}$ is the diagonal matrix of $W$ . Barlow Twins applies batch normalization instead of $\\ell_{2}$ normalization to the representation $v$ . ", "page_idx": 29}, {"type": "text", "text": "Relation to VICReg [3]. VICReg modifies Barlow Twins with the following loss function: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{l}{{\\displaystyle{\\cal L}=\\frac{1}{N}\\sum_{\\substack{v_{1}^{o},v_{2}^{s}\\in\\mathcal{V}_{\\mathrm{batch}}}}||v_{1}^{o}-v_{2}^{s}||_{2}^{2}+\\frac{\\lambda_{1}}{C}\\sum_{i=1}^{C}\\sum_{j\\neq i}^{C}W_{i j}^{\\prime2}}}\\\\ {~~~~~~+\\displaystyle\\frac{\\lambda_{2}}{C}\\sum_{i=1}^{C}\\operatorname*{max}(0,\\gamma-\\mathrm{std}(v_{1}^{o})_{i}),}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\begin{array}{r}{W^{\\prime}=\\frac{1}{N-1}\\sum_{v_{1}^{o}\\in\\mathcal{V}_{\\mathrm{batch}}}(v_{1}^{o}-\\bar{v}_{1}^{o})(v_{1}^{o}-\\bar{v}_{1}^{o})^{T}}\\end{array}$ is the covariance matrix of the same view, $\\operatorname{std}(v)_{i}$ denotes the standard deviation of the $i$ -th channel of $v,\\gamma$ is a constant target value, and $\\lambda_{1},\\lambda_{2}$ are balancing weights. ", "page_idx": 29}, {"type": "text", "text": "The gradient is derived as follows: ", "page_idx": 29}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\displaystyle\\frac{\\partial L}{\\partial\\boldsymbol{u}_{1}^{o}}=\\frac{2}{N}\\left(-\\boldsymbol{u}_{2}^{s}+\\lambda\\sum_{\\boldsymbol{v}_{1}^{o}\\in\\mathcal{V}_{\\mathrm{batch}}}\\frac{\\tilde{\\boldsymbol{u}}_{1}^{o T}\\tilde{\\boldsymbol{v}}_{1}^{o}}{N}\\tilde{\\boldsymbol{v}}_{1}^{o}\\right)}\\\\ &{\\quad\\quad\\quad+\\displaystyle\\frac{2\\lambda}{N}\\left(\\frac{1}{\\lambda}\\boldsymbol{u}_{1}^{o}-\\boldsymbol{B}\\tilde{\\boldsymbol{u}}_{1}^{o}\\right)\\!,}\\end{array}\n$$", "text_format": "latex", "page_idx": 29}, {"type": "text", "text": "where $\\tilde{v}~=~v\\,-\\,\\bar{v}$ is the de-centered sample, $\\begin{array}{r}{\\lambda\\ =\\ \\frac{2\\lambda_{1}N^{2}}{c(N-1)^{2}}}\\end{array}$ , and $\\begin{array}{r}{B\\ =\\ \\frac{N}{c\\lambda(N-1)}(2\\lambda_{1}W_{\\mathrm{diag}}^{\\prime}\\ +}\\end{array}$ $\\begin{array}{r}{\\frac{\\lambda_{2}}{2}\\mathrm{diag}({\\bf1}(\\gamma-\\mathrm{std}(v_{1}^{o})\\,>\\,0)\\odot\\mathrm{std}(v_{1}^{o})))}\\end{array}$ . Here, $\\operatorname{diag}(x)$ is a matrix with the vector $x$ on its diagonal, $\\mathbf{1}(\\cdot)$ is the indicator function, and $\\oslash$ denotes element-wise division. ", "page_idx": 30}, {"type": "text", "text": "VICReg does not normalize $v$ ; instead, it uses de-centering and a standard deviation term in the loss function. ", "page_idx": 30}, {"type": "text", "text": "Unified Gradient. Given the equivalence of $v^{s}$ and $v^{o}$ , the gradient for feature decorrelation methods can be unified as: ", "page_idx": 30}, {"type": "equation", "text": "$$\n\\frac{\\partial L}{\\partial u_{1}^{o}}=\\frac{2}{N}\\left(-u_{2}^{t}+\\lambda\\sum_{v^{o}\\in\\mathcal{V}_{\\mathrm{batch}}}\\frac{u^{o T}v^{o}}{N}v_{1}^{o}\\right),\n$$", "text_format": "latex", "page_idx": 30}, {"type": "text", "text": "where \u2212ut2 is the positive gradient, and  vo\u2208Vbatch u $\\sum_{v^{o}\\in\\mathcal{V}_{\\mathrm{batch}}}\\left(\\frac{\\boldsymbol{u}^{o T}\\boldsymbol{v}^{o}}{N}\\right)\\boldsymbol{v}_{1}^{o}$ is the negative gradient. $\\lambda$ is a balancing factor. The difference between methods lies in the subscript for the negative coefficient. Feature decorrelation methods function similarly to other self-supervised methods, with the positive and negative gradients derived from the diagonal and off-diagonal elements of the correlation matrix. ", "page_idx": 30}, {"type": "text", "text": "H Budget of RELA for Data Synthesis ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "While training on the distilled dataset is both efficient and effective, the distillation process in optimization-based approaches [11, 72, 65] is computationally intensive [18, 56], often exceeding the computational load of training on the full dataset. ", "page_idx": 30}, {"type": "text", "text": "In contrast, the synthetic data generated by our RELA framework requires a budget less than that of training for a single epoch (c.f. Section 4.1). This is because the synthesis budget of our RELA is equivalent to performing inference over the entire dataset $D_{X}$ . Consequently, this computational expense is negligible given that training epochs typically number around 100. ", "page_idx": 30}, {"type": "text", "text": "I RELA in Labeled Dataset Distillation and Human-Supervised Learning ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "We apply our RELA in labeled dataset distillation and human-supervised learning. ", "page_idx": 30}, {"type": "text", "text": "I.1 Experimental Setup ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Datasets and Neural Network Architectures. We conduct experiments on datasets of varying scales and resolutions. ", "page_idx": 30}, {"type": "text", "text": "\u2022 Small-scale: We evaluate on CIFAR-10 $\\mathrm{32\\times32})$ [35] and CIFAR-100 $32\\times32)$ [34].   \n\u2022 Large-scale: We utilize Tiny-ImageNet $(64\\times64)$ [36] and ImageNet-1K $(224\\times224)$ [20]. ", "page_idx": 30}, {"type": "text", "text": "Consistent with previous works on dataset distillation [67, 73, 24], we use ConvNet [24] and ResNet{18,50} [26] as our backbone networks across all datasets. Specifically, Conv-3 is employed for CIFAR-10/100, and Conv-4 for Tiny-ImageNet and ImageNet-1K. ", "page_idx": 30}, {"type": "text", "text": "Baselines. We compare our method with several SOTA distillation methods capable of scaling to large high-resolution datasets, including G-VBSM [53], $\\mathrm{SRe^{2}L}$ [67], and RDED [56]. To the best of our knowledge, $\\mathrm{SRe^{2}L}$ , G-VBSM, and RDED are the only published works that efficiently scale to datasets of any size, making them our closest baselines. All distilled datasets synthesized from these baselines undergo the same post-training process. Results are reported in Table 5. For our RELA, the prior models used for distillation are identical to the pre-trained models employed in the baseline methods. ", "page_idx": 30}, {"type": "text", "text": "I.2 Main Results ", "text_level": 1, "page_idx": 30}, {"type": "text", "text": "Table 5 demonstrates the superiority of our RELA, despite incorporating a zero-cost sample synthesis process. ", "page_idx": 30}, {"type": "table", "img_path": "UPxmISfNCO/tmp/ace7d9da54f8920d5343833aadda0ccc032dc36245c2fd5da0748c89d3728d75.jpg", "table_caption": ["Table 5: Comparison with SOTA Baseline Dataset Distillation Methods. In the table, bold indicates the best result. IPC refers to the number of Images Per Class for distilled datasets. "], "table_footnote": [], "page_idx": 31}, {"type": "text", "text": "J RELA Algorithm ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "Algorithm 1 Adaptive Loss Weighting Algorithm for RELA and Self-Supervised Learning ", "page_idx": 31}, {"type": "text", "text": "Require: Number of training steps $T$ , Initial $\\ell_{s}=2.0$ , Initial $\\ell_{f}=1.0$ , Initial $\\lambda=1$ 1: for each training step $t=1$ to $T$ do ", "page_idx": 31}, {"type": "text", "text": "2: if $\\lambda=1$ then   \n3: $\\ell_{c}\\gets$ Value of $\\mathcal{L}_{\\mathrm{RELA}}$ {Retrieve the current loss value from the optimization process}   \n4: $\\ell_{f}\\gets0.999\\times\\ell_{f}+0.001\\times\\ell_{c}$ {Calculate the short-term loss}   \n5: $\\ell_{s}\\gets0.99\\times\\ell_{s}+0.01\\times\\ell_{f}$ {Calculate the long-term loss}   \n6: end if   \n7: if $\\exp(-\\operatorname*{max}\\{\\ell_{s}-\\ell_{f},0\\})\\geq0.995$ then   \n8: $\\lambda\\leftarrow0$ {RELA learning is converged and over}   \n9: end if   \n10: $\\mathcal{L}\\gets\\lambda\\cdot\\mathcal{L}_{\\mathrm{RELA}}+(1-\\lambda)\\cdot\\mathcal{L}_{\\mathrm{SSL}}$ {Control the RELA and the SSL states using $\\lambda$ }   \n11: end for ", "page_idx": 31}, {"type": "text", "text": "In essence, this algorithm is designed to detect the convergence of RELA. Upon convergence, it transitions to the original algorithm for self-supervised learning. This implicitly segments the learning procedure into two phases: the fast stage (RELA) and the slow stage (SSL). Moreover, Figure 5 depicts the dynamics of the training process. ", "page_idx": 31}, {"type": "image", "img_path": "UPxmISfNCO/tmp/60ff5652a55d877a933e2c599e00cee19264c5fdab6c8bb8bc0807a2a7b9e6e6.jpg", "img_caption": ["(a) RELA with CF10-T as the prior model (b) RELA with Rand. as the prior model Figure 5: Comparison of training dynamics between RELA and the original (Orig.) BYOL algorithm. "], "img_footnote": [], "page_idx": 31}, {"type": "text", "text": "K Experimental Details ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "K.1 Detailed Setup for Experiments in Section 3.2 ", "text_level": 1, "page_idx": 31}, {"type": "text", "text": "We show the detailed setup for the experiments in Section 3.2 in Tables 6 and 7. ", "page_idx": 31}, {"type": "table", "img_path": "UPxmISfNCO/tmp/c0154fa2b8c48f11e85c1d87b063698ddee236b7602f0b38b830293d3e9db263.jpg", "table_caption": ["Table 6: Architecture of the simple neural network model. "], "table_footnote": [], "page_idx": 32}, {"type": "table", "img_path": "UPxmISfNCO/tmp/b2e3fb9b865e5fce4590e87f3e5a690e52d3060023c53f6cf5007140ee81edc7.jpg", "table_caption": ["Table 7: Optimization parameters for the simple neural network model. "], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "K.2 Detailed Setup for Experiments in Section 5 ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Training details for self-supervised learning methods. We show the details of training in Table 8. ", "page_idx": 32}, {"type": "text", "text": "Table 8: Training parameters and optimizer settings for self-supervised learning methods. ", "page_idx": 32}, {"type": "table", "img_path": "UPxmISfNCO/tmp/4c5875d1ab4b12be59a0670d2bd258c823d1b306a98f8ac1ea09417ade444b87.jpg", "table_caption": [], "table_footnote": [], "page_idx": 32}, {"type": "text", "text": "Linear evaluation details. We show the details of training the linear model in Table 9. ", "page_idx": 32}, {"type": "text", "text": "L Batch PCA Reduction ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "To begin, we provide a full and rigorous mathematical framework for Principal Component Analysis (PCA). ", "page_idx": 32}, {"type": "text", "text": "L.1 Principal Component Analysis ", "text_level": 1, "page_idx": 32}, {"type": "text", "text": "Given a data matrix $\\mathbf{Y}\\in\\mathbb{R}^{n\\times d}$ and the desired number of components $k$ , where $k\\leq d$ , PCA aims to extract a reduced data matrix $\\mathbf{Y}_{\\mathrm{reduced}}\\in\\mathbb{R}^{n\\times k}$ that retains the maximum variance from the original dataset. ", "page_idx": 32}, {"type": "text", "text": "First, the data needs to be centered by subtracting the mean of each feature. This can be mathematically represented as: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbf{Y}_{\\mathrm{centered}}=\\mathbf{Y}-{\\frac{1}{n}}\\mathbf{1}_{n}\\mathbf{1}_{n}^{T}\\mathbf{Y}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "text", "text": "where ${\\bf1}_{n}$ is a column vector of ones with length $n$ . Next, the covariance matrix of the centered data is computed: ", "page_idx": 32}, {"type": "equation", "text": "$$\n\\mathbf{C}={\\frac{1}{n-1}}\\mathbf{Y}_{\\mathrm{centered}}^{T}\\mathbf{Y}_{\\mathrm{centered}}\n$$", "text_format": "latex", "page_idx": 32}, {"type": "table", "img_path": "UPxmISfNCO/tmp/59f89b0ad3efb55a0fc533d8dab1e36c95fc2764a97d959a60efc05e00a59254.jpg", "table_caption": ["Table 9: Linear evaluation parameters for various datasets. "], "table_footnote": [], "page_idx": 33}, {"type": "text", "text": "To identify the principal components, eigendecomposition is performed on the covariance matrix: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbf{C}=\\mathbf{V}\\mathbf{A}\\mathbf{V}^{T}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\mathbf{V}$ is the matrix of eigenvectors and $\\pmb{\\Lambda}$ is a diagonal matrix of eigenvalues. The eigenvectors are then sorted in descending order based on their corresponding eigenvalues. The top $k$ eigenvectors are selected to form the projection matrix: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbf{W}=[\\mathbf{v}_{1},\\mathbf{v}_{2},\\ldots,\\mathbf{v}_{k}]\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "where $\\mathbf{v}_{i}$ represents the $i$ -th eigenvector. Finally, the centered data is projected onto the new subspace: ", "page_idx": 33}, {"type": "equation", "text": "$$\n\\mathbf{Y}_{\\mathrm{reduced}}=\\mathbf{Y}_{\\mathrm{centered}}\\mathbf{W}\n$$", "text_format": "latex", "page_idx": 33}, {"type": "text", "text": "The resulting reduced data matrix $\\mathbf{Y}_{\\mathrm{reduced}}$ contains the principal components of the original data. Each principal component is a linear combination of the original features, designed to capture as much variance as possible in the reduced space. ", "page_idx": 33}, {"type": "text", "text": "Algorithm 2 Batch PCA Reduction ", "text_level": 1, "page_idx": 33}, {"type": "text", "text": "Require: Data matrix $\\mathbf{Y}\\in\\mathbb{R}^{n\\times d}$ , Number of components $k$   \nEnsure: Reduced data matrix $\\mathbf{Y}_{\\mathrm{reduced}}\\in\\mathbb{R}^{n\\times k}$   \n1: $n\\leftarrow$ number of rows in $\\mathbf{Y}$   \n2: $m\\leftarrow$ BATCHSIZE {Pre-set Batch size}   \n3: $\\begin{array}{r}{\\mu\\leftarrow\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{Y}_{i,}}\\end{array}$ \u00b7 {Compute the mean of $\\mathbf{Y}$ }   \n4: Ycentered $\\gets\\mathbf{Y}-\\mu$ {Center the data}   \n5: $\\Sigma\\leftarrow{\\bf0}_{d\\times d}$ {Initialize the covariance matrix}   \n6: for $i=0$ to $n$ step $m$ do   \n7: $j\\gets\\operatorname*{min}(i+m,n)$   \n8: $\\mathbf{B}\\gets\\mathbf{Y}_{\\mathrm{centered}}[i:j,\\cdot]$   \n9: $\\Sigma\\leftarrow\\Sigma+\\mathbf{B}^{\\top}\\mathbf{B}$ {Update the covariance matrix}   \n10: end for   \n11: $\\textstyle\\Sigma\\leftarrow{\\frac{\\Sigma}{n-1}}$ {Normalize the covariance matrix}   \n12: $\\mathbf{V}\\gets$ eigenvectors of $\\Sigma$ corresponding to the largest $k$ eigenvalues   \n13: $\\mathbf{Y}_{\\mathrm{reduced}}\\leftarrow\\mathbf{0}_{n\\times k}$ {Initialize the reduced data matrix}   \n14: for $i=0$ to $n$ step $m$ do   \n15: $j\\gets\\operatorname*{min}(i+m,n)$   \n16: $\\mathbf{B}\\gets\\mathbf{Y}_{\\mathrm{centered}}[i:j,\\cdot]$   \n17: $\\mathbf{Y}_{\\mathrm{reduced}}[i:j,\\cdot]\\leftarrow\\mathbf{B}\\mathbf{\\dot{V}}$   \n18: end for   \n19: return Yreduced ", "page_idx": 33}, {"type": "text", "text": "Then we prove that the batch PCA we utilized here is exactly same to the standard PCA. ", "page_idx": 33}, {"type": "text", "text": "Proof. To prove that the Batch PCA algorithm achieves the same result as the standard PCA, we need to show that the covariance matrix and the reduced data matrix computed by the Batch PCA are equivalent to those computed by the standard PCA. ", "page_idx": 33}, {"type": "text", "text": "First, let\u2019s show that the covariance matrix $\\Sigma$ computed in the Batch PCA is equivalent to the covariance matrix C in the standard PCA. ", "page_idx": 33}, {"type": "text", "text": "In the standard PCA, the covariance matrix is computed as: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbf{C}={\\frac{1}{n-1}}\\mathbf{Y}_{\\mathrm{centered}}^{T}\\mathbf{Y}_{\\mathrm{centered}}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "In the Batch PCA, the covariance matrix is computed as: ", "page_idx": 34}, {"type": "equation", "text": "$$\n{\\begin{array}{r l}&{{\\boldsymbol{\\Sigma}}={\\cfrac{1}{n-1}}\\sum_{i=0}^{n-1}\\mathbf{B}_{i}^{T}\\mathbf{B}_{i}}\\\\ &{\\,\\,\\,\\,={\\cfrac{1}{n-1}}\\left(\\mathbf{B}_{0}^{T}\\mathbf{B}_{0}+\\mathbf{B}_{1}^{T}\\mathbf{B}_{1}+\\dots+\\mathbf{B}_{b-1}^{T}\\mathbf{B}_{b-1}\\right)}\\\\ &{\\,\\,\\,\\,={\\cfrac{1}{n-1}}\\mathbf{Y}_{{\\mathrm{centered}}}^{T}\\mathbf{Y}_{{\\mathrm{centered}}}}\\end{array}}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $\\mathbf{B}_{i}$ is the $i$ -th batch of the centered data matrix $\\mathbf{Y}_{\\mathrm{centered}}$ , and $b$ is the number of batches. ", "page_idx": 34}, {"type": "text", "text": "Therefore, the covariance matrix computed by the Batch PCA is equivalent to the covariance matrix computed by the standard PCA. ", "page_idx": 34}, {"type": "text", "text": "Next, let\u2019s show that the reduced data matrix $\\mathbf{Y}_{\\mathrm{reduced}}$ computed in the Batch PCA is equivalent to the one computed in the standard PCA. ", "page_idx": 34}, {"type": "text", "text": "In the standard PCA, the reduced data matrix is computed as: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\mathbf{Y}_{\\mathrm{reduced}}=\\mathbf{Y}_{\\mathrm{centered}}\\mathbf{W}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where W is the matrix of the top $k$ eigenvectors of the covariance matrix. ", "page_idx": 34}, {"type": "text", "text": "In the Batch PCA, the reduced data matrix is computed as: ", "page_idx": 34}, {"type": "equation", "text": "$$\n\\begin{array}{r l}&{\\mathbf{Y}_{\\mathrm{reduced}}=\\left[\\begin{array}{c}{\\mathbf{B}_{0}\\mathbf{V}}\\\\ {\\mathbf{B}_{1}\\mathbf{V}}\\\\ {\\vdots}\\\\ {\\mathbf{B}_{b-1}\\mathbf{V}}\\end{array}\\right]}\\\\ &{=\\left[\\begin{array}{c}{\\mathbf{Y}_{\\mathrm{centered}}[0:m,\\cdot]}\\\\ {\\mathbf{Y}_{\\mathrm{centered}}[m:2m,\\cdot]}\\\\ {\\vdots}\\\\ {\\mathbf{Y}_{\\mathrm{centered}}[(b-1)m:n,\\cdot]}\\end{array}\\right]\\mathbf{V}}\\\\ &{=\\mathbf{Y}_{\\mathrm{centered}}\\mathbf{V}}\\end{array}\n$$", "text_format": "latex", "page_idx": 34}, {"type": "text", "text": "where $\\mathbf{V}$ is the matrix of the top $k$ eigenvectors of the covariance matrix $\\Sigma$ , and $m$ is the batch size. ", "page_idx": 34}, {"type": "text", "text": "Since we have shown that the covariance matrices $\\mathbf{C}$ and $\\Sigma$ are equivalent, their eigenvectors $\\mathbf{W}$ and $\\mathbf{V}$ are also equivalent. Therefore, the reduced data matrix computed by the Batch PCA is equivalent to the one computed by the standard PCA. ", "page_idx": 34}, {"type": "text", "text": "In conclusion, we have proven that the Batch PCA algorithm achieves the same result as the standard PCA by showing that the covariance matrix and the reduced data matrix computed by the Batch PCA are equivalent to those computed by the standard PCA. \u53e3 ", "page_idx": 34}, {"type": "text", "text": "M Analyze the Practical Data Augmentations ", "text_level": 1, "page_idx": 34}, {"type": "text", "text": "Here we presents a rigorous mathematical proof demonstrating that higher data augmentation intensity leads to increased overlap between samples from different classes. By modeling the sample distributions and augmentation process, we show that the variance of the augmented distributions increases with augmentation strength, resulting in wider and more overlapping distributions. We provide an exact calculation of the intersection point and approximate the overlap area using the cumulative distribution function of the standard normal distribution. Our theoretical analysis confirms the positive correlation between data augmentation intensity and inter-class overlap rate. ", "page_idx": 34}, {"type": "text", "text": "M.1 Assumptions and Definitions ", "text_level": 1, "page_idx": 34}, {"type": "image", "img_path": "UPxmISfNCO/tmp/337cf4529ab75850aedea40266626de1e3afbd0234aefaaaac7f19229027460f.jpg", "img_caption": ["Figure 6: A scenario of similar views targeting different targets. In some cases, we may randomly crop two similar views from the original images as augmentations and input them into the machine learning model. This can lead to confusion in the model due to the differing targets assigned by humans or generated autonomously. "], "img_footnote": [], "page_idx": 35}, {"type": "text", "text": "Assumption 1 . There are two classes $C_{1}$ and $C_{2}$ , with samples drawn from probability distributions $P_{1}$ and $P_{2}$ , respectively. The data augmentation intensity is denoted by $\\alpha$ . ", "page_idx": 35}, {"type": "text", "text": "Definition 10 (Data Distribution and Augmentation) . The samples are satisfied $X_{1}\\sim P_{1}$ , $X_{2}\\sim P_{2}$ . The augmented samples are represented as $X_{1}^{\\prime}=X_{1}+\\epsilon$ and $X_{2}^{\\prime}=X_{2}+\\epsilon,$ , where $\\epsilon$ denotes the augmentation perturbation following the distribution $Q_{\\alpha}$ . The variance of $Q_{\\alpha}$ is proportional to $\\alpha$ . ", "page_idx": 35}, {"type": "text", "text": "M.2 Augmented Distributions ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "Let $P_{1,\\alpha}$ and $P_{2,\\alpha}$ denote the augmented sample distributions: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P_{1,\\alpha}(x^{\\prime})=(P_{1}*Q_{\\alpha})(x^{\\prime})}\\\\ {P_{2,\\alpha}(x^{\\prime})=(P_{2}*Q_{\\alpha})(x^{\\prime})}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "where $^*$ represents the convolution operation. ", "page_idx": 35}, {"type": "text", "text": "Variance of Augmented Distributions Let $\\sigma_{\\alpha}^{2}$ be the variance of $Q_{\\alpha}$ , with $\\sigma_{\\alpha}^{2}=k\\alpha$ , where $k$ is a constant. The variances of the augmented distributions are: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{\\sigma_{P_{1,\\alpha}}^{2}=\\sigma_{P_{1}}^{2}+\\sigma_{\\alpha}^{2}=\\sigma_{P_{1}}^{2}+k\\alpha}\\\\ {\\sigma_{P_{2,\\alpha}}^{2}=\\sigma_{P_{2}}^{2}+\\sigma_{\\alpha}^{2}=\\sigma_{P_{2}}^{2}+k\\alpha}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "Overlap Rate Definition Let $R$ denote the overlap region. The overlap rate $O(\\alpha)$ is defined as: ", "page_idx": 35}, {"type": "equation", "text": "$$\nO(\\alpha)=\\int_{R}P_{1,\\alpha}(x^{\\prime})\\,d x^{\\prime}+\\int_{R}P_{2,\\alpha}(x^{\\prime})\\,d x^{\\prime}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "M.3 Increased Variance Leads to Increased Overlap ", "text_level": 1, "page_idx": 35}, {"type": "text", "text": "As $\\alpha$ increases, $\\sigma_{\\alpha}^{2}$ increases, resulting in larger variances of $P_{1,\\alpha}$ and $P_{2,\\alpha}$ . This makes the distributions wider and more dispersed, increasing the overlap region. ", "page_idx": 35}, {"type": "text", "text": "Specific Derivation for One-Dimensional Gaussian Distributions Assume $P_{1}$ and $P_{2}$ are two Gaussian distributions: ", "page_idx": 35}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P_{1}(x)=\\mathcal{N}(\\mu_{1},\\sigma_{P_{1}}^{2})}\\\\ {P_{2}(x)=\\mathcal{N}(\\mu_{2},\\sigma_{P_{2}}^{2})}\\end{array}\n$$", "text_format": "latex", "page_idx": 35}, {"type": "text", "text": "The augmented distributions are: ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\begin{array}{r}{P_{1,\\alpha}(x^{\\prime})=\\mathcal{N}(\\mu_{1},\\sigma_{P_{1}}^{2}+k\\alpha)}\\\\ {P_{2,\\alpha}(x^{\\prime})=\\mathcal{N}(\\mu_{2},\\sigma_{P_{2}}^{2}+k\\alpha)}\\end{array}\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Exact Calculation of Intersection Point Equating the two Gaussian probability density functions and solving for $x$ : ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\frac{1}{\\sqrt{2\\pi(\\sigma_{P_{1}}^{2}+k\\alpha)}}\\exp\\left(-\\frac{(x-\\mu_{1})^{2}}{2(\\sigma_{P_{1}}^{2}+k\\alpha)}\\right)=\\frac{1}{\\sqrt{2\\pi(\\sigma_{P_{2}}^{2}+k\\alpha)}}\\exp\\left(-\\frac{(x-\\mu_{2})^{2}}{2(\\sigma_{P_{2}}^{2}+k\\alpha)}\\right)\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "Simplifying the equation yields an analytical solution for the intersection point. To simplify the analysis, we assume \u03c32P1 $\\sigma_{P_{1}}^{2}=\\sigma_{P_{2}}^{2}$ , in which case the intersection point is $(\\mu_{1}+\\mu_{2})/2$ . ", "page_idx": 36}, {"type": "text", "text": "Area of Overlap Region Let $\\Delta\\mu=|\\mu_{1}-\\mu_{2}|$ . The overlap region can be represented using the cumulative distribution function (CDF) of the standard normal distribution, denoted as $\\Phi(z)$ : ", "page_idx": 36}, {"type": "equation", "text": "$$\n\\Phi(z)={\\frac{1}{2}}\\left[1+\\operatorname{erf}\\left({\\frac{z}{\\sqrt{2}}}\\right)\\right]\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "The variance of the augmented distributions is $\\sigma_{\\alpha}^{2}=\\sigma_{P_{1}}^{2}+k\\alpha$ . Therefore, the approximate area of the overlap region is: ", "page_idx": 36}, {"type": "equation", "text": "$$\nO(\\alpha)\\approx2\\Phi\\left(\\frac{\\Delta\\mu}{\\sqrt{2(\\sigma_{P_{1}}^{2}+k\\alpha)}}\\right)-1\n$$", "text_format": "latex", "page_idx": 36}, {"type": "text", "text": "M.4 Conclusion ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Theorem 4 . As the data augmentation intensity $\\alpha$ increases, the overlap rate $O(\\alpha)$ between samples from different classes increases. ", "page_idx": 36}, {"type": "text", "text": "Proof. Increasing $\\alpha$ leads to an increase in the variance of the sample distributions, making them wider and more likely to overlap. ", "page_idx": 36}, {"type": "text", "text": "Specifically, increasing $\\alpha$ increases $\\sigma_{P_{1}}^{2}+k\\alpha$ in the denominator, thereby decreasing the argument of $\\Phi(\\cdot)$ . Since $\\Phi(z)$ increases as $z$ decreases for $z>0$ , $O(\\alpha)$ increases as $\\alpha$ increases. \u53e3 ", "page_idx": 36}, {"type": "text", "text": "In summary, we have rigorously proven the positive correlation between data augmentation intensity and inter-class overlap rate from a statistical and probabilistic perspective. ", "page_idx": 36}, {"type": "text", "text": "M.5 Empirical Analysis for Real-world Datasets ", "text_level": 1, "page_idx": 36}, {"type": "text", "text": "Beyond the theoretical analysis for a simple case, we also provide empirical analysis for four realworld datasets under different intensities of data augmentation. We utilize TSNE [61] to visualize the (augmented) samples in Figure 7. All the results demonstrate that: ", "page_idx": 36}, {"type": "text", "text": "1. as minscale increases from 0.02 to 0.5, the data points gradually change from being tightly clustered to more dispersed. This indicates that higher data augmentation intensity expands the range of sample distribution;   \n2. when minscale $=\\!0.02$ , the data points of the two classes exhibit significant overlap, and the boundary becomes blurred. In contrast, larger minscale values such as 0.2 and 0.5 allow for better separation between classes; ", "page_idx": 36}, {"type": "image", "img_path": "UPxmISfNCO/tmp/0a4438942b65bd64b34f3c0adac0facdf2e82efad3bd30979d958a21e2755553.jpg", "img_caption": ["Figure 7: Visualization of samples with varying levels of data augmentation intensity. We visualize the samples of 2 classes from CIFAR-10 in 2-dimensional space, respectively setting minscale in RandomResizeCrop as $\\left\\lbrace0.02,0.04,0.08,0.2,0.5\\right\\rbrace$ and without data augmentation. "], "img_footnote": [], "page_idx": 37}, {"type": "text", "text": "3. when data augmentation is not used (the last figure), there is a clear gap between the two classes, and the data points within each class are very compact. This suggests that the feature distribution of the original samples is relatively concentrated. ", "page_idx": 37}, {"type": "text", "text": "NeurIPS Paper Checklist ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "1. Claims ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 38}, {"type": "text", "text": "Justification: We show them in Section 1 and our Abstract. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the abstract and introduction do not include the claims made in the paper.   \n\u2022 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.   \n\u2022 The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.   \n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. ", "page_idx": 38}, {"type": "text", "text": "2. Limitations ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: Does the paper discuss the limitations of the work performed by the authors? ", "page_idx": 38}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Justification: We show them in Section 6. ", "page_idx": 38}, {"type": "text", "text": "Guidelines: ", "page_idx": 38}, {"type": "text", "text": "\u2022 The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.   \n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.   \n\u2022 The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.   \n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.   \n\u2022 The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.   \n\u2022 The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.   \n\u2022 If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.   \n\u2022 While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. ", "page_idx": 38}, {"type": "text", "text": "3. Theory Assumptions and Proofs ", "text_level": 1, "page_idx": 38}, {"type": "text", "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? ", "page_idx": 38}, {"type": "text", "text": "Answer: [No] ", "page_idx": 38}, {"type": "text", "text": "Justification: Most of them are proven in Appendix. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include theoretical results.   \n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.   \n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.   \n\u2022 The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.   \n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.   \n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced. ", "page_idx": 39}, {"type": "text", "text": "4. Experimental Result Reproducibility ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 39}, {"type": "text", "text": "Justification: We show the details of our techniques in Appendix. ", "page_idx": 39}, {"type": "text", "text": "Guidelines: ", "page_idx": 39}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.   \n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.   \n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.   \n\u2022 While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. ", "page_idx": 39}, {"type": "text", "text": "5. Open access to data and code ", "text_level": 1, "page_idx": 39}, {"type": "text", "text": "Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? ", "page_idx": 39}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We use the public datasets. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that paper does not include experiments requiring code.   \n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 While we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).   \n\u2022 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.   \n\u2022 The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.   \n\u2022 The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.   \n\u2022 At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).   \n\u2022 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. ", "page_idx": 40}, {"type": "text", "text": "6. Experimental Setting/Details ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 40}, {"type": "text", "text": "Justification: We show the experimental details in Appendix. Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments. \u2022 The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. \u2022 The full details can be provided either with the code, in appendix, or as supplemental material. ", "page_idx": 40}, {"type": "text", "text": "7. Experiment Statistical Significance ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? ", "page_idx": 40}, {"type": "text", "text": "Answer: [Yes] ", "text_level": 1, "page_idx": 40}, {"type": "text", "text": "Justification: We show them in Section 5. ", "page_idx": 40}, {"type": "text", "text": "Guidelines: ", "page_idx": 40}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.   \n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).   \n\u2022 The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)   \n\u2022 The assumptions made should be given (e.g., Normally distributed errors).   \n\u2022 It should be clear whether the error bar is the standard deviation or the standard error of the mean.   \n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a $96\\%$ CI, if the hypothesis of Normality of errors is not verified.   \n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).   \n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. ", "page_idx": 40}, {"type": "text", "text": "", "page_idx": 41}, {"type": "text", "text": "8. Experiments Compute Resources ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? ", "page_idx": 41}, {"type": "text", "text": "Answer: [No] ", "page_idx": 41}, {"type": "text", "text": "Justification: We provide most of them in Appendix. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not include experiments.   \n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.   \n\u2022 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.   \n\u2022 The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper). ", "page_idx": 41}, {"type": "text", "text": "9. Code Of Ethics ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? ", "page_idx": 41}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 41}, {"type": "text", "text": "Justification: The research presented in the paper adheres to all aspects of the NeurIPS Code of Ethics. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.   \n\u2022 If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.   \n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). ", "page_idx": 41}, {"type": "text", "text": "10. Broader Impacts ", "text_level": 1, "page_idx": 41}, {"type": "text", "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? ", "page_idx": 41}, {"type": "text", "text": "Answer: [No] ", "page_idx": 41}, {"type": "text", "text": "Justification: The paper does not comprehensively discuss the potential societal impacts, both positive and negative, of the work performed. ", "page_idx": 41}, {"type": "text", "text": "Guidelines: ", "page_idx": 41}, {"type": "text", "text": "\u2022 The answer NA means that there is no societal impact of the work performed.   \n\u2022 If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.   \n\u2022 Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.   \n\u2022 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.   \n\u2022 The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.   \n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). ", "page_idx": 41}, {"type": "text", "text": "", "page_idx": 42}, {"type": "text", "text": "11. Safeguards ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? ", "page_idx": 42}, {"type": "text", "text": "Answer: [No] ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Justification: The paper does not describe any safeguards for the responsible release of data or models that carry a high risk for misuse. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper poses no such risks.   \n\u2022 Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.   \n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.   \n\u2022 We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort. ", "page_idx": 42}, {"type": "text", "text": "12. Licenses for existing assets ", "text_level": 1, "page_idx": 42}, {"type": "text", "text": "Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? ", "page_idx": 42}, {"type": "text", "text": "Answer: [Yes] ", "page_idx": 42}, {"type": "text", "text": "Justification: All assets utilized in this paper, including code, data, and models, have been properly attributed to their respective creators or original owners. ", "page_idx": 42}, {"type": "text", "text": "Guidelines: ", "page_idx": 42}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not use existing assets.   \n\u2022 The authors should cite the original paper that produced the code package or dataset.   \n\u2022 The authors should state which version of the asset is used and, if possible, include a URL.   \n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.   \n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.   \n\u2022 If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.   \n\u2022 For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.   \n\u2022 If this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators. ", "page_idx": 42}, {"type": "text", "text": "", "page_idx": 43}, {"type": "text", "text": "13. New Assets ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: The paper does not introduce any new assets; therefore, the question of documentation is not applicable. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not release new assets.   \n\u2022 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.   \n\u2022 The paper should discuss whether and how consent was obtained from people whose asset is used.   \n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. ", "page_idx": 43}, {"type": "text", "text": "14. Crowdsourcing and Research with Human Subjects ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: The paper does not involve any crowdsourcing experiments or research with human subjects. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.   \n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. ", "page_idx": 43}, {"type": "text", "text": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects ", "text_level": 1, "page_idx": 43}, {"type": "text", "text": "Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? ", "page_idx": 43}, {"type": "text", "text": "Answer: [NA] ", "page_idx": 43}, {"type": "text", "text": "Justification: The paper does not involve research with human subjects. Therefore, the discussion of potential risks, disclosure to subjects, and Institutional Review Board (IRB) approvals are not applicable. ", "page_idx": 43}, {"type": "text", "text": "Guidelines: ", "page_idx": 43}, {"type": "text", "text": "\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.   \n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.   \n\u2022 We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.   \n\u2022 For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. ", "page_idx": 43}, {"type": "text", "text": "", "page_idx": 44}]